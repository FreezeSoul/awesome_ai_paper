## Updated on 2024.09.30
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#多模态>多模态</a></li>
    <li><a href=#6dof-object-pose>6DOF Object Pose</a></li>
    <li><a href=#nerf>nerf</a></li>
    <li><a href=#分类/检测/识别/分割>分类/检测/识别/分割</a></li>
    <li><a href=#生成模型>生成模型</a></li>
    <li><a href=#llm>LLM</a></li>
    <li><a href=#transformer>Transformer</a></li>
  </ol>
</details>

## 多模态

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-09-27**|[Image-guided topic modeling for interpretable privacy classification](http://arxiv.org/abs/2409.18674)|**[link](https://github.com/idiap/itm)**|用人类可理解的术语预测和解释图像中包含的隐私信息是一项复杂且依赖于上下文的的任务。即使对于大型语言模型来说，这项任务也具有挑战性。为了促进对隐私决策的理解，我们建议根据一组自然语言内容描述符来预测图像隐私。这些内容描述符与隐私分数相关联，这些分数反映了人们如何看待图像内容。我们使用我们新颖的图像引导主题建模（ITM）方法生成描述符。ITM 通过多模态对齐，利用来自视觉语言模型的视觉信息和图像文本描述。我们使用 ITM 生成的描述符来学习隐私预测器 Priv×ITM，其决策在设计上是可解释的。我们的 Priv×ITM 分类器在准确率方面比参考的可解释方法高出 5 个百分点，并且性能与当前最先进的不可解释模型相当。|
|**2024-09-26**|[LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness](http://arxiv.org/abs/2409.18125)|null|大型多模态模型 (LMM) 近期的进步极大地提高了其在 2D 视觉理解任务中的能力，使其能够有效地处理和理解图像和视频。然而，由于缺乏大规模 3D 视觉语言数据集和强大的 3D 编码器，具有 3D 感知能力的 LMM 在 3D 场景理解方面的开发一直受到阻碍。在本文中，我们介绍了一种简单而有效的框架，称为 LLaVA-3D。LLaVA-3D 利用 LLaVA 强大的 2D 理解先验知识，有效地将 LLaVA 应用于 3D 场景理解，而不会影响其 2D 理解能力。为了实现这一点，我们采用了一种简单有效的表示方法，即 3D Patch，它将 2D CLIP 图像块特征与其在 3D 空间中的对应位置连接起来。通过将 3D Patch 集成到 2D LMM 中，并采用联合 2D 和 3D 视觉语言指令微调，我们建立了一个用于 2D 图像理解和 3D 场景理解的统一架构。实验结果表明，在 3D 视觉语言数据集上训练时，LLaVA-3D 的收敛速度比现有 3D LMM 快 3.5 倍。此外，LLaVA-3D 不仅在各种 3D 任务上实现了最先进的性能，而且还保持了与 LLaVA 相当的 2D 图像理解和视觉语言对话能力。|
|**2024-09-26**|[EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions](http://arxiv.org/abs/2409.18042)|null|GPT-4o，一个能够进行带有不同情感和语调的语音对话的多模态模型，标志着多模态基础模型的一个里程碑。然而，在开源社区中，使用公开可用的数据赋予大型语言模型以端到端的方式感知和生成图像、文本和语音仍然具有挑战性。现有的视觉语言模型依赖于外部工具进行语音处理，而语音语言模型仍然存在视觉理解能力有限甚至没有的问题。为了解决这个问题，我们提出了EMOVA（情感无所不在的语音助手），它使大型语言模型具备端到端的语音能力，同时保持领先的视觉语言性能。利用语义-声学解耦的语音标记器，我们惊奇地发现，与相应的双模态对齐模型相比，多模态对齐可以进一步增强视觉语言和语音能力。此外，我们还提出了一个轻量级的风格模块，用于灵活控制语音风格（例如情感和音调）。EMOVA首次在视觉语言和语音基准测试中均实现了最先进的性能，同时支持具有生动情感的多模态语音对话。|
|**2024-09-26**|[DARE: Diverse Visual Question Answering with Robustness Evaluation](http://arxiv.org/abs/2409.18023)|null|视觉语言模型 (VLM) 扩展了仅文本大型语言模型和仅视觉模型的卓越能力，并且能够从多模态视觉文本输入中学习和处理。虽然现代 VLM 在许多标准图像分类和图像文本匹配任务中表现良好，但它们仍然难以应对许多关键的视觉语言 (VL) 推理能力，例如计数和空间推理。此外，虽然它们可能对指令和/或评估协议的微小变化非常脆弱，但现有基准测试未能评估它们的稳健性（或者更确切地说是缺乏稳健性）。为了将具有挑战性的 VL 场景与全面的稳健性评估相结合，我们引入了 DARE，即具有稳健性评估的多样化视觉问答，这是一个精心创建和策划的多项选择 VQA 基准。DARE 评估 VLM 在五个不同类别上的性能，并包括四个基于以下变化的面向稳健性的评估：提示、答案选项子集、输出格式和正确答案的数量。在一系列其他发现中，我们报告说，最先进的 VLM 仍然难以回答大多数类别中的问题，并且无法在测试的稳健性评估中始终如一地提供其峰值性能。选项子集的最差情况性能比标准情况下的性能低 34%。诸如 LLaVA 1.6 和 Idefics2 等开源 VLM 的稳健性无法与 GPT-4 和 Gemini 等闭源模型相提并论，但即使是后者仍然非常容易受到不同变化的影响。|
|**2024-09-26**|[The Hard Positive Truth about Vision-Language Compositionality](http://arxiv.org/abs/2409.17958)|null|多项基准测试得出结论，我们最好的视觉语言模型（例如 CLIP）缺乏组合性。给定一张图像，这些基准测试会探测模型从一组组合干扰项中识别其关联标题的能力。作为回应，最近涌现出大量提案，表明通过使用干扰项作为强负例对 CLIP 进行微调可以改进模型。我们的调查表明，这些改进实际上被严重夸大了——因为现有的基准测试没有探究微调后的视觉语言模型是否对强正例保持不变。通过使用 112,382 个强负例和强正例整理评估数据集，我们发现包含强正例会使 CLIP 的性能降低 12.9%，而人类则可以毫不费力地达到 99% 的准确率。使用强负例微调 CLIP 会导致更大的性能下降，高达 38.7%。基于这一发现，我们制作了一个包含 1,775,259 个图像文本的训练集，其中包含强负例和强正例标题。通过同时使用两者进行训练，我们看到现有基准测试的性能有所提高，同时强正例的性能也有所提高，这表明组合性得到了更稳健的改进。我们的工作表明，未来的研究需要严格测试和改进 CLIP 对相关“正”概念之间语义关系的理解。|
|**2024-09-26**|[A Multimodal Single-Branch Embedding Network for Recommendation in Cold-Start and Missing Modality Scenarios](http://arxiv.org/abs/2409.17864)|null|大多数推荐系统采用协同过滤 (CF) 并根据过去的集体交互提供推荐。因此，当可用交互很少或没有交互时，CF 算法的性能会下降，这种情况称为冷启动。为了解决这个问题，以前的工作依赖于利用协作数据和用户或项目辅助信息的模型。类似于多模态学习，这些模型旨在将协作和内容表示组合到共享嵌入空间中。在这项工作中，我们提出了一种新的多模态推荐技术，它依赖于用于推荐的多模态单分支嵌入网络 (SiBraR)。SiBraR 利用权重共享，在不同模态上使用相同的单分支嵌入网络对交互数据以及多模态辅助信息进行编码。这使得 SiBraR 在缺少模态的情况下（包括冷启动）非常有效。我们对来自三个不同推荐域（音乐、电影和电子商务）并提供多模态内容信息（音频、文本、图像、标签和交互）的大规模推荐数据集进行了广泛实验，结果表明，SiBraR 在冷启动场景下明显优于 CF 以及最先进的基于内容的 RS，并且在热启动场景下也具有竞争力。我们证明了 SiBraR 的推荐在缺少模态的情况下是准确的，并且该模型能够将不同的模态映射到共享嵌入空间的同一区域，从而减少了模态差距。|
|**2024-09-26**|[Cascade Prompt Learning for Vision-Language Model Adaptation](http://arxiv.org/abs/2409.17805)|null|提示学习已成为一种有效的方法，可以提高视觉语言模型 (VLM)（如 CLIP）在下游任务中的性能。然而，当前的可学习提示标记主要用于适应任务的单一阶段（即，调整提示），容易导致过拟合风险。在这项工作中，我们提出了一种新颖的级联提示学习 CasPL 框架，使提示学习能够同时服务于通用和特定专业知识（即，增强和调整提示）。具体来说，CasPL 是一种新的学习范式，包括两个不同阶段的可学习提示：第一个增强提示旨在通过使用大量未标记的域图像对齐其预测的 logits，从高级更大的 CLIP 教师模型中提取域通用知识。然后，第二个调整提示与冻结的第一组级联，以微调下游任务，遵循先前研究中采用的方法。通过这种方式，CasPL 可以有效地将域通用和任务特定表示捕获到明确不同的渐进提示组中，从而潜在地缓解目标域中的过拟合问题。值得注意的是，CasPL 作为一个即插即用的模块，可以无缝集成到任何现有的提示学习方法中。CasPL 在性能和推理速度之间实现了显著更好的平衡，这对于在资源受限的环境中部署较小的 VLM 模型特别有利。与先前最先进的方法 PromptSRC 相比，CasPL 在 11 个图像分类数据集上，基本类别平均提高了 1.85%，新类别平均提高了 3.44%，调和平均值平均提高了 2.72%。代码公开地址：https://github.com/megvii-research/CasPL。|
|**2024-09-26**|[Harnessing Shared Relations via Multimodal Mixup Contrastive Learning for Multimodal Classification](http://arxiv.org/abs/2409.17777)|null|深度多模态学习通过利用对比学习来捕捉模态之间显式的一对一关系，已经展现出显著的成果。然而，现实世界的数据往往表现出超越简单成对关联的共享关系。我们提出了M3CoL，一种多模态混合对比学习方法，用于捕捉多模态数据中固有的细微共享关系。我们的主要贡献是一种基于混合的对比损失函数，它通过将来自一种模态的混合样本与其来自其他模态的对应样本对齐来学习鲁棒的表示，从而捕捉它们之间的共享关系。对于多模态分类任务，我们引入了一个框架，该框架将融合模块与单模态预测模块相结合，以便在训练期间进行辅助监督，并辅以我们提出的基于混合的对比损失函数。通过对不同数据集（N24News、ROSMAP、BRCA 和 Food-101）的广泛实验，我们证明了 M3CoL 可以有效地捕捉共享的多模态关系并在不同领域泛化。它在 N24News、ROSMAP 和 BRCA 上的表现优于最先进的方法，同时在 Food-101 上取得了可比的性能。我们的工作突出了学习共享关系对于鲁棒的多模态学习的重要性，为未来的研究开辟了有希望的途径。|
|**2024-09-26**|[Robotic-CLIP: Fine-tuning CLIP on Action Data for Robotic Applications](http://arxiv.org/abs/2409.17727)|null|视觉语言模型在为各种机器人应用提取有意义的特征方面发挥了关键作用。其中，对比语言-图像预训练 (CLIP) 广泛应用于需要视觉和自然语言理解的机器人任务。然而，CLIP 仅在与文本提示配对的静态图像上进行训练，尚未完全适应涉及动态动作的机器人任务。在本文中，我们介绍了 Robotic-CLIP 来增强机器人的感知能力。我们首先收集和标记大规模动作数据，然后使用对比学习在 309,433 个视频（约 740 万帧）的动作数据上微调 CLIP，构建我们的 Robotic-CLIP。通过利用动作数据，Robotic-CLIP 继承了 CLIP 强大的图像性能，同时获得了理解机器人环境中动作的能力。大量实验表明，我们的 Robotic-CLIP 在各种语言驱动的机器人任务中优于其他基于 CLIP 的模型。此外，我们还展示了 Robotic-CLIP 在现实世界抓取应用中的实际有效性。|
|**2024-09-26**|[MIO: A Foundation Model on Multimodal Tokens](http://arxiv.org/abs/2409.17692)|null|本文介绍了一种基于多模态token的新型基础模型MIO，它能够以端到端、自回归的方式理解和生成语音、文本、图像和视频。尽管大型语言模型（LLM）和多模态大型语言模型（MM-LLM）凭借其多功能性推动了人工智能通用性的进步，但它们仍然缺乏真正的任意模态之间理解和生成的能力。最近，GPT-4o的发布展示了任意模态之间LLM在处理复杂现实世界任务方面的巨大潜力，它能够实现图像、语音和文本之间的全向输入和输出。然而，它是一个闭源模型，并且不支持生成多模态交错序列。为了解决这个问题，我们提出了MIO，它使用因果多模态建模在四种模态的离散token混合数据集上进行训练。MIO经历了四个训练阶段：（1）对齐预训练，（2）交错预训练，（3）语音增强预训练，以及（4）针对不同文本、视觉和语音任务的综合监督微调。我们的实验结果表明，与之前的双模态基线、任意模态之间模型基线，甚至是特定模态基线相比，MIO表现出具有竞争力的性能，在某些情况下甚至更胜一筹。此外，MIO还展示了其任意模态之间功能所带来的高级能力，例如交错视频文本生成、视觉思维链推理、视觉指南生成、指令图像编辑等。|
|**2024-09-26**|[P4Q: Learning to Prompt for Quantization in Visual-language Models](http://arxiv.org/abs/2409.17634)|null|大规模预训练的视觉语言模型（VLM）在各种视觉和多模态任务中取得了显著成果，但由于其对训练样本和计算资源的巨大需求，将VLM部署到下游应用平台仍然具有挑战性。对VLM进行微调和量化可以显著降低样本和计算成本，因此迫切需要这方面的研究。量化领域目前存在两种主要范式：量化感知训练（QAT）可以有效地量化大规模VLM，但会产生巨大的训练成本；而低比特位后训练量化（PTQ）则存在明显的性能下降问题。我们提出了一种平衡微调和量化的方法，称为“量化提示”（P4Q），其中我们设计了一种轻量级架构，利用对比损失监督来增强PTQ模型的识别性能。我们的方法可以有效地减少由低比特位量化引起的图像特征和文本特征之间的差距，其方法是基于可学习的提示来重组文本表示，并使用低比特位适配器重新调整图像和文本特征的分布。我们还引入了一种基于余弦相似度预测的蒸馏损失，以使用全精度教师模型对量化模型进行蒸馏。大量的实验结果表明，我们的P4Q方法优于现有技术，甚至可以达到与其全精度模型相当的结果。例如，我们的8位P4Q理论上可以将CLIP-ViT/B-32压缩4倍，同时在ImageNet数据集上实现66.94%的Top-1准确率，比可学习提示微调的全精度模型高出2.24%，而额外的参数可以忽略不计。||
|**2024-09-18**|[Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution](http://arxiv.org/abs/2409.12191)|**[link](https://github.com/qwenlm/qwen2-vl)**|我们推出了Qwen2-VL系列，这是对先前Qwen-VL模型的先进升级，它重新定义了视觉处理中传统的预定分辨率方法。Qwen2-VL引入了朴素动态分辨率机制，使模型能够将不同分辨率的图像动态处理成不同数量的视觉标记。这种方法允许模型生成更高效、更准确的视觉表示，与人类的感知过程紧密一致。该模型还集成了多模态旋转位置嵌入（M-RoPE），促进了文本、图像和视频中位置信息的有效融合。我们采用统一的范式来处理图像和视频，增强了模型的视觉感知能力。为了探索大型多模态模型的潜力，Qwen2-VL研究了大型视觉语言模型（LVLM）的缩放规律。通过扩展模型规模（包括2B、8B和72B参数的版本）和训练数据量，Qwen2-VL系列实现了极具竞争力的性能。值得注意的是，Qwen2-VL-72B模型在各种多模态基准测试中取得了与GPT-4o和Claude3.5-Sonnet等领先模型相当的结果，优于其他通用模型。代码可在\url{https://github.com/QwenLM/Qwen2-VL}获取。||
|**2024-09-18**|[GauTOAO: Gaussian-based Task-Oriented Affordance of Objects](http://arxiv.org/abs/2409.11941)|null|当您的机器人使用灵巧的手或抓手抓取物体时，它应该理解物体的面向任务的可操作性 (TOAO)，因为不同的任务通常需要关注物体的特定部分。为了应对这一挑战，我们提出了 GauTOAO，这是一个基于高斯的物体面向任务可操作性框架，它以零样本的方式利用视觉语言模型，在给定自然语言查询的情况下预测物体上与可操作性相关的区域。我们的方法引入了一种新的范式：“静态相机，移动物体”，使机器人在操作过程中能够更好地观察和理解手中的物体。GauTOAO 解决了现有方法的局限性，这些方法通常缺乏有效的空间分组，它使用 DINO 特征提取完整的 3D 物体掩码。然后，该掩码用于有条件地查询高斯分布，从而生成针对特定任务的、在物体上的精细语义分布。这种方法可以更准确地提取 TOAO，增强机器人对物体的理解并提高任务性能。我们通过现实世界实验验证了 GauTOAO 的有效性，证明了它能够泛化到各种任务。||
|**2024-09-18**|[LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Foundation Models](http://arxiv.org/abs/2409.11919)|null|视觉语言模型 (VLM) 在众多任务中都表现出色，但与其专用或微调模型相比，它们的零样本能力可能有限。然而，微调 VLM 存在局限性，因为它需要对模型架构和权重的“白盒”访问权限，以及设计微调目标和优化超参数的专业知识，这些都特定于每个 VLM 和下游任务。在这项工作中，我们提出了 LLM-wrapper，这是一种通过利用大型语言模型 (LLM) 来推理其输出，以“黑盒”方式调整 VLM 的新方法。我们通过指代表达理解 (REC) 证明了 LLM-wrapper 的有效性，这是一项需要空间和语义推理的具有挑战性的开放词汇任务。我们的方法显著提高了现成模型的性能，与经典微调相比获得了具有竞争力的结果。||
|**2024-09-17**|[NVLM: Open Frontier-Class Multimodal LLMs](http://arxiv.org/abs/2409.11402)|null|我们推出了 NVLM 1.0，这是一系列前沿的多模态大型语言模型 (LLM)，在视觉语言任务上取得了最先进的结果，可与领先的专有模型（例如 GPT-4o）和开放访问模型（例如 Llama 3-V 405B 和 InternVL 2）相媲美。 值得注意的是，NVLM 1.0 在多模态训练后，其纯文本性能优于其 LLM 骨干模型。 在模型设计方面，我们对仅解码器多模态 LLM（例如 LLaVA）和基于交叉注意力的模型（例如 Flamingo）进行了全面比较。 基于这两种方法的优缺点，我们提出了一种新颖的架构，可以提高训练效率和多模态推理能力。 此外，我们为基于图块的动态高分辨率图像引入了 1-D 图块标记设计，这显着提高了多模态推理和 OCR 相关任务的性能。 关于训练数据，我们精心策划并提供有关我们多模态预训练和监督微调数据集的详细信息。 我们的研究结果表明，即使在预训练阶段，在所有架构中，数据集质量和任务多样性都比规模更重要。 值得注意的是，我们为 NVLM-1.0 模型开发了生产级多模态，使其能够在视觉语言任务中表现出色，同时保持甚至改进与其 LLM 骨干模型相比的纯文本性能。 为此，我们将高质量的纯文本数据集与大量的多模态数学和推理数据一起制作并集成到多模态训练中，从而增强了跨模态的数学和编码能力。 为了推动该领域的  研究，我们将发布模型权重，并将开源代码供社区使用：https://nvlm-project.github.io/。||
|**2024-09-17**|[CAST: Cross-modal Alignment Similarity Test for Vision Language Models](http://arxiv.org/abs/2409.11007)|**[link](https://github.com/gautierdag/cast)**|视觉语言模型 (VLM) 通常通过视觉问答 (VQA) 任务进行评估，这些任务评估模型对场景的理解。良好的 VQA 性能被视为该模型能够在需要视觉和语言输入的更广泛任务中表现良好的证据。然而，场景感知 VQA 并不能完全捕捉输入偏差，也不能评估由模态之间错位引起的幻觉。为了解决这个问题，我们提出了跨模态对齐相似性测试 (CAST) 来探测 VLM 在不同模态之间的自洽性。该测试包括要求模型仅通过文本、仅通过图像或两者兼用来识别两个场景之间的相似性，然后评估它们生成的相似性的真实性。由于没有可供比较的真实情况，因此该评估的重点不是客观准确性，而是 VLM 在输出方面是否内部一致。我们认为，虽然并非所有自洽模型都具有能力或准确性，但所有有能力的 VLM 都必须是自洽的。||
|**2024-09-17**|[KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph](http://arxiv.org/abs/2409.10921)|**[link](https://github.com/yanbei-jiang/artwork-interpretation)**|Exploring the narratives conveyed by fine-art paintings is a challenge in image captioning, where the goal is to generate descriptions that not only precisely represent the visual content but also offer a in-depth interpretation of the artwork's meaning. The task is particularly complex for artwork images due to their diverse interpretations and varied aesthetic principles across different artistic schools and styles. In response to this, we present KALE Knowledge-Augmented vision-Language model for artwork Elaborations), a novel approach that enhances existing vision-language models by integrating artwork metadata as additional knowledge. KALE incorporates the metadata in two ways: firstly as direct textual input, and secondly through a multimodal heterogeneous knowledge graph. To optimize the learning of graph representations, we introduce a new cross-modal alignment loss that maximizes the similarity between the image and its corresponding metadata. Experimental results demonstrate that KALE achieves strong performance (when evaluated with CIDEr, in particular) over existing state-of-the-art work across several artwork datasets. Source code of the project is available at https://github.com/Yanbei-Jiang/Artwork-Interpretation.||
|**2024-09-16**|[Do Pre-trained Vision-Language Models Encode Object States?](http://arxiv.org/abs/2409.10488)|null|For a vision-language model (VLM) to understand the physical world, such as cause and effect, a first step is to capture the temporal dynamics of the visual world, for example how the physical states of objects evolve over time (e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMs pre-trained on web-scale data learn to encode object states, which can be extracted with zero-shot text prompts. We curate an object state recognition dataset ChangeIt-Frames, and evaluate nine open-source VLMs, including models trained with contrastive and generative objectives. We observe that while these state-of-the-art vision-language models can reliably perform object recognition, they consistently fail to accurately distinguish the objects' physical states. Through extensive experiments, we identify three areas for improvements for VLMs to better encode object states, namely the quality of object localization, the architecture to bind concepts to objects, and the objective to learn discriminative visual and language encoders on object states. Data and code are released.||
|**2024-09-16**|[CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera](http://arxiv.org/abs/2409.10441)|null|Camera-to-robot calibration is crucial for vision-based robot control and requires effort to make it accurate. Recent advancements in markerless pose estimation methods have eliminated the need for time-consuming physical setups for camera-to-robot calibration. While the existing markerless pose estimation methods have demonstrated impressive accuracy without the need for cumbersome setups, they rely on the assumption that all the robot joints are visible within the camera's field of view. However, in practice, robots usually move in and out of view, and some portion of the robot may stay out-of-frame during the whole manipulation task due to real-world constraints, leading to a lack of sufficient visual features and subsequent failure of these approaches. To address this challenge and enhance the applicability to vision-based robot control, we propose a novel framework capable of estimating the robot pose with partially visible robot manipulators. Our approach leverages the Vision-Language Models for fine-grained robot components detection, and integrates it into a keypoint-based pose estimation network, which enables more robust performance in varied operational conditions. The framework is evaluated on both public robot datasets and self-collected partial-view datasets to demonstrate our robustness and generalizability. As a result, this method is effective for robot pose estimation in a wider range of real-world manipulation scenarios.||
|**2024-09-16**|[HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping Using Vision-Language Models](http://arxiv.org/abs/2409.10419)|null|能够通过自然语言与人类交互的机器人可以解锁许多应用，例如参考抓取合成（RGS）。给定一个文本查询，RGS确定一个稳定的抓取姿态来操纵机器人工作空间中所指的对象。RGS包括两个步骤：视觉定位和抓取姿态估计。最近的研究利用强大的视觉语言模型（VLM）将自由流动的自然语言视觉定位到现实世界的机器人执行中。然而，在具有多个相同对象实例的复杂、杂乱环境中的比较仍然缺乏。本文介绍了HiFi-CS，它采用特征线性调制（FiLM）的分层应用来融合图像和文本嵌入，增强了机器人抓取中遇到的复杂属性丰富文本查询的视觉定位。视觉定位将二维/三维空间中的对象与自然语言输入相关联，并在两种情况下进行研究：封闭词汇和开放词汇。HiFi-CS具有一个轻量级的解码器，结合了一个冻结的VLM，在封闭词汇设置中优于竞争基线，同时尺寸缩小了100倍。我们的模型可以有效地指导像GroundedSAM这样的开放集目标检测器，以提高开放词汇性能。我们使用一个7自由度机械臂，通过真实的RGS实验验证了我们的方法，在15个桌面场景中实现了90.33%的视觉定位精度。我们在补充材料中包含了我们的代码库。||
|**2024-09-19**|[IRIS: Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis](http://arxiv.org/abs/2409.10078)|null|大型语言和视觉语言模型的最新进展显著增强了多模态理解，然而将高级语言指令转换为精确的3D空间机器人动作仍然具有挑战性。本文介绍了IRIS（交互式响应智能分割），这是一种用于3D功能分割的全新免训练多模态系统，以及一个用于评估日常环境中交互式语言引导功能的基准。IRIS将大型多模态模型与专门的3D视觉网络相结合，实现了2D和3D视觉理解与语言理解的无缝融合。为了便于评估，我们提供了一个包含10个典型室内环境的数据集，每个环境包含50张标注了物体动作和3D功能分割的图像。大量实验表明，IRIS能够处理各种环境下的交互式3D功能分割任务，并在各种指标上均展现出具有竞争力的性能。我们的结果突出了IRIS在增强基于复杂室内环境中功能理解的人机交互方面的潜力，推进了更直观、更高效的机器人系统在现实世界应用中的发展。||
|**2024-09-15**|[FSL-LVLM: Friction-Aware Safety Locomotion using Large Vision Language Model in Wheeled Robots](http://arxiv.org/abs/2409.09845)|null|轮腿式机器人在移动性和多功能性方面具有显著优势，但在湿滑地形上运行时面临着巨大挑战。这些机器人的传统基于模型的控制器假设没有滑动。虽然强化学习（RL）可以帮助四足机器人适应不同的表面，但从滑动中恢复仍然具有挑战性，特别是对于接触点较少的系统。估计地面摩擦系数是另一个开放的挑战。在本文中，我们提出了一种新颖的摩擦感知安全运动框架，该框架将大型视觉语言模型（LLM）与RL策略相结合。我们的方法将估计的摩擦系数明确纳入RL策略，使机器人能够在到达表面之前根据表面类型提前调整其行为。我们引入了一个“视觉摩擦”（FFV）模块，该模块利用LLM估计地面摩擦系数，从而无需大型数据集和大量训练。该框架在定制的轮式倒立摆上进行了验证，实验结果表明，我们的框架通过根据地形类型调整速度来提高完成驾驶任务的成功率，同时与基线方法相比实现了更好的跟踪性能。我们的框架可以轻松地与任何其他RL策略集成。||
|**2024-09-15**|[Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models](http://arxiv.org/abs/2409.09788)|null|尽管近期研究表明视觉语言模型 (VLM) 能够使用自然语言描述图像中的复杂关系，但其对物体大小和距离进行定量推理的能力仍有待探索。在这项工作中，我们引入了一个手动标注的基准测试集 Q-Spatial Bench，其中包含 271 个跨越五个类别的、专为定量空间推理设计的问题，并系统地研究了最先进的 VLM 在这项任务上的性能。我们的分析表明，推理物体之间的距离对 SoTA VLM 来说尤其具有挑战性；然而，一些 VLM 的性能明显优于其他模型，表现最好的两个模型之间存在超过 40 个百分点的差距。我们还惊奇地观察到，当响应中自然出现使用参考对象的推理路径时，性能最佳的 VLM 的成功率提高了 19 个百分点。受此观察结果的启发，我们开发了一种零样本提示技术 SpatialPrompt，该技术鼓励 VLM 使用参考对象作为视觉线索来回答定量空间问题。通过 SpatialPrompt 指导 VLM 在其推理路径中使用参考对象，Gemini 1.5 Pro、Gemini 1.5 Flash 和 GPT-4V 的成功率分别提高了 40、20 和 30 个百分点以上。我们强调，这些显著的改进无需更多数据、模型架构修改或微调即可实现。||
|**2024-09-15**|[Finetuning CLIP to Reason about Pairwise Differences](http://arxiv.org/abs/2409.09721)|**[link](https://github.com/dsam99/pc_clip)**|视觉语言模型 (VLM) 如 CLIP 是通过文本和图像对之间的对比学习进行训练的，从而产生对齐的图像和文本嵌入，这对许多下游任务非常有用。然而，CLIP 的一个显著缺点是，由此产生的嵌入空间似乎缺乏其纯文本替代方案所具有的一些结构。例如，长期以来，人们一直注意到文本嵌入可以使用向量算术来满足嵌入空间中的\emph{类比}，而 CLIP 则没有这种特性。在本文中，我们提出了一种以对比方式原生训练 CLIP 的方法，以便推理嵌入空间中的差异。我们对 CLIP 进行了微调，以便图像嵌入空间中的差异对应于\emph{图像差异的文本描述}，我们使用大型语言模型在图像-标题配对数据集上合成地生成了这些描述。我们首先证明，我们的方法在按特定属性对图像进行排序（例如，大象比猫大）方面产生了显著改进的能力，这在检索或构建基于属性的分类器中非常有用，并且提高了许多下游图像分类任务上的零样本分类性能。此外，我们的方法还实现了一种新的推理机制，我们将其称为比较提示，其中我们利用对感兴趣类别之间差异的文本描述的先验知识，在分类中实现了更大的性能提升。最后，我们说明了生成的嵌入在嵌入空间中遵循更大程度的几何特性，例如在文本到图像的生成中。||
|**2024-09-13**|[Interactive Masked Image Modeling for Multimodal Object Detection in Remote Sensing](http://arxiv.org/abs/2409.08885)|null|遥感影像中的目标检测在地球观测的各种应用中发挥着至关重要的作用。然而，与自然场景图像中的目标检测不同，这项任务特别具有挑战性，因为在不同的地形中存在大量的小型且通常难以察觉的目标。为了应对这些挑战，可以使用多模态学习来整合来自不同数据模态的特征，从而提高检测精度。然而，多模态学习的性能往往受到标记数据集大小的限制。在本文中，我们建议使用掩蔽图像建模（MIM）作为一种预训练技术，利用无标记数据的自监督学习来提高检测性能。然而，传统的MIM方法（如MAE）使用没有上下文信息的掩蔽标记，由于缺乏与图像其他部分的交互，难以捕捉到细粒度的细节。为了解决这个问题，我们提出了一种新的交互式MIM方法，可以在不同的标记之间建立交互，这对于遥感中的目标检测特别有利。大量的消融研究和评估证明了我们方法的有效性。||
|**2024-09-13**|[A Multimodal Approach for Fluid Overload Prediction: Integrating Lung Ultrasound and Clinical Data](http://arxiv.org/abs/2409.08790)|null|维持透析患者的体液平衡至关重要，因为管理不当会导致严重并发症。在本文中，我们提出了一种多模态方法，该方法整合了肺部超声图像的视觉特征和临床数据，以增强对体内多余液体预测的准确性。我们的框架采用独立的编码器来提取每种模态的特征，并通过跨域注意力机制将它们组合起来，以捕获互补信息。通过将预测构建为分类任务，该模型实现了比回归模型更好的性能。结果表明，多模态模型始终优于单模态模型，尤其是在注意力机制优先考虑表格数据时。伪样本生成进一步有助于缓解分类问题中的数据不平衡问题，实现了 88.31% 的最高准确率。这项研究强调了多模态学习对透析患者液体超负荷管理的有效性，为改善临床结果提供了宝贵的见解。||
|**2024-09-13**|[ChangeChat: An Interactive Model for Remote Sensing Change Analysis via Multimodal Instruction Tuning](http://arxiv.org/abs/2409.08582)|null|遥感 (RS) 变化分析通过检测图像随时间的变化来监测地球动态过程，至关重要。传统的变点检测擅长识别像素级的变化，但缺乏将这些变化置于背景中的能力。虽然最近在变化描述方面的进展提供了对变化的自然语言描述，但它们不支持交互式的、用户特定的查询。为了解决这些限制，我们引入了 ChangeChat，这是第一个专为 RS 变化分析设计的双时态视觉语言模型 (VLM)。ChangeChat 利用多模态指令微调，使其能够处理复杂的查询，例如变化描述、特定类别的量化和变化定位。为了提高模型的性能，我们开发了 ChangeChat-87k 数据集，该数据集是使用基于规则的方法和 GPT 辅助技术相结合生成的。实验表明，ChangeChat 为 RS 变化分析提供了一个全面、交互式的解决方案，在特定任务上的性能达到甚至优于最先进 (SOTA) 方法，并显着超过了最新的通用模型 GPT-4。代码和预训练权重可在 https://github.com/hanlinwu/ChangeChat 获取。||
|**2024-09-13**|[Generalization Boosted Adapter for Open-Vocabulary Segmentation](http://arxiv.org/abs/2409.08468)|null|视觉语言模型 (VLM) 已展现出卓越的开放词汇对象识别能力，这促使它们被应用于密集预测任务，例如分割。然而，由于缺乏像素级粒度以及可用于微调的数据有限，直接将 VLM 应用于此类任务仍然具有挑战性，导致过度拟合和泛化能力差。为了解决这些限制，我们提出了泛化增强适配器 (GBA)，这是一种新颖的适配器策略，可以增强 VLM 对开放词汇分割的泛化能力和鲁棒性。GBA 包含两个核心组件：(1) 风格多样化适配器 (SDA)，它将特征解耦为幅度和相位分量，仅对幅度进行操作以丰富特征空间表示，同时保持语义一致性；(2) 相关性约束适配器 (CCA)，它采用交叉注意力机制在文本类别和目标区域之间建立更紧密的语义关联，抑制不相关的低频“噪声”信息并避免错误关联。通过浅层 SDA 和深层 CCA 的协同效应，GBA 有效地缓解了过度拟合问题，并增强了特征表示的语义相关性。作为一个简单、高效、即插即用的组件，GBA 可以灵活地集成到各种基于 CLIP 的方法中，展现出广泛的适用性，并在多个开放词汇分割基准测试中实现了最先进的性能。||
|**2024-09-12**|[Rethinking Prompting Strategies for Multi-Label Recognition with Partial Annotations](http://arxiv.org/abs/2409.08381)|null|像 CLIP 这样的视觉语言模型 (VLM) 已被应用于部分标注的多标签识别 (MLR)，其方法是利用提示学习，为每个类别学习正负提示，以便将它们的嵌入与共享视觉文本特征空间中的类别存在或不存在相关联。虽然这种方法通过依赖 VLM 先验信息提高了 MLR 性能，但我们假设学习负面提示可能不是最优的，因为用于训练 VLM 的数据集缺乏明确关注类别缺失的图像-标题对。为了分析正负提示学习对 MLR 的影响，我们引入了 PositiveCoOp 和 NegativeCoOp，其中只有一个提示是在 VLM 指导下学习的，而另一个提示则被直接在共享特征空间中学习的嵌入向量所取代，而不依赖于文本编码器。通过实证分析，我们观察到负面提示会降低 MLR 性能，并且仅学习正面提示并结合学习到的负面嵌入（PositiveCoOp）优于双提示学习方法。此外，我们量化了提示学习相对于仅使用视觉特征的简单基线的性能优势，观察到当缺失标签的比例较低时，基线表现出与双提示学习方法 (DualCoOp) 相当的强劲性能，同时所需的训练计算量减少一半，参数数量减少 16 倍。||
|**2024-09-12**|[What Makes a Maze Look Like a Maze?](http://arxiv.org/abs/2409.08202)|null|人类视觉理解的一个独特之处在于能够灵活地解释抽象概念：获取解释其象征意义的提升规则，将它们应用于熟悉和不熟悉的语境，并对其进行预测或推理。虽然现成的视觉语言模型擅长对图像进行字面解释（例如，识别树枝等物体类别），但它们仍然难以理解此类视觉抽象概念（例如，树枝的排列方式如何形成迷宫的墙壁）。为了应对这一挑战，我们引入了深度模式基础（DSG），这是一个利用视觉抽象的显式结构化表示进行基础化和推理的框架。DSG 的核心是模式——抽象概念的依赖图描述，将它们分解成更原始级别的符号。DSG 使用大型语言模型来提取模式，然后使用视觉语言模型将模式的具体组件到抽象组件分层地基础化到图像上。基础化的模式用于增强视觉抽象理解。我们在新的视觉抽象数据集上系统地评估了 DSG 和不同的推理方法，该数据集包含各种现实世界中抽象概念的图像以及由人类标记的相应问答对。我们表明，DSG 显着提高了视觉语言模型的抽象视觉推理性能，并且是朝着人类一致的视觉抽象理解迈出的一步。||
|**2024-09-13**|[A Comprehensive Survey on Deep Multimodal Learning with Missing Modality](http://arxiv.org/abs/2409.07825)|null|在多模态模型训练和推理过程中，由于传感器限制、成本限制、隐私问题、数据丢失以及时间和空间因素，数据样本可能会缺少某些模态，从而导致模型性能下降。本综述概述了缺失模态的多模态学习 (MLMM) 的最新进展，重点关注深度学习技术。它是第一个涵盖历史背景和 MLMM 与标准多模态学习设置之间区别的综合性综述，然后详细分析了当前的 MLMM 方法、应用和数据集，最后讨论了该领域的挑战和潜在的未来方向。||
|**2024-09-12**|[Top-down Activity Representation Learning for Video Question Answering](http://arxiv.org/abs/2409.07748)|null|从原子动作（例如，拿起一个礼物，移动到沙发，打开礼物）到上下文事件（例如，庆祝圣诞节）捕捉复杂的分层人类活动对于实现高性能视频问答 (VideoQA) 至关重要。 最近的工作已经扩展了多模态模型（例如，CLIP，LLaVA）来处理连续视频序列，增强了模型的时间推理能力。 然而，这些方法通常无法捕捉可以分解为多个原子动作的上下文事件，这些动作非连续地分布在相对长期的序列中。 在本文中，为了利用 CLIP 模型的空间视觉上下文表示能力来获得视频中上下文事件方面的非连续视觉表示，我们将长期视频序列转换为空间图像域，并针对 VideoQA 任务微调多模态模型 LLaVA。 我们的方法在 STAR 任务上取得了具有竞争力的性能，特别是在 NExTQA 任务上，获得了 78.4% 的准确率，超过了当前最先进的得分 2.8 个百分点。||
|**2024-09-12**|[DSBench: How Far Are Data Science Agents to Becoming Data Science Experts?](http://arxiv.org/abs/2409.07703)|**[link](https://github.com/liqiangjing/dsbench)**|大型语言模型（LLM）和大型视觉语言模型（LVLM）已经展现出令人印象深刻的语言/视觉推理能力，引发了构建针对特定应用（如购物助手或AI软件工程师）的代理的最新趋势。最近，许多数据科学基准测试被提出，以研究其在数据科学领域的性能。然而，现有的数据科学基准测试与现实世界的数据科学应用相比仍然存在不足，因为它们的设置过于简化。为了弥合这一差距，我们引入了 DSBench，这是一个全面的基准测试，旨在评估具有现实任务的数据科学代理。该基准测试包括 466 个数据分析任务和 74 个数据建模任务，这些任务来自 Eloquence 和 Kaggle 竞赛。DSBench 通过包含长上下文、多模态任务背景、对大型数据文件和多表结构进行推理以及执行端到端数据建模任务，提供了一个真实的设置。我们对最先进的 LLM、LVLM 和代理的评估表明，它们难以完成大多数任务，最好的代理仅能解决 34.12% 的数据分析任务，并实现了 34.74% 的相对性能差距 (RPG)。这些发现强调了进一步发展更实用、更智能、更自主的数据科学代理的必要性。||
|**2024-09-12**|[Open-Vocabulary Remote Sensing Image Semantic Segmentation](http://arxiv.org/abs/2409.07683)|null|开放词汇图像语义分割 (OVS) 旨在将图像分割成跨开放类别集的语义区域。现有的 OVS 方法通常依赖于基础视觉语言模型，并利用相似度计算来处理 OVS 任务。然而，这些方法主要针对自然图像量身定制，难以应对遥感图像的独特特征，例如快速变化的方向和显著的尺度变化。这些挑战使地球视觉中的 OVS 任务变得复杂，需要专门的方法。为了解决这一难题，我们借鉴了独特的遥感特征，提出了第一个专门为遥感图像设计的 OVS 框架。特别是，为了解决不同的方向问题，我们引入了一种旋转聚合相似度计算模块，该模块生成方向自适应相似度图作为初始语义图。随后，这些图会在空间和类别级别进行细化，以生成更准确的语义图。此外，为了管理显著的尺度变化，我们将多尺度图像特征集成到上采样过程中，从而得到最终的尺度感知语义掩码。为了推进地球视觉中的 OVS 并鼓励可重复研究，我们建立了第一个用于遥感图像的开源 OVS 基准，包括四个公共遥感数据集。在这个基准上的大量实验表明，我们提出的方法达到了最先进的性能。所有代码和数据集都可以在 https://github.com/caoql98/OVRS 获取。||
|**2024-09-11**|[Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks](http://arxiv.org/abs/2409.07353)|**[link](https://github.com/speedlab-git/robust-encoder-against-jailbreak-attack)**|基于多模态大数据集训练的大型视觉语言模型 (LVLM) 在视觉语言任务方面表现出色，极大地推进了人工智能的发展。然而，这些模型仍然容易受到对抗性攻击，尤其是越狱攻击，这些攻击会绕过安全协议，导致模型生成误导性或有害的响应。这种脆弱性源于大型语言模型 (LLM) 固有的敏感性以及视觉模态引入的扩大攻击面。我们提出了 Sim-CLIP+，这是一种新颖的防御机制，它利用 Siamese 架构通过对抗性微调 CLIP 视觉编码器。这种方法最大限度地提高了扰动样本和干净样本之间的余弦相似度，增强了对对抗性操作的抵抗力。Sim-CLIP+ 提供了一种即插即用的解决方案，允许作为强大的视觉编码器无缝集成到现有的 LVLM 架构中。与以前的防御措施不同，我们的方法不需要对 LVLM 进行结构修改，并且计算开销最小。Sim-CLIP+ 证明了其对基于梯度的对抗性攻击和各种越狱技术的有效性。我们针对三种不同的越狱攻击策略评估了 Sim-CLIP+，并使用标准下游数据集（包括用于图像字幕的 COCO 和用于视觉问答的 OKVQA）执行了干净评估。大量实验表明，Sim-CLIP+ 在保持高清洁精度的同时，显着提高了对基于梯度的对抗性攻击和越狱技术的鲁棒性。我们的代码和强大的视觉编码器可在 https://github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git 获取。||
|**2024-09-11**|[MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D Features as Text Tokens for Autonomous Driving](http://arxiv.org/abs/2409.07267)|**[link](https://github.com/emzucas/minidrive)**|视觉语言模型 (VLM) 作为自动驾驶中的通用端到端模型，通过问答交互执行预测、规划和感知等子任务。然而，大多数现有方法依赖于计算成本高昂的视觉编码器和大型语言模型 (LLM)，这使得它们难以部署在现实世界场景和实时应用程序中。同时，大多数现有 VLM 缺乏处理多图像的能力，难以适应自动驾驶中的多摄像头感知。为了解决这些问题，我们提出了一种名为 MiniDrive 的新型框架，该框架结合了我们提出的特征工程混合专家 (FE-MoE) 模块和动态指令适配器 (DI-Adapter)。FE-MoE 在输入语言模型之前，将 2D 特征有效地映射到视觉标记嵌入中。DI-Adapter 使视觉标记嵌入能够随指令文本嵌入动态变化，解决了以往方法中同一图像的静态视觉标记嵌入问题。与之前的工作相比，MiniDrive 在参数大小、浮点运算和响应效率方面实现了最先进的性能，最小版本仅包含 83M 参数。||
|**2024-09-11**|[MVLLaVA: An Intelligent Agent for Unified and Flexible Novel View Synthesis](http://arxiv.org/abs/2409.07129)|null|本文介绍了MVLLaVA，这是一种专为新视角合成任务设计的智能代理。MVLLaVA将多个多视图扩散模型与大型多模态模型LLaVA相结合，使其能够高效地处理各种任务。MVLLaVA代表了一个通用且统一的平台，可以适应不同的输入类型，包括单个图像、描述性标题或视角方位角的特定变化，并以语言指令指导视角生成。我们精心设计了特定于任务的指令模板，随后用于微调LLaVA。因此，MVLLaVA获得了根据用户指令生成新视角图像的能力，展示了其在不同任务中的灵活性。实验验证了MVLLaVA的有效性，证明了其在应对各种新视角合成挑战时的强大性能和多功能性。||
|**2024-09-11**|[FSMDet: Vision-guided feature diffusion for fully sparse 3D detector](http://arxiv.org/abs/2409.06945)|null|近年来，全稀疏三维目标检测引起了越来越多的关注。然而，这些框架中特征的稀疏性由于扩散过程有限，对候选框的生成提出了挑战。此外，对效率的追求导致对视觉辅助的全稀疏模型的研究很少。在本文中，我们提出了FSMDet（全稀疏多模态检测），它使用视觉信息来指导激光雷达特征扩散过程，同时仍然保持管道的效率。具体来说，大多数全稀疏工作都集中在复杂的定制中心融合扩散/回归算子上。然而，我们观察到，如果执行了适当的目标补全，即使是最简单的插值算子也能得到令人满意的结果。受此观察的启发，我们将视觉引导的扩散过程分为两个模块：形状恢复层（SRLayer）和自扩散层（SDLayer）。前者使用RGB信息来恢复物体可见部分的形状，后者使用视觉先验将特征进一步扩散到中心区域。实验表明，我们的方法成功地提高了以往仅使用激光雷达的全稀疏模型的性能，并在多模态模型中达到了SOTA性能。同时，由于采用了稀疏架构，我们的方法在推理过程中比以往的SOTA方法效率最高可提高5倍。||
|**2024-09-10**|[ExIQA: Explainable Image Quality Assessment Using Distortion Attributes](http://arxiv.org/abs/2409.06853)|null|盲图像质量评估 (BIQA) 旨在开发无需参考图像即可估计图像质量分数的方法。在本文中，我们从失真识别角度探讨 BIQA，主要目标是利用视觉语言模型 (VLM)（如 CLIP）预测失真类型和强度，因为它们具有广泛的知识和泛化能力。基于这些预测的失真，我们然后估计图像的质量分数。为此，我们提出了一种基于属性学习的可解释失真识别方法。我们没有使用失真名称提示 VLM，而是使用失真的属性或影响提示它们，并汇总这些信息以推断失真强度。此外，我们为每张图像考虑了多种失真，使我们的方法更具可扩展性。为此，我们生成了一个包含 100,000 张图像的数据集，用于高效训练。最后，检索属性概率并将其输入回归器以预测图像质量分数。结果表明，我们的方法除了具有可解释性和透明度外，还在多个数据集的 PLCC 和 SRCC 指标上均达到了最先进 (SOTA) 的性能。此外，零样本结果证明了该方法的泛化能力。||
|**2024-09-10**|[MAGDA: Multi-agent guideline-driven diagnostic assistance](http://arxiv.org/abs/2409.06351)|null|在急诊科、乡村医院或欠发达地区的诊所，临床医生往往缺乏训练有素的放射科医生进行快速图像分析，这可能对患者的医疗保健产生不利影响。大型语言模型 (LLM) 有可能通过提供有助于临床医生做出决策的见解，从而减轻他们的一些压力。虽然这些 LLM 在医学考试中取得了很高的测试成绩，展示了其丰富的理论医学知识，但它们往往不遵循医学指南。在这项工作中，我们介绍了一种新的零样本指南驱动决策支持方法。我们模拟了一个由多个 LLM 代理组成的系统，该系统增强了对比视觉语言模型，这些代理协作以达成患者诊断。在向代理提供简单的诊断指南后，他们将根据这些指南合成提示并筛选图像以查找结果。最后，他们为自己的诊断提供易于理解的思维链推理，然后对其进行自我完善，以考虑疾病之间的相互依赖性。由于我们的方法是零样本的，因此它适用于罕见疾病的设置，在这些情况下，训练数据有限，但可以使用专家制定的疾病描述。我们在两个胸部 X 光数据集 CheXpert 和 ChestX-ray 14 Longtail 上评估了我们的方法，展示了其相对于现有零样本方法的性能改进以及对罕见疾病的泛化能力。||
|**2024-09-10**|[INTRA: Interaction Relationship-aware Weakly Supervised Affordance Grounding](http://arxiv.org/abs/2409.06210)|null|可供性是指物体固有的潜在交互方式。对可供性的感知可以让智能体高效地在新环境中导航和交互。弱监督可供性基础可以让智能体在没有昂贵的像素级标注的情况下学习可供性的概念，但需要使用以环境为中心的图像。尽管弱监督可供性基础的最新进展已经取得了可喜的成果，但仍然存在一些挑战，例如需要配对的以环境为中心和以自我为中心的图像数据集，以及为单个物体基础多种可供性的复杂性。为了解决这些问题，我们提出了交互关系感知的弱监督可供性基础 (INTRA)。与现有技术不同，INTRA 将这个问题重新定义为表征学习，通过仅使用以环境为中心的图像进行对比学习来识别交互的独特特征，从而消除了对配对数据集的需求。此外，我们利用视觉语言模型嵌入来灵活地使用任何文本进行可供性基础，设计了以文本为条件的可供性映射生成，以反映交互关系以进行对比学习，并通过我们的文本同义词增强来增强鲁棒性。我们的方法在 AGD20K、IIT-AFF、CAD 和 UMD 等不同的数据集上优于现有技术。此外，实验结果表明，我们的方法对合成图像/插图具有显著的领域可扩展性，并且能够对新的交互和物体进行可供性基础。||
|**2024-09-10**|[Revisiting Prompt Pretraining of Vision-Language Models](http://arxiv.org/abs/2409.06166)|null|提示学习是一种有效的定制视觉语言模型 (VLM) 以适应各种下游任务的方法，它仅需微调输入提示词符的少量参数。近年来，在大规模数据集（例如 ImageNet-21K）上进行提示预训练已成为通用视觉识别提示学习的关键。然而，我们重新审视并观察到，在提示预训练期间，鉴于图像数量庞大，有限的可学习提示可能会面临欠拟合的风险，同时导致泛化能力较差。为了解决上述问题，本文提出了一种名为“重新审视提示预训练”（RPP）的通用框架，旨在从提示结构和提示监督两个方面提高拟合和泛化能力。对于提示结构，我们打破了查询、键和值向量均来自共享的可学习提示词符的常见做法的限制。相反，我们引入了非共享的独立查询、键和值可学习提示，从而通过增加参数多样性来增强模型的拟合能力。对于提示监督，我们还利用了由预训练的对比语言图像预训练 (CLIP) 教师模型提供的零样本概率预测得到的软标签。这些软标签可以更细致、更全面地洞察类间关系，从而赋予预训练过程更好的泛化能力。RPP 产生更稳健的提示初始化，增强其在各种视觉识别任务中的鲁棒迁移能力。跨多个基准的实验一致证实了我们预训练提示的最新性能。代码和模型将很快发布。||
|**2024-09-09**|[PEERNet: An End-to-End Profiling Tool for Real-Time Networked Robotic Systems](http://arxiv.org/abs/2409.06078)|**[link](https://github.com/utaustin-swarmlab/peernet)**|网络机器人系统在自动驾驶汽车、无人机群和远程手术等应用中需要平衡计算、功耗和延迟约束。该领域的核心问题是何时将计算量大的任务卸载到云端（远程服务器）以换取通信延迟。任务卸载算法通常依赖于对系统特定性能指标的精确了解，例如传感器数据速率、网络带宽和机器学习模型延迟。虽然这些指标可以在系统设计期间进行建模，但连接质量、服务器负载和硬件条件的不确定性会导致实时性能变化，从而影响整体性能。我们推出了 PEERNet，这是一种用于云机器人的端到端实时分析工具。PEERNet 通过对传感器、网络、深度学习管道和设备等系统组件进行有针对性但自适应的分析，从而能够在异构硬件上进行性能监控。我们通过网络机器人任务展示了 PEERNet 的功能，例如基于图像的 Franka Emika Panda 机械臂远程操作和使用 Nvidia Jetson Orin 查询视觉语言模型。PEERNet 揭示了机器人系统中非直观的的行为，例如非对称网络传输和双峰语言模型输出。我们的评估强调了网络机器人中基准测试的有效性和重要性，证明了 PEERNet 的适应性。我们的代码是开源的，可在 github.com/UTAustin-SwarmLab/PEERNet 获取。||
|**2024-09-07**|[Unlocking Potential Binders: Multimodal Pretraining DEL-Fusion for Denoising DNA-Encoded Libraries](http://arxiv.org/abs/2409.05916)|null|在药物发现领域，DNA 编码化合物库 (DEL) 筛选技术已成为识别高亲和力化合物的有效方法。然而，DEL 筛选面临着一个重大挑战：复杂生物系统中非特异性相互作用产生的噪声。在 DEL 库上训练的神经网络已被用于提取化合物特征，旨在对数据进行去噪并发现潜在的治疗靶点结合剂。然而，DEL 的固有结构受限于结构单元的有限多样性，这影响了化合物编码器的性能。此外，现有方法仅在单一级别捕获化合物特征，进一步限制了去噪策略的有效性。为了缓解这些问题，我们提出了一种多模态预训练 DEL-Fusion 模型 (MPDF)，该模型通过预训练增强编码器能力，并在不同尺度上整合化合物特征。我们开发了在不同化合物表示及其文本描述之间应用对比目标的预训练任务，增强了化合物编码器获取通用特征的能力。此外，我们提出了一种新颖的 DEL-fusion 框架，该框架融合了原子、亚分子和分子水平的化合物信息，这些信息由各种化合物编码器捕获。这些创新的协同作用使 MPDF 具备丰富的多尺度特征，从而实现全面的下游去噪。在三个 DEL 数据集上进行的评估表明，MPDF 在验证任务的数据处理和分析方面表现出优异的性能。值得注意的是，MPDF 为识别高亲和力分子提供了新的见解，为改进 DEL 在药物发现中的应用铺平了道路。||
|**2024-09-09**|[DexDiff: Towards Extrinsic Dexterity Manipulation of Ungraspable Objects in Unrestricted Environments](http://arxiv.org/abs/2409.05493)|null|抓取又大又平的物体（例如书或平底锅）通常被认为是一项无法完成的任务，因为抓取姿势无法企及，这带来了重大挑战。以前的工作利用墙壁或桌子边缘等外部灵活性来抓取此类物体。然而，它们仅限于特定于任务的策略，并且缺乏寻找预抓取条件的任务规划。这使得适应各种环境和外部灵活性约束变得困难。因此，我们提出了 DexDiff，一种用于具有外部灵活性的长视野规划的稳健机器人操作方法。具体来说，我们利用视觉语言模型 (VLM) 来感知环境状态并生成高级任务计划，然后使用目标条件动作扩散 (GCAD) 模型来预测低级动作序列。该模型从离线数据中学习低级策略，并将高级规划引导的累积奖励作为目标条件，从而可以改进对机器人动作的预测。实验结果表明，我们的方法不仅可以有效地执行无法完成的任务，而且可以泛化到以前从未见过的物体。它在模拟中的成功率比基线高 47%，并有助于在现实场景中高效部署和操作。||
|**2024-09-08**|[PIP: Detecting Adversarial Examples in Large Vision-Language Models via Attention Patterns of Irrelevant Probe Questions](http://arxiv.org/abs/2409.05076)|**[link](https://github.com/btzyd/pip)**|大型视觉语言模型 (LVLM) 已经展示出强大的多模态能力。然而，它们也面临着严重的安全问题，因为攻击者可以通过精心设计的对抗样本在 LVLM 中引发鲁棒性问题。因此，LVLM 迫切需要针对对抗样本的检测工具，以防止出现错误响应。在这项工作中，我们首先发现，当使用探测问题时，LVLM 对干净图像表现出规律的注意力模式。我们提出了一种名为 PIP 的非常规方法，它利用一个随机选择的无关探测问题（例如，“有钟表吗？”）的注意力模式来区分对抗样本和干净样本。无论待测图像及其对应的问题是什么，PIP 只需要对待测图像和探测问题进行一次额外的推理，即可成功检测对抗样本。即使在黑盒攻击和开放数据集场景下，我们的 PIP 与简单的 SVM 相结合，仍然可以实现超过 98% 的召回率和超过 90% 的精确率。我们的 PIP 是首次尝试通过简单的无关探测问题来检测针对 LVLM 的对抗攻击，为更深入地理解和反思 LVLM 提供了思路。代码可在 https://github.com/btzyd/pip 获取。||
|**2024-09-07**|[POINTS: Improving Your Vision-language Model with Affordable Strategies](http://arxiv.org/abs/2409.04828)|null|近年来，视觉语言模型取得了重大进展，在光学字符识别和几何问题解决等任务中表现出色。然而，仍然存在几个关键问题：1）专有模型的架构往往缺乏透明度，而开源模型需要对其训练策略进行更详细的消融研究。2）开源工作中的预训练数据尚未得到充分探索，数据集是根据经验添加的，这使得过程变得繁琐。3）微调通常侧重于添加数据集，导致收益递减。为了解决这些问题，我们提出以下贡献：1）我们使用视觉语言模型的最新进展训练了一个强大的基线模型，引入了有效的改进，并对每种技术进行了全面的消融和验证。2）受近期大型语言模型工作的启发，我们使用困惑度对预训练数据进行过滤，选择困惑度最低的数据进行训练。这种方法使我们能够在精选的 1M 数据集上进行训练，并取得了具有竞争力的性能。3）在视觉指令微调期间，当添加更多数据集的收益微乎其微时，我们对不同数据集使用了模型融合。这些创新产生了一个 9B 参数的模型，其性能与最先进的模型相比具有竞争力。我们的策略高效且轻量级，因此社区很容易采用。||
|**2024-09-07**|[Enhancing Outlier Knowledge for Few-Shot Out-of-Distribution Detection with Extensible Local Prompts](http://arxiv.org/abs/2409.04796)|null|分布外 (OOD) 检测旨在区分已知类别之外的异常值，在实际场景中已变得越来越重要。近年来，视觉语言模型 (VLM) 的出现激发了人们对通过少量样本微调来增强 VLM 的 OOD 检测的兴趣。然而，现有方法主要侧重于优化全局提示，而忽略了对异常值的局部信息的精细利用。基于此，我们冻结全局提示，并引入了一种新颖的从粗到精的微调范式，以强调使用局部提示进行区域增强。我们的方法包括两个组成部分：全局提示引导的负增强和局部提示增强的区域正则化。前者利用冻结的、粗略的全局提示作为指导线索来合并负增强，从而利用局部异常值知识。后者采用可训练的局部提示和区域正则化来有效地捕获局部信息，从而帮助识别异常值。我们还提出了区域相关指标，以增强 OOD 检测的丰富性。此外，由于我们的方法仅探索增强局部提示，因此可以在推理过程中与训练好的全局提示无缝集成，以提高性能。综合实验结果证明了我们方法的有效性和潜力。值得注意的是，在 ImageNet-1k 数据集上进行的 4 次样本微调中，我们的方法相对于最先进的方法将平均 FPR95 降低了 5.17%，甚至优于先前方法的 16 次样本微调结果。||
|**2024-09-06**|[COLUMBUS: Evaluating COgnitive Lateral Understanding through Multiple-choice reBUSes](http://arxiv.org/abs/2409.04053)|null|虽然视觉问答 (VQA) 基准测试推动了推理技术的发展，但它们一直专注于垂直思维。有效的解决问题还需要横向思维，而横向思维在人工智能领域仍未得到充分研究，也没有用于测试视觉感知系统。为了弥合这一差距，我们将视觉横向思维形式化为一个多项选择题问答任务，并描述了一个由分类法驱动的三步法来实例化任务示例。然后，我们开发了 COLUMBUS，这是一个合成基准测试，它应用任务管道，根据公开可用的化合物和常用短语集合，创建带有文本和图标字谜的 QA 集。COLUMBUS 包含超过 1,000 个谜题，每个谜题有四个候选答案。虽然最先进的视觉语言模型 (VLM) 取得了不错的性能，但我们的评估表明人类和模型之间存在巨大差距。VLM 受益于人工策划的描述，但在正确的抽象级别上难以自行生成此类表示。||
|**2024-09-06**|[Generating Faithful and Salient Text from Multimodal Data](http://arxiv.org/abs/2409.03961)|**[link](https://github.com/TahsinaHashem/FaithD2T)**|虽然大型多模态模型 (LMM) 在许多多模态任务中取得了良好的性能，但它们在生成文本时仍可能会出现幻觉。它们在从视觉数据中检测显著特征方面的性能也不清楚。在本文中，我们开发了一个框架，用于从混合模态数据（包括图像和结构化数据（以知识图谱或表格表示））生成忠实且显著的文本。具体来说，我们训练了一个小型视觉评论家模型，用于从图像模态中识别幻觉和非显著特征。评论家模型还会生成显著图像特征列表。此信息用于后期编辑步骤，以提高生成质量。在两个数据集上的实验表明，我们的框架提高了 LMM 在忠实度和显著性方面的生成质量，优于最近旨在减少幻觉的技术。||
|**2024-09-05**|[Few-shot Adaptation of Medical Vision-Language Models](http://arxiv.org/abs/2409.03868)|**[link](https://github.com/fereshteshakeri/few-shot-medvlms)**|Integrating image and text data through multi-modal learning has emerged as a new approach in medical imaging research, following its successful deployment in computer vision. While considerable efforts have been dedicated to establishing medical foundation models and their zero-shot transfer to downstream tasks, the popular few-shot setting remains relatively unexplored. Following on from the currently strong emergence of this setting in computer vision, we introduce the first structured benchmark for adapting medical vision-language models (VLMs) in a strict few-shot regime and investigate various adaptation strategies commonly used in the context of natural images. Furthermore, we evaluate a simple generalization of the linear-probe adaptation baseline, which seeks an optimal blending of the visual prototypes and text embeddings via learnable class-wise multipliers. Surprisingly, such a text-informed linear probe yields competitive performances in comparison to convoluted prompt-learning and adapter-based strategies, while running considerably faster and accommodating the black-box setting. Our extensive experiments span three different medical modalities and specialized foundation models, nine downstream tasks, and several state-of-the-art few-shot adaptation methods. We made our benchmark and code publicly available to trigger further developments in this emergent subject: \url{https://github.com/FereshteShakeri/few-shot-MedVLMs}.||
|**2024-09-05**|[Have Large Vision-Language Models Mastered Art History?](http://arxiv.org/abs/2409.03521)|null|The emergence of large Vision-Language Models (VLMs) has recently established new baselines in image classification across multiple domains. However, the performance of VLMs in the specific task of artwork classification, particularly art style classification of paintings - a domain traditionally mastered by art historians - has not been explored yet. Artworks pose a unique challenge compared to natural images due to their inherently complex and diverse structures, characterized by variable compositions and styles. Art historians have long studied the unique aspects of artworks, with style prediction being a crucial component of their discipline. This paper investigates whether large VLMs, which integrate visual and textual data, can effectively predict the art historical attributes of paintings. We conduct an in-depth analysis of four VLMs, namely CLIP, LLaVA, OpenFlamingo, and GPT-4o, focusing on zero-shot classification of art style, author and time period using two public benchmarks of artworks. Additionally, we present ArTest, a well-curated test set of artworks, including pivotal paintings studied by art historians.||
|**2024-09-04**|[Can LVLMs Obtain a Driver's License? A Benchmark Towards Reliable AGI for Autonomous Driving](http://arxiv.org/abs/2409.02914)|null|Large Vision-Language Models (LVLMs) have recently garnered significant attention, with many efforts aimed at harnessing their general knowledge to enhance the interpretability and robustness of autonomous driving models. However, LVLMs typically rely on large, general-purpose datasets and lack the specialized expertise required for professional and safe driving. Existing vision-language driving datasets focus primarily on scene understanding and decision-making, without providing explicit guidance on traffic rules and driving skills, which are critical aspects directly related to driving safety. To bridge this gap, we propose IDKB, a large-scale dataset containing over one million data items collected from various countries, including driving handbooks, theory test data, and simulated road test data. Much like the process of obtaining a driver's license, IDKB encompasses nearly all the explicit knowledge needed for driving from theory to practice. In particular, we conducted comprehensive tests on 15 LVLMs using IDKB to assess their reliability in the context of autonomous driving and provided extensive analysis. We also fine-tuned popular models, achieving notable performance improvements, which further validate the significance of our dataset. The project page can be found at: \url{https://4dvlab.github.io/project_page/idkb.html}||
|**2024-09-04**|[Benchmarking Spurious Bias in Few-Shot Image Classifiers](http://arxiv.org/abs/2409.02882)|**[link](https://github.com/gtzheng/fewstab)**|Few-shot image classifiers are designed to recognize and classify new data with minimal supervision and limited data but often show reliance on spurious correlations between classes and spurious attributes, known as spurious bias. Spurious correlations commonly hold in certain samples and few-shot classifiers can suffer from spurious bias induced from them. There is an absence of an automatic benchmarking system to assess the robustness of few-shot classifiers against spurious bias. In this paper, we propose a systematic and rigorous benchmark framework, termed FewSTAB, to fairly demonstrate and quantify varied degrees of robustness of few-shot classifiers to spurious bias. FewSTAB creates few-shot evaluation tasks with biased attributes so that using them for predictions can demonstrate poor performance. To construct these tasks, we propose attribute-based sample selection strategies based on a pre-trained vision-language model, eliminating the need for manual dataset curation. This allows FewSTAB to automatically benchmark spurious bias using any existing test data. FewSTAB offers evaluation results in a new dimension along with a new design guideline for building robust classifiers. Moreover, it can benchmark spurious bias in varied degrees and enable designs for varied degrees of robustness. Its effectiveness is demonstrated through experiments on ten few-shot learning methods across three datasets. We hope our framework can inspire new designs of robust few-shot classifiers. Our code is available at https://github.com/gtzheng/FewSTAB.||
|**2024-09-06**|[CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of Large Multimodal Models](http://arxiv.org/abs/2409.02834)|null|Large language models (LLMs) have obtained promising results in mathematical reasoning, which is a foundational skill for human intelligence. Most previous studies focus on improving and measuring the performance of LLMs based on textual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few researchers have released English multimodal math datasets (e.g., MATHVISTA and MATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In this paper, we release a Chinese multimodal math (CMM-Math) dataset, including benchmark and training parts, to evaluate and enhance the mathematical reasoning of LMMs. CMM-Math contains over 28,000 high-quality samples, featuring a variety of problem types (e.g., multiple-choice, fill-in-the-blank, and so on) with detailed solutions across 12 grade levels from elementary to high school in China. Specifically, the visual context may be present in the questions or opinions, which makes this dataset more challenging. Through comprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math dataset face challenges, emphasizing the necessity for further improvements in LMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to handle the problems with mixed input of multiple images and text segments. We train our model using three stages, including foundational pre-training, foundational fine-tuning, and mathematical fine-tuning. The extensive experiments indicate that our model effectively improves math reasoning performance by comparing it with the SOTA LMMs over three multimodal mathematical datasets.||
|**2024-09-04**|[MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark](http://arxiv.org/abs/2409.02813)|null|This paper introduces MMMU-Pro, a robust version of the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark. MMMU-Pro rigorously assesses multimodal models' true understanding and reasoning capabilities through a three-step process based on MMMU: (1) filtering out questions answerable by text-only models, (2) augmenting candidate options, and (3) introducing a vision-only input setting where questions are embedded within images. This setting challenges AI to truly "see" and "read" simultaneously, testing a fundamental human cognitive skill of seamlessly integrating visual and textual information. Results show that model performance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8% to 26.9% across models. We explore the impact of OCR prompts and Chain of Thought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT generally improves performance. MMMU-Pro provides a more rigorous evaluation tool, closely mimicking real-world scenarios and offering valuable directions for future research in multimodal AI.||
|**2024-09-04**|[Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection](http://arxiv.org/abs/2409.02664)|null|The proliferation of deepfake faces poses huge potential negative impacts on our daily lives. Despite substantial advancements in deepfake detection over these years, the generalizability of existing methods against forgeries from unseen datasets or created by emerging generative models remains constrained. In this paper, inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach that repurposes a well-trained VLM for general deepfake detection. Motivated by the model reprogramming paradigm that manipulates the model prediction via data perturbations, our method can reprogram a pretrained VLM model (e.g., CLIP) solely based on manipulating its input without tuning the inner parameters. Furthermore, we insert a pseudo-word guided by facial identity into the text prompt. Extensive experiments on several popular benchmarks demonstrate that (1) the cross-dataset and cross-manipulation performances of deepfake detection can be significantly and consistently improved (e.g., over 88% AUC in cross-dataset setting from FF++ to WildDeepfake) using a pre-trained CLIP model with our proposed reprogramming method; (2) our superior performances are at less cost of trainable parameters, making it a promising approach for real-world applications.||
|**2024-09-04**|[Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models](http://arxiv.org/abs/2409.02530)|null|The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of kidney function in clinical practice. Although traditional equations and Machine Learning (ML) models using clinical and laboratory data can estimate eGFR, accurately predicting future eGFR levels remains a significant challenge for nephrologists and ML researchers. Recent advances demonstrate that Large Language Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust foundation models for diverse applications. This study investigates the potential of LMMs to predict future eGFR levels with a dataset consisting of laboratory and clinical values from 50 patients. By integrating various prompting techniques and ensembles of LMMs, our findings suggest that these models, when combined with precise prompts and visual representations of eGFR trajectories, offer predictive performance comparable to existing ML models. This research extends the application of foundation models and suggests avenues for future studies to harness these models in addressing complex medical forecasting challenges.||
|**2024-09-03**|[Evaluation and Comparison of Visual Language Models for Transportation Engineering Problems](http://arxiv.org/abs/2409.02278)|null|近年来，视觉语言模型（VLM）的最新发展显示出其在图像理解相关应用方面的巨大潜力。在本研究中，我们探索了最先进的VLM模型在基于视觉的交通工程任务中的应用，例如图像分类和目标检测。图像分类任务包括拥堵检测和裂缝识别，而目标检测任务则用于识别未佩戴头盔的行为。我们应用了CLIP、BLIP、OWL-ViT、Llava-Next等开源模型和闭源模型GPT-4o，评估了这些最先进的VLM模型的性能，以利用语言理解能力来完成基于视觉的交通任务。这些任务是通过对VLM模型应用零样本提示来执行的，因为零样本提示允许在不对任务进行任何训练的情况下执行任务。它消除了对特定任务进行标注数据集或微调的需求。虽然这些模型在图像分类任务中取得了与基准卷积神经网络（CNN）模型相当的结果，但在目标定位任务中仍有改进的空间。因此，本研究对最先进的VLM模型进行了全面评估，突出了这些模型的优势和局限性，可以作为未来改进和大规模实施的基线。||
|**2024-09-03**|[How to Determine the Preferred Image Distribution of a Black-Box Vision-Language Model?](http://arxiv.org/abs/2409.02253)|**[link](https://github.com/asgsaeid/cad_vqa)**|大型基础模型彻底改变了该领域，但针对特定视觉任务优化多模态模型仍然存在挑战。我们提出了一种新颖且通用的方法，通过测量不同输入提示下输出的一致性，来确定黑盒视觉语言模型 (VLM) 的首选图像分布。我们将其应用于 3D 对象的不同渲染类型，证明了其在需要精确解释复杂结构的各个领域的有效性，重点关注计算机辅助设计 (CAD) 作为示例领域。我们使用人类反馈的上下文学习进一步完善了 VLM 输出，显著提高了解释质量。为了解决专业领域缺乏基准的问题，我们引入了 CAD-VQA，这是一个用于评估 VLM 在 CAD 相关视觉问答任务上的新数据集。我们对 CAD-VQA 上最先进的 VLM 进行了评估，建立了基线性能水平，为在需要专家级视觉解释的各个领域推进 VLM 在复杂视觉推理任务中的能力提供了一个框架。我们在 \url{https://github.com/asgsaeid/cad_vqa} 上发布了数据集和评估代码。||
|**2024-09-03**|[Towards Real-World Adverse Weather Image Restoration: Enhancing Clearness and Semantics with Vision-Language Models](http://arxiv.org/abs/2409.02101)|**[link](https://github.com/jiaqixuac/WResVLM)**|本文探讨了在合成数据上训练的恶劣天气图像恢复方法应用于现实场景时的局限性。我们构建了一个半监督学习框架，利用视觉语言模型来增强现实环境中不同恶劣天气条件下的恢复性能。我们的方法包括使用视觉语言模型对真实数据进行图像清晰度评估和语义提供，作为训练恢复模型的监督信号。对于清晰度增强，我们使用真实数据，采用双重策略，即利用视觉语言模型评估的伪标签和天气提示学习。对于语义增强，我们通过调整视觉语言模型描述中的天气条件，同时保留语义，来整合真实世界的数据。此外，我们引入了一种有效的训练策略来提升恢复性能。我们的方法在真实世界的恶劣天气图像恢复方面取得了优异的结果，通过与现有最佳工作的定性和定量比较证明了这一点。||
|**2024-09-03**|[GraspSplats: Efficient Manipulation with 3D Feature Splatting](http://arxiv.org/abs/2409.02084)|null|The ability for robots to perform efficient and zero-shot grasping of object parts is crucial for practical applications and is becoming prevalent with recent advances in Vision-Language Models (VLMs). To bridge the 2D-to-3D gap for representations to support such a capability, existing methods rely on neural fields (NeRFs) via differentiable rendering or point-based projection methods. However, we demonstrate that NeRFs are inappropriate for scene changes due to their implicitness and point-based methods are inaccurate for part localization without rendering-based optimization. To amend these issues, we propose GraspSplats. Using depth supervision and a novel reference feature computation method, GraspSplats generates high-quality scene representations in under 60 seconds. We further validate the advantages of Gaussian-based representation by showing that the explicit and optimized geometry in GraspSplats is sufficient to natively support (1) real-time grasp sampling and (2) dynamic and articulated object manipulation with point trackers. With extensive experiments on a Franka robot, we demonstrate that GraspSplats significantly outperforms existing methods under diverse task settings. In particular, GraspSplats outperforms NeRF-based methods like F3RM and LERF-TOGO, and 2D detection methods.||

<p align=right>(<a href=#updated-on-20240930>back to top</a>)</p>

## 6DOF Object Pose

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-09-24**|[LaPose: Laplacian Mixture Shape Modeling for RGB-Based Category-Level Object Pose Estimation](http://arxiv.org/abs/2409.15727)|null|虽然基于RGBD的类别级物体姿态估计方法很有前景，但它们对深度数据的依赖限制了其在各种场景中的适用性。因此，最近的研究转向了基于RGB的方法；然而，由于缺乏深度信息，它们面临着重大挑战。一方面，深度信息的缺乏加剧了处理类内形状变化的难度，导致形状预测的不确定性增加。另一方面，仅RGB输入引入了固有的尺度模糊性，使得物体大小和平移的估计成为一个不适定问题。为了应对这些挑战，我们提出了LaPose，一个将物体形状建模为拉普拉斯混合模型的新型姿态估计框架。通过将每个点表示为概率分布，我们明确地量化了形状的不确定性。LaPose利用通用的3D信息流和专门的特征流来独立地预测每个点的拉普拉斯分布，从而捕捉物体几何形状的不同方面。然后，这两个分布被整合为一个拉普拉斯混合模型，以建立2D-3D对应关系，用于通过PnP模块求解姿态。为了减轻尺度模糊性，我们引入了物体大小和平移的尺度不变表示，从而提高了训练效率和整体鲁棒性。在NOCS数据集上的大量实验验证了LaPose的有效性，在基于RGB的类别级物体姿态估计方面取得了最先进的性能。代码发布在https://github.com/lolrudy/LaPose。|
|**2024-09-22**|[Tactile Functasets: Neural Implicit Representations of Tactile Datasets](http://arxiv.org/abs/2409.14592)|null|现代触觉传感器的化身会产生高维度的原始感官反馈（例如图像），这使得高效存储、处理和概括传感器变得具有挑战性。为了解决这些问题，我们为触觉传感器反馈引入了一种新颖的隐函数表示。我们没有直接使用原始触觉图像，而是提出了经过训练的神经隐函数来重建触觉数据集，从而产生捕获感官输入底层结构的紧凑表示。与原始表示相比，这些表示具有几个优势：它们紧凑，能够进行概率可解释的推理，并促进不同传感器之间的泛化。我们在手持物体姿态估计的下游任务上证明了这种表示的有效性，在简化下游模型的同时实现了优于基于图像的方法的性能。我们在https://www.mmintlab.com/tactile-functasets上发布了代码、演示和数据集。|
|**2024-09-18**|[FAST GDRNPP: Improving the Speed of State-of-the-Art 6D Object Pose Estimation](http://arxiv.org/abs/2409.12720)|null|6D物体姿态估计涉及确定场景中物体相对于所选坐标系的三维平移和旋转。这个问题在工业任务中的许多实际应用中尤其重要，例如质量控制、零件拾取和机器人操作，在这些应用中，速度和精度对于现实部署都至关重要。当前的模型，无论是经典的还是基于深度学习的模型，通常难以在精度和延迟之间进行权衡。我们的研究重点是在保持其高精度的同时，提高最先进的深度学习模型GDRNPP的速度。我们采用了几种技术来减小模型大小并缩短推理时间。这些技术包括使用更小、更快的骨干网络、修剪不必要的参数以及通过蒸馏将知识从大型、高性能模型转移到更小、更高效的学生模型。我们的研究结果表明，所提出的配置在显着缩短推理时间的同时，保持了与最先进技术相当的精度。这一进步可以促进在各种工业场景中更有效和更实际的应用，从而提高6D物体姿态估计模型在现实环境中的整体适用性。|
|**2024-09-12**|[Touch2Touch: Cross-Modal Tactile Generation for Object Manipulation](http://arxiv.org/abs/2409.08269)|null|现今的触觉传感器形态各异，尺寸不一。由于模型通常与特定的传感器设计绑定，这使得开发通用的触觉处理方法变得困难重重。我们通过在触觉传感器之间进行跨模态预测来解决这个问题：给定来自一个传感器的触觉信号，我们使用生成模型来估计另一个传感器如何感知相同的物理接触。这使得我们能够将特定于传感器的算法应用于生成的信号。我们通过训练一个扩散模型来实现这一想法，该模型可以在流行的 GelSlim 和 Soft Bubble 传感器之间进行转换。作为下游任务，我们使用 GelSlim 传感器进行手持物体姿态估计，同时使用仅对 Soft Bubble 信号进行操作的算法。数据集、代码和更多详细信息可以在 https://www.mmintlab.com/research/touch2touch/ 上找到。|
|**2024-09-04**|[Object Gaussian for Monocular 6D Pose Estimation from Sparse Views](http://arxiv.org/abs/2409.02581)|null|单目物体姿态估计作为计算机视觉和机器人技术中的关键任务，高度依赖于精确的2D-3D对应关系，而这通常需要昂贵的CAD模型，而这些模型可能并不容易获得。物体三维重建方法提供了一种替代方案，其中近期三维高斯 splatting (3DGS) 的进展展现出巨大的潜力。然而，它的性能仍然存在不足，并且在输入视图较少的情况下容易出现过拟合现象。为了应对这一挑战，我们引入了SGPose，这是一个使用基于高斯方法进行稀疏视图物体姿态估计的新框架。SGPose仅需少至十个视图，即可通过从随机长方体初始化开始生成几何感知表示，从而避免依赖传统3DGS方法所需的运动恢复结构 (SfM) 流程生成的几何形状。SGPose通过回归稀疏输入和随机初始化的图像与重建模型之间的密集2D-3D对应关系，消除了对CAD模型的依赖，同时几何一致性深度监督和在线合成视图扭曲是其成功的关键。在典型基准数据集，特别是在遮挡LM-O数据集上的实验表明，即使在稀疏视图约束下，SGPose的性能也优于现有方法，突显了其在现实应用中的潜力。|
|**2024-08-29**|[OP-Align: Object-level and Part-level Alignment for Self-supervised Category-level Articulated Object Pose Estimation](http://arxiv.org/abs/2408.16547)|**[link](https://github.com/yc-che/op-align)**|类别级关节对象姿态估计侧重于估计已知类别中未知关节对象的姿态。尽管意义重大，但这项任务仍然具有挑战性，原因在于对象的形状和姿态各不相同、数据集标注成本高昂以及现实环境复杂。在本文中，我们提出了一种新颖的自监督方法，利用单帧点云来解决此任务。我们的模型一致地生成具有规范姿态和整个输入对象的关节状态的重建，并估计对象级姿态（减少整体姿态差异）和零件级姿态（将输入的每个零件与其对应的重建零件对齐）。实验结果表明，我们的方法明显优于以往的自监督方法，并且与最先进的监督方法相当。为了评估我们的模型在现实场景中的性能，我们还引入了一个新的现实世界关节对象基准数据集。|
|**2024-08-19**|[RUMI: Rummaging Using Mutual Information](http://arxiv.org/abs/2408.10450)|null|本文提出了基于互信息的翻找方法 (RUMI)，该方法用于在线生成机器人在视觉遮挡环境中收集已知可移动物体姿态信息的动作序列。我们的方法侧重于接触丰富的翻找，利用物体姿态分布和机器人轨迹之间的互信息进行动作规划。RUMI 从观察到的部分点云推断出兼容的物体姿态分布，并实时计算其与工作空间占用率的互信息。在此基础上，我们开发了一种信息增益成本函数和可达性成本函数，以将物体保持在机器人的触及范围内。这些函数被集成到具有随机动力学模型的模型预测控制 (MPC) 框架中，以闭环方式更新姿态分布。主要贡献包括一种新的物体姿态估计置信框架、一种高效的信息增益计算策略以及一种鲁棒的基于 MPC 的控制方案。与基线方法相比，RUMI 在仿真和实际任务中均表现出优越的性能。|
|**2024-08-15**|[Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation](http://arxiv.org/abs/2408.08234)|**[link](https://github.com/varunburde/reconstruction_pose_benchmark)**|物体姿态估计对于涉及机器人操作、导航和增强现实的许多工业应用至关重要。当前通用的物体姿态估计器，即不需要针对每个物体进行训练的方法，依赖于精确的 3D 模型。目前，主要使用 CAD 模型，但在实践中很难获得。同时，通常可以获取物体的图像。自然而然地，这就引出了一个问题：从图像重建的 3D 模型是否足以促进准确的物体姿态估计？我们旨在通过提出一种新的基准来回答这个问题，该基准用于衡量 3D 重建质量对姿态估计精度的影响。我们的基准提供了用于物体重建的校准图像，这些图像与 YCB-V 数据集的测试图像配准，用于在 BOP 基准格式下进行姿态评估。使用多种最先进的 3D 重建和物体姿态估计方法进行的详细实验表明，现代重建方法生成的几何形状通常足以进行准确的姿态估计。我们的实验得出了一些有趣的观察结果：(1) 用于衡量 3D 重建质量的标准指标不一定能指示姿态估计的准确性，这表明需要像我们这样的专用基准。(2) 传统的、非基于学习的方法可以与现代的基于学习的重建技术相媲美，甚至可以提供更好的重建时间-姿态精度权衡。(3) 使用重建模型和 CAD 模型的性能之间仍然存在相当大的差距。为了促进缩小这一差距的研究，我们的基准在 https://github.com/VarunBurde/reconstruction_pose_benchmark 上公开可用。|
|**2024-07-16**|[NeuSurfEmb: A Complete Pipeline for Dense Correspondence-based 6D Object Pose Estimation without CAD Models](http://arxiv.org/abs/2407.12207)|**[link](https://github.com/ethz-asl/neusurfemb)**|当前最先进的6D物体姿态估计方法假设可以使用CAD模型，并且需要用户手动设置基于物理的渲染(PBR)流程来生成合成训练数据。这两个因素都限制了这些方法在现实场景中的应用。在这项工作中，我们提出了一个不需要CAD模型的流程，并且只需提供一小部分真实图像作为输入，就可以训练出最先进的姿态估计器。我们的方法基于NeuS2物体表示，我们通过基于运动恢复结构(SfM)和物体无关分割的半自动化程序来学习这种表示。我们利用NeuS2的新视图合成能力和简单的剪切粘贴增强技术来自动生成逼真的物体渲染，我们使用这些渲染来训练基于对应的SurfEmb姿态估计器。我们在LINEMOD-Occlusion数据集上评估了我们的方法，广泛研究了其各个组件的影响，并展示了其相对于基于CAD模型和PBR数据的方法的竞争性能。我们还展示了我们的流程在自收集的现实世界物体上的易用性和有效性，表明我们的方法优于最先进的无CAD模型方法，具有更好的精度和对轻度遮挡的鲁棒性。为了让机器人社区能够从这个系统中受益，我们将在https://www.github.com/ethz-asl/neusurfemb公开发布它。|
|**2024-06-06**|[Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking](http://arxiv.org/abs/2406.04316)|null|6D物体姿态估计是计算机视觉中一项至关重要但极具挑战性的任务，其主要难点在于缺乏大规模数据集。这种数据稀缺性阻碍了对模型性能的全面评估，限制了研究进展。此外，有限的实例或类别数量也限制了其应用范围。为了解决这些问题，本文提出了Omni6DPose，这是一个以对象类别多样性、规模大和材质丰富为特征的大型数据集。Omni6DPose主要由三个部分组成：ROPE（真实6D物体姿态估计数据集），包含33.2万张图像，涵盖149个类别、581个实例的超过150万个标注；SOPE（模拟6D物体姿态估计数据集），包含47.5万张在混合现实环境中创建并进行深度模拟的图像，涵盖与ROPE相同的149个类别、4162个实例的超过500万个标注；以及在ROPE和SOPE中均使用的手动对齐的真实扫描物体模型。由于存在大量变化和歧义，Omni6DPose本身就极具挑战性。为了应对这一挑战，我们引入了GenPose++，它是SOTA类别级姿态估计框架的增强版本，它包含两个关键改进：语义感知特征提取和基于聚类的聚合。此外，我们还提供了全面的基准测试分析，以评估先前方法在这个大规模数据集上在6D物体姿态估计和姿态跟踪方面的性能。|
|**2024-06-05**|[Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices](http://arxiv.org/abs/2406.02977)|null|随着机器人和增强现实应用越来越依赖于精确高效的6D物体姿态估计，边缘设备上的实时性能对于实现更具交互性和响应能力的系统至关重要。我们提出的稀疏颜色代码网络（SCCN）体现了一种清晰简洁的流程设计，以有效满足这一需求。SCCN对RGB图像中的目标物体进行像素级预测，利用基本物体几何特征的稀疏性来加速Perspective-n-Point（PnP）计算过程。此外，它引入了一种新颖的基于像素级几何的物体对称表示，该表示与初始姿态预测无缝集成，有效地解决了对称物体歧义问题。SCCN在英伟达Jetson AGX Xavier上分别实现了在基准LINEMOD数据集和遮挡LINEMOD数据集上每秒19帧（FPS）和6帧的估计速率，同时在这些速率下始终保持较高的估计精度。||
|**2024-05-31**|[Deep Learning-Based Object Pose Estimation: A Comprehensive Survey](http://arxiv.org/abs/2405.07801)|**[link](https://github.com/cnjianliu/awesome-object-pose-estimation)**|物体姿态估计是计算机视觉中的一个基本问题，在增强现实和机器人技术中有着广泛的应用。在过去的十年中，深度学习模型由于其卓越的准确性和鲁棒性，越来越多地取代了依赖于工程点对特征的传统算法。然而，当代方法仍然存在若干挑战，包括它们对标记训练数据的依赖性、模型紧凑性、在挑战性条件下的鲁棒性以及泛化到未见过的新物体能力。目前缺乏一篇综述来讨论该领域的进展、面临的挑战和未来有希望的方向。为了填补这一空白，我们讨论了基于深度学习的物体姿态估计的最新进展，涵盖了该问题的所有三种形式，即实例级、类别级和未见过物体的姿态估计。我们的综述还涵盖了多种输入数据模态、输出姿态的自由度、物体属性和下游任务，为读者提供了对该领域的全面理解。此外，它还讨论了不同领域的训练范式、推理模式、应用领域、评估指标和基准数据集，并报告了当前最先进方法在这些基准上的性能，从而方便读者为其应用选择最合适的方法。最后，该综述指出了关键挑战，回顾了当前的趋势及其优缺点，并确定了未来研究的有希望的方向。我们还在 https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation 上持续跟踪最新的工作。||
|**2024-03-28**|[Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation](http://arxiv.org/abs/2403.19527)|**[link](https://github.com/leeiieeo/ag-pose)**|类别级 6D 物体姿态估计旨在估计特定类别中未见实例的旋转、平移和大小。在这一领域，基于密集对应的方法取得了领先的性能。然而，它们没有明确考虑不同实例的局部和全局几何信息，导致对形状变化显著的未见实例的泛化能力较差。为了解决这个问题，我们提出了一种新颖的实例自适应和几何感知的关键点学习方法，用于类别级 6D 物体姿态估计 (AG-Pose)，它包括两个关键设计：（1）第一个设计是实例自适应关键点检测模块，它可以自适应地检测一组稀疏的关键点，用于表示各种实例的几何结构。(2) 第二个设计是几何感知特征聚合模块，它可以有效地将局部和全局几何信息整合到关键点特征中。这两个模块可以协同工作，为未见实例建立鲁棒的关键点级对应关系，从而增强模型的泛化能力。在 CAMERA25 和 REAL275 数据集上的实验结果表明，所提出的 AG-Pose 在没有类别特定形状先验的情况下，大大优于最先进的方法。||
|**2024-06-01**|[Object Pose Estimation via the Aggregation of Diffusion Features](http://arxiv.org/abs/2403.18791)|**[link](https://github.com/tianfu18/diff-feats-pose)**|从图像中估计物体姿态是3D场景理解的关键任务，最近的方法在非常大的基准测试中显示出可喜的结果。然而，这些方法在处理未见过的物体时性能会显著下降。我们认为这是由于图像特征的泛化能力有限造成的。为了解决这个问题，我们对扩散模型（例如Stable Diffusion）的特征进行了深入分析，这些模型在对未见过的物体建模方面具有巨大潜力。在此分析的基础上，我们创新性地将这些扩散特征引入物体姿态估计。为此，我们提出了三种不同的架构，可以有效地捕获和聚合不同粒度的扩散特征，极大地提高了物体姿态估计的泛化能力。我们的方法在三个流行的基准数据集LM、O-LM和T-LESS上，以相当大的优势优于最先进的方法。特别是，我们的方法在未见过的物体上取得了比先前最佳结果更高的精度：在Unseen LM上为98.2%对93.5%，在Unseen O-LM上为85.9%对76.3%，显示了我们方法强大的泛化能力。我们的代码发布在https://github.com/Tianfu18/diff-feats-pose。||

<p align=right>(<a href=#updated-on-20240930>back to top</a>)</p>

## nerf

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-09-25**|[SeaSplat: Representing Underwater Scenes with 3D Gaussian Splatting and a Physically Grounded Image Formation Model](http://arxiv.org/abs/2409.17345)|null|我们介绍了SeaSplat，这是一种利用3D辐射场最新进展实现水下场景实时渲染的方法。水下场景是具有挑战性的视觉环境，因为通过水等介质进行渲染会在图像捕捉中引入距离和颜色相关的影响。我们使用物理基础的水下成像模型来约束3D高斯 splatting (3DGS)，这是辐射场领域的最新进展，可以实现完整3D场景的快速训练和实时渲染。将SeaSplat应用于SeaThru-NeRF数据集中的真实场景、由美属维尔京群岛的水下航行器收集的场景以及模拟退化的真实场景，我们不仅看到在存在介质的情况下渲染场景新视点的定量性能有所提高，而且还能够恢复场景的底层真实颜色，并将渲染恢复到不存在介入介质的状态。我们证明了水下成像模型有助于学习场景结构，获得更好的深度图，并证明了我们的改进保持了利用3D高斯表示带来的显著计算改进。|
|**2024-09-25**|[Let's Make a Splan: Risk-Aware Trajectory Optimization in a Normalized Gaussian Splat](http://arxiv.org/abs/2409.16915)|null|神经辐射场和高斯 splatting 通过实现复杂场景的逼真表示，改变了计算机视觉领域。尽管取得了成功，但它们在现实世界机器人任务（如轨迹优化）中的应用却很有限。造成这种有限成功的原因主要有两个。首先，在辐射模型中难以推断碰撞。其次，很难快速执行辐射模型的推理以进行实时轨迹合成。本文针对这些挑战，提出了一种名为 SPLANNING 的风险感知轨迹优化器，该优化器在高斯 splatting 模型中运行。本文首先推导了一种严格限制机器人与辐射场之间碰撞概率上限的方法。其次，本文介绍了高斯 splatting 的归一化重构，以便在高斯 splat 中高效计算碰撞边界。第三，提出了一种在避免与高斯 splat 表示的场景发生碰撞的同时优化轨迹的方法。实验表明，在高度杂乱的环境中，SPLANNING 在生成无碰撞轨迹方面优于最先进的方法。所提出的系统还在真实的机器人机械臂上进行了测试。项目页面位于 https://roahmlab.github.io/splanning。|
|**2024-09-22**|[MVPGS: Excavating Multi-view Priors for Gaussian Splatting from Sparse Input Views](http://arxiv.org/abs/2409.14316)|null|近年来，神经辐射场（NeRF）的进步促进了少样本新视角合成（NVS）的发展，这是三维视觉应用中的一个重大挑战。尽管人们做了很多尝试来减少NeRF中密集的输入需求，但它仍然存在训练和渲染过程耗时的缺点。最近，三维高斯散射（3DGS）通过一种显式的基于点的表示实现了实时高质量渲染。然而，与NeRF类似，由于缺乏约束，它往往会对训练视图过拟合。在本文中，我们提出了MVPGS，一种基于三维高斯散射挖掘多视图先验的少样本NVS方法。我们利用最近基于学习的多视图立体视觉（MVS）来提高3DGS几何初始化的质量。为了减轻过拟合，我们提出了一种前向扭曲方法，用于根据计算的几何形状对符合场景的附加外观约束进行建模。此外，我们引入了一种视图一致性几何约束来约束高斯参数，以促进适当的优化收敛，并利用单目深度正则化作为补偿。实验表明，该方法在实时渲染速度下取得了最先进的性能。项目页面：https://zezeaaa.github.io/projects/MVPGS/|
|**2024-09-10**|[Sources of Uncertainty in 3D Scene Reconstruction](http://arxiv.org/abs/2409.06407)|**[link](https://github.com/aaltoml/uncertainty-nerf-gs)**|三维场景重建过程会受到现实世界场景中众多不确定性来源的影响。虽然神经辐射场 (NeRF) 和三维高斯 splatting (GS) 可以实现高保真渲染，但它们缺乏内置机制来直接解决或量化由噪声、遮挡、混杂异常值和不精确的相机姿态输入引起的不确定性。在本文中，我们引入了一种分类法，对这些方法中固有的不同不确定性来源进行分类。此外，我们使用不确定性估计技术扩展了基于 NeRF 和 GS 的方法，包括学习不确定性输出和集成，并进行了实证研究以评估它们捕捉重建敏感性的能力。我们的研究强调了在设计基于 NeRF/GS 的不确定性感知三维重建方法时需要解决各种不确定性方面的问题。|
|**2024-09-05**|[Optimizing 3D Gaussian Splatting for Sparse Viewpoint Scene Reconstruction](http://arxiv.org/abs/2409.03213)|null|三维高斯 splatting (3DGS) 已成为一种很有前景的三维场景表示方法，与神经辐射场 (NeRF) 相比，它可以减少计算开销。然而，3DGS 容易出现高频伪影，并且在稀疏视点条件下表现不佳，从而限制了其在机器人和计算机视觉中的应用。为了解决这些限制，我们引入了 SVS-GS，这是一种用于稀疏视点场景重建的新框架，它集成了三维高斯平滑滤波器来抑制伪影。此外，我们的方法结合了深度梯度剖面先验 (DGPP) 损失和动态深度掩码来锐化边缘，并结合了具有分数蒸馏采样 (SDS) 损失的二维扩散来增强新视图合成中的几何一致性。在 MipNeRF-360 和 SeaThru-NeRF 数据集上的实验评估表明，SVS-GS 显着改善了稀疏视点下的三维重建，为机器人和计算机视觉应用中的场景理解提供了一种稳健且高效的解决方案。|
|**2024-08-20**|[Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark Images Using Gaussian Splatting](http://arxiv.org/abs/2408.09130)|**[link](https://github.com/yec22/Gaussian-DK)**|三维高斯 splatting 近期成为一种强大的表示方法，可以使用一致的多视图图像作为输入合成出色的新视图。然而，我们注意到，在场景未被完全照亮的黑暗环境中拍摄的图像可能会出现明显的亮度变化和多视图不一致性，这对三维高斯 splatting 提出了巨大挑战，并严重降低了其性能。为了解决这个问题，我们提出了 Gaussian-DK。观察到不一致性主要由相机成像引起，我们使用一组各向异性三维高斯函数来表示物理世界中一致的辐射场，并设计了一个相机响应模块来补偿多视图不一致性。我们还引入了一种基于步长的梯度缩放策略，以约束靠近相机的高斯函数（结果证明是漂浮物）的分割和克隆。在我们提出的基准数据集上的实验表明，Gaussian-DK 可以生成高质量的渲染结果，不会出现重影和漂浮物伪影，并且明显优于现有方法。此外，我们还可以通过控制曝光级别来合成亮度更高的图像，从而清晰地显示阴影区域的细节。|
|**2024-09-05**|[EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian Splatting](http://arxiv.org/abs/2407.13520)|null|近年来，随着神经辐射场（NeRF）和3D高斯散射（3DGS）的发展，3D去模糊重建技术取得了显著进展。尽管这些技术可以从模糊的图像输入中恢复相对清晰的3D重建，但它们在处理严重模糊和复杂相机运动方面仍然面临局限性。为了解决这些问题，我们提出了事件辅助的3D高斯散射去模糊重建（EaDeblur-GS），它集成了事件相机数据，以增强3DGS对运动模糊的鲁棒性。通过采用自适应偏差估计器（ADE）网络来估计高斯中心偏差并使用新的损失函数，EaDeblur-GS实时实现了清晰的3D重建，其性能可与最先进的方法相媲美。|
|**2024-07-10**|[3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes](http://arxiv.org/abs/2407.07090)|null|基于粒子的辐射场表示法，例如三维高斯 splatting，已经在复杂场景的重建和重新渲染方面取得了巨大成功。大多数现有方法通过光栅化渲染粒子，将它们投影到屏幕空间图块中，并按排序顺序进行处理。而这项工作则考虑对粒子进行光线追踪，构建边界体积层次结构，并使用高性能 GPU 光线追踪硬件为每个像素投射光线。为了有效处理大量半透明粒子，我们描述了一种专门的渲染算法，该算法使用边界网格封装粒子，以利用快速的光线三角形相交，并按深度顺序对成批的相交进行着色。光线追踪在计算机图形学中的优势是众所周知的：处理非相干光线以实现阴影和反射等二次照明效果，从机器人技术中常见的高度失真相机进行渲染，对光线进行随机采样等等。使用我们的渲染器，与光栅化相比，这种灵活性几乎不需要任何成本。实验结果证明了我们方法的速度和准确性，以及在计算机图形学和视觉方面的若干应用。我们还提出了对基本高斯表示的相关改进，包括简单地使用广义核函数，这可以显著减少粒子命中次数。|
|**2024-07-07**|[GaussReg: Fast 3D Registration with Gaussian Splatting](http://arxiv.org/abs/2407.05254)|null|点云配准是大规模三维场景扫描和重建的基本问题。在深度学习的帮助下，配准方法取得了显著的进展，已接近成熟阶段。随着神经辐射场（NeRF）的引入，它凭借强大的视图合成能力成为了最受欢迎的三维场景表示方法。对于 NeRF 表示，大规模场景重建也需要对其进行配准。然而，这一课题极度缺乏探索。这是因为对具有隐式表示的两个场景之间的几何关系进行建模存在固有的挑战。现有方法通常将隐式表示转换为显式表示以进行进一步配准。最近，引入了高斯 splatting (GS)，它采用显式的三维高斯函数。这种方法在保持高质量渲染的同时，显著提高了渲染速度。给定两个具有显式 GS 表示的场景，在这项工作中，我们探索了它们之间的三维配准任务。为此，我们提出了 GaussReg，一种新颖的从粗到精的框架，它既快速又准确。粗阶段遵循现有的点云配准方法，并估计来自 GS 的点云的粗略对齐。我们进一步提出了一种新的图像引导的精细配准方法，该方法从 GS 渲染图像，为精确对齐提供更详细的几何信息。为了支持全面评估，我们仔细构建了一个名为 ScanNet-GSReg 的场景级数据集，其中包含从 ScanNet 数据集中获得的 1379 个场景，并收集了一个名为 GSReg 的真实世界数据集。实验结果表明，我们的方法在多个数据集上实现了最先进的性能。我们的 GaussReg 比 HLoc（SuperPoint 作为特征提取器，SuperGlue 作为匹配器）快 44 倍，并且精度相当。|
|**2024-07-04**|[CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion Blur Images](http://arxiv.org/abs/2407.03923)|null|由于神经辐射场 (NeRFs) 能够高质量地渲染新视角，因此备受关注，这促使人们对其在各种真实场景中的应用进行研究。其中一个关键挑战是相机在曝光时间内移动造成的相机运动模糊，这阻碍了精确的三维场景重建。在本研究中，我们提出了连续刚体运动感知高斯散射 (CRiM-GS)，以实时渲染速度从模糊图像中重建精确的三维场景。考虑到实际的相机运动模糊过程包含复杂的运动模式，我们基于神经常微分方程 (ODEs) 预测相机的连续运动。具体来说，我们利用刚体变换来模拟相机运动并进行适当的正则化，以保持对象的形状和大小。此外，我们在\textit{SE(3)} 场中引入连续可变形三维变换，通过确保更高的自由度使刚体变换适应现实问题。通过重新审视基本相机理论并采用先进的神经网络训练技术，我们实现了对连续相机轨迹的精确建模。我们进行了大量的实验，在基准数据集上定量和定性地证明了其最先进的性能。|
|**2024-07-29**|[Trimming the Fat: Efficient Compression of 3D Gaussian Splats through Pruning](http://arxiv.org/abs/2406.18214)|**[link](https://github.com/salmanali96/trimming-the-fat)**|近年来，由于神经辐射场和最近出现的3D高斯样条曲线(3DGS)模型提供了端到端训练的能力，3D模型的使用得到了推广。后者在训练过程中能够轻松地快速收敛并提供广泛的可编辑性，因此具有显著的优势。然而，尽管发展迅速，但关于这些模型可扩展性的文献仍处于起步阶段。在本研究中，我们为解决这一差距采取了一些初步措施，展示了一种能够实现此类模型内存和计算可扩展性的方法。具体来说，我们提出了“Trimming the fat”，这是一种基于梯度的迭代式后剪枝技术，用于消除模型中编码的冗余信息。我们在广泛认可的基准测试集上的实验结果证明了我们方法的有效性，结果表明，在保持甚至提高基线性能的同时，最多可以移除75%的高斯函数。我们的方法实现了大约50倍的压缩，同时保持了与基线模型相似的性能，并且能够将计算速度提高到600 FPS。||
|**2024-06-21**|[Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks](http://arxiv.org/abs/2406.15149)|null|模拟器是自动机器人学习的强大工具，因为它们可以提供可扩展的数据生成、灵活的设计和轨迹优化。然而，将从模拟数据中学习到的行为迁移到现实世界中被证明是困难的，通常需要通过计算量大的域随机化方法或进一步的模型微调来缓解。我们提出了一种方法来提高模拟到真实视觉四旋翼导航任务中对分布变化的泛化能力和鲁棒性。为此，我们首先通过将高斯 splatting 与四旋翼飞行动力学相结合来构建模拟器，然后使用 Liquid 神经网络训练鲁棒的导航策略。通过这种方式，我们获得了一个完整的模仿学习协议，它结合了 3D 高斯 splatting 辐射场渲染的进步、专家演示训练数据的巧妙编程以及 Liquid 网络的任务理解能力。通过一系列定量飞行测试，我们证明了在单个模拟场景中学习到的导航技能可以直接稳健地迁移到现实世界。我们进一步展示了在剧烈的分布和物理环境变化下，在训练环境之外保持性能的能力。我们学习的 Liquid 策略，仅在从真实感室内模拟飞行中提取的单个目标操作上进行训练，可以泛化到户外真实硬件平台上的多步远足。||
|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|在非结构化的旅游环境中拍摄的照片经常表现出多变的外观和短暂的遮挡，这对准确的场景重建提出了挑战，并在新视角合成中导致了伪影。虽然先前的方法已经将神经辐射场 (NeRF) 与其他可学习模块相结合来处理动态外观并消除瞬态对象，但其大量的训练需求和缓慢的渲染速度限制了实际部署。最近，3D 高斯 splatting (3DGS) 已成为 NeRF 的一种有前途的替代方案，它提供了卓越的训练和推理效率以及更好的渲染质量。本文介绍了 Wild-GS，这是一种针对不受约束的照片集优化的 3DGS 创新改编，同时保留了其效率优势。Wild-GS 通过每张图像的固有材质属性、全局照明和相机属性以及逐点反射率的局部变化来确定每个 3D 高斯的外观。与先前在图像空间中对参考特征进行建模的方法不同，Wild-GS 通过对从参考图像中提取的三平面进行采样，将像素外观特征明确地与相应的局部高斯对齐。这种新颖的设计有效地将参考视图的高频细节外观转移到 3D 空间，并显着加快了训练过程。此外，2D 可见性图和深度正则化分别用于减轻瞬态效应和约束几何形状。大量实验表明，Wild-GS 在所有现有技术中实现了最先进的渲染性能以及最高的训练和推理效率。||
|**2024-06-06**|[A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation](http://arxiv.org/abs/2406.04253)|null|3D modeling has long been an important area in computer vision and computer graphics. Recently, thanks to the breakthroughs in neural representations and generative models, we witnessed a rapid development of 3D modeling. 3D human modeling, lying at the core of many real-world applications, such as gaming and animation, has attracted significant attention. Over the past few years, a large body of work on creating 3D human avatars has been introduced, forming a new and abundant knowledge base for 3D human modeling. The scale of the literature makes it difficult for individuals to keep track of all the works. This survey aims to provide a comprehensive overview of these emerging techniques for 3D human avatar modeling, from both reconstruction and generation perspectives. Firstly, we review representative methods for 3D human reconstruction, including methods based on pixel-aligned implicit function, neural radiance field, and 3D Gaussian Splatting, etc. We then summarize representative methods for 3D human generation, especially those using large language models like CLIP, diffusion models, and various 3D representations, which demonstrate state-of-the-art performance. Finally, we discuss our reflection on existing methods and open challenges for 3D human avatar modeling, shedding light on future research.||
|**2024-06-13**|[3D-HGS: 3D Half-Gaussian Splatting](http://arxiv.org/abs/2406.02720)|**[link](https://github.com/lihaolin88/3d-half-gaussian-splatting)**|照片级逼真的三维重建是三维计算机视觉中的一个基本问题。由于最近神经渲染技术的出现，该领域取得了相当大的进步。这些技术主要集中于学习三维场景的体积表示，并通过渲染得到的损失函数来细化这些表示。其中，三维高斯散射（3D-GS）已成为一种重要的方法，其性能超过了神经辐射场（NeRFs）。3D-GS使用参数化的三维高斯函数来建模空间位置和颜色信息，并结合基于图块的快速渲染技术。尽管其渲染性能和速度都很出色，但使用三维高斯核函数在准确表示不连续函数方面存在固有限制，特别是在形状不连续的边缘和角落，以及在颜色不连续的不同纹理之间。为了解决这个问题，我们建议采用三维半高斯（3D-HGS）核函数，它可以作为一种即插即用的核函数。我们的实验表明，它们能够提高当前与3D-GS相关方法的性能，并在不影响渲染速度的情况下，在各种数据集上实现最先进的渲染性能。||

<p align=right>(<a href=#updated-on-20240930>back to top</a>)</p>

## 分类/检测/识别/分割

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-09-27**|[Spectral Wavelet Dropout: Regularization in the Wavelet Domain](http://arxiv.org/abs/2409.18951)|null|正则化技术有助于防止过拟合，从而提高卷积神经网络 (CNN) 的泛化能力。过拟合的原因之一是网络不同部分之间复杂的相互适应，这使得 CNN 依赖于它们的联合响应，而不是鼓励每个部分独立学习有用的特征表示。频域处理是一种强大的策略，它利用频率分解来修改具有时间和空间一致性的数据。这项工作介绍了一种新颖的正则化方法——谱小波丢弃 (SWD)，它包括两种变体：1D-SWD 和 2D-SWD。这些变体通过随机丢弃特征图的离散小波分解中的详细频带，从而提高 CNN 的泛化能力。我们的方法区别于预先存在的谱“傅立叶”丢弃 (2D-SFD)，后者消除了傅立叶域中的系数。值得注意的是，SWD 只需要一个超参数，不像 SFD 需要两个。我们还通过实现一维版本的谱“傅立叶”丢弃 (1D-SFD) 来扩展文献，为全面比较奠定了基础。我们的评估表明，相对于 1D-SFD 和 2D-SFD，1D 和 2D SWD 变体在 CIFAR-10/100 基准测试中均具有竞争力的性能。具体来说，与 1D/2D-SFD 相比，1D-SWD 具有显著更低的计算复杂度。在 Pascal VOC 目标检测基准测试中，SWD 变体的性能优于 1D-SFD 和 2D-SFD，并且在训练期间表现出更低的计算复杂度。|
|**2024-09-27**|[Unconditional stability of a recurrent neural circuit implementing divisive normalization](http://arxiv.org/abs/2409.18946)|null|递归神经模型的稳定性是一个重大挑战，特别是在开发可以无缝训练的生物学上合理的 neurodynamical 模型方面。传统的皮质回路模型由于动力系统中存在广泛的非线性，因此难以训练，导致优化问题具有难以施加的非线性稳定性约束。相反，递归神经网络 (RNN) 在涉及序列数据的任务中表现出色，但缺乏生物学上的合理性和可解释性。在这项工作中，我们通过将动态除法归一化 (DN) 与 ORGaNICs 的稳定性联系起来来解决这些挑战，ORGaNICs 是一种生物学上合理的递归皮质回路模型，它可以动态地实现 DN，并且已被证明可以模拟广泛的神经生理学现象。通过使用 Lyapunov 的间接方法，我们证明了当递归权重矩阵是单位矩阵时，任意维度的 ORGaNICs 电路具有无条件局部稳定性的显著特性。因此，我们将 ORGaNICs 连接到一个耦合阻尼谐振子的系统，这使我们能够推导出电路的能量函数，从而提供电路和单个神经元旨在实现的目标的规范原则。此外，对于一般的递归权重矩阵，我们证明了二维模型的稳定性，并通过经验证明了稳定性在更高维度上成立。最后，我们表明 ORGaNICs 可以通过时间反向传播进行训练，而无需梯度裁剪/缩放，这得益于其内在的稳定性特性和自适应时间常数，解决了梯度爆炸、消失和振荡的问题。通过评估模型在 RNN 基准测试中的性能，我们发现 ORGaNICs 在静态图像分类任务上优于其他神经动力学模型，并且在序列任务上的性能与 LSTM 相当。|
|**2024-09-27**|[Subspace Preserving Quantum Convolutional Neural Network Architectures](http://arxiv.org/abs/2409.18918)|null|子空间保持量子电路是一类量子算法，它依赖于计算中的某些对称性，可以为其训练提供理论上的保证。这些算法之所以受到广泛关注，是因为它们可以提供多项式加速，并且可以用来模拟经典的机器学习算法。在这项工作中，我们提出了一种基于汉明重量保持量子电路的新型卷积神经网络架构模型。特别是，我们引入了卷积层和基于测量的池化层，它们在保持量子态对称性的同时，使用非子空间保持的门来实现非线性。与经典的深度学习架构相比，我们的方案在多项式运行时间上具有显著的优势。我们提供了一个用于汉明重量保持量子电路的开源仿真库，可以使用面向GPU的库更有效地仿真我们的技术。使用此代码，我们提供了一些架构示例，这些示例突出了在量子比特数量有限且参数少于经典深度学习架构的情况下，在复杂图像分类任务上的出色性能。|
|**2024-09-27**|[MCUBench: A Benchmark of Tiny Object Detectors on MCUs](http://arxiv.org/abs/2409.18866)|**[link](https://github.com/deeplite/deeplite-torch-zoo)**|我们推出了 MCUBench，这是一个基准测试平台，涵盖了 100 多个基于 YOLO 的目标检测模型，这些模型在 VOC 数据集上针对七种不同的 MCU 进行了评估。该基准测试平台提供了各种输入分辨率和基于 YOLO 的单阶段检测器的平均精度、延迟、RAM 和 Flash 使用情况的详细信息。通过使用固定的训练流程进行受控比较，我们收集了全面的性能指标。我们的帕累托最优分析表明，集成现代检测头和训练技术可以让各种 YOLO 架构（包括 YOLOv3 等传统模型）在平均精度 (mAP) 和延迟之间实现高效的权衡。MCUBench 是一个有价值的工具，可用于对当代目标检测器的 MCU 性能进行基准测试，并根据特定限制条件帮助进行模型选择。|
|**2024-09-27**|[A Novel Unified Architecture for Low-Shot Counting by Detection and Segmentation](http://arxiv.org/abs/2409.18686)|null|少样本目标计数器可以使用少量甚至没有标注样本估计图像中的目标数量。目标定位通过将目标与原型进行匹配来实现，原型是通过对图像范围内的目标外观进行无监督聚合构建的。由于目标外观可能存在多样性，现有方法通常会导致过度泛化和误报。此外，性能最佳的方法通过预测每个目标中心的单位高斯分布的代理损失来训练目标定位。这种损失对标注误差和超参数很敏感，并且没有直接优化检测任务，导致计数结果欠佳。我们引入了GeCo，这是一种新颖的少样本计数器，可以在统一的架构中实现准确的目标检测、分割和计数估计。GeCo 通过一种新颖的密集目标查询公式，可以稳健地泛化不同目标外观的原型。此外，我们还提出了一种新的计数损失，它直接优化检测任务，避免了标准代理损失的问题。GeCo 在总计数平均绝对误差方面比领先的基于少样本检测的计数器高出约 25%，实现了卓越的检测精度，并在所有少样本计数设置中都树立了新的最先进的结果。|
|**2024-09-27**|[Query matching for spatio-temporal action detection with query-based object detector](http://arxiv.org/abs/2409.18408)|null|本文提出了一种扩展基于查询的目标检测模型DETR的方法，将其应用于时空动作检测，该任务需要在视频中保持时间一致性。我们提出的方法将DETR应用于每一帧，并使用特征偏移来整合时间信息。然而，每帧中DETR的对象查询可能对应于不同的对象，使得简单的特征偏移无效。为了克服这个问题，我们提出了跨不同帧的查询匹配，确保对同一对象的查询能够匹配并用于特征偏移。实验结果表明，当使用所提出的查询匹配对查询特征进行偏移时，JHMDB21数据集上的性能显著提高。|
|**2024-09-27**|[Simpler Gradient Methods for Blind Super-Resolution with Lower Iteration Complexity](http://arxiv.org/abs/2409.18387)|**[link](https://github.com/Jinshengg/SimplerGDs-VHL)**|我们研究了盲超分辨率问题，它可以通过向量化汉克尔提升（VHL）公式化为一个低秩矩阵恢复问题。先前基于VHL的名为PGD-VHL的梯度下降方法依赖于额外的正则化，例如投影和平衡惩罚，表现出次优的迭代复杂度。在本文中，我们提出了一个更简单的无约束优化问题，无需上述两种类型的正则化，并开发了两种新的可证梯度方法，分别名为VGD-VHL和ScalGD-VHL。我们为算法的理论保证提供了新颖而清晰的分析，证明了我们的方法比PGD-VHL具有更低的迭代复杂度。此外，ScalGD-VHL具有最低的迭代复杂度，同时与条件数无关。此外，我们的新分析表明，盲超分辨率问题对不相干性的要求较低，从而无需不相干投影即可实现线性收敛。实验结果表明，我们的方法在实现与现有技术相当的恢复性能的同时，还具有更高的计算效率。|
|**2024-09-26**|[Realistic Evaluation of Model Merging for Compositional Generalization](http://arxiv.org/abs/2409.18314)|null|模型融合已成为一种广泛使用的方法，可以将单个模型廉价地组合成一个模型，该模型继承了它们的性能并获得了更好的性能。这种流行促进了许多新融合方法的快速发展，这些方法通常在不同的实验环境中得到验证，并且经常在对模型架构、数据可用性和计算预算做出的假设方面有所不同。在这项工作中，我们通过在共享实验环境中评估不同的融合方法并精确识别每种方法的实际要求，来描述它们的相对优点。具体来说，我们的设置侧重于使用融合来实现图像分类、图像生成和自然语言处理中功能的组合泛化。此外，我们还测量了不同融合方法的计算成本，以及它们在扩展融合模型数量时的性能。总的来说，我们的结果阐明了模型融合领域的现状，并提供了一个全面而严谨的实验设置来测试新方法。|
|**2024-09-26**|[Advancing Object Detection in Transportation with Multimodal Large Language Models (MLLMs): A Comprehensive Review and Empirical Testing](http://arxiv.org/abs/2409.18286)|null|本研究旨在全面回顾和实证评估多模态大型语言模型 (MLLM) 和大型视觉模型 (VLM) 在交通系统目标检测中的应用。首先，我们介绍了 MLLM 在交通应用中的潜在优势，并对以往研究中现有的 MLLM 技术进行了全面回顾。我们重点介绍了它们在各种交通场景下目标检测的有效性和局限性。其次，我们概述了交通应用中端到端目标检测的分类以及未来方向。在此基础上，我们提出了实证分析，在三个现实交通问题上测试 MLLM，这些问题包括目标检测任务，即道路安全属性提取、安全关键事件检测和热图像视觉推理。我们的研究结果提供了对 MLLM 性能的详细评估，揭示了其优势和需要改进的方面。最后，我们讨论了 MLLM 在增强交通目标检测方面的实际局限性和挑战，从而为该关键领域的未来研究和开发提供了路线图。|
|**2024-09-26**|[DARE: Diverse Visual Question Answering with Robustness Evaluation](http://arxiv.org/abs/2409.18023)|null|视觉语言模型 (VLM) 扩展了仅文本大型语言模型和仅视觉模型的卓越能力，并且能够从多模态视觉文本输入中学习和处理。 虽然现代 VLM 在许多标准图像分类和图像文本匹配任务中表现良好，但它们仍然难以应对许多关键的视觉语言 (VL) 推理能力，例如计数和空间推理。 此外，虽然它们可能对指令和/或评估协议的微小变化非常脆弱，但现有基准测试未能评估它们的稳健性（或者更确切地说是缺乏稳健性）。 为了将具有挑战性的 VL 场景与全面的稳健性评估相结合，我们引入了 DARE，即具有稳健性评估的多样化视觉问答，这是一个精心创建和策划的多项选择 VQA 基准测试。 DARE 评估 VLM 在五个不同类别上的性能，并包括四个基于以下变化的稳健性评估：提示、答案选项子集、输出格式和正确答案的数量。 在其他一系列发现中，我们报告说，最先进的 VLM 仍然难以回答大多数类别的问题，并且无法在测试的稳健性评估中始终如一地提供其峰值性能。 选项子集的最坏情况性能比标准情况下的性能低 34%。 LLaVA 1.6 和 Idefics2 等开源 VLM 的稳健性无法与 GPT-4 和 Gemini 等闭源模型相提并论，但即使是后者仍然非常容易受到不同变化的影响。|
|**2024-09-26**|[A New Dataset for Monocular Depth Estimation Under Viewpoint Shifts](http://arxiv.org/abs/2409.17851)|null|单目深度估计是自动驾驶和许多其他计算机视觉应用的关键任务。虽然该领域已经取得了重大进展，但视角变化对深度估计模型的影响在很大程度上仍未得到充分探索。本文介绍了一种新的数据集和评估方法，用于量化不同相机位置和方向对单目深度估计性能的影响。我们提出了一种基于单应性估计和目标检测的真值策略，无需昂贵的激光雷达传感器。我们从多个视点收集了道路场景的多样化数据集，并用它来评估现代深度估计模型对几何偏移的鲁棒性。在公共数据集上评估了我们策略的有效性后，我们提供了对当前模型局限性的宝贵见解，并强调了在实际应用中考虑视点变化的重要性。||
|**2024-09-26**|[Cascade Prompt Learning for Vision-Language Model Adaptation](http://arxiv.org/abs/2409.17805)|null|提示学习已成为一种有效的方法，可以提高视觉语言模型（VLM）在下游任务中的性能，例如CLIP。然而，当前可学习的提示标记主要用于适应任务的单一阶段（即，调整提示），容易导致过拟合风险。在这项工作中，我们提出了一种新颖的级联提示学习CasPL框架，使提示学习能够同时服务于通用和特定专业知识（即，增强和调整提示）。具体来说，CasPL是一种新的学习范式，包括两个不同阶段的可学习提示：第一个增强提示旨在通过使用大量未标记的域图像对齐其预测的logits，从高级更大的CLIP教师模型中提取域一般知识。然后，第二个调整提示与冻结的第一组级联，以微调下游任务，遵循先前研究中采用的方法。通过这种方式，CasPL可以有效地将域一般表示和任务特定表示捕获到明确不同的渐进提示组中，从而潜在地缓解目标域中的过拟合问题。值得注意的是，CasPL是一个即插即用模块，可以无缝集成到任何现有的提示学习方法中。CasPL在性能和推理速度之间取得了显著更好的平衡，这对于在资源受限的环境中部署较小的VLM模型尤其有利。与之前的最先进方法PromptSRC相比，CasPL在11个图像分类数据集上，基础类的平均改进率为1.85%，新类的平均改进率为3.44%，调和平均值的平均改进率为2.72%。代码公开地址：https://github.com/megvii-research/CasPL。||
|**2024-09-26**|[Taming Diffusion Prior for Image Super-Resolution with Domain Shift SDEs](http://arxiv.org/abs/2409.17778)|**[link](https://github.com/qinpengcui/dossr)**|基于扩散的图像超分辨率 (SR) 模型因其强大的图像恢复能力而引起了广泛关注。然而，现有的扩散模型通常难以在效率和性能之间取得最佳平衡。它们通常要么忽略了利用现有大量预训练模型的潜力，限制了其生成能力，要么需要从随机噪声开始进行数十次前向传递，从而降低了推理效率。在本文中，我们提出了 DoSSR，一种基于域迁移扩散的 SR 模型，它利用预训练扩散模型的生成能力，并通过以低分辨率 (LR) 图像初始化扩散过程来显著提高效率。我们方法的核心是一个与现有扩散模型无缝集成的域迁移方程。这种集成不仅提高了扩散先验的利用，还提高了推理效率。此外，我们通过将离散迁移过程转换为连续公式（称为 DoS-SDE）来推进我们的方法。这一进步带来了快速且定制化的求解器，进一步提高了采样效率。实验结果表明，我们提出的方法在合成数据集和真实世界数据集上均达到了最先进的性能，同时仅需 5 个采样步骤。与之前基于扩散先验的方法相比，我们的方法实现了 5-7 倍的显著加速，证明了其卓越的效率。代码：https://github.com/QinpengCui/DoSSR。||
|**2024-09-26**|[LGFN: Lightweight Light Field Image Super-Resolution using Local Convolution Modulation and Global Attention Feature Extraction](http://arxiv.org/abs/2409.17759)|null|光场（LF）能够将三维场景信息编码成四维光场图像，在诸如后期重聚焦和深度感知等领域有着广泛的应用。光场图像超分辨率（SR）旨在提升受限于光场相机传感器性能的图像分辨率。尽管现有方法已经取得了可喜的成果，但由于模型不够轻量化，限制了其实际应用。本文提出了一种名为LGFN的轻量级模型，它集成了不同视角的局部和全局特征以及不同通道的特征，用于光场图像超分辨率。具体来说，由于不同子孔径图像中相同像素位置的相邻区域表现出相似的结构关系，我们设计了一个基于轻量级CNN的特征提取模块（DGCE），通过特征调制更好地提取局部特征。同时，由于光场图像中超出边界的像素位置存在较大差异，我们提出了一个高效的空间注意力模块（ESAM），它使用可分解的大核卷积来获得更大的感受野，以及一个高效的通道注意力模块（ECAM）。与现有参数量大的光场图像超分辨率模型相比，我们的模型参数量为0.45M，FLOPs为19.33G，取得了具有竞争力的效果。大量的消融实验验证了我们提出的方法的有效性，在NTIRE2024光场超分辨率挑战赛的Track 2保真度和效率赛道中排名第二，在Track 1保真度赛道中排名第七。||
|**2024-09-26**|[Scene Understanding in Pick-and-Place Tasks: Analyzing Transformations Between Initial and Final Scenes](http://arxiv.org/abs/2409.17720)|null|随着机器人在日常任务中越来越多地与人类合作，采取措施使机器人系统能够理解环境变得至关重要。这项工作侧重于场景理解，以根据场景的初始图像和最终图像检测拾取和放置任务。为此，我们收集了一个用于目标检测和拾取放置任务检测的数据集。随后训练了一个 YOLOv5 网络来检测初始场景和最终场景中的目标。给定检测到的目标及其边界框，我们提出了两种方法来检测将初始场景转换为最终场景的拾取和放置任务。一种是几何方法，它跟踪目标在两个场景中的运动，并根据场景内移动的边界框的交集进行工作。相反，基于 CNN 的方法利用卷积神经网络将具有相交边界框的目标分类为 5 类，显示相关目标之间的空间关系。然后，通过分析包含这两个场景的实验，得出执行的拾取和放置任务。结果表明，在某些场景下，使用 VGG16 骨干网络的基于 CNN 的方法的成功率比几何方法高出约 12 个百分点，总体成功率为 84.3%。||
|**2024-09-26**|[Unifying Dimensions: A Linear Adaptive Approach to Lightweight Image Super-Resolution](http://arxiv.org/abs/2409.17597)|null|基于窗口的 Transformer 由于其通过局部自注意力机制 (SA) 进行自适应建模的能力，在超分辨率任务中展现出卓越的性能。然而，与卷积神经网络相比，它们表现出更高的计算复杂度和推理延迟。在本文中，我们首先确定 Transformer 的适应性源于其自适应空间聚合和先进的结构设计，而其高延迟则源于与局部 SA 相关的计算成本和内存布局转换。为了模拟这种聚合方法，我们提出了一种有效的基于卷积的线性焦点可分离注意力机制 (FSA)，允许以线性复杂度进行长距离动态建模。此外，我们引入了一种有效的双分支结构，结合超轻量级信息交换模块 (IEM)，以增强 Token Mixer 对信息的聚合能力。最后，在结构方面，我们通过结合自门控机制来修改现有的基于空间门控的前馈神经网络，以保留高维通道信息，从而能够对更复杂的关系进行建模。基于这些改进，我们构建了一个名为线性自适应混合网络 (LAMNet) 的基于卷积的 Transformer 框架。大量实验表明，LAMNet 在保持卷积神经网络计算效率的同时，实现了比现有基于 SA 的 Transformer 方法更好的性能，推理时间可达 \(3\times\) 加速。代码将公开发布在：https://github.com/zononhzy/LAMNet。||
|**2024-09-26**|[Let the Quantum Creep In: Designing Quantum Neural Network Models by Gradually Swapping Out Classical Components](http://arxiv.org/abs/2409.17583)|**[link](https://github.com/peiyong-addwater/let-the-quantum-creep-in)**|人工智能 (AI) 凭借其乘数效应和在多个领域的广泛应用，可能成为量子计算的重要应用领域。由于现代人工智能系统通常建立在神经网络之上，因此量子神经网络的设计成为将量子计算集成到人工智能中的关键挑战。为了更细致地描述量子组件对神经网络性能的影响，我们提出了一个框架，在该框架中，经典神经网络层逐渐被具有相同输入和输出类型、同时保持层间信息流不变的量子层所取代，这不同于目前大多数量子神经网络的研究，后者倾向于端到端的量子模型。我们从一个没有任何标准化层或激活函数的简单三层经典神经网络开始，逐步将经典层更改为相应的量子版本。我们对 MNIST、FashionMNIST 和 CIFAR-10 等图像分类数据集进行了数值实验，以证明系统引入量子组件所带来的性能变化。通过这个框架，我们的研究为未来量子神经网络模型的设计提供了新的思路，在这些模型中，寻找能够利用经典世界和量子世界优势的方法和框架可能更为有利。||
|**2024-09-26**|[General Compression Framework for Efficient Transformer Object Tracking](http://arxiv.org/abs/2409.17564)|null|基于Transformer的跟踪器在视觉目标跟踪领域占据主导地位。虽然这些跟踪器表现出良好的性能，但由于效率低下，它们在资源受限设备上的部署仍然具有挑战性。为了提高推理效率并降低计算成本，先前的方法旨在设计轻量级跟踪器或将知识从较大的教师模型提炼到更紧凑的学生模型中。然而，这些解决方案通常以牺牲精度为代价来提高速度。因此，我们提出了一种通用的高效Transformer目标跟踪模型压缩框架CompressTracker，以将预训练的跟踪模型压缩成轻量级跟踪器，同时最大限度地减少性能下降。我们的方法采用了一种新颖的阶段划分策略，将教师模型的Transformer层划分为不同的阶段，使学生模型能够更有效地模拟每个相应的教师阶段。此外，我们还设计了一种独特的替换训练技术，该技术涉及用教师模型中的相应阶段随机替换学生模型中的特定阶段，而不是孤立地训练学生模型。替换训练增强了学生模型复制教师模型行为的能力。为了进一步迫使学生模型模拟教师模型，我们引入了预测指导和阶段性特征模拟，以便在教师模型的压缩过程中提供额外的监督。我们的框架CompressTracker在结构上是不可知的，使其与任何Transformer架构兼容。我们进行了一系列实验，以验证CompressTracker的有效性和通用性。我们的CompressTracker-4具有4个Transformer层，它是从OSTrack压缩而来的，在LaSOT上保留了约96%的性能（66.1% AUC），同时实现了2.17倍的加速。||
|**2024-09-26**|[CAMOT: Camera Angle-aware Multi-Object Tracking](http://arxiv.org/abs/2409.17533)|null|本文提出了CAMOT，一种用于多目标跟踪的简单相机角度估计器，用于解决两个问题：1）遮挡和2）深度方向上的距离估计不准确。在假设每个视频帧中的多个目标位于平面上，CAMOT 使用目标检测来估计相机角度。此外，它还给出了每个目标的深度，从而实现了伪 3D MOT。我们通过将其添加到 MOT17 和 MOT20 数据集上的各种 2D MOT 方法中来评估其性能，并确认了其有效性。将 CAMOT 应用于 ByteTrack，我们在 MOT17 中获得了 63.8% 的 HOTA、80.6% 的 MOTA 和 78.5% 的 IDF1，这些都是最先进的结果。它的计算成本明显低于现有的基于深度学习的跟踪深度估计器。||
|**2024-09-18**|[Applications of Knowledge Distillation in Remote Sensing: A Survey](http://arxiv.org/abs/2409.12111)|null|随着遥感 (RS) 领域模型复杂性的不断提高，对平衡模型精度和计算效率的解决方案的需求也日益增长。知识蒸馏 (KD) 已成为满足这一需求的强大工具，能够在不显著降低性能的情况下，将知识从大型复杂模型迁移到更小、更高效的模型。这篇综述文章广泛考察了 KD 及其在遥感领域的创新应用。KD 是一种将知识从复杂、通常笨重的模型（教师）迁移到更紧凑、更高效的模型（学生）的技术，已经在各个领域得到了显著的发展和应用。首先，我们介绍了 KD 方法的基本概念和历史进程。文章重点介绍了采用 KD 的优势，特别是在模型压缩、计算效率提高和性能改善方面，这些优势对于 RS 场景中的实际部署至关重要。文章提供了 KD 技术的全面分类，其中每个类别都经过严格分析，以证明替代方案的广度和深度，并通过具体的案例研究展示了 KD 方法在 RS 任务中的实际应用，例如实例分割和目标检测。此外，该综述还讨论了 KD 在遥感领域面临的挑战和局限性，包括实际约束和未来的发展方向，为遥感领域的研究人员和从业者提供了全面的概述。通过这种组织方式，本文不仅阐明了 KD 研究的现状，而且为未来的研究方向奠定了基础，从而为学术研究和实际应用做出了重大贡献。||
|**2024-09-18**|[Unraveling the Hessian: A Key to Smooth Convergence in Loss Function Landscapes](http://arxiv.org/abs/2409.11995)|**[link](https://github.com/kisnikser/landscape-hessian)**|神经网络的损失景观是其训练的一个关键方面，理解其属性对于提高其性能至关重要。在本文中，我们研究了当样本量增加时损失曲面如何变化，这是一个以前未被探索的问题。我们从理论上分析了全连接神经网络中损失景观的收敛性，并推导出在样本中添加新对象时损失函数值差异的上界。我们的实证研究在各种数据集上证实了这些结果，证明了图像分类任务中损失函数曲面的收敛性。我们的发现为神经损失景观的局部几何提供了见解，并对样本量确定技术的发展具有意义。||
|**2024-09-18**|[Agglomerative Token Clustering](http://arxiv.org/abs/2409.11923)|null|我们提出了聚合式Token聚类（ATC），这是一种新颖的Token合并方法，在图像分类、图像合成以及目标检测和分割任务中始终优于以前的Token合并和剪枝方法。ATC通过自下而上的层次聚类来合并聚类，无需引入额外的可学习参数。我们发现ATC在所有任务中都实现了最先进的性能，甚至在应用于现成模型时（即无需微调）也能与之前的最先进技术相媲美。当应用于低保留率时，ATC特别有效，在这种情况下，只有一小部分Token被保留，并且保持任务性能特别困难。||
|**2024-09-18**|[Distillation-free Scaling of Large SSMs for Images and Videos](http://arxiv.org/abs/2409.11867)|null|State-space models (SSMs), exemplified by S4, have introduced a novel context modeling method by integrating state-space techniques into deep learning. However, they struggle with global context modeling due to their data-independent matrices. The Mamba model addressed this with data-dependent variants via the S6 selective-scan algorithm, enhancing context modeling, especially for long sequences. However, Mamba-based architectures are difficult to scale with respect to the number of parameters, which is a major limitation for vision applications. This paper addresses the scalability issue of large SSMs for image classification and action recognition without requiring additional techniques like knowledge distillation. We analyze the distinct characteristics of Mamba-based and Attention-based models, proposing a Mamba-Attention interleaved architecture that enhances scalability, robustness, and performance. We demonstrate that the stable and efficient interleaved architecture resolves the scalability issue of Mamba-based architectures for images and videos and increases robustness to common artifacts like JPEG compression. Our thorough evaluation on the ImageNet-1K, Kinetics-400 and Something-Something-v2 benchmarks demonstrates that our approach improves the accuracy of state-of-the-art Mamba-based architectures by up to $+1.7$ .||
|**2024-09-18**|[RockTrack: A 3D Robust Multi-Camera-Ken Multi-Object Tracking Framework](http://arxiv.org/abs/2409.11749)|null|随着3D目标检测技术的快速发展，尤其是在经济高效的多相机设置中，3D多目标跟踪（MOT）获得了显著的性能提升。然而，目前流行的端到端多相机跟踪器训练方法会导致模型依赖于特定的检测器，从而限制了其通用性。此外，现有的通用跟踪器忽略了多相机检测器的独特特征，即运动观测的不可靠性和视觉信息的可用性。为了应对这些挑战，我们提出了RockTrack，一种面向多相机检测器的3D MOT方法。RockTrack遵循“检测跟踪”框架，兼容各种现成的检测器。RockTrack包含一个置信度引导的预处理模块，用于从单个检测器的不同表示空间中提取可靠的运动和图像观测结果。然后，这些观测结果会在关联模块中融合，该模块利用几何和外观线索来最大程度地减少错配。最终的匹配结果通过分阶段估计过程进行传播，形成启发式噪声建模的基础。此外，我们引入了一种新颖的外观相似性度量方法，用于在多相机设置中明确表征目标亲和度。RockTrack在nuScenes仅视觉跟踪排行榜上实现了最先进的性能，AMOTA达到59.1%，同时展现出惊人的计算效率。||
|**2024-09-18**|[Few-Shot Learning Approach on Tuberculosis Classification Based on Chest X-Ray Images](http://arxiv.org/abs/2409.11644)|null|Tuberculosis (TB) is caused by the bacterium Mycobacterium tuberculosis, primarily affecting the lungs. Early detection is crucial for improving treatment effectiveness and reducing transmission risk. Artificial intelligence (AI), particularly through image classification of chest X-rays, can assist in TB detection. However, class imbalance in TB chest X-ray datasets presents a challenge for accurate classification. In this paper, we propose a few-shot learning (FSL) approach using the Prototypical Network algorithm to address this issue. We compare the performance of ResNet-18, ResNet-50, and VGG16 in feature extraction from the TBX11K Chest X-ray dataset. Experimental results demonstrate classification accuracies of 98.93% for ResNet-18, 98.60% for ResNet-50, and 33.33% for VGG16. These findings indicate that the proposed method outperforms others in mitigating data imbalance, which is particularly beneficial for disease classification applications.||
|**2024-09-17**|[VALO: A Versatile Anytime Framework for LiDAR-based Object Detection Deep Neural Networks](http://arxiv.org/abs/2409.11542)|**[link](https://github.com/csl-ku/valo)**|This work addresses the challenge of adapting dynamic deadline requirements for LiDAR object detection deep neural networks (DNNs). The computing latency of object detection is critically important to ensure safe and efficient navigation. However, state-of-the-art LiDAR object detection DNNs often exhibit significant latency, hindering their real-time performance on resource-constrained edge platforms. Therefore, a tradeoff between detection accuracy and latency should be dynamically managed at runtime to achieve optimum results.   In this paper, we introduce VALO (Versatile Anytime algorithm for LiDAR Object detection), a novel data-centric approach that enables anytime computing of 3D LiDAR object detection DNNs. VALO employs a deadline-aware scheduler to selectively process input regions, making execution time and accuracy tradeoffs without architectural modifications. Additionally, it leverages efficient forecasting of past detection results to mitigate possible loss of accuracy due to partial processing of input. Finally, it utilizes a novel input reduction technique within its detection heads to significantly accelerate execution without sacrificing accuracy.   We implement VALO on state-of-the-art 3D LiDAR object detection networks, namely CenterPoint and VoxelNext, and demonstrate its dynamic adaptability to a wide range of time constraints while achieving higher accuracy than the prior state-of-the-art. Code is available athttps://github.com/CSL-KU/VALO}{github.com/CSL-KU/VALO.||
|**2024-09-17**|[Enhancing the Reliability of LiDAR Point Cloud Sampling: A Colorization and Super-Resolution Approach Based on LiDAR-Generated Images](http://arxiv.org/abs/2409.11532)|null|In recent years, Light Detection and Ranging (LiDAR) technology, a critical sensor in robotics and autonomous systems, has seen significant advancements. These improvements include enhanced resolution of point clouds and the capability to provide 360{\deg} low-resolution images. These images encode various data such as depth, reflectivity, and near-infrared light within the pixels. However, an excessive density of points and conventional point cloud sampling can be counterproductive, particularly in applications such as LiDAR odometry, where misleading points and degraded geometry information may induce drift errors. Currently, extensive research efforts are being directed towards leveraging LiDAR-generated images to improve situational awareness. This paper presents a comprehensive review of current deep learning (DL) techniques, including colorization and super-resolution, which are traditionally utilized in conventional computer vision tasks. These techniques are applied to LiDAR-generated images and are analyzed qualitatively. Based on this analysis, we have developed a novel approach that selectively integrates the most suited colorization and super-resolution methods with LiDAR imagery to sample reliable points from the LiDAR point cloud. This approach aims to not only improve the accuracy of point cloud registration but also avoid mismatching caused by lacking geometry information, thereby augmenting the utility and precision of LiDAR systems in practical applications. In our evaluation, the proposed approach demonstrates superior performance compared to our previous work, achieving lower translation and rotation errors with a reduced number of points.||
|**2024-09-19**|[Super Resolution On Global Weather Forecasts](http://arxiv.org/abs/2409.11502)|null|Weather forecasting is a vitally important tool for tasks ranging from planning day to day activities to disaster response planning. However, modeling weather has proven to be challenging task due to its chaotic and unpredictable nature. Each variable, from temperature to precipitation to wind, all influence the path the environment will take. As a result, all models tend to rapidly lose accuracy as the temporal range of their forecasts increase. Classical forecasting methods use a myriad of physics-based, numerical, and stochastic techniques to predict the change in weather variables over time. However, such forecasts often require a very large amount of data and are extremely computationally expensive. Furthermore, as climate and global weather patterns change, classical models are substantially more difficult and time-consuming to update for changing environments. Fortunately, with recent advances in deep learning and publicly available high quality weather datasets, deploying learning methods for estimating these complex systems has become feasible. The current state-of-the-art deep learning models have comparable accuracy to the industry standard numerical models and are becoming more ubiquitous in practice due to their adaptability. Our group seeks to improve upon existing deep learning based forecasting methods by increasing spatial resolutions of global weather predictions. Specifically, we are interested in performing super resolution (SR) on GraphCast temperature predictions by increasing the global precision from 1 degree of accuracy to 0.5 degrees, which is approximately 111km and 55km respectively.||
|**2024-09-17**|[SLAck: Semantic, Location, and Appearance Aware Open-Vocabulary Tracking](http://arxiv.org/abs/2409.11235)|**[link](https://github.com/siyuanliii/slack)**|Open-vocabulary Multiple Object Tracking (MOT) aims to generalize trackers to novel categories not in the training set. Currently, the best-performing methods are mainly based on pure appearance matching. Due to the complexity of motion patterns in the large-vocabulary scenarios and unstable classification of the novel objects, the motion and semantics cues are either ignored or applied based on heuristics in the final matching steps by existing methods. In this paper, we present a unified framework SLAck that jointly considers semantics, location, and appearance priors in the early steps of association and learns how to integrate all valuable information through a lightweight spatial and temporal object graph. Our method eliminates complex post-processing heuristics for fusing different cues and boosts the association performance significantly for large-scale open-vocabulary tracking. Without bells and whistles, we outperform previous state-of-the-art methods for novel classes tracking on the open-vocabulary MOT and TAO TETA benchmarks. Our code is available at \href{https://github.com/siyuanliii/SLAck}{github.com/siyuanliii/SLAck}.||
|**2024-09-17**|[STCMOT: Spatio-Temporal Cohesion Learning for UAV-Based Multiple Object Tracking](http://arxiv.org/abs/2409.11234)|**[link](https://github.com/ydhcg-bobo/stcmot)**|Multiple object tracking (MOT) in Unmanned Aerial Vehicle (UAV) videos is important for diverse applications in computer vision. Current MOT trackers rely on accurate object detection results and precise matching of target reidentification (ReID). These methods focus on optimizing target spatial attributes while overlooking temporal cues in modelling object relationships, especially for challenging tracking conditions such as object deformation and blurring, etc. To address the above-mentioned issues, we propose a novel Spatio-Temporal Cohesion Multiple Object Tracking framework (STCMOT), which utilizes historical embedding features to model the representation of ReID and detection features in a sequential order. Concretely, a temporal embedding boosting module is introduced to enhance the discriminability of individual embedding based on adjacent frame cooperation. While the trajectory embedding is then propagated by a temporal detection refinement module to mine salient target locations in the temporal field. Extensive experiments on the VisDrone2019 and UAVDT datasets demonstrate our STCMOT sets a new state-of-the-art performance in MOTA and IDF1 metrics. The source codes are released at https://github.com/ydhcg-BoBo/STCMOT.||
|**2024-09-17**|[Vision foundation models: can they be applied to astrophysics data?](http://arxiv.org/abs/2409.11175)|**[link](https://github.com/elastufka/fm4astro)**|Vision foundation models, which have demonstrated significant potential in many multimedia applications, are often underutilized in the natural sciences. This is primarily due to mismatches between the nature of domain-specific scientific data and the typical training data used for foundation models, leading to distribution shifts. Scientific data often differ substantially in structure and characteristics; researchers frequently face the challenge of optimizing model performance with limited labeled data of only a few hundred or thousand images. To adapt foundation models effectively requires customized approaches in preprocessing, data augmentation, and training techniques. Additionally, each vision foundation model exhibits unique strengths and limitations, influenced by differences in architecture, training procedures, and the datasets used for training. In this work, we evaluate the application of various vision foundation models to astrophysics data, specifically images from optical and radio astronomy. Our results show that using features extracted by specific foundation models improves the classification accuracy of optical galaxy images compared to conventional supervised training. Similarly, these models achieve equivalent or better performance in object detection tasks with radio images. However, their performance in classifying radio galaxy images is generally poor and often inferior to traditional supervised training results. These findings suggest that selecting suitable vision foundation models for astrophysics applications requires careful consideration of the model characteristics and alignment with the specific requirements of the downstream tasks.||
|**2024-09-17**|[Unleashing the Potential of Mamba: Boosting a LiDAR 3D Sparse Detector by Using Cross-Model Knowledge Distillation](http://arxiv.org/abs/2409.11018)|null|The LiDAR-based 3D object detector that strikes a balance between accuracy and speed is crucial for achieving real-time perception in autonomous driving and robotic navigation systems. To enhance the accuracy of point cloud detection, integrating global context for visual understanding improves the point clouds ability to grasp overall spatial information. However, many existing LiDAR detection models depend on intricate feature transformation and extraction processes, leading to poor real-time performance and high resource consumption, which limits their practical effectiveness. In this work, we propose a Faster LiDAR 3D object detection framework, called FASD, which implements heterogeneous model distillation by adaptively uniform cross-model voxel features. We aim to distill the transformer's capacity for high-performance sequence modeling into Mamba models with low FLOPs, achieving a significant improvement in accuracy through knowledge transfer. Specifically, Dynamic Voxel Group and Adaptive Attention strategies are integrated into the sparse backbone, creating a robust teacher model with scale-adaptive attention for effective global visual context modeling. Following feature alignment with the Adapter, we transfer knowledge from the Transformer to the Mamba through latent space feature supervision and span-head distillation, resulting in improved performance and an efficient student model. We evaluated the framework on the Waymo and nuScenes datasets, achieving a 4x reduction in resource consumption and a 1-2\% performance improvement over the current SoTA methods.||
|**2024-09-17**|[TrajSSL: Trajectory-Enhanced Semi-Supervised 3D Object Detection](http://arxiv.org/abs/2409.10901)|null|Semi-supervised 3D object detection is a common strategy employed to circumvent the challenge of manually labeling large-scale autonomous driving perception datasets. Pseudo-labeling approaches to semi-supervised learning adopt a teacher-student framework in which machine-generated pseudo-labels on a large unlabeled dataset are used in combination with a small manually-labeled dataset for training. In this work, we address the problem of improving pseudo-label quality through leveraging long-term temporal information captured in driving scenes. More specifically, we leverage pre-trained motion-forecasting models to generate object trajectories on pseudo-labeled data to further enhance the student model training. Our approach improves pseudo-label quality in two distinct manners: first, we suppress false positive pseudo-labels through establishing consistency across multiple frames of motion forecasting outputs. Second, we compensate for false negative detections by directly inserting predicted object tracks into the pseudo-labeled scene. Experiments on the nuScenes dataset demonstrate the effectiveness of our approach, improving the performance of standard semi-supervised approaches in a variety of settings.||
|**2024-09-17**|[Single-Layer Learnable Activation for Implicit Neural Representation (SL $^{2}$A-INR)](http://arxiv.org/abs/2409.10836)|null|隐式神经表示 (INR) 利用神经网络将坐标输入转换为相应的属性，近年来在多个视觉相关领域取得了重大进展。然而，INR 的性能很大程度上受其多层感知器 (MLP) 架构中使用的非线性激活函数选择的影响。目前已经研究了多种非线性方法；然而，当前的 INR 在捕获高频分量、多样信号类型和处理逆问题方面面临局限性。我们已经确定，通过引入 INR 的范式转变可以大大缓解这些问题。我们发现，在初始层具有可学习激活函数的架构可以表示底层信号中的精细细节。具体来说，我们提出了 SL$^{2}$A-INR，这是一种用于 INR 的混合网络，具有单层可学习激活函数，从而提高了传统基于 ReLU 的 MLP 的有效性。我们的方法在各种任务中均表现出色，包括图像表示、3D 形状重建、图像修复、单图像超分辨率、CT 重建和新视图合成。通过综合实验，SL$^{2}$ A-INR 在 INR 的准确性、质量和收敛速度方面树立了新的基准。||
|**2024-09-17**|[Context-Dependent Interactable Graphical User Interface Element Detection for VR Applications](http://arxiv.org/abs/2409.10811)|null|In recent years, Virtual Reality (VR) has emerged as a transformative technology, offering users immersive and interactive experiences across diversified virtual environments. Users can interact with VR apps through interactable GUI elements (IGEs) on the stereoscopic three-dimensional (3D) graphical user interface (GUI). The accurate recognition of these IGEs is instrumental, serving as the foundation of many software engineering tasks, including automated testing and effective GUI search. The most recent IGE detection approaches for 2D mobile apps typically train a supervised object detection model based on a large-scale manually-labeled GUI dataset, usually with a pre-defined set of clickable GUI element categories like buttons and spinners. Such approaches can hardly be applied to IGE detection in VR apps, due to a multitude of challenges including complexities posed by open-vocabulary and heterogeneous IGE categories, intricacies of context-sensitive interactability, and the necessities of precise spatial perception and visual-semantic alignment for accurate IGE detection results. Thus, it is necessary to embark on the IGE research tailored to VR apps. In this paper, we propose the first zero-shot cOntext-sensitive inteRactable GUI ElemeNT dEtection framework for virtual Reality apps, named Orienter. By imitating human behaviors, Orienter observes and understands the semantic contexts of VR app scenes first, before performing the detection. The detection process is iterated within a feedback-directed validation and reflection loop. Specifically, Orienter contains three components, including (1) Semantic context comprehension, (2) Reflection-directed IGE candidate detection, and (3) Context-sensitive interactability classification. Extensive experiments on the dataset demonstrate that Orienter is more effective than the state-of-the-art GUI element detection approaches.||
|**2024-09-16**|[Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?](http://arxiv.org/abs/2409.10775)|null|图像分类模型，包括卷积神经网络（CNN），在各种分类任务中表现良好，但在部分遮挡的情况下表现不佳，例如，物体被部分遮挡在相机视野之外的情况。已经出现了一些方法来提高遮挡情况下的性能，包括数据增强、基于部分的聚类，以及更强大的架构，包括视觉Transformer（ViT）模型，这些方法在一定程度上已经根据其在部分遮挡下对物体进行分类的能力进行了评估。然而，对这些方法的评估很大程度上依赖于包含人工遮挡的图像，这些图像通常是计算机生成的，因此标注成本低廉。此外，这些方法很少相互比较，许多方法是与早期、现在已经过时的深度学习模型进行比较的。我们贡献了遮挡下图像识别（IRUO）数据集，该数据集基于最近开发的遮挡视频实例分割（OVIS）数据集（arXiv:2102.01558）。IRUO利用真实世界和人工遮挡的图像来测试和比较领先方法在视觉识别任务中对部分遮挡的鲁棒性。此外，我们还贡献了使用IRUO图像进行的人类研究的设计和结果，该研究评估了人类在多个级别和类型的遮挡下的分类性能。我们发现，与早期的基于CNN的模型相比，现代基于CNN的模型在遮挡图像上的识别精度有所提高，并且基于ViT的模型在遮挡图像上的精度高于基于CNN的模型，其性能仅略低于人类精度。我们还发现，某些类型的遮挡，包括漫射遮挡，即相关物体通过栅栏和树叶等遮挡物上的“孔洞”可见，与人类相比，这种遮挡会大大降低深度识别模型的精度，尤其是那些具有CNN骨干的模型。||
|**2024-09-16**|[CoMamba: Real-time Cooperative Perception Unlocked with State Space Models](http://arxiv.org/abs/2409.10699)|null|Cooperative perception systems play a vital role in enhancing the safety and efficiency of vehicular autonomy. Although recent studies have highlighted the efficacy of vehicle-to-everything (V2X) communication techniques in autonomous driving, a significant challenge persists: how to efficiently integrate multiple high-bandwidth features across an expanding network of connected agents such as vehicles and infrastructure. In this paper, we introduce CoMamba, a novel cooperative 3D detection framework designed to leverage state-space models for real-time onboard vehicle perception. Compared to prior state-of-the-art transformer-based models, CoMamba enjoys being a more scalable 3D model using bidirectional state space models, bypassing the quadratic complexity pain-point of attention mechanisms. Through extensive experimentation on V2X/V2V datasets, CoMamba achieves superior performance compared to existing methods while maintaining real-time processing capabilities. The proposed framework not only enhances object detection accuracy but also significantly reduces processing time, making it a promising solution for next-generation cooperative perception systems in intelligent transportation networks.||
|**2024-09-16**|[Frequency-Guided Masking for Enhanced Vision Self-Supervised Learning](http://arxiv.org/abs/2409.10362)|null|We present a novel frequency-based Self-Supervised Learning (SSL) approach that significantly enhances its efficacy for pre-training. Prior work in this direction masks out pre-defined frequencies in the input image and employs a reconstruction loss to pre-train the model. While achieving promising results, such an implementation has two fundamental limitations as identified in our paper. First, using pre-defined frequencies overlooks the variability of image frequency responses. Second, pre-trained with frequency-filtered images, the resulting model needs relatively more data to adapt to naturally looking images during fine-tuning. To address these drawbacks, we propose FOurier transform compression with seLf-Knowledge distillation (FOLK), integrating two dedicated ideas. First, inspired by image compression, we adaptively select the masked-out frequencies based on image frequency responses, creating more suitable SSL tasks for pre-training. Second, we employ a two-branch framework empowered by knowledge distillation, enabling the model to take both the filtered and original images as input, largely reducing the burden of downstream tasks. Our experimental results demonstrate the effectiveness of FOLK in achieving competitive performance to many state-of-the-art SSL methods across various downstream tasks, including image classification, few-shot learning, and semantic segmentation.||
|**2024-09-13**|[Optically-Validated Microvascular Phantom for Super-Resolution Ultrasound Imaging](http://arxiv.org/abs/2409.09031)|null|超分辨率超声 (SRUS) 通过定位和跟踪空间隔离的微泡造影剂，可视化超声衍射极限（波长 ( $λ$ )/2）以外的微血管结构。SRUS 模型通常由简单的管状结构组成，其中直径小于 100 微米的通道不可用。此外，这些模型通常易碎且不稳定，真值验证有限，并且其简单的结构限制了 SRUS 算法的评估。为了帮助 SRUS 的开发，需要具有已知且生理相关的微血管结构的坚固耐用的模型，以便进行可重复的 SRUS 测试。这项工作提出了一种制造耐用微血管模型的方法，该模型允许进行光学测量以进行 SRUS 验证。该方法使用嵌入聚二甲基硅氧烷中的微血管阴模来制造微血管模型。展示了具有可变微血管密度的分支微血管模型，其光学验证的血管直径低至约 60 微米（λ/5.8；λ = 约 350 微米）。进行了 SRUS 成像并通过光学测量进行了验证。平均 SRUS 误差为 15.61 微米（λ/22），标准偏差误差为 11.44 微米。一旦定位的微泡数量超过每个估计直径 1000 个，平均误差降低至 7.93 微米（λ/44）。此外，制造一年后测得的声学和光学特性变化小于 10% 以及模型的机械韧性证明了其长期耐用性。这项工作提出了一种制造耐用且经过光学验证的复杂微血管模型的方法，该模型可用于量化 SRUS 性能并促进其进一步发展。||
|**2024-09-13**|[Pushing Joint Image Denoising and Classification to the Edge](http://arxiv.org/abs/2409.08943)|null|本文中，我们将图像分类和图像去噪相结合，旨在增强人类对边缘设备（如低照度监控摄像头）所拍摄噪声图像的感知能力。在这种情况下，重要的是要保留人类验证自动分类决策的能力，从而联合对图像进行去噪以增强人类感知。由于边缘设备计算能力有限，我们通过提出一种集成这两项任务的新型架构来明确优化效率。此外，我们还修改了一种神经架构搜索（NAS）方法，该方法搜索分类器以搜索集成模型，同时优化目标延迟、分类精度和去噪性能。NAS 架构在去噪和分类方面均优于我们手动设计的方案，可显著改善人类感知。我们的方法使用户能够构建针对医疗成像、监控系统和工业检测等领域的定制架构。||
|**2024-09-13**|[Interactive Masked Image Modeling for Multimodal Object Detection in Remote Sensing](http://arxiv.org/abs/2409.08885)|null|遥感影像中的目标检测在地球观测的各个应用中都起着至关重要的作用。然而，与自然场景图像中的目标检测不同，由于不同地形中存在大量的小型且通常难以察觉的目标，这项任务尤其具有挑战性。为了应对这些挑战，可以使用多模态学习来整合来自不同数据模态的特征，从而提高检测精度。然而，多模态学习的性能往往受到标记数据集大小有限的限制。在本文中，我们建议使用掩蔽图像建模（MIM）作为预训练技术，利用未标记数据的自监督学习来提高检测性能。然而，传统的 MIM 方法（如 MAE）使用不包含任何上下文信息的掩码标记，由于缺乏与图像其他部分的交互，难以捕捉到细粒度的细节。为了解决这个问题，我们提出了一种新的交互式 MIM 方法，可以在不同标记之间建立交互，这对于遥感中的目标检测特别有利。大量的消融研究和评估证明了我们方法的有效性。||
|**2024-09-13**|[Direct-CP: Directed Collaborative Perception for Connected and Autonomous Vehicles via Proactive Attention](http://arxiv.org/abs/2409.08840)|null|协同感知 (CP) 利用来自联网和自动驾驶车辆 (CAV) 的视觉数据来增强自车视野 (FoV)。尽管最近取得了进展，但目前的 CP 方法几乎平等地扩展了自车的 360 度感知范围，这面临着两个关键挑战。首先，在交通分布不均匀的地区，关注交通流量小的方向带来的好处有限。其次，在有限的通信预算下，为不太重要的方向分配过多的带宽会降低更重要区域的感知精度。为了解决这些问题，我们提出了 Direct-CP，一种主动且方向感知的 CP 系统，旨在改善特定方向的 CP。我们的核心理念是使自车能够主动发出其感兴趣方向的信号，并重新调整其注意力以增强局部方向性 CP 性能。为此，我们首先提出了一种 RSU 辅助方向掩蔽机制，以帮助自车识别重要方向。此外，我们设计了一个方向感知的选择性注意模块，根据自车的方向优先级、通信预算和 CAV 的位置数据，明智地聚合相关特征。此外，我们引入了方向加权检测损失 (DWLoss) 来捕捉方向性 CP 结果与真实情况之间的差异，从而促进有效的模型训练。在 V2X-Sim 2.0 数据集上进行的大量实验表明，与最先进的协作 3D 目标检测方法相比，我们的方法在感兴趣方向的局部感知精度提高了 19.8%，整体感知精度提高了 2.5%。||
|**2024-09-13**|[Test-time Training for Hyperspectral Image Super-resolution](http://arxiv.org/abs/2409.08667)|null|高光谱图像 (HSI) 超分辨率 (SR) 的研究进展仍然落后于 RGB 图像 SR 的研究。HSI 通常具有大量的波段，因此准确地模拟 HSI SR 的波段间交互非常困难。此外，HSI SR 的训练数据难以获取，因此数据集通常很小。在这项工作中，我们提出了一种新的测试时训练方法来解决这个问题。具体来说，我们开发了一个新的自训练框架，可以生成更准确的伪标签和更准确的 LR-HR 关系，以便模型可以使用它们进行进一步训练以提高性能。为了更好地支持我们的测试时训练方法，我们还提出了一种新的网络架构来学习 HSI SR，而无需对波段间交互进行建模，并提出了一种新的数据增强方法 Spectral Mixup，以增加测试时训练数据的的多样性。我们还收集了一个新的 HSI 数据集，其中包含从食物到植被、材料和一般场景等各种有趣对象的图像。在多个数据集上的大量实验表明，我们的方法可以在测试时训练后显着提高预训练模型的性能，并在 HSI SR 方面显着优于竞争方法。||
|**2024-09-13**|[Low Complexity DoA-ToA Signature Estimation for Multi-Antenna Multi-Carrier Systems](http://arxiv.org/abs/2409.08650)|null|准确的方向估计 (DoA) 和到达时间 (ToA) 估计是声纳、雷达、通信和双功能雷达通信 (DFRC) 等多种无线系统的严格要求。由于使用高载波频率和带宽，这些系统大多数设计有多个天线和子载波。尽管大阵列机制下的分辨率很高，但由于频谱泄漏效应，实际的网格估计方法的 DoA-ToA 估计精度仍然存在估计不准确的问题。在本文中，我们提出了针对具有正交频分复用 (OFDM) 信号的多天线多载波系统的 DoA-ToA 估计方法。在第一种方法中，我们应用了基于离散傅立叶变换 (DFT) 的粗略特征估计，并提出了一种低复杂度的多级微调方法，以极大地提高估计精度。第二种方法基于压缩感知，其中我们通过采用比天线和子载波基数实际数量更多的二维过完备角度延迟字典来实现超分辨率。与向量化一维正交匹配追踪 (OMP) 方法不同，我们将低复杂度的二维 OMP 方法应用于矩阵数据模型，这使得在大型阵列机制中使用压缩感知方法变得切实可行。通过数值仿真，我们表明我们提出的方法实现了与基于子空间的二维多重信号分类 (MUSIC) 方法相似的性能，并且计算复杂度显着降低。||
|**2024-09-13**|[Byzantine-Robust and Communication-Efficient Distributed Learning via Compressed Momentum Filtering](http://arxiv.org/abs/2409.08640)|null|分布式学习已成为跨私有数据孤岛训练大规模机器学习模型的标准方法。虽然分布式学习增强了隐私保护和训练效率，但它也面临着与拜占庭鲁棒性和通信减少相关的重大挑战。现有的拜占庭鲁棒且高效通信的方法依赖于每次迭代或以一定概率在某些迭代中获得完整的梯度信息，并且它们仅收敛到解周围一个不必要的大的邻域。基于这些问题，我们提出了一种新颖的拜占庭鲁棒且高效通信的随机分布式学习方法，该方法对批量大小没有任何要求，并且收敛到比所有现有方法都更接近最优解的小邻域，与理论下界一致。我们的关键创新是利用 Polyak 动量来减轻由有偏压缩器和随机梯度引起的噪声，从而在信息压缩的情况下防御拜占庭工作者。我们提供了在非凸平滑损失函数的背景下，我们算法的紧复杂度界限的证明，证明这些界限与无拜占庭场景中的下界相匹配。最后，我们通过一系列广泛的实验验证了我们算法的实际意义，对二进制分类和图像分类任务的性能进行了基准测试。||
|**2024-09-13**|[Think Twice Before You Act: Improving Inverse Problem Solving With MCMC](http://arxiv.org/abs/2409.08551)|null|最近的研究表明，扩散模型可以作为解决逆问题的强有力先验。一个突出的例子是扩散后验采样（DPS），它使用Tweedie公式来近似给定测量值的数据后验分布。尽管DPS在解决各种逆问题时具有无需重新训练的优点，但由于这种后验近似可能不准确，特别是在高噪声水平下，因此其性能受到限制。因此，我们提出了扩散后验MCMC（DPMC），这是一种基于退火MCMC的新型推理算法，用于解决使用预训练扩散模型的逆问题。我们定义了一系列中间分布，其灵感来自DPS使用的近似条件分布。通过退火MCMC采样，我们鼓励样本在移动到噪声水平较低的下一个分布之前，更紧密地遵循每个中间分布，从而减少沿路径累积的误差。我们在各种逆问题中测试了我们的算法，包括超分辨率、高斯去模糊、运动去模糊、修复和相位检索。我们的算法在几乎所有任务中都优于DPS，并且评估次数更少，并且与现有方法相比具有竞争力。||
|**2024-09-12**|[Learned Compression for Images and Point Clouds](http://arxiv.org/abs/2409.08376)|**[link](https://github.com/multimedialabsfu/learned-point-cloud-compression-for-classification)**|在过去十年中，深度学习在执行计算机视觉任务（包括分类、超分辨率和风格迁移）方面表现出色。现在，我们将其应用于数据压缩，以帮助构建下一代多媒体编解码器。本论文对这一新兴的学习压缩领域做出了三个主要贡献。首先，我们提出了一种高效的低复杂度熵模型，它通过将编码分布本身作为边信息进行压缩和传输，从而动态地使编码分布适应特定的输入。其次，我们提出了一种新颖的轻量级低复杂度点云编解码器，该编解码器专门针对分类进行了高度优化，与非专门编解码器相比，可以显著降低比特率。最后，我们探讨了连续视频帧之间输入域内的运动是如何体现在相应的卷积导出的潜在空间中的。||
|**2024-09-12**|[FACT: Feature Adaptive Continual-learning Tracker for Multiple Object Tracking](http://arxiv.org/abs/2409.07904)|null|多目标跟踪 (MOT) 涉及识别视频序列中的多个目标并为其分配相应的 ID，其中经常遇到遮挡。最近的方法通过在线学习技术解决遮挡问题，以提高适应性，或通过离线学习技术利用视频中的时间信息。然而，大多数现有的基于在线学习的 MOT 方法无法从所有过去的跟踪信息中学习，从而在保持实时跟踪速度的同时提高对长期遮挡的适应性。另一方面，基于时间信息的离线学习方法维护一个长期记忆来存储过去的跟踪信息，但这种方法限制了它们在跟踪过程中只能使用局部的过去信息。为了应对这些挑战，我们提出了一种新的 MOT 框架，称为特征自适应持续学习跟踪器 (FACT)，它通过利用所有过去的跟踪信息实现目标的实时跟踪和特征学习。我们证明了该框架可以与各种最先进的基于特征的跟踪器集成，从而提高它们的跟踪能力。具体来说，我们开发了特征自适应持续学习 (FAC) 模块，这是一个神经网络，可以在线训练以自适应地学习特征，并在跟踪过程中使用所有过去的跟踪信息。此外，我们还介绍了一个专为所提出的基于持续学习的跟踪而设计的两阶段关联模块。大量实验结果表明，所提出的方法在 MOT17 和 MOT20 基准测试中实现了最先进的在线跟踪性能。代码将在接收后发布。||
|**2024-09-12**|[Microscopic-Mamba: Revealing the Secrets of Microscopic Images with Just 4M Parameters](http://arxiv.org/abs/2409.07896)|**[link](https://github.com/zs1314/microscopic-mamba)**|在医学显微图像分类 (MIC) 领域，基于 CNN 和 Transformer 的模型已被广泛研究。然而，CNN 难以建模远程依赖关系，限制了其充分利用图像语义信息的能力。相反，Transformer 则受到二次计算复杂性的阻碍。为了解决这些挑战，我们提出了一种基于 Mamba 架构的模型：Microscopic-Mamba。具体来说，我们设计了部分选择前馈网络（PSFFN）来替换视觉状态空间模块（VSSM）的最后一个线性层，增强了 Mamba 的局部特征提取能力。此外，我们引入了调制交互特征聚合（MIFA）模块，以有效地调制和动态聚合全局和局部特征。我们还结合了并行 VSSM 机制，以改善通道间的信息交互，同时减少参数数量。大量实验表明，我们的方法在五个公共数据集上实现了最先进的性能。代码可在 https://github.com/zs1314/Microscopic-Mamba 获取。||
|**2024-09-12**|[What is YOLOv9: An In-Depth Exploration of the Internal Features of the Next-Generation Object Detector](http://arxiv.org/abs/2409.07813)|null|本研究全面分析了 YOLOv9 对象检测模型，重点关注其架构创新、训练方法以及相较于先前版本的性能改进。关键的改进，例如广义高效层聚合网络 (GELAN) 和可编程梯度信息 (PGI)，显著增强了特征提取和梯度流，从而提高了准确性和效率。通过结合深度卷积和轻量级 C3Ghost 架构，YOLOv9 在保持高精度的同时降低了计算复杂度。在 Microsoft COCO 上的基准测试表明，它具有优越的平均精度均值 (mAP) 和更快的推理时间，在多个指标上优于 YOLOv8。该模型的多功能性体现在它可以无缝部署到从边缘设备到高性能 GPU 的各种硬件平台上，并内置支持 PyTorch 和 TensorRT 集成。本文首次深入探讨了 YOLOv9 的内部特征及其在现实世界中的适用性，将其确立为跨行业的实时对象检测的最新解决方案，从物联网设备到大型工业应用。||
|**2024-09-12**|[Mesh-based Super-Resolution of Fluid Flows with Multiscale Graph Neural Networks](http://arxiv.org/abs/2409.07769)|null|这项工作介绍了一种图神经网络 (GNN) 方法，能够对流体流动进行基于网格的三维超分辨率重建。在此框架中，GNN 的设计不是一次性在整个基于网格的场上运行，而是直接在局部元素（或单元）网格上运行。为了以类似于谱（或有限）元素离散化的方式促进基于网格的 GNN 表示，修改了基线 GNN 层（称为消息传递层，用于更新局部节点属性）以考虑重合图节点的同步，从而使其与常用的基于元素的网格连接兼容。该架构本质上是多尺度的，由粗尺度和细尺度消息传递层序列（称为处理器）组合而成，这些序列之间通过图解池层进行分离。粗尺度处理器使用粗尺度同步消息传递在元素邻域上将查询元素（以及一组相邻的粗元素）嵌入到单个潜在图表示中，而细尺度处理器利用此潜在图上的其他消息传递操作来校正插值误差。使用来自雷诺数为 1600 和 3200 的泰勒-格林涡流模拟的六面体网格数据进行演示研究。通过分析全局和局部误差，结果最终表明，与粗尺度和多尺度模型配置中的目标相比，GNN 如何能够生成准确的超分辨率场。发现固定架构的重建误差与雷诺数成正比，而包含周围粗元素邻居被发现可以改善 Re=1600 时的预测，但在 Re=3200 时则不然。||
|**2024-09-12**|[DFDG: Data-Free Dual-Generator Adversarial Distillation for One-Shot Federated Learning](http://arxiv.org/abs/2409.07734)|null|联邦学习 (FL) 是一种分布式机器学习方案，其中客户端通过共享模型信息而不是其私有数据集来共同参与全局模型的协作训练。考虑到与通信和隐私相关的担忧，具有一轮通信的单次联邦学习已成为事实上的有希望的解决方案。然而，现有的单次联邦学习方法要么需要公共数据集，要么侧重于模型同构设置，要么从本地模型中提取的知识有限，这使得训练鲁棒的全局模型变得困难甚至不切实际。为了解决这些限制，我们提出了一种新的用于单次联邦学习的无数据双生成器对抗蒸馏方法 (即 DFDG)，该方法可以通过训练双生成器来探索更广泛的本地模型训练空间。DFDG 以对抗方式执行，包括两部分：双生成器训练和双模型蒸馏。在双生成器训练中，我们深入研究了每个生成器在保真度、可迁移性和多样性方面的内容，以确保其效用，并额外定制了交叉散度损失以减少双生成器输出空间的重叠。在双模型蒸馏中，训练好的双生成器协同工作，为全局模型的更新提供训练数据。最后，我们对各种图像分类任务的广泛实验表明，与 SOTA 基线相比，DFDG 在准确性方面取得了显着的性能提升。||
|**2024-09-12**|[Cooperative Inference with Interleaved Operator Partitioning for CNNs](http://arxiv.org/abs/2409.07693)|null|将深度学习模型部署在物联网（IoT）设备上通常会面临内存资源和计算能力有限的挑战。协同推理是解决这一问题的重要方法，需要对智能模型进行分区和分布式部署。为了执行水平分区，现有的协同推理方法要么采用算子的输出通道，要么采用特征图的高度和宽度作为分区维度。在这种方式下，由于算子的激活是分布式的，因此必须将它们连接在一起，然后才能将其馈送到下一个算子，这会导致协同推理的延迟。在本文中，我们为CNN模型提出了交错算子分区（IOP）策略。通过基于输出通道维度对一个算子进行分区，并基于输入通道维度对其后续算子进行分区，可以避免激活连接，从而减少通信连接的数量，从而减少协同推理延迟。基于IOP，我们进一步提出了一种模型分割算法，用于最小化协同推理时间，该算法根据获得的推理延迟收益，贪婪地选择用于IOP配对的算子。实验结果表明，与CoEdge中使用的最先进的分区方法相比，IOP策略在三个经典图像分类模型上实现了6.39%~16.83%的加速，并将峰值内存占用减少了21.22%~49.98%。||
|**2024-09-11**|[Minimizing Embedding Distortion for Robust Out-of-Distribution Performance](http://arxiv.org/abs/2409.07582)|null|基于庞大且多样化数据集训练的基础模型在各种零样本任务中展现出跨不同领域和分布泛化的非凡能力。我们的工作解决了在通过微调使基础模型适应特定下游任务时，如何保留这些强大的泛化能力的挑战。为此，我们引入了一种名为“相似性损失”的新方法，它可以融入到任何任务的微调过程中。通过最小化微调嵌入与预训练嵌入之间的扭曲，我们的方法在特定任务适应和保持广泛泛化能力之间取得了平衡。我们在两个不同的任务上评估了我们的方法：卫星图像的图像分类和人脸识别，重点关注开放类别和领域迁移场景，以评估分布外 (OOD) 性能。我们证明，这种方法在保持强大的分布内 (ID) 性能的同时，显著提高了 OOD 性能。||
|**2024-09-11**|[ENACT: Entropy-based Clustering of Attention Input for Improving the Computational Performance of Object Detection Transformers](http://arxiv.org/abs/2409.07541)|**[link](https://github.com/gsavathrakis/enact)**|Transformer在基于视觉的目标检测问题上表现出具有竞争力的精度。然而，由于注意力权重的平方大小，它们需要相当大的计算资源。在这项工作中，我们建议根据输入信息熵对transformer输入进行聚类。这样做的原因是，每个像素的自信息（其总和为熵）在对应于同一对象的像素之间可能是相似的。聚类减少了作为transformer输入的数据量，因此减少了训练时间和GPU内存使用量，同时保留了要传递到网络其余部分的有意义信息。建议的过程组织在一个名为ENACT的模块中，该模块可以插入任何在其编码器中包含多头自注意力计算的transformer架构。我们使用COCO目标检测数据集和三个检测transformer进行了广泛的实验。获得的结果表明，在所有测试案例中，所需的计算资源都持续减少，而检测任务的精度仅略有下降。ENACT模块的代码将在https://github.com/GSavathrakis/ENACT上提供。||
|**2024-09-11**|[A Contrastive Symmetric Forward-Forward Algorithm (SFFA) for Continual Learning Tasks](http://arxiv.org/abs/2409.07387)|null|所谓的“正向-正向算法”(FFA) 近期作为一种替代传统神经网络学习中反向传播算法的新方法获得了关注，在各种建模任务中展现出具有竞争力的性能。通过用两次对比正向传递代替梯度反向传播的反向传递，FFA 通过启用逐层训练启发式方法，避免了其前身所经历的几个缺点（例如梯度消失/爆炸）。在分类任务中，这种对比方法已被证明可以有效地创建输入数据的潜在稀疏表示，最终有利于区分性。然而，由于正负数据之间损失函数的不平衡，FFA 表现出固有的不对称梯度行为，这会对模型的泛化能力产生负面影响并导致准确性下降。为了解决这个问题，这项工作提出了对称正向-正向算法 (SFFA)，这是对原始 FFA 的一种新颖改进，它将每一层划分为正神经元和负神经元。这允许将局部适应度函数定义为正神经元激活与整体层活动之间的比率，从而在训练阶段产生对称的损失情况。为了评估我们方法增强的收敛性，我们使用多个图像分类基准进行了多项实验，比较了使用 SFFA 训练的模型与其使用 FFA 训练的模型的准确性。作为这种重新表述的副产品，我们探索了将逐层训练算法用于持续学习 (CL) 任务的优势。逐层训练算法引起的神经元特化及其激活的稀疏性使得能够实现有效的 CL 策略，将新知识（类别）整合到神经网络中，同时防止灾难性地遗忘先前...||
|**2024-09-11**|[Three-Dimensional, Multimodal Synchrotron Data for Machine Learning Applications](http://arxiv.org/abs/2409.07322)|**[link](https://github.com/calum-green/xct-xdrct_paper_code)**|Machine learning techniques are being increasingly applied in medical and physical sciences across a variety of imaging modalities; however, an important issue when developing these tools is the availability of good quality training data. Here we present a unique, multimodal synchrotron dataset of a bespoke zinc-doped Zeolite 13X sample that can be used to develop advanced deep learning and data fusion pipelines. Multi-resolution micro X-ray computed tomography was performed on a zinc-doped Zeolite 13X fragment to characterise its pores and features, before spatially resolved X-ray diffraction computed tomography was carried out to characterise the homogeneous distribution of sodium and zinc phases. Zinc absorption was controlled to create a simple, spatially isolated, two-phase material. Both raw and processed data is available as a series of Zenodo entries. Altogether we present a spatially resolved, three-dimensional, multimodal, multi-resolution dataset that can be used for the development of machine learning techniques. Such techniques include development of super-resolution, multimodal data fusion, and 3D reconstruction algorithm development.||
|**2024-09-10**|[A comprehensive study on Blood Cancer detection and classification using Convolutional Neural Network](http://arxiv.org/abs/2409.06689)|null|多年来，在目标检测领域，一些高效的卷积神经网络 (CNN)，如 DenseNet201、InceptionV3、ResNet152v2、SEresNet152、VGG19、Xception 因其性能而备受关注。此外，CNN 范式已经扩展到从原始 CNN 架构进行迁移学习和集成模型。研究表明，迁移学习和集成模型能够提高深度学习 (DL) 模型的准确性。然而，很少有研究利用这些技术对血液恶性肿瘤进行检测和定位的综合实验。意识到这一差距，本研究进行了三个实验；在第一个实验中，使用了六个原始 CNN，在第二个实验中，使用了迁移学习，在第三个实验中，开发了一个新的集成模型 DIX（DenseNet201、InceptionV3 和 Xception）来检测和分类血癌。统计结果表明，DIX 的性能优于原始模型和迁移学习，准确率达到 99.12%。然而，这项研究也提供了一个关于迁移学习的负面结果，因为迁移学习并没有提高原始 CNN 的准确性。与许多其他癌症一样，血癌疾病需要及时识别，才能制定有效的治疗方案并提高生存机会。使用 CNN 检测和分类血癌的高精度表明，CNN 模型在血癌检测中很有前景。这项研究在生物医学工程、计算机辅助疾病诊断和基于机器学习的疾病检测领域具有重要意义。||
|**2024-09-10**|[Lightweight Multiscale Feature Fusion Super-Resolution Network Based on Two-branch Convolution and Transformer](http://arxiv.org/abs/2409.06590)|null|目前，深度学习下的单图像超分辨率(SISR)算法主要有两大模型，一种是基于卷积神经网络的模型，另一种是基于Transformer的模型。前者采用不同卷积核大小的卷积层堆叠的方式来设计模型，使得模型能够更好地提取图像的局部特征；后者采用自注意力机制来设计模型，通过自注意力机制可以让模型建立图像像素点之间的长距离依赖关系，进而更好地提取图像的全局特征。然而，上述两种方法都面临着自己的问题。基于此，本文提出了一种基于双向互补卷积和Transformer的新型轻量级多尺度特征融合网络模型，该模型通过双分支网络架构，融合Transformer和卷积神经网络各自的特点，实现全局和局部信息的相互融合。同时，考虑到深度神经网络训练的低像素图像造成的局部信息丢失，本文设计了一种多阶段特征补充的模块化连接方式，将模型浅层阶段提取的特征图与模型深层阶段提取的特征图进行融合，以最大限度地减少特征图像中信息的丢失，有利于图像的复原，便于获得更高质量的复原图像。最终的实践结果表明，与其他参数量相同的轻量级模型相比，本文提出的模型在图像恢复性能方面是最优的。||
|**2024-09-10**|[Transtreaming: Adaptive Delay-aware Transformer for Real-time Streaming Perception](http://arxiv.org/abs/2409.06584)|null|实时目标检测对于许多现实应用（如自动驾驶中的防撞和路径规划）的决策过程至关重要。本研究提出了一种创新的实时流感知方法 Transtreaming，它解决了具有动态计算延迟的实时目标检测挑战。Transtreaming 的核心创新在于其自适应延迟感知转换器，它可以同时预测多个未来帧并选择与现实世界当前时间最匹配的输出，从而补偿任何系统引起的计算延迟。即使在单帧检测场景中，所提出的模型也通过利用基于转换器的方法优于现有的最先进方法。它在从强大的 V100 到适度的 2080Ti 的各种设备上均表现出强大的性能，在所有平台上都实现了最高水平的感知精度。与大多数难以在功能较弱的设备上在一帧内完成计算的最先进方法不同，Transtreaming 可以满足各种设备上的严格实时处理要求。实验结果强调了该系统的适应性和其显着提高许多现实系统（如自动驾驶）的安全性和可靠性的潜力。||
|**2024-09-10**|[Semi-Supervised 3D Object Detection with Chanel Augmentation using Transformation Equivariance](http://arxiv.org/abs/2409.06583)|null|对于自动驾驶汽车和机器人来说，精确的三维物体检测对于其安全有效地导航和与环境交互至关重要。同时，三维检测器的性能依赖于数据规模和标注，而这通常成本高昂。因此，使用有限的标注数据进行训练的需求日益增长。本文探索了一种新颖的师生框架，该框架采用通道增强技术进行三维半监督目标检测。师生SSL通常对教师和学生分别采用弱增强和强增强。在本工作中，我们使用变换等变检测器（TED）对两个网络应用了多通道增强。TED使我们能够探索点云上增强的不同组合，并有效地聚合多通道变换等变特征。原则上，通过对教师网络采用固定的通道增强，学生可以在可靠的伪标签上稳定地训练。采用强通道增强可以丰富数据的多样性，增强对变换的鲁棒性，提高学生网络的泛化性能。我们使用SOTA层次监督作为基线，并将其双阈值调整到TED，称为通道IoU一致性。我们使用KITTI数据集对我们的方法进行了评估，取得了显著的性能提升，超越了SOTA三维半监督目标检测模型。||
|**2024-09-10**|[Dynamic Decoupling of Placid Terminal Attractor-based Gradient Descent Algorithm](http://arxiv.org/abs/2409.06542)|null|梯度下降 (GD) 和随机梯度下降 (SGD) 已广泛应用于众多应用领域。因此，理解 GD 的动力学并提高其收敛速度仍然非常重要。本文根据梯度流不同阶段的终端吸引子，仔细分析了 GD 的动力学。基于终端滑模理论和终端吸引子理论，设计了四种自适应学习率。并通过详细的理论研究考察了它们的性能，并对学习过程的运行时间进行了评估和比较。此外，还详细研究了它们学习过程的总时间。为了评估其有效性，在函数逼近问题和图像分类问题上对各种仿真结果进行了研究。||
|**2024-09-10**|[Knowledge Distillation via Query Selection for Detection Transformer](http://arxiv.org/abs/2409.06443)|null|Transformer 通过引入 DETR 为目标检测领域带来了革命性的变化，DETR 以其简洁性和有效性而备受赞誉。尽管有这些优势，但这些模型的庞大规模对其在实际部署中，尤其是在资源受限的环境中，提出了重大挑战。本文利用知识蒸馏技术解决了压缩 DETR 的挑战，该技术有望在保持模型性能的同时减小模型规模。DETR 性能的一个关键方面是它们依赖查询来准确解释对象表示。传统的蒸馏方法通常只关注通过二分匹配识别的正查询，而忽略了硬负查询中存在的信息。我们的视觉分析表明，关注前景元素的硬负查询对于增强蒸馏结果至关重要。为此，我们引入了一种新颖的组查询选择策略，该策略通过根据查询与真实对象的广义交并比 (GIoU) 对查询进行分段，从而发现有价值的硬负查询用于蒸馏，这与 DETR 蒸馏中的传统查询选择不同。此外，我们提出了基于查询选择的 DETR 知识蒸馏 (QSKD) 框架，该框架结合了注意力引导特征蒸馏 (AGFD) 和局部对齐预测蒸馏 (LAPD)。这些组件通过关注教师模型中间特征和输出中最有信息的部分来优化蒸馏过程。我们对 MS-COCO 数据集的综合实验评估证明了我们方法的有效性，在不增加大量计算成本的情况下，显着提高了各种 DETR 架构的平均精度 (AP)。具体来说，Conditional DETR ResNet-18 的 AP 从 35.8 提高到 39.9。||
|**2024-09-10**|[Seam Carving as Feature Pooling in CNN](http://arxiv.org/abs/2409.06311)|null|这项工作研究了将接缝裁剪作为卷积神经网络 (CNN) 中的一种特征池化技术用于图像分类任务的潜力。我们建议用接缝裁剪操作替换传统的最大池化层。我们在 Caltech-UCSD Birds 200-2011 数据集上进行的实验表明，基于接缝裁剪的 CNN 与采用最大池化的模型相比，在准确率、精确率、召回率和 F1 分数等指标上均取得了更好的性能。我们通过特征图可视化进一步分析了这两种方法的行为，表明接缝裁剪在池化过程中可能保留了更多结构信息。此外，我们还讨论了我们方法的局限性，并提出了未来研究的潜在方向。||
|**2024-09-10**|[An Attribute-Enriched Dataset and Auto-Annotated Pipeline for Open Detection](http://arxiv.org/abs/2409.06300)|null|通过语言检测感兴趣的对象经常会遇到挑战，特别是对于那些不常见或难以描述的对象，因为自动化模型和人类标注者之间存在感知差异。这些挑战凸显了对综合数据集的需求，这些数据集需要超越标准的对象标签，并结合详细的属性描述。为了满足这一需求，我们引入了 Objects365-Attr 数据集，它是对现有 Objects365 数据集的扩展，其特点是具有属性标注。该数据集通过整合广泛的属性（包括颜色、材质、状态、纹理和色调）来减少对象检测中的不一致性。它包含 560 万个对象级属性描述的扩展集合，这些描述在 140 万个边界框中进行了精心标注。此外，为了验证数据集的有效性，我们对不同规模的 YOLO-World 进行了严格的评估，测量了它们的检测性能，并展示了该数据集对推进对象检测的贡献。||
|**2024-09-09**|[Replay Consolidation with Label Propagation for Continual Object Detection](http://arxiv.org/abs/2409.05650)|null|目标检测是一个与机器人技术和自动驾驶等许多应用高度相关的计算机视觉问题。持续学习 (CL) 考虑的是模型在保留先前获得的知识的同时逐步学习新信息的设置。这尤其具有挑战性，因为深度学习模型在训练新数据时往往会灾难性地忘记旧知识。特别是，与用于分类的持续学习相比，用于目标检测的持续学习 (CLOD) 带来了额外的困难。在 CLOD 中，来自先前任务的图像可能包含未知的类别，这些类别可能会在未来的任务中重新出现并被标记。这些缺失的注释会导致基于重放的方法出现任务干扰问题。因此，文献中的大多数工作都集中在基于蒸馏的方法上。然而，这些方法只有在不同任务之间存在强大的类别重叠时才有效。为了解决当前方法的问题，我们提出了一种解决 CLOD 的新技术，称为用于目标检测的标签传播重放整合 (RCLPOD)。基于重放方法，我们的解决方案通过增强缓冲区内存样本来避免任务干扰问题。我们的方法在 CLOD 文献中的现有技术基础上进行了评估，证明了其在 VOC 和 COCO 等既定基准测试中的优越性能。||
|**2024-09-09**|[LEROjD: Lidar Extended Radar-Only Object Detection](http://arxiv.org/abs/2409.05564)|**[link](https://github.com/rst-tu-dortmund/lerojd)**|对于自动驾驶而言，精确的三维物体检测至关重要。激光雷达传感器非常适合这项任务，但它们价格昂贵，并且在恶劣天气条件下存在局限性。3+1D 成像雷达传感器提供了一种经济高效且稳健的替代方案，但由于其分辨率低和测量噪声高而面临挑战。现有的 3+1D 成像雷达数据集包括雷达和激光雷达数据，可以改进跨模态模型。尽管不应在推理过程中使用激光雷达，但它可以帮助训练仅使用雷达的物体检测器。我们探索了两种将知识从激光雷达域迁移到雷达域和仅使用雷达的物体检测器的策略：1. 使用顺序激光雷达点云细化的多阶段训练，以及 2. 跨模态知识蒸馏。在多阶段过程中，我们研究了三种细化方法。我们的结果表明，通过多阶段训练，平均精度 (mAP) 显着提高了 4.2 个百分点，通过使用教师模型的权重初始化学生模型进行知识蒸馏，平均精度提高了 3.9 个百分点。这些方法的主要优点是它们适用于其他 3D 物体检测网络，而无需改变其架构，正如我们通过在两个不同的物体检测器上进行分析所展示的那样。我们的代码可在 https://github.com/rst-tu-dortmund/lerojd 获取。||
|**2024-09-08**|[Can OOD Object Detectors Learn from Foundation Models?](http://arxiv.org/abs/2409.05162)|**[link](https://github.com/cvmi-lab/syncood)**|Out-of-distribution (OOD) object detection is a challenging task due to the absence of open-set OOD data. Inspired by recent advancements in text-to-image generative models, such as Stable Diffusion, we study the potential of generative models trained on large-scale open-set data to synthesize OOD samples, thereby enhancing OOD object detection. We introduce SyncOOD, a simple data curation method that capitalizes on the capabilities of large foundation models to automatically extract meaningful OOD data from text-to-image generative models. This offers the model access to open-world knowledge encapsulated within off-the-shelf foundation models. The synthetic OOD samples are then employed to augment the training of a lightweight, plug-and-play OOD detector, thus effectively optimizing the in-distribution (ID)/OOD decision boundaries. Extensive experiments across multiple benchmarks demonstrate that SyncOOD significantly outperforms existing methods, establishing new state-of-the-art performance with minimal synthetic data usage.||
|**2024-09-08**|[Visual Grounding with Multi-modal Conditional Adaptation](http://arxiv.org/abs/2409.04999)|**[link](https://github.com/mr-bigworth/mmca)**|Visual grounding is the task of locating objects specified by natural language expressions. Existing methods extend generic object detection frameworks to tackle this task. They typically extract visual and textual features separately using independent visual and textual encoders, then fuse these features in a multi-modal decoder for final prediction. However, visual grounding presents unique challenges. It often involves locating objects with different text descriptions within the same image. Existing methods struggle with this task because the independent visual encoder produces identical visual features for the same image, limiting detection performance. Some recently approaches propose various language-guided visual encoders to address this issue, but they mostly rely solely on textual information and require sophisticated designs. In this paper, we introduce Multi-modal Conditional Adaptation (MMCA), which enables the visual encoder to adaptively update weights, directing its focus towards text-relevant regions. Specifically, we first integrate information from different modalities to obtain multi-modal embeddings. Then we utilize a set of weighting coefficients, which generated from the multimodal embeddings, to reorganize the weight update matrices and apply them to the visual encoder of the visual grounding model. Extensive experiments on four widely used datasets demonstrate that MMCA achieves significant improvements and state-of-the-art results. Ablation experiments further demonstrate the lightweight and efficiency of our method. Our source code is available at: https://github.com/Mr-Bigworth/MMCA.||
|**2024-09-08**|[RCBEVDet++: Toward High-accuracy Radar-Camera Fusion 3D Perception Network](http://arxiv.org/abs/2409.04979)|null|Perceiving the surrounding environment is a fundamental task in autonomous driving. To obtain highly accurate perception results, modern autonomous driving systems typically employ multi-modal sensors to collect comprehensive environmental data. Among these, the radar-camera multi-modal perception system is especially favored for its excellent sensing capabilities and cost-effectiveness. However, the substantial modality differences between radar and camera sensors pose challenges in fusing information. To address this problem, this paper presents RCBEVDet, a radar-camera fusion 3D object detection framework. Specifically, RCBEVDet is developed from an existing camera-based 3D object detector, supplemented by a specially designed radar feature extractor, RadarBEVNet, and a Cross-Attention Multi-layer Fusion (CAMF) module. Firstly, RadarBEVNet encodes sparse radar points into a dense bird's-eye-view (BEV) feature using a dual-stream radar backbone and a Radar Cross Section aware BEV encoder. Secondly, the CAMF module utilizes a deformable attention mechanism to align radar and camera BEV features and adopts channel and spatial fusion layers to fuse them. To further enhance RCBEVDet's capabilities, we introduce RCBEVDet++, which advances the CAMF through sparse fusion, supports query-based multi-view camera perception models, and adapts to a broader range of perception tasks. Extensive experiments on the nuScenes show that our method integrates seamlessly with existing camera-based 3D perception models and improves their performance across various perception tasks. Furthermore, our method achieves state-of-the-art radar-camera fusion results in 3D object detection, BEV semantic segmentation, and 3D multi-object tracking tasks. Notably, with ViT-L as the image backbone, RCBEVDet++ achieves 72.73 NDS and 67.34 mAP in 3D object detection without test-time augmentation or model ensembling.||
|**2024-09-08**|[PatchAlign:Fair and Accurate Skin Disease Image Classification by Alignment with Clinical Labels](http://arxiv.org/abs/2409.04975)|**[link](https://github.com/aayushmanace/patchalign24)**|深度学习模型在皮肤病变诊断自动化方面取得了巨大成功。然而，在部署这些模型之前，需要解决其预测中存在的种族差异问题。我们介绍了一种名为 PatchAlign 的新方法，通过与皮肤病临床文本表征对齐来提高皮肤病图像分类的准确性和公平性。PatchAlign 使用图最优传输 (GOT) 损失作为正则化器来执行跨域对齐。即使在训练样本有限的情况下，获得的表征也是稳健的，并且可以很好地泛化到不同的肤色。为了减少临床皮肤病图像中噪声和伪影的影响，我们提出了一种可学习的掩码图最优传输，用于跨域对齐，进一步改善了公平性指标。我们在两个具有不同皮肤类型的皮肤病变数据集上将我们的模型与最先进的 FairDisCo 进行了比较：Fitzpatrick17k 和 Diverse Dermatology Images (DDI)。与 FairDisCo 相比，PatchAlign 在 Fitzpatrick17k 上将皮肤病图像分类的准确性提高了 2.8%（域内）和 6.2%（跨域），在 DDI 上提高了 4.2%（域内）。此外，它持续改善了不同肤色真实阳性率的公平性。用于实现的源代码可在以下 GitHub 存储库中获取：https://github.com/aayushmanace/PatchAlign24，可以轻松复现和进一步试验。||
|**2024-09-07**|[Activation Function Optimization Scheme for Image Classification](http://arxiv.org/abs/2409.04915)|**[link](https://github.com/abdurrahman1828/afos)**|Activation function has a significant impact on the dynamics, convergence, and performance of deep neural networks. The search for a consistent and high-performing activation function has always been a pursuit during deep learning model development. Existing state-of-the-art activation functions are manually designed with human expertise except for Swish. Swish was developed using a reinforcement learning-based search strategy. In this study, we propose an evolutionary approach for optimizing activation functions specifically for image classification tasks, aiming to discover functions that outperform current state-of-the-art options. Through this optimization framework, we obtain a series of high-performing activation functions denoted as Exponential Error Linear Unit (EELU). The developed activation functions are evaluated for image classification tasks from two perspectives: (1) five state-of-the-art neural network architectures, such as ResNet50, AlexNet, VGG16, MobileNet, and Compact Convolutional Transformer which cover computationally heavy to light neural networks, and (2) eight standard datasets, including CIFAR10, Imagenette, MNIST, Fashion MNIST, Beans, Colorectal Histology, CottonWeedID15, and TinyImageNet which cover from typical machine vision benchmark, agricultural image applications to medical image applications. Finally, we statistically investigate the generalization of the resultant activation functions developed through the optimization scheme. With a Friedman test, we conclude that the optimization scheme is able to generate activation functions that outperform the existing standard ones in 92.8% cases among 28 different cases studied, and $-x\cdot erf(e^{-x})$ is found to be the best activation function for image classification generated by the optimization scheme.||
|**2024-09-07**|[SSFam: Scribble Supervised Salient Object Detection Family](http://arxiv.org/abs/2409.04817)|**[link](https://github.com/liuzywen/ssfam)**|Scribble supervised salient object detection (SSSOD) constructs segmentation ability of attractive objects from surroundings under the supervision of sparse scribble labels. For the better segmentation, depth and thermal infrared modalities serve as the supplement to RGB images in the complex scenes. Existing methods specifically design various feature extraction and multi-modal fusion strategies for RGB, RGB-Depth, RGB-Thermal, and Visual-Depth-Thermal image input respectively, leading to similar model flood. As the recently proposed Segment Anything Model (SAM) possesses extraordinary segmentation and prompt interactive capability, we propose an SSSOD family based on SAM, named SSFam, for the combination input with different modalities. Firstly, different modal-aware modulators are designed to attain modal-specific knowledge which cooperates with modal-agnostic information extracted from the frozen SAM encoder for the better feature ensemble. Secondly, a siamese decoder is tailored to bridge the gap between the training with scribble prompt and the testing with no prompt for the stronger decoding ability. Our model demonstrates the remarkable performance among combinations of different modalities and refreshes the highest level of scribble supervised methods and comes close to the ones of fully supervised methods. https://github.com/liuzywen/SSFam||
|**2024-09-07**|[SpotActor: Training-Free Layout-Controlled Consistent Image Generation](http://arxiv.org/abs/2409.04801)|null|Text-to-image diffusion models significantly enhance the efficiency of artistic creation with high-fidelity image generation. However, in typical application scenarios like comic book production, they can neither place each subject into its expected spot nor maintain the consistent appearance of each subject across images. For these issues, we pioneer a novel task, Layout-to-Consistent-Image (L2CI) generation, which produces consistent and compositional images in accordance with the given layout conditions and text prompts. To accomplish this challenging task, we present a new formalization of dual energy guidance with optimization in a dual semantic-latent space and thus propose a training-free pipeline, SpotActor, which features a layout-conditioned backward update stage and a consistent forward sampling stage. In the backward stage, we innovate a nuanced layout energy function to mimic the attention activations with a sigmoid-like objective. While in the forward stage, we design Regional Interconnection Self-Attention (RISA) and Semantic Fusion Cross-Attention (SFCA) mechanisms that allow mutual interactions across images. To evaluate the performance, we present ActorBench, a specified benchmark with hundreds of reasonable prompt-box pairs stemming from object detection datasets. Comprehensive experiments are conducted to demonstrate the effectiveness of our method. The results prove that SpotActor fulfills the expectations of this task and showcases the potential for practical applications with superior layout alignment, subject consistency, prompt conformity and background diversity.||
|**2024-09-07**|[LoCa: Logit Calibration for Knowledge Distillation](http://arxiv.org/abs/2409.04778)|null|Knowledge Distillation (KD), aiming to train a better student model by mimicking the teacher model, plays an important role in model compression. One typical way is to align the output logits. However, we find a common issue named mis-instruction, that the student would be misled when the predictions based on teacher logits do not follow the labels. Meanwhile, there is other useful dark knowledge in the logits such as the class discriminability, which is vital for distillation. In this paper, we propose a simple yet effective Logit Calibration (LoCa) method, which calibrates the logits from the teacher model based on the ground-truth labels. The key insight is to correct the prediction (to address the mis-instruction issue) and maintain useful dark knowledge simultaneously. Our proposed LoCa does not require any additional parameters. Empirical results on image classification and text generation tasks demonstrate that LoCa can effectively improve the performance of baselines.||
|**2024-09-05**|[Use of triplet loss for facial restoration in low-resolution images](http://arxiv.org/abs/2409.03530)|null|近年来，人脸识别 (FR) 模型已成为应用最广泛的生物识别工具，在众多数据集上取得了令人瞩目的成果。然而，硬件的固有挑战或拍摄距离 often 导致低分辨率图像，这会严重影响人脸识别模型的性能。为了解决这个问题，人们提出了几种解决方案，包括生成高度逼真的人脸的超分辨率 (SR) 模型。尽管做出了这些努力，但人脸识别算法并未取得显著改进。我们提出了一种新颖的超分辨率模型 FTLGAN，它侧重于生成保留个人身份的高分辨率图像，而不仅仅是提高图像质量，从而最大限度地提高人脸识别模型的性能。结果令人信服，表明 d' 的平均值比当前最先进的模型高出 21%，具体而言，14x14 像素时 d' = 1.099，AUC = 0.78，28x28 像素时 d' = 2.112，AUC = 0.92，56x56 像素时 d' = 3.049，AUC = 0.98。这项研究的贡献在几个关键领域意义重大。首先，在低分辨率图像（特别是 14x14、28x28 和 56x56 像素的分辨率）中，人脸识别性能取得了显着提高。其次，FTLGAN 所展示的增强功能在所有分辨率下都表现出一致的响应，与其他比较模型不同，它始终如一地提供出色的性能。第三，使用三元组损失逻辑实施了一种创新方法，能够仅使用真实图像训练超分辨率模型，这与当前模型形成对比，并扩展了潜在的现实应用。最后，本研究引入了一种新颖的模型，该模型通过在模型训练期间将人脸识别质量作为损失纳入其中，专门解决了提高人脸识别系统分类性能的挑战。||
|**2024-09-05**|[Have Large Vision-Language Models Mastered Art History?](http://arxiv.org/abs/2409.03521)|null|大型视觉语言模型 (VLM) 的出现最近在跨多个领域的图像分类方面建立了新的基准。然而，VLM 在艺术品分类这一特定任务中的表现，特别是绘画艺术风格分类——传统上由艺术史学家掌握的领域——尚未得到探索。与自然图像相比，艺术品由于其固有的复杂性和多样性结构（以多变的构图和风格为特征）而构成了独特的挑战。艺术史学家长期以来一直在研究艺术品的独特方面，而风格预测是其学科的一个重要组成部分。本文研究了集成视觉和文本数据的大型 VLM 是否可以有效地预测绘画的艺术史属性。我们对四种 VLM（即 CLIP、LLaVA、OpenFlamingo 和 GPT-4o）进行了深入分析，重点关注使用两个公共艺术品基准对艺术风格、作者和时间段进行零样本分类。此外，我们还介绍了 ArTest，这是一个精心策划的艺术品测试集，其中包括艺术史学家研究的关键绘画作品。||
|**2024-09-05**|[LMLT: Low-to-high Multi-Level Vision Transformer for Image Super-Resolution](http://arxiv.org/abs/2409.03516)|**[link](https://github.com/jwgdmkj/lmlt)**|近年来，基于视觉Transformer (ViT) 的图像超分辨率方法展现出令人印象深刻的性能。然而，它们存在复杂性高的问题，导致推理时间和内存使用量大。此外，使用窗口自注意力机制(WSA) 的ViT模型在处理窗口区域外的信息时面临挑战。为了解决这些问题，我们提出了低到高多级Transformer (LMLT)，它对每个头采用不同特征大小的注意力机制。LMLT 沿通道维度划分图像特征，逐渐减小低层头的空间大小，并对每个头应用自注意力机制。这种方法有效地捕获了局部和全局信息。通过将低层头的结果整合到高层头中，LMLT 克服了自注意力机制中的窗口边界问题。大量实验表明，我们的模型在保持甚至超越最先进的基于 ViT 的图像超分辨率方法的性能的同时，显著减少了推理时间和 GPU 内存使用量。我们的代码可在 https://github.com/jwgdmkj/LMLT 获取。||
|**2024-09-05**|[Non-Uniform Illumination Attack for Fooling Convolutional Neural Networks](http://arxiv.org/abs/2409.03458)|**[link](https://github.com/Akshayjain97/Non-Uniform_Illumination)**|卷积神经网络（CNN）虽然取得了显著进步，但仍然容易受到攻击，特别是在面对人类容易识别的微小图像扰动时。这种弱点通常被称为“攻击”，突显了CNN的鲁棒性有限，需要研究如何增强其抵抗此类操纵的能力。本研究介绍了一种新颖的非均匀照明（NUI）攻击技术，该技术使用不同的NUI掩码对图像进行细微 alteration。我们在广泛接受的数据集（包括CIFAR10、TinyImageNet和CalTech256）上进行了大量实验，重点关注12种不同NUI攻击模型的图像分类。评估了VGG、ResNet、MobilenetV3-small和InceptionV3模型对NUI攻击的抵抗力。我们的结果表明，CNN模型在遭受NUI攻击时，分类精度大幅下降，表明它们在非均匀照明下的脆弱性。为了缓解这种情况，我们提出了一种防御策略，将通过新的NUI变换生成的NUI攻击图像包含到训练集中。结果表明，当CNN模型面对受NUI攻击影响的扰动图像时，其性能得到显著提升。该策略旨在增强CNN模型对NUI攻击的抵抗力。||
|**2024-09-05**|[Raw Speech Enhancement with Deep State Space Modeling](http://arxiv.org/abs/2409.03377)|**[link](https://github.com/Brainchip-Inc/aTENNuate)**|我们提出了 aTENNuate，这是一种简单的深度状态空间自编码器，专为高效的在线原始语音增强而配置，采用端到端的方式。该网络的性能主要在原始语音去噪方面进行评估，并在超分辨率和去量化等任务上进行了额外评估。我们在 VoiceBank + DEMAND 和 Microsoft DNS1 合成测试集上对 aTENNuate 进行了基准测试。该网络在 PESQ 分数、参数数量、MAC 和延迟方面优于以前的实时去噪模型。即使作为原始波形处理模型，该模型也能保持对干净信号的高保真度，并且可听见的伪影极少。此外，即使将噪声输入压缩至 4000Hz 和 4 位，该模型仍能保持良好的性能，这表明它在资源受限的环境中具有一般的语音增强能力。||
|**2024-09-05**|[Training-free Conversion of Pretrained ANNs to SNNs for Low-Power and High-Performance Applications](http://arxiv.org/abs/2409.03368)|null|脉冲神经网络 (SNN) 由于其推理速度快、功耗低等优势，已成为人工神经网络 (ANN) 的一种很有前途的替代方案。然而，缺乏有效的训练算法阻碍了它们的广泛应用。现有的 SNN 监督学习算法比 ANN 需要更多的内存和时间。即使是常用的 ANN-SNN 转换方法也需要重新训练 ANN 以提高转换效率，从而产生额外的计算成本。为了应对这些挑战，我们提出了一种新颖的免训练 ANN-SNN 转换流程。我们的方法将预先训练好的 ANN 模型直接转换为高性能 SNN，无需额外的训练。该转换流程包括一个基于局部学习的阈值平衡算法，该算法能够有效地计算最佳阈值并通过通道缩放对阈值进行细粒度调整。我们展示了我们的框架在三个典型的计算机视觉任务中的可扩展性：图像分类、语义分割和目标检测。这展示了其对分类和回归任务的适用性。此外，我们评估了转换后的 SNN 的能耗，证明了它们与传统 ANN 相比具有优越的低功耗优势。我们的免训练算法优于现有方法，突出了其实用性和效率。这种方法通过利用开源预训练 ANN 模型和神经形态硬件简化了 SNN 的部署，从而实现了快速、低功耗的推理，并且性能损失可以忽略不计。||
|**2024-09-05**|[YOLO-PPA based Efficient Traffic Sign Detection for Cruise Control in Autonomous Driving](http://arxiv.org/abs/2409.03320)|null|在自动驾驶系统中高效、准确地检测交通标志至关重要。然而，距离越远，交通标志越小。现有的目标检测算法很难检测到这些小尺寸的标志。此外，车载嵌入式设备的性能限制了检测模型的规模。为了应对这些挑战，本文提出了一种基于 YOLO PPA 的交通标志检测算法。在 GTSDB 数据集上的实验结果表明，与原始 YOLO 相比，该方法将推理效率提高了 11.2%，mAP 50 也提高了 93.2%，证明了所提出的 YOLO PPA 的有效性。||
|**2024-09-05**|[PEPL: Precision-Enhanced Pseudo-Labeling for Fine-Grained Image Classification in Semi-Supervised Learning](http://arxiv.org/abs/2409.03192)|null|细粒度图像分类随着深度学习和计算机视觉技术的出现取得了显著的进步。然而，详细标注的缺乏仍然是一个主要挑战，特别是在获取高质量标记数据的成本高昂或耗时的情况下。为了解决这一限制，我们引入了专为半监督学习框架内的细粒度图像分类设计的精度增强型伪标签（PEPL）方法。我们的方法通过生成高质量的伪标签来利用丰富的未标记数据，这些伪标签通过两个关键阶段逐步细化：初始伪标签生成和语义混合伪标签生成。这些阶段利用类激活图（CAM）来准确估计语义内容并生成细化标签，这些标签捕获了细粒度分类所需的基本细节。通过关注语义级信息，我们的方法有效地解决了标准数据增强和图像混合技术在保留关键细粒度特征方面的局限性。我们在基准数据集上实现了最先进的性能，证明了相对于现有半监督策略的显著改进，在准确性和鲁棒性方面都有显著提升。我们的代码已在https://github.com/TianSuya/SemiFG开源。||
|**2024-09-05**|[The AdEMAMix Optimizer: Better, Faster, Older](http://arxiv.org/abs/2409.03137)|null|基于动量的优化器是众多机器学习应用的核心。这些优化器通常依赖于梯度的指数移动平均 (EMA)，它会以指数方式衰减旧梯度对当前梯度的贡献。这是因为梯度是局部的线性近似，当迭代点在损失函数曲面上移动时，旧梯度的相关性会降低。这项工作对使用单个 EMA 来累积过去梯度的做法提出了质疑，并通过经验证明了这种选择可能是次优的：单个 EMA 无法同时对最近的梯度赋予高权重，并对较旧的梯度赋予不可忽略的权重。基于这一观察，我们提出了 AdEMAMix，它是对 Adam 优化器的一种简单修改，它混合了两个 EMA，以更好地利用过去的梯度。我们在语言建模和图像分类方面的实验表明，令人惊讶的是，梯度在数万步内仍然具有相关性。它们有助于更快地收敛，并且通常收敛到更低的最小值：例如，一个在 1010 亿个词符上训练的具有 13 亿个参数的 AdEMAMix LLM 的性能与在一个 1970 亿个词符上训练的 AdamW 模型相当（+95%）。此外，我们的方法显著减缓了训练过程中的模型遗忘。我们的工作鼓励进一步探索利用过去梯度的不同类型的函数，而不仅仅是 EMA。||
|**2024-09-04**|[Boundless: Generating Photorealistic Synthetic Data for Object Detection in Urban Streetscapes](http://arxiv.org/abs/2409.03022)|**[link](https://github.com/zk2172-columbia/boundless)**|我们介绍Boundless，这是一个用于在密集的城市街景中实现高度准确的目标检测的逼真合成数据生成系统。Boundless可以用自动化和可配置的过程取代大规模的现实世界数据收集和手动地面实况目标注释（标记）。Boundless基于虚幻引擎5 (UE5) 城市示例项目，并进行了改进，能够在不同的照明和场景变化条件下准确收集3D边界框。我们评估了在Boundless生成的数据集上训练的目标检测模型在从中空相机获取的真实数据集上进行推理时的性能。我们将Boundless训练模型的性能与CARLA训练模型的性能进行了比较，观察到7.8 mAP的改进。我们取得的结果支持了合成数据生成是一种可靠的方法，可以用于训练/微调用于城市场景的可扩展目标检测模型。||
|**2024-09-04**|[iConFormer: Dynamic Parameter-Efficient Tuning with Input-Conditioned Adaptation](http://arxiv.org/abs/2409.02838)|null|基于预训练编码器的完整微调（FFT）和任务特定解码器的迁移学习随着深度模型的指数级增长而变得越来越复杂。使用由小型可学习层组成的适配器的参数高效微调（PEFT）方法已成为 FFT 的替代方案，在保持高训练效率的同时实现了可比的性能。然而，适配器对输入实例的不灵活限制了其在不同下游任务中学习任务特定信息的能力。在本文中，我们提出了一种新的 PEFT 方法，即输入条件化的 Transformer，称为 iConFormer，它利用了以输入实例为条件的动态适配器。为了确保在各种下游任务中对输入实例的灵活学习能力，我们在动态适配器中引入了输入条件化网络（iCoN），从而实现实例级特征转换。具体来说，iCoN 为每个特征生成通道级的卷积核，并使用自适应卷积过程对其进行转换，以有效捕获针对下游任务的任务特定和细粒度细节。实验结果表明，通过仅调整 Transformer 主干参数的 1.6% 到 2.8%，iConFormer 在单目深度估计和语义分割方面实现了与 FFT 相当的性能，同时在图像分类和实例分割方面优于 FFT。此外，所提出的方法在所有上述任务中始终优于最近的 PEFT 方法。||
|**2024-09-04**|[Real-Time Dynamic Scale-Aware Fusion Detection Network: Take Road Damage Detection as an example](http://arxiv.org/abs/2409.02546)|null|基于无人机的道路损坏检测 (RDD) 对城市的日常维护和安全至关重要，特别是在显著降低劳动力成本方面。然而，当前基于无人机的 RDD 研究仍面临许多挑战。例如，形状和方向不规则的损坏、背景对损坏的遮挡以及难以区分损坏和背景，这些因素都显著影响了无人机在日常巡检中检测道路损坏的能力。为了解决这些问题并提高无人机实时道路损坏检测的性能，我们设计并提出了三个相应的模块：一个能够灵活适应形状和背景的特征提取模块；一个融合多尺度感知并适应形状和背景的模块；一个高效的下采样模块。 基于这些模块，我们设计了一种具有自动去除背景干扰能力的多尺度自适应道路损坏检测模型，称为动态尺度感知融合检测模型 (RT-DSAFDet)。在 UAV-PDD2023 公开数据集上的实验结果表明，我们的模型 RT-DSAFDet 的 mAP50 达到了 54.2%，比最新实时目标检测模型 YOLOv10 的高效变体 YOLOv10-m 高 11.1%，而参数量减少到 1.8M，FLOPs 减少到 4.6G，分别降低了 88% 和 93%。此外，在大型通用目标检测公开数据集 MS COCO2017 上也展现了我们模型的优越性，其 mAP50-95 与 YOLOv9-t 相同，但 mAP50 高出 0.5%，参数量减少 10%，FLOPs 减少 40%。||
|**2024-09-04**|[Boosting Generalizability towards Zero-Shot Cross-Dataset Single-Image Indoor Depth by Meta-Initialization](http://arxiv.org/abs/2409.02486)|null|室内机器人的导航或障碍物检测等任务依赖于深度信息，而单图像深度估计被广泛用于辅助感知。大多数室内单图像深度预测较少关注模型对未见数据集的泛化能力，而更关注系统部署的野外鲁棒性。这项工作利用基于梯度的元学习在零样本跨数据集推理中获得更高的泛化能力。与研究最多的、与显式类别标签相关的图像分类元学习不同，对于与物体排列和场景构成方面高度变化的室内环境相关的连续深度值，不存在明确的任务边界。我们提出了细粒度任务，在我们的元学习公式中将每个RGB-D小批量视为一个任务。我们首先展示了我们的方法在有限数据上诱导出更好的先验（RMSE 最高降低 27.8%）。然后，在元学习初始化上进行微调始终优于没有元方法的基线。为了实现泛化，我们提出了零样本跨数据集协议，并验证了由我们的元初始化诱导的更高泛化能力，作为许多现有深度估计方法的简单而有用的插件。深度和元学习交叉领域的工作有可能推动这两项研究更接近实际的机器人和机器感知应用。||
|**2024-09-03**|[Site Selection for the Second Flyeye Telescope: A Simulation Study for Optimizing Near-Earth Object Discovery](http://arxiv.org/abs/2409.02329)|null|欧洲航天局 (ESA) 正在开发一个名为 Flyeye 的广域巡天望远镜网络，以改进近地天体 (NEO) 的发现。该网络中的第一个望远镜将位于北半球的穆法拉山（意大利），而第二个具有增强探测能力的 Flyeye 望远镜刚刚开始关键设计阶段。通过对撞击轨迹上的近地天体进行模拟，研究了第二个 Flyeye 望远镜的潜在位置。对大约 3000 个撞击小行星（绝对星等为 H=25 和 H=28）进行了传播，并测试了主要现有巡天项目（Catalina、Pan-STARRS、ATLAS）、即将投入使用的薇拉·鲁宾天文台 (LSST) 以及 Flyeye 可能选址的可探测性。 考虑了智利、南非和北半球的第二个设施。对于每个天文台，在模拟中都考虑了它们过去或计划的指向策略。在 LSST 部署之前，南半球的一个 Flyeye 的性能与北半球的一个望远镜相似。结合起来，在北方和南方各放置一台望远镜可以最大限度地提高探测率和探测到的独特物体的数量。LSST 之后，南部和北部的 Flyeye 望远镜仍然是互补的。总体而言，模拟表明，无论是在 LSST 之前还是之后，位于南部的第二个 Flyeye 都可以补充位于北部的 Flyeye 望远镜。位于拉西拉的 Flyeye 将利用其优越的大气条件，同时平衡南北半球的资产。||
|**2024-09-03**|[K-Origins: Better Colour Quantification for Neural Networks](http://arxiv.org/abs/2409.02281)|**[link](https://github.com/lewismmason/Thesis-Public)**|K-Origins是一种神经网络层，旨在在学习颜色或强度有利时提高基于图像的网络性能。 超过 250 个编码器-解码器卷积网络在 16 位合成数据上进行了训练和测试，结果表明，在两种情况下，K-Origins 提高了语义分割精度：低信噪比下的目标检测，以及分割形状相同但颜色不同的多个目标。 对于每个可训练参数 $w_k$，K-Origins 通过公式 $\textbf{Y}_k = \textbf{X}-\textbf{J}\cdot w_k$ 从输入特征 $\textbf{X}$ 生成输出特征，其中 $\textbf{J}$ 是一个全 1 矩阵。 此外，还训练了具有不同感受野的网络，以根据目标类别的维度确定最佳网络深度，这表明感受野长度应超过目标大小。 通过确保足够的感受野长度并结合 K-Origins，我们可以获得更好的语义网络性能。||
|**2024-09-03**|[Evaluation and Comparison of Visual Language Models for Transportation Engineering Problems](http://arxiv.org/abs/2409.02278)|null|近年来，视觉语言模型（VLM）的快速发展展现出其在图像理解相关应用方面的巨大潜力。本研究探索了最先进的VLM模型在基于视觉的交通工程任务中的应用，例如图像分类和目标检测。图像分类任务包括拥堵检测和裂缝识别，而目标检测任务则用于识别未佩戴头盔的行为。我们应用了开源模型（如CLIP、BLIP、OWL-ViT、Llava-Next）和闭源模型GPT-4o，评估了这些最先进的VLM模型的性能，以利用语言理解能力来完成基于视觉的交通任务。这些任务通过对VLM模型应用零样本提示来完成，因为零样本提示可以在不对任务进行任何训练的情况下执行任务。这消除了对特定任务进行标注数据集或微调的需求。虽然这些模型在图像分类任务中取得了与基准卷积神经网络（CNN）模型相当的结果，但在目标定位任务中仍有改进的空间。因此，本研究对最先进的VLM模型进行了全面评估，突出了这些模型的优势和局限性，可以作为未来改进和广泛实施的基准。||
|**2024-09-03**|[A Modern Take on Visual Relationship Reasoning for Grasp Planning](http://arxiv.org/abs/2409.02035)|null|与现实世界杂乱场景交互对机器人代理提出了若干挑战，这些代理需要理解观察到的物体之间复杂的的空间依赖性，以确定最佳拾取顺序或有效的物体检索策略。 现有的解决方案通常管理简化的场景，并侧重于在初始物体检测阶段之后预测成对物体关系，但往往忽略全局上下文或难以处理冗余和缺失的物体关系。 在这项工作中，我们提出了一种用于抓取规划的视觉关系推理的现代方法。 我们介绍了 D3GD，这是一个新的测试平台，其中包括包含来自 97 个不同类别的多达 35 个物体的分拣场景。 此外，我们还提出了 D3G，这是一种新的基于端到端 transformer 的依赖图生成模型，它可以同时检测物体并生成表示其空间关系的邻接矩阵。 认识到标准指标的局限性，我们首次采用关系平均精度来评估模型性能，进行了广泛的实验基准测试。 获得的结果表明我们的方法是这项任务的最新技术，为机器人操作的未来研究奠定了基础。 我们在 https://paolotron.github.io/d3g.github.io 上公开发布代码和数据集。||
|**2024-09-03**|[Compressed learning based onboard semantic compression for remote sensing platforms](http://arxiv.org/abs/2409.01988)|**[link](https://github.com/protim1191/glodismo_classifier)**|地球观测 (EO) 在创建和维持一个具有弹性和繁荣的社会方面发挥着至关重要的作用，这对所有生命和地球本身都具有深远的影响。卫星、航空平台以及最近的无人机和无人驾驶飞行器等遥感平台都用于 EO。它们收集大量数据，需要将其下传到地球进行进一步处理和分析。这种高吞吐量采集的瓶颈是下行链路带宽。需要以数据为中心的图像压缩解决方案来应对这种海量数据。在这项工作中，通过压缩学习框架研究了语义压缩，该框架仅利用快速和稀疏的矩阵向量乘法来编码数据。相机噪声和通信信道是造成失真的主要来源。然后，完整的语义通信管道由一个学习到的低复杂度压缩矩阵组成，该矩阵作用于噪声相机输出，以在机载生成一个观测向量，该向量通过通信信道下行链路传输，通过展开网络处理，然后馈送到执行必要下游任务的深度学习模型；研究了图像分类。通过使用小波稀疏先验展开 NA-ALISTA 的层来补偿失真。因此，解码是一种根据相机/环境信息和下游任务设计的即插即用方法。用于下游任务的深度学习模型通过端到端方式的损失函数与压缩矩阵和展开网络联合微调。结果表明，在低压缩比的噪声环境中，添加恢复损失以及任务相关损失可以提高下游性能。||
|**2024-09-03**|[Latent Distillation for Continual Object Detection at the Edge](http://arxiv.org/abs/2409.01872)|**[link](https://github.com/pastifra/Continual_Nanodet)**|虽然在目标检测文献中存在许多性能卓越的方法，但解决数据分布偏移仍然具有挑战性。持续学习（CL）为这个问题提供了解决方案，使模型能够适应新数据，同时保持对先前数据的性能。这对于边缘设备尤其重要，这些设备在汽车和机器人等动态环境中很常见。在这项工作中，我们解决了目标检测持续学习（CLOD）场景中边缘设备的内存和计算限制。具体来说，（i）我们研究了一种开源、轻量级和快速的检测器 NanoDet 对边缘设备上 CLOD 的适用性，改进了文献中使用的较大架构。此外，（ii）我们提出了一种名为潜在蒸馏（LD）的新型 CL 方法，该方法在不显着影响检测性能的情况下减少了最先进的 CL 方法所需的运算次数和内存。我们的方法使用著名的 VOC 和 COCO 基准测试集进行了验证，与其他蒸馏方法相比，每次模型更新可将蒸馏参数开销减少 74%，将浮点运算（FLOPs）减少 56%。||
|**2024-09-03**|[GeoBEV: Learning Geometric BEV Representation for Multi-view 3D Object Detection](http://arxiv.org/abs/2409.01816)|null|鸟瞰图 (BEV) 表示已成为多视图 3D 对象检测的主流范式，展现出令人印象深刻的感知能力。然而，现有方法忽略了 BEV 表示的几何质量，使其处于低分辨率状态，无法恢复场景真实的几何信息。在本文中，我们确定了先前方法受限于低 BEV 表示分辨率的原因，并提出了径向-笛卡尔 BEV 采样 (RC-Sampling)，从而能够高效生成高分辨率密集 BEV 表示，而无需复杂的算子。此外，我们设计了一种新颖的盒内标签来替代从激光雷达点生成的传统深度标签。此标签反映了对象的实际几何结构，而不仅仅是它们的表面，将现实世界的几何信息注入 BEV 表示中。此外，结合盒内标签，开发了一种质心感知内部损失 (CAI 损失) 来捕捉对象的细粒度内部几何结构。最后，我们将上述模块集成到一个名为 GeoBEV 的新型多视图 3D 对象检测框架中。在 nuScenes 数据集上的大量实验表明，GeoBEV 实现了最先进的性能，突出了其有效性。||

<p align=right>(<a href=#updated-on-20240930>back to top</a>)</p>

## 生成模型

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-09-27**|[ $O(d/T)$ Convergence Theory for Diffusion Probabilistic Models under Minimal Assumptions](http://arxiv.org/abs/2409.18959)|null|基于分数的扩散模型通过学习逆转将目标分布数据扰动为噪声的扩散过程来生成新数据，已经在各种生成任务中取得了显著成功。尽管它们具有优越的经验性能，但现有的理论保证通常受到严格假设或次优收敛速度的限制。在本文中，我们以最小的假设建立了流行的基于 SDE 的采样器的快速收敛理论。我们的分析表明，如果提供分数函数的 $\ell_{2}$ 精度估计，则目标分布和生成分布之间的总变差距离的上限为 $O(d/T)$（忽略对数因子），其中 $d$ 是数据维度，$T$ 是步数。该结果适用于任何具有一阶矩有限的目标分布。据我们所知，这改进了基于 SDE 的采样器和另一种基于 ODE 的采样器的现有收敛理论，同时对目标数据分布和分数估计施加了最小假设。这是通过一组新颖的分析工具实现的，该工具提供了对误差在反向过程的每个步骤中如何传播的细粒度表征。|
|**2024-09-27**|[ReviveDiff: A Universal Diffusion Model for Restoring Images in Adverse Weather Conditions](http://arxiv.org/abs/2409.18932)|null|在诸如夜间、雾天、雨天和水下等挑战性环境中拍摄的图像经常会遭受严重的质量下降，导致视觉质量大幅降低。有效地恢复这些退化的图像对于后续的视觉任务至关重要。虽然许多现有方法已经成功地结合了针对个任务的特定先验知识，但这些定制解决方案限制了它们对其他退化的适用性。在这项工作中，我们提出了一个通用的网络架构，称为“ReviveDiff”，它可以解决各种退化问题，并通过增强和恢复图像质量使其恢复生机。我们的方法受到以下观察结果的启发：与运动或电子问题造成的退化不同，恶劣条件下的质量退化主要源于自然介质（如雾、水和低亮度），这些介质通常保留了物体的原始结构。为了恢复此类图像的质量，我们利用了扩散模型的最新进展，并开发了ReviveDiff，从宏观和微观层面恢复图像质量，涵盖决定图像质量的一些关键因素，如清晰度、失真、噪声水平、动态范围和色彩准确度。我们在涵盖五种退化条件（雨天、水下、低光、烟雾和夜间雾霾）的七个基准数据集上对ReviveDiff进行了严格评估。我们的实验结果表明，ReviveDiff在定量和视觉上都优于最先进的方法。|
|**2024-09-27**|[Unsupervised Low-light Image Enhancement with Lookup Tables and Diffusion Priors](http://arxiv.org/abs/2409.18899)|null|弱光图像增强 (LIE) 旨在精确有效地恢复在弱光环境下降质的图像。最近先进的 LIE 技术正在使用深度神经网络，这需要大量的弱光-正常光图像对、网络参数和计算资源。因此，它们的实用性受到限制。在这项工作中，我们设计了一种基于扩散先验和查找表 (DPLUT) 的新型无监督 LIE 框架，以实现高效的弱光图像恢复。所提出的方法包括两个关键组件：光照调整查找表 (LLUT) 和噪声抑制查找表 (NLUT)。LLUT 使用一组无监督损失进行优化。它旨在预测特定图像动态范围调整的逐像素曲线参数。NLUT 旨在去除光线变亮后放大的噪声。由于扩散模型对噪声很敏感，因此引入了扩散先验以实现高性能的噪声抑制。大量实验表明，我们的方法在视觉质量和效率方面优于最先进的方法。|
|**2024-09-27**|[Detecting Dataset Abuse in Fine-Tuning Stable Diffusion Models for Text-to-Image Synthesis](http://arxiv.org/abs/2409.18897)|null|文图生成在生成逼真和风格化的图像方面已经变得非常流行，这通常需要使用特定领域的数据库对生成模型进行微调以完成专门的任务。然而，这些有价值的数据库面临着未经授权使用和未经批准共享的风险，损害了所有者的权利。在本文中，我们解决了在对 Stable Diffusion 模型进行文图生成的微调过程中出现的数据库滥用问题。我们提出了一个数据库水印框架，旨在检测未经授权的使用并追踪数据泄露。该框架在多个水印方案中采用了两种关键策略，对大规模数据库授权有效。大量实验表明，该框架有效，对数据库的影响最小（只需修改 2% 的数据即可实现高检测精度），并且能够追踪数据泄露。我们的结果还突出了该框架的鲁棒性和可迁移性，证明了其在检测数据库滥用方面的实际适用性。|
|**2024-09-27**|[Explainable Artifacts for Synthetic Western Blot Source Attribution](http://arxiv.org/abs/2409.18881)|null|人工智能领域的最新进展使得生成模型能够生成与真实图像难以区分的合成科学图像，这对习惯于处理此类内容的专业科学家也构成了挑战。当被称为“论文工厂”的组织利用这些技术系统地生成虚假文章时，它们可能会助长关于无根据科学的错误信息的传播，从而有可能破坏对科学研究的信任。虽然之前的研究已经探索了黑盒解决方案（例如卷积神经网络）来识别合成内容，但只有一部分研究解决了跨不同模型进行泛化并深入了解合成图像中可用于检测过程的人工痕迹的挑战。本研究旨在识别由最先进的生成模型（例如，生成对抗网络和扩散模型）产生的可解释的人工痕迹，并利用它们进行开放集识别和来源归因（即，指出创建图像的模型）。|
|**2024-09-27**|[Emu3: Next-Token Prediction is All You Need](http://arxiv.org/abs/2409.18869)|null|虽然下一词预测被认为是通向人工通用智能的有希望的途径，但它在多模态任务中一直难以取得优异表现，而多模态任务仍然由扩散模型（例如，Stable Diffusion）和组合方法（例如，CLIP 与 LLM 相结合）主导。在本文中，我们介绍了 Emu3，这是一套全新的最先进的多模态模型，仅使用下一词预测进行训练。通过将图像、文本和视频标记化为离散空间，我们在多模态序列的混合上从头开始训练单个变换器。Emu3 在生成和感知任务中均优于多个完善的特定任务模型，超越了 SDXL 和 LLaVA-1.6 等旗舰模型，同时无需扩散或组合架构。Emu3 还能够通过预测视频序列中的下一个标记来生成高保真视频。我们通过专注于单一焦点：标记，简化了复杂的多模态模型设计，从而在训练和推理过程中释放了巨大的扩展潜力。我们的结果表明，下一词预测是构建超越语言的通用多模态智能的有希望的途径。我们开源了关键技术和模型，以支持在该方向上的进一步研究。|
|**2024-09-27**|[Challenges of Generating Structurally Diverse Graphs](http://arxiv.org/abs/2409.18859)|null|对于许多与图相关的问题，拥有一组结构多样化的图至关重要。例如，此类图可用于测试图算法或其神经网络近似。然而，据我们所知，生成结构多样化图的问题尚未在文献中得到探讨。在本文中，我们填补了这一空白。首先，我们讨论了如何定义一组图的多样性，为什么这项任务不简单，以及如何选择合适的度量标准。然后，对于给定的多样性度量标准，我们提出并比较了几种优化它的算法：我们考虑了基于标准随机图模型、局部图优化、遗传算法和神经生成模型的方法。我们证明，相较于基本的随机图生成器，可以显著提高多样性。此外，我们对生成图的分析使我们能够更好地理解图距离的特性：根据用于优化的多样性度量标准，获得的图可能具有非常不同的结构特性，这为了解多样性度量标准中使用的图距离的敏感性提供了见解。|
|**2024-09-27**|[Convergence of Diffusion Models Under the Manifold Hypothesis in High-Dimensions](http://arxiv.org/abs/2409.18804)|null|去噪扩散概率模型 (DDPM) 是一种强大的最先进方法，用于从高维数据分布生成合成数据，并广泛用于图像、音频和视频生成以及科学及其他领域的更多应用。流形假设指出高维数据通常位于环境空间内的低维流形上，并且被广泛认为在提供的示例中成立。虽然最近的结果为了解扩散模型如何适应流形假设提供了宝贵的见解，但它们没有捕捉到这些模型的巨大经验成功，这使其成为一个非常富有成果的研究方向。在这项工作中，我们研究了流形假设下的 DDPM，并证明了它们在学习分数方面实现了与环境维度无关的速率。在采样方面，我们获得了关于 Kullback-Leibler 散度的与环境维度无关的速率，以及关于 Wasserstein 距离的  $O(\sqrt{D})$ 。我们通过开发一个新的框架来做到这一点，该框架将扩散模型连接到经过充分研究的高斯过程极值理论。|
|**2024-09-27**|[Geometric deep learning for galaxy-halo connection: a case study for galaxy intrinsic alignments](http://arxiv.org/abs/2409.18761)|null|即将进行的宇宙学成像巡天，例如 Rubin Observatory LSST，需要包含真实星系群的大规模模拟，以用于各种科学应用。其中一个特别值得关注的现象是内禀排列 (IA)，即星系倾向于朝向超密度区域排列，如果不对其进行适当建模，可能会在弱引力透镜分析中引入显著的系统偏差。由于计算限制，在广阔的体积范围内模拟与 IA 相关的星系形成和演化的复杂细节是不切实际的。作为替代方案，我们提出了一种在 IllustrisTNG-100 模拟上训练的深度生成模型，用于对 3D 星系形状和方向进行采样，以准确地再现内禀排列以及相关的标量特征。我们将宇宙网建模为一组图，每个图代表一个晕，节点代表子晕/星系。该架构由一个 SO(3) $\times$ $\mathbb{R}^n$ 扩散生成模型组成，用于星系方向和 $n$ 个标量，并使用明确遵守宇宙欧几里德对称性的 E(3) 等变图神经网络实现。该模型能够学习和预测与参考模拟在统计上一致的特征，例如星系方向。值得注意的是，我们的模型展示了联合建模欧几里德值标量（星系大小、形状和颜色）以及非欧几里德值 SO(3) 量（星系方向）的能力，这些量受非线性尺度上高度复杂的星系物理支配。|
|**2024-09-27**|[Unsupervised Fingerphoto Presentation Attack Detection With Diffusion Models](http://arxiv.org/abs/2409.18636)|null|基于智能手机的非接触式指纹认证由于智能手机相机技术的快速发展，已成为传统接触式指纹生物识别系统的可靠替代方案。尽管其便利性很高，但通过指纹照片进行的指纹认证更容易受到伪造攻击，这促使最近的研究工作致力于开发指纹照片呈现攻击检测 (PAD) 技术。然而，先前的 PAD 方法利用了监督学习方法，这些方法需要真实和攻击样本的标记训练数据。这可能会遇到两个关键问题，即 (i)  泛化性：检测训练数据中未见过的呈现攻击工具 (PAI)，以及 (ii) 可扩展性：使用不同的 PAI 收集大型攻击样本数据集。为了应对这些挑战，我们提出了一种基于最先进的深度学习扩散模型的新型无监督方法，即去噪扩散概率模型 (DDPM)，该模型仅使用真实样本进行训练。所提出的方法通过计算 DDPM 的输入和输出对之间的重建相似性来检测呈现攻击 (PA)。我们展示了跨三个 PAI 数据集的大量实验，以测试我们方法的准确性和泛化能力。结果表明，与其他基线无监督方法相比，所提出的基于 DDPM 的 PAD 方法在多个 PAI 类别上实现了显着更好的检测错误率。|
|**2024-09-26**|[FlowTurbo: Towards Real-time Flow-Based Image Generation with Velocity Refiner](http://arxiv.org/abs/2409.18128)|**[link](https://github.com/shiml20/flowturbo)**|基于扩散模型在视觉生成方面的成功，基于流的模型作为另一类重要的生成模型重新兴起，在视觉质量和推理速度方面都取得了与之相当或更好的性能。通过流匹配学习速度场，基于流的模型倾向于产生更直的采样轨迹，这在采样过程中是有利的。然而，与快速采样器已经得到很好发展的扩散模型不同，基于流的生成模型的有效采样还很少被探索。在本文中，我们提出了一个名为FlowTurbo的框架，以加速基于流的模型的采样，同时提高采样质量。我们的主要观察结果是，基于流模型中的速度预测器输出在采样过程中会变得稳定，从而可以通过轻量级速度优化器估计速度。此外，我们还引入了一些技术，包括伪校正器和样本感知编译，以进一步减少推理时间。由于FlowTurbo没有改变多步采样范式，因此可以有效地应用于图像编辑、修复等各种任务。通过将FlowTurbo集成到不同的基于流的模型中，我们在类别条件生成上获得了53.1% $\sim$58.3%的加速比，在文本到图像生成上获得了29.8%$\sim$ 38.5%的加速比。值得注意的是，FlowTurbo在ImageNet上实现了100 (ms / img)时FID为2.12，38 (ms / img)时FID为3.93，实现了实时图像生成，并建立了新的最先进水平。代码可在https://github.com/shiml20/FlowTurbo获取。||
|**2024-09-26**|[Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction](http://arxiv.org/abs/2409.18124)|null|利用预训练文本到图像扩散模型的视觉先验知识为增强密集预测任务中的零样本泛化能力提供了一种很有前景的解决方案。然而，现有方法通常不加批判地使用原始的扩散公式，由于密集预测和图像生成之间的根本差异，这可能不是最佳选择。在本文中，我们对用于密集预测的扩散公式进行了系统分析，重点关注质量和效率。我们发现，用于图像生成的原始参数化类型（学习预测噪声）对密集预测是有害的；多步加噪/去噪扩散过程也是不必要的，并且难以优化。基于这些见解，我们推出了Lotus，这是一个基于扩散的视觉基础模型，它采用了一种简单而有效的密集预测适应协议。具体来说，Lotus被训练成直接预测注释而不是噪声，从而避免了有害的方差。我们还将扩散过程重新定义为单步过程，简化了优化并显著提高了推理速度。此外，我们引入了一种称为细节保留器的新型调整策略，它可以实现更准确、更细粒度的预测。在不扩大训练数据或模型容量的情况下，Lotus在各种数据集上的零样本深度和法线估计方面均达到了最先进的性能。它还显著提高了效率，比大多数现有的基于扩散的方法快数百倍。||
|**2024-09-26**|[EdgeRunner: Auto-regressive Auto-encoder for Artistic Mesh Generation](http://arxiv.org/abs/2409.18114)|null|目前的自动回归网格生成方法存在着诸如网格不完整、细节不足和泛化能力差等问题。在本文中，我们提出了一种自回归自动编码器（ArAE）模型，能够生成高达4,000个面片、空间分辨率为 $512^3$ 的高质量三维网格。我们引入了一种新颖的网格标记化算法，可以有效地将三角网格压缩成一维标记序列，显著提高了训练效率。此外，我们的模型将变长三角网格压缩成固定长度的潜在空间，从而能够训练潜在扩散模型以获得更好的泛化能力。大量实验表明，我们的模型在点云和图像条件网格生成任务中均表现出优越的质量、多样性和泛化能力。||
|**2024-09-26**|[StackGen: Generating Stable Structures from Silhouettes via Diffusion](http://arxiv.org/abs/2409.18098)|null|Humans naturally obtain intuition about the interactions between and the stability of rigid objects by observing and interacting with the world. It is this intuition that governs the way in which we regularly configure objects in our environment, allowing us to build complex structures from simple, everyday objects. Robotic agents, on the other hand, traditionally require an explicit model of the world that includes the detailed geometry of each object and an analytical model of the environment dynamics, which are difficult to scale and preclude generalization. Instead, robots would benefit from an awareness of intuitive physics that enables them to similarly reason over the stable interaction of objects in their environment. Towards that goal, we propose StackGen, a diffusion model that generates diverse stable configurations of building blocks matching a target silhouette. To demonstrate the capability of the method, we evaluate it in a simulated environment and deploy it in the real setting using a robotic arm to assemble structures generated by the model.||
|**2024-09-26**|[DiffSSC: Semantic LiDAR Scan Completion using Denoising Diffusion Probabilistic Models](http://arxiv.org/abs/2409.18092)|null|感知系统在自动驾驶中起着至关重要的作用，它结合了多个传感器和相应的计算机视觉算法。3D 激光雷达传感器被广泛用于捕捉车辆周围环境的稀疏点云。然而，由于这些点云的稀疏性和缺乏语义信息，此类系统难以感知遮挡区域和场景中的间隙。为了应对这些挑战，语义场景补全 (SSC) 在给定原始激光雷达测量值的情况下，联合预测场景中未观察到的几何形状和语义信息，旨在实现更完整的场景表示。基于扩散模型在图像生成和超分辨率任务中的良好结果，我们建议将其扩展到 SSC，方法是在点空间和语义空间中分别实现去噪和加噪扩散过程。为了控制生成过程，我们采用语义激光雷达点云作为条件输入，并设计了局部和全局正则化损失来稳定去噪过程。我们在自动驾驶数据集上评估了我们的方法，我们的方法在 SSC 方面的性能优于最先进的方法。||
|**2024-09-26**|[Stable Video Portraits](http://arxiv.org/abs/2409.18083)|null|Rapid advances in the field of generative AI and text-to-image methods in particular have transformed the way we interact with and perceive computer-generated imagery today. In parallel, much progress has been made in 3D face reconstruction, using 3D Morphable Models (3DMM). In this paper, we present SVP, a novel hybrid 2D/3D generation method that outputs photorealistic videos of talking faces leveraging a large pre-trained text-to-image prior (2D), controlled via a 3DMM (3D). Specifically, we introduce a person-specific fine-tuning of a general 2D stable diffusion model which we lift to a video model by providing temporal 3DMM sequences as conditioning and by introducing a temporal denoising procedure. As an output, this model generates temporally smooth imagery of a person with 3DMM-based controls, i.e., a person-specific avatar. The facial appearance of this person-specific avatar can be edited and morphed to text-defined celebrities, without any fine-tuning at test time. The method is analyzed quantitatively and qualitatively, and we show that our method outperforms state-of-the-art monocular head avatar methods.||
|**2024-09-26**|[PhoCoLens: Photorealistic and Consistent Reconstruction in Lensless Imaging](http://arxiv.org/abs/2409.17996)|null|Lensless cameras offer significant advantages in size, weight, and cost compared to traditional lens-based systems. Without a focusing lens, lensless cameras rely on computational algorithms to recover the scenes from multiplexed measurements. However, current algorithms struggle with inaccurate forward imaging models and insufficient priors to reconstruct high-quality images. To overcome these limitations, we introduce a novel two-stage approach for consistent and photorealistic lensless image reconstruction. The first stage of our approach ensures data consistency by focusing on accurately reconstructing the low-frequency content with a spatially varying deconvolution method that adjusts to changes in the Point Spread Function (PSF) across the camera's field of view. The second stage enhances photorealism by incorporating a generative prior from pre-trained diffusion models. By conditioning on the low-frequency content retrieved in the first stage, the diffusion model effectively reconstructs the high-frequency details that are typically lost in the lensless imaging process, while also maintaining image fidelity. Our method achieves a superior balance between data fidelity and visual quality compared to existing methods, as demonstrated with two popular lensless systems, PhlatCam and DiffuserCam. Project website: https://phocolens.github.io/.||
|**2024-09-26**|[Joint Localization and Planning using Diffusion](http://arxiv.org/abs/2409.17995)|null|Diffusion models have been successfully applied to robotics problems such as manipulation and vehicle path planning. In this work, we explore their application to end-to-end navigation -- including both perception and planning -- by considering the problem of jointly performing global localization and path planning in known but arbitrary 2D environments. In particular, we introduce a diffusion model which produces collision-free paths in a global reference frame given an egocentric LIDAR scan, an arbitrary map, and a desired goal position. To this end, we implement diffusion in the space of paths in SE(2), and describe how to condition the denoising process on both obstacles and sensor observations. In our evaluation, we show that the proposed conditioning techniques enable generalization to realistic maps of considerably different appearance than the training environment, demonstrate our model's ability to accurately describe ambiguous solutions, and run extensive simulation experiments showcasing our model's use as a real-time, end-to-end localization and planning stack.||
|**2024-09-26**|[CNCA: Toward Customizable and Natural Generation of Adversarial Camouflage for Vehicle Detectors](http://arxiv.org/abs/2409.17963)|null|Prior works on physical adversarial camouflage against vehicle detectors mainly focus on the effectiveness and robustness of the attack. The current most successful methods optimize 3D vehicle texture at a pixel level. However, this results in conspicuous and attention-grabbing patterns in the generated camouflage, which humans can easily identify. To address this issue, we propose a Customizable and Natural Camouflage Attack (CNCA) method by leveraging an off-the-shelf pre-trained diffusion model. By sampling the optimal texture image from the diffusion model with a user-specific text prompt, our method can generate natural and customizable adversarial camouflage while maintaining high attack performance. With extensive experiments on the digital and physical worlds and user studies, the results demonstrate that our proposed method can generate significantly more natural-looking camouflage than the state-of-the-art baselines while achieving competitive attack performance. Our code is available at \href{https://anonymous.4open.science/r/CNCA-1D54}{https://anonymous.4open.science/r/CNCA-1D54}||
|**2024-09-26**|[Relativistic diffusion model for hadron production in p-Pb collisions at the LHC](http://arxiv.org/abs/2409.17960)|null|We investigate charged-hadron production in relativistic heavy-ion collisions of asymmetric systems within a nonequilibrium-statistical framework. Calculated centrality-dependent pseudorapidity distributions for p-Pb collisions at sqrt(s_NN)=5.02 and 8.16 TeV are compared with data from the Large Hadron Collider (LHC). Our approach combines a relativistic diffusion model with formulations based on quantum chromodynamics while utilizing numerical solutions of a Fokker-Planck equation to account for the shift and broadening of the fragmentation sources for particle-production with respect to the stopping (net-baryon) rapidity distributions. To represent the centrality dependence of charged-hadron production in asymmetric systems over a broad region of pseudorapidities, the consideration and precise modelling of the fragmentation sources - along with the central gluon-gluon source - is found to be essential. Specifically, this results in an inversion of the particle-production amplitude from backward- to forward-dominance when transitioning from central to peripheral collisions, in agreement with recent ATLAS and ALICE p-Pb data at sqrt(s_NN)=5.02 TeV.||
|**2024-09-18**|[Massively Multi-Person 3D Human Motion Forecasting with Scene Context](http://arxiv.org/abs/2409.12189)|null|Forecasting long-term 3D human motion is challenging: the stochasticity of human behavior makes it hard to generate realistic human motion from the input sequence alone. Information on the scene environment and the motion of nearby people can greatly aid the generation process. We propose a scene-aware social transformer model (SAST) to forecast long-term (10s) human motion motion. Unlike previous models, our approach can model interactions between both widely varying numbers of people and objects in a scene. We combine a temporal convolutional encoder-decoder architecture with a Transformer-based bottleneck that allows us to efficiently combine motion and scene information. We model the conditional motion distribution using denoising diffusion models. We benchmark our approach on the Humans in Kitchens dataset, which contains 1 to 16 persons and 29 to 50 objects that are visible simultaneously. Our model outperforms other approaches in terms of realism and diversity on different metrics and in a user study. Code is available at https://github.com/felixbmuller/SAST.||
|**2024-09-18**|[MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion](http://arxiv.org/abs/2409.12140)|null|We introduce MoRAG, a novel multi-part fusion based retrieval-augmented generation strategy for text-based human motion generation. The method enhances motion diffusion models by leveraging additional knowledge obtained through an improved motion retrieval process. By effectively prompting large language models (LLMs), we address spelling errors and rephrasing issues in motion retrieval. Our approach utilizes a multi-part retrieval strategy to improve the generalizability of motion retrieval across the language space. We create diverse samples through the spatial composition of the retrieved motions. Furthermore, by utilizing low-level, part-specific motion information, we can construct motion samples for unseen text descriptions. Our experiments demonstrate that our framework can serve as a plug-and-play module, improving the performance of motion diffusion models. Code, pretrained models and sample videos will be made available at: https://motion-rag.github.io/||
|**2024-09-18**|[Brain-Streams: fMRI-to-Image Reconstruction with Multi-modal Guidance](http://arxiv.org/abs/2409.12099)|null|Understanding how humans process visual information is one of the crucial steps for unraveling the underlying mechanism of brain activity. Recently, this curiosity has motivated the fMRI-to-image reconstruction task; given the fMRI data from visual stimuli, it aims to reconstruct the corresponding visual stimuli. Surprisingly, leveraging powerful generative models such as the Latent Diffusion Model (LDM) has shown promising results in reconstructing complex visual stimuli such as high-resolution natural images from vision datasets. Despite the impressive structural fidelity of these reconstructions, they often lack details of small objects, ambiguous shapes, and semantic nuances. Consequently, the incorporation of additional semantic knowledge, beyond mere visuals, becomes imperative. In light of this, we exploit how modern LDMs effectively incorporate multi-modal guidance (text guidance, visual guidance, and image layout) for structurally and semantically plausible image generations. Specifically, inspired by the two-streams hypothesis suggesting that perceptual and semantic information are processed in different brain regions, our framework, Brain-Streams, maps fMRI signals from these brain regions to appropriate embeddings. That is, by extracting textual guidance from semantic information regions and visual guidance from perceptual information regions, Brain-Streams provides accurate multi-modal guidance to LDMs. We validate the reconstruction ability of Brain-Streams both quantitatively and qualitatively on a real fMRI dataset comprising natural image stimuli and fMRI data.||
|**2024-09-18**|[Design of Ligand-Binding Proteins with Atomic Flow Matching](http://arxiv.org/abs/2409.12080)|null|Designing novel proteins that bind to small molecules is a long-standing challenge in computational biology, with applications in developing catalysts, biosensors, and more. Current computational methods rely on the assumption that the binding pose of the target molecule is known, which is not always feasible, as conformations of novel targets are often unknown and tend to change upon binding. In this work, we formulate proteins and molecules as unified biotokens, and present AtomFlow, a novel deep generative model under the flow-matching framework for the design of ligand-binding proteins from the 2D target molecular graph alone. Operating on representative atoms of biotokens, AtomFlow captures the flexibility of ligands and generates ligand conformations and protein backbone structures iteratively. We consider the multi-scale nature of biotokens and demonstrate that AtomFlow can be effectively trained on a subset of structures from the Protein Data Bank, by matching flow vector field using an SE(3) equivariant structure prediction network. Experimental results show that our method can generate high fidelity ligand-binding proteins and achieve performance comparable to the state-of-the-art model RFDiffusionAA, while not requiring bound ligand structures. As a general framework, AtomFlow holds the potential to be applied to various biomolecule generation tasks in the future.||
|**2024-09-18**|[LEMON: Localized Editing with Mesh Optimization and Neural Shaders](http://arxiv.org/abs/2409.12024)|null|In practical use cases, polygonal mesh editing can be faster than generating new ones, but it can still be challenging and time-consuming for users. Existing solutions for this problem tend to focus on a single task, either geometry or novel view synthesis, which often leads to disjointed results between the mesh and view. In this work, we propose LEMON, a mesh editing pipeline that combines neural deferred shading with localized mesh optimization. Our approach begins by identifying the most important vertices in the mesh for editing, utilizing a segmentation model to focus on these key regions. Given multi-view images of an object, we optimize a neural shader and a polygonal mesh while extracting the normal map and the rendered image from each view. By using these outputs as conditioning data, we edit the input images with a text-to-image diffusion model and iteratively update our dataset while deforming the mesh. This process results in a polygonal mesh that is edited according to the given text instruction, preserving the geometric characteristics of the initial mesh while focusing on the most significant areas. We evaluate our pipeline using the DTU dataset, demonstrating that it generates finely-edited meshes more rapidly than the current state-of-the-art methods. We include our code and additional results in the supplementary material.||
|**2024-09-18**|[Generation of Complex 3D Human Motion by Temporal and Spatial Composition of Diffusion Models](http://arxiv.org/abs/2409.11920)|null|In this paper, we address the challenge of generating realistic 3D human motions for action classes that were never seen during the training phase. Our approach involves decomposing complex actions into simpler movements, specifically those observed during training, by leveraging the knowledge of human motion contained in GPTs models. These simpler movements are then combined into a single, realistic animation using the properties of diffusion models. Our claim is that this decomposition and subsequent recombination of simple movements can synthesize an animation that accurately represents the complex input action. This method operates during the inference phase and can be integrated with any pre-trained diffusion model, enabling the synthesis of motion classes not present in the training data. We evaluate our method by dividing two benchmark human motion datasets into basic and complex actions, and then compare its performance against the state-of-the-art.||
|**2024-09-18**|[Finding the Subjective Truth: Collecting 2 Million Votes for Comprehensive Gen-AI Model Evaluation](http://arxiv.org/abs/2409.11904)|null|Efficiently evaluating the performance of text-to-image models is difficult as it inherently requires subjective judgment and human preference, making it hard to compare different models and quantify the state of the art. Leveraging Rapidata's technology, we present an efficient annotation framework that sources human feedback from a diverse, global pool of annotators. Our study collected over 2 million annotations across 4,512 images, evaluating four prominent models (DALL-E 3, Flux.1, MidJourney, and Stable Diffusion) on style preference, coherence, and text-to-image alignment. We demonstrate that our approach makes it feasible to comprehensively rank image generation models based on a vast pool of annotators and show that the diverse annotator demographics reflect the world population, significantly decreasing the risk of biases.||
|**2024-09-18**|[NT-ViT: Neural Transcoding Vision Transformers for EEG-to-fMRI Synthesis](http://arxiv.org/abs/2409.11836)|null|This paper introduces the Neural Transcoding Vision Transformer (\modelname), a generative model designed to estimate high-resolution functional Magnetic Resonance Imaging (fMRI) samples from simultaneous Electroencephalography (EEG) data. A key feature of \modelname is its Domain Matching (DM) sub-module which effectively aligns the latent EEG representations with those of fMRI volumes, enhancing the model's accuracy and reliability. Unlike previous methods that tend to struggle with fidelity and reproducibility of images, \modelname addresses these challenges by ensuring methodological integrity and higher-quality reconstructions which we showcase through extensive evaluation on two benchmark datasets; \modelname outperforms the current state-of-the-art by a significant margin in both cases, e.g. achieving a $10\times$ reduction in RMSE and a $3.14\times$ increase in SSIM on the Oddball dataset. An ablation study also provides insights into the contribution of each component to the model's overall effectiveness. This development is critical in offering a new approach to lessen the time and financial constraints typically linked with high-resolution brain imaging, thereby aiding in the swift and precise diagnosis of neurological disorders. Although it is not a replacement for actual fMRI but rather a step towards making such imaging more accessible, we believe that it represents a pivotal advancement in clinical practice and neuroscience research. Code is available at \url{https://github.com/rom42pla/ntvit}.||
|**2024-09-18**|[DPI-TTS: Directional Patch Interaction for Fast-Converging and Style Temporal Modeling in Text-to-Speech](http://arxiv.org/abs/2409.11835)|null|In recent years, speech diffusion models have advanced rapidly. Alongside the widely used U-Net architecture, transformer-based models such as the Diffusion Transformer (DiT) have also gained attention. However, current DiT speech models treat Mel spectrograms as general images, which overlooks the specific acoustic properties of speech. To address these limitations, we propose a method called Directional Patch Interaction for Text-to-Speech (DPI-TTS), which builds on DiT and achieves fast training without compromising accuracy. Notably, DPI-TTS employs a low-to-high frequency, frame-by-frame progressive inference approach that aligns more closely with acoustic properties, enhancing the naturalness of the generated speech. Additionally, we introduce a fine-grained style temporal modeling method that further improves speaker style similarity. Experimental results demonstrate that our method increases the training speed by nearly 2 times and significantly outperforms the baseline models.||
|**2024-09-18**|[RaggeDi: Diffusion-based State Estimation of Disordered Rags, Sheets, Towels and Blankets](http://arxiv.org/abs/2409.11831)|null|Cloth state estimation is an important problem in robotics. It is essential for the robot to know the accurate state to manipulate cloth and execute tasks such as robotic dressing, stitching, and covering/uncovering human beings. However, estimating cloth state accurately remains challenging due to its high flexibility and self-occlusion. This paper proposes a diffusion model-based pipeline that formulates the cloth state estimation as an image generation problem by representing the cloth state as an RGB image that describes the point-wise translation (translation map) between a pre-defined flattened mesh and the deformed mesh in a canonical space. Then we train a conditional diffusion-based image generation model to predict the translation map based on an observation. Experiments are conducted in both simulation and the real world to validate the performance of our method. Results indicate that our method outperforms two recent methods in both accuracy and speed.||
|**2024-09-17**|[Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion](http://arxiv.org/abs/2409.11406)|null|In 3D modeling, designers often use an existing 3D model as a reference to create new ones. This practice has inspired the development of Phidias, a novel generative model that uses diffusion for reference-augmented 3D generation. Given an image, our method leverages a retrieved or user-provided 3D reference model to guide the generation process, thereby enhancing the generation quality, generalization ability, and controllability. Our model integrates three key components: 1) meta-ControlNet that dynamically modulates the conditioning strength, 2) dynamic reference routing that mitigates misalignment between the input image and 3D reference, and 3) self-reference augmentations that enable self-supervised training with a progressive curriculum. Collectively, these designs result in a clear improvement over existing methods. Phidias establishes a unified framework for 3D generation using text, image, and 3D conditions with versatile applications.||
|**2024-09-17**|[Teaching dark matter simulations to speak the halo language](http://arxiv.org/abs/2409.11401)|**[link](https://github.com/shivampcosmo/gotham)**|We develop a transformer-based conditional generative model for discrete point objects and their properties. We use it to build a model for populating cosmological simulations with gravitationally collapsed structures called dark matter halos. Specifically, we condition our model with dark matter distribution obtained from fast, approximate simulations to recover the correct three-dimensional positions and masses of individual halos. This leads to a first model that can recover the statistical properties of the halos at small scales to better than 3% level using an accelerated dark matter simulation. This trained model can then be applied to simulations with significantly larger volumes which would otherwise be computationally prohibitive with traditional simulations, and also provides a crucial missing link in making end-to-end differentiable cosmological simulations. The code, named GOTHAM (Generative cOnditional Transformer for Halo's Auto-regressive Modeling) is publicly available at \url{https://github.com/shivampcosmo/GOTHAM}.||
|**2024-09-17**|[Ultrasound Image Enhancement with the Variance of Diffusion Models](http://arxiv.org/abs/2409.11380)|**[link](https://github.com/yuxin-zhang-jasmine/ius2024_diffusion)**|Ultrasound imaging, despite its widespread use in medicine, often suffers from various sources of noise and artifacts that impact the signal-to-noise ratio and overall image quality. Enhancing ultrasound images requires a delicate balance between contrast, resolution, and speckle preservation. This paper introduces a novel approach that integrates adaptive beamforming with denoising diffusion-based variance imaging to address this challenge. By applying Eigenspace-Based Minimum Variance (EBMV) beamforming and employing a denoising diffusion model fine-tuned on ultrasound data, our method computes the variance across multiple diffusion-denoised samples to produce high-quality despeckled images. This approach leverages both the inherent multiplicative noise of ultrasound and the stochastic nature of diffusion models. Experimental results on a publicly available dataset demonstrate the effectiveness of our method in achieving superior image reconstructions from single plane-wave acquisitions. The code is available at: https://github.com/Yuxin-Zhang-Jasmine/IUS2024_Diffusion.||
|**2024-09-17**|[OSV: One Step is Enough for High-Quality Image to Video Generation](http://arxiv.org/abs/2409.11367)|null|Video diffusion models have shown great potential in generating high-quality videos, making them an increasingly popular focus. However, their inherent iterative nature leads to substantial computational and time costs. While efforts have been made to accelerate video diffusion by reducing inference steps (through techniques like consistency distillation) and GAN training (these approaches often fall short in either performance or training stability). In this work, we introduce a two-stage training framework that effectively combines consistency distillation with GAN training to address these challenges. Additionally, we propose a novel video discriminator design, which eliminates the need for decoding the video latents and improves the final performance. Our model is capable of producing high-quality videos in merely one-step, with the flexibility to perform multi-step refinement for further performance enhancement. Our quantitative evaluation on the OpenWebVid-1M benchmark shows that our model significantly outperforms existing methods. Notably, our 1-step performance(FVD 171.15) exceeds the 8-step performance of the consistency distillation based method, AnimateLCM (FVD 184.79), and approaches the 25-step performance of advanced Stable Video Diffusion (FVD 156.94).||
|**2024-09-17**|[Fine-Tuning Image-Conditional Diffusion Models is Easier than You Think](http://arxiv.org/abs/2409.11355)|**[link](https://github.com/VisualComputingInstitute/diffusion-e2e-ft)**|Recent work showed that large diffusion models can be reused as highly precise monocular depth estimators by casting depth estimation as an image-conditional image generation task. While the proposed model achieved state-of-the-art results, high computational demands due to multi-step inference limited its use in many scenarios. In this paper, we show that the perceived inefficiency was caused by a flaw in the inference pipeline that has so far gone unnoticed. The fixed model performs comparably to the best previously reported configuration while being more than 200 $\times$ faster. To optimize for downstream task performance, we perform end-to-end fine-tuning on top of the single-step model with task-specific losses and get a deterministic model that outperforms all other diffusion-based depth and normal estimation models on common zero-shot benchmarks. We surprisingly find that this fine-tuning protocol also works directly on Stable Diffusion and achieves comparable performance to current state-of-the-art diffusion-based depth and normal estimation models, calling into question some of the conclusions drawn from prior works.||
|**2024-09-17**|[OmniGen: Unified Image Generation](http://arxiv.org/abs/2409.11340)|**[link](https://github.com/vectorspacelab/omnigen)**|In this work, we introduce OmniGen, a new diffusion model for unified image generation. Unlike popular diffusion models (e.g., Stable Diffusion), OmniGen no longer requires additional modules such as ControlNet or IP-Adapter to process diverse control conditions. OmniGenis characterized by the following features: 1) Unification: OmniGen not only demonstrates text-to-image generation capabilities but also inherently supports other downstream tasks, such as image editing, subject-driven generation, and visual-conditional generation. Additionally, OmniGen can handle classical computer vision tasks by transforming them into image generation tasks, such as edge detection and human pose recognition. 2) Simplicity: The architecture of OmniGen is highly simplified, eliminating the need for additional text encoders. Moreover, it is more user-friendly compared to existing diffusion models, enabling complex tasks to be accomplished through instructions without the need for extra preprocessing steps (e.g., human pose estimation), thereby significantly simplifying the workflow of image generation. 3) Knowledge Transfer: Through learning in a unified format, OmniGen effectively transfers knowledge across different tasks, manages unseen tasks and domains, and exhibits novel capabilities. We also explore the model's reasoning capabilities and potential applications of chain-of-thought mechanism. This work represents the first attempt at a general-purpose image generation model, and there remain several unresolved issues. We will open-source the related resources at https://github.com/VectorSpaceLab/OmniGen to foster advancements in this field.||
|**2024-09-17**|[fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction](http://arxiv.org/abs/2409.11315)|null|Reconstructing 3D visuals from functional Magnetic Resonance Imaging (fMRI) data, introduced as Recon3DMind in our conference work, is of significant interest to both cognitive neuroscience and computer vision. To advance this task, we present the fMRI-3D dataset, which includes data from 15 participants and showcases a total of 4768 3D objects. The dataset comprises two components: fMRI-Shape, previously introduced and accessible at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape, and fMRI-Objaverse, proposed in this paper and available at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse. fMRI-Objaverse includes data from 5 subjects, 4 of whom are also part of the Core set in fMRI-Shape, with each subject viewing 3142 3D objects across 117 categories, all accompanied by text captions. This significantly enhances the diversity and potential applications of the dataset. Additionally, we propose MinD-3D, a novel framework designed to decode 3D visual information from fMRI signals. The framework first extracts and aggregates features from fMRI data using a neuro-fusion encoder, then employs a feature-bridge diffusion model to generate visual features, and finally reconstructs the 3D object using a generative transformer decoder. We establish new benchmarks by designing metrics at both semantic and structural levels to evaluate model performance. Furthermore, we assess our model's effectiveness in an Out-of-Distribution setting and analyze the attribution of the extracted features and the visual ROIs in fMRI signals. Our experiments demonstrate that MinD-3D not only reconstructs 3D objects with high semantic and spatial accuracy but also deepens our understanding of how human brain processes 3D visual information. Project page at: https://jianxgao.github.io/MinD-3D.||
|**2024-09-17**|[SpMis: An Investigation of Synthetic Spoken Misinformation Detection](http://arxiv.org/abs/2409.11308)|null|In recent years, speech generation technology has advanced rapidly, fueled by generative models and large-scale training techniques. While these developments have enabled the production of high-quality synthetic speech, they have also raised concerns about the misuse of this technology, particularly for generating synthetic misinformation. Current research primarily focuses on distinguishing machine-generated speech from human-produced speech, but the more urgent challenge is detecting misinformation within spoken content. This task requires a thorough analysis of factors such as speaker identity, topic, and synthesis. To address this need, we conduct an initial investigation into synthetic spoken misinformation detection by introducing an open-source dataset, SpMis. SpMis includes speech synthesized from over 1,000 speakers across five common topics, utilizing state-of-the-art text-to-speech systems. Although our results show promising detection capabilities, they also reveal substantial challenges for practical implementation, underscoring the importance of ongoing research in this critical area.||
|**2024-09-17**|[DroneDiffusion: Robust Quadrotor Dynamics Learning with Diffusion Models](http://arxiv.org/abs/2409.11292)|null|An inherent fragility of quadrotor systems stems from model inaccuracies and external disturbances. These factors hinder performance and compromise the stability of the system, making precise control challenging. Existing model-based approaches either make deterministic assumptions, utilize Gaussian-based representations of uncertainty, or rely on nominal models, all of which often fall short in capturing the complex, multimodal nature of real-world dynamics. This work introduces DroneDiffusion, a novel framework that leverages conditional diffusion models to learn quadrotor dynamics, formulated as a sequence generation task. DroneDiffusion achieves superior generalization to unseen, complex scenarios by capturing the temporal nature of uncertainties and mitigating error propagation. We integrate the learned dynamics with an adaptive controller for trajectory tracking with stability guarantees. Extensive experiments in both simulation and real-world flights demonstrate the robustness of the framework across a range of scenarios, including unfamiliar flight paths and varying payloads, velocities, and wind disturbances.||
|**2024-09-17**|[Learning Source Disentanglement in Neural Audio Codec](http://arxiv.org/abs/2409.11228)|null|Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.||
|**2024-09-13**|[Closed-Loop Visuomotor Control with Generative Expectation for Robotic Manipulation](http://arxiv.org/abs/2409.09016)|**[link](https://github.com/OpenDriveLab/CLOVER)**|Despite significant progress in robotics and embodied AI in recent years, deploying robots for long-horizon tasks remains a great challenge. Majority of prior arts adhere to an open-loop philosophy and lack real-time feedback, leading to error accumulation and undesirable robustness. A handful of approaches have endeavored to establish feedback mechanisms leveraging pixel-level differences or pre-trained visual representations, yet their efficacy and adaptability have been found to be constrained. Inspired by classic closed-loop control systems, we propose CLOVER, a closed-loop visuomotor control framework that incorporates feedback mechanisms to improve adaptive robotic control. CLOVER consists of a text-conditioned video diffusion model for generating visual plans as reference inputs, a measurable embedding space for accurate error quantification, and a feedback-driven controller that refines actions from feedback and initiates replans as needed. Our framework exhibits notable advancement in real-world robotic tasks and achieves state-of-the-art on CALVIN benchmark, improving by 8% over previous open-loop counterparts. Code and checkpoints are maintained at https://github.com/OpenDriveLab/CLOVER.||
|**2024-09-13**|[A Diffusion Approach to Radiance Field Relighting using Multi-Illumination Synthesis](http://arxiv.org/abs/2409.08947)|null|Relighting radiance fields is severely underconstrained for multi-view data, which is most often captured under a single illumination condition; It is especially hard for full scenes containing multiple objects. We introduce a method to create relightable radiance fields using such single-illumination data by exploiting priors extracted from 2D image diffusion models. We first fine-tune a 2D diffusion model on a multi-illumination dataset conditioned by light direction, allowing us to augment a single-illumination capture into a realistic -- but possibly inconsistent -- multi-illumination dataset from directly defined light directions. We use this augmented data to create a relightable radiance field represented by 3D Gaussian splats. To allow direct control of light direction for low-frequency lighting, we represent appearance with a multi-layer perceptron parameterized on light direction. To enforce multi-view consistency and overcome inaccuracies we optimize a per-image auxiliary feature vector. We show results on synthetic and real multi-view data under single illumination, demonstrating that our method successfully exploits 2D diffusion model priors to allow realistic 3D relighting for complete scenes. Project site https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/||
|**2024-09-13**|[Latent Space Score-based Diffusion Model for Probabilistic Multivariate Time Series Imputation](http://arxiv.org/abs/2409.08917)|**[link](https://github.com/gorgen2020/LSSDM_imputation)**|Accurate imputation is essential for the reliability and success of downstream tasks. Recently, diffusion models have attracted great attention in this field. However, these models neglect the latent distribution in a lower-dimensional space derived from the observed data, which limits the generative capacity of the diffusion model. Additionally, dealing with the original missing data without labels becomes particularly problematic. To address these issues, we propose the Latent Space Score-Based Diffusion Model (LSSDM) for probabilistic multivariate time series imputation. Observed values are projected onto low-dimensional latent space and coarse values of the missing data are reconstructed without knowing their ground truth values by this unsupervised learning approach. Finally, the reconstructed values are fed into a conditional diffusion model to obtain the precise imputed values of the time series. In this way, LSSDM not only possesses the power to identify the latent distribution but also seamlessly integrates the diffusion model to obtain the high-fidelity imputed values and assess the uncertainty of the dataset. Experimental results demonstrate that LSSDM achieves superior imputation performance while also providing a better explanation and uncertainty analysis of the imputation mechanism. The website of the code is \textit{https://github.com/gorgen2020/LSSDM\_imputation}.||
|**2024-09-13**|[Gaussian is All You Need: A Unified Framework for Solving Inverse Problems via Diffusion Posterior Sampling](http://arxiv.org/abs/2409.08906)|null|Diffusion models can generate a variety of high-quality images by modeling complex data distributions. Trained diffusion models can also be very effective image priors for solving inverse problems. Most of the existing diffusion-based methods integrate data consistency steps within the diffusion reverse sampling process. The data consistency steps rely on an approximate likelihood function. In this paper, we show that the existing approximations are either insufficient or computationally inefficient. To address these issues, we propose a unified likelihood approximation method that incorporates a covariance correction term to enhance the performance and avoids propagating gradients through the diffusion model. The correction term, when integrated into the reverse diffusion sampling process, achieves better convergence towards the true data posterior for selected distributions and improves performance on real-world natural image datasets. Furthermore, we present an efficient way to factorize and invert the covariance matrix of the likelihood function for several inverse problems. We present comprehensive experiments to demonstrate the effectiveness of our method over several existing approaches.||
|**2024-09-13**|[Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control](http://arxiv.org/abs/2409.08861)|null|Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there has not been many theoretically-sound methods for improving these models with reward fine-tuning. In this work, we cast reward fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific memoryless noise schedule must be enforced during fine-tuning, in order to account for the dependency between the noise variable and the generated samples. We also propose a new algorithm named Adjoint Matching which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We find that our approach significantly improves over existing methods for reward fine-tuning, achieving better consistency, realism, and generalization to unseen human preference reward models, while retaining sample diversity.||
|**2024-09-13**|[InstantDrag: Improving Interactivity in Drag-based Image Editing](http://arxiv.org/abs/2409.08857)|null|Drag-based image editing has recently gained popularity for its interactivity and precision. However, despite the ability of text-to-image models to generate samples within a second, drag editing still lags behind due to the challenge of accurately reflecting user interaction while maintaining image content. Some existing approaches rely on computationally intensive per-image optimization or intricate guidance-based methods, requiring additional inputs such as masks for movable regions and text prompts, thereby compromising the interactivity of the editing process. We introduce InstantDrag, an optimization-free pipeline that enhances interactivity and speed, requiring only an image and a drag instruction as input. InstantDrag consists of two carefully designed networks: a drag-conditioned optical flow generator (FlowGen) and an optical flow-conditioned diffusion model (FlowDiffusion). InstantDrag learns motion dynamics for drag-based image editing in real-world video datasets by decomposing the task into motion generation and motion-conditioned image generation. We demonstrate InstantDrag's capability to perform fast, photo-realistic edits without masks or text prompts through experiments on facial video datasets and general scenes. These results highlight the efficiency of our approach in handling drag-based image editing, making it a promising solution for interactive, real-time applications.||
|**2024-09-13**|[DX2CT: Diffusion Model for 3D CT Reconstruction from Bi or Mono-planar 2D X-ray(s)](http://arxiv.org/abs/2409.08850)|null|Computational tomography (CT) provides high-resolution medical imaging, but it can expose patients to high radiation. X-ray scanners have low radiation exposure, but their resolutions are low. This paper proposes a new conditional diffusion model, DX2CT, that reconstructs three-dimensional (3D) CT volumes from bi or mono-planar X-ray image(s). Proposed DX2CT consists of two key components: 1) modulating feature maps extracted from two-dimensional (2D) X-ray(s) with 3D positions of CT volume using a new transformer and 2) effectively using the modulated 3D position-aware feature maps as conditions of DX2CT. In particular, the proposed transformer can provide conditions with rich information of a target CT slice to the conditional diffusion model, enabling high-quality CT reconstruction. Our experiments with the bi or mono-planar X-ray(s) benchmark datasets show that proposed DX2CT outperforms several state-of-the-art methods. Our codes and model will be available at: https://www.github.com/intyeger/DX2CT.||
|**2024-09-13**|[DFADD: The Diffusion and Flow-Matching Based Audio Deepfake Dataset](http://arxiv.org/abs/2409.08731)|**[link](https://github.com/dfadd-dataset/dfadd_demo_pages)**|Mainstream zero-shot TTS production systems like Voicebox and Seed-TTS achieve human parity speech by leveraging Flow-matching and Diffusion models, respectively. Unfortunately, human-level audio synthesis leads to identity misuse and information security issues. Currently, many antispoofing models have been developed against deepfake audio. However, the efficacy of current state-of-the-art anti-spoofing models in countering audio synthesized by diffusion and flowmatching based TTS systems remains unknown. In this paper, we proposed the Diffusion and Flow-matching based Audio Deepfake (DFADD) dataset. The DFADD dataset collected the deepfake audio based on advanced diffusion and flowmatching TTS models. Additionally, we reveal that current anti-spoofing models lack sufficient robustness against highly human-like audio generated by diffusion and flow-matching TTS systems. The proposed DFADD dataset addresses this gap and provides a valuable resource for developing more resilient anti-spoofing models.||
|**2024-09-13**|[STA-V2A: Video-to-Audio Generation with Semantic and Temporal Alignment](http://arxiv.org/abs/2409.08601)|null|Visual and auditory perception are two crucial ways humans experience the world. Text-to-video generation has made remarkable progress over the past year, but the absence of harmonious audio in generated video limits its broader applications. In this paper, we propose Semantic and Temporal Aligned Video-to-Audio (STA-V2A), an approach that enhances audio generation from videos by extracting both local temporal and global semantic video features and combining these refined video features with text as cross-modal guidance. To address the issue of information redundancy in videos, we propose an onset prediction pretext task for local temporal feature extraction and an attentive pooling module for global semantic feature extraction. To supplement the insufficient semantic information in videos, we propose a Latent Diffusion Model with Text-to-Audio priors initialization and cross-modal guidance. We also introduce Audio-Audio Align, a new metric to assess audio-temporal alignment. Subjective and objective metrics demonstrate that our method surpasses existing Video-to-Audio models in generating audio with better quality, semantic consistency, and temporal alignment. The ablation experiment validated the effectiveness of each module. Audio samples are available at https://y-ren16.github.io/STAV2A.||
|**2024-09-13**|[LHQ-SVC: Lightweight and High Quality Singing Voice Conversion Modeling](http://arxiv.org/abs/2409.08583)|null|Singing Voice Conversion (SVC) has emerged as a significant subfield of Voice Conversion (VC), enabling the transformation of one singer's voice into another while preserving musical elements such as melody, rhythm, and timbre. Traditional SVC methods have limitations in terms of audio quality, data requirements, and computational complexity. In this paper, we propose LHQ-SVC, a lightweight, CPU-compatible model based on the SVC framework and diffusion model, designed to reduce model size and computational demand without sacrificing performance. We incorporate features to improve inference quality, and optimize for CPU execution by using performance tuning tools and parallel computing frameworks. Our experiments demonstrate that LHQ-SVC maintains competitive performance, with significant improvements in processing speed and efficiency across different devices. The results suggest that LHQ-SVC can meet||
|**2024-09-12**|[DreamHOI: Subject-Driven Generation of 3D Human-Object Interactions with Diffusion Priors](http://arxiv.org/abs/2409.08278)|null|We present DreamHOI, a novel method for zero-shot synthesis of human-object interactions (HOIs), enabling a 3D human model to realistically interact with any given object based on a textual description. This task is complicated by the varying categories and geometries of real-world objects and the scarcity of datasets encompassing diverse HOIs. To circumvent the need for extensive data, we leverage text-to-image diffusion models trained on billions of image-caption pairs. We optimize the articulation of a skinned human mesh using Score Distillation Sampling (SDS) gradients obtained from these models, which predict image-space edits. However, directly backpropagating image-space gradients into complex articulation parameters is ineffective due to the local nature of such gradients. To overcome this, we introduce a dual implicit-explicit representation of a skinned mesh, combining (implicit) neural radiance fields (NeRFs) with (explicit) skeleton-driven mesh articulation. During optimization, we transition between implicit and explicit forms, grounding the NeRF generation while refining the mesh articulation. We validate our approach through extensive experiments, demonstrating its effectiveness in generating realistic HOIs.||
|**2024-09-12**|[Hand-Object Interaction Pretraining from Videos](http://arxiv.org/abs/2409.08273)|null|我们提出了一种从 3D 手-物体交互轨迹中学习通用机器人操作先验的方法。我们构建了一个框架，利用野外视频生成感觉运动机器人轨迹。为此，我们将人手和被操纵物体提升到共享的 3D 空间中，并将人体动作重定向到机器人动作。对这些数据进行生成建模，我们得到了一个与任务无关的基础策略。该策略捕获了一个通用而灵活的操作先验。我们通过经验证明，使用强化学习 (RL) 和行为克隆 (BC) 对该策略进行微调，可以实现对下游任务的样本高效适应，同时与先前的方法相比，提高了鲁棒性和泛化能力。定性实验结果可见：\url{https://hgaurav2k.github.io/hop/}。||
|**2024-09-12**|[Click2Mask: Local Editing with Dynamic Mask Generation](http://arxiv.org/abs/2409.08272)|null|生成模型的最新进展彻底改变了图像生成和编辑领域，使非专业人士也能轻松完成这些任务。本文重点关注局部图像编辑，特别是向大致指定区域添加新内容的任务。现有方法通常需要精确的掩码或对位置的详细描述，这可能既麻烦又容易出错。我们提出了 Click2Mask，这是一种新颖的方法，它只需一个参考点（以及内容描述）即可简化局部编辑过程。在混合潜在扩散 (BLD) 过程中，掩码会围绕该点动态增长，并以基于 CLIP 的语义损失为指导。Click2Mask 超越了基于分割和依赖微调的方法的局限性，提供了一种对用户更友好且上下文更准确的解决方案。我们的实验表明，根据人类判断和自动指标，与 SoTA 方法相比，Click2Mask 不仅最大限度地减少了用户的工作量，而且还提供了具有竞争力或更优的局部图像处理结果。主要贡献包括简化用户输入、能够不受现有分割限制地自由添加对象，以及将我们的动态掩码方法集成到其他编辑方法中的潜力。||
|**2024-09-12**|[DreamBeast: Distilling 3D Fantastical Animals with Part-Aware Knowledge Transfer](http://arxiv.org/abs/2409.08271)|null|We present DreamBeast, a novel method based on score distillation sampling (SDS) for generating fantastical 3D animal assets composed of distinct parts. Existing SDS methods often struggle with this generation task due to a limited understanding of part-level semantics in text-to-image diffusion models. While recent diffusion models, such as Stable Diffusion 3, demonstrate a better part-level understanding, they are prohibitively slow and exhibit other common problems associated with single-view diffusion models. DreamBeast overcomes this limitation through a novel part-aware knowledge transfer mechanism. For each generated asset, we efficiently extract part-level knowledge from the Stable Diffusion 3 model into a 3D Part-Affinity implicit representation. This enables us to instantly generate Part-Affinity maps from arbitrary camera views, which we then use to modulate the guidance of a multi-view diffusion model during SDS to create 3D assets of fantastical animals. DreamBeast significantly enhances the quality of generated 3D creatures with user-specified part compositions while reducing computational overhead, as demonstrated by extensive quantitative and qualitative evaluations.||
|**2024-09-12**|[Touch2Touch: Cross-Modal Tactile Generation for Object Manipulation](http://arxiv.org/abs/2409.08269)|null|现今的触觉传感器形态各异，尺寸不一。由于模型通常与特定的传感器设计绑定，因此开发通用的触觉处理方法变得极具挑战性。我们通过在触觉传感器之间进行跨模态预测来解决这个问题：给定来自一个传感器的触觉信号，我们使用生成模型来估计另一个传感器如何感知相同的物理接触。这允许我们将特定于传感器的算法应用于生成的信号。我们通过训练一个扩散模型来实现这个想法，该模型可以在流行的 GelSlim 和 Soft Bubble 传感器之间进行转换。作为一个下游任务，我们使用 GelSlim 传感器执行手持物体姿态估计，同时使用仅对 Soft Bubble 信号进行操作的算法。数据集、代码和更多详细信息可以在 https://www.mmintlab.com/research/touch2touch/ 上找到。||
|**2024-09-12**|[Improving Text-guided Object Inpainting with Semantic Pre-inpainting](http://arxiv.org/abs/2409.08260)|**[link](https://github.com/nnn-s/catdiffusion)**|Recent years have witnessed the success of large text-to-image diffusion models and their remarkable potential to generate high-quality images. The further pursuit of enhancing the editability of images has sparked significant interest in the downstream task of inpainting a novel object described by a text prompt within a designated region in the image. Nevertheless, the problem is not trivial from two aspects: 1) Solely relying on one single U-Net to align text prompt and visual object across all the denoising timesteps is insufficient to generate desired objects; 2) The controllability of object generation is not guaranteed in the intricate sampling space of diffusion model. In this paper, we propose to decompose the typical single-stage object inpainting into two cascaded processes: 1) semantic pre-inpainting that infers the semantic features of desired objects in a multi-modal feature space; 2) high-fieldity object generation in diffusion latent space that pivots on such inpainted semantic features. To achieve this, we cascade a Transformer-based semantic inpainter and an object inpainting diffusion model, leading to a novel CAscaded Transformer-Diffusion (CAT-Diffusion) framework for text-guided object inpainting. Technically, the semantic inpainter is trained to predict the semantic features of the target object conditioning on unmasked context and text prompt. The outputs of the semantic inpainter then act as the informative visual prompts to guide high-fieldity object generation through a reference adapter layer, leading to controllable object inpainting. Extensive evaluations on OpenImages-V6 and MSCOCO validate the superiority of CAT-Diffusion against the state-of-the-art methods. Code is available at \url{https://github.com/Nnn-s/CATdiffusion}.||
|**2024-09-12**|[Improving Virtual Try-On with Garment-focused Diffusion Models](http://arxiv.org/abs/2409.08258)|null|Diffusion models have led to the revolutionizing of generative modeling in numerous image synthesis tasks. Nevertheless, it is not trivial to directly apply diffusion models for synthesizing an image of a target person wearing a given in-shop garment, i.e., image-based virtual try-on (VTON) task. The difficulty originates from the aspect that the diffusion process should not only produce holistically high-fidelity photorealistic image of the target person, but also locally preserve every appearance and texture detail of the given garment. To address this, we shape a new Diffusion model, namely GarDiff, which triggers the garment-focused diffusion process with amplified guidance of both basic visual appearance and detailed textures (i.e., high-frequency details) derived from the given garment. GarDiff first remoulds a pre-trained latent diffusion model with additional appearance priors derived from the CLIP and VAE encodings of the reference garment. Meanwhile, a novel garment-focused adapter is integrated into the UNet of diffusion model, pursuing local fine-grained alignment with the visual appearance of reference garment and human pose. We specifically design an appearance loss over the synthesized garment to enhance the crucial, high-frequency details. Extensive experiments on VITON-HD and DressCode datasets demonstrate the superiority of our GarDiff when compared to state-of-the-art VTON approaches. Code is publicly available at: \href{https://github.com/siqi0905/GarDiff/tree/master}{https://github.com/siqi0905/GarDiff/tree/master}.||
|**2024-09-12**|[LoRID: Low-Rank Iterative Diffusion for Adversarial Purification](http://arxiv.org/abs/2409.08255)|null|This work presents an information-theoretic examination of diffusion-based purification methods, the state-of-the-art adversarial defenses that utilize diffusion models to remove malicious perturbations in adversarial examples. By theoretically characterizing the inherent purification errors associated with the Markov-based diffusion purifications, we introduce LoRID, a novel Low-Rank Iterative Diffusion purification method designed to remove adversarial perturbation with low intrinsic purification errors. LoRID centers around a multi-stage purification process that leverages multiple rounds of diffusion-denoising loops at the early time-steps of the diffusion models, and the integration of Tucker decomposition, an extension of matrix factorization, to remove adversarial noise at high-noise regimes. Consequently, LoRID increases the effective diffusion time-steps and overcomes strong adversarial attacks, achieving superior robustness performance in CIFAR-10/100, CelebA-HQ, and ImageNet datasets under both white-box and black-box settings.||
|**2024-09-12**|[Dynamic Prompting of Frozen Text-to-Image Diffusion Models for Panoptic Narrative Grounding](http://arxiv.org/abs/2409.08251)|null|Panoptic narrative grounding (PNG), whose core target is fine-grained image-text alignment, requires a panoptic segmentation of referred objects given a narrative caption. Previous discriminative methods achieve only weak or coarse-grained alignment by panoptic segmentation pretraining or CLIP model adaptation. Given the recent progress of text-to-image Diffusion models, several works have shown their capability to achieve fine-grained image-text alignment through cross-attention maps and improved general segmentation performance. However, the direct use of phrase features as static prompts to apply frozen Diffusion models to the PNG task still suffers from a large task gap and insufficient vision-language interaction, yielding inferior performance. Therefore, we propose an Extractive-Injective Phrase Adapter (EIPA) bypass within the Diffusion UNet to dynamically update phrase prompts with image features and inject the multimodal cues back, which leverages the fine-grained image-text alignment capability of Diffusion models more sufficiently. In addition, we also design a Multi-Level Mutual Aggregation (MLMA) module to reciprocally fuse multi-level image and phrase features for segmentation refinement. Extensive experiments on the PNG benchmark show that our method achieves new state-of-the-art performance.||
|**2024-09-12**|[IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation](http://arxiv.org/abs/2409.08240)|null|While Text-to-Image (T2I) diffusion models excel at generating visually appealing images of individual instances, they struggle to accurately position and control the features generation of multiple instances. The Layout-to-Image (L2I) task was introduced to address the positioning challenges by incorporating bounding boxes as spatial control signals, but it still falls short in generating precise instance features. In response, we propose the Instance Feature Generation (IFG) task, which aims to ensure both positional accuracy and feature fidelity in generated instances. To address the IFG task, we introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhances feature depiction by incorporating additional appearance tokens and utilizing an Instance Semantic Map to align instance-level features with spatial locations. The IFAdapter guides the diffusion process as a plug-and-play module, making it adaptable to various community models. For evaluation, we contribute an IFG benchmark and develop a verification pipeline to objectively compare models' abilities to generate instances with accurate positioning and features. Experimental results demonstrate that IFAdapter outperforms other models in both quantitative and qualitative evaluations.||
|**2024-09-10**|[SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse Low-Rank Adaptation](http://arxiv.org/abs/2409.06633)|null|近年来，扩散模型的发展推动了图像和视频生成任务的显著进步，其中像Stable Diffusion系列这样的预训练模型发挥了至关重要的作用。受模型剪枝技术的启发，该技术通过移除不重要的参数来减轻大型预训练模型的负担，我们提出了一种新颖的模型微调方法，可以充分利用这些无效参数，并使预训练模型具备新的任务特定能力。本研究首先调查了预训练扩散模型中参数的重要性，发现按绝对值计算，最小的10%到20%的参数对生成过程没有贡献。基于这一观察，我们提出了一种名为SaRA的方法，该方法重新利用这些暂时无效的参数，相当于优化一个稀疏权重矩阵来学习特定任务的知识。为了减轻过拟合，我们提出了一种基于核范数的低秩稀疏训练方案，以实现高效的微调。此外，我们设计了一种新的渐进式参数调整策略，以充分利用重新训练/微调的参数。最后，我们提出了一种新颖的非结构化反向传播策略，可显著降低微调过程中的内存成本。我们的方法增强了预训练模型在下游应用中的生成能力，并且在保持模型泛化能力方面优于LoRA等传统微调方法。我们通过在SD模型上的微调实验验证了我们的方法，结果表明SaRA取得了显著的改进。SaRA还具有一个实际优势，即只需修改一行代码即可实现高效实施，并且与现有方法无缝兼容。||
|**2024-09-10**|[MVGaussian: High-Fidelity text-to-3D Content Generation with Multi-View Guidance and Surface Densification](http://arxiv.org/abs/2409.06620)|null|文本到3D内容生成领域在生成逼真的3D对象方面取得了重大进展，像分数蒸馏采样（SDS）这样的现有方法提供了有希望的指导。然而，由于指导不精确，这些方法经常遇到“两面神”问题——多面歧义。此外，虽然最近3D高斯分裂的进步已经显示出其在表示3D体积方面的功效，但这种表示的优化在很大程度上仍未得到探索。本文介绍了一个用于文本到3D内容生成的统一框架，以解决这些关键差距。我们的方法利用多视图指导迭代形成3D模型的结构，逐步增强细节和准确性。我们还引入了一种新的密集化算法，使高斯接近表面，优化生成模型的结构完整性和保真度。大量实验验证了我们的方法，表明它能够以最少的时间成本生成高质量的视觉输出。值得注意的是，我们的方法在半小时的训练时间内就能获得高质量的结果，与大多数需要数小时训练时间才能获得类似结果的现有方法相比，效率显著提高。||
|**2024-09-10**|[A Primer on Variational Inference for Physics-Informed Deep Generative Modelling](http://arxiv.org/abs/2409.06560)|null|变分推断（VI）是一种计算高效且可扩展的近似贝叶斯推断方法。它在不确定性量化的准确性和实际可处理性之间取得了平衡。由于其内置的贝叶斯正则化和灵活性，它在生成建模和反演任务中表现出色，这对于物理相关问题至关重要。推导 VI 的核心学习目标通常必须针对新的学习任务进行调整，其中问题的性质决定了感兴趣变量之间的条件依赖性，例如物理问题中出现的情况。在本文中，我们为正向和反向问题提供了 VI 的易于理解且全面的技术介绍，引导读者了解 VI 框架的标准推导及其如何通过深度学习得到最佳实现。然后，我们回顾并统一了最近的文献，这些文献例证了 VI 所允许的创造性灵活性。本文面向希望解决基于物理的问题并强调不确定性量化的一般科学受众。||
|**2024-09-10**|[From LIMA to DeepLIMA: following a new path of interoperability](http://arxiv.org/abs/2409.06550)|null|本文描述了 LIMA（Libre Multilingual Analyzer）框架的体系结构及其最新发展，其中新增了基于深度神经网络的文本分析模块。我们在保留现有可配置架构以及先前开发的基于规则和统计的分析组件的可用性的同时，扩展了 LIMA 在支持语言数量方面的功能。我们在 Universal Dependencies 2.5 语料库、WikiNer 语料库和 CoNLL-03 数据集上针对 60 多种语言训练了模型。Universal Dependencies 允许我们增加支持的语言数量，并生成可以集成到其他平台的模型。这种普遍存在的深度学习自然语言处理模型的集成以及使用 Universal Dependencies 的标准注释集合的使用可以被视为一种新的互操作性途径，通过模型和数据的规范化，与更标准的技术互操作性相辅相成，在 LIMA 中通过 Docker Hub 上 Docker 容器中可用的服务实现。||
|**2024-09-10**|[Enhancing Emotional Text-to-Speech Controllability with Natural Language Guidance through Contrastive Learning and Diffusion Models](http://arxiv.org/abs/2409.06451)|null|虽然当前的情感文本到语音（TTS）系统可以生成高度智能的情感语音，但在输出语音的情感渲染方面实现精细控制仍然是一项重大挑战。在本文中，我们介绍了 ParaEVITS，这是一种新颖的情感 TTS 框架，它利用自然语言的组合性来增强对情感渲染的控制。通过结合受 ParaCLAP（一种用于计算语用学的对比性语言-音频预训练（CLAP）模型）启发的文本-音频编码器，我们训练扩散模型以根据文本情感风格描述生成情感嵌入。我们的框架首先使用音频编码器在参考音频上进行训练，然后微调扩散模型以处理来自 ParaCLAP 文本编码器的文本输入。在推理过程中，仅使用文本条件就可以操纵音调、抖动和响度等语音属性。我们的实验表明，ParaEVITS 可以有效地控制情感渲染，而不会影响语音质量。语音演示公开可用。||
|**2024-09-10**|[Prompt2Fashion: An automatically generated fashion dataset](http://arxiv.org/abs/2409.06442)|**[link](https://github.com/georgiarg/prompt2fashion)**|尽管语言和视觉生成模型在快速发展且效率不断提高，但仍然缺乏将个性化时尚需求与人工智能驱动设计联系起来的综合数据集，这限制了真正包容和定制化时尚解决方案的潜力。在这项工作中，我们利用生成模型自动构建了一个时尚图像数据集，该数据集根据用户的指示针对不同的场合、风格和体型量身定制。我们使用不同的生成式预训练模型（LLM）和提示策略，为专家和非专家用户提供具有高质量审美、细节和相关性的个性化服装，并通过定性分析证明了这一点。到目前为止，生成的服装的评估一直由非专家的人类受试者进行。尽管对生成的质量和相关性提供了细致入微的见解，但我们就专家知识对于评估此类艺术性人工智能生成数据集的重要性展开了进一步的讨论。我们的数据集可在 GitHub 上公开获取，网址为 https://github.com/georgiarg/Prompt2Fashion。||
|**2024-09-10**|[Fast nonparametric inference of network backbones for graph sparsification](http://arxiv.org/abs/2409.06417)|**[link](https://github.com/aleckirkley/mdl-network-backbones)**|网络骨干通过仅保留最重要的链接来提供加权网络的有用稀疏表示，从而实现一系列计算加速并简化复杂的网络可视化。判断链接是否重要的标准有很多，因此已经开发了许多用于图稀疏化网络骨干提取的方法。这些方法根据它们是在整个网络还是在单个节点邻域的上下文中评估边的重要性，可以分为全局或局部方法。现有网络骨干提取方法的一个关键限制是，它们要么人为地将骨干的拓扑结构限制为特定形式（例如树），要么需要指定一个自由参数（例如显著性水平）来确定骨干中要保留的边数。在这里，我们开发了一个完全非参数的框架来推断加权网络的骨干，该框架通过使用信息论中的最小描述长度（MDL）原则自动选择保留在骨干中的最佳边数来克服这些限制。我们开发了两种编码方案，作为全局和局部网络骨干的目标函数，以及有效的优化算法，以根据这些目标识别最佳骨干，其运行时复杂度在边数上是对数线性的。我们表明，所提出的框架可以使用最大后验（MAP）估计程序和渐近等效的贝叶斯骨干生成模型推广到边上的任何离散权重分布。我们在真实和合成网络上的一系列任务中将所提出的方法与现有方法进行了比较。||
|**2024-09-10**|[Distilling Generative-Discriminative Representations for Very Low-Resolution Face Recognition](http://arxiv.org/abs/2409.06371)|null|由于分辨率下降会导致信息丰富的面部细节严重丢失，因此极低分辨率人脸识别极具挑战性。在本文中，我们提出了一种结合了生成表示和跨分辨率对齐知识蒸馏的生成-判别表示蒸馏方法。这种方法通过两个蒸馏模块联合蒸馏生成模型和判别模型，促进了极低分辨率人脸识别。首先，生成表示蒸馏将预先训练用于人脸超分辨率的扩散模型的编码器作为生成教师，通过特征回归来监督学生骨干网络的学习，然后冻结学生骨干网络。之后，判别表示蒸馏进一步考虑将预先训练好的人脸识别器作为判别教师，通过跨分辨率关系对比蒸馏来监督学生头部的学习。通过这种方式，可以将通用的骨干网络表示转换为判别头部表示，从而形成一个鲁棒的、具有判别力的学生模型，用于极低分辨率人脸识别。我们的方法改进了极低分辨率人脸中缺失细节的恢复，并实现了更好的知识迁移。在人脸数据集上的大量实验表明，我们的方法提高了极低分辨率人脸的识别精度，展示了其有效性和适应性。||
|**2024-09-10**|[What happens to diffusion model likelihood when your model is conditional?](http://arxiv.org/abs/2409.06364)|null|Diffusion Models (DMs) iteratively denoise random samples to produce high-quality data. The iterative sampling process is derived from Stochastic Differential Equations (SDEs), allowing a speed-quality trade-off chosen at inference. Another advantage of sampling with differential equations is exact likelihood computation. These likelihoods have been used to rank unconditional DMs and for out-of-domain classification. Despite the many existing and possible uses of DM likelihoods, the distinct properties captured are unknown, especially in conditional contexts such as Text-To-Image (TTI) or Text-To-Speech synthesis (TTS). Surprisingly, we find that TTS DM likelihoods are agnostic to the text input. TTI likelihood is more expressive but cannot discern confounding prompts. Our results show that applying DMs to conditional tasks reveals inconsistencies and strengthens claims that the properties of DM likelihood are unknown. This impact sheds light on the previously unknown nature of DM likelihoods. Although conditional DMs maximise likelihood, the likelihood in question is not as sensitive to the conditioning input as one expects. This investigation provides a new point-of-view on diffusion likelihoods.||
|**2024-09-10**|[DiffQRCoder: Diffusion-based Aesthetic QR Code Generation with Scanning Robustness Guided Iterative Refinement](http://arxiv.org/abs/2409.06355)|null|With the success of Diffusion Models for image generation, the technologies also have revolutionized the aesthetic Quick Response (QR) code generation. Despite significant improvements in visual attractiveness for the beautified codes, their scannabilities are usually sacrificed and thus hinder their practical uses in real-world scenarios. To address this issue, we propose a novel Diffusion-based QR Code generator (DiffQRCoder) to effectively craft both scannable and visually pleasing QR codes. The proposed approach introduces Scanning-Robust Perceptual Guidance (SRPG), a new diffusion guidance for Diffusion Models to guarantee the generated aesthetic codes to obey the ground-truth QR codes while maintaining their attractiveness during the denoising process. Additionally, we present another post-processing technique, Scanning Robust Manifold Projected Gradient Descent (SR-MPGD), to further enhance their scanning robustness through iterative latent space optimization. With extensive experiments, the results demonstrate that our approach not only outperforms other compared methods in Scanning Success Rate (SSR) with better or comparable CLIP aesthetic score (CLIP-aes.) but also significantly improves the SSR of the ControlNet-only approach from 60% to 99%. The subjective evaluation indicates that our approach achieves promising visual attractiveness to users as well. Finally, even with different scanning angles and the most rigorous error tolerance settings, our approach robustly achieves over 95% SSR, demonstrating its capability for real-world applications.||
|**2024-09-09**|[Enhancing Preference-based Linear Bandits via Human Response Time](http://arxiv.org/abs/2409.05798)|null|Binary human choice feedback is widely used in interactive preference learning for its simplicity, but it provides limited information about preference strength. To overcome this limitation, we leverage human response times, which inversely correlate with preference strength, as complementary information. Our work integrates the EZ-diffusion model, which jointly models human choices and response times, into preference-based linear bandits. We introduce a computationally efficient utility estimator that reformulates the utility estimation problem using both choices and response times as a linear regression problem. Theoretical and empirical comparisons with traditional choice-only estimators reveal that for queries with strong preferences ("easy" queries), choices alone provide limited information, while response times offer valuable complementary information about preference strength. As a result, incorporating response times makes easy queries more useful. We demonstrate this advantage in the fixed-budget best-arm identification problem, with simulations based on three real-world datasets, consistently showing accelerated learning when response times are incorporated.||
|**2024-09-09**|[Predicting Critical Heat Flux with Uncertainty Quantification and Domain Generalization Using Conditional Variational Autoencoders and Deep Neural Networks](http://arxiv.org/abs/2409.05790)|null|Deep generative models (DGMs) have proven to be powerful in generating realistic data samples. Their capability to learn the underlying distribution of a dataset enable them to generate synthetic data samples that closely resemble the original training dataset, thus addressing the challenge of data scarcity. In this work, we investigated the capabilities of DGMs by developing a conditional variational autoencoder (CVAE) model to augment the critical heat flux (CHF) measurement data that was used to generate the 2006 Groeneveld lookup table. To determine how this approach compared to traditional methods, a fine-tuned deep neural network (DNN) regression model was created and evaluated with the same dataset. Both the CVAE and DNN models achieved small mean absolute relative errors, with the CVAE model maintaining more favorable results. To quantify the uncertainty in the model's predictions, uncertainty quantification (UQ) was performed with repeated sampling of the CVAE model and ensembling of the DNN model. Following UQ, the DNN ensemble notably improved performance when compared to the baseline DNN model, while the CVAE model achieved similar results to its non-UQ results. The CVAE model was shown to have significantly less variability and a higher confidence after assessment of the prediction-wise relative standard deviations. Evaluating domain generalization, both models achieved small mean error values when predicting both inside and outside the training domain, with predictions outside the training domain showing slightly larger errors. Overall, the CVAE model was comparable to the DNN regression model in predicting CHF values but with better uncertainty behavior.||
|**2024-09-09**|[Vector Quantized Diffusion Model Based Speech Bandwidth Extension](http://arxiv.org/abs/2409.05784)|null|神经音频编解码器 (NAC) 的最新进展为音频信号处理解锁了新的潜力。越来越多的研究探索利用 NAC 的潜在特征来完成各种语音信号处理任务。本文介绍了第一种利用从 NAC 获得的离散特征进行语音带宽扩展 (BWE) 的方法。通过恢复高度压缩的离散标记中的高频细节，该方法增强了语音的清晰度和自然度。所提出的框架基于矢量量化扩散，结合了先进 NAC、扩散模型和 Mamba-2 的优势，以重建高频语音成分。大量实验表明，该方法在对数谱距离和 ViSQOL 方面均表现出优异的性能，显着提高了语音质量。||
|**2024-09-09**|[AS-Speech: Adaptive Style For Speech Synthesis](http://arxiv.org/abs/2409.05730)|null|近年来，文本到语音（TTS）合成技术取得了显著进展，能够在常见场景下合成高质量的语音。在未知情况下，自适应TTS需要强大的泛化能力来适应说话人的风格特征。然而，现有的自适应方法只能分别提取和整合粗粒度的音色或混合的韵律属性。在本文中，我们提出了AS-Speech，一种将说话人音色特征和韵律属性整合到一个统一框架中的自适应风格方法，用于文本到语音合成。具体来说，AS-Speech可以通过细粒度的基于文本的音色特征和全局韵律信息准确地模拟风格特征，并通过扩散模型实现高保真语音合成。实验表明，与一系列自适应TTS模型相比，该模型生成的语音在音色和韵律方面具有更高的自然度和相似性。||
|**2024-09-09**|[pFedGPA: Diffusion-based Generative Parameter Aggregation for Personalized Federated Learning](http://arxiv.org/abs/2409.05701)|null|联邦学习 (FL) 是一种去中心化的模型训练方法，数据保留在本地，只有模型参数在客户端和中心服务器之间共享。传统的联邦平均 (FedAvg) 等方法对这些通常在异构数据分布上训练的参数进行线性聚合，这可能忽略了参数空间复杂、高维的性质，导致聚合模型的性能下降。虽然个性化联邦学习方法可以在一定程度上缓解异构数据问题，但线性聚合的局限性仍然没有解决。为了缓解这个问题，我们研究了扩散模型的生成方法，并提出了一种新的个性化联邦学习生成参数聚合框架，即 pFedGPA。在这个框架中，我们在服务器上部署了一个扩散模型，以整合不同的参数分布，并提出了一种参数反演方法，为每个客户端有效地生成一组个性化参数。这种反演方法将上传的参数转换为一个潜在代码，然后通过去噪采样进行聚合，生成最终的个性化参数。通过使用高容量扩散模型对客户端模型参数对其特定数据分布的依赖性进行编码，pFedGPA 可以有效地将所有客户端模型参数的总体分布的复杂性与每个客户端参数分布的复杂性解耦。我们的实验结果一致地证明了所提出的方法在多个数据集上的优越性能，超过了基线方法。||
|**2024-09-09**|[Unlearning or Concealment? A Critical Analysis and Evaluation Metrics for Unlearning in Diffusion Models](http://arxiv.org/abs/2409.05668)|null|近期的研究已经看到人们对扩散模型中概念去除和目标遗忘方法的浓厚兴趣。在本文中，我们对现有的扩散模型遗忘方法进行了全面的白盒分析，以揭示其存在的重大漏洞。我们发现，现有方法中用于遗忘的目标函数导致了要遗忘的目标概念与相应提示之间的解耦。这是一种隐蔽行为，而不是真正的遗忘，而真正的遗忘才是最初的目标。当前方法的无效性主要源于它们只关注降低特定提示集的生成概率，而忽略了推理过程中使用的中间引导的多种形式。本文对四种常用的扩散模型遗忘技术进行了严格的理论和实证检验。我们引入了两个新的评估指标：概念检索分数（CRS）和概念置信度分数（CCS）。这些指标基于一个成功的对抗攻击设置，可以从遗忘的扩散模型中恢复被遗忘的概念。CRS 衡量的是遗忘后的遗忘模型和完全训练模型的潜在表示之间的相似性。它反映了随着引导量增加，被遗忘概念的检索程度。CCS 量化了模型将目标概念分配给被操纵数据的置信度。它反映了随着引导量增加，未遗忘模型的生成结果与原始领域知识一致的概率。我们使用提出的针对扩散模型的严格指标对现有的遗忘方法进行评估，结果揭示了它们在真正遗忘概念方面的重大缺陷。源代码：https://respailab.github.io/unlearning-or-concealment||
|**2024-09-09**|[Forward KL Regularized Preference Optimization for Aligning Diffusion Policies](http://arxiv.org/abs/2409.05622)|null|扩散模型通过在策略学习中利用高度表达的模型能力，在序列决策中取得了显著的成功。学习扩散策略的一个核心问题是如何在各种任务中使策略输出与人类意图保持一致。为了实现这一点，先前的方法进行了回报条件策略生成或基于强化学习（RL）的策略优化，但它们都依赖于预先定义的奖励函数。在这项工作中，我们提出了一种新的框架，即用于对齐扩散策略的前向 KL 正则化偏好优化，以直接将扩散策略与偏好对齐。我们首先从离线数据集中训练一个不考虑偏好的扩散策略，然后通过直接偏好优化将该策略与偏好数据对齐。在对齐阶段，我们在扩散策略中制定了直接偏好学习，其中在前向偏好优化中采用了 KL 正则化，以避免生成分布外动作。我们对 MetaWorld 操作和 D4RL 任务进行了广泛的实验。结果表明，我们的方法在偏好一致性方面表现出色，并且优于先前最先进的算法。||
|**2024-09-09**|[Latent 3D Brain MRI Counterfactual](http://arxiv.org/abs/2409.05585)|null|结构性脑部MRI研究中的样本数量通常过小，无法充分训练深度学习模型。生成模型通过有效学习数据分布和生成高保真MRI，为解决这一问题带来了希望。然而，它们难以生成训练数据分布之外的多样化、高质量数据。解决这一问题的一种方法是使用针对3D体积反事实开发的因果模型。然而，在高维空间中准确建模因果关系是一项挑战，因此这些模型通常生成质量较低的3D脑部MRI。为了应对这些挑战，我们提出了一种两阶段方法，在潜在空间内构建结构因果模型（SCM）。在第一阶段，我们采用VQ-VAE学习MRI体积的紧凑嵌入。随后，我们将因果模型整合到这个潜在空间中，并使用封闭形式的广义线性模型（GLM）执行三步反事实程序。我们对真实世界的高分辨率MRI数据（1mm）进行的实验表明，我们的方法可以生成高质量的3D MRI反事实。||
|**2024-09-09**|[Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation](http://arxiv.org/abs/2409.05583)|**[link](https://github.com/gmuraleekrishna/sas)**|具身人工智能旨在开发能够理解和执行人类语言指令并以自然语言进行交流的机器人。为此，我们研究了生成高度详细的导航指令以供具身机器人遵循的任务。尽管最近的研究表明，从图像序列生成逐步指令方面取得了重大进展，但生成的指令在指称物体和地标方面缺乏多样性。现有的说话者模型学习了一些策略来规避评估指标，即使对于低质量的句子也能获得更高的分数。在这项工作中，我们提出了SAS（空间感知说话者），这是一种指令生成器或“说话者”模型，它利用环境的结构和语义知识来生成更丰富的指令。为了进行训练，我们在对抗性设置中采用了奖励学习方法，以避免语言评估指标引入的系统性偏差。根据经验，我们的方法优于现有的指令生成模型，并使用标准指标进行了评估。我们的代码可在以下网址获得：https://github.com/gmuraleekrishna/SAS。||
|**2024-09-09**|[A Taxonomy of Miscompressions: Preparing Image Forensics for Neural Compression](http://arxiv.org/abs/2409.05490)|null|神经压缩有可能彻底改变有损图像压缩技术。基于生成模型，最近的方案在高感知质量下实现了前所未有的压缩率，但牺牲了语义保真度。解压缩图像的细节可能看起来在视觉上是完美的，但在语义上与原始图像不同，这使得压缩错误难以或不可能被检测到。我们探索了这个问题的空间，并提出了一个暂定的错误压缩分类法。它定义了三种类型的“发生了什么”，并有一个二进制的“高影响”标志，表示改变符号的错误压缩。我们讨论了该分类法如何促进风险沟通和缓解措施的研究。||
|**2024-09-05**|[Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding](http://arxiv.org/abs/2409.03757)|**[link](https://github.com/yunzeman/lexicon3d)**|复杂三维场景理解近年来备受关注，场景编码策略在其中发挥着至关重要的作用。然而，针对不同场景的最佳场景编码策略仍不明确，特别是与基于图像的编码策略相比。为了解决这个问题，我们对用于三维场景理解的各种视觉编码模型进行了全面研究，确定了每个模型在不同场景下的优势和局限性。我们的评估涵盖了七种视觉基础编码器，包括基于图像、基于视频和三维基础模型。我们在四个任务中评估这些模型：视觉语言场景推理、视觉定位、分割和配准，每个任务都侧重于场景理解的不同方面。我们的评估得出了以下主要发现：DINOv2 表现出优越的性能，视频模型在对象级任务中表现出色，扩散模型有利于几何任务，而语言预训练模型在语言相关任务中表现出意想不到的局限性。这些见解挑战了一些传统认知，为利用视觉基础模型提供了新的视角，并强调了在未来的视觉语言和场景理解任务中需要更灵活的编码器选择。||
|**2024-09-05**|[ArtiFade: Learning to Generate High-quality Subject from Blemished Images](http://arxiv.org/abs/2409.03745)|null|以主题为主导的文本到图像生成技术在学习和捕捉主题特征方面取得了显著进步，即使只使用有限数量的图像。然而，现有方法通常依赖于高质量的图像进行训练，当输入图像存在瑕疵时，可能难以生成合理的图像。这主要归因于当前技术在区分主题相关特征和干扰性瑕疵方面的能力不足。在本文中，我们引入了ArtiFade来解决这个问题，并成功地从有瑕疵的数据集中生成了高质量的无瑕疵图像。具体来说，ArtiFade利用预先训练的文本到图像模型的微调来消除瑕疵。通过在微调过程中使用包含无瑕疵图像及其对应的有瑕疵图像的专门数据集来实现瑕疵的消除。ArtiFade还确保了保留扩散模型中固有的原始生成能力，从而提高了主题驱动方法在生成高质量和无瑕疵图像方面的整体性能。我们进一步为这项任务设计了评估基准。通过广泛的定性和定量实验，我们证明了ArtiFade在分布内和分布外情况下都能有效去除瑕疵的泛化能力。||
|**2024-09-05**|[RealisHuman: A Two-Stage Approach for Refining Malformed Human Parts in Generated Images](http://arxiv.org/abs/2409.03644)|null|近年来，扩散模型彻底改变了视觉生成领域，其性能超越了生成对抗网络 (GANs) 等传统框架。然而，由于人类及其语义部分（如手和脸）复杂的结构，生成具有真实感的人类图像仍然是一项重大挑战。为了解决这个问题，我们提出了一种名为 RealisHuman 的新型后处理解决方案。RealisHuman 框架分两个阶段运行。首先，它使用原始的畸形部分作为参考，生成逼真的人体部位（如手或脸），确保细节与原始图像一致。其次，它通过重新绘制周围区域将校正后的人体部位无缝地融入到其对应的位置，以确保平滑逼真的融合。RealisHuman 框架显著增强了人类生成的真实感，这可以通过定性和定量指标的显著改进得到证明。代码可在 https://github.com/Wangbenzhi/RealisHuman 获取。||
|**2024-09-05**|[DiffEVC: Any-to-Any Emotion Voice Conversion with Expressive Guidance](http://arxiv.org/abs/2409.03636)|null|情感语音转换 (EVC) 通过放大积极线索和减少消极线索来改变语音情感，从而增强沟通。这项复杂的任务涉及语音质量、说话者特征和内容等纠缠不清的因素。传统的深度学习模型（如 GAN 和自动编码器）通过学习映射或解耦特征在 EVC 中取得了一定的成功，但面临着不稳定性和语音质量下降等挑战。扩散模型提供了稳定的训练和高质量的生成。我们提出了一个基于扩散的 EVC 框架，该框架使用互信息损失和辅助模型来解耦情感和说话者身份。引入了一种表达性引导机制，以改善情感转换，同时保持说话者特征。实验结果表明，我们的方法对于未知说话者和情感的有效性，在 EVC 任务中实现了最先进的性能。||
|**2024-09-05**|[TCDiff: Triple Condition Diffusion Model with 3D Constraints for Stylizing Synthetic Faces](http://arxiv.org/abs/2409.03600)|**[link](https://github.com/bovifocr/tcdiff)**|一个鲁棒的人脸识别模型需要使用包含大量个体以及每个个体在不同条件（例如姿态、表情、年龄、噪声和遮挡）下的大量样本的数据集进行训练。由于伦理和隐私问题，大型真实人脸数据集（例如 MS1MV3）已被停用，并且已经提出了利用 GAN 和扩散模型的合成人脸生成器，例如 SYNFace、SFace、DigiFace-1M、IDiff-Face、DCFace 和 GANDiffFace，旨在满足这一需求。其中一些方法可以生成高保真度的真实人脸，但类内差异较低，而另一些方法则生成具有高差异性但身份一致性较低的人脸。在本文中，我们提出了一种三重条件扩散模型（TCDiff），通过 2D 和 3D 人脸约束来改进从真实人脸到合成人脸的人脸风格迁移，在保持必要的类内高差异性的同时增强人脸身份一致性。使用我们新的数据集的 1k、2k 和 5k 类进行训练的人脸识别实验在 LFW、CFP-FP、AgeDB 和 BUPT 等真实人脸基准测试中优于最先进的合成数据集。我们的源代码可在以下网址获得：https://github.com/BOVIFOCR/tcdiff。||
|**2024-09-05**|[DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any Architecture](http://arxiv.org/abs/2409.03550)|null|扩散模型 (DM) 在各个领域都表现出卓越的生成能力，但其部署过程中的推理速度慢和计算需求高却阻碍了其发展。加速DM最常用的方法是减少生成过程中的去噪步骤，这可以通过更快的采样求解器或知识蒸馏 (KD) 来实现。与先前的方法不同，我们提出了一种新方法，可以将大型预训练DM的功能迁移到更快的架构中。具体来说，我们以独特的方式使用KD，通过将生成能力提炼到更快的变体中来压缩DM。此外，考虑到源数据不可访问或对于当前的生成模型来说存储量太大，我们引入了一种新的无源数据蒸馏范式，称为扩散模型的无数据知识蒸馏 (DKDM)。通常，我们建立的DKDM框架包含两个主要组件：1) DKDM目标函数，它使用预训练DM生成的合成去噪数据来优化更快的DM，而无需源数据；2) 动态迭代蒸馏方法，它可以灵活地组织去噪数据的合成，防止由于生成速度慢而减慢优化过程。据我们所知，这是首次尝试使用KD以无数据的方式将DM提炼到任何架构中。重要的是，我们的DKDM与大多数现有的加速方法（例如减少去噪步骤、量化和剪枝）是正交的。实验表明，我们的DKDM能够推导出速度提高2倍的DM，其性能与基线保持一致。值得注意的是，我们的DKDM使预训练的DM能够作为“数据集”来训练新的DM。||
|**2024-09-05**|[Blended Latent Diffusion under Attention Control for Real-World Video Editing](http://arxiv.org/abs/2409.03514)|null|由于缺乏完全公开可用的文本到视频模型，目前的视频编辑方法倾向于建立在预训练的文本到图像生成模型之上，然而，在处理具有时间信息的视频局部编辑方面，它们仍然面临着巨大的挑战。首先，尽管现有方法试图通过预先定义的掩码来关注局部区域编辑，但由于每一帧的空间整体生成，外部区域背景的保留并不理想。此外，由用户专门提供掩码是一项额外的昂贵工作，因此需要一种集成到编辑过程中的自主掩码策略。最后但同样重要的是，图像级预训练模型没有学习视频帧之间的时间信息，而这对于表达运动和动态至关重要。在本文中，我们建议采用图像级混合潜在扩散模型来执行局部视频编辑任务。具体来说，我们利用 DDIM 反演来获取潜在向量作为背景潜在向量，而不是随机噪声的潜在向量，以更好地保留输入视频的背景信息。我们进一步介绍了一种从扩散步骤中的交叉注意图衍生的自主掩码制造机制。最后，我们通过将 U-Net 的自注意力块转换为时空块来增强视频帧之间的时间一致性。通过大量的实验，我们提出的方法在不同的现实世界视频编辑任务中表现出有效性。||
|**2024-09-05**|[Data-free Distillation with Degradation-prompt Diffusion for Multi-weather Image Restoration](http://arxiv.org/abs/2409.03455)|null|多天气图像复原取得了令人瞩目的进展，但模型容量的增加和昂贵的数据获取限制了其在内存有限设备上的应用。无数据蒸馏提供了一种替代方案，允许从预训练的教师模型中学习轻量级学生模型，而无需依赖原始训练数据。现有的无数据学习方法主要利用GAN生成的伪数据或从互联网收集的真实数据来优化模型。然而，它们不可避免地会遇到训练不稳定或与原始数据存在域偏移的问题。在本文中，我们提出了一种新的基于退化提示扩散的无数据蒸馏多天气图像复原框架（D4IR）。它用预训练的扩散模型代替GAN以避免模型崩溃，并结合了退化感知提示适配器，以促进内容驱动的条件扩散，从而生成与域相关的图像。具体来说，首先设计了一种基于对比的退化提示适配器，用于从网络收集的退化图像中捕获退化感知提示。然后，将收集到的未配对的干净图像扰动到稳定扩散的潜在特征中，并以退化感知提示为条件，合成新的域相关退化图像，用于知识蒸馏。实验表明，我们的方法取得了与使用原始训练数据蒸馏的模型相当的性能，甚至优于其他主流的无监督方法。||
|**2024-09-05**|[Convergence Rates for the Maximum A Posteriori Estimator in PDE-Regression Models with Random Design](http://arxiv.org/abs/2409.03417)|null|我们考虑从高斯回归问题 $Y = \mathscr{G}(\theta)(Z)+\varepsilon$产生的数据中恢复参数$\theta\in H^\alpha$的统计逆问题，其中$\mathscr{G}:\mathbb{L}^2\to\mathbb{L}^2$是非线性正向映射，$Z$是随机设计点，$\varepsilon$是高斯噪声。估计策略基于$\Vert\cdot\Vert_{H^\alpha}$-约束下的最小二乘法。我们在正向映射$\mathscr{G}$满足Lipschitz类型假设的情况下，建立了最小二乘估计量$\hat{\theta}$作为给定泛函的最大值的存在性。证明了一个一般的浓度结果，并用它来证明预测误差的一致性和上界。相应的收敛速度不仅反映了目标参数的平滑性，还反映了潜在逆问题的适定性。我们将一般模型应用于达西问题，其中PDE的未知系数函数$f$ 的恢复是令人感兴趣的。对于这个例子，我们还提供了预测误差和估计误差的相应收敛速度。此外，我们还简要讨论了该一般模型对其他问题的适用性。||
|**2024-09-05**|[RoVi-Aug: Robot and Viewpoint Augmentation for Cross-Embodiment Robot Learning](http://arxiv.org/abs/2409.03403)|null|扩大机器人学习规模需要庞大而多样化的数据集，如何有效地重复使用收集到的数据并将策略迁移到新的机器人平台仍然是一个悬而未决的问题。诸如Open-X Embodiment (OXE) 项目等新兴研究已经表明，通过组合包含不同机器人的数据集来利用技能是有希望的。然而，许多数据集中机器人类型和相机角度分布的不平衡使得策略容易过拟合。为了缓解这个问题，我们提出了RoVi-Aug，它利用最先进的图像到图像生成模型，通过合成具有不同机器人和相机视角的演示来增强机器人数据。通过广泛的物理实验，我们证明了通过在机器人和视点增强数据上进行训练，RoVi-Aug 可以在具有显著不同相机角度的未知机器人上进行零样本部署。与 Mirage 等测试时自适应算法相比，RoVi-Aug 在测试时不需要额外的处理，不假设已知相机角度，并且允许策略微调。此外，通过在原始机器人数据集和增强机器人数据集上进行联合训练，RoVi-Aug 可以学习多机器人和多任务策略，从而实现机器人和技能之间更有效的迁移，并将成功率提高高达 30%。||
|**2024-09-04**|[HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts](http://arxiv.org/abs/2409.02919)|**[link](https://github.com/Liuxinyv/HiPrompt)**|利用预训练扩散模型生成更高分辨率图像的潜力巨大，但这些模型在处理物体重复和结构伪影方面常常遇到困难，尤其是在扩展到 4K 及更高分辨率时。我们发现问题在于，单个提示生成多个尺度的方式效率低下。为此，我们提出了 HiPrompt，这是一种无须微调的新解决方案，它通过引入分层提示来解决上述问题。分层提示提供全局和局部指导。具体来说，全局指导来自描述整体内容的用户输入，而局部指导则利用来自 MLLM 的逐块描述来精心指导局部结构和纹理的生成。此外，在逆向去噪过程中，生成的噪声被分解为低频和高频空间分量。这些分量以多个提示级别为条件，包括详细的逐块描述和更广泛的图像级提示，从而促进在分层语义指导下的提示引导去噪。它进一步允许生成过程更多地关注局部空间区域，并确保生成的图像在高清晰度下保持一致的局部和全局语义、结构和纹理。大量实验表明，HiPrompt 在高分辨率图像生成方面优于现有技术，显著减少了物体重复并提高了结构质量。||
|**2024-09-04**|[Latent Watermarking of Audio Generative Models](http://arxiv.org/abs/2409.02915)|null|音频生成模型的进步给其负责任的披露和滥用检测带来了新的挑战。为了应对这些挑战，我们介绍了一种通过对其训练数据进行特定水印来标记潜在生成模型的方法。由此产生的水印模型生成的潜在表示，其解码输出可以被高置信度地检测到，而无论使用何种解码方法。这种方法无需进行事后水印步骤即可检测生成的内容。它为开源模型提供了更安全的解决方案，并有助于识别那些在未遵守许可条款的情况下对这些模型进行微调或使用的衍生作品。例如，我们的结果表明，即使在对潜在生成模型进行微调后，生成输出的检测精度也能在假阳性率为 $10^{-3}$ 的情况下达到 75% 以上。||
|**2024-09-04**|[Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling](http://arxiv.org/abs/2409.02908)|null|掩码扩散模型 (MDM) 由于其相较于其他离散扩散模型的优越性能，已成为离散数据生成建模的热门研究课题，并在语言建模任务中与自回归模型 (ARM) 展开竞争。最近简化掩码扩散框架的努力进一步使其与连续空间扩散模型保持一致，并获得了更有原则的训练和采样方法。然而，在本文中，我们揭示了 MDM 的训练和采样在理论上都可以摆脱时间变量（可以说是扩散模型的关键特征），并且等效于掩码模型。我们在采样方面的联系是通过我们提出的首次命中采样器 (FHS) 建立的。具体来说，我们证明了 FHS 在理论上等效于 MDM 的原始生成过程，同时显著减少了耗时的分类采样，并实现了 20 倍的加速。此外，我们的研究对先前关于 MDM 在生成困惑度方面可以超越 ARM 的说法提出了质疑。我们首次发现了一个潜在的数值问题，即使使用 32 位浮点精度，也会导致不准确的分类采样。我们表明，该数值问题在理论上和经验上都降低了有效温度，导致先前文献中对 MDM 生成结果的评估不公平。||
|**2024-09-04**|[Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video Diffusion Models](http://arxiv.org/abs/2409.02851)|**[link](https://github.com/Human-VDM/Human-VDM)**|从单张RGB图像生成逼真3D人体是计算机视觉中一项具有挑战性的任务，因为它需要精确的几何建模、高质量的纹理和合理的不可见部分生成。现有方法通常使用多视角扩散模型进行3D人体生成，但它们经常面临视角不一致的问题，这阻碍了高质量3D人体的生成。为了解决这个问题，我们提出了Human-VDM，一种使用视频扩散模型从单张RGB图像生成3D人体的新方法。Human-VDM使用高斯渲染为3D人体生成提供了时间上一致的视图。它由三个模块组成：视图一致的人体视频扩散模块、视频增强模块和高斯渲染模块。首先，将单张图像输入人体视频扩散模块以生成连贯的人体视频。接下来，视频增强模块应用超分辨率和视频插值来增强生成视频的纹理和几何平滑度。最后，3D人体高斯渲染模块在这些高分辨率和视角一致的图像的指导下学习逼真的人体。实验表明，Human-VDM可以从单张图像生成高质量的3D人体，在生成质量和数量方面均优于现有最佳方法。项目页面：https://human-vdm.github.io/Human-VDM/||
|**2024-09-04**|[Multi-Track MusicLDM: Towards Versatile Music Generation with Latent Diffusion Model](http://arxiv.org/abs/2409.02845)|null|扩散模型在涉及音频和音乐的跨模态生成任务中展现出巨大的潜力，例如文本到声音和文本到音乐的生成。这些文本控制的音乐生成模型通常侧重于通过捕捉全局音乐属性（如流派和情绪）来生成音乐。然而，音乐创作是一项复杂的多层次任务，通常将音乐编排作为创作过程的一个组成部分。此过程涉及创作每个乐器部分，使其在节奏、力度、和声和旋律方面与现有部分保持一致，这需要比文本提示通常提供的更精确的音轨控制。在这项工作中，我们通过将 MusicLDM（一种用于音乐的潜在扩散模型）扩展为多轨生成模型来应对这些挑战。通过学习共享上下文的音轨的联合概率，我们的模型能够跨多个音轨生成彼此良好对应的音乐，无论是有条件地还是无条件地。此外，我们的模型还能够进行编曲生成，其中模型可以在给定其他音轨的情况下生成任何音轨子集（例如，生成与给定贝斯和鼓音轨互补的钢琴音轨）。我们将我们的模型与现有的多轨生成模型进行了比较，结果表明，我们的模型在总生成任务和编曲生成任务的客观指标上都取得了相当大的改进。||
|**2024-09-04**|[Rethinking HTG Evaluation: Bridging Generation and Recognition](http://arxiv.org/abs/2409.02683)|**[link](https://github.com/koninik/htg_evaluation)**|生成模型在自然图像任务中的评估已得到广泛研究。即使在诸如手写生成（HTG）等具有独特特殊性的情况下，也使用了类似的协议和指标，即使它们可能并非完全合适。在这项工作中，我们介绍了三种专为 HTG 评估量身定制的度量指标： $\text{HTG}_{\text{HTR}} $、$ \text{HTG}_{\text{style}} $ 和 $ \text{HTG}_{\text{OOV}}$ ，并认为它们更便于评估生成手写图像的质量。这些指标依赖于手写文本识别和书写者识别模型的识别错误/准确率，并强调书写风格、文本内容和多样性是符合手写图像内容的主要方面。我们在 IAM 手写数据库上进行了全面的实验，结果表明，诸如 FID 之类的广泛使用的指标无法正确量化生成手写样本的多样性和实用性。我们的研究结果表明，我们的指标信息更丰富，并强调了 HTG 中标准化评估协议的必要性。所提出的指标为评估 HTG 质量提供了更稳健、信息更丰富的协议，有助于提高 HTR 的性能。评估协议的代码可在以下网址获得：https://github.com/koninik/HTG_evaluation。||
|**2024-09-04**|[Introduction to Machine Learning](http://arxiv.org/abs/2409.02668)|null|本书介绍了机器学习中许多算法的开发和分析所依赖的数学基础和技术。本书首先介绍了贯穿全书的符号表示，并回顾了微积分、线性代数和概率论的基本概念，还介绍了一些测度论术语，可作为使用这些工具的部分的阅读指南。导论章节还提供了矩阵分析和优化的背景知识。后面的章节为本书中使用的许多算法提供了理论支持，包括随机梯度下降、近似方法等。在讨论了统计预测的基本概念之后，本书介绍了再生核理论和希尔伯特空间技术，这些技术在许多地方都有应用，然后介绍了各种监督统计学习算法，包括线性方法、支持向量机、决策树、boosting和神经网络。接下来转向生成方法，首先介绍了采样方法和马尔可夫链理论。接下来的章节描述了图模型理论，介绍了潜变量模型的变分方法，以及基于深度学习的生成模型。接下来的章节重点介绍无监督学习方法，包括聚类、因子分析和流形学习。本书的最后一章偏向理论，讨论了集中不等式和泛化界。||
|**2024-09-04**|[Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection](http://arxiv.org/abs/2409.02664)|null|The proliferation of deepfake faces poses huge potential negative impacts on our daily lives. Despite substantial advancements in deepfake detection over these years, the generalizability of existing methods against forgeries from unseen datasets or created by emerging generative models remains constrained. In this paper, inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach that repurposes a well-trained VLM for general deepfake detection. Motivated by the model reprogramming paradigm that manipulates the model prediction via data perturbations, our method can reprogram a pretrained VLM model (e.g., CLIP) solely based on manipulating its input without tuning the inner parameters. Furthermore, we insert a pseudo-word guided by facial identity into the text prompt. Extensive experiments on several popular benchmarks demonstrate that (1) the cross-dataset and cross-manipulation performances of deepfake detection can be significantly and consistently improved (e.g., over 88% AUC in cross-dataset setting from FF++ to WildDeepfake) using a pre-trained CLIP model with our proposed reprogramming method; (2) our superior performances are at less cost of trainable parameters, making it a promising approach for real-world applications.||
|**2024-09-04**|[PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for One-Shot Talking Head Generation](http://arxiv.org/abs/2409.02657)|null|While previous audio-driven talking head generation (THG) methods generate head poses from driving audio, the generated poses or lips cannot match the audio well or are not editable. In this study, we propose \textbf{PoseTalk}, a THG system that can freely generate lip-synchronized talking head videos with free head poses conditioned on text prompts and audio. The core insight of our method is using head pose to connect visual, linguistic, and audio signals. First, we propose to generate poses from both audio and text prompts, where the audio offers short-term variations and rhythm correspondence of the head movements and the text prompts describe the long-term semantics of head motions. To achieve this goal, we devise a Pose Latent Diffusion (PLD) model to generate motion latent from text prompts and audio cues in a pose latent space. Second, we observe a loss-imbalance problem: the loss for the lip region contributes less than 4\% of the total reconstruction loss caused by both pose and lip, making optimization lean towards head movements rather than lip shapes. To address this issue, we propose a refinement-based learning strategy to synthesize natural talking videos using two cascaded networks, i.e., CoarseNet, and RefineNet. The CoarseNet estimates coarse motions to produce animated images in novel poses and the RefineNet focuses on learning finer lip motions by progressively estimating lip motions from low-to-high resolutions, yielding improved lip-synchronization performance. Experiments demonstrate our pose prediction strategy achieves better pose diversity and realness compared to text-only or audio-only, and our video generator model outperforms state-of-the-art methods in synthesizing talking videos with natural head motions. Project: https://junleen.github.io/projects/posetalk.||
|**2024-09-04**|[Skip-and-Play: Depth-Driven Pose-Preserved Image Generation for Any Objects](http://arxiv.org/abs/2409.02653)|null|The emergence of diffusion models has enabled the generation of diverse high-quality images solely from text, prompting subsequent efforts to enhance the controllability of these models. Despite the improvement in controllability, pose control remains limited to specific objects (e.g., humans) or poses (e.g., frontal view) due to the fact that pose is generally controlled via camera parameters (e.g., rotation angle) or keypoints (e.g., eyes, nose). Specifically, camera parameters-conditional pose control models generate unrealistic images depending on the object, owing to the small size of 3D datasets for training. Also, keypoint-based approaches encounter challenges in acquiring reliable keypoints for various objects (e.g., church) or poses (e.g., back view). To address these limitations, we propose depth-based pose control, as depth maps are easily obtainable from a single depth estimation model regardless of objects and poses, unlike camera parameters and keypoints. However, depth-based pose control confronts issues of shape dependency, as depth maps influence not only the pose but also the shape of the generated images. To tackle this issue, we propose Skip-and-Play (SnP), designed via analysis of the impact of three components of depth-conditional ControlNet on the pose and the shape of the generated images. To be specific, based on the analysis, we selectively skip parts of the components to mitigate shape dependency on the depth map while preserving the pose. Through various experiments, we demonstrate the superiority of SnP over baselines and showcase the ability of SnP to generate images of diverse objects and poses. Remarkably, SnP exhibits the ability to generate images even when the objects in the condition (e.g., a horse) and the prompt (e.g., a hedgehog) differ from each other.||

<p align=right>(<a href=#updated-on-20240930>back to top</a>)</p>

## LLM

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-09-27**|[Suicide Phenotyping from Clinical Notes in Safety-Net Psychiatric Hospital Using Multi-Label Classification with Pre-Trained Language Models](http://arxiv.org/abs/2409.18878)|null|在高危精神病治疗环境中，准确识别和分类自杀事件可以产生更好的自杀预防措施，减少运营负担并提高护理质量。预训练的语言模型有望从非结构化的临床叙述中识别出自杀倾向。我们使用两种微调策略（多单标签和单多标签），评估了四种基于 BERT 的模型在 500 份带注释的精神病评估记录中检测共存自杀事件的表现。这些记录被标记为自杀意念 (SI)、自杀企图 (SA)、自杀暴露 (ES) 和非自杀性自残 (NSSI)。RoBERTa 在使用二元相关性方面优于其他模型（acc=0.86，F1=0.78）。MentalBERT (F1=0.74) 也超过了 BioClinicalBERT (F1=0.72)。使用单一多标签分类器微调的 RoBERTa 进一步提高了性能（acc=0.88，F1=0.81），这突出表明，在领域相关数据上进行预训练的模型和单一多标签分类策略可提高效率和性能。关键词：基于 EHR 的表型分析；自然语言处理；EHR 数据的二次使用；自杀分类；基于 BERT 的模型；精神病学；心理健康|
|**2024-09-26**|[Infer Human's Intentions Before Following Natural Language Instructions](http://arxiv.org/abs/2409.18073)|null|为了使人工智能体能够帮助人类，它们应该能够遵循自然语言指令，在人类环境中完成日常的合作任务。然而，真实的人类指令天生就具有模糊性，因为说话者假设听者对其隐藏的目标和意图有足够的先验知识。标准的语言基础和规划方法无法解决这种模糊性，因为它们没有将人类内部目标建模为环境中额外的部分可观察因素。我们提出了一个新的框架，即基于社会和具身推理的指令遵循（FISER），旨在更好地遵循协作具身任务中的自然语言指令。我们的框架将对人类目标和意图的明确推断作为中间推理步骤。我们实现了一组基于 Transformer 的模型，并在一个具有挑战性的基准测试 HandMeThat 上对其进行了评估。我们通过经验证明，在制定行动计划之前使用社会推理明确推断人类意图优于纯粹的端到端方法。我们还将我们的实现与强大的基线进行了比较，包括在最大的可用预训练语言模型上进行思维链提示，发现 FISER 在所研究的具身社会推理任务上提供了更好的性能，达到了 HandMeThat 上的最新水平。|
|**2024-09-26**|[Are Transformers in Pre-trained LM A Good ASR Encoder? An Empirical Study](http://arxiv.org/abs/2409.17750)|null|In this study, we delve into the efficacy of transformers within pre-trained language models (PLMs) when repurposed as encoders for Automatic Speech Recognition (ASR). Our underlying hypothesis posits that, despite being initially trained on text-based corpora, these transformers possess a remarkable capacity to extract effective features from the input sequence. This inherent capability, we argue, is transferrable to speech data, thereby augmenting the acoustic modeling ability of ASR. Through rigorous empirical analysis, our findings reveal a notable improvement in Character Error Rate (CER) and Word Error Rate (WER) across diverse ASR tasks when transformers from pre-trained LMs are incorporated. Particularly, they serve as an advantageous starting point for initializing ASR encoders. Furthermore, we uncover that these transformers, when integrated into a well-established ASR encoder, can significantly boost performance, especially in scenarios where profound semantic comprehension is pivotal. This underscores the potential of leveraging the semantic prowess embedded within pre-trained transformers to advance ASR systems' capabilities.|
|**2024-09-24**|[HLB: Benchmarking LLMs' Humanlikeness in Language Use](http://arxiv.org/abs/2409.15890)|null|As synthetic data becomes increasingly prevalent in training language models, particularly through generated dialogue, concerns have emerged that these models may deviate from authentic human language patterns, potentially losing the richness and creativity inherent in human communication. This highlights the critical need to assess the humanlikeness of language models in real-world language use. In this paper, we present a comprehensive humanlikeness benchmark (HLB) evaluating 20 large language models (LLMs) using 10 psycholinguistic experiments designed to probe core linguistic aspects, including sound, word, syntax, semantics, and discourse (see https://huggingface.co/spaces/XufengDuan/HumanLikeness). To anchor these comparisons, we collected responses from over 2,000 human participants and compared them to outputs from the LLMs in these experiments.   For rigorous evaluation, we developed a coding algorithm that accurately identified language use patterns, enabling the extraction of response distributions for each task. By comparing the response distributions between human participants and LLMs, we quantified humanlikeness through distributional similarity. Our results reveal fine-grained differences in how well LLMs replicate human responses across various linguistic levels. Importantly, we found that improvements in other performance metrics did not necessarily lead to greater humanlikeness, and in some cases, even resulted in a decline. By introducing psycholinguistic methods to model evaluation, this benchmark offers the first framework for systematically assessing the humanlikeness of LLMs in language use.|
|**2024-09-23**|[DSG-KD: Knowledge Distillation from Domain-Specific to General Language Models](http://arxiv.org/abs/2409.14904)|**[link](https://github.com/josangyeon/dsg-kd)**|The use of pre-trained language models fine-tuned to address specific downstream tasks is a common approach in natural language processing (NLP). However, acquiring domain-specific knowledge via fine-tuning is challenging. Traditional methods involve pretraining language models using vast amounts of domain-specific data before fine-tuning for particular tasks. This study investigates emergency/non-emergency classification tasks based on electronic medical record (EMR) data obtained from pediatric emergency departments (PEDs) in Korea. Our findings reveal that existing domain-specific pre-trained language models underperform compared to general language models in handling N-lingual free-text data characteristics of non-English-speaking regions. To address these limitations, we propose a domain knowledge transfer methodology that leverages knowledge distillation to infuse general language models with domain-specific knowledge via fine-tuning. This study demonstrates the effective transfer of specialized knowledge between models by defining a general language model as the student model and a domain-specific pre-trained model as the teacher model. In particular, we address the complexities of EMR data obtained from PEDs in non-English-speaking regions, such as Korea, and demonstrate that the proposed method enhances classification performance in such contexts. The proposed methodology not only outperforms baseline models on Korean PED EMR data, but also promises broader applicability in various professional and technical domains. In future works, we intend to extend this methodology to include diverse non-English-speaking regions and address additional downstream tasks, with the aim of developing advanced model architectures using state-of-the-art KD techniques. The code is available in https://github.com/JoSangYeon/DSG-KD.|
|**2024-09-23**|[Pre-trained Language Model and Knowledge Distillation for Lightweight Sequential Recommendation](http://arxiv.org/abs/2409.14810)|null|Sequential recommendation models user interests based on historical behaviors to provide personalized recommendation. Previous sequential recommendation algorithms primarily employ neural networks to extract features of user interests, achieving good performance. However, due to the recommendation system datasets sparsity, these algorithms often employ small-scale network frameworks, resulting in weaker generalization capability. Recently, a series of sequential recommendation algorithms based on large pre-trained language models have been proposed. Nonetheless, given the real-time demands of recommendation systems, the challenge remains in applying pre-trained language models for rapid recommendations in real scenarios. To address this, we propose a sequential recommendation algorithm based on a pre-trained language model and knowledge distillation. The key of proposed algorithm is to transfer pre-trained knowledge across domains and achieve lightweight inference by knowledge distillation. The algorithm operates in two stages: in the first stage, we fine-tune the pre-trained language model on the recommendation dataset to transfer the pre-trained knowledge to the recommendation task; in the second stage, we distill the trained language model to transfer the learned knowledge to a lightweight model. Extensive experiments on multiple public recommendation datasets show that the proposed algorithm enhances recommendation accuracy and provide timely recommendation services.|
|**2024-09-21**|[Probing Context Localization of Polysemous Words in Pre-trained Language Model Sub-Layers](http://arxiv.org/abs/2409.14097)|null|In the era of high performing Large Language Models, researchers have widely acknowledged that contextual word representations are one of the key drivers in achieving top performances in downstream tasks. In this work, we investigate the degree of contextualization encoded in the fine-grained sub-layer representations of a Pre-trained Language Model (PLM) by empirical experiments using linear probes. Unlike previous work, we are particularly interested in identifying the strength of contextualization across PLM sub-layer representations (i.e. Self-Attention, Feed-Forward Activation and Output sub-layers). To identify the main contributions of sub-layers to contextualisation, we first extract the sub-layer representations of polysemous words in minimally different sentence pairs, and compare how these representations change through the forward pass of the PLM network. Second, by probing on a sense identification classification task, we try to empirically localize the strength of contextualization information encoded in these sub-layer representations. With these probing experiments, we also try to gain a better understanding of the influence of context length and context richness on the degree of contextualization. Our main conclusion is cautionary: BERT demonstrates a high degree of contextualization in the top sub-layers if the word in question is in a specific position in the sentence with a shorter context window, but this does not systematically generalize across different word positions and context sizes.|
|**2024-09-20**|[Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation](http://arxiv.org/abs/2409.13928)|null|We study the code generation behavior of instruction-tuned models built on top of code pre-trained language models when they could access an auxiliary function to implement a function. We design several ways to provide auxiliary functions to the models by adding them to the query or providing a response prefix to incorporate the ability to utilize auxiliary functions with the instruction-following capability. Our experimental results show the effectiveness of combining the base models' auxiliary function utilization ability with the instruction following ability. In particular, the performance of adopting our approaches with the open-sourced language models surpasses that of the recent powerful proprietary language models, i.e., gpt-4o.|
|**2024-09-20**|[Demystifying and Extracting Fault-indicating Information from Logs for Failure Diagnosis](http://arxiv.org/abs/2409.13561)|**[link](https://github.com/jun-jie-huang/lofi)**|Logs are imperative in the maintenance of online service systems, which often encompass important information for effective failure mitigation. While existing anomaly detection methodologies facilitate the identification of anomalous logs within extensive runtime data, manual investigation of log messages by engineers remains essential to comprehend faults, which is labor-intensive and error-prone. Upon examining the log-based troubleshooting practices at CloudA, we find that engineers typically prioritize two categories of log information for diagnosis. These include fault-indicating descriptions, which record abnormal system events, and fault-indicating parameters, which specify the associated entities. Motivated by this finding, we propose an approach to automatically extract such faultindicating information from logs for fault diagnosis, named LoFI. LoFI comprises two key stages. In the first stage, LoFI performs coarse-grained filtering to collect logs related to the faults based on semantic similarity. In the second stage, LoFI leverages a pre-trained language model with a novel prompt-based tuning method to extract fine-grained information of interest from the collected logs. We evaluate LoFI on logs collected from Apache Spark and an industrial dataset from CloudA. The experimental results demonstrate that LoFI outperforms all baseline methods by a significant margin, achieving an absolute improvement of 25.8~37.9 in F1 over the best baseline method, ChatGPT. This highlights the effectiveness of LoFI in recognizing fault-indicating information. Furthermore, the successful deployment of LoFI at CloudA and user studies validate the utility of our method. The code and data are available at https://github.com/Jun-jie-Huang/LoFI.|
|**2024-09-20**|[HUT: A More Computation Efficient Fine-Tuning Method With Hadamard Updated Transformation](http://arxiv.org/abs/2409.13501)|null|Fine-tuning pre-trained language models for downstream tasks has achieved impressive results in NLP. However, fine-tuning all parameters becomes impractical due to the rapidly increasing size of model parameters. To address this, Parameter Efficient Fine-Tuning (PEFT) methods update only a subset of parameters. Most PEFT methods, such as LoRA, use incremental updates, which involve adding learned weight matrix increments to the original parameters. Although effective, these methods face limitations in capturing complex parameter dynamics and do not maintain a strong correlation between the original and updated parameters. To overcome these challenges, we propose the direct Updated Transformation (UT) paradigm, which constructs a transformation directly from the original to the updated parameters. This approach ensures that the correlation between the original and updated parameters is preserved, leveraging the semantic features learned during pre-training. Building on this paradigm, we present the Hadamard Updated Transformation (HUT) method. HUT efficiently updates the original weight matrix using the Hadamard transformation with two low-rank matrices, offering a more expressive and flexible update mechanism. This allows HUT to capture richer parameter features through functional transformations, reducing computational complexity while maintaining or improving model quality. Theoretical analysis and extensive experiments on RoBERTa and GPT-2 validate the effectiveness of HUT. Results show that HUT performs on par with or better than other PEFT methods in terms of model quality, while significantly reducing computational complexity.|
|**2024-09-19**|[Exploring Large Language Models for Product Attribute Value Identification](http://arxiv.org/abs/2409.12695)|null|Product attribute value identification (PAVI) involves automatically identifying attributes and their values from product information, enabling features like product search, recommendation, and comparison. Existing methods primarily rely on fine-tuning pre-trained language models, such as BART and T5, which require extensive task-specific training data and struggle to generalize to new attributes. This paper explores large language models (LLMs), such as LLaMA and Mistral, as data-efficient and robust alternatives for PAVI. We propose various strategies: comparing one-step and two-step prompt-based approaches in zero-shot settings and utilizing parametric and non-parametric knowledge through in-context learning examples. We also introduce a dense demonstration retriever based on a pre-trained T5 model and perform instruction fine-tuning to explicitly train LLMs on task-specific instructions. Extensive experiments on two product benchmarks show that our two-step approach significantly improves performance in zero-shot settings, and instruction fine-tuning further boosts performance when using training data, demonstrating the practical benefits of using LLMs for PAVI.||
|**2024-09-16**|[Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large Language Models](http://arxiv.org/abs/2409.10695)|null|We introduce Playground v3 (PGv3), our latest text-to-image model that achieves state-of-the-art (SoTA) performance across multiple testing benchmarks, excels in graphic design abilities and introduces new capabilities. Unlike traditional text-to-image generative models that rely on pre-trained language models like T5 or CLIP text encoders, our approach fully integrates Large Language Models (LLMs) with a novel structure that leverages text conditions exclusively from a decoder-only LLM. Additionally, to enhance image captioning quality-we developed an in-house captioner, capable of generating captions with varying levels of detail, enriching the diversity of text structures. We also introduce a new benchmark CapsBench to evaluate detailed image captioning performance. Experimental results demonstrate that PGv3 excels in text prompt adherence, complex reasoning, and accurate text rendering. User preference studies indicate the super-human graphic design ability of our model for common design applications, such as stickers, posters, and logo designs. Furthermore, PGv3 introduces new capabilities, including precise RGB color control and robust multilingual understanding.||
|**2024-09-14**|[Protecting Copyright of Medical Pre-trained Language Models: Training-Free Backdoor Watermarking](http://arxiv.org/abs/2409.10570)|null|Pre-training language models followed by fine-tuning on specific tasks is standard in NLP, but traditional models often underperform when applied to the medical domain, leading to the development of specialized medical pre-trained language models (Med-PLMs). These models are valuable assets but are vulnerable to misuse and theft, requiring copyright protection. However, no existing watermarking methods are tailored for Med-PLMs, and adapting general PLMs watermarking techniques to the medical domain faces challenges such as task incompatibility, loss of fidelity, and inefficiency. To address these issues, we propose the first training-free backdoor watermarking method for Med-PLMs. Our method uses rare special symbols as trigger words, which do not impact downstream task performance, embedding watermarks by replacing their original embeddings with those of specific medical terms in the Med-PLMs' word embeddings layer. After fine-tuning the watermarked Med-PLMs on various medical downstream tasks, the final models (FMs) respond to the trigger words in the same way they would to the corresponding medical terms. This property can be utilized to extract the watermark. Experiments demonstrate that our method achieves high fidelity while effectively extracting watermarks across various medical downstream tasks. Additionally, our method demonstrates robustness against various attacks and significantly enhances the efficiency of watermark embedding, reducing the embedding time from 10 hours to 10 seconds.||
|**2024-09-14**|[Synthetic4Health: Generating Annotated Synthetic Clinical Letters](http://arxiv.org/abs/2409.09501)|null|Since clinical letters contain sensitive information, clinical-related datasets can not be widely applied in model training, medical research, and teaching. This work aims to generate reliable, various, and de-identified synthetic clinical letters. To achieve this goal, we explored different pre-trained language models (PLMs) for masking and generating text. After that, we worked on Bio\_ClinicalBERT, a high-performing model, and experimented with different masking strategies. Both qualitative and quantitative methods were used for evaluation. Additionally, a downstream task, Named Entity Recognition (NER), was also implemented to assess the usability of these synthetic letters.   The results indicate that 1) encoder-only models outperform encoder-decoder models. 2) Among encoder-only models, those trained on general corpora perform comparably to those trained on clinical data when clinical information is preserved. 3) Additionally, preserving clinical entities and document structure better aligns with our objectives than simply fine-tuning the model. 4) Furthermore, different masking strategies can impact the quality of synthetic clinical letters. Masking stopwords has a positive impact, while masking nouns or verbs has a negative effect. 5) For evaluation, BERTScore should be the primary quantitative evaluation metric, with other metrics serving as supplementary references. 6) Contextual information does not significantly impact the models' understanding, so the synthetic clinical letters have the potential to replace the original ones in downstream tasks.||
|**2024-09-12**|[Knowledge Tagging with Large Language Model based Multi-Agent System](http://arxiv.org/abs/2409.08406)|null|Knowledge tagging for questions is vital in modern intelligent educational applications, including learning progress diagnosis, practice question recommendations, and course content organization. Traditionally, these annotations have been performed by pedagogical experts, as the task demands not only a deep semantic understanding of question stems and knowledge definitions but also a strong ability to link problem-solving logic with relevant knowledge concepts. With the advent of advanced natural language processing (NLP) algorithms, such as pre-trained language models and large language models (LLMs), pioneering studies have explored automating the knowledge tagging process using various machine learning models. In this paper, we investigate the use of a multi-agent system to address the limitations of previous algorithms, particularly in handling complex cases involving intricate knowledge definitions and strict numerical constraints. By demonstrating its superior performance on the publicly available math question knowledge tagging dataset, MathKnowCT, we highlight the significant potential of an LLM-based multi-agent system in overcoming the challenges that previous methods have encountered. Finally, through an in-depth discussion of the implications of automating knowledge tagging, we underscore the promising results of deploying LLM-based algorithms in educational contexts.||
|**2024-09-12**|[Fine-tuning Large Language Models for Entity Matching](http://arxiv.org/abs/2409.08185)|**[link](https://github.com/wbsg-uni-mannheim/tailormatch)**|Generative large language models (LLMs) are a promising alternative to pre-trained language models for entity matching due to their high zero-shot performance and their ability to generalize to unseen entities. Existing research on using LLMs for entity matching has focused on prompt engineering and in-context learning. This paper explores the potential of fine-tuning LLMs for entity matching. We analyze fine-tuning along two dimensions: 1) The representation of training examples, where we experiment with adding different types of LLM-generated explanations to the training set, and 2) the selection and generation of training examples using LLMs. In addition to the matching performance on the source dataset, we investigate how fine-tuning affects the model's ability to generalize to other in-domain datasets as well as across topical domains. Our experiments show that fine-tuning significantly improves the performance of the smaller models while the results for the larger models are mixed. Fine-tuning also improves the generalization to in-domain datasets while hurting cross-domain transfer. We show that adding structured explanations to the training set has a positive impact on the performance of three out of four LLMs, while the proposed example selection and generation methods only improve the performance of Llama 3.1 8B while decreasing the performance of GPT-4o Mini.||
|**2024-09-10**|[Exploring Italian sentence embeddings properties through multi-tasking](http://arxiv.org/abs/2409.06622)|null|We investigate to what degree existing LLMs encode abstract linguistic information in Italian in a multi-task setting. We exploit curated synthetic data on a large scale -- several Blackbird Language Matrices (BLMs) problems in Italian -- and use them to study how sentence representations built using pre-trained language models encode specific syntactic and semantic information. We use a two-level architecture to model separately a compression of the sentence embeddings into a representation that contains relevant information for a task, and a BLM task. We then investigate whether we can obtain compressed sentence representations that encode syntactic and semantic information relevant to several BLM tasks. While we expected that the sentence structure -- in terms of sequence of phrases/chunks -- and chunk properties could be shared across tasks, performance and error analysis show that the clues for the different tasks are encoded in different manners in the sentence embeddings, suggesting that abstract linguistic notions such as constituents or thematic roles does not seem to be present in the pretrained sentence embeddings.||
|**2024-09-09**|[TransformerRanker: A Tool for Efficiently Finding the Best-Suited Language Models for Downstream Classification Tasks](http://arxiv.org/abs/2409.05997)|**[link](https://github.com/flairnlp/transformer-ranker)**|Classification tasks in NLP are typically addressed by selecting a pre-trained language model (PLM) from a model hub, and fine-tuning it for the task at hand. However, given the very large number of PLMs that are currently available, a practical challenge is to determine which of them will perform best for a specific downstream task. With this paper, we introduce TransformerRanker, a lightweight library that efficiently ranks PLMs for classification tasks without the need for computationally costly fine-tuning. Our library implements current approaches for transferability estimation (LogME, H-Score, kNN), in combination with layer aggregation options, which we empirically showed to yield state-of-the-art rankings of PLMs (Garbas et al., 2024). We designed the interface to be lightweight and easy to use, allowing users to directly connect to the HuggingFace Transformers and Dataset libraries. Users need only select a downstream classification task and a list of PLMs to create a ranking of likely best-suited PLMs for their task. We make TransformerRanker available as a pip-installable open-source library https://github.com/flairNLP/transformer-ranker.||
|**2024-09-08**|[Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?](http://arxiv.org/abs/2409.05197)|**[link](https://github.com/zawedcvg/are-large-language-models-attentive-readers)**|State-of-the-art Large Language Models (LLMs) are accredited with an increasing number of different capabilities, ranging from reading comprehension, over advanced mathematical and reasoning skills to possessing scientific knowledge. In this paper we focus on their multi-hop reasoning capability: the ability to identify and integrate information from multiple textual sources.   Given the concerns with the presence of simplifying cues in existing multi-hop reasoning benchmarks, which allow models to circumvent the reasoning requirement, we set out to investigate, whether LLMs are prone to exploiting such simplifying cues. We find evidence that they indeed circumvent the requirement to perform multi-hop reasoning, but they do so in more subtle ways than what was reported about their fine-tuned pre-trained language model (PLM) predecessors. Motivated by this finding, we propose a challenging multi-hop reasoning benchmark, by generating seemingly plausible multi-hop reasoning chains, which ultimately lead to incorrect answers. We evaluate multiple open and proprietary state-of-the-art LLMs, and find that their performance to perform multi-hop reasoning is affected, as indicated by up to 45% relative decrease in F1 score when presented with such seemingly plausible alternatives. We conduct a deeper analysis and find evidence that while LLMs tend to ignore misleading lexical cues, misleading reasoning paths indeed present a significant challenge.||
|**2024-08-21**|[CoPRA: Bridging Cross-domain Pretrained Sequence Models with Complex Structures for Protein-RNA Binding Affinity Prediction](http://arxiv.org/abs/2409.03773)|null|准确测量蛋白质-RNA结合亲和力在许多生物过程和药物设计中至关重要。以前的蛋白质-RNA结合亲和力预测计算方法依赖于序列或结构特征，无法全面捕捉结合机制。最近出现的在大量无监督蛋白质和RNA序列上训练的预训练语言模型，在包括结合位点预测在内的各种域内下游任务中表现出强大的表示能力。然而，协同应用不同领域的语言模型来完成复杂级别的任务仍未得到探索。在本文中，我们提出了CoPRA，通过蛋白质-RNA结合亲和力预测的复合物结构，将来自不同生物领域的预训练语言模型连接起来。我们首次证明了跨生物模态语言模型可以协同提高结合亲和力预测。我们提出了一个Co-Former来结合跨模态序列和结构信息，并提出了一种双范围预训练策略来提高Co-Former的交互理解能力。同时，我们构建了最大的蛋白质-RNA结合亲和力数据集PRA310用于性能评估。我们还在一个公共数据集上测试了我们模型的突变效应预测能力。CoPRA在所有数据集上都达到了最先进的性能。我们提供了广泛的分析，并验证了CoPRA可以（1）准确预测蛋白质-RNA结合亲和力；（2）理解由突变引起的结合亲和力变化；（3）受益于数据和模型规模的扩大。||
|**2024-09-03**|[LUK: Empowering Log Understanding with Expert Knowledge from Large Language Models](http://arxiv.org/abs/2409.01909)|**[link](https://github.com/LeaperOvO/LUK)**|Logs play a critical role in providing essential information for system monitoring and troubleshooting. Recently, with the success of pre-trained language models (PLMs) and large language models (LLMs) in natural language processing (NLP), smaller PLMs (such as BERT) and LLMs (like ChatGPT) have become the current mainstream approaches for log analysis. While LLMs possess rich knowledge, their high computational costs and unstable performance make LLMs impractical for analyzing logs directly. In contrast, smaller PLMs can be fine-tuned for specific tasks even with limited computational resources, making them more practical. However, these smaller PLMs face challenges in understanding logs comprehensively due to their limited expert knowledge. To better utilize the knowledge embedded within LLMs for log understanding, this paper introduces a novel knowledge enhancement framework, called LUK, which acquires expert knowledge from LLMs to empower log understanding on a smaller PLM. Specifically, we design a multi-expert collaboration framework based on LLMs consisting of different roles to acquire expert knowledge. In addition, we propose two novel pre-training tasks to enhance the log pre-training with expert knowledge. LUK achieves state-of-the-art results on different log analysis tasks and extensive experiments demonstrate expert knowledge from LLMs can be utilized more effectively to understand logs.||
|**2024-09-04**|[MARS: Matching Attribute-aware Representations for Text-based Sequential Recommendation](http://arxiv.org/abs/2409.00702)|null|Sequential recommendation aims to predict the next item a user is likely to prefer based on their sequential interaction history. Recently, text-based sequential recommendation has emerged as a promising paradigm that uses pre-trained language models to exploit textual item features to enhance performance and facilitate knowledge transfer to unseen datasets. However, existing text-based recommender models still struggle with two key challenges: (i) representing users and items with multiple attributes, and (ii) matching items with complex user interests. To address these challenges, we propose a novel model, Matching Attribute-aware Representations for Text-based Sequential Recommendation (MARS). MARS extracts detailed user and item representations through attribute-aware text encoding, capturing diverse user intents with multiple attribute-aware representations. It then computes user-item scores via attribute-wise interaction matching, effectively capturing attribute-level user preferences. Our extensive experiments demonstrate that MARS significantly outperforms existing sequential models, achieving improvements of up to 24.43% and 29.26% in Recall@10 and NDCG@10 across five benchmark datasets. Code is available at https://github.com/junieberry/MARS||
|**2024-08-31**|[From Prediction to Application: Language Model-based Code Knowledge Tracing with Domain Adaptive Pre-Training and Automatic Feedback System with Pedagogical Prompting for Comprehensive Programming Education](http://arxiv.org/abs/2409.00323)|null|Knowledge Tracing (KT) is a critical component in online learning, but traditional approaches face limitations in interpretability and cross-domain adaptability. This paper introduces Language Model-based Code Knowledge Tracing (CodeLKT), an innovative application of Language model-based Knowledge Tracing (LKT) to programming education. CodeLKT leverages pre-trained language models to process learning data, demonstrating superior performance over existing KT and Code KT models. We explore Domain Adaptive Pre-Training (DAPT) and Task Adaptive Pre-Training (TAPT), showing enhanced performance in the coding domain and investigating cross-domain transfer between mathematics and coding. Additionally, we present an theoretically-informed integrated system combining CodeLKT with large language models to generate personalized, in-depth feedback to support students' programming learning. This work advances the field of Code Knowledge Tracing by expanding the knowledge base with language model-based approach and offering practical implications for programming education through data-informed feedback.||
|**2024-08-30**|[Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage](http://arxiv.org/abs/2408.17354)|null|Fine-tuning large language models on private data for downstream applications poses significant privacy risks in potentially exposing sensitive information. Several popular community platforms now offer convenient distribution of a large variety of pre-trained models, allowing anyone to publish without rigorous verification. This scenario creates a privacy threat, as pre-trained models can be intentionally crafted to compromise the privacy of fine-tuning datasets. In this study, we introduce a novel poisoning technique that uses model-unlearning as an attack tool. This approach manipulates a pre-trained language model to increase the leakage of private data during the fine-tuning process. Our method enhances both membership inference and data extraction attacks while preserving model utility. Experimental results across different models, datasets, and fine-tuning setups demonstrate that our attacks significantly surpass baseline performance. This work serves as a cautionary note for users who download pre-trained models from unverified sources, highlighting the potential risks involved.||
|**2024-08-24**|[Empowering Pre-Trained Language Models for Spatio-Temporal Forecasting via Decoupling Enhanced Discrete Reprogramming](http://arxiv.org/abs/2408.14505)|null|Spatio-temporal time series forecasting plays a critical role in various real-world applications, such as transportation optimization, energy management, and climate analysis. The recent advancements in Pre-trained Language Models (PLMs) have inspired efforts to reprogram these models for time series forecasting tasks, by leveraging their superior reasoning and generalization capabilities. However, existing approaches fall short in handling complex spatial inter-series dependencies and intrinsic intra-series frequency components, limiting their spatio-temporal forecasting performance. Moreover, the linear mapping of continuous time series to a compressed subset vocabulary in reprogramming constrains the spatio-temporal semantic expressivity of PLMs and may lead to potential information bottleneck. To overcome the above limitations, we propose \textsc{RePST}, a tailored PLM reprogramming framework for spatio-temporal forecasting. The key insight of \textsc{RePST} is to decouple the spatio-temporal dynamics in the frequency domain, allowing better alignment with the PLM text space. Specifically, we first decouple spatio-temporal data in Fourier space and devise a structural diffusion operator to obtain temporal intrinsic and spatial diffusion signals, making the dynamics more comprehensible and predictable for PLMs. To avoid information bottleneck from a limited vocabulary, we further propose a discrete reprogramming strategy that selects relevant discrete textual information from an expanded vocabulary space in a differentiable manner. Extensive experiments on four real-world datasets show that our proposed approach significantly outperforms state-of-the-art spatio-temporal forecasting models, particularly in data-scarce scenarios.||
|**2024-08-23**|[SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks](http://arxiv.org/abs/2408.13040)|null|Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both storage and computation. Additionally, prompting modifies only the LM's inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. This significantly reduces the need for human labor in designing task-specific models. These advantages become even more evident as the number of tasks served by the LM scales up. Motivated by the strengths of prompting, we are the first to explore the potential of prompting speech LMs in the domain of speech processing. Recently, there has been a growing interest in converting speech into discrete units for language modeling. Our pioneer research demonstrates that these quantized speech units are highly versatile within our unified prompting framework. Not only can they serve as class labels, but they also contain rich phonetic information that can be re-synthesized back into speech signals for speech generation tasks. Specifically, we reformulate speech processing tasks into speech-to-unit generation tasks. As a result, we can seamlessly integrate tasks such as speech classification, sequence generation, and speech generation within a single, unified prompting framework. The experiment results show that the prompting method can achieve competitive performance compared to the strong fine-tuning method based on self-supervised learning models with a similar number of trainable parameters. The prompting method also shows promising results in the few-shot setting. Moreover, with the advanced speech LMs coming into the stage, the proposed prompting framework attains great potential.||
|**2024-08-23**|[Investigating LLM Applications in E-Commerce](http://arxiv.org/abs/2408.12779)|null|The emergence of Large Language Models (LLMs) has revolutionized natural language processing in various applications especially in e-commerce. One crucial step before the application of such LLMs in these fields is to understand and compare the performance in different use cases in such tasks. This paper explored the efficacy of LLMs in the e-commerce domain, focusing on instruction-tuning an open source LLM model with public e-commerce datasets of varying sizes and comparing the performance with the conventional models prevalent in industrial applications. We conducted a comprehensive comparison between LLMs and traditional pre-trained language models across specific tasks intrinsic to the e-commerce domain, namely classification, generation, summarization, and named entity recognition (NER). Furthermore, we examined the effectiveness of the current niche industrial application of very large LLM, using in-context learning, in e-commerce specific tasks. Our findings indicate that few-shot inference with very large LLMs often does not outperform fine-tuning smaller pre-trained models, underscoring the importance of task-specific model optimization.Additionally, we investigated different training methodologies such as single-task training, mixed-task training, and LoRA merging both within domain/tasks and between different tasks. Through rigorous experimentation and analysis, this paper offers valuable insights into the potential effectiveness of LLMs to advance natural language processing capabilities within the e-commerce industry.||
|**2024-08-22**|[AutoTest: Evolutionary Code Solution Selection with Test Cases](http://arxiv.org/abs/2408.12125)|null|随着代码生成技术的发展，从多个候选方案中选择正确的代码方案已成为一项至关重要的任务。本研究提出了一种名为AutoTest的新技术，该技术将自动测试用例生成与代码方案执行相结合，利用进化遗传算法优化选择过程。首先，AutoTest利用诸如codegen-16B、code-davinci-002和incoder-6B等大型预训练语言模型来提供代码方案及其相应的测试用例。然后，通过执行代码方案并评估其在测试用例上的性能，形成共识集。基于进化遗传算法的选择、变异和交叉机制，通过调整alpha和beta参数，实现细粒度排名。最后，选择最佳代码方案。AutoTest在HumanEval基准测试中展现出显著的性能提升。HumanEval数据集包含164个编程问题，AutoTest在pass@1分数方面比基线方法提高了约10%。||
|**2024-08-24**|[SarcasmBench: Towards Evaluating Large Language Models on Sarcasm Understanding](http://arxiv.org/abs/2408.11319)|null|In the era of large language models (LLMs), the task of ``System I''~-~the fast, unconscious, and intuitive tasks, e.g., sentiment analysis, text classification, etc., have been argued to be successfully solved. However, sarcasm, as a subtle linguistic phenomenon, often employs rhetorical devices like hyperbole and figuration to convey true sentiments and intentions, involving a higher level of abstraction than sentiment analysis. There is growing concern that the argument about LLMs' success may not be fully tenable when considering sarcasm understanding. To address this question, we select eleven SOTA LLMs and eight SOTA pre-trained language models (PLMs) and present comprehensive evaluations on six widely used benchmark datasets through different prompting approaches, i.e., zero-shot input/output (IO) prompting, few-shot IO prompting, chain of thought (CoT) prompting. Our results highlight three key findings: (1) current LLMs underperform supervised PLMs based sarcasm detection baselines across six sarcasm benchmarks. This suggests that significant efforts are still required to improve LLMs' understanding of human sarcasm. (2) GPT-4 consistently and significantly outperforms other LLMs across various prompting methods, with an average improvement of 14.0\% $\uparrow$ . Claude 3 and ChatGPT demonstrate the next best performance after GPT-4. (3) Few-shot IO prompting method outperforms the other two methods: zero-shot IO and few-shot CoT. The reason is that sarcasm detection, being a holistic, intuitive, and non-rational cognitive process, is argued not to adhere to step-by-step logical reasoning, making CoT less effective in understanding sarcasm compared to its effectiveness in mathematical reasoning tasks.||
|**2024-08-20**|[Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution](http://arxiv.org/abs/2408.10548)|**[link](https://github.com/lanxiang1017/language-modeling-on-tabular-data-survey)**|Tabular data, a prevalent data type across various domains, presents unique challenges due to its heterogeneous nature and complex structural relationships. Achieving high predictive performance and robustness in tabular data analysis holds significant promise for numerous applications. Influenced by recent advancements in natural language processing, particularly transformer architectures, new methods for tabular data modeling have emerged. Early techniques concentrated on pre-training transformers from scratch, often encountering scalability issues. Subsequently, methods leveraging pre-trained language models like BERT have been developed, which require less data and yield enhanced performance. The recent advent of large language models, such as GPT and LLaMA, has further revolutionized the field, facilitating more advanced and diverse applications with minimal fine-tuning. Despite the growing interest, a comprehensive survey of language modeling techniques for tabular data remains absent. This paper fills this gap by providing a systematic review of the development of language modeling for tabular data, encompassing: (1) a categorization of different tabular data structures and data types; (2) a review of key datasets used in model training and tasks used for evaluation; (3) a summary of modeling techniques including widely-adopted data processing methods, popular architectures, and training objectives; (4) the evolution from adapting traditional Pre-training/Pre-trained language models to the utilization of large language models; (5) an identification of persistent challenges and potential future research directions in language modeling for tabular data analysis. GitHub page associated with this survey is available at: https://github.com/lanxiang1017/Language-Modeling-on-Tabular-Data-Survey.git.||

<p align=right>(<a href=#updated-on-20240930>back to top</a>)</p>

## Transformer

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-09-27**|[Cottention: Linear Transformers With Cosine Attention](http://arxiv.org/abs/2409.18747)|**[link](https://github.com/gmongaras/Cottention_Transformer)**|Attention mechanisms, particularly softmax attention, have been instrumental in the success of transformer-based models such as GPT. However, the quadratic memory complexity of softmax attention with respect to sequence length poses significant challenges for processing longer sequences. We introduce Cottention, a novel attention mechanism that replaces the softmax operation with cosine similarity. By leveraging the properties of cosine similarity and rearranging the attention equation, Cottention achieves native linear memory complexity with respect to sequence length, making it inherently more memory-efficient than softmax attention. We demonstrate that Cottention can be reformulated as a recurrent neural network (RNN) with a finite hidden state, allowing for constant memory usage during inference. We evaluate Cottention on both the bidirectional BERT and causal GPT tasks, demonstrating comparable performance to softmax attention while significantly reducing memory requirements. To ensure efficient computation, we develop a custom CUDA kernel for Cottention. Our results show that Cottention is a promising alternative to softmax attention, enabling the processing of longer sequences without sacrificing performance, due to its native linear memory complexity and ability to maintain a constant memory footprint during inference.|
|**2024-09-27**|[Token Caching for Diffusion Transformer Acceleration](http://arxiv.org/abs/2409.18523)|null|Diffusion transformers have gained substantial interest in diffusion generative modeling due to their outstanding performance. However, their high computational cost, arising from the quadratic computational complexity of attention mechanisms and multi-step inference, presents a significant bottleneck. To address this challenge, we propose TokenCache, a novel post-training acceleration method that leverages the token-based multi-block architecture of transformers to reduce redundant computations among tokens across inference steps. TokenCache specifically addresses three critical questions in the context of diffusion transformers: (1) which tokens should be pruned to eliminate redundancy, (2) which blocks should be targeted for efficient pruning, and (3) at which time steps caching should be applied to balance speed and quality. In response to these challenges, TokenCache introduces a Cache Predictor that assigns importance scores to tokens, enabling selective pruning without compromising model performance. Furthermore, we propose an adaptive block selection strategy to focus on blocks with minimal impact on the network's output, along with a Two-Phase Round-Robin (TPRR) scheduling policy to optimize caching intervals throughout the denoising process. Experimental results across various models demonstrate that TokenCache achieves an effective trade-off between generation quality and inference speed for diffusion transformers. Our code will be publicly available.|
|**2024-09-26**|[Decomposable Transformer Point Processes](http://arxiv.org/abs/2409.18158)|null|The standard paradigm of modeling marked point processes is by parameterizing the intensity function using an attention-based (Transformer-style) architecture. Despite the flexibility of these methods, their inference is based on the computationally intensive thinning algorithm. In this work, we propose a framework where the advantages of the attention-based architecture are maintained and the limitation of the thinning algorithm is circumvented. The framework depends on modeling the conditional distribution of inter-event times with a mixture of log-normals satisfying a Markov property and the conditional probability mass function for the marks with a Transformer-based architecture. The proposed method attains state-of-the-art performance in predicting the next event of a sequence given its history. The experiments also reveal the efficacy of the methods that do not rely on the thinning algorithm during inference over the ones they do. Finally, we test our method on the challenging long-horizon prediction task and find that it outperforms a baseline developed specifically for tackling this task; importantly, inference requires just a fraction of time compared to the thinning-based baseline.|
|**2024-09-26**|[Supra-Laplacian Encoding for Transformer on Dynamic Graphs](http://arxiv.org/abs/2409.17986)|null|Fully connected Graph Transformers (GT) have rapidly become prominent in the static graph community as an alternative to Message-Passing models, which suffer from a lack of expressivity, oversquashing, and under-reaching. However, in a dynamic context, by interconnecting all nodes at multiple snapshots with self-attention, GT loose both structural and temporal information. In this work, we introduce Supra-LAplacian encoding for spatio-temporal TransformErs (SLATE), a new spatio-temporal encoding to leverage the GT architecture while keeping spatio-temporal information. Specifically, we transform Discrete Time Dynamic Graphs into multi-layer graphs and take advantage of the spectral properties of their associated supra-Laplacian matrix. Our second contribution explicitly model nodes' pairwise relationships with a cross-attention mechanism, providing an accurate edge representation for dynamic link prediction. SLATE outperforms numerous state-of-the-art methods based on Message-Passing Graph Neural Networks combined with recurrent models (e.g LSTM), and Dynamic Graph Transformers, on 9 datasets. Code and instructions to reproduce our results will be open-sourced.|
|**2024-09-26**|[Self-supervised Monocular Depth Estimation with Large Kernel Attention](http://arxiv.org/abs/2409.17895)|null|Self-supervised monocular depth estimation has emerged as a promising approach since it does not rely on labeled training data. Most methods combine convolution and Transformer to model long-distance dependencies to estimate depth accurately. However, Transformer treats 2D image features as 1D sequences, and positional encoding somewhat mitigates the loss of spatial information between different feature blocks, tending to overlook channel features, which limit the performance of depth estimation. In this paper, we propose a self-supervised monocular depth estimation network to get finer details. Specifically, we propose a decoder based on large kernel attention, which can model long-distance dependencies without compromising the two-dimension structure of features while maintaining feature channel adaptivity. In addition, we introduce a up-sampling module to accurately recover the fine details in the depth map. Our method achieves competitive results on the KITTI dataset.|
|**2024-09-26**|[CASPFormer: Trajectory Prediction from BEV Images with Deformable Attention](http://arxiv.org/abs/2409.17790)|null|Motion prediction is an important aspect for Autonomous Driving (AD) and Advance Driver Assistance Systems (ADAS). Current state-of-the-art motion prediction methods rely on High Definition (HD) maps for capturing the surrounding context of the ego vehicle. Such systems lack scalability in real-world deployment as HD maps are expensive to produce and update in real-time. To overcome this issue, we propose Context Aware Scene Prediction Transformer (CASPFormer), which can perform multi-modal motion prediction from rasterized Bird-Eye-View (BEV) images. Our system can be integrated with any upstream perception module that is capable of generating BEV images. Moreover, CASPFormer directly decodes vectorized trajectories without any postprocessing. Trajectories are decoded recurrently using deformable attention, as it is computationally efficient and provides the network with the ability to focus its attention on the important spatial locations of the BEV images. In addition, we also address the issue of mode collapse for generating multiple scene-consistent trajectories by incorporating learnable mode queries. We evaluate our model on the nuScenes dataset and show that it reaches state-of-the-art across multiple metrics|
|**2024-09-26**|[Paraformer-v2: An improved non-autoregressive transformer for noise-robust speech recognition](http://arxiv.org/abs/2409.17746)|null|Attention-based encoder-decoder, e.g. transformer and its variants, generates the output sequence in an autoregressive (AR) manner. Despite its superior performance, AR model is computationally inefficient as its generation requires as many iterations as the output length. In this paper, we propose Paraformer-v2, an improved version of Paraformer, for fast, accurate, and noise-robust non-autoregressive speech recognition. In Paraformer-v2, we use a CTC module to extract the token embeddings, as the alternative to the continuous integrate-and-fire module in Paraformer. Extensive experiments demonstrate that Paraformer-v2 outperforms Paraformer on multiple datasets, especially on the English datasets (over 14% improvement on WER), and is more robust in noisy environments.|
|**2024-09-26**|[Optimal Memorization Capacity of Transformers](http://arxiv.org/abs/2409.17677)|null|Recent research in the field of machine learning has increasingly focused on the memorization capacity of Transformers, but how efficient they are is not yet well understood. We demonstrate that Transformers can memorize labels with $\tilde{O}(\sqrt{N})$ parameters in a next-token prediction setting for $N$ input sequences of length $n$, which is proved to be optimal up to logarithmic factors. This indicates that Transformers can efficiently perform memorization with little influence from the input length $n$ owing to the benefit of parameter sharing. We also analyze the memorization capacity in the sequence-to-sequence setting, and find that $\tilde{O}(\sqrt{nN})$ parameters are not only sufficient, but also necessary at least for Transformers with hardmax. These results suggest that while self-attention mechanisms can efficiently identify input sequences, the feed-forward network becomes a bottleneck when associating a label to each token.|
|**2024-09-26**|[Benign or Not-Benign Overfitting in Token Selection of Attention Mechanism](http://arxiv.org/abs/2409.17625)|null|Modern over-parameterized neural networks can be trained to fit the training data perfectly while still maintaining a high generalization performance. This "benign overfitting" phenomenon has been studied in a surge of recent theoretical work; however, most of these studies have been limited to linear models or two-layer neural networks. In this work, we analyze benign overfitting in the token selection mechanism of the attention architecture, which characterizes the success of transformer models. We first show the existence of a benign overfitting solution and explain its mechanism in the attention architecture. Next, we discuss whether the model converges to such a solution, raising the difficulties specific to the attention architecture. We then present benign overfitting cases and not-benign overfitting cases by conditioning different scenarios based on the behavior of attention probabilities during training. To the best of our knowledge, this is the first study to characterize benign overfitting for the attention mechanism.|
|**2024-09-26**|[Dynamic Subframe Splitting and Spatio-Temporal Motion Entangled Sparse Attention for RGB-E Tracking](http://arxiv.org/abs/2409.17560)|null|Event-based bionic camera asynchronously captures dynamic scenes with high temporal resolution and high dynamic range, offering potential for the integration of events and RGB under conditions of illumination degradation and fast motion. Existing RGB-E tracking methods model event characteristics utilising attention mechanism of Transformer before integrating both modalities. Nevertheless, these methods involve aggregating the event stream into a single event frame, lacking the utilisation of the temporal information inherent in the event stream.Moreover, the traditional attention mechanism is well-suited for dense semantic features, while the attention mechanism for sparse event features require revolution. In this paper, we propose a dynamic event subframe splitting strategy to split the event stream into more fine-grained event clusters, aiming to capture spatio-temporal features that contain motion cues. Based on this, we design an event-based sparse attention mechanism to enhance the interaction of event features in temporal and spatial dimensions. The experimental results indicate that our method outperforms existing state-of-the-art methods on the FE240 and COESOT datasets, providing an effective processing manner for the event data.|
|**2024-09-26**|[MASSFormer: Mobility-Aware Spectrum Sensing using Transformer-Driven Tiered Structure](http://arxiv.org/abs/2409.17546)|null|In this paper, we develop a novel mobility-aware transformer-driven tiered structure (MASSFormer) based cooperative spectrum sensing method that effectively models the spatio-temporal dynamics of user movements. Unlike existing methods, our method considers a dynamic scenario involving mobile primary users (PUs) and secondary users (SUs)and addresses the complexities introduced by user mobility. The transformer architecture utilizes an attention mechanism, enabling the proposed method to adeptly model the temporal dynamics of user mobility by effectively capturing long-range dependencies within the input data. The proposed method first computes tokens from the sequence of covariance matrices (CMs) for each SU and processes them in parallel using the SUtransformer network to learn the spatio-temporal features at SUlevel. Subsequently, the collaborative transformer network learns the group-level PU state from all SU-level feature representations. The attention-based sequence pooling method followed by the transformer encoder adjusts the contributions of all tokens. The main goal of predicting the PU states at each SU-level and group-level is to improve detection performance even more. We conducted a sufficient amount of simulations and compared the detection performance of different SS methods. The proposed method is tested under imperfect reporting channel scenarios to show robustness. The efficacy of our method is validated with the simulation results demonstrating its higher performance compared with existing methods in terms of detection probability, sensing error, and classification accuracy.||
|**2024-09-26**|[NeuroPath: A Neural Pathway Transformer for Joining the Dots of Human Connectomes](http://arxiv.org/abs/2409.17510)|null|Although modern imaging technologies allow us to study connectivity between two distinct brain regions in-vivo, an in-depth understanding of how anatomical structure supports brain function and how spontaneous functional fluctuations emerge remarkable cognition is still elusive. Meanwhile, tremendous efforts have been made in the realm of machine learning to establish the nonlinear mapping between neuroimaging data and phenotypic traits. However, the absence of neuroscience insight in the current approaches poses significant challenges in understanding cognitive behavior from transient neural activities. To address this challenge, we put the spotlight on the coupling mechanism of structural connectivity (SC) and functional connectivity (FC) by formulating such network neuroscience question into an expressive graph representation learning problem for high-order topology. Specifically, we introduce the concept of topological detour to characterize how a ubiquitous instance of FC (direct link) is supported by neural pathways (detour) physically wired by SC, which forms a cyclic loop interacted by brain structure and function. In the clich\'e of machine learning, the multi-hop detour pathway underlying SC-FC coupling allows us to devise a novel multi-head self-attention mechanism within Transformer to capture multi-modal feature representation from paired graphs of SC and FC. Taken together, we propose a biological-inspired deep model, coined as NeuroPath, to find putative connectomic feature representations from the unprecedented amount of neuroimages, which can be plugged into various downstream applications such as task recognition and disease diagnosis. We have evaluated NeuroPath on large-scale public datasets including HCP and UK Biobank under supervised and zero-shot learning, where the state-of-the-art performance by our NeuroPath indicates great potential in network neuroscience.||
|**2024-09-25**|[Non-asymptotic Convergence of Training Transformers for Next-token Prediction](http://arxiv.org/abs/2409.17335)|null|Transformers have achieved extraordinary success in modern machine learning due to their excellent ability to handle sequential data, especially in next-token prediction (NTP) tasks. However, the theoretical understanding of their performance in NTP is limited, with existing studies focusing mainly on asymptotic performance. This paper provides a fine-grained non-asymptotic analysis of the training dynamics of a one-layer transformer consisting of a self-attention module followed by a feed-forward layer. We first characterize the essential structural properties of training datasets for NTP using a mathematical framework based on partial orders. Then, we design a two-stage training algorithm, where the pre-processing stage for training the feed-forward layer and the main stage for training the attention layer exhibit fast convergence performance. Specifically, both layers converge sub-linearly to the direction of their corresponding max-margin solutions. We also show that the cross-entropy loss enjoys a linear convergence rate. Furthermore, we show that the trained transformer presents non-trivial prediction ability with dataset shift, which sheds light on the remarkable generalization performance of transformers. Our analysis technique involves the development of novel properties on the attention gradient and further in-depth analysis of how these properties contribute to the convergence of the training process. Our experiments further validate our theoretical findings.||
|**2024-09-24**|[MonoFormer: One Transformer for Both Diffusion and Autoregression](http://arxiv.org/abs/2409.16280)|null|Most existing multimodality methods use separate backbones for autoregression-based discrete text generation and diffusion-based continuous visual generation, or the same backbone by discretizing the visual data to use autoregression for both text and visual generation. In this paper, we propose to study a simple idea: share one transformer for both autoregression and diffusion. The feasibility comes from two main aspects: (i) Transformer is successfully applied to diffusion for visual generation, and (ii) transformer training for autoregression and diffusion is very similar, and the difference merely lies in that diffusion uses bidirectional attention mask and autoregression uses causal attention mask. Experimental results show that our approach achieves comparable image generation performance to current state-of-the-art methods as well as maintains the text generation capability. The project is publicly available at https://monoformer.github.io/.||
|**2024-09-24**|[TE-PINN: Quaternion-Based Orientation Estimation using Transformer-Enhanced Physics-Informed Neural Networks](http://arxiv.org/abs/2409.16214)|null|This paper introduces a Transformer-Enhanced Physics-Informed Neural Network (TE-PINN) designed for accurate quaternion-based orientation estimation in high-dynamic environments, particularly within the field of robotics. By integrating transformer networks with physics-informed learning, our approach innovatively captures temporal dependencies in sensor data while enforcing the fundamental physical laws governing rotational motion. TE-PINN leverages a multi-head attention mechanism to handle sequential data from inertial sensors, such as accelerometers and gyroscopes, ensuring temporal consistency. Simultaneously, the model embeds quaternion kinematics and rigid body dynamics into the learning process, aligning the network's predictions with mechanical principles like Euler's laws of motion. The physics-informed loss function incorporates the dynamics of angular velocity and external forces, enhancing the network's ability to generalize in complex scenarios. Our experimental evaluation demonstrates that TE-PINN consistently outperforms traditional methods such as Extended Kalman Filters (EKF) and LSTM-based estimators, particularly in scenarios characterized by high angular velocities and noisy sensor data. The results show a significant reduction in mean quaternion error and improved gyroscope bias estimation compared to the state-of-the-art. An ablation study further isolates the contributions of both the transformer architecture and the physics-informed constraints, highlighting the synergistic effect of both components in improving model performance. The proposed model achieves real-time performance on embedded systems typical of mobile robots, offering a scalable and efficient solution for orientation estimation in autonomous systems.||
|**2024-09-24**|[Self-attention as an attractor network: transient memories without backpropagation](http://arxiv.org/abs/2409.16112)|**[link](https://github.com/francill99/self_attention_attractor_network)**|Transformers are one of the most successful architectures of modern neural networks. At their core there is the so-called attention mechanism, which recently interested the physics community as it can be written as the derivative of an energy function in certain cases: while it is possible to write the cross-attention layer as a modern Hopfield network, the same is not possible for the self-attention, which is used in the GPT architectures and other autoregressive models. In this work we show that it is possible to obtain the self-attention layer as the derivative of local energy terms, which resemble a pseudo-likelihood. We leverage the analogy with pseudo-likelihood to design a recurrent model that can be trained without backpropagation: the dynamics shows transient states that are strongly correlated with both train and test examples. Overall we present a novel framework to interpret self-attention as an attractor network, potentially paving the way for new theoretical approaches inspired from physics to understand transformers.||
|**2024-09-24**|[Whisper in Medusa's Ear: Multi-head Efficient Decoding for Transformer-based ASR](http://arxiv.org/abs/2409.15869)|**[link](https://github.com/aiola-lab/whisper-medusa)**|Large transformer-based models have significant potential for speech transcription and translation. Their self-attention mechanisms and parallel processing enable them to capture complex patterns and dependencies in audio sequences. However, this potential comes with challenges, as these large and computationally intensive models lead to slow inference speeds. Various optimization strategies have been proposed to improve performance, including efficient hardware utilization and algorithmic enhancements. In this paper, we introduce Whisper-Medusa, a novel approach designed to enhance processing speed with minimal impact on Word Error Rate (WER). The proposed model extends the OpenAI's Whisper architecture by predicting multiple tokens per iteration, resulting in a 50% reduction in latency. We showcase the effectiveness of Whisper-Medusa across different learning setups and datasets.||
|**2024-09-23**|[SOFI: Multi-Scale Deformable Transformer for Camera Calibration with Enhanced Line Queries](http://arxiv.org/abs/2409.15553)|**[link](https://github.com/sebastianjanampa/sofi)**|Camera calibration consists of estimating camera parameters such as the zenith vanishing point and horizon line. Estimating the camera parameters allows other tasks like 3D rendering, artificial reality effects, and object insertion in an image. Transformer-based models have provided promising results; however, they lack cross-scale interaction. In this work, we introduce \textit{multi-Scale defOrmable transFormer for camera calibratIon with enhanced line queries}, SOFI. SOFI improves the line queries used in CTRL-C and MSCC by using both line content and line geometric features. Moreover, SOFI's line queries allow transformer models to adopt the multi-scale deformable attention mechanism to promote cross-scale interaction between the feature maps produced by the backbone. SOFI outperforms existing methods on the \textit {Google Street View}, \textit {Horizon Line in the Wild}, and \textit {Holicity} datasets while keeping a competitive inference speed.||
|**2024-09-23**|[Diffusion-based RGB-D Semantic Segmentation with Deformable Attention Transformer](http://arxiv.org/abs/2409.15117)|null|Vision-based perception and reasoning is essential for scene understanding in any autonomous system. RGB and depth images are commonly used to capture both the semantic and geometric features of the environment. Developing methods to reliably interpret this data is critical for real-world applications, where noisy measurements are often unavoidable. In this work, we introduce a diffusion-based framework to address the RGB-D semantic segmentation problem. Additionally, we demonstrate that utilizing a Deformable Attention Transformer as the encoder to extract features from depth images effectively captures the characteristics of invalid regions in depth measurements. Our generative framework shows a greater capacity to model the underlying distribution of RGB-D images, achieving robust performance in challenging scenarios with significantly less training time compared to discriminative methods. Experimental results indicate that our approach achieves State-of-the-Art performance on both the NYUv2 and SUN-RGBD datasets in general and especially in the most challenging of their image data. Our project page will be available at https://diffusionmms.github.io/||
|**2024-09-24**|[Efficiently Dispatching Flash Attention For Partially Filled Attention Masks](http://arxiv.org/abs/2409.15097)|null|Transformers are widely used across various applications, many of which yield sparse or partially filled attention matrices. Examples include attention masks designed to reduce the quadratic complexity of attention, sequence packing techniques, and recent innovations like tree masking for fast validation in MEDUSA. Despite the inherent sparsity in these matrices, the state-of-the-art algorithm Flash Attention still processes them with quadratic complexity as though they were dense. In this paper, we introduce Binary Block Masking, a highly efficient modification that enhances Flash Attention by making it mask-aware. We further propose two optimizations: one tailored for masks with contiguous non-zero patterns and another for extremely sparse masks. Our experiments on attention masks derived from real-world scenarios demonstrate up to a 9x runtime improvement. The implementation will be publicly released to foster further research and application.||
|**2024-09-23**|[Kriformer: A Novel Spatiotemporal Kriging Approach Based on Graph Transformers](http://arxiv.org/abs/2409.14906)|null|Accurately estimating data in sensor-less areas is crucial for understanding system dynamics, such as traffic state estimation and environmental monitoring. This study addresses challenges posed by sparse sensor deployment and unreliable data by framing the problem as a spatiotemporal kriging task and proposing a novel graph transformer model, Kriformer. This model estimates data at locations without sensors by mining spatial and temporal correlations, even with limited resources. Kriformer utilizes transformer architecture to enhance the model's perceptual range and solve edge information aggregation challenges, capturing spatiotemporal information effectively. A carefully constructed positional encoding module embeds the spatiotemporal features of nodes, while a sophisticated spatiotemporal attention mechanism enhances estimation accuracy. The multi-head spatial interaction attention module captures subtle spatial relationships between observed and unobserved locations. During training, a random masking strategy prompts the model to learn with partial information loss, allowing the spatiotemporal embedding and multi-head attention mechanisms to synergistically capture correlations among locations. Experimental results show that Kriformer excels in representation learning for unobserved locations, validated on two real-world traffic speed datasets, demonstrating its effectiveness in spatiotemporal kriging tasks.||
|**2024-09-23**|[A-VL: Adaptive Attention for Large Vision-Language Models](http://arxiv.org/abs/2409.14846)|null|The Large Vision-Language Model (LVLM) integrates computer vision and natural language processing techniques, offering substantial application potential. However, these models demand extensive resources during inference. Adaptive attention techniques can dynamically reduce computational redundancy and thus improve efficiency. Although current adaptive attention methods significantly reduce the memory requirements of Transformer-based language models, they are not tailored for LVLMs. We observe that LVLMs generate responses from both remote image tokens and local text tokens, and different modalities have different attention patterns. This observation inspires us to manage the attention for each modality separately. Specifically, for visual input, we store the cache of potentially useful information but only compute the most critical parts. For language input, we care more about local information. Based on our observation and analysis of vision-language attention patterns, we develop A-VL, a plug-and-play adaptive attention tailored for LVLM inference. Extensive evaluations on three vision-language tasks and five datasets show the effectiveness of our designs. Our approach A-VL outperforms existing adaptive attention methods in reducing memory usage and computational load without compromising performance.||
|**2024-09-23**|[RoWSFormer: A Robust Watermarking Framework with Swin Transformer for Enhanced Geometric Attack Resilience](http://arxiv.org/abs/2409.14829)|null|In recent years, digital watermarking techniques based on deep learning have been widely studied. To achieve both imperceptibility and robustness of image watermarks, most current methods employ convolutional neural networks to build robust watermarking frameworks. However, despite the success of CNN-based watermarking models, they struggle to achieve robustness against geometric attacks due to the limitations of convolutional neural networks in capturing global and long-range relationships. To address this limitation, we propose a robust watermarking framework based on the Swin Transformer, named RoWSFormer. Specifically, we design the Locally-Channel Enhanced Swin Transformer Block as the core of both the encoder and decoder. This block utilizes the self-attention mechanism to capture global and long-range information, thereby significantly improving adaptation to geometric distortions. Additionally, we construct the Frequency-Enhanced Transformer Block to extract frequency domain information, which further strengthens the robustness of the watermarking framework. Experimental results demonstrate that our RoWSFormer surpasses existing state-of-the-art watermarking methods. For most non-geometric attacks, RoWSFormer improves the PSNR by 3 dB while maintaining the same extraction accuracy. In the case of geometric attacks (such as rotation, scaling, and affine transformations), RoWSFormer achieves over a 6 dB improvement in PSNR, with extraction accuracy exceeding 97\%.||
|**2024-09-18**|[On Vision Transformers for Classification Tasks in Side-Scan Sonar Imagery](http://arxiv.org/abs/2409.12026)|null|Side-scan sonar (SSS) imagery presents unique challenges in the classification of man-made objects on the seafloor due to the complex and varied underwater environments. Historically, experts have manually interpreted SSS images, relying on conventional machine learning techniques with hand-crafted features. While Convolutional Neural Networks (CNNs) significantly advanced automated classification in this domain, they often fall short when dealing with diverse seafloor textures, such as rocky or ripple sand bottoms, where false positive rates may increase. Recently, Vision Transformers (ViTs) have shown potential in addressing these limitations by utilizing a self-attention mechanism to capture global information in image patches, offering more flexibility in processing spatial hierarchies. This paper rigorously compares the performance of ViT models alongside commonly used CNN architectures, such as ResNet and ConvNext, for binary classification tasks in SSS imagery. The dataset encompasses diverse geographical seafloor types and is balanced between the presence and absence of man-made objects. ViT-based models exhibit superior classification performance across f1-score, precision, recall, and accuracy metrics, although at the cost of greater computational resources. CNNs, with their inductive biases, demonstrate better computational efficiency, making them suitable for deployment in resource-constrained environments like underwater vehicles. Future research directions include exploring self-supervised learning for ViTs and multi-modal fusion to further enhance performance in challenging underwater environments.||
|**2024-09-17**|[A short trajectory is all you need: A transformer-based model for long-time dissipative quantum dynamics](http://arxiv.org/abs/2409.11320)|**[link](https://github.com/kananenka-group/Transformer-spin-boson)**|In this communication we demonstrate that a deep artificial neural network based on a transformer architecture with self-attention layers can predict the long-time population dynamics of a quantum system coupled to a dissipative environment provided that the short-time population dynamics of the system is known. The transformer neural network model developed in this work predicts the long-time dynamics of spin-boson model efficiently and very accurately across different regimes, from weak system-bath coupling to strong coupling non-Markovian regimes. Our model is more accurate than classical forecasting models, such as recurrent neural networks and is comparable to the state-of-the-art models for simulating the dynamics of quantum dissipative systems, based on kernel ridge regression.||
|**2024-09-17**|[Linear Recency Bias During Training Improves Transformers' Fit to Reading Times](http://arxiv.org/abs/2409.11250)|null|Recent psycholinguistic research has compared human reading times to surprisal estimates from language models to study the factors shaping human sentence processing difficulty. Previous studies have shown a strong fit between surprisal values from Transformers and reading times. However, standard Transformers work with a lossless representation of the entire previous linguistic context, unlike models of human language processing that include memory decay. To bridge this gap, this paper evaluates a modification of the Transformer model that uses ALiBi (Press et al., 2022), a recency bias added to attention scores. Surprisal estimates with ALiBi show an improved fit to human reading times compared to a standard Transformer baseline. A subsequent analysis of attention heads suggests that ALiBi's mixture of slopes -- which determine the rate of memory decay in each attention head -- may play a role in the improvement by helping models with ALiBi to track different kinds of linguistic dependencies.||
|**2024-09-17**|[Contrasformer: A Brain Network Contrastive Transformer for Neurodegenerative Condition Identification](http://arxiv.org/abs/2409.10944)|**[link](https://github.com/angusmonroe/contrasformer)**|Understanding neurological disorder is a fundamental problem in neuroscience, which often requires the analysis of brain networks derived from functional magnetic resonance imaging (fMRI) data. Despite the prevalence of Graph Neural Networks (GNNs) and Graph Transformers in various domains, applying them to brain networks faces challenges. Specifically, the datasets are severely impacted by the noises caused by distribution shifts across sub-populations and the neglect of node identities, both obstruct the identification of disease-specific patterns. To tackle these challenges, we propose Contrasformer, a novel contrastive brain network Transformer. It generates a prior-knowledge-enhanced contrast graph to address the distribution shifts across sub-populations by a two-stream attention mechanism. A cross attention with identity embedding highlights the identity of nodes, and three auxiliary losses ensure group consistency. Evaluated on 4 functional brain network datasets over 4 different diseases, Contrasformer outperforms the state-of-the-art methods for brain networks by achieving up to 10.8\% improvement in accuracy, which demonstrates its efficacy in neurological disorder identification. Case studies illustrate its interpretability, especially in the context of neuroscience. This paper provides a solution for analyzing brain networks, offering valuable insights into neurological disorders. Our code is available at \url{https://github.com/AngusMonroe/Contrasformer}.||
|**2024-09-17**|[Adaptive Large Language Models By Layerwise Attention Shortcuts](http://arxiv.org/abs/2409.10870)|null|Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.||
|**2024-09-16**|[Recurrent Graph Transformer Network for Multiple Fault Localization in Naval Shipboard Systems](http://arxiv.org/abs/2409.10792)|null|The integration of power electronics building blocks in modern MVDC 12kV Naval ship systems enhances energy management and functionality but also introduces complex fault detection and control challenges. These challenges strain traditional fault diagnostic methods, making it difficult to detect and manage faults across multiple locations while maintaining system stability and performance. This paper proposes a temporal recurrent graph transformer network for fault diagnosis in naval MVDC 12kV shipboard systems. The deep graph neural network uses gated recurrent units to capture temporal features and a multi-head attention mechanism to extract spatial features, enhancing diagnostic accuracy. The approach effectively identifies and evaluates successive multiple faults with high precision. The method is implemented and validated on the MVDC 12kV shipboard system designed by the ESDRC team, incorporating all key components. Results show significant improvements in fault localization accuracy, with a 1-4% increase in performance metrics compared to other machine learning methods.||
|**2024-09-16**|[Self-Attention Limits Working Memory Capacity of Transformer-Based Models](http://arxiv.org/abs/2409.10715)|null|Recent work on Transformer-based large language models (LLMs) has revealed striking limits in their working memory capacity, similar to what has been found in human behavioral studies. Specifically, these models' performance drops significantly on N-back tasks as N increases. However, there is still a lack of mechanistic interpretability as to why this phenomenon would arise. Inspired by the executive attention theory from behavioral sciences, we hypothesize that the self-attention mechanism within Transformer-based models might be responsible for their working memory capacity limits. To test this hypothesis, we train vanilla decoder-only transformers to perform N-back tasks and find that attention scores gradually aggregate to the N-back positions over training, suggesting that the model masters the task by learning a strategy to pay attention to the relationship between the current position and the N-back position. Critically, we find that the total entropy of the attention score matrix increases as N increases, suggesting that the dispersion of attention scores might be the cause of the capacity limit observed in N-back tasks.||
|**2024-09-16**|[Logic Synthesis Optimization with Predictive Self-Supervision via Causal Transformers](http://arxiv.org/abs/2409.10653)|null|Contemporary hardware design benefits from the abstraction provided by high-level logic gates, streamlining the implementation of logic circuits. Logic Synthesis Optimization (LSO) operates at one level of abstraction within the Electronic Design Automation (EDA) workflow, targeting improvements in logic circuits with respect to performance metrics such as size and speed in the final layout. Recent trends in the field show a growing interest in leveraging Machine Learning (ML) for EDA, notably through ML-guided logic synthesis utilizing policy-based Reinforcement Learning (RL) methods.Despite these advancements, existing models face challenges such as overfitting and limited generalization, attributed to constrained public circuits and the expressiveness limitations of graph encoders. To address these hurdles, and tackle data scarcity issues, we introduce LSOformer, a novel approach harnessing Autoregressive transformer models and predictive SSL to predict the trajectory of Quality of Results (QoR). LSOformer integrates cross-attention modules to merge insights from circuit graphs and optimization sequences, thereby enhancing prediction accuracy for QoR metrics. Experimental studies validate the effectiveness of LSOformer, showcasing its superior performance over baseline architectures in QoR prediction tasks, where it achieves improvements of 5.74%, 4.35%, and 17.06% on the EPFL, OABCD, and proprietary circuits datasets, respectively, in inductive setup.||
|**2024-09-16**|[Garment Attribute Manipulation with Multi-level Attention](http://arxiv.org/abs/2409.10206)|null|In the rapidly evolving field of online fashion shopping, the need for more personalized and interactive image retrieval systems has become paramount. Existing methods often struggle with precisely manipulating specific garment attributes without inadvertently affecting others. To address this challenge, we propose GAMMA (Garment Attribute Manipulation with Multi-level Attention), a novel framework that integrates attribute-disentangled representations with a multi-stage attention-based architecture. GAMMA enables targeted manipulation of fashion image attributes, allowing users to refine their searches with high accuracy. By leveraging a dual-encoder Transformer and memory block, our model achieves state-of-the-art performance on popular datasets like Shopping100k and DeepFashion.||
|**2024-09-14**|[Planning Transformer: Long-Horizon Offline Reinforcement Learning with Planning Tokens](http://arxiv.org/abs/2409.09513)|null|Supervised learning approaches to offline reinforcement learning, particularly those utilizing the Decision Transformer, have shown effectiveness in continuous environments and for sparse rewards. However, they often struggle with long-horizon tasks due to the high compounding error of auto-regressive models. To overcome this limitation, we go beyond next-token prediction and introduce Planning Tokens, which contain high-level, long time-scale information about the agent's future. Predicting dual time-scale tokens at regular intervals enables our model to use these long-horizon Planning Tokens as a form of implicit planning to guide its low-level policy and reduce compounding error. This architectural modification significantly enhances performance on long-horizon tasks, establishing a new state-of-the-art in complex D4RL environments. Additionally, we demonstrate that Planning Tokens improve the interpretability of the model's policy through the interpretable plan visualisations and attention map.||
|**2024-09-14**|[TransformerMPC: Accelerating Model Predictive Control via Transformers](http://arxiv.org/abs/2409.09266)|null|In this paper, we address the problem of reducing the computational burden of Model Predictive Control (MPC) for real-time robotic applications. We propose TransformerMPC, a method that enhances the computational efficiency of MPC algorithms by leveraging the attention mechanism in transformers for both online constraint removal and better warm start initialization. Specifically, TransformerMPC accelerates the computation of optimal control inputs by selecting only the active constraints to be included in the MPC problem, while simultaneously providing a warm start to the optimization process. This approach ensures that the original constraints are satisfied at optimality. TransformerMPC is designed to be seamlessly integrated with any MPC solver, irrespective of its implementation. To guarantee constraint satisfaction after removing inactive constraints, we perform an offline verification to ensure that the optimal control inputs generated by the MPC solver meet all constraints. The effectiveness of TransformerMPC is demonstrated through extensive numerical simulations on complex robotic systems, achieving up to 35x improvement in runtime without any loss in performance.||
|**2024-09-13**|[SGFormer: Single-Layer Graph Transformers with Approximation-Free Linear Complexity](http://arxiv.org/abs/2409.09007)|**[link](https://github.com/qitianwu/sgformer)**|Learning representations on large graphs is a long-standing challenge due to the inter-dependence nature. Transformers recently have shown promising performance on small graphs thanks to its global attention for capturing all-pair interactions beyond observed structures. Existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated architectures by stacking deep attention-based propagation layers. In this paper, we attempt to evaluate the necessity of adopting multi-layer attentions in Transformers on graphs, which considerably restricts the efficiency. Specifically, we analyze a generic hybrid propagation layer, comprised of all-pair attention and graph-based propagation, and show that multi-layer propagation can be reduced to one-layer propagation, with the same capability for representation learning. It suggests a new technical path for building powerful and efficient Transformers on graphs, particularly through simplifying model architectures without sacrificing expressiveness. As exemplified by this work, we propose a Simplified Single-layer Graph Transformers (SGFormer), whose main component is a single-layer global attention that scales linearly w.r.t. graph sizes and requires none of any approximation for accommodating all-pair interactions. Empirically, SGFormer successfully scales to the web-scale graph ogbn-papers100M, yielding orders-of-magnitude inference acceleration over peer Transformers on medium-sized graphs, and demonstrates competitiveness with limited labeled data.||
|**2024-09-13**|[Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry](http://arxiv.org/abs/2409.08769)|**[link](https://github.com/ybkurt/vift)**|In recent years, transformer-based architectures become the de facto standard for sequence modeling in deep learning frameworks. Inspired by the successful examples, we propose a causal visual-inertial fusion transformer (VIFT) for pose estimation in deep visual-inertial odometry. This study aims to improve pose estimation accuracy by leveraging the attention mechanisms in transformers, which better utilize historical data compared to the recurrent neural network (RNN) based methods seen in recent methods. Transformers typically require large-scale data for training. To address this issue, we utilize inductive biases for deep VIO networks. Since latent visual-inertial feature vectors encompass essential information for pose estimation, we employ transformers to refine pose estimates by updating latent vectors temporally. Our study also examines the impact of data imbalance and rotation learning methods in supervised end-to-end learning of visual inertial odometry by utilizing specialized gradients in backpropagation for the elements of SE $(3)$ group. The proposed method is end-to-end trainable and requires only a monocular camera and IMU during inference. Experimental results demonstrate that VIFT increases the accuracy of monocular VIO networks, achieving state-of-the-art results when compared to previous methods on the KITTI dataset. The code will be made available at https://github.com/ybkurt/VIFT.||
|**2024-09-13**|[SkinFormer: Learning Statistical Texture Representation with Transformer for Skin Lesion Segmentation](http://arxiv.org/abs/2409.08652)|**[link](https://github.com/rongtao-xu/skinformer)**|Accurate skin lesion segmentation from dermoscopic images is of great importance for skin cancer diagnosis. However, automatic segmentation of melanoma remains a challenging task because it is difficult to incorporate useful texture representations into the learning process. Texture representations are not only related to the local structural information learned by CNN, but also include the global statistical texture information of the input image. In this paper, we propose a trans\textbf{Former} network (\textbf{SkinFormer}) that efficiently extracts and fuses statistical texture representation for \textbf{Skin} lesion segmentation. Specifically, to quantify the statistical texture of input features, a Kurtosis-guided Statistical Counting Operator is designed. We propose Statistical Texture Fusion Transformer and Statistical Texture Enhance Transformer with the help of Kurtosis-guided Statistical Counting Operator by utilizing the transformer's global attention mechanism. The former fuses structural texture information and statistical texture information, and the latter enhances the statistical texture of multi-scale features. {Extensive experiments on three publicly available skin lesion datasets validate that our SkinFormer outperforms other SOAT methods, and our method achieves 93.2\% Dice score on ISIC 2018. It can be easy to extend SkinFormer to segment 3D images in the future.} Our code is available at https://github.com/Rongtao-Xu/SkinFormer.||
|**2024-09-13**|[VistaFormer: Scalable Vision Transformers for Satellite Image Time Series Segmentation](http://arxiv.org/abs/2409.08461)|**[link](https://github.com/macdonaldezra/VistaFormer)**|We introduce VistaFormer, a lightweight Transformer-based model architecture for the semantic segmentation of remote-sensing images. This model uses a multi-scale Transformer-based encoder with a lightweight decoder that aggregates global and local attention captured in the encoder blocks. VistaFormer uses position-free self-attention layers which simplifies the model architecture and removes the need to interpolate temporal and spatial codes, which can reduce model performance when training and testing image resolutions differ. We investigate simple techniques for filtering noisy input signals like clouds and demonstrate that improved model scalability can be achieved by substituting Multi-Head Self-Attention (MHSA) with Neighbourhood Attention (NA). Experiments on the PASTIS and MTLCC crop-type segmentation benchmarks show that VistaFormer achieves better performance than comparable models and requires only 8% of the floating point operations using MHSA and 11% using NA while also using fewer trainable parameters. VistaFormer with MHSA improves on state-of-the-art mIoU scores by 0.1% on the PASTIS benchmark and 3% on the MTLCC benchmark while VistaFormer with NA improves on the MTLCC benchmark by 3.7%.||
|**2024-09-12**|[SDformer: Efficient End-to-End Transformer for Depth Completion](http://arxiv.org/abs/2409.08159)|**[link](https://github.com/jamesqian11/sdformer-for-depth-completion)**|Depth completion aims to predict dense depth maps with sparse depth measurements from a depth sensor. Currently, Convolutional Neural Network (CNN) based models are the most popular methods applied to depth completion tasks. However, despite the excellent high-end performance, they suffer from a limited representation area. To overcome the drawbacks of CNNs, a more effective and powerful method has been presented: the Transformer, which is an adaptive self-attention setting sequence-to-sequence model. While the standard Transformer quadratically increases the computational cost from the key-query dot-product of input resolution which improperly employs depth completion tasks. In this work, we propose a different window-based Transformer architecture for depth completion tasks named Sparse-to-Dense Transformer (SDformer). The network consists of an input module for the depth map and RGB image features extraction and concatenation, a U-shaped encoder-decoder Transformer for extracting deep features, and a refinement module. Specifically, we first concatenate the depth map features with the RGB image features through the input model. Then, instead of calculating self-attention with the whole feature maps, we apply different window sizes to extract the long-range depth dependencies. Finally, we refine the predicted features from the input module and the U-shaped encoder-decoder Transformer module to get the enriching depth features and employ a convolution layer to obtain the dense depth map. In practice, the SDformer obtains state-of-the-art results against the CNN-based depth completion models with lower computing loads and parameters on the NYU Depth V2 and KITTI DC datasets.||
|**2024-09-12**|[InterACT: Inter-dependency Aware Action Chunking with Hierarchical Attention Transformers for Bimanual Manipulation](http://arxiv.org/abs/2409.07914)|null|We present InterACT: Inter-dependency aware Action Chunking with Hierarchical Attention Transformers, a novel imitation learning framework for bimanual manipulation that integrates hierarchical attention to capture inter-dependencies between dual-arm joint states and visual inputs. InterACT consists of a Hierarchical Attention Encoder and a Multi-arm Decoder, both designed to enhance information aggregation and coordination. The encoder processes multi-modal inputs through segment-wise and cross-segment attention mechanisms, while the decoder leverages synchronization blocks to refine individual action predictions, providing the counterpart's prediction as context. Our experiments on a variety of simulated and real-world bimanual manipulation tasks demonstrate that InterACT significantly outperforms existing methods. Detailed ablation studies validate the contributions of key components of our work, including the impact of CLS tokens, cross-segment encoders, and synchronization blocks.||
|**2024-09-12**|[Lagrange Duality and Compound Multi-Attention Transformer for Semi-Supervised Medical Image Segmentation](http://arxiv.org/abs/2409.07793)|**[link](https://github.com/lzeeorno/lagrange-duality-and-cmaformer)**|Medical image segmentation, a critical application of semantic segmentation in healthcare, has seen significant advancements through specialized computer vision techniques. While deep learning-based medical image segmentation is essential for assisting in medical diagnosis, the lack of diverse training data causes the long-tail problem. Moreover, most previous hybrid CNN-ViT architectures have limited ability to combine various attentions in different layers of the Convolutional Neural Network. To address these issues, we propose a Lagrange Duality Consistency (LDC) Loss, integrated with Boundary-Aware Contrastive Loss, as the overall training objective for semi-supervised learning to mitigate the long-tail problem. Additionally, we introduce CMAformer, a novel network that synergizes the strengths of ResUNet and Transformer. The cross-attention block in CMAformer effectively integrates spatial attention and channel attention for multi-scale feature fusion. Overall, our results indicate that CMAformer, combined with the feature fusion framework and the new consistency loss, demonstrates strong complementarity in semi-supervised learning ensembles. We achieve state-of-the-art results on multiple public medical image datasets. Example code are available at: \url{https://github.com/lzeeorno/Lagrange-Duality-and-CMAformer}.||
|**2024-09-11**|[ENACT: Entropy-based Clustering of Attention Input for Improving the Computational Performance of Object Detection Transformers](http://arxiv.org/abs/2409.07541)|**[link](https://github.com/gsavathrakis/enact)**|Transformers demonstrate competitive performance in terms of precision on the problem of vision-based object detection. However, they require considerable computational resources due to the quadratic size of the attention weights. In this work, we propose to cluster the transformer input on the basis of its entropy. The reason for this is that the self-information of each pixel (whose sum is the entropy), is likely to be similar among pixels corresponding to the same objects. Clustering reduces the size of data given as input to the transformer and therefore reduces training time and GPU memory usage, while at the same time preserves meaningful information to be passed through the remaining parts of the network. The proposed process is organized in a module called ENACT, that can be plugged-in any transformer architecture that consists of a multi-head self-attention computation in its encoder. We ran extensive experiments using the COCO object detection dataset, and three detection transformers. The obtained results demonstrate that in all tested cases, there is consistent reduction in the required computational resources, while the precision of the detection task is only slightly reduced. The code of the ENACT module will become available at https://github.com/GSavathrakis/ENACT||
|**2024-09-11**|[Gated Slot Attention for Efficient Linear-Time Sequence Modeling](http://arxiv.org/abs/2409.07146)|**[link](https://github.com/sustcsonglin/flash-linear-attention)**|Linear attention Transformers and their gated variants, celebrated for enabling parallel training and efficient recurrent inference, still fall short in recall-intensive tasks compared to traditional Transformers and demand significant resources for training from scratch. This paper introduces Gated Slot Attention (GSA), which enhances Attention with Bounded-memory-Control (ABC) by incorporating a gating mechanism inspired by Gated Linear Attention (GLA). Essentially, GSA comprises a two-layer GLA linked via softmax, utilizing context-aware memory reading and adaptive forgetting to improve memory capacity while maintaining compact recurrent state size. This design greatly enhances both training and inference efficiency through GLA's hardware-efficient training algorithm and reduced state size. Additionally, retaining the softmax operation is particularly beneficial in "finetuning pretrained Transformers to RNNs" (T2R) settings, reducing the need for extensive training from scratch. Extensive experiments confirm GSA's superior performance in scenarios requiring in-context recall and in T2R settings.||
|**2024-09-11**|[Enhancing Cross-domain Pre-Trained Decision Transformers with Adaptive Attention](http://arxiv.org/abs/2409.06985)|null|Recently, the pre-training of decision transformers (DT) using a different domain, such as natural language text, has generated significant attention in offline reinforcement learning (Offline RL). Although this cross-domain pre-training approach achieves superior performance compared to training from scratch in environments required short-term planning ability, the mechanisms by which pre-training benefits the fine-tuning phase remain unclear. Furthermore, we point out that the cross-domain pre-training approach hinders the extraction of distant information in environments like PointMaze that require long-term planning ability, leading to performance that is much worse than training DT from scratch. This work first analyzes these issues and found that Markov Matrix, a component that exists in pre-trained attention heads, is the key to explain the significant performance disparity of pre-trained models in different planning abilities. Inspired by our analysis, we propose a general method GPT-DTMA, which equips a pre-trained DT with Mixture of Attention (MoA), to enable adaptive learning and accommodating diverse attention requirements during fine-tuning. Extensive experiments demonstrate that the effectiveness of GPT-DTMA: it achieves superior performance in short-term environments compared to baselines, and in long-term environments, it mitigates the negative impact caused by Markov Matrix, achieving results comparable to those of DT trained from scratch.||
|**2024-09-11**|[Brain-Inspired Stepwise Patch Merging for Vision Transformers](http://arxiv.org/abs/2409.06963)|null|The hierarchical architecture has become a mainstream design paradigm for Vision Transformers (ViTs), with Patch Merging serving as the pivotal component that transforms a columnar architecture into a hierarchical one. Drawing inspiration from the brain's ability to integrate global and local information for comprehensive visual understanding, we propose a novel technique called Stepwise Patch Merging (SPM), which enhances the subsequent attention mechanism's ability to 'see' better. SPM comprises two critical modules: Multi-Scale Aggregation (MSA) and Guided Local Enhancement (GLE). The MSA module integrates multi-scale features to enrich feature representation, while the GLE module focuses on refining local detail extraction, thus achieving an optimal balance between long-range dependency modeling and local feature enhancement. Extensive experiments conducted on benchmark datasets, including ImageNet-1K, COCO, and ADE20K, demonstrate that SPM significantly improves the performance of various models, particularly in dense prediction tasks such as object detection and semantic segmentation. These results underscore the efficacy of SPM in enhancing model accuracy and robustness across a wide range of computer vision tasks.||
|**2024-09-10**|[A Practical Gated Recurrent Transformer Network Incorporating Multiple Fusions for Video Denoising](http://arxiv.org/abs/2409.06603)|null|State-of-the-art (SOTA) video denoising methods employ multi-frame simultaneous denoising mechanisms, resulting in significant delays (e.g., 16 frames), making them impractical for real-time cameras. To overcome this limitation, we propose a multi-fusion gated recurrent Transformer network (GRTN) that achieves SOTA denoising performance with only a single-frame delay. Specifically, the spatial denoising module extracts features from the current frame, while the reset gate selects relevant information from the previous frame and fuses it with current frame features via the temporal denoising module. The update gate then further blends this result with the previous frame features, and the reconstruction module integrates it with the current frame. To robustly compute attention for noisy features, we propose a residual simplified Swin Transformer with Euclidean distance (RSSTE) in the spatial and temporal denoising modules. Comparative objective and subjective results show that our GRTN achieves denoising performance comparable to SOTA multi-frame delay networks, with only a single-frame delay.||
|**2024-09-10**|[Lightweight Multiscale Feature Fusion Super-Resolution Network Based on Two-branch Convolution and Transformer](http://arxiv.org/abs/2409.06590)|null|The single image super-resolution(SISR) algorithms under deep learning currently have two main models, one based on convolutional neural networks and the other based on Transformer. The former uses the stacking of convolutional layers with different convolutional kernel sizes to design the model, which enables the model to better extract the local features of the image; the latter uses the self-attention mechanism to design the model, which allows the model to establish long-distance dependencies between image pixel points through the self-attention mechanism and then better extract the global features of the image. However, both of the above methods face their problems. Based on this, this paper proposes a new lightweight multi-scale feature fusion network model based on two-way complementary convolutional and Transformer, which integrates the respective features of Transformer and convolutional neural networks through a two-branch network architecture, to realize the mutual fusion of global and local information. Meanwhile, considering the partial loss of information caused by the low-pixel images trained by the deep neural network, this paper designs a modular connection method of multi-stage feature supplementation to fuse the feature maps extracted from the shallow stage of the model with those extracted from the deep stage of the model, to minimize the loss of the information in the feature images that is beneficial to the image restoration as much as possible, to facilitate the obtaining of a higher-quality restored image. The practical results finally show that the model proposed in this paper is optimal in image recovery performance when compared with other lightweight models with the same amount of parameters.||
|**2024-09-10**|[Knowledge Distillation via Query Selection for Detection Transformer](http://arxiv.org/abs/2409.06443)|null|Transformers have revolutionized the object detection landscape by introducing DETRs, acclaimed for their simplicity and efficacy. Despite their advantages, the substantial size of these models poses significant challenges for practical deployment, particularly in resource-constrained environments. This paper addresses the challenge of compressing DETR by leveraging knowledge distillation, a technique that holds promise for maintaining model performance while reducing size. A critical aspect of DETRs' performance is their reliance on queries to interpret object representations accurately. Traditional distillation methods often focus exclusively on positive queries, identified through bipartite matching, neglecting the rich information present in hard-negative queries. Our visual analysis indicates that hard-negative queries, focusing on foreground elements, are crucial for enhancing distillation outcomes. To this end, we introduce a novel Group Query Selection strategy, which diverges from traditional query selection in DETR distillation by segmenting queries based on their Generalized Intersection over Union (GIoU) with ground truth objects, thereby uncovering valuable hard-negative queries for distillation. Furthermore, we present the Knowledge Distillation via Query Selection for DETR (QSKD) framework, which incorporates Attention-Guided Feature Distillation (AGFD) and Local Alignment Prediction Distillation (LAPD). These components optimize the distillation process by focusing on the most informative aspects of the teacher model's intermediate features and output. Our comprehensive experimental evaluation of the MS-COCO dataset demonstrates the effectiveness of our approach, significantly improving average precision (AP) across various DETR architectures without incurring substantial computational costs. Specifically, the AP of Conditional DETR ResNet-18 increased from 35.8 to 39.9.||
|**2024-09-10**|[AgileIR: Memory-Efficient Group Shifted Windows Attention for Agile Image Restoration](http://arxiv.org/abs/2409.06206)|null|Image Transformers show a magnificent success in Image Restoration tasks. Nevertheless, most of transformer-based models are strictly bounded by exorbitant memory occupancy. Our goal is to reduce the memory consumption of Swin Transformer and at the same time speed up the model during training process. Thus, we introduce AgileIR, group shifted attention mechanism along with window attention, which sparsely simplifies the model in architecture. We propose Group Shifted Window Attention (GSWA) to decompose Shift Window Multi-head Self Attention (SW-MSA) and Window Multi-head Self Attention (W-MSA) into groups across their attention heads, contributing to shrinking memory usage in back propagation. In addition to that, we keep shifted window masking and its shifted learnable biases during training, in order to induce the model interacting across windows within the channel. We also re-allocate projection parameters to accelerate attention matrix calculation, which we found a negligible decrease in performance. As a result of experiment, compared with our baseline SwinIR and other efficient quantization models, AgileIR keeps the performance still at 32.20 dB on Set5 evaluation dataset, exceeding other methods with tailor-made efficient methods and saves over 50% memory while a large batch size is employed.||
|**2024-09-09**|[ReL-SAR: Representation Learning for Skeleton Action Recognition with Convolutional Transformers and BYOL](http://arxiv.org/abs/2409.05749)|null|为了提取鲁棒且可泛化的骨架动作识别特征，通常需要大量精心标注的数据，而标注和计算成本的限制使得这项任务极具挑战性。因此，利用无标签骨架数据的无监督表征学习至关重要。本研究探讨了用于骨架动作识别的无监督表征学习方法。为此，我们设计了一个轻量级卷积Transformer框架，名为ReL-SAR，它利用卷积层和注意力层的互补性来联合建模骨架序列中的空间和时间线索。我们还对骨架关节采用了选择-排列策略，以确保从骨骼数据中获取更多信息。最后，我们利用Bootstrap Your Own Latent（BYOL）从无标签骨架序列数据中学习鲁棒的表征。我们在有限大小的数据集：MCAD、IXMAS、JHMDB和NW-UCLA上取得了非常有竞争力的结果，证明了我们提出的方法在性能和计算效率方面相对于现有技术的有效性。为了确保可重复性和可复用性，我们在以下链接提供了包含所有实现参数的源代码：https://github.com/SafwenNaimi/Representation-Learning-for-Skeleton-Action-Recognition-with-Convolutional-Transformers-and-BYOL||
|**2024-09-09**|[DSDFormer: An Innovative Transformer-Mamba Framework for Robust High-Precision Driver Distraction Identification](http://arxiv.org/abs/2409.05587)|null|Driver distraction remains a leading cause of traffic accidents, posing a critical threat to road safety globally. As intelligent transportation systems evolve, accurate and real-time identification of driver distraction has become essential. However, existing methods struggle to capture both global contextual and fine-grained local features while contending with noisy labels in training datasets. To address these challenges, we propose DSDFormer, a novel framework that integrates the strengths of Transformer and Mamba architectures through a Dual State Domain Attention (DSDA) mechanism, enabling a balance between long-range dependencies and detailed feature extraction for robust driver behavior recognition. Additionally, we introduce Temporal Reasoning Confident Learning (TRCL), an unsupervised approach that refines noisy labels by leveraging spatiotemporal correlations in video sequences. Our model achieves state-of-the-art performance on the AUC-V1, AUC-V2, and 100-Driver datasets and demonstrates real-time processing efficiency on the NVIDIA Jetson AGX Orin platform. Extensive experimental results confirm that DSDFormer and TRCL significantly improve both the accuracy and robustness of driver distraction detection, offering a scalable solution to enhance road safety.||
|**2024-09-10**|[Retrofitting Temporal Graph Neural Networks with Transformer](http://arxiv.org/abs/2409.05477)|**[link](https://github.com/qianghuangwhu/tf-tgn)**|Temporal graph neural networks (TGNNs) outperform regular GNNs by incorporating time information into graph-based operations. However, TGNNs adopt specialized models (e.g., TGN, TGAT, and APAN ) and require tailored training frameworks (e.g., TGL and ETC). In this paper, we propose TF-TGN, which uses Transformer decoder as the backbone model for TGNN to enjoy Transformer's codebase for efficient training. In particular, Transformer achieves tremendous success for language modeling, and thus the community developed high-performance kernels (e.g., flash-attention and memory-efficient attention) and efficient distributed training schemes (e.g., PyTorch FSDP, DeepSpeed, and Megatron-LM). We observe that TGNN resembles language modeling, i.e., the message aggregation operation between chronologically occurring nodes and their temporal neighbors in TGNNs can be structured as sequence modeling. Beside this similarity, we also incorporate a series of algorithm designs including suffix infilling, temporal graph attention with self-loop, and causal masking self-attention to make TF-TGN work. During training, existing systems are slow in transforming the graph topology and conducting graph sampling. As such, we propose methods to parallelize the CSR format conversion and graph sampling. We also adapt Transformer codebase to train TF-TGN efficiently with multiple GPUs. We experiment with 9 graphs and compare with 2 state-of-the-art TGNN training frameworks. The results show that TF-TGN can accelerate training by over 2.20 while providing comparable or even superior accuracy to existing SOTA TGNNs. TF-TGN is available at https://github.com/qianghuangwhu/TF-TGN.||
|**2024-09-08**|[Low Latency Transformer Inference on FPGAs for Physics Applications with hls4ml](http://arxiv.org/abs/2409.05207)|null|This study presents an efficient implementation of transformer architectures in Field-Programmable Gate Arrays(FPGAs) using hls4ml. We demonstrate the strategy for implementing the multi-head attention, softmax, and normalization layer and evaluate three distinct models. Their deployment on VU13P FPGA chip achieved latency less than 2us, demonstrating the potential for real-time applications. HLS4ML compatibility with any TensorFlow-built transformer model further enhances the scalability and applicability of this work. Index Terms: FPGAs, machine learning, transformers, high energy physics, LIGO||
|**2024-09-08**|[MHS-STMA: Multimodal Hate Speech Detection via Scalable Transformer-Based Multilevel Attention Framework](http://arxiv.org/abs/2409.05136)|null|Social media has a significant impact on people's lives. Hate speech on social media has emerged as one of society's most serious issues recently. Text and pictures are two forms of multimodal data distributed within articles. Unimodal analysis has been the primary emphasis of earlier approaches. Additionally, when doing multimodal analysis, researchers neglect to preserve the distinctive qualities associated with each modality. The present article suggests a scalable architecture for multimodal hate content detection called transformer-based multilevel attention (STMA) to address these shortcomings. This architecture consists of three main parts: a combined attention-based deep learning mechanism, a vision attention mechanism encoder, and a caption attention-mechanism encoder. To identify hate content, each component uses various attention processes and uniquely handles multimodal data. Several studies employing multiple assessment criteria on three hate speech datasets: Hateful memes, MultiOff, and MMHS150K, validate the suggested architecture's efficacy. The outcomes demonstrate that on all three datasets, the suggested strategy performs better than the baseline approaches.||
|**2024-09-08**|[An Analog and Digital Hybrid Attention Accelerator for Transformers with Charge-based In-memory Computing](http://arxiv.org/abs/2409.04940)|null|The attention mechanism is a key computing kernel of Transformers, calculating pairwise correlations across the entire input sequence. The computing complexity and frequent memory access in computing self-attention put a huge burden on the system especially when the sequence length increases. This paper presents an analog and digital hybrid processor to accelerate the attention mechanism for transformers in 65nm CMOS technology. We propose an analog computing-in-memory (CIM) core, which prunes ~75% of low-score tokens on average during runtime at ultra-low power and delay. Additionally, a digital processor performs precise computations only for ~25% unpruned tokens selected by the analog CIM core, preventing accuracy degradation. Measured results show peak energy efficiency of 14.8 and 1.65 TOPS/W, and peak area efficiency of 976.6 and 79.4 GOPS/mm $^\mathrm{2}$ in the analog core and the system-on-chip (SoC), respectively.||
|**2024-09-07**|[Efficient Training of Transformers for Molecule Property Prediction on Small-scale Datasets](http://arxiv.org/abs/2409.04909)|null|血脑屏障（BBB）是一道保护性屏障，将大脑与循环系统隔开，调节物质进入中枢神经系统的通道。评估潜在药物的BBB渗透性对于有效的药物靶向至关重要。然而，传统的BBB渗透性测量实验方法具有挑战性，并且对于大规模筛选来说不切实际。因此，需要开发计算方法来预测BBB渗透性。本文提出了一种增强了自注意力机制的GPS Transformer架构，旨在在低数据情况下表现良好。所提出的方法在使用BBBP数据集的BBB渗透性预测任务上实现了最先进的性能，超过了现有模型。该方法的ROC-AUC为78.8%，比现有最佳水平提高了5.5%。我们证明了标准的自注意力机制与GPS Transformer结合使用比其他注意力机制变体与GPS Transformer结合使用表现更好。||
|**2024-09-07**|[Cross-attention Inspired Selective State Space Models for Target Sound Extraction](http://arxiv.org/abs/2409.04803)|null|Transformer模型，特别是其交叉注意力模块，广泛应用于目标声音提取中的特征融合，该任务基于给定的线索提取感兴趣的信号。尽管有效，但这种方法的计算效率较低。状态空间模型的最新进展，特别是最近的Mamba模型，在各种任务中表现出与基于Transformer的方法相当的性能，同时显著降低了计算复杂度。然而，由于Mamba无法像交叉注意力那样捕捉不同序列之间的依赖关系，因此它在目标声音提取中的适用性受到限制。在本文中，我们提出了用于目标声音提取的CrossMamba模型，它利用Mamba的隐藏注意力机制来计算给定线索和音频混合物之间的依赖关系。Mamba的计算可以分为查询、键和值。我们利用线索生成查询，并利用音频混合物导出键和值，遵循Transformer中交叉注意力机制的原理。来自两种具有代表性的目标声音提取方法的实验结果验证了所提出的CrossMamba的有效性。||
|**2024-09-06**|[Theory, Analysis, and Best Practices for Sigmoid Self-Attention](http://arxiv.org/abs/2409.04431)|**[link](https://github.com/apple/ml-sigmoid-attention)**|注意力是 Transformer 架构的关键组成部分。它是一种序列到序列的映射，将每个序列元素转换为值的加权和。权重通常是通过键和查询之间的点积的 softmax 获得的。最近的工作探索了 Transformer 中 softmax 注意力的替代方案，例如 ReLU 和 sigmoid 激活函数。在这项工作中，我们重新审视 sigmoid 注意力，并对其进行深入的理论和实证分析。理论上，我们证明了具有 sigmoid 注意力的 Transformer 是通用函数逼近器，并且与 softmax 注意力相比，具有更好的正则性。通过详细的实证分析，我们发现，在训练的早期阶段稳定较大的初始注意力范数是成功训练具有 sigmoid 注意力模型的关键因素，其性能优于先前的尝试。我们还介绍了 FLASHSIGMOID，这是一种硬件感知且内存高效的 sigmoid 注意力实现，在 H100 GPU 上，其推理内核速度比 FLASHATTENTION2 提高了 17%。跨语言、视觉和语音的实验表明，经过适当标准化的 sigmoid 注意力在广泛的领域和规模上与 softmax 注意力的强大性能相匹配，这是先前尝试 sigmoid 注意力所无法完全实现的。我们的工作统一了现有技术，并为 sigmoid 注意力作为 Transformer 中 softmax 的直接替代品建立了最佳实践。||
|**2024-09-09**|[AttentionX: Exploiting Consensus Discrepancy In Attention from A Distributed Optimization Perspective](http://arxiv.org/abs/2409.04275)|null|在本文中，我们从分布式优化的角度出发，利用共识差异来扩展Transformer中的标准注意力机制，我们称之为AttentionX。值得注意的是，乘子交替方向法（PDMM）\cite{Zhang16PDMM}旨在迭代地解决点对点（P2P）网络上的一大类分布式优化问题，其中相邻节点根据优化过程中预定义的线性边约束逐渐达成共识。特别是在PDMM的每次迭代中，网络中的每个节点首先从邻居节点收集信息，然后执行本地信息融合。从高层次来看，注意力机制中基于 $KQ$-softmax的$V$表示加权求和对应于从邻居节点收集信息，而Transformer中通过前馈网络（FFN）进行的特征处理对应于本地信息融合。PDMM利用拉格朗日乘子以线性边约束的残差形式捕获历史共识差异，这对于算法的收敛至关重要。受PDMM的启发，我们提出了AttentionX，将共识差异纳入标准注意力机制的输出更新表达式中。AttentionX中的共识差异是指$V$表示的加权求和与其缩放后的$V$ 表示本身之间的差异。在ViT和nanoGPT上的实验表明了其良好的性能。||
|**2024-09-05**|[Attend First, Consolidate Later: On the Importance of Attention in Different LLM Layers](http://arxiv.org/abs/2409.03621)|null|In decoder-based LLMs, the representation of a given layer serves two purposes: as input to the next layer during the computation of the current token; and as input to the attention mechanism of future tokens. In this work, we show that the importance of the latter role might be overestimated. To show that, we start by manipulating the representations of previous tokens; e.g. by replacing the hidden states at some layer k with random vectors. Our experimenting with four LLMs and four tasks show that this operation often leads to small to negligible drop in performance. Importantly, this happens if the manipulation occurs in the top part of the model-k is in the final 30-50% of the layers. In contrast, doing the same manipulation in earlier layers might lead to chance level performance. We continue by switching the hidden state of certain tokens with hidden states of other tokens from another prompt; e.g., replacing the word "Italy" with "France" in "What is the capital of Italy?". We find that when applying this switch in the top 1/3 of the model, the model ignores it (answering "Rome"). However if we apply it before, the model conforms to the switch ("Paris"). Our results hint at a two stage process in transformer-based LLMs: the first part gathers input from previous tokens, while the second mainly processes that information internally.||
|**2024-09-05**|[LMLT: Low-to-high Multi-Level Vision Transformer for Image Super-Resolution](http://arxiv.org/abs/2409.03516)|**[link](https://github.com/jwgdmkj/lmlt)**|Recent Vision Transformer (ViT)-based methods for Image Super-Resolution have demonstrated impressive performance. However, they suffer from significant complexity, resulting in high inference times and memory usage. Additionally, ViT models using Window Self-Attention (WSA) face challenges in processing regions outside their windows. To address these issues, we propose the Low-to-high Multi-Level Transformer (LMLT), which employs attention with varying feature sizes for each head. LMLT divides image features along the channel dimension, gradually reduces spatial size for lower heads, and applies self-attention to each head. This approach effectively captures both local and global information. By integrating the results from lower heads into higher heads, LMLT overcomes the window boundary issues in self-attention. Extensive experiments show that our model significantly reduces inference time and GPU memory usage while maintaining or even surpassing the performance of state-of-the-art ViT-based Image Super-Resolution methods. Our codes are availiable at https://github.com/jwgdmkj/LMLT.||
|**2024-09-05**|[Blended Latent Diffusion under Attention Control for Real-World Video Editing](http://arxiv.org/abs/2409.03514)|null|由于缺乏完全公开可用的文本到视频模型，当前的视频编辑方法倾向于建立在预训练的文本到图像生成模型之上，然而，它们在处理具有时间信息的视频局部编辑方面仍然面临巨大挑战。首先，尽管现有方法试图通过预定义的掩码专注于局部区域编辑，但由于每一帧的空间整体生成，区域外背景的保留并不理想。此外，用户专门提供掩码是一项额外的昂贵工作，因此需要一种集成到编辑过程中的自主掩码策略。最后但同样重要的是，图像级预训练模型没有学习视频帧之间的时间信息，而这对于表达运动和动态至关重要。在本文中，我们建议采用图像级混合潜在扩散模型来执行局部视频编辑任务。具体来说，我们利用 DDIM 反演来获取潜在代码作为背景潜在代码，而不是随机噪声的潜在代码，以更好地保留输入视频的背景信息。我们进一步介绍了一种从扩散步骤中的交叉注意图派生的自主掩码制造机制。最后，我们通过将 U-Net 的自注意力块转换为时空块来增强视频帧之间的时间一致性。通过大量实验，我们提出的方法在不同的现实世界视频编辑任务中展示了有效性。||
|**2024-09-05**|[Characterizing Massive Activations of Attention Mechanism in Graph Neural Networks](http://arxiv.org/abs/2409.03463)|**[link](https://github.com/msorbi/gnn-ma)**|Graph Neural Networks (GNNs) have become increasingly popular for effectively modeling data with graph structures. Recently, attention mechanisms have been integrated into GNNs to improve their ability to capture complex patterns. This paper presents the first comprehensive study revealing a critical, unexplored consequence of this integration: the emergence of Massive Activations (MAs) within attention layers. We introduce a novel method for detecting and analyzing MAs, focusing on edge features in different graph transformer architectures. Our study assesses various GNN models using benchmark datasets, including ZINC, TOX21, and PROTEINS. Key contributions include (1) establishing the direct link between attention mechanisms and MAs generation in GNNs, (2) developing a robust definition and detection method for MAs based on activation ratio distributions, (3) introducing the Explicit Bias Term (EBT) as a potential countermeasure and exploring it as an adversarial framework to assess models robustness based on the presence or absence of MAs. Our findings highlight the prevalence and impact of attention-induced MAs across different architectures, such as GraphTransformer, GraphiT, and SAN. The study reveals the complex interplay between attention mechanisms, model architecture, dataset characteristics, and MAs emergence, providing crucial insights for developing more robust and reliable graph models.||
|**2024-09-05**|[LowFormer: Hardware Efficient Design for Convolutional Transformer Backbones](http://arxiv.org/abs/2409.03460)|**[link](https://github.com/altair199797/lowformer)**|Research in efficient vision backbones is evolving into models that are a mixture of convolutions and transformer blocks. A smart combination of both, architecture-wise and component-wise is mandatory to excel in the speedaccuracy trade-off. Most publications focus on maximizing accuracy and utilize MACs (multiply accumulate operations) as an efficiency metric. The latter however often do not measure accurately how fast a model actually is due to factors like memory access cost and degree of parallelism. We analyzed common modules and architectural design choices for backbones not in terms of MACs, but rather in actual throughput and latency, as the combination of the latter two is a better representation of the efficiency of models in real applications. We applied the conclusions taken from that analysis to create a recipe for increasing hardware-efficiency in macro design. Additionally we introduce a simple slimmed-down version of MultiHead Self-Attention, that aligns with our analysis. We combine both macro and micro design to create a new family of hardware-efficient backbone networks called LowFormer. LowFormer achieves a remarkable speedup in terms of throughput and latency, while achieving similar or better accuracy than current state-of-the-art efficient backbones. In order to prove the generalizability of our hardware-efficient design, we evaluate our method on GPU, mobile GPU and ARM CPU. We further show that the downstream tasks object detection and semantic segmentation profit from our hardware-efficient architecture. Code and models are available at https://github.com/ altair199797/LowFormer.||
|**2024-09-05**|[Masked Sensory-Temporal Attention for Sensor Generalization in Quadruped Locomotion](http://arxiv.org/abs/2409.03332)|null|With the rising focus on quadrupeds, a generalized policy capable of handling different robot models and sensory inputs will be highly beneficial. Although several methods have been proposed to address different morphologies, it remains a challenge for learning-based policies to manage various combinations of proprioceptive information. This paper presents Masked Sensory-Temporal Attention (MSTA), a novel transformer-based model with masking for quadruped locomotion. It employs direct sensor-level attention to enhance sensory-temporal understanding and handle different combinations of sensor data, serving as a foundation for incorporating unseen information. This model can effectively understand its states even with a large portion of missing information, and is flexible enough to be deployed on a physical system despite the long input sequence.||
|**2024-09-05**|[Why mamba is effective? Exploit Linear Transformer-Mamba Network for Multi-Modality Image Fusion](http://arxiv.org/abs/2409.03223)|null|Multi-modality image fusion aims to integrate the merits of images from different sources and render high-quality fusion images. However, existing feature extraction and fusion methods are either constrained by inherent local reduction bias and static parameters during inference (CNN) or limited by quadratic computational complexity (Transformers), and cannot effectively extract and fuse features. To solve this problem, we propose a dual-branch image fusion network called Tmamba. It consists of linear Transformer and Mamba, which has global modeling capabilities while maintaining linear complexity. Due to the difference between the Transformer and Mamba structures, the features extracted by the two branches carry channel and position information respectively. T-M interaction structure is designed between the two branches, using global learnable parameters and convolutional layers to transfer position and channel information respectively. We further propose cross-modal interaction at the attention level to obtain cross-modal attention. Experiments show that our Tmamba achieves promising results in multiple fusion tasks, including infrared-visible image fusion and medical image fusion. Code with checkpoints will be available after the peer-review process.||
|**2024-09-04**|[Probing self-attention in self-supervised speech models for cross-linguistic differences](http://arxiv.org/abs/2409.03115)|null|Speech models have gained traction thanks to increase in accuracy from novel transformer architectures. While this impressive increase in performance across automatic speech recognition (ASR) benchmarks is noteworthy, there is still much that is unknown about the use of attention mechanisms for speech-related tasks. For example, while it is assumed that these models are learning language-independent (i.e., universal) speech representations, there has not yet been an in-depth exploration of what it would mean for the models to be language-independent. In the current paper, we explore this question within the realm of self-attention mechanisms of one small self-supervised speech transformer model (TERA). We find that even with a small model, the attention heads learned are diverse ranging from almost entirely diagonal to almost entirely global regardless of the training language. We highlight some notable differences in attention patterns between Turkish and English and demonstrate that the models do learn important phonological information during pretraining. We also present a head ablation study which shows that models across languages primarily rely on diagonal heads to classify phonemes.||
|**2024-09-04**|[Leveraging Interpretability in the Transformer to Automate the Proactive Scaling of Cloud Resources](http://arxiv.org/abs/2409.03103)|null|现代Web服务采用云原生原则来利用微服务的优势。为了根据服务等级协议（SLA）持续保证高质量的服务（QoS），确保令人满意的用户体验并最大程度地降低运营成本，必须为每个微服务配置适量的资源。然而，准确地为微服务配置充足的资源非常复杂，并且取决于许多因素，包括工作负载强度和微服务之间复杂的互连关系。为了应对这一挑战，我们开发了一个模型，该模型捕获了端到端延迟、前端级别的请求和资源利用率之间的关系。然后，我们使用开发的模型来预测端到端延迟。我们的解决方案利用了时间融合Transformer（TFT），这是一种具有可解释性特征的基于注意力的架构。当预测结果表明不符合SLA时，我们使用TFT提供的特征重要性作为核岭回归（KRR）中的协变量，并将响应变量设置为期望延迟，以学习与特征重要性相关的参数。这些学习到的参数反映了为确保符合SLA而需要对特征进行的调整。我们通过一个基于微服务的应用程序证明了我们方法的优点，并提供了一个部署路线图。||
|**2024-09-05**|[Pooling And Attention: What Are Effective Designs For LLM-Based Embedding Models?](http://arxiv.org/abs/2409.02727)|**[link](https://github.com/yixuantt/poolingandattn)**|The significant advancements of Large Language Models (LLMs) in generative tasks have led to a growing body of work exploring LLM-based embedding models. While these models, employing different pooling and attention strategies, have achieved state-of-the-art performance on public embedding benchmarks, questions still arise about what constitutes an effective design for LLM-based embedding models. However, these models are often trained on different datasets, using different LLM base models or training settings. Moreover, evaluations on public embedding benchmarks often fail to report statistical significance, making it difficult to determine which designs truly contribute to final performance. This complicates the process for practitioners seeking optimal training recipes for LLM-based embedding models. In this study, we conduct a large-scale experiment by training a series of LLM-based embedding models using the same training data and base model but differing in their pooling and attention strategies. The results show that there is no one-size-fits-all solution: while bidirectional attention and an additional trainable pooling layer outperform in text similarity and information retrieval tasks, they do not significantly surpass simpler designs like EOS-last token pooling and default causal attention in clustering and classification tasks. Furthermore, we propose a new pooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs of all hidden layers, rather than just the last layer, using a cross-attention network. This method proves to be statistically superior in text similarity and retrieval tasks compared to existing pooling methods. Overall, this paper sheds light on effective training strategies for LLM-based embedding models.||
|**2024-09-04**|[UniTT-Stereo: Unified Training of Transformer for Enhanced Stereo Matching](http://arxiv.org/abs/2409.02545)|null|Unlike other vision tasks where Transformer-based approaches are becoming increasingly common, stereo depth estimation is still dominated by convolution-based approaches. This is mainly due to the limited availability of real-world ground truth for stereo matching, which is a limiting factor in improving the performance of Transformer-based stereo approaches. In this paper, we propose UniTT-Stereo, a method to maximize the potential of Transformer-based stereo architectures by unifying self-supervised learning used for pre-training with stereo matching framework based on supervised learning. To be specific, we explore the effectiveness of reconstructing features of masked portions in an input image and at the same time predicting corresponding points in another image from the perspective of locality inductive bias, which is crucial in training models with limited training data. Moreover, to address these challenging tasks of reconstruction-and-prediction, we present a new strategy to vary a masking ratio when training the stereo model with stereo-tailored losses. State-of-the-art performance of UniTT-Stereo is validated on various benchmarks such as ETH3D, KITTI 2012, and KITTI 2015 datasets. Lastly, to investigate the advantages of the proposed approach, we provide a frequency analysis of feature maps and the analysis of locality inductive bias based on attention maps.||
|**2024-09-03**|[F2former: When Fractional Fourier Meets Deep Wiener Deconvolution and Selective Frequency Transformer for Image Deblurring](http://arxiv.org/abs/2409.02056)|null|Recent progress in image deblurring techniques focuses mainly on operating in both frequency and spatial domains using the Fourier transform (FT) properties. However, their performance is limited due to the dependency of FT on stationary signals and its lack of capability to extract spatial-frequency properties. In this paper, we propose a novel approach based on the Fractional Fourier Transform (FRFT), a unified spatial-frequency representation leveraging both spatial and frequency components simultaneously, making it ideal for processing non-stationary signals like images. Specifically, we introduce a Fractional Fourier Transformer (F2former), where we combine the classical fractional Fourier based Wiener deconvolution (F2WD) as well as a multi-branch encoder-decoder transformer based on a new fractional frequency aware transformer block (F2TB). We design F2TB consisting of a fractional frequency aware self-attention (F2SA) to estimate element-wise product attention based on important frequency components and a novel feed-forward network based on frequency division multiplexing (FM-FFN) to refine high and low frequency features separately for efficient latent clear image restoration. Experimental results for the cases of both motion deblurring as well as defocus deblurring show that the performance of our proposed method is superior to other state-of-the-art (SOTA) approaches.||
|**2024-09-03**|[TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation](http://arxiv.org/abs/2409.02018)|null|In healthcare, medical image segmentation is crucial for accurate disease diagnosis and the development of effective treatment strategies. Early detection can significantly aid in managing diseases and potentially prevent their progression. Machine learning, particularly deep convolutional neural networks, has emerged as a promising approach to addressing segmentation challenges. Traditional methods like U-Net use encoding blocks for local representation modeling and decoding blocks to uncover semantic relationships. However, these models often struggle with multi-scale objects exhibiting significant variations in texture and shape, and they frequently fail to capture long-range dependencies in the input data. Transformers designed for sequence-to-sequence predictions have been proposed as alternatives, utilizing global self-attention mechanisms. Yet, they can sometimes lack precise localization due to insufficient granular details. To overcome these limitations, we introduce TransDAE: a novel approach that reimagines the self-attention mechanism to include both spatial and channel-wise associations across the entire feature space, while maintaining computational efficiency. Additionally, TransDAE enhances the skip connection pathway with an inter-scale interaction module, promoting feature reuse and improving localization accuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on the Synaps multi-organ dataset, even without relying on pre-trained weights.||
|**2024-09-03**|[TASL-Net: Tri-Attention Selective Learning Network for Intelligent Diagnosis of Bimodal Ultrasound Video](http://arxiv.org/abs/2409.01557)|null|In the intelligent diagnosis of bimodal (gray-scale and contrast-enhanced) ultrasound videos, medical domain knowledge such as the way sonographers browse videos, the particular areas they emphasize, and the features they pay special attention to, plays a decisive role in facilitating precise diagnosis. Embedding medical knowledge into the deep learning network can not only enhance performance but also boost clinical confidence and reliability of the network. However, it is an intractable challenge to automatically focus on these person- and disease-specific features in videos and to enable networks to encode bimodal information comprehensively and efficiently. This paper proposes a novel Tri-Attention Selective Learning Network (TASL-Net) to tackle this challenge and automatically embed three types of diagnostic attention of sonographers into a mutual transformer framework for intelligent diagnosis of bimodal ultrasound videos. Firstly, a time-intensity-curve-based video selector is designed to mimic the temporal attention of sonographers, thus removing a large amount of redundant information while improving computational efficiency of TASL-Net. Then, to introduce the spatial attention of the sonographers for contrast-enhanced video analysis, we propose the earliest-enhanced position detector based on structural similarity variation, on which the TASL-Net is made to focus on the differences of perfusion variation inside and outside the lesion. Finally, by proposing a mutual encoding strategy that combines convolution and transformer, TASL-Net possesses bimodal attention to structure features on gray-scale videos and to perfusion variations on contrast-enhanced videos. These modules work collaboratively and contribute to superior performance. We conduct a detailed experimental validation of TASL-Net's performance on three datasets, including lung, breast, and liver.||
|**2024-09-02**|[Spectron: Target Speaker Extraction using Conditional Transformer with Adversarial Refinement](http://arxiv.org/abs/2409.01352)|**[link](https://github.com/tatban/Spectron)**|Recently, attention-based transformers have become a de facto standard in many deep learning applications including natural language processing, computer vision, signal processing, etc.. In this paper, we propose a transformer-based end-to-end model to extract a target speaker's speech from a monaural multi-speaker mixed audio signal. Unlike existing speaker extraction methods, we introduce two additional objectives to impose speaker embedding consistency and waveform encoder invertibility and jointly train both speaker encoder and speech separator to better capture the speaker conditional embedding. Furthermore, we leverage a multi-scale discriminator to refine the perceptual quality of the extracted speech. Our experiments show that the use of a dual path transformer in the separator backbone along with proposed training paradigm improves the CNN baseline by $3.12$ dB points. Finally, we compare our approach with recent state-of-the-arts and show that our model outperforms existing methods by $4.1$ dB points on an average without creating additional data dependency.||
|**2024-09-02**|[CLIBE: Detecting Dynamic Backdoors in Transformer-based NLP Models](http://arxiv.org/abs/2409.01193)|**[link](https://github.com/raytsang123/clibe)**|Backdoors can be injected into NLP models to induce misbehavior when the input text contains a specific feature, known as a trigger, which the attacker secretly selects. Unlike fixed words, phrases, or sentences used in the static text trigger, NLP dynamic backdoor attacks design triggers associated with abstract and latent text features, making them considerably stealthier than traditional static backdoor attacks. However, existing research on NLP backdoor detection primarily focuses on defending against static backdoor attacks, while detecting dynamic backdoors in NLP models remains largely unexplored. This paper presents CLIBE, the first framework to detect dynamic backdoors in Transformer-based NLP models. CLIBE injects a "few-shot perturbation" into the suspect Transformer model by crafting optimized weight perturbation in the attention layers to make the perturbed model classify a limited number of reference samples as a target label. Subsequently, CLIBE leverages the generalization ability of this few-shot perturbation to determine whether the original model contains a dynamic backdoor. Extensive evaluation on three advanced NLP dynamic backdoor attacks, two widely-used Transformer frameworks, and four real-world classification tasks strongly validates the effectiveness of CLIBE. We also demonstrate the robustness of CLIBE against various adaptive attacks. Furthermore, we employ CLIBE to scrutinize 49 popular Transformer models on Hugging Face and discover one exhibiting a high probability of containing a dynamic backdoor. We have contacted Hugging Face and provided detailed evidence of this model's backdoor behavior. Moreover, we extend CLIBE to detect backdoor text generation models modified to exhibit toxic behavior. To the best of our knowledge, CLIBE is the first framework capable of detecting backdoors in text generation models without access to trigger input test samples.||
|**2024-09-02**|[Progressive Retinal Image Registration via Global and Local Deformable Transformations](http://arxiv.org/abs/2409.01068)|**[link](https://github.com/lyp-deeplearning/awesome-retinal-registration)**|Retinal image registration plays an important role in the ophthalmological diagnosis process. Since there exist variances in viewing angles and anatomical structures across different retinal images, keypoint-based approaches become the mainstream methods for retinal image registration thanks to their robustness and low latency. These methods typically assume the retinal surfaces are planar, and adopt feature matching to obtain the homography matrix that represents the global transformation between images. Yet, such a planar hypothesis inevitably introduces registration errors since retinal surface is approximately curved. This limitation is more prominent when registering image pairs with significant differences in viewing angles. To address this problem, we propose a hybrid registration framework called HybridRetina, which progressively registers retinal images with global and local deformable transformations. For that, we use a keypoint detector and a deformation network called GAMorph to estimate the global transformation and local deformable transformation, respectively. Specifically, we integrate multi-level pixel relation knowledge to guide the training of GAMorph. Additionally, we utilize an edge attention module that includes the geometric priors of the images, ensuring the deformation field focuses more on the vascular regions of clinical interest. Experiments on two widely-used datasets, FIRE and FLoRI21, show that our proposed HybridRetina significantly outperforms some state-of-the-art methods. The code is available at https://github.com/lyp-deeplearning/awesome-retinal-registration.||
|**2024-09-02**|[Multi-scale Temporal Fusion Transformer for Incomplete Vehicle Trajectory Prediction](http://arxiv.org/abs/2409.00904)|null|Motion prediction plays an essential role in autonomous driving systems, enabling autonomous vehicles to achieve more accurate local-path planning and driving decisions based on predictions of the surrounding vehicles. However, existing methods neglect the potential missing values caused by object occlusion, perception failures, etc., which inevitably degrades the trajectory prediction performance in real traffic scenarios. To address this limitation, we propose a novel end-to-end framework for incomplete vehicle trajectory prediction, named Multi-scale Temporal Fusion Transformer (MTFT), which consists of the Multi-scale Attention Head (MAH) and the Continuity Representation-guided Multi-scale Fusion (CRMF) module. Specifically, the MAH leverages the multi-head attention mechanism to parallelly capture multi-scale motion representation of trajectory from different temporal granularities, thus mitigating the adverse effect of missing values on prediction. Furthermore, the multi-scale motion representation is input into the CRMF module for multi-scale fusion to obtain the robust temporal feature of the vehicle. During the fusion process, the continuity representation of vehicle motion is first extracted across time steps to guide the fusion, ensuring that the resulting temporal feature incorporates both detailed information and the overall trend of vehicle motion, which facilitates the accurate decoding of future trajectory that is consistent with the vehicle's motion trend. We evaluate the proposed model on four datasets derived from highway and urban traffic scenarios. The experimental results demonstrate its superior performance in the incomplete vehicle trajectory prediction task compared with state-of-the-art models, e.g., a comprehensive performance improvement of more than 39% on the HighD dataset.||
|**2024-09-01**|[Attention-Guided Multi-scale Interaction Network for Face Super-Resolution](http://arxiv.org/abs/2409.00591)|null|Recently, CNN and Transformer hybrid networks demonstrated excellent performance in face super-resolution (FSR) tasks. Since numerous features at different scales in hybrid networks, how to fuse these multi-scale features and promote their complementarity is crucial for enhancing FSR. However, existing hybrid network-based FSR methods ignore this, only simply combining the Transformer and CNN. To address this issue, we propose an attention-guided Multi-scale interaction network (AMINet), which contains local and global feature interactions as well as encoder-decoder phases feature interactions. Specifically, we propose a Local and Global Feature Interaction Module (LGFI) to promote fusions of global features and different receptive fields' local features extracted by our Residual Depth Feature Extraction Module (RDFE). Additionally, we propose a Selective Kernel Attention Fusion Module (SKAF) to adaptively select fusions of different features within LGFI and encoder-decoder phases. Our above design allows the free flow of multi-scale features from within modules and between encoder and decoder, which can promote the complementarity of different scale features to enhance FSR. Comprehensive experiments confirm that our method consistently performs well with less computational consumption and faster inference.||

<p align=right>(<a href=#updated-on-20240930>back to top</a>)</p>

