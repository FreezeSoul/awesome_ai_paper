## Updated on 2024.06.19
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#多模态>多模态</a></li>
    <li><a href=#6dof-object-pose>6DOF Object Pose</a></li>
    <li><a href=#nerf>nerf</a></li>
    <li><a href=#分类/检测/识别/分割>分类/检测/识别/分割</a></li>
    <li><a href=#模型压缩/优化>模型压缩/优化</a></li>
    <li><a href=#ocr>OCR</a></li>
    <li><a href=#生成模型>生成模型</a></li>
    <li><a href=#llm>LLM</a></li>
    <li><a href=#transformer>Transformer</a></li>
    <li><a href=#3dgs>3DGS</a></li>
    <li><a href=#各类学习方式>各类学习方式</a></li>
  </ol>
</details>

## 多模态

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-18**|[Synergizing Foundation Models and Federated Learning: A Survey](http://arxiv.org/abs/2406.12844)|null|近年来，以大型语言模型、视觉Transformer和多模态模型为代表的Foundation Models (FMs)发展迅速，对学术界和工业界都产生了重大影响。与小规模模型相比，FMs在预训练阶段对海量数据的需求更大。虽然通用FMs可以使用互联网等公开来源收集的数据进行预训练，但特定领域FMs需要专有数据，由于隐私问题，可用数据量成为一个实际挑战。联邦学习 (FL) 是一种协作学习范式，它打破了不同参与者之间数据可用的障碍。因此，它提供了一个很有前景的解决方案，可以使用分布式数据集定制和调整FMs以适应各种特定领域的任务，同时保护隐私。这篇综述文章讨论了融合FL和FMs的潜力和挑战，并总结了核心技术、未来方向和应用。关于FM-FL的定期更新的论文合集可在https://github.com/lishenghui/awesome-fm-fl获取。|
|**2024-06-18**|[Adversarial Attacks on Multimodal Agents](http://arxiv.org/abs/2406.12814)|**[link](https://github.com/chenwu98/agent-attack)**|具有视觉功能的语言模型 (VLM) 现在被用于构建能够在真实环境中采取行动的自主多模态代理。在本文中，我们展示了多模态代理会引发新的安全风险，尽管由于对环境的访问和了解有限，攻击代理比之前的攻击更具挑战性。我们的攻击使用对抗性文本字符串来指导环境中一个触发图像上的基于梯度的扰动：(1) 我们的字幕攻击器攻击白盒字幕器（如果它们被用于将图像处理成字幕作为 VLM 的附加输入）；(2) 我们的 CLIP 攻击共同攻击一组 CLIP 模型，这些模型可以迁移到专有的 VLM。为了评估攻击，我们策划了 VisualWebArena-Adv，这是一组基于 VisualWebArena 的对抗性任务，VisualWebArena 是一个用于基于 Web 的多模态代理任务的环境。在单个图像上 L-infinity 范数为 $16/256$ 的情况下，字幕攻击器攻击可以让字幕增强型 GPT-4V 代理以 75% 的成功率执行对抗目标。当我们移除字幕生成器或使用 GPT-4V 生成自己的字幕时，CLIP 攻击可以分别达到 21% 和 43% 的成功率。对基于其他 VLM（例如 Gemini-1.5、Claude-3 和 GPT-4o）的代理进行的实验表明，它们的稳健性存在有趣的差异。进一步的分析揭示了攻击成功的几个关键因素，我们还讨论了对防御的影响。项目页面：https://chenwu.io/attack-agent 代码和数据：https://github.com/ChenWu98/agent-attack|
|**2024-06-18**|[OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI](http://arxiv.org/abs/2406.12753)|**[link](https://github.com/gair-nlp/olympicarena)**|人工智能 (AI) 的发展得益于大型语言模型 (LLM) 和大型多模态模型 (LMM) 的进步，逐渐展现出在解决问题和科学发现（即 AI4Science）方面的潜在认知推理能力，而这些能力曾经是人类智能所独有的。为了全面评估当前模型在认知推理能力方面的表现，我们推出了 OlympicArena，其中包含 11,163 道双语问题，涵盖纯文本和交错图文两种形式。这些挑战涵盖了七个领域和 62 个国际奥林匹克竞赛项目的广泛学科，并经过严格的数据泄露检查。我们认为，奥林匹克竞赛问题中的挑战是评估人工智能认知推理的理想选择，因为它们具有复杂性和跨学科性，这对于应对复杂的科学挑战和促进发现至关重要。除了使用仅答案标准评估各个学科的表现外，我们还从多个角度进行了详细的实验和分析。我们深入研究了模型的认知推理能力、它们在不同模态下的表现，以及它们在过程级评估中的结果，这些对于需要复杂推理和冗长解决方案的任务至关重要。我们广泛的评估表明，即使是像 GPT-4o 这样先进的模型，总体准确率也只有 39.97%，这说明了当前人工智能在复杂推理和多模态整合方面的局限性。通过 OlympicArena，我们旨在推动人工智能向超级智能发展，使其能够应对科学及其他领域更复杂的挑战。我们还提供了一套全面的资源来支持人工智能研究，包括一个基准数据集、一个开源标注平台、一个详细的评估工具，以及一个具有自动提交功能的排行榜。|
|**2024-06-18**|[Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning](http://arxiv.org/abs/2406.12742)|**[link](https://github.com/dtennant/mirb_eval)**|大型语言模型 (LLM) 的进步显著拓宽了自然语言处理应用的范围，多模态 LLM 扩展了这些能力，以整合和解释视觉数据。然而，现有的视觉语言模型 (VLM) 基准主要集中在单图像输入上，忽略了多图像理解的关键方面。在本文中，我们介绍了一个多图像关系基准 MIRB，旨在评估 VLM 比较、分析和推理多个图像的能力。我们的基准涵盖四个类别：感知、视觉世界知识、推理和多跳推理。通过对各种开源和闭源模型的全面评估，我们证明，虽然开源 VLM 在单图像任务中表现出接近 GPT-4V 的性能，但在多图像推理任务中仍然存在显著的性能差距。我们的研究结果还表明，即使是最先进的 GPT-4V 模型在我们的基准测试中也遇到了困难，这凸显了在该领域进一步研究和开发的必要性。我们相信，我们对 MIRB 的贡献可以作为开发下一代多模态模型的测试平台。|
|**2024-06-18**|[AGLA: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention](http://arxiv.org/abs/2406.12718)|null|尽管大型视觉语言模型 (LVLMs) 在各种多模态任务中取得了巨大成功，但它们面临着一个普遍存在的问题，即对象幻觉，即生成的文本响应与给定图像中的真实对象不一致。本文研究了各种 LVLMs，并指出对判别性局部图像特征的注意力不足是导致对象幻觉的一个根本原因。具体来说，LVLMs 主要关注与提示无关的全局图像特征，而未能捕捉到与提示相关的局部特征，从而破坏了 LVLMs 的视觉基础能力并导致幻觉。为此，我们提出了全局和局部注意力组合 (AGLA) 方法，这是一种无需训练且即插即用的方法，通过探索全局特征集合以生成响应，同时利用局部特征进行视觉区分来减轻对象幻觉。我们的方法展示了一种图像-提示匹配方案，可以从图像中捕获与提示相关的局部特征，从而形成输入图像的增强视图，其中保留了与提示相关的内容，同时屏蔽了不相关的干扰。借助增强视图，可以通过整合来自原始图像的生成性全局特征和来自增强图像的判别性局部特征来获得校准后的解码分布。大量实验表明，AGLA 能够持续减轻各种判别性和生成性基准测试中 LVLMs 的对象幻觉，并增强其一般感知能力。我们的代码将在 https://github.com/Lackel/AGLA 上发布。|
|**2024-06-18**|[Disturbing Image Detection Using LMM-Elicited Emotion Embeddings](http://arxiv.org/abs/2406.12668)|null|本文探讨了利用大型多模态模型 (LMM) 中编码的知识进行令人不安的图像检测 (DID) 的任务。具体来说，我们提出了以两种方式利用 LMM 知识：首先是提取通用的语义描述，其次是提取引发的 emotions。随后，我们使用 CLIP 的文本编码器来获得通用语义描述和 LMM 引发的 emotions 的文本嵌入。最后，我们使用上述文本嵌入以及相应的 CLIP 图像嵌入来执行 DID 任务。所提出的方法显著提高了基线分类精度，在增强后的令人不安的图像检测数据集上实现了最先进的性能。|
|**2024-06-18**|[Do More Details Always Introduce More Hallucinations in LVLM-based Image Captioning?](http://arxiv.org/abs/2406.12663)|null|大型视觉语言模型（LVLM）擅长整合视觉和语言上下文以生成详细的内容，促进了图像描述等应用。然而，使用 LVLM 生成描述时，经常面临对象幻觉（OH）的挑战，即输出文本错误地表示输入图像中的实际对象。虽然之前的研究将 OH 的发生归因于包含更多细节，但我们的研究发现现有指标存在技术缺陷，导致对模型的评估和关于 OH 的结论不可靠。这引发了关于以下问题的争论：在基于 LVLM 的图像描述中，更多细节是否总是会导致更多幻觉？在本文中，我们通过提出一种新颖的解码策略，即差异化波束解码（DBD）以及一组可靠的新评估指标：CLIP-Precision、CLIP-Recall 和 CLIP-F1，来解决这场争论。DBD 将隐藏在视觉输入中的丰富信息并行解码为称为单元事实的不同语言表示。这种解码是通过精心设计的差异分数实现的，该分数指导并行搜索和候选筛选。然后聚合选定的单元事实以生成最终的描述。我们提出的指标通过比较真实图像区域和生成的文本分区的嵌入组，来评估图像描述的全面性和准确性。在视觉基因组数据集上的大量实验验证了我们方法的有效性，证明了它在生成详细描述的同时保持了低幻觉水平。|
|**2024-06-18**|[Efficient and Long-Tailed Generalization for Pre-trained Vision-Language Model](http://arxiv.org/abs/2406.12638)|null|像CLIP这样的预训练视觉语言模型通过图像-文本匹配展现出强大的零样本推理能力，并被证明是各种下游任务中的强大少样本学习器。然而，在现实场景中，将CLIP应用于下游任务可能会遇到以下挑战：1）数据可能呈现长尾分布，并且可能没有所有类别的充足样本；2）可能出现完全没有样本的新类别的新兴任务。为了克服这些挑战，我们提出了一个名为Candle的新框架来实现高效的长尾泛化。在训练过程中，我们提出了补偿对数调整损失，以鼓励原型的大间隔，并缓解基本类别内部以及基本类别和新类别之间的不平衡。为了实现高效的适应性，我们将CLIP模型视为黑盒，并利用提取的特征来获得用于预测的视觉和文本原型。为了充分利用多模态信息，我们还提出了跨模态注意力机制来丰富来自两种模态的特征。为了实现有效的泛化，我们为新类别引入了虚拟原型，以弥补它们缺乏训练图像的不足。Candle在11个不同数据集上的大量实验中取得了最先进的结果，同时大大减少了训练时间，证明了我们方法的优越性。源代码可在https://github.com/shijxcs/Candle获取。|
|**2024-06-18**|[RS-GPT4V: A Unified Multimodal Instruction-Following Dataset for Remote Sensing Image Understanding](http://arxiv.org/abs/2406.12479)|**[link](https://github.com/geox-lab/rs-gpt4v)**|遥感图像智能理解模型正在经历一场由多模态大语言模型 (MLLM) 推动的深刻范式转变，即从学习领域模型 (LaDM) 的范式转变为学习预训练的通用基础模型，然后再进行领域自适应模型 (LaGD) 的范式。在新的 LaGD 范式下，过去十年推动遥感图像智能理解发展的旧数据集已不再适用于全新的任务。我们认为，必须设计一个新的数据集来简化任务，该数据集应具有以下特征：1）泛化性：训练模型学习任务之间的共享知识并适应不同的任务；2）复杂场景理解：训练模型理解感兴趣目标的细粒度属性，并能够用自然语言描述场景；3）推理能力：训练模型以实现高级视觉推理。在本文中，我们利用 GPT-4V 和现有数据集设计了一个高质量、多样化和统一的多模态指令遵循数据集，用于遥感图像理解，我们称之为 RS-GPT4V。为了实现泛化性，我们使用从 GPT-4V 通过指令遵循推导出的（问题，答案）来统一诸如字幕生成和目标定位等任务；为了实现复杂场景理解，我们提出了一种结合局部策略和全局策略的分层指令描述方法，其中局部策略描述了目标对象的细粒度属性及其空间关系，全局策略将所有局部信息整合起来生成详细的指令描述；为了实现推理能力，我们设计了多轮问答对，为模型提供推理能力。实验结果表明，通过 RS-GPT4V 微调的 MLLM 可以描述细粒度的信息。该数据集可在以下网址获取：https://github.com/GeoX-Lab/RS-GPT4V。|
|**2024-06-18**|[VRSBench: A Versatile Vision-Language Benchmark Dataset for Remote Sensing Image Understanding](http://arxiv.org/abs/2406.12384)|**[link](https://github.com/lx709/vrsbench)**|我们引入了一个新的基准，旨在推进用于遥感图像的通用、大规模视觉语言模型的开发。尽管已经提出了一些遥感领域的视觉语言数据集来实现这一目标，但现有数据集通常针对单一任务，缺乏详细的对象信息，或者质量控制不足。为了探索这些改进机会，我们提出了一个用于遥感图像理解的多功能视觉语言基准，称为VRSBench。该基准包含29,614张图像，其中包括29,614条经过人工验证的详细描述、52,472个对象参考和123,221个问答对。它有助于在广泛的遥感图像理解任务中训练和评估视觉语言模型。我们进一步在这个基准上评估了最先进的模型在三个视觉语言任务上的表现：图像描述、视觉定位和视觉问答。我们的工作旨在为遥感领域先进视觉语言模型的开发做出重大贡献。数据和代码可在https://github.com/lx709/VRSBench获取。|
|**2024-06-17**|[LLaNA: Large Language and NeRF Assistant](http://arxiv.org/abs/2406.11840)|null|多模态大型语言模型 (MLLM) 在理解图像和 3D 数据方面表现出色。然而，这两种模态在整体捕捉物体的外观和几何形状方面都存在缺陷。同时，神经辐射场 (NeRF) 已成为一种日益普遍的模态，它将信息编码在简单多层感知器 (MLP) 的权重中，可以同时编码物体的几何形状和逼真的外观。本文研究了将 NeRF 纳入 MLLM 的可行性和有效性。我们创建了 LLaNA，这是第一个能够执行 NeRF 描述和问答等新任务的通用 NeRF 语言助手。值得注意的是，我们的方法直接处理 NeRF 的 MLP 权重以提取有关所表示对象的信息，而无需渲染图像或实例化 3D 数据结构。此外，我们构建了一个包含 NeRF 数据集以及用于各种 NeRF 语言任务的文本注释，无需人工干预。基于此数据集，我们开发了一个基准来评估我们方法的 NeRF 理解能力。结果表明，处理 NeRF 权重比从 NeRF 中提取 2D 或 3D 表示表现更好。||
|**2024-06-17**|[MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs](http://arxiv.org/abs/2406.11833)|**[link](https://github.com/liuziyu77/mmdu)**|生成自然且有意义的响应以与多模态人类输入进行交流是大视觉语言模型 (LVLM) 的一项基本能力。虽然当前的开源 LVLM 在简化场景（例如单轮单图像输入）中表现出良好的性能，但它们在现实世界的对话场景中却表现不佳，例如在具有多轮和多图像的长上下文历史中遵循指令。现有的 LVLM 基准主要集中在单项选择题或简短回答上，这些并不能充分评估 LVLM 在现实世界人机交互应用中的能力。因此，我们引入了 MMDU，一个综合基准测试，以及 MMDU-45k，一个大规模指令微调数据集，旨在评估和改进 LVLM 在多轮和多图像对话中的能力。我们采用聚类算法从开源维基百科中找到相关的图像和文本描述，并在 GPT-4o 模型的帮助下由人工标注者构建问答对。MMDU 最多包含 18k 个图像+文本标记、20 张图像和 27 轮对话，这比以前的基准测试至少长 5 倍，对当前的 LVLM 构成了挑战。我们使用 MMDU 对 15 个代表性 LVLM 进行的深入分析表明，由于对话指令微调数据有限，开源 LVLM 落后于闭源 LVLM。我们证明，在 MMDU-45k 上对开源 LVLM 进行微调可以显著缩小这一差距，生成更长、更准确的对话，并提高 MMDU 和现有基准测试的得分（MMStar：+1.1%，MathVista：+1.5%，ChartQA：+1.2%）。我们的贡献为弥合当前 LVLM 模型与现实应用需求之间的差距铺平了道路。该项目可在 https://github.com/Liuziyu77/MMDU 获取。||
|**2024-06-17**|[Unveiling Encoder-Free Vision-Language Models](http://arxiv.org/abs/2406.11832)|**[link](https://github.com/baaivision/eve)**|现有的视觉语言模型 (VLM) 主要依赖视觉编码器提取视觉特征，然后利用大型语言模型 (LLM) 执行视觉语言任务。然而，视觉编码器在抽象视觉表示（例如分辨率、纵横比和语义先验）方面设置了强烈的归纳偏差，这可能会阻碍 VLM 的灵活性和效率。训练接受无缝视觉和语言输入（即没有视觉编码器）的纯 VLM 仍然具有挑战性，并且很少被探索。实证观察表明，没有编码器的直接训练会导致收敛缓慢和巨大的性能差距。在这项工作中，我们弥合了基于编码器和无编码器模型之间的差距，并提出了一个简单而有效的训练方法来实现纯 VLM。具体来说，我们通过彻底的实验揭示了有效训练无编码器 VLM 的关键方面：(1) 在一个统一的解码器内桥接视觉语言表示；(2) 通过额外监督增强视觉识别能力。借助这些策略，我们推出了 EVE，这是一种可以高效训练和推理的无编码器视觉语言模型。值得注意的是，仅利用 35M 公开可访问的数据，EVE 就可以在多个视觉语言基准测试中与具有类似能力的基于编码器的 VLM 相媲美。它明显优于具有神秘训练程序和未公开训练数据的对应模型 Fuyu-8B。我们相信 EVE 为开发跨模态的纯解码器架构提供了一条透明且有效的途径。我们的代码和模型可在以下网址公开获取：https://github.com/baaivision/EVE。||
|**2024-06-17**|[On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning](http://arxiv.org/abs/2406.11823)|**[link](https://github.com/naver-ai/elva)**|近年来，语言和视觉助手的进步展现出令人印象深刻的能力，但缺乏透明度，限制了更广泛的研究和可重复性。虽然开源模型可以有效地处理一般的图像任务，但它们在处理复杂的视觉情境文本理解时面临着计算需求高的挑战。此类任务通常需要增加标记输入和大型视觉模块来利用高分辨率信息。如何在模型大小和数据重要性之间取得平衡仍然是一个悬而未决的问题。本研究旨在通过识别关键组件并创建具有受限推理成本的高效模型来重新定义视觉语言模型的设计。通过策略性地制定数据集、优化视觉模块和增强监督技术，我们在保持高性能的同时，显著提高了推理吞吐量。从 1.6 亿到 130 亿参数不等的模型的广泛实验为模型优化提供了见解。我们将在 https://github.com/naver-ai/elva 上完全开源我们的代码库、模型和数据集。||
|**2024-06-17**|[VideoLLM-online: Online Video Large Language Model for Streaming Video](http://arxiv.org/abs/2406.11816)|null|最近的大型语言模型增强了视觉能力，使其能够理解图像、视频和交错的视觉语言内容。然而，这些大型多模态模型的学习方法通常将视频视为预先确定的片段，这使得它们在处理流式视频输入方面效率较低。在本文中，我们提出了一种新颖的视频流学习 (LIVE) 框架，该框架支持在连续视频流中进行时间对齐、长上下文和实时对话。我们的 LIVE 框架包含实现视频流对话的综合方法，包括：（1）旨在对连续流输入执行语言建模的训练目标，（2）将离线时间注释转换为流对话格式的数据生成方案，以及（3）优化的推理管道，以加速模型在现实世界视频流中的响应速度。借助我们的 LIVE 框架，我们在 Llama-2/Llama-3 的基础上构建了 VideoLLM-online 模型，并展示了其在处理流式视频方面的显著优势。例如，平均而言，我们的模型可以在 A100 GPU 上以超过 10 FPS 的速度支持 5 分钟视频片段中的流式对话。此外，它还在公共离线视频基准测试中展示了最先进的性能，例如识别、字幕和预测。代码、模型、数据和演示已在 https://showlab.github.io/videollm-online 上提供。||
|**2024-06-17**|[LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning](http://arxiv.org/abs/2406.11815)|null|近年来，指令微调的大型多模态模型 (LMM) 在图像字幕和视觉问答等多项任务中取得了成功；然而，如何将这些模型应用于机器人领域仍然是一个开放性问题。以往用于机器人应用的 LMM 已经接受了大量语言和动作数据的训练，但它们在不同环境下的泛化能力往往不尽如人意。为了解决这个问题，我们引入了 LLARVA，这是一种使用新型指令微调方法训练的模型，该方法利用结构化提示来统一各种机器人学习任务、场景和环境。此外，我们还表明，预测中间二维表示（我们将其称为“视觉轨迹”）有助于进一步对齐机器人学习的视觉和动作空间。我们从 Open X-Embodiment 数据集中生成了 850 万个图像-视觉轨迹对，用于预训练我们的模型，并在 RLBench 模拟器中的 12 个不同任务以及实体 Franka Emika Panda 7 自由度机器人上进行了评估。我们的实验取得了良好的性能，表明 LLARVA 使用二维和语言表示，与其他几个当代基线模型相比表现出色，并且可以泛化到各种机器人环境和配置中。||
|**2024-06-17**|[See It from My Perspective: Diagnosing the Western Cultural Bias of Large Vision-Language Models in Image Understanding](http://arxiv.org/abs/2406.11665)|**[link](https://github.com/amith-ananthram/see-it-from-my-perspective)**|视觉-语言模型 (VLM) 可以用多种语言回答关于图像的查询。然而，除了语言之外，文化也会影响我们看待事物的方式。例如，来自西方文化的人更关注图像中的中心人物，而来自东方文化的人则更关注场景环境。在这项工作中，我们提出了一项新颖的研究，该研究证明并定位了 VLM 在图像理解中的西方偏见。我们使用文化多样性的图像和注释，在主观和客观视觉任务中评估大型 VLM。我们发现，在每个任务中，VLM 在西方子集上的表现优于东方子集。追踪这种偏见来源的对照实验强调了，即使推理是用英语进行的，在纯文本预训练中使用多样化的语言组合对于构建公平的 VLM 至关重要。此外，虽然以目标文化的语言进行提示可以减少偏见，但这并不能替代构建更能代表世界语言的人工智能。||
|**2024-06-17**|[Multimodal Learning To Improve Segmentation With Intraoperative CBCT & Preoperative CT](http://arxiv.org/abs/2406.11650)|null|术中医学影像，特别是锥束计算机断层扫描 (CBCT)，尽管视觉质量较低，但仍是促进计算机辅助介入治疗的重要工具。虽然这种降级的图像质量会影响下游分割，但高质量术前扫描的可用性为改进提供了潜力。在这里，我们考虑一种可以使用术前 CT 和术中 CBCT 扫描的情况，但是扫描之间的对齐（配准）并不完美。我们提出了一种多模态学习方法，融合了粗略对齐的 CBCT 和 CT 扫描，并研究了 CBCT 质量和错位（促进错位的仿射和弹性变换）对最终分割性能的影响。作为一个应用场景，我们专注于肝脏和肝脏肿瘤语义分割，并评估术中图像质量和错位对分割性能的影响。为此，将高质量、标记的 CT 定义为术前数据，并将其用作模拟术中 CBCT 的基础。我们表明，融合术前 CT 和模拟的术中 CBCT 大多可以提高分割性能，并且即使明显错位的术前数据也有可能提高分割性能。||
|**2024-06-17**|[AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic Manipulation](http://arxiv.org/abs/2406.11548)|null|对于机器人系统与现实世界物体进行稳定交互而言，反思和纠正错误的能力至关重要。观察到多模态大语言模型 (MLLM) 的泛化和推理能力，先前的方法旨在利用这些模型来相应地增强机器人系统。然而，这些方法通常侧重于使用额外的 MLLM 进行高级规划校正，而对失败样本的利用有限，无法校正低级接触姿态。为了解决这一差距，我们提出了一种自主交互校正 (AIC) MLLM，它利用先前的低级交互经验来校正 SE(3) 姿态预测。具体来说，AIC MLLM 最初经过微调，以获得姿态预测和反馈提示理解能力。我们通过与物体的交互精心设计了两种类型的提示指令：1) 视觉掩码，用于突出显示不可移动的部分以进行位置校正；2) 文本描述，用于指示旋转校正的潜在方向。在推理过程中，引入了反馈信息提取模块来识别失败原因，从而允许 AIC MLLM 使用相应的提示自适应地校正姿态预测。为了进一步增强操作稳定性，我们设计了一种测试时适应策略，使 AIC MLLM 能够更好地适应当前场景配置。最后，我们在模拟和现实环境中进行了广泛的实验来评估所提出的方法。结果表明，我们的 AIC MLLM 可以通过利用交互体验提示有效地纠正失败样本。现实世界的演示可以在 https://sites.google.com/view/aic-mllm 找到。||
|**2024-06-17**|[MedThink: Inducing Medical Large-scale Visual Language Models to Hallucinate Less by Thinking More](http://arxiv.org/abs/2406.11451)|null|当大型视觉语言模型（LVLM）应用于多模态医学生成任务时，它们会遇到严重的模型幻觉问题。这严重损害了模型的生成准确性，使得LVLM难以在现实世界的医疗场景中实施以协助医生进行诊断。增强下游医学生成任务的训练数据是解决模型幻觉问题的有效方法。此外，医学领域训练数据的有限可用性和隐私问题极大地阻碍了模型的准确性和泛化能力。在本文中，我们介绍了一种模仿人类认知过程来构建细粒度指令对的方法，并将思维链（CoT）的概念从推理场景应用于训练场景，从而提出了一种称为MedThink的方法。我们对各种LVLM的实验表明，我们专为医学领域量身定制的新型数据构建方法显着提高了模型在医学图像报告生成任务中的性能，并大大减少了幻觉。这项工作的所有资源将很快发布。||

<p align=right>(<a href=#updated-on-20240619>back to top</a>)</p>

## 6DOF Object Pose

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-06**|[Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking](http://arxiv.org/abs/2406.04316)|null|6D物体姿态估计是计算机视觉中一项至关重要但极具挑战性的任务，其面临的主要问题是大规模数据集的严重缺乏。这种匮乏阻碍了对模型性能的全面评估，限制了研究进展。此外，可用实例或类别的数量有限也限制了其应用。为了解决这些问题，本文提出了Omni6DPose，这是一个以对象类别多样性、规模大和对象材质多样性为特征的大型数据集。Omni6DPose主要分为三个部分：ROPE（真实6D物体姿态估计数据集），包含332K张图像，涵盖149个类别、581个实例的超过150万个标注；SOPE（模拟6D物体姿态估计数据集），由在混合现实环境中创建的475K张图像组成，使用深度模拟技术，对相同的149个类别、4162个实例进行了超过500万个标注；以及在ROPE和SOPE中使用的、经过手动对齐的真实扫描物体。由于存在大量的变化和歧义，Omni6DPose本身就极具挑战性。为了应对这一挑战，我们引入了GenPose++，这是对SOTA类别级姿态估计框架的增强版本，它包含两个关键改进：语义感知特征提取和基于聚类的聚合。此外，我们还提供了全面的基准分析，以评估先前方法在这个大规模数据集上在6D物体姿态估计和姿态跟踪方面的性能。|
|**2024-06-05**|[Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices](http://arxiv.org/abs/2406.02977)|null|随着机器人和增强现实应用越来越依赖于精确高效的6D物体姿态估计，边缘设备上的实时性能对于实现更具交互性和响应性的系统至关重要。我们提出的稀疏颜色代码网络（SCCN）体现了一种清晰简洁的流程设计，可以有效地满足这一需求。SCCN利用基本物体几何特征的稀疏性来加速透视n点（PnP）计算过程，对RGB图像中的目标物体进行像素级预测。此外，它引入了一种新颖的基于像素级几何的物体对称性表示，该表示与初始姿态预测无缝集成，有效地解决了对称物体的歧义性。值得注意的是，SCCN在NVIDIA Jetson AGX Xavier上分别实现了在基准LINEMOD数据集和遮挡LINEMOD数据集上每秒19帧（FPS）和6 FPS的估计速率，同时在这些速率下始终保持较高的估计精度。|
|**2024-05-19**|[Advancing 6-DoF Instrument Pose Estimation in Variable X-Ray Imaging Geometries](http://arxiv.org/abs/2405.11677)|**[link](https://github.com/cviviers/YOLOv5-6D-Pose)**|在微创手术中准确估计手术器械的 6 自由度 (6-DoF) 位姿可以显著改进治疗策略并最终改善手术结果。现有的深度学习方法已经取得了准确的结果，但它们需要针对每个对象定制方法，并且需要费力的设置和训练环境（通常扩展到广泛的模拟），同时缺乏实时计算能力。我们提出了一种用于 X 射线系统中 6-DoF 位姿估计任务的通用数据采集方法，一种新颖且通用的 YOLOv5-6D 位姿架构，用于准确快速地进行目标位姿估计，以及一种在考虑单目锥束 X 射线图像采集几何形状的情况下进行手术螺钉位姿估计的完整方法。所提出的 YOLOv5-6D 位姿模型在公共基准测试中取得了具有竞争力的结果，同时在 GPU 上的速度 considerably faster at 42 FPS。此外，该方法可以泛化到不同的 X 射线采集几何形状和语义图像复杂性，从而能够在不同领域进行准确的位姿估计。最后，所提出的方法在脊柱手术期间对骨螺钉位姿估计进行了测试，以用于计算机辅助引导。该模型通过 0.1 ADD-S 指标实现了 92.41% 的精度，展示了提高手术精度和患者预后的 promising approach。YOLOv5-6D 的代码可在 https://github.com/cviviers/YOLOv5-6D-Pose 公开获取。|
|**2024-05-18**|[PS6D: Point Cloud Based Symmetry-Aware 6D Object Pose Estimation in Robot Bin-Picking](http://arxiv.org/abs/2405.11257)|null|6D物体姿态估计在许多领域都发挥着至关重要的作用，特别是在工业工件抓取方面。针对锈蚀、高反射率和缺乏纹理等挑战，本文介绍了一种基于点云的姿态估计框架（PS6D）。PS6D专注于细长和多对称物体。它通过注意力引导的特征提取模块提取多尺度特征，设计了对称感知旋转损失和中心距离敏感平移损失来回归每个点到实例质心的姿态，然后使用两阶段聚类方法完成实例分割和姿态估计。来自Sil'eane和IPA数据集的物体以及工业实践中的典型工件被用于生成数据和评估算法。与最先进的方法相比，PS6D在F $_{1_{inst}}$ 上提高了11.5%，在召回率上提高了14.8%。PS6D的主要部分已部署到Mech-Mind的软件中，并在分拣实验中取得了91.7%的成功率，标志着其在工业姿态估计任务中的应用。|
|**2024-05-31**|[Deep Learning-Based Object Pose Estimation: A Comprehensive Survey](http://arxiv.org/abs/2405.07801)|**[link](https://github.com/cnjianliu/awesome-object-pose-estimation)**|物体姿态估计是计算机视觉中的一个基本问题，在增强现实和机器人领域有着广泛的应用。在过去的十年中，深度学习模型由于其卓越的精度和鲁棒性，已经逐渐取代了依赖于工程点对特征的传统算法。然而，当代方法仍然存在若干挑战，包括其对标记训练数据的依赖性、模型紧凑性、在挑战性条件下的鲁棒性以及泛化到未见过的新物体能力。最近缺少一项关于该领域不同方面取得的进展、突出挑战和未来发展方向的综述。为了填补这一空白，我们讨论了基于深度学习的物体姿态估计的最新进展，涵盖了该问题的所有三种形式，即实例级、类别级和未见过物体的姿态估计。我们的综述还涵盖了多种输入数据模态、输出姿态的自由度、物体属性和下游任务，为读者提供了对该领域的全面理解。此外，它还讨论了不同领域的训练范式、推理模式、应用领域、评估指标和基准数据集，并报告了当前最先进方法在这些基准数据集上的性能，从而帮助读者为其应用选择最合适的方法。最后，该综述指出了关键挑战，回顾了当前的趋势及其优缺点，并为未来的研究指明了有希望的方向。我们还将持续跟踪https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation上的最新工作。|
|**2024-05-02**|[IntervenGen: Interventional Data Generation for Robust and Data-Efficient Robot Imitation Learning](http://arxiv.org/abs/2405.01472)|null|模仿学习是一种很有前景的机器人控制策略训练范式，但这些策略可能会受到分布偏移的影响，即评估时的条件与训练数据中的条件不同。一种提高策略对分布偏移鲁棒性的流行方法是交互式模仿学习（即DAgger及其变体），其中人类操作员在策略执行过程中提供纠正性干预。然而，收集足够数量的干预措施以涵盖策略错误的分布对于人类操作员来说可能是一项繁重的任务。我们提出了IntervenGen（I-Gen），这是一种新颖的数据生成系统，它可以从少量的人类干预中自动生成大量具有丰富状态空间覆盖范围的纠正性干预。我们将I-Gen应用于4个模拟环境和1个具有物体姿态估计误差的物理环境，结果表明，它只需10次人类干预即可将策略鲁棒性提高多达39倍。视频和更多结果可在https://sites.google.com/view/intervengen2024上获得。|
|**2024-04-17**|[GeoReF: Geometric Alignment Across Shape Variation for Category-level Object Pose Refinement](http://arxiv.org/abs/2404.11139)|null|物体姿态精修对于鲁棒的物体姿态估计至关重要。先前的工作在实例级物体姿态精修方面取得了重大进展。然而，由于类别内较大的形状变化以及目标物体与形状先验之间的差异，类别级姿态精修是一个更具挑战性的问题。为了应对这些挑战，我们引入了一种新颖的类别级物体姿态精修架构。我们的方法集成了 HS 层和可学习的仿射变换，旨在增强几何信息的提取和对齐。此外，我们引入了一种跨点云变换机制，可以有效地融合不同的数据源。最后，我们通过结合形状先验信息进行平移和尺寸误差预测，突破了模型的极限。我们进行了大量的实验来证明所提出框架的有效性。通过广泛的定量实验，我们证明了在所有指标上都比基线方法有显着改进。|
|**2024-04-08**|[Learning a Category-level Object Pose Estimator without Pose Annotations](http://arxiv.org/abs/2404.05626)|null|三维物体姿态估计是一项具有挑战性的任务。以往的工作总是需要数千张带有标注姿态的物体图像来学习三维姿态对应关系，这对于标注来说既费力又耗时。在本文中，我们提出了一种无需姿态标注即可学习类别级三维物体姿态估计器的方法。我们没有使用手动标注的图像，而是利用扩散模型（例如 Zero-1-to-3）生成一组在受控姿态差异下的图像，并提出使用这些图像来学习我们的物体姿态估计器。直接使用原始的扩散模型会导致图像出现姿态噪声和伪影。为了解决这个问题，首先，我们利用从专门设计的对比姿态学习中学习到的图像编码器来过滤不合理的细节并提取图像特征图。此外，我们提出了一种新的学习策略，允许模型从那些生成的图像集中学习物体姿态，而无需知道其规范姿态的对齐方式。实验结果表明，我们的方法能够从单次拍摄设置（作为姿态定义）中进行类别级物体姿态估计，同时在少样本类别级物体姿态估计基准测试中显著优于其他最先进的方法。|
|**2024-03-28**|[Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation](http://arxiv.org/abs/2403.19527)|**[link](https://github.com/leeiieeo/ag-pose)**|类别级 6D 物体姿态估计旨在估计特定类别中未见过实例的旋转、平移和大小。在这个领域，基于密集对应的算法已经取得了领先的性能。然而，它们没有明确考虑不同实例的局部和全局几何信息，导致对具有显著形状变化的未见过实例的泛化能力较差。为了解决这个问题，我们提出了一种新的实例自适应和几何感知关键点学习方法，用于类别级 6D 物体姿态估计 (AG-Pose)，它包括两个关键设计：(1) 第一个设计是实例自适应关键点检测模块，它可以自适应地检测一组稀疏关键点，用于表示各种实例的几何结构。(2) 第二个设计是几何感知特征聚合模块，它可以有效地将局部和全局几何信息整合到关键点特征中。这两个模块可以协同工作，为未见过的实例建立鲁棒的关键点级对应关系，从而增强模型的泛化能力。在 CAMERA25 和 REAL275 数据集上的实验结果表明，所提出的 AG-Pose 在没有类别特定形状先验的情况下，大大优于现有技术方法。|
|**2024-06-01**|[Object Pose Estimation via the Aggregation of Diffusion Features](http://arxiv.org/abs/2403.18791)|**[link](https://github.com/tianfu18/diff-feats-pose)**|从图像中估计物体姿态是3D场景理解的关键任务，近期的方法在大型基准数据集上已经展现出可喜的成果。然而，这些方法在处理未知物体时性能会显著下降。我们认为这是由图像特征的泛化能力有限导致的。为了解决这个问题，我们深入分析了扩散模型（例如Stable Diffusion）的特征，这些模型在对未知物体建模方面具有巨大潜力。基于这一分析，我们创新性地将这些扩散特征引入到物体姿态估计中。为此，我们提出了三种不同的架构，可以有效地捕捉和聚合不同粒度的扩散特征，极大地提高了物体姿态估计的泛化能力。在三个流行的基准数据集LM、O-LM和T-LESS上，我们的方法以相当大的优势优于现有技术水平。特别是在未知物体上，我们的方法比之前最佳方法取得了更高的准确率：在Unseen LM上为98.2% vs. 93.5%，在Unseen O-LM上为85.9% vs. 76.3%，显示出我们方法强大的泛化能力。我们的代码已发布在https://github.com/Tianfu18/diff-feats-pose。|

<p align=right>(<a href=#updated-on-20240619>back to top</a>)</p>

## nerf

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-18**|[Fast Global Localization on Neural Radiance Field](http://arxiv.org/abs/2406.12202)|null|神经辐射场 (NeRF) 提出了一种表示场景的新方法，允许从 2D 图像进行高质量的 3D 重建。在其取得显著成就之后，NeRF 地图中的全局定位成为实现广泛应用的一项重要任务。最近，Loc-NeRF 展示了一种将传统的蒙特卡洛定位与 NeRF 相结合的定位方法，展示了使用 NeRF 作为环境地图的良好结果。然而，尽管取得了进步，Loc-NeRF 仍然面临着光线渲染过程耗时的挑战，这在实际应用中可能是一个很大的限制。为了解决这个问题，我们引入了 Fast Loc-NeRF，它利用从粗到精的方法来实现更高效、更准确的基于 NeRF 地图的全局定位。具体来说，Fast Loc-NeRF 将渲染的像素与观察到的图像从低分辨率到高分辨率进行多尺度匹配。因此，它在保持精确定位结果的同时，加快了代价高昂的粒子更新过程。此外，为了剔除异常粒子，我们提出了粒子拒绝加权方法，该方法利用 NeRF 的特性估计粒子的不确定性，并在粒子加权过程中考虑它们。我们的 Fast Loc-NeRF 在多个基准测试中创造了新的最先进的定位性能，证明了其准确性和效率。|
|**2024-06-17**|[DistillNeRF: Perceiving 3D Scenes from Single-Glance Images by Distilling Neural Fields and Foundation Model Features](http://arxiv.org/abs/2406.12095)|null|我们提出了 DistillNeRF，这是一个自监督学习框架，旨在解决自动驾驶中从有限的 2D 观察中理解 3D 环境的挑战。我们的方法是一个可泛化的前馈模型，可以从稀疏的单帧多视图相机输入中预测丰富的场景神经表示，并使用可微渲染进行自监督训练，以重建 RGB、深度或特征图像。我们的第一个见解是通过生成密集深度和虚拟相机目标来利用每个场景优化的神经辐射场 (NeRF)，从而帮助我们的模型从稀疏的非重叠图像输入中学习 3D 几何形状。其次，为了学习语义丰富的 3D 表示，我们建议从预训练的 2D 基础模型（如 CLIP 或 DINOv2）中提取特征，从而无需昂贵的 3D 人工标注即可实现各种下游任务。为了利用这两个见解，我们引入了一种新颖的模型架构，该架构具有两阶段提升-splat-shoot 编码器和参数化的稀疏分层体素表示。在 NuScenes 数据集上的实验结果表明，DistillNeRF 在场景重建、新视图合成和深度估计方面明显优于现有的可比自监督方法；并且它允许进行有竞争力的零样本 3D 语义占用预测，以及通过提取的基础模型特征进行开放世界场景理解。演示和代码将在 https://distillnerf.github.io/ 上提供。|
|**2024-06-17**|[Uncertainty modeling for fine-tuned implicit functions](http://arxiv.org/abs/2406.12082)|null|诸如神经辐射场 (NeRFs)、占据网络和符号距离函数 (SDFs) 等隐函数已成为计算机视觉中从稀疏视图重建详细物体形状的关键。由于输入的极度稀疏性和数据损坏引起的分布偏移，使用这些模型实现最佳性能可能具有挑战性。为此，大型的、无噪声的合成数据集可以作为形状先验，帮助模型填补空白，但必须谨慎对待由此产生的重建结果。不确定性估计对于评估这些重建的质量至关重要，特别是在识别模型对其从先验推断的部分不确定的区域方面。在本文中，我们介绍了 Dropsembles，这是一种用于调整后的隐函数中的不确定性估计的新方法。我们通过一系列实验证明了我们方法的有效性，从玩具示例开始，逐步发展到真实场景。具体来说，我们在合成的解剖数据上训练了一个卷积占据网络，并在腰椎的低分辨率 MRI 分割上对其进行了测试。我们的结果表明，Dropsembles 实现了深度集成方法的准确性和校准水平，但计算成本显著降低。|
|**2024-06-17**|[LLaNA: Large Language and NeRF Assistant](http://arxiv.org/abs/2406.11840)|null|多模态大型语言模型 (MLLM) 在理解图像和 3D 数据方面表现出色。然而，这两种模态在全面捕捉物体的外观和几何形状方面都存在缺陷。同时，神经辐射场 (NeRF) 已成为一种日益普遍的模态，它将信息编码在简单多层感知器 (MLP) 的权重中，可以同时编码物体的几何形状和逼真的外观。本文研究了将 NeRF 纳入 MLLM 的可行性和有效性。我们创建了 LLaNA，这是第一个能够执行 NeRF 图像描述和问答等新任务的通用 NeRF 语言助手。值得注意的是，我们的方法直接处理 NeRF 的 MLP 权重来提取有关表示对象的信息，而无需渲染图像或物化 3D 数据结构。此外，我们构建了一个包含 NeRF 数据集，其中包含针对各种 NeRF 语言任务的文本注释，无需人工干预。基于此数据集，我们开发了一个基准来评估我们方法的 NeRF 理解能力。结果表明，处理 NeRF 权重比从 NeRF 中提取 2D 或 3D 表示更有效。|
|**2024-06-17**|[InterNeRF: Scaling Radiance Fields via Parameter Interpolation](http://arxiv.org/abs/2406.11737)|null|神经辐射场 (NeRF) 在大型真实场景中具有无与伦比的保真度。扩展 NeRF 的一种常见方法是将场景划分为多个区域，每个区域都有自己的参数。如果采用简单实现，这种方法会受到测试时缩放性差以及外观和几何形状不一致的限制。我们提出了一种名为 InterNeRF 的新型架构，用于使用模型参数的子集来渲染目标视图。我们的方法支持核外训练和渲染，在仅略微增加训练时间的情况下增加了模型的总容量。我们展示了在多房间场景中的显著改进，同时在标准基准测试中保持竞争力。|
|**2024-06-17**|[Projecting Radiance Fields to Mesh Surfaces](http://arxiv.org/abs/2406.11570)|null|辐射场能够生成高保真度和高渲染速度的图像，但难以操作。我们结合了辐射场和网格表面的优点，有效地实现了不同外观之间的人物化身纹理迁移。我们使用三维高斯散射将源表示为辐射场，然后将高斯投影到目标网格上。我们的流程包括源预处理、目标矢量化和纹理投影。投影在纯CPU计算中只需1.12秒即可完成，而基线技术“逐面纹理投影”和“光线投射”分别需要31秒和4.1分钟。这种方法降低了计算需求，使其适用于从低端移动设备到高端计算机的更广泛设备。|
|**2024-06-16**|[Learning Relighting and Intrinsic Decomposition in Neural Radiance Fields](http://arxiv.org/abs/2406.11077)|null|从神经辐射场中提取内在成分（如反射率和阴影）的任务越来越受到关注。然而，当前的方法主要集中在合成场景和孤立物体上，忽略了具有背景的真实场景的复杂性。为了弥补这一差距，我们的研究引入了一种将重新照明与内在分解相结合的方法。通过利用场景中的光线变化来生成伪标签，我们的方法为内在分解提供了指导，而无需地面真实数据。我们的方法基于物理约束，确保了跨不同场景类型的鲁棒性，并减少了对预训练模型或手工先验的依赖。我们在合成数据集和真实世界数据集上验证了我们的方法，取得了令人信服的结果。此外，我们的方法在图像编辑任务中的适用性也展现出可喜的结果。|
|**2024-06-15**|[fNeRF: High Quality Radiance Fields from Practical Cameras](http://arxiv.org/abs/2406.10633)|null|近年来，神经辐射场的开发使得从多视图相机数据中对场景和物体进行逼真的三维重建达到了前所未有的水平。然而，以往的方法使用过于简化的针孔相机模型，导致散焦模糊被“烘焙”到重建的辐射场中。我们提出了一种对射线投射的改进，利用透镜的光学特性来增强存在散焦模糊时的场景重建。这使我们能够利用有限孔径的实际相机的测量结果来提高辐射场重建的质量。我们证明，与针孔模型和其他散焦模糊模型的近似相比，所提出的模型更接近地匹配了实际相机的散焦模糊行为，特别是在存在部分遮挡的情况下。这使我们能够实现更清晰的重建，在合成数据集和真实数据集上，将所有聚焦图像验证的 PSNR 提高了 3 dB。|
|**2024-06-15**|[Federated Neural Radiance Field for Distributed Intelligence](http://arxiv.org/abs/2406.10474)|null|新视角合成（NVS）是许多增强现实（AR）和虚拟现实（VR）应用中的重要技术。最近提出的神经辐射场（NeRF）方法在 NVS 任务中表现出优异的性能，并已应用于其他相关领域。然而，由于严格的法规和隐私问题，某些具有分布式数据存储的应用场景可能会给 NeRF 方法获取训练图像带来挑战。为了克服这一挑战，我们专注于 FedNeRF，这是一种基于联邦学习（FL）的 NeRF 方法，它利用不同数据所有者可用的图像，同时保护数据隐私。在本文中，我们首先构建了一个资源丰富、功能多样的联邦学习测试平台。然后，我们在这样一个实际的联邦学习系统中部署 FedNeRF 算法，并在部分客户端选择的情况下进行 FedNeRF 实验。预计本文提出的 FedNeRF 方法研究将有助于促进 NeRF 方法在分布式数据存储场景中的未来应用。|
|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|在非结构化旅游环境中拍摄的照片经常呈现出多变的外观和短暂的遮挡，这对精确的场景重建提出了挑战，并在新视角合成中导致了伪影。尽管先前的方法已经将神经辐射场 (NeRF) 与其他可学习模块相结合来处理动态外观和消除瞬态对象，但其大量的训练需求和缓慢的渲染速度限制了实际部署。最近，3D 高斯 splatting (3DGS) 已成为 NeRF 的一种很有前景的替代方案，它提供了卓越的训练和推理效率以及更好的渲染质量。本文介绍了 Wild-GS，这是一种针对不受约束的照片集优化的 3DGS 创新改编，同时保留了其效率优势。Wild-GS 通过每张图像的固有材质属性、全局照明和相机属性以及逐点反射率的局部方差来确定每个 3D 高斯的外观。与先前在图像空间中对参考特征进行建模的方法不同，Wild-GS 通过对从参考图像中提取的三平面进行采样，将像素外观特征显式对齐到相应的局部高斯。这种新颖的设计有效地将参考视图的高频细节外观转移到 3D 空间，并显着加快了训练过程。此外，利用 2D 可见性图和深度正则化分别减轻瞬态效应和约束几何形状。大量实验表明，Wild-GS 在所有现有技术中实现了最先进的渲染性能以及最高的训练和推理效率。|
|**2024-06-14**|[GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors](http://arxiv.org/abs/2406.10111)|null|从低分辨率输入视图实现高分辨率新视图合成 (HRNVS) 是一项具有挑战性的任务，因为缺乏高分辨率数据。以前的方法从低分辨率输入视图优化高分辨率神经辐射场 (NeRF)，但渲染速度缓慢。在这项工作中，我们基于 3D 高斯渲染 (3DGS) 开发了我们的方法，因为它能够以更快的渲染速度生成高质量的图像。为了缓解高分辨率合成的数据短缺问题，我们建议利用现成的 2D 扩散先验，通过分数蒸馏采样 (SDS) 将 2D 知识提取到 3D。然而，由于生成先验带来的随机性，将 SDS 直接应用于基于高斯的 3D 超分辨率会导致不希望和冗余的 3D 高斯基元。为了缓解这个问题，我们引入了两种简单而有效的技术来减少 SDS 引入的随机扰动。具体来说，我们 1) 使用退火策略缩小 SDS 中扩散时间步长的范围；2) 在密集化过程中随机丢弃冗余的高斯基元。大量实验表明，我们提出的 GaussainSR 可以在合成和真实世界数据集上仅使用低分辨率输入就能获得高质量的 HRNVS 结果。项目页面：https://chchnii.github.io/GaussianSR/||
|**2024-06-14**|[RaNeuS: Ray-adaptive Neural Surface Reconstruction](http://arxiv.org/abs/2406.09801)|**[link](https://github.com/wangyida/ra-neus)**|我们的目标是利用可微分的辐射场（例如 NeRF）来重建详细的 3D 表面，以及生成标准的新颖视图渲染。已经有相关的方法来执行此类任务，通常是利用带符号距离场 (SDF)。然而，最先进的方法仍然无法正确重建小尺度细节，例如树叶、绳索和纺织品表面。考虑到不同的方法通过全局常数 Eikonal 正则化来制定和优化从 SDF 到辐射场的投影，我们通过逐射线加权因子进行改进，以优先考虑渲染和零交叉表面拟合，并在建立完美 SDF 的基础上进行。我们建议自适应地调整带符号距离场上的正则化，以便不令人满意的渲染光线不会强制执行无效的强 Eikonal 正则化，并允许来自具有良好学习辐射的区域的梯度有效地反向传播到 SDF。因此，平衡这两个目标以生成准确和详细的表面。此外，关于 SDF 中的零交叉表面和辐射场中的渲染点之间是否存在几何偏差，投影也会根据优化期间不同的 3D 位置进行调整。我们提出的 RaNeuS 在合成数据集和真实数据集上都进行了广泛的评估，在新颖视图合成和几何重建方面均取得了最先进的结果。||
|**2024-06-13**|[Neural NeRF Compression](http://arxiv.org/abs/2406.08943)|null|神经辐射场 (NeRFs) 已成为通过连续体积表示捕获详细 3D 场景的强大工具。最近的 NeRF 利用特征网格来提高渲染质量和速度；然而，这些表示引入了大量的存储开销。本文提出了一种有效压缩基于网格的 NeRF 模型的新方法，解决了存储开销问题。我们的方法基于非线性变换编码范式，采用神经压缩来压缩模型的特征网格。由于缺乏涉及许多独立同分布场景的训练数据，我们为单个场景设计了一种无编码器、端到端优化的轻量级解码器方法。为了利用潜在特征网格的空间不均匀性，我们引入了重要性加权的率失真目标函数和采用掩蔽机制的稀疏熵模型。我们的实验结果表明，我们提出的方法在基于网格的 NeRF 压缩效率和重建质量方面优于现有工作。||

<p align=right>(<a href=#updated-on-20240619>back to top</a>)</p>

## 分类/检测/识别/分割

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-18**|[LayerMerge: Neural Network Depth Compression through Layer Pruning and Merging](http://arxiv.org/abs/2406.12837)|**[link](https://github.com/snu-mllab/layermerge)**|最近的研究表明，减少卷积神经网络中的层数可以在保持网络性能的同时提高效率。现有的深度压缩方法移除冗余的非线性激活函数，并将连续的卷积层合并为一层。然而，这些方法存在一个严重的缺陷：合并后的层的内核大小会变大，这大大削弱了减少网络深度所带来的延迟降低。我们发现，这个问题可以通过联合剪枝卷积层和激活函数来解决。为此，我们提出了LayerMerge，这是一种新颖的深度压缩方法，它选择要移除的激活层和卷积层，以在最小化性能损失的同时实现所需的推理速度提升。由于相应的选择问题涉及指数级的搜索空间，我们提出了一个新的代理优化问题，并通过动态规划有效地解决了它。实验结果表明，我们的方法在图像分类和生成任务的各种网络架构上始终优于现有的深度压缩和层剪枝方法。我们在https://github.com/snu-mllab/LayerMerge上发布了代码。|
|**2024-06-18**|[Privacy Preserving Federated Learning in Medical Imaging with Uncertainty Estimation](http://arxiv.org/abs/2406.12815)|**[link](https://github.com/niko-k98/awesome-list-federated-learning-review)**|机器学习 (ML) 和人工智能 (AI) 推动了显著的进步，特别是在医疗保健领域。在医学影像方面，机器学习模型有望改进疾病诊断、治疗计划和治疗后监测。各种计算机视觉任务，如图像分类、目标检测和图像分割，有望成为临床分析的常规手段。然而，围绕患者数据的隐私问题阻碍了构建开发和训练准确、稳健和可泛化模型所需的大型训练数据集。联邦学习 (FL) 作为一种引人注目的解决方案应运而生，它使各机构能够通过共享模型训练信息（梯度）而不是数据（例如医学图像）来协作进行机器学习模型训练。联邦学习的分布式学习框架促进了机构间的协作，同时保护了患者隐私。然而，联邦学习虽然在隐私保护方面很强大，但也面临着一些挑战。在模型训练过程中，从机构之间传递的共享梯度中仍然可以收集到敏感信息。此外，在医学影像中，由于数据中存在噪声和伪影，因此准确量化模型置信度/不确定性至关重要。由于各机构之间的数据异构性，联邦学习中的不确定性估计遇到了独特的障碍。本文全面回顾了联邦学习、隐私保护和不确定性估计，重点关注医学影像。除了对当前研究进行综述外，我们还指出了该领域的差距，并为联邦学习研究提出了未来方向，以增强隐私并应对嘈杂的医学影像数据挑战。|
|**2024-06-18**|[Online Anchor-based Training for Image Classification Tasks](http://arxiv.org/abs/2406.12662)|null|在本文中，我们致力于提升深度学习模型在图像分类任务上的性能，提出了一种名为“在线基于锚点的训练”（OAT）的新型基于锚点的训练方法。OAT 方法受到基于锚点的目标检测方法的启发，建议训练模型学习相对于定义锚点的类别标签的百分比变化，而不是直接学习类别标签。我们将模型输出处的批次中心定义为锚点。然后，在测试阶段，将预测转换回原始类别标签空间，并评估性能。OAT 方法的有效性在四个数据集上得到了验证。|
|**2024-06-18**|[Structured Detection for Simultaneous Super-Resolution and Optical Sectioning in Laser Scanning Microscopy](http://arxiv.org/abs/2406.12542)|null|快速且灵敏的探测器阵列实现了图像扫描显微镜 (ISM)，克服了共聚焦显微镜中空间分辨率和信噪比 (SNR) 之间的典型权衡问题。然而，目前的 ISM 方法无法提供光学切片，并且在处理厚样本时会失败，除非限制探测器的尺寸。因此，光学切片和 SNR 之间的另一个权衡仍然存在。在这里，我们提出了一种没有缺点的方法，它结合了不折不扣的超分辨率、高信噪比和光学切片。此外，我们的方法可以对图像进行超采样，将奈奎斯特准则放宽了两倍。基于这样的观察，即使用探测器阵列成像本身就嵌入了样本的轴向信息，我们设计了一种简单的重建算法，可以反转 ISM 的物理模型。我们提出了全面的理论框架，并使用配备单光子雪崩二极管 (SPAD) 阵列探测器的定制装置捕获的生物样本的合成和实验图像验证了我们的方法。我们证明了我们的方法在激发线性和非线性状态下的荧光发射方面的可行性。此外，我们将算法推广到荧光寿命成像，充分利用了 SPAD 阵列探测器的单光子计时能力。我们的方法优于传统的 ISM 方法，并且可以扩展到任何 LSM 技术。|
|**2024-06-18**|[ViDSOD-100: A New Dataset and a Baseline Model for RGB-D Video Salient Object Detection](http://arxiv.org/abs/2406.12536)|**[link](https://github.com/jhl-det/rgbd_video_sod)**|随着深度传感器的快速发展，越来越多的RGB-D视频得以获取。识别RGB-D视频中的前景是一项基础且重要的任务。然而，现有的显著性目标检测（SOD）工作只关注静态RGB-D图像或RGB视频，忽略了RGB-D和视频信息的协同作用。在本文中，我们首先收集了一个新的带注释的RGB-D视频SOD（ViDSOD-100）数据集，该数据集包含100个视频，共计9,362帧，这些视频采集自不同的自然场景。每个视频中的所有帧都经过人工标注，具有高质量的显著性标注。此外，我们提出了一个新的基线模型，名为注意力三重融合网络（ATF-Net），用于RGB-D视频显著性目标检测。我们的方法通过设计三个模态特定分支和一个多模态融合分支来聚合来自输入RGB图像的外观信息、来自估计运动图的时空信息以及来自深度图的几何信息。模态特定分支提取不同输入的表示，而多模态融合分支通过引入编码器特征聚合（MEA）模块和解码器特征聚合（MDA）模块来组合多级模态特定特征。在我们新引入的ViDSOD-100数据集和完善的DAVSOD数据集上进行的实验结果突出了所提出的ATF-Net的优越性能。这种性能的提升在定量和定性上都得到了证明，超过了当前最先进技术在各个领域的能力，包括RGB-D显著性检测、视频显著性检测和视频目标分割。我们的数据和代码可在github.com/jhl-Det/RGBD_Video_SOD获取。|
|**2024-06-18**|[LFMamba: Light Field Image Super-Resolution with State Space Model](http://arxiv.org/abs/2406.12463)|null|近年来，由于现代神经网络的进步，光场图像超分辨率（LFSR）取得了显著进展。然而，这些方法在捕获远程依赖关系（基于CNN）或遇到二次计算复杂度（基于Transformer）方面经常面临挑战，这限制了它们的性能。最近，具有选择性扫描机制（S6）的状态空间模型（SSM），以Mamba为例，已成为各种视觉任务中优于传统基于CNN和基于Transformer的方法的替代方案，这得益于其有效的远程序列建模能力和线性时间复杂度。因此，将S6集成到LFSR变得很有吸引力，特别是考虑到4D光场的巨大数据量。然而，主要挑战在于\emph{为4D光场设计一种有效的扫描方法，以有效地对光场特征进行建模}。为了解决这个问题，我们在4D LF的信息丰富的2D切片上采用SSM，以充分探索空间上下文信息、互补的角度信息和结构信息。为此，我们精心设计了一个基本的SSM块，其特征是高效的SS2D机制，有助于在这些2D切片上进行更有效和高效的特征学习。基于以上两种设计，我们进一步介绍了一种用于LFSR的基于SSM的网络，称为LFMamba。在LF基准数据集上的实验结果证明了LFMamba的优越性能。此外，还进行了广泛的消融研究，以验证我们提出的方法的有效性和泛化能力。我们希望我们的LFMamba能够为利用状态空间模型进行LF的有效表示学习提供启示。|
|**2024-06-18**|[SDNIA-YOLO: A Robust Object Detection Model for Extreme Weather Conditions](http://arxiv.org/abs/2406.12395)|null|虽然当前基于深度学习的目标检测模型在许多传统基准数据集上取得了优异的结果，但它们在极端条件下拍摄的真实世界图像上的性能会显著下降。现有方法要么使用基于传统图像处理算法的图像增强，要么应用定制的、场景受限的图像自适应技术来进行鲁棒建模。因此，本研究提出了一种风格化数据驱动的图像自适应神经网络YOLO（SDNIA-YOLO），它通过自适应地提高图像质量和从神经风格迁移（NST）合成的图像中学习与极端天气条件相关的有价值信息来提高模型的鲁棒性。实验表明，与基线模型相比，所开发的SDNIA-YOLOv3在真实世界雾天（RTTS）和低光（ExDark）测试集上的mAP@.5显著提高了至少15%。此外，实验还突出了风格化数据在模拟极端天气条件方面的巨大潜力。所开发的SDNIA-YOLO在很大程度上保留了原生YOLO的优良特性，如端到端的一阶段、数据驱动和快速。|
|**2024-06-18**|[Competitive Learning for Achieving Content-specific Filters in Video Coding for Machines](http://arxiv.org/abs/2406.12367)|null|本文研究了联合优化特定于内容的后处理滤波器以将面向人类的视频/图像编解码器调整为适用于机器视觉任务的编解码器的功效。通过观察视频/图像编解码器产生的伪像是依赖于内容的，我们提出了一种基于竞争学习原理的新型训练策略。该策略以模糊的方式将训练样本动态分配给滤波器，从而进一步优化给定样本上的获胜滤波器。受模拟退火优化技术的启发，我们采用具有温度变量的 softmax 函数作为权重分配函数，以减轻随机初始化的影响。我们在一个在多功能视频编码 (VVC) 编解码器框架内利用多个后处理滤波器的系统上进行的评估表明，使用我们提出的策略训练的特定于内容的滤波器的优越性，特别是在图像分块处理时。使用 VVC 参考软件 VTM 12.0 作为锚点，在 OpenImages 数据集上进行的实验表明，与独立训练的滤波器相比，用于目标检测和实例分割任务的 BD 率降低分别从 -41.3% 和 -44.6% 提高到 -42.3% 和 -44.7%。滤波器使用统计数据与我们的假设一致，并强调了联合优化滤波器的内容和重建质量的重要性。我们的发现为进一步提高视频/图像编解码器的性能铺平了道路。|
|**2024-06-18**|[Certified ML Object Detection for Surveillance Missions](http://arxiv.org/abs/2406.12362)|null|本文介绍了一种无人机探测系统的开发过程，该系统包含一个机器学习目标检测组件。目的是达到可接受的性能目标，并根据ED 324 / ARP 6983标准（即将发布）的建议提供充分的证据，以增强对所设计系统可靠性的信心。|
|**2024-06-18**|[Unleashing the Potential of Open-set Noisy Samples Against Label Noise for Medical Image Classification](http://arxiv.org/abs/2406.12293)|null|在医学图像分类中，解决混合闭集和开集标签噪声的挑战在很大程度上仍未得到探索。与自然图像分类不同，自然图像分类通常将闭集和开集噪声样本与干净样本分开并分别处理，而医学图像分类由于类间相似性高，难以识别开集噪声样本，因此面临着困难。此外，现有方法没有充分利用开集噪声样本的潜力来减轻标签噪声，通常导致它们被排除在外或应用统一的软标签。为了解决这些问题，我们提出了一种扩展的噪声鲁棒对比和开集特征增强（ENCOFA）框架。ENCOFA 包括扩展的噪声鲁棒监督对比（ENSC）损失，它有助于区分不同类别的特征。ENSC 损失将开集噪声样本视为一个扩展类别，并通过用标签可靠性对对比对进行加权来减轻标签噪声。此外，我们开发了一个开集特征增强（OSFeatAug）模块，利用模型的额外容量来丰富开集样本的特征，以防止对噪声数据的过拟合。我们在一个合成噪声数据集和一个真实世界的噪声数据集上进行了实验。我们的结果表明 ENCOFA 的优越性以及利用开集噪声样本对抗标签噪声的有效性。|
|**2024-06-17**|[Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%](http://arxiv.org/abs/2406.11837)|**[link](https://github.com/zh460045050/vqgan-lc)**|在以 VQGAN 为代表的图像量化领域，图像被编码成从预定义大小的码本中提取的离散token。近期的研究进展，特别是 LLAMA 3，表明扩大码本可以显著提高模型性能。然而，VQGAN 及其衍生模型，如 VQGAN-FC（因子化码本）和 VQGAN-EMA，在扩大码本大小和提高码本利用率方面仍然面临挑战。例如，VQGAN-FC 只能学习最大大小为 16,384 的码本，在 ImageNet 上的利用率通常低于 12%。本文提出了一种新的图像量化模型，称为 VQGAN-LC（大型码本），它将码本大小扩展到 100,000，实现了超过 99% 的利用率。与之前优化每个码本条目的方法不同，我们的方法首先使用预训练的视觉编码器提取的 100,000 个特征初始化码本。然后，优化集中于训练一个投影器，使整个码本与 VQGAN-LC 中编码器的特征分布对齐。我们证明了我们的模型在各种任务上的性能优于现有模型，包括图像重建、图像分类、使用 GPT 的自回归图像生成，以及使用基于扩散和基于流的生成模型进行图像创作。代码和模型可在 https://github.com/zh460045050/VQGAN-LC 获取。||
|**2024-06-17**|[V3Det Challenge 2024 on Vast Vocabulary and Open Vocabulary Object Detection: Methods and Results](http://arxiv.org/abs/2406.11739)|null|在现实场景中检测物体是一项复杂的任务，因为它面临着各种挑战，包括物体类别的广泛性以及可能遇到以前未知或未见过的物体。 这些挑战使得开发公共基准和挑战成为必要，以推动物体检测领域的发展。 受之前COCO和LVIS挑战赛成功的启发，我们与第四届开放世界视觉研讨会：开放世界中的视觉感知学习（VPLOW）合作，在2024年美国西雅图举行的CVPR上组织了V3Det挑战赛2024。这项挑战赛旨在推动物体检测研究的边界，并鼓励该领域的创新。V3Det挑战赛2024包括两个赛道：1）海量词汇物体检测：该赛道侧重于从13204个类别的庞大集合中检测物体，测试检测算法识别和定位不同物体能力。 2）开放词汇物体检测：该赛道更进一步，要求算法从开放的类别集中检测物体，包括未知物体。 在接下来的部分中，我们将对参与者提交的解决方案进行全面总结和分析。 通过分析提出的方法和解决方案，我们旨在启发海量词汇和开放词汇物体检测的未来研究方向，推动该领域的进步。 挑战赛主页：https://v3det.openxlab.org.cn/challenge||
|**2024-06-17**|[YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection](http://arxiv.org/abs/2406.11641)|null|目前基于图像的无人机检测方法主要依赖于YOLOv5等通用目标检测算法。虽然这些算法能够有效地在均匀背景下识别无人机，但在复杂、纹理丰富的环境中往往表现不佳。在这种情况下，无人机可以无缝地融入背景，产生伪装效果，从而对检测质量产生负面影响。为了解决这个问题，我们提出了一种名为YOLO-FEDER FusionNet的新型深度学习架构。与传统方法不同，YOLO-FEDER FusionNet将通用目标检测方法与伪装目标检测技术的专业优势相结合，以增强无人机检测能力。对YOLO-FEDER FusionNet的全面评估表明，该模型效率高，并在减少漏检和误报方面都有显著改进。||
|**2024-06-17**|[Cross-domain Open-world Discovery](http://arxiv.org/abs/2406.11422)|null|在许多实际应用中，测试数据通常会出现类别偏移，其特点是出现新的类别，以及由于特征分布与模型训练时的特征分布不同而产生的分布偏移。然而，现有方法要么在开放世界环境中发现新类别，要么假设存在领域偏移但无法发现新类别。在这项工作中，我们考虑了一个跨领域开放世界发现设置，目标是在领域偏移的情况下将样本分配给已见类别并发现未见类别。为了解决这一挑战性问题，我们提出了 CROW，这是一种基于原型的方法，它引入了由基础模型结构良好的表示空间支持的“先聚类后匹配”策略。通过这种方式，CROW 通过将聚类与先前见过的类别进行稳健匹配来发现新类别，然后使用专为跨领域开放世界发现设计的目标函数对表示空间进行微调。在图像分类基准数据集上的大量实验结果表明，CROW 优于其他基线方法，在 75 个实验设置中平均性能提高了 8%。||
|**2024-06-17**|[A Dictionary Based Approach for Removing Out-of-Focus Blur](http://arxiv.org/abs/2406.11330)|null|随着深度学习模型的兴起，图像去模糊领域取得了巨大进步。这些模型虽然高效，但计算成本高，能源消耗大。基于字典学习的方法在图像去噪和单图像超分辨率方面已显示出良好的结果。我们提出将 Isidoro、Romano 和 Milanfar 提出的快速准确图像超分辨率 (RAISR) 算法扩展到散焦模糊去除任务。我们定义了一种清晰度质量度量，它与图像的感知质量非常吻合。还提出了一种基于资产配置管理的基于度量的混合策略。与流行的去模糊方法相比，我们的方法平均提高了约 13% (PSNR) 和 10% (SSIM)。此外，我们的混合方案减少了恢复后的振铃伪影。||
|**2024-06-17**|[Low-power Ship Detection in Satellite Images Using Neuromorphic Hardware](http://arxiv.org/abs/2406.11319)|null|将地球观测图像数据从卫星传输到地面站会消耗大量的电力和带宽。对于海上船舶检测，机载数据处理可以识别船舶并减少发送到地面的数据量。然而，大多数在轨捕获的图像只包含水体或陆地，空中客车船舶检测数据集显示只有 22.1% 的图像包含船舶。我们设计了一个低功耗的两阶段系统来优化性能，而不是依赖于单个复杂模型。第一阶段是一个轻量级的二元分类器，充当检测船舶存在的门控机制。此阶段运行在 BrainChip 的 Akida 1.0 上，它利用激活稀疏性来最大程度地减少动态功耗。第二阶段采用 YOLOv5 目标检测模型来识别船舶的位置和大小。这种方法实现了 76.9% 的平均精度 (mAP)，通过减少误报，仅在包含船舶的图像上评估时，该精度提高到 79.3%。此外，我们计算出在 NVIDIA Jetson Nano 设备上评估完整验证集需要 111.4 kJ 的能量。我们的两阶段系统将此能耗降低到 27.3 kJ，不到四分之一，证明了异构计算系统的效率。||
|**2024-06-17**|[Semi-Supervised Domain Adaptation Using Target-Oriented Domain Augmentation for 3D Object Detection](http://arxiv.org/abs/2406.11313)|**[link](https://github.com/rasd3/toda)**|三维目标检测对于自动驾驶和机器人等应用至关重要。然而，在现实环境中，由于传感器升级、天气变化和地理差异导致的传感器数据分布变化会对检测性能产生负面影响。半监督域适应（SSDA）旨在通过将知识从具有丰富标记数据的源域迁移到标记数据稀缺的目标域来应对这些挑战。本文提出了一种新的SSDA方法，称为面向目标的域增强（TODA），该方法专为基于激光雷达的三维目标检测而设计。TODA有效地利用了所有可用数据，包括源域中的标记数据以及目标域中的标记数据和未标记数据，以提高域适应性能。TODA由两个阶段组成：TargetMix和AdvMix。TargetMix采用混合增强技术，结合激光雷达传感器特性，以促进源域和目标域之间的特征对齐。AdvMix将逐点对抗性增强与混合增强相结合，扰动未标记数据，以对齐目标域中标记数据和未标记数据中的特征。我们在具有挑战性的域适应任务上进行的实验表明，TODA的性能明显优于现有的专为三维目标检测设计的域适应技术。代码可在以下网址获取：https://github.com/rasd3/TODA。||
|**2024-06-17**|[Syn-to-Real Unsupervised Domain Adaptation for Indoor 3D Object Detection](http://arxiv.org/abs/2406.11311)|null|在室内3D物体检测中使用合成数据，可以大大减少3D标注所需的人工，并训练出有效的零样本检测器。然而，跨合成到真实室内数据集的复杂域迁移问题仍未得到充分探索。本文提出了一种新的面向对象的层次化域对齐（OHDA）框架，用于室内3D物体检测的合成到真实无监督域自适应。我们的方法包括一种对象感知增强策略，以有效地使源域数据多样化，并且我们引入了一个由对抗训练分支和伪标签分支组成的双分支自适应框架，以便同时实现整体级别和类别级别的域对齐。针对室内无监督域自适应的特点，我们提出了两种方案来进一步优化伪标签。我们从合成数据集3D-FRONT到真实世界数据集ScanNetV2和SUN RGB-D的适应结果表明，与仅使用源数据的基线相比，mAP25分别显著提高了9.7%和9.1%，并且始终优于从2D和3D室外场景改编的方法。代码将在论文被接收后公开。||
|**2024-06-17**|[BaFTA: Backprop-Free Test-Time Adaptation For Zero-Shot Vision-Language Models](http://arxiv.org/abs/2406.11309)|null|像CLIP这样的大规模预训练视觉语言模型在不同领域展现出卓越的零样本图像分类能力。为了在保持零样本范式的情况下提高CLIP的性能，各种测试时提示调优方法被引入，通过无监督学习目标在推理过程中优化类别嵌入。然而，这些方法在选择合适的学习率以防止在测试时适应过程中缺乏验证数据导致的训练崩溃方面经常遇到挑战。在本研究中，我们提出了一种新的免反向传播算法BaFTA，用于视觉语言模型的测试时适应。我们的方法不是微调文本提示来优化类别嵌入，而是在对齐文本和视觉嵌入的投影嵌入空间内使用在线聚类直接估计类别中心点。我们通过使用Rényi熵评估每个预测的可靠性，动态聚合来自估计和原始类别嵌入以及不同增强视图的预测。通过大量实验，我们证明了BaFTA在有效性和效率方面始终优于最先进的测试时适应方法。||
|**2024-06-17**|[VideoVista: A Versatile Benchmark for Video Understanding and Reasoning](http://arxiv.org/abs/2406.11303)|null|尽管在大型多模态模型（LMM）的快速发展的推动下，视频分析取得了重大突破，但仍然缺乏一个通用的评估基准来全面评估这些模型在视频理解和推理方面的性能。为了解决这个问题，我们提出了 VideoVista，这是一个视频问答基准，它整合了跨越不同内容类别、持续时间和能力的挑战。具体来说，VideoVista 包含从 3,400 个视频中提取的 25,000 个问题，涵盖 14 个类别（例如，Howto、电影和娱乐），持续时间从几秒到超过 10 分钟不等。此外，它还包含 19 种理解任务（例如，异常检测、交互理解）和 8 种推理任务（例如，逻辑推理、因果推理）。为了实现这一点，我们提出了一个自动数据构建框架，利用强大的 GPT-4o 以及先进的分析工具（例如，视频分割、对象分割和跟踪）。我们还利用此框架构建训练数据，以增强与视频相关的 LMM（Video-LMM）的能力。通过对尖端模型进行全面和定量的评估，我们发现：1）Video-LMM 在涉及时间定位、对象跟踪和异常检测的细粒度视频任务中面临困难；2）Video-LMM 的逻辑和关系推理能力较差；3）开源 Video-LMM 的性能明显低于 GPT-4o 和 Gemini-1.5，落后 20 个百分点。这突出了 VideoVista 在推进能够准确理解视频并执行精确推理的 LMM 方面将发挥的关键作用。||

<p align=right>(<a href=#updated-on-20240619>back to top</a>)</p>

## 模型压缩/优化

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-18**|[Vernacular? I Barely Know Her: Challenges with Style Control and Stereotyping](http://arxiv.org/abs/2406.12679)|null|大型语言模型 (LLM) 越来越多地应用于教育和学习领域。研究表明，控制语言风格以适应学习者的需求可以促进理解、促进包容并有助于知识蒸馏。为了了解当代 LLM 在风格控制方面的能力和局限性，我们评估了五种最先进的模型：GPT-3.5、GPT-4、GPT-4o、Llama-3 和 Mistral-instruct-7B，评估内容涵盖两项风格控制任务。我们观察到，在第一项任务中存在显著的不一致性，模型性能平均在小学五年级到八年级的阅读水平之间（针对一年级学生的任务），标准差高达 27.6。对于第二项任务，我们观察到性能有统计学意义的显著提高，从 0.02 提高到 0.26。然而，我们发现即使在参考文本中没有刻板印象的情况下，LLM 在执行任务时也经常生成文化上不敏感的内容。我们对结果进行了全面的分析和讨论。|
|**2024-06-18**|[Federated Learning with a Single Shared Image](http://arxiv.org/abs/2406.12658)|**[link](https://github.com/sunnysoni97/single_image_fl)**|联邦学习 (FL) 支持多台机器协同训练机器学习模型，而无需共享私人训练数据。然而，尤其对于异构模型，一个关键瓶颈仍然是如何将每个客户端模型获得的知识传递给服务器。一种流行的方法 FedDF 使用知识蒸馏来解决这个问题，它使用一个通用的共享数据集来交换预测结果。但是，在许多情况下，由于隐私问题，这样的数据集可能难以获取，而且客户端可能不允许存储大型共享数据集。为此，我们在本文中介绍了一种新方法，改进了这种知识蒸馏方法，仅依赖于客户端和服务器之间共享的单个图像。具体来说，我们提出了一种新颖的自适应数据集剪枝算法，该算法从单个图像生成的作物中选择信息量最大的作物。由此，我们证明了在有限的共享数据集预算下，使用单个图像进行知识蒸馏的联邦学习比使用多个单独图像效果更好。最后，我们扩展了我们的方法，通过结合非均匀蒸馏策略和服务器端的客户端模型镜像来允许训练异构客户端架构。|
|**2024-06-18**|[From Instance Training to Instruction Learning: Task Adapters Generation from Instructions](http://arxiv.org/abs/2406.12382)|null|大型语言模型 (LLM) 通过利用指令微调 (IFT) 获得了解决一般任务的能力。然而，IFT 仍然严重依赖于大量任务数据的实例训练，这极大地限制了 LLM 对现实世界场景的适应性，在这些场景中，标记的任务实例稀少，而更广泛的任务泛化能力至关重要。与 LLM 相反，人类获得技能和完成任务不仅仅是通过反复练习，而且是通过理解和遵循指导方针。本文致力于模拟人类学习以解决实例训练的 shortcomings，重点关注指令学习以增强跨任务泛化能力。在此背景下，我们介绍了从指令生成任务适配器 (TAGI)，它可以在给定任务指令的情况下，以参数生成的方式自动构建特定于任务的模型，而无需针对未见过的任务进行重新训练。具体来说，我们利用知识蒸馏来增强通过指令学习开发的 TAGI 与通过实例训练开发的任务特定模型之间的一致性，方法是在它们之间对齐标签、输出 logits 和适配器参数。TAGI 通过包含超网络预训练和微调的两阶段训练过程被赋予了跨任务泛化能力。我们在 Super-Natural Instructions 和 P3 数据集上评估了 TAGI。实验结果表明，TAGI 可以匹配甚至优于传统的元训练模型和其他超网络模型，同时显着降低计算需求。|
|**2024-06-18**|[Enhancing Single-Slice Segmentation with 3D-to-2D Unpaired Scan Distillation](http://arxiv.org/abs/2406.12254)|null|二维单层腹部计算机断层扫描 (CT) 能够以低辐射剂量评估体型和器官健康状况。然而，单层数据需要使用二维网络进行分割，但这些网络通常难以有效捕捉上下文信息。因此，即使在相同数据集上训练，三维网络通常也能获得更好的分割结果。在这项工作中，我们提出了一种新颖的三维到二维的知识蒸馏框架，利用预训练的三维模型来增强二维单层分割。具体来说，我们从三维表示中提取预测分布的中心，通过学习类内和类间相关性来指导二维学生模型。与需要相同数据输入的传统知识蒸馏方法不同，我们的方法采用未配对的、具有任意对比度的三维 CT 扫描来指导二维学生模型。在来自单层巴尔的摩老龄化纵向研究 (BLSA) 数据集的 707 名受试者上进行的实验表明，最先进的二维多器官分割方法可以受益于三维教师模型，在单层多器官分割中实现更高的性能。值得注意的是，我们的方法在低数据情况下表现出相当高的效率，即使仅使用 200 名训练受试者，其性能也优于使用所有可用训练受试者训练的模型。因此，这项工作强调了减轻手动标注负担的潜力。|
|**2024-06-18**|[Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs for Information Retrieval](http://arxiv.org/abs/2406.12169)|null|最近的研究探索了从大型语言模型 (LLM) 中提取知识以优化检索模型，尤其是在检索增强生成 (RAG) 框架内。然而，大多数现有的训练方法依赖于从 LLM 的权重或输出概率中提取监督信号，这不仅资源密集型，而且与黑盒 LLM 不兼容。在本文中，我们介绍了“中间蒸馏”，这是一种数据高效的知识蒸馏训练方案，它将 LLM 视为黑盒，并通过创新的 LLM-排序器-检索器管道提取其知识，仅使用 LLM 的排名生成作为监督信号。大量实验表明，我们提出的方法可以显著提高检索模型的性能，只需 1,000 个训练实例。此外，我们蒸馏的检索模型显著提高了 RAG 框架内问答任务的性能，证明了 LLM 在经济高效地训练小型模型方面的潜力。|
|**2024-06-17**|[Mutual Learning for Finetuning Click-Through Rate Prediction Models](http://arxiv.org/abs/2406.12087)|null|点击率 (CTR) 预测已成为数字行业（如数字广告或在线购物）中的一项基本任务。许多基于深度学习的方法已被实施，并已成为该领域的最新模型。为了进一步提高 CTR 模型的性能，基于知识蒸馏的方法已被广泛使用。然而，目前大多数 CTR 预测模型都没有很复杂的架构，因此很难称其中一个模型“笨重”，而另一个模型“微小”。另一方面，复杂模型和简单模型之间的性能差距也不是很大。因此，将知识从一个模型提炼到另一个模型可能不值得付出努力。在这些考虑下，相互学习可能是一种更好的方法，因为所有模型都可以相互改进。在本文中，我们展示了相互学习算法在平等模型之间使用时的效果。在我们对 Criteo 和 Avazu 数据集的实验中，相互学习算法将模型的性能提高了高达 0.66% 的相对改进。|
|**2024-06-17**|[On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning](http://arxiv.org/abs/2406.11823)|**[link](https://github.com/naver-ai/elva)**|近年来，语言和视觉助手的进步展现出令人印象深刻的能力，但缺乏透明度，限制了更广泛的研究和可重复性。虽然开源模型可以有效地处理一般的图像任务，但它们在处理复杂的视觉定位文本理解任务时面临着计算量大的挑战。这类任务通常需要增加token输入和大型视觉模块来利用高分辨率信息。如何在模型规模和数据重要性之间取得平衡仍然是一个悬而未决的问题。本研究旨在通过识别关键组件并创建具有受限推理成本的高效模型来重新定义视觉语言模型的设计。通过策略性地制定数据集、优化视觉模块和增强监督技术，我们在保持高性能的同时，显著提高了推理吞吐量。从1.6亿到130亿参数不等的模型的广泛实验为模型优化提供了见解。我们将在https://github.com/naver-ai/elva 上完全开源我们的代码库、模型和数据集。|
|**2024-06-17**|[NLDF: Neural Light Dynamic Fields for Efficient 3D Talking Head Generation](http://arxiv.org/abs/2406.11259)|null|基于神经辐射场模型的说话头生成已经展现出良好的视觉效果。然而，由于NeRF需要对数百个采样点进行繁重的计算才能合成一个像素，其渲染速度缓慢严重限制了其应用。为了解决这一问题，本文提出了一种新的神经光动态场模型（NLDF），旨在实现高质量的三维说话人脸生成，并显著提高生成速度。NLDF基于光段表示光场，并使用深度网络一次性学习整个光束的信息。在学习过程中，应用了知识蒸馏技术，并使用基于NeRF的合成结果来指导NLDF中光段的正确着色。此外，本文还提出了一种新的主动池训练策略，以关注高频运动，特别是说话者的嘴巴和眉毛。该方法有效地表示了三维说话视频生成中的面部光线动态，与基于NeRF的最先进方法相比，其速度提高了约30倍，且生成视觉质量相当。|
|**2024-06-17**|[ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking](http://arxiv.org/abs/2406.11257)|**[link](https://github.com/gaffey/excp)**|大型语言模型（LLM）最近在人工智能领域引起了广泛关注。然而，这些模型的训练过程在计算和存储容量方面提出了重大挑战，因此压缩检查点已成为一个迫切问题。在本文中，我们提出了一种新颖的极限检查点压缩（ExCP）框架，该框架可显著减少训练检查点所需的存储空间，同时实现近乎无损的性能。我们首先计算相邻检查点的残差，以获得对更高压缩比至关重要的稀疏信息。为了进一步挖掘检查点中的冗余参数，我们提出了一种权重-动量联合缩减方法，以利用模型优化过程中的另一个重要信息，即动量。具体来说，我们利用模型和优化器的信息来尽可能多地丢弃参数，同时保留关键信息以确保最佳性能。此外，我们利用非均匀量化来进一步压缩检查点的存储空间。我们对从4.1亿到70亿参数不等的多个模型广泛评估了我们提出的ExCP框架，并展示了在保持强大性能的同时显著减少了存储空间。例如，我们对Pythia-410M模型实现了大约70倍的压缩，最终性能在各种下游任务上与原始模型一样准确。代码将在https://github.com/Gaffey/ExCP上提供。|
|**2024-06-17**|[STEVE Series: Step-by-Step Construction of Agent Systems in Minecraft](http://arxiv.org/abs/2406.11247)|null|以大型语言模型 (LLM) 为核心构建具身智能体系统是一个很有前景的方向。由于在现实世界中部署和训练此类智能体的巨大成本和不可控因素，我们决定在 Minecraft 环境中开始探索。我们的 STEVE 系列智能体可以在虚拟环境中完成基本任务以及更具挑战性的任务，例如导航甚至创造性任务，其效率远超以往最先进的方法 2.5 到 7.3 倍。我们首先使用一个原始的大型语言模型，为其增强了视觉编码器和在我们收集的高质量数据集 STEVE-21K 上训练的动作代码库。随后，我们使用 Critic 和记忆模块对其进行增强，将其转变为一个复杂的系统。最后，我们构建了一个分层的多个智能体系统。我们最近的工作探索了如何通过知识蒸馏来简化智能体系统。未来，我们将探索 STEVE 智能体在现实世界中的更多潜在应用。|
|**2024-06-16**|[Outlier Reduction with Gated Attention for Improved Post-training Quantization in Large Sequence-to-sequence Speech Foundation Models](http://arxiv.org/abs/2406.11022)|null|本文探讨了在 Whisper 语音基础模型系列中，知识蒸馏后训练后量化 (PTQ) 的改进。我们解决了权重和激活张量中异常值的问题，众所周知，这些异常值会影响基于 Transformer 的语言和视觉模型的量化质量。将这一观察结果扩展到 Whisper，我们证明了当基于 Transformer 的模型被训练用于执行自动语音识别时，这些异常值也存在，因此需要针对 PTQ 采取缓解策略。我们表明，最近提出的学生模型注意力块中的门控机制可以减少异常值，从而实现有效的 8 位量化，并且与没有采用门控机制的学生模型相比，可以降低词错误率。||
|**2024-06-16**|[Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions](http://arxiv.org/abs/2406.10861)|null|联邦学习 (FL) 是一种分布式且保护隐私的机器学习范式，它协调多个客户端训练模型，同时将原始数据保存在本地。然而，这种传统的联邦学习带来了一些挑战，包括隐私风险、数据异构性、通信瓶颈和系统异构性问题。为了应对这些挑战，知识蒸馏 (KD) 自 2020 年以来已广泛应用于联邦学习。知识蒸馏是一种经过验证且有效的模型压缩和增强算法。知识蒸馏的核心概念是通过在中间层或输出层交换 logits 来促进模型之间的知识转移。这些特性使知识蒸馏成为解决联邦学习中长期挑战的绝佳方案。迄今为止，很少有综述总结和分析知识蒸馏如何有效应用于联邦学习的当前趋势和方法。本文旨在全面概述基于知识蒸馏的联邦学习，重点关注解决上述挑战。首先，我们概述了基于知识蒸馏的联邦学习，包括其动机、基础知识、分类以及与传统联邦学习的比较以及知识蒸馏应该在哪里执行。我们还在附录中分析了基于知识蒸馏的联邦学习中的关键因素，包括教师、知识、数据和方法。我们讨论了知识蒸馏如何应对联邦学习中的挑战，包括隐私保护、数据异构性、通信效率和个性化。最后，我们讨论了基于知识蒸馏的联邦学习算法面临的挑战和未来的研究方向。我们希望本次调查能够为联邦学习领域的研究人员和从业者提供见解和指导。||
|**2024-06-14**|[Self-Knowledge Distillation for Learning Ambiguity](http://arxiv.org/abs/2406.09719)|null|近期的语言模型在自然语言理解（NLU）任务中展现出卓越的性能。然而，当面对可以多重解读的歧义样本时，它们往往表现不佳，过度自信地预测单一标签而未考虑其正确性。为了解决这个问题，我们提出了一种新颖的自我知识蒸馏方法，通过利用从模型底层蒸馏出的知识，使模型能够更准确地学习标签分布。这种方法还包括一个学习阶段，根据蒸馏出的分布知识，重新校准被判定为极其模糊的训练样本的不必要增强的置信度。我们在不同的 NLU 基准数据集上验证了我们的方法，实验结果证明了其在生成更好的标签分布方面的有效性。特别是，通过重新校准高度模糊样本的置信度的过程，显著缓解了未见样本的预测与其真实标签不匹配时的过度自信问题。这已被证明有助于生成比现有最先进方法更好的分布。此外，与现有方法相比，我们的方法在训练模型方面效率更高，因为它不涉及额外的训练过程来优化标签分布。||
|**2024-06-14**|[Frequency-mix Knowledge Distillation for Fake Speech Detection](http://arxiv.org/abs/2406.09664)|null|在电话场景中，用于对抗语音欺骗攻击的假语音检测 (FSD) 任务极具挑战性。数据增强 (DA) 方法被认为是解决电话场景中 FSD 任务的有效手段，通常分为时域和频域两个阶段。虽然每种方法都有其优势，但都可能导致信息丢失。为了解决这个问题，我们提出了一种新的数据增强方法，即频率混合 (Freqmix)，并引入了 Freqmix 知识蒸馏 (FKD) 来增强模型的信息提取和泛化能力。具体来说，我们使用 Freqmix 增强的数据作为教师模型的输入，而学生模型的输入则经过时域数据增强方法处理。我们使用多级特征蒸馏方法来恢复信息并提高模型的泛化能力。我们的方法在 ASVspoof 2021 LA 数据集上取得了最先进的结果，与基线相比提高了 31%，并在 ASVspoof 2021 DF 数据集上表现出竞争力。||
|**2024-06-13**|[RobustSAM: Segment Anything Robustly on Degraded Images](http://arxiv.org/abs/2406.09627)|null|Segment Anything Model (SAM)作为一种变革性的图像分割方法已经出现，它以其强大的零样本分割能力和灵活的提示系统而备受赞誉。然而，它的性能在处理低质量图像时会受到挑战。为了解决这一局限性，我们提出了Robust Segment Anything Model (RobustSAM)，它在保留SAM的可提示性和零样本泛化能力的同时，增强了其在低质量图像上的性能。我们的方法利用了预训练的SAM模型，仅增加了少量参数和计算需求。RobustSAM的额外参数可以在8个GPU上用30小时内完成优化，这证明了其对于典型研究实验室的可行性和实用性。我们还介绍了Robust-Seg数据集，这是一个包含688K图像-掩码对的集合，这些图像-掩码对具有不同的退化程度，旨在优化我们模型的训练和评估。跨多个分割任务和数据集的大量实验结果证实了RobustSAM的优越性能，特别是在零样本条件下，突出了其在广泛现实应用中的潜力。此外，我们的方法已被证明可以有效提高基于SAM的下游任务（如单图像去雾和去模糊）的性能。||
|**2024-06-13**|[Contextual Distillation Model for Diversified Recommendation](http://arxiv.org/abs/2406.09021)|null|推荐的多样性与准确性在改善用户体验方面同样重要。现有研究，例如行列式点过程（DPP）和最大边缘相关性（MMR），采用贪婪范式来迭代地选择同时优化准确性和多样性的项目。然而，先前的方法通常表现出二次复杂度，将其应用限制在重排序阶段，并且不适用于具有更大候选项目池的其他推荐阶段，例如预排序和排序阶段。在本文中，我们提出了上下文蒸馏模型（CDM），这是一种解决多样化的有效推荐模型，适用于工业推荐流程的所有阶段的部署。具体来说，CDM利用同一用户请求中的候选项目作为上下文来增强结果的多样性。我们提出了一种对比上下文编码器，它采用注意力机制来对正面和负面上下文进行建模。对于CDM的训练，我们将每个目标项目与其上下文嵌入进行比较，并利用知识蒸馏框架来学习MMR算法下每个目标项目的获胜概率，其中教师来自MMR输出。在推理过程中，排名是通过推荐模型得分和学生模型得分的线性组合来执行的，从而确保了多样性和效率。我们在两个工业数据集上执行离线评估，并在短视频平台快手上进行CDM的在线A/B测试。正如指标所示，在推荐质量和多样性方面观察到的显着增强，为CDM的有效性提供了强大的优势。||

<p align=right>(<a href=#updated-on-20240619>back to top</a>)</p>

## OCR

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-18**|[Sample-Based Matroid Prophet Inequalities](http://arxiv.org/abs/2406.12799)|null|我们研究了当分布未知且只能通过样本获取时的拟阵先知不等式。虽然已知针对特定拟阵的单样本先知不等式，但对于一般拟阵，即使样本数量为亚线性，也没有已知的具有恒定竞争比的算法。更重要的是，一般拟阵的单样本问题版本与长期存在的拟阵秘书猜想有着密切的（双向）联系。在这项工作中，我们给出了一种仅需 $O_\varepsilon(\mathrm{poly} \log n)$个样本的$(\frac14 - \varepsilon)$竞争比拟阵先知不等式。我们的算法由两部分组成：(i) 一种新颖的基于分位数的归约，将拟阵先知不等式归约为在线竞争解决方案 (OCRS)，仅需$O_\varepsilon(\log n)$个样本，以及 (ii) 一种$(\frac14 - \varepsilon)$可选的拟阵OCRS，仅需$O_\varepsilon(\mathrm{poly} \log n)$ 个样本，它仔细地解决了自适应性挑战。|
|**2024-06-17**|[GUICourse: From General Vision Language Models to Versatile GUI Agents](http://arxiv.org/abs/2406.11317)|**[link](https://github.com/yiye3/guicourse)**|利用图形用户界面（GUI）进行人机交互对于访问各种数字工具至关重要。视觉语言模型（VLM）的最新进展突出了开发多功能代理以帮助人类完成GUI导航任务的巨大潜力。然而，当前的VLM在基本能力（OCR和基础）和GUI知识（GUI元素的功能和控制方法）方面面临挑战，这阻碍了它们成为实用的GUI代理。为了解决这些挑战，我们贡献了GUICourse，这是一套用于从通用VLM训练基于视觉的GUI代理的数据集。首先，我们介绍了GUIEnv数据集，以增强VLM的OCR和基础能力。然后，我们介绍了GUIAct和GUIChat数据集，以丰富它们对GUI组件和交互的了解。实验表明，我们的GUI代理在常见的GUI任务上比其基线VLM具有更好的性能。即使是小尺寸的GUI代理（具有31亿个参数）仍然可以在单步和多步GUI任务上良好运行。最后，我们通过消融研究分析了该代理训练阶段的不同变体。我们的源代码和数据集已发布在https://github.com/yiye3/GUICourse。|
|**2024-06-17**|[Unifying Multimodal Retrieval via Document Screenshot Embedding](http://arxiv.org/abs/2406.11251)|null|在现实世界中，文档以不同的格式和多种模态组织。传统的检索流水线需要定制化的文档解析技术和内容提取模块来准备用于索引的输入。这个过程繁琐、容易出错，并且会导致信息丢失。为此，我们提出了文档截图嵌入（DSE），这是一种新颖的检索范式，将文档截图视为统一的输入格式，它不需要任何内容提取预处理，并保留文档中的所有信息（例如，文本、图像和布局）。DSE 利用大型视觉语言模型将文档截图直接编码为用于检索的密集表示。为了评估我们的方法，我们首先构建了 Wiki-SS 数据集，这是一个包含 130 万个维基百科网页截图的语料库，用于回答自然问题数据集中的问题。在这种文本密集型文档检索环境中，与其他依赖解析的文本检索方法相比，DSE 显示出具有竞争力的有效性。例如，在 top-1 检索精度方面，DSE 优于 BM25 17 个百分点。此外，在幻灯片检索的混合模态任务中，DSE 在 nDCG@10 指标上明显优于 OCR 文本检索方法 15 个百分点以上。这些实验表明，DSE 是一种有效的文档检索范式，适用于各种类型的文档。模型检查点、代码和 Wiki-SS 集合将被发布。|
|**2024-06-14**|[Enhancing Question Answering on Charts Through Effective Pre-training Tasks](http://arxiv.org/abs/2406.10085)|null|为了完全理解一个文档，仅使用文本信息是不够的。理解视觉线索，例如布局和图表，也是必要的。虽然目前最先进的文档理解方法（基于 OCR 和无 OCR 的方法）效果良好，但尚未对其功能和局限性进行彻底分析。因此，在这项工作中，我们解决了当前 VisualQA 模型应用于图表时存在的局限性。为了调查最先进模型的缺陷，我们以 ChartQA 为例进行了全面的行为分析。我们的研究结果表明，现有模型在回答与图表的结构和视觉上下文以及数值信息相关的问题时表现 particularly 不佳。为了解决这些问题，我们提出了三个简单的预训练任务，这些任务从结构-视觉知识及其对数值问题的理解两方面增强了现有模型。我们在三个图表数据集（包括抽取式和抽象式问题数据集）上评估了我们预训练的模型（称为 MatCha-v2），观察到它比基线模型平均提高了 1.7%。|
|**2024-06-14**|[OSPC: Detecting Harmful Memes with Large Language Model as a Catalyst](http://arxiv.org/abs/2406.09779)|null|作为网络上快速传播个人观点和立场的媒介，模因图片在传播社会偏见和歧视方面也构成了重大挑战。本研究提出了一种新的方法来检测有害模因图片，特别是在新加坡多元文化和多语言的背景下。我们的方法整合了图像描述、光学字符识别 (OCR) 和大型语言模型 (LLM) 分析，以全面理解和分类有害模因图片。该系统利用 BLIP 模型进行图像描述，PP-OCR 和 TrOCR 进行多种语言的文本识别，以及 Qwen LLM 进行细致入微的语言理解，能够识别以英语、中文、马来语和泰米尔语创建的模因图片中的有害内容。为了提高系统的性能，我们利用 GPT-4V 标记的额外数据对我们的方法进行了微调，旨在将 GPT-4V 对有害模因图片的理解能力提炼到我们的系统中。我们的框架在由新加坡人工智能举办的网络安全奖挑战赛的公开排行榜上名列前茅，AUROC 为 0.7749，准确率为 0.7087，远远领先于其他团队。值得注意的是，我们的方法优于之前的基准，FLAVA 的 AUROC 为 0.5695，VisualBERT 的 AUROC 为 0.5561。|
|**2024-06-12**|[M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation](http://arxiv.org/abs/2406.08255)|**[link](https://github.com/amazon-science/m3t-multi-modal-translation-bench)**|对于神经机器翻译 (NMT) 系统来说，文档翻译是一项挑战。大多数文档级 NMT 系统依赖于精心整理的句子级平行数据，假设可以从文档中完美地提取文本及其精确的阅读顺序。这些系统也倾向于忽略额外的视觉线索，例如文档布局，认为它们无关紧要。然而，现实世界的文档通常具有复杂的文本布局，这与这些假设相悖。从光学字符识别 (OCR) 或启发式规则中提取信息可能会导致错误，并且布局（例如，段落、标题）可能会传达文本不同部分之间的关系。这种复杂性在广泛使用的 PDF 文档中尤为明显，这些文档以视觉方式呈现信息。本文通过引入 M3T 来解决这一差距，M3T 是一个新颖的基准数据集，专为评估 NMT 系统在翻译半结构化文档的综合任务方面的性能而设计。该数据集旨在弥合文档级 NMT 系统中的评估差距，承认现实应用程序中丰富文本布局带来的挑战。|
|**2024-06-10**|[VCR: Visual Caption Restoration](http://arxiv.org/abs/2406.06462)|**[link](https://github.com/tianyu-z/vcr)**|我们引入了视觉字幕修复 (VCR)，这是一项新颖的视觉语言任务，它挑战模型使用图像中的像素级提示准确地恢复部分遮挡的文本。这项任务源于这样的观察：嵌入在图像中的文本与常见的视觉元素和自然语言有着本质的不同，因为它需要对视觉、文本和嵌入在图像中的文本模式进行对齐。虽然许多工作已经将嵌入在图像中的文本整合到视觉问答任务中，但这些任务的方法通常依赖于光学字符识别或掩码语言建模，从而将任务简化为主要基于文本的处理。然而，基于文本的处理在 VCR 中变得无效，因为准确的文本恢复依赖于来自提供的图像、上下文和来自掩码文本的微小暴露区域的微妙线索的组合信息。我们开发了一个管道，使用图像-字幕对生成 VCR 任务的合成图像，并可调节字幕可见性以控制任务难度。利用这个管道，我们使用来自维基百科的带有字幕的图像构建了一个名为 VCR-Wiki 的 VCR 数据集，包含 211 万个英文实体和 34.6 万个中文实体，分为简单和困难两种变体。我们的结果表明，当前的视觉语言模型在 VCR 任务中明显落后于人类的表现，仅仅在我们数据集上微调模型并不能带来显著的改进。我们发布了 VCR-Wiki 和数据构建代码，以促进未来的研究。|
|**2024-06-07**|[Scaling Automatic Extraction of Pseudocode](http://arxiv.org/abs/2406.04635)|null|学术论文中的伪代码提供了一种表达其中算法的简洁方法。伪代码也可以被认为是一种中间表示，有助于弥合编程语言和自然语言之间的差距。访问大量伪代码集合可以带来各种好处，从增强算法理解、促进进一步的算法设计，到支持基于 NLP 或计算机视觉的模型，用于自动代码生成和光学字符识别 (OCR) 等任务。我们通过从 arXiv 论文中提取近 320,000 个伪代码示例，创建了一个大型伪代码集合。这个过程涉及扫描超过 220 万篇学术论文，其中 1,000 篇经过人工检查和标记。鉴于集合固有的异质性，我们的方法包括一个为优化覆盖范围而定制的提取机制，以及一个基于随机抽样的验证机制，以检查其准确性和可靠性。此外，我们还提供了对常见伪代码结构的见解，并辅以聚群和统计分析。值得注意的是，这些分析表明伪代码的使用呈指数级增长，突出了它们日益增长的重要性。|
|**2024-06-06**|[CORU: Comprehensive Post-OCR Parsing and Receipt Understanding Dataset](http://arxiv.org/abs/2406.04493)|**[link](https://github.com/update-for-integrated-business-ai/coru)**|在光学字符识别 (OCR) 和自然语言处理 (NLP) 领域，集成多语言功能仍然是一项关键挑战，尤其是在考虑阿拉伯语等复杂文字时。本文介绍了综合性 OCR 后解析和收据理解数据集 (CORU)，这是一个专门设计用于增强多语言环境（包括阿拉伯语和英语）下收据的 OCR 和信息提取的新数据集。CORU 包含来自不同零售环境（包括超市和服装店）的 20,000 多张带注释的收据，以及 30,000 张用于 OCR 的带注释的图像，这些图像用于识别检测到的每一行，以及 10,000 个为详细信息提取而注释的项目。这些注释捕获了基本细节，例如商家名称、商品描述、总价、收据编号和日期。它们的结构支持三个主要的计算任务：目标检测、OCR 和信息提取。我们在 CORU 上建立了一系列模型的基线性能，以评估传统方法（如 Tesseract OCR）和更先进的基于神经网络的方法的有效性。这些基线对于处理现实世界收据中常见的复杂且嘈杂的文档布局以及推进自动多语言文档处理的现状至关重要。我们的数据集可公开访问 (https://github.com/Update-For-Integrated-Business-AI/CORU)。|
|**2024-06-03**|[Generalized Jersey Number Recognition Using Multi-task Learning With Orientation-guided Weight Refinement](http://arxiv.org/abs/2406.01033)|null|球衣号码识别 (JNR) 一直是体育分析中的一项重要任务。由于图像存在模糊、遮挡、变形和低分辨率等问题，提高识别精度仍然是一项持续的挑战。最近的研究已经使用数字定位和光学字符识别来解决这些问题。一些方法将球员识别方案应用于图像序列，而忽略了人体旋转角度对球衣数字识别的影响。通过使用多任务方案来识别每个数字，可以更准确地预测球衣数字的数量，从而获得更可靠的结果。基于上述考虑，本文提出了一种称为角度数字细化方案 (ADRS) 的多任务学习方法，该方法结合人体方向角度和数字线索来识别运动球衣号码。根据我们的实验结果，我们的方法增加了推理信息，显着提高了预测精度。与只能处理单一类型运动的现有技术方法相比，所提出的方法产生了更加多样化和实用的 JNR 应用。将足球、橄榄球、篮球、排球和棒球等多种类型的团队运动纳入我们的数据集中，极大地促进了体育分析中通用的 JNR。我们的准确率在 Top-1 上达到了 64.07%，在 Top-2 上达到了 89.97%，相应的 F1 分数分别为 67.46% 和 90.64%。|
|**2024-05-30**|[Scaling up archival text analysis with the blockmodeling of n-gram networks -- A case study of Bulgaria's representation in the Osservatore Romano (January-May 1877)](http://arxiv.org/abs/2405.20156)|null|本文旨在通过应用网络聚类方法分析 1877 年 1 月至 5 月期间出版的 123 期《罗马观察报》中对保加利亚的报道，从而弥合档案文本分析与网络分析之间的差距。本研究利用光学字符识别和广义同质性块模型构建相关关键词网络。包括“保加利亚”和“俄罗斯”这两个词集在内的网络结构基本相同，并且与“德国”、“英国”和“战争”的网络结构有很大程度的重叠。从结构上看，这两个网络的块模型呈现出清晰的核心-半边缘-边缘结构，反映了报纸报道中各概念之间的关系。该报的词汇选择有效地消解了保加利亚民族复兴运动的合法性，突出了罗马教廷对该报编辑路线的影响。||

<p align=right>(<a href=#updated-on-20240619>back to top</a>)</p>

## 生成模型

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-18**|[Evaluating the design space of diffusion-based generative models](http://arxiv.org/abs/2406.12839)|null|大多数现有的关于扩散模型精度理论研究虽然意义重大，但都假设分数函数已被逼近到一定精度，然后使用这个先验界限来控制生成误差。而本文则对整个生成过程，即训练和采样，提供了第一个量化的理解。更准确地说，它对梯度下降下的去噪分数匹配进行了非渐近收敛性分析。此外，还对变异爆炸模型的采样误差进行了改进分析。这两个结果的结合产生了一个完整的误差分析，它阐明了（同样是，但这次是从理论上）如何设计训练和采样过程以实现有效的生成。例如，我们的理论暗示了对噪声分布和损失权重的偏好，这与[Karras et al. 2022]中使用的定性一致。它还提供了一些关于为什么[Karras et al. 2022]中使用的时间和方差调度可以比[Song et al. 2020]中的先驱版本更好地调整的观点。|
|**2024-06-18**|[Neural Approximate Mirror Maps for Constrained Diffusion Models](http://arxiv.org/abs/2406.12816)|null|扩散模型擅长创建视觉上有说服力的图像，但它们往往难以满足训练数据中固有的细微约束。这些约束可以是基于物理的（例如，满足偏微分方程）、几何的（例如，保持对称性）或语义的（例如，包含特定数量的对象）。当训练数据都满足某个约束时，在扩散模型上强制执行此约束不仅可以提高其分布匹配精度，还可以使其在生成有效的合成数据和解决约束逆问题方面更加可靠。然而，现有的约束扩散模型方法对于不同类型的约束缺乏灵活性。最近的研究提出在由镜像映射定义的无约束空间中学习镜像扩散模型 (MDM)，并使用逆镜像映射施加约束，但对于复杂的约束，解析镜像映射难以推导。我们针对一般约束提出了神经近似镜像映射 (NAMM)。我们的方法只需要约束集的可微距离函数。我们学习一个将数据推送到无约束空间的近似镜像映射，以及一个将数据映射回约束集的相应近似逆映射。然后，可以在学习的镜像空间中训练生成模型（例如 MDM），并通过逆映射将其样本恢复到约束集。我们在各种约束条件下验证了我们的方法，结果表明，与无约束扩散模型相比，基于 NAMM 的 MDM 显着提高了约束满足度。我们还演示了如何轻松地将现有的基于扩散的逆问题求解器应用于学习的镜像空间，以解决约束逆问题。|
|**2024-06-18**|[AITTI: Learning Adaptive Inclusive Token for Text-to-Image Generation](http://arxiv.org/abs/2406.12805)|**[link](https://github.com/itsmag11/aitti)**|尽管文本到图像生成取得了高质量的结果，但在生成的内容中发现了刻板印象，这损害了生成模型的公平性。在这项工作中，我们建议学习自适应包容性标记来改变最终生成输出的属性分布。与现有的去偏方法不同，我们的方法既不需要明确的属性规范，也不需要预先了解偏差分布。具体来说，我们方法的核心是一个轻量级的自适应映射网络，它可以为要去偏的概念定制包容性标记，使标记可以推广到未见过的概念，而不管其原始偏差分布如何。这是通过使用锚定损失用少量平衡且包容的样本调整自适应映射网络来实现的。实验结果表明，我们的方法优于以前没有属性规范的偏差缓解方法，同时保留了生成结果和文本描述之间的一致性。此外，我们的方法实现了与需要特定属性或编辑方向才能生成的模型相当的性能。大量实验表明，我们的自适应包容性标记在减轻文本到图像生成中的刻板印象方面非常有效。代码将在 https://github.com/itsmag11/AITTI 上提供。|
|**2024-06-18**|[Extracting Training Data from Unconditional Diffusion Models](http://arxiv.org/abs/2406.12752)|null|随着扩散概率模型 (DPM) 作为生成式人工智能 (AI) 的主流模型得到应用，对其原始训练数据记忆的研究也引起了越来越多的关注。该方向上的现有工作旨在了解 DPM 是否或在多大程度上通过记忆进行学习。这种理解对于识别扩散模型中数据泄露和版权侵权的潜在风险，以及更重要的是，对于更可控地生成和可信地应用人工智能生成内容 (AIGC) 至关重要。尽管先前的工作已经对 DPM 何时容易发生记忆做出了重要观察，但这些发现大多是经验性的，并且开发的数据提取方法仅适用于条件扩散模型。在这项工作中，我们旨在通过以下方面建立对 DPM 记忆的理论理解：1) 用于理论分析的记忆度量，2) 对具有信息标签和随机标签的条件记忆的分析，以及 3) 两个用于衡量记忆的更好评估指标。基于理论分析，我们进一步提出了一种名为“代理条件数据提取 (SIDE)”的新型数据提取方法，该方法利用在生成数据上训练的分类器作为代理条件，直接从无条件扩散模型中提取训练数据。我们的实验结果表明，SIDE 可以从以前方法失败的扩散模型中提取训练数据，并且在不同规模的 CelebA 数据集上平均效率提高了 50% 以上。|
|**2024-06-18**|[SUPER: Selfie Undistortion and Head Pose Editing with Identity Preservation](http://arxiv.org/abs/2406.12700)|null|由于严重的扭曲导致面部特征变形以及头部姿势不当，近距离拍摄的自拍照可能看起来不自然甚至不美观。在本文中，我们提出了 SUPER，这是一种消除近距离面部裁剪中扭曲和调整头部姿势的新方法。我们通过优化相机参数和面部潜在代码对面部图像执行 3D GAN 反演，从而生成图像。此外，我们从获得的潜在代码估计深度，创建深度诱导的 3D 网格，并使用更新的相机参数对其进行渲染以获得扭曲的肖像。最后，我们应用基于可见性的混合，以便重新投影可见区域，并使用生成模型恢复遮挡部分。在面部去扭曲基准数据集和我们自己收集的头部旋转数据集 (HeRo) 上的实验表明，SUPER 在质量和数量上都优于以前的方法，为逼真的自拍编辑开辟了新的可能性。|
|**2024-06-18**|[Speak in the Scene: Diffusion-based Acoustic Scene Transfer toward Immersive Speech Generation](http://arxiv.org/abs/2406.12688)|null|本文介绍了语音生成领域的一项新任务，即声场景迁移（AST），旨在将语音信号的声场景迁移到不同的环境中。AST 通过将语音信号背后的声场景调整到所需的环境，有望在语音感知方面带来沉浸式体验。我们针对 AST 任务提出了 AST-LDM，它可以生成伴随参考提示目标声场景的语音信号。具体来说，AST-LDM 是一种潜在扩散模型，它以描述音频或文本模态目标声场景的 CLAP 嵌入为条件。本文的贡献包括引入 AST 任务并实现其基线模型。对于 AST-LDM，我们强调其核心框架，即保留输入语音并生成与给定语音和目标声环境一致的音频。包括客观和主观测试在内的实验验证了我们方法的可行性和有效性。|
|**2024-06-18**|[GeoBench: Benchmarking and Analyzing Monocular Geometry Estimation Models](http://arxiv.org/abs/2406.12671)|**[link](https://github.com/aim-uofa/geobench)**|近年来，判别性和生成性预训练的进步已经产生了具有强大泛化能力的几何估计模型。虽然判别性单目几何估计方法依赖于大规模微调数据来实现零样本泛化，但一些基于生成的方法通过利用预训练的扩散模型并在小规模合成训练数据上进行微调，显示出在未见场景中实现令人印象深刻的泛化性能的潜力。令人沮丧的是，这些模型在不同的数据集上使用不同的方法进行训练，因此很难找出决定评估性能的关键因素。此外，当前的几何评估基准存在两个主要缺点，可能会阻碍该领域的发展，即场景多样性有限和标签质量不佳。为了解决上述问题，（1）我们在统一的代码库中构建了公平且强大的基线，用于评估和分析几何估计模型；（2）我们在更具挑战性的几何估计任务基准上评估单目几何估计器，这些基准具有多样化的场景和高质量的标注。我们的结果表明，在相同的训练配置下，使用大量数据进行预训练的判别性模型（如 DINOv2）可以优于生成模型，即使后者使用少量高质量的合成数据，这表明微调数据质量比数据规模和模型架构更重要。我们的观察结果还提出了一个问题：如果仅使用少量合成深度数据微调 DINOv2 等通用视觉模型就能产生最先进的结果，那么我们真的需要复杂的生成模型来进行深度估计吗？我们相信这项工作可以推动几何估计任务以及广泛的下游应用的进步。|
|**2024-06-18**|[Research and Implementation of Data Enhancement Techniques for Graph Neural Networks](http://arxiv.org/abs/2406.12640)|null|数据、算法和算力是深度学习在应用领域取得成效的三个基础条件，数据是发展深度学习算法的重点。在实际工程应用中，一些数据受到获取条件的制约，无法获取更多的数据或者获取数据的成本过高，导致数据集规模较小（一般几百到几千个），数据量远小于大数据集的规模（几万到几十万个）。以上两种方法都是基于原始数据集进行生成，在原始数据量不足的情况下，可能无法反映真实环境的所有情况，例如真实环境的光照、轮廓等信息，如果数据量不够，则难以使用简单的变换或神经网络生成模型来生成所需的数据。本文的研究首先分析了图神经网络数据增强技术的关键点，同时深入介绍了图神经网络的组成基础，在此基础上对图神经网络的数据增强技术进行了优化和分析。|
|**2024-06-18**|[Learning Diffusion at Lightspeed](http://arxiv.org/abs/2406.12616)|null|扩散现象调节着大量自然过程以及许多成功生成模型的动力学。现有的从观测数据中学习扩散项的模型依赖于复杂的两级优化问题，并且只能正确地模拟系统的漂移。我们提出了一种新的简单模型JKOnet*，它完全绕过了现有架构的复杂性，同时呈现出显著增强的表示能力：JKOnet*可以恢复潜在的、交互的以及内部能量成分的底层扩散过程。JKOnet*最小化了一个简单的二次损失，以极快的速度运行，并且在实践中大大优于其他基线。此外，对于线性参数化泛函，JKOnet*提供了一个封闭形式的最优解。我们的方法论基于将扩散过程解释为概率空间中通过所谓的JKO方案实现的能量最小化轨迹，我们根据其一阶最优性条件，并根据概率空间优化方面的最新进展对其进行研究。|
|**2024-06-18**|[Unmasking the Veil: An Investigation into Concept Ablation for Privacy and Copyright Protection in Images](http://arxiv.org/abs/2406.12592)|**[link](https://github.com/vlgiitr/unmasking-the-veil)**|在本文中，我们扩展了对预训练模型中概念消融的研究，如 (Kumari et al.,2022) 在“文本到图像扩散模型中的概念消融”一文中所述。我们的工作重点是重现通过预定义指标提出和验证的不同概念消融变体所取得的结果。我们还介绍了一种新的概念消融变体，即“商标消融”。该变体结合了记忆和实例消融的原理，以解决专有或品牌元素对模型输出的细微影响。此外，我们的研究贡献还包括对模型局限性的观察分析。此外，我们还研究了模型对消融泄漏诱导提示的响应行为，该提示旨在间接消融概念，从而揭示模型的弹性和适应性。我们还观察到，对于远离其目标消融概念的概念所生成的图像，模型的性能会下降，这在附录中有所记录。|
|**2024-06-17**|[Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%](http://arxiv.org/abs/2406.11837)|**[link](https://github.com/zh460045050/vqgan-lc)**|在以 VQGAN 为代表的图像量化领域中，该过程将图像编码为从具有预定义大小的码本中提取的离散token。近期的进展，特别是 LLAMA 3，表明扩大码本可以显著提高模型性能。然而，VQGAN 及其衍生模型，如 VQGAN-FC（因子化码本）和 VQGAN-EMA，在扩大码本大小和提高码本利用率方面仍然面临挑战。例如，VQGAN-FC 被限制学习最大大小为 16,384 的码本，在 ImageNet 上的利用率通常低于 12%。在这项工作中，我们提出了一种名为 VQGAN-LC（大型码本）的新型图像量化模型，它将码本大小扩展到 100,000，实现了超过 99% 的利用率。与之前优化每个码本条目的方法不同，我们的方法首先使用由预训练视觉编码器提取的 100,000 个特征初始化码本。然后，优化集中于训练一个投影器，该投影器使整个码本与 VQGAN-LC 中编码器的特征分布对齐。我们证明了我们的模型在各种任务中优于其 counterparts，包括图像重建、图像分类、使用 GPT 的自回归图像生成，以及使用基于扩散和基于流的生成模型进行图像创建。代码和模型可在 https://github.com/zh460045050/VQGAN-LC 获取。||
|**2024-06-17**|[RetinaGS: Scalable Training for Dense Scene Rendering with Billion-Scale 3D Gaussians](http://arxiv.org/abs/2406.11836)|null|在这项工作中，我们探索了在大规模、高分辨率数据集上训练高参数 3D 高斯样条 (3DGS) 模型的可能性。我们为 3DGS 设计了一种通用的模型并行训练方法，称为 RetinaGS，它使用适当的渲染方程，可以应用于任何场景和任意分布的高斯基元。它使我们能够探索 3DGS 在基元数量和训练分辨率方面的缩放行为，这些行为以前难以探索，并超越了先前最先进的重建质量。当使用我们的方法增加基元数量时，我们观察到视觉质量明显提高的积极趋势。我们还首次尝试在完整 MatrixCity 数据集上训练具有超过 10 亿个基元的 3DGS 模型，该模型获得了良好的视觉质量。||
|**2024-06-17**|[Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models](http://arxiv.org/abs/2406.11831)|null|基于解码器-only transformer的大型语言模型（LLM）在文本理解能力方面已经表现出优于CLIP和T5系列模型的能力。然而，在文本到图像扩散模型中利用当前先进LLM的范式仍有待探索。我们观察到一个不寻常的现象：直接使用大型语言模型作为prompt编码器会显著降低图像生成中的prompt遵循能力。我们确定了造成这个问题背后的两个主要障碍。一个是LLM中下一个token预测训练与扩散模型中对判别性prompt特征的要求之间的不匹配。另一个是由解码器-only架构引入的固有位置偏差。为了解决这个问题，我们提出了一个新的框架来充分利用LLM的能力。通过精心设计的使用指南，我们有效地增强了用于prompt编码的文本表示能力，并消除了其固有的位置偏差。这使我们能够灵活地将最先进的LLM集成到文本到图像生成模型中。此外，我们还提供了一种将多个LLM融合到我们框架中的有效方法。考虑到transformer架构所展现出的出色性能和扩展能力，我们进一步设计了一个基于该框架的LLM-Infused Diffusion Transformer（LI-DiT）。我们进行了广泛的实验，以验证LI-DiT在模型规模和数据规模上的有效性。受益于LLM的固有能力和我们的创新设计，LI-DiT的prompt理解性能轻松超越了最先进的开源模型以及包括Stable Diffusion 3、DALL-E 3和Midjourney V6在内的主流闭源商业模型。功能强大的LI-DiT-10B将在进一步优化和安全检查后发布。||
|**2024-06-17**|[MegaScenes: Scene-Level View Synthesis at Scale](http://arxiv.org/abs/2406.11819)|null|场景级新视角合成（NVS）是许多视觉和图形应用的基础。最近，姿势条件扩散模型通过从二维基础模型中提取三维信息取得了显著进展，但这些方法受到缺乏场景级训练数据的限制。常见的数据库选择要么包含孤立的对象（Objaverse），要么包含具有有限姿势分布的以对象为中心的场景（DTU、CO3D）。在本文中，我们利用互联网照片集创建了一个大型场景级数据库，称为MegaScenes，其中包含来自世界各地的超过10万个运动结构（SfM）重建。互联网照片代表了一种可扩展的数据源，但也带来了光照和瞬态对象等挑战。我们解决了这些问题，进一步创建了一个适合NVS任务的子集。此外，我们分析了最先进的NVS方法的失败案例，并显著提高了生成的一致性。通过大量的实验，我们验证了我们的数据库和方法在生成真实场景中的有效性。有关数据库和代码的详细信息，请参见我们的项目页面：https://megascenes.github.io。||
|**2024-06-17**|[DiffMM: Multi-Modal Diffusion Model for Recommendation](http://arxiv.org/abs/2406.11781)|null|像抖音和YouTube这样的在线多模态分享平台的兴起使得个性化推荐系统能够将多种模态（如视觉、文本和音频）纳入用户表示中。然而，解决这些系统中的数据稀疏性挑战仍然是一个关键问题。为了解决这一局限性，最近的研究引入了自监督学习技术来增强推荐系统。然而，这些方法通常依赖于简单的随机增强或直观的跨视图信息，这可能会引入不相关的噪声，并且无法准确地将多模态上下文与用户-项目交互建模相一致。为了填补这一研究空白，我们提出了一种新的用于推荐的多模态图扩散模型，称为DiffMM。我们的框架将模态感知图扩散模型与跨模态对比学习范式相结合，以改进模态感知用户表示学习。这种集成促进了多模态特征信息与协同关系建模之间更好的对齐。我们的方法利用扩散模型的生成能力自动生成一个感知不同模态的用户-项目图，从而促进在用户-项目交互建模中纳入有用的多模态知识。我们在三个公开数据集上进行了广泛的实验，一致证明了我们的DiffMM相对于各种竞争基线的优越性。有关开源模型实现的详细信息，您可以访问以下网址获取我们提出的框架的源代码：https://github.com/HKUDS/DiffMM。||
|**2024-06-17**|[Transcendence: Generative Models Can Outperform The Experts That Train Them](http://arxiv.org/abs/2406.11741)|null|生成模型的训练目标很简单，即模仿其训练数据所诱导的条件概率分布。因此，当使用人类生成的数据进行训练时，我们不能期望人工智能模型在其原始目标上超越人类。在这项工作中，我们研究了超越现象：即生成模型实现的能力超过生成其数据的专家的能力。我们通过训练一个自回归Transformer从棋谱中学习下棋来证明超越现象，并表明训练后的模型有时可以取得比数据集中所有棋手都好的表现。我们从理论上证明了低温采样能够实现超越，并在实验上对其进行了严格的评估。最后，我们讨论了超越的其他来源，为在更广泛的背景下进一步研究这一现象奠定了基础。||
|**2024-06-17**|[Latent Denoising Diffusion GAN: Faster sampling, Higher image quality](http://arxiv.org/abs/2406.11713)|**[link](https://github.com/thanhluantrinh/lddgan)**|扩散模型正在成为生成高保真度和多样化图像的强大解决方案，在许多情况下甚至优于GAN。然而，其缓慢的推理速度阻碍了其在实时应用中的潜力。为了解决这个问题，DiffusionGAN利用条件GAN大幅减少了去噪步骤并加快了推理速度。其改进版本Wavelet Diffusion通过将数据转换为小波空间进一步加快了这一过程，从而提高了效率。尽管如此，这些模型在速度和图像质量方面仍落后于GAN。为了弥合这些差距，本文介绍了潜在去噪扩散GAN（Latent Denoising Diffusion GAN），它采用预训练的自编码器将图像压缩到紧凑的潜在空间中，从而显著提高推理速度和图像质量。此外，我们提出了一种加权学习策略来增强图像的多样性和质量。在CIFAR-10、CelebA-HQ和LSUN-Church数据集上的实验结果证明，我们的模型在扩散模型中实现了最先进的运行速度。与之前的DiffusionGAN和Wavelet Diffusion相比，我们的模型在所有评估指标上都显示出显著的改进。代码和预训练模型：\url{https://github.com/thanhluantrinh/LDDGAN.git}||
|**2024-06-17**|[Diffusion Generative Modelling for Divide-and-Conquer MCMC](http://arxiv.org/abs/2406.11664)|null|分而治之马尔可夫链蒙特卡洛 (MCMC) 是一种并行化马尔可夫链蒙特卡洛采样的策略，它在数据集的不相交子集上运行独立采样器并合并它们的输出。文献中一个持续的挑战是如何有效地执行这种合并，而不必对后验分布做出分布假设。我们建议使用扩散生成模型来拟合子后验分布的密度近似。这种方法在具有挑战性的合并问题上优于现有方法，同时与现有的密度估计方法相比，其计算成本可以更有效地扩展到高维问题。||
|**2024-06-17**|[An approach to non-equilibrium statistical physics using variational Bayesian inference](http://arxiv.org/abs/2406.11630)|null|我们讨论了一种对由耦合在一起的对象组成的系统进行数学建模的方法，该方法使用生成模型来描述构成此类系统的物体状态（或轨迹）之间的依赖关系。这类系统范围很广，包括开放系统或非平衡系统，与自组织系统尤其相关。由此产生的变分自由能原理 (FEP) 与直接使用随机动力系统相比具有一定的优势，特别是它更易于处理，并根据系统组件之间的耦合特性，对联合系统的演化方式提供了一种简洁的解释。使用 FEP，我们可以将一个物体的动力学建模为一个变分推理过程，因为变分自由能（或惊奇）是其动力学的李雅普诺夫函数。简而言之，我们认为使用生成模型来表示和跟踪子系统之间的关系，可以引导我们得到一种关于交互系统的特定统计理论。反过来，该理论使我们能够构建尊重子系统之间已知关系的嵌套模型。我们指出，一个物理对象符合 FEP 并不一定意味着该对象在字面上执行推理；相反，这是一种有用的解释性虚构，它用自由能梯度上的“隐式”流动代替了对象的“显式”动力学——这种虚构可能被对象本身所接受，也可能不被接受。||
|**2024-06-17**|[GECOBench: A Gender-Controlled Text Dataset and Benchmark for Quantifying Biases in Explanations](http://arxiv.org/abs/2406.11547)|**[link](https://github.com/braindatalab/gecobench)**|大型预训练语言模型已在许多应用中流行起来，并构成自然语言处理 (NLP) 中许多下游任务的重要支柱。应用“可解释人工智能”(XAI) 技术来丰富此类模型的输出，对于确保其质量和阐明其内部工作机制至关重要。然而，大型语言模型是在包含各种偏差（例如性别偏差）的大量数据上训练的，这会影响模型权重以及潜在的行为。目前，尚不清楚这种偏差在何种程度上也会以可能不利的方式影响模型解释。我们创建了一个性别控制文本数据集 GECO，其中其他相同的句子以男性和女性形式出现。这产生了用于性别分类任务的基本事实“世界解释”，从而能够客观地评估 XAI 方法的正确性。我们还提供了 GECOBench，这是一个严格的定量评估框架，对流行的 XAI 方法进行基准测试，将它们应用于经过不同程度微调的预训练语言模型。这使我们能够研究预训练如何在模型解释中引发不必要的偏差，以及微调可以在多大程度上减轻这种解释偏差。我们展示了解释性能与微调层数之间的明确依赖关系，其中观察到 XAI 方法特别受益于微调或嵌入层的完整再训练。值得注意的是，这种关系适用于在同一任务上实现相似分类性能的模型。因此，我们强调了所提出的性别控制数据集和新颖的基准测试方法对于新型 XAI 方法的研发具有实用性。所有代码，包括数据集生成、模型训练、评估和可视化，都可以在以下网址获得：https://github.com/braindatalab/gecobench||

<p align=right>(<a href=#updated-on-20240619>back to top</a>)</p>

## LLM

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-18**|[Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?](http://arxiv.org/abs/2406.12822)|null|大型语言模型，特别是多语言模型，其设计、宣传和预期目标都是服务于各种语言的母语使用者。我们假设，由于严重依赖翻译，这些模型目前的微调和评估实践可能与这一目标不符，因为翻译可能会引入翻译错误和缺陷。目前尚不清楚指令数据的性质是否会对模型输出产生影响；另一方面，翻译后的测试集是否能够捕捉到这些细微差别也值得怀疑。由于在这两个阶段经常使用翻译数据的做法，这种缺陷可能被忽视了。这项工作通过在指令微调和评估阶段使用受控的母语或翻译数据并观察模型结果来研究这些问题。对8个基础模型和8个不同基准的实验表明，母语或生成基准在母语和翻译指令数据之间表现出显著差异，尤其是在模型性能较高时，而其他类型的测试集则没有这种差异。最后，我们证明了正则化有利于弥合结构化任务（而非生成任务）上的差距。|
|**2024-06-18**|[Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?](http://arxiv.org/abs/2406.12809)|null|大型语言模型 (LLM) 表现出令人印象深刻的能力，但仍然存在不一致的问题（例如，LLM 对重新措辞或无关紧要的顺序更改等干扰的反应可能不同）。除了这些不一致之外，我们还观察到，尽管 LLM 能够解决难题，但反常的是，它们却可能在更简单的难题上失败。为了评估这种由难到易的不一致性，我们开发了 ConsisEval 基准测试，其中每个条目包含一对难度严格排序的问题。此外，我们引入了“一致性得分”的概念，以定量衡量这种不一致性，并通过“相对一致性得分”分析一致性改进的潜力。基于对各种现有模型的综合实验，我们发现：(1) GPT-4 取得了 92.2% 的最高一致性得分，但由于冗余信息的干扰、对问题的误解等原因，在特定问题上仍然存在不一致性；(2) 能力越强的模型通常表现出更高的一致性，但也存在例外情况；(3) 硬数据增强了微调和上下文学习的一致性。我们的数据和代码将在 GitHub 上公开。|
|**2024-06-18**|[Supporting Human Raters with the Detection of Harmful Content using Large Language Models](http://arxiv.org/abs/2406.12800)|null|本文探讨了利用大型语言模型 (LLM) 自动化或辅助人工评估员识别有害内容（包括仇恨言论、骚扰、暴力极端主义和选举虚假信息）的可行性。我们使用包含 50,000 条评论的数据集证明，与人工判决相比，LLM 可以达到 90% 的准确率。我们探索了如何最好地利用这些能力，提出了五种将 LLM 与人工评估相结合的设计模式，例如预先过滤非违规内容、检测人工评估中的潜在错误或显示关键上下文以支持人工评估。我们概述了如何使用单一、优化的提示来支持所有这些设计模式。除了这些合成实验之外，我们还分享了在现实世界审核队列中试行我们提出的技术如何使可用人工评估员的能力优化了 41.5%，并将识别违规内容的精确率和召回率提高了 9% 到 11%（绝对值）。|
|**2024-06-18**|[ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools](http://arxiv.org/abs/2406.12793)|null|我们介绍 ChatGLM，这是一个随着时间的推移而不断发展的系列大型语言模型。本报告主要关注 GLM-4 语言系列，其中包括 GLM-4、GLM-4-Air 和 GLM-4-9B。它们代表了我们功能最强大的模型，这些模型利用了前三代 ChatGLM 的所有见解和经验教训进行训练。迄今为止，GLM-4 模型已经接受了 10 万亿个标记的预训练，这些标记主要来自中文和英文，以及来自 24 种语言的一小部分语料库，并且主要针对中文和英文的使用进行了调整。高质量的对齐是通过多阶段的训练后过程实现的，该过程包括监督微调和从人类反馈中学习。评估表明，GLM-4 1) 在 MMLU、GSM8K、MATH、BBH、GPQA 和 HumanEval 等一般指标方面与 GPT-4 相当或优于 GPT-4，2) 在 IFEval 衡量的指令遵循方面接近 GPT-4-Turbo，3) 在长上下文任务方面与 GPT-4 Turbo (128K) 和 Claude 3 相当，以及 4) 在 AlignBench 衡量的中文对齐方面优于 GPT-4。GLM-4 All Tools 模型经过进一步调整，可以理解用户意图并自主决定何时以及使用哪些工具（包括网络浏览器、Python 解释器、文本到图像模型和用户定义的函数）来有效完成复杂的任务。在实际应用中，它在通过网络浏览访问在线信息和使用 Python 解释器解决数学问题等任务中，与 GPT-4 All Tools 相当甚至超越了它。在此过程中，我们开源了一系列模型，包括 ChatGLM-6B（三代）、GLM-4-9B（128K、1M）、GLM-4V-9B、WebGLM 和 CodeGeeX，仅在 2023 年就吸引了 Hugging face 上超过 1000 万次的下载量。开源模型可以通过 https://github.com/THUDM 和 https://huggingface.co/THUDM 访问。|
|**2024-06-18**|[UBENCH: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions](http://arxiv.org/abs/2406.12784)|null|大型语言模型（LLM）的快速发展已经展现出可观的应用前景。然而，它们的可解释性较低，这常常导致在不可预见的情况下出现错误，限制了它们的实用性。许多研究致力于创建全面的评估体系，但以往的基准测试主要评估解决问题的能力，而忽略了响应的不确定性，这可能导致不可靠的结果。最近测量LLM可靠性的方法非常消耗资源，并且无法测试黑盒模型。为了解决这个问题，我们提出了UBENCH，这是一个用于评估LLM可靠性的综合基准测试。UBENCH包含3,978道多项选择题，涵盖知识、语言、理解和推理能力。实验结果表明，UBENCH取得了最先进的性能，同时其单次采样方法与需要多次采样的基线方法相比，显著节省了计算资源。此外，我们基于UBENCH评估了15种流行LLM的可靠性，发现GLM4表现最为出色，紧随其后的是GPT-4。我们还探讨了思维链提示、角色扮演提示、选项顺序和温度对LLM可靠性的影响，分析了它们对不同LLM的不同影响。|
|**2024-06-18**|[Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries](http://arxiv.org/abs/2406.12775)|null|大型语言模型 (LLM) 可以解决复杂的多步骤问题，但人们对其内部计算机制知之甚少。基于此，我们研究了 LLM 如何回答多跳查询，例如“Imagine 的演唱者的配偶是”。这些查询需要两个信息提取步骤：一个潜在步骤，用于将第一个跳跃（“Imagine 的演唱者”）解析为桥接实体（约翰·列侬），另一个步骤用于将第二个跳跃（“约翰·列侬的配偶”）解析为目标实体（小野洋子）。理解潜在步骤如何在内部计算是理解整体计算的关键。通过仔细分析基于 Transformer 的 LLM 的内部计算，我们发现桥接实体是在模型的早期层中解析的。然后，只有在这个解析之后，两跳查询才会在后面的层中得到解决。由于第二个跳跃始于后面的层，因此在某些情况下，这些层可能不再编码正确预测答案所需的知识。基于此，我们提出了一种新颖的“反向补丁”分析方法，将来自后面层的隐藏表示“补丁”回较早的层。我们发现在高达 57% 的先前错误案例中，存在一个反向补丁，可以导致正确生成答案，这表明后面的层确实有时缺乏必要的功能。总的来说，我们的方法和发现为理解和改进基于 Transformer 的 LLM 中的潜在推理提供了进一步的机会。|
|**2024-06-18**|[Large Language Model as a Universal Clinical Multi-task Decoder](http://arxiv.org/abs/2406.12738)|null|开发有效的机器学习方法来提高临床系统的效率和准确性至关重要。尽管进行了大量的研究，但管理大量多样化的临床任务和适应新出现的任务仍然是重大挑战。本文提出了一种新颖的范式，该范式采用预训练的大型语言模型作为通用的临床多任务解码器。这种方法利用语言表达的灵活性和多样性来处理任务主题变化和相关参数。引入新任务只需添加新的指令模板。我们在数百项任务中验证了此框架，证明了其在促进多任务预测方面的稳健性，其性能与传统的  多任务学习和单任务学习方法相当。此外，它还表现出对新任务的出色适应能力，在某些情况下具有令人印象深刻的零样本性能，并且在少样本场景中具有卓越的数据效率。这种新方法为管理临床应用中的大量新兴任务提供了统一的解决方案。|
|**2024-06-18**|[Can Large Language Models Code Like a Linguist?: A Case Study in Low Resource Sound Law Induction](http://arxiv.org/abs/2406.12725)|null|长期以来，历史语言学家一直在编写一种未完全形式化的“程序”，用于将祖先语言中的重建词转换为其已证实的后代语言中的词，该程序由一系列有序的字符串重写函数（称为语音规律）组成。他们通过观察重建语言（原始形式）和后代语言（反射形式）中的词对，并构建一个将原始形式转换为反射形式的程序来做到这一点。然而，编写这些程序容易出错且耗时。先前的工作已经成功地用计算方法为这个过程搭建了脚手架，但很少有研究人员处理语音规律归纳（SLI）问题，我们在本文中将其视为示例编程来处理。我们提出了一种语言无关的解决方案，该解决方案利用大型语言模型（LLM）的编程能力，从语音变化示例中生成 Python 语音规律程序。我们评估了我们的方法对各种 LLM 的有效性，提出了生成额外的语言无关合成数据的有效方法，以微调用于 SLI 的 LLM，并将我们的方法与现有的自动 SLI 方法进行了比较，结果表明，虽然 LLM 落后于它们，但它们可以弥补它们的一些弱点。|
|**2024-06-18**|[Estimating Knowledge in Large Language Models Without Generating a Single Token](http://arxiv.org/abs/2406.12673)|null|为了评估大型语言模型 (LLM) 的知识，当前的方法是查询模型，然后评估其生成的响应。在这项工作中，我们询问是否可以在模型生成任何文本之前完成评估。具体来说，是否可以仅根据模型的内部计算来估计模型对某个实体的了解程度？我们通过两个任务来研究这个问题：给定一个主题实体，目标是预测 (a) 模型回答有关该实体的常见问题的能力，以及 (b) 模型生成的有关该实体的响应的事实性。对各种 LLM 进行的实验表明，KEEN 是一种基于内部主题表示训练的简单探针，在这两项任务中都取得了成功——与每个主题的模型问答准确率和 FActScore（最近在开放式生成中的事实性指标）密切相关。此外，KEEN 自然地与模型的 hedging 行为相一致，并忠实地反映了微调后模型知识的变化。最后，我们展示了一个更具解释性但性能相当的 KEEN 变体，它突出显示了一小组与模型缺乏知识相关的标记。KEEN 简单轻便，可用于识别 LLM 中实体知识的差距和集群，并指导决策，例如使用检索增强查询。|
|**2024-06-18**|[Stealth edits for provably fixing or attacking large language models](http://arxiv.org/abs/2406.12670)|**[link](https://github.com/qinghua-zhou/stealth-edits)**|We reveal new methods and the theoretical foundations of techniques for editing large language models. We also show how the new theory can be used to assess the editability of models and to expose their susceptibility to previously unknown malicious attacks. Our theoretical approach shows that a single metric (a specific measure of the intrinsic dimensionality of the model's features) is fundamental to predicting the success of popular editing approaches, and reveals new bridges between disparate families of editing methods. We collectively refer to these approaches as stealth editing methods, because they aim to directly and inexpensively update a model's weights to correct the model's responses to known hallucinating prompts without otherwise affecting the model's behaviour, without requiring retraining. By carefully applying the insight gleaned from our theoretical investigation, we are able to introduce a new network block -- named a jet-pack block -- which is optimised for highly selective model editing, uses only standard network operations, and can be inserted into existing networks. The intrinsic dimensionality metric also determines the vulnerability of a language model to a stealth attack: a small change to a model's weights which changes its response to a single attacker-chosen prompt. Stealth attacks do not require access to or knowledge of the model's training data, therefore representing a potent yet previously unrecognised threat to redistributed foundation models. They are computationally simple enough to be implemented in malware in many cases. Extensive experimental results illustrate and support the method and its theoretical underpinnings. Demos and source code for editing language models are available at https://github.com/qinghua-zhou/stealth-edits.|
|**2024-06-17**|[mDPO: Conditional Preference Optimization for Multimodal Large Language Models](http://arxiv.org/abs/2406.11839)|null|直接偏好优化 (DPO) 已被证明是一种有效的大语言模型 (LLM) 对齐方法。最近的研究尝试将 DPO 应用于多模态场景，但发现难以实现一致的改进。通过对比实验，我们发现了多模态偏好优化中的无条件偏好问题，即模型忽略了图像条件。为了解决这个问题，我们提出了 mDPO，这是一种多模态 DPO 目标，通过同时优化图像偏好来防止过度优先考虑仅语言偏好。此外，我们引入了一个奖励锚点，强制选择响应的奖励为正，从而避免了其可能性降低——这是相对偏好优化固有的问题。在两个不同规模的多模态 LLM 和三个广泛使用的基准测试上的实验表明，mDPO 有效地解决了多模态偏好优化中的无条件偏好问题，并显着提高了模型性能，特别是在减少幻觉方面。||
|**2024-06-17**|[Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models](http://arxiv.org/abs/2406.11831)|null|基于解码器Transformer的大语言模型（LLM）在文本理解能力方面已经展现出优于CLIP和T5系列模型的优势。然而，如何将当前先进的LLM应用于文本到图像的扩散模型仍然是一个有待探索的领域。我们观察到一个不寻常的现象：直接使用大型语言模型作为提示编码器会显著降低图像生成中的提示遵循能力。我们确定了造成这个问题背后的两个主要障碍。一个是LLM中预测下一个词的训练目标与扩散模型中对判别性提示特征的要求之间存在偏差。另一个是由仅解码器架构引入的固有位置偏差。为了解决这个问题，我们提出了一个全新的框架来充分利用LLM的能力。通过精心设计的使用指南，我们有效地增强了用于提示编码的文本表示能力，并消除了其固有的位置偏差。这使我们能够灵活地将最先进的LLM集成到文本到图像的生成模型中。此外，我们还提供了一种将多个LLM融合到我们框架中的有效方法。考虑到Transformer架构所展现出的出色性能和扩展能力，我们进一步设计了一种基于该框架的LLM注入式扩散Transformer（LI-DiT）。我们进行了广泛的实验，以验证LI-DiT在模型规模和数据规模上的有效性。得益于LLM的固有能力和我们的创新设计，LI-DiT的提示理解性能轻松超越了最先进的开源模型以及包括Stable Diffusion 3、DALL-E 3和Midjourney V6在内的主流闭源商业模型。功能强大的LI-DiT-10B将在经过进一步优化和安全检查后发布。||
|**2024-06-17**|[VideoLLM-online: Online Video Large Language Model for Streaming Video](http://arxiv.org/abs/2406.11816)|null|近年来，大型语言模型已经具备了视觉能力，能够理解图像、视频和交错的视觉语言内容。然而，这些大型多模态模型的学习方法通常将视频视为预先确定的片段，这使得它们在处理流式视频输入方面效率较低。在本文中，我们提出了一种新颖的视频流学习（LIVE）框架，该框架支持在连续视频流中进行时间对齐、长上下文和实时的对话。我们的LIVE框架包含实现视频流对话的综合方法，包括：（1）旨在对连续流输入执行语言建模的训练目标，（2）将离线时间注释转换为流对话格式的数据生成方案，以及（3）优化的推理流程，以加快模型在现实世界视频流中的响应速度。借助我们的LIVE框架，我们在Llama-2/Llama-3的基础上构建了VideoLLM-online模型，并展示了其在处理流媒体视频方面的显著优势。例如，平均而言，我们的模型可以在A100 GPU上以超过10 FPS的速度支持5分钟视频片段中的流媒体对话。此外，它还在公共离线视频基准测试中展示了最先进的性能，例如识别、字幕和预测。代码、模型、数据和演示已在https://showlab.github.io/videollm-online上提供。||
|**2024-06-17**|[How Do Large Language Models Acquire Factual Knowledge During Pretraining?](http://arxiv.org/abs/2406.11813)|null|尽管近期观察到大型语言模型 (LLM) 可以存储大量的 factual knowledge，但对于它们如何通过预训练获取 factual knowledge 的机制，人们的理解仍然有限。这项工作通过研究 LLM 在预训练期间如何获取 factual knowledge 来解决这一差距。研究结果揭示了关于预训练期间 factual knowledge 获取动态的几个重要见解。首先，与直觉相反，我们观察到，对更多数据的预训练并没有显著提高模型获取和维护 factual knowledge 的能力。其次，训练步数与 factual knowledge 的记忆遗忘和泛化之间存在幂律关系，并且使用重复训练数据训练的 LLM 表现出更快的遗忘速度。第三，使用更大的批大小训练 LLM 可以增强模型对遗忘的鲁棒性。总的来说，我们的观察表明 LLM 预训练中的 factual knowledge 获取是通过逐步增加在每个步骤中预训练数据中出现的 factual knowledge 的概率来实现的。然而，这种增加会被随后的遗忘所稀释。基于这种解释，我们可以对最近观察到的 LLM 行为提供合理的解释，例如 LLM 在长尾知识上的糟糕表现以及对预训练语料库进行重复数据删除的好处。||
|**2024-06-17**|[DataComp-LM: In search of the next generation of training sets for language models](http://arxiv.org/abs/2406.11794)|null|我们推出了面向语言模型的数据比较测试平台 (DCLM)，这是一个用于控制数据集实验以改进语言模型的测试平台。作为 DCLM 的一部分，我们提供了一个从 Common Crawl 中提取的包含 240 万亿个标记的标准化语料库、基于 OpenLM 框架的有效预训练方案以及一套包含 53 个下游评估的广泛套件。DCLM 基准测试的参与者可以尝试各种数据整理策略，例如去重、过滤和数据混合，模型规模从 4.12 亿到 70 亿个参数不等。作为 DCLM 的基线，我们进行了广泛的实验，发现基于模型的过滤是组装高质量训练集的关键。生成的数据集 DCLM-Baseline 使从头开始训练一个包含 70 亿个参数的语言模型成为可能，该模型在 MMLU 上的 5-shot 准确率达到 64%，训练使用了 2.6 万亿个标记。与之前最先进的开放数据语言模型 MAP-Neo 相比，DCLM-Baseline 在 MMLU 上实现了 6.6 个百分点的改进，而计算量减少了 40%。我们的基线模型在 MMLU 上也与 Mistral-7B-v0.3 和 Llama 3 8B 相当（63% 和 66%），并且在平均 53 个自然语言理解任务上的表现相似，而训练使用的计算量比 Llama 3 8B 少 6.6 倍。我们的结果突出了数据集设计对训练语言模型的重要性，并为进一步研究数据整理提供了一个起点。||
|**2024-06-17**|[CELL your Model: Contrastive Explanation Methods for Large Language Models](http://arxiv.org/abs/2406.11785)|null|黑盒深度神经网络分类模型的出现引发了对其决策进行解释的需求。然而，对于生成式人工智能（如大型语言模型（LLM））来说，没有类别预测需要解释。相反，人们可以询问LLM为何对给定提示输出特定响应。在本文中，我们通过提出据我们所知第一个仅需要黑盒/查询访问的对比解释方法来回答这个问题。我们的解释表明，LLM对给定提示输出回复的原因是，如果稍微修改提示，LLM会给出不同的回复，这些回复要么不太可取，要么与原始回复相矛盾。关键见解是，对比解释只需要对用户有意义的距离函数，而不是特定响应（即类别标签）的实际值表示。我们提供了两种寻找对比解释的算法：i）一种短视算法，虽然在创建对比方面有效，但需要多次模型调用；ii）一种预算算法，这是我们的主要算法贡献，它可以智能地创建符合查询预算的对比，这对于较长的上下文是必要的。我们展示了这些方法在各种自然语言任务上的有效性，例如开放文本生成、自动红队和解释对话降级。||
|**2024-06-17**|[Multi-Layer Ranking with Large Language Models for News Source Recommendation](http://arxiv.org/abs/2406.11745)|null|为了寻找新闻事件的可靠信息来源，我们引入了一项新的专家推荐任务，旨在根据专家先前引用的陈述来识别可信赖的来源。为此，我们构建了一个名为 NewsQuote 的新数据集，该数据集包含从新闻文章集合中提取的 23,571 对引言-发言者对。我们将推荐任务制定为根据专家与给定查询相关的可能性来检索专家。我们还提出了一个采用大型语言模型的多层排名框架，以提高推荐性能。我们的结果表明，采用基于上下文学习的 LLM 排名器和基于多层排名的过滤器可以显着提高推荐系统的预测质量和行为质量。||
|**2024-06-17**|[Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models](http://arxiv.org/abs/2406.11736)|null|大型语言模型 (LLM) 性能优越的主要驱动力之一是用于对齐微调的大量人工标注自然语言数据的可用性。这促使研究人员探索自训练方法，以减少对人工标注的过度依赖。然而，目前自训练的成功主要是在自然语言场景中观察到的，而不是在越来越重要的神经符号场景中。为此，我们提出了一个名为 ENVISIONS 的环境引导神经符号自训练框架。它旨在克服两个主要挑战：(1) 符号数据的稀缺性，以及 (2) LLM 在处理符号语言方面的能力有限。在三个不同领域进行的广泛评估证明了我们方法的有效性。此外，我们还进行了全面分析，以揭示 ENVISIONS 成功的影响因素，从而为该领域的未来研究提供宝贵见解。代码将在 \url{https://github.com/xufangzhi/ENVISIONS} 提供。||
|**2024-06-17**|[Meta Reasoning for Large Language Models](http://arxiv.org/abs/2406.11698)|null|我们介绍了元推理提示 (MRP)，这是一种受人类元推理启发，为大型语言模型 (LLM) 设计的、新颖且高效的系统提示方法。传统的基于上下文学习的推理技术，例如思维树，虽然很有前景，但由于其专业性，缺乏跨不同任务的一致最先进性能。MRP 通过引导 LLM 根据每个任务的特定要求动态选择和应用不同的推理方法来解决这一限制，从而优化性能和计算效率。使用 MRP，LLM 推理分两个阶段运行。最初，LLM 使用任务输入线索和可用方法的目标描述来确定最合适的推理方法。随后，它应用所选方法来完成任务。这种动态策略反映了人类的元推理，使模型能够在广泛的问题领域中脱颖而出。我们通过综合基准评估了 MRP 的有效性。结果表明，MRP 在不同任务中均达到或接近最先进的性能。MRP 代表了使 LLM 能够识别跨问题的认知挑战并利用不同推理方法的优势的重大进步，从而增强了它们高效处理多样化和复杂问题领域的能力。每个 LLM 都应该有一个元推理提示，以充分发挥其潜力，并确保在不断变化的挑战和应用环境中的适应性。||
|**2024-06-17**|[HoLLMwood: Unleashing the Creativity of Large Language Models in Screenwriting via Role Playing](http://arxiv.org/abs/2406.11683)|null|生成式人工智能在计算机视觉领域展现出了前所未有的创造力，但在自然语言处理领域尚未观察到类似现象。特别是，由于文学创作的高度复杂性，大型语言模型（LLM）难以创作出达到人类专家水平的文学作品。在本文中，我们提出了 HoLLMwood，这是一个自动化框架，旨在释放 LLM 的创造力并探索其在剧本创作方面的潜力，而剧本创作是一项要求极高的任务。模仿人类创作过程，我们将 LLM 分配给现实场景中的不同角色。除了将 LLM 视为“编剧”的常见做法外，我们还将 LLM 用作“编辑”，负责向“编剧”提供反馈和修改建议。此外，为了丰富角色和深化情节，我们引入了角色扮演机制，并将 LLM 用作可以相互交流和互动的“演员”。对自动生成剧本的评估表明，HoLLMwood 在连贯性、相关性、趣味性和整体质量方面明显优于强大的基线模型。||

<p align=right>(<a href=#updated-on-20240619>back to top</a>)</p>

## Transformer

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-18**|[The Wisdom of a Crowd of Brains: A Universal Brain Encoder](http://arxiv.org/abs/2406.12179)|null|图像到功能性磁共振成像(fMRI)的编码对于神经科学研究和实际应用都至关重要。然而，这种“大脑编码器”通常是针对每个受试者和每个 fMRI 数据集进行训练的，因此训练数据的局限性很大。在本文中，我们提出了一种通用大脑编码器，它可以利用来自许多不同受试者/数据集/机器的数据进行联合训练。实现这一点的关键是我们新的以体素为中心的编码器架构，它可以为每个大脑体素学习一个独特的“体素嵌入”。我们的编码器通过直接计算大脑体素嵌入和多级深度图像特征之间的交叉注意，来训练预测每个大脑体素对每个图像的响应。这种以体素为中心的架构允许每个大脑体素的功能作用从体素-图像交叉注意中自然地浮现出来。我们展示了这种方法的强大功能：(i) 结合来自多个不同受试者的数据（“大脑人群”）以改进每个个体的大脑编码，(ii) 跨受试者、数据集和机器（例如，3 特斯拉、7 特斯拉）进行快速有效的迁移学习，只需少量训练样本，以及 (iii) 使用学习到的体素嵌入作为探索大脑功能的强大工具（例如，大脑中哪些区域编码了哪些信息）。|
|**2024-06-17**|[Crossfusor: A Cross-Attention Transformer Enhanced Conditional Diffusion Model for Car-Following Trajectory Prediction](http://arxiv.org/abs/2406.11941)|null|车辆轨迹预测对于推进自动驾驶和高级驾驶辅助系统 (ADAS) 至关重要，可提高道路安全和交通效率。虽然传统方法奠定了基础，但现代深度学习技术，特别是基于 Transformer 的模型和生成方法，通过捕捉车辆运动和交通交互中复杂和非线性的模式，显著提高了预测精度。然而，这些模型往往忽略了现实驾驶场景中必不可少的详细跟车行为和车辆间交互。本研究介绍了一种专为跟车轨迹预测设计的交叉注意力 Transformer 增强型条件扩散模型 (Crossfusor)。Crossfusor 将详细的车辆间交互和跟车动力学集成到一个强大的扩散框架中，提高了预测轨迹的准确性和真实性。该模型利用一种结合了 GRU、基于位置的注意力机制和傅里叶嵌入的新型时间特征编码框架来捕捉历史车辆动力学。它在正向扩散过程中采用由这些编码的历史特征缩放的噪声，并在反向去噪过程中使用交叉注意力 Transformer 对复杂的车辆间依赖关系进行建模。在 NGSIM 数据集上的实验结果表明，Crossfusor 优于最先进的模型，特别是在长期预测方面，展示了其增强自动驾驶系统预测能力的潜力。|
|**2024-06-17**|[Composing Object Relations and Attributes for Image-Text Matching](http://arxiv.org/abs/2406.11820)|null|我们研究了用于图文匹配的视觉语义嵌入问题。大多数现有工作利用定制的交叉注意力机制来执行图像和文本两种模态之间的局部对齐。尽管这种方法比单模态双编码器方法更强大，但计算成本很高。这项工作介绍了一种双编码器图文匹配模型，利用场景图来表示字幕，其中节点表示对象和属性，并通过关系边相互连接。我们的模型利用图注意力网络有效地编码了对象-属性和对象-对象语义关系，从而形成了一个鲁棒且快速的系统。将字幕表示为场景图，可以利用图神经网络强大的关系归纳偏差来有效地学习对象-属性和对象-对象关系。为了训练该模型，我们提出了在整体级别（图像-字幕）和局部级别（图像-对象实体）上对齐图像和字幕的损失函数，我们证明这是模型成功的关键。我们的模型被称为对象关系和属性组合模型，简称CORA。在两个著名的图文检索基准数据集Flickr30K和MSCOCO上的实验结果表明，CORA在召回率方面优于现有的计算成本高昂的交叉注意力方法，同时实现了双编码器的快速计算速度。|
|**2024-06-17**|[AnyMaker: Zero-shot General Object Customization via Decoupled Dual-Level ID Injection](http://arxiv.org/abs/2406.11643)|null|基于文本到图像的对象定制旨在根据文本提示和参考图像生成具有相同身份（ID）的图像，这一领域已取得重大进展。然而，最近的定制研究主要集中在特定任务上，例如人物定制或虚拟试穿，而忽略了通用的对象定制。为此，我们推出了 AnyMaker，这是一个创新的零样本对象定制框架，能够生成具有高度ID保真度和灵活文本可编辑性的通用对象。AnyMaker 的功效源于其新颖的通用 ID 提取、双层 ID 注入和 ID 感知解耦。具体来说，通用 ID 提取模块使用一组自监督模型提取足够的 ID 信息，以处理针对通用对象的各种定制任务。然后，为了在不损害生成过程中文本可编辑性的情况下，为扩散 UNet 提供尽可能多的提取 ID 信息，我们设计了一个全局-局部双层 ID 注入模块，其中全局语义 ID 被注入文本描述中，而局部 ID 细节则通过新添加的交叉注意模块直接注入模型。此外，我们提出了一个 ID 感知解耦模块，用于从提取的表示中分离与 ID 相关的信息和非 ID 元素，以实现身份和文本描述的高保真生成。为了验证我们的方法并促进通用对象定制的研究，我们创建了第一个大规模通用 ID 数据集，即多类别 ID 一致性（MC-IDC）数据集，包含 31.5 万个文本图像样本和 1 万个类别。实验表明，AnyMaker 在通用对象定制方面表现出色，并在相应任务中优于专门方法。代码和数据集将很快发布。|
|**2024-06-17**|[Simple Yet Efficient: Towards Self-Supervised FG-SBIR with Unified Sample Feature Alignment](http://arxiv.org/abs/2406.11551)|null|细粒度草图图像检索 (FG-SBIR) 旨在最小化嵌入空间中草图和对应图像之间的距离。然而，解决方案日益复杂，主要是因为细粒度草图的抽象性，阻碍了可扩展性。在本文中，我们提出了一种简单而有效的方法来缩小两种模式之间的差距。它主要促进了样本内和样本间统一的互信息共享，而不是将它们视为模态之间的单一特征对齐问题。具体来说，我们的方法包括：(i) 采用双权重共享网络来优化草图和图像域内的对齐，这也有效地缓解了模型学习饱和问题。(ii) 引入基于对比损失的目标优化函数，以增强模型在样本内和样本间对齐特征的能力。(iii) 提出了一种由自注意力和交叉注意力相结合的可学习 TRSM，以促进标记之间的特征表示，进一步增强嵌入空间中的样本对齐。我们的框架在基于 CNN 和 ViT 的骨干网络上取得了优异的结果。大量实验表明，它优于现有方法。我们还介绍了 Cloths-V1，这是第一个专业的服装草图和图像数据集，用于验证我们的方法，并将有利于其他应用。|
|**2024-06-17**|[Learning from Exemplars for Interactive Image Segmentation](http://arxiv.org/abs/2406.11472)|null|交互式图像分割使用户能够以最少的交互与机器进行交互，从而逐步细化目标对象的分割掩码。先前的研究已经证明了通过交互式分割提取单个目标掩码的出色性能。然而，现有方法忽略了先前交互对象的语义线索，这些线索可以被进一步探索以加速对同一类别中多个目标的交互式分割。为此，我们针对单一对象和同一类别中的多个对象引入了新颖的交互式分割框架。具体来说，我们的模型利用 Transformer 骨干网络从图像和交互中提取以交互为中心的视觉特征，以获得令人满意的目标掩码作为样本。对于多个对象，我们提出了一个样本信息模块，以增强对目标类别对象之间相似性的学习。为了组合来自不同模块的注意力特征，我们结合了交叉注意力块和特征融合模块。在主流基准数据集上进行的实验表明，与以前的方法相比，我们的模型取得了优越的性能。特别是，我们的模型将用户的劳动量减少了约 15%，需要少点击两次才能达到目标 IoU 85% 和 90%。结果突出了我们的模型作为灵活实用的标注工具的潜力。源代码将在发表后发布。|
|**2024-06-17**|[Analysing the Behaviour of Tree-Based Neural Networks in Regression Tasks](http://arxiv.org/abs/2406.11437)|**[link](https://github.com/petersamoaa/tree_based_nn_error_analysis)**|深度学习领域，特别是利用抽象语法树 (AST) 等结构化表示，极大地扩展了源代码分析的边界。虽然这些方法已在分类任务中表现出有效性，但它们在回归应用（例如根据源代码预测执行时间）中的效果仍有待探索。本文致力于解码基于树的神经网络模型在此类回归挑战中的行为。我们将已建立的模型（基于树的卷积神经网络 (CNN)、Code2Vec 和基于 Transformer 的方法）的应用扩展到通过将源代码解析为 AST 来预测其执行时间。我们的比较分析表明，虽然这些模型是代码表示的基准，但在执行回归任务时表现出局限性。为了解决这些缺陷，我们提出了一种新颖的双 Transformer 方法，该方法同时对源代码标记和 AST 表示进行操作，并采用交叉注意力机制来增强两个域之间的可解释性。此外，我们还探索了图神经网络 (GNN) 对此基于树的问题的适应性，并从理论上论证了由于 AST 的图形性质而导致的 inherent 兼容性。对真实世界数据集的实证评估表明，我们的双 Transformer 模型优于所有其他基于树的神经网络和基于 GNN 的模型。此外，我们提出的双 Transformer 在不同的数据集上表现出卓越的适应性和鲁棒性能。|
|**2024-06-17**|[DiTTo-TTS: Efficient and Scalable Zero-Shot Text-to-Speech with Diffusion Transformer](http://arxiv.org/abs/2406.11427)|null|大型扩散模型在图像、视频和音频等多种模态中展现出卓越的生成能力。然而，文本到语音（TTS）系统通常涉及特定领域的建模因素（例如，音素和音素级时长）以确保文本和语音之间精确的时间对齐，这阻碍了扩散模型在 TTS 中的效率和可扩展性。在这项工作中，我们提出了一种高效且可扩展的扩散Transformer（DiT），它利用现成的预训练文本和语音编码器。我们的方法通过交叉注意力机制和语音表示总长度的预测来解决文本-语音对齐的挑战。为此，我们增强了 DiT 架构以适应 TTS，并通过将语义指导融入语音的潜在空间来改进对齐。我们将训练数据集和模型规模分别扩展到 82K 小时和 7.9 亿个参数。我们广泛的实验表明，这种无需特定领域建模的大型 TTS 扩散模型不仅简化了训练流程，而且在自然度、清晰度和说话人相似度方面，其零样本性能优于或可与最先进的 TTS 模型相媲美。我们的语音样本可在 https://ditto-tts.github.io 获取。|
|**2024-06-16**|[STAR: Scale-wise Text-to-image generation via Auto-Regressive representations](http://arxiv.org/abs/2406.10797)|**[link](https://github.com/krennic999/STAR)**|我们提出了STAR，这是一个采用尺度自回归范式的文本到图像模型。与仅限于在预定类别集合内进行类别条件合成的VAR不同，我们的STAR通过三个关键设计实现了文本驱动的开放式生成：为了提高具有未见对象和概念组合的多样性和泛化能力，我们引入了一个预训练的文本编码器来提取文本约束的表示，然后将其用作指导。为了改善生成的图像与细粒度文本指导之间的交互，使结果更可控，我们在每个尺度上都加入了额外的交叉注意力层。鉴于不同尺度之间存在自然的结构关联，我们利用二维旋转位置编码（RoPE）并将其调整为归一化版本。这确保了对不同尺度标记图中相对位置的一致解释，并稳定了训练过程。大量实验表明，STAR在保真度、图像文本一致性和美学质量方面都超过了现有基准。我们的研究结果强调了自回归方法在高质量图像合成领域的潜力，为目前以扩散方法为主导的T2I领域提供了新的方向。|
|**2024-06-15**|[CrossFuse: A Novel Cross Attention Mechanism based Infrared and Visible Image Fusion Approach](http://arxiv.org/abs/2406.10581)|**[link](https://github.com/hli1221/crossfuse)**|多模态视觉信息融合旨在将多传感器数据集成到单个图像中，该图像包含更多互补信息和更少的冗余特征。然而，互补信息很难提取，特别是对于红外图像和可见图像，这两种模态之间存在很大的相似性差距。常见的交叉注意力模块只考虑相关性，相反，图像融合任务需要关注互补性（非相关性）。因此，本文提出了一种新的交叉注意机制（CAM）来增强互补信息。此外，提出了一种基于两阶段训练策略的融合方案来生成融合图像。对于第一阶段，为每个模态训练两个具有相同架构的自编码器网络。然后，在固定编码器的情况下，在第二阶段训练CAM和解码器。通过训练好的CAM，从两种模态中提取的特征被集成到一个融合特征中，其中互补信息得到增强，冗余特征减少。最后，可以通过训练好的解码器生成融合图像。实验结果表明，与现有的融合网络相比，我们提出的融合方法获得了最先进的融合性能。代码可在https://github.com/hli1221/CrossFuse获取。|
|**2024-06-14**|[Whisper-Flamingo: Integrating Visual Features into Whisper for Audio-Visual Speech Recognition and Translation](http://arxiv.org/abs/2406.10082)|**[link](https://github.com/roudimit/whisper-flamingo)**|音频-视觉语音识别 (AVSR) 利用基于嘴唇的视频来提高在噪声环境下的性能。由于视频比音频更难获取，AVSR 模型的视频训练数据通常只有几千小时。相比之下，Whisper 等语音模型使用数十万小时的数据进行训练，因此可以学习到更好的语音到文本解码器。巨大的训练数据差异促使我们调整 Whisper 以处理视频输入。受 Flamingo 将视觉特征注入语言模型的启发，我们提出了 Whisper-Flamingo，它使用门控交叉注意力将视觉特征集成到 Whisper 语音识别和翻译模型中。在嘈杂条件下，我们的音频-视觉 Whisper-Flamingo 在 6 种语言的英语语音识别和英语-X 翻译方面优于纯音频 Whisper。此外，Whisper-Flamingo 是一个多功能模型，可以使用一组参数执行所有这些任务，而先前的方法是在每种语言上单独训练的。||
|**2024-06-14**|[Simul-Whisper: Attention-Guided Streaming Whisper with Truncation Detection](http://arxiv.org/abs/2406.10052)|null|作为一种强大且规模庞大的多语言语音识别模型，Whisper 在许多低资源和 out-of-distribution 场景中表现出色。然而，其编码器-解码器结构阻碍了其在流式语音识别中的应用。在本文中，我们介绍了 Simul-Whisper，它利用 Whisper 交叉注意力机制中嵌入的时间对齐信息来指导自回归解码，并在无需对预训练模型进行任何微调的情况下实现基于块的流式自动语音识别。此外，我们观察到块边界处截断词对解码结果的负面影响，并提出了一种基于脉冲神经元的截断检测模型来解决这个问题。在多种语言和 Whisper 架构上的实验表明，Simul-Whisper 在 1 秒的块大小下平均绝对词错误率仅下降 1.46%，明显优于当前最先进的基线模型。||

<p align=right>(<a href=#updated-on-20240619>back to top</a>)</p>

## 3DGS

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-18**|[HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors](http://arxiv.org/abs/2406.12459)|**[link](https://github.com/humansplat/humansplat.github.io)**|尽管高保真人体重建技术取得了最新进展，但对密集捕捉图像或耗时的逐实例优化的要求极大地阻碍了其在更广泛场景中的应用。为了解决这些问题，我们提出了 HumanSplat，它可以从单个输入图像中以可泛化的方式预测任何人的 3D 高斯样条属性。具体来说，HumanSplat 包含一个 2D 多视图扩散模型和一个具有人体结构先验的潜在重建变换器，它能够将几何先验和语义特征巧妙地集成在一个统一的框架中。此外，我们还设计了一个包含人体语义信息的分层损失函数，以实现高保真纹理建模并更好地约束估计的多个视图。在标准基准数据集和真实世界图像上的综合实验表明，HumanSplat 在实现逼真的新视图合成方面优于现有的最先进方法。|
|**2024-06-17**|[A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets](http://arxiv.org/abs/2406.12080)|null|近年来，新视角合成技术取得了重大进展，其中 3D 高斯 splatting 以其卓越的视觉质量、快速的训练速度和实时渲染能力而著称。然而，训练和渲染所需的资源不可避免地限制了能够以良好视觉质量表示的捕获场景的大小。为此，我们引入了一种 3D 高斯层次结构，它可以在保留超大场景视觉质量的同时，为有效渲染远处内容提供高效的细节层次 (LOD) 解决方案，并在不同层次之间实现有效的级别选择和平滑过渡。我们引入了一种分治算法，允许我们以独立块的形式训练超大场景。我们将这些块合并成一个层次结构，可以通过优化该结构来进一步提高合并到中间节点的高斯的视觉质量。超大规模的捕获通常场景覆盖率稀疏，这对原始 3D 高斯 splatting 训练方法提出了许多挑战；我们调整并规范了训练过程以解决这些问题。我们提出了一个完整的解决方案，借助我们的 LOD 方法，该方案能够实时渲染超大场景并适应可用资源。我们展示了使用简单且经济的设备捕获的包含多达数万张图像的场景的结果，这些场景涵盖了长达数公里、持续时间长达一小时的轨迹。项目页面：https://repo-sam.inria.fr/fungraph/hierarchical-3d-gaussians/|
|**2024-06-17**|[RetinaGS: Scalable Training for Dense Scene Rendering with Billion-Scale 3D Gaussians](http://arxiv.org/abs/2406.11836)|null|在这项工作中，我们探索了在大规模、高分辨率数据集上训练高参数 3D 高斯样条 (3DGS) 模型的可能性。我们为 3DGS 设计了一种通用的模型并行训练方法，名为 RetinaGS，它使用适当的渲染方程，可以应用于任何场景和任意分布的高斯基元。它使我们能够探索 3DGS 在基元数量和训练分辨率方面的缩放行为，这些在以前难以探索，并超越了先前最先进的重建质量。当使用我们的方法增加基元数量时，我们观察到视觉质量明显提高的积极趋势。我们还首次尝试在完整 MatrixCity 数据集上训练具有超过 10 亿个基元的 3DGS 模型，该模型获得了良好的视觉质量。|
|**2024-06-18**|[Effective Rank Analysis and Regularization for Enhanced 3D Gaussian Splatting](http://arxiv.org/abs/2406.11672)|null|从多视图图像进行三维重建是计算机视觉和图形学中的基本挑战之一。近年来，三维高斯 splatting  (3DGS) 已成为一种很有前景的技术，能够实现高质量三维重建的实时渲染。该方法利用三维高斯表示和平铺式 splatting 技术，绕过了昂贵的神经场查询。尽管具有潜力，但由于高斯函数会收敛成具有一个主导方差的各向异性高斯函数，3DGS 仍面临着一些挑战，包括针状伪影、欠佳的几何形状和不准确的法线。我们建议使用有效秩分析来检查三维高斯基元的形状统计数据，并确定高斯函数确实会收敛成有效秩为 1 的针状形状。为了解决这个问题，我们引入了有效秩作为正则化项，它限制了高斯函数的结构。我们新的正则化方法增强了法线和几何重建，同时减少了针状伪影。该方法可以作为附加模块集成到其他 3DGS 变体中，在不影响视觉保真度的情况下提高其质量。|
|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|在非结构化的旅游环境中拍摄的照片经常表现出多变的外观和短暂的遮挡，这对精确的场景重建提出了挑战，并在新视角合成中引入了伪影。尽管先前的方法已经将神经辐射场 (NeRF) 与其他可学习模块相结合来处理动态外观和消除瞬态对象，但其大量的训练需求和缓慢的渲染速度限制了实际部署。最近，三维高斯 splatting (3DGS) 已成为 NeRF 的一种有前途的替代方案，它提供了卓越的训练和推理效率以及更好的渲染质量。本文提出了 Wild-GS，这是一种针对不受约束的照片集优化的 3DGS 创新改编，同时保留了其效率优势。Wild-GS 根据每张图像的固有材质属性、全局照明和相机属性以及点级反射率的局部变化来确定每个 3D 高斯的外观。与先前在图像空间中对参考特征进行建模的方法不同，Wild-GS 通过对从参考图像中提取的三平面进行采样，将像素外观特征显式地与相应的局部高斯对齐。这种新颖的设计有效地将参考视图的高频细节外观转移到 3D 空间，并显着加快了训练过程。此外，二维可见性图和深度正则化分别用于减轻瞬态效应和约束几何形状。大量实验表明，Wild-GS 在所有现有技术中实现了最先进的渲染性能以及最高的训练和推理效率。|
|**2024-06-14**|[L4GM: Large 4D Gaussian Reconstruction Model](http://arxiv.org/abs/2406.10324)|null|我们提出了L4GM，这是第一个从单视角视频输入生成动画对象的4D大型重建模型，只需一次前馈传递，耗时仅需一秒钟。我们成功的关键是一个新颖的多视角视频数据集，其中包含从Objaverse中精选的、渲染的动画对象。该数据集描绘了4.4万个不同的对象，包含11万个动画，以48个视角渲染，产生了1200万个视频，总计3亿帧。为了实现可扩展性，我们保持L4GM的简洁性，并直接构建在LGM的基础之上，LGM是一个预训练的3D大型重建模型，可以从多视角图像输入中输出3D高斯椭球体。L4GM从以低帧率采样的视频帧中输出每帧3D高斯 splatting 表示，然后将表示上采样到更高的帧率以实现时间平滑度。我们在基础LGM中添加了时间自注意力层，以帮助它学习跨时间的一致性，并利用每时间步多视角渲染损失来训练模型。通过训练一个插值模型将表示上采样到更高的帧率，该模型生成中间3D高斯表示。我们展示了仅在合成数据上训练的L4GM在实际视频中也能很好地泛化，生成高质量的动画3D资源。|
|**2024-06-14**|[PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting](http://arxiv.org/abs/2406.10219)|null|最近，新视角合成技术的进步实现了实时渲染速度和高重建精度。三维高斯 splatting (3D-GS) 是一种基础的基于点的参数化三维场景表示方法，它将场景建模为大量三维高斯的集合。复杂的场景可能包含数百万个高斯函数，导致巨大的存储和内存需求，从而限制了 3D-GS 在资源有限的设备上的可行性。目前通过修剪高斯函数来压缩这些预训练模型的技术依赖于结合启发式方法来确定要移除哪些高斯函数。在本文中，我们提出了一种基于原理的空间敏感性修剪评分，其性能优于这些方法。它被计算为训练视图上的重建误差相对于每个高斯函数的空间参数的二阶近似。此外，我们提出了一个多轮修剪-精炼流程，该流程可以应用于任何预训练的 3D-GS 模型，而无需更改训练流程。在修剪了 88.44% 的高斯函数后，我们观察到我们的 PUP 3D-GS 流程将 3D-GS 的平均渲染速度提高了 2.65 倍，同时保留了更多显著的前景信息，并在 Mip-NeRF 360、Tanks & Temples 和 Deep Blending 数据集的场景上实现了比以往修剪技术更高的图像质量指标。|
|**2024-06-14**|[GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors](http://arxiv.org/abs/2406.10111)|null|从低分辨率输入视图实现高分辨率新视角合成 (HRNVS) 是一项具有挑战性的任务，因为缺乏高分辨率数据。先前的方法从低分辨率输入视图优化高分辨率神经辐射场 (NeRF)，但渲染速度缓慢。在这项工作中，我们基于 3D 高斯散射 (3DGS) 来开发我们的方法，因为它能够以更快的渲染速度生成高质量的图像。为了缓解更高分辨率合成的数据短缺问题，我们建议利用现成的 2D 扩散先验，通过分数蒸馏采样 (SDS) 将 2D 知识提取到 3D。然而，由于生成先验带来的随机性，将 SDS 直接应用于基于高斯的 3D 超分辨率会导致不希望的和冗余的 3D 高斯图元。为了缓解这个问题，我们引入了两种简单而有效的技术来减少 SDS 引入的随机扰动。具体来说，我们 1) 使用退火策略缩小 SDS 中扩散时间步长的范围；2) 在密集化过程中随机丢弃冗余的高斯图元。大量实验表明，我们提出的 GaussainSR 仅使用合成数据集和真实世界数据集上的低分辨率输入，就可以获得 HRNVS 的高质量结果。项目页面：https://chchnii.github.io/GaussianSR/|
|**2024-06-14**|[GradeADreamer: Enhanced Text-to-3D Generation Using Gaussian Splatting and Multi-View Diffusion](http://arxiv.org/abs/2406.09850)|**[link](https://github.com/trapoom555/gradeadreamer)**|文本到3D生成已经展现出可观的结果，但仍然面临着一些常见的挑战，例如多面 Janus 问题以及高质量资源生成时间过长的问题。在本文中，我们通过引入一种名为 GradeADreamer 的新型三阶段训练流程来解决这些问题。该流程能够仅使用单个 RTX 3090 GPU 在 30 分钟内生成高质量的资源。我们提出的方法采用多视图扩散模型 MVDream 生成高斯球体作为先验，然后使用 StableDiffusion 优化几何形状和纹理。实验结果表明，与之前最先进的方法相比，我们的方法显著减轻了多面 Janus 问题，并实现了最高的平均用户偏好排名。项目代码可在 https://github.com/trapoom555/GradeADreamer 获取。|
|**2024-06-14**|[Unified Gaussian Primitives for Scene Representation and Rendering](http://arxiv.org/abs/2406.09733)|null|在计算机图形学中，寻找一种统一的场景表示方法仍然是一个挑战。传统的基于网格的表示方法不适用于密集、模糊的元素，并且在过滤和可微渲染方面引入了额外的复杂性。相反，基于体素的表示方法难以模拟坚硬的表面，并且存在内存需求大的问题。我们提出了一种基于三维高斯分布的通用渲染基元，用于统一场景表示，其特点是具有从光滑表面到模糊元素的多样化外观，以及基于物理的散射，以实现精确的全局照明。我们基于非指数传输制定了基元的渲染理论，并推导了与蒙特卡洛路径追踪兼容的高效渲染操作。这种新的表示方法可以从不同的来源转换而来，包括网格和三维高斯 splatting，并且由于其可微性，可以通过透射率优化进一步细化。我们展示了该表示方法在全局照明和外观编辑等各种渲染应用中的多功能性，同时本质上支持任意照明条件。此外，我们将我们的表示方法与现有的体积表示方法进行了比较，突出了其在细节再现方面的效率。|
|**2024-06-13**|[Modeling Ambient Scene Dynamics for Free-view Synthesis](http://arxiv.org/abs/2406.09395)|null|我们提出了一种新颖的动态自由视点合成方法，可以从单目捕捉中合成环境场景，为观看体验带来沉浸式的质量。我们的方法建立在3D高斯渲染(3DGS)的最新进展之上，该技术可以忠实地重建复杂的静态场景。先前将3DGS扩展到表示动态的尝试仅限于有界场景或需要多相机捕捉，并且通常无法泛化到未见过的运动，从而限制了它们的实际应用。我们的方法通过利用环境运动的周期性来学习运动轨迹模型，并结合仔细的正则化来克服这些限制。我们还提出了一些重要的实用策略，以提高基线3DGS静态重建的视觉质量，并提高对GPU内存密集型学习至关重要的内存效率。我们展示了几个具有复杂纹理和精细结构元素的周围自然场景的高质量逼真新颖视图合成。||
|**2024-06-13**|[GGHead: Fast and Generalizable 3D Gaussian Heads](http://arxiv.org/abs/2406.09377)|null|从大型二维图像集中学习三维头部先验信息是实现高质量三维感知人体建模的重要步骤。其核心要求是构建一种能够很好地扩展到大型数据集和高分辨率图像的有效架构。遗憾的是，现有的三维生成对抗网络（GAN）由于其训练和渲染速度相对较慢，难以扩展到生成高分辨率样本，并且通常不得不依赖二维超分辨率网络，但这是以牺牲全局三维一致性为代价的。为了应对这些挑战，我们提出了生成式高斯头部（GGHead）方法，该方法在三维 GAN 框架内采用了最新的三维高斯渲染表示。为了生成三维表示，我们采用了一个强大的二维卷积神经网络（CNN）生成器来预测模板头部网格的 UV 空间中的高斯属性。通过这种方式，GGHead 利用了模板 UV 布局的规律性，极大地促进了预测非结构化三维高斯集这一具有挑战性的任务。我们进一步通过一种新颖的渲染 UV 坐标上的总变差损失来提高生成的三维表示的几何保真度。直观地说，这种正则化鼓励相邻渲染像素应该来自模板 UV 空间中的相邻高斯。综上所述，我们的流程可以有效地生成仅从单视图二维图像观察中训练得到的三维头部。我们提出的框架在 FFHQ 数据集上的质量与现有的三维头部 GAN 相当，同时速度明显更快，并且完全保持了三维一致性。因此，我们首次展示了以 1024² 分辨率实时生成和渲染高质量、三维一致的头部。||

<p align=right>(<a href=#updated-on-20240619>back to top</a>)</p>

## 各类学习方式

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-18**|[GroPrompt: Efficient Grounded Prompting and Adaptation for Referring Video Object Segmentation](http://arxiv.org/abs/2406.12834)|null|指代视频目标分割 (RVOS) 旨在分割整个视频中查询语句所指代的目标。大多数现有方法需要使用密集掩码注释进行端到端训练，这可能非常耗时且可扩展性较差。在这项工作中，我们旨在利用所提出的 Grounded Prompting (GroPrompt) 框架，有效地调整基础分割模型，以便从弱监督中解决 RVOS 问题。更具体地说，我们提出了文本感知提示对比学习 (TAP-CL)，仅使用边界框监督来增强位置提示和指代表达式之间的关联，包括分别在帧级别和视频级别的文本对比提示学习 (TextCon) 和模态对比提示学习 (ModalCon)。借助所提出的 TAP-CL，我们的 GroPrompt 框架可以生成描述视频中所指对象的位置和移动的时间一致且文本感知的位置提示。在标准 RVOS 基准测试（Ref-YouTube-VOS、Ref-DAVIS17、A2D-Sentences 和 JHMDB-Sentences）中的实验结果表明，在仅给出边界框弱监督的情况下，我们提出的 GroPrompt 框架具有竞争力的性能。|
|**2024-06-18**|[Unsupervised explainable activity prediction in competitive Nordic Walking from experimental data](http://arxiv.org/abs/2406.12762)|null|人工智能 (AI) 已应用于竞技体育中的人体活动识别 (HAR)。迄今为止，大多数用于 HAR 的机器学习 (ML) 方法都依赖于离线（批处理）训练，与在线处理无监督方法相比，这带来了更高的计算和标记负担。此外，传统 ML 预测器背后的决策是不透明的，需要人工解释。在这项工作中，我们应用了一种基于低成本可穿戴惯性测量单元 (IMU) 的在线处理无监督聚类方法。系统生成的结果允许在这些集群内自动扩展有限的可用标记（例如，由裁判标记），从而为可解释分类阶段生成相关信息。具体来说，我们的工作重点是实现与运动员活动相关的预测的自动可解释性，区分北欧式健走中的正确、错误和作弊行为。所提出的解决方案实现了接近 100% 的平均性能指标。|
|**2024-06-18**|[BIOSCAN-5M: A Multimodal Dataset for Insect Biodiversity](http://arxiv.org/abs/2406.12723)|**[link](https://github.com/zahrag/BIOSCAN-5M)**|作为一项持续进行的理解和监测昆虫生物多样性的全球努力的一部分，本文向机器学习社区推出了 BIOSCAN-5M 昆虫数据集，并建立了多个基准任务。BIOSCAN-5M 是一个综合数据集，包含超过 500 万个昆虫样本的多模态信息，它通过包含分类标签、原始核苷酸条形码序列、分配的条形码索引号和地理信息，显著扩展了现有的基于图像的生物数据集。我们提出了三个基准实验，以证明多模态数据类型对分类和聚类准确性的影响。首先，我们在 BIOSCAN-5M 数据集的 DNA 条形码序列上预训练了一个掩码语言模型，并证明了使用这个大型参考库对物种和属级别分类性能的影响。其次，我们提出了一个应用于图像和 DNA 条形码的零样本迁移学习任务，以对从自监督学习中获得的特征嵌入进行聚类，以研究是否可以从这些表示嵌入中导出有意义的聚类。第三，我们通过对 DNA 条形码、图像数据和分类信息进行对比学习来对多模态进行基准测试。这产生了一个通用的共享嵌入空间，可以使用多种类型的信息和模态进行分类。BIOSCAN-5M 昆虫数据集的代码库可在以下网址获得：{\url{https://github.com/zahrag/BIOSCAN-5M}}|
|**2024-06-18**|[Reproducibility of predictive networks for mouse visual cortex](http://arxiv.org/abs/2406.12625)|null|近年来，神经元活动的深度预测模型使得关于视觉皮层神经元选择性和不变性的若干新发现成为可能。这些模型学习一组共享的非线性基函数，这些基函数通过学习的权重向量进行线性组合，以表示神经元的函数。这种权重向量可以被认为是神经元功能的嵌入，已被提议通过无监督聚类来定义功能细胞类型。然而，由于深度模型通常高度过参数化，学习问题不太可能有唯一的解决方案，这就引发了一个问题，即这种嵌入是否可以以有意义的方式用于下游分析。在本文中，我们研究了神经元嵌入相对于模型架构和初始化变化的稳定性。我们发现 $L_1$ 正则是结构化嵌入的重要因素，并开发了一种自适应正则化，可以根据每个神经元调整正则化的强度。与统一正则化相比，这种正则化提高了预测性能，以及神经元嵌入在模型拟合中聚类的稳定性。为了克服过参数化，我们提出了一种迭代特征剪枝策略，可以在不损失性能的情况下将性能优化模型的维数减少一半，并提高神经元嵌入在聚类神经元方面的一致性。这一结果表明，为了实现细胞类型的客观分类或功能景观的紧凑表示，我们需要新的架构或学习技术来提高可识别性。我们将在发表时提供代码。|
|**2024-06-18**|[PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental Learning for Document Retrieval](http://arxiv.org/abs/2406.12593)|null|可微搜索索引 (DSI) 利用预训练语言模型 (PLM) 实现高效的文档检索，而无需依赖外部索引。然而，DSI 需要完全重新训练才能处理动态语料库中的更新，这会导致严重的计算效率低下问题。我们引入了 PromptDSI，这是一种基于提示的免排练实例级增量学习方法，用于文档检索。PromptDSI 将提示附加到 DSI 冻结的 PLM 编码器上，利用其强大的表示能力有效地索引新语料库，同时保持稳定性和可塑性之间的平衡。我们消除了基于提示的持续学习方法的初始前向传递，该传递会使训练和推理时间加倍。此外，我们提出了一个主题感知提示池，它使用神经主题嵌入作为固定键。该策略确保了多样化和有效的提示使用，解决了由于查询键匹配机制崩溃导致的参数利用不足的挑战。我们的实证评估表明，PromptDSI 在管理遗忘方面与 IncDSI 相匹配，同时在新语料库上显着提高了 4% 以上的召回率。|
|**2024-06-18**|[Unsupervised Online Continual Learning for Automatic Speech Recognition](http://arxiv.org/abs/2406.12503)|null|将自动语音识别 (ASR) 模型适应新领域会导致对先前学习信息的灾难性遗忘 (CF)。本文解决了在线持续学习 (OCL) 挑战性背景下的 CF 问题，其中任务以具有未知边界的连续数据流的形式呈现。我们通过利用自训练 (ST) 来促进无监督适应，将 ASR 的 OCL 扩展到无监督领域，使模型能够在不依赖标签且不忘记先前知识的情况下不断适应。通过对两个领域适应实验中各种 OCL 和 ST 方法的比较分析，我们发现与监督 OCL 相比，无监督 OCL 遭受的遗忘要少得多，这使得 UOCL 方法能够接近监督 OCL 的性能水平。我们提出的 UOCL 扩展进一步提高了 UOCL 的效率。我们的研究结果代表着向持续适应性 ASR 系统迈出的重要一步，该系统能够利用跨不同领域的未标记数据。|
|**2024-06-18**|[RIGL: A Unified Reciprocal Approach for Tracing the Independent and Group Learning Processes](http://arxiv.org/abs/2406.12465)|null|在教育领域，自主学习和协作学习都被视为最经典的范式。前者允许学习者自主指导学习，而后者通常以教师指导为特征。近年来，智能教育领域的研究利用深度时间模型来追踪学习过程，捕捉学生知识状态的动态变化，并取得了显著成果。然而，现有方法主要集中于对自主学习过程的建模，而对协作学习范式的关注较少。此外，两种学习过程之间的相互影响，特别是它们在促进学生全面发展方面的综合潜力，仍未得到充分探索。为此，在本文中，我们提出了RIGL，一个统一的交互模型，用于在个体和群体层面追踪知识状态，该模型借鉴了自主学习和协作学习过程。具体而言，我们首先引入了一个时间帧感知的交互嵌入模块，用于同时对不同时间帧中学生和群体响应交互进行建模。随后，我们采用交互增强的学习模型来充分利用两种行为之间全面和互补的信息。此外，我们设计了一个关系引导的时间注意力网络，该网络由动态图建模和时间自注意力机制组成，用于深入研究个体和群体交互在整个学习过程中的动态影响。最后，我们引入了一个偏差感知的对比学习模块，以增强模型训练的稳定性。在四个真实世界教育数据集上的大量实验清楚地证明了所提出的RIGL模型的有效性。|
|**2024-06-18**|[Effective Generation of Feasible Solutions for Integer Programming via Guided Diffusion](http://arxiv.org/abs/2406.12349)|null|可行解对于整数规划 (IP) 至关重要，因为它们可以大大加快求解过程。在许多应用中，类似的 IP 实例通常表现出相似的结构和共享的解分布，这可以使用深度学习方法进行建模。不幸的是，现有的基于深度学习的算法，例如神经潜水和预测搜索框架，仅限于生成部分可行解，并且它们必须依赖 SCIP 和 Gurobi 等求解器来完成给定 IP 问题的解。在本文中，我们提出了一个新的框架，可以端到端地生成完整的可行解。我们的框架利用对比学习来表征 IP 实例和解之间的关系，并学习 IP 实例及其解的潜在嵌入。此外，该框架采用扩散模型来学习以 IP 表示为条件的解嵌入分布，并采用专门的引导采样策略来考虑约束和目标。我们根据 IP 问题的四个典型数据集对我们的框架进行了实证评估，结果表明，它可以有效地生成完整的可行解，并且概率很高（> 89.7%），而无需依赖求解器，并且解的质量可与 Gurobi 最好的启发式解相媲美。此外，通过将我们方法采样的部分解与 SCIP 的 CompleteSol 启发式算法相结合，所得的可行解优于所有数据集中的最新方法，与最优值的差距提高了 3.7% 到 33.7%，并在所有数据集中保持了超过 99.7% 的可行比率。|
|**2024-06-18**|[Toward Exploring the Code Understanding Capabilities of Pre-trained Code Generation Models](http://arxiv.org/abs/2406.12326)|null|近年来，以自监督方式在大量未标记编程语言数据上训练的大型代码生成模型取得了显著成功。虽然这些模型获得了大量的代码知识，但由于它们专门针对生成进行了训练，因此在代码理解任务（如代码搜索和克隆检测）中表现不佳。从头开始在海量代码数据上预训练一个更大的仅编码器架构模型可以提高理解性能。然而，这种方法成本高昂且耗时，因此并非最佳选择。在本文中，我们率先将知识从预训练的代码生成模型迁移到代码理解任务，显著降低了训练成本。我们研究了使仅解码器模型能够获得稳健代码表示的有效策略。此外，我们还引入了CL4D，这是一种对比学习方法，旨在增强仅解码器模型的表示能力。综合实验表明，我们的方法在代码搜索和克隆检测等理解任务中实现了最先进的性能。我们的分析表明，我们的方法有效地减少了表示空间中语义相同样本之间的距离。这些发现表明，可以使用仅解码器结构模型来统一代码理解和生成任务的潜力。|
|**2024-06-18**|[COT: A Generative Approach for Hate Speech Counter-Narratives via Contrastive Optimal Transport](http://arxiv.org/abs/2406.12304)|null|反叙事，即由基于事实的非攻击性论点组成的直接回应，已成为打击仇恨言论扩散的一种非常有效的方法。以前的方法主要侧重于微调和后期编辑技术，以确保生成内容的流畅性，而忽略了与特定仇恨目标（如LGBT群体、移民等）相关的个性化和相关性等关键方面。本研究论文介绍了一种基于对比最优传输的新框架，该框架有效地解决了在生成反叙事时维护目标交互和促进多样化的挑战。首先，利用最优传输核（OTK）模块将仇恨目标信息纳入到词元表示中，其中在原始特征和传输特征之间提取比较对。其次，采用自对比学习模块来解决模型退化的问题。该模块通过生成词元表示的各向异性分布来实现这一点。最后，集成了面向目标的搜索方法作为一种改进的解码策略，以在推理过程中明确促进领域相关性和多样性。该策略通过同时考虑词元相似性和目标相关性来修改模型的置信度得分。在两个基准数据集上进行了定量和定性实验，结果表明，我们提出的模型在多个方面的指标评估中明显优于当前的方法。|
|**2024-06-17**|[Mix-Domain Contrastive Learning for Unpaired H&E-to-IHC Stain Translation](http://arxiv.org/abs/2406.11799)|null|苏木精-伊红 (H&E) 染色到免疫组化 (IHC) 染色的图像转换技术为精确的癌症诊断提供了一种很有前景的解决方案，特别是在缺乏专业医疗人员和昂贵设备资源匮乏的地区。考虑到 H&E-IHC 图像对之间存在像素级别的错位，当前的研究探索了图像对相同位置的图像块之间的病理学一致性。然而，大多数方法过度强调域或图像块之间的对应关系，而忽略了非对应目标提供的辅助信息。在本文中，我们提出了一种混合域对比学习 (MDCL) 方法，以利用非配对 H&E 到 IHC 染色转换中的监督信息。具体来说，所提出的 MDCL 方法通过估计锚点图像块与匹配图像中所有图像块之间的相关性来聚合域间和域内的病理学信息，鼓励网络从混合域中学习额外的对比知识。通过混合域病理信息聚合，MDCL 增强了对应图像块之间的病理学一致性，以及生成的 IHC 图像中不同位置的图像块之间的成分差异。在两个 H&E 到 IHC 染色转换数据集（即 MIST 和 BCI）上的大量实验表明，所提出的方法在多个指标上均达到了最先进的性能。||
|**2024-06-17**|[A Brief Survey on Leveraging Large Scale Vision Models for Enhanced Robot Grasping](http://arxiv.org/abs/2406.11786)|null|在现实世界场景中，机器人抓取是一项艰巨的运动任务，构成了在各行各业部署高性能机器人的主要障碍。值得注意的是，数据的稀缺使得学习模型的抓取尤其具有挑战性。近年来，计算机视觉领域的进步见证了基于互联网海量数据的成功无监督训练机制的发展，现在几乎所有杰出的模型都利用了预训练的骨干网络。在此背景下，我们开始研究大规模视觉预训练在提高机器人抓取性能方面的潜在优势。这篇初步的文献综述阐明了关键挑战，并为机器人操作的视觉预训练的未来研究方向进行了展望。||
|**2024-06-17**|[DiffMM: Multi-Modal Diffusion Model for Recommendation](http://arxiv.org/abs/2406.11781)|null|像抖音和YouTube这样的在线多模态分享平台的兴起使得个性化推荐系统能够将多种模态（如视觉、文本和音频）纳入用户表示中。然而，解决这些系统中数据稀疏性的挑战仍然是一个关键问题。为了解决这一局限性，最近的研究引入了自监督学习技术来增强推荐系统。然而，这些方法通常依赖于简单的随机增强或直观的跨视图信息，这可能会引入不相关的噪声，并且无法准确地将多模态上下文与用户-项目交互建模对齐。为了填补这一研究空白，我们提出了一种新的用于推荐的多模态图扩散模型，称为DiffMM。我们的框架将模态感知图扩散模型与跨模态对比学习范式相结合，以改进模态感知用户表示学习。这种整合促进了多模态特征信息与协同关系建模之间更好的对齐。我们的方法利用扩散模型的生成能力自动生成一个感知不同模态的用户-项目图，从而促进在用户-项目交互建模中纳入有用的多模态知识。我们在三个公共数据集上进行了广泛的实验，一致证明了我们的DiffMM相对于各种竞争基线的优越性。有关开源模型实现的详细信息，您可以访问以下地址获取我们提出的框架的源代码：https://github.com/HKUDS/DiffMM。||
|**2024-06-17**|[Zero-Shot Generalization during Instruction Tuning: Insights from Similarity and Granularity](http://arxiv.org/abs/2406.11721)|**[link](https://github.com/hbx-hbx/dynamics_of_zero-shot_generalization)**|理解对齐技术始于理解指令微调带来的零样本泛化能力，但人们对其机制知之甚少。现有的研究主要局限于任务层面，而没有考虑到任务是人为定义的，对于大型语言模型来说，仅仅是由token和表示组成的。这条研究路线仅限于从任务对的角度研究任务之间的迁移，很少有研究从数据本身的角度理解零样本泛化。为了弥合这一差距，我们首先通过多个指标证明，指令微调过程中的零样本泛化发生在非常早期的阶段。接下来，我们从数据相似性和粒度两个角度研究了零样本泛化的促进因素，证实了在指令微调的早期阶段遇到高度相似和细粒度的训练数据，而不受限于定义的“任务”，能够实现更好的泛化。最后，我们提出了一种更合理的训练数据编排方法，即以测试为中心的多轮编排，并展示了其在促进持续学习和进一步降低损失方面的有效性。我们首次证明，指令微调过程中的零样本泛化是训练数据和测试数据之间在实例级别的基于相似性的泛化形式。我们希望我们的分析能够促进对指令微调过程中零样本泛化的理解，并有助于开发更对齐的大型语言模型。我们的代码发布在https://github.com/HBX-hbx/dynamics_of_zero-shot_generalization。||
|**2024-06-17**|[Multiple Descents in Unsupervised Learning: The Role of Noise, Domain Shift and Anomalies](http://arxiv.org/abs/2406.11703)|null|双下降现象最近在监督学习中受到关注。它挑战了偏差-方差权衡的传统观点，展示了一种令人惊讶的行为。随着模型复杂度的增加，测试误差最初会下降，直到达到模型开始过拟合训练集的某个点，导致测试误差上升。然而，与经典理论不同的是，当超过一定程度的过参数化时，误差会再次下降。我们研究了双下降现象在无监督学习中的存在，这是一个很少受到关注且尚未完全理解的领域。我们使用欠完备自动编码器 (AE) 对各种应用进行了广泛的实验，例如处理噪声数据、域偏移和异常。我们使用合成数据和真实数据，并确定了上述所有应用中模型、时期和样本方面的双下降现象。最后，我们评估了自动编码器在检测异常和减轻数据集之间域偏移方面的可用性。我们的研究结果表明，过参数化模型不仅可以提高重建性能，还可以增强下游任务的能力。||
|**2024-06-17**|[Making Old Things New: A Unified Algorithm for Differentially Private Clustering](http://arxiv.org/abs/2406.11649)|null|作为数据分析和无监督学习的支柱，私有聚类问题已在各种隐私模型下得到了广泛研究。集中式差分隐私是其中最早的模型，并且该问题也已针对本地和随机变换进行了研究。在每种情况下，目标都是设计一种算法，以尽可能小的误差私下计算聚类。对每种变体的研究都催生了新的算法：因此，私有聚类算法的图景相当复杂。在本文中，我们展示了一种已有 20 年历史的算法，只需稍作修改即可适用于任何这些模型。这提供了一个统一的视角：在匹配几乎所有先前已知结果的同时，它允许我们改进其中的一些结果，并将其扩展到一个新的隐私模型，即持续观察设置，其中输入随时间而变化，并且算法必须在每个时间步输出一个新的解决方案。||
|**2024-06-17**|[Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!](http://arxiv.org/abs/2406.11629)|null|利用大型语言模型 (LLM) 作为评判者来评估 LLM 的性能最近受到了关注。然而，这种方法同时引入了来自 LLM 的潜在偏差，引发了对评估结果可靠性的担忧。为了缓解这个问题，我们提出并研究了两种版本的少样本上下文提示，即强化和无监督 ICL，用于帮助 GPT-4o 作为评判者进行单答案评分。基于设计的提示，我们研究了扩展上下文示例数量对评估的一致性和质量的影响。此外，我们首先揭示了 GPT-4o 作为评判者在成对比较中的符号偏差，然后提出了一种简单而有效的方法来减轻它。实验结果表明，先进的长上下文 LLM，例如 GPT-4o，在少样本机制中的表现优于零样本机制。同时，实验结果进一步验证了符号偏差缓解方法的有效性。||
|**2024-06-17**|[Wide Area VISTA Extra-galactic Survey (WAVES): Unsupervised star-galaxy separation on the WAVES-Wide photometric input catalogue using UMAP and ${\rm{\scriptsize HDBSCAN}}$](http://arxiv.org/abs/2406.11611)|null|星系分离是创建河外光谱巡天目标星表的关键步骤。倾向于包容性的分类器可能会将伪星包含在内，浪费光纤时间，而更保守的分类器可能会忽略星系，从而损害完整性，进而影响巡天目标。为了避免监督方法中训练集引入的偏差，我们采用了一种无监督机器学习方法。我们利用广域 VISTA 河外星系巡天 (WAVES)-Wide 星表的光度数据，该星表包含 9 波段 $u-K_s$ 数据，并利用 ${\rm P{\scriptsize RO} F{\scriptsize OUND}}$ 提取颜色、流量和视尺寸信息，创建了一个特征空间。我们应用非线性降维方法 UMAP（均匀流形近似和投影）结合分类器 ${\rm{\scriptsize HDBSCAN}}$ 对恒星和星系进行分类。我们使用来自 Gaia、SDSS、GAMA 和 DESI 的真实星表，根据基线颜色和形态方法验证了我们的方法。在 AB 星等限制为 $Z = 21.2$ 的情况下，我们正确识别了 99.72% 的星系，在整个真实样本中，F1 得分为 0.9970，而基线方法的 F1 得分为 0.9871。与基线方法 (0.9780) 相比，我们的方法具有更高的纯度 (0.9966)，从而提高了效率，识别出的星系或不明来源减少了 11%，在 4MOST 仪器上节省了大约 70,000 个光纤小时。我们获得了具有挑战性的来源（包括类星体、致密星系和低表面亮度星系）的可靠分类统计数据，分别检索到其中的 95.1%、84.6% 和 99.5%。角聚类分析验证了我们的分类，表明无论基线分类如何，都与预期的星系聚类一致。||
|**2024-06-17**|[CM2-Net: Continual Cross-Modal Mapping Network for Driver Action Recognition](http://arxiv.org/abs/2406.11340)|null|驾驶员动作识别通过整合红外和深度等多种模态，在增强驾驶员与车辆的交互和确保驾驶安全方面取得了显著进展。然而，与仅使用RGB模态相比，在车厢环境中为所有类型的非RGB模态收集大量数据始终是一项费力且昂贵的工作。因此，以前的工作建议通过微调在RGB视频上预训练的模型来独立学习每个非RGB模态，但这些方法在面对新出现的模态时，由于较大的域差异，在提取信息特征方面效果较差。相比之下，我们提出了一种连续跨模态映射网络（CM2-Net），利用先前学习的模态的指导性提示，持续学习每个新出现的模态。具体来说，我们开发了累积跨模态映射提示（ACMP），将从先前模态中学习到的判别性和信息性特征映射到新出现的模态的特征空间中。然后，当面对新出现的模态时，这些映射的特征能够为应该提取和优先考虑哪些特征提供有效的提示。这些提示在整个持续学习过程中不断积累，从而进一步提高识别性能。在Drive&Act数据集上进行的大量实验表明，CM2-Net在单模态和多模态驾驶员动作识别方面均具有优越的性能。||
|**2024-06-17**|[Syn-to-Real Unsupervised Domain Adaptation for Indoor 3D Object Detection](http://arxiv.org/abs/2406.11311)|null|在室内3D目标检测中使用合成数据，可以大大减少3D标注所需的人工，并训练有效的零样本检测器。然而，跨越合成到真实室内数据集的复杂域偏移问题仍未得到充分探索。本文提出了一种新的面向对象的层次化域对齐（OHDA）框架，用于室内3D目标检测中的合成到真实无监督域自适应。我们的方法包括一种对象感知增强策略，以有效地使源域数据多样化，并且我们引入了一个由对抗训练分支和伪标签分支组成的双分支自适应框架，以便同时实现整体级别和类别级别的域对齐。针对室内无监督域自适应，我们提出了两种专门设计的方案，进一步改进了伪标签。我们从合成数据集3D-FRONT到真实世界数据集ScanNetV2和SUN RGB-D的适应结果表明，与仅使用源数据的基线相比，mAP25分别显著提高了9.7%和9.1%，并且始终优于从2D和3D室外场景适应的方法。代码将在论文被接收后公开。||

<p align=right>(<a href=#updated-on-20240619>back to top</a>)</p>

