## Updated on 2024.09.05
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#多模态>多模态</a></li>
    <li><a href=#6dof-object-pose>6DOF Object Pose</a></li>
    <li><a href=#nerf>nerf</a></li>
    <li><a href=#分类/检测/识别/分割>分类/检测/识别/分割</a></li>
    <li><a href=#生成模型>生成模型</a></li>
    <li><a href=#llm>LLM</a></li>
    <li><a href=#transformer>Transformer</a></li>
  </ol>
</details>

## 多模态

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-09-04**|[Can LVLMs Obtain a Driver's License? A Benchmark Towards Reliable AGI for Autonomous Driving](http://arxiv.org/abs/2409.02914)|null|近年来，大型视觉语言模型 (LVLM) 引起了广泛关注，许多研究致力于利用其通用知识来增强自动驾驶模型的可解释性和鲁棒性。然而，LVLM 通常依赖于大型通用数据集，缺乏专业安全驾驶所需的专业知识。现有的视觉语言驾驶数据集主要关注场景理解和决策，没有提供与驾驶安全直接相关的交通规则和驾驶技能方面的明确指导。为了弥合这一差距，我们提出了 IDKB，这是一个包含来自多个国家的一百多万个数据项的大型数据集，其中包括驾驶手册、理论测试数据和模拟道路测试数据。就像获得驾驶执照的过程一样，IDKB 涵盖了从理论到实践驾驶所需的几乎所有显性知识。特别是，我们使用 IDKB 对 15 个 LVLM 进行了全面测试，以评估其在自动驾驶环境中的可靠性，并提供了广泛的分析。我们还微调了流行模型，实现了显著的性能提升，这进一步验证了我们数据集的重要性。项目页面可以在以下网址找到：\url{https://4dvlab.github.io/project_page/idkb.html}|
|**2024-09-04**|[Benchmarking Spurious Bias in Few-Shot Image Classifiers](http://arxiv.org/abs/2409.02882)|**[link](https://github.com/gtzheng/fewstab)**|少样本图像分类器旨在用最少的监督和有限的数据识别和分类新数据，但通常表现出对类和虚假属性之间虚假相关性的依赖，这被称为虚假偏差。虚假相关性通常存在于某些样本中，而少样本分类器可能会受到由它们引起的虚假偏差的影响。目前缺乏一个自动化的基准测试系统来评估少样本分类器针对虚假偏差的鲁棒性。在本文中，我们提出了一个系统且严格的基准测试框架，称为 FewSTAB，以公平地展示和量化少样本分类器对虚假偏差的不同程度的鲁棒性。FewSTAB 创建了具有偏差属性的少样本评估任务，因此使用它们进行预测可以证明性能不佳。为了构建这些任务，我们提出了一种基于预训练的视觉语言模型的基于属性的样本选择策略，从而无需手动进行数据集管理。这使得 FewSTAB 可以使用任何现有的测试数据自动对虚假偏差进行基准测试。FewSTAB 提供了一个新维度的评估结果，以及构建鲁棒分类器的新设计指南。此外，它可以对不同程度的虚假偏差进行基准测试，并支持针对不同程度的鲁棒性进行设计。通过对三个数据集上的十种少样本学习方法进行实验，证明了其有效性。我们希望我们的框架能够启发鲁棒的少样本分类器的新设计。我们的代码可在 https://github.com/gtzheng/FewSTAB 获取。|
|**2024-09-04**|[CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of Large Multimodal Models](http://arxiv.org/abs/2409.02834)|null|Large language models (LLMs) have obtained promising results in mathematical reasoning, which is a foundational skill for human intelligence. Most previous studies focus on improving and measuring the performance of LLMs based on textual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few researchers have released English multimodal math datasets (e.g., MATHVISTA and MATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In this paper, we release a Chinese multimodal math (CMM-Math) dataset, including benchmark and training parts, to evaluate and enhance the mathematical reasoning of LMMs. CMM-Math contains over 28,000 high-quality samples, featuring a variety of problem types (e.g., multiple-choice, fill-in-the-blank, and so on) with detailed solutions across 12 grade levels from elementary to high school in China. Specifically, the visual context may be present in the questions or opinions, which makes this dataset more challenging. Through comprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math dataset face challenges, emphasizing the necessity for further improvements in LMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to handle the problems with mixed input of multiple images and text segments. We train our model using three stages, including foundational pre-training, foundational fine-tuning, and mathematical fine-tuning. The extensive experiments indicate that our model effectively improves math reasoning performance by comparing it with the SOTA LMMs over three multimodal mathematical datasets.|
|**2024-09-04**|[MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark](http://arxiv.org/abs/2409.02813)|null|This paper introduces MMMU-Pro, a robust version of the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark. MMMU-Pro rigorously assesses multimodal models' true understanding and reasoning capabilities through a three-step process based on MMMU: (1) filtering out questions answerable by text-only models, (2) augmenting candidate options, and (3) introducing a vision-only input setting where questions are embedded within images. This setting challenges AI to truly "see" and "read" simultaneously, testing a fundamental human cognitive skill of seamlessly integrating visual and textual information. Results show that model performance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8% to 26.9% across models. We explore the impact of OCR prompts and Chain of Thought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT generally improves performance. MMMU-Pro provides a more rigorous evaluation tool, closely mimicking real-world scenarios and offering valuable directions for future research in multimodal AI.|
|**2024-09-04**|[Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection](http://arxiv.org/abs/2409.02664)|null|The proliferation of deepfake faces poses huge potential negative impacts on our daily lives. Despite substantial advancements in deepfake detection over these years, the generalizability of existing methods against forgeries from unseen datasets or created by emerging generative models remains constrained. In this paper, inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach that repurposes a well-trained VLM for general deepfake detection. Motivated by the model reprogramming paradigm that manipulates the model prediction via data perturbations, our method can reprogram a pretrained VLM model (e.g., CLIP) solely based on manipulating its input without tuning the inner parameters. Furthermore, we insert a pseudo-word guided by facial identity into the text prompt. Extensive experiments on several popular benchmarks demonstrate that (1) the cross-dataset and cross-manipulation performances of deepfake detection can be significantly and consistently improved (e.g., over 88% AUC in cross-dataset setting from FF++ to WildDeepfake) using a pre-trained CLIP model with our proposed reprogramming method; (2) our superior performances are at less cost of trainable parameters, making it a promising approach for real-world applications.|
|**2024-09-04**|[Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models](http://arxiv.org/abs/2409.02530)|null|The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of kidney function in clinical practice. Although traditional equations and Machine Learning (ML) models using clinical and laboratory data can estimate eGFR, accurately predicting future eGFR levels remains a significant challenge for nephrologists and ML researchers. Recent advances demonstrate that Large Language Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust foundation models for diverse applications. This study investigates the potential of LMMs to predict future eGFR levels with a dataset consisting of laboratory and clinical values from 50 patients. By integrating various prompting techniques and ensembles of LMMs, our findings suggest that these models, when combined with precise prompts and visual representations of eGFR trajectories, offer predictive performance comparable to existing ML models. This research extends the application of foundation models and suggests avenues for future studies to harness these models in addressing complex medical forecasting challenges.|
|**2024-09-03**|[Evaluation and Comparison of Visual Language Models for Transportation Engineering Problems](http://arxiv.org/abs/2409.02278)|null|Recent developments in vision language models (VLM) have shown great potential for diverse applications related to image understanding. In this study, we have explored state-of-the-art VLM models for vision-based transportation engineering tasks such as image classification and object detection. The image classification task involves congestion detection and crack identification, whereas, for object detection, helmet violations were identified. We have applied open-source models such as CLIP, BLIP, OWL-ViT, Llava-Next, and closed-source GPT-4o to evaluate the performance of these state-of-the-art VLM models to harness the capabilities of language understanding for vision-based transportation tasks. These tasks were performed by applying zero-shot prompting to the VLM models, as zero-shot prompting involves performing tasks without any training on those tasks. It eliminates the need for annotated datasets or fine-tuning for specific tasks. Though these models gave comparative results with benchmark Convolutional Neural Networks (CNN) models in the image classification tasks, for object localization tasks, it still needs improvement. Therefore, this study provides a comprehensive evaluation of the state-of-the-art VLM models highlighting the advantages and limitations of the models, which can be taken as the baseline for future improvement and wide-scale implementation.|
|**2024-09-03**|[How to Determine the Preferred Image Distribution of a Black-Box Vision-Language Model?](http://arxiv.org/abs/2409.02253)|null|Large foundation models have revolutionized the field, yet challenges remain in optimizing multi-modal models for specialized visual tasks. We propose a novel, generalizable methodology to identify preferred image distributions for black-box Vision-Language Models (VLMs) by measuring output consistency across varied input prompts. Applying this to different rendering types of 3D objects, we demonstrate its efficacy across various domains requiring precise interpretation of complex structures, with a focus on Computer-Aided Design (CAD) as an exemplar field. We further refine VLM outputs using in-context learning with human feedback, significantly enhancing explanation quality. To address the lack of benchmarks in specialized domains, we introduce CAD-VQA, a new dataset for evaluating VLMs on CAD-related visual question answering tasks. Our evaluation of state-of-the-art VLMs on CAD-VQA establishes baseline performance levels, providing a framework for advancing VLM capabilities in complex visual reasoning tasks across various fields requiring expert-level visual interpretation. We release the dataset and evaluation codes at \url{https://github.com/asgsaeid/cad_vqa}.|
|**2024-09-03**|[Towards Real-World Adverse Weather Image Restoration: Enhancing Clearness and Semantics with Vision-Language Models](http://arxiv.org/abs/2409.02101)|null|This paper addresses the limitations of adverse weather image restoration approaches trained on synthetic data when applied to real-world scenarios. We formulate a semi-supervised learning framework employing vision-language models to enhance restoration performance across diverse adverse weather conditions in real-world settings. Our approach involves assessing image clearness and providing semantics using vision-language models on real data, serving as supervision signals for training restoration models. For clearness enhancement, we use real-world data, utilizing a dual-step strategy with pseudo-labels assessed by vision-language models and weather prompt learning. For semantic enhancement, we integrate real-world data by adjusting weather conditions in vision-language model descriptions while preserving semantic meaning. Additionally, we introduce an effective training strategy to bootstrap restoration performance. Our approach achieves superior results in real-world adverse weather image restoration, demonstrated through qualitative and quantitative comparisons with state-of-the-art works.|
|**2024-09-03**|[GraspSplats: Efficient Manipulation with 3D Feature Splatting](http://arxiv.org/abs/2409.02084)|null|The ability for robots to perform efficient and zero-shot grasping of object parts is crucial for practical applications and is becoming prevalent with recent advances in Vision-Language Models (VLMs). To bridge the 2D-to-3D gap for representations to support such a capability, existing methods rely on neural fields (NeRFs) via differentiable rendering or point-based projection methods. However, we demonstrate that NeRFs are inappropriate for scene changes due to their implicitness and point-based methods are inaccurate for part localization without rendering-based optimization. To amend these issues, we propose GraspSplats. Using depth supervision and a novel reference feature computation method, GraspSplats generates high-quality scene representations in under 60 seconds. We further validate the advantages of Gaussian-based representation by showing that the explicit and optimized geometry in GraspSplats is sufficient to natively support (1) real-time grasp sampling and (2) dynamic and articulated object manipulation with point trackers. With extensive experiments on a Franka robot, we demonstrate that GraspSplats significantly outperforms existing methods under diverse task settings. In particular, GraspSplats outperforms NeRF-based methods like F3RM and LERF-TOGO, and 2D detection methods.|

<p align=right>(<a href=#updated-on-20240905>back to top</a>)</p>

## 6DOF Object Pose

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-09-04**|[Object Gaussian for Monocular 6D Pose Estimation from Sparse Views](http://arxiv.org/abs/2409.02581)|null|Monocular object pose estimation, as a pivotal task in computer vision and robotics, heavily depends on accurate 2D-3D correspondences, which often demand costly CAD models that may not be readily available. Object 3D reconstruction methods offer an alternative, among which recent advancements in 3D Gaussian Splatting (3DGS) afford a compelling potential. Yet its performance still suffers and tends to overfit with fewer input views. Embracing this challenge, we introduce SGPose, a novel framework for sparse view object pose estimation using Gaussian-based methods. Given as few as ten views, SGPose generates a geometric-aware representation by starting with a random cuboid initialization, eschewing reliance on Structure-from-Motion (SfM) pipeline-derived geometry as required by traditional 3DGS methods. SGPose removes the dependence on CAD models by regressing dense 2D-3D correspondences between images and the reconstructed model from sparse input and random initialization, while the geometric-consistent depth supervision and online synthetic view warping are key to the success. Experiments on typical benchmarks, especially on the Occlusion LM-O dataset, demonstrate that SGPose outperforms existing methods even under sparse view constraints, under-scoring its potential in real-world applications.|
|**2024-08-29**|[OP-Align: Object-level and Part-level Alignment for Self-supervised Category-level Articulated Object Pose Estimation](http://arxiv.org/abs/2408.16547)|**[link](https://github.com/yc-che/op-align)**|类别级铰接物体姿态估计侧重于估计已知类别中未知铰接物体的姿态。尽管意义重大，但由于物体的形状和姿态各不相同、数据集标注成本高昂以及现实环境复杂，这项任务仍然具有挑战性。在本文中，我们提出了一种新颖的自监督方法，利用单帧点云来解决此任务。我们的模型始终如一地生成具有规范姿态和关节状态的完整输入对象的重建，并估计对象级姿态（减少整体姿态差异）和零件级姿态（将输入的每个零件与其对应零件对齐）。实验结果表明，我们的方法明显优于以往的自监督方法，并且与最先进的监督方法相当。为了评估我们的模型在现实场景中的性能，我们还引入了一个新的现实世界铰接物体基准数据集。|
|**2024-08-19**|[RUMI: Rummaging Using Mutual Information](http://arxiv.org/abs/2408.10450)|null|本文提出了基于互信息的翻找方法 (RUMI)，该方法用于在线生成机器人在视觉遮挡环境中收集已知可移动物体姿态信息的动作序列。我们的方法侧重于富接触式翻找，利用物体姿态分布和机器人轨迹之间的互信息进行动作规划。RUMI 从观察到的部分点云推断出兼容的物体姿态分布，并实时计算其与工作空间占用率的互信息。在此基础上，我们开发了一种信息增益成本函数和可达性成本函数，以将物体保持在机器人的可及范围内。这些函数被集成到具有随机动力学模型的模型预测控制 (MPC) 框架中，在闭环中更新姿态分布。主要贡献包括一种新的物体姿态估计置信框架、一种高效的信息增益计算策略以及一种鲁棒的基于 MPC 的控制方案。与基线方法相比，RUMI 在模拟和真实任务中均表现出优越的性能。|
|**2024-08-15**|[Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation](http://arxiv.org/abs/2408.08234)|**[link](https://github.com/varunburde/reconstruction_pose_benchmark)**|物体姿态估计对于许多涉及机器人操作、导航和增强现实的工业应用至关重要。当前通用的物体姿态估计器，即不需要针对每个物体进行训练的方法，依赖于精确的 3D 模型。目前主要使用 CAD 模型，但在实践中很难获取。同时，通常可以获取物体的图像。自然而然地，这就引出了一个问题：从图像重建的 3D 模型是否足以实现准确的物体姿态估计？为了回答这个问题，我们提出了一个新的基准测试，用于测量 3D 重建质量对姿态估计精度的影响。我们的基准测试提供了用于物体重建的校准图像，这些图像与 YCB-V 数据集的测试图像进行了配准，用于在 BOP 基准测试格式下进行姿态评估。对多种最先进的 3D 重建和物体姿态估计方法进行的详细实验表明，现代重建方法生成的几何结构通常足以实现准确的姿态估计。我们的实验得出了一些有趣的观察结果：(1) 用于测量 3D 重建质量的标准指标不一定能反映姿态估计的精度，这表明需要专门的基准测试，例如我们的基准测试。(2) 传统的、非基于学习的方法可以与现代的基于学习的重建技术相媲美，甚至可以提供更好的重建时间-姿态精度权衡。(3) 使用重建模型和使用 CAD 模型的性能之间仍然存在相当大的差距。为了促进缩小这一差距的研究，我们的基准测试已在 https://github.com/VarunBurde/reconstruction_pose_benchmark 上公开发布。|
|**2024-07-16**|[NeuSurfEmb: A Complete Pipeline for Dense Correspondence-based 6D Object Pose Estimation without CAD Models](http://arxiv.org/abs/2407.12207)|**[link](https://github.com/ethz-asl/neusurfemb)**|目前最先进的 6D 物体姿态估计方法假设可以使用 CAD 模型，并且需要用户手动设置基于物理的渲染 (PBR) 管道来生成合成训练数据。这两个因素都限制了这些方法在现实世界场景中的应用。在这项工作中，我们提出了一个不需要 CAD 模型的流程，并且只需要一小组真实图像作为输入即可训练最先进的姿态估计器。我们的方法基于 NeuS2 对象表示，我们通过基于运动恢复结构 (SfM) 和对象无关分割的半自动化程序来学习该表示。我们利用 NeuS2 的新视图合成能力和简单的剪切粘贴增强功能来自动生成逼真的对象渲染，我们使用这些渲染来训练基于对应的 SurfEmb 姿态估计器。我们在 LINEMOD-Occlusion 数据集上评估了我们的方法，广泛研究了其各个组件的影响，并显示了相对于基于 CAD 模型和 PBR 数据的方法的竞争性能。我们还展示了我们的流程在自收集的现实世界对象上的易用性和有效性，表明我们的方法优于最先进的无 CAD 模型方法，具有更好的精度和对轻度遮挡的鲁棒性。为了让机器人社区能够从该系统中受益，我们将在 https://www.github.com/ethz-asl/neusurfemb 上公开发布它。|
|**2024-06-06**|[Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking](http://arxiv.org/abs/2406.04316)|null|6D物体姿态估计是计算机视觉中一项至关重要但极具挑战性的任务，其面临的主要挑战是缺乏大规模数据集。这种数据稀缺性阻碍了对模型性能的全面评估，限制了研究进展。此外，可用实例或类别的数量有限也限制了其应用。为了解决这些问题，本文提出了Omni6DPose，这是一个以对象类别多样性、规模大和对象材质多样性为特征的大型数据集。Omni6DPose主要由三个部分组成：ROPE（真实6D物体姿态估计数据集），包含332K张图像，涵盖149个类别、581个实例，超过150万个标注；SOPE（模拟6D物体姿态估计数据集），包含475K张图像，这些图像是利用深度模拟技术在混合现实环境中创建的，涵盖与ROPE相同的149个类别，4162个实例，超过500万个标注；以及在ROPE和SOPE中均使用的手动对齐的真实扫描物体。由于存在大量的变化和歧义，Omni6DPose本身就具有挑战性。为了应对这一挑战，我们引入了GenPose++，它是SOTA类别级姿态估计框架的增强版本，它包含两个关键改进：语义感知特征提取和基于聚类的聚合。此外，我们还提供了全面的基准分析，以评估先前方法在这个大规模数据集上在6D物体姿态估计和姿态跟踪方面的性能。|
|**2024-06-05**|[Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices](http://arxiv.org/abs/2406.02977)|null|随着机器人和增强现实应用越来越依赖于精确高效的6D物体姿态估计，边缘设备上的实时性能对于更具交互性和响应性的系统变得至关重要。我们提出的稀疏颜色代码网络（SCCN）体现了一种清晰简洁的流程设计，可以有效地满足这一需求。SCCN利用目标物体基本几何特征的稀疏性，对RGB图像中的目标物体进行像素级预测，从而加快透视n点（PnP）计算过程。此外，它引入了一种新颖的基于像素级几何的物体对称性表示，该表示与初始姿态预测无缝集成，有效地解决了对称物体歧义问题。值得注意的是，SCCN在英伟达Jetson AGX Xavier上分别在基准LINEMOD数据集和遮挡LINEMOD数据集上实现了每秒19帧（FPS）和6 FPS的估计速率，同时在这些速率下始终保持较高的估计精度。|
|**2024-05-31**|[Deep Learning-Based Object Pose Estimation: A Comprehensive Survey](http://arxiv.org/abs/2405.07801)|**[link](https://github.com/cnjianliu/awesome-object-pose-estimation)**|物体姿态估计是计算机视觉中的一个基本问题，在增强现实和机器人技术中有着广泛的应用。在过去的十年中，深度学习模型由于其优越的精度和鲁棒性，越来越多地取代了依赖于工程点对特征的传统算法。尽管如此，当代方法仍然存在一些挑战，包括它们对标记训练数据的依赖性、模型紧凑性、在挑战性条件下的鲁棒性以及泛化到未见过的新物体能力。最近缺少一项调查来讨论该领域各个方面的进展、未解决的挑战和有希望的未来方向。为了填补这一空白，我们讨论了基于深度学习的物体姿态估计的最新进展，涵盖了该问题的所有三种形式，即实例级、类别级和未见过物体的姿态估计。我们的综述还涵盖了多种输入数据模态、输出姿态的自由度、物体属性和下游任务，为读者提供了对该领域的全面理解。此外，它还讨论了不同领域的训练范式、推理模式、应用领域、评估指标和基准数据集，并报告了当前最先进方法在这些基准上的性能，从而帮助读者为其应用选择最合适的方法。最后，该综述指出了关键挑战，回顾了当前的趋势及其优缺点，并指出了未来研究的有希望的方向。我们还将继续跟踪https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation 上的最新工作。|
|**2024-03-28**|[Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation](http://arxiv.org/abs/2403.19527)|**[link](https://github.com/leeiieeo/ag-pose)**|类别级 6D 对象姿态估计旨在估计特定类别中未见过实例的旋转、平移和大小。在这个领域，基于密集对应的方法已经取得了领先的性能。然而，它们没有明确考虑不同实例的局部和全局几何信息，导致对具有显著形状变化的未见过实例的泛化能力较差。为了解决这个问题，我们提出了一种新的实例自适应和几何感知关键点学习方法，用于类别级 6D 对象姿态估计 (AG-Pose)，它包括两个关键设计：(1) 第一个设计是实例自适应关键点检测模块，它可以自适应地检测一组稀疏关键点，用于表示各种实例的几何结构。(2) 第二个设计是几何感知特征聚合模块，它可以有效地将局部和全局几何信息整合到关键点特征中。这两个模块可以协同工作，为未见过的实例建立鲁棒的关键点级对应关系，从而增强模型的泛化能力。在 CAMERA25 和 REAL275 数据集上的实验结果表明，所提出的 AG-Pose 在没有类别特定形状先验的情况下，性能明显优于现有技术方法。|
|**2024-06-01**|[Object Pose Estimation via the Aggregation of Diffusion Features](http://arxiv.org/abs/2403.18791)|**[link](https://github.com/tianfu18/diff-feats-pose)**|从图像中估计物体姿态是 3D 场景理解的一项关键任务，最近的方法在非常大的基准测试中显示出良好的结果。然而，这些方法在处理未见过物体时性能会显著下降。我们认为这是由于图像特征的泛化能力有限造成的。为了解决这个问题，我们深入分析了扩散模型（例如 Stable Diffusion）的特征，这些特征在对未见过物体进行建模方面具有巨大潜力。基于这一分析，我们创新性地将这些扩散特征引入到物体姿态估计中。为此，我们提出了三种不同的架构，可以有效地捕获和聚合不同粒度的扩散特征，极大地提高了物体姿态估计的泛化能力。在三个流行的基准数据集 LM、O-LM 和 T-LESS 上，我们的方法明显优于最先进的方法。特别是，我们的方法在未见过物体上实现了比先前最佳方法更高的准确率：在 Unseen LM 上为 98.2% 对比 93.5%，在 Unseen O-LM 上为 85.9% 对比 76.3%，显示了我们方法强大的泛化能力。我们的代码已发布在 https://github.com/Tianfu18/diff-feats-pose。|

<p align=right>(<a href=#updated-on-20240905>back to top</a>)</p>

## nerf

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-08-20**|[Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark Images Using Gaussian Splatting](http://arxiv.org/abs/2408.09130)|**[link](https://github.com/yec22/Gaussian-DK)**|三维高斯体渲染技术近年来成为一种强大的表示方法，可以使用一致的多视图图像作为输入合成非凡的新视图。然而，我们注意到在场景未被完全照亮的黑暗环境中拍摄的图像可能表现出相当大的亮度变化和多视图不一致性，这对三维高斯体渲染技术提出了巨大的挑战，并严重降低了其性能。为了解决这个问题，我们提出了 Gaussian-DK。观察到不一致性主要是由相机成像引起的，我们使用一组各向异性三维高斯函数来表示物理世界的一致辐射场，并设计了一个相机响应模块来补偿多视图不一致性。我们还引入了一种基于步长的梯度缩放策略来约束靠近相机的高斯函数（结果证明是漂浮物）的拆分和克隆。在我们提出的基准数据集上的实验表明，Gaussian-DK 可以生成高质量的渲染结果，没有重影和漂浮物伪影，并且明显优于现有方法。此外，我们还可以通过控制曝光级别来合成亮度增强的图像，从而清晰地显示阴影区域的细节。|
|**2024-08-29**|[EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian Splatting](http://arxiv.org/abs/2407.13520)|null|近年来，随着神经辐射场 (NeRF) 和 3D 高斯散射 (3DGS) 的发展，3D 去模糊重建技术取得了显著进展。 尽管这些技术可以从模糊的图像输入中恢复相对清晰的 3D 重建，但它们在处理严重模糊和复杂相机运动方面仍然面临局限性。为了解决这些问题，我们提出了事件辅助的 3D 高斯散射去模糊重建 (EaDeblur-GS)，它集成了事件相机数据以增强 3DGS 对运动模糊的鲁棒性。通过采用自适应偏差估计器 (ADE) 网络来估计高斯中心偏差并使用新的损失函数，EaDeblur-GS 可以实时实现清晰的 3D 重建，其性能可与最先进的方法相媲美。|
|**2024-07-10**|[3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes](http://arxiv.org/abs/2407.07090)|null|基于粒子的辐射场表示方法，例如三维高斯 splatting，已经在复杂场景的重建和重新渲染方面取得了巨大成功。大多数现有方法通过光栅化渲染粒子，将它们投影到屏幕空间图块中，并按排序顺序进行处理。而这项工作则考虑对粒子进行光线追踪，构建边界体积层次结构，并使用高性能 GPU 光线追踪硬件为每个像素投射光线。为了有效处理大量的半透明粒子，我们描述了一种专门的渲染算法，该算法使用边界网格封装粒子，以利用快速的光线三角形相交测试，并按深度顺序对成批的相交点进行着色。光线追踪在计算机图形学中的优势是众所周知的：处理非相干光线以获得阴影和反射等二次照明效果、从机器人技术中常见的高度扭曲的相机进行渲染、对光线进行随机采样等等。与光栅化相比，我们的渲染器以很小的成本实现了这种灵活性。实验结果证明了我们方法的速度和准确性，以及其在计算机图形学和视觉领域的多种应用。我们还提出了对基本高斯表示的相关改进，包括简单使用广义核函数，这可以显著减少粒子命中次数。|
|**2024-07-07**|[GaussReg: Fast 3D Registration with Gaussian Splatting](http://arxiv.org/abs/2407.05254)|null|点云配准是大规模三维场景扫描和重建的基本问题。在深度学习的帮助下，配准方法得到了显著发展，已接近成熟阶段。随着神经辐射场 (NeRF) 的引入，它因其强大的视图合成能力而成为最流行的三维场景表示方法。对于 NeRF 表示，大规模场景重建也需要对其进行配准。然而，这个主题极度缺乏探索。这是因为在用隐式表示对两个场景之间的几何关系进行建模方面存在固有的挑战。现有方法通常将隐式表示转换为显式表示，以便进一步配准。最近，引入了高斯 splatting (GS)，它采用显式三维高斯函数。这种方法在保持高质量渲染的同时，显著提高了渲染速度。给定两个具有显式 GS 表示的场景，我们在这项工作中探索了它们之间的三维配准任务。为此，我们提出了 GaussReg，一种新颖的由粗到精的框架，既快速又准确。粗略阶段遵循现有的点云配准方法，并估计来自 GS 的点云的粗略对齐。我们进一步提出了一种新的图像引导的精细配准方法，该方法从 GS 渲染图像，为精确对齐提供更详细的几何信息。为了支持全面评估，我们精心构建了一个名为 ScanNet-GSReg 的场景级数据集，其中包含从 ScanNet 数据集中获得的 1379 个场景，并收集了一个名为 GSReg 的真实世界数据集。实验结果表明，我们的方法在多个数据集上实现了最先进的性能。我们的 GaussReg 比 HLoc（SuperPoint 作为特征提取器，SuperGlue 作为匹配器）快 44 倍，并且具有相当的精度。|
|**2024-07-04**|[CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion Blur Images](http://arxiv.org/abs/2407.03923)|null|神经辐射场 (NeRFs) 因其高质量的新颖视图渲染能力而备受关注，促使人们对其在各种现实世界案例中的应用进行研究。其中一个关键挑战是相机在曝光时间内的运动导致的运动模糊，这阻碍了对 3D 场景的准确重建。在本研究中，我们提出了连续刚体运动感知高斯散射 (CRiM-GS)，以实时渲染速度从模糊图像重建精确的 3D 场景。考虑到实际相机运动模糊过程包含复杂的运动模式，我们基于神经常微分方程 (ODE) 预测相机的连续运动。具体来说，我们利用刚体变换对相机运动进行建模，并进行适当的正则化，以保持物体的形状和大小。此外，我们在 \textit{SE(3)} 场中引入了连续可变形 3D 变换，通过确保更高的自由度使刚体变换适应现实世界的问题。通过回顾基本相机理论并采用先进的神经网络训练技术，我们实现了对连续相机轨迹的精确建模。我们进行了广泛的实验，证明了该方法在基准数据集上的定量和定性性能均达到了最先进水平。|
|**2024-07-29**|[Trimming the Fat: Efficient Compression of 3D Gaussian Splats through Pruning](http://arxiv.org/abs/2406.18214)|**[link](https://github.com/salmanali96/trimming-the-fat)**|In recent times, the utilization of 3D models has gained traction, owing to the capacity for end-to-end training initially offered by Neural Radiance Fields and more recently by 3D Gaussian Splatting (3DGS) models. The latter holds a significant advantage by inherently easing rapid convergence during training and offering extensive editability. However, despite rapid advancements, the literature still lives in its infancy regarding the scalability of these models. In this study, we take some initial steps in addressing this gap, showing an approach that enables both the memory and computational scalability of such models. Specifically, we propose "Trimming the fat", a post-hoc gradient-informed iterative pruning technique to eliminate redundant information encoded in the model. Our experimental findings on widely acknowledged benchmarks attest to the effectiveness of our approach, revealing that up to 75% of the Gaussians can be removed while maintaining or even improving upon baseline performance. Our approach achieves around 50 $\times$ compression while preserving performance similar to the baseline model, and is able to speed-up computation up to 600 FPS.|
|**2024-06-21**|[Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks](http://arxiv.org/abs/2406.15149)|null|仿真器是自动机器人学习的强大工具，因为它可以提供可扩展的数据生成、灵活的设计和轨迹优化。然而，将从仿真数据中学习到的行为迁移到现实世界中被证明是困难的，通常需要通过计算量大的域随机化方法或进一步的模型微调来缓解。我们提出了一种方法来提高现实世界中视觉四旋翼飞行器导航任务的泛化能力和对分布变化的鲁棒性。为此，我们首先通过将高斯样条函数与四旋翼飞行器飞行动力学相结合来构建仿真器，然后使用 Liquid 神经网络训练鲁棒的导航策略。通过这种方式，我们获得了一个完整的模仿学习协议，它结合了 3D 高斯样条函数辐射场渲染、专家演示训练数据的巧妙编程以及 Liquid 网络的任务理解能力方面的进步。通过一系列定量飞行测试，我们证明了在单个仿真场景中学习到的导航技能可以直接稳健地迁移到现实世界。我们进一步展示了在剧烈的分布和物理环境变化下，在训练环境之外保持性能的能力。我们学习的 Liquid 策略，仅在从真实感室内飞行模拟中提取的单目标机动上进行训练，可以泛化到户外真实硬件平台上的多步远足。|
|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|在非结构化的旅游环境中拍摄的照片经常呈现出多变的外观和短暂的遮挡，这对精确的场景重建提出了挑战，并在新颖视图合成中导致了伪影。尽管先前的方法已经将神经辐射场 (NeRF) 与其他可学习模块相结合来处理动态外观并消除瞬态对象，但其大量的训练需求和缓慢的渲染速度限制了实际部署。最近，3D 高斯 splatting (3DGS) 已成为 NeRF 的一种很有前途的替代方案，它提供了卓越的训练和推理效率以及更好的渲染质量。本文介绍了 Wild-GS，这是一种针对不受约束的照片集优化的 3DGS 创新改编，同时保留了其效率优势。Wild-GS 通过每个 3D 高斯的固有材质属性、每张图像的全局照明和相机属性以及逐点反射率的局部方差来确定其外观。与先前在图像空间中对参考特征进行建模的方法不同，Wild-GS 通过对从参考图像中提取的三平面进行采样，将像素外观特征明确地与相应的局部高斯对齐。这种新颖的设计有效地将参考视图的高频细节外观转移到 3D 空间，并显着加快了训练过程。此外，利用 2D 可见性图和深度正则化分别减轻瞬态效应和约束几何形状。大量实验表明，Wild-GS 在所有现有技术中实现了最先进的渲染性能以及最高的训练和推理效率。|
|**2024-06-06**|[A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation](http://arxiv.org/abs/2406.04253)|null|三维建模一直是计算机视觉和计算机图形学的重要领域。近年来，由于神经表示和生成模型的突破，我们见证了三维建模的快速发展。三维人体建模作为游戏和动画等众多现实应用的核心，引起了人们的广泛关注。在过去的几年里，出现了大量关于创建三维人体化身的工作，形成了一个新的、丰富的三维人体建模知识库。文献的规模之大，使得个人难以跟踪所有的工作。本次综述旨在从重建和生成两个角度，全面概述这些新兴的三维人体化身建模技术。首先，我们回顾了具有代表性的三维人体重建方法，包括基于像素对齐隐函数、神经辐射场和三维高斯散射等方法。然后，我们总结了具有代表性的三维人体生成方法，特别是那些使用大型语言模型（如 CLIP）、扩散模型和各种三维表示的方法，这些方法展示了最先进的性能。最后，我们讨论了我们对现有方法的反思以及三维人体化身建模面临的开放性挑战，为未来的研究指明了方向。|
|**2024-06-13**|[3D-HGS: 3D Half-Gaussian Splatting](http://arxiv.org/abs/2406.02720)|**[link](https://github.com/lihaolin88/3d-half-gaussian-splatting)**|照片级逼真的三维重建是三维计算机视觉中的一个基本问题。由于最近神经渲染技术的出现，该领域取得了相当大的进步。这些技术主要集中于学习三维场景的体积表示，并通过渲染得到的损失函数来细化这些表示。其中，三维高斯散射（3D-GS）已成为一种重要的方法，其性能超过了神经辐射场（NeRFs）。3D-GS使用参数化的三维高斯函数来建模空间位置和颜色信息，并结合基于图块的快速渲染技术。尽管其渲染性能和速度都很出色，但使用三维高斯核函数在准确表示不连续函数方面存在固有限制，特别是在形状不连续的边缘和角落，以及在颜色不连续的不同纹理之间。为了解决这个问题，我们建议采用三维半高斯（3D-HGS）核函数，它可以作为一种即插即用的核函数。我们的实验表明，它们能够提高当前与3D-GS相关方法的性能，并在不影响渲染速度的情况下，在各种数据集上实现最先进的渲染性能。|

<p align=right>(<a href=#updated-on-20240905>back to top</a>)</p>

## 分类/检测/识别/分割

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-09-04**|[iConFormer: Dynamic Parameter-Efficient Tuning with Input-Conditioned Adaptation](http://arxiv.org/abs/2409.02838)|null|基于预训练编码器的完整微调（FFT）和任务特定解码器的迁移学习随着深度模型的指数级增长而变得越来越复杂。使用由小型可学习层组成的适配器的参数高效微调（PEFT）方法已成为 FFT 的替代方案，在保持高训练效率的同时实现了可比的性能。然而，适配器对输入实例的不灵活限制了其在不同下游任务中学习任务特定信息的能力。在本文中，我们提出了一种新的 PEFT 方法，即输入条件化的 Transformer，称为 iConFormer，它利用了以输入实例为条件的动态适配器。为了确保在各种下游任务中对输入实例的灵活学习能力，我们在动态适配器中引入了输入条件化网络（iCoN），从而实现实例级特征转换。具体来说，iCoN 为每个特征生成通道级的卷积核，并使用自适应卷积过程对其进行转换，以有效捕获针对下游任务的任务特定和细粒度细节。实验结果表明，通过仅调整 Transformer 主干参数的 1.6% 到 2.8%，iConFormer 在单目深度估计和语义分割方面实现了与 FFT 相当的性能，同时在图像分类和实例分割方面优于 FFT。此外，所提出的方法在所有上述任务中始终优于最近的 PEFT 方法。|
|**2024-09-04**|[Real-Time Dynamic Scale-Aware Fusion Detection Network: Take Road Damage Detection as an example](http://arxiv.org/abs/2409.02546)|null|基于无人机的道路损坏检测 (RDD) 对城市的日常维护和安全至关重要，特别是在显著降低劳动力成本方面。然而，当前基于无人机的 RDD 研究仍面临许多挑战。例如，形状和方向不规则的损坏、背景对损坏的遮挡以及难以区分损坏和背景，这些因素都显著影响了无人机在日常巡检中检测道路损坏的能力。为了解决这些问题并提高无人机实时道路损坏检测的性能，我们设计并提出了三个相应的模块：一个能够灵活适应形状和背景的特征提取模块；一个融合多尺度感知并适应形状和背景的模块；一个高效的下采样模块。 基于这些模块，我们设计了一种具有自动去除背景干扰能力的多尺度自适应道路损坏检测模型，称为动态尺度感知融合检测模型 (RT-DSAFDet)。在 UAV-PDD2023 公开数据集上的实验结果表明，我们的模型 RT-DSAFDet 的 mAP50 达到了 54.2%，比最新实时目标检测模型 YOLOv10 的高效变体 YOLOv10-m 高 11.1%，而参数量减少到 1.8M，FLOPs 减少到 4.6G，分别降低了 88% 和 93%。此外，在大型通用目标检测公开数据集 MS COCO2017 上也展现了我们模型的优越性，其 mAP50-95 与 YOLOv9-t 相同，但 mAP50 高出 0.5%，参数量减少 10%，FLOPs 减少 40%。|
|**2024-09-04**|[Boosting Generalizability towards Zero-Shot Cross-Dataset Single-Image Indoor Depth by Meta-Initialization](http://arxiv.org/abs/2409.02486)|null|室内机器人的导航或障碍物检测等任务依赖于深度信息，而单图像深度估计被广泛用于辅助感知。大多数室内单图像深度预测较少关注模型对未见数据集的泛化能力，而更关注系统部署的野外鲁棒性。这项工作利用基于梯度的元学习在零样本跨数据集推理中获得更高的泛化能力。与研究最多的、与显式类别标签相关的图像分类元学习不同，对于与物体排列和场景构成方面高度变化的室内环境相关的连续深度值，不存在明确的任务边界。我们提出了细粒度任务，在我们的元学习公式中将每个RGB-D小批量视为一个任务。我们首先展示了我们的方法在有限数据上诱导出更好的先验（RMSE 最高降低 27.8%）。然后，在元学习初始化上进行微调始终优于没有元方法的基线。为了实现泛化，我们提出了零样本跨数据集协议，并验证了由我们的元初始化诱导的更高泛化能力，作为许多现有深度估计方法的简单而有用的插件。深度和元学习交叉领域的工作有可能推动这两项研究更接近实际的机器人和机器感知应用。|
|**2024-09-03**|[Site Selection for the Second Flyeye Telescope: A Simulation Study for Optimizing Near-Earth Object Discovery](http://arxiv.org/abs/2409.02329)|null|欧洲航天局 (ESA) 正在开发一个名为 Flyeye 的广域巡天望远镜网络，以改进近地天体 (NEO) 的发现。该网络中的第一个望远镜将位于北半球的穆法拉山（意大利），而第二个具有增强探测能力的 Flyeye 望远镜刚刚开始关键设计阶段。通过对撞击轨迹上的近地天体进行模拟，研究了第二个 Flyeye 望远镜的潜在位置。对大约 3000 个撞击小行星（绝对星等为 H=25 和 H=28）进行了传播，并测试了主要现有巡天项目（Catalina、Pan-STARRS、ATLAS）、即将投入使用的薇拉·鲁宾天文台 (LSST) 以及 Flyeye 可能选址的可探测性。 考虑了智利、南非和北半球的第二个设施。对于每个天文台，在模拟中都考虑了它们过去或计划的指向策略。在 LSST 部署之前，南半球的一个 Flyeye 的性能与北半球的一个望远镜相似。结合起来，在北方和南方各放置一台望远镜可以最大限度地提高探测率和探测到的独特物体的数量。LSST 之后，南部和北部的 Flyeye 望远镜仍然是互补的。总体而言，模拟表明，无论是在 LSST 之前还是之后，位于南部的第二个 Flyeye 都可以补充位于北部的 Flyeye 望远镜。位于拉西拉的 Flyeye 将利用其优越的大气条件，同时平衡南北半球的资产。|
|**2024-09-03**|[K-Origins: Better Colour Quantification for Neural Networks](http://arxiv.org/abs/2409.02281)|null|K-Origins是一种神经网络层，旨在在学习颜色或强度有利时提高基于图像的网络性能。 超过 250 个编码器-解码器卷积网络在 16 位合成数据上进行了训练和测试，结果表明，在两种情况下，K-Origins 提高了语义分割精度：低信噪比下的目标检测，以及分割形状相同但颜色不同的多个目标。 对于每个可训练参数 $w_k$，K-Origins 通过公式 $\textbf{Y}_k = \textbf{X}-\textbf{J}\cdot w_k$ 从输入特征 $\textbf{X}$ 生成输出特征，其中 $\textbf{J}$ 是一个全 1 矩阵。 此外，还训练了具有不同感受野的网络，以根据目标类别的维度确定最佳网络深度，这表明感受野长度应超过目标大小。 通过确保足够的感受野长度并结合 K-Origins，我们可以获得更好的语义网络性能。|
|**2024-09-03**|[Evaluation and Comparison of Visual Language Models for Transportation Engineering Problems](http://arxiv.org/abs/2409.02278)|null|近年来，视觉语言模型（VLM）的快速发展展现出其在图像理解相关应用方面的巨大潜力。本研究探索了最先进的VLM模型在基于视觉的交通工程任务中的应用，例如图像分类和目标检测。图像分类任务包括拥堵检测和裂缝识别，而目标检测任务则用于识别未佩戴头盔的行为。我们应用了开源模型（如CLIP、BLIP、OWL-ViT、Llava-Next）和闭源模型GPT-4o，评估了这些最先进的VLM模型的性能，以利用语言理解能力来完成基于视觉的交通任务。这些任务通过对VLM模型应用零样本提示来完成，因为零样本提示可以在不对任务进行任何训练的情况下执行任务。这消除了对特定任务进行标注数据集或微调的需求。虽然这些模型在图像分类任务中取得了与基准卷积神经网络（CNN）模型相当的结果，但在目标定位任务中仍有改进的空间。因此，本研究对最先进的VLM模型进行了全面评估，突出了这些模型的优势和局限性，可以作为未来改进和广泛实施的基准。|
|**2024-09-03**|[A Modern Take on Visual Relationship Reasoning for Grasp Planning](http://arxiv.org/abs/2409.02035)|null|与现实世界杂乱场景交互对机器人代理提出了若干挑战，这些代理需要理解观察到的物体之间复杂的的空间依赖性，以确定最佳拾取顺序或有效的物体检索策略。 现有的解决方案通常管理简化的场景，并侧重于在初始物体检测阶段之后预测成对物体关系，但往往忽略全局上下文或难以处理冗余和缺失的物体关系。 在这项工作中，我们提出了一种用于抓取规划的视觉关系推理的现代方法。 我们介绍了 D3GD，这是一个新的测试平台，其中包括包含来自 97 个不同类别的多达 35 个物体的分拣场景。 此外，我们还提出了 D3G，这是一种新的基于端到端 transformer 的依赖图生成模型，它可以同时检测物体并生成表示其空间关系的邻接矩阵。 认识到标准指标的局限性，我们首次采用关系平均精度来评估模型性能，进行了广泛的实验基准测试。 获得的结果表明我们的方法是这项任务的最新技术，为机器人操作的未来研究奠定了基础。 我们在 https://paolotron.github.io/d3g.github.io 上公开发布代码和数据集。|
|**2024-09-03**|[Compressed learning based onboard semantic compression for remote sensing platforms](http://arxiv.org/abs/2409.01988)|null|地球观测 (EO) 在创建和维持一个具有弹性和繁荣的社会方面发挥着至关重要的作用，这对所有生命和地球本身都具有深远的影响。卫星、航空平台以及最近的无人机和无人驾驶飞行器等遥感平台都用于 EO。它们收集大量数据，需要将其下传到地球进行进一步处理和分析。这种高吞吐量采集的瓶颈是下行链路带宽。需要以数据为中心的图像压缩解决方案来应对这种海量数据。在这项工作中，通过压缩学习框架研究了语义压缩，该框架仅利用快速和稀疏的矩阵向量乘法来编码数据。相机噪声和通信信道是造成失真的主要来源。然后，完整的语义通信管道由一个学习到的低复杂度压缩矩阵组成，该矩阵作用于噪声相机输出，以在机载生成一个观测向量，该向量通过通信信道下行链路传输，通过展开网络处理，然后馈送到执行必要下游任务的深度学习模型；研究了图像分类。通过使用小波稀疏先验展开 NA-ALISTA 的层来补偿失真。因此，解码是一种根据相机/环境信息和下游任务设计的即插即用方法。用于下游任务的深度学习模型通过端到端方式的损失函数与压缩矩阵和展开网络联合微调。结果表明，在低压缩比的噪声环境中，添加恢复损失以及任务相关损失可以提高下游性能。|
|**2024-09-03**|[Latent Distillation for Continual Object Detection at the Edge](http://arxiv.org/abs/2409.01872)|**[link](https://github.com/pastifra/Continual_Nanodet)**|虽然在目标检测文献中存在许多性能卓越的方法，但解决数据分布偏移仍然具有挑战性。持续学习（CL）为这个问题提供了解决方案，使模型能够适应新数据，同时保持对先前数据的性能。这对于边缘设备尤其重要，这些设备在汽车和机器人等动态环境中很常见。在这项工作中，我们解决了目标检测持续学习（CLOD）场景中边缘设备的内存和计算限制。具体来说，（i）我们研究了一种开源、轻量级和快速的检测器 NanoDet 对边缘设备上 CLOD 的适用性，改进了文献中使用的较大架构。此外，（ii）我们提出了一种名为潜在蒸馏（LD）的新型 CL 方法，该方法在不显着影响检测性能的情况下减少了最先进的 CL 方法所需的运算次数和内存。我们的方法使用著名的 VOC 和 COCO 基准测试集进行了验证，与其他蒸馏方法相比，每次模型更新可将蒸馏参数开销减少 74%，将浮点运算（FLOPs）减少 56%。|
|**2024-09-03**|[GeoBEV: Learning Geometric BEV Representation for Multi-view 3D Object Detection](http://arxiv.org/abs/2409.01816)|null|鸟瞰图 (BEV) 表示已成为多视图 3D 对象检测的主流范式，展现出令人印象深刻的感知能力。然而，现有方法忽略了 BEV 表示的几何质量，使其处于低分辨率状态，无法恢复场景真实的几何信息。在本文中，我们确定了先前方法受限于低 BEV 表示分辨率的原因，并提出了径向-笛卡尔 BEV 采样 (RC-Sampling)，从而能够高效生成高分辨率密集 BEV 表示，而无需复杂的算子。此外，我们设计了一种新颖的盒内标签来替代从激光雷达点生成的传统深度标签。此标签反映了对象的实际几何结构，而不仅仅是它们的表面，将现实世界的几何信息注入 BEV 表示中。此外，结合盒内标签，开发了一种质心感知内部损失 (CAI 损失) 来捕捉对象的细粒度内部几何结构。最后，我们将上述模块集成到一个名为 GeoBEV 的新型多视图 3D 对象检测框架中。在 nuScenes 数据集上的大量实验表明，GeoBEV 实现了最先进的性能，突出了其有效性。|

<p align=right>(<a href=#updated-on-20240905>back to top</a>)</p>

## 生成模型

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-09-04**|[HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts](http://arxiv.org/abs/2409.02919)|**[link](https://github.com/Liuxinyv/HiPrompt)**|利用预训练扩散模型生成更高分辨率图像的潜力巨大，但这些模型在处理物体重复和结构伪影方面常常遇到困难，尤其是在扩展到 4K 及更高分辨率时。我们发现问题在于，单个提示生成多个尺度的方式效率低下。为此，我们提出了 HiPrompt，这是一种无须微调的新解决方案，它通过引入分层提示来解决上述问题。分层提示提供全局和局部指导。具体来说，全局指导来自描述整体内容的用户输入，而局部指导则利用来自 MLLM 的逐块描述来精心指导局部结构和纹理的生成。此外，在逆向去噪过程中，生成的噪声被分解为低频和高频空间分量。这些分量以多个提示级别为条件，包括详细的逐块描述和更广泛的图像级提示，从而促进在分层语义指导下的提示引导去噪。它进一步允许生成过程更多地关注局部空间区域，并确保生成的图像在高清晰度下保持一致的局部和全局语义、结构和纹理。大量实验表明，HiPrompt 在高分辨率图像生成方面优于现有技术，显著减少了物体重复并提高了结构质量。|
|**2024-09-04**|[Latent Watermarking of Audio Generative Models](http://arxiv.org/abs/2409.02915)|null|音频生成模型的进步给其负责任的披露和滥用检测带来了新的挑战。为了应对这些挑战，我们介绍了一种通过对其训练数据进行特定水印来标记潜在生成模型的方法。由此产生的水印模型生成的潜在表示，其解码输出可以被高置信度地检测到，而无论使用何种解码方法。这种方法无需进行事后水印步骤即可检测生成的内容。它为开源模型提供了更安全的解决方案，并有助于识别那些在未遵守许可条款的情况下对这些模型进行微调或使用的衍生作品。例如，我们的结果表明，即使在对潜在生成模型进行微调后，生成输出的检测精度也能在假阳性率为 $10^{-3}$ 的情况下达到 75% 以上。|
|**2024-09-04**|[Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling](http://arxiv.org/abs/2409.02908)|null|掩码扩散模型 (MDM) 由于其相较于其他离散扩散模型的优越性能，已成为离散数据生成建模的热门研究课题，并在语言建模任务中与自回归模型 (ARM) 展开竞争。最近简化掩码扩散框架的努力进一步使其与连续空间扩散模型保持一致，并获得了更有原则的训练和采样方法。然而，在本文中，我们揭示了 MDM 的训练和采样在理论上都可以摆脱时间变量（可以说是扩散模型的关键特征），并且等效于掩码模型。我们在采样方面的联系是通过我们提出的首次命中采样器 (FHS) 建立的。具体来说，我们证明了 FHS 在理论上等效于 MDM 的原始生成过程，同时显著减少了耗时的分类采样，并实现了 20 倍的加速。此外，我们的研究对先前关于 MDM 在生成困惑度方面可以超越 ARM 的说法提出了质疑。我们首次发现了一个潜在的数值问题，即使使用 32 位浮点精度，也会导致不准确的分类采样。我们表明，该数值问题在理论上和经验上都降低了有效温度，导致先前文献中对 MDM 生成结果的评估不公平。|
|**2024-09-04**|[Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video Diffusion Models](http://arxiv.org/abs/2409.02851)|null|从单张RGB图像生成逼真3D人体是计算机视觉中一项具有挑战性的任务，因为它需要精确的几何建模、高质量的纹理和合理的不可见部分生成。现有方法通常使用多视角扩散模型进行3D人体生成，但它们经常面临视角不一致的问题，这阻碍了高质量3D人体的生成。为了解决这个问题，我们提出了Human-VDM，一种使用视频扩散模型从单张RGB图像生成3D人体的新方法。Human-VDM使用高斯渲染为3D人体生成提供了时间上一致的视图。它由三个模块组成：视图一致的人体视频扩散模块、视频增强模块和高斯渲染模块。首先，将单张图像输入人体视频扩散模块以生成连贯的人体视频。接下来，视频增强模块应用超分辨率和视频插值来增强生成视频的纹理和几何平滑度。最后，3D人体高斯渲染模块在这些高分辨率和视角一致的图像的指导下学习逼真的人体。实验表明，Human-VDM可以从单张图像生成高质量的3D人体，在生成质量和数量方面均优于现有最佳方法。项目页面：https://human-vdm.github.io/Human-VDM/|
|**2024-09-04**|[Multi-Track MusicLDM: Towards Versatile Music Generation with Latent Diffusion Model](http://arxiv.org/abs/2409.02845)|null|扩散模型在涉及音频和音乐的跨模态生成任务中展现出巨大的潜力，例如文本到声音和文本到音乐的生成。这些文本控制的音乐生成模型通常侧重于通过捕捉全局音乐属性（如流派和情绪）来生成音乐。然而，音乐创作是一项复杂的多层次任务，通常将音乐编排作为创作过程的一个组成部分。此过程涉及创作每个乐器部分，使其在节奏、力度、和声和旋律方面与现有部分保持一致，这需要比文本提示通常提供的更精确的音轨控制。在这项工作中，我们通过将 MusicLDM（一种用于音乐的潜在扩散模型）扩展为多轨生成模型来应对这些挑战。通过学习共享上下文的音轨的联合概率，我们的模型能够跨多个音轨生成彼此良好对应的音乐，无论是有条件地还是无条件地。此外，我们的模型还能够进行编曲生成，其中模型可以在给定其他音轨的情况下生成任何音轨子集（例如，生成与给定贝斯和鼓音轨互补的钢琴音轨）。我们将我们的模型与现有的多轨生成模型进行了比较，结果表明，我们的模型在总生成任务和编曲生成任务的客观指标上都取得了相当大的改进。|
|**2024-09-04**|[Rethinking HTG Evaluation: Bridging Generation and Recognition](http://arxiv.org/abs/2409.02683)|null|生成模型在自然图像任务中的评估已得到广泛研究。即使在诸如手写生成（HTG）等具有独特特殊性的情况下，也使用了类似的协议和指标，即使它们可能并非完全合适。在这项工作中，我们介绍了三种专为 HTG 评估量身定制的度量指标： $\text{HTG}_{\text{HTR}} $、$ \text{HTG}_{\text{style}} $ 和 $ \text{HTG}_{\text{OOV}}$ ，并认为它们更便于评估生成手写图像的质量。这些指标依赖于手写文本识别和书写者识别模型的识别错误/准确率，并强调书写风格、文本内容和多样性是符合手写图像内容的主要方面。我们在 IAM 手写数据库上进行了全面的实验，结果表明，诸如 FID 之类的广泛使用的指标无法正确量化生成手写样本的多样性和实用性。我们的研究结果表明，我们的指标信息更丰富，并强调了 HTG 中标准化评估协议的必要性。所提出的指标为评估 HTG 质量提供了更稳健、信息更丰富的协议，有助于提高 HTR 的性能。评估协议的代码可在以下网址获得：https://github.com/koninik/HTG_evaluation。|
|**2024-09-04**|[Introduction to Machine Learning](http://arxiv.org/abs/2409.02668)|null|本书介绍了机器学习中许多算法的开发和分析所依赖的数学基础和技术。本书首先介绍了贯穿全书的符号表示，并回顾了微积分、线性代数和概率论的基本概念，还介绍了一些测度论术语，可作为使用这些工具的部分的阅读指南。导论章节还提供了矩阵分析和优化的背景知识。后面的章节为本书中使用的许多算法提供了理论支持，包括随机梯度下降、近似方法等。在讨论了统计预测的基本概念之后，本书介绍了再生核理论和希尔伯特空间技术，这些技术在许多地方都有应用，然后介绍了各种监督统计学习算法，包括线性方法、支持向量机、决策树、boosting和神经网络。接下来转向生成方法，首先介绍了采样方法和马尔可夫链理论。接下来的章节描述了图模型理论，介绍了潜变量模型的变分方法，以及基于深度学习的生成模型。接下来的章节重点介绍无监督学习方法，包括聚类、因子分析和流形学习。本书的最后一章偏向理论，讨论了集中不等式和泛化界。|
|**2024-09-04**|[Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection](http://arxiv.org/abs/2409.02664)|null|The proliferation of deepfake faces poses huge potential negative impacts on our daily lives. Despite substantial advancements in deepfake detection over these years, the generalizability of existing methods against forgeries from unseen datasets or created by emerging generative models remains constrained. In this paper, inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach that repurposes a well-trained VLM for general deepfake detection. Motivated by the model reprogramming paradigm that manipulates the model prediction via data perturbations, our method can reprogram a pretrained VLM model (e.g., CLIP) solely based on manipulating its input without tuning the inner parameters. Furthermore, we insert a pseudo-word guided by facial identity into the text prompt. Extensive experiments on several popular benchmarks demonstrate that (1) the cross-dataset and cross-manipulation performances of deepfake detection can be significantly and consistently improved (e.g., over 88% AUC in cross-dataset setting from FF++ to WildDeepfake) using a pre-trained CLIP model with our proposed reprogramming method; (2) our superior performances are at less cost of trainable parameters, making it a promising approach for real-world applications.|
|**2024-09-04**|[PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for One-Shot Talking Head Generation](http://arxiv.org/abs/2409.02657)|null|While previous audio-driven talking head generation (THG) methods generate head poses from driving audio, the generated poses or lips cannot match the audio well or are not editable. In this study, we propose \textbf{PoseTalk}, a THG system that can freely generate lip-synchronized talking head videos with free head poses conditioned on text prompts and audio. The core insight of our method is using head pose to connect visual, linguistic, and audio signals. First, we propose to generate poses from both audio and text prompts, where the audio offers short-term variations and rhythm correspondence of the head movements and the text prompts describe the long-term semantics of head motions. To achieve this goal, we devise a Pose Latent Diffusion (PLD) model to generate motion latent from text prompts and audio cues in a pose latent space. Second, we observe a loss-imbalance problem: the loss for the lip region contributes less than 4\% of the total reconstruction loss caused by both pose and lip, making optimization lean towards head movements rather than lip shapes. To address this issue, we propose a refinement-based learning strategy to synthesize natural talking videos using two cascaded networks, i.e., CoarseNet, and RefineNet. The CoarseNet estimates coarse motions to produce animated images in novel poses and the RefineNet focuses on learning finer lip motions by progressively estimating lip motions from low-to-high resolutions, yielding improved lip-synchronization performance. Experiments demonstrate our pose prediction strategy achieves better pose diversity and realness compared to text-only or audio-only, and our video generator model outperforms state-of-the-art methods in synthesizing talking videos with natural head motions. Project: https://junleen.github.io/projects/posetalk.|
|**2024-09-04**|[Skip-and-Play: Depth-Driven Pose-Preserved Image Generation for Any Objects](http://arxiv.org/abs/2409.02653)|null|The emergence of diffusion models has enabled the generation of diverse high-quality images solely from text, prompting subsequent efforts to enhance the controllability of these models. Despite the improvement in controllability, pose control remains limited to specific objects (e.g., humans) or poses (e.g., frontal view) due to the fact that pose is generally controlled via camera parameters (e.g., rotation angle) or keypoints (e.g., eyes, nose). Specifically, camera parameters-conditional pose control models generate unrealistic images depending on the object, owing to the small size of 3D datasets for training. Also, keypoint-based approaches encounter challenges in acquiring reliable keypoints for various objects (e.g., church) or poses (e.g., back view). To address these limitations, we propose depth-based pose control, as depth maps are easily obtainable from a single depth estimation model regardless of objects and poses, unlike camera parameters and keypoints. However, depth-based pose control confronts issues of shape dependency, as depth maps influence not only the pose but also the shape of the generated images. To tackle this issue, we propose Skip-and-Play (SnP), designed via analysis of the impact of three components of depth-conditional ControlNet on the pose and the shape of the generated images. To be specific, based on the analysis, we selectively skip parts of the components to mitigate shape dependency on the depth map while preserving the pose. Through various experiments, we demonstrate the superiority of SnP over baselines and showcase the ability of SnP to generate images of diverse objects and poses. Remarkably, SnP exhibits the ability to generate images even when the objects in the condition (e.g., a horse) and the prompt (e.g., a hedgehog) differ from each other.|

<p align=right>(<a href=#updated-on-20240905>back to top</a>)</p>

## LLM

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-09-03**|[LUK: Empowering Log Understanding with Expert Knowledge from Large Language Models](http://arxiv.org/abs/2409.01909)|null|Logs play a critical role in providing essential information for system monitoring and troubleshooting. Recently, with the success of pre-trained language models (PLMs) and large language models (LLMs) in natural language processing (NLP), smaller PLMs (such as BERT) and LLMs (like ChatGPT) have become the current mainstream approaches for log analysis. While LLMs possess rich knowledge, their high computational costs and unstable performance make LLMs impractical for analyzing logs directly. In contrast, smaller PLMs can be fine-tuned for specific tasks even with limited computational resources, making them more practical. However, these smaller PLMs face challenges in understanding logs comprehensively due to their limited expert knowledge. To better utilize the knowledge embedded within LLMs for log understanding, this paper introduces a novel knowledge enhancement framework, called LUK, which acquires expert knowledge from LLMs to empower log understanding on a smaller PLM. Specifically, we design a multi-expert collaboration framework based on LLMs consisting of different roles to acquire expert knowledge. In addition, we propose two novel pre-training tasks to enhance the log pre-training with expert knowledge. LUK achieves state-of-the-art results on different log analysis tasks and extensive experiments demonstrate expert knowledge from LLMs can be utilized more effectively to understand logs.|
|**2024-09-04**|[MARS: Matching Attribute-aware Representations for Text-based Sequential Recommendation](http://arxiv.org/abs/2409.00702)|null|Sequential recommendation aims to predict the next item a user is likely to prefer based on their sequential interaction history. Recently, text-based sequential recommendation has emerged as a promising paradigm that uses pre-trained language models to exploit textual item features to enhance performance and facilitate knowledge transfer to unseen datasets. However, existing text-based recommender models still struggle with two key challenges: (i) representing users and items with multiple attributes, and (ii) matching items with complex user interests. To address these challenges, we propose a novel model, Matching Attribute-aware Representations for Text-based Sequential Recommendation (MARS). MARS extracts detailed user and item representations through attribute-aware text encoding, capturing diverse user intents with multiple attribute-aware representations. It then computes user-item scores via attribute-wise interaction matching, effectively capturing attribute-level user preferences. Our extensive experiments demonstrate that MARS significantly outperforms existing sequential models, achieving improvements of up to 24.43% and 29.26% in Recall@10 and NDCG@10 across five benchmark datasets. Code is available at https://github.com/junieberry/MARS|
|**2024-08-31**|[From Prediction to Application: Language Model-based Code Knowledge Tracing with Domain Adaptive Pre-Training and Automatic Feedback System with Pedagogical Prompting for Comprehensive Programming Education](http://arxiv.org/abs/2409.00323)|null|Knowledge Tracing (KT) is a critical component in online learning, but traditional approaches face limitations in interpretability and cross-domain adaptability. This paper introduces Language Model-based Code Knowledge Tracing (CodeLKT), an innovative application of Language model-based Knowledge Tracing (LKT) to programming education. CodeLKT leverages pre-trained language models to process learning data, demonstrating superior performance over existing KT and Code KT models. We explore Domain Adaptive Pre-Training (DAPT) and Task Adaptive Pre-Training (TAPT), showing enhanced performance in the coding domain and investigating cross-domain transfer between mathematics and coding. Additionally, we present an theoretically-informed integrated system combining CodeLKT with large language models to generate personalized, in-depth feedback to support students' programming learning. This work advances the field of Code Knowledge Tracing by expanding the knowledge base with language model-based approach and offering practical implications for programming education through data-informed feedback.|
|**2024-08-30**|[Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage](http://arxiv.org/abs/2408.17354)|null|Fine-tuning large language models on private data for downstream applications poses significant privacy risks in potentially exposing sensitive information. Several popular community platforms now offer convenient distribution of a large variety of pre-trained models, allowing anyone to publish without rigorous verification. This scenario creates a privacy threat, as pre-trained models can be intentionally crafted to compromise the privacy of fine-tuning datasets. In this study, we introduce a novel poisoning technique that uses model-unlearning as an attack tool. This approach manipulates a pre-trained language model to increase the leakage of private data during the fine-tuning process. Our method enhances both membership inference and data extraction attacks while preserving model utility. Experimental results across different models, datasets, and fine-tuning setups demonstrate that our attacks significantly surpass baseline performance. This work serves as a cautionary note for users who download pre-trained models from unverified sources, highlighting the potential risks involved.|
|**2024-08-24**|[Empowering Pre-Trained Language Models for Spatio-Temporal Forecasting via Decoupling Enhanced Discrete Reprogramming](http://arxiv.org/abs/2408.14505)|null|Spatio-temporal time series forecasting plays a critical role in various real-world applications, such as transportation optimization, energy management, and climate analysis. The recent advancements in Pre-trained Language Models (PLMs) have inspired efforts to reprogram these models for time series forecasting tasks, by leveraging their superior reasoning and generalization capabilities. However, existing approaches fall short in handling complex spatial inter-series dependencies and intrinsic intra-series frequency components, limiting their spatio-temporal forecasting performance. Moreover, the linear mapping of continuous time series to a compressed subset vocabulary in reprogramming constrains the spatio-temporal semantic expressivity of PLMs and may lead to potential information bottleneck. To overcome the above limitations, we propose \textsc{RePST}, a tailored PLM reprogramming framework for spatio-temporal forecasting. The key insight of \textsc{RePST} is to decouple the spatio-temporal dynamics in the frequency domain, allowing better alignment with the PLM text space. Specifically, we first decouple spatio-temporal data in Fourier space and devise a structural diffusion operator to obtain temporal intrinsic and spatial diffusion signals, making the dynamics more comprehensible and predictable for PLMs. To avoid information bottleneck from a limited vocabulary, we further propose a discrete reprogramming strategy that selects relevant discrete textual information from an expanded vocabulary space in a differentiable manner. Extensive experiments on four real-world datasets show that our proposed approach significantly outperforms state-of-the-art spatio-temporal forecasting models, particularly in data-scarce scenarios.|
|**2024-08-23**|[SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks](http://arxiv.org/abs/2408.13040)|null|Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both storage and computation. Additionally, prompting modifies only the LM's inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. This significantly reduces the need for human labor in designing task-specific models. These advantages become even more evident as the number of tasks served by the LM scales up. Motivated by the strengths of prompting, we are the first to explore the potential of prompting speech LMs in the domain of speech processing. Recently, there has been a growing interest in converting speech into discrete units for language modeling. Our pioneer research demonstrates that these quantized speech units are highly versatile within our unified prompting framework. Not only can they serve as class labels, but they also contain rich phonetic information that can be re-synthesized back into speech signals for speech generation tasks. Specifically, we reformulate speech processing tasks into speech-to-unit generation tasks. As a result, we can seamlessly integrate tasks such as speech classification, sequence generation, and speech generation within a single, unified prompting framework. The experiment results show that the prompting method can achieve competitive performance compared to the strong fine-tuning method based on self-supervised learning models with a similar number of trainable parameters. The prompting method also shows promising results in the few-shot setting. Moreover, with the advanced speech LMs coming into the stage, the proposed prompting framework attains great potential.|
|**2024-08-23**|[Investigating LLM Applications in E-Commerce](http://arxiv.org/abs/2408.12779)|null|The emergence of Large Language Models (LLMs) has revolutionized natural language processing in various applications especially in e-commerce. One crucial step before the application of such LLMs in these fields is to understand and compare the performance in different use cases in such tasks. This paper explored the efficacy of LLMs in the e-commerce domain, focusing on instruction-tuning an open source LLM model with public e-commerce datasets of varying sizes and comparing the performance with the conventional models prevalent in industrial applications. We conducted a comprehensive comparison between LLMs and traditional pre-trained language models across specific tasks intrinsic to the e-commerce domain, namely classification, generation, summarization, and named entity recognition (NER). Furthermore, we examined the effectiveness of the current niche industrial application of very large LLM, using in-context learning, in e-commerce specific tasks. Our findings indicate that few-shot inference with very large LLMs often does not outperform fine-tuning smaller pre-trained models, underscoring the importance of task-specific model optimization.Additionally, we investigated different training methodologies such as single-task training, mixed-task training, and LoRA merging both within domain/tasks and between different tasks. Through rigorous experimentation and analysis, this paper offers valuable insights into the potential effectiveness of LLMs to advance natural language processing capabilities within the e-commerce industry.|
|**2024-08-22**|[AutoTest: Evolutionary Code Solution Selection with Test Cases](http://arxiv.org/abs/2408.12125)|null|With the development of code generation techniques, selecting the correct code solution from multiple candidate solutions has become a crucial task. This study proposes AutoTest, a novel technique that combines automated test case generation with code solution execution to optimize the selection process using an evolutionary genetic algorithm. Firstly, AutoTest utilizes large pre-trained language models such as codegen-16B, code-davinci-002, and incoder-6B to provide code solutions and their corresponding test cases. Then, by executing the code solutions and evaluating their performance on the test cases, a consensus set is formed. Fine-grained ranking is achieved through the selection, mutation, and crossover mechanisms based on the evolutionary genetic algorithm, with the adjustment of alpha and beta parameters. Finally, the best code solution is chosen. AutoTest demonstrates significant performance improvements on the HumanEval benchmark test. The HumanEval dataset consists of 164 programming problems, and AutoTest achieves approximately a 10% improvement over the baseline method in terms of pass@1 score.|
|**2024-08-24**|[SarcasmBench: Towards Evaluating Large Language Models on Sarcasm Understanding](http://arxiv.org/abs/2408.11319)|null|In the era of large language models (LLMs), the task of ``System I''~-~the fast, unconscious, and intuitive tasks, e.g., sentiment analysis, text classification, etc., have been argued to be successfully solved. However, sarcasm, as a subtle linguistic phenomenon, often employs rhetorical devices like hyperbole and figuration to convey true sentiments and intentions, involving a higher level of abstraction than sentiment analysis. There is growing concern that the argument about LLMs' success may not be fully tenable when considering sarcasm understanding. To address this question, we select eleven SOTA LLMs and eight SOTA pre-trained language models (PLMs) and present comprehensive evaluations on six widely used benchmark datasets through different prompting approaches, i.e., zero-shot input/output (IO) prompting, few-shot IO prompting, chain of thought (CoT) prompting. Our results highlight three key findings: (1) current LLMs underperform supervised PLMs based sarcasm detection baselines across six sarcasm benchmarks. This suggests that significant efforts are still required to improve LLMs' understanding of human sarcasm. (2) GPT-4 consistently and significantly outperforms other LLMs across various prompting methods, with an average improvement of 14.0\% $\uparrow$ . Claude 3 and ChatGPT demonstrate the next best performance after GPT-4. (3) Few-shot IO prompting method outperforms the other two methods: zero-shot IO and few-shot CoT. The reason is that sarcasm detection, being a holistic, intuitive, and non-rational cognitive process, is argued not to adhere to step-by-step logical reasoning, making CoT less effective in understanding sarcasm compared to its effectiveness in mathematical reasoning tasks.|
|**2024-08-20**|[Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution](http://arxiv.org/abs/2408.10548)|**[link](https://github.com/lanxiang1017/language-modeling-on-tabular-data-survey)**|Tabular data, a prevalent data type across various domains, presents unique challenges due to its heterogeneous nature and complex structural relationships. Achieving high predictive performance and robustness in tabular data analysis holds significant promise for numerous applications. Influenced by recent advancements in natural language processing, particularly transformer architectures, new methods for tabular data modeling have emerged. Early techniques concentrated on pre-training transformers from scratch, often encountering scalability issues. Subsequently, methods leveraging pre-trained language models like BERT have been developed, which require less data and yield enhanced performance. The recent advent of large language models, such as GPT and LLaMA, has further revolutionized the field, facilitating more advanced and diverse applications with minimal fine-tuning. Despite the growing interest, a comprehensive survey of language modeling techniques for tabular data remains absent. This paper fills this gap by providing a systematic review of the development of language modeling for tabular data, encompassing: (1) a categorization of different tabular data structures and data types; (2) a review of key datasets used in model training and tasks used for evaluation; (3) a summary of modeling techniques including widely-adopted data processing methods, popular architectures, and training objectives; (4) the evolution from adapting traditional Pre-training/Pre-trained language models to the utilization of large language models; (5) an identification of persistent challenges and potential future research directions in language modeling for tabular data analysis. GitHub page associated with this survey is available at: https://github.com/lanxiang1017/Language-Modeling-on-Tabular-Data-Survey.git.|

<p align=right>(<a href=#updated-on-20240905>back to top</a>)</p>

## Transformer

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-09-04**|[Pooling And Attention: What Are Effective Designs For LLm-Based Embedding Models?](http://arxiv.org/abs/2409.02727)|**[link](https://github.com/yixuantt/poolingandattn)**|The significant advancements of Large Language Models (LLMs) in generative tasks have led to a growing body of work exploring LLM-based embedding models. While these models, employing different pooling and attention strategies, have achieved state-of-the-art performance on public embedding benchmarks, questions still arise about what constitutes an effective design for LLM-based embedding models. However, these models are often trained on different datasets, using different LLM base models or training settings. Moreover, evaluations on public embedding benchmarks often fail to report statistical significance, making it difficult to determine which designs truly contribute to final performance. This complicates the process for practitioners seeking optimal training recipes for LLM-based embedding models. In this study, we conduct a large-scale experiment by training a series of LLM-based embedding models using the same training data and base model but differing in their pooling and attention strategies. The results show that there is no one-size-fits-all solution: while bidirectional attention and an additional trainable pooling layer outperform in text similarity and information retrieval tasks, they do not significantly surpass simpler designs like EOS-last token pooling and default causal attention in clustering and classification tasks. Furthermore, we propose a new pooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs of all hidden layers, rather than just the last layer, using a cross-attention network. This method proves to be statistically superior in text similarity and retrieval tasks compared to existing pooling methods. Overall, this paper sheds light on effective training strategies for LLM-based embedding models.|
|**2024-09-04**|[UniTT-Stereo: Unified Training of Transformer for Enhanced Stereo Matching](http://arxiv.org/abs/2409.02545)|null|Unlike other vision tasks where Transformer-based approaches are becoming increasingly common, stereo depth estimation is still dominated by convolution-based approaches. This is mainly due to the limited availability of real-world ground truth for stereo matching, which is a limiting factor in improving the performance of Transformer-based stereo approaches. In this paper, we propose UniTT-Stereo, a method to maximize the potential of Transformer-based stereo architectures by unifying self-supervised learning used for pre-training with stereo matching framework based on supervised learning. To be specific, we explore the effectiveness of reconstructing features of masked portions in an input image and at the same time predicting corresponding points in another image from the perspective of locality inductive bias, which is crucial in training models with limited training data. Moreover, to address these challenging tasks of reconstruction-and-prediction, we present a new strategy to vary a masking ratio when training the stereo model with stereo-tailored losses. State-of-the-art performance of UniTT-Stereo is validated on various benchmarks such as ETH3D, KITTI 2012, and KITTI 2015 datasets. Lastly, to investigate the advantages of the proposed approach, we provide a frequency analysis of feature maps and the analysis of locality inductive bias based on attention maps.|
|**2024-09-03**|[F2former: When Fractional Fourier Meets Deep Wiener Deconvolution and Selective Frequency Transformer for Image Deblurring](http://arxiv.org/abs/2409.02056)|null|Recent progress in image deblurring techniques focuses mainly on operating in both frequency and spatial domains using the Fourier transform (FT) properties. However, their performance is limited due to the dependency of FT on stationary signals and its lack of capability to extract spatial-frequency properties. In this paper, we propose a novel approach based on the Fractional Fourier Transform (FRFT), a unified spatial-frequency representation leveraging both spatial and frequency components simultaneously, making it ideal for processing non-stationary signals like images. Specifically, we introduce a Fractional Fourier Transformer (F2former), where we combine the classical fractional Fourier based Wiener deconvolution (F2WD) as well as a multi-branch encoder-decoder transformer based on a new fractional frequency aware transformer block (F2TB). We design F2TB consisting of a fractional frequency aware self-attention (F2SA) to estimate element-wise product attention based on important frequency components and a novel feed-forward network based on frequency division multiplexing (FM-FFN) to refine high and low frequency features separately for efficient latent clear image restoration. Experimental results for the cases of both motion deblurring as well as defocus deblurring show that the performance of our proposed method is superior to other state-of-the-art (SOTA) approaches.|
|**2024-09-03**|[TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation](http://arxiv.org/abs/2409.02018)|null|In healthcare, medical image segmentation is crucial for accurate disease diagnosis and the development of effective treatment strategies. Early detection can significantly aid in managing diseases and potentially prevent their progression. Machine learning, particularly deep convolutional neural networks, has emerged as a promising approach to addressing segmentation challenges. Traditional methods like U-Net use encoding blocks for local representation modeling and decoding blocks to uncover semantic relationships. However, these models often struggle with multi-scale objects exhibiting significant variations in texture and shape, and they frequently fail to capture long-range dependencies in the input data. Transformers designed for sequence-to-sequence predictions have been proposed as alternatives, utilizing global self-attention mechanisms. Yet, they can sometimes lack precise localization due to insufficient granular details. To overcome these limitations, we introduce TransDAE: a novel approach that reimagines the self-attention mechanism to include both spatial and channel-wise associations across the entire feature space, while maintaining computational efficiency. Additionally, TransDAE enhances the skip connection pathway with an inter-scale interaction module, promoting feature reuse and improving localization accuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on the Synaps multi-organ dataset, even without relying on pre-trained weights.|
|**2024-09-03**|[TASL-Net: Tri-Attention Selective Learning Network for Intelligent Diagnosis of Bimodal Ultrasound Video](http://arxiv.org/abs/2409.01557)|null|In the intelligent diagnosis of bimodal (gray-scale and contrast-enhanced) ultrasound videos, medical domain knowledge such as the way sonographers browse videos, the particular areas they emphasize, and the features they pay special attention to, plays a decisive role in facilitating precise diagnosis. Embedding medical knowledge into the deep learning network can not only enhance performance but also boost clinical confidence and reliability of the network. However, it is an intractable challenge to automatically focus on these person- and disease-specific features in videos and to enable networks to encode bimodal information comprehensively and efficiently. This paper proposes a novel Tri-Attention Selective Learning Network (TASL-Net) to tackle this challenge and automatically embed three types of diagnostic attention of sonographers into a mutual transformer framework for intelligent diagnosis of bimodal ultrasound videos. Firstly, a time-intensity-curve-based video selector is designed to mimic the temporal attention of sonographers, thus removing a large amount of redundant information while improving computational efficiency of TASL-Net. Then, to introduce the spatial attention of the sonographers for contrast-enhanced video analysis, we propose the earliest-enhanced position detector based on structural similarity variation, on which the TASL-Net is made to focus on the differences of perfusion variation inside and outside the lesion. Finally, by proposing a mutual encoding strategy that combines convolution and transformer, TASL-Net possesses bimodal attention to structure features on gray-scale videos and to perfusion variations on contrast-enhanced videos. These modules work collaboratively and contribute to superior performance. We conduct a detailed experimental validation of TASL-Net's performance on three datasets, including lung, breast, and liver.|
|**2024-09-02**|[Spectron: Target Speaker Extraction using Conditional Transformer with Adversarial Refinement](http://arxiv.org/abs/2409.01352)|null|Recently, attention-based transformers have become a de facto standard in many deep learning applications including natural language processing, computer vision, signal processing, etc.. In this paper, we propose a transformer-based end-to-end model to extract a target speaker's speech from a monaural multi-speaker mixed audio signal. Unlike existing speaker extraction methods, we introduce two additional objectives to impose speaker embedding consistency and waveform encoder invertibility and jointly train both speaker encoder and speech separator to better capture the speaker conditional embedding. Furthermore, we leverage a multi-scale discriminator to refine the perceptual quality of the extracted speech. Our experiments show that the use of a dual path transformer in the separator backbone along with proposed training paradigm improves the CNN baseline by $3.12$ dB points. Finally, we compare our approach with recent state-of-the-arts and show that our model outperforms existing methods by $4.1$ dB points on an average without creating additional data dependency.|
|**2024-09-02**|[CLIBE: Detecting Dynamic Backdoors in Transformer-based NLP Models](http://arxiv.org/abs/2409.01193)|null|Backdoors can be injected into NLP models to induce misbehavior when the input text contains a specific feature, known as a trigger, which the attacker secretly selects. Unlike fixed words, phrases, or sentences used in the static text trigger, NLP dynamic backdoor attacks design triggers associated with abstract and latent text features, making them considerably stealthier than traditional static backdoor attacks. However, existing research on NLP backdoor detection primarily focuses on defending against static backdoor attacks, while detecting dynamic backdoors in NLP models remains largely unexplored. This paper presents CLIBE, the first framework to detect dynamic backdoors in Transformer-based NLP models. CLIBE injects a "few-shot perturbation" into the suspect Transformer model by crafting optimized weight perturbation in the attention layers to make the perturbed model classify a limited number of reference samples as a target label. Subsequently, CLIBE leverages the generalization ability of this few-shot perturbation to determine whether the original model contains a dynamic backdoor. Extensive evaluation on three advanced NLP dynamic backdoor attacks, two widely-used Transformer frameworks, and four real-world classification tasks strongly validates the effectiveness of CLIBE. We also demonstrate the robustness of CLIBE against various adaptive attacks. Furthermore, we employ CLIBE to scrutinize 49 popular Transformer models on Hugging Face and discover one exhibiting a high probability of containing a dynamic backdoor. We have contacted Hugging Face and provided detailed evidence of this model's backdoor behavior. Moreover, we extend CLIBE to detect backdoor text generation models modified to exhibit toxic behavior. To the best of our knowledge, CLIBE is the first framework capable of detecting backdoors in text generation models without access to trigger input test samples.|
|**2024-09-02**|[Progressive Retinal Image Registration via Global and Local Deformable Transformations](http://arxiv.org/abs/2409.01068)|**[link](https://github.com/lyp-deeplearning/awesome-retinal-registration)**|Retinal image registration plays an important role in the ophthalmological diagnosis process. Since there exist variances in viewing angles and anatomical structures across different retinal images, keypoint-based approaches become the mainstream methods for retinal image registration thanks to their robustness and low latency. These methods typically assume the retinal surfaces are planar, and adopt feature matching to obtain the homography matrix that represents the global transformation between images. Yet, such a planar hypothesis inevitably introduces registration errors since retinal surface is approximately curved. This limitation is more prominent when registering image pairs with significant differences in viewing angles. To address this problem, we propose a hybrid registration framework called HybridRetina, which progressively registers retinal images with global and local deformable transformations. For that, we use a keypoint detector and a deformation network called GAMorph to estimate the global transformation and local deformable transformation, respectively. Specifically, we integrate multi-level pixel relation knowledge to guide the training of GAMorph. Additionally, we utilize an edge attention module that includes the geometric priors of the images, ensuring the deformation field focuses more on the vascular regions of clinical interest. Experiments on two widely-used datasets, FIRE and FLoRI21, show that our proposed HybridRetina significantly outperforms some state-of-the-art methods. The code is available at https://github.com/lyp-deeplearning/awesome-retinal-registration.|
|**2024-09-02**|[Multi-scale Temporal Fusion Transformer for Incomplete Vehicle Trajectory Prediction](http://arxiv.org/abs/2409.00904)|null|Motion prediction plays an essential role in autonomous driving systems, enabling autonomous vehicles to achieve more accurate local-path planning and driving decisions based on predictions of the surrounding vehicles. However, existing methods neglect the potential missing values caused by object occlusion, perception failures, etc., which inevitably degrades the trajectory prediction performance in real traffic scenarios. To address this limitation, we propose a novel end-to-end framework for incomplete vehicle trajectory prediction, named Multi-scale Temporal Fusion Transformer (MTFT), which consists of the Multi-scale Attention Head (MAH) and the Continuity Representation-guided Multi-scale Fusion (CRMF) module. Specifically, the MAH leverages the multi-head attention mechanism to parallelly capture multi-scale motion representation of trajectory from different temporal granularities, thus mitigating the adverse effect of missing values on prediction. Furthermore, the multi-scale motion representation is input into the CRMF module for multi-scale fusion to obtain the robust temporal feature of the vehicle. During the fusion process, the continuity representation of vehicle motion is first extracted across time steps to guide the fusion, ensuring that the resulting temporal feature incorporates both detailed information and the overall trend of vehicle motion, which facilitates the accurate decoding of future trajectory that is consistent with the vehicle's motion trend. We evaluate the proposed model on four datasets derived from highway and urban traffic scenarios. The experimental results demonstrate its superior performance in the incomplete vehicle trajectory prediction task compared with state-of-the-art models, e.g., a comprehensive performance improvement of more than 39% on the HighD dataset.|
|**2024-09-01**|[Attention-Guided Multi-scale Interaction Network for Face Super-Resolution](http://arxiv.org/abs/2409.00591)|null|Recently, CNN and Transformer hybrid networks demonstrated excellent performance in face super-resolution (FSR) tasks. Since numerous features at different scales in hybrid networks, how to fuse these multi-scale features and promote their complementarity is crucial for enhancing FSR. However, existing hybrid network-based FSR methods ignore this, only simply combining the Transformer and CNN. To address this issue, we propose an attention-guided Multi-scale interaction network (AMINet), which contains local and global feature interactions as well as encoder-decoder phases feature interactions. Specifically, we propose a Local and Global Feature Interaction Module (LGFI) to promote fusions of global features and different receptive fields' local features extracted by our Residual Depth Feature Extraction Module (RDFE). Additionally, we propose a Selective Kernel Attention Fusion Module (SKAF) to adaptively select fusions of different features within LGFI and encoder-decoder phases. Our above design allows the free flow of multi-scale features from within modules and between encoder and decoder, which can promote the complementarity of different scale features to enhance FSR. Comprehensive experiments confirm that our method consistently performs well with less computational consumption and faster inference.|

<p align=right>(<a href=#updated-on-20240905>back to top</a>)</p>

