## Updated on 2024.07.08
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#多模态>多模态</a></li>
    <li><a href=#6dof-object-pose>6DOF Object Pose</a></li>
    <li><a href=#nerf>nerf</a></li>
    <li><a href=#分类/检测/识别/分割>分类/检测/识别/分割</a></li>
    <li><a href=#生成模型>生成模型</a></li>
    <li><a href=#llm>LLM</a></li>
    <li><a href=#transformer>Transformer</a></li>
  </ol>
</details>

## 多模态

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-07-05**|[Multimodal Classification via Modal-Aware Interactive Enhancement](http://arxiv.org/abs/2407.04587)|null|由于存在臭名昭著的模态不平衡问题，多模态学习（MML）会导致优化不平衡现象，从而难以达到令人满意的性能。最近，一些具有代表性的方法被提出用于提高性能，主要集中在自适应调整每个模态的优化，以重新平衡主导模态和非主导模态的学习速度。为了更好地促进多模态学习中模型信息的交互，在本文中，我们提出了一种新的多模态学习方法，称为模态感知交互增强（MIE）。具体来说，我们首先利用基于锐度感知最小化（SAM）的优化策略在前向阶段平滑学习目标。然后，借助SAM的几何特性，我们提出了一种梯度修正策略，在反向阶段施加不同模态之间的影响。因此，我们可以提高泛化能力，同时缓解多模态学习中的模态遗忘现象。在广泛使用的数据集上进行的大量实验表明，我们提出的方法可以优于各种最先进的基线，以实现最佳性能。|
|**2024-07-04**|[MiniGPT-Med: Large Language Model as a General Interface for Radiology Diagnosis](http://arxiv.org/abs/2407.04106)|null|近年来，人工智能 (AI) 的快速发展为医疗保健领域带来了重大的突破，特别是在诊断程序的改进方面。然而，以往的研究往往局限于有限的功能。本研究介绍了 MiniGPT-Med，这是一个源于大规模语言模型并专为医疗应用而设计的视觉语言模型。MiniGPT-Med 在各种成像模式（包括 X 光、CT 扫描和 MRI）中均表现出非凡的多功能性，从而增强了其实用性。该模型能够执行医学报告生成、视觉问答 (VQA) 以及医学图像疾病识别等任务。它对图像和文本临床数据的集成处理显著提高了诊断准确性。我们的实证评估证实，MiniGPT-Med 在疾病定位、医学报告生成和 VQA 基准测试中均表现出色，这标志着在缩小放射学实践辅助差距方面迈出了重要一步。此外，它在医学报告生成方面达到了最先进的性能，比之前的最佳模型提高了19%的准确率。MiniGPT-Med 有望成为放射学诊断的通用接口，从而提高各种医学影像应用的诊断效率。|
|**2024-07-04**|[Fully Fine-tuned CLIP Models are Efficient Few-Shot Learners](http://arxiv.org/abs/2407.04003)|null|提示调优通过训练一小部分参数，可以有效地增强预训练视觉语言模型 (VLM) 在下游任务上的性能。然而，当将调优后的模型应用于不同的数据集或领域时，它们往往会牺牲灵活性和适应性。在本文中，我们探索了通过精细微调整个 VLM 来捕获特定任务信息的可能性，同时最大限度地减少参数调整。在有限的监督下对特定任务进行整个 VLM 微调时，过拟合和灾难性遗忘成为事实上的因素。为了缓解这些问题，我们提出了一个名为 CLIP-CITE 的框架，通过设计一个判别性的视觉-文本任务，进一步以监督的方式对齐视觉-文本语义，并集成知识蒸馏技术来保留获得的知识。在少样本学习、基础到新泛化、域泛化和跨域泛化设置下的广泛实验结果表明，我们的方法在有限监督下有效地提高了特定任务的性能，同时保留了 VLM 在其他数据集上的通用性。|
|**2024-07-04**|[Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks](http://arxiv.org/abs/2407.03967)|**[link](https://github.com/amitkparekh/cogelot)**|仅根据多模态模型在分布外数据上的性能来评估其泛化能力，无法捕捉其真正的鲁棒性。本研究引入了一个全面的评估框架，系统地检验了指令和输入在这些模型泛化能力中的作用，并考虑了架构设计、跨语言和视觉模态的输入扰动以及任务复杂性的增加。所提出的框架揭示了多模态模型对极端指令扰动的弹性和它们对观察变化的脆弱性，引发了对过度拟合虚假相关性的担忧。通过在当前基于 Transformer 的机器人操作任务多模态模型上应用此评估框架，我们发现了局限性，并建议未来的改进应侧重于架构和训练创新，以更好地整合多模态输入，通过优先考虑对输入内容的敏感性而不是偶然的相关性来增强模型的泛化能力。|
|**2024-07-04**|[Concept Bottleneck Models Without Predefined Concepts](http://arxiv.org/abs/2407.03921)|null|近年来，可解释的概念型模型，如概念瓶颈模型 (CBM)，引起了人们的广泛兴趣。这类模型首先预测人类可解释的概念，然后将这些概念映射到输出类别。为了减少对人工标注概念的依赖，最近的研究工作已将预训练的黑盒模型后验地转换为可解释的 CBM。然而，这些方法预先定义了一组概念，假设黑盒模型在其表示中编码了哪些概念。在这项工作中，我们通过利用无监督概念发现来自动提取概念，从而消除了这一假设，无需人工标注或预定义的概念集。我们进一步引入了一种依赖于输入的概念选择机制，以确保在所有类别中仅使用一小部分概念。我们证明，我们的方法提高了下游性能，并缩小了与黑盒模型的性能差距，同时在分类中使用的概念要少得多。最后，我们演示了大型视觉语言模型如何干预最终的模型权重以纠正模型错误。|
|**2024-07-04**|[M $\mathbf5$ -- A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks](http://arxiv.org/abs/2407.03791)|null|自ChatGPT发布以来，自然语言处理领域经历了快速发展，特别是在大型语言模型（LLM）及其多模态对应物大型多模态模型（LMM）方面。尽管LLM具有令人印象深刻的能力，但正如各种纯文本基准测试所证明的那样，LLM在不同语言和文化背景下 often 表现出显著的性能差异。然而，目前的研究缺乏针对多模态视觉语言环境的此类基准。为了弥补这一差距，本研究引入了M5，这是第一个旨在评估多语言和多文化背景下不同视觉语言任务的LMM的综合基准。M5包括涵盖五个任务和41种语言的八个数据集，重点关注代表性不足的语言和文化多样化的图像。此外，我们还介绍了两个新的数据集，M5-VGR和M5-VLOD，其中包括一项新的视觉语言异常检测任务，在该任务中，所有评估的开源模型都未能显著超过随机基线。通过广泛的评估和分析，我们重点强调了资源丰富语言和资源匮乏语言之间存在巨大的、与任务无关的性能差异。此外，我们还发现，在多语言环境中，更大的模型不一定优于较小的模型。|
|**2024-07-04**|[Meta-optimized Angular Margin Contrastive Framework for Video-Language Representation Learning](http://arxiv.org/abs/2407.03788)|null|数据质量是决定视频-语言表示学习效果的首要因素。然而，以往数据中的视频-文本对通常不能完美对齐，这可能导致视频-语言表示不能准确反映跨模态语义。此外，以往数据还存在概念分布不均匀的问题，从而影响了在不受欢迎主题上的下游性能。为了解决这些问题，我们提出了一个带有减法角度边际的对比目标函数，以规范跨模态表示，使其达到完美的相似性。此外，为了适应非均匀的概念分布，我们提出了一个多层感知器（MLP）参数化的加权函数，将损失值映射到样本权重，从而能够在整个训练过程中动态调整模型的关注点。在少量无偏元数据的指导下，并通过大型视觉-语言模型生成的视频-文本数据进行增强，我们改进了视频-语言表示，并在常用的视频问答和文本-视频检索数据集上取得了优异的性能。|
|**2024-07-04**|[Visualizing Dialogues: Enhancing Image Selection through Dialogue Understanding with Large Language Models](http://arxiv.org/abs/2407.03615)|null|近年来，对话系统的进步凸显了整合多模态响应的重要性，这种响应能够通过多种模态来传达信息，而不仅仅依赖于基于文本的交互。这种丰富性不仅提高了整体的交流效率，还增强了对话体验的质量。然而，现有的对话到图像检索方法由于预训练视觉语言模型 (VLM) 在准确理解复杂对话方面的局限性而面临挑战。为了解决这个问题，我们提出了一种新方法，利用大型语言模型 (LLM) 强大的推理能力来生成精确的对话相关视觉描述符，从而促进与图像的无缝连接。在基准数据上进行的大量实验验证了我们提出的方法在提取简洁准确的视觉描述符方面的有效性，从而显著提高了对话到图像检索的性能。此外，我们的研究结果证明了该方法在不同视觉线索、各种 LLM 和不同数据集上的泛化能力，突出了其在实际应用中的实用性和潜在影响。|
|**2024-07-04**|[Lateralization LoRA: Interleaved Instruction Tuning with Modality-Specialized Adaptations](http://arxiv.org/abs/2407.03604)|null|视觉语言模型 (VLM) 的最新进展导致了能够理解和生成交错图像和文本的视觉语言通用模型 (VLG) 的发展。尽管取得了这些进步，但 VLG 在遵循用户指令进行交错文本和图像生成方面仍然存在困难。为了解决这个问题，我们引入了 LeafInstruct，这是第一个开源的交错指令调整数据，包含跨 10 多个领域的 30,000 多个高质量实例。由于现有 VLG 的规模庞大，我们选择进行参数高效的调整。然而，我们观察到使用标准 LoRA 调整的 VLG 通常在交错文本图像生成中表现出较差的性能。我们将此问题归因于模态干扰和缺乏模态专用适应性设计。因此，我们提出了一种受大脑偏侧化概念启发的新型模态专用适应方法——Lateralization LoRA。Lateralization LoRA 采用混合方法，结合了传统的线性 LoRA 和用于生成文本和图像的卷积 LoRA，通过利用模态特定的结构和参数集来生成高质量的文本和图像。我们使用 LeafInstruct 数据集对 VLG（即 EMU2）进行 Lateralization LoRA 指令调整。大量实验表明，使用 Lateralization LoRA 调整的 EMU2 实现了最先进的性能，在复杂的交错任务中明显优于基线模型。|
|**2024-07-03**|[HEMM: Holistic Evaluation of Multimodal Foundation Models](http://arxiv.org/abs/2407.03418)|**[link](https://github.com/pliang279/hemm)**|能够全面处理文本、图像、视频、音频和其他感官模态的多模态基础模型正越来越多地应用于各种现实应用中。然而，考虑到可能存在的各种建模决策、任务和领域，描述和研究多模态基础模型的进展具有挑战性。在本文中，我们介绍了多模态模型的整体评估 (HEMM)，以系统地评估多模态基础模型在一组 3 个维度上的能力：基本技能、信息流和现实用例。基本的多模态技能是解决问题所需的内部能力，例如学习跨模态的交互、细粒度对齐、多步骤推理以及处理外部知识的能力。信息流研究多模态内容在任务期间如何通过查询、翻译、编辑和融合发生变化。用例涵盖了现实世界多媒体、情感计算、自然科学、医疗保健和人机交互应用中引入的特定领域挑战。通过对 HEMM 中 30 个任务的全面实验，我们 (1) 确定了对当今模型构成挑战的关键数据集维度（例如，基本技能、信息流和用例），以及 (2) 提炼了关于不同建模维度（例如，规模、预训练数据、多模态对齐、预训练和指令微调目标）如何影响性能的性能趋势。我们关于具有挑战性的多模态交互、用例以及需要推理和外部知识的任务、数据和模型规模的好处以及指令微调的影响的结论，为多模态基础模型的未来工作提供了可操作的见解。|
|**2024-07-03**|[Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation](http://arxiv.org/abs/2407.03056)|**[link](https://github.com/miccunifi/kdpl)**|视觉语言模型 (VLM) 在未见过的任务上表现出非凡的零样本泛化能力，但在有限数据下泛化到下游任务的性能不如监督方法。提示学习正在成为一种参数高效的 VLM 自适应方法，但最先进的方法需要带注释的样本。在本文中，我们提出了一种基于无监督知识蒸馏的新型提示学习方法，该方法从更强大的模型中提取知识。我们的方法称为知识蒸馏提示学习 (KDPL)，可以集成到现有的提示学习技术中，并消除了适应过程中对标记示例的需求。我们对十多个标准基准数据集进行的实验表明，KDPL 在提高学习提示的泛化能力方面非常有效，可以解决零样本域泛化、零样本跨数据集泛化和零样本基础到新类泛化问题。KDPL 不需要用于适应的基本事实标签，此外，我们还表明，即使在没有任何训练类名知识的情况下，它也可以用于有效地迁移知识。代码可在 https://github.com/miccunifi/KDPL 公开获取。|
|**2024-07-03**|[SAFT: Towards Out-of-Distribution Generalization in Fine-Tuning](http://arxiv.org/abs/2407.03036)|null|在机器学习领域，处理训练数据中的分布变化，即所谓的分布外 (OOD) 泛化，是一项重大挑战。虽然像 CLIP 这样的预训练视觉语言模型已经展现出卓越的零样本性能，但模型对下游任务的进一步适应会导致 OOD 数据出现不良的性能下降。在这项工作中，我们引入了用于微调的稀疏适应 (SAFT) 方法，该方法可以防止微调过程中遗忘预训练模型中的通用知识。SAFT 仅更新梯度幅度较大的一小部分重要参数，同时保持其他参数冻结。SAFT 易于实现且概念简单。大量实验表明，仅使用 0.1% 的模型参数，SAFT 就可以显著提高 CLIP 的性能。在多个基准测试中，它始终优于基线方法。在 ImageNet 及其变体的少样本学习基准测试中，在 OOD 设置下，SAFT 比传统的微调方法平均提高了 5.15% 的性能。|
|**2024-07-03**|[Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective](http://arxiv.org/abs/2407.02814)|null|在大型数据集上预训练的视觉语言模型 (VLM) 可能会通过将性别信息与特定对象或场景相关联而无意中学习到偏见。当前的方法侧重于修改输入并监控模型输出概率分数的变化，但往往难以从模型组件的角度全面理解偏见。我们提出了一个结合因果中介分析的框架，用于测量和映射 VLM 内偏见产生和传播的路径。这种方法使我们能够确定干预措施对模型偏差的直接影响，以及干预措施通过不同模型组件介导的对偏差的间接影响。我们的结果表明，图像特征是偏见的主要来源，其影响远高于文本特征，具体而言，在 MSCOCO 和 PASCAL-SENTENCE 数据集中分别占偏见的 32.57% 和 12.63%。值得注意的是，图像编码器的贡献超过了文本编码器和深度融合编码器。进一步的实验表明，语言和视觉模态的贡献是一致且不冲突的。因此，专注于模糊图像编码器中对模型偏见贡献最大的性别表征，可以有效地将 MSCOCO 和 PASCAL-SENTENCE 数据集中的偏见分别减少 22.03% 和 9.04%，而性能损失最小，计算量也没有增加。|
|**2024-07-03**|[MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context](http://arxiv.org/abs/2407.02730)|**[link](https://github.com/dongzizhu/medvh)**|大型视觉语言模型 (LVLM) 最近在自然图像和文本数据的各种任务中取得了优异的性能，这激发了大量关于 LVLM 微调和训练的研究。尽管取得了这些进步，但很少有研究关注这些模型在更小的数据集上微调时对幻觉的鲁棒性。在这项研究中，我们引入了一个新的基准数据集，即医学视觉幻觉测试 (MedVH)，用于评估特定领域 LVLM 的幻觉。 MedVH 包含五项任务，用于评估医学环境中 LVLM 的幻觉，其中包括全面理解文本和视觉输入以及生成长文本响应的任务。我们对通用 LVLM 和医学 LVLM 进行的广泛实验表明，尽管医学 LVLM 在标准医学任务中表现出良好的性能，但它们特别容易受到幻觉的影响，通常比通用模型更容易受到影响，这引发了人们对这些特定领域模型可靠性的严重担忧。为了使医学 LVLM 在实际应用中真正发挥价值，它们不仅必须准确地整合医学知识，还必须保持强大的推理能力以防止幻觉。我们的工作为未来对这些研究的评估铺平了道路。|
|**2024-07-02**|[Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models](http://arxiv.org/abs/2407.02716)|null|对预训练的视觉语言模型 (VLM) 进行微调已在医学图像和文本描述协同作用方面展现出卓越的能力。然而，许多预训练数据集受到患者隐私问题的限制，可能包含会对下游性能产生负面影响的噪声。此外，对多模态生成的日益依赖加剧了这个问题，因为它容易受到对抗性攻击。为了研究在对抗性噪声数据上训练的 VLM 如何在下游医学任务中执行，我们首先使用多模态对抗性攻击来制作噪声上游数据集。通过我们的综合分析，我们揭示了适度的噪声增强了模型的鲁棒性和可迁移性，但增加噪声水平会对下游任务性能产生负面影响。为了缓解这个问题，我们提出了校正对抗性噪声 (RAN) 框架，这是一种旨在有效防御对抗性攻击并在微调期间纠正上游噪声影响的方法。|
|**2024-07-02**|[D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data and eXpert model predictions](http://arxiv.org/abs/2407.02604)|null|大型视觉语言模型（VLM）已经从研究阶段发展到适用于通用用例的阶段，取得了令人难以置信的进步。LLaVA-Med 是一种开创性的生物医学大型语言和视觉助手，可以执行多模态生物医学图像和数据分析，为放射科医生提供自然语言界面。虽然它具有高度的通用性，并且可以处理多模态数据，但它目前受到大型语言模型领域现有挑战的限制。回复中的幻觉和不精确性可能导致误诊，这在目前阻碍了 VLM 的临床适应性。为了在医疗保健领域创建精确、用户友好的模型，我们提出了 D-Rax，这是一种特定领域、对话式的放射学辅助工具，可用于获取有关特定放射图像的见解。在这项研究中，我们增强了胸部 X 光（CXR）图像的对话分析，以支持放射学报告，提供来自医学成像的全面见解，并帮助制定准确的诊断。D-Rax 的实现是通过在我们策划的增强型指令跟随数据上微调 LLaVA-Med 架构来实现的，这些数据包括图像、指令以及从 MIMIC-CXR 成像数据、CXR 相关视觉问答 (VQA) 对和多个专家 AI 模型的预测结果中得出的疾病诊断和人口统计学预测。我们观察到，在对开放式和封闭式对话进行评估时，响应在统计学上都有显著改善。D-Rax 利用最先进的诊断模型与 VLM 相结合的力量，使临床医生能够使用自然语言与医学图像进行交互，这有可能简化他们的决策过程，提高诊断准确性并节省他们的时间。|
|**2024-07-02**|[Understanding Alignment in Multimodal LLMs: A Comprehensive Study](http://arxiv.org/abs/2407.02477)|null|偏好对齐已成为提升大型语言模型 (LLM) 性能的关键组成部分，但其对多模态大型语言模型 (MLLM) 的影响仍相对缺乏研究。与语言模型类似，用于图像理解任务的 MLLM 也面临着诸如幻觉之类的挑战。在 MLLM 中，幻觉不仅可以通过陈述错误的事实发生，还可以通过产生与图像内容不一致的响应来发生。MLLM 对齐的主要目标是鼓励这些模型使响应与图像信息更加一致。最近，多项工作引入了 MLLM 的偏好数据集，并研究了不同的对齐方法，包括直接偏好优化 (DPO) 和近端策略优化 (PPO)。然而，由于数据集、基础模型类型和对齐方法的不同，目前尚不清楚哪些具体因素对这些工作中报告的改进贡献最大。在本文中，我们独立分析了 MLLM 中偏好对齐的各个方面。我们首先将对齐算法分为两组，离线（如 DPO）和在线（如在线 DPO），并表明结合离线和在线方法可以在某些情况下提高模型的性能。我们回顾了各种已发布的多模态偏好数据集，并讨论了其构建细节如何影响模型性能。基于这些见解，我们引入了一种创建多模态偏好数据的新方法，称为偏差驱动幻觉采样 (BDHS)，它既不需要额外的注释也不需要外部模型，并表明它可以在各种基准测试中实现与先前发布的多模态模型对齐工作相当的性能。|
|**2024-07-02**|[Conceptual Codebook Learning for Vision-Language Models](http://arxiv.org/abs/2407.02350)|null|在本文中，我们提出了概念码本学习（CoCoLe），这是一种针对视觉语言模型（VLM）的新型微调方法，旨在解决在少量样本情况下对下游任务进行微调时提高VLM泛化能力的挑战。我们认识到，视觉概念（如纹理、形状和颜色）可以自然地跨域迁移，并且在泛化任务中发挥着至关重要的作用。受这一有趣发现的启发，我们学习了一个由视觉概念作为键、概念提示作为值的概念码本，它充当图像编码器输出和文本编码器输入之间的桥梁。具体来说，对于给定的图像，我们利用码本识别与类嵌入相关的最相关的概念提示，以执行分类。此外，我们还结合了一个手工制作的概念缓存作为正则化，以缓解低样本情况下出现的过拟合问题。我们观察到，这种概念码本学习方法能够增强视觉和语言模态之间的对齐。大量的实验结果表明，我们的CoCoLe方法在各种评估设置（包括从基础到新的泛化、跨数据集评估和域泛化任务）中都明显优于现有的最先进方法。详细的消融研究进一步证实了CoCoLe中每个组件的有效性。|
|**2024-07-02**|[Synthetic Multimodal Question Generation](http://arxiv.org/abs/2407.02233)|null|多模态检索增强生成 (MMRAG) 是一种强大的多模态文档问答方法。评估 MMRAG 的一个关键挑战是缺乏与目标问题风格和模态相匹配的高质量数据集。鉴于此，我们提出了 SMMQG，一个合成数据生成框架。SMMQG 利用检索器、大型语言模型 (LLM) 和大型多模态模型 (LMM) 之间的相互作用，直接从多模态文档中生成问答对，并使问题符合指定的风格和模态。我们使用 SMMQG 从维基百科文档中生成了一个包含 1024 个问题的 MMRAG 数据集，并使用该数据集评估了最先进的模型，揭示了只有通过特定风格和模态的评估数据才能获得的模型性能洞察。接下来，我们通过人工研究来衡量 SMMQG 产生的数据的质量。我们发现，我们的合成数据的质量与众包基准 MMQA 的质量相当，并且使用这两个数据集的下游评估结果非常一致。|
|**2024-07-02**|[Multi-Modal Video Dialog State Tracking in the Wild](http://arxiv.org/abs/2407.02218)|null|我们提出了 MST-MIXER，这是一个基于通用多模态状态跟踪方案的新型视频对话模型。目前声称能够执行多模态状态跟踪的模型在两个主要方面存在不足：(1) 它们要么只跟踪一种模态（主要是视觉输入），要么 (2) 它们针对的是不能反映现实世界复杂性的合成数据集。我们的模型解决了这两个限制，试图弥合这一关键的研究差距。具体来说，MST-MIXER 首先跟踪每个输入模态中最重要的成分。然后，它通过使用一种新颖的多模态图结构学习方法学习局部潜在图，从而预测每个模态所选成分缺失的底层结构。随后，将学习到的局部图和特征一起解析，形成一个在所有模态混合上运行的全局图，从而进一步细化其结构和节点嵌入。最后，利用细粒度的图节点特征来增强骨干视觉语言模型 (VLM) 的隐藏状态。MST-MIXER 在五个具有挑战性的基准测试中取得了新的最先进成果。|

<p align=right>(<a href=#updated-on-20240708>back to top</a>)</p>

## 6DOF Object Pose

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-06**|[Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking](http://arxiv.org/abs/2406.04316)|null|6D 物体姿态估计是计算机视觉中一项至关重要但极具挑战性的任务，其主要困难在于缺乏大规模数据集。这种数据稀缺性阻碍了对模型性能的全面评估，限制了研究进展。此外，可用实例或类别的数量有限也限制了其应用。为了解决这些问题，本文介绍了 Omni6DPose，这是一个以对象类别多样性、规模大和对象材质多样性为特征的大型数据集。Omni6DPose 主要由三个部分组成：ROPE（真实 6D 物体姿态估计数据集），包含 332K 张图像，涵盖 149 个类别、581 个实例的超过 150 万个标注；SOPE（模拟 6D 物体姿态估计数据集），包含 475K 张在混合现实环境中创建的、具有深度模拟的图像，涵盖与 ROPE 相同的 149 个类别、4162 个实例的超过 500 万个标注；以及在 ROPE 和 SOPE 中均使用的手动对齐的真实扫描物体。由于存在大量变化和歧义，Omni6DPose 本身就极具挑战性。为了应对这一挑战，我们引入了 GenPose++，它是 SOTA 类别级姿态估计框架的增强版本，包含两项关键改进：语义感知特征提取和基于聚类的聚合。此外，我们还提供了全面的基准分析，以评估先前方法在这个大规模数据集上在 6D 物体姿态估计和姿态跟踪方面的性能。|
|**2024-06-05**|[Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices](http://arxiv.org/abs/2406.02977)|null|随着机器人和增强现实应用越来越依赖于精确高效的6D物体姿态估计，边缘设备上的实时性能对于实现更具交互性和响应能力的系统至关重要。我们提出的稀疏颜色代码网络 (SCCN) 采用清晰简洁的流程设计来有效地满足这一需求。SCCN利用RGB图像中目标物体基本几何特征的稀疏性，对目标物体进行像素级预测，以加速Perspective-n-Point (PnP) 计算过程。此外，它引入了一种新颖的基于像素级几何的物体对称性表示，该表示与初始姿态预测无缝集成，有效地解决了对称物体的歧义问题。SCCN在NVIDIA Jetson AGX Xavier上分别实现了在基准LINEMOD数据集和遮挡LINEMOD数据集上每秒19帧 (FPS) 和6帧的估计速率，同时在这些速率下始终保持较高的估计精度。|
|**2024-05-31**|[Deep Learning-Based Object Pose Estimation: A Comprehensive Survey](http://arxiv.org/abs/2405.07801)|**[link](https://github.com/cnjianliu/awesome-object-pose-estimation)**|目标姿态估计是计算机视觉中的一个基本问题，在增强现实和机器人领域有着广泛的应用。在过去的十年中，深度学习模型由于其优越的准确性和鲁棒性，越来越多地取代了依赖于工程点对特征的传统算法。然而，当代方法仍然存在一些挑战，包括它们对标记训练数据的依赖性、模型紧凑性、在挑战性条件下的鲁棒性以及对新颖的未见过对象的泛化能力。目前缺少一篇综述来讨论该领域在不同方面取得的进展、面临的挑战以及未来有希望的方向。为了填补这一空白，我们讨论了基于深度学习的目标姿态估计的最新进展，涵盖了该问题的所有三种形式，即实例级、类别级和未见过目标姿态估计。我们的综述还涵盖了多种输入数据模态、输出姿态的自由度、目标属性和下游任务，为读者提供了对该领域的全面理解。此外，它还讨论了不同领域的训练范式、推理模式、应用领域、评估指标和基准数据集，并报告了当前最先进方法在这些基准上的性能，从而方便读者根据自己的应用选择最合适的方法。最后，该综述指出了关键挑战，回顾了当前的趋势及其优缺点，并指出了未来研究的有希望的方向。我们还将持续跟踪https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation 上的最新工作。|
|**2024-03-28**|[Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation](http://arxiv.org/abs/2403.19527)|**[link](https://github.com/leeiieeo/ag-pose)**|类别级6D物体姿态估计旨在估计特定类别中未见过实例的旋转、平移和尺寸。在这个领域，基于密集对应的方法已经取得了领先的性能。然而，它们没有明确考虑不同实例的局部和全局几何信息，导致对形状变化显著的未见过实例的泛化能力较差。为了解决这个问题，我们提出了一种新的实例自适应和几何感知的关键点学习方法，用于类别级6D物体姿态估计（AG-Pose），它包括两个关键设计：（1）第一个设计是实例自适应关键点检测模块，它可以自适应地检测一组稀疏的关键点，用于表示各种实例的几何结构。（2）第二个设计是几何感知特征聚合模块，它可以有效地将局部和全局几何信息整合到关键点特征中。这两个模块可以协同工作，为未见过的实例建立鲁棒的关键点级对应关系，从而增强模型的泛化能力。在CAMERA25和REAL275数据集上的实验结果表明，所提出的AG-Pose在没有类别特定形状先验的情况下，大大优于现有技术方法。|
|**2024-06-01**|[Object Pose Estimation via the Aggregation of Diffusion Features](http://arxiv.org/abs/2403.18791)|**[link](https://github.com/tianfu18/diff-feats-pose)**|从图像中估计物体姿态是 3D 场景理解的一项关键任务，最近的方法在非常大的基准测试中显示出可喜的成果。但是，这些方法在处理未知物体时性能会显著下降。我们认为这是由于图像特征的泛化能力有限造成的。为了解决这个问题，我们深入分析了扩散模型（例如 Stable Diffusion）的特征，这些模型在对未知物体建模方面具有巨大潜力。基于此分析，我们创新性地将这些扩散特征引入物体姿态估计。为此，我们提出了三种不同的架构，可以有效地捕获和聚合不同粒度的扩散特征，从而大大提高了物体姿态估计的泛化能力。我们的方法在三个流行的基准数据集 LM、O-LM 和 T-LESS 上以相当大的优势优于最先进的方法。特别是，我们的方法在未知物体上实现了比先前最佳技术更高的准确率：在 Unseen LM 上为 98.2% vs. 93.5%，在 Unseen O-LM 上为 85.9% vs. 76.3%，显示了我们方法强大的泛化能力。我们的代码发布在 https://github.com/Tianfu18/diff-feats-pose。|
|**2024-03-24**|[KITchen: A Real-World Benchmark and Dataset for 6D Object Pose Estimation in Kitchen Environments](http://arxiv.org/abs/2403.16238)|null|尽管最近在用于机器人抓取的6D物体姿态估计方法方面取得了进展，但这些方法在现有数据集上的性能与其在现实世界移动操作任务中的效率之间仍然存在很大差距，尤其是在机器人完全依赖其单目以自我为中心的视野（FOV）时。现有的现实世界数据集主要集中在桌面抓取场景，其中机械臂放置在固定位置，物体集中在固定外部相机视野的中心。评估此类数据集的性能可能无法准确反映在厨房环境中日常移动操作任务中遇到的挑战，例如从更高的架子、水槽、洗碗机、烤箱、冰箱或微波炉中取回物体。为了解决这一差距，我们提出了KITchen，这是一个专门为估计位于厨房环境中不同位置的物体的6D姿态而设计的新基准。为此，我们记录了一个包含大约205k个真实世界RGBD图像的综合数据集，这些图像用于在两个不同的厨房中捕获的111个厨房物体，利用一个具有人形机器人以自我为中心的视角。随后，我们开发了一个半自动注释管道，以简化此类数据集的标记过程，从而以最少的人力生成2D对象标签、2D对象分割掩码和6D对象姿态。基准、数据集和注释管道可在https://kitchen-dataset.github.io/KITchen获取。|
|**2024-03-22**|[DITTO: Demonstration Imitation by Trajectory Transformation](http://arxiv.org/abs/2403.15203)|null|快速便捷地教会机器人新技能对于机器人系统的广泛应用至关重要。在这项工作中，我们提出了一种两阶段方法，用于解决从单个RGB-D视频录制的演示中进行一次性模仿的问题。在第一阶段（离线阶段），我们提取演示的轨迹。这需要分割被操作物体并确定它们相对于次要物体（如容器）的相对运动。随后，在实时在线轨迹生成阶段，我们首先重新检测所有物体，然后将演示轨迹扭曲到当前场景，最后，我们使用机器人跟踪轨迹。为了完成这些步骤，我们的方法利用了几个辅助模型，包括用于分割、相对物体姿态估计和抓取预测的模型。我们系统地评估了对应方法和重新检测方法的不同组合，以验证我们跨各种任务的设计决策。具体来说，我们收集了十种不同任务的演示，包括拾放任务以及铰接物体操作。最后，我们在真实的机器人系统上进行了广泛的评估，以证明我们的方法在现实世界场景中的有效性和实用性。我们在http://ditto.cs.uni-freiburg.de上公开代码。|
|**2024-03-21**|[Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation](http://arxiv.org/abs/2403.14559)|null|在二维图像中定位预定义的三维关键点是建立用于六自由度物体姿态估计的三维-二维对应关系的有效方法。然而，不可见关键点的不可靠定位结果会降低对应关系的质量。在本文中，我们通过定位可见性方面的关键点来解决这个问题。由于关键点可见性信息在当前的数据集收集过程中缺失，我们提出了一种有效的方法，可以从可用的物体级标注中生成二进制可见性标签，用于非对称物体和对称物体的关键点。我们进一步基于PageRank算法从二进制标签中推导出实值的可见性感知重要性。利用我们可见性感知重要性的灵活性，我们将可见性感知重要性与最先进的姿态估计算法以及额外的 positional encoding 相结合，构建了VAPO（可见性感知姿态估计器）。在流行的姿态估计基准测试（包括Linemod、Linemod-Occlusion和YCB-V）上进行了大量实验。结果表明，VAPO 提高了关键点对应关系和最终估计姿态的质量，并明显达到了最先进的性能。|
|**2024-03-18**|[GenFlow: Generalizable Recurrent Flow for 6D Pose Refinement of Novel Objects](http://arxiv.org/abs/2403.11510)|null|尽管基于学习的方法在 6D 物体姿态估计方面取得了进展，但新物体在精度和可扩展性之间的权衡仍然存在。具体来说，以前针对新物体的方法没有很好地利用目标物体的 3D 形状信息，因为它们侧重于通过间接处理形状来实现泛化，从而降低了它们的有效性。我们提出了 GenFlow，这是一种在目标物体形状的指导下实现精度和对新物体的泛化的方法。我们的方法预测渲染图像和观察图像之间的光流，并迭代地细化 6D 姿态。它通过 3D 形状的约束和从端到端可微系统学习到的可泛化几何知识来提高性能。我们通过设计级联网络架构来进一步改进我们的模型，以利用多尺度相关性和从粗到精的细化。GenFlow 在 RGB 和 RGB-D 情况下均在未知物体姿态估计基准测试中排名第一。它还在没有任何微调的情况下，在已知物体姿态估计方面实现了与现有最先进方法相当的性能。|
|**2024-03-14**|[MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion](http://arxiv.org/abs/2403.09309)|null|杂乱的料仓拾取环境对姿态估计模型具有挑战性。尽管深度学习取得了令人瞩目的进步，但单视图RGB姿态估计模型在杂乱的动态环境中表现不佳。利用场景视频中包含的丰富时间信息有可能增强模型处理遮挡和环境动态性的不利影响的能力。此外，联合目标检测和姿态估计模型更适合利用任务的相互依赖性来提高两项任务的准确性。为此，我们提出了一种基于注意力的多目标6D姿态估计时间融合方法，该方法可以在视频序列的多个帧中积累信息。我们的MOTPose方法将一系列图像作为输入，并在一次前向传递中对所有目标执行联合目标检测和姿态估计。它学习使用基于交叉注意力的融合模块在多个时间步长上聚合目标嵌入和目标参数。我们在物理逼真的杂乱料仓拾取数据集SynPick和YCB-Video数据集上评估了我们的方法，并证明了姿态估计精度和目标检测精度的提高。|

<p align=right>(<a href=#updated-on-20240708>back to top</a>)</p>

## nerf

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-07-04**|[CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion Blur Images](http://arxiv.org/abs/2407.03923)|null|神经辐射场 (NeRFs) 因其高质量的新视角渲染能力而备受关注，促使人们对其在各种现实世界案例中的应用进行研究。其中一个关键挑战是相机在曝光时间内移动造成的运动模糊，这阻碍了对 3D 场景的准确重建。在本研究中，我们提出了连续刚体运动感知高斯散射 (CRiM-GS)，以实时渲染速度从模糊图像中重建准确的 3D 场景。考虑到实际的相机运动模糊过程包含复杂的运动模式，我们基于神经常微分方程 (ODEs) 预测相机的连续运动。具体而言，我们利用刚体变换对相机运动进行建模，并进行适当的正则化，以保持物体的形状和大小。此外，我们在 \textit{SE(3)} 场中引入连续可变形 3D 变换，通过确保更高的自由度使刚体变换适应现实世界的问题。通过重新审视基本的相机理论并采用先进的神经网络训练技术，我们实现了对连续相机轨迹的精确建模。我们进行了广泛的实验，在基准数据集上定量和定性地证明了其最先进的性能。|
|**2024-06-26**|[Trimming the Fat: Efficient Compression of 3D Gaussian Splats through Pruning](http://arxiv.org/abs/2406.18214)|null|近年来，由于神经辐射场和最近出现的3D高斯 splatting (3DGS) 模型提供了端到端训练的能力，3D模型的使用得到了推广。后者具有显著优势，因为它本身可以简化训练期间的快速收敛并提供广泛的可编辑性。然而，尽管发展迅速，但关于这些模型的可扩展性的文献仍处于起步阶段。在本研究中，我们针对解决这一差距采取了一些初步措施，展示了一种能够实现此类模型的内存和计算可扩展性的方法。具体来说，我们提出了“Trimming the fat”方法，这是一种基于梯度的迭代式后剪枝技术，用于消除模型中编码的冗余信息。我们在广泛认可的基准测试集上的实验结果证明了我们方法的有效性，表明在保持甚至改进基线性能的同时，可以去除高达75%的高斯函数。我们的方法实现了约50倍的压缩，同时保持了与基线模型相似的性能，并且能够将计算速度提高到600 FPS。|
|**2024-06-21**|[Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks](http://arxiv.org/abs/2406.15149)|null|仿真器是自主机器人学习的强大工具，因为它们提供了可扩展的数据生成、灵活的设计和轨迹优化。然而，将从仿真数据中学习到的行为迁移到现实世界中被证明是困难的，通常需要通过计算量大的域随机化方法或进一步的模型微调来缓解。我们提出了一种改进泛化能力和对真实视觉四旋翼导航任务中分布偏移的鲁棒性的方法。为此，我们首先通过将高斯 splatting 与四旋翼飞行动力学相结合来构建模拟器，然后使用 Liquid 神经网络训练鲁棒的导航策略。通过这种方式，我们获得了一个全栈模仿学习协议，它结合了 3D 高斯 splatting 辐射场渲染的进步、专家演示训练数据的巧妙编程以及 Liquid 网络的任务理解能力。通过一系列定量飞行测试，我们证明了在单个模拟场景中学习到的导航技能可以直接稳健地迁移到现实世界。我们进一步展示了在剧烈的分布和物理环境变化下，在训练环境之外保持性能的能力。我们学习到的 Liquid 策略，仅在从逼真的模拟室内飞行中精选的单目标机动上进行训练，可以推广到户外真实硬件平台上的多步远足。|
|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|在非结构化旅游环境中拍摄的照片经常表现出多变的外观和短暂的遮挡，这对精确的场景重建提出了挑战，并在新视角合成中导致了伪影。尽管先前的方法已经将神经辐射场 (NeRF) 与其他可学习模块相结合来处理动态外观并消除瞬态对象，但其大量的训练需求和缓慢的渲染速度限制了实际部署。最近，3D 高斯 splatting (3DGS) 已成为 NeRF 的一个有前途的替代方案，它提供了卓越的训练和推理效率以及更好的渲染质量。本文介绍了 Wild-GS，这是一种针对不受约束的照片集优化的 3DGS 创新改编，同时保留了其效率优势。Wild-GS 通过每个图像的固有材质属性、全局照明和相机属性以及点级反射率的局部变化来确定每个 3D 高斯的外观。与先前在图像空间中对参考特征进行建模的方法不同，Wild-GS 通过对从参考图像中提取的三平面进行采样，将像素外观特征明确地与其对应的局部高斯对齐。这种新颖的设计有效地将参考视图的高频细节外观转移到 3D 空间，并显着加快了训练过程。此外，利用 2D 可见性图和深度正则化分别减轻瞬态效应和约束几何形状。大量实验表明，Wild-GS 在所有现有技术中实现了最先进的渲染性能以及最高的训练和推理效率。|
|**2024-06-06**|[A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation](http://arxiv.org/abs/2406.04253)|null|三维建模一直是计算机视觉和计算机图形学中的重要领域。近年来，得益于神经表示和生成模型的突破，我们目睹了三维建模的快速发展。三维人体建模作为游戏和动画等众多现实世界应用的核心，受到了广泛关注。在过去几年中，出现了大量关于创建三维人体化身的著作，为三维人体建模形成了一个新的、丰富的知识库。文献的规模之大，使得个人难以跟踪所有的工作。本次综述旨在从重建和生成两个角度，全面概述这些新兴的三维人体化身建模技术。首先，我们回顾了三维人体重建的代表性方法，包括基于像素对齐隐函数、神经辐射场和三维高斯散射等方法。然后，我们总结了三维人体生成的代表性方法，特别是那些使用大型语言模型（如CLIP）、扩散模型和各种三维表示的方法，这些方法展示了最先进的性能。最后，我们讨论了对现有方法的反思以及三维人体化身建模面临的挑战，为未来的研究指明了方向。|
|**2024-06-13**|[3D-HGS: 3D Half-Gaussian Splatting](http://arxiv.org/abs/2406.02720)|**[link](https://github.com/lihaolin88/3d-half-gaussian-splatting)**|逼真的三维重建是三维计算机视觉中的一个基本问题。由于最近神经渲染技术的出现，该领域取得了相当大的进展。这些技术主要致力于学习三维场景的体积表示，并通过渲染得到的损失函数来细化这些表示。其中，三维高斯散射（3D-GS）已成为一种重要的方法，其性能超过了神经辐射场（NeRF）。3D-GS 使用参数化的三维高斯模型来建模空间位置和颜色信息，并结合基于图块的快速渲染技术。尽管其渲染性能和速度都很出色，但使用三维高斯核在准确表示不连续函数方面存在固有限制，特别是在形状不连续的边缘和角落，以及颜色不连续的不同纹理之间。为了解决这个问题，我们建议采用三维半高斯（3D-HGS）核，它可以作为一种即插即用的核函数。我们的实验表明，它们能够提高当前与 3D-GS 相关方法的性能，并在不影响渲染速度的情况下，在各种数据集上实现最先进的渲染性能。|
|**2024-06-04**|[FastLGS: Speeding up Language Embedded Gaussians with Feature Grid Mapping](http://arxiv.org/abs/2406.01916)|null|语义交互式辐射场因其具有促进用户友好且自动化的现实世界3D场景理解应用的潜力而一直是一项吸引人的任务。然而，要在辐射场中同时实现高质量、高效率和零样本能力的语义理解是一项具有挑战性的任务。在这项工作中，我们提出了FastLGS，这是一种支持在高分辨率3D Gaussian Splatting (3DGS)下进行实时开放词汇查询的方法。我们提出了语义特征网格来保存基于Segment Anything Model (SAM)掩码提取的多视图CLIP特征，并通过3DGS将网格映射到低维特征以进行语义场训练。一旦训练完成，我们可以通过渲染特征的特征网格恢复像素对齐的CLIP嵌入，用于开放词汇查询。与其他最先进方法的比较证明，FastLGS在速度和精度方面均取得了第一名的成绩，其中FastLGS比LERF快98倍，比LangSplat快4倍。同时，实验表明，FastLGS具有适应性，并兼容许多下游任务，例如3D分割和3D对象修复，可以轻松应用于其他3D操作系统。|
|**2024-05-30**|[ $\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving](http://arxiv.org/abs/2405.20323)|**[link](https://github.com/nnanhuang/s3gaussian)**|逼真的街道场景三维重建是开发自动驾驶真实世界模拟器的关键技术。尽管神经辐射场 (NeRF) 对驾驶场景非常有效，但三维高斯散射 (3DGS) 凭借其更快的速度和更明确的表示成为了一个有前途的方向。然而，大多数现有的街道 3DGS 方法都需要跟踪的三维车辆边界框来分解静态和动态元素以进行有效重建，这限制了它们在野外场景中的应用。为了在没有昂贵注释的情况下促进高效的三维场景重建，我们提出了一种自监督的街道高斯（$\textit{S}^3$Gaussian）方法，用于从 4D 一致性中分解动态和静态元素。我们使用三维高斯函数来表示每个场景以保持其明确性，并进一步将其与时空场网络相结合，以紧凑地模拟 4D 动态。我们在具有挑战性的 Waymo-Open 数据集上进行了大量实验，以评估我们方法的有效性。我们的 $\textit{S}^3$ Gaussian 展示了在不使用 3D 注释的情况下分解静态和动态场景的能力，并实现了最佳性能。代码地址：https://github.com/nnanhuang/S3Gaussian/。|
|**2024-05-28**|[RT-GS2: Real-Time Generalizable Semantic Segmentation for 3D Gaussian Representations of Radiance Fields](http://arxiv.org/abs/2405.18033)|null|高斯渲染技术通过实现实时的高性能渲染，彻底改变了新视角合成的世界。最近，研究重点集中在为下游任务丰富这些3D表示的语义信息。在本文中，我们介绍了RT-GS2，这是第一个采用高斯渲染技术的可泛化语义分割方法。虽然现有的基于高斯渲染的方法依赖于场景特定的训练，但RT-GS2展示了泛化到未见场景的能力。我们的方法采用了一种新方法，首先以自监督的方式提取视图无关的3D高斯特征，然后进行新颖的视图依赖/视图无关（VDVI）特征融合，以增强不同视图之间的语义一致性。在三个不同数据集上的大量实验表明，RT-GS2在语义分割质量方面优于最先进的方法，例如在Replica数据集上的mIoU提高了8.01%。此外，我们的方法实现了27.03 FPS的实时性能，与现有方法相比实现了惊人的901倍加速。据我们所知，这项工作通过引入第一个用于辐射场3D高斯表示的实时可泛化语义分割方法，代表了该领域的重大进步。|
|**2024-05-29**|[PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting](http://arxiv.org/abs/2405.16829)|null|神经辐射场 (NeRFs) 在合成大规模场景的逼真图像方面表现出了非凡的能力。然而，它们经常受到细节丢失和渲染时间长的困扰。三维高斯 splatting 最近被引入作为一种有效的替代方案，可以实现高保真视觉效果和更快的渲染性能。尽管如此，扩展三维高斯 splatting 仍然充满了挑战。具体来说，大规模场景需要整合来自多个尺度和不同视点的对象，这通常会导致效率下降，因为高斯需要在细节级别之间取得平衡。此外，从大规模数据集中通过 COLMAP 生成初始化点不仅计算量大，而且容易导致重建不完整。为了应对这些挑战，我们提出了采用 NeRF 初始化的金字塔式三维高斯 splatting (PyGS)。我们的方法采用以金字塔形式排列的分层高斯集合来表示场景。金字塔的顶层由一些大的高斯函数组成，而随后的每一层都包含更密集的小高斯函数集合。我们通过以不同的频率对快速训练的基于网格的 NeRF 进行采样，从而有效地初始化这些金字塔高斯函数。我们将这些金字塔高斯函数分组到簇中，并使用紧凑的加权网络在渲染过程中动态确定每个簇中每个金字塔级别的影响，同时考虑相机视点。我们的方法在多个大规模数据集上实现了显著的性能飞跃，渲染速度比当前最先进的方法快 400 多倍。|
|**2024-05-11**|[Direct Learning of Mesh and Appearance via 3D Gaussian Splatting](http://arxiv.org/abs/2405.06945)|null|准确重建包含显式几何信息的3D场景既有吸引力又具有挑战性。几何重建可以受益于结合可微分的表观模型，例如神经辐射场和3D高斯 splatting (3DGS)。在这项工作中，我们提出了一个可学习的场景模型，它将3DGS与显式几何表示（即网格）结合起来。我们的模型以端到端的方式学习网格和外观，我们将3D高斯函数绑定到网格面上，并执行3DGS的可微分渲染以获得光度监督。该模型创建了一个有效的信息通路来监督场景学习，包括网格。实验结果表明，学习到的场景模型不仅实现了最先进的渲染质量，而且还支持使用显式网格进行操作。此外，由于网格和外观的端到端学习，我们的模型在适应场景更新方面具有独特优势。|

<p align=right>(<a href=#updated-on-20240708>back to top</a>)</p>

## 分类/检测/识别/分割

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-07-05**|[SH17: A Dataset for Human Safety and Personal Protective Equipment Detection in Manufacturing Industry](http://arxiv.org/abs/2407.04590)|**[link](https://github.com/ahmadmughees/sh17dataset)**|工作场所事故持续对人类安全构成重大风险，特别是在建筑和制造等行业，因此，有效的个人防护装备 (PPE) 合规性变得越来越重要。我们的研究重点是开发基于目标检测 (OD) 和卷积神经网络 (CNN) 的非侵入性技术，以检测和验证各种类型 PPE 的正确使用，例如安全帽、安全眼镜、口罩和防护服。本研究提出了 SH17 数据集，其中包含从不同工业环境中收集的 8,099 张带注释的图像，这些图像包含 75,994 个 17 个类别的实例，用于训练和验证 OD 模型。我们已经训练了最先进的 OD 模型进行基准测试，初步结果表明，You Only Look Once (YOLO)v9-e 模型变体的 PPE 检测精度超过 70.9%，达到了令人满意的水平。跨域数据集上的模型验证性能表明，集成这些技术可以显着改进安全管理系统，为努力满足人类安全法规和保护员工的行业提供可扩展且高效的解决方案。该数据集可在 https://github.com/ahmadmughees/sh17dataset 获取。|
|**2024-07-05**|[Multi-Branch Auxiliary Fusion YOLO with Re-parameterization Heterogeneous Convolutional for accurate object detection](http://arxiv.org/abs/2407.04381)|**[link](https://github.com/yang-0201/MAF-YOLO)**|由于多尺度特征融合的有效性，路径聚合特征金字塔网络 (PAFPN) 广泛应用于 YOLO 检测器中。然而，它无法同时高效且自适应地融合高级语义信息和低级空间信息。在本文中，我们提出了一个名为 MAF-YOLO 的新模型，它是一个具有名为多分支辅助特征金字塔网络 (MAFPN) 的新型目标检测框架。在 MAFPN 中，浅层辅助融合 (SAF) 模块旨在将主干网络的输出与颈部结合起来，保留最佳级别的浅层信息，以便于后续学习。同时，深度嵌入颈部的高级辅助融合 (AAF) 模块将更多样化的梯度信息传递到输出层。此外，我们提出的重新参数化的异构高效层聚合网络 (RepHELAN) 模块确保了整体模型架构和卷积设计都采用了异构大卷积核。因此，这保证了保留与小目标相关的信息，同时实现了多尺度感受野。最后，以 MAF-YOLO 的纳米版本为例，它在 COCO 数据集上仅用 3.76M 可学习参数和 10.51G FLOPs 即可达到 42.4% 的 AP，性能优于 YOLOv8n 约 5.1%。本研究的源代码可在以下网址获取：https://github.com/yang-0201/MAF-YOLO。|
|**2024-07-05**|[FeatureSORT: Essential Features for Effective Tracking](http://arxiv.org/abs/2407.04249)|null|在这项工作中，我们介绍了一种新颖的跟踪器，该跟踪器专为在线多目标跟踪而设计，其重点在于简单而有效。我们提供了多个特征模块，每个模块代表一个特定的外观信息。通过整合不同的外观特征，包括服装颜色、款式和目标方向，以及用于鲁棒嵌入提取的 ReID 网络，我们的跟踪器显著提高了在线跟踪精度。此外，我们建议结合更强大的检测器，并提供先进的后处理方法，进一步提升跟踪器的性能。在实时操作期间，我们建立测量来跟踪关联距离函数，其中包括 IoU、方向、颜色、样式和 ReID 特征相似性信息，其中每个指标分别计算。通过我们设计的特征相关距离函数，可以跟踪物体更长时间的遮挡，同时保持相对较低的身份切换次数。广泛的实验评估表明，跟踪精度和可靠性显着提高，身份切换减少和遮挡处理增强证明了这一点。这些进步不仅有助于推动目标跟踪领域的最新技术水平，而且为未来需要高精度和可靠性的研究和实际应用开辟了新途径。|
|**2024-07-05**|[AnySR: Realizing Image Super-Resolution as Any-Scale, Any-Resource](http://arxiv.org/abs/2407.04241)|**[link](https://github.com/crispyfeso4/anysr)**|为了提高单图像超分辨率 (SISR) 应用的效率和可扩展性，我们引入了 AnySR，将现有的任意尺度 SR 方法重建为任意尺度、任意资源的实现。与使用相同计算成本解决各种规模的 SR 任务的现成方法相比，我们的 AnySR 创新在于：1) 将任意尺度任务构建为任意资源实现，在不增加额外参数的情况下减少了较小规模的资源需求；2) 以特征交织的方式增强任意尺度性能，将尺度对以规则的间隔插入特征中，并确保正确的特征/尺度处理。我们通过重建大多数现有的任意尺度 SISR 方法并在五个流行的 SISR 测试数据集上进行验证，充分证明了 AnySR 的有效性。结果表明，我们的 AnySR 以计算效率更高的方式实现了 SISR 任务，并且性能与现有的任意尺度 SISR 方法相当。我们首次实现了 SISR 任务，不仅在文献中是任意尺度的，而且是任意资源的。代码可在 https://github.com/CrispyFeSo4/AnySR 获取。|
|**2024-07-05**|[AMD: Automatic Multi-step Distillation of Large-scale Vision Models](http://arxiv.org/abs/2407.04208)|null|基于Transformer的架构因其优越的性能已成为各种视觉任务的标准模型。随着模型规模的不断扩大，模型蒸馏在各种实际应用中变得极其重要，特别是在受计算资源限制的设备上。然而，当教师模型和学生模型之间存在较大的容量差距时，例如10倍的压缩率，现有的知识蒸馏方法效果会下降。在本文中，我们提出了一种名为自动多步蒸馏（AMD）的新方法，用于大规模视觉模型压缩。具体来说，我们的蒸馏过程分为多个步骤。首先，对教师模型进行蒸馏，形成一个中间的助教模型，然后进一步蒸馏到学生模型。我们引入了一个高效且有效的优化框架，来自动识别能够使学生模型性能最大化的最佳助教模型。我们在多个图像分类数据集上进行了广泛的实验，包括CIFAR-10、CIFAR-100和ImageNet。结果一致表明，我们的方法优于几种已建立的基线方法，为未来大规模视觉模型的知识蒸馏方法铺平了道路。|
|**2024-07-04**|[Attention Normalization Impacts Cardinality Generalization in Slot Attention](http://arxiv.org/abs/2407.04170)|null|以对象为中心的场景分解对于计算机视觉和机器人等领域的下游任务非常重要。最近提出的槽位注意力模块已经被一些衍生作品用于图像分割和视频目标跟踪，它是一种深度学习组件，可以在输入图像上执行无监督的以对象为中心的场景分解。它基于一种注意力架构，其中潜在的槽位向量（包含对象的压缩信息）关注来自输入图像的局部感知特征。在本文中，我们发现对注意力架构中聚合值进行归一化的设计决策对槽位注意力泛化到训练期间所见到的更多槽位和对象的能力有相当大的影响。我们认为，原始的槽位注意力归一化方案丢弃了像素先前分配给槽位的概率信息，这损害了其泛化能力。基于这些发现，我们提出并研究了替代的归一化方法，这些方法可以提高槽位注意力对不同槽位和对象数量的泛化能力，从而提高无监督图像分割任务的性能。|
|**2024-07-04**|[Detect Closer Surfaces that can be Seen: New Modeling and Evaluation in Cross-domain 3D Object Detection](http://arxiv.org/abs/2407.04061)|null|目前，域适应技术在自动驾驶三维目标检测领域的性能尚未达到理想水平，这主要是由于车辆尺寸的显著差异以及跨域应用时运行环境的不同。这些因素共同阻碍了从特定数据集中学习到的知识的有效迁移和应用。由于现有的评估指标最初是通过计算预测边界框和真实边界框之间的二维或三维重叠来设计用于单个域上的评估，因此它们经常会遇到由数据集之间的大小差异引起的过拟合问题。这引发了一个与评估三维目标检测模型跨域性能相关的基本问题：我们是否真的需要模型在跨域应用后保持其原始三维边界框的出色性能？从实际应用的角度来看，我们的主要关注点之一实际上是防止车辆与其他障碍物发生碰撞，特别是在跨域场景中，正确预测车辆尺寸要困难得多。换句话说，只要模型能够准确识别出距离自动驾驶车辆最近的表面，就足以有效避开障碍物。在本文中，我们提出了两个指标来衡量三维目标检测模型检测自动驾驶车辆传感器附近表面的能力，这可以用来更全面、更合理地评估其跨域性能。此外，我们提出了一个名为EdgeHead的优化头，用于引导模型更加关注可学习的较近表面，这可以极大地提高现有模型在我们的新指标下以及在原始BEV/3D指标下的跨域性能。|
|**2024-07-04**|[TrackPGD: A White-box Attack using Binary Masks against Robust Transformer Trackers](http://arxiv.org/abs/2407.03946)|null|使用Transformer骨干网络的目标跟踪器在视觉目标跟踪数据集上取得了强大的性能。然而，这些跟踪器的对抗鲁棒性在文献中尚未得到很好的研究。由于骨干网络的差异，为目标跟踪提出的对抗性白盒攻击不能迁移到所有类型的跟踪器。例如，像MixFormerM这样的Transformer跟踪器在黑盒攻击后仍然运行良好，特别是在预测目标二进制掩码方面。我们提出了一种名为TrackPGD的新型白盒攻击，它依靠预测的目标二进制掩码来攻击鲁棒的Transformer跟踪器。这种新攻击通过采用著名的SegPGD分割攻击来关注标注掩码，从而能够成功地对依赖Transformer骨干网络的跟踪器进行白盒攻击。实验结果表明，TrackPGD能够有效攻击基于Transformer的跟踪器，例如MixFormerM、OSTrackSTS和TransT-SEG，并在多个跟踪数据集上取得了成功。|
|**2024-07-04**|[DocXplain: A Novel Model-Agnostic Explainability Method for Document Image Classification](http://arxiv.org/abs/2407.03830)|null|深度学习（DL）彻底改变了文档图像分析领域，在各种任务中展现出超越人类的表现。然而，深度学习模型固有的黑盒性质仍然是其在行业中安全稳健部署的重大挑战。遗憾的是，尽管近年来大量研究致力于开发基于深度学习的文档分析系统，但解决其透明性方面的研究却相对较少。在本文中，我们旨在通过介绍 DocXplain 来弥合这一研究差距，这是一种新颖的模型无关的可解释性方法，专门设计用于为文档图像分类任务生成高可解释性的特征归因图。具体来说，我们的方法涉及将文档的前景和背景特征独立地分割成不同的文档元素，然后消融这些元素以分配特征重要性。我们在文档图像分类的背景下广泛评估了我们提出的方法，利用 4 种不同的评估指标、2 个广泛认可的文档基准数据集和 10 个最先进的文档图像分类模型。通过对 9 种现有的最先进的归因方法进行全面的定量和定性分析，我们证明了我们的方法在保真度和可解释性方面的优越性。据作者所知，这项工作提出了第一个专门针对文档图像量身定制的模型无关的基于归因的可解释性方法。我们预计我们的工作将极大地促进文档图像分类模型的透明度、公平性和鲁棒性研究的进展。|
|**2024-07-04**|[M^3:Manipulation Mask Manufacturer for Arbitrary-Scale Super-Resolution Mask](http://arxiv.org/abs/2407.03695)|null|在图像篡改定位（IML）领域，现有数据集数量少、质量差一直是主要问题。包含各种篡改类型的数据集将极大地提高IML模型的准确性。互联网上的图像（例如百度贴吧PS吧的图像）使用各种技术进行篡改，利用这些图像创建数据集将显著丰富我们数据中的篡改类型。然而，互联网上的图像存在分辨率和清晰度问题，通过简单地从原始图像中减去篡改图像获得的掩码包含各种噪声。这些噪声很难去除，导致掩码无法用于IML模型。受变化检测领域的启发，我们将原始图像和篡改图像视为同一图像随时间的变化，并将数据生成任务视为变化检测任务。然而，由于图像之间的清晰度问题，传统的变化检测模型表现不佳。因此，我们引入了一个超分辨率模块，并提出了篡改掩码生成器（MMM）框架。它增强了原始图像和篡改图像的分辨率，从而改善了图像细节，以便更好地进行比较。同时，该框架将原始图像和篡改图像转换为特征嵌入并进行拼接，有效地对上下文进行建模。此外，我们创建了篡改掩码生成器数据集（MMMD），这是一个涵盖了各种篡改技术的数据集。我们的目标是通过MMM和MMMD提供更真实的篡改数据，为图像取证和篡改检测领域做出贡献。有关MMMD和下载链接的详细信息，请访问：代码和数据集将公开。|
|**2024-07-03**|[Visual Grounding with Attention-Driven Constraint Balancing](http://arxiv.org/abs/2407.03243)|null|不同于目标检测，视觉定位任务需要检测由复杂的自由形式语言描述的对象。为了同时对这种复杂的语义和视觉表示进行建模，最近最先进的研究采用基于 Transformer 的模型来融合来自两种模态的特征，并进一步引入了各种模块来调节视觉特征，使其与语言表达保持一致并消除不相关的冗余信息。然而，它们的损失函数仍然采用常见的目标检测损失，只控制边界框回归输出，无法完全优化上述目标。为了解决这个问题，本文首先分析了基于 Transformer 模型的注意力机制。在此基础上，我们进一步提出了一个名为注意力驱动约束平衡（AttBalance）的新框架，以优化语言相关区域内视觉特征的行为。大量的实验结果表明，我们的方法带来了令人印象深刻的改进。具体来说，我们在四个不同基准上评估的五种不同模型上实现了持续的改进。此外，通过将我们的方法集成到 QRNet 中，我们获得了新的最先进的性能。|
|**2024-07-03**|[Category-Aware Dynamic Label Assignment with High-Quality Oriented Proposal](http://arxiv.org/abs/2407.03205)|null|航拍图像中的物体通常嵌入在复杂的背景中，并呈现任意方向。当使用定向边界框 (OBB) 表示任意方向的物体时，角度的周期性可能导致边界处标签回归值的不连续性，从而导致损失函数出现剧烈波动。为了解决这个问题，在定向检测框架中引入了一种基于复平面的 OBB 表示方法，并提出了一种三角损失函数。此外，利用对复杂背景环境和航拍图像中大型物体显著差异的先验知识，构建了一个 Conformer RPN 头部来预测角度信息。所提出的损失函数和 Conformer RPN 头部共同生成高质量的定向建议。针对仅依靠 IoU 进行建议标签分配的局限性，提出了一种基于预测类别反馈的类别感知动态标签分配方法。该方法使负样本选择更具代表性，确保分类和回归特征之间的一致性。在四个真实的定向检测数据集上进行了实验，结果表明，在参数调整和时间成本最小的情况下，定向目标检测的性能更优。具体而言，在 DOTA-v1.0、DOTA-v1.5、DIOR-R 和 HRSC2016 数据集上分别实现了 82.02%、71.99%、69.87% 和 98.77% 的平均精度均值 (mAP) 分数。|
|**2024-07-03**|[SegVG: Transferring Object Bounding Box to Segmentation for Visual Grounding](http://arxiv.org/abs/2407.03200)|**[link](https://github.com/weitaikang/segvg)**|与目标检测不同，视觉定位（Visual Grounding）旨在为每个文本-图像对检测一个边界框。这种为每个文本-图像数据提供一个边界框的方式提供了稀疏的监督信号。尽管先前的工作取得了令人瞩目的成果，但它们对标注的被动利用，即将边界框标注仅用作回归真值，导致了性能欠佳。在本文中，我们提出了SegVG，这是一种将边界框级标注转换为分割信号的新方法，以便为视觉定位提供额外的像素级监督。具体来说，我们提出了多层多任务编码器-解码器作为目标定位阶段，在该阶段中，我们学习回归查询和多个分割查询，分别通过在每个解码层中对边界框进行回归和分割来定位目标。这种方法使我们能够迭代地利用标注作为边界框级回归和像素级分割的信号。此外，由于骨干网络通常由从单模态任务中学习到的预训练参数初始化，并且用于回归和分割的查询都是静态可学习的嵌入，因此这三种类型的特征之间存在域差异，这会损害后续的目标定位。为了减轻这种差异，我们引入了三重对齐模块，其中查询、文本和视觉标记通过三重注意力机制进行三角更新，以共享相同的空间。在五个广泛使用的数据集上进行的大量实验验证了我们的方法达到了最先进的性能 (SOTA)。|
|**2024-07-03**|[Global Context Modeling in YOLOv8 for Pediatric Wrist Fracture Detection](http://arxiv.org/abs/2407.03163)|**[link](https://github.com/ruiyangju/yolov8_global_context_fracture_detection)**|儿童在日常生活中经常遭受腕部损伤，而骨折损伤放射科医生通常需要在外科医生进行手术治疗之前分析和解释 X 光图像。深度学习的发展使神经网络模型能够作为计算机辅助诊断 (CAD) 工具来帮助医生和专家进行诊断。由于 YOLOv8 模型在目标检测任务中取得了令人满意的成功，因此它已被应用于骨折检测。全局上下文 (GC) 模块以轻量级的方式有效地对全局上下文进行建模，将其融入 YOLOv8 可以极大地提高模型性能。本文提出了用于骨折检测的 YOLOv8+GC 模型，它是具有 GC 模块的 YOLOv8 模型的改进版本。实验结果表明，与原始的 YOLOv8 模型相比，所提出的 YOLOv8-GC 模型在 GRAZPEDWRI-DX 数据集上将交并比阈值为 0.5 时的平均精度均值 (mAP 50) 从 63.58% 提高到 66.32%，达到了最先进的水平 (SOTA)。这项工作的实现代码可在 GitHub 上获取：https://github.com/RuiyangJu/YOLOv8_Global_Context_Fracture_Detection。|
|**2024-07-03**|[Applying Extended Object Tracking for Self-Localization of Roadside Radar Sensors](http://arxiv.org/abs/2407.03084)|null|智能交通系统 (ITS) 可以受益于路边 4D 毫米波雷达传感器，用于大规模交通监控，因为它们具有全天候功能、长感应范围和低制造成本。然而，在城市环境中，使用外部测量设备的定位方法存在局限性。此外，如果传感器安装由于环境影响而出现变化，则在仅在安装期间执行测量时无法对其进行校正。在本文中，我们提出了使用扩展目标跟踪 (EOT) 的路边雷达数据自定位方法。该方法分析传感器观察到的车辆跟踪轨迹和城市街道的航空激光扫描，将“直行”、“左转”、“右转”等驾驶行为标签分配给轨迹段和路段，并执行语义迭代最近点 (SICP) 算法来配准点云。该方法利用下游任务（目标跟踪）的结果进行定位。我们展示了亚米范围内的高精度以及非常低的方位误差。该方法还显示出良好的数据效率。评估在仿真和实际测试中均已完成。|
|**2024-07-03**|[YOLOv5, YOLOv8 and YOLOv10: The Go-To Detectors for Real-time Vision](http://arxiv.org/abs/2407.02988)|null|本文全面回顾了YOLO（You Only Look Once）目标检测算法的演进过程，重点关注YOLOv5、YOLOv8和YOLOv10。我们分析了这些版本在架构改进、性能提升以及边缘部署适用性方面的差异。YOLOv5引入了CSPDarknet骨干网络和Mosaic数据增强等重大创新，实现了速度和精度之间的平衡。YOLOv8在此基础上，通过增强特征提取和无锚框检测，提高了算法的通用性和性能。YOLOv10则凭借无NMS训练、空间通道解耦下采样以及大核卷积等技术实现了跨越式发展，以更低的计算开销实现了最先进的性能。我们的研究结果突出了YOLO算法在精度、效率和实时性能方面的逐步提升，特别强调了其在资源受限环境中的适用性。本综述提供了模型复杂度和检测精度之间权衡的见解，为针对特定边缘计算应用选择最合适的YOLO版本提供了指导。|
|**2024-07-03**|[ShiftAddAug: Augment Multiplication-Free Tiny Neural Network with Hybrid Computation](http://arxiv.org/abs/2407.02881)|null|缺乏乘法运算符（例如移位和加法）因其与硬件的兼容性而受到关注。然而，与具有相同结构的传统神经网络 (NN) 相比，采用这些运算符的神经网络 (NN) 通常表现出较低的精度。ShiftAddAug 使用成本高昂的乘法来增强高效但功能较弱的无乘法运算符，从而在没有任何推理开销的情况下提高性能。它将一个 ShiftAdd 小型神经网络放入一个大型乘法模型中，并鼓励将其训练为子模型以获得额外的监督。为了解决混合运算符之间的权重差异问题，提出了一种新的权重共享方法。此外，一种新颖的两阶段神经架构搜索用于为更小但更强的无乘法小型神经网络获得更好的增强效果。ShiftAddAug 的优越性通过图像分类和语义分割实验得到验证，始终如一地提供显着的增强。值得注意的是，与直接训练的对应模型相比，它在 CIFAR100 上的准确率提高了 4.95%，甚至超过了乘法神经网络的性能。|
|**2024-07-03**|[A Pairwise DomMix Attentive Adversarial Network for Unsupervised Domain Adaptive Object Detection](http://arxiv.org/abs/2407.02835)|null|无监督域自适应目标检测 (DAOD) 可以使在一个源域上训练的模型适应未标记的目标域，以进行目标检测。现有的无监督 DAOD 方法通常执行从目标域到源域的特征对齐。单向域迁移会忽略有关目标样本的信息，并在存在较大域差异时导致欠佳的自适应。因此，我们提出了一种具有域混合 (DomMix) 模块的成对注意力对抗网络，以缓解上述挑战。具体来说，采用深度混合来构建一个中间域，允许来自两个域的特征共享它们的差异。然后，应用成对注意力对抗网络，在不同尺度的图像级和实例级特征上进行注意力编码，并通过对抗学习优化域对齐。这使得网络能够专注于具有不同上下文信息的区域，并学习它们在不同域之间的相似性。在几个基准数据集上进行了广泛的实验，证明了我们提出的方法的优越性。|
|**2024-07-03**|[Data Overfitting for On-Device Super-Resolution with Dynamic Algorithm and Compiler Co-Design](http://arxiv.org/abs/2407.02813)|**[link](https://github.com/coulsonlee/dy-dca-eccv2024)**|深度神经网络 (DNN) 经常被应用于各种计算机视觉应用。如今，当前视频分发系统中一个新兴趋势是利用 DNN 的过拟合特性来执行视频分辨率提升。通过将视频分割成块并应用超分辨率 (SR) 模型对每个块进行过拟合，这种 SR 模型加视频块的方案能够取代传统的视频传输，从而提高视频质量和传输效率。然而，为了保证高性能，需要许多模型和块，这会导致用户端的模型切换和内存占用方面产生巨大的开销。为了解决这些问题，我们提出了一种由内容感知数据处理管道辅助的动态深度神经网络，以将模型数量减少到一个 (Dy-DCA)，这有助于在节省计算资源的同时提高性能。此外，为了在用户端实现真正的加速，我们设计了一个框架来优化 Dy-DCA 中的动态特征（例如，动态形状、大小和控制流），从而实现一系列编译优化，包括融合代码生成、静态执行计划等。通过采用这些技术，我们的方法在现成的手机上实现了更好的 PSNR 和实时性能 (33 FPS)。同时，在我们的编译优化的辅助下，我们实现了 1.7 倍的加速，同时节省了高达 1.61 倍的内存消耗。代码可在 https://github.com/coulsonlee/Dy-DCA-ECCV2024 获取。|
|**2024-07-03**|[Fine-Grained Scene Image Classification with Modality-Agnostic Adapter](http://arxiv.org/abs/2407.02769)|**[link](https://github.com/qunilcs/maa)**|在处理细粒度场景图像分类任务时，以往的大多数工作在进行多模态特征融合时都非常重视全局视觉特征。换句话说，模型的设计是基于对不同模态重要性的先验直觉。在本文中，我们提出了一种新的多模态特征融合方法，称为MAA（模态无关适配器），试图使模型自适应地学习不同模态在不同情况下的重要性，而无需在模型架构中进行先验设置。更具体地说，我们消除了分布中的模态差异，然后使用模态无关的Transformer编码器进行语义级别的特征融合。我们的实验表明，通过应用与以前方法相同的模态，MAA在基准测试中取得了最先进的结果。此外，值得一提的是，在使用MAA时，可以轻松添加新的模态，并进一步提升性能。代码可在https://github.com/quniLcs/MAA获取。|

<p align=right>(<a href=#updated-on-20240708>back to top</a>)</p>

## 生成模型

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-07-05**|[Structural Constraint Integration in Generative Model for Discovery of Quantum Material Candidates](http://arxiv.org/abs/2407.04557)|null|已知的有机分子数以十亿计，但已发现的功能性无机材料却只占很小一部分，这对寻找新型量子材料的研究群体来说是一个特别突出的问题。基于机器学习的生成模型，尤其是扩散模型的最新进展，为生成新型稳定材料带来了巨大希望。然而，将几何模式融入材料生成仍然是一项挑战。在此，我们介绍了在生成模型中集成结构约束的方法 (SCIGEN)。我们的方法可以通过在每个扩散步骤之前，用扩散约束结构对去噪结构进行策略性掩蔽，从而将生成引导至约束输出，来修改任何经过训练的生成扩散模型。此外，我们从数学上证明了 SCIGEN 可以有效地从原始分布中执行条件采样，这对于生成稳定的约束材料至关重要。我们使用阿基米德格作为原型约束生成了 800 万种化合物，其中超过 10% 通过了多阶段稳定性预筛选。对 26,000 种存活化合物的 DFT（密度泛函理论）高通量计算表明，超过 50% 的化合物通过了 DFT 级别的结构优化。由于量子材料的性质与几何模式密切相关，我们的结果表明 SCIGEN 为生成量子材料候选材料提供了一个通用框架。|
|**2024-07-05**|[Unified continuous-time q-learning for mean-field game and mean-field control problems](http://arxiv.org/abs/2407.04521)|null|本文从代表性智能体的角度研究了平均场跳扩散模型中的连续时间q学习。为了克服无法直接观察到总体分布时的挑战，我们引入了解耦形式的集成q函数（解耦Iq函数），并建立了其与价值函数的鞅表征，这为平均场博弈（MFG）和平均场控制（MFC）问题提供了统一的策略评估规则。此外，根据求解MFG或MFC问题的任务，我们可以通过不同的方式利用解耦Iq函数来分别学习平均场均衡策略或平均场最优策略。因此，我们利用所有源于平均场交互的测试策略，设计了一种适用于MFG和MFC问题的统一q学习算法。对于跳扩散环境下的几个例子，包括LQ框架内外的例子，我们可以获得解耦Iq函数和价值函数的精确参数化，并从代表性智能体的角度说明我们的算法具有令人满意的性能。|
|**2024-07-05**|[Speed-accuracy trade-off for the diffusion models: Wisdom from nonequlibrium thermodynamics and optimal transport](http://arxiv.org/abs/2407.04495)|null|我们探讨了生成模型（称为扩散模型）与非平衡热力学中用于描述福克-普朗克方程的随机热力学之间的联系。基于随机热力学技术，我们推导出了扩散模型的速度-精度权衡，这是扩散模型中数据生成速度和精度之间的权衡关系。我们的结果表明，正向过程中的熵产生率会影响数据生成的误差。从随机热力学的角度来看，我们的结果为如何在扩散模型中最好地生成数据提供了定量见解。最佳学习方案由随机热力学中的保守力和最优传输理论中 2-Wasserstein 距离的空间测地线引入。我们用数值方法说明了具有不同噪声方案（如余弦方案、条件最优传输和最优传输）的扩散模型的速度-精度权衡的有效性。|
|**2024-07-05**|[PROUD: PaRetO-gUided Diffusion Model for Multi-objective Generation](http://arxiv.org/abs/2407.04493)|null|深度生成模型领域的最新进展集中于生成满足多个期望属性的样本。然而，普遍的方法是独立优化这些属性函数，从而忽略了它们之间的权衡。此外，属性优化通常没有被恰当地整合到生成模型中，导致生成质量（即生成样本的质量）的无必要妥协。为了解决这些问题，我们提出了一个约束优化问题。它寻求在确保生成样本位于多个属性目标的帕累托前沿的同时优化生成质量。这样的公式能够生成在相互冲突的属性函数上无法同时进一步改进的样本，并保持生成样本的良好质量。在此公式的基础上，我们引入了帕累托引导扩散模型 (PROUD)，其中去噪过程中的梯度被动态调整以提高生成质量，同时生成样本遵循帕累托最优性。在图像生成和蛋白质生成任务上的实验评估表明，与各种基线相比，我们的 PROUD 在逼近多个属性函数的帕累托最优性的同时，始终保持着卓越的生成质量。|
|**2024-07-05**|[VCD-Texture: Variance Alignment based 3D-2D Co-Denoising for Text-Guided Texturing](http://arxiv.org/abs/2407.04461)|null|最近关于三维形状纹理合成的研究极大地受益于快速发展的二维文本到图像的扩散模型，包括基于修复和基于优化的方案。然而，这些方法忽略了二维扩散模型和三维物体之间的模态差距，它们主要将三维物体渲染成二维图像并分别对每个图像进行纹理处理。在本文中，我们重新审视了纹理合成，并提出了一个基于方差对齐的三维-二维协同去噪框架，称为VCD-Texture，以解决这些问题。具体来说，我们首先在具有重新投影的三维注意力感受野的扩散自注意力模块中统一了二维和三维潜在特征学习。随后，将去噪后的多视图二维潜在特征聚合到三维空间，然后将其光栅化回二维空间，从而形成更一致的二维预测。然而，光栅化过程存在难以处理的方差偏差，我们提出的方差对齐从理论上解决了这个问题，实现了高保真纹理合成。此外，我们还提出了一种修复细化方法，以进一步改善存在冲突区域的细节。值得注意的是，目前还没有公开可用的基准来评估纹理合成，这阻碍了其发展。因此，我们构建了一个基于三个开源三维数据集的新评估集，并建议使用四个指标来全面验证纹理性能。综合实验表明，VCD-Texture相较于其他方法取得了优越的性能。|
|**2024-07-05**|[Benchmarking structure-based three-dimensional molecular generative models using GenBench3D: ligand conformation quality matters](http://arxiv.org/abs/2407.04424)|null|三维 (3D) 深度分子生成模型提供了基于 3D 依赖属性的目标导向生成的优势，例如结合腔内基于结构的设计的结合亲和力。 为评估 SMILES 或分子图生成器而创建的传统基准，例如 GuacaMol 或 MOSES，由于它们不评估生成的分子构象的质量，因此在评估 3D 生成器方面受到限制。 因此，我们在这项工作中开发了 GenBench3D，它实现了一个用于在结合腔内生成分子的新基准。 我们的主要贡献是 Validity3D 指标，它使用基于剑桥结构数据库中观察到的参考值的键长和价角的可能性来评估构象质量。 对 LiGAN、3D-SBDD、Pocket2Mol、TargetDiff、DiffSBDD 和 ResGen 模型进行了基准测试。 我们发现只有 0% 到 11% 的生成分子具有有效的构象。 对口袋中生成的分子进行局部弛豫，通过至少增加 40% 的 Validity3D，大大提高了所有模型的 Validity3D。 对于 LiGAN、3D-SBDD 或 TargetDiff，有效弛豫分子集显示的平均 Vina 分数（即更差）高于原始生成分子集，表明原始生成分子的结合亲和力可能被高估了。 使用其他评分函数（更重视配体应变）仅在使用有效的弛豫分子时才会产生改进的分数。 使用有效的弛豫分子，TargetDiff 和 Pocket2Mol 显示出比其他模型更好的中位 Vina、Glide 和 Gold PLP 分数。 我们已经在 GitHub 上公开发布了 GenBench3D 以供更广泛地使用：https://github.com/bbaillif/genbench3d|
|**2024-07-05**|[Improving Audio Generation with Visual Enhanced Caption](http://arxiv.org/abs/2407.04416)|null|生成模型在音频生成任务中取得了显著成果，但现有模型难以处理复杂和详细的提示，导致潜在的性能下降。我们假设这个问题源于训练数据的低质量和相对较小的数量。在这项工作中，我们的目标是创建一个带有丰富描述的大规模音频数据集，用于改进音频生成模型。我们开发了一个自动化管道，通过使用大型语言模型 (LLM) 将预测的视觉字幕、音频字幕和标签标签转换为全面的描述，从而为视听数据集生成详细的字幕。我们介绍了 Sound-VECaps，这是一个包含 166 万个高质量音频-字幕对的数据集，其中包含丰富的细节，包括音频事件顺序、发生地点和环境信息。我们证明，使用 Sound-VECaps 进行训练可以显著增强文本到音频生成模型理解和从复杂输入提示生成音频的能力，从而提高整体系统性能。此外，我们对 Sound-VECaps 在多个音频-语言任务中进行了消融研究，表明其在推进音频-文本表示学习方面的潜力。我们的数据集和模型可在网上获得。|
|**2024-07-05**|[Unsupervised Learning of Category-Level 3D Pose from Object-Centric Videos](http://arxiv.org/abs/2407.04384)|**[link](https://github.com/genintel/uns-obj-pose3d)**|类别级三维姿态估计是计算机视觉和机器人技术中的一个基本问题，例如对于具身代理或训练三维生成模型。然而，到目前为止，估计类别级物体姿态的方法要么需要大量的人工标注、CAD模型，要么需要来自RGB-D传感器的输入。相比之下，我们致力于解决仅从随意拍摄的以物体为中心的视频中学习估计类别级三维姿态的问题，无需人工监督。我们提出了一个两步流程：首先，我们引入了一种多视图对齐程序，该程序使用一种新颖且鲁棒的循环距离公式来确定视频之间的规范相机姿态，该公式使用重建的粗网格和DINOv2特征进行几何和外观匹配。其次，规范姿态和重建的网格使我们能够从单个图像中训练用于三维姿态估计的模型。具体来说，我们的模型通过预测二维图像中每个像素的模板网格中对应顶点的特征向量，来学习估计图像和原型三维模板之间的密集对应关系。我们证明，我们的方法在以物体为中心的视频的无监督对齐方面大大优于所有基线，并在实际应用中提供了可靠且鲁棒的预测。我们的代码和数据可在https://github.com/GenIntel/uns-obj-pose3d获取。|
|**2024-07-05**|[A Mapping Strategy for Interacting with Latent Audio Synthesis Using Artistic Materials](http://arxiv.org/abs/2407.04379)|null|本文提出了一种与生成式人工智能模型的潜在空间交互的映射策略。我们的方法涉及使用无监督特征学习对人类控制空间进行编码，并将其映射到音频合成模型的潜在空间。为了演示这种映射策略如何将高维传感器数据转化为深度生成模型的控制机制，我们提出了一个概念验证系统，该系统使用视觉草图来控制音频合成模型。我们借鉴 XAIxArts 中的新兴论述来讨论这种方法如何为艺术和创意环境中的 XAI 做出贡献，我们还讨论了它目前的局限性并提出了未来的研究方向。|
|**2024-07-05**|[MuseBarControl: Enhancing Fine-Grained Control in Symbolic Music Generation through Pre-Training and Counterfactual Loss](http://arxiv.org/abs/2407.04331)|null|自动生成符号音乐——根据特定人类需求量身定制的乐谱——对音乐家和爱好者来说非常有利。最近的研究表明，使用大型数据集和先进的Transformer架构取得了可喜的成果。然而，这些最先进的模型通常只对整首作品的节奏和风格等方面提供基本的控制，缺乏管理更精细细节的能力，例如在单个小节级别的控制。虽然微调预先训练好的符号音乐生成模型似乎是实现这种更精细控制的直接方法，但我们的研究表明这种方法存在挑战。该模型通常无法对新的、细粒度的小节级控制信号做出充分响应。为了解决这个问题，我们提出了两个创新的解决方案。首先，我们引入了一个预训练任务，旨在将控制信号直接与其相应的音乐符号链接起来，这有助于为后续的微调实现更有效的初始化。其次，我们实施了一种新的反事实损失，以促进生成的音乐与控制提示之间更好地保持一致。总的来说，这些技术显著增强了我们在小节级别控制音乐生成的能力，比传统方法提高了13.06%。我们的主观评价也证实，这种增强的控制并没有损害原始预训练生成模型的音乐质量。|
|**2024-07-03**|[DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents](http://arxiv.org/abs/2407.03300)|null|扩散模型 (DM) 为生成式学习带来了革命性的变化。它们利用扩散过程将数据编码为简单的 Gaussian 分布。然而，将复杂且可能具有多模态的数据分布编码为单个连续 Gaussian 分布无疑是一个不必要的具有挑战性的学习问题。我们提出离散-连续潜在变量扩散模型 (DisCo-Diff)，通过引入互补的离散潜在变量来简化此任务。我们使用可学习的离散潜在变量增强 DM，并使用编码器进行推断，并对 DM 和编码器进行端到端训练。DisCo-Diff 不依赖于预先训练的网络，这使得该框架具有普遍适用性。离散潜在变量通过降低 DM 生成 ODE 的曲率，显著简化了学习 DM 复杂噪声到数据映射的过程。一个额外的自回归 Transformer 模型对离散潜在变量的分布进行建模，这是一个简单的步骤，因为 DisCo-Diff 只需要具有少量码本的少量离散变量。我们在玩具数据、多个图像合成任务以及分子对接上验证了 DisCo-Diff，发现引入离散潜在变量始终可以提高模型性能。例如，DisCo-Diff 在使用 ODE 采样器的类条件 ImageNet-64/128 数据集上实现了最先进的 FID 分数。|
|**2024-07-03**|[Improved Noise Schedule for Diffusion Training](http://arxiv.org/abs/2407.03297)|null|扩散模型已成为生成视觉信号的首选方法。然而，训练单个模型来预测不同级别的噪声提出了重大挑战，需要多次迭代并导致巨大的计算成本。为了加快收敛速度，人们引入了各种方法，例如损失加权策略设计和架构改进。在本研究中，我们提出了一种设计噪声调度的新方法，以增强扩散模型的训练。我们的主要见解是，对数信噪比（logSNR）的重要性采样（理论上等效于修改后的噪声调度）对于提高训练效率特别有利，特别是在增加 $\log \text{SNR}=0$ 附近的采样频率时。我们通过经验证明了我们的噪声调度优于标准余弦调度。此外，我们还重点介绍了我们的噪声调度设计在 ImageNet 基准测试中的优势，表明所设计的调度始终有利于不同的预测目标。|
|**2024-07-03**|[Spatio-Temporal Adaptive Diffusion Models for EEG Super-Resolution in Epilepsy Diagnosis](http://arxiv.org/abs/2407.03089)|null|脑电图 (EEG) 技术，特别是高密度脑电图 (HD EEG) 设备，广泛应用于神经科学等领域。HD EEG 设备通过在头皮上放置更多电极来提高 EEG 的空间分辨率，满足癫痫病灶定位等临床诊断应用的要求。然而，该技术面临着采集成本高、使用场景有限等挑战。本文提出了时空自适应扩散模型 (STADM)，率先利用扩散模型实现从低分辨率 (LR, 64 通道或更少) EEG 到高分辨率 (HR, 256 通道) EEG 的空间超分辨率 (SR) 重建。具体而言，设计了一种时空条件模块来提取 LR EEG 的时空特征，然后将其作为条件输入来指导扩散模型的反向去噪过程。此外，构建了一个多尺度 Transformer 去噪模块，利用多尺度卷积块和基于交叉注意力的扩散 Transformer 块进行条件引导，生成自适应于受试者的 SR EEG。实验结果表明，该方法有效提高了 LR EEG 的空间分辨率，并在数量上优于现有方法。此外，STADM 通过将合成的 SR EEG 应用于癫痫患者的分类和源定位任务，证明了其价值，表明其具有显著提高 LR EEG 空间分辨率的潜力。|
|**2024-07-03**|[Artificial Inductive Bias for Synthetic Tabular Data Generation in Data-Scarce Scenarios](http://arxiv.org/abs/2407.03080)|null|虽然使用深度生成模型 (DGM) 生成合成表格数据为数据稀缺和隐私问题提供了一种引人注目的解决方案，但其有效性依赖于大量的训练数据，而这些数据在现实应用中通常不可用。本文提出了一种新颖的方法，用于在有限的真实数据环境中使用 DGM 生成真实可靠的合成表格数据，从而解决了这一挑战。我们的方法提出了几种通过迁移学习和元学习技术在 DGM 中生成人工归纳偏差的方法。我们在该框架内探索并比较了四种不同的方法，证明了预训练和模型平均等迁移学习策略优于模型无关元学习和域随机搜索等元学习方法。我们使用两种最先进的 DGM（变分自动编码器和生成对抗网络）验证了我们的方法，表明我们的人工归纳偏差提高了合成数据的质量（通过 Jensen-Shannon 散度衡量），在使用我们提出的方法时，相对增益高达 50%。这种方法在各种 DGM 和机器学习任务中具有广泛的适用性，特别是在医疗保健和金融等数据稀缺通常是关键问题的领域。|
|**2024-07-03**|[Electromagnetic Property Sensing Based on Diffusion Model in ISAC System](http://arxiv.org/abs/2407.03075)|null|集成传感与通信 (ISAC) 为未来的无线系统开辟了许多颠覆性的机遇。在本文中，我们开发了一种新颖的 ISAC 方案，利用扩散模型来感知预定传感区域中目标的电磁 (EM) 特性。具体来说，我们首先利用从目标反射回来的通信和传感信号来估计传感信道。然后，我们采用扩散模型生成代表目标的点云，从而实现目标电磁特性分布的 3D 可视化。为了最小化真实点云和估计点云之间的平均 Chamfer 距离 (MCD)，我们在最大发射功率和每个用户设备 (UE) 的最小通信可达速率的约束下，进一步设计了通信和传感波束赋形矩阵。仿真结果证明了该方法在实现目标形状、相对介电常数和电导率的高质量重建方面的有效性。此外，该方法可以在传感区域的任何位置有效地感知目标的电磁特性。|
|**2024-07-03**|[Semantic-Aware Power Allocation for Generative Semantic Communications with Foundation Models](http://arxiv.org/abs/2407.03050)|null|扩散模型的最新进展为生成建模带来了重大突破。生成模型与语义通信 (SemCom) 的结合能够以超低速率实现高保真语义信息交换。本文提出了一种用于图像任务的新型生成式 SemCom 框架，其中预训练的基础模型分别充当语义编码器和解码器，用于语义特征提取和图像再生。文章对传输可靠性与再生图像的感知质量以及语义特征的语义值之间的数学关系进行了建模，这些关系是通过对 Kodak 数据集进行数值模拟获得的。我们还研究了语义感知功率分配问题，目标是在保证语义性能的同时最小化总功耗。为了解决这个问题，分别通过约束解耦和二分搜索提出了两种语义感知功率分配方法。数值结果表明，与传统方法相比，所提出的语义感知方法在总功耗方面表现出优越的性能。|
|**2024-07-03**|[SlerpFace: Face Template Protection via Spherical Linear Interpolation](http://arxiv.org/abs/2407.03043)|null|当代人脸识别系统使用从人脸图像中提取的特征模板来识别身份。为了增强隐私性，人脸模板保护技术被广泛用于隐藏存储在模板中的敏感身份和外观信息。本文识别了一种新兴的利用扩散模型的隐私攻击形式，它可以使先前的保护无效，称为反演攻击。这种攻击可以从模板中合成高质量、保留身份的人脸图像，从而暴露人的外貌。基于对扩散模型生成能力的研究，本文提出了一种防御措施来削弱这种攻击，即通过将模板旋转到类似噪声的分布。这是通过在其所在的超球面上对模板进行球面和线性插值（slerp）来有效实现的。为了增强旋转模板的不可逆性，本文进一步提出对模板的特征维度进行分组划分和丢弃。组的划分和每个组内的丢弃以有利于识别的的方式学习。所提出的技术被具体化为一种新的人脸模板保护技术，SlerpFace。大量实验表明，SlerpFace 提供了令人满意的识别精度和全面的隐私保护，可以抵御反演和其他攻击形式，优于现有技术。|
|**2024-07-03**|[An Organism Starts with a Single Pix-Cell: A Neural Cellular Diffusion for High-Resolution Image Synthesis](http://arxiv.org/abs/2407.03018)|null|生成模型旨在逼近真实数据的统计特性，从而能够合成与原始分布非常相似的新数据。生成对抗网络 (GAN) 和去噪扩散概率模型 (DDPM) 代表了生成模型的重大进步，它们分别从博弈论和热力学中汲取灵感。然而，从生物进化的角度探索生成模型在很大程度上仍未得到开发。在本文中，我们介绍了一种称为生成元胞自动机 (GeCA) 的新型模型系列，其灵感来自于生物体从单细胞的进化。GeCA 被评估为一种有效的视网膜疾病分类增强工具，可用于两种成像模式：眼底和光学相干断层扫描 (OCT)。在 OCT 成像中，数据稀缺且类别分布存在固有的偏差，GeCA 显着提高了 11 种不同眼科疾病的表现，与传统基线相比，平均 F1 分数提高了 12%。在类似的参数约束下，GeCA 的性能优于包含 UNet 或基于 Transformer 的最新去噪模型的扩散方法。代码可在以下网址获取：https://github.com/xmed-lab/GeCA。|
|**2024-07-03**|[Frequency-Controlled Diffusion Model for Versatile Text-Guided Image-to-Image Translation](http://arxiv.org/abs/2407.03006)|**[link](https://github.com/xianggao1102/fcdiffusion)**|近年来，大规模文本到图像 (T2I) 扩散模型已成为一种强大的图像到图像转换 (I2I) 工具，允许通过用户提供的文本提示进行开放域图像转换。本文提出了频率控制扩散模型 (FCDiffusion)，这是一个基于扩散的端到端框架，从频域角度为文本引导的 I2I 提供了一种新颖的解决方案。我们框架的核心是一个基于离散余弦变换的特征空间频域滤波模块，它在 DCT 域中滤波源图像的潜在特征，产生具有不同 DCT 谱带的滤波图像特征，作为预训练的潜在扩散模型的不同控制信号。我们发现，不同 DCT 谱带的控制信号在不同的相关性（例如，风格、结构、布局、轮廓等）上桥接了源图像和 T2I 生成的图像，从而使多功能 I2I 应用能够强调不同的 I2I 相关性，包括风格引导的内容创建、图像语义操作、图像场景转换和图像风格转换。与相关方法不同，FCDiffusion 建立了一个统一的文本引导 I2I 框架，只需在推理时切换不同的频率控制分支，即可适用于各种图像转换任务。广泛的定性和定量实验都证明了我们的方法在文本引导 I2I 方面的有效性和优越性。代码公开于：https://github.com/XiangGao1102/FCDiffusion。|
|**2024-07-03**|[Towards a Scalable Reference-Free Evaluation of Generative Models](http://arxiv.org/abs/2407.02961)|**[link](https://github.com/aziksh-ospanov/fkea)**|虽然生成模型的标准评估分数大多是基于参考的，但由于缺乏适用的参考数据集，对生成模型进行依赖参考的评估通常很困难。最近，人们提出了无参考的熵分数 VENDI 和 RKE 来评估生成数据的多样性。然而，从数据中估计这些分数会导致大规模生成模型的计算成本很高。在这项工作中，我们利用随机傅里叶特征框架来降低计算成本，并提出了基于傅里叶的核熵逼近 (FKEA) 方法。我们利用 FKEA 对核矩阵的近似特征谱来有效地估计上述熵分数。此外，我们展示了 FKEA 代理特征向量的应用，以揭示该方法在评估生成样本多样性时识别的模式。我们提供了 FKEA 评估算法的随机实现，其复杂度为 $O(n)$，随样本大小 $n$ 线性增长。我们广泛评估了 FKEA 在标准图像、文本和视频数据集中的数值性能。我们的实验结果表明，该方法应用于大规模生成模型具有可扩展性和可解释性。代码库可在 https://github.com/aziksh-ospanov/FKEA 获取。|

<p align=right>(<a href=#updated-on-20240708>back to top</a>)</p>

## LLM

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-07-03**|[MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language Models](http://arxiv.org/abs/2407.02775)|null|知识蒸馏是一种有效的预训练语言模型压缩技术。虽然现有的知识蒸馏方法对于最典型的模型BERT表现良好，但它们在两个方面仍有提升空间：可以进一步探索关系级知识以提高模型性能；学生注意力头数的设置可以更加灵活，以减少推理时间。因此，我们提出了一种新的知识蒸馏方法MLKD-BERT，用于在师生框架中提取多级知识。在GLUE基准测试和提取式问答任务上的大量实验表明，我们的方法优于BERT上最先进的知识蒸馏方法。此外，MLKD-BERT可以灵活设置学生注意力头数，从而在性能下降很小的情况下大幅减少推理时间。|
|**2024-07-03**|[Supporting Cross-language Cross-project Bug Localization Using Pre-trained Language Models](http://arxiv.org/abs/2407.02732)|null|自动定位大型代码库中的错误对于开发人员来说仍然是一项重大挑战。现有技术由于依赖于特定于应用程序的数据和大型模型，因此常常难以推广和部署。本文提出了一种新的基于预训练语言模型（PLM）的错误定位技术，该技术超越了项目和语言的界限。我们的方法利用对比学习来增强错误报告和源代码的表示。然后，它利用一种结合提交消息和代码段的新型排名方法。此外，我们引入了一种知识蒸馏技术，可以在不影响性能的情况下减小模型大小，以便实际部署。本文提出了几个主要优点。通过将代码段和提交消息分析与传统的代码文件级别检查相结合，我们的技术实现了更高的错误定位精度。此外，我们的模型在泛化性方面表现出色——在来自各种项目和语言的代码上进行训练后，它可以有效地识别未见过的代码库中的错误。为了解决计算限制，我们提出了一种兼容CPU的解决方案。总而言之，提出的工作提出了一种高效、通用且有效的错误定位技术，具有实际部署的潜力。|
|**2024-07-02**|[Ensemble of pre-trained language models and data augmentation for hate speech detection from Arabic tweets](http://arxiv.org/abs/2407.02448)|null|如今，从阿拉伯语推文中识别仇恨言论引起了众多研究者的关注。为了解决这一分类任务，人们开发了许多系统和技术。然而，在这种情况下，面临的两大挑战是性能有限和数据不平衡问题。在本研究中，我们提出了一种新方法，利用集成学习和基于先前手动标记数据的半监督学习。我们通过将阿拉伯语推文分为 5 个不同的类别：非仇恨、一般仇恨、种族仇恨、宗教仇恨或性别歧视，在一个基准数据集上进行了实验。实验结果表明：(1) 基于预训练语言模型的集成学习优于现有的相关工作；(2) 我们提出的数据增强方法提高了阿拉伯语推文中仇恨言论检测的准确率，并优于现有的相关工作。我们的主要贡献是在阿拉伯语仇恨言论检测方面取得了令人鼓舞的结果。|
|**2024-07-02**|[Efficient Nearest Neighbor based Uncertainty Estimation for Natural Language Processing Tasks](http://arxiv.org/abs/2407.02138)|null|深度神经网络 (DNN) 中的可信预测，包括预训练语言模型 (PLM)，对于现实世界中安全关键型应用至关重要。然而，DNN 通常会遇到不确定性估计问题，例如校准错误。特别是，需要多次随机推理的方法可以缓解这个问题，但推理成本高昂，使其不切实际。在本研究中，我们提出了 $k$最近邻不确定性估计（$k$NN-UE），这是一种利用来自邻居的距离和邻居的标签存在率进行不确定性估计的方法。在情感分析、自然语言推理和命名实体识别方面的实验表明，我们提出的方法在置信度校准、选择性预测和分布外检测方面优于基线或最近基于密度的方法。此外，我们的分析表明，引入降维或受最近$k$ NN-LM 研究启发的近似最近邻搜索可以在不显着降低估计性能的情况下降低推理开销，前提是将它们适当地结合起来。|
|**2024-07-01**|[Bridging the Gap: Transfer Learning from English PLMs to Malaysian English](http://arxiv.org/abs/2407.01374)|null|马来西亚英语是一种低资源的混合语，除了标准英语之外，它还融合了马来语、汉语和泰米尔语的元素。由于其独特的形态句法适应性、语义特征和语码转换（混合英语和马来语），命名实体识别 (NER) 模型在从马来西亚英语文本中捕获实体时表现不佳。考虑到这些差距，我们引入了 MENmBERT 和 MENBERT，这是一种具有上下文理解能力的预训练语言模型，专为马来西亚英语量身定制。我们使用来自马来西亚英语新闻文章 (MEN) 数据集的手动注释实体和关系对 MENmBERT 和 MENBERT 进行了微调。这种微调过程使 PLM 能够学习表示，从而捕捉与 NER 和 RE 任务相关的马来西亚英语的细微差别。与 bert-base-multilingual-cased 模型相比，MENmBERT 在 NER 和 RE 任务上分别实现了 1.52% 和 26.27% 的改进。尽管 NER 的整体性能没有显着提高，但我们进一步的分析表明，按 12 个实体标签进行评估时，性能有显着提高。这些发现表明，在特定语言和地理位置的语料库上预训练语言模型可能是提高低资源环境中 NER 性能的一种很有前景的方法。本文发布的数据集和代码为关注马来西亚英语的 NLP 研究工作提供了宝贵的资源。|
|**2024-07-01**|[Language Portability Strategies for Open-domain Dialogue with Pre-trained Language Models from High to Low Resource Languages](http://arxiv.org/abs/2407.01315)|null|本文旨在研究用于开放域对话系统的大型预训练语言模型 (PLM) 在高资源语言中的语言可移植性策略。具体而言，目标低资源语言 (L_T) 将使用法语进行模拟，因为它缺乏特定于任务的资源，并且允许我们进行人工评估，而源语言 (L_S) 为英语。出于显而易见的原因，最近使用此类模型进行开放域对话的工作大多是用英语开发的。然而，为每个可能的目标语言构建特定的 PLM 需要收集新的数据集，成本高昂。出于这个原因，我们希望尝试利用 L_S 和 L_T 中的所有现有资源（PLM 和数据），以评估使用不同方法在 L_T 中可实现的性能。前两种方法评估了神经机器翻译 (NMT) 在不同级别的使用：TrainOnTarget，其中在 L_T 中进行微调之前先翻译 L_S 数据集，以及 TestOnSource，其中在推理过程中将 L_S 模型与 NMT 模块耦合。然后，全球第一个开放获取的多语言大型 PLM BLOOM [2] 的出现，使研究人员能够开发新的方法，旨在不仅利用模型的完全可访问性，还利用其多语言性和翻译能力。在这种情况下，任务首先在 L_S 中学习，然后使用 MAD-X 适配器架构 [16] 适应 L_T。在这两组实验中，模型是在与人类进行口语对话的条件下进行评估的，并且可以根据感知的交互质量来比较这些策略。|
|**2024-07-01**|[A Fingerprint for Large Language Models](http://arxiv.org/abs/2407.01235)|null|近期研究表明，扩展预训练语言模型可以在许多下游任务上实现最先进的性能，这使得大型语言模型（LLM）成为人工智能领域的热门研究课题。然而，由于从头开始训练LLM需要大量的资源，因此保护LLM的知识产权免遭侵权至关重要且紧迫。这促使本文作者提出了一种新颖的LLM黑盒指纹识别技术，该技术既不需要模型训练也不需要模型微调。我们首先证明LLM的输出跨越与每个模型相关的唯一向量空间。我们将所有权认证问题建模为评估受害模型空间与嫌疑模型输出空间之间相似性的任务。为了解决这个问题，我们提出了两种解决方案，其中第一个解决方案涉及验证可疑大型模型的输出是否与受害模型的输出位于相同的空间中，从而能够快速识别模型侵权；第二个解决方案重建LLM输出和受害模型的向量空间的并集，以解决受害模型遭受参数高效微调（PEFT）攻击的情况。实验结果表明，所提出的技术在所有权验证和抵御PEFT攻击方面取得了优异的性能。这项工作揭示了LLM的固有特性，并为黑盒场景下的LLM所有权验证提供了一种有前景的解决方案，确保了效率、通用性和实用性。|
|**2024-07-01**|[Development of Cognitive Intelligence in Pre-trained Language Models](http://arxiv.org/abs/2407.01047)|null|最近的研究表明，大型预训练语言模型 (PLM) 出现了认知能力。这些模型不断提高的认知一致性使其成为认知科学理论的候选者。先前对 PLM 涌现认知能力的研究很大程度上与模型训练路径无关，即侧重于最终的模型权重而不是中间步骤。然而，使用 PLM 构建合理的人类认知模型将受益于考虑其在训练期间的表现与儿童思维轨迹的发展一致性。在人类智力心理测量测试的指导下，我们选择了四组任务来研究十个流行的 PLM 家族的一致性，并评估它们可用的中间和最终训练步骤。这些任务是数字能力、语言能力、概念理解和流体推理。我们发现了一个惊人的规律：无论模型大小如何，PLM 的发展轨迹始终表现出一个与人类认知发展最大程度一致的窗口。在该窗口之前，训练似乎赋予“空白石板”模型以必要的结构，使其能够从经验中快速学习。在该窗口之后，训练似乎服务于降低损失的工程目标，而不是提高与人类认知一致性的科学目标。|
|**2024-07-01**|[Cross-Modal Attention Alignment Network with Auxiliary Text Description for zero-shot sketch-based image retrieval](http://arxiv.org/abs/2407.00979)|null|本文研究了基于零样本草图的图像检索问题 (ZS-SBIR)。先前的方法在只有类别标签甚至没有文本信息的双模态设置中解决该问题。然而，大规模预训练语言模型 (LLM) 的日益普及，展现出从网络规模数据中学习到的丰富知识，为我们提供了一个总结集体文本信息的机会。我们的主要创新在于使用文本数据作为图像的辅助信息，从而利用语言提供的固有的零样本泛化能力。为此，我们提出了一种名为“基于辅助文本描述的跨模态注意力对齐网络”的方法，用于零样本草图图像检索。该网络由三个部分组成：(i) 描述生成模块，通过使用几个疑问句提示LLM，为每个训练类别生成文本描述；(ii) 特征提取模块，包括用于草图和图像数据的两个ViT、用于提取每个训练类别句子标记的转换器；最后 (iii) 跨模态对齐模块，使用交叉注意力机制交换文本-草图和文本-图像的标记特征，并在局部和全局范围内对齐标记。在三个基准数据集上的大量实验表明，我们的方法优于最先进的 ZS-SBIR 方法。|
|**2024-06-30**|[NAIST Simultaneous Speech Translation System for IWSLT 2024](http://arxiv.org/abs/2407.00826)|null|本文描述了NAIST提交给IWSLT 2024评测活动同步赛道的系统：英语到{德语、日语、汉语}的语音到文本翻译和英语到日语的语音到语音翻译。我们开发了一种多语言端到端语音到文本翻译模型，该模型结合了两个预训练语言模型HuBERT和mBART。我们使用两种解码策略训练该模型：局部一致性（LA）和AlignAtt。提交的模型采用LA策略，因为它在之前的模型中优于AlignAtt策略。我们的语音到语音翻译方法是上述语音到文本模型与增量文本到语音（TTS）模块的级联，该模块包含音素估计模型、并行声学模型和并行WaveGAN声码器。我们通过将采用AlignAtt策略的Transformer架构应用于估计模型来改进增量TTS。结果表明，我们升级后的TTS模块有助于提高系统性能。|

<p align=right>(<a href=#updated-on-20240708>back to top</a>)</p>

## Transformer

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-07-05**|[Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations](http://arxiv.org/abs/2407.04543)|null|模型需要适当的归纳偏置才能有效地从少量数据中学习并在训练分布之外进行系统泛化。 虽然 Transformer 具有高度的通用性和强大的功能，但它们仍然可以从增强的结构性归纳偏置中受益，以完成 seq2seq 任务，尤其是那些涉及句法转换的任务，例如将主动语态转换为被动语态或语义解析。在本文中，我们建议通过中间预训练来增强 Transformer 的结构性归纳偏置，以根据转换的描述对依存树执行合成生成的句法转换。我们的实验表明，这有助于对组块等句法任务进行少样本学习，并且还提高了语义解析的结构泛化能力。我们的分析表明，中间预训练会导致注意力头能够跟踪需要对哪些标记应用哪些句法转换，并且模型可以在下游任务中利用这些注意力头。|
|**2024-07-05**|[LayerShuffle: Enhancing Robustness in Vision Transformers by Randomizing Layer Execution Order](http://arxiv.org/abs/2407.04513)|null|由于人工神经网络的架构和训练方式，它们通常对测试时的剪枝、替换或打乱层顺序的操作缺乏鲁棒性。然而，这些特性对于不同的应用场景非常重要，例如在分布式神经网络架构中，执行顺序无法得到保证，或者网络的某些部分在推理过程中可能发生故障。为了解决这些问题，我们提出了一系列针对视觉Transformer的训练方法，其最重要的组成部分是在训练时随机化注意力模块的执行顺序。我们证明，使用我们提出的方法，假设可以容忍在相同模型规模下精度降低（约20%），视觉Transformer确实能够适应测试时任意层的执行顺序。我们还发现，我们训练的模型可以彼此随机合并，生成功能完备的“科学怪人”模型，并且与源模型相比，性能没有损失。最后，我们在测试时对模型进行层剪枝，发现其性能下降平缓。|
|**2024-07-05**|[Hard-Attention Gates with Gradient Routing for Endoscopic Image Computing](http://arxiv.org/abs/2407.04400)|null|为了解决胃肠道息肉大小评估中的过拟合问题并增强模型泛化能力，我们的研究引入了特征选择门（FSG）或硬注意力门（HAG）以及梯度路由（GR）来进行动态特征选择。该技术旨在通过促进稀疏连接来增强卷积神经网络（CNN）和视觉Transformer（ViT），从而减少过拟合并增强泛化能力。HAG 通过使用可学习权重的稀疏化来实现这一点，作为一种正则化策略。GR 通过独立于主模型的两次前向传递优化 HAG 参数，进一步完善了这一过程，以改进特征重新加权。我们的评估涵盖了多个数据集，包括用于广泛影响评估的 CIFAR-100 和专注于息肉大小估计的专业内窥镜数据集（REAL-Colon、Misawa 和 SUN），涵盖超过 370,000 帧图像中的 200 多个息肉。研究结果表明，我们增强的 HAG 网络大大提高了与息肉大小相关的二分类和三分类任务的性能。具体而言，CNN 在二分类中的 F1 分数提高到 87.8%，而在三分类中，ViT-T 模型的 F1 分数达到 76.5%，优于传统的 CNN 和 ViT-T 模型。为了促进进一步的研究，我们发布了代码库，其中包括 CNN、多流 CNN、ViT 和 HAG 增强变体的实现。该资源旨在标准化内窥镜数据集的使用，为胃肠道息肉大小估计提供公开的训练-验证-测试拆分，以进行可靠和可比较的研究。代码库可在 github.com/cosmoimd/feature-selection-gates 获取。|
|**2024-07-05**|[Batch Transformer: Look for Attention in Batch](http://arxiv.org/abs/2407.04218)|null|人脸表情识别 (FER) 在计算机视觉领域受到了广泛关注，尤其是在人机交互等“自然环境”中。然而，FER 图像存在遮挡、低分辨率、姿态变化、光照变化和主观性等不确定因素，其中包括一些与目标标签不匹配的表情。因此，从一张包含噪声的单个图像中获取的信息很少，而且不可信。这可能会显著降低 FER 任务的性能。为了解决这个问题，我们提出了一种批量转换器 (BT)，它包含所提出的类别批量注意力 (CBA) 模块，通过训练一批图像中反映的特征，而不是来自单个图像的信息，来防止噪声数据过拟合并提取可靠信息。我们还提出了多级注意力 (MLA) 机制，通过捕获每个级别之间的相关性来防止过度拟合特定特征。在本文中，我们提出了一个结合上述方案的批量转换器网络 (BTN)。在各种 FER 基准数据集上的实验结果表明，所提出的 BTN 在 FER 数据集中始终优于最先进的方法。代表性结果证明了所提出的 BTN 在 FER 中的前景。|
|**2024-07-04**|[Adaptive Step-size Perception Unfolding Network with Non-local Hybrid Attention for Hyperspectral Image Reconstruction](http://arxiv.org/abs/2407.04024)|null|深度展开方法和Transformer架构最近在高光谱图像（HSI）重建方面展现出良好的结果。然而，仍然存在两个问题：（1）在数据子问题中，大多数方法使用可学习参数表示步长。然而，对于不同的光谱通道，特征和真实值之间的误差是不相等的。(2) Transformer难以平衡感受野大小和像素级细节信息。为了克服上述缺点，我们提出了一种自适应步长感知展开网络（ASPUN），这是一种基于FISTA算法的深度展开网络，它使用自适应步长感知模块来估计每个光谱通道的更新步长。此外，我们设计了一个非局部混合注意力Transformer（NHAT）模块，用于充分利用Transformer的感受野优势。通过将NLHA插入非局部信息聚合（NLIA）模块，展开网络可以获得更好的重建结果。实验结果表明，我们的ASPUN优于现有的SOTA算法，并取得了最佳性能。|
|**2024-07-03**|[Towards Attention-based Contrastive Learning for Audio Spoof Detection](http://arxiv.org/abs/2407.03514)|null|视觉Transformer（ViT）在计算机视觉分类任务中取得了重大进展。最近，Gong等人（2021）将基于注意力的建模方法引入了几项音频任务。然而，使用ViT进行音频欺骗检测任务的研究相对较少。我们弥合了这一差距，并将ViT引入到这项任务中。基于对SSAST（Gong等人，2022）音频ViT模型进行微调的朴素基线模型实现了次优的等错误率（EER）。为了提高性能，我们提出了一种新颖的基于注意力的对比学习框架（SSAST-CL），该框架使用交叉注意力来辅助表示学习。实验表明，我们的框架成功地 disentangled 了真实和欺骗类别，并有助于学习更好的分类器来完成这项任务。通过适当的数据增强策略，在我们的框架上训练的模型在ASVSpoof 2021挑战赛中取得了具有竞争力的性能。我们提供了比较和消融研究来证明我们的观点。|
|**2024-07-03**|[STF: Sentence Transformer Fine-Tuning For Topic Categorization With Limited Data](http://arxiv.org/abs/2407.03253)|null|如今，从推文中进行主题分类引起了相当多的研究关注。由于这些研究工作，人们提出了不同的分类系统。然而，由于标记数据的数量有限，导致性能指标低下，它们面临着重大挑战。我们提出了句子转换器微调 (STF)，这是一个主题检测系统，它利用预训练的句子转换器模型和微调来准确地对推文主题进行分类。此外，我们还进行了广泛的参数敏感性分析，以针对我们的主题分类任务微调 STF 参数，从而获得最佳性能结果。在两个基准数据集上的实验表明：(1) 所提出的 STF 可以有效地用于对推文主题进行分类，并且优于最新的最先进方法，(2) 所提出的 STF 不需要大量的标记推文就能达到良好的准确性，而这是许多最先进方法的局限性。我们的主要贡献是通过应用预训练的句子转换器语言模型在推文主题分类方面取得了可喜的成果。|
|**2024-07-03**|[Visual Grounding with Attention-Driven Constraint Balancing](http://arxiv.org/abs/2407.03243)|null|与目标检测不同，视觉定位任务需要检测由复杂的自由形式语言描述的对象。为了同时对这种复杂的语义和视觉表示进行建模，最近最先进的研究采用基于Transformer的模型来融合来自两种模态的特征，并进一步引入了各种模块来调制视觉特征，使其与语言表达对齐并消除不相关的冗余信息。然而，它们的损失函数仍然采用常见的目标检测损失，仅仅控制边界框回归输出，未能完全优化上述目标。为了解决这个问题，本文首先分析了基于Transformer模型的注意力机制。在此基础上，我们进一步提出了一个名为注意力驱动约束平衡（AttBalance）的新框架，以优化语言相关区域内视觉特征的行为。大量的实验结果表明，我们的方法带来了令人印象深刻的改进。具体来说，我们在四个不同基准上评估的五种不同模型上都取得了持续的改进。此外，通过将我们的方法集成到QRNet中，我们实现了新的最先进的性能。|
|**2024-07-03**|[Learning Disentangled Representation in Object-Centric Models for Visual Dynamics Prediction via Transformers](http://arxiv.org/abs/2407.03216)|null|最近的研究表明，以对象为中心的表示可以极大地提高学习动力学的准确性，同时也增强了可解释性。在这项工作中，我们将这一想法更进一步，提出了以下问题：“学习解耦表示能否进一步提高以对象为中心的模型中视觉动力学预测的准确性？” 虽然之前已经有一些尝试学习静态图像的解耦表示\citep{nsb}，但据我们所知，我们的工作是第一个尝试在视频的通用环境中做到这一点的工作，而没有对对象可能具有的属性类型做出任何特定假设。我们架构的关键构建块是“块”的概念，其中多个块共同构成一个对象。每个块都表示为给定数量的可学习概念向量的线性组合，并在学习过程中迭代优化。我们模型中的块是以无监督的方式发现的，通过类似于发现槽\citep{slot_attention}的方式关注对象掩码，以学习密集的以对象为中心的表示。我们在发现的块上采用 Transformer 进行自注意力机制来预测下一个状态，从而发现视觉动力学。我们在几个二维和三维基准数据集上进行了一系列实验，证明我们的架构 (1) 可以发现语义上有意义的块 (2) 与最先进的以对象为中心的模型相比，有助于提高动力学预测的准确性 (3) 在训练期间未见过特定属性组合的 OOD 设置中表现明显更好。我们的实验强调了发现解耦表示对于视觉动力学预测的重要性。|
|**2024-07-03**|[Relating CNN-Transformer Fusion Network for Change Detection](http://arxiv.org/abs/2407.03178)|**[link](https://github.com/nust-machine-intelligence-laboratory/rctnet)**|虽然深度学习，特别是卷积神经网络（CNN），已经彻底改变了遥感（RS）变化检测（CD），但现有方法由于忽略了全局上下文和不完整的变化学习，经常遗漏关键特征。此外，transformer 网络难以处理低级细节。RCTNet 通过引入以下内容解决了这些限制：\textbf{(1)} 早期融合骨干网络，用于尽早利用空间和时间特征；\textbf{(2)} 跨阶段聚合（CSA）模块，用于增强时间表示；\textbf{(3)} 多尺度特征融合（MSF）模块，用于在解码器中丰富特征提取；\textbf{(4)} 高效自解密注意力（ESA）模块，利用 Transformer 捕获全局信息和细粒度细节，以实现准确的变化检测。大量实验表明，RCTNet 明显优于传统的遥感图像变化检测方法，显示出显著的改进，并在准确性和计算成本之间取得了最佳平衡。|
|**2024-07-03**|[ISWSST: Index-space-wave State Superposition Transformers for Multispectral Remotely Sensed Imagery Semantic Segmentation](http://arxiv.org/abs/2407.03033)|null|目前，多光谱遥感图像(MSRSI) 语义分割任务面临以下问题：1) 通常只考虑单域特征（即空间域或频率域）；2) 编码器中的下采样操作通常会导致边缘提取精度损失；3) 没有充分考虑 MSRSI 的多通道特征；4) 没有充分利用遥感的先验知识。为了解决上述问题，受量子力学的启发，首次提出了一种用于 MSRSI 语义分割的指标-空间-波态叠加Transformer (ISWSST)，其优势如下：1) 通过自适应投票决策（即集成学习思想）叠加或融合指标、空间和波态来模拟量子叠加，从而成为更强大的分类器并提高分割精度；2) 设计了一种无损小波金字塔编码器-解码器模块，基于小波变换和逆小波变换对图像进行无损重建，模拟量子纠缠，避免边缘提取损失；3) 提出结合多光谱特征（即遥感指数和通道注意力机制）从原始分辨率图像中准确提取地面物体；4) 引入量子力学来解释 ISWSST 的潜在优势。实验表明，ISWSST 在 MSRSI 分割任务中得到了验证，并且优于最先进的架构，有效地提高了分割和边缘提取的精度。代码将在我们的论文被接受后公开。|
|**2024-07-03**|[Graph and Skipped Transformer: Exploiting Spatial and Temporal Modeling Capacities for Efficient 3D Human Pose Estimation](http://arxiv.org/abs/2407.02990)|null|近年来，单目三维人体姿态估计 (HPE) 中的 2D 到 3D 姿态提升引起了广泛的研究兴趣。基于 GNN 的方法和基于 Transformer 的方法由于其先进的空间和时间特征学习能力，已成为主流架构。然而，现有方法通常在空间和时间域中构建关节和帧注意力对齐，导致密集连接，从而引入相当大的局部冗余和计算开销。在本文中，我们采用全局方法来利用时空信息，并通过简洁的图和跳跃 Transformer 架构实现高效的 3D HPE。具体来说，在空间编码阶段，部署粗粒度身体部位以构建具有完全数据驱动自适应拓扑的空间图网络，确保模型在各种姿态下的灵活性和泛化能力。在时间编码和解码阶段，提出了一种简单而有效的跳跃 Transformer 来捕获长期时间依赖性并实现分层特征聚合。还开发了一种直接的数据滚动策略，将动态信息引入 2D 姿态序列。在 Human3.6M、MPI-INF-3DHP 和 Human-Eva 基准测试中进行了广泛的实验。G-SFormer 系列方法与以前的最先进技术相比，仅用大约 10% 的参数就实现了卓越的性能，并显着降低了计算复杂度。此外，G-SFormer 还表现出对检测到的 2D 姿态不准确性的出色鲁棒性。|
|**2024-07-03**|[ADFQ-ViT: Activation-Distribution-Friendly Post-Training Quantization for Vision Transformers](http://arxiv.org/abs/2407.02763)|null|视觉Transformer（ViT）在各种计算机视觉任务中都表现出色，但其庞大的参数量导致内存和计算需求显著增加，阻碍了其在资源受限设备上的有效推理。量化已成为缓解这些挑战的一种很有前景的解决方案，但现有方法在低比特情况下仍然存在显著的精度损失。我们将此问题归因于ViT中LayerNorm后和GELU后激活的独特分布，这使得传统的硬件友好型量化器效率低下，尤其是在低比特情况下。为了解决这个问题，我们提出了一种名为Activation-Distribution-Friendly post-training Quantization for Vision Transformers (ADFQ-ViT)的新颖框架。具体来说，我们引入了Per-Patch Outlier-aware Quantizer来处理LayerNorm后激活中的不规则异常值。该量化器在保持阈值以上最小值子集全精度的情况下，将均匀量化器的粒度细化到每个补丁级别。为了处理GELU后激活在正负区域之间的非均匀分布，我们设计了Shift-Log2 Quantizer，它将所有元素移位到正区域，然后应用log2量化。此外，我们提出了Attention-score enhanced Module-wise Optimization，通过重构误差来调整每个量化器的参数，以进一步减轻量化误差。大量实验表明，ADFQ-ViT在4比特图像分类、目标检测和实例分割任务中比各种基线都有显著改进。具体来说，在将ViT-B模型量化到4比特时，我们在ImageNet数据集上的Top-1准确率提高了10.23%。|
|**2024-07-02**|[A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models](http://arxiv.org/abs/2407.02646)|**[link](https://github.com/dakingrai/awesome-mechanistic-interpretability-lm-papers)**|机械可解释性 (MI) 是可解释性的一个新兴子领域，旨在通过逆向工程神经网络模型的内部计算来理解它。近年来，MI 在解释基于 Transformer 的语言模型 (LM) 方面引起了极大的关注，产生了许多新颖的见解，但也带来了新的挑战。然而，目前还没有工作全面回顾这些见解和挑战，特别是作为该领域新人的指南。为了填补这一空白，我们提供了一份全面的综述，概述了 MI 中的基本研究对象、用于其研究的技术、评估 MI 结果的方法，以及使用 MI 理解 LM 所产生的重要发现和应用。特别是，我们为初学者提供了一个路线图，以帮助他们在该领域导航并利用 MI 为自己谋福利。最后，我们还指出了该领域目前的差距，并讨论了未来可能的发展方向。|
|**2024-07-02**|[On the Anatomy of Attention](http://arxiv.org/abs/2407.02423)|null|我们引入一种范畴论图表形式体系，以便系统地关联和推理机器学习模型。我们的图表以直观的方式呈现架构，但不会丢失必要的细节，其中模型之间的自然关系通过图形变换来捕捉，并且重要的差异和相似性可以一目了然地识别出来。在本文中，我们将重点关注注意力机制：将民间传说转化为数学推导，并构建文献中注意力变体的分类法。作为以我们的形式主义为基础的实证研究的第一个例子，我们确定了注意力机制中反复出现的解剖学成分，我们对其进行了详尽的重组，以探索注意力机制的变化空间。|
|**2024-07-02**|[Efficient Sparse Attention needs Adaptive Token Release](http://arxiv.org/abs/2407.02328)|null|近年来，大型语言模型 (LLM) 在各种以文本为中心的的任务中展现出卓越的能力。然而，其“大”规模带来了巨大的计算和存储挑战，尤其是在管理 Transformer 的键值状态方面，这限制了其更广泛的适用性。因此，我们建议自适应地从缓存中释放资源并重建必要的键值状态。特别是，我们通过一个轻量级的控制器模块来近似理想的top- $K$稀疏注意力来实现这一点。该模块保留具有最高 top-$K$ 注意力权重的标记，并同时重建已丢弃但必要的标记，这些标记可能对未来的解码至关重要。自然语言生成和建模方面的综合实验表明，我们的方法不仅在性能方面与完全注意力机制相比具有竞争力，而且还实现了高达 221.8% 的显著吞吐量提升。复制代码可在 https://github.com/WHUIR/ADORE 上获得。|

<p align=right>(<a href=#updated-on-20240708>back to top</a>)</p>

