## Updated on 2024.06.22
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#多模态>多模态</a></li>
    <li><a href=#6dof-object-pose>6DOF Object Pose</a></li>
    <li><a href=#nerf>nerf</a></li>
    <li><a href=#分类/检测/识别/分割>分类/检测/识别/分割</a></li>
    <li><a href=#模型压缩/优化>模型压缩/优化</a></li>
    <li><a href=#ocr>OCR</a></li>
    <li><a href=#生成模型>生成模型</a></li>
    <li><a href=#llm>LLM</a></li>
    <li><a href=#transformer>Transformer</a></li>
    <li><a href=#3dgs>3DGS</a></li>
    <li><a href=#各类学习方式>各类学习方式</a></li>
  </ol>
</details>

## 多模态

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-20**|[Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities](http://arxiv.org/abs/2406.14562)|null|当面对涉及视觉思维的问题时，人类会自然地切换推理模式，经常形成心理图像或绘制视觉辅助工具。大型语言模型在算术和符号推理方面表现出良好的效果，它们通过将中间推理过程以文本形式表达为思维链来实现，然而，即使经过广泛的多模态预训练，它们仍然难以扩展这种能力来回答那些通过视觉推理很容易解决的文本查询。我们引入了一种简单的方法，即“思维白板”提示，以释放跨模态多模态大型语言模型的视觉推理能力。“思维白板”提示为多模态大型语言模型提供了一个隐喻的“白板”，以便将推理步骤绘制成图像，然后将这些图像返回给模型以进行进一步处理。我们发现，无需演示或专门的模块就可以实现这一点，而是利用模型现有的能力，使用 Matplotlib 和 Turtle 等库编写代码。这种简单的方法在涉及视觉和空间推理的四项困难的自然语言任务上显示出最先进的结果。我们确定了 GPT-4o 使用思维链方法会严重失败的多种情况，包括超过一种情况下的准确率为 0%，而“思维白板”在这些相同的情况下能够实现高达 92% 的准确率。我们详细探讨了该技术的成功之处及其错误来源。||
|**2024-06-20**|[Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs](http://arxiv.org/abs/2406.14544)|**[link](https://github.com/sparksjoe/prism)**|视觉语言模型 (VLM) 在解决各种视觉问题方面表现出色，这需要强大的感知和推理能力。尽管由于现有 VLM 中视觉和推理的本质交织在一起，独立评估这两种能力存在固有难度，但这对于模型改进至关重要。为了解决这个问题，我们提出了 Prism，这是一个旨在解耦视觉问答中涉及的感知和推理过程的创新框架。Prism 包含两个不同的阶段：一个利用 VLM 以文本形式提取和表达视觉信息的感知阶段，以及一个使用大型语言模型 (LLM) 根据提取的视觉信息制定响应的推理阶段。这种模块化设计能够系统地比较和评估专有和开源 VLM 的感知和推理优势。我们的分析框架提供了一些有价值的见解，强调了 Prism 作为视觉语言任务的经济高效解决方案的潜力。通过将专注于感知的简化 VLM 与专为推理设计的强大 LLM 相结合，Prism 在一般的视觉语言任务中取得了优异的结果，同时大幅削减了训练和运营成本。定量评估表明，当使用 vanilla 2B LLaVA 和免费提供的 GPT-3.5 配置时，Prism 在严格的多模态基准 MMStar 上的性能与规模大 10 倍的 VLM 相当。该项目已发布在：https://github.com/SparksJoe/Prism。||
|**2024-06-20**|[MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding](http://arxiv.org/abs/2406.14515)|**[link](https://github.com/open-compass/vlmevalkit)**|大型视觉语言模型 (LVLM) 的出现促进了对其在多模态环境下应用的研究，特别是在视频理解方面。传统的 VideoQA 基准测试虽然提供了量化指标，但往往无法涵盖视频内容的全部范围，并且无法充分评估模型对时间信息的理解能力。为了解决这些局限性，我们推出了 MMBench-Video，这是一个定量基准测试，旨在严格评估 LVLM 在视频理解方面的能力。MMBench-Video 收录了来自 YouTube 的长视频，并采用自由形式的问题，以反映实际用例。该基准测试经过精心设计，以探究模型的时间推理能力，所有问题都根据精心构建的能力分类法进行了人工标注。我们采用 GPT-4 进行自动评估，证明其相较于早期基于 LLM 的评估方法具有更高的准确性和鲁棒性。我们利用 MMBench-Video 对包括图像和视频领域的专有和开源 LVLM 进行了全面评估。MMBench-Video 是研究界宝贵的资源，有助于改进 LVLM 的评估并推动视频理解领域的发展。MMBench-Video 的评估代码将集成到 VLMEvalKit 中：https://github.com/open-compass/VLMEvalKit。||
|**2024-06-20**|[African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification](http://arxiv.org/abs/2406.14496)|null|近来的大型视觉语言模型 (LVLM) 在众多图像理解和推理任务中展现出惊人的能力。然而，细粒度目标分类任务（例如，区分\textit{动物物种}）尽管具有下游重要性，但尚未得到充分探索。我们通过创建\texttt{FOCI}（\textbf{F}ine-grained \textbf{O}bject \textbf{C}lass\textbf{I}fication，细粒度目标分类）来填补这一评估空白，这是一个从现有目标分类数据集中构建的困难多项选择基准测试：(1) 多项选择避免了将分类视为开放式问答任务所带来的答案模糊性；(2) 我们通过使用 CLIP 模型挖掘负标签来保持分类难度。\texttt{FOCI} 使用来自 ImageNet-21k 的四个特定领域子集对五个流行的分类数据集进行了补充。我们对 \texttt{FOCI} 上的 12 个公共 LVLMs 进行了基准测试，结果表明，它测试了对已建立的图像理解和推理基准的\textit{补充技能}。至关重要的是，CLIP 模型表现出比 LVLMs 好得多的性能。由于 LVLMs 的图像编码器来自这些 CLIP 模型，这表明编码器和 LLM 之间在细粒度目标区分方面存在对齐不足的问题，并且需要具有更细粒度注释的（预）训练数据。我们在 \url{https://github.com/gregor-ge/FOCI-Benchmark} 发布我们的代码。||
|**2024-06-20**|[Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?](http://arxiv.org/abs/2406.14492)|null|大型视觉语言模型 (LVLM) 近期极大地推动了图像描述和许多图像理解任务（例如，视觉问答）的发展。然而，LVLM 经常会“出现幻觉”，生成描述中包含图像中不存在的概念。这些幻觉削弱了 LVLM 的可信度，并且可以说是其广泛应用的主要障碍之一。最近的研究表明，添加 grounding 目标（明确将图像区域或对象与文本范围对齐的目标）可以减少 LVLM 幻觉的发生。虽然直观，但这种说法并没有得到经验证据的支持，因为我们认为，减少效果是通过有缺陷的评估方案建立的，这些方案 (i) 依赖于 LVLM 训练中广泛使用的数据（即 MSCOCO），以及 (ii) 通过问答而不是开放式描述生成来衡量幻觉。相比之下，在这项工作中，我们首次系统地分析了细粒度对象 grounding 对 LVLM 幻觉的影响，该评估方案更真实地反映了开放式生成中的 LVLM 幻觉。我们对三个骨干 LLM 进行的大量实验表明，grounding 目标对开放式描述生成中的对象幻觉几乎没有影响。||
|**2024-06-20**|[Revealing Vision-Language Integration in the Brain with Multimodal Networks](http://arxiv.org/abs/2406.14481)|null|我们使用（多）模态深度神经网络 (DNN) 来探测人脑中多模态整合的位点，方法是预测人类受试者在观看电影时记录的脑电图 (SEEG)。我们将多模态整合的位点定义为：多模态视觉语言模型比单模态语言、单模态视觉或线性整合的语言视觉模型能更好地预测记录的区域。我们的目标 DNN 模型涵盖不同的架构（例如，卷积网络和变换器）和多模态训练技术（例如，交叉注意力和对比学习）。作为一个关键的实现步骤，我们首先证明训练过的视觉和语言模型在预测 SEEG 信号的能力方面系统地优于随机初始化的模型。然后，我们将单模态和多模态模型相互比较。由于我们的目标 DNN 模型通常具有不同的架构、参数数量和训练集（可能会掩盖那些可归因于整合的差异），因此我们对两个模型（SLIP 和 SimCLR）进行了受控比较，这两个模型除了输入模态外，其他所有属性都保持不变。使用这种方法，我们确定了大量的神经位点（平均 1090 个位点中的 141 个，或 12.94%）和似乎发生多模态整合的大脑区域。此外，我们发现在我们评估的多模态训练技术变体中，CLIP 风格的训练最适合在下游预测这些位点的神经活动。||
|**2024-06-20**|[Towards Truthful Multilingual Large Language Models: Benchmarking and Alignment Strategies](http://arxiv.org/abs/2406.14434)|null|在大型语言模型 (LLM) 时代，构建能够为全球用户服务的多语言大型语言模型 (MLLM) 具有重要意义。然而，现有研究很少关注 MLLM 的真实性。同时，当代多语言对齐技术难以平衡大量语言，并且经常表现出不同语言之间严重的真实性差距，尤其是那些与英语差异很大的语言。在我们的工作中，我们构建了一个用于多语言场景下真实性评估的基准，并探索了跨语言对齐事实以增强 MLLM 真实性的方法。此外，我们提出了事实感知多语言选择性协同 (FaMSS) 来优化大量语言和不同数据类型的数据分配。实验结果表明，我们的方法可以有效地减少多语言表示差异并增强 LLM 的多语言能力。||
|**2024-06-20**|[iWISDM: Assessing instruction following in multimodal models at scale](http://arxiv.org/abs/2406.14343)|null|能够按照详细指令执行复杂任务是我们人类取得众多非凡成就的关键。作为人类，我们不仅能够执行各种各样的任务，而且还能够执行可能需要数百甚至数千个步骤才能完成的非常复杂的任务。大型语言模型及其最新的多模态模型（集成了文本和视觉输入）在执行复杂任务方面取得了前所未有的成功。然而，大多数现有的基准测试很大程度上局限于单一模态输入（文本或视觉），这缩小了多模态评估的范围，特别是在多模态环境下的指令遵循方面。为了弥合这一差距，我们引入了指令式虚拟视觉决策 (iWISDM) 环境，该环境旨在生成无限量的具有不同复杂程度的视觉语言任务。我们使用 iWISDM 编制了三个不同的指令遵循视觉任务基准测试，涵盖不同的复杂程度，并评估了这些基准测试中几个新开发的多模态模型。我们的研究结果表明，iWISDM 是一个强大的基准测试，用于评估现有和新兴多模态模型的指令遵循性，并突出了这些模型与人类在精确遵循指令的能力方面存在巨大差距。||
|**2024-06-20**|[E-ANT: A Large-Scale Dataset for Efficient Automatic GUI NavigaTion](http://arxiv.org/abs/2406.14250)|null|近年来，由于移动设备上的在线 GUI 导航对许多实际应用都有贡献，因此受到了广泛关注。随着大型语言模型（LLM）的快速发展，多模态大型语言模型（MLLM）在这项任务中具有巨大潜力。然而，现有的 MLLM 需要高质量的数据来提高其根据人类用户输入做出正确导航决策的能力。在本文中，我们开发了一个新颖且非常有价值的数据集，名为 E-ANT，它是第一个包含真实人类行为和高质量带注释屏幕截图的中文 GUI 导航数据集，包含超过 5000 多个不同小程序的近 40,000 条真实人类轨迹。此外，我们在 E-ANT 上评估了各种强大的 MLLM，并通过充分的消融实验展示了它们的实验结果。我们相信，我们提出的数据集将有利于 GUI 导航和 LLM/MLLM 决策能力的评估和发展。||
|**2024-06-20**|[VLBiasBench: A Comprehensive Benchmark for Evaluating Bias in Large Vision-Language Model](http://arxiv.org/abs/2406.14194)|**[link](https://github.com/xiangkui-cao/vlbiasbench)**|大型视觉语言模型 (LVLM) 的出现标志着迈向通用人工智能的重要一步。然而，这些进步受到输出结果经常反映偏差的影响，这个问题尚未得到广泛研究。现有的基准测试由于其数据规模有限、单一的问题格式和狭窄的偏差来源，在评估偏差方面不够全面。为了解决这个问题，我们引入了 VLBiasBench，这是一个旨在全面评估 LVLM 中偏差的基准测试。在 VLBiasBench 中，我们构建了一个包含九种不同社会偏差类别的数据集，包括年龄、残疾状况、性别、国籍、外貌、种族、宗教、职业、社会经济地位以及两个交叉偏差类别（种族 x 性别和种族 x 社会经济地位）。为了创建大规模数据集，我们使用 Stable Diffusion XL 模型生成了 46,848 张高质量图像，并将它们与不同的问题相结合，形成了 128,342 个样本。这些问题分为开放式和封闭式，充分考虑了偏差的来源，并从多个角度全面评估了 LVLM 的偏差。随后，我们对 15 个开源模型以及一个先进的闭源模型进行了广泛的评估，为揭示这些模型的偏差提供了一些新的见解。我们的基准测试可在 https://github.com/Xiangkui-Cao/VLBiasBench 获取。||
|**2024-06-18**|[Synergizing Foundation Models and Federated Learning: A Survey](http://arxiv.org/abs/2406.12844)|null|近年来，以大型语言模型、视觉Transformer和多模态模型为代表的Foundation Models (FMs)发展迅速，对学术界和工业界都产生了重大影响。与小规模模型相比，FMs在预训练阶段对海量数据的需求更大。虽然通用FMs可以使用互联网等公开来源收集的数据进行预训练，但特定领域FMs需要专有数据，由于隐私问题，可用数据量成为一个实际挑战。联邦学习 (FL) 是一种协作学习范式，它打破了不同参与者之间数据可用的障碍。因此，它提供了一个很有前景的解决方案，可以使用分布式数据集定制和调整FMs以适应各种特定领域的任务，同时保护隐私。这篇综述文章讨论了融合FL和FMs的潜力和挑战，并总结了核心技术、未来方向和应用。关于FM-FL的定期更新的论文合集可在https://github.com/lishenghui/awesome-fm-fl获取。||
|**2024-06-18**|[Adversarial Attacks on Multimodal Agents](http://arxiv.org/abs/2406.12814)|**[link](https://github.com/chenwu98/agent-attack)**|具有视觉功能的语言模型 (VLM) 现在被用于构建能够在真实环境中采取行动的自主多模态代理。在本文中，我们展示了多模态代理会引发新的安全风险，尽管由于对环境的访问和了解有限，攻击代理比之前的攻击更具挑战性。我们的攻击使用对抗性文本字符串来指导环境中一个触发图像上的基于梯度的扰动：(1) 我们的字幕攻击器攻击白盒字幕器（如果它们被用于将图像处理成字幕作为 VLM 的附加输入）；(2) 我们的 CLIP 攻击共同攻击一组 CLIP 模型，这些模型可以迁移到专有的 VLM。为了评估攻击，我们策划了 VisualWebArena-Adv，这是一组基于 VisualWebArena 的对抗性任务，VisualWebArena 是一个用于基于 Web 的多模态代理任务的环境。在单个图像上 L-infinity 范数为 $16/256$ 的情况下，字幕攻击器攻击可以让字幕增强型 GPT-4V 代理以 75% 的成功率执行对抗目标。当我们移除字幕生成器或使用 GPT-4V 生成自己的字幕时，CLIP 攻击可以分别达到 21% 和 43% 的成功率。对基于其他 VLM（例如 Gemini-1.5、Claude-3 和 GPT-4o）的代理进行的实验表明，它们的稳健性存在有趣的差异。进一步的分析揭示了攻击成功的几个关键因素，我们还讨论了对防御的影响。项目页面：https://chenwu.io/attack-agent 代码和数据：https://github.com/ChenWu98/agent-attack||
|**2024-06-18**|[OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI](http://arxiv.org/abs/2406.12753)|**[link](https://github.com/gair-nlp/olympicarena)**|人工智能 (AI) 的发展得益于大型语言模型 (LLM) 和大型多模态模型 (LMM) 的进步，逐渐展现出在解决问题和科学发现（即 AI4Science）方面的潜在认知推理能力，而这些能力曾经是人类智能所独有的。为了全面评估当前模型在认知推理能力方面的表现，我们推出了 OlympicArena，其中包含 11,163 道双语问题，涵盖纯文本和交错图文两种形式。这些挑战涵盖了七个领域和 62 个国际奥林匹克竞赛项目的广泛学科，并经过严格的数据泄露检查。我们认为，奥林匹克竞赛问题中的挑战是评估人工智能认知推理的理想选择，因为它们具有复杂性和跨学科性，这对于应对复杂的科学挑战和促进发现至关重要。除了使用仅答案标准评估各个学科的表现外，我们还从多个角度进行了详细的实验和分析。我们深入研究了模型的认知推理能力、它们在不同模态下的表现，以及它们在过程级评估中的结果，这些对于需要复杂推理和冗长解决方案的任务至关重要。我们广泛的评估表明，即使是像 GPT-4o 这样先进的模型，总体准确率也只有 39.97%，这说明了当前人工智能在复杂推理和多模态整合方面的局限性。通过 OlympicArena，我们旨在推动人工智能向超级智能发展，使其能够应对科学及其他领域更复杂的挑战。我们还提供了一套全面的资源来支持人工智能研究，包括一个基准数据集、一个开源标注平台、一个详细的评估工具，以及一个具有自动提交功能的排行榜。||
|**2024-06-18**|[Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning](http://arxiv.org/abs/2406.12742)|**[link](https://github.com/dtennant/mirb_eval)**|大型语言模型 (LLM) 的进步显著拓宽了自然语言处理应用的范围，多模态 LLM 扩展了这些能力，以整合和解释视觉数据。然而，现有的视觉语言模型 (VLM) 基准主要集中在单图像输入上，忽略了多图像理解的关键方面。在本文中，我们介绍了一个多图像关系基准 MIRB，旨在评估 VLM 比较、分析和推理多个图像的能力。我们的基准涵盖四个类别：感知、视觉世界知识、推理和多跳推理。通过对各种开源和闭源模型的全面评估，我们证明，虽然开源 VLM 在单图像任务中表现出接近 GPT-4V 的性能，但在多图像推理任务中仍然存在显著的性能差距。我们的研究结果还表明，即使是最先进的 GPT-4V 模型在我们的基准测试中也遇到了困难，这凸显了在该领域进一步研究和开发的必要性。我们相信，我们对 MIRB 的贡献可以作为开发下一代多模态模型的测试平台。||
|**2024-06-18**|[AGLA: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention](http://arxiv.org/abs/2406.12718)|**[link](https://github.com/lackel/agla)**|尽管大型视觉语言模型 (LVLMs) 在各种多模态任务中取得了巨大成功，但它们面临着一个普遍存在的问题，即对象幻觉，即生成的文本响应与给定图像中的真实对象不一致。本文研究了各种 LVLMs，并指出对判别性局部图像特征的注意力不足是导致对象幻觉的一个根本原因。具体来说，LVLMs 主要关注与提示无关的全局图像特征，而未能捕捉到与提示相关的局部特征，从而破坏了 LVLMs 的视觉基础能力并导致幻觉。为此，我们提出了全局和局部注意力组合 (AGLA) 方法，这是一种无需训练且即插即用的方法，通过探索全局特征集合以生成响应，同时利用局部特征进行视觉区分来减轻对象幻觉。我们的方法展示了一种图像-提示匹配方案，可以从图像中捕获与提示相关的局部特征，从而形成输入图像的增强视图，其中保留了与提示相关的内容，同时屏蔽了不相关的干扰。借助增强视图，可以通过整合来自原始图像的生成性全局特征和来自增强图像的判别性局部特征来获得校准后的解码分布。大量实验表明，AGLA 能够持续减轻各种判别性和生成性基准测试中 LVLMs 的对象幻觉，并增强其一般感知能力。我们的代码将在 https://github.com/Lackel/AGLA 上发布。||
|**2024-06-18**|[Disturbing Image Detection Using LMM-Elicited Emotion Embeddings](http://arxiv.org/abs/2406.12668)|null|本文探讨了利用大型多模态模型 (LMM) 中编码的知识进行令人不安的图像检测 (DID) 的任务。具体来说，我们提出了以两种方式利用 LMM 知识：首先是提取通用的语义描述，其次是提取引发的 emotions。随后，我们使用 CLIP 的文本编码器来获得通用语义描述和 LMM 引发的 emotions 的文本嵌入。最后，我们使用上述文本嵌入以及相应的 CLIP 图像嵌入来执行 DID 任务。所提出的方法显著提高了基线分类精度，在增强后的令人不安的图像检测数据集上实现了最先进的性能。||
|**2024-06-18**|[Do More Details Always Introduce More Hallucinations in LVLM-based Image Captioning?](http://arxiv.org/abs/2406.12663)|null|大型视觉语言模型（LVLM）擅长整合视觉和语言上下文以生成详细的内容，促进了图像描述等应用。然而，使用 LVLM 生成描述时，经常面临对象幻觉（OH）的挑战，即输出文本错误地表示输入图像中的实际对象。虽然之前的研究将 OH 的发生归因于包含更多细节，但我们的研究发现现有指标存在技术缺陷，导致对模型的评估和关于 OH 的结论不可靠。这引发了关于以下问题的争论：在基于 LVLM 的图像描述中，更多细节是否总是会导致更多幻觉？在本文中，我们通过提出一种新颖的解码策略，即差异化波束解码（DBD）以及一组可靠的新评估指标：CLIP-Precision、CLIP-Recall 和 CLIP-F1，来解决这场争论。DBD 将隐藏在视觉输入中的丰富信息并行解码为称为单元事实的不同语言表示。这种解码是通过精心设计的差异分数实现的，该分数指导并行搜索和候选筛选。然后聚合选定的单元事实以生成最终的描述。我们提出的指标通过比较真实图像区域和生成的文本分区的嵌入组，来评估图像描述的全面性和准确性。在视觉基因组数据集上的大量实验验证了我们方法的有效性，证明了它在生成详细描述的同时保持了低幻觉水平。||
|**2024-06-18**|[Efficient and Long-Tailed Generalization for Pre-trained Vision-Language Model](http://arxiv.org/abs/2406.12638)|null|像CLIP这样的预训练视觉语言模型通过图像-文本匹配展现出强大的零样本推理能力，并被证明是各种下游任务中的强大少样本学习器。然而，在现实场景中，将CLIP应用于下游任务可能会遇到以下挑战：1）数据可能呈现长尾分布，并且可能没有所有类别的充足样本；2）可能出现完全没有样本的新类别的新兴任务。为了克服这些挑战，我们提出了一个名为Candle的新框架来实现高效的长尾泛化。在训练过程中，我们提出了补偿对数调整损失，以鼓励原型的大间隔，并缓解基本类别内部以及基本类别和新类别之间的不平衡。为了实现高效的适应性，我们将CLIP模型视为黑盒，并利用提取的特征来获得用于预测的视觉和文本原型。为了充分利用多模态信息，我们还提出了跨模态注意力机制来丰富来自两种模态的特征。为了实现有效的泛化，我们为新类别引入了虚拟原型，以弥补它们缺乏训练图像的不足。Candle在11个不同数据集上的大量实验中取得了最先进的结果，同时大大减少了训练时间，证明了我们方法的优越性。源代码可在https://github.com/shijxcs/Candle获取。||
|**2024-06-18**|[RS-GPT4V: A Unified Multimodal Instruction-Following Dataset for Remote Sensing Image Understanding](http://arxiv.org/abs/2406.12479)|**[link](https://github.com/geox-lab/rs-gpt4v)**|遥感图像智能理解模型正在经历一场由多模态大语言模型 (MLLM) 推动的深刻范式转变，即从学习领域模型 (LaDM) 的范式转变为学习预训练的通用基础模型，然后再进行领域自适应模型 (LaGD) 的范式。在新的 LaGD 范式下，过去十年推动遥感图像智能理解发展的旧数据集已不再适用于全新的任务。我们认为，必须设计一个新的数据集来简化任务，该数据集应具有以下特征：1）泛化性：训练模型学习任务之间的共享知识并适应不同的任务；2）复杂场景理解：训练模型理解感兴趣目标的细粒度属性，并能够用自然语言描述场景；3）推理能力：训练模型以实现高级视觉推理。在本文中，我们利用 GPT-4V 和现有数据集设计了一个高质量、多样化和统一的多模态指令遵循数据集，用于遥感图像理解，我们称之为 RS-GPT4V。为了实现泛化性，我们使用从 GPT-4V 通过指令遵循推导出的（问题，答案）来统一诸如字幕生成和目标定位等任务；为了实现复杂场景理解，我们提出了一种结合局部策略和全局策略的分层指令描述方法，其中局部策略描述了目标对象的细粒度属性及其空间关系，全局策略将所有局部信息整合起来生成详细的指令描述；为了实现推理能力，我们设计了多轮问答对，为模型提供推理能力。实验结果表明，通过 RS-GPT4V 微调的 MLLM 可以描述细粒度的信息。该数据集可在以下网址获取：https://github.com/GeoX-Lab/RS-GPT4V。||
|**2024-06-18**|[VRSBench: A Versatile Vision-Language Benchmark Dataset for Remote Sensing Image Understanding](http://arxiv.org/abs/2406.12384)|**[link](https://github.com/lx709/vrsbench)**|我们引入了一个新的基准，旨在推进用于遥感图像的通用、大规模视觉语言模型的开发。尽管已经提出了一些遥感领域的视觉语言数据集来实现这一目标，但现有数据集通常针对单一任务，缺乏详细的对象信息，或者质量控制不足。为了探索这些改进机会，我们提出了一个用于遥感图像理解的多功能视觉语言基准，称为VRSBench。该基准包含29,614张图像，其中包括29,614条经过人工验证的详细描述、52,472个对象参考和123,221个问答对。它有助于在广泛的遥感图像理解任务中训练和评估视觉语言模型。我们进一步在这个基准上评估了最先进的模型在三个视觉语言任务上的表现：图像描述、视觉定位和视觉问答。我们的工作旨在为遥感领域先进视觉语言模型的开发做出重大贡献。数据和代码可在https://github.com/lx709/VRSBench获取。||
|**2024-06-17**|[LLaNA: Large Language and NeRF Assistant](http://arxiv.org/abs/2406.11840)|null|多模态大型语言模型 (MLLM) 在理解图像和 3D 数据方面表现出色。然而，这两种模态在整体捕捉物体的外观和几何形状方面都存在缺陷。同时，神经辐射场 (NeRF) 已成为一种日益普遍的模态，它将信息编码在简单多层感知器 (MLP) 的权重中，可以同时编码物体的几何形状和逼真的外观。本文研究了将 NeRF 纳入 MLLM 的可行性和有效性。我们创建了 LLaNA，这是第一个能够执行 NeRF 描述和问答等新任务的通用 NeRF 语言助手。值得注意的是，我们的方法直接处理 NeRF 的 MLP 权重以提取有关所表示对象的信息，而无需渲染图像或实例化 3D 数据结构。此外，我们构建了一个包含 NeRF 数据集以及用于各种 NeRF 语言任务的文本注释，无需人工干预。基于此数据集，我们开发了一个基准来评估我们方法的 NeRF 理解能力。结果表明，处理 NeRF 权重比从 NeRF 中提取 2D 或 3D 表示表现更好。||
|**2024-06-17**|[MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs](http://arxiv.org/abs/2406.11833)|**[link](https://github.com/liuziyu77/mmdu)**|生成自然且有意义的响应以与多模态人类输入进行交流是大视觉语言模型 (LVLM) 的一项基本能力。虽然当前的开源 LVLM 在简化场景（例如单轮单图像输入）中表现出良好的性能，但它们在现实世界的对话场景中却表现不佳，例如在具有多轮和多图像的长上下文历史中遵循指令。现有的 LVLM 基准主要集中在单项选择题或简短回答上，这些并不能充分评估 LVLM 在现实世界人机交互应用中的能力。因此，我们引入了 MMDU，一个综合基准测试，以及 MMDU-45k，一个大规模指令微调数据集，旨在评估和改进 LVLM 在多轮和多图像对话中的能力。我们采用聚类算法从开源维基百科中找到相关的图像和文本描述，并在 GPT-4o 模型的帮助下由人工标注者构建问答对。MMDU 最多包含 18k 个图像+文本标记、20 张图像和 27 轮对话，这比以前的基准测试至少长 5 倍，对当前的 LVLM 构成了挑战。我们使用 MMDU 对 15 个代表性 LVLM 进行的深入分析表明，由于对话指令微调数据有限，开源 LVLM 落后于闭源 LVLM。我们证明，在 MMDU-45k 上对开源 LVLM 进行微调可以显著缩小这一差距，生成更长、更准确的对话，并提高 MMDU 和现有基准测试的得分（MMStar：+1.1%，MathVista：+1.5%，ChartQA：+1.2%）。我们的贡献为弥合当前 LVLM 模型与现实应用需求之间的差距铺平了道路。该项目可在 https://github.com/Liuziyu77/MMDU 获取。||
|**2024-06-17**|[Unveiling Encoder-Free Vision-Language Models](http://arxiv.org/abs/2406.11832)|**[link](https://github.com/baaivision/eve)**|现有的视觉语言模型 (VLM) 主要依赖视觉编码器提取视觉特征，然后利用大型语言模型 (LLM) 执行视觉语言任务。然而，视觉编码器在抽象视觉表示（例如分辨率、纵横比和语义先验）方面设置了强烈的归纳偏差，这可能会阻碍 VLM 的灵活性和效率。训练接受无缝视觉和语言输入（即没有视觉编码器）的纯 VLM 仍然具有挑战性，并且很少被探索。实证观察表明，没有编码器的直接训练会导致收敛缓慢和巨大的性能差距。在这项工作中，我们弥合了基于编码器和无编码器模型之间的差距，并提出了一个简单而有效的训练方法来实现纯 VLM。具体来说，我们通过彻底的实验揭示了有效训练无编码器 VLM 的关键方面：(1) 在一个统一的解码器内桥接视觉语言表示；(2) 通过额外监督增强视觉识别能力。借助这些策略，我们推出了 EVE，这是一种可以高效训练和推理的无编码器视觉语言模型。值得注意的是，仅利用 35M 公开可访问的数据，EVE 就可以在多个视觉语言基准测试中与具有类似能力的基于编码器的 VLM 相媲美。它明显优于具有神秘训练程序和未公开训练数据的对应模型 Fuyu-8B。我们相信 EVE 为开发跨模态的纯解码器架构提供了一条透明且有效的途径。我们的代码和模型可在以下网址公开获取：https://github.com/baaivision/EVE。||
|**2024-06-17**|[On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning](http://arxiv.org/abs/2406.11823)|**[link](https://github.com/naver-ai/elva)**|近年来，语言和视觉助手的进步展现出令人印象深刻的能力，但缺乏透明度，限制了更广泛的研究和可重复性。虽然开源模型可以有效地处理一般的图像任务，但它们在处理复杂的视觉情境文本理解时面临着计算需求高的挑战。此类任务通常需要增加标记输入和大型视觉模块来利用高分辨率信息。如何在模型大小和数据重要性之间取得平衡仍然是一个悬而未决的问题。本研究旨在通过识别关键组件并创建具有受限推理成本的高效模型来重新定义视觉语言模型的设计。通过策略性地制定数据集、优化视觉模块和增强监督技术，我们在保持高性能的同时，显著提高了推理吞吐量。从 1.6 亿到 130 亿参数不等的模型的广泛实验为模型优化提供了见解。我们将在 https://github.com/naver-ai/elva 上完全开源我们的代码库、模型和数据集。||
|**2024-06-17**|[VideoLLM-online: Online Video Large Language Model for Streaming Video](http://arxiv.org/abs/2406.11816)|null|最近的大型语言模型增强了视觉能力，使其能够理解图像、视频和交错的视觉语言内容。然而，这些大型多模态模型的学习方法通常将视频视为预先确定的片段，这使得它们在处理流式视频输入方面效率较低。在本文中，我们提出了一种新颖的视频流学习 (LIVE) 框架，该框架支持在连续视频流中进行时间对齐、长上下文和实时对话。我们的 LIVE 框架包含实现视频流对话的综合方法，包括：（1）旨在对连续流输入执行语言建模的训练目标，（2）将离线时间注释转换为流对话格式的数据生成方案，以及（3）优化的推理管道，以加速模型在现实世界视频流中的响应速度。借助我们的 LIVE 框架，我们在 Llama-2/Llama-3 的基础上构建了 VideoLLM-online 模型，并展示了其在处理流式视频方面的显著优势。例如，平均而言，我们的模型可以在 A100 GPU 上以超过 10 FPS 的速度支持 5 分钟视频片段中的流式对话。此外，它还在公共离线视频基准测试中展示了最先进的性能，例如识别、字幕和预测。代码、模型、数据和演示已在 https://showlab.github.io/videollm-online 上提供。||
|**2024-06-17**|[LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning](http://arxiv.org/abs/2406.11815)|null|近年来，指令微调的大型多模态模型 (LMM) 在图像字幕和视觉问答等多项任务中取得了成功；然而，如何将这些模型应用于机器人领域仍然是一个开放性问题。以往用于机器人应用的 LMM 已经接受了大量语言和动作数据的训练，但它们在不同环境下的泛化能力往往不尽如人意。为了解决这个问题，我们引入了 LLARVA，这是一种使用新型指令微调方法训练的模型，该方法利用结构化提示来统一各种机器人学习任务、场景和环境。此外，我们还表明，预测中间二维表示（我们将其称为“视觉轨迹”）有助于进一步对齐机器人学习的视觉和动作空间。我们从 Open X-Embodiment 数据集中生成了 850 万个图像-视觉轨迹对，用于预训练我们的模型，并在 RLBench 模拟器中的 12 个不同任务以及实体 Franka Emika Panda 7 自由度机器人上进行了评估。我们的实验取得了良好的性能，表明 LLARVA 使用二维和语言表示，与其他几个当代基线模型相比表现出色，并且可以泛化到各种机器人环境和配置中。||
|**2024-06-17**|[See It from My Perspective: Diagnosing the Western Cultural Bias of Large Vision-Language Models in Image Understanding](http://arxiv.org/abs/2406.11665)|**[link](https://github.com/amith-ananthram/see-it-from-my-perspective)**|视觉-语言模型 (VLM) 可以用多种语言回答关于图像的查询。然而，除了语言之外，文化也会影响我们看待事物的方式。例如，来自西方文化的人更关注图像中的中心人物，而来自东方文化的人则更关注场景环境。在这项工作中，我们提出了一项新颖的研究，该研究证明并定位了 VLM 在图像理解中的西方偏见。我们使用文化多样性的图像和注释，在主观和客观视觉任务中评估大型 VLM。我们发现，在每个任务中，VLM 在西方子集上的表现优于东方子集。追踪这种偏见来源的对照实验强调了，即使推理是用英语进行的，在纯文本预训练中使用多样化的语言组合对于构建公平的 VLM 至关重要。此外，虽然以目标文化的语言进行提示可以减少偏见，但这并不能替代构建更能代表世界语言的人工智能。||
|**2024-06-17**|[Multimodal Learning To Improve Segmentation With Intraoperative CBCT & Preoperative CT](http://arxiv.org/abs/2406.11650)|null|术中医学影像，特别是锥束计算机断层扫描 (CBCT)，尽管视觉质量较低，但仍是促进计算机辅助介入治疗的重要工具。虽然这种降级的图像质量会影响下游分割，但高质量术前扫描的可用性为改进提供了潜力。在这里，我们考虑一种可以使用术前 CT 和术中 CBCT 扫描的情况，但是扫描之间的对齐（配准）并不完美。我们提出了一种多模态学习方法，融合了粗略对齐的 CBCT 和 CT 扫描，并研究了 CBCT 质量和错位（促进错位的仿射和弹性变换）对最终分割性能的影响。作为一个应用场景，我们专注于肝脏和肝脏肿瘤语义分割，并评估术中图像质量和错位对分割性能的影响。为此，将高质量、标记的 CT 定义为术前数据，并将其用作模拟术中 CBCT 的基础。我们表明，融合术前 CT 和模拟的术中 CBCT 大多可以提高分割性能，并且即使明显错位的术前数据也有可能提高分割性能。||
|**2024-06-17**|[AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic Manipulation](http://arxiv.org/abs/2406.11548)|null|对于机器人系统与现实世界物体进行稳定交互而言，反思和纠正错误的能力至关重要。观察到多模态大语言模型 (MLLM) 的泛化和推理能力，先前的方法旨在利用这些模型来相应地增强机器人系统。然而，这些方法通常侧重于使用额外的 MLLM 进行高级规划校正，而对失败样本的利用有限，无法校正低级接触姿态。为了解决这一差距，我们提出了一种自主交互校正 (AIC) MLLM，它利用先前的低级交互经验来校正 SE(3) 姿态预测。具体来说，AIC MLLM 最初经过微调，以获得姿态预测和反馈提示理解能力。我们通过与物体的交互精心设计了两种类型的提示指令：1) 视觉掩码，用于突出显示不可移动的部分以进行位置校正；2) 文本描述，用于指示旋转校正的潜在方向。在推理过程中，引入了反馈信息提取模块来识别失败原因，从而允许 AIC MLLM 使用相应的提示自适应地校正姿态预测。为了进一步增强操作稳定性，我们设计了一种测试时适应策略，使 AIC MLLM 能够更好地适应当前场景配置。最后，我们在模拟和现实环境中进行了广泛的实验来评估所提出的方法。结果表明，我们的 AIC MLLM 可以通过利用交互体验提示有效地纠正失败样本。现实世界的演示可以在 https://sites.google.com/view/aic-mllm 找到。||
|**2024-06-17**|[MedThink: Inducing Medical Large-scale Visual Language Models to Hallucinate Less by Thinking More](http://arxiv.org/abs/2406.11451)|null|当大型视觉语言模型（LVLM）应用于多模态医学生成任务时，它们会遇到严重的模型幻觉问题。这严重损害了模型的生成准确性，使得LVLM难以在现实世界的医疗场景中实施以协助医生进行诊断。增强下游医学生成任务的训练数据是解决模型幻觉问题的有效方法。此外，医学领域训练数据的有限可用性和隐私问题极大地阻碍了模型的准确性和泛化能力。在本文中，我们介绍了一种模仿人类认知过程来构建细粒度指令对的方法，并将思维链（CoT）的概念从推理场景应用于训练场景，从而提出了一种称为MedThink的方法。我们对各种LVLM的实验表明，我们专为医学领域量身定制的新型数据构建方法显着提高了模型在医学图像报告生成任务中的性能，并大大减少了幻觉。这项工作的所有资源将很快发布。||

<p align=right>(<a href=#updated-on-20240622>back to top</a>)</p>

## 6DOF Object Pose

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-19**|[NeRF-Feat: 6D Object Pose Estimation using Feature Rendering](http://arxiv.org/abs/2406.13796)|null|物体姿态估计是机器人抓取和增强现实中的一个关键组成部分。基于学习的方法通常需要来自高度精确的 CAD 模型或使用复杂设置获取的标记训练数据的训练数据。我们通过学习从没有已知 CAD 模型的弱标记数据中估计姿态来解决这个问题。我们建议使用 NeRF 隐式地学习物体形状，然后将其用于使用对比损失与 CNN 一起学习视图不变特征。虽然 NeRF 有助于学习视图一致的特征，但 CNN 确保学习到的特征尊重对称性。在推理过程中，CNN 用于预测视图不变特征，这些特征可用于建立与 NeRF 中隐式 3D 模型的对应关系。然后使用对应关系来估计 NeRF 参考系中的姿态。与使用类似训练设置的其他方法不同，我们的方法还可以处理对称对象。具体来说，我们使用 NeRF 学习视点不变的判别特征，这些特征稍后用于姿态估计。我们在 LM、LM-Occlusion 和 T-Less 数据集上评估了我们的方法，并在使用弱标记数据的情况下实现了基准精度。||
|**2024-06-06**|[Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking](http://arxiv.org/abs/2406.04316)|null|6D物体姿态估计是计算机视觉中一项至关重要但极具挑战性的任务，其面临的主要挑战是大规模数据集的严重缺乏。这种匮乏阻碍了对模型性能的全面评估，限制了研究进展。此外，可用实例或类别的数量有限也限制了其应用。为了解决这些问题，本文提出了Omni6DPose，这是一个以对象类别多样性、规模大和对象材质多样性为特征的大型数据集。Omni6DPose主要由三个部分组成：ROPE（真实6D物体姿态估计数据集），包含332K张图像，涵盖149个类别中581个实例的超过150万个标注；SOPE（模拟6D物体姿态估计数据集），包含475K张在混合现实环境中创建并进行深度模拟的图像，涵盖149个类别中4162个实例的超过500万个标注；以及在ROPE和SOPE中均使用的手动对齐的真实扫描物体。由于存在大量的变化和歧义，Omni6DPose本身就极具挑战性。为了应对这一挑战，我们引入了GenPose++，这是对SOTA类别级姿态估计框架的增强版本，它包含两个关键改进：语义感知特征提取和基于聚类的聚合。此外，我们还提供了全面的基准分析，以评估先前方法在这个大规模数据集上在6D物体姿态估计和姿态跟踪方面的性能。||
|**2024-06-05**|[Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices](http://arxiv.org/abs/2406.02977)|null|随着机器人和增强现实应用越来越依赖于精确高效的6D物体姿态估计，边缘设备上的实时性能对于更具交互性和响应性的系统变得至关重要。我们提出的稀疏颜色代码网络（SCCN）体现了一种清晰简洁的流程设计，可以有效地满足这一需求。SCCN对RGB图像中的目标物体进行像素级预测，利用基本物体几何特征的稀疏性来加速透视n点（PnP）计算过程。此外，它引入了一种新颖的基于像素级几何的物体对称性表示，该表示与初始姿态预测无缝集成，有效地解决了对称物体的歧义问题。SCCN在英伟达Jetson AGX Xavier上分别在基准LINEMOD数据集和遮挡LINEMOD数据集上实现了每秒19帧（FPS）和6 FPS的估计速率，同时在这些速率下始终保持较高的估计精度。||
|**2024-05-19**|[Advancing 6-DoF Instrument Pose Estimation in Variable X-Ray Imaging Geometries](http://arxiv.org/abs/2405.11677)|**[link](https://github.com/cviviers/YOLO-6D-Pose)**|在微创手术中，对手术器械进行精确的 6 自由度 (6-DoF) 姿态估计可以显著改进治疗策略并最终改善手术结果。现有的深度学习方法已经取得了准确的结果，但它们需要针对每个对象定制方法，并且需要费力的设置和训练环境（通常扩展到广泛的模拟），同时缺乏实时计算能力。我们提出了一种用于 X 射线系统中 6-DoF 姿态估计任务的通用数据采集方法，一种新颖且通用的 YOLOv5-6D 姿态架构，用于准确快速地进行目标姿态估计，以及一种在单目锥束 X 射线图像采集几何形状考虑下进行手术螺钉姿态估计的完整方法。所提出的 YOLOv5-6D 姿态模型在公共基准测试中取得了具有竞争力的结果，同时在 GPU 上的速度 considerably 更快，达到 42 FPS。此外，该方法可以泛化各种 X 射线采集几何形状和语义图像复杂性，从而能够在不同领域进行准确的姿态估计。最后，所提出的方法在脊柱手术期间对骨螺钉姿态估计进行了测试，以用于计算机辅助引导。该模型通过 0.1 ADD-S 指标达到了 92.41% 的精度，展示了一种提高手术精度和患者预后的 promising 方法。YOLOv5-6D 的代码在 https://github.com/cviviers/YOLOv5-6D-Pose 公开可用。||
|**2024-05-18**|[PS6D: Point Cloud Based Symmetry-Aware 6D Object Pose Estimation in Robot Bin-Picking](http://arxiv.org/abs/2405.11257)|null|6D物体姿态估计在各个领域都发挥着至关重要的作用，特别是在工业工件抓取方面。针对锈蚀、高反射率和缺乏纹理等挑战，本文介绍了一种基于点云的姿态估计框架（PS6D）。PS6D侧重于细长和多对称物体。它通过注意力引导的特征提取模块提取多尺度特征，设计了对称感知旋转损失和中心距离敏感平移损失来回归每个点到实例质心的姿态，然后使用两阶段聚类方法完成实例分割和姿态估计。来自Sil'eane和IPA数据集的对象以及来自工业实践的典型工件被用于生成数据和评估算法。与最先进的方法相比，PS6D在F $_{1_{inst}}$ 方面提高了11.5%，在召回率方面提高了14.8%。PS6D的主要部分已部署到Mech-Mind的软件中，并在分拣实验中取得了91.7%的成功率，标志着其在工业姿态估计任务中的应用。||
|**2024-05-31**|[Deep Learning-Based Object Pose Estimation: A Comprehensive Survey](http://arxiv.org/abs/2405.07801)|**[link](https://github.com/cnjianliu/awesome-object-pose-estimation)**|目标姿态估计是计算机视觉中的一个基本问题，在增强现实和机器人领域有着广泛的应用。在过去的十年中，深度学习模型由于其优越的准确性和鲁棒性，越来越多地取代了依赖于工程点对特征的传统算法。然而，当代方法仍然存在若干挑战，包括它们对标记训练数据的依赖性、模型紧凑性、在挑战性条件下的鲁棒性以及泛化到新的未知物体的能力。目前缺少一篇综述文章来讨论该领域的进展、面临的挑战以及未来有希望的方向。为了填补这一空白，我们讨论了基于深度学习的目标姿态估计的最新进展，涵盖了该问题的所有三种形式，即实例级、类别级和未知目标姿态估计。我们的综述还涵盖了多种输入数据模态、输出姿态的自由度、目标属性和下游任务，为读者提供了对该领域的全面理解。此外，它还讨论了不同领域的训练范式、推理模式、应用领域、评估指标和基准数据集，并报告了当前最先进方法在这些基准数据集上的性能，从而帮助读者为其应用选择最合适的方法。最后，该综述指出了关键挑战，回顾了当前的趋势及其优缺点，并确定了未来研究的有希望的方向。我们还将持续跟踪https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation上的最新工作。||
|**2024-05-02**|[IntervenGen: Interventional Data Generation for Robust and Data-Efficient Robot Imitation Learning](http://arxiv.org/abs/2405.01472)|null|模仿学习是训练机器人控制策略的一种很有前景的范式，但这些策略可能会受到分布偏移的影响，即评估时的条件与训练数据中的条件不同。提高策略对分布偏移鲁棒性的一种流行方法是交互式模仿学习（即DAgger及其变体），其中人类操作员在策略执行期间提供纠正性干预。然而，收集足够数量的干预以覆盖策略错误的分布对于人类操作员来说可能是一项繁重的任务。我们提出了IntervenGen (I-Gen)，这是一种新颖的数据生成系统，它可以从少量的认为干预中自主生成大量覆盖状态空间的纠正性干预。我们将 I-Gen 应用于 4 个模拟环境和 1 个具有物体姿态估计误差的物理环境，结果表明，它只需 10 次人为干预即可将策略的鲁棒性提高多达 39 倍。视频和更多结果可在 https://sites.google.com/view/intervengen2024 获取。||
|**2024-04-17**|[GeoReF: Geometric Alignment Across Shape Variation for Category-level Object Pose Refinement](http://arxiv.org/abs/2404.11139)|null|物体姿态细化对于稳健的物体姿态估计至关重要。先前的工作在实例级物体姿态细化方面取得了重大进展。然而，由于类别内较大的形状变化以及目标物体与形状先验之间的差异，类别级姿态细化是一个更具挑战性的问题。为了应对这些挑战，我们引入了一种用于类别级物体姿态细化的全新架构。我们的方法集成了 HS 层和可学习的仿射变换，旨在增强几何信息的提取和对齐。此外，我们引入了一种跨点云变换机制，可以有效地融合不同的数据源。最后，我们通过结合形状先验信息进行平移和尺寸误差预测，来突破模型的极限。我们进行了大量的实验来证明所提出框架的有效性。通过广泛的定量实验，我们证明了在所有指标上，我们的方法都比基线方法有显著的改进。||
|**2024-04-08**|[Learning a Category-level Object Pose Estimator without Pose Annotations](http://arxiv.org/abs/2404.05626)|null|三维物体姿态估计是一项具有挑战性的任务。以往的工作总是需要数千张带有标注姿态的物体图像来学习三维姿态对应关系，这对于标注来说既费力又耗时。在本文中，我们提出在没有姿态标注的情况下学习类别级别的三维物体姿态估计器。我们没有使用手动标注的图像，而是利用扩散模型（例如，Zero-1-to-3）生成一组具有受控姿态差异的图像，并建议使用这些图像来学习我们的物体姿态估计器。直接使用原始扩散模型会导致图像出现姿态噪声和伪影。为了解决这个问题，首先，我们利用从专门设计的对比姿态学习中学习到的图像编码器来过滤不合理的细节并提取图像特征图。此外，我们提出了一种新的学习策略，允许模型从那些生成的图像集中学习物体姿态，而无需知道其规范姿态的对齐方式。实验结果表明，我们的方法能够从单次拍摄设置（作为姿态定义）中进行类别级别的物体姿态估计，同时在少样本类别级别物体姿态估计基准测试中明显优于其他最先进的方法。||
|**2024-03-28**|[Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation](http://arxiv.org/abs/2403.19527)|**[link](https://github.com/leeiieeo/ag-pose)**|类别级 6D 物体姿态估计旨在估计特定类别中未见实例的旋转、平移和大小。在这一领域，基于密集对应的算法取得了领先的性能。然而，它们没有明确考虑不同实例的局部和全局几何信息，导致对形状变化显著的未见实例的泛化能力较差。为了解决这个问题，我们提出了一种新的用于类别级 6D 物体姿态估计的实例自适应和几何感知关键点学习方法 (AG-Pose)，它包括两个关键设计：(1) 第一个设计是实例自适应关键点检测模块，它可以自适应地检测一组稀疏关键点，用于表示各种实例的几何结构。(2) 第二个设计是几何感知特征聚合模块，它可以有效地将局部和全局几何信息整合到关键点特征中。这两个模块可以协同工作，为未见实例建立鲁棒的关键点级对应关系，从而增强模型的泛化能力。在 CAMERA25 和 REAL275 数据集上的实验结果表明，所提出的 AG-Pose 在没有类别特定形状先验的情况下，大大优于现有最佳方法。||
|**2024-06-01**|[Object Pose Estimation via the Aggregation of Diffusion Features](http://arxiv.org/abs/2403.18791)|**[link](https://github.com/tianfu18/diff-feats-pose)**|从图像中估计物体姿态是3D场景理解的关键任务，近期的方法在大型基准数据集上已经展现出可喜的成果。然而，这些方法在处理未知物体时性能会显著下降。我们认为这是由图像特征的泛化能力有限导致的。为了解决这个问题，我们深入分析了扩散模型（例如Stable Diffusion）的特征，这些模型在对未知物体建模方面具有巨大潜力。基于这一分析，我们创新性地将这些扩散特征引入到物体姿态估计中。为此，我们提出了三种不同的架构，可以有效地捕捉和聚合不同粒度的扩散特征，极大地提高了物体姿态估计的泛化能力。在三个流行的基准数据集LM、O-LM和T-LESS上，我们的方法以相当大的优势优于现有技术水平。特别是在未知物体上，我们的方法比之前最佳方法取得了更高的准确率：在Unseen LM上为98.2% vs. 93.5%，在Unseen O-LM上为85.9% vs. 76.3%，显示出我们方法强大的泛化能力。我们的代码已发布在https://github.com/Tianfu18/diff-feats-pose。||

<p align=right>(<a href=#updated-on-20240622>back to top</a>)</p>

## nerf

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-20**|[Deblurring Neural Radiance Fields with Event-driven Bundle Adjustment](http://arxiv.org/abs/2406.14360)|null|神经辐射场 (NeRF) 利用高质量的多视图图像作为输入，在 3D 表征学习和新视图合成方面取得了令人印象深刻的结果。然而，图像中的运动模糊经常出现在低光照和高速运动场景中，这会显著降低 NeRF 的重建质量。以前的去模糊 NeRF 方法难以估计曝光时间内的信息，无法准确地模拟运动模糊。相比之下，受生物启发的事件相机以高时间分辨率测量强度变化，弥补了这种信息不足。在本文中，我们提出了事件驱动的去模糊神经辐射场的束调整方法（EBAD-NeRF），通过利用混合事件-RGB 数据联合优化可学习的姿态和 NeRF 参数。引入了强度变化度量事件损失和光度模糊损失，以加强相机运动模糊的显式建模。在合成数据和真实捕获数据上的实验结果表明，与先前的工作相比，EBAD-NeRF 可以在曝光时间内获得准确的相机姿态并学习更清晰的 3D 表征。||
|**2024-06-19**|[Freq-Mip-AA : Frequency Mip Representation for Anti-Aliasing Neural Radiance Fields](http://arxiv.org/abs/2406.13251)|**[link](https://github.com/yi0109/freqmipaa)**|神经辐射场 (NeRF) 在表征 3D 场景和生成新视图方面已展现出非凡的成功。然而，它们经常难以应对锯齿伪影，特别是在渲染与训练视图相机距离不同的图像时。为了解决这个问题，Mip-NeRF 提出使用体积视锥体来渲染像素，并建议使用集成位置编码 (IPE)。虽然有效，但这种方法由于依赖 MLP 架构而需要较长的训练时间。在这项工作中，我们提出了一种利用基于网格的表示的新型抗锯齿技术，该技术通常显示出显著更快的训练时间。此外，受采样定理的启发，我们利用频域表示来处理锯齿问题。所提出的方法 FreqMipAA 利用了尺度特定的低通滤波 (LPF) 和可学习的频率掩码。尺度特定的低通滤波器 (LPF) 可防止锯齿并优先考虑重要的图像细节，而可学习的掩码则可有效去除有问题的   高频元素，同时保留必要的信息。通过采用尺度特定的 LPF 和可训练掩码，FreqMipAA 可以有效消除锯齿因子，同时保留重要细节。我们通过将所提出的技术整合到广泛使用的基于网格的方法中来验证其有效性。实验结果表明，FreqMipAA   有效地解决了锯齿问题，并在多尺度 Blender 数据集中取得了最先进的结果。我们的代码可在 https://github.com/yi0109/FreqMipAA 获取。||
|**2024-06-18**|[Head Pose Estimation and 3D Neural Surface Reconstruction via Monocular Camera in situ for Navigation and Safe Insertion into Natural Openings](http://arxiv.org/abs/2406.13048)|null|随着仿真在医疗和介入中的重要性日益增加，预计将会出现一种简化且低成本的平台来执行个性化诊断和治疗。3D Slicer不仅可以执行医学图像分析和可视化，还可以提供手术导航和手术计划功能。在本文中，我们选择3D Slicer作为我们的基础平台，并使用单目相机作为传感器。然后，我们使用神经辐射场（NeRF）算法完成了人头的三维模型重建。我们比较了NeRF算法生成三维人头场景的准确性，并利用MarchingCube算法生成了相应的三维网格模型。通过单目视觉获得的个人头部姿态实时传输到3D Slicer中创建的场景。本文展示的演示包括3D Slicer场景中的人头模型与检测到的头部姿态之间变换的实时同步。此外，我们测试了一个场景，其中一个工具用ArUco标记，并由单个相机跟踪，同步指向头部姿势的实时变换。这些演示表明，我们的方法可以为鼻咽拭子采集或插管提供可行的实时模拟平台。||
|**2024-06-18**|[Fast Global Localization on Neural Radiance Field](http://arxiv.org/abs/2406.12202)|null|神经辐射场 (NeRF) 提出了一种表示场景的新方法，允许从 2D 图像进行高质量的 3D 重建。在其取得显著成就之后，NeRF 地图中的全局定位成为实现广泛应用的一项基本任务。最近，Loc-NeRF 展示了一种将传统的蒙特卡洛定位与 NeRF 相结合的定位方法，展示了使用 NeRF 作为环境地图的良好结果。然而，尽管取得了进步，Loc-NeRF 仍然面临着光线渲染过程耗时的挑战，这在实际应用中可能是一个很大的限制。为了解决这个问题，我们引入了 Fast Loc-NeRF，它利用从粗到精的方法来实现更高效、更准确的基于 NeRF 地图的全局定位。具体来说，Fast Loc-NeRF 将渲染的像素和观察到的图像从低分辨率到高分辨率的多分辨率上进行匹配。因此，它在保持精确定位结果的同时，加快了代价高昂的粒子更新过程。此外，为了剔除异常粒子，我们提出了粒子剔除加权方法，该方法利用 NeRF 的特性估计粒子的不确定性，并在粒子加权过程中考虑它们。我们的 Fast Loc-NeRF 在多个基准测试中都设定了新的最先进的定位性能，证明了其准确性和效率。||
|**2024-06-17**|[DistillNeRF: Perceiving 3D Scenes from Single-Glance Images by Distilling Neural Fields and Foundation Model Features](http://arxiv.org/abs/2406.12095)|null|我们提出了 DistillNeRF，这是一个自监督学习框架，旨在解决自动驾驶中从有限的 2D 观察中理解 3D 环境的挑战。我们的方法是一个可泛化的前馈模型，可以从稀疏的单帧多视图相机输入中预测丰富的场景神经表示，并通过可微渲染进行自监督训练，以重建 RGB、深度或特征图像。我们的第一个见解是通过生成密集的深度和虚拟相机目标来利用每个场景优化的神经辐射场 (NeRF)，从而帮助我们的模型从稀疏的非重叠图像输入中学习 3D 几何。其次，为了学习语义丰富的 3D 表示，我们建议从预训练的 2D 基础模型（如 CLIP 或 DINOv2）中提取特征，从而无需昂贵的 3D 人工标注即可实现各种下游任务。为了利用这两个见解，我们引入了一种新颖的模型架构，该架构具有两阶段提升-splat-shoot编码器和参数化的稀疏分层体素表示。在 NuScenes 数据集上的实验结果表明，DistillNeRF 在场景重建、新视图合成和深度估计方面明显优于现有的可比自监督方法；并且它允许进行有竞争力的零样本 3D 语义占用预测，以及通过提取的基础模型特征进行开放世界场景理解。演示和代码将在 https://distillnerf.github.io/ 上提供。||
|**2024-06-17**|[Uncertainty modeling for fine-tuned implicit functions](http://arxiv.org/abs/2406.12082)|null|隐函数如神经辐射场 (NeRFs)、占据网络和符号距离函数 (SDFs) 已成为计算机视觉中从稀疏视图重建详细物体形状的关键。由于输入的极度稀疏性和数据损坏引起的分布偏移，使用这些模型实现最佳性能可能具有挑战性。为此，大型的、无噪声的合成数据集可以作为形状先验，帮助模型填补空白，但必须谨慎对待由此产生的重建结果。不确定性估计对于评估这些重建的质量至关重要，尤其是在识别模型对其从先验推断的部分不确定的区域方面。在本文中，我们介绍了 Dropsembles，这是一种用于调整后的隐函数中的不确定性估计的新方法。我们通过一系列实验证明了我们方法的有效性，从玩具示例开始，逐步发展到真实场景。具体来说，我们在合成的解剖数据上训练了一个卷积占据网络，并在腰椎的低分辨率 MRI 分割上对其进行测试。我们的结果表明，Dropsembles 实现了深度集成的准确性和校准水平，但计算成本显着降低。||
|**2024-06-17**|[LLaNA: Large Language and NeRF Assistant](http://arxiv.org/abs/2406.11840)|null|多模态大型语言模型 (MLLM) 在理解图像和 3D 数据方面表现出色。然而，这两种模态都存在无法全面捕捉物体外观和几何形状的缺点。同时，神经辐射场 (NeRF) 作为一种新兴的、日益普及的模态，将信息编码在简单多层感知器 (MLP) 的权重中，能够同时编码物体的几何形状和逼真的外观。本文研究了将 NeRF 纳入 MLLM 的可行性和有效性。我们创建了 LLaNA，这是第一个能够执行 NeRF 图像描述和问答等新任务的通用 NeRF-语言助手。值得注意的是，我们的方法直接处理 NeRF MLP 的权重来提取有关所表示对象的信息，而无需渲染图像或实例化 3D 数据结构。此外，我们构建了一个包含 NeRF 及其文本注释的数据集，用于各种 NeRF-语言任务，无需人工干预。基于此数据集，我们开发了一个基准来评估我们方法对 NeRF 理解能力。结果表明，处理 NeRF 权重比从 NeRF 中提取 2D 或 3D 表示更有效。||
|**2024-06-17**|[InterNeRF: Scaling Radiance Fields via Parameter Interpolation](http://arxiv.org/abs/2406.11737)|null|神经辐射场 (NeRF) 在大型真实场景中具有无与伦比的保真度。扩展 NeRF 的一种常见方法是将场景划分为多个区域，每个区域都有自己的参数。如果采用简单的实现方式，这种方法会受到测试时缩放性差以及外观和几何形状不一致的限制。我们提出了一种名为 InterNeRF 的新颖架构，用于使用模型参数的子集来渲染目标视图。我们的方法支持核外训练和渲染，在仅略微增加训练时间的情况下增加了总模型容量。我们展示了在多房间场景中的显著改进，同时在标准基准测试中保持了竞争力。||
|**2024-06-17**|[Projecting Radiance Fields to Mesh Surfaces](http://arxiv.org/abs/2406.11570)|null|辐射场能够生成高保真度且渲染速度快的图像，但难以操控。我们结合了辐射场和网格表面的优点，有效地实现了不同外观之间的人物化身纹理迁移。我们使用三维高斯散射将源表示为辐射场，然后将高斯投影到目标网格上。我们的流程包括源预处理、目标矢量化和纹理投影。投影在纯CPU计算中只需1.12秒即可完成，而基线技术“逐面纹理投影”和“光线投射”分别需要31秒和4.1分钟。这种方法降低了计算需求，使其适用于从低端移动设备到高端计算机的更广泛设备。||
|**2024-06-16**|[Learning Relighting and Intrinsic Decomposition in Neural Radiance Fields](http://arxiv.org/abs/2406.11077)|null|从神经辐射场中提取内在成分（如反射率和阴影）的任务越来越受到关注。然而，当前的方法主要集中在合成场景和孤立物体上，而忽略了具有背景的真实场景的复杂性。为了解决这一差距，我们的研究引入了一种将重新照明与内在分解相结合的方法。通过利用场景中的光线变化来生成伪标签，我们的方法为内在分解提供了指导，而无需地面真实数据。我们的方法基于物理约束，确保了跨不同场景类型的鲁棒性，并减少了对预训练模型或手工先验的依赖。我们在合成数据集和真实世界数据集上验证了我们的方法，取得了令人信服的结果。此外，我们的方法在图像编辑任务中的适用性也显示出良好的效果。||
|**2024-06-15**|[fNeRF: High Quality Radiance Fields from Practical Cameras](http://arxiv.org/abs/2406.10633)|null|近年来，神经辐射场的开发使得从多视图相机数据中对场景和物体进行逼真的三维重建达到了前所未有的水平。然而，以往的方法使用过于简化的针孔相机模型，导致散焦模糊被“烘焙”到重建的辐射场中。我们提出了一种对射线投射的改进，利用透镜的光学特性来增强存在散焦模糊时的场景重建。这使我们能够利用有限孔径的实际相机的测量结果来提高辐射场重建的质量。我们证明，与针孔模型和其他散焦模糊模型的近似相比，所提出的模型更接近地匹配了实际相机的散焦模糊行为，特别是在存在部分遮挡的情况下。这使我们能够实现更清晰的重建，在合成数据集和真实数据集上，将所有聚焦图像验证的 PSNR 提高了 3 dB。||
|**2024-06-15**|[Federated Neural Radiance Field for Distributed Intelligence](http://arxiv.org/abs/2406.10474)|null|新视角合成（NVS）是许多增强现实（AR）和虚拟现实（VR）应用中的重要技术。最近提出的神经辐射场（NeRF）方法在 NVS 任务中表现出优异的性能，并已应用于其他相关领域。然而，由于严格的法规和隐私问题，某些具有分布式数据存储的应用场景可能会给 NeRF 方法获取训练图像带来挑战。为了克服这一挑战，我们专注于 FedNeRF，这是一种基于联邦学习（FL）的 NeRF 方法，它利用不同数据所有者可用的图像，同时保护数据隐私。在本文中，我们首先构建了一个资源丰富、功能多样的联邦学习测试平台。然后，我们在这样一个实际的联邦学习系统中部署 FedNeRF 算法，并在部分客户端选择的情况下进行 FedNeRF 实验。预计本文提出的 FedNeRF 方法研究将有助于促进 NeRF 方法在分布式数据存储场景中的未来应用。||
|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|在非结构化旅游环境中拍摄的照片经常呈现出多变的外观和短暂的遮挡，这对精确的场景重建提出了挑战，并在新视角合成中导致了伪影。尽管先前的方法已经将神经辐射场 (NeRF) 与其他可学习模块相结合来处理动态外观和消除瞬态对象，但其大量的训练需求和缓慢的渲染速度限制了实际部署。最近，3D 高斯 splatting (3DGS) 已成为 NeRF 的一种很有前景的替代方案，它提供了卓越的训练和推理效率以及更好的渲染质量。本文介绍了 Wild-GS，这是一种针对不受约束的照片集优化的 3DGS 创新改编，同时保留了其效率优势。Wild-GS 通过每张图像的固有材质属性、全局照明和相机属性以及逐点反射率的局部方差来确定每个 3D 高斯的外观。与先前在图像空间中对参考特征进行建模的方法不同，Wild-GS 通过对从参考图像中提取的三平面进行采样，将像素外观特征显式对齐到相应的局部高斯。这种新颖的设计有效地将参考视图的高频细节外观转移到 3D 空间，并显着加快了训练过程。此外，利用 2D 可见性图和深度正则化分别减轻瞬态效应和约束几何形状。大量实验表明，Wild-GS 在所有现有技术中实现了最先进的渲染性能以及最高的训练和推理效率。||
|**2024-06-14**|[GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors](http://arxiv.org/abs/2406.10111)|null|从低分辨率输入视图实现高分辨率新视图合成 (HRNVS) 是一项具有挑战性的任务，因为缺乏高分辨率数据。以前的方法从低分辨率输入视图优化高分辨率神经辐射场 (NeRF)，但渲染速度缓慢。在这项工作中，我们基于 3D 高斯渲染 (3DGS) 开发了我们的方法，因为它能够以更快的渲染速度生成高质量的图像。为了缓解高分辨率合成的数据短缺问题，我们建议利用现成的 2D 扩散先验，通过分数蒸馏采样 (SDS) 将 2D 知识提取到 3D。然而，由于生成先验带来的随机性，将 SDS 直接应用于基于高斯的 3D 超分辨率会导致不希望和冗余的 3D 高斯基元。为了缓解这个问题，我们引入了两种简单而有效的技术来减少 SDS 引入的随机扰动。具体来说，我们 1) 使用退火策略缩小 SDS 中扩散时间步长的范围；2) 在密集化过程中随机丢弃冗余的高斯基元。大量实验表明，我们提出的 GaussainSR 可以在合成和真实世界数据集上仅使用低分辨率输入就能获得高质量的 HRNVS 结果。项目页面：https://chchnii.github.io/GaussianSR/||
|**2024-06-14**|[RaNeuS: Ray-adaptive Neural Surface Reconstruction](http://arxiv.org/abs/2406.09801)|**[link](https://github.com/wangyida/ra-neus)**|我们的目标是利用可微分的辐射场（例如 NeRF）来重建详细的 3D 表面，以及生成标准的新颖视图渲染。已经有相关的方法来执行此类任务，通常是利用带符号距离场 (SDF)。然而，最先进的方法仍然无法正确重建小尺度细节，例如树叶、绳索和纺织品表面。考虑到不同的方法通过全局常数 Eikonal 正则化来制定和优化从 SDF 到辐射场的投影，我们通过逐射线加权因子进行改进，以优先考虑渲染和零交叉表面拟合，并在建立完美 SDF 的基础上进行。我们建议自适应地调整带符号距离场上的正则化，以便不令人满意的渲染光线不会强制执行无效的强 Eikonal 正则化，并允许来自具有良好学习辐射的区域的梯度有效地反向传播到 SDF。因此，平衡这两个目标以生成准确和详细的表面。此外，关于 SDF 中的零交叉表面和辐射场中的渲染点之间是否存在几何偏差，投影也会根据优化期间不同的 3D 位置进行调整。我们提出的 RaNeuS 在合成数据集和真实数据集上都进行了广泛的评估，在新颖视图合成和几何重建方面均取得了最先进的结果。||
|**2024-06-13**|[Neural NeRF Compression](http://arxiv.org/abs/2406.08943)|null|神经辐射场 (NeRFs) 已成为通过连续体积表示捕获详细 3D 场景的强大工具。最近的 NeRF 利用特征网格来提高渲染质量和速度；然而，这些表示引入了大量的存储开销。本文提出了一种有效压缩基于网格的 NeRF 模型的新方法，解决了存储开销问题。我们的方法基于非线性变换编码范式，采用神经压缩来压缩模型的特征网格。由于缺乏涉及许多独立同分布场景的训练数据，我们为单个场景设计了一种无编码器、端到端优化的轻量级解码器方法。为了利用潜在特征网格的空间不均匀性，我们引入了重要性加权的率失真目标函数和采用掩蔽机制的稀疏熵模型。我们的实验结果表明，我们提出的方法在基于网格的 NeRF 压缩效率和重建质量方面优于现有工作。||

<p align=right>(<a href=#updated-on-20240622>back to top</a>)</p>

## 分类/检测/识别/分割

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-20**|[Visible-Thermal Tiny Object Detection: A Benchmark Dataset and Baselines](http://arxiv.org/abs/2406.14482)|**[link](https://github.com/xinyiying24/rgbt-tiny)**|小目标检测（SOD）几十年来一直是一项长期存在的挑战性任务，已经开发了许多数据集和算法。然而，它们主要集中在可见光或热成像模态，而很少探索可见光-热成像（RGBT）双模态。尽管最近开发了一些RGBT数据集，但数量不足、类别有限、图像未对齐以及目标尺寸过大等问题导致无法提供一个公正的基准来评估多类别可见光-热成像小目标检测（RGBT SOD）算法。在本文中，我们构建了第一个用于RGBT SOD的大规模、高多样性基准（即RGBT-Tiny），包括115个配对序列、9.3万帧和120万个手动标注。RGBT-Tiny包含丰富的目标类别（7类）和高度多样化的场景（8种，涵盖不同的光照和密度变化）。值得注意的是，超过81%的目标小于16x16像素，我们提供了带有跟踪ID的配对边界框标注，为RGBT融合、检测和跟踪等广泛应用提供了一个极具挑战性的基准。此外，我们提出了一种尺度自适应适应度（SAFit）度量方法，该方法对大小目标均表现出较高的鲁棒性。所提出的SAFit可以提供合理的性能评估，并提升检测性能。基于所提出的RGBT-Tiny数据集和SAFit度量方法，我们对23种最新的算法进行了广泛的评估，涵盖了四种不同类型（即可见光通用目标检测、可见光SOD、热成像SOD和RGBT目标检测）。项目地址：https://github.com/XinyiYing24/RGBT-Tiny。||
|**2024-06-20**|[Enhanced Bank Check Security: Introducing a Novel Dataset and Transformer-Based Approach for Detection and Verification](http://arxiv.org/abs/2406.14370)|**[link](https://github.com/saifkhichi96/ssbi-dataset)**|银行支票上的自动签名验证对于防止欺诈和确保交易真实性至关重要。由于现实世界文档中签名与其他文本和图形元素共存，这项任务具有挑战性。验证系统必须首先检测签名，然后验证其真实性，这是当前数据集和方法经常忽略的双重挑战，它们只关注验证。为了解决这一差距，我们引入了一个专门为银行支票上的签名验证设计的新数据集。该数据集包括嵌入在典型支票元素中的各种签名样式，为高级检测方法提供了真实的测试平台。此外，我们提出了一种使用目标检测网络进行独立于书写者的签名验证的新方法。我们基于检测的验证方法将真实签名和伪造签名视为目标检测框架内的不同类别，有效地处理了检测和验证。我们采用基于 DINO 的网络，并增加了扩张模块，以同时检测和验证支票图像上的签名。我们的方法在真实签名和伪造签名上分别达到了 99.2 和 99.4 的 AP，相较于 DINO 基线（分别为 93.1 和 89.3）有了显著提高。这种改进凸显了我们的扩张模块在减少误报和漏报方面的有效性。我们的结果证明了基于检测的签名验证技术的实质性进步，为金融文档处理提供了更高的安全性和效率。||
|**2024-06-20**|[HoTPP Benchmark: Are We Good at the Long Horizon Events Forecasting?](http://arxiv.org/abs/2406.14341)|**[link](https://github.com/ivan-chai/hotpp-benchmark)**|在序列事件预测中，该技术在金融、零售、社交网络和医疗保健等领域有着广泛应用，一项至关重要的任务是在规定的时间范围内预测多个未来事件。传统上，这通过使用下一个事件预测模型（如标记时间点过程）的自回归生成来解决。然而，自回归方法使用自身的输出进行未来预测，可能会随着预测范围的扩大而降低质量。在本文中，我们挑战传统方法，引入了一个名为 HoTPP 的全新基准，专门用于评估模型预测一段时间内事件序列的能力。该基准测试采用了一种受计算机视觉目标检测启发的新指标，解决了现有指标在评估时间步长预测不精确的模型方面的局限性。我们对使用各种模型的已建立数据集的评估表明，下一个事件预测的高准确性并不一定意味着出色的范围预测，反之亦然。HoTPP 旨在作为开发更稳健的事件序列预测方法的有价值的工具，最终为该领域的进一步发展铺平道路。||
|**2024-06-20**|[Adaptive Adversarial Cross-Entropy Loss for Sharpness-Aware Minimization](http://arxiv.org/abs/2406.14329)|null|近期的学习算法研究表明，损失曲面的锐度是改善泛化能力的有效指标。基于此概念，人们提出了锐度感知最小化（SAM）算法来增强模型泛化能力，并取得了最先进的结果。SAM 包含两个主要步骤：权重扰动步骤和权重更新步骤。然而，SAM 中的扰动仅由训练损失或交叉熵损失的梯度决定。当模型接近驻点时，该梯度会变得很小并发生震荡，导致扰动方向不一致，并且还有可能导致梯度消失。我们的研究引入了一种创新方法来进一步增强模型泛化能力。我们提出了自适应对抗交叉熵（AACE）损失函数来代替 SAM 扰动中的标准交叉熵损失。AACE 损失及其梯度在模型接近收敛时会独特地增加，确保扰动方向一致，并解决了梯度消失问题。此外，我们还提出了一种利用未经归一化的 AACE 损失生成扰动的新型函数，增强了模型在接近最优阶段的探索能力。实证测试证实了 AACE 的有效性，实验表明，使用 Wide ResNet 和 PyramidNet 在各种数据集上的图像分类任务中，AACE 的性能均有所提高。代码可在网上获取。||
|**2024-06-20**|[Zero-Shot Image Denoising for High-Resolution Electron Microscopy](http://arxiv.org/abs/2406.14264)|null|高分辨率电子显微镜 (HREM) 成像技术是一种强大的工具，可以在真实空间中直接可视化各种材料。然而，由于超低的信噪比 (SNR) 和有限的数据可用性，它在去噪方面面临着挑战。在这项工作中，我们提出了 Noise2SR，一个用于 HREM 的零样本自监督学习 (ZS-SSL) 去噪框架。在我们的框架内，我们提出了一种基于超分辨率 (SR) 的自监督训练策略，并结合了随机子采样器模块。随机子采样器旨在从单个噪声图像生成近似无限的噪声对，作为零样本去噪中的有效数据增强。Noise2SR 使用不同分辨率的成对噪声图像训练网络，这是通过 SR 策略进行的。基于 SR 的训练有助于网络采用更多像素进行监督，随机子采样有助于迫使网络学习连续信号，从而增强鲁棒性。同时，我们通过对去噪结果采用最小均方误差 (MMSE) 估计来减轻随机采样引起的不确定性。通过训练策略和提出的设计的独特集成，Noise2SR 可以使用单个噪声 HREM 图像实现卓越的去噪性能。我们在模拟和真实 HREM 去噪任务中评估了 Noise2SR 的性能。它优于最先进的 ZS-SSL 方法，并实现了与监督方法相当的去噪性能。Noise2SR 的成功表明它在提高材料成像领域图像的 SNR 方面具有潜力。||
|**2024-06-20**|[LeYOLO, New Scalable and Efficient CNN Architecture for Object Detection](http://arxiv.org/abs/2406.14239)|**[link](https://github.com/LilianHollard/LeYOLO)**|深度神经网络中的计算效率对于目标检测至关重要，尤其是当较新的模型优先考虑速度而非高效计算（FLOP）时。这种演变在某种程度上使得面向嵌入式和移动设备的AI目标检测应用落后。在本文中，我们关注于基于FLOP的高效目标检测计算的神经网络架构设计选择，并提出了一些优化措施来提高基于YOLO的模型的效率。首先，我们介绍了一种受倒置瓶颈和信息瓶颈原理的理论见解启发的有效骨干网络缩放方法。其次，我们提出了快速金字塔架构网络（FPAN），旨在促进快速的多尺度特征共享，同时减少计算资源。最后，我们提出了一个解耦网络（DNiN）检测头，该检测头经过精心设计，可以为分类和回归任务提供快速而轻量级的计算。在这些优化的基础上，并利用更高效的骨干网络，本文为目标检测和以YOLO为中心的模型贡献了一种新的缩放范式，称为LeYOLO。我们的贡献在各种资源限制下始终优于现有模型，实现了前所未有的精度和flop比率。值得注意的是，LeYOLO-Small在COCO数据集上仅用4.5 FLOP(G)就实现了38.2%的mAP得分，与最新的YOLOv9-Tiny模型相比，计算量减少了42%，同时实现了相似的精度。我们的新型模型系列实现了前所未有的FLOP-精度比率，提供了从超低神经网络配置（< 1 GFLOP）到高效但要求苛刻的目标检测设置（> 4 GFLOPs）的可扩展性，在0.66、1.47、2.53、4.51、5.8和8.4 FLOP(G)时，mAP分别为25.2、31.3、35.2、38.2、39.3和41。||
|**2024-06-20**|[Seg-LSTM: Performance of xLSTM for Semantic Segmentation of Remotely Sensed Images](http://arxiv.org/abs/2406.14086)|**[link](https://github.com/zhuqinfeng1999/seg-lstm)**|近年来，线性复杂度的自回归网络取得了重大进展，并在大型语言模型中展现出卓越的性能。扩展长短期记忆网络（xLSTM）就是一个典型的例子，它结合了门控机制和记忆结构，在长序列语言任务中的表现可与Transformer架构相媲美。像xLSTM这样的自回归网络可以利用图像序列化将其应用扩展到视觉任务，如分类和分割。尽管现有研究表明Vision-LSTM在图像分类方面取得了令人印象深刻的结果，但其在图像语义分割方面的性能仍未得到验证。我们的研究首次尝试评估Vision-LSTM在遥感图像语义分割中的有效性。此评估基于一个专门设计的编码器-解码器架构，名为Seg-LSTM，并与最先进的分割网络进行了比较。我们的研究发现，Vision-LSTM在语义分割方面的性能有限，并且在大多数比较测试中普遍逊于基于Vision-Transformers和Vision-Mamba的模型。我们还提出了未来增强Vision-LSTM的研究方向。源代码可在https://github.com/zhuqinfeng1999/Seg-LSTM获取。||
|**2024-06-20**|[SSAD: Self-supervised Auxiliary Detection Framework for Panoramic X-ray based Dental Disease Diagnosis](http://arxiv.org/abs/2406.13963)|**[link](https://github.com/dylonsword/ssad)**|全景X射线是一种简单有效的临床牙科疾病诊断工具。在开发深度学习模型以辅助牙医解读全景X射线图像时，大多数模型的性能受到有限的标注数据的限制，因为标注需要牙医的专业知识和大量的时间成本。尽管已经提出了自监督学习（SSL）来应对这一挑战，但预训练和微调的两阶段过程需要更多的训练时间和计算资源。在本文中，我们提出了一种自监督辅助检测（SSAD）框架，该框架即插即用，并与任何检测器兼容。它由重建分支和检测分支组成。两个分支同时训练，共享相同的编码器，无需微调。重建分支学习恢复健康或患病牙齿的牙齿纹理，而检测分支利用这些学习到的特征进行诊断。为了增强编码器捕获细粒度特征的能力，我们结合了SAM的图像编码器来构建纹理一致性（TC）损失，该损失从重建分支的输入和输出中提取图像嵌入，然后将两个嵌入强制到相同的特征空间中。在公共DENTEX数据集上通过三个检测任务进行的大量实验表明，与主流的目标检测方法和SSL方法相比，所提出的SSAD框架实现了最先进的性能。代码可在https://github.com/Dylonsword/SSAD获取。||
|**2024-06-20**|[Towards the in-situ Trunk Identification and Length Measurement of Sea Cucumbers via Bézier Curve Modelling](http://arxiv.org/abs/2406.13951)|**[link](https://github.com/OUCVisionGroup/TISC-Net)**|我们提出了一种基于视觉的新型海参原位躯干识别和长度测量框架，这在海洋牧场资源监测和机械化收获中起着至关重要的作用。为了对具有不同弯曲程度的海参躯干曲线进行建模，我们利用了参数化贝塞尔曲线，因为它计算简单、稳定且具有广泛的变换可能性。然后，我们提出了一个端到端的统一框架，将参数化贝塞尔曲线建模与广泛使用的You-Only-Look-Once (YOLO) 流程（缩写为TISC-Net）相结合，并结合了有效的漏斗激活和高效的多尺度注意力模块，以增强曲线特征的感知和学习。此外，我们建议将躯干端点损失作为额外的约束条件，以有效减轻端点偏差对整体曲线的影响。最后，利用双目相机捕获的沿躯干曲线分布的像素深度信息，我们提出了通过空间曲线积分准确估计海参原位长度的方法。我们为基于曲线的原位海参躯干识别建立了两个具有挑战性的基准数据集。这些数据集包含1000多张真实海洋环境中的海参图像，以及贝塞尔格式的标注。我们在SC-ISTI数据集上进行了评估，我们的方法在目标检测和躯干识别任务中都实现了高于0.9的mAP50。广泛的长度测量实验表明，平均绝对相对误差约为0.15。||
|**2024-06-20**|[Communication-Efficient Adaptive Batch Size Strategies for Distributed Local Gradient Methods](http://arxiv.org/abs/2406.13936)|null|由于现代深度神经网络规模庞大，它们通常需要多个工作节点进行分布式训练。随着工作节点数量的增加，每次迭代梯度同步的数据并行小批量随机梯度下降法中，通信开销成为主要瓶颈。局部梯度方法，如局部随机梯度下降（Local SGD），通过仅在若干个局部步骤后才进行同步来减少通信。尽管我们了解它们在独立同分布和异构设置下的收敛性，并且知道批量大小对效率和泛化性的重要性，但确定最佳局部批量大小仍然很困难。我们为局部梯度方法引入了自适应批量大小策略，该策略自适应地增加批量大小以减少小批量梯度方差。我们在同构数据条件下提供了收敛性保证，并通过图像分类实验支持我们的论点，证明了我们的策略在训练和泛化方面的有效性。||
|**2024-06-18**|[LayerMerge: Neural Network Depth Compression through Layer Pruning and Merging](http://arxiv.org/abs/2406.12837)|**[link](https://github.com/snu-mllab/layermerge)**|最近的研究表明，减少卷积神经网络中的层数可以在保持网络性能的同时提高效率。现有的深度压缩方法移除冗余的非线性激活函数，并将连续的卷积层合并为一层。然而，这些方法存在一个严重的缺陷：合并后的层的内核大小会变大，这大大削弱了减少网络深度所带来的延迟降低。我们发现，这个问题可以通过联合剪枝卷积层和激活函数来解决。为此，我们提出了LayerMerge，这是一种新颖的深度压缩方法，它选择要移除的激活层和卷积层，以在最小化性能损失的同时实现所需的推理速度提升。由于相应的选择问题涉及指数级的搜索空间，我们提出了一个新的代理优化问题，并通过动态规划有效地解决了它。实验结果表明，我们的方法在图像分类和生成任务的各种网络架构上始终优于现有的深度压缩和层剪枝方法。我们在https://github.com/snu-mllab/LayerMerge上发布了代码。||
|**2024-06-18**|[Privacy Preserving Federated Learning in Medical Imaging with Uncertainty Estimation](http://arxiv.org/abs/2406.12815)|**[link](https://github.com/niko-k98/awesome-list-federated-learning-review)**|机器学习 (ML) 和人工智能 (AI) 推动了显著的进步，特别是在医疗保健领域。在医学影像方面，机器学习模型有望改进疾病诊断、治疗计划和治疗后监测。各种计算机视觉任务，如图像分类、目标检测和图像分割，有望成为临床分析的常规手段。然而，围绕患者数据的隐私问题阻碍了构建开发和训练准确、稳健和可泛化模型所需的大型训练数据集。联邦学习 (FL) 作为一种引人注目的解决方案应运而生，它使各机构能够通过共享模型训练信息（梯度）而不是数据（例如医学图像）来协作进行机器学习模型训练。联邦学习的分布式学习框架促进了机构间的协作，同时保护了患者隐私。然而，联邦学习虽然在隐私保护方面很强大，但也面临着一些挑战。在模型训练过程中，从机构之间传递的共享梯度中仍然可以收集到敏感信息。此外，在医学影像中，由于数据中存在噪声和伪影，因此准确量化模型置信度/不确定性至关重要。由于各机构之间的数据异构性，联邦学习中的不确定性估计遇到了独特的障碍。本文全面回顾了联邦学习、隐私保护和不确定性估计，重点关注医学影像。除了对当前研究进行综述外，我们还指出了该领域的差距，并为联邦学习研究提出了未来方向，以增强隐私并应对嘈杂的医学影像数据挑战。||
|**2024-06-18**|[Online Anchor-based Training for Image Classification Tasks](http://arxiv.org/abs/2406.12662)|null|在本文中，我们致力于提升深度学习模型在图像分类任务上的性能，提出了一种名为“在线基于锚点的训练”（OAT）的新型基于锚点的训练方法。OAT 方法受到基于锚点的目标检测方法的启发，建议训练模型学习相对于定义锚点的类别标签的百分比变化，而不是直接学习类别标签。我们将模型输出处的批次中心定义为锚点。然后，在测试阶段，将预测转换回原始类别标签空间，并评估性能。OAT 方法的有效性在四个数据集上得到了验证。||
|**2024-06-18**|[Structured Detection for Simultaneous Super-Resolution and Optical Sectioning in Laser Scanning Microscopy](http://arxiv.org/abs/2406.12542)|null|快速且灵敏的探测器阵列实现了图像扫描显微镜 (ISM)，克服了共聚焦显微镜中空间分辨率和信噪比 (SNR) 之间的典型权衡问题。然而，目前的 ISM 方法无法提供光学切片，并且在处理厚样本时会失败，除非限制探测器的尺寸。因此，光学切片和 SNR 之间的另一个权衡仍然存在。在这里，我们提出了一种没有缺点的方法，它结合了不折不扣的超分辨率、高信噪比和光学切片。此外，我们的方法可以对图像进行超采样，将奈奎斯特准则放宽了两倍。基于这样的观察，即使用探测器阵列成像本身就嵌入了样本的轴向信息，我们设计了一种简单的重建算法，可以反转 ISM 的物理模型。我们提出了全面的理论框架，并使用配备单光子雪崩二极管 (SPAD) 阵列探测器的定制装置捕获的生物样本的合成和实验图像验证了我们的方法。我们证明了我们的方法在激发线性和非线性状态下的荧光发射方面的可行性。此外，我们将算法推广到荧光寿命成像，充分利用了 SPAD 阵列探测器的单光子计时能力。我们的方法优于传统的 ISM 方法，并且可以扩展到任何 LSM 技术。||
|**2024-06-18**|[ViDSOD-100: A New Dataset and a Baseline Model for RGB-D Video Salient Object Detection](http://arxiv.org/abs/2406.12536)|**[link](https://github.com/jhl-det/rgbd_video_sod)**|随着深度传感器的快速发展，越来越多的RGB-D视频得以获取。识别RGB-D视频中的前景是一项基础且重要的任务。然而，现有的显著性目标检测（SOD）工作只关注静态RGB-D图像或RGB视频，忽略了RGB-D和视频信息的协同作用。在本文中，我们首先收集了一个新的带注释的RGB-D视频SOD（ViDSOD-100）数据集，该数据集包含100个视频，共计9,362帧，这些视频采集自不同的自然场景。每个视频中的所有帧都经过人工标注，具有高质量的显著性标注。此外，我们提出了一个新的基线模型，名为注意力三重融合网络（ATF-Net），用于RGB-D视频显著性目标检测。我们的方法通过设计三个模态特定分支和一个多模态融合分支来聚合来自输入RGB图像的外观信息、来自估计运动图的时空信息以及来自深度图的几何信息。模态特定分支提取不同输入的表示，而多模态融合分支通过引入编码器特征聚合（MEA）模块和解码器特征聚合（MDA）模块来组合多级模态特定特征。在我们新引入的ViDSOD-100数据集和完善的DAVSOD数据集上进行的实验结果突出了所提出的ATF-Net的优越性能。这种性能的提升在定量和定性上都得到了证明，超过了当前最先进技术在各个领域的能力，包括RGB-D显著性检测、视频显著性检测和视频目标分割。我们的数据和代码可在github.com/jhl-Det/RGBD_Video_SOD获取。||
|**2024-06-18**|[LFMamba: Light Field Image Super-Resolution with State Space Model](http://arxiv.org/abs/2406.12463)|null|近年来，由于现代神经网络的进步，光场图像超分辨率（LFSR）取得了显著进展。然而，这些方法在捕获远程依赖关系（基于CNN）或遇到二次计算复杂度（基于Transformer）方面经常面临挑战，这限制了它们的性能。最近，具有选择性扫描机制（S6）的状态空间模型（SSM），以Mamba为例，已成为各种视觉任务中优于传统基于CNN和基于Transformer的方法的替代方案，这得益于其有效的远程序列建模能力和线性时间复杂度。因此，将S6集成到LFSR变得很有吸引力，特别是考虑到4D光场的巨大数据量。然而，主要挑战在于\emph{为4D光场设计一种有效的扫描方法，以有效地对光场特征进行建模}。为了解决这个问题，我们在4D LF的信息丰富的2D切片上采用SSM，以充分探索空间上下文信息、互补的角度信息和结构信息。为此，我们精心设计了一个基本的SSM块，其特征是高效的SS2D机制，有助于在这些2D切片上进行更有效和高效的特征学习。基于以上两种设计，我们进一步介绍了一种用于LFSR的基于SSM的网络，称为LFMamba。在LF基准数据集上的实验结果证明了LFMamba的优越性能。此外，还进行了广泛的消融研究，以验证我们提出的方法的有效性和泛化能力。我们希望我们的LFMamba能够为利用状态空间模型进行LF的有效表示学习提供启示。||
|**2024-06-18**|[SDNIA-YOLO: A Robust Object Detection Model for Extreme Weather Conditions](http://arxiv.org/abs/2406.12395)|null|虽然当前基于深度学习的目标检测模型在许多传统基准数据集上取得了优异的结果，但它们在极端条件下拍摄的真实世界图像上的性能会显著下降。现有方法要么使用基于传统图像处理算法的图像增强，要么应用定制的、场景受限的图像自适应技术来进行鲁棒建模。因此，本研究提出了一种风格化数据驱动的图像自适应神经网络YOLO（SDNIA-YOLO），它通过自适应地提高图像质量和从神经风格迁移（NST）合成的图像中学习与极端天气条件相关的有价值信息来提高模型的鲁棒性。实验表明，与基线模型相比，所开发的SDNIA-YOLOv3在真实世界雾天（RTTS）和低光（ExDark）测试集上的mAP@.5显著提高了至少15%。此外，实验还突出了风格化数据在模拟极端天气条件方面的巨大潜力。所开发的SDNIA-YOLO在很大程度上保留了原生YOLO的优良特性，如端到端的一阶段、数据驱动和快速。||
|**2024-06-18**|[Competitive Learning for Achieving Content-specific Filters in Video Coding for Machines](http://arxiv.org/abs/2406.12367)|null|本文研究了联合优化特定于内容的后处理滤波器以将面向人类的视频/图像编解码器调整为适用于机器视觉任务的编解码器的功效。通过观察视频/图像编解码器产生的伪像是依赖于内容的，我们提出了一种基于竞争学习原理的新型训练策略。该策略以模糊的方式将训练样本动态分配给滤波器，从而进一步优化给定样本上的获胜滤波器。受模拟退火优化技术的启发，我们采用具有温度变量的 softmax 函数作为权重分配函数，以减轻随机初始化的影响。我们在一个在多功能视频编码 (VVC) 编解码器框架内利用多个后处理滤波器的系统上进行的评估表明，使用我们提出的策略训练的特定于内容的滤波器的优越性，特别是在图像分块处理时。使用 VVC 参考软件 VTM 12.0 作为锚点，在 OpenImages 数据集上进行的实验表明，与独立训练的滤波器相比，用于目标检测和实例分割任务的 BD 率降低分别从 -41.3% 和 -44.6% 提高到 -42.3% 和 -44.7%。滤波器使用统计数据与我们的假设一致，并强调了联合优化滤波器的内容和重建质量的重要性。我们的发现为进一步提高视频/图像编解码器的性能铺平了道路。||
|**2024-06-18**|[Certified ML Object Detection for Surveillance Missions](http://arxiv.org/abs/2406.12362)|null|本文介绍了一种无人机探测系统的开发过程，该系统包含一个机器学习目标检测组件。目的是达到可接受的性能目标，并根据ED 324 / ARP 6983标准（即将发布）的建议提供充分的证据，以增强对所设计系统可靠性的信心。||
|**2024-06-18**|[Unleashing the Potential of Open-set Noisy Samples Against Label Noise for Medical Image Classification](http://arxiv.org/abs/2406.12293)|null|在医学图像分类中，解决混合闭集和开集标签噪声的挑战在很大程度上仍未得到探索。与自然图像分类不同，自然图像分类通常将闭集和开集噪声样本与干净样本分开并分别处理，而医学图像分类由于类间相似性高，难以识别开集噪声样本，因此面临着困难。此外，现有方法没有充分利用开集噪声样本的潜力来减轻标签噪声，通常导致它们被排除在外或应用统一的软标签。为了解决这些问题，我们提出了一种扩展的噪声鲁棒对比和开集特征增强（ENCOFA）框架。ENCOFA 包括扩展的噪声鲁棒监督对比（ENSC）损失，它有助于区分不同类别的特征。ENSC 损失将开集噪声样本视为一个扩展类别，并通过用标签可靠性对对比对进行加权来减轻标签噪声。此外，我们开发了一个开集特征增强（OSFeatAug）模块，利用模型的额外容量来丰富开集样本的特征，以防止对噪声数据的过拟合。我们在一个合成噪声数据集和一个真实世界的噪声数据集上进行了实验。我们的结果表明 ENCOFA 的优越性以及利用开集噪声样本对抗标签噪声的有效性。||
|**2024-06-17**|[Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%](http://arxiv.org/abs/2406.11837)|**[link](https://github.com/zh460045050/vqgan-lc)**|在以 VQGAN 为代表的图像量化领域，图像被编码成从预定义大小的码本中提取的离散token。近期的研究进展，特别是 LLAMA 3，表明扩大码本可以显著提高模型性能。然而，VQGAN 及其衍生模型，如 VQGAN-FC（因子化码本）和 VQGAN-EMA，在扩大码本大小和提高码本利用率方面仍然面临挑战。例如，VQGAN-FC 只能学习最大大小为 16,384 的码本，在 ImageNet 上的利用率通常低于 12%。本文提出了一种新的图像量化模型，称为 VQGAN-LC（大型码本），它将码本大小扩展到 100,000，实现了超过 99% 的利用率。与之前优化每个码本条目的方法不同，我们的方法首先使用预训练的视觉编码器提取的 100,000 个特征初始化码本。然后，优化集中于训练一个投影器，使整个码本与 VQGAN-LC 中编码器的特征分布对齐。我们证明了我们的模型在各种任务上的性能优于现有模型，包括图像重建、图像分类、使用 GPT 的自回归图像生成，以及使用基于扩散和基于流的生成模型进行图像创作。代码和模型可在 https://github.com/zh460045050/VQGAN-LC 获取。||
|**2024-06-17**|[V3Det Challenge 2024 on Vast Vocabulary and Open Vocabulary Object Detection: Methods and Results](http://arxiv.org/abs/2406.11739)|null|在现实场景中检测物体是一项复杂的任务，因为它面临着各种挑战，包括物体类别的广泛性以及可能遇到以前未知或未见过的物体。 这些挑战使得开发公共基准和挑战成为必要，以推动物体检测领域的发展。 受之前COCO和LVIS挑战赛成功的启发，我们与第四届开放世界视觉研讨会：开放世界中的视觉感知学习（VPLOW）合作，在2024年美国西雅图举行的CVPR上组织了V3Det挑战赛2024。这项挑战赛旨在推动物体检测研究的边界，并鼓励该领域的创新。V3Det挑战赛2024包括两个赛道：1）海量词汇物体检测：该赛道侧重于从13204个类别的庞大集合中检测物体，测试检测算法识别和定位不同物体能力。 2）开放词汇物体检测：该赛道更进一步，要求算法从开放的类别集中检测物体，包括未知物体。 在接下来的部分中，我们将对参与者提交的解决方案进行全面总结和分析。 通过分析提出的方法和解决方案，我们旨在启发海量词汇和开放词汇物体检测的未来研究方向，推动该领域的进步。 挑战赛主页：https://v3det.openxlab.org.cn/challenge||
|**2024-06-17**|[YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection](http://arxiv.org/abs/2406.11641)|null|目前基于图像的无人机检测方法主要依赖于YOLOv5等通用目标检测算法。虽然这些算法能够有效地在均匀背景下识别无人机，但在复杂、纹理丰富的环境中往往表现不佳。在这种情况下，无人机可以无缝地融入背景，产生伪装效果，从而对检测质量产生负面影响。为了解决这个问题，我们提出了一种名为YOLO-FEDER FusionNet的新型深度学习架构。与传统方法不同，YOLO-FEDER FusionNet将通用目标检测方法与伪装目标检测技术的专业优势相结合，以增强无人机检测能力。对YOLO-FEDER FusionNet的全面评估表明，该模型效率高，并在减少漏检和误报方面都有显著改进。||
|**2024-06-17**|[Cross-domain Open-world Discovery](http://arxiv.org/abs/2406.11422)|**[link](https://github.com/mlbio-epfl/crow)**|在许多实际应用中，测试数据通常会出现类别偏移，其特点是出现新的类别，以及由于特征分布与模型训练时的特征分布不同而产生的分布偏移。然而，现有方法要么在开放世界环境中发现新类别，要么假设存在领域偏移但无法发现新类别。在这项工作中，我们考虑了一个跨领域开放世界发现设置，目标是在领域偏移的情况下将样本分配给已见类别并发现未见类别。为了解决这一挑战性问题，我们提出了 CROW，这是一种基于原型的方法，它引入了由基础模型结构良好的表示空间支持的“先聚类后匹配”策略。通过这种方式，CROW 通过将聚类与先前见过的类别进行稳健匹配来发现新类别，然后使用专为跨领域开放世界发现设计的目标函数对表示空间进行微调。在图像分类基准数据集上的大量实验结果表明，CROW 优于其他基线方法，在 75 个实验设置中平均性能提高了 8%。||
|**2024-06-17**|[A Dictionary Based Approach for Removing Out-of-Focus Blur](http://arxiv.org/abs/2406.11330)|**[link](https://github.com/aurangau/ICIP2024)**|随着深度学习模型的兴起，图像去模糊领域取得了巨大进步。这些模型虽然高效，但计算成本高，能源消耗大。基于字典学习的方法在图像去噪和单图像超分辨率方面已显示出良好的结果。我们提出将 Isidoro、Romano 和 Milanfar 提出的快速准确图像超分辨率 (RAISR) 算法扩展到散焦模糊去除任务。我们定义了一种清晰度质量度量，它与图像的感知质量非常吻合。还提出了一种基于资产配置管理的基于度量的混合策略。与流行的去模糊方法相比，我们的方法平均提高了约 13% (PSNR) 和 10% (SSIM)。此外，我们的混合方案减少了恢复后的振铃伪影。||
|**2024-06-17**|[Low-power Ship Detection in Satellite Images Using Neuromorphic Hardware](http://arxiv.org/abs/2406.11319)|null|将地球观测图像数据从卫星传输到地面站会消耗大量的电力和带宽。对于海上船舶检测，机载数据处理可以识别船舶并减少发送到地面的数据量。然而，大多数在轨捕获的图像只包含水体或陆地，空中客车船舶检测数据集显示只有 22.1% 的图像包含船舶。我们设计了一个低功耗的两阶段系统来优化性能，而不是依赖于单个复杂模型。第一阶段是一个轻量级的二元分类器，充当检测船舶存在的门控机制。此阶段运行在 BrainChip 的 Akida 1.0 上，它利用激活稀疏性来最大程度地减少动态功耗。第二阶段采用 YOLOv5 目标检测模型来识别船舶的位置和大小。这种方法实现了 76.9% 的平均精度 (mAP)，通过减少误报，仅在包含船舶的图像上评估时，该精度提高到 79.3%。此外，我们计算出在 NVIDIA Jetson Nano 设备上评估完整验证集需要 111.4 kJ 的能量。我们的两阶段系统将此能耗降低到 27.3 kJ，不到四分之一，证明了异构计算系统的效率。||
|**2024-06-17**|[Semi-Supervised Domain Adaptation Using Target-Oriented Domain Augmentation for 3D Object Detection](http://arxiv.org/abs/2406.11313)|**[link](https://github.com/rasd3/toda)**|三维目标检测对于自动驾驶和机器人等应用至关重要。然而，在现实环境中，由于传感器升级、天气变化和地理差异导致的传感器数据分布变化会对检测性能产生负面影响。半监督域适应（SSDA）旨在通过将知识从具有丰富标记数据的源域迁移到标记数据稀缺的目标域来应对这些挑战。本文提出了一种新的SSDA方法，称为面向目标的域增强（TODA），该方法专为基于激光雷达的三维目标检测而设计。TODA有效地利用了所有可用数据，包括源域中的标记数据以及目标域中的标记数据和未标记数据，以提高域适应性能。TODA由两个阶段组成：TargetMix和AdvMix。TargetMix采用混合增强技术，结合激光雷达传感器特性，以促进源域和目标域之间的特征对齐。AdvMix将逐点对抗性增强与混合增强相结合，扰动未标记数据，以对齐目标域中标记数据和未标记数据中的特征。我们在具有挑战性的域适应任务上进行的实验表明，TODA的性能明显优于现有的专为三维目标检测设计的域适应技术。代码可在以下网址获取：https://github.com/rasd3/TODA。||
|**2024-06-17**|[Syn-to-Real Unsupervised Domain Adaptation for Indoor 3D Object Detection](http://arxiv.org/abs/2406.11311)|null|在室内3D物体检测中使用合成数据，可以大大减少3D标注所需的人工，并训练出有效的零样本检测器。然而，跨合成到真实室内数据集的复杂域迁移问题仍未得到充分探索。本文提出了一种新的面向对象的层次化域对齐（OHDA）框架，用于室内3D物体检测的合成到真实无监督域自适应。我们的方法包括一种对象感知增强策略，以有效地使源域数据多样化，并且我们引入了一个由对抗训练分支和伪标签分支组成的双分支自适应框架，以便同时实现整体级别和类别级别的域对齐。针对室内无监督域自适应的特点，我们提出了两种方案来进一步优化伪标签。我们从合成数据集3D-FRONT到真实世界数据集ScanNetV2和SUN RGB-D的适应结果表明，与仅使用源数据的基线相比，mAP25分别显著提高了9.7%和9.1%，并且始终优于从2D和3D室外场景改编的方法。代码将在论文被接收后公开。||
|**2024-06-17**|[BaFTA: Backprop-Free Test-Time Adaptation For Zero-Shot Vision-Language Models](http://arxiv.org/abs/2406.11309)|null|像CLIP这样的大规模预训练视觉语言模型在不同领域展现出卓越的零样本图像分类能力。为了在保持零样本范式的情况下提高CLIP的性能，各种测试时提示调优方法被引入，通过无监督学习目标在推理过程中优化类别嵌入。然而，这些方法在选择合适的学习率以防止在测试时适应过程中缺乏验证数据导致的训练崩溃方面经常遇到挑战。在本研究中，我们提出了一种新的免反向传播算法BaFTA，用于视觉语言模型的测试时适应。我们的方法不是微调文本提示来优化类别嵌入，而是在对齐文本和视觉嵌入的投影嵌入空间内使用在线聚类直接估计类别中心点。我们通过使用Rényi熵评估每个预测的可靠性，动态聚合来自估计和原始类别嵌入以及不同增强视图的预测。通过大量实验，我们证明了BaFTA在有效性和效率方面始终优于最先进的测试时适应方法。||
|**2024-06-17**|[VideoVista: A Versatile Benchmark for Video Understanding and Reasoning](http://arxiv.org/abs/2406.11303)|null|尽管在大型多模态模型（LMM）的快速发展的推动下，视频分析取得了重大突破，但仍然缺乏一个通用的评估基准来全面评估这些模型在视频理解和推理方面的性能。为了解决这个问题，我们提出了 VideoVista，这是一个视频问答基准，它整合了跨越不同内容类别、持续时间和能力的挑战。具体来说，VideoVista 包含从 3,400 个视频中提取的 25,000 个问题，涵盖 14 个类别（例如，Howto、电影和娱乐），持续时间从几秒到超过 10 分钟不等。此外，它还包含 19 种理解任务（例如，异常检测、交互理解）和 8 种推理任务（例如，逻辑推理、因果推理）。为了实现这一点，我们提出了一个自动数据构建框架，利用强大的 GPT-4o 以及先进的分析工具（例如，视频分割、对象分割和跟踪）。我们还利用此框架构建训练数据，以增强与视频相关的 LMM（Video-LMM）的能力。通过对尖端模型进行全面和定量的评估，我们发现：1）Video-LMM 在涉及时间定位、对象跟踪和异常检测的细粒度视频任务中面临困难；2）Video-LMM 的逻辑和关系推理能力较差；3）开源 Video-LMM 的性能明显低于 GPT-4o 和 Gemini-1.5，落后 20 个百分点。这突出了 VideoVista 在推进能够准确理解视频并执行精确推理的 LMM 方面将发挥的关键作用。||

<p align=right>(<a href=#updated-on-20240622>back to top</a>)</p>

## 模型压缩/优化

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-20**|[Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs](http://arxiv.org/abs/2406.14282)|null|提高大型语言模型 (LLM) 在复杂问答 (QA) 场景中的性能一直是研究的焦点。 最近的研究试图通过将逐步规划与外部检索相结合来提高 LLM 的性能。 虽然这种方法对 GPT-3.5 等高级模型有效，但较小的 LLM 在分解复杂问题方面面临挑战，需要有监督的微调。 以前的工作依赖于手动注释和来自教师 LLM 的知识蒸馏，这些方法既耗时又不准确。 在本文中，我们介绍了一种新颖的框架，通过使用从知识图 (KG) 派生的规划数据来增强 LLM 的规划能力。 使用此数据进行微调的 LLM 具有改进的规划能力，使其能够更好地处理涉及检索的复杂问答任务。 对多个数据集（包括我们新提出的基准）的评估突出了我们框架的有效性和 KG 派生的规划数据的优势。||
|**2024-06-20**|[SeCoKD: Aligning Large Language Models for In-Context Learning with Fewer Shots](http://arxiv.org/abs/2406.14208)|null|先前研究表明，演示可以显著帮助大型语言模型（LLM）更好地执行给定任务。然而，这种所谓的上下文学习（ICL）能力对呈现的上下文非常敏感，通常需要数十个演示。在这项工作中，我们研究了是否可以在保持竞争性能的同时减少样本数量。我们提出了SeCoKD，一种自知识蒸馏（KD）训练框架，它将学生模型与经过大量提示的变体对齐，从而提高了单个演示的利用率。我们用SeCoKD在三个LLM和六个主要关注推理任务的基准上进行了实验。结果表明，我们的方法优于基础模型和监督微调（SFT），特别是在零样本和单样本设置中，分别提高了30%和10%。此外，SeCoKD在评估新任务时几乎不会带来负面影响，这比监督微调更加稳健。||
|**2024-06-20**|[Failure-Resilient Distributed Inference with Model Compression over Heterogeneous Edge Devices](http://arxiv.org/abs/2406.14185)|null|分布式推理范式允许计算工作负载分布在多个设备上，从而促进在资源极其受限的物联网 (IoT) 场景中实现基于深度学习的智能服务。然而，它对执行依赖于物联网设备集群的复杂推理任务提出了巨大的挑战，这些设备在计算/通信能力方面存在异构性，并且容易发生崩溃或超时故障。在本文中，我们提出了 RoCoIn，这是一种强大的协作推理机制，用于在异构边缘设备上本地分布式执行基于深度神经网络的推理任务。它创建了一组独立且紧凑的学生模型，这些模型是使用知识蒸馏从大型模型中学习而来的，用于分布式部署。特别是，对设备进行战略性分组以冗余部署和执行相同的学生模型，以便推理过程能够抵御任何本地故障，同时联合知识分区和学生模型分配方案旨在最大限度地减少存在不同容量设备的情况下分布式推理系统的响应延迟。进行了广泛的仿真以证实我们的 RoCoIn 与几个基线相比在分布式推理方面的优越性能，结果证明了其在及时推理和故障恢复方面的有效性。||
|**2024-06-19**|[Can Low-Rank Knowledge Distillation in LLMs be Useful for Microelectronic Reasoning?](http://arxiv.org/abs/2406.13808)|null|在这项工作中，我们提供了关于在电子设计自动化 (EDA) 中使用离线大型语言模型 (LLM) 的可行性的实证结果。目标是调查和评估当代语言模型 (Llama-2-7B) 作为微电子问答专家以及其在解决微电子相关问题中的推理和生成能力。Llama-2-7B 在各种适应方法中进行了测试，包括引入一种新颖的低秩知识蒸馏 (LoRA-KD) 方案。我们的实验产生了定性和定量结果。||
|**2024-06-19**|[BiLD: Bi-directional Logits Difference Loss for Large Language Model Distillation](http://arxiv.org/abs/2406.13555)|null|近年来，大型语言模型 (LLM) 在各种自然语言处理 (NLP) 任务中表现出卓越的能力。然而，这种令人印象深刻的性能通常伴随着参数规模的增加，这对广泛部署构成了重大挑战。知识蒸馏 (KD) 通过将知识从大型教师模型迁移到较小的学生模型来提供解决方案。在本文中，我们探讨了 LLM 在 logits 级别的任务特定蒸馏。我们的研究表明，经过微调的 LLM 的 logits 比视觉模型表现出更极端的“长尾”分布，长尾中的隐藏“噪声”会影响蒸馏性能。此外，现有的 logits 蒸馏方法通常难以有效利用 logits 的内部排序信息。为了解决这些问题，我们提出了双向 Logits 差异 (BiLD) 损失。BiLD 损失通过仅利用 top- $k$ 个教师和学生 logits 来滤除长尾噪声，并通过构建 logits 差异来利用内部 logits 排序信息。为了评估 BiLD 损失，我们使用两种类型的 LLM 在 13 个数据集上进行了全面的实验。我们的结果表明，仅使用 top-8 个 logits 的 BiLD 损失优于监督微调 (SFT)、vanilla KL 损失以及来自 NLP 和 CV 领域的五种其他蒸馏方法。||
|**2024-06-19**|[Towards Cyber Threat Intelligence for the IoT](http://arxiv.org/abs/2406.13543)|null|随着数字化在关键部门的普及和应用，在组织的威胁缓解策略中纳入有关网络威胁发生和评估的信息变得至关重要。这种网络威胁情报（CTI）对于国家和工业关键基础设施来说正变得越来越重要，或者说是必不可少。目前的CTI解决方案大多是联邦式的，不适合共享来自低功耗物联网设备的威胁信息。本文提出了对当今可用的CTI框架和CTI交换平台的分类和分析。它提出了一种新的CTI架构，该架构依赖于MISP威胁情报共享平台，并针对物联网环境进行了定制和重点关注。本文还介绍了一种定制版本的STIX（我们称之为tinySTIX），它是CTI数据建模采用的最突出标准之一，使用新的轻量级编码和加密解决方案针对低功耗物联网设备进行了优化。所提出的CTI架构将对保护物联网安全非常有利，尤其是在恶劣和对抗性环境中工作的物联网。||
|**2024-06-19**|[Improving Zero-Shot Cross-Lingual Transfer via Progressive Code-Switching](http://arxiv.org/abs/2406.13361)|null|代码切换是一种数据增强方案，它将来自多种语言的单词混合到源语言文本中。通过对齐跨语言上下文词表示，它在跨语言迁移任务中取得了相当可观的泛化性能。然而，不受控制和过度替换的代码切换会给模型训练增加脏样本。换句话说，过多的代码切换文本样本会对模型的跨语言迁移能力产生负面影响。为此，我们提出了一种渐进式代码切换 (PCS) 方法，逐渐生成难度适中的代码切换示例，供模型从易到难地进行区分。其想法是使用更容易的代码切换数据逐步整合先前学习到的多语言知识，以指导模型对后续更难的代码切换数据的优化。具体来说，我们首先设计了一个难度测量器，根据词语相关性分数来衡量替换句子中每个词语的影响。然后，代码切换器通过可控的温度变量生成难度递增的代码切换数据。此外，训练调度器决定何时对模型训练采样更难的代码切换数据。实验表明，我们的模型在跨越十种语言的三种不同的零样本跨语言迁移任务上取得了最先进的结果。||
|**2024-06-19**|[WaterMono: Teacher-Guided Anomaly Masking and Enhancement Boosting for Robust Underwater Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2406.13344)|**[link](https://github.com/oucvisiongroup/watermono)**|深度信息是陆地或水下各种视觉任务的关键先决条件。近年来，尽管没有深度注释，但自监督方法在多个陆地基准测试中取得了显著成果。然而，在更具挑战性的水下场景中，它们遇到了许多全新的障碍，例如海洋生物的影响和水下图像的退化，这些障碍分别打破了静态场景的假设并带来了低质量的图像。此外，水下图像的相机角度更加多样化。幸运的是，我们发现知识蒸馏为应对这些挑战提供了一种有前景的方法。在本文中，我们提出了 WaterMono，这是一种用于深度估计和图像增强的新型框架。它包含以下关键措施：(1) 我们提出了一种教师引导的异常掩码来识别图像中的动态区域；(2) 我们利用深度信息结合水下图像形成模型生成增强图像，进而有助于深度估计任务；(3) 我们利用旋转蒸馏策略来增强模型的旋转鲁棒性。综合实验表明，我们提出的方法对深度估计和图像增强都是有效的。源代码和预训练模型可在项目主页上找到：https://github.com/OUCVisionGroup/WaterMono。||
|**2024-06-19**|[PathoLM: Identifying pathogenicity from the DNA sequence through the Genome Foundation Model](http://arxiv.org/abs/2406.13133)|null|病原体鉴定对于疾病的诊断、治疗和预防至关重要，这对于控制感染和维护公众健康至关重要。传统的基于比对的方法虽然应用广泛，但计算量大，依赖于庞大的参考数据库，并且由于其敏感性和特异性较低，往往无法检测到新的病原体。同样，传统的机器学习技术虽然很有前景，但需要大量标注的数据集和广泛的特征工程，并且容易出现过拟合。为了应对这些挑战，我们推出了 PathoLM，这是一种尖端的病原体语言模型，经过优化，可以识别细菌和病毒序列中的致病性。PathoLM 利用了核苷酸转换器等预训练 DNA 模型的优势，只需要最少的数据进行微调，从而增强了病原体检测能力。它有效地捕捉了更广泛的基因组背景，显著提高了对新型和变异病原体的识别能力。我们开发了一个包含大约 30 种病毒和细菌的综合数据集，包括 ESKAPEE 病原体和七种对抗生素具有耐药性的著名剧毒细菌菌株。此外，我们还策划了一个专门针对 ESKAPEE 组的物种分类数据集。在比较评估中，PathoLM 的性能远远优于 DciPatho 等现有模型，表现出强大的零样本和小样本能力。此外，我们扩展了 PathoLM-Sp 用于 ESKAPEE 物种分类，尽管任务复杂，但与其他先进的深度学习方法相比，它仍表现出优异的性能。||
|**2024-06-19**|[Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation](http://arxiv.org/abs/2406.13114)|null|大型语言模型（LLM）在各种自然语言处理任务中取得了显著进展，但部署它们仍然计算成本高昂。知识蒸馏（KD）是一种很有前景的解决方案，可以将能力从较大的教师LLM转移到更紧凑的学生模型。特别是，序列级KD蒸馏的是基于推理过程的推理过程，而不仅仅是最终结果，在增强学生的推理能力方面显示出巨大潜力。然而，当前的方法在长尾数据分布下难以进行序列级KD，不利地影响了稀疏表示域的泛化能力。我们引入了多阶段平衡蒸馏（BalDistill）框架，该框架在固定的计算预算内迭代地平衡训练数据。通过动态选择代表性的头部域示例和合成尾部域示例，BalDistill在不同的长尾数据集上实现了最先进的性能，增强了蒸馏模型的效率和效果。||
|**2024-06-18**|[Vernacular? I Barely Know Her: Challenges with Style Control and Stereotyping](http://arxiv.org/abs/2406.12679)|null|大型语言模型 (LLM) 越来越多地应用于教育和学习领域。研究表明，控制语言风格以适应学习者的需求可以促进理解、促进包容并有助于知识蒸馏。为了了解当代 LLM 在风格控制方面的能力和局限性，我们评估了五种最先进的模型：GPT-3.5、GPT-4、GPT-4o、Llama-3 和 Mistral-instruct-7B，评估内容涵盖两项风格控制任务。我们观察到，在第一项任务中存在显著的不一致性，模型性能平均在小学五年级到八年级的阅读水平之间（针对一年级学生的任务），标准差高达 27.6。对于第二项任务，我们观察到性能有统计学意义的显著提高，从 0.02 提高到 0.26。然而，我们发现即使在参考文本中没有刻板印象的情况下，LLM 在执行任务时也经常生成文化上不敏感的内容。我们对结果进行了全面的分析和讨论。||
|**2024-06-18**|[Federated Learning with a Single Shared Image](http://arxiv.org/abs/2406.12658)|**[link](https://github.com/sunnysoni97/single_image_fl)**|联邦学习 (FL) 支持多台机器协同训练机器学习模型，而无需共享私人训练数据。然而，尤其对于异构模型，一个关键瓶颈仍然是如何将每个客户端模型获得的知识传递给服务器。一种流行的方法 FedDF 使用知识蒸馏来解决这个问题，它使用一个通用的共享数据集来交换预测结果。但是，在许多情况下，由于隐私问题，这样的数据集可能难以获取，而且客户端可能不允许存储大型共享数据集。为此，我们在本文中介绍了一种新方法，改进了这种知识蒸馏方法，仅依赖于客户端和服务器之间共享的单个图像。具体来说，我们提出了一种新颖的自适应数据集剪枝算法，该算法从单个图像生成的作物中选择信息量最大的作物。由此，我们证明了在有限的共享数据集预算下，使用单个图像进行知识蒸馏的联邦学习比使用多个单独图像效果更好。最后，我们扩展了我们的方法，通过结合非均匀蒸馏策略和服务器端的客户端模型镜像来允许训练异构客户端架构。||
|**2024-06-18**|[From Instance Training to Instruction Learning: Task Adapters Generation from Instructions](http://arxiv.org/abs/2406.12382)|**[link](https://github.com/Xnhyacinth/TAGI)**|大型语言模型 (LLM) 通过利用指令微调 (IFT) 获得了解决一般任务的能力。然而，IFT 仍然严重依赖于大量任务数据的实例训练，这极大地限制了 LLM 对现实世界场景的适应性，在这些场景中，标记的任务实例稀少，而更广泛的任务泛化能力至关重要。与 LLM 相反，人类获得技能和完成任务不仅仅是通过反复练习，而且是通过理解和遵循指导方针。本文致力于模拟人类学习以解决实例训练的 shortcomings，重点关注指令学习以增强跨任务泛化能力。在此背景下，我们介绍了从指令生成任务适配器 (TAGI)，它可以在给定任务指令的情况下，以参数生成的方式自动构建特定于任务的模型，而无需针对未见过的任务进行重新训练。具体来说，我们利用知识蒸馏来增强通过指令学习开发的 TAGI 与通过实例训练开发的任务特定模型之间的一致性，方法是在它们之间对齐标签、输出 logits 和适配器参数。TAGI 通过包含超网络预训练和微调的两阶段训练过程被赋予了跨任务泛化能力。我们在 Super-Natural Instructions 和 P3 数据集上评估了 TAGI。实验结果表明，TAGI 可以匹配甚至优于传统的元训练模型和其他超网络模型，同时显着降低计算需求。||
|**2024-06-18**|[Enhancing Single-Slice Segmentation with 3D-to-2D Unpaired Scan Distillation](http://arxiv.org/abs/2406.12254)|null|二维单层腹部计算机断层扫描 (CT) 能够以低辐射剂量评估体型和器官健康状况。然而，单层数据需要使用二维网络进行分割，但这些网络通常难以有效捕捉上下文信息。因此，即使在相同数据集上训练，三维网络通常也能获得更好的分割结果。在这项工作中，我们提出了一种新颖的三维到二维的知识蒸馏框架，利用预训练的三维模型来增强二维单层分割。具体来说，我们从三维表示中提取预测分布的中心，通过学习类内和类间相关性来指导二维学生模型。与需要相同数据输入的传统知识蒸馏方法不同，我们的方法采用未配对的、具有任意对比度的三维 CT 扫描来指导二维学生模型。在来自单层巴尔的摩老龄化纵向研究 (BLSA) 数据集的 707 名受试者上进行的实验表明，最先进的二维多器官分割方法可以受益于三维教师模型，在单层多器官分割中实现更高的性能。值得注意的是，我们的方法在低数据情况下表现出相当高的效率，即使仅使用 200 名训练受试者，其性能也优于使用所有可用训练受试者训练的模型。因此，这项工作强调了减轻手动标注负担的潜力。||
|**2024-06-18**|[Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs for Information Retrieval](http://arxiv.org/abs/2406.12169)|null|最近的研究探索了从大型语言模型 (LLM) 中提取知识以优化检索模型，尤其是在检索增强生成 (RAG) 框架内。然而，大多数现有的训练方法依赖于从 LLM 的权重或输出概率中提取监督信号，这不仅资源密集型，而且与黑盒 LLM 不兼容。在本文中，我们介绍了“中间蒸馏”，这是一种数据高效的知识蒸馏训练方案，它将 LLM 视为黑盒，并通过创新的 LLM-排序器-检索器管道提取其知识，仅使用 LLM 的排名生成作为监督信号。大量实验表明，我们提出的方法可以显著提高检索模型的性能，只需 1,000 个训练实例。此外，我们蒸馏的检索模型显著提高了 RAG 框架内问答任务的性能，证明了 LLM 在经济高效地训练小型模型方面的潜力。||
|**2024-06-17**|[Mutual Learning for Finetuning Click-Through Rate Prediction Models](http://arxiv.org/abs/2406.12087)|null|点击率 (CTR) 预测已成为数字行业（如数字广告或在线购物）中的一项基本任务。许多基于深度学习的方法已被实施，并已成为该领域的最新模型。为了进一步提高 CTR 模型的性能，基于知识蒸馏的方法已被广泛使用。然而，目前大多数 CTR 预测模型都没有很复杂的架构，因此很难称其中一个模型“笨重”，而另一个模型“微小”。另一方面，复杂模型和简单模型之间的性能差距也不是很大。因此，将知识从一个模型提炼到另一个模型可能不值得付出努力。在这些考虑下，相互学习可能是一种更好的方法，因为所有模型都可以相互改进。在本文中，我们展示了相互学习算法在平等模型之间使用时的效果。在我们对 Criteo 和 Avazu 数据集的实验中，相互学习算法将模型的性能提高了高达 0.66% 的相对改进。||
|**2024-06-17**|[On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning](http://arxiv.org/abs/2406.11823)|**[link](https://github.com/naver-ai/elva)**|近年来，语言和视觉助手的进步展现出令人印象深刻的能力，但缺乏透明度，限制了更广泛的研究和可重复性。虽然开源模型可以有效地处理一般的图像任务，但它们在处理复杂的视觉定位文本理解任务时面临着计算量大的挑战。这类任务通常需要增加token输入和大型视觉模块来利用高分辨率信息。如何在模型规模和数据重要性之间取得平衡仍然是一个悬而未决的问题。本研究旨在通过识别关键组件并创建具有受限推理成本的高效模型来重新定义视觉语言模型的设计。通过策略性地制定数据集、优化视觉模块和增强监督技术，我们在保持高性能的同时，显著提高了推理吞吐量。从1.6亿到130亿参数不等的模型的广泛实验为模型优化提供了见解。我们将在https://github.com/naver-ai/elva 上完全开源我们的代码库、模型和数据集。||
|**2024-06-17**|[NLDF: Neural Light Dynamic Fields for Efficient 3D Talking Head Generation](http://arxiv.org/abs/2406.11259)|null|基于神经辐射场模型的说话头生成已经展现出良好的视觉效果。然而，由于NeRF需要对数百个采样点进行繁重的计算才能合成一个像素，其渲染速度缓慢严重限制了其应用。为了解决这一问题，本文提出了一种新的神经光动态场模型（NLDF），旨在实现高质量的三维说话人脸生成，并显著提高生成速度。NLDF基于光段表示光场，并使用深度网络一次性学习整个光束的信息。在学习过程中，应用了知识蒸馏技术，并使用基于NeRF的合成结果来指导NLDF中光段的正确着色。此外，本文还提出了一种新的主动池训练策略，以关注高频运动，特别是说话者的嘴巴和眉毛。该方法有效地表示了三维说话视频生成中的面部光线动态，与基于NeRF的最先进方法相比，其速度提高了约30倍，且生成视觉质量相当。||
|**2024-06-17**|[ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking](http://arxiv.org/abs/2406.11257)|**[link](https://github.com/gaffey/excp)**|大型语言模型（LLM）最近在人工智能领域引起了广泛关注。然而，这些模型的训练过程在计算和存储容量方面提出了重大挑战，因此压缩检查点已成为一个迫切问题。在本文中，我们提出了一种新颖的极限检查点压缩（ExCP）框架，该框架可显著减少训练检查点所需的存储空间，同时实现近乎无损的性能。我们首先计算相邻检查点的残差，以获得对更高压缩比至关重要的稀疏信息。为了进一步挖掘检查点中的冗余参数，我们提出了一种权重-动量联合缩减方法，以利用模型优化过程中的另一个重要信息，即动量。具体来说，我们利用模型和优化器的信息来尽可能多地丢弃参数，同时保留关键信息以确保最佳性能。此外，我们利用非均匀量化来进一步压缩检查点的存储空间。我们对从4.1亿到70亿参数不等的多个模型广泛评估了我们提出的ExCP框架，并展示了在保持强大性能的同时显著减少了存储空间。例如，我们对Pythia-410M模型实现了大约70倍的压缩，最终性能在各种下游任务上与原始模型一样准确。代码将在https://github.com/Gaffey/ExCP上提供。||
|**2024-06-17**|[STEVE Series: Step-by-Step Construction of Agent Systems in Minecraft](http://arxiv.org/abs/2406.11247)|null|以大型语言模型 (LLM) 为核心构建具身智能体系统是一个很有前景的方向。由于在现实世界中部署和训练此类智能体的巨大成本和不可控因素，我们决定在 Minecraft 环境中开始探索。我们的 STEVE 系列智能体可以在虚拟环境中完成基本任务以及更具挑战性的任务，例如导航甚至创造性任务，其效率远超以往最先进的方法 2.5 到 7.3 倍。我们首先使用一个原始的大型语言模型，为其增强了视觉编码器和在我们收集的高质量数据集 STEVE-21K 上训练的动作代码库。随后，我们使用 Critic 和记忆模块对其进行增强，将其转变为一个复杂的系统。最后，我们构建了一个分层的多个智能体系统。我们最近的工作探索了如何通过知识蒸馏来简化智能体系统。未来，我们将探索 STEVE 智能体在现实世界中的更多潜在应用。||
|**2024-06-16**|[Outlier Reduction with Gated Attention for Improved Post-training Quantization in Large Sequence-to-sequence Speech Foundation Models](http://arxiv.org/abs/2406.11022)|null|本文探讨了在 Whisper 语音基础模型系列中，知识蒸馏后训练后量化 (PTQ) 的改进。我们解决了权重和激活张量中异常值的问题，众所周知，这些异常值会影响基于 Transformer 的语言和视觉模型的量化质量。将这一观察结果扩展到 Whisper，我们证明了当基于 Transformer 的模型被训练用于执行自动语音识别时，这些异常值也存在，因此需要针对 PTQ 采取缓解策略。我们表明，最近提出的学生模型注意力块中的门控机制可以减少异常值，从而实现有效的 8 位量化，并且与没有采用门控机制的学生模型相比，可以降低词错误率。||
|**2024-06-16**|[Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions](http://arxiv.org/abs/2406.10861)|null|联邦学习 (FL) 是一种分布式且保护隐私的机器学习范式，它协调多个客户端训练模型，同时将原始数据保存在本地。然而，这种传统的联邦学习带来了一些挑战，包括隐私风险、数据异构性、通信瓶颈和系统异构性问题。为了应对这些挑战，知识蒸馏 (KD) 自 2020 年以来已广泛应用于联邦学习。知识蒸馏是一种经过验证且有效的模型压缩和增强算法。知识蒸馏的核心概念是通过在中间层或输出层交换 logits 来促进模型之间的知识转移。这些特性使知识蒸馏成为解决联邦学习中长期挑战的绝佳方案。迄今为止，很少有综述总结和分析知识蒸馏如何有效应用于联邦学习的当前趋势和方法。本文旨在全面概述基于知识蒸馏的联邦学习，重点关注解决上述挑战。首先，我们概述了基于知识蒸馏的联邦学习，包括其动机、基础知识、分类以及与传统联邦学习的比较以及知识蒸馏应该在哪里执行。我们还在附录中分析了基于知识蒸馏的联邦学习中的关键因素，包括教师、知识、数据和方法。我们讨论了知识蒸馏如何应对联邦学习中的挑战，包括隐私保护、数据异构性、通信效率和个性化。最后，我们讨论了基于知识蒸馏的联邦学习算法面临的挑战和未来的研究方向。我们希望本次调查能够为联邦学习领域的研究人员和从业者提供见解和指导。||
|**2024-06-14**|[Self-Knowledge Distillation for Learning Ambiguity](http://arxiv.org/abs/2406.09719)|null|近期的语言模型在自然语言理解（NLU）任务中展现出卓越的性能。然而，当面对可以多重解读的歧义样本时，它们往往表现不佳，过度自信地预测单一标签而未考虑其正确性。为了解决这个问题，我们提出了一种新颖的自我知识蒸馏方法，通过利用从模型底层蒸馏出的知识，使模型能够更准确地学习标签分布。这种方法还包括一个学习阶段，根据蒸馏出的分布知识，重新校准被判定为极其模糊的训练样本的不必要增强的置信度。我们在不同的 NLU 基准数据集上验证了我们的方法，实验结果证明了其在生成更好的标签分布方面的有效性。特别是，通过重新校准高度模糊样本的置信度的过程，显著缓解了未见样本的预测与其真实标签不匹配时的过度自信问题。这已被证明有助于生成比现有最先进方法更好的分布。此外，与现有方法相比，我们的方法在训练模型方面效率更高，因为它不涉及额外的训练过程来优化标签分布。||
|**2024-06-14**|[Frequency-mix Knowledge Distillation for Fake Speech Detection](http://arxiv.org/abs/2406.09664)|null|在电话场景中，用于对抗语音欺骗攻击的假语音检测 (FSD) 任务极具挑战性。数据增强 (DA) 方法被认为是解决电话场景中 FSD 任务的有效手段，通常分为时域和频域两个阶段。虽然每种方法都有其优势，但都可能导致信息丢失。为了解决这个问题，我们提出了一种新的数据增强方法，即频率混合 (Freqmix)，并引入了 Freqmix 知识蒸馏 (FKD) 来增强模型的信息提取和泛化能力。具体来说，我们使用 Freqmix 增强的数据作为教师模型的输入，而学生模型的输入则经过时域数据增强方法处理。我们使用多级特征蒸馏方法来恢复信息并提高模型的泛化能力。我们的方法在 ASVspoof 2021 LA 数据集上取得了最先进的结果，与基线相比提高了 31%，并在 ASVspoof 2021 DF 数据集上表现出竞争力。||
|**2024-06-13**|[RobustSAM: Segment Anything Robustly on Degraded Images](http://arxiv.org/abs/2406.09627)|null|Segment Anything Model (SAM)作为一种变革性的图像分割方法已经出现，它以其强大的零样本分割能力和灵活的提示系统而备受赞誉。然而，它的性能在处理低质量图像时会受到挑战。为了解决这一局限性，我们提出了Robust Segment Anything Model (RobustSAM)，它在保留SAM的可提示性和零样本泛化能力的同时，增强了其在低质量图像上的性能。我们的方法利用了预训练的SAM模型，仅增加了少量参数和计算需求。RobustSAM的额外参数可以在8个GPU上用30小时内完成优化，这证明了其对于典型研究实验室的可行性和实用性。我们还介绍了Robust-Seg数据集，这是一个包含688K图像-掩码对的集合，这些图像-掩码对具有不同的退化程度，旨在优化我们模型的训练和评估。跨多个分割任务和数据集的大量实验结果证实了RobustSAM的优越性能，特别是在零样本条件下，突出了其在广泛现实应用中的潜力。此外，我们的方法已被证明可以有效提高基于SAM的下游任务（如单图像去雾和去模糊）的性能。||
|**2024-06-13**|[Contextual Distillation Model for Diversified Recommendation](http://arxiv.org/abs/2406.09021)|null|推荐的多样性与准确性在改善用户体验方面同样重要。现有研究，例如行列式点过程（DPP）和最大边缘相关性（MMR），采用贪婪范式来迭代地选择同时优化准确性和多样性的项目。然而，先前的方法通常表现出二次复杂度，将其应用限制在重排序阶段，并且不适用于具有更大候选项目池的其他推荐阶段，例如预排序和排序阶段。在本文中，我们提出了上下文蒸馏模型（CDM），这是一种解决多样化的有效推荐模型，适用于工业推荐流程的所有阶段的部署。具体来说，CDM利用同一用户请求中的候选项目作为上下文来增强结果的多样性。我们提出了一种对比上下文编码器，它采用注意力机制来对正面和负面上下文进行建模。对于CDM的训练，我们将每个目标项目与其上下文嵌入进行比较，并利用知识蒸馏框架来学习MMR算法下每个目标项目的获胜概率，其中教师来自MMR输出。在推理过程中，排名是通过推荐模型得分和学生模型得分的线性组合来执行的，从而确保了多样性和效率。我们在两个工业数据集上执行离线评估，并在短视频平台快手上进行CDM的在线A/B测试。正如指标所示，在推荐质量和多样性方面观察到的显着增强，为CDM的有效性提供了强大的优势。||

<p align=right>(<a href=#updated-on-20240622>back to top</a>)</p>

## OCR

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-20**|[Online Matching and Contention Resolution for Edge Arrivals with Vanishing Probabilities](http://arxiv.org/abs/2406.14506)|null|我们研究了在具有消失边概率的随机图上，序列竞争解决和匹配算法的性能。当图的边按照对抗性选择的顺序处理时，我们推导出一个新的OCRS，它是0.382可选的，在消失边概率假设下达到了文献中的“独立性基准”。作为对这一积极结果的补充，我们证明了没有任何OCRS可以超过0.390可选，这显著改进了文献中0.428的上界。我们还推导出专门针对二部图或OCRS子族的负面结果。同时，当图的边以均匀随机顺序处理时，我们证明了接受所有活动和可行边的简单贪婪竞争解决方案是1/2可选的。由于已知的上界，该结果是紧的。最后，当算法可以选择处理顺序时，我们证明对随机顺序稍作调整——给每个顶点一个随机优先级并按字典顺序处理边——会产生一个严格更好的竞争解决方案，它是1-ln(2-1/e)≈0.510可选的。我们的积极结果也适用于具有消失（非相同）边概率的1-均匀随机图上的在线匹配，扩展和统一了随机图文献中的一些结果。||
|**2024-06-19**|[GUI Action Narrator: Where and When Did That Action Take Place?](http://arxiv.org/abs/2406.13719)|null|多模态大型语言模型 (LLM) 的出现显著增强了图像 OCR 识别能力，使得 GUI 自动化成为提高数字化任务效率的可行现实。开发 GUI 自动化系统的一个基本方面是理解基本的 GUI 操作。这种理解至关重要，因为它使代理能够从用户演示中学习，这是自动化的一个基本要素。为了严格评估这些能力，我们开发了一个用于 GUI 操作的视频字幕基准，其中包含 4,189 个不同的视频字幕样本。与自然场景视频字幕相比，这项任务提出了独特的挑战：1) GUI 屏幕截图通常包含比自然场景更密集的信息，以及 2) GUI 内的事件更微妙且发生得更快，需要精确关注适当的时间跨度和空间区域才能准确理解。为了应对这些挑战，我们引入了 GUI 操作数据集 Act2Cap 以及一个简单而有效的框架 GUI Narrator，用于 GUI 视频字幕，它利用光标作为视觉提示来增强对高分辨率屏幕截图的解释。具体来说，在我们的数据集上训练光标检测器，并且具有用于选择关键帧和关键区域的机制的多模态 LLM 模型生成字幕。实验结果表明，即使对于当今最先进的多模态模型（例如 GPT-4o），这项任务仍然极具挑战性。此外，我们的评估表明，无论是在开源模型的微调中集成还是在闭源模型中用作提示策略，我们的策略都能有效地提高模型性能。||
|**2024-06-18**|[Sample-Based Matroid Prophet Inequalities](http://arxiv.org/abs/2406.12799)|null|我们研究了在分布未知且只能通过样本访问的情况下拟阵先知不等式问题。虽然针对特殊拟阵的单样本先知不等式是已知的，但对于一般拟阵，即使使用亚线性数量的样本，也没有已知的具有恒定竞争比的算法。更重要的是，一般拟阵的单样本版本问题与长期存在的拟阵秘书猜想有着密切的（双向）联系。在这项工作中，我们给出了一种仅需 $O_\varepsilon(\mathrm{poly} \log n)$个样本的$(\frac14 - \varepsilon)$竞争比拟阵先知不等式。我们的算法由两部分组成：（i）一种新的基于分位数的归约方法，使用$O_\varepsilon(\log n)$个样本将拟阵先知不等式问题归约为在线竞争解决方案（OCRS）问题，以及（ii）一种$(\frac14 - \varepsilon)$可选的拟阵OCRS，使用$O_\varepsilon(\mathrm{poly} \log n)$ 个样本，它仔细地解决了自适应性挑战。||
|**2024-06-17**|[GUICourse: From General Vision Language Models to Versatile GUI Agents](http://arxiv.org/abs/2406.11317)|**[link](https://github.com/yiye3/guicourse)**|利用图形用户界面（GUI）进行人机交互对于访问各种数字工具至关重要。视觉语言模型（VLM）的最新进展突出了开发多功能代理以帮助人类完成 GUI 导航任务的巨大潜力。然而，当前的 VLM 在基本能力（OCR 和 grounding）和 GUI 知识（GUI 元素的功能和控制方法）方面面临挑战，这阻碍了它们成为实用的 GUI 代理。为了解决这些挑战，我们贡献了 GUICourse，这是一套用于从通用 VLM 训练基于视觉的 GUI 代理的数据集。首先，我们介绍了 GUIEnv 数据集，以增强 VLM 的 OCR 和 grounding 能力。然后，我们介绍了 GUIAct 和 GUIChat 数据集，以丰富它们对 GUI 组件和交互的知识。实验表明，我们的 GUI 代理在常见的 GUI 任务上比其基线 VLM 具有更好的性能。即使是小型的 GUI 代理（具有 3.1B 参数）仍然可以在单步和多步 GUI 任务上良好运行。最后，我们通过消融研究分析了该代理训练阶段的不同变体。我们的源代码和数据集已发布在 https://github.com/yiye3/GUICourse。||
|**2024-06-17**|[Unifying Multimodal Retrieval via Document Screenshot Embedding](http://arxiv.org/abs/2406.11251)|null|在现实世界中，文档以不同的格式和多种形式组织。传统的检索流程需要定制化的文档解析技术和内容提取模块来准备索引的输入。这个过程繁琐且容易出错，并且会导致信息丢失。为此，我们提出了文档截图嵌入（DSE），这是一种新的检索范式，将文档截图视为统一的输入格式，它不需要任何内容提取预处理，并保留了文档中的所有信息（例如，文本、图像和布局）。DSE利用大型视觉语言模型将文档截图直接编码为密集表示以进行检索。为了评估我们的方法，我们首先制作了Wiki-SS数据集，这是一个包含130万个维基百科网页截图的语料库，用于回答自然问题数据集中的问题。在这种文本密集型文档检索环境中，与其他依赖解析的文本检索方法相比，DSE展现出具有竞争力的效果。例如，在top-1检索精度方面，DSE比BM25高出17个百分点。此外，在幻灯片检索的混合模态任务中，DSE在nDCG@10指标上明显优于OCR文本检索方法，超过15个百分点。这些实验表明，DSE是一种有效的文档检索范式，适用于各种类型的文档。模型检查点、代码和Wiki-SS数据集将被发布。||
|**2024-06-14**|[Enhancing Question Answering on Charts Through Effective Pre-training Tasks](http://arxiv.org/abs/2406.10085)|null|为了完全理解一个文档，仅使用文本信息是不够的。理解视觉线索，例如布局和图表，也是必要的。尽管目前最先进的文档理解方法（基于 OCR 和无 OCR 的方法）都很有效，但尚未对其功能和局限性进行全面分析。因此，在这项工作中，我们解决了当前视觉问答模型应用于图表时存在的局限性。为了调查最先进模型的缺陷，我们以 ChartQA 为例进行了全面的行为分析。我们的研究结果表明，现有模型在回答与图表的结构和视觉上下文以及数值信息相关的问题时表现 particularly 不佳。为了解决这些问题，我们提出了三个简单的预训练任务，这些任务从结构-视觉知识及其对数值问题的理解两方面加强了现有模型。我们在三个图表数据集（包括提取性和抽象性问题数据集）上评估了我们预训练的模型（称为 MatCha-v2），并观察到它比基线模型平均提高了 1.7%。||
|**2024-06-14**|[OSPC: Detecting Harmful Memes with Large Language Model as a Catalyst](http://arxiv.org/abs/2406.09779)|null|作为一种在互联网上快速传播个人观点和立场的媒介，迷因也给传播社会偏见和歧视带来了重大挑战。本研究提出了一种新的方法来检测有害迷因，特别是在新加坡的多元文化和多语言环境中。我们的方法集成了图像描述、光学字符识别 (OCR) 和大型语言模型 (LLM) 分析，以全面理解和分类有害迷因。该系统利用 BLIP 模型进行图像描述，利用 PP-OCR 和 TrOCR 进行多种语言的文本识别，并利用 Qwen LLM 进行细化的语言理解，能够识别以英语、中文、马来语和泰米尔语创建的迷因中的有害内容。为了提高系统性能，我们利用 GPT-4V 标注的额外数据对方法进行了微调，旨在将 GPT-4V 对有害迷因的理解能力提炼到我们的系统中。我们的框架在由新加坡人工智能举办的网络安全奖挑战赛的公开排行榜上名列第一，AUROC 为 0.7749，准确率为 0.7087，远远领先于其他团队。值得注意的是，我们的方法优于之前的基准，FLAVA 的 AUROC 为 0.5695，VisualBERT 的 AUROC 为 0.5561。||
|**2024-06-12**|[M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation](http://arxiv.org/abs/2406.08255)|**[link](https://github.com/amazon-science/m3t-multi-modal-translation-bench)**|对于神经机器翻译 (NMT) 系统而言，文档翻译是一项挑战。大多数文档级 NMT 系统依赖于精心策划的句子级平行数据，假设可以完美地从文档中提取文本及其精确的阅读顺序。这些系统也倾向于忽略额外的视觉线索（如文档布局），认为它们无关紧要。然而，现实世界的文档通常具有复杂的文本布局，这与这些假设相悖。从光学字符识别 (OCR) 或启发式规则中提取信息可能会导致错误，并且布局（例如，段落、标题）可能会传达文本中相距较远的部分之间的关系。这种复杂性在广泛使用的 PDF 文档中尤为明显，这些文档以视觉方式呈现信息。本文通过引入 M3T 来解决这一差距，M3T 是一个新颖的基准数据集，专为评估 NMT 系统在翻译半结构化文档的综合任务方面的性能而定制。该数据集旨在弥合文档级 NMT 系统中的评估差距，承认现实应用程序中丰富的文本布局带来的挑战。||
|**2024-06-10**|[VCR: Visual Caption Restoration](http://arxiv.org/abs/2406.06462)|**[link](https://github.com/tianyu-z/vcr)**|我们引入了一种新的视觉语言任务——视觉字幕修复 (VCR)，该任务挑战模型使用图像中的像素级提示准确地恢复部分遮挡的文本。这项任务源于这样一种观察，即嵌入在图像中的文本与常见的视觉元素和自然语言本质上不同，因为它需要对视觉、文本和嵌入在图像中的文本进行模态对齐。虽然许多工作已将嵌入在图像中的文本集成到视觉问答任务中，但这些任务的方法通常依赖于光学字符识别或掩码语言建模，从而将任务简化为主要基于文本的处理。然而，基于文本的处理在 VCR 中变得无效，因为准确的文本恢复依赖于来自提供的图像、上下文和来自掩码文本的微小暴露区域的微妙线索的组合信息。我们开发了一个管道，使用图像-字幕对生成 VCR 任务的合成图像，并可调节字幕可见性以控制任务难度。利用该管道，我们使用来自维基百科的带有字幕的图像构建了一个名为 VCR-Wiki 的 VCR 数据集，该数据集包含 211 万个英文实体和 34.6 万个中文实体，分为简单和困难两种变体。我们的结果表明，当前的视觉语言模型在 VCR 任务中的表现明显落后于人类，并且仅在我们数据集上微调模型不会带来显着的改进。我们发布了 VCR-Wiki 和数据构建代码，以促进未来的研究。||
|**2024-06-07**|[Scaling Automatic Extraction of Pseudocode](http://arxiv.org/abs/2406.04635)|null|学术论文中的伪代码提供了一种表达其中算法的简洁方法。伪代码也可以被认为是一种中间表示，有助于弥合编程语言和自然语言之间的差距。访问大量伪代码集合可以带来各种好处，从增强算法理解、促进进一步的算法设计，到支持基于 NLP 或计算机视觉的模型，用于自动代码生成和光学字符识别 (OCR) 等任务。我们通过从 arXiv 论文中提取近 320,000 个伪代码示例，创建了一个大型伪代码集合。这个过程涉及扫描超过 220 万篇学术论文，其中 1,000 篇经过人工检查和标记。鉴于集合固有的异质性，我们的方法包括一个为优化覆盖范围而定制的提取机制，以及一个基于随机抽样的验证机制，以检查其准确性和可靠性。此外，我们还提供了对常见伪代码结构的见解，并辅以聚类和统计分析。值得注意的是，这些分析表明伪代码的使用呈指数级增长，突出了它们日益增长的重要性。||
|**2024-06-06**|[CORU: Comprehensive Post-OCR Parsing and Receipt Understanding Dataset](http://arxiv.org/abs/2406.04493)|**[link](https://github.com/update-for-integrated-business-ai/coru)**|在光学字符识别 (OCR) 和自然语言处理 (NLP) 领域，集成多语言功能仍然是一项关键挑战，尤其是在考虑阿拉伯语等复杂文字时。本文介绍了综合性 OCR 后解析和收据理解数据集 (CORU)，这是一个专门设计用于增强多语言环境（包括阿拉伯语和英语）下收据的 OCR 和信息提取的新数据集。CORU 包含来自不同零售环境（包括超市和服装店）的 20,000 多张带注释的收据，以及 30,000 张用于 OCR 的带注释的图像，这些图像用于识别检测到的每一行，以及 10,000 个为详细信息提取而注释的项目。这些注释捕获了基本细节，例如商家名称、商品描述、总价、收据编号和日期。它们的结构支持三个主要的计算任务：目标检测、OCR 和信息提取。我们在 CORU 上建立了一系列模型的基线性能，以评估传统方法（如 Tesseract OCR）和更先进的基于神经网络的方法的有效性。这些基线对于处理现实世界收据中常见的复杂且嘈杂的文档布局以及推进自动多语言文档处理的现状至关重要。我们的数据集可公开访问 (https://github.com/Update-For-Integrated-Business-AI/CORU)。||
|**2024-06-03**|[Generalized Jersey Number Recognition Using Multi-task Learning With Orientation-guided Weight Refinement](http://arxiv.org/abs/2406.01033)|null|球衣号码识别 (JNR) 一直是体育分析中的一项重要任务。由于图像存在模糊、遮挡、变形和低分辨率等问题，提高识别精度仍然是一项持续的挑战。最近的研究已经使用数字定位和光学字符识别来解决这些问题。一些方法将球员识别方案应用于图像序列，而忽略了人体旋转角度对球衣数字识别的影响。通过使用多任务方案来识别每个数字，可以更准确地预测球衣数字的数量，从而获得更可靠的结果。基于上述考虑，本文提出了一种称为角度数字细化方案 (ADRS) 的多任务学习方法，该方法结合人体方向角度和数字线索来识别运动球衣号码。根据我们的实验结果，我们的方法增加了推理信息，显着提高了预测精度。与只能处理单一类型运动的现有技术方法相比，所提出的方法产生了更加多样化和实用的 JNR 应用。将足球、橄榄球、篮球、排球和棒球等多种类型的团队运动纳入我们的数据集中，极大地促进了体育分析中通用的 JNR。我们的准确率在 Top-1 上达到了 64.07%，在 Top-2 上达到了 89.97%，相应的 F1 分数分别为 67.46% 和 90.64%。||
|**2024-05-30**|[Scaling up archival text analysis with the blockmodeling of n-gram networks -- A case study of Bulgaria's representation in the Osservatore Romano (January-May 1877)](http://arxiv.org/abs/2405.20156)|null|本文旨在通过应用网络聚类方法分析 1877 年 1 月至 5 月期间出版的 123 期《罗马观察报》中对保加利亚的报道，从而弥合档案文本分析与网络分析之间的差距。本研究利用光学字符识别和广义同质性块模型构建相关关键词网络。包括“保加利亚”和“俄罗斯”这两个词集在内的网络结构基本相同，并且与“德国”、“英国”和“战争”的网络结构有很大程度的重叠。从结构上看，这两个网络的块模型呈现出清晰的核心-半边缘-边缘结构，反映了报纸报道中各概念之间的关系。该报的词汇选择有效地消解了保加利亚民族复兴运动的合法性，突出了罗马教廷对该报编辑路线的影响。||

<p align=right>(<a href=#updated-on-20240622>back to top</a>)</p>

## 生成模型

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-20**|[A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models](http://arxiv.org/abs/2406.14555)|**[link](https://github.com/xinchengshuai/awesome-image-editing)**|图像编辑旨在编辑给定的合成图像或真实图像，以满足用户的特定需求。近年来，作为人工智能生成内容（AIGC）领域中一个充满希望和挑战性的方向，图像编辑得到了广泛的研究。该领域最近取得的重大进展是基于文本到图像 (T2I) 扩散模型的发展，该模型根据文本提示生成图像。这些模型展示了卓越的生成能力，并已成为广泛使用的图像编辑工具。基于 T2I 的图像编辑方法显著提高了编辑性能，并为修改多模态输入引导的内容提供了用户友好的界面。在本次调查中，我们全面回顾了利用 T2I 扩散模型的多模态引导图像编辑技术。首先，我们从整体角度定义图像编辑的范围，并详细介绍各种控制信号和编辑场景。然后，我们提出了一个统一的框架来形式化编辑过程，将其分为两个主要的算法系列。该框架为用户提供了一个设计空间来实现特定目标。随后，我们对该框架内的每个组件进行了深入分析，检查了不同组合的特性和适用场景。鉴于基于训练的方法学会了在用户指导下将源图像直接映射到目标图像，我们分别讨论了它们，并介绍了在不同场景下源图像的注入方案。此外，我们回顾了二维技术在视频编辑中的应用，重点介绍了帧间不一致的解决方案。最后，我们讨论了该领域的开放挑战，并提出了未来可能的研究方向。我们将持续跟踪相关工作，网址为https://github.com/xinchengshuai/Awesome-Image-Editing。||
|**2024-06-20**|[Advancing Fine-Grained Classification by Structure and Subject Preserving Augmentation](http://arxiv.org/abs/2406.14551)|**[link](https://github.com/eyalmichaeli/saspa-aug)**|细粒度视觉分类 (FGVC) 涉及对密切相关的子类别进行分类。由于类别之间的细微差异和较高的类内差异，这项任务非常困难。此外，FGVC 数据集通常很小，而且难以收集，因此迫切需要有效的数据增强方法。文本到图像扩散模型的最新进展为增强分类数据集提供了新的可能性。虽然这些模型已被用于生成分类任务的训练数据，但它们在 FGVC 模型的全数据集训练中的有效性仍未得到充分探索。最近依赖于 Text2Image 生成或 Img2Img 方法的技术通常难以生成既能准确表示类别又能对其进行修改以显著增加数据集多样性的图像。为了应对这些挑战，我们提出了 SaSPA：结构和主题保留增强。与最近的方法相反，我们的方法不使用真实图像作为指导，从而提高了生成灵活性并促进了更大的多样性。为了确保准确的类别表示，我们采用了调节机制，特别是通过调节图像边缘和主题表示。我们进行了广泛的实验，并将 SaSPA 与传统的和最近的生成数据增强方法进行了基准测试。SaSPA 在多种设置（包括全数据集训练、上下文偏差和少样本分类）中始终优于所有已建立的基线。此外，我们的结果揭示了将合成数据用于 FGVC 模型的有趣模式；例如，我们发现了使用的真实数据量与合成数据的最佳比例之间的关系。代码可在 https://github.com/EyalMichaeli/SaSPA-Aug 获取。||
|**2024-06-20**|[Consistency Models Made Easy](http://arxiv.org/abs/2406.14548)|**[link](https://github.com/locuslab/ect)**|一致性模型 (CM) 是一类新兴的生成模型，与传统的扩散模型相比，它们能够提供更快的采样速度。CM 强制要求采样轨迹上的所有点都映射到相同的初始点。但这一目标导致了资源密集型的训练：例如，截至 2024 年，在 CIFAR-10 上训练一个 SoTA CM 需要 8 个 GPU 运行一周。在这项工作中，我们提出了一种训练 CM 的替代方案，极大地提高了构建此类模型的效率。具体来说，通过使用特定的微分方程表示 CM 轨迹，我们认为扩散模型可以被视为具有特定离散化的 CM 的特例。因此，我们可以从预先训练的扩散模型开始微调一致性模型，并在训练过程中逐步将完全一致性条件逼近到更强的程度。我们提出的方法称为简易一致性调整 (ECT)，它极大地缩短了训练时间，同时确实提高了先前方法的质量：例如，ECT 在单个 A100 GPU 上仅用 1 小时即可在 CIFAR10 上实现 2.73 的 2 步 FID，与经过数百 GPU 小时训练的一致性蒸馏相匹配。由于这种计算效率，我们研究了 ECT 下 CM 的缩放规律，表明它们似乎遵循经典的幂律缩放，暗示了它们在更大规模上提高效率和性能的能力。代码 (https://github.com/locuslab/ect) 已开源。||
|**2024-06-20**|[IRASim: Learning Interactive Real-Robot Action Simulators](http://arxiv.org/abs/2406.14540)|null|在现实世界中，可扩展的机器人学习受到真实机器人成本和安全问题的限制。此外，在现实世界中部署机器人轨迹可能非常耗时且劳动强度大。在本文中，我们建议学习一种交互式真实机器人动作模拟器作为替代方案。我们介绍了一种新方法 IRASim，它利用生成模型的力量来生成极其逼真的机器人手臂视频，该视频从给定的初始帧开始执行给定的动作轨迹。为了验证我们方法的有效性，我们基于三个真实机器人数据集创建了一个新的基准测试 IRASim Benchmark，并在该基准测试上进行了广泛的实验。结果表明，IRASim 优于所有基线方法，并且在人类评估中更受欢迎。我们希望 IRASim 可以作为一种有效且可扩展的方法来增强现实世界中的机器人学习。为了促进对生成式真实机器人动作模拟器的研究，我们在 https://gen-irasim.github.io 上开源了代码、基准测试和检查点。||
|**2024-06-20**|[Invertible Consistency Distillation for Text-Guided Image Editing in Around 7 Steps](http://arxiv.org/abs/2406.14539)|null|扩散模型是通过少量采样步骤实现逼真文本到图像生成的很有前途的方向。 然而，尽管最近取得了成功，但现有的蒸馏模型仍然不能提供全面的扩散能力，例如真实图像反演，而反演能力是许多精确图像处理方法的基础。 这项工作旨在丰富蒸馏文本到图像扩散模型，使其能够有效地将真实图像编码到其潜在空间中。 为此，我们引入了可逆一致性蒸馏 (iCD)，这是一种通用的 consistency distillation 框架，可以在仅 3-4 个推理步骤中促进高质量的图像合成和准确的图像编码。 尽管高分类器引导尺度加剧了文本到图像扩散模型的反演问题，但我们注意到动态引导显着减少了重建误差，而生成性能没有明显下降。 因此，我们证明了配备动态引导的 iCD 可以作为零样本文本引导图像编辑的高效工具，与更昂贵的现有最佳方案相比具有竞争力。||
|**2024-06-20**|[Fantastic Copyrighted Beasts and How (Not) to Generate Them](http://arxiv.org/abs/2406.14526)|null|最近的研究表明，图像和视频生成模型可能会被提示从其训练数据中复制受版权保护的内容，引发了围绕版权侵权的严重法律问题。尤其是受版权保护的角色，对图像生成服务提出了艰巨的挑战，至少有一起诉讼已经基于生成这些角色而判给损害赔偿。然而，很少有研究对这个问题进行实证检验。我们进行了系统的评估来填补这一空白。首先，我们构建了 CopyCat，这是一个评估套件，包含各种受版权保护的角色和一个新颖的评估流程。我们的评估同时考虑了与受版权保护角色的相似性检测以及生成的图像与用户输入的一致性。我们的评估系统地表明，即使在提示中没有明确提及角色名称的情况下，图像和视频生成模型仍然可以生成角色，有时只需两个通用关键字（例如，使用“电子游戏、水管工”提示始终会生成任天堂的马里奥角色）。然后，我们介绍了半自动识别此类关键字或描述的技术，这些关键字或描述会触发角色生成。我们使用评估套件研究了运行时缓解策略，包括现有方法和我们提出的新策略。我们的研究结果表明，常用的策略（例如 DALL-E 系统中的提示重写）不足以作为独立的防护措施。这些策略必须与其他方法（如负面提示）相结合，才能有效减少意外生成受版权保护的角色。我们的工作为版权缓解策略的讨论提供了经验证据，并为积极实施这些策略的模型部署者提供了可操作的见解。||
|**2024-06-20**|[V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data](http://arxiv.org/abs/2406.14510)|null|基于扩散的生成模型最近在图像和视频编辑方面展现出非凡的能力。然而，局部视频编辑，特别是去除眼镜等小属性，仍然是一个挑战。现有方法要么过度改变视频，要么产生不真实的伪影，要么无法在整个视频中一致地执行请求的编辑。在这项工作中，我们专注于一致且保留身份的视频眼镜去除，并将其作为视频中一致的局部属性去除的案例研究。由于缺乏配对数据，我们采用弱监督方法，并使用调整后的预训练扩散模型生成合成的非完美数据。我们表明，尽管数据不完美，但通过学习我们生成的数据并利用预训练扩散模型的先验知识，我们的模型能够在保留原始视频内容的同时一致地执行所需的编辑。此外，我们通过将我们的方法成功应用于面部贴纸去除，例证了其对其他局部视频编辑任务的泛化能力。我们的方法证明了相对于现有方法的显着改进，展示了利用合成数据和强大的视频先验知识进行局部视频编辑任务的潜力。||
|**2024-06-20**|[CodeRAG-Bench: Can Retrieval Augment Code Generation?](http://arxiv.org/abs/2406.14497)|null|虽然语言模型（LM）在生成代码方面表现出色，但许多程序对于仅使用参数化知识的LM来说难以生成。提供外部上下文（如库文档）可以帮助生成准确且可用的代码。尽管检索增强生成（RAG）在各种面向文本的任务中取得了成功，但其在改进代码生成方面的潜力仍未得到充分探索。在这项工作中，我们进行了系统的、大规模的分析，提出以下问题：在什么情况下检索可以使代码生成模型受益？以及还存在哪些挑战？我们首先策划了一个全面的评估基准，CodeRAG-Bench，涵盖了三类代码生成任务，包括基础编程、开放领域和代码库级别的难题。我们汇总了五个来源的文档供模型检索上下文：竞赛解决方案、在线教程、库文档、StackOverflow 帖子和 GitHub 代码库。我们通过提供从一个或多个来源检索到的上下文，来检查 CodeRAG-Bench 上表现最佳的模型。虽然通过在各种设置中检索高质量的上下文，最终代码生成取得了显著的进步，但我们的分析表明仍有改进的空间——当前的检索器仍然难以获取有用的上下文，尤其是在词汇重叠有限的情况下，而生成器在上下文长度有限或整合额外上下文的能力有限的情况下无法改进。我们希望 CodeRAG-Bench 能够成为一个有效的测试平台，鼓励进一步开发先进的面向代码的 RAG 方法。||
|**2024-06-20**|[SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset](http://arxiv.org/abs/2406.14477)|**[link](https://github.com/pku-alignment/safe-sora)**|为了降低大型视觉模型 (LVM) 产生有害输出的风险，我们引入了 SafeSora 数据集，旨在促进文本到视频生成与人类价值观相一致的研究。该数据集涵盖了文本到视频生成任务中人类偏好的两个主要维度：有用性和无害性。为了深入捕捉人类偏好并促进众包工作者的结构化推理，我们将有用性细分为 4 个子维度，将无害性细分为 12 个子类别，作为试点标注的基础。SafeSora 数据集包含 14,711 个独特的提示、由 4 个不同的 LVM 生成的 57,333 个独特的视频以及 51,691 对由人类标记的偏好注释。我们通过几个应用进一步证明了 SafeSora 数据集的效用，包括训练文本-视频审核模型以及通过微调提示增强模块或扩散模型使 LVM 与人类偏好保持一致。这些应用突出了其作为文本到视频对齐研究基础的潜力，例如人类偏好建模以及对齐算法的开发和验证。||
|**2024-06-20**|[CollaFuse: Collaborative Diffusion Models](http://arxiv.org/abs/2406.14429)|**[link](https://github.com/simeonallmendinger/collafuse)**|在生成式人工智能领域，基于扩散的模型已成为一种很有前景的合成图像生成方法。然而，扩散模型的应用面临着诸多挑战，特别是在数据可用性、计算需求和隐私方面。传统的解决这些缺陷的方法，如联邦学习，通常会给单个客户端带来沉重的计算负担，尤其是那些资源受限的客户端。为了应对这些挑战，我们引入了一种受拆分学习启发的新型分布式协作扩散模型方法。我们的方法促进了扩散模型的协作训练，同时减轻了图像合成过程中客户端的计算负担。这种计算负担的减少是通过将数据和计算成本低廉的流程保留在每个客户端本地，同时将计算成本高昂的流程外包给共享的、更高效的服务器资源来实现的。通过对常见 CelebA 数据集的实验，我们的方法通过减少共享原始数据的必要性来增强隐私。这些功能在各种应用领域，包括边缘计算解决方案的设计，都具有巨大的潜力。因此，我们的工作通过促进协作扩散模型的发展，推动了分布式机器学习的进步。||
|**2024-06-18**|[Evaluating the design space of diffusion-based generative models](http://arxiv.org/abs/2406.12839)|null|大多数现有的关于扩散模型精度理论研究虽然意义重大，但都假设分数函数已被逼近到一定精度，然后使用这个先验界限来控制生成误差。而本文则对整个生成过程，即训练和采样，提供了第一个量化的理解。更准确地说，它对梯度下降下的去噪分数匹配进行了非渐近收敛性分析。此外，还对变异爆炸模型的采样误差进行了改进分析。这两个结果的结合产生了一个完整的误差分析，它阐明了（同样是，但这次是从理论上）如何设计训练和采样过程以实现有效的生成。例如，我们的理论暗示了对噪声分布和损失权重的偏好，这与[Karras et al. 2022]中使用的定性一致。它还提供了一些关于为什么[Karras et al. 2022]中使用的时间和方差调度可以比[Song et al. 2020]中的先驱版本更好地调整的观点。||
|**2024-06-18**|[Neural Approximate Mirror Maps for Constrained Diffusion Models](http://arxiv.org/abs/2406.12816)|null|扩散模型擅长创建视觉上有说服力的图像，但它们往往难以满足训练数据中固有的细微约束。这些约束可以是基于物理的（例如，满足偏微分方程）、几何的（例如，保持对称性）或语义的（例如，包含特定数量的对象）。当训练数据都满足某个约束时，在扩散模型上强制执行此约束不仅可以提高其分布匹配精度，还可以使其在生成有效的合成数据和解决约束逆问题方面更加可靠。然而，现有的约束扩散模型方法对于不同类型的约束缺乏灵活性。最近的研究提出在由镜像映射定义的无约束空间中学习镜像扩散模型 (MDM)，并使用逆镜像映射施加约束，但对于复杂的约束，解析镜像映射难以推导。我们针对一般约束提出了神经近似镜像映射 (NAMM)。我们的方法只需要约束集的可微距离函数。我们学习一个将数据推送到无约束空间的近似镜像映射，以及一个将数据映射回约束集的相应近似逆映射。然后，可以在学习的镜像空间中训练生成模型（例如 MDM），并通过逆映射将其样本恢复到约束集。我们在各种约束条件下验证了我们的方法，结果表明，与无约束扩散模型相比，基于 NAMM 的 MDM 显着提高了约束满足度。我们还演示了如何轻松地将现有的基于扩散的逆问题求解器应用于学习的镜像空间，以解决约束逆问题。||
|**2024-06-18**|[AITTI: Learning Adaptive Inclusive Token for Text-to-Image Generation](http://arxiv.org/abs/2406.12805)|**[link](https://github.com/itsmag11/aitti)**|尽管文本到图像生成取得了高质量的结果，但在生成的内容中发现了刻板印象，这损害了生成模型的公平性。在这项工作中，我们建议学习自适应包容性标记来改变最终生成输出的属性分布。与现有的去偏方法不同，我们的方法既不需要明确的属性规范，也不需要预先了解偏差分布。具体来说，我们方法的核心是一个轻量级的自适应映射网络，它可以为要去偏的概念定制包容性标记，使标记可以推广到未见过的概念，而不管其原始偏差分布如何。这是通过使用锚定损失用少量平衡且包容的样本调整自适应映射网络来实现的。实验结果表明，我们的方法优于以前没有属性规范的偏差缓解方法，同时保留了生成结果和文本描述之间的一致性。此外，我们的方法实现了与需要特定属性或编辑方向才能生成的模型相当的性能。大量实验表明，我们的自适应包容性标记在减轻文本到图像生成中的刻板印象方面非常有效。代码将在 https://github.com/itsmag11/AITTI 上提供。||
|**2024-06-18**|[Extracting Training Data from Unconditional Diffusion Models](http://arxiv.org/abs/2406.12752)|null|随着扩散概率模型 (DPM) 作为生成式人工智能 (AI) 的主流模型得到应用，对其原始训练数据记忆的研究也引起了越来越多的关注。该方向上的现有工作旨在了解 DPM 是否或在多大程度上通过记忆进行学习。这种理解对于识别扩散模型中数据泄露和版权侵权的潜在风险，以及更重要的是，对于更可控地生成和可信地应用人工智能生成内容 (AIGC) 至关重要。尽管先前的工作已经对 DPM 何时容易发生记忆做出了重要观察，但这些发现大多是经验性的，并且开发的数据提取方法仅适用于条件扩散模型。在这项工作中，我们旨在通过以下方面建立对 DPM 记忆的理论理解：1) 用于理论分析的记忆度量，2) 对具有信息标签和随机标签的条件记忆的分析，以及 3) 两个用于衡量记忆的更好评估指标。基于理论分析，我们进一步提出了一种名为“代理条件数据提取 (SIDE)”的新型数据提取方法，该方法利用在生成数据上训练的分类器作为代理条件，直接从无条件扩散模型中提取训练数据。我们的实验结果表明，SIDE 可以从以前方法失败的扩散模型中提取训练数据，并且在不同规模的 CelebA 数据集上平均效率提高了 50% 以上。||
|**2024-06-18**|[SUPER: Selfie Undistortion and Head Pose Editing with Identity Preservation](http://arxiv.org/abs/2406.12700)|null|由于严重的扭曲导致面部特征变形以及头部姿势不当，近距离拍摄的自拍照可能看起来不自然甚至不美观。在本文中，我们提出了 SUPER，这是一种消除近距离面部裁剪中扭曲和调整头部姿势的新方法。我们通过优化相机参数和面部潜在代码对面部图像执行 3D GAN 反演，从而生成图像。此外，我们从获得的潜在代码估计深度，创建深度诱导的 3D 网格，并使用更新的相机参数对其进行渲染以获得扭曲的肖像。最后，我们应用基于可见性的混合，以便重新投影可见区域，并使用生成模型恢复遮挡部分。在面部去扭曲基准数据集和我们自己收集的头部旋转数据集 (HeRo) 上的实验表明，SUPER 在质量和数量上都优于以前的方法，为逼真的自拍编辑开辟了新的可能性。||
|**2024-06-18**|[Speak in the Scene: Diffusion-based Acoustic Scene Transfer toward Immersive Speech Generation](http://arxiv.org/abs/2406.12688)|null|本文介绍了语音生成领域的一项新任务，即声场景迁移（AST），旨在将语音信号的声场景迁移到不同的环境中。AST 通过将语音信号背后的声场景调整到所需的环境，有望在语音感知方面带来沉浸式体验。我们针对 AST 任务提出了 AST-LDM，它可以生成伴随参考提示目标声场景的语音信号。具体来说，AST-LDM 是一种潜在扩散模型，它以描述音频或文本模态目标声场景的 CLAP 嵌入为条件。本文的贡献包括引入 AST 任务并实现其基线模型。对于 AST-LDM，我们强调其核心框架，即保留输入语音并生成与给定语音和目标声环境一致的音频。包括客观和主观测试在内的实验验证了我们方法的可行性和有效性。||
|**2024-06-18**|[GeoBench: Benchmarking and Analyzing Monocular Geometry Estimation Models](http://arxiv.org/abs/2406.12671)|**[link](https://github.com/aim-uofa/geobench)**|近年来，判别性和生成性预训练的进步已经产生了具有强大泛化能力的几何估计模型。虽然判别性单目几何估计方法依赖于大规模微调数据来实现零样本泛化，但一些基于生成的方法通过利用预训练的扩散模型并在小规模合成训练数据上进行微调，显示出在未见场景中实现令人印象深刻的泛化性能的潜力。令人沮丧的是，这些模型在不同的数据集上使用不同的方法进行训练，因此很难找出决定评估性能的关键因素。此外，当前的几何评估基准存在两个主要缺点，可能会阻碍该领域的发展，即场景多样性有限和标签质量不佳。为了解决上述问题，（1）我们在统一的代码库中构建了公平且强大的基线，用于评估和分析几何估计模型；（2）我们在更具挑战性的几何估计任务基准上评估单目几何估计器，这些基准具有多样化的场景和高质量的标注。我们的结果表明，在相同的训练配置下，使用大量数据进行预训练的判别性模型（如 DINOv2）可以优于生成模型，即使后者使用少量高质量的合成数据，这表明微调数据质量比数据规模和模型架构更重要。我们的观察结果还提出了一个问题：如果仅使用少量合成深度数据微调 DINOv2 等通用视觉模型就能产生最先进的结果，那么我们真的需要复杂的生成模型来进行深度估计吗？我们相信这项工作可以推动几何估计任务以及广泛的下游应用的进步。||
|**2024-06-18**|[Research and Implementation of Data Enhancement Techniques for Graph Neural Networks](http://arxiv.org/abs/2406.12640)|null|数据、算法和算力是深度学习在应用领域取得成效的三个基础条件，数据是发展深度学习算法的重点。在实际工程应用中，一些数据受到获取条件的制约，无法获取更多的数据或者获取数据的成本过高，导致数据集规模较小（一般几百到几千个），数据量远小于大数据集的规模（几万到几十万个）。以上两种方法都是基于原始数据集进行生成，在原始数据量不足的情况下，可能无法反映真实环境的所有情况，例如真实环境的光照、轮廓等信息，如果数据量不够，则难以使用简单的变换或神经网络生成模型来生成所需的数据。本文的研究首先分析了图神经网络数据增强技术的关键点，同时深入介绍了图神经网络的组成基础，在此基础上对图神经网络的数据增强技术进行了优化和分析。||
|**2024-06-18**|[Learning Diffusion at Lightspeed](http://arxiv.org/abs/2406.12616)|null|扩散现象调节着大量自然过程以及许多成功生成模型的动力学。现有的从观测数据中学习扩散项的模型依赖于复杂的两级优化问题，并且只能正确地模拟系统的漂移。我们提出了一种新的简单模型JKOnet*，它完全绕过了现有架构的复杂性，同时呈现出显著增强的表示能力：JKOnet*可以恢复潜在的、交互的以及内部能量成分的底层扩散过程。JKOnet*最小化了一个简单的二次损失，以极快的速度运行，并且在实践中大大优于其他基线。此外，对于线性参数化泛函，JKOnet*提供了一个封闭形式的最优解。我们的方法论基于将扩散过程解释为概率空间中通过所谓的JKO方案实现的能量最小化轨迹，我们根据其一阶最优性条件，并根据概率空间优化方面的最新进展对其进行研究。||
|**2024-06-18**|[Unmasking the Veil: An Investigation into Concept Ablation for Privacy and Copyright Protection in Images](http://arxiv.org/abs/2406.12592)|**[link](https://github.com/vlgiitr/unmasking-the-veil)**|在本文中，我们扩展了对预训练模型中概念消融的研究，如 (Kumari et al.,2022) 在“文本到图像扩散模型中的概念消融”一文中所述。我们的工作重点是重现通过预定义指标提出和验证的不同概念消融变体所取得的结果。我们还介绍了一种新的概念消融变体，即“商标消融”。该变体结合了记忆和实例消融的原理，以解决专有或品牌元素对模型输出的细微影响。此外，我们的研究贡献还包括对模型局限性的观察分析。此外，我们还研究了模型对消融泄漏诱导提示的响应行为，该提示旨在间接消融概念，从而揭示模型的弹性和适应性。我们还观察到，对于远离其目标消融概念的概念所生成的图像，模型的性能会下降，这在附录中有所记录。||
|**2024-06-17**|[Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%](http://arxiv.org/abs/2406.11837)|**[link](https://github.com/zh460045050/vqgan-lc)**|在以 VQGAN 为代表的图像量化领域中，该过程将图像编码为从具有预定义大小的码本中提取的离散token。近期的进展，特别是 LLAMA 3，表明扩大码本可以显著提高模型性能。然而，VQGAN 及其衍生模型，如 VQGAN-FC（因子化码本）和 VQGAN-EMA，在扩大码本大小和提高码本利用率方面仍然面临挑战。例如，VQGAN-FC 被限制学习最大大小为 16,384 的码本，在 ImageNet 上的利用率通常低于 12%。在这项工作中，我们提出了一种名为 VQGAN-LC（大型码本）的新型图像量化模型，它将码本大小扩展到 100,000，实现了超过 99% 的利用率。与之前优化每个码本条目的方法不同，我们的方法首先使用由预训练视觉编码器提取的 100,000 个特征初始化码本。然后，优化集中于训练一个投影器，该投影器使整个码本与 VQGAN-LC 中编码器的特征分布对齐。我们证明了我们的模型在各种任务中优于其 counterparts，包括图像重建、图像分类、使用 GPT 的自回归图像生成，以及使用基于扩散和基于流的生成模型进行图像创建。代码和模型可在 https://github.com/zh460045050/VQGAN-LC 获取。||
|**2024-06-17**|[RetinaGS: Scalable Training for Dense Scene Rendering with Billion-Scale 3D Gaussians](http://arxiv.org/abs/2406.11836)|null|在这项工作中，我们探索了在大规模、高分辨率数据集上训练高参数 3D 高斯样条 (3DGS) 模型的可能性。我们为 3DGS 设计了一种通用的模型并行训练方法，称为 RetinaGS，它使用适当的渲染方程，可以应用于任何场景和任意分布的高斯基元。它使我们能够探索 3DGS 在基元数量和训练分辨率方面的缩放行为，这些行为以前难以探索，并超越了先前最先进的重建质量。当使用我们的方法增加基元数量时，我们观察到视觉质量明显提高的积极趋势。我们还首次尝试在完整 MatrixCity 数据集上训练具有超过 10 亿个基元的 3DGS 模型，该模型获得了良好的视觉质量。||
|**2024-06-17**|[Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models](http://arxiv.org/abs/2406.11831)|null|基于解码器-only transformer的大型语言模型（LLM）在文本理解能力方面已经表现出优于CLIP和T5系列模型的能力。然而，在文本到图像扩散模型中利用当前先进LLM的范式仍有待探索。我们观察到一个不寻常的现象：直接使用大型语言模型作为prompt编码器会显著降低图像生成中的prompt遵循能力。我们确定了造成这个问题背后的两个主要障碍。一个是LLM中下一个token预测训练与扩散模型中对判别性prompt特征的要求之间的不匹配。另一个是由解码器-only架构引入的固有位置偏差。为了解决这个问题，我们提出了一个新的框架来充分利用LLM的能力。通过精心设计的使用指南，我们有效地增强了用于prompt编码的文本表示能力，并消除了其固有的位置偏差。这使我们能够灵活地将最先进的LLM集成到文本到图像生成模型中。此外，我们还提供了一种将多个LLM融合到我们框架中的有效方法。考虑到transformer架构所展现出的出色性能和扩展能力，我们进一步设计了一个基于该框架的LLM-Infused Diffusion Transformer（LI-DiT）。我们进行了广泛的实验，以验证LI-DiT在模型规模和数据规模上的有效性。受益于LLM的固有能力和我们的创新设计，LI-DiT的prompt理解性能轻松超越了最先进的开源模型以及包括Stable Diffusion 3、DALL-E 3和Midjourney V6在内的主流闭源商业模型。功能强大的LI-DiT-10B将在进一步优化和安全检查后发布。||
|**2024-06-17**|[MegaScenes: Scene-Level View Synthesis at Scale](http://arxiv.org/abs/2406.11819)|null|场景级新视角合成（NVS）是许多视觉和图形应用的基础。最近，姿势条件扩散模型通过从二维基础模型中提取三维信息取得了显著进展，但这些方法受到缺乏场景级训练数据的限制。常见的数据库选择要么包含孤立的对象（Objaverse），要么包含具有有限姿势分布的以对象为中心的场景（DTU、CO3D）。在本文中，我们利用互联网照片集创建了一个大型场景级数据库，称为MegaScenes，其中包含来自世界各地的超过10万个运动结构（SfM）重建。互联网照片代表了一种可扩展的数据源，但也带来了光照和瞬态对象等挑战。我们解决了这些问题，进一步创建了一个适合NVS任务的子集。此外，我们分析了最先进的NVS方法的失败案例，并显著提高了生成的一致性。通过大量的实验，我们验证了我们的数据库和方法在生成真实场景中的有效性。有关数据库和代码的详细信息，请参见我们的项目页面：https://megascenes.github.io。||
|**2024-06-17**|[DiffMM: Multi-Modal Diffusion Model for Recommendation](http://arxiv.org/abs/2406.11781)|**[link](https://github.com/hkuds/diffmm)**|像抖音和YouTube这样的在线多模态分享平台的兴起使得个性化推荐系统能够将多种模态（如视觉、文本和音频）纳入用户表示中。然而，解决这些系统中的数据稀疏性挑战仍然是一个关键问题。为了解决这一局限性，最近的研究引入了自监督学习技术来增强推荐系统。然而，这些方法通常依赖于简单的随机增强或直观的跨视图信息，这可能会引入不相关的噪声，并且无法准确地将多模态上下文与用户-项目交互建模相一致。为了填补这一研究空白，我们提出了一种新的用于推荐的多模态图扩散模型，称为DiffMM。我们的框架将模态感知图扩散模型与跨模态对比学习范式相结合，以改进模态感知用户表示学习。这种集成促进了多模态特征信息与协同关系建模之间更好的对齐。我们的方法利用扩散模型的生成能力自动生成一个感知不同模态的用户-项目图，从而促进在用户-项目交互建模中纳入有用的多模态知识。我们在三个公开数据集上进行了广泛的实验，一致证明了我们的DiffMM相对于各种竞争基线的优越性。有关开源模型实现的详细信息，您可以访问以下网址获取我们提出的框架的源代码：https://github.com/HKUDS/DiffMM。||
|**2024-06-17**|[Transcendence: Generative Models Can Outperform The Experts That Train Them](http://arxiv.org/abs/2406.11741)|null|生成模型的训练目标很简单，即模仿其训练数据所诱导的条件概率分布。因此，当使用人类生成的数据进行训练时，我们不能期望人工智能模型在其原始目标上超越人类。在这项工作中，我们研究了超越现象：即生成模型实现的能力超过生成其数据的专家的能力。我们通过训练一个自回归Transformer从棋谱中学习下棋来证明超越现象，并表明训练后的模型有时可以取得比数据集中所有棋手都好的表现。我们从理论上证明了低温采样能够实现超越，并在实验上对其进行了严格的评估。最后，我们讨论了超越的其他来源，为在更广泛的背景下进一步研究这一现象奠定了基础。||
|**2024-06-17**|[Latent Denoising Diffusion GAN: Faster sampling, Higher image quality](http://arxiv.org/abs/2406.11713)|**[link](https://github.com/thanhluantrinh/lddgan)**|扩散模型正在成为生成高保真度和多样化图像的强大解决方案，在许多情况下甚至优于GAN。然而，其缓慢的推理速度阻碍了其在实时应用中的潜力。为了解决这个问题，DiffusionGAN利用条件GAN大幅减少了去噪步骤并加快了推理速度。其改进版本Wavelet Diffusion通过将数据转换为小波空间进一步加快了这一过程，从而提高了效率。尽管如此，这些模型在速度和图像质量方面仍落后于GAN。为了弥合这些差距，本文介绍了潜在去噪扩散GAN（Latent Denoising Diffusion GAN），它采用预训练的自编码器将图像压缩到紧凑的潜在空间中，从而显著提高推理速度和图像质量。此外，我们提出了一种加权学习策略来增强图像的多样性和质量。在CIFAR-10、CelebA-HQ和LSUN-Church数据集上的实验结果证明，我们的模型在扩散模型中实现了最先进的运行速度。与之前的DiffusionGAN和Wavelet Diffusion相比，我们的模型在所有评估指标上都显示出显著的改进。代码和预训练模型：\url{https://github.com/thanhluantrinh/LDDGAN.git}||
|**2024-06-17**|[Diffusion Generative Modelling for Divide-and-Conquer MCMC](http://arxiv.org/abs/2406.11664)|**[link](https://github.com/ctrojan/DiffusionDnC)**|分而治之马尔可夫链蒙特卡洛 (MCMC) 是一种并行化马尔可夫链蒙特卡洛采样的策略，它在数据集的不相交子集上运行独立采样器并合并它们的输出。文献中一个持续的挑战是如何有效地执行这种合并，而不必对后验分布做出分布假设。我们建议使用扩散生成模型来拟合子后验分布的密度近似。这种方法在具有挑战性的合并问题上优于现有方法，同时与现有的密度估计方法相比，其计算成本可以更有效地扩展到高维问题。||
|**2024-06-17**|[An approach to non-equilibrium statistical physics using variational Bayesian inference](http://arxiv.org/abs/2406.11630)|null|我们讨论了一种对由耦合在一起的对象组成的系统进行数学建模的方法，该方法使用生成模型来描述构成此类系统的物体状态（或轨迹）之间的依赖关系。这类系统范围很广，包括开放系统或非平衡系统，与自组织系统尤其相关。由此产生的变分自由能原理 (FEP) 与直接使用随机动力系统相比具有一定的优势，特别是它更易于处理，并根据系统组件之间的耦合特性，对联合系统的演化方式提供了一种简洁的解释。使用 FEP，我们可以将一个物体的动力学建模为一个变分推理过程，因为变分自由能（或惊奇）是其动力学的李雅普诺夫函数。简而言之，我们认为使用生成模型来表示和跟踪子系统之间的关系，可以引导我们得到一种关于交互系统的特定统计理论。反过来，该理论使我们能够构建尊重子系统之间已知关系的嵌套模型。我们指出，一个物理对象符合 FEP 并不一定意味着该对象在字面上执行推理；相反，这是一种有用的解释性虚构，它用自由能梯度上的“隐式”流动代替了对象的“显式”动力学——这种虚构可能被对象本身所接受，也可能不被接受。||
|**2024-06-17**|[GECOBench: A Gender-Controlled Text Dataset and Benchmark for Quantifying Biases in Explanations](http://arxiv.org/abs/2406.11547)|**[link](https://github.com/braindatalab/gecobench)**|大型预训练语言模型已在许多应用中流行起来，并构成自然语言处理 (NLP) 中许多下游任务的重要支柱。应用“可解释人工智能”(XAI) 技术来丰富此类模型的输出，对于确保其质量和阐明其内部工作机制至关重要。然而，大型语言模型是在包含各种偏差（例如性别偏差）的大量数据上训练的，这会影响模型权重以及潜在的行为。目前，尚不清楚这种偏差在何种程度上也会以可能不利的方式影响模型解释。我们创建了一个性别控制文本数据集 GECO，其中其他相同的句子以男性和女性形式出现。这产生了用于性别分类任务的基本事实“世界解释”，从而能够客观地评估 XAI 方法的正确性。我们还提供了 GECOBench，这是一个严格的定量评估框架，对流行的 XAI 方法进行基准测试，将它们应用于经过不同程度微调的预训练语言模型。这使我们能够研究预训练如何在模型解释中引发不必要的偏差，以及微调可以在多大程度上减轻这种解释偏差。我们展示了解释性能与微调层数之间的明确依赖关系，其中观察到 XAI 方法特别受益于微调或嵌入层的完整再训练。值得注意的是，这种关系适用于在同一任务上实现相似分类性能的模型。因此，我们强调了所提出的性别控制数据集和新颖的基准测试方法对于新型 XAI 方法的研发具有实用性。所有代码，包括数据集生成、模型训练、评估和可视化，都可以在以下网址获得：https://github.com/braindatalab/gecobench||

<p align=right>(<a href=#updated-on-20240622>back to top</a>)</p>

## LLM

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-20**|[Asynchronous Large Language Model Enhanced Planner for Autonomous Driving](http://arxiv.org/abs/2406.14556)|null|尽管实时规划器在自动驾驶领域表现出色，但大型语言模型（LLM）的不断发展为增强运动规划的可解释性和可控性开辟了道路。然而，基于LLM的规划器仍然面临着重大挑战，包括资源消耗高和推理时间长，这对其在实际应用中构成了巨大障碍。鉴于这些挑战，我们引入了AsyncDriver，这是一个新的异步LLM增强闭环框架，旨在利用由LLM生成的场景相关指令特征来指导实时规划器进行精确且可控的轨迹预测。一方面，我们的方法突出了LLM在理解和推理矢量化场景数据以及一系列路线指令方面的能力，展示了其对实时规划器的有效辅助作用。另一方面，所提出的框架将LLM和实时规划器的推理过程解耦。通过利用其推理频率的异步特性，我们的方法成功地降低了LLM引入的计算成本，同时保持了相当的性能。实验表明，我们的方法在nuPlan的挑战性场景中实现了优越的闭环评估性能。||
|**2024-06-20**|[GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models](http://arxiv.org/abs/2406.14550)|null|长上下文能力对于大型语言模型 (LLM) 处理复杂和长输入的任务至关重要。尽管为优化 LLM 以适应长上下文做出了许多努力，但在稳健地处理长输入方面仍然存在挑战。在本文中，我们介绍了 GraphReader，一种基于图的代理系统，旨在通过将长文本构建成图并利用代理自主探索该图来处理长文本。在收到问题后，代理首先进行逐步分析并制定合理的计划。然后，它调用一组预定义的函数来读取节点内容及其邻居，从而促进对图的从粗到细的探索。在整个探索过程中，代理不断记录新的见解并反思当前情况以优化过程，直到它收集到足够的信息来生成答案。在 LV-Eval 数据集上的实验结果表明，GraphReader 使用 4k 上下文窗口，在 16k 到 256k 的上下文长度范围内，始终大幅优于 GPT-4-128k。此外，我们的方法在四个具有挑战性的单跳和多跳基准测试中均表现出优异的性能。||
|**2024-06-20**|[Uncovering Latent Memories: Assessing Data Leakage and Memorization Patterns in Large Language Models](http://arxiv.org/abs/2406.14549)|null|大型语言模型的兴起彻底改变了自然语言处理任务，但也引发了人们对数据隐私和安全的深刻担忧。语言模型在包含潜在敏感或专有信息的海量语料库上进行训练，而数据泄露的风险（即模型响应泄露此类信息的片段）仍然未得到充分理解。本研究通过量化机器学习模型中的记忆现象来检验其对数据泄露的敏感性，重点关注训练过程中记忆模式的演变。我们通过评估重复如何影响记忆来研究训练数据的统计特征如何影响编码在模型中的记忆。我们重现了以下发现：记忆序列的概率与其在数据中出现的次数成对数关系。此外，我们发现，即使在第一次遇到后没有明显记住的序列，也可以在整个训练过程中被发现，即使没有后续的遇到。这些潜在记忆序列的存在对数据隐私提出了挑战，因为它们可能隐藏在模型的最终检查点中。为此，我们开发了一种诊断测试，通过考虑其交叉熵损失来揭示这些潜在的记忆序列。||
|**2024-06-20**|[PostMark: A Robust Blackbox Watermark for Large Language Models](http://arxiv.org/abs/2406.14517)|**[link](https://github.com/lilakk/postmark)**|目前，检测LLM生成文本最有效的方法是在模型解码过程中插入可检测的签名（或水印）。大多数现有的水印方法都需要访问底层LLM的logits，而LLM API提供商出于对模型蒸馏的担忧，不愿分享这些信息。因此，这些水印必须由每个LLM提供商独立实施。在本文中，我们开发了PostMark，一种模块化的后置水印程序，它在解码过程完成后将一组依赖于输入的单词（通过语义嵌入确定）插入到文本中。重要的是，PostMark不需要访问logit，这意味着它可以由第三方实现。我们还表明，PostMark比现有的水印方法更能抵抗释义攻击：我们的实验涵盖了8种基线算法、5种基础LLM和3个数据集。最后，我们使用自动评估和人工评估来评估PostMark对文本质量的影响，突出了质量和抵抗释义攻击之间的权衡。我们在https://github.com/lilakk/PostMark上发布了我们的代码、输出和注释。||
|**2024-06-20**|[Evidence of a log scaling law for political persuasion with large language models](http://arxiv.org/abs/2406.14508)|**[link](https://github.com/kobihackenburg/scaling-llm-persuasion)**|大型语言模型现在可以生成与人类撰写的政治信息一样具有说服力的信息，这引发了人们对这种说服力可能随着模型规模的增加而持续增强的担忧。在这里，我们使用 24 个规模跨越多个数量级的语言模型，生成了 720 条关于 10 个美国政治议题的说服性信息。然后，我们在一个大规模随机调查实验 (N = 25,982) 中部署了这些信息，以估计每个模型的说服能力。我们的发现有两方面。首先，我们发现了对数比例定律的证据：模型说服力的特点是收益递减，因此当前的前沿模型的说服力仅略高于规模小一个或多个数量级的模型。其次，仅仅完成任务（连贯性、保持主题）似乎是大型模型具有说服力优势的原因。这些发现表明，进一步扩大模型规模不会大幅提高静态 LLM 生成信息的说服力。||
|**2024-06-20**|[Improving Expert Radiology Report Summarization by Prompting Large Language Models with a Layperson Summary](http://arxiv.org/abs/2406.14500)|null|放射学报告总结 (RRS) 对于患者护理至关重要，需要从详细的“结果”中得出简洁的“印象”。 本文介绍了一种新的提示策略，通过首先生成外行摘要来增强 RRS。这种方法使用受医患互动启发的非专家交流技巧，对关键观察结果进行标准化并简化复杂信息。结合少样本上下文学习，这种方法提高了模型将通用术语与特定发现联系起来的能力。我们在 MIMIC-CXR、CheXpert 和 MIMIC-III 数据集上评估了这种方法，将其与 7B/8B 参数最先进的开源大型语言模型 (LLM)（如 Meta-Llama-3-8B-Instruct）进行基准测试。我们的结果表明，总结的准确性和可访问性有所提高，尤其是在域外测试中，某些指标的改进高达 5%。||
|**2024-06-20**|[Data-Centric AI in the Age of Large Language Models](http://arxiv.org/abs/2406.14473)|null|这篇立场论文提出了以数据为中心的AI研究视角，重点关注大型语言模型（LLM）。我们首先注意到一个关键观察结果：数据在LLM的发展阶段（例如，预训练和微调）和推理阶段（例如，上下文学习）中都起着重要作用，但它受到研究界的关注却少之又少。我们确定了四个以数据为中心的具体场景，涵盖以数据为中心的基准和数据整理、数据溯源、知识迁移和推理语境化。在每个场景中，我们都强调数据的重要性，突出有前景的研究方向，并阐明对研究界以及在适用的情况下对整个社会的潜在影响。例如，我们倡导一套针对LLM的数据规模和复杂性量身定制的以数据为中心的基准。这些基准可用于开发新的数据整理方法和记录研究工作和结果，这有助于促进AI和LLM研究的开放性和透明度。||
|**2024-06-20**|[Explicit and Implicit Large Language Model Personas Generate Opinions but Fail to Replicate Deeper Perceptions and Biases](http://arxiv.org/abs/2406.14462)|null|大型语言模型 (LLM) 正越来越多地应用于以人为中心的社会科学任务中，例如数据标注、合成数据创建以及参与对话。然而，这些任务具有高度的主观性，并依赖于环境、态度、信仰和生活经历等人类因素。因此，在这些任务中使用缺乏此类人类因素的 LLM 可能会导致数据缺乏变化，无法反映人类经验的多样性。在本文中，我们探讨了使用类人角色提示 LLM 并要求模型像特定的人一样回答的作用。这是通过明确的、具有确切人口统计、政治信仰和生活经历的角色，或通过特定人群中普遍存在的姓名隐含地完成的。然后，通过 (1) 主观标注任务（例如，检测毒性）和 (2) 信念生成任务来评估 LLM 角色，这两个任务都因人类因素而异。我们检查了显式和隐式角色的影响，并调查了 LLM 识别和响应的人类因素。结果表明，LLM 角色在再现已知的人类偏见方面表现出混合的结果，但通常无法表现出隐性偏见。我们得出结论，LLM 缺乏人类思维的内在认知机制，而只是捕捉了人们说话的统计模式，这可能会限制其在复杂社会科学应用中的有效性。||
|**2024-06-20**|[APEER: Automatic Prompt Engineering Enhances Large Language Model Reranking](http://arxiv.org/abs/2406.14449)|**[link](https://github.com/jincan333/apeer)**|大型语言模型（LLM）显著增强了信息检索（IR）各个模块的能力，例如重排序。尽管性能出色，但当前使用 LLM 进行的零样本相关性排序严重依赖于人工提示工程。现有的自动提示工程算法主要集中在语言建模和分类任务上，而信息检索领域，特别是重排序领域，尚未得到充分探索。由于输入中集成了查询和长段落对，将当前的提示工程算法直接应用于相关性排序具有挑战性，因为其排序复杂性超过了分类任务。为了减少人力投入并释放提示优化在重排序中的潜力，我们引入了一种名为 APEER 的新型自动提示工程算法。APEER 通过反馈和偏好优化迭代地生成改进的提示。对四个 LLM 和十个数据集进行的大量实验表明，APEER 相比现有的最先进（SoTA）人工提示，性能有了显著提高。此外，我们发现 APEER 生成的提示在不同任务和 LLM 之间表现出更好的可迁移性。代码可在 https://github.com/jincan333/APEER 获取。||
|**2024-06-20**|[LLM4CP: Adapting Large Language Models for Channel Prediction](http://arxiv.org/abs/2406.14440)|null|信道预测是降低大规模多输入多输出 (m-MIMO) 系统反馈或估计开销的有效方法。然而，现有的信道预测方法由于模型失配误差或网络泛化问题而缺乏精度。大型语言模型 (LLM) 已展现出强大的建模和泛化能力，并已成功应用于跨模态任务，包括时间序列分析。利用 LLM的表达能力，我们提出了一种基于预训练LLM的信道预测方法 (LLM4CP)，用于根据历史上下行信道状态信息 (CSI) 序列预测未来下行CSI序列。我们对网络进行微调，同时冻结预训练 LLM 的大部分参数，以实现更好的跨模态知识迁移。为了弥合信道数据与 LLM 特征空间之间的差距，我们专门针对独特的信道特性定制了预处理器、嵌入模块和输出模块。仿真结果表明，所提出的方法在全样本、小样本和泛化测试中均实现了最先进的预测性能，同时训练和推理成本较低。||
|**2024-06-18**|[Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?](http://arxiv.org/abs/2406.12822)|null|大型语言模型，特别是多语言模型，其设计、宣传和预期目标都是服务于各种语言的母语使用者。我们假设，由于严重依赖翻译，这些模型目前的微调和评估实践可能与这一目标不符，因为翻译可能会引入翻译错误和缺陷。目前尚不清楚指令数据的性质是否会对模型输出产生影响；另一方面，翻译后的测试集是否能够捕捉到这些细微差别也值得怀疑。由于在这两个阶段经常使用翻译数据的做法，这种缺陷可能被忽视了。这项工作通过在指令微调和评估阶段使用受控的母语或翻译数据并观察模型结果来研究这些问题。对8个基础模型和8个不同基准的实验表明，母语或生成基准在母语和翻译指令数据之间表现出显著差异，尤其是在模型性能较高时，而其他类型的测试集则没有这种差异。最后，我们证明了正则化有利于弥合结构化任务（而非生成任务）上的差距。||
|**2024-06-18**|[Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?](http://arxiv.org/abs/2406.12809)|null|大型语言模型 (LLM) 表现出令人印象深刻的能力，但仍然存在不一致的问题（例如，LLM 对重新措辞或无关紧要的顺序更改等干扰的反应可能不同）。除了这些不一致之外，我们还观察到，尽管 LLM 能够解决难题，但反常的是，它们却可能在更简单的难题上失败。为了评估这种由难到易的不一致性，我们开发了 ConsisEval 基准测试，其中每个条目包含一对难度严格排序的问题。此外，我们引入了“一致性得分”的概念，以定量衡量这种不一致性，并通过“相对一致性得分”分析一致性改进的潜力。基于对各种现有模型的综合实验，我们发现：(1) GPT-4 取得了 92.2% 的最高一致性得分，但由于冗余信息的干扰、对问题的误解等原因，在特定问题上仍然存在不一致性；(2) 能力越强的模型通常表现出更高的一致性，但也存在例外情况；(3) 硬数据增强了微调和上下文学习的一致性。我们的数据和代码将在 GitHub 上公开。||
|**2024-06-18**|[Supporting Human Raters with the Detection of Harmful Content using Large Language Models](http://arxiv.org/abs/2406.12800)|null|本文探讨了利用大型语言模型 (LLM) 自动化或辅助人工评估员识别有害内容（包括仇恨言论、骚扰、暴力极端主义和选举虚假信息）的可行性。我们使用包含 50,000 条评论的数据集证明，与人工判决相比，LLM 可以达到 90% 的准确率。我们探索了如何最好地利用这些能力，提出了五种将 LLM 与人工评估相结合的设计模式，例如预先过滤非违规内容、检测人工评估中的潜在错误或显示关键上下文以支持人工评估。我们概述了如何使用单一、优化的提示来支持所有这些设计模式。除了这些合成实验之外，我们还分享了在现实世界审核队列中试行我们提出的技术如何使可用人工评估员的能力优化了 41.5%，并将识别违规内容的精确率和召回率提高了 9% 到 11%（绝对值）。||
|**2024-06-18**|[ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools](http://arxiv.org/abs/2406.12793)|null|我们介绍 ChatGLM，这是一个随着时间的推移而不断发展的系列大型语言模型。本报告主要关注 GLM-4 语言系列，其中包括 GLM-4、GLM-4-Air 和 GLM-4-9B。它们代表了我们功能最强大的模型，这些模型利用了前三代 ChatGLM 的所有见解和经验教训进行训练。迄今为止，GLM-4 模型已经接受了 10 万亿个标记的预训练，这些标记主要来自中文和英文，以及来自 24 种语言的一小部分语料库，并且主要针对中文和英文的使用进行了调整。高质量的对齐是通过多阶段的训练后过程实现的，该过程包括监督微调和从人类反馈中学习。评估表明，GLM-4 1) 在 MMLU、GSM8K、MATH、BBH、GPQA 和 HumanEval 等一般指标方面与 GPT-4 相当或优于 GPT-4，2) 在 IFEval 衡量的指令遵循方面接近 GPT-4-Turbo，3) 在长上下文任务方面与 GPT-4 Turbo (128K) 和 Claude 3 相当，以及 4) 在 AlignBench 衡量的中文对齐方面优于 GPT-4。GLM-4 All Tools 模型经过进一步调整，可以理解用户意图并自主决定何时以及使用哪些工具（包括网络浏览器、Python 解释器、文本到图像模型和用户定义的函数）来有效完成复杂的任务。在实际应用中，它在通过网络浏览访问在线信息和使用 Python 解释器解决数学问题等任务中，与 GPT-4 All Tools 相当甚至超越了它。在此过程中，我们开源了一系列模型，包括 ChatGLM-6B（三代）、GLM-4-9B（128K、1M）、GLM-4V-9B、WebGLM 和 CodeGeeX，仅在 2023 年就吸引了 Hugging face 上超过 1000 万次的下载量。开源模型可以通过 https://github.com/THUDM 和 https://huggingface.co/THUDM 访问。||
|**2024-06-18**|[UBENCH: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions](http://arxiv.org/abs/2406.12784)|**[link](https://github.com/Cyno2232/UBENCH)**|大型语言模型（LLM）的快速发展已经展现出可观的应用前景。然而，它们的可解释性较低，这常常导致在不可预见的情况下出现错误，限制了它们的实用性。许多研究致力于创建全面的评估体系，但以往的基准测试主要评估解决问题的能力，而忽略了响应的不确定性，这可能导致不可靠的结果。最近测量LLM可靠性的方法非常消耗资源，并且无法测试黑盒模型。为了解决这个问题，我们提出了UBENCH，这是一个用于评估LLM可靠性的综合基准测试。UBENCH包含3,978道多项选择题，涵盖知识、语言、理解和推理能力。实验结果表明，UBENCH取得了最先进的性能，同时其单次采样方法与需要多次采样的基线方法相比，显著节省了计算资源。此外，我们基于UBENCH评估了15种流行LLM的可靠性，发现GLM4表现最为出色，紧随其后的是GPT-4。我们还探讨了思维链提示、角色扮演提示、选项顺序和温度对LLM可靠性的影响，分析了它们对不同LLM的不同影响。||
|**2024-06-18**|[Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries](http://arxiv.org/abs/2406.12775)|**[link](https://github.com/edenbiran/HoppingTooLate)**|大型语言模型 (LLM) 可以解决复杂的多步骤问题，但人们对其内部计算机制知之甚少。基于此，我们研究了 LLM 如何回答多跳查询，例如“Imagine 的演唱者的配偶是”。这些查询需要两个信息提取步骤：一个潜在步骤，用于将第一个跳跃（“Imagine 的演唱者”）解析为桥接实体（约翰·列侬），另一个步骤用于将第二个跳跃（“约翰·列侬的配偶”）解析为目标实体（小野洋子）。理解潜在步骤如何在内部计算是理解整体计算的关键。通过仔细分析基于 Transformer 的 LLM 的内部计算，我们发现桥接实体是在模型的早期层中解析的。然后，只有在这个解析之后，两跳查询才会在后面的层中得到解决。由于第二个跳跃始于后面的层，因此在某些情况下，这些层可能不再编码正确预测答案所需的知识。基于此，我们提出了一种新颖的“反向补丁”分析方法，将来自后面层的隐藏表示“补丁”回较早的层。我们发现在高达 57% 的先前错误案例中，存在一个反向补丁，可以导致正确生成答案，这表明后面的层确实有时缺乏必要的功能。总的来说，我们的方法和发现为理解和改进基于 Transformer 的 LLM 中的潜在推理提供了进一步的机会。||
|**2024-06-18**|[Large Language Model as a Universal Clinical Multi-task Decoder](http://arxiv.org/abs/2406.12738)|null|开发有效的机器学习方法来提高临床系统的效率和准确性至关重要。尽管进行了大量的研究，但管理大量多样化的临床任务和适应新出现的任务仍然是重大挑战。本文提出了一种新颖的范式，该范式采用预训练的大型语言模型作为通用的临床多任务解码器。这种方法利用语言表达的灵活性和多样性来处理任务主题变化和相关参数。引入新任务只需添加新的指令模板。我们在数百项任务中验证了此框架，证明了其在促进多任务预测方面的稳健性，其性能与传统的  多任务学习和单任务学习方法相当。此外，它还表现出对新任务的出色适应能力，在某些情况下具有令人印象深刻的零样本性能，并且在少样本场景中具有卓越的数据效率。这种新方法为管理临床应用中的大量新兴任务提供了统一的解决方案。||
|**2024-06-18**|[Can Large Language Models Code Like a Linguist?: A Case Study in Low Resource Sound Law Induction](http://arxiv.org/abs/2406.12725)|null|长期以来，历史语言学家一直在编写一种未完全形式化的“程序”，用于将祖先语言中的重建词转换为其已证实的后代语言中的词，该程序由一系列有序的字符串重写函数（称为语音规律）组成。他们通过观察重建语言（原始形式）和后代语言（反射形式）中的词对，并构建一个将原始形式转换为反射形式的程序来做到这一点。然而，编写这些程序容易出错且耗时。先前的工作已经成功地用计算方法为这个过程搭建了脚手架，但很少有研究人员处理语音规律归纳（SLI）问题，我们在本文中将其视为示例编程来处理。我们提出了一种语言无关的解决方案，该解决方案利用大型语言模型（LLM）的编程能力，从语音变化示例中生成 Python 语音规律程序。我们评估了我们的方法对各种 LLM 的有效性，提出了生成额外的语言无关合成数据的有效方法，以微调用于 SLI 的 LLM，并将我们的方法与现有的自动 SLI 方法进行了比较，结果表明，虽然 LLM 落后于它们，但它们可以弥补它们的一些弱点。||
|**2024-06-18**|[Estimating Knowledge in Large Language Models Without Generating a Single Token](http://arxiv.org/abs/2406.12673)|null|为了评估大型语言模型 (LLM) 的知识，当前的方法是查询模型，然后评估其生成的响应。在这项工作中，我们询问是否可以在模型生成任何文本之前完成评估。具体来说，是否可以仅根据模型的内部计算来估计模型对某个实体的了解程度？我们通过两个任务来研究这个问题：给定一个主题实体，目标是预测 (a) 模型回答有关该实体的常见问题的能力，以及 (b) 模型生成的有关该实体的响应的事实性。对各种 LLM 进行的实验表明，KEEN 是一种基于内部主题表示训练的简单探针，在这两项任务中都取得了成功——与每个主题的模型问答准确率和 FActScore（最近在开放式生成中的事实性指标）密切相关。此外，KEEN 自然地与模型的 hedging 行为相一致，并忠实地反映了微调后模型知识的变化。最后，我们展示了一个更具解释性但性能相当的 KEEN 变体，它突出显示了一小组与模型缺乏知识相关的标记。KEEN 简单轻便，可用于识别 LLM 中实体知识的差距和集群，并指导决策，例如使用检索增强查询。||
|**2024-06-18**|[Stealth edits for provably fixing or attacking large language models](http://arxiv.org/abs/2406.12670)|**[link](https://github.com/qinghua-zhou/stealth-edits)**|We reveal new methods and the theoretical foundations of techniques for editing large language models. We also show how the new theory can be used to assess the editability of models and to expose their susceptibility to previously unknown malicious attacks. Our theoretical approach shows that a single metric (a specific measure of the intrinsic dimensionality of the model's features) is fundamental to predicting the success of popular editing approaches, and reveals new bridges between disparate families of editing methods. We collectively refer to these approaches as stealth editing methods, because they aim to directly and inexpensively update a model's weights to correct the model's responses to known hallucinating prompts without otherwise affecting the model's behaviour, without requiring retraining. By carefully applying the insight gleaned from our theoretical investigation, we are able to introduce a new network block -- named a jet-pack block -- which is optimised for highly selective model editing, uses only standard network operations, and can be inserted into existing networks. The intrinsic dimensionality metric also determines the vulnerability of a language model to a stealth attack: a small change to a model's weights which changes its response to a single attacker-chosen prompt. Stealth attacks do not require access to or knowledge of the model's training data, therefore representing a potent yet previously unrecognised threat to redistributed foundation models. They are computationally simple enough to be implemented in malware in many cases. Extensive experimental results illustrate and support the method and its theoretical underpinnings. Demos and source code for editing language models are available at https://github.com/qinghua-zhou/stealth-edits.||
|**2024-06-17**|[mDPO: Conditional Preference Optimization for Multimodal Large Language Models](http://arxiv.org/abs/2406.11839)|null|直接偏好优化 (DPO) 已被证明是一种有效的大语言模型 (LLM) 对齐方法。最近的研究尝试将 DPO 应用于多模态场景，但发现难以实现一致的改进。通过对比实验，我们发现了多模态偏好优化中的无条件偏好问题，即模型忽略了图像条件。为了解决这个问题，我们提出了 mDPO，这是一种多模态 DPO 目标，通过同时优化图像偏好来防止过度优先考虑仅语言偏好。此外，我们引入了一个奖励锚点，强制选择响应的奖励为正，从而避免了其可能性降低——这是相对偏好优化固有的问题。在两个不同规模的多模态 LLM 和三个广泛使用的基准测试上的实验表明，mDPO 有效地解决了多模态偏好优化中的无条件偏好问题，并显着提高了模型性能，特别是在减少幻觉方面。||
|**2024-06-17**|[Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models](http://arxiv.org/abs/2406.11831)|null|基于解码器Transformer的大语言模型（LLM）在文本理解能力方面已经展现出优于CLIP和T5系列模型的优势。然而，如何将当前先进的LLM应用于文本到图像的扩散模型仍然是一个有待探索的领域。我们观察到一个不寻常的现象：直接使用大型语言模型作为提示编码器会显著降低图像生成中的提示遵循能力。我们确定了造成这个问题背后的两个主要障碍。一个是LLM中预测下一个词的训练目标与扩散模型中对判别性提示特征的要求之间存在偏差。另一个是由仅解码器架构引入的固有位置偏差。为了解决这个问题，我们提出了一个全新的框架来充分利用LLM的能力。通过精心设计的使用指南，我们有效地增强了用于提示编码的文本表示能力，并消除了其固有的位置偏差。这使我们能够灵活地将最先进的LLM集成到文本到图像的生成模型中。此外，我们还提供了一种将多个LLM融合到我们框架中的有效方法。考虑到Transformer架构所展现出的出色性能和扩展能力，我们进一步设计了一种基于该框架的LLM注入式扩散Transformer（LI-DiT）。我们进行了广泛的实验，以验证LI-DiT在模型规模和数据规模上的有效性。得益于LLM的固有能力和我们的创新设计，LI-DiT的提示理解性能轻松超越了最先进的开源模型以及包括Stable Diffusion 3、DALL-E 3和Midjourney V6在内的主流闭源商业模型。功能强大的LI-DiT-10B将在经过进一步优化和安全检查后发布。||
|**2024-06-17**|[VideoLLM-online: Online Video Large Language Model for Streaming Video](http://arxiv.org/abs/2406.11816)|null|近年来，大型语言模型已经具备了视觉能力，能够理解图像、视频和交错的视觉语言内容。然而，这些大型多模态模型的学习方法通常将视频视为预先确定的片段，这使得它们在处理流式视频输入方面效率较低。在本文中，我们提出了一种新颖的视频流学习（LIVE）框架，该框架支持在连续视频流中进行时间对齐、长上下文和实时的对话。我们的LIVE框架包含实现视频流对话的综合方法，包括：（1）旨在对连续流输入执行语言建模的训练目标，（2）将离线时间注释转换为流对话格式的数据生成方案，以及（3）优化的推理流程，以加快模型在现实世界视频流中的响应速度。借助我们的LIVE框架，我们在Llama-2/Llama-3的基础上构建了VideoLLM-online模型，并展示了其在处理流媒体视频方面的显著优势。例如，平均而言，我们的模型可以在A100 GPU上以超过10 FPS的速度支持5分钟视频片段中的流媒体对话。此外，它还在公共离线视频基准测试中展示了最先进的性能，例如识别、字幕和预测。代码、模型、数据和演示已在https://showlab.github.io/videollm-online上提供。||
|**2024-06-17**|[How Do Large Language Models Acquire Factual Knowledge During Pretraining?](http://arxiv.org/abs/2406.11813)|null|尽管近期观察到大型语言模型 (LLM) 可以存储大量的 factual knowledge，但对于它们如何通过预训练获取 factual knowledge 的机制，人们的理解仍然有限。这项工作通过研究 LLM 在预训练期间如何获取 factual knowledge 来解决这一差距。研究结果揭示了关于预训练期间 factual knowledge 获取动态的几个重要见解。首先，与直觉相反，我们观察到，对更多数据的预训练并没有显著提高模型获取和维护 factual knowledge 的能力。其次，训练步数与 factual knowledge 的记忆遗忘和泛化之间存在幂律关系，并且使用重复训练数据训练的 LLM 表现出更快的遗忘速度。第三，使用更大的批大小训练 LLM 可以增强模型对遗忘的鲁棒性。总的来说，我们的观察表明 LLM 预训练中的 factual knowledge 获取是通过逐步增加在每个步骤中预训练数据中出现的 factual knowledge 的概率来实现的。然而，这种增加会被随后的遗忘所稀释。基于这种解释，我们可以对最近观察到的 LLM 行为提供合理的解释，例如 LLM 在长尾知识上的糟糕表现以及对预训练语料库进行重复数据删除的好处。||
|**2024-06-17**|[DataComp-LM: In search of the next generation of training sets for language models](http://arxiv.org/abs/2406.11794)|null|我们推出了面向语言模型的数据比较测试平台 (DCLM)，这是一个用于控制数据集实验以改进语言模型的测试平台。作为 DCLM 的一部分，我们提供了一个从 Common Crawl 中提取的包含 240 万亿个标记的标准化语料库、基于 OpenLM 框架的有效预训练方案以及一套包含 53 个下游评估的广泛套件。DCLM 基准测试的参与者可以尝试各种数据整理策略，例如去重、过滤和数据混合，模型规模从 4.12 亿到 70 亿个参数不等。作为 DCLM 的基线，我们进行了广泛的实验，发现基于模型的过滤是组装高质量训练集的关键。生成的数据集 DCLM-Baseline 使从头开始训练一个包含 70 亿个参数的语言模型成为可能，该模型在 MMLU 上的 5-shot 准确率达到 64%，训练使用了 2.6 万亿个标记。与之前最先进的开放数据语言模型 MAP-Neo 相比，DCLM-Baseline 在 MMLU 上实现了 6.6 个百分点的改进，而计算量减少了 40%。我们的基线模型在 MMLU 上也与 Mistral-7B-v0.3 和 Llama 3 8B 相当（63% 和 66%），并且在平均 53 个自然语言理解任务上的表现相似，而训练使用的计算量比 Llama 3 8B 少 6.6 倍。我们的结果突出了数据集设计对训练语言模型的重要性，并为进一步研究数据整理提供了一个起点。||
|**2024-06-17**|[CELL your Model: Contrastive Explanation Methods for Large Language Models](http://arxiv.org/abs/2406.11785)|null|黑盒深度神经网络分类模型的出现引发了对其决策进行解释的需求。然而，对于生成式人工智能（如大型语言模型（LLM））来说，没有类别预测需要解释。相反，人们可以询问LLM为何对给定提示输出特定响应。在本文中，我们通过提出据我们所知第一个仅需要黑盒/查询访问的对比解释方法来回答这个问题。我们的解释表明，LLM对给定提示输出回复的原因是，如果稍微修改提示，LLM会给出不同的回复，这些回复要么不太可取，要么与原始回复相矛盾。关键见解是，对比解释只需要对用户有意义的距离函数，而不是特定响应（即类别标签）的实际值表示。我们提供了两种寻找对比解释的算法：i）一种短视算法，虽然在创建对比方面有效，但需要多次模型调用；ii）一种预算算法，这是我们的主要算法贡献，它可以智能地创建符合查询预算的对比，这对于较长的上下文是必要的。我们展示了这些方法在各种自然语言任务上的有效性，例如开放文本生成、自动红队和解释对话降级。||
|**2024-06-17**|[Multi-Layer Ranking with Large Language Models for News Source Recommendation](http://arxiv.org/abs/2406.11745)|null|为了寻找新闻事件的可靠信息来源，我们引入了一项新的专家推荐任务，旨在根据专家先前引用的陈述来识别可信赖的来源。为此，我们构建了一个名为 NewsQuote 的新数据集，该数据集包含从新闻文章集合中提取的 23,571 对引言-发言者对。我们将推荐任务制定为根据专家与给定查询相关的可能性来检索专家。我们还提出了一个采用大型语言模型的多层排名框架，以提高推荐性能。我们的结果表明，采用基于上下文学习的 LLM 排名器和基于多层排名的过滤器可以显着提高推荐系统的预测质量和行为质量。||
|**2024-06-17**|[Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models](http://arxiv.org/abs/2406.11736)|**[link](https://github.com/xufangzhi/envisions)**|大型语言模型 (LLM) 性能优越的主要驱动力之一是用于对齐微调的大量人工标注自然语言数据的可用性。这促使研究人员探索自训练方法，以减少对人工标注的过度依赖。然而，目前自训练的成功主要是在自然语言场景中观察到的，而不是在越来越重要的神经符号场景中。为此，我们提出了一个名为 ENVISIONS 的环境引导神经符号自训练框架。它旨在克服两个主要挑战：(1) 符号数据的稀缺性，以及 (2) LLM 在处理符号语言方面的能力有限。在三个不同领域进行的广泛评估证明了我们方法的有效性。此外，我们还进行了全面分析，以揭示 ENVISIONS 成功的影响因素，从而为该领域的未来研究提供宝贵见解。代码将在 \url{https://github.com/xufangzhi/ENVISIONS} 提供。||
|**2024-06-17**|[Meta Reasoning for Large Language Models](http://arxiv.org/abs/2406.11698)|null|我们介绍了元推理提示 (MRP)，这是一种受人类元推理启发，为大型语言模型 (LLM) 设计的、新颖且高效的系统提示方法。传统的基于上下文学习的推理技术，例如思维树，虽然很有前景，但由于其专业性，缺乏跨不同任务的一致最先进性能。MRP 通过引导 LLM 根据每个任务的特定要求动态选择和应用不同的推理方法来解决这一限制，从而优化性能和计算效率。使用 MRP，LLM 推理分两个阶段运行。最初，LLM 使用任务输入线索和可用方法的目标描述来确定最合适的推理方法。随后，它应用所选方法来完成任务。这种动态策略反映了人类的元推理，使模型能够在广泛的问题领域中脱颖而出。我们通过综合基准评估了 MRP 的有效性。结果表明，MRP 在不同任务中均达到或接近最先进的性能。MRP 代表了使 LLM 能够识别跨问题的认知挑战并利用不同推理方法的优势的重大进步，从而增强了它们高效处理多样化和复杂问题领域的能力。每个 LLM 都应该有一个元推理提示，以充分发挥其潜力，并确保在不断变化的挑战和应用环境中的适应性。||
|**2024-06-17**|[HoLLMwood: Unleashing the Creativity of Large Language Models in Screenwriting via Role Playing](http://arxiv.org/abs/2406.11683)|null|生成式人工智能在计算机视觉领域展现出了前所未有的创造力，但在自然语言处理领域尚未观察到类似现象。特别是，由于文学创作的高度复杂性，大型语言模型（LLM）难以创作出达到人类专家水平的文学作品。在本文中，我们提出了 HoLLMwood，这是一个自动化框架，旨在释放 LLM 的创造力并探索其在剧本创作方面的潜力，而剧本创作是一项要求极高的任务。模仿人类创作过程，我们将 LLM 分配给现实场景中的不同角色。除了将 LLM 视为“编剧”的常见做法外，我们还将 LLM 用作“编辑”，负责向“编剧”提供反馈和修改建议。此外，为了丰富角色和深化情节，我们引入了角色扮演机制，并将 LLM 用作可以相互交流和互动的“演员”。对自动生成剧本的评估表明，HoLLMwood 在连贯性、相关性、趣味性和整体质量方面明显优于强大的基线模型。||

<p align=right>(<a href=#updated-on-20240622>back to top</a>)</p>

## Transformer

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-20**|[Revealing Vision-Language Integration in the Brain with Multimodal Networks](http://arxiv.org/abs/2406.14481)|null|我们使用（多）模态深度神经网络 (DNN) 来探测人脑中多模态整合的位点，方法是预测人类受试者观看电影时记录的脑电图 (SEEG)。我们将多模态整合的位点定义为：多模态视觉语言模型比单模态语言模型、单模态视觉模型或线性整合的语言视觉模型能更好地预测记录的区域。我们的目标 DNN 模型涵盖不同的架构（例如，卷积网络和变换器）和多模态训练技术（例如，交叉注意力和对比学习）。作为一个关键的实现步骤，我们首先证明训练过的视觉和语言模型在预测 SEEG 信号的能力方面系统地优于随机初始化的模型。然后，我们比较了单模态和多模态模型。由于我们的目标 DNN 模型通常具有不同的架构、参数数量和训练集（可能会掩盖那些可归因于整合的差异），因此我们对两个模型（SLIP 和 SimCLR）进行了控制比较，这两个模型除了输入模态外，其他所有属性都保持相同。使用这种方法，我们确定了大量的神经位点（平均 1090 个位点中的 141 个，或 12.94%）和似乎发生多模态整合的大脑区域。此外，我们发现在我们评估的多模态训练技术变体中，CLIP 风格的训练最适合于下游预测这些位点的神经活动。||
|**2024-06-20**|[SimulSeamless: FBK at IWSLT 2024 Simultaneous Speech Translation](http://arxiv.org/abs/2406.14177)|**[link](https://github.com/hlt-mt/fbk-fairseq)**|本文介绍了FBK团队参与IWSLT 2024同步翻译评测活动的情况。在今年的语音到文本翻译 (ST) 子赛道中，我们提出了SimulSeamless模型，该模型结合了AlignAtt和中等配置的SeamlessM4T模型。SeamlessM4T模型采用“开箱即用”的方式使用，并通过AlignAtt实现同步推理。AlignAtt是一种基于交叉注意力的SimulST策略，无需对底层模型进行任何重新训练或针对同步任务的调整即可应用。我们参与了所有共享任务语言（英语到德语、日语、汉语，以及捷克语到英语），取得了与去年提交的结果相当甚至更好的成绩。SimulSeamless模型涵盖143种以上源语言和200种目标语言，已发布在：https://github.com/hlt-mt/FBK-fairseq/。||
|**2024-06-20**|[Feature Fusion Based on Mutual-Cross-Attention Mechanism for EEG Emotion Recognition](http://arxiv.org/abs/2406.14014)|**[link](https://github.com/ztony0712/MCA)**|对于心理学家，特别是在治疗因病理原因难以沟通的患者时，客观准确的情绪诊断参考至关重要。然而，目前基于脑电图 (EEG) 数据用于情感识别系统存在一些问题，包括模型过于复杂、准确率平庸以及可解释性有限。因此，我们提出了一种新颖有效的特征融合机制，称为互交叉注意力机制（MCA）。这种纯粹的数学机制与专门定制的三维卷积神经网络 (3D-CNN) 相结合，巧妙地发现了脑电图数据中时域和频域特征之间的互补关系。此外，新设计的 Channel-PSD-DE 3D 特征也有助于实现高性能。所提出的方法最终在 DEAP 数据集上实现了 99.49%（效价）和 99.30%（唤醒度）的准确率。||
|**2024-06-19**|[StableSemantics: A Synthetic Language-Vision Dataset of Semantic Representations in Naturalistic Images](http://arxiv.org/abs/2406.13735)|null|理解视觉场景的语义是计算机视觉中的一个基本挑战。这项挑战的一个关键方面是，具有相似语义含义或功能的对象可能表现出显著的视觉差异，这使得准确识别和分类变得困难。文本到图像框架的最新进展催生了能够隐式捕捉自然场景统计数据的模型。这些框架考虑了对象的视觉可变性，以及复杂的对象共现和噪声源，如不同的光照条件。通过利用大规模数据集和交叉注意力调节，这些模型可以生成详细且上下文丰富的场景表示。这种能力为改进在多样化和具有挑战性的环境中的对象识别和场景理解开辟了新的途径。我们的工作提出了 StableSemantics，这是一个包含 224,000 个人工策划的提示、经过处理的自然语言标题、超过 200 万张合成图像以及对应于各个名词组块的 1,000 万个注意力图的数据集。我们明确利用了与视觉上有趣的稳定扩散生成相对应的人工生成的提示，每个短语提供 10 个生成，并提取每个图像的交叉注意力图。我们探索了生成图像的语义分布，检查了图像中对象的分布，并在我们的数据上对字幕和开放词汇分割方法进行了基准测试。据我们所知，我们是第一个发布具有语义属性的扩散数据集。我们期望我们提出的数据集能够推动视觉语义理解的进步，并为开发更复杂、更有效的视觉模型奠定基础。网站：https://stablesemantics.github.io/StableSemantics||
|**2024-06-19**|[In-Context Former: Lightning-fast Compressing Context for Large Language Model](http://arxiv.org/abs/2406.13618)|null|随着基于Transformer的大型语言模型（LLM）的日益普及，降低其高昂的推理成本已成为一个重要的研究方向。一种有效的方法是压缩较长的输入上下文。现有方法通常利用LLM本身的自注意力机制进行上下文压缩。虽然这些方法取得了显著成果，但压缩过程仍然涉及二次时间复杂度，这限制了它们的适用性。为了缓解这一限制，我们提出了上下文内编码器（IC-Former）。与之前的方法不同，IC-Former不依赖于目标LLM。相反，它利用交叉注意力机制和少量可学习的摘要标记来直接从上下文词嵌入中压缩信息。这种方法显著减少了推理时间，在压缩范围内实现了时间复杂度的线性增长。实验结果表明，我们的方法在压缩过程中仅需基线模型1/32的浮点运算量，并在保持90%以上基线性能指标的同时，将处理速度提高了68到112倍。总的来说，我们的模型有效地降低了压缩成本，并使实时压缩场景成为可能。||
|**2024-06-19**|[ECAFormer: Low-light Image Enhancement using Cross Attention](http://arxiv.org/abs/2406.13281)|null|弱光图像增强 (LLIE) 对于自动驾驶至关重要。尽管很重要，但现有的 LLIE 方法通常优先考虑整体亮度调整的稳健性，但这可能会以牺牲细节保留为代价。为了克服这一限制，我们提出了通过交叉注意转换器进行分层相互增强 (ECAFormer) 的方法，这是一种利用双多头自注意力 (DMSA) 来增强跨尺度的视觉和语义特征的新型网络，并在过程中显着保留细节。ECAFormer 中的交叉注意机制不仅改进了传统的增强技术，而且在保持全局亮度调整和局部细节保留之间的平衡方面表现出色。我们在著名的低照度数据集（包括 SID 和 LOL）上进行了广泛的实验验证，并在黑暗道路场景中进行了额外测试。在照明增强和降噪方面，我们的性能优于现有方法，同时还优化了计算复杂度和参数数量，进一步提升了 SSIM 和 PSNR 指标。我们的项目可在 https://github.com/ruanyudi/ECAFormer 获取。||
|**2024-06-19**|[Diffusion Model-based FOD Restoration from High Distortion in dMRI](http://arxiv.org/abs/2406.13209)|null|纤维方向分布 (FOD) 是表示扩散 MRI (dMRI) 数据的常用模型。然而，dMRI 中的成像伪影（如磁化率引起的失真）会导致信号丢失，并导致 FOD 重建损坏，从而阻碍在受影响的大脑区域（如脑干）中成功进行纤维追踪和连接分析。生成模型（如扩散模型）已成功应用于各种图像恢复任务。然而，它们在 FOD 图像上的应用带来了独特的挑战，因为 FOD 是由球谐函数 (SPHARM) 表示的 4 维数据，其第 4 维表现出与阶数相关的依赖性。在本文中，我们提出了一种新的用于 FOD 恢复的扩散模型，该模型可以恢复由失真伪影引起的信号丢失。我们使用体积阶数编码来增强扩散模型生成所有 SPHARM 阶数的单个 FOD 体积的能力。此外，我们在生成每个单独的 FOD 体积时，添加了跨所有 SPHARM 阶数提取的交叉注意特征，以捕获 FOD 体积之间与阶数相关的依赖性。我们还使用围绕高失真区域的低失真 FOD 对扩散模型进行条件化，以保持生成的 FOD 的几何一致性。我们使用来自英国生物银行的数据（n = 1315）训练和测试了我们的模型。在一个具有真实情况的测试集 (n = 43) 上，我们证明了生成的 FOD 在 FOD 体积的均方根误差和 FOD 峰值的角误差方面的精度很高。我们还将我们的方法应用于脑干区域失真较大的测试集 (n = 1172)，并证明了我们的方法在恢复 FOD 完整性方面的有效性，因此，极大地提高了受影响大脑区域的纤维束成像性能。||
|**2024-06-19**|[High-Fidelity Facial Albedo Estimation via Texture Quantization](http://arxiv.org/abs/2406.13149)|null|近年来，三维人脸重建方法在形状估计方面取得了显著进展，但高保真面部反照率重建仍然具有挑战性。现有方法依赖于昂贵的光场采集数据来学习面部反照率图。然而，受试者缺乏多样性限制了其恢复高保真结果的能力。在本文中，我们提出了一种新颖的面部反照率重建模型 HiFiAlbedo，它可以直接从单个图像中恢复反照率图，而无需采集反照率数据。我们的主要见解是，反照率图是不受光照影响的纹理图，这使我们能够使用廉价的纹理数据，通过消除光照来推导出反照率估计。为此，我们首先收集了大规模的超高分辨率人脸图像，并训练了一个高保真人脸纹理码本。通过使用 FFHQ 数据集和有限的 UV 纹理，我们随后微调编码器，以便在图像和 UV 空间中使用对抗性监督从输入图像中重建纹理。最后，我们训练了一个交叉注意模块，并利用组身份损失来学习从面部纹理到反照率域的适应。大量实验表明，我们的方法具有出色的泛化能力，并且能够实现野外人脸反照率恢复的高保真结果。我们的代码、预训练权重和训练数据将在 https://hifialbedo.github.io/ 上公开发布。||
|**2024-06-18**|[The Wisdom of a Crowd of Brains: A Universal Brain Encoder](http://arxiv.org/abs/2406.12179)|null|图像到功能性磁共振成像 (fMRI) 的编码对于神经科学研究和实际应用都至关重要。然而，这种“大脑编码器”通常是针对每个受试者和每个 fMRI 数据集进行训练的，因此仅限于非常有限的训练数据。在本文中，我们提出了一种通用大脑编码器，它可以利用来自许多不同受试者/数据集/机器的数据进行联合训练。 使这一切成为可能的是我们新的以体素为中心的编码器架构，它可以为每个大脑体素学习一个独特的“体素嵌入”。 我们的编码器通过直接计算大脑体素嵌入和多级深度图像特征之间的交叉注意力，来训练预测每个大脑体素对每个图像的响应。 这种以体素为中心的架构允许每个大脑体素的功能作用从体素-图像交叉注意力中自然地浮现出来。 我们展示了这种方法的强大功能，可以 (i) 结合来自多个不同受试者的数据（“大脑人群”）来改进每个个体的大脑编码，(ii) 跨受试者、数据集和机器（例如，3-特斯拉、7-特斯拉）快速有效的迁移学习，只需很少的训练样本，以及 (iii) 使用学习到的体素嵌入作为探索大脑功能的强大工具（例如，大脑中哪个位置编码了什么信息）。||
|**2024-06-17**|[Crossfusor: A Cross-Attention Transformer Enhanced Conditional Diffusion Model for Car-Following Trajectory Prediction](http://arxiv.org/abs/2406.11941)|null|车辆轨迹预测对于推进自动驾驶和高级驾驶辅助系统 (ADAS) 至关重要，可提高道路安全和交通效率。虽然传统方法奠定了基础，但现代深度学习技术，特别是基于 Transformer 的模型和生成方法，通过捕捉车辆运动和交通交互中复杂和非线性的模式，显著提高了预测精度。然而，这些模型往往忽略了现实驾驶场景中必不可少的详细跟车行为和车辆间交互。本研究介绍了一种专为跟车轨迹预测设计的交叉注意力 Transformer 增强型条件扩散模型 (Crossfusor)。Crossfusor 将详细的车辆间交互和跟车动力学集成到一个强大的扩散框架中，提高了预测轨迹的准确性和真实性。该模型利用一种结合了 GRU、基于位置的注意力机制和傅里叶嵌入的新型时间特征编码框架来捕捉历史车辆动力学。它在正向扩散过程中采用由这些编码的历史特征缩放的噪声，并在反向去噪过程中使用交叉注意力 Transformer 对复杂的车辆间依赖关系进行建模。在 NGSIM 数据集上的实验结果表明，Crossfusor 优于最先进的模型，特别是在长期预测方面，展示了其增强自动驾驶系统预测能力的潜力。||
|**2024-06-17**|[Composing Object Relations and Attributes for Image-Text Matching](http://arxiv.org/abs/2406.11820)|null|我们研究了用于图像-文本匹配的视觉语义嵌入问题。大多数现有工作利用定制的交叉注意力机制来执行跨图像和文本两种模态的局部对齐。尽管这种方法比单模态双编码器方法更强大，但计算成本很高。本工作介绍了一种双编码器图像-文本匹配模型，利用场景图来表示字幕，其中节点表示对象和属性，并通过关系边相互连接。我们的模型利用图注意力网络，有效地编码了对象-属性和对象-对象语义关系，从而形成了一个鲁棒且快速的系统。将字幕表示为场景图，可以利用图神经网络强大的关系归纳偏差来有效地学习对象-属性和对象-对象关系。为了训练模型，我们提出了在整体级别（图像-字幕）和局部级别（图像-对象实体）上对齐图像和字幕的损失函数，我们证明这是模型成功的关键。我们的模型被称为对象关系和属性组合模型，简称CORA。在两个著名的图像-文本检索基准数据集Flickr30K和MSCOCO上的实验结果表明，CORA在召回率方面优于现有的计算成本高昂的交叉注意力方法，同时实现了双编码器的快速计算速度。||
|**2024-06-17**|[AnyMaker: Zero-shot General Object Customization via Decoupled Dual-Level ID Injection](http://arxiv.org/abs/2406.11643)|**[link](https://github.com/lingjiekong-fdu/anymaker)**|基于文本到图像的对象定制旨在根据文本提示和参考图像生成具有相同身份（ID）的图像，这一领域已取得重大进展。然而，最近的定制研究主要集中在特定任务上，例如人物定制或虚拟试穿，而忽略了通用的对象定制。为此，我们推出了 AnyMaker，这是一个创新的零样本对象定制框架，能够生成具有高度ID保真度和灵活文本可编辑性的通用对象。AnyMaker 的功效源于其新颖的通用 ID 提取、双层 ID 注入和 ID 感知解耦。具体来说，通用 ID 提取模块使用一组自监督模型提取足够的 ID 信息，以处理针对通用对象的各种定制任务。然后，为了在不损害生成过程中文本可编辑性的情况下，为扩散 UNet 提供尽可能多的提取 ID 信息，我们设计了一个全局-局部双层 ID 注入模块，其中全局语义 ID 被注入文本描述中，而局部 ID 细节则通过新添加的交叉注意模块直接注入模型。此外，我们提出了一个 ID 感知解耦模块，用于从提取的表示中分离与 ID 相关的信息和非 ID 元素，以实现身份和文本描述的高保真生成。为了验证我们的方法并促进通用对象定制的研究，我们创建了第一个大规模通用 ID 数据集，即多类别 ID 一致性（MC-IDC）数据集，包含 31.5 万个文本图像样本和 1 万个类别。实验表明，AnyMaker 在通用对象定制方面表现出色，并在相应任务中优于专门方法。代码和数据集将很快发布。||
|**2024-06-17**|[Simple Yet Efficient: Towards Self-Supervised FG-SBIR with Unified Sample Feature Alignment](http://arxiv.org/abs/2406.11551)|null|细粒度草图图像检索 (FG-SBIR) 旨在最小化嵌入空间中草图和对应图像之间的距离。然而，解决方案日益复杂，主要是因为细粒度草图的抽象性，阻碍了可扩展性。在本文中，我们提出了一种简单而有效的方法来缩小两种模式之间的差距。它主要促进了样本内和样本间统一的互信息共享，而不是将它们视为模态之间的单一特征对齐问题。具体来说，我们的方法包括：(i) 采用双权重共享网络来优化草图和图像域内的对齐，这也有效地缓解了模型学习饱和问题。(ii) 引入基于对比损失的目标优化函数，以增强模型在样本内和样本间对齐特征的能力。(iii) 提出了一种由自注意力和交叉注意力相结合的可学习 TRSM，以促进标记之间的特征表示，进一步增强嵌入空间中的样本对齐。我们的框架在基于 CNN 和 ViT 的骨干网络上取得了优异的结果。大量实验表明，它优于现有方法。我们还介绍了 Cloths-V1，这是第一个专业的服装草图和图像数据集，用于验证我们的方法，并将有利于其他应用。||
|**2024-06-17**|[Learning from Exemplars for Interactive Image Segmentation](http://arxiv.org/abs/2406.11472)|null|交互式图像分割使用户能够以最少的交互与机器进行交互，从而逐步细化目标对象的分割掩码。先前的研究已经证明了通过交互式分割提取单个目标掩码的出色性能。然而，现有方法忽略了先前交互对象的语义线索，这些线索可以被进一步探索以加速对同一类别中多个目标的交互式分割。为此，我们针对单一对象和同一类别中的多个对象引入了新颖的交互式分割框架。具体来说，我们的模型利用 Transformer 骨干网络从图像和交互中提取以交互为中心的视觉特征，以获得令人满意的目标掩码作为样本。对于多个对象，我们提出了一个样本信息模块，以增强对目标类别对象之间相似性的学习。为了组合来自不同模块的注意力特征，我们结合了交叉注意力块和特征融合模块。在主流基准数据集上进行的实验表明，与以前的方法相比，我们的模型取得了优越的性能。特别是，我们的模型将用户的劳动量减少了约 15%，需要少点击两次才能达到目标 IoU 85% 和 90%。结果突出了我们的模型作为灵活实用的标注工具的潜力。源代码将在发表后发布。||
|**2024-06-17**|[Analysing the Behaviour of Tree-Based Neural Networks in Regression Tasks](http://arxiv.org/abs/2406.11437)|**[link](https://github.com/petersamoaa/tree_based_nn_error_analysis)**|深度学习领域，特别是利用抽象语法树 (AST) 等结构化表示，极大地扩展了源代码分析的边界。虽然这些方法已在分类任务中表现出有效性，但它们在回归应用（例如根据源代码预测执行时间）中的效果仍有待探索。本文致力于解码基于树的神经网络模型在此类回归挑战中的行为。我们将已建立的模型（基于树的卷积神经网络 (CNN)、Code2Vec 和基于 Transformer 的方法）的应用扩展到通过将源代码解析为 AST 来预测其执行时间。我们的比较分析表明，虽然这些模型是代码表示的基准，但在执行回归任务时表现出局限性。为了解决这些缺陷，我们提出了一种新颖的双 Transformer 方法，该方法同时对源代码标记和 AST 表示进行操作，并采用交叉注意力机制来增强两个域之间的可解释性。此外，我们还探索了图神经网络 (GNN) 对此基于树的问题的适应性，并从理论上论证了由于 AST 的图形性质而导致的 inherent 兼容性。对真实世界数据集的实证评估表明，我们的双 Transformer 模型优于所有其他基于树的神经网络和基于 GNN 的模型。此外，我们提出的双 Transformer 在不同的数据集上表现出卓越的适应性和鲁棒性能。||
|**2024-06-17**|[DiTTo-TTS: Efficient and Scalable Zero-Shot Text-to-Speech with Diffusion Transformer](http://arxiv.org/abs/2406.11427)|null|大型扩散模型在图像、视频和音频等多种模态中展现出卓越的生成能力。然而，文本到语音（TTS）系统通常涉及特定领域的建模因素（例如，音素和音素级时长）以确保文本和语音之间精确的时间对齐，这阻碍了扩散模型在 TTS 中的效率和可扩展性。在这项工作中，我们提出了一种高效且可扩展的扩散Transformer（DiT），它利用现成的预训练文本和语音编码器。我们的方法通过交叉注意力机制和语音表示总长度的预测来解决文本-语音对齐的挑战。为此，我们增强了 DiT 架构以适应 TTS，并通过将语义指导融入语音的潜在空间来改进对齐。我们将训练数据集和模型规模分别扩展到 82K 小时和 7.9 亿个参数。我们广泛的实验表明，这种无需特定领域建模的大型 TTS 扩散模型不仅简化了训练流程，而且在自然度、清晰度和说话人相似度方面，其零样本性能优于或可与最先进的 TTS 模型相媲美。我们的语音样本可在 https://ditto-tts.github.io 获取。||
|**2024-06-16**|[STAR: Scale-wise Text-to-image generation via Auto-Regressive representations](http://arxiv.org/abs/2406.10797)|**[link](https://github.com/krennic999/STAR)**|我们提出了STAR，这是一个采用尺度自回归范式的文本到图像模型。与仅限于在预定类别集合内进行类别条件合成的VAR不同，我们的STAR通过三个关键设计实现了文本驱动的开放式生成：为了提高具有未见对象和概念组合的多样性和泛化能力，我们引入了一个预训练的文本编码器来提取文本约束的表示，然后将其用作指导。为了改善生成的图像与细粒度文本指导之间的交互，使结果更可控，我们在每个尺度上都加入了额外的交叉注意力层。鉴于不同尺度之间存在自然的结构关联，我们利用二维旋转位置编码（RoPE）并将其调整为归一化版本。这确保了对不同尺度标记图中相对位置的一致解释，并稳定了训练过程。大量实验表明，STAR在保真度、图像文本一致性和美学质量方面都超过了现有基准。我们的研究结果强调了自回归方法在高质量图像合成领域的潜力，为目前以扩散方法为主导的T2I领域提供了新的方向。||
|**2024-06-15**|[CrossFuse: A Novel Cross Attention Mechanism based Infrared and Visible Image Fusion Approach](http://arxiv.org/abs/2406.10581)|**[link](https://github.com/hli1221/crossfuse)**|多模态视觉信息融合旨在将多传感器数据集成到单个图像中，该图像包含更多互补信息和更少的冗余特征。然而，互补信息很难提取，特别是对于红外图像和可见图像，这两种模态之间存在很大的相似性差距。常见的交叉注意力模块只考虑相关性，相反，图像融合任务需要关注互补性（非相关性）。因此，本文提出了一种新的交叉注意机制（CAM）来增强互补信息。此外，提出了一种基于两阶段训练策略的融合方案来生成融合图像。对于第一阶段，为每个模态训练两个具有相同架构的自编码器网络。然后，在固定编码器的情况下，在第二阶段训练CAM和解码器。通过训练好的CAM，从两种模态中提取的特征被集成到一个融合特征中，其中互补信息得到增强，冗余特征减少。最后，可以通过训练好的解码器生成融合图像。实验结果表明，与现有的融合网络相比，我们提出的融合方法获得了最先进的融合性能。代码可在https://github.com/hli1221/CrossFuse获取。||
|**2024-06-14**|[Whisper-Flamingo: Integrating Visual Features into Whisper for Audio-Visual Speech Recognition and Translation](http://arxiv.org/abs/2406.10082)|**[link](https://github.com/roudimit/whisper-flamingo)**|音频-视觉语音识别 (AVSR) 利用基于嘴唇的视频来提高在噪声环境下的性能。由于视频比音频更难获取，AVSR 模型的视频训练数据通常只有几千小时。相比之下，Whisper 等语音模型使用数十万小时的数据进行训练，因此可以学习到更好的语音到文本解码器。巨大的训练数据差异促使我们调整 Whisper 以处理视频输入。受 Flamingo 将视觉特征注入语言模型的启发，我们提出了 Whisper-Flamingo，它使用门控交叉注意力将视觉特征集成到 Whisper 语音识别和翻译模型中。在嘈杂条件下，我们的音频-视觉 Whisper-Flamingo 在 6 种语言的英语语音识别和英语-X 翻译方面优于纯音频 Whisper。此外，Whisper-Flamingo 是一个多功能模型，可以使用一组参数执行所有这些任务，而先前的方法是在每种语言上单独训练的。||
|**2024-06-14**|[Simul-Whisper: Attention-Guided Streaming Whisper with Truncation Detection](http://arxiv.org/abs/2406.10052)|**[link](https://github.com/backspacetg/simul_whisper)**|作为一种强大且规模庞大的多语言语音识别模型，Whisper 在许多低资源和 out-of-distribution 场景中表现出色。然而，其编码器-解码器结构阻碍了其在流式语音识别中的应用。在本文中，我们介绍了 Simul-Whisper，它利用 Whisper 交叉注意力机制中嵌入的时间对齐信息来指导自回归解码，并在无需对预训练模型进行任何微调的情况下实现基于块的流式自动语音识别。此外，我们观察到块边界处截断词对解码结果的负面影响，并提出了一种基于脉冲神经元的截断检测模型来解决这个问题。在多种语言和 Whisper 架构上的实验表明，Simul-Whisper 在 1 秒的块大小下平均绝对词错误率仅下降 1.46%，明显优于当前最先进的基线模型。||

<p align=right>(<a href=#updated-on-20240622>back to top</a>)</p>

## 3DGS

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-18**|[Sampling 3D Gaussian Scenes in Seconds with Latent Diffusion Models](http://arxiv.org/abs/2406.13099)|null|我们提出了一种基于三维场景的潜在扩散模型，该模型可以使用二维图像数据进行训练。为了实现这一目标，我们首先设计了一个自动编码器，将多视图图像映射到三维高斯球体，并同时构建这些球体的压缩潜在表示。然后，我们在潜在空间上训练一个多视图扩散模型，以学习一个高效的生成模型。该流程不需要对象掩码或深度信息，适用于具有任意相机位置的复杂场景。我们在两个大型复杂真实场景数据集（MVImgNet 和 RealEstate10K）上进行了仔细的实验。结果表明，我们的方法能够在短短 0.2 秒内生成三维场景，无论是从头开始生成、从单个输入视图生成还是从稀疏输入视图生成。它可以生成多样化的高质量结果，同时运行速度比非潜在扩散模型和早期的基于 NeRF 的生成模型快一个数量级。||
|**2024-06-18**|[HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors](http://arxiv.org/abs/2406.12459)|**[link](https://github.com/humansplat/humansplat.github.io)**|尽管高保真人体重建技术近期取得了进步，但对密集捕捉图像或耗时的逐实例优化的要求极大地阻碍了其在更广泛场景中的应用。为了解决这些问题，我们提出了 HumanSplat，它可以从单个输入图像中以可泛化的方式预测任何人的 3D 高斯 Splatting 属性。具体来说，HumanSplat 包含一个 2D 多视图扩散模型和一个具有人体结构先验的潜在重建Transformer，它能够将几何先验和语义特征巧妙地整合到一个统一的框架中。此外，我们还设计了一个包含人体语义信息的分层损失函数，以实现高保真纹理建模并更好地约束估计的多个视图。在标准基准数据集和真实世界图像上的综合实验表明，HumanSplat 在实现逼真的新视图合成方面超越了现有的最先进方法。||
|**2024-06-17**|[A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets](http://arxiv.org/abs/2406.12080)|null|近年来，新视角合成技术取得了重大进展，其中 3D 高斯 splatting 技术提供了出色的视觉质量、快速的训练速度和实时渲染能力。然而，训练和渲染所需的资源不可避免地限制了能够以良好视觉质量表示的捕获场景的大小。我们引入了一种 3D 高斯层次结构，它可以在保留超大场景视觉质量的同时，为有效地渲染远处内容提供高效的细节层次 (LOD) 解决方案，并在不同层次之间实现有效的级别选择和平滑过渡。我们引入了一种分治法，使我们能够在独立的块中训练超大场景。我们将这些块整合到一个层次结构中，该结构可以进行优化，以进一步提高合并到中间节点的高斯的视觉质量。超大规模的捕获通常场景覆盖率稀疏，这对原始的 3D 高斯 splatting 训练方法提出了许多挑战；我们对训练进行了调整和规范化，以解决这些问题。我们提出了一个完整的解决方案，借助我们的 LOD 方法，该解决方案能够实时渲染超大场景并适应可用资源。我们展示了使用简单且经济的设备捕获的场景的结果，这些场景包含多达数万张图像，涵盖了长达数公里、持续时间长达一小时的轨迹。项目页面：https://repo-sam.inria.fr/fungraph/hierarchical-3d-gaussians/||
|**2024-06-17**|[RetinaGS: Scalable Training for Dense Scene Rendering with Billion-Scale 3D Gaussians](http://arxiv.org/abs/2406.11836)|null|在这项工作中，我们探索了在大规模、高分辨率数据集上训练高参数三维高斯样条（3DGS）模型的可能性。我们为 3DGS 设计了一种通用的模型并行训练方法，称为 RetinaGS，它使用适当的渲染方程，可以应用于任何场景和任意分布的高斯基元。它使我们能够探索 3DGS 在基元数量和训练分辨率方面的缩放行为，这在以前难以探索，并超越了先前最先进的重建质量。我们观察到，使用我们的方法增加基元数量时，视觉质量有明显的积极趋势。我们还首次尝试在完整的 MatrixCity 数据集上训练具有超过十亿个基元的 3DGS 模型，该模型获得了良好的视觉质量。||
|**2024-06-18**|[Effective Rank Analysis and Regularization for Enhanced 3D Gaussian Splatting](http://arxiv.org/abs/2406.11672)|null|从多视图图像进行三维重建是计算机视觉和图形学中的基本挑战之一。最近，三维高斯 splatting  (3DGS) 已成为一种很有前途的技术，能够以高质量的三维重建实现实时渲染。该方法利用三维高斯表示和平铺 splatting 技术，绕过了昂贵的神经场查询。尽管具有潜力，但由于高斯函数会收敛成具有一个主要方差的各向异性高斯函数，3DGS 仍面临着挑战，包括针状伪影、次优几何形状和不准确的法线。我们建议使用有效秩分析来检查三维高斯基元的形状统计数据，并确定高斯函数确实会收敛成有效秩为 1 的针状形状。为了解决这个问题，我们引入了有效秩作为正则化，它限制了高斯函数的结构。我们新的正则化方法增强了法线和几何重建，同时减少了针状伪影。该方法可以作为附加模块集成到其他 3DGS 变体中，在不影响视觉保真度的情况下提高其质量。||
|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|在非结构化旅游环境中拍摄的照片经常表现出多变的外观和短暂的遮挡，这对精确的场景重建提出了挑战，并在新颖视图合成中引入了伪影。尽管先前的方法已经将神经辐射场 (NeRF) 与其他可学习模块相结合来处理动态外观并消除瞬态对象，但其广泛的训练需求和缓慢的渲染速度限制了实际部署。最近，3D 高斯 splatting (3DGS) 已成为 NeRF 的一种有前途的替代方案，它提供了卓越的训练和推理效率以及更好的渲染质量。本文介绍了 Wild-GS，这是一种针对不受约束的照片集优化的 3DGS 创新改编版，同时保留了其效率优势。Wild-GS 通过每张图像的固有材质属性、全局照明和相机属性以及逐点反射率的局部方差来确定每个 3D 高斯的外观。与先前在图像空间中对参考特征进行建模的方法不同，Wild-GS 通过对从参考图像中提取的三平面进行采样，将像素外观特征显式对齐到相应的局部高斯。这种新颖的设计有效地将参考视图的高频细节外观转移到 3D 空间，并显着加快了训练过程。此外，利用 2D 可见性图和深度正则化分别减轻瞬态效应和约束几何形状。大量实验表明，Wild-GS 在所有现有技术中实现了最先进的渲染性能以及最高的训练和推理效率。||
|**2024-06-14**|[L4GM: Large 4D Gaussian Reconstruction Model](http://arxiv.org/abs/2406.10324)|null|我们提出了L4GM，第一个能够从单视角视频输入生成动画对象的4D大型重建模型，只需一次几秒钟的前馈传递即可完成。我们成功的关键在于一个新颖的多视角视频数据集，其中包含从Objaverse中精选和渲染的动画对象。该数据集描绘了4.4万个不同的对象，包含11万个动画，以48个视角渲染，产生了1200万个视频，共计3亿帧。为了实现可扩展性，我们保持L4GM的简洁性，并直接构建在LGM之上，LGM是一个预训练的3D大型重建模型，可以从多视角图像输入中输出3D高斯椭球体。L4GM从以低帧率采样的视频帧中输出每帧3D高斯 splatting 表示，然后将表示上采样到更高的帧率以实现时间平滑度。我们在基础LGM中添加了时间自注意力层，以帮助它学习跨时间的一致性，并利用每时间步多视角渲染损失来训练模型。通过训练一个插值模型将表示上采样到更高的帧率，该模型产生中间3D高斯表示。我们展示了仅在合成数据上训练的L4GM在实际视频中也能很好地泛化，生成高质量的动画3D资产。||
|**2024-06-14**|[PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting](http://arxiv.org/abs/2406.10219)|null|近年来，新视角合成技术的进步实现了实时渲染速度和高重建精度。三维高斯 splatting (3D-GS) 是一种基础的基于点的三维场景参数化表示方法，它将场景建模为大量三维高斯函数的集合。复杂的场景可能包含数百万个高斯函数，导致巨大的存储和内存需求，限制了 3D-GS 在资源有限的设备上的可行性。目前，通过修剪高斯函数来压缩这些预训练模型的技术依赖于结合启发式方法来确定要移除哪些高斯函数。在本文中，我们提出了一种基于原理的空间敏感性修剪评分，其性能优于这些方法。它被计算为训练视图上的重建误差相对于每个高斯函数的空间参数的二阶近似。此外，我们提出了一个多轮修剪-优化流程，可以应用于任何预训练的 3D-GS 模型，而无需改变训练流程。在修剪了 88.44% 的高斯函数后，我们观察到我们的 PUP 3D-GS 流程将 3D-GS 的平均渲染速度提高了 2.65 倍，同时保留了更多显著的前景信息，并在来自 Mip-NeRF 360、Tanks & Temples 和 Deep Blending 数据集的场景上实现了比以往修剪技术更高的图像质量指标。||
|**2024-06-14**|[GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors](http://arxiv.org/abs/2406.10111)|null|从低分辨率输入视图实现高分辨率新视角合成（HRNVS）是一项具有挑战性的任务，因为缺乏高分辨率数据。以前的方法从低分辨率输入视图优化高分辨率神经辐射场（NeRF），但渲染速度慢。在这项工作中，我们基于 3D 高斯 splatting（3DGS）开发了我们的方法，因为它能够以更快的渲染速度生成高质量的图像。为了缓解高分辨率合成的数据短缺问题，我们建议利用现成的 2D 扩散先验，通过分数蒸馏采样（SDS）将 2D 知识提取到 3D 中。然而，由于生成先验带来的随机性，将 SDS 直接应用于基于高斯的 3D 超分辨率会导致不希望出现且冗余的 3D 高斯基元。为了缓解这个问题，我们引入了两种简单而有效的技术来减少 SDS 引入的随机扰动。具体来说，我们 1）使用退火策略缩小 SDS 中扩散时间步长的范围；2）在密集化过程中随机丢弃冗余的高斯基元。大量实验表明，我们提出的 GaussainSR 可以在合成和真实世界数据集上仅使用低分辨率输入就能获得高质量的 HRNVS 结果。项目页面：https://chchnii.github.io/GaussianSR/||
|**2024-06-14**|[GradeADreamer: Enhanced Text-to-3D Generation Using Gaussian Splatting and Multi-View Diffusion](http://arxiv.org/abs/2406.09850)|**[link](https://github.com/trapoom555/gradeadreamer)**|文本到3D生成已经展现出可观的结果，但仍然存在一些普遍的挑战，例如多面 Janus 问题以及生成高质量资源所需的时间过长。在本文中，我们通过引入一种名为 GradeADreamer 的新型三阶段训练流程来解决这些问题。该流程能够仅使用单个 RTX 3090 GPU 在 30 分钟内生成高质量的资源。我们提出的方法采用多视图扩散模型 MVDream 生成高斯点云作为先验，然后使用 StableDiffusion 优化几何形状和纹理。实验结果表明，与之前最先进的方法相比，我们的方法显著减轻了多面 Janus 问题，并实现了最高的平均用户偏好排名。项目代码可在 https://github.com/trapoom555/GradeADreamer 获取。||
|**2024-06-14**|[Unified Gaussian Primitives for Scene Representation and Rendering](http://arxiv.org/abs/2406.09733)|null|在计算机图形学中，寻找一种统一的场景表示方法仍然是一个挑战。传统的基于网格的表示方法不适用于密集、模糊的元素，并且在过滤和可微渲染方面引入了额外的复杂性。相反，基于体素的表示方法难以模拟坚硬的表面，并且存在内存需求大的问题。我们提出了一种基于三维高斯分布的通用渲染基元，用于统一场景表示，其特点是具有从光滑表面到模糊元素的多样化外观，以及基于物理的散射，以实现精确的全局照明。我们基于非指数传输制定了基元的渲染理论，并推导了与蒙特卡洛路径追踪兼容的高效渲染操作。这种新的表示方法可以从不同的来源转换而来，包括网格和三维高斯 splatting，并且由于其可微性，可以通过透射率优化进一步细化。我们展示了该表示方法在全局照明和外观编辑等各种渲染应用中的多功能性，同时本质上支持任意照明条件。此外，我们将我们的表示方法与现有的体积表示方法进行了比较，突出了其在细节再现方面的效率。||
|**2024-06-13**|[Modeling Ambient Scene Dynamics for Free-view Synthesis](http://arxiv.org/abs/2406.09395)|null|我们提出了一种新颖的动态自由视点合成方法，可以从单目捕捉中合成环境场景，为观看体验带来沉浸式的质量。我们的方法建立在3D高斯渲染(3DGS)的最新进展之上，该技术可以忠实地重建复杂的静态场景。先前将3DGS扩展到表示动态的尝试仅限于有界场景或需要多相机捕捉，并且通常无法泛化到未见过的运动，从而限制了它们的实际应用。我们的方法通过利用环境运动的周期性来学习运动轨迹模型，并结合仔细的正则化来克服这些限制。我们还提出了一些重要的实用策略，以提高基线3DGS静态重建的视觉质量，并提高对GPU内存密集型学习至关重要的内存效率。我们展示了几个具有复杂纹理和精细结构元素的周围自然场景的高质量逼真新颖视图合成。||
|**2024-06-13**|[GGHead: Fast and Generalizable 3D Gaussian Heads](http://arxiv.org/abs/2406.09377)|null|从大型二维图像集中学习三维头部先验信息是实现高质量三维感知人体建模的重要步骤。其核心要求是构建一种能够很好地扩展到大型数据集和高分辨率图像的有效架构。遗憾的是，现有的三维生成对抗网络（GAN）由于其训练和渲染速度相对较慢，难以扩展到生成高分辨率样本，并且通常不得不依赖二维超分辨率网络，但这是以牺牲全局三维一致性为代价的。为了应对这些挑战，我们提出了生成式高斯头部（GGHead）方法，该方法在三维 GAN 框架内采用了最新的三维高斯渲染表示。为了生成三维表示，我们采用了一个强大的二维卷积神经网络（CNN）生成器来预测模板头部网格的 UV 空间中的高斯属性。通过这种方式，GGHead 利用了模板 UV 布局的规律性，极大地促进了预测非结构化三维高斯集这一具有挑战性的任务。我们进一步通过一种新颖的渲染 UV 坐标上的总变差损失来提高生成的三维表示的几何保真度。直观地说，这种正则化鼓励相邻渲染像素应该来自模板 UV 空间中的相邻高斯。综上所述，我们的流程可以有效地生成仅从单视图二维图像观察中训练得到的三维头部。我们提出的框架在 FFHQ 数据集上的质量与现有的三维头部 GAN 相当，同时速度明显更快，并且完全保持了三维一致性。因此，我们首次展示了以 1024² 分辨率实时生成和渲染高质量、三维一致的头部。||

<p align=right>(<a href=#updated-on-20240622>back to top</a>)</p>

## 各类学习方式

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-20**|[Instruction Pre-Training: Language Models are Supervised Multitask Learners](http://arxiv.org/abs/2406.14491)|**[link](https://github.com/microsoft/lmops)**|无监督多任务预训练是近年来语言模型（LM）取得成功的关键方法。然而，监督多任务学习仍然具有巨大的潜力，因为在后训练阶段对其进行扩展往往会带来更好的泛化能力。在本文中，我们通过提出指令预训练来探索监督多任务预训练，该框架使用指令-响应对可扩展地扩充大量原始语料库，以预训练LM。指令-响应对由一个基于开源模型构建的高效指令合成器生成。在我们的实验中，我们合成了2亿个指令-响应对，涵盖40多个任务类别，以验证指令预训练的有效性。在从头开始的预训练中，指令预训练不仅持续增强了预训练的基础模型，而且从进一步的指令微调中获益更多。在持续预训练中，指令预训练使Llama3-8B能够与Llama3-70B相媲美，甚至超越后者。我们的模型、代码和数据可在https://github.com/microsoft/LMOps获取。||
|**2024-06-20**|[Revealing Vision-Language Integration in the Brain with Multimodal Networks](http://arxiv.org/abs/2406.14481)|null|我们使用（多）模态深度神经网络 (DNN) 来探测人脑中多模态整合的位点，方法是预测人类受试者观看电影时记录的脑电图 (SEEG)。我们将多模态整合的位点定义为：多模态视觉语言模型比单模态语言、单模态视觉或线性整合的语言视觉模型能更好地预测记录的区域。我们的目标 DNN 模型涵盖不同的架构（例如，卷积网络和变换器）和多模态训练技术（例如，交叉注意力和对比学习）。作为一个关键的启动步骤，我们首先证明经过训练的视觉和语言模型在预测 SEEG 信号的能力方面系统地优于随机初始化的模型。然后，我们将单模态和多模态模型相互比较。因为我们的目标 DNN 模型通常具有不同的架构、参数数量和训练集（可能会掩盖那些可归因于整合的差异），所以我们对两种模型（SLIP 和 SimCLR）进行了受控比较，这两种模型除了输入模态外，所有这些属性都保持不变。使用这种方法，我们确定了大量的神经位点（平均 1090 个位点中的 141 个，或 12.94%）和似乎发生多模态整合的大脑区域。此外，我们发现，在我们评估的各种多模态训练技术中，CLIP 风格的训练最适合下游预测这些位点的神经活动。||
|**2024-06-20**|[Podcast Outcasts: Understanding Rumble's Podcast Dynamics](http://arxiv.org/abs/2406.14460)|null|Rumble是一个替代性的视频分享平台，其播客吸引了以传播分裂性和经常误导性的内容而闻名的争议性人物，这与YouTube更加规范的环境形成了鲜明对比。鉴于播客对政治话语的影响越来越大，例如乔·罗根和安德鲁·泰特等人物，本文探讨了这些平台使用的政治偏见和内容策略。在本文中，我们对来自 YouTube 和 Rumble 的 1.3 万多个播客视频进行了全面分析，重点关注其政治内容和受众动态。使用先进的语音转文本转录、主题建模和对比学习技术，我们探索了三个关键方面：播客频道中政治偏见的存在、推动播客观看的内容的性质以及这些播客中视觉元素的使用。我们的研究结果表明，Rumble 的播客具有明显的右翼倾向，而 YouTube 的内容则更加多样化且非政治化。||
|**2024-06-20**|[Capturing Temporal Components for Time Series Classification](http://arxiv.org/abs/2406.14456)|null|在许多领域，分析序列数据至关重要，特别是考虑到物联网范式收集的大量数据。时间序列分类，即对序列数据进行分类的任务，已经变得越来越重要，机器学习方法在公共基准数据集上表现出卓越的性能。然而，目前的进展主要集中在设计架构，以便从固定（或理想）时间尺度的原始数据中学习表示，而这种架构可能无法推广到更长的序列。这项工作介绍了一种\textit{组合表示学习}方法，该方法在从序列数据中提取的统计一致的组件上进行训练。基于多尺度变化空间，提出了一种无监督方法，将序列数据分割成具有相似统计特性的块。在多任务环境下训练基于序列的编码器模型，以从这些时间组件中学习用于时间序列分类的组合表示。我们通过对公开可用的时间序列分类基准进行的大量实验，证明了其有效性。评估分段组件的一致性表明，它在无监督分段任务中具有竞争力。||
|**2024-06-20**|[Maintenance Required: Updating and Extending Bootstrapped Human Activity Recognition Systems for Smart Homes](http://arxiv.org/abs/2406.14446)|null|由于住宅布局、个性化设置以及居民特异行为的不同，为智能家居开发人类活动识别（HAR）系统并非易事。因此，现成的人类活动识别系统在单个家庭中的有效性有限，而且人类活动识别系统通常需要“从头开始”构建，这需要付出巨大的努力，而且往往会给居民带来负担。先前的工作已经成功地针对了初始阶段。在初始阶段结束时，我们确定了种子点。我们在自举人类活动识别系统的基础上，引入了一种有效的更新和扩展程序，用于持续改进人类活动识别系统，旨在跟上不断变化的生活环境。我们的方法利用了初始自举阶段结束时确定的种子点。我们使用这些种子点和为其获得的标签训练对比学习框架。然后，该模型用于提高识别的突出活动的分割精度。通过此过程改进活动识别系统有助于对智能家居中的大多数日常活动进行建模。我们通过 CASAS 数据集上的实验展示了我们程序的有效性，这些实验表明了我们方法的实用价值。||
|**2024-06-20**|[ATAC-Net: Zoomed view works better for Anomaly Detection](http://arxiv.org/abs/2406.14398)|null|由于深度学习在质量控制和制造业中的潜在用途，其在视觉异常检测中的应用已获得广泛普及。当前的标准方法是无监督的，它利用干净的数据集来检测偏差并在测试期间标记异常。然而，如果事先知道异常类型，则加入一些样本可以显著提高性能。因此，我们提出了 ATAC-Net，这是一个训练从最少的一组已知先验异常中检测异常的框架。此外，我们引入了注意力引导裁剪，它可以在训练阶段更近距离地观察可疑区域。我们的框架是一个可靠且易于理解的异常检测系统，并且我们证实了它在类似环境下优于当前一些最先进技术的优越性。||
|**2024-06-20**|[LARP: Language Audio Relational Pre-training for Cold-Start Playlist Continuation](http://arxiv.org/abs/2406.14333)|**[link](https://github.com/rsalganik1123/larp)**|随着在线音乐消费越来越倾向于基于播放列表的收听方式，播放列表延续的任务，即算法以个性化和音乐衔接的方式推荐歌曲以扩展播放列表，已经成为音乐流媒体成功的关键。目前，许多现有的播放列表延续方法依赖于协同过滤方法来执行推荐。然而，此类方法难以推荐缺乏交互数据的歌曲，这个问题被称为冷启动问题。目前应对这一挑战的方法是设计复杂的机制，从稀疏的协同数据中提取关系信号，并将其整合到内容表示中。然而，这些方法忽略了内容表示学习，并利用了预先训练好的、可能与特定音乐环境的分布或格式不一致的冻结内容模型。此外，即使是最先进的音乐内容模块也存在以下问题：(1) 与冷启动设置不兼容，或 (2) 无法有效地整合跨模态和关系信号。在本文中，我们介绍了 LARP，一个多模态冷启动播放列表延续模型，以有效地克服这些限制。LARP 是一个三阶段对比学习框架，它将多模态和关系信号整合到其学习的表示中。我们的框架使用递增的任务特定抽象阶段：曲目内（语言-音频）对比损失、曲目间对比损失和曲目-播放列表对比损失。在两个公开数据集上的实验结果表明，在冷启动设置下，LARP 在播放列表延续方面优于单模态和多模态模型。代码和数据集发布在：https://github.com/Rsalganik1123/LARP。||
|**2024-06-20**|[Revisiting Modularity Maximization for Graph Clustering: A Contrastive Learning Perspective](http://arxiv.org/abs/2406.14288)|**[link](https://github.com/edisonleeeee/magi)**|图聚类是图挖掘中一项基本且具有挑战性的任务，旨在将图中的节点分类为几个不相交的簇。近年来，图对比学习 (GCL) 已成为图聚类研究的主流方向，并推动了新的技术发展。然而，基于 GCL 的方法严重依赖于图增强和对比方案，这可能会带来语义漂移和可扩展性问题等挑战。另一个有前途的研究方向是采用模块化最大化作为聚类任务的指导原则，模块化最大化是一种流行且有效的社区检测方法。尽管最近取得了一些进展，但模块化最大化的潜在机制仍不清楚。在这项工作中，我们深入研究了模块化最大化在图聚类中的隐藏成功之处。我们的分析揭示了模块化最大化与图对比学习之间的密切联系，其中正例和负例由模块化自然定义。根据我们的结果，我们提出了一个社区感知图聚类框架，称为 MAGI，它利用模块化最大化作为对比性预训练任务，有效地揭示图中社区的底层信息，同时避免了语义漂移问题。在多个图数据集上的大量实验验证了 MAGI 在可扩展性和聚类性能方面优于最先进的图聚类方法。值得注意的是，MAGI 可以轻松扩展到具有 1 亿个节点的大型图，同时优于强大的基线方法。||
|**2024-06-20**|[Unleashing the Potential of Tracklets for Unsupervised Video Person Re-Identification](http://arxiv.org/abs/2406.14261)|null|基于视频的人员重识别方法具有丰富的时空信息，展现出广阔的应用前景。虽然可以使用现成的跟踪模型轻松获得轨迹，但对身份进行标注仍然成本高昂且不切实际。因此，一些基于视频的方法建议仅使用少量身份标注或摄像头标签来促进特征学习。这些方法通常简单地平均每个轨迹的帧特征，而忽略了轨迹内出现的意外变化和固有的身份一致性。在本文中，我们提出了自监督精细化聚类（SSR-C）框架，该框架不依赖任何标注或辅助信息来促进无监督视频人员重识别。具体来说，我们首先提出了噪声过滤轨迹划分（NFTP）模块，以减少由噪声跟踪结果引起的轨迹特征偏差，并将噪声过滤后的轨迹依次划分为“子轨迹”。然后，我们使用来自轨迹划分的自监督信号对子轨迹进行聚类和进一步合并，并通过渐进式策略增强该信号以生成可靠的伪标签，从而促进类内跨轨迹聚合。此外，我们提出了类别平滑分类（CSC）损失来有效地促进模型学习。在 MARS 和 DukeMTMC-VideoReID 数据集上的大量实验表明，我们提出的用于无监督视频人员重识别的 SSR-C 框架取得了最先进的结果，并且与先进的有监督方法相媲美。||
|**2024-06-20**|[Self-Supervised Pretext Tasks for Alzheimer's Disease Classification using 3D Convolutional Neural Networks on Large-Scale Synthetic Neuroimaging Dataset](http://arxiv.org/abs/2406.14210)|null|结构磁共振成像 (MRI) 研究表明，阿尔茨海默病 (AD) 会导致整个大脑发生局部和广泛的神经退行性改变。然而，缺乏突出大脑退行性改变的分割对以监督方式训练基于 CNN 的分类器提出了独特的挑战。在这项工作中，我们评估了几种无监督方法来训练特征提取器，用于下游 AD 与 CN 分类。使用来自合成神经影像 LDM100K 数据集的认知正常 (CN) 受试者的 3D T1 加权 MRI 数据，训练轻量级 3D 基于 CNN 的模型进行脑龄预测、脑图像旋转分类、脑图像重建以及将所有三个任务组合成一个的多头任务。在 LDM100K 合成数据集上训练的特征提取器与使用真实世界数据的相同模型相比取得了类似的性能。这支持了利用大规模合成数据进行预训练任务的可行性。所有训练和测试拆分均在受试者级别上执行，以防止数据泄漏问题。除了简单的预处理步骤外，随机裁剪数据增强技术在所有实验中都显示出一致的改进。||
|**2024-06-18**|[GroPrompt: Efficient Grounded Prompting and Adaptation for Referring Video Object Segmentation](http://arxiv.org/abs/2406.12834)|null|指代视频目标分割 (RVOS) 旨在分割整个视频中查询语句所指代的目标。大多数现有方法需要使用密集掩码注释进行端到端训练，这可能非常耗时且可扩展性较差。在这项工作中，我们旨在利用所提出的 Grounded Prompting (GroPrompt) 框架，有效地调整基础分割模型，以便从弱监督中解决 RVOS 问题。更具体地说，我们提出了文本感知提示对比学习 (TAP-CL)，仅使用边界框监督来增强位置提示和指代表达式之间的关联，包括分别在帧级别和视频级别的文本对比提示学习 (TextCon) 和模态对比提示学习 (ModalCon)。借助所提出的 TAP-CL，我们的 GroPrompt 框架可以生成描述视频中所指对象的位置和移动的时间一致且文本感知的位置提示。在标准 RVOS 基准测试（Ref-YouTube-VOS、Ref-DAVIS17、A2D-Sentences 和 JHMDB-Sentences）中的实验结果表明，在仅给出边界框弱监督的情况下，我们提出的 GroPrompt 框架具有竞争力的性能。||
|**2024-06-18**|[Unsupervised explainable activity prediction in competitive Nordic Walking from experimental data](http://arxiv.org/abs/2406.12762)|null|人工智能 (AI) 已应用于竞技体育中的人体活动识别 (HAR)。迄今为止，大多数用于 HAR 的机器学习 (ML) 方法都依赖于离线（批处理）训练，与在线处理无监督方法相比，这带来了更高的计算和标记负担。此外，传统 ML 预测器背后的决策是不透明的，需要人工解释。在这项工作中，我们应用了一种基于低成本可穿戴惯性测量单元 (IMU) 的在线处理无监督聚类方法。系统生成的结果允许在这些集群内自动扩展有限的可用标记（例如，由裁判标记），从而为可解释分类阶段生成相关信息。具体来说，我们的工作重点是实现与运动员活动相关的预测的自动可解释性，区分北欧式健走中的正确、错误和作弊行为。所提出的解决方案实现了接近 100% 的平均性能指标。||
|**2024-06-18**|[BIOSCAN-5M: A Multimodal Dataset for Insect Biodiversity](http://arxiv.org/abs/2406.12723)|**[link](https://github.com/zahrag/BIOSCAN-5M)**|作为一项持续进行的理解和监测昆虫生物多样性的全球努力的一部分，本文向机器学习社区推出了 BIOSCAN-5M 昆虫数据集，并建立了多个基准任务。BIOSCAN-5M 是一个综合数据集，包含超过 500 万个昆虫样本的多模态信息，它通过包含分类标签、原始核苷酸条形码序列、分配的条形码索引号和地理信息，显著扩展了现有的基于图像的生物数据集。我们提出了三个基准实验，以证明多模态数据类型对分类和聚类准确性的影响。首先，我们在 BIOSCAN-5M 数据集的 DNA 条形码序列上预训练了一个掩码语言模型，并证明了使用这个大型参考库对物种和属级别分类性能的影响。其次，我们提出了一个应用于图像和 DNA 条形码的零样本迁移学习任务，以对从自监督学习中获得的特征嵌入进行聚类，以研究是否可以从这些表示嵌入中导出有意义的聚类。第三，我们通过对 DNA 条形码、图像数据和分类信息进行对比学习来对多模态进行基准测试。这产生了一个通用的共享嵌入空间，可以使用多种类型的信息和模态进行分类。BIOSCAN-5M 昆虫数据集的代码库可在以下网址获得：{\url{https://github.com/zahrag/BIOSCAN-5M}}||
|**2024-06-18**|[Reproducibility of predictive networks for mouse visual cortex](http://arxiv.org/abs/2406.12625)|null|近年来，神经元活动的深度预测模型使得关于视觉皮层神经元选择性和不变性的若干新发现成为可能。这些模型学习一组共享的非线性基函数，这些基函数通过学习的权重向量进行线性组合，以表示神经元的函数。这种权重向量可以被认为是神经元功能的嵌入，已被提议通过无监督聚类来定义功能细胞类型。然而，由于深度模型通常高度过参数化，学习问题不太可能有唯一的解决方案，这就引发了一个问题，即这种嵌入是否可以以有意义的方式用于下游分析。在本文中，我们研究了神经元嵌入相对于模型架构和初始化变化的稳定性。我们发现 $L_1$ 正则是结构化嵌入的重要因素，并开发了一种自适应正则化，可以根据每个神经元调整正则化的强度。与统一正则化相比，这种正则化提高了预测性能，以及神经元嵌入在模型拟合中聚类的稳定性。为了克服过参数化，我们提出了一种迭代特征剪枝策略，可以在不损失性能的情况下将性能优化模型的维数减少一半，并提高神经元嵌入在聚类神经元方面的一致性。这一结果表明，为了实现细胞类型的客观分类或功能景观的紧凑表示，我们需要新的架构或学习技术来提高可识别性。我们将在发表时提供代码。||
|**2024-06-18**|[PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental Learning for Document Retrieval](http://arxiv.org/abs/2406.12593)|null|可微搜索索引 (DSI) 利用预训练语言模型 (PLM) 实现高效的文档检索，而无需依赖外部索引。然而，DSI 需要完全重新训练才能处理动态语料库中的更新，这会导致严重的计算效率低下问题。我们引入了 PromptDSI，这是一种基于提示的免排练实例级增量学习方法，用于文档检索。PromptDSI 将提示附加到 DSI 冻结的 PLM 编码器上，利用其强大的表示能力有效地索引新语料库，同时保持稳定性和可塑性之间的平衡。我们消除了基于提示的持续学习方法的初始前向传递，该传递会使训练和推理时间加倍。此外，我们提出了一个主题感知提示池，它使用神经主题嵌入作为固定键。该策略确保了多样化和有效的提示使用，解决了由于查询键匹配机制崩溃导致的参数利用不足的挑战。我们的实证评估表明，PromptDSI 在管理遗忘方面与 IncDSI 相匹配，同时在新语料库上显着提高了 4% 以上的召回率。||
|**2024-06-18**|[Unsupervised Online Continual Learning for Automatic Speech Recognition](http://arxiv.org/abs/2406.12503)|**[link](https://github.com/stevenvdeeckt/unsupervised-ocl-for-asr)**|将自动语音识别 (ASR) 模型适应新领域会导致对先前学习信息的灾难性遗忘 (CF)。本文解决了在线持续学习 (OCL) 挑战性背景下的 CF 问题，其中任务以具有未知边界的连续数据流的形式呈现。我们通过利用自训练 (ST) 来促进无监督适应，将 ASR 的 OCL 扩展到无监督领域，使模型能够在不依赖标签且不忘记先前知识的情况下不断适应。通过对两个领域适应实验中各种 OCL 和 ST 方法的比较分析，我们发现与监督 OCL 相比，无监督 OCL 遭受的遗忘要少得多，这使得 UOCL 方法能够接近监督 OCL 的性能水平。我们提出的 UOCL 扩展进一步提高了 UOCL 的效率。我们的研究结果代表着向持续适应性 ASR 系统迈出的重要一步，该系统能够利用跨不同领域的未标记数据。||
|**2024-06-18**|[RIGL: A Unified Reciprocal Approach for Tracing the Independent and Group Learning Processes](http://arxiv.org/abs/2406.12465)|null|在教育领域，自主学习和协作学习都被视为最经典的范式。前者允许学习者自主指导学习，而后者通常以教师指导为特征。近年来，智能教育领域的研究利用深度时间模型来追踪学习过程，捕捉学生知识状态的动态变化，并取得了显著成果。然而，现有方法主要集中于对自主学习过程的建模，而对协作学习范式的关注较少。此外，两种学习过程之间的相互影响，特别是它们在促进学生全面发展方面的综合潜力，仍未得到充分探索。为此，在本文中，我们提出了RIGL，一个统一的交互模型，用于在个体和群体层面追踪知识状态，该模型借鉴了自主学习和协作学习过程。具体而言，我们首先引入了一个时间帧感知的交互嵌入模块，用于同时对不同时间帧中学生和群体响应交互进行建模。随后，我们采用交互增强的学习模型来充分利用两种行为之间全面和互补的信息。此外，我们设计了一个关系引导的时间注意力网络，该网络由动态图建模和时间自注意力机制组成，用于深入研究个体和群体交互在整个学习过程中的动态影响。最后，我们引入了一个偏差感知的对比学习模块，以增强模型训练的稳定性。在四个真实世界教育数据集上的大量实验清楚地证明了所提出的RIGL模型的有效性。||
|**2024-06-18**|[Effective Generation of Feasible Solutions for Integer Programming via Guided Diffusion](http://arxiv.org/abs/2406.12349)|null|可行解对于整数规划 (IP) 至关重要，因为它们可以大大加快求解过程。在许多应用中，类似的 IP 实例通常表现出相似的结构和共享的解分布，这可以使用深度学习方法进行建模。不幸的是，现有的基于深度学习的算法，例如神经潜水和预测搜索框架，仅限于生成部分可行解，并且它们必须依赖 SCIP 和 Gurobi 等求解器来完成给定 IP 问题的解。在本文中，我们提出了一个新的框架，可以端到端地生成完整的可行解。我们的框架利用对比学习来表征 IP 实例和解之间的关系，并学习 IP 实例及其解的潜在嵌入。此外，该框架采用扩散模型来学习以 IP 表示为条件的解嵌入分布，并采用专门的引导采样策略来考虑约束和目标。我们根据 IP 问题的四个典型数据集对我们的框架进行了实证评估，结果表明，它可以有效地生成完整的可行解，并且概率很高（> 89.7%），而无需依赖求解器，并且解的质量可与 Gurobi 最好的启发式解相媲美。此外，通过将我们方法采样的部分解与 SCIP 的 CompleteSol 启发式算法相结合，所得的可行解优于所有数据集中的最新方法，与最优值的差距提高了 3.7% 到 33.7%，并在所有数据集中保持了超过 99.7% 的可行比率。||
|**2024-06-18**|[Toward Exploring the Code Understanding Capabilities of Pre-trained Code Generation Models](http://arxiv.org/abs/2406.12326)|null|近年来，以自监督方式在大量未标记编程语言数据上训练的大型代码生成模型取得了显著成功。虽然这些模型获得了大量的代码知识，但由于它们专门针对生成进行了训练，因此在代码理解任务（如代码搜索和克隆检测）中表现不佳。从头开始在海量代码数据上预训练一个更大的仅编码器架构模型可以提高理解性能。然而，这种方法成本高昂且耗时，因此并非最佳选择。在本文中，我们率先将知识从预训练的代码生成模型迁移到代码理解任务，显著降低了训练成本。我们研究了使仅解码器模型能够获得稳健代码表示的有效策略。此外，我们还引入了CL4D，这是一种对比学习方法，旨在增强仅解码器模型的表示能力。综合实验表明，我们的方法在代码搜索和克隆检测等理解任务中实现了最先进的性能。我们的分析表明，我们的方法有效地减少了表示空间中语义相同样本之间的距离。这些发现表明，可以使用仅解码器结构模型来统一代码理解和生成任务的潜力。||
|**2024-06-18**|[COT: A Generative Approach for Hate Speech Counter-Narratives via Contrastive Optimal Transport](http://arxiv.org/abs/2406.12304)|null|反叙事，即由基于事实的非攻击性论点组成的直接回应，已成为打击仇恨言论扩散的一种非常有效的方法。以前的方法主要侧重于微调和后期编辑技术，以确保生成内容的流畅性，而忽略了与特定仇恨目标（如LGBT群体、移民等）相关的个性化和相关性等关键方面。本研究论文介绍了一种基于对比最优传输的新框架，该框架有效地解决了在生成反叙事时维护目标交互和促进多样化的挑战。首先，利用最优传输核（OTK）模块将仇恨目标信息纳入到词元表示中，其中在原始特征和传输特征之间提取比较对。其次，采用自对比学习模块来解决模型退化的问题。该模块通过生成词元表示的各向异性分布来实现这一点。最后，集成了面向目标的搜索方法作为一种改进的解码策略，以在推理过程中明确促进领域相关性和多样性。该策略通过同时考虑词元相似性和目标相关性来修改模型的置信度得分。在两个基准数据集上进行了定量和定性实验，结果表明，我们提出的模型在多个方面的指标评估中明显优于当前的方法。||
|**2024-06-17**|[Mix-Domain Contrastive Learning for Unpaired H&E-to-IHC Stain Translation](http://arxiv.org/abs/2406.11799)|null|苏木精-伊红 (H&E) 染色到免疫组化 (IHC) 染色的图像转换技术为精确的癌症诊断提供了一种很有前景的解决方案，特别是在缺乏专业医疗人员和昂贵设备资源匮乏的地区。考虑到 H&E-IHC 图像对之间存在像素级别的错位，当前的研究探索了图像对相同位置的图像块之间的病理学一致性。然而，大多数方法过度强调域或图像块之间的对应关系，而忽略了非对应目标提供的辅助信息。在本文中，我们提出了一种混合域对比学习 (MDCL) 方法，以利用非配对 H&E 到 IHC 染色转换中的监督信息。具体来说，所提出的 MDCL 方法通过估计锚点图像块与匹配图像中所有图像块之间的相关性来聚合域间和域内的病理学信息，鼓励网络从混合域中学习额外的对比知识。通过混合域病理信息聚合，MDCL 增强了对应图像块之间的病理学一致性，以及生成的 IHC 图像中不同位置的图像块之间的成分差异。在两个 H&E 到 IHC 染色转换数据集（即 MIST 和 BCI）上的大量实验表明，所提出的方法在多个指标上均达到了最先进的性能。||
|**2024-06-17**|[A Brief Survey on Leveraging Large Scale Vision Models for Enhanced Robot Grasping](http://arxiv.org/abs/2406.11786)|null|在现实世界场景中，机器人抓取是一项艰巨的运动任务，构成了在各行各业部署高性能机器人的主要障碍。值得注意的是，数据的稀缺使得学习模型的抓取尤其具有挑战性。近年来，计算机视觉领域的进步见证了基于互联网海量数据的成功无监督训练机制的发展，现在几乎所有杰出的模型都利用了预训练的骨干网络。在此背景下，我们开始研究大规模视觉预训练在提高机器人抓取性能方面的潜在优势。这篇初步的文献综述阐明了关键挑战，并为机器人操作的视觉预训练的未来研究方向进行了展望。||
|**2024-06-17**|[DiffMM: Multi-Modal Diffusion Model for Recommendation](http://arxiv.org/abs/2406.11781)|**[link](https://github.com/hkuds/diffmm)**|像抖音和YouTube这样的在线多模态分享平台的兴起使得个性化推荐系统能够将多种模态（如视觉、文本和音频）纳入用户表示中。然而，解决这些系统中数据稀疏性的挑战仍然是一个关键问题。为了解决这一局限性，最近的研究引入了自监督学习技术来增强推荐系统。然而，这些方法通常依赖于简单的随机增强或直观的跨视图信息，这可能会引入不相关的噪声，并且无法准确地将多模态上下文与用户-项目交互建模对齐。为了填补这一研究空白，我们提出了一种新的用于推荐的多模态图扩散模型，称为DiffMM。我们的框架将模态感知图扩散模型与跨模态对比学习范式相结合，以改进模态感知用户表示学习。这种整合促进了多模态特征信息与协同关系建模之间更好的对齐。我们的方法利用扩散模型的生成能力自动生成一个感知不同模态的用户-项目图，从而促进在用户-项目交互建模中纳入有用的多模态知识。我们在三个公共数据集上进行了广泛的实验，一致证明了我们的DiffMM相对于各种竞争基线的优越性。有关开源模型实现的详细信息，您可以访问以下地址获取我们提出的框架的源代码：https://github.com/HKUDS/DiffMM。||
|**2024-06-17**|[Zero-Shot Generalization during Instruction Tuning: Insights from Similarity and Granularity](http://arxiv.org/abs/2406.11721)|**[link](https://github.com/hbx-hbx/dynamics_of_zero-shot_generalization)**|理解对齐技术始于理解指令微调带来的零样本泛化能力，但人们对其机制知之甚少。现有的研究主要局限于任务层面，而没有考虑到任务是人为定义的，对于大型语言模型来说，仅仅是由token和表示组成的。这条研究路线仅限于从任务对的角度研究任务之间的迁移，很少有研究从数据本身的角度理解零样本泛化。为了弥合这一差距，我们首先通过多个指标证明，指令微调过程中的零样本泛化发生在非常早期的阶段。接下来，我们从数据相似性和粒度两个角度研究了零样本泛化的促进因素，证实了在指令微调的早期阶段遇到高度相似和细粒度的训练数据，而不受限于定义的“任务”，能够实现更好的泛化。最后，我们提出了一种更合理的训练数据编排方法，即以测试为中心的多轮编排，并展示了其在促进持续学习和进一步降低损失方面的有效性。我们首次证明，指令微调过程中的零样本泛化是训练数据和测试数据之间在实例级别的基于相似性的泛化形式。我们希望我们的分析能够促进对指令微调过程中零样本泛化的理解，并有助于开发更对齐的大型语言模型。我们的代码发布在https://github.com/HBX-hbx/dynamics_of_zero-shot_generalization。||
|**2024-06-17**|[Multiple Descents in Unsupervised Learning: The Role of Noise, Domain Shift and Anomalies](http://arxiv.org/abs/2406.11703)|null|双下降现象最近在监督学习中受到关注。它挑战了偏差-方差权衡的传统观点，展示了一种令人惊讶的行为。随着模型复杂度的增加，测试误差最初会下降，直到达到模型开始过拟合训练集的某个点，导致测试误差上升。然而，与经典理论不同的是，当超过一定程度的过参数化时，误差会再次下降。我们研究了双下降现象在无监督学习中的存在，这是一个很少受到关注且尚未完全理解的领域。我们使用欠完备自动编码器 (AE) 对各种应用进行了广泛的实验，例如处理噪声数据、域偏移和异常。我们使用合成数据和真实数据，并确定了上述所有应用中模型、时期和样本方面的双下降现象。最后，我们评估了自动编码器在检测异常和减轻数据集之间域偏移方面的可用性。我们的研究结果表明，过参数化模型不仅可以提高重建性能，还可以增强下游任务的能力。||
|**2024-06-17**|[Making Old Things New: A Unified Algorithm for Differentially Private Clustering](http://arxiv.org/abs/2406.11649)|null|作为数据分析和无监督学习的支柱，私有聚类问题已在各种隐私模型下得到了广泛研究。集中式差分隐私是其中最早的模型，并且该问题也已针对本地和随机变换进行了研究。在每种情况下，目标都是设计一种算法，以尽可能小的误差私下计算聚类。对每种变体的研究都催生了新的算法：因此，私有聚类算法的图景相当复杂。在本文中，我们展示了一种已有 20 年历史的算法，只需稍作修改即可适用于任何这些模型。这提供了一个统一的视角：在匹配几乎所有先前已知结果的同时，它允许我们改进其中的一些结果，并将其扩展到一个新的隐私模型，即持续观察设置，其中输入随时间而变化，并且算法必须在每个时间步输出一个新的解决方案。||
|**2024-06-17**|[Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!](http://arxiv.org/abs/2406.11629)|null|利用大型语言模型 (LLM) 作为评判者来评估 LLM 的性能最近受到了关注。然而，这种方法同时引入了来自 LLM 的潜在偏差，引发了对评估结果可靠性的担忧。为了缓解这个问题，我们提出并研究了两种版本的少样本上下文提示，即强化和无监督 ICL，用于帮助 GPT-4o 作为评判者进行单答案评分。基于设计的提示，我们研究了扩展上下文示例数量对评估的一致性和质量的影响。此外，我们首先揭示了 GPT-4o 作为评判者在成对比较中的符号偏差，然后提出了一种简单而有效的方法来减轻它。实验结果表明，先进的长上下文 LLM，例如 GPT-4o，在少样本机制中的表现优于零样本机制。同时，实验结果进一步验证了符号偏差缓解方法的有效性。||
|**2024-06-17**|[Wide Area VISTA Extra-galactic Survey (WAVES): Unsupervised star-galaxy separation on the WAVES-Wide photometric input catalogue using UMAP and ${\rm{\scriptsize HDBSCAN}}$](http://arxiv.org/abs/2406.11611)|null|星系分离是创建河外光谱巡天目标星表的关键步骤。倾向于包容性的分类器可能会将伪星包含在内，浪费光纤时间，而更保守的分类器可能会忽略星系，从而损害完整性，进而影响巡天目标。为了避免监督方法中训练集引入的偏差，我们采用了一种无监督机器学习方法。我们利用广域 VISTA 河外星系巡天 (WAVES)-Wide 星表的光度数据，该星表包含 9 波段 $u-K_s$ 数据，并利用 ${\rm P{\scriptsize RO} F{\scriptsize OUND}}$ 提取颜色、流量和视尺寸信息，创建了一个特征空间。我们应用非线性降维方法 UMAP（均匀流形近似和投影）结合分类器 ${\rm{\scriptsize HDBSCAN}}$ 对恒星和星系进行分类。我们使用来自 Gaia、SDSS、GAMA 和 DESI 的真实星表，根据基线颜色和形态方法验证了我们的方法。在 AB 星等限制为 $Z = 21.2$ 的情况下，我们正确识别了 99.72% 的星系，在整个真实样本中，F1 得分为 0.9970，而基线方法的 F1 得分为 0.9871。与基线方法 (0.9780) 相比，我们的方法具有更高的纯度 (0.9966)，从而提高了效率，识别出的星系或不明来源减少了 11%，在 4MOST 仪器上节省了大约 70,000 个光纤小时。我们获得了具有挑战性的来源（包括类星体、致密星系和低表面亮度星系）的可靠分类统计数据，分别检索到其中的 95.1%、84.6% 和 99.5%。角聚类分析验证了我们的分类，表明无论基线分类如何，都与预期的星系聚类一致。||
|**2024-06-17**|[CM2-Net: Continual Cross-Modal Mapping Network for Driver Action Recognition](http://arxiv.org/abs/2406.11340)|null|驾驶员动作识别通过整合红外和深度等多种模态，在增强驾驶员与车辆的交互和确保驾驶安全方面取得了显著进展。然而，与仅使用RGB模态相比，在车厢环境中为所有类型的非RGB模态收集大量数据始终是一项费力且昂贵的工作。因此，以前的工作建议通过微调在RGB视频上预训练的模型来独立学习每个非RGB模态，但这些方法在面对新出现的模态时，由于较大的域差异，在提取信息特征方面效果较差。相比之下，我们提出了一种连续跨模态映射网络（CM2-Net），利用先前学习的模态的指导性提示，持续学习每个新出现的模态。具体来说，我们开发了累积跨模态映射提示（ACMP），将从先前模态中学习到的判别性和信息性特征映射到新出现的模态的特征空间中。然后，当面对新出现的模态时，这些映射的特征能够为应该提取和优先考虑哪些特征提供有效的提示。这些提示在整个持续学习过程中不断积累，从而进一步提高识别性能。在Drive&Act数据集上进行的大量实验表明，CM2-Net在单模态和多模态驾驶员动作识别方面均具有优越的性能。||
|**2024-06-17**|[Syn-to-Real Unsupervised Domain Adaptation for Indoor 3D Object Detection](http://arxiv.org/abs/2406.11311)|null|在室内3D目标检测中使用合成数据，可以大大减少3D标注所需的人工，并训练有效的零样本检测器。然而，跨越合成到真实室内数据集的复杂域偏移问题仍未得到充分探索。本文提出了一种新的面向对象的层次化域对齐（OHDA）框架，用于室内3D目标检测中的合成到真实无监督域自适应。我们的方法包括一种对象感知增强策略，以有效地使源域数据多样化，并且我们引入了一个由对抗训练分支和伪标签分支组成的双分支自适应框架，以便同时实现整体级别和类别级别的域对齐。针对室内无监督域自适应，我们提出了两种专门设计的方案，进一步改进了伪标签。我们从合成数据集3D-FRONT到真实世界数据集ScanNetV2和SUN RGB-D的适应结果表明，与仅使用源数据的基线相比，mAP25分别显著提高了9.7%和9.1%，并且始终优于从2D和3D室外场景适应的方法。代码将在论文被接收后公开。||

<p align=right>(<a href=#updated-on-20240622>back to top</a>)</p>

