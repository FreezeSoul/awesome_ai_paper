## Updated on 2024.07.17
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#多模态>多模态</a></li>
    <li><a href=#6dof-object-pose>6DOF Object Pose</a></li>
    <li><a href=#nerf>nerf</a></li>
    <li><a href=#分类/检测/识别/分割>分类/检测/识别/分割</a></li>
    <li><a href=#生成模型>生成模型</a></li>
    <li><a href=#llm>LLM</a></li>
    <li><a href=#transformer>Transformer</a></li>
  </ol>
</details>

## 多模态

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-07-16**|[Reflective Instruction Tuning: Mitigating Hallucinations in Large Vision-Language Models](http://arxiv.org/abs/2407.11422)|null|大型视觉语言模型 (LVLM) 在各种视觉语言任务中表现出了良好的性能。然而，它们仍然容易产生幻觉，生成与视觉内容或指令不一致的输出。虽然已经提出了各种缓解策略，但它们往往忽略了导致幻觉的一个关键因素：训练期间缺乏细粒度推理监督。如果没有中间推理步骤，模型可能会在指令和响应之间建立肤浅的捷径，无法内化固有的推理逻辑。为了解决这一挑战，我们提出了反思性指令微调，它将逻辑推理学习融入到视觉指令微调中。与以往仅从响应中学习的方法不同，我们的方法要求模型预测逻辑推理，解释为什么响应是正确或不正确的。这促使模型更深入地参与每个响应背后的细粒度推理，从而提高模型的推理能力。为了促进这种方法，我们提出了 REVERIE，这是第一个具有反思性逻辑推理注释的大规模指令微调数据集。REVERIE 包含 115k 个机器生成的推理指令，每个指令都经过精心标注，包含一对正确的和令人困惑的响应，以及解释每个响应正确性或错误性背后的理由的全面逻辑推理。在多个 LVLM 基准测试上的实验结果表明，使用 REVERIE 数据集进行反思性指令微调比基线模型产生了明显的性能提升，证明了从逻辑推理中反思的有效性。项目页面位于 https://zjr2000.github.io/projects/reverie。|
|**2024-07-16**|[Mask-Free Neuron Concept Annotation for Interpreting Neural Networks in Medical Domain](http://arxiv.org/abs/2407.11375)|**[link](https://github.com/ailab-kyunghee/mammi)**|近年来，深度神经网络的进步为辅助疾病诊断和医疗决策带来了希望。然而，为了确保人工智能模型的决策过程符合法规要求，需要全面了解模型的内部运作机制。然而，以往的方法在很大程度上依赖于昂贵的像素级标注数据集来解释模型，这在医学领域是一个很大的缺陷。在本文中，我们提出了一种新的医学神经元概念标注方法，称为无掩码医学模型解释 (MAMMI)，以解决这些挑战。通过使用视觉语言模型，我们的方法不再需要像素级掩码来进行神经元概念标注。与其他解释方法相比，MAMMI 取得了更优越的性能，证明了其在为医学图像分析中的神经元提供丰富表示方面的有效性。我们对在 NIH 胸部 X 光片上训练的模型进行的实验验证了 MAMMI 的有效性，展示了其在医学领域实现透明临床决策的潜力。代码可在 https://github.com/ailab-kyunghee/MAMMI 获取。|
|**2024-07-16**|[LaMI-DETR: Open-Vocabulary Detection with Language Model Instruction](http://arxiv.org/abs/2407.11335)|null|现有的开放词汇目标检测方法通过利用视觉语言模型 (VLM)（例如 CLIP）强大的开放词汇识别能力来增强性能。然而，出现了两个主要挑战：(1) 概念表示的缺陷，其中 CLIP 文本空间中的类别名称缺乏文本和视觉知识。(2) 对基本类别的过度拟合趋势，在从 VLM 到检测器的迁移过程中，开放词汇知识偏向于基本类别。为了应对这些挑战，我们提出了语言模型指令 (LaMI) 策略，该策略利用视觉概念之间的关系，并在一种简单而有效的类 DETR 检测器（称为 LaMI-DETR）中应用它们。LaMI 利用 GPT 来构建视觉概念，并利用 T5 来研究跨类别的视觉相似性。这些类别间关系改进了概念表示，并避免了对基本类别的过度拟合。全面的实验验证了我们的方法在相同的严格设置下优于现有方法，并且不依赖于外部训练资源。LaMI-DETR 在 OV-LVIS 上实现了 43.4 的罕见框 AP，超过了之前的最佳结果 7.8 个罕见框 AP。|
|**2024-07-16**|[Large Vision-Language Models as Emotion Recognizers in Context Awareness](http://arxiv.org/abs/2407.11300)|null|上下文感知情绪识别 (CAER) 是一项复杂且重要的任务，需要从各种上下文线索中感知情绪。先前的方法主要侧重于设计复杂的架构来从图像中提取情绪线索。然而，它们的知识仅限于特定的训练数据集，并且可能反映了标注者主观的情绪偏差。此外，在现实应用中获取大量标记数据通常具有挑战性。在本文中，我们系统地探索了利用大型视觉语言模型 (LVLM) 从三个范式赋能 CAER 任务的潜力：1) 我们在两个 CAER 数据集上微调 LVLM，这是将大型模型迁移到下游任务的最常见方式。2) 我们设计了零样本和少样本模式来评估 LVLM 在数据有限甚至完全不可见的情况下的性能。在这种情况下，我们提出了一个免训练框架来充分利用 LVLM 的上下文学习 (ICL) 能力。具体来说，我们开发了一种基于图像相似度的排序算法来检索示例；随后，将指令、检索到的示例和测试示例组合起来输入 LVLM 以获得相应的情绪判断。3) 为了利用 LVLM 丰富的知识库，我们将思维链 (CoT) 纳入我们的框架，以增强模型的推理能力并提供可解释的结果。广泛的实验和分析表明，LVLM 在不同范式下的 CAER 任务中均取得了具有竞争力的性能。值得注意的是，少样本设置下的优越性能表明 LVLM 在无需大量训练的情况下完成特定任务的可行性。|
|**2024-07-15**|[OpenPSG: Open-set Panoptic Scene Graph Generation via Large Multimodal Models](http://arxiv.org/abs/2407.11213)|null|全景场景图生成 (PSG) 旨在分割对象并识别它们之间的关系，从而实现对图像的结构化理解。先前的方法侧重于预测预定义的对象和关系类别，因此限制了它们在开放世界场景中的应用。随着大型多模态模型 (LMM) 的快速发展，开放集目标检测和分割取得了重大进展，但 PSG 中的开放集关系预测仍未得到探索。在本文中，我们专注于与预训练的开放集全景分割模型相结合的开放集关系预测任务，以实现真正的开放集全景场景图生成 (OpenPSG)。我们的 OpenPSG 利用 LMM 以自回归的方式实现开放集关系预测。我们引入了一个关系查询转换器来有效地提取对象对的视觉特征并估计它们之间关系的存在。后者可以通过过滤不相关的对来提高预测效率。最后，我们设计了生成和判断指令，以在 PSG 中以自回归的方式执行开放集关系预测。据我们所知，我们是第一个提出开放集 PSG 任务的人。大量实验表明，我们的方法在开放集关系预测和全景场景图生成方面实现了最先进的性能。代码可在 \url{https://github.com/franciszzj/OpenPSG} 获取。|
|**2024-07-15**|[Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?](http://arxiv.org/abs/2407.10956)|**[link](https://github.com/xlang-ai/spider2-v)**|数据科学和工程工作流程通常跨越多个阶段，从仓储到编排，使用BigQuery、dbt和Airbyte等工具。随着视觉语言模型（VLM）在多模态理解和代码生成方面的进步，基于VLM的代理可以通过生成SQL查询、Python代码和GUI操作来自动化这些工作流程。这种自动化可以提高专家的生产力，同时使大规模数据分析更容易获得。在本文中，我们介绍了Spider2-V，这是第一个专注于专业数据科学和工程工作流程的多模态代理基准测试，它包含494个真实计算机环境中的真实任务，并整合了20个企业级专业应用程序。这些任务源自现实世界的用例，评估多模态代理通过在企业数据软件系统中编写代码和管理GUI来执行数据相关任务的能力。为了平衡真实模拟和评估的简单性，我们致力于开发任务设置的自动配置，并为每个任务精心设计评估指标。此外，我们还为多模态代理提供了这些企业数据软件系统的全面文档。我们的实证评估表明，现有的最先进的基于LLM/VLM的代理无法可靠地自动化完整的数据工作流程（成功率为14.0%）。即使在逐步指导下，这些代理在需要细粒度、知识密集型GUI操作（16.2%）和涉及远程云托管工作区（10.6%）的任务中仍然表现不佳。我们希望Spider2-V能够为自主多模态代理转变数据科学和工程工作流程的自动化铺平道路。我们的代码和数据可在https://spider2-v.github.io获取。|
|**2024-07-15**|[Benchmarking Vision Language Models for Cultural Understanding](http://arxiv.org/abs/2407.10920)|null|Foundation models and vision-language pre-training have notably advanced Vision Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their performance has been typically assessed on general scene understanding - recognizing objects, attributes, and actions - rather than cultural comprehension. This study introduces CulturalVQA, a visual question-answering benchmark aimed at assessing VLM's geo-diverse cultural understanding. We curate a collection of 2,378 image-question pairs with 1-5 answers per question representing cultures from 11 countries across 5 continents. The questions probe understanding of various facets of culture such as clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on CulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of cultural understanding across regions, with strong cultural understanding capabilities for North America while significantly lower performance for Africa. We observe disparity in their performance across cultural facets too, with clothing, rituals, and traditions seeing higher performances than food and drink. These disparities help us identify areas where VLMs lack cultural understanding and demonstrate the potential of CulturalVQA as a comprehensive evaluation set for gauging VLM progress in understanding diverse cultures.|
|**2024-07-15**|[GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via VLM](http://arxiv.org/abs/2407.10870)|null|大型视觉语言模型（LVLM），例如生成式预训练 Transformer 4-omni（GPT-4o），是新兴的多模态基础模型，在医疗保健、工业和学术领域等无数应用中，作为强大的人工智能（AI）辅助工具具有巨大潜力。 尽管此类基础模型在各种通用任务中表现良好，但在没有微调的情况下，它们在专业任务中的能力通常有限。 然而，由于巨大的计算/内存/数据集要求，对大型基础模型进行全面微调具有挑战性。 我们证明，即使没有微调，GPT-4o 也可以解码来自前臂超声数据的  手势，并且可以通过少量样本的上下文学习得到改进。|
|**2024-07-15**|[FabGPT: An Efficient Large Multimodal Model for Complex Wafer Defect Knowledge Queries](http://arxiv.org/abs/2407.10810)|null|智能是推进集成电路（IC）制造的关键。大型多模态模型（LMM）的最新突破解锁了图像和文本理解的空前能力，促进了智能制造的发展。我们利用LMM的强大功能，推出了FabGPT，这是一种用于晶圆缺陷知识查询的定制化IC制造大型多模态模型。FabGPT在扫描电子显微镜（SEM）图像缺陷检测、根本原因分析以及提供有关制造工艺的专家问答（Q&A）方面表现出色。FabGPT匹配增强的多模态特征，自动检测复杂晶圆背景下的微小缺陷，并减少手动阈值设置的主观性。此外，所提出的调制模块和交互式语料库训练策略将晶圆缺陷知识嵌入到预训练模型中，有效地平衡了与缺陷知识和原始知识相关的问答查询，并减轻了模态偏差问题。对内部晶圆厂数据（SEM-WaD）的实验表明，我们的FabGPT在晶圆缺陷检测和知识查询方面实现了显著的性能提升。|
|**2024-07-16**|[Qwen2 Technical Report](http://arxiv.org/abs/2407.10671)|null|This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning.   The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach.   To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.|
|**2024-07-12**|[Open Vocabulary Multi-Label Video Classification](http://arxiv.org/abs/2407.09073)|null|预训练的视觉语言模型（VLM）在开放词汇计算机视觉任务（例如图像分类、目标检测和图像分割）方面取得了重大进展。最近的一些工作集中于将VLM扩展到视频中的开放词汇单标签动作分类。然而，先前的方法在整体视频理解方面存在不足，整体视频理解需要在开放词汇环境中同时识别多个动作和实体（例如视频中的对象）的能力。我们将此问题表述为开放词汇多标签视频分类，并提出了一种方法来调整预训练的VLM（例如CLIP）以解决此任务。我们利用大型语言模型（LLM）为VLM提供关于类别标签的语义指导，通过两个关键贡献来提高其开放词汇性能。首先，我们提出了一种端到端可训练架构，该架构学习提示LLM为CLIP文本编码器生成软属性，使其能够识别新类别。其次，我们将时间建模模块集成到CLIP的视觉编码器中，以有效地对视频概念的时空动态进行建模，并提出了一种新颖的正则化微调技术，以确保在视频域中强大的开放词汇分类性能。我们广泛的实验结果证明了我们的方法在多个基准数据集上的有效性。|
|**2024-07-12**|[LAPT: Label-driven Automated Prompt Tuning for OOD Detection with Vision-Language Models](http://arxiv.org/abs/2407.08966)|**[link](https://github.com/ybzh/lapt)**|分布外 (OOD) 检测对于模型可靠性至关重要，因为它可以识别来自未知类的样本并减少由于意外输入造成的错误。视觉语言模型 (VLM)（例如 CLIP）通过集成多模态信息，正在成为用于 OOD 检测的强大工具。然而，此类系统的实际应用受到手动提示工程的挑战，这需要领域专业知识并且对语言细微差别很敏感。在本文中，我们介绍了标签驱动的自动提示调整 (LAPT)，这是一种减少手动提示工程需求的新型 OOD 检测方法。我们使用自动挖掘的分布内 (ID) 类名和负标签来开发感知分布的提示。通过图像合成和检索方法自动收集链接到这些类标签的训练样本，从而无需手动即可进行提示学习。我们利用简单的交叉熵损失进行提示优化，并采用跨模态和跨分布混合策略分别减少图像噪声和探索分布之间的中间空间。LAPT 框架自主运行，只需要 ID 类名作为输入，无需人工干预。通过大量实验，LAPT 始终优于手动制作的提示，为 OOD 检测树立了新标准。此外，LAPT 不仅增强了 ID 和 OOD 样本之间的区别，还提高了 ID 分类精度并增强了对协变量偏移的泛化鲁棒性，从而在具有挑战性的全谱 OOD 检测任务中获得了出色的性能。代码可在 \url{https://github.com/YBZh/LAPT} 获取。|
|**2024-07-11**|[CXR-Agent: Vision-language models for chest X-ray interpretation with uncertainty aware radiology reporting](http://arxiv.org/abs/2407.08811)|null|近年来，大型视觉语言模型在解释复杂图像和使用高级推理生成自然语言描述方面展现出潜力。医学本质上是多模态的，它结合了扫描图像和基于文本的病史来撰写报告，这使得它有利于从人工智能能力的飞跃中受益。我们评估了几个数据集和基准上公开可用的、最先进的、基础视觉语言模型在胸部 X 光片解释方面的性能。我们使用线性探针来评估各种组件的性能，包括 CheXagent 的视觉转换器和 Q-former，它们在许多不同的数据集上的表现优于行业标准 Torch X-ray Vision 模型，显示出强大的泛化能力。重要的是，我们发现视觉语言模型经常会自信地产生幻觉语言，这会减慢临床解释的速度。基于这些发现，我们开发了一种基于代理的视觉语言方法，用于报告生成，使用 CheXagent 的线性探针和 BioViL-T 的短语 grounding 工具，根据病理的可能性生成具有不确定性感知的放射学报告，并对其进行定位和描述。我们通过开发一个评估平台，与呼吸系统专家进行用户研究，使用 NLP 指标、胸部 X 光片基准和临床评估，彻底评估了我们的视觉语言代理。我们的结果表明，人工智能生成的报告在准确性、可解释性和安全性方面都有相当大的改进。我们强调分别分析正常和异常扫描结果的重要性。最后，我们强调需要更大的配对（扫描和报告）数据集以及数据增强，以解决在这些大型视觉语言模型中出现的过拟合问题。|
|**2024-07-11**|[HiRes-LLaVA: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models](http://arxiv.org/abs/2407.08706)|null|高分辨率输入使大型视觉语言模型 (LVLM) 能够识别更精细的视觉细节，从而增强其理解能力。为了降低高分辨率输入带来的训练和计算成本，一种有前景的方向是使用滑动窗口将输入切片成均匀的块，每个块都与经过良好训练的视觉编码器的输入大小相匹配。虽然这种切片策略效率很高，但它会导致原始输入的碎片化，即跨块丢失上下文信息和空间几何的连续性，从而对跨块上下文感知和位置特定任务的性能产生负面影响。为了克服这些缺点，我们引入了 HiRes-LLaVA，这是一种新颖的框架，旨在有效处理任何大小的高分辨率输入，而不会改变原始的上下文和几何信息。HiRes-LLaVA 包含两个创新组件：(i) SliceRestore 适配器，它将切片的块重建为其原始形式，通过下上采样和卷积层有效地提取全局和局部特征，以及 (ii) 自挖掘采样器，用于根据自身压缩视觉标记，在保留原始上下文和位置信息的同时减少训练开销。为了评估处理上下文碎片的能力，我们构建了一个新的基准测试 EntityGrid-QA，其中包含与边缘相关和与位置相关的任务。我们的综合实验表明 HiRes-LLaVA 在现有公共基准测试和 EntityGrid-QA 上的优越性，尤其是在面向文档的任务上，为处理高分辨率输入树立了新标准。|
|**2024-07-11**|[Robotic Control via Embodied Chain-of-Thought Reasoning](http://arxiv.org/abs/2407.08693)|null|学习机器人控制策略的一个关键限制是它们无法在训练数据之外进行泛化。最近关于视觉语言动作模型 (VLA) 的研究表明，使用大型互联网预训练视觉语言模型作为学习机器人策略的支柱可以显著提高其鲁棒性和泛化能力。然而，大型视觉语言模型在其他领域最令人兴奋的能力之一是它们能够通过复杂问题进行迭代推理。这种能力能否应用于机器人领域，使策略能够在采取行动之前通过推理给定任务来提高性能？由于可用的训练示例相对简单，因此简单地使用“思维链”(CoT) 风格的提示对于标准 VLA 的效果要差得多。此外，纯粹对子任务进行语义推理（这在常规 CoT 中很常见）对于需要根据感官观察和机器人状态进行推理的机器人策略来说是不够的。为此，我们为 VLA 引入了具身思维链推理 (ECoT)，其中我们训练 VLA 在预测机器人动作之前对计划、子任务、动作以及视觉基础特征（如对象边界框和末端执行器位置）执行多个推理步骤。我们设计了一个可扩展的管道，用于在大型机器人数据集上为 ECoT 生成合成训练数据。我们证明，ECoT 将 OpenVLA（当前最强大的开源 VLA 策略）在具有挑战性的泛化任务中的绝对成功率提高了 28%，而无需任何额外的机器人训练数据。此外，ECoT 使人类更容易解释策略的失败并使用自然语言纠正其行为。|
|**2024-07-11**|[Bootstrapping Vision-language Models for Self-supervised Remote Physiological Measurement](http://arxiv.org/abs/2407.08507)|null|基于面部视频的远程生理测量是一个很有前景的研究领域，它以非接触方式检测人体生命体征（例如心率、呼吸频率）。传统方法大多是有监督学习，需要收集大量的面部视频和同步记录的光电容积脉搏波 (PPG) 信号。为了解决这个问题，自监督学习最近受到了关注；然而，由于缺乏真实PPG信号，其性能有限。在本文中，我们提出了一种新颖的自监督框架，成功地将流行的视觉语言模型（VLM）集成到远程生理测量任务中。给定一段面部视频，我们首先使用不同的rPPG信号频率增强其正负视频样本。接下来，我们引入了一种面向频率的视觉文本对生成方法，通过从正负样本中仔细创建对比时空图，并设计适当的文本提示来描述它们信号频率的相对比率。我们采用预训练的VLM来提取这些形成的视觉文本对的特征，然后估计rPPG信号。我们开发了一系列生成性和对比性学习机制来优化VLM，包括文本引导的视觉地图重建任务、视觉文本对比学习任务以及频率对比和排序任务。总的来说，我们的方法首次将VLM应用于视觉和文本模态中频率相关知识的提取和对齐。在四个基准数据集上的大量实验表明，它明显优于现有的自监督方法。|
|**2024-07-11**|[Specialist vision-language models for clinical ophthalmology](http://arxiv.org/abs/2407.08410)|null|临床医生需要花费大量时间查看医学影像，并以文本形式记录他们对患者诊断、转诊和治疗的发现。视觉语言模型 (VLM) 可以自动解读图像并将其发现总结为文本，在减轻临床工作量和增加患者获得高质量医疗服务的机会方面具有巨大潜力。虽然基础模型引起了医学界的极大兴趣，但尚不清楚它们的一般能力是否能转化为现实世界的临床效用。在这项工作中，我们发现，与执业眼科医生相比，基础 VLM 在对老年性黄斑变性 (AMD) 患者的护理至关重要的专家任务中表现明显不佳。为了解决这个问题，我们首先确定了基于图像的临床决策所需的基本能力，然后制定了一个课程，以有选择地训练 VLM 掌握这些技能。由此产生的模型 RetinaVLM 可以被指示编写报告，该报告在疾病分期（F1 分数为 0.63 对 0.11）和患者转诊（0.67 对 0.39）方面明显优于领先的基础医学 VLM 编写的报告，并且接近初级眼科医生的诊断性能（他们在各自任务上的得分分别为 0.77 和 0.78）。此外，在一项由两位具有长达 32 年经验的资深眼科医生参与的读者研究中，RetinaVLM 的报告被认为与具有长达 10 年经验的初级眼科医生编写的报告同样正确（78.6% 对 82.1%）和完整（均为 78.6%）。这些结果表明，我们基于课程的方法为将通用的基础医学 VLM 专业化以处理现实世界的临床任务提供了一个蓝图。|
|**2024-07-11**|[Enhancing Robustness of Vision-Language Models through Orthogonality Learning and Cross-Regularization](http://arxiv.org/abs/2407.08374)|null|像 CLIP 这类视觉语言模型 (VLM)  如何进行有效的微调以适应特定的下游任务正受到越来越多的关注。先前的工作主要集中在使用 prompt learning 来使 CLIP 适应各种下游任务，然而，当在小数据集上进行微调时，这种方法容易出现过拟合问题。在本文中，我们介绍了一种正交微调方法，可以有效地更新预训练权重，增强鲁棒性和泛化能力，同时进一步利用交叉正则化策略来保持 VLM 零样本泛化能力的稳定性，我们称之为 \textbf{\textit{OrthCR}}。具体来说，我们在 Transformer 架构中无缝地注入了可训练的正交矩阵，并使用 Cayley 参数化来强制执行正交约束，受益于范数保持特性，从而实现稳定和更快的收敛。为了减轻训练过程中与正交约束的偏差，我们进一步采用了一种交叉正则化策略，以旁路的方式利用初始预训练权重。此外，为了丰富下游任务的样本多样性，我们首先探索了 Cutout 数据增强技术，以促进高效的微调，并从正交学习的角度理解我们的方法如何提高特定下游任务的性能并保持泛化能力。除了现有的 prompt learning 技术之外，我们还进行了广泛的实验，以证明我们的方法能够明确地引导预训练权重空间来表示特定于任务的知识，并在\textit{base-to-base/base-to-new}、\textit{跨数据集迁移}和\textit{领域泛化}评估中表现出具有竞争力的泛化能力。|
|**2024-07-11**|[Explore the Potential of CLIP for Training-Free Open Vocabulary Semantic Segmentation](http://arxiv.org/abs/2407.08268)|null|作为一种视觉语言模型，CLIP凭借其零样本能力极大地推进了开放词汇语义分割（OVSS）的发展。尽管取得了成功，但由于其最初的图像级对齐训练，其在OVSS中的应用面临着挑战，这影响了其在需要详细局部上下文的任务中的性能。我们的研究深入探讨了CLIP的[CLS]标记对图像块特征相关性的影响，揭示了“全局”图像块的主导地位阻碍了局部特征的区分。为了克服这个问题，我们提出了CLIPtrase，这是一种新颖的无需训练的语义分割策略，通过重新校准图像块之间的自相关性来增强局部特征感知。这种方法在分割精度和保持对象之间语义连贯性方面表现出显著的改进。实验表明，我们在9个分割基准测试中平均领先CLIP 22.3%，优于现有的最先进的无需训练方法。代码已在以下地址公开：https://github.com/leaves162/CLIPtrase。|
|**2024-07-11**|[AddressCLIP: Empowering Vision-Language Models for City-wide Image Address Localization](http://arxiv.org/abs/2407.08156)|**[link](https://github.com/xsx1001/addressclip)**|本研究介绍了社交媒体和摄影新闻带来的一个新问题，称为图像地址定位（IAL），旨在预测拍摄图像的可读文本地址。现有的两阶段方法涉及预测地理坐标并将其转换为人类可读的地址，这可能会导致歧义并且资源密集。相比之下，我们提出了一个名为 AddressCLIP 的端到端框架来解决语义更丰富的问题，它包含两个关键要素：i）图像-文本对齐，通过对比学习将图像与地址和场景描述对齐，以及 ii）图像-地理匹配，根据流形学习在空间距离方面约束图像特征。此外，我们还构建了三个来自匹兹堡和旧金山的不同规模的数据集，专门用于 IAL 问题。实验表明，我们的方法在所提出的数据集上取得了令人信服的性能，并且优于视觉语言模型的代表性迁移学习方法。此外，广泛的消融和可视化展示了所提出方法的有效性。数据集和源代码可在 https://github.com/xsx1001/AddressCLIP 获取。|
|**2024-07-10**|[RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization](http://arxiv.org/abs/2407.08044)|**[link](https://github.com/huangowen/rolora)**|低秩适应 (LoRA) 作为一种典型的参数高效微调 (PEFT) 方法，通过仅更新大型语言模型 (LLM) 中的一小部分权重，显著提高了训练效率。近年来，仅权重量化技术也被应用于 LoRA 方法，以减少微调的内存占用。然而，将权重-激活量化应用于 LoRA 流程的研究还不够充分，我们观察到性能大幅下降，这主要是由于激活异常值的存在。在这项工作中，我们提出了 RoLoRA，这是第一个基于 LoRA 的有效权重-激活量化方案。RoLoRA 利用旋转来消除异常值，并提出了旋转感知微调，以在旋转后的 LLM 中保留无异常值的特性。实验结果表明，在权重-激活设置中，RoLoRA 持续提高了低比特 LoRA 的收敛性和训练后量化的鲁棒性。我们在 LLaMA2-7B/13B、LLaMA3-8B 模型上评估了 RoLoRA，与 LoRA 基线相比，在常识推理任务上，4 位权重-激活量化的 LLaMA2-13B 的绝对精度提高了 29.5%。我们进一步证明了其在大型多模态模型 (LLaVA-1.5-7B) 上的有效性。代码可在 https://github.com/HuangOwen/RoLoRA 获取。||
|**2024-07-10**|[LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models](http://arxiv.org/abs/2407.07895)|**[link](https://github.com/LLaVA-VL/LLaVA-NeXT)**|视觉指令微调在增强大型多模态模型 (LMM) 的能力方面取得了重大进展。然而，现有的开放 LMM 主要集中在单图像任务上，它们在多图像场景中的应用仍有待探索。此外，先前的 LMM 研究分别解决了不同的场景，使得无法通过新兴功能泛化跨场景。为此，我们引入了 LLaVA-NeXT-Interleave，它在 LMM 中同时处理多图像、多帧（视频）、多视图（3D）和多patch（单图像）场景。为了实现这些功能，我们将交错数据格式视为通用模板，并使用 1,177.6k 个样本编译了 M4-Instruct 数据集，涵盖 4 个主要领域，包括 14 个任务和 41 个数据集。我们还策划了 LLaVA-Interleave Bench 以全面评估 LMM 的多图像性能。通过大量实验，LLaVA-NeXT-Interleave 在多图像、视频和 3D 基准测试中取得了领先的结果，同时保持了单图像任务的性能。此外，我们的模型还展现出一些新兴功能，例如跨不同设置和模态迁移任务。代码可在 https://github.com/LLaVA-VL/LLaVA-NeXT 获取。||
|**2024-07-10**|[Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs](http://arxiv.org/abs/2407.07775)|null|导航研究的一个难以实现的目标是构建一个能够理解包括自然语言和图像在内的多模态指令并执行有用导航的智能体。为了实现这一目标，我们研究了一类广泛适用的导航任务，我们称之为带有演示路径的多模态指令导航（MINT），其中环境先验是通过先前记录的演示视频提供的。视觉语言模型（VLM）的最新进展为实现这一目标指明了一条充满希望的道路，因为它展示了感知和推理多模态输入的能力。然而，VLM 通常被训练用于预测文本输出，如何最好地将其用于导航是一个开放的研究问题。为了解决 MINT 问题，我们提出了 Mobility VLA，这是一种分层的视觉-语言-动作（VLA）导航策略，它结合了长上下文 VLM 的环境理解和常识推理能力，以及基于拓扑图的鲁棒的低级导航策略。高级策略由一个长上下文 VLM 组成，它将演示路径视频和多模态用户指令作为输入，以在路径视频中找到目标帧。接下来，低级策略使用目标帧和离线构建的拓扑图在每个时间步长生成机器人动作。我们在一个 836 平方米的真实世界环境中评估了 Mobility VLA，结果表明，Mobility VLA 在以前未解决的多模态指令（例如“我应该把这个还到哪里？”）上具有很高的端到端成功率，同时手持一个塑料箱。||
|**2024-07-09**|[Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model](http://arxiv.org/abs/2407.07053)|**[link](https://github.com/zwq2018/multi-modal-self-instruct)**|虽然目前大多数大型多模态模型 (LMM) 已经可以理解自然场景和肖像照片，但它们对抽象图像（例如图表、地图或布局）的理解和视觉推理能力仍然相当初级。它们经常难以完成简单的日常任务，例如从时钟读取时间、理解流程图或使用路线图规划路线。鉴于此，我们设计了一种多模态自指示方法，利用大型语言模型及其代码能力，在日常场景中合成大量抽象图像和视觉推理指令。我们的策略轻松创建了一个包含 11,193 条指令的多模态基准测试，涵盖八个视觉场景：图表、表格、模拟地图、仪表盘、流程图、关系图、平面图和视觉谜题。这个由简单的线条和几何元素构建的基准测试暴露了大多数先进的 LMM（如 Claude-3.5-Sonnet 和 GPT-4o）在抽象图像理解、空间关系推理和视觉元素归纳方面的不足。此外，为了验证我们合成数据的质量，我们使用 62,476 条合成的图表、表格和路线图指令微调了一个 LMM。结果表明，图表理解和地图导航性能有所提高，也显示出对其他视觉推理任务的潜在好处。我们的代码可在以下网址获得：https://github.com/zwq2018/Multi-modal-Self-instruct。||
|**2024-07-09**|[Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action Localization](http://arxiv.org/abs/2407.07024)|**[link](https://github.com/hyunjs/stov-tal)**|时序动作定位 (TAL) 中的词汇量受到大规模标注数据集稀缺性的限制。为了解决这个问题，最近的研究结合了强大的预训练视觉语言模型 (VLM)，例如 CLIP，来执行开放词汇表 TAL (OV-TAL)。然而，与在大量图像/视频-文本对上训练的 VLM 不同，现有的 OV-TAL 方法仍然依赖于小型、完全标记的 TAL 数据集来训练动作定位器。在本文中，我们探索了使用未标记的 YouTube 视频进行 OV-TAL 自训练的可扩展性。我们的自训练方法包括两个阶段。首先，在人工标记的 TAL 数据集上训练一个类别无关的动作定位器，并使用它为未标记的视频生成伪标签。其次，将大规模伪标签数据集与人工标记的数据集相结合，以训练定位器。大量实验表明，在自训练中利用网络规模的视频可以显着增强动作定位器的泛化能力。此外，我们重点介绍了现有 OV-TAL 评估方案中存在的问题，并提出了一种新的评估方案。代码发布在 https://github.com/HYUNJS/STOV-TAL||
|**2024-07-09**|[CoLA: Conditional Dropout and Language-driven Robust Dual-modal Salient Object Detection](http://arxiv.org/abs/2407.06780)|**[link](https://github.com/ssecv/CoLA)**|深度/热信息有利于使用传统RGB图像检测显著性目标。然而，在双模态显著性目标检测（SOD）模型中，针对噪声输入和模态缺失的鲁棒性至关重要，但很少被研究。为了解决这个问题，我们引入了条件Dropout和语言驱动（CoLA）框架，该框架包含两个核心组件。1）语言驱动质量评估（LQA）：利用带有提示学习器的预训练视觉语言模型，LQA在不需要额外质量标注的情况下重新校准图像贡献。这种方法有效地减轻了噪声输入的影响。2）条件Dropout（CD）：一种学习方法，用于增强模型在模态缺失场景下的适应性，同时保持其在完整模态下的性能。CD作为一种插件式训练方案，将模态缺失视为条件，增强了各种双模态SOD模型的整体鲁棒性。大量实验表明，所提出的方法在模态完整和模态缺失条件下均优于最先进的双模态SOD模型。代码将在论文被接收后开源。||
|**2024-07-09**|[LVLM-empowered Multi-modal Representation Learning for Visual Place Recognition](http://arxiv.org/abs/2407.06730)|null|视觉位置识别 (VPR) 由于视角变化和外观变化很大，因此仍然具有挑战性。主流工作通过开发各种特征聚合方法将深度特征转换为稳健而紧凑的全局表示来应对这些挑战。不幸的是，在具有挑战性的条件下无法获得令人满意的结果。我们从一个新的角度出发，尝试通过融合图像数据和视觉场景的文本描述来构建具有判别性的全局表示。动机有两个：（1）当前的大型视觉语言模型 (LVLM) 在视觉指令跟随方面表现出非凡的涌现能力，因此提供了一种高效灵活的图像文本描述生成方式；（2）文本描述提供了对场景的高级理解，对环境变化表现出很强的鲁棒性。尽管很有前景，但利用 LVLM 构建多模态 VPR 解决方案在高效的多模态融合方面仍然具有挑战性。此外，LVLM 不可避免地会产生一些不准确的描述，这使得情况变得更加困难。为了应对这些挑战，我们提出了一种新颖的多模态 VPR 解决方案。它首先使预训练的视觉和语言基础模型适应 VPR，以提取图像和文本特征，然后将这些特征输入特征组合器以相互增强。作为主要组件，特征组合器首先提出了一个逐符号注意力块，以根据文本符号与图像数据的相关性自适应地重新校准文本符号，然后开发了一个高效的交叉注意力融合模块，以在不同模态之间传播信息。增强的多模态特征被压缩到特征描述符中以执行检索。实验结果表明，我们的方法在图像描述符维度明显较小的情况下，大大优于最先进的方法。||
|**2024-07-08**|[A Single Transformer for Scalable Vision-Language Modeling](http://arxiv.org/abs/2407.06438)|**[link](https://github.com/yangyi-chen/solo)**|我们提出了 SOLO，一个用于可扩展视觉语言建模的单一 Transformer 模型。目前的大型视觉语言模型 (LVLM)，例如 LLaVA，大多采用异构架构，将预训练的视觉编码器与大型语言模型 (LLM) 连接起来，以促进视觉识别和复杂推理。虽然通过相对轻量级的训练获得了显著的性能，但我们发现了四个主要的扩展性限制：(1) 视觉能力受到预训练视觉编码器的限制，这些编码器通常比 LLM 小一个数量级。(2) 异构架构使已建立的硬件和软件基础设施的使用变得复杂。(3) 对这种架构进行规模法则研究必须考虑三个独立的组件——视觉编码器、连接器和 LLM，这使得分析变得复杂。(4) 使用现有的视觉编码器通常需要遵循预定义的图像输入预处理规范，例如，通过将输入整形为固定分辨率的方形图像，这在处理和训练高分辨率图像或具有不寻常纵横比的图像时会遇到困难。像 SOLO 这样的统一单一 Transformer 架构有效地解决了 LVLMs 中的这些可扩展性问题；然而，它在现代环境中的有限采用可能是由于缺乏可靠的训练方法来平衡两种模态并确保数十亿级模型的稳定训练。在本文中，我们介绍了第一个用于开发 SOLO 的开源训练方法，SOLO 是一个使用中等学术资源的开源 7B LVLM。训练方法包括从 LLM 初始化、在 ImageNet 和网络规模数据上进行顺序预训练，以及在我们策划的高质量数据集上进行指令微调。在广泛的评估中，SOLO 表现出与 LLaVA-v1.5-7B 相当的性能，尤其是在视觉数学推理方面表现出色。||
|**2024-07-08**|[Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision](http://arxiv.org/abs/2407.06189)|**[link](https://github.com/orrzohar/Video-STaR)**|大型视觉语言模型 (LVLM) 的性能取决于其训练数据集的规模和质量。现有的视频指令调整数据集缺乏多样性，因为它们是通过提示大型语言模型使用视频字幕生成问答对而得出的，因此大多是描述性的。同时，存在许多具有不同标签和监督的标记视频数据集——然而，我们发现将它们集成到 LVLM 中并非易事。在此，我们提出了使用增强推理的视频自训练 (Video-STaR)，这是第一个视频自训练方法。Video-STaR 允许利用任何标记的视频数据集进行视频指令调整。在 Video-STaR 中，LVLM 在指令生成和微调之间循环，我们证明这 (I) 提高了一般视频理解能力，并且 (II) 使 LVLM 能够适应现有监督下的新型下游任务。在生成过程中，LVLM 被提示提出答案。然后过滤答案，只保留包含原始视频标签的答案，然后在生成的数据集上重新训练 LVLM。通过仅对包含正确视频标签的生成答案进行训练，Video-STaR 利用这些现有的视频标签作为视频指令调整的弱监督。我们的结果表明，经过 Video-STaR 增强后的 LVLM 在 (I) 常规视频问答（TempCompass 性能提高了 10%）和 (II) 下游任务（Video-STaR 将 Kinetics700-QA 的准确率提高了 20%，并将 FineDiving 上的动作质量评估提高了 15%）中均表现出更好的性能。||
|**2024-07-09**|[HyCIR: Boosting Zero-Shot Composed Image Retrieval with Synthetic Labels](http://arxiv.org/abs/2407.05795)|null|组合图像检索 (CIR) 旨在根据带有文本的查询图像检索图像。当前的零样本 CIR (ZS-CIR) 方法试图在不使用昂贵的三元组标记训练数据集的情况下解决 CIR 任务。然而，ZS-CIR 和三元组监督 CIR 之间的差距仍然很大。在这项工作中，我们提出了混合 CIR (HyCIR)，它使用合成标签来提高 ZS-CIR 的性能。提出了一种新的 CIR 标签合成流程 (SynCir)，其中只需要未标记的图像。首先，根据视觉相似度提取图像对。其次，基于视觉语言模型和 LLM 为每个图像对生成查询文本。第三，基于语义相似度在语言空间中进一步过滤数据。为了提高 ZS-CIR 的性能，我们提出了一种混合训练策略，可以同时使用 ZS-CIR 监督和合成 CIR 三元组。采用了两种对比学习方法。一种是使用大规模未标记图像数据集来学习具有良好泛化能力的图像到文本映射。另一种是使用合成的 CIR 三元组来学习 CIR 任务的更好映射。我们的方法在常见的 CIR 基准测试：CIRR 和 CIRCO 上实现了最先进的零样本性能。||
|**2024-07-07**|[Multimodal Language Models for Domain-Specific Procedural Video Summarization](http://arxiv.org/abs/2407.05419)|null|视频是一种强大的媒介，可以通过长格式教程传达思想、讲述故事和提供详细的说明。此类教程对于按照自己的节奏学习新技能非常有价值，但由于其长度和密集的内容，可能会让人不知所措。观众经常会寻找特定信息，例如精确的测量值或分步执行细节，因此必须有效地提取和总结关键片段。人们非常需要一个能够总结和检测长视频中的亮点的智能、时间敏感的视频助手。多模态大型语言模型的最新进展为开发此类助手提供了有希望的解决方案。我们的研究探索了使用多模态模型来增强特定领域内的视频摘要和分步指令生成。这些模型需要理解跨视频帧的动作之间的时间事件和关系。我们的方法侧重于微调 TimeChat，以提高其在特定领域（烹饪和医疗程序）中的性能。通过在特定领域的数据集（如烹饪领域的 Tasty 和医疗程序领域的 MedVidQA）上训练模型，我们旨在增强其生成简洁、准确的教学视频摘要的能力。我们整理并重构了这些数据集，以创建高质量的以视频为中心的指令数据。我们的研究结果表明，当在特定领域的程序数据上进行微调时，TimeChat 可以显着改善长格式视频中关键指令步骤的提取和总结。这项研究证明了专门的多模式模型通过提供针对每个领域的独特方面量身定制的个性化分步指导来协助完成实际任务的潜力。||
|**2024-07-07**|[Multimodal Prompt Learning with Missing Modalities for Sentiment Analysis and Emotion Recognition](http://arxiv.org/abs/2407.05374)|**[link](https://github.com/zrguo/MPLMM)**|多模态模型的发展显著推进了多模态情感分析和情绪识别。然而，在现实应用中，各种缺失模态情况的存在常常导致模型性能下降。本文提出了一种新颖的使用提示学习的多模态Transformer框架来解决模态缺失问题。我们的方法引入了三种类型的提示：生成提示、缺失信号提示和缺失类型提示。这些提示能够生成缺失的模态特征，并促进模态内和模态间信息的学习。通过提示学习，我们实现了可训练参数数量的大幅减少。我们提出的方法在所有评估指标上都明显优于其他方法。大量的实验和消融研究证明了我们方法的有效性和鲁棒性，展示了其有效处理缺失模态的能力。||
|**2024-07-07**|[WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks](http://arxiv.org/abs/2407.05291)|**[link](https://github.com/servicenow/workarena)**|大型语言模型 (LLM) 模仿人类智能的能力导致了基于 LLM 的自主代理的激增。尽管最近的 LLM 似乎能够根据用户指令进行计划和推理，但它们将这些能力应用于自主任务解决的有效性仍未得到充分探索。在企业环境中尤其如此，因为自动化代理有望产生重大影响。为了填补这一空白，我们提出了 WorkArena++，这是一个包含 682 个任务的新基准，这些任务对应于知识工作者日常执行的现实工作流程。WorkArena++ 旨在评估 Web 代理的计划、解决问题、逻辑/算术推理、检索和上下文理解能力。我们对最先进的 LLM 和视觉语言模型 (VLM) 以及人类工作者的实证研究表明，此类模型要成为工作场所中有用的助手面临着若干挑战。除了基准之外，我们还提供了一种机制，可以毫不费力地生成数千个真实观察/行动轨迹，这些轨迹可用于微调现有模型。总的来说，我们希望这项工作能够成为帮助社区朝着有能力的自主代理方向发展的一种有用资源。该基准可以在 https://github.com/ServiceNow/WorkArena/tree/workarena-plus-plus 找到。||
|**2024-07-05**|[Multimodal Classification via Modal-Aware Interactive Enhancement](http://arxiv.org/abs/2407.04587)|null|由于存在臭名昭著的模态不平衡问题，多模态学习（MML）会导致优化不平衡现象，从而难以达到令人满意的性能。最近，一些具有代表性的方法被提出用于提高性能，主要集中在自适应调整每个模态的优化，以重新平衡主导模态和非主导模态的学习速度。为了更好地促进多模态学习中模型信息的交互，在本文中，我们提出了一种新的多模态学习方法，称为模态感知交互增强（MIE）。具体来说，我们首先利用基于锐度感知最小化（SAM）的优化策略在前向阶段平滑学习目标。然后，借助SAM的几何特性，我们提出了一种梯度修正策略，在反向阶段施加不同模态之间的影响。因此，我们可以提高泛化能力，同时缓解多模态学习中的模态遗忘现象。在广泛使用的数据集上进行的大量实验表明，我们提出的方法可以优于各种最先进的基线，以实现最佳性能。||
|**2024-07-04**|[MiniGPT-Med: Large Language Model as a General Interface for Radiology Diagnosis](http://arxiv.org/abs/2407.04106)|**[link](https://github.com/vision-cair/minigpt-med)**|近年来，人工智能 (AI) 的快速发展为医疗保健领域带来了重大的突破，特别是在诊断程序的改进方面。然而，以往的研究往往局限于有限的功能。本研究介绍了 MiniGPT-Med，这是一个源于大规模语言模型并专为医疗应用而设计的视觉语言模型。MiniGPT-Med 在各种成像模式（包括 X 光、CT 扫描和 MRI）中均表现出非凡的多功能性，从而增强了其实用性。该模型能够执行医学报告生成、视觉问答 (VQA) 以及医学图像疾病识别等任务。它对图像和文本临床数据的集成处理显著提高了诊断准确性。我们的实证评估证实，MiniGPT-Med 在疾病定位、医学报告生成和 VQA 基准测试中均表现出色，这标志着在缩小放射学实践辅助差距方面迈出了重要一步。此外，它在医学报告生成方面达到了最先进的性能，比之前的最佳模型提高了19%的准确率。MiniGPT-Med 有望成为放射学诊断的通用接口，从而提高各种医学影像应用的诊断效率。||
|**2024-07-04**|[Fully Fine-tuned CLIP Models are Efficient Few-Shot Learners](http://arxiv.org/abs/2407.04003)|null|提示调优通过训练一小部分参数，可以有效地增强预训练视觉语言模型 (VLM) 在下游任务上的性能。然而，当将调优后的模型应用于不同的数据集或领域时，它们往往会牺牲灵活性和适应性。在本文中，我们探索了通过精细微调整个 VLM 来捕获特定任务信息的可能性，同时最大限度地减少参数调整。在有限的监督下对特定任务进行整个 VLM 微调时，过拟合和灾难性遗忘成为事实上的因素。为了缓解这些问题，我们提出了一个名为 CLIP-CITE 的框架，通过设计一个判别性的视觉-文本任务，进一步以监督的方式对齐视觉-文本语义，并集成知识蒸馏技术来保留获得的知识。在少样本学习、基础到新泛化、域泛化和跨域泛化设置下的广泛实验结果表明，我们的方法在有限监督下有效地提高了特定任务的性能，同时保留了 VLM 在其他数据集上的通用性。||
|**2024-07-04**|[Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks](http://arxiv.org/abs/2407.03967)|**[link](https://github.com/amitkparekh/cogelot)**|仅根据多模态模型在分布外数据上的性能来评估其泛化能力，无法捕捉其真正的鲁棒性。本研究引入了一个全面的评估框架，系统地检验了指令和输入在这些模型泛化能力中的作用，并考虑了架构设计、跨语言和视觉模态的输入扰动以及任务复杂性的增加。所提出的框架揭示了多模态模型对极端指令扰动的弹性和它们对观察变化的脆弱性，引发了对过度拟合虚假相关性的担忧。通过在当前基于 Transformer 的机器人操作任务多模态模型上应用此评估框架，我们发现了局限性，并建议未来的改进应侧重于架构和训练创新，以更好地整合多模态输入，通过优先考虑对输入内容的敏感性而不是偶然的相关性来增强模型的泛化能力。||
|**2024-07-04**|[Concept Bottleneck Models Without Predefined Concepts](http://arxiv.org/abs/2407.03921)|null|近年来，可解释的概念型模型，如概念瓶颈模型 (CBM)，引起了人们的广泛兴趣。这类模型首先预测人类可解释的概念，然后将这些概念映射到输出类别。为了减少对人工标注概念的依赖，最近的研究工作已将预训练的黑盒模型后验地转换为可解释的 CBM。然而，这些方法预先定义了一组概念，假设黑盒模型在其表示中编码了哪些概念。在这项工作中，我们通过利用无监督概念发现来自动提取概念，从而消除了这一假设，无需人工标注或预定义的概念集。我们进一步引入了一种依赖于输入的概念选择机制，以确保在所有类别中仅使用一小部分概念。我们证明，我们的方法提高了下游性能，并缩小了与黑盒模型的性能差距，同时在分类中使用的概念要少得多。最后，我们演示了大型视觉语言模型如何干预最终的模型权重以纠正模型错误。||
|**2024-07-04**|[M $\mathbf5$ -- A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks](http://arxiv.org/abs/2407.03791)|null|自ChatGPT发布以来，自然语言处理领域经历了快速发展，特别是在大型语言模型（LLM）及其多模态对应物大型多模态模型（LMM）方面。尽管LLM具有令人印象深刻的能力，但正如各种纯文本基准测试所证明的那样，LLM在不同语言和文化背景下 often 表现出显著的性能差异。然而，目前的研究缺乏针对多模态视觉语言环境的此类基准。为了弥补这一差距，本研究引入了M5，这是第一个旨在评估多语言和多文化背景下不同视觉语言任务的LMM的综合基准。M5包括涵盖五个任务和41种语言的八个数据集，重点关注代表性不足的语言和文化多样化的图像。此外，我们还介绍了两个新的数据集，M5-VGR和M5-VLOD，其中包括一项新的视觉语言异常检测任务，在该任务中，所有评估的开源模型都未能显著超过随机基线。通过广泛的评估和分析，我们重点强调了资源丰富语言和资源匮乏语言之间存在巨大的、与任务无关的性能差异。此外，我们还发现，在多语言环境中，更大的模型不一定优于较小的模型。||
|**2024-07-04**|[Meta-optimized Angular Margin Contrastive Framework for Video-Language Representation Learning](http://arxiv.org/abs/2407.03788)|null|数据质量是决定视频-语言表示学习效果的首要因素。然而，以往数据中的视频-文本对通常不能完美对齐，这可能导致视频-语言表示不能准确反映跨模态语义。此外，以往数据还存在概念分布不均匀的问题，从而影响了在不受欢迎主题上的下游性能。为了解决这些问题，我们提出了一个带有减法角度边际的对比目标函数，以规范跨模态表示，使其达到完美的相似性。此外，为了适应非均匀的概念分布，我们提出了一个多层感知器（MLP）参数化的加权函数，将损失值映射到样本权重，从而能够在整个训练过程中动态调整模型的关注点。在少量无偏元数据的指导下，并通过大型视觉-语言模型生成的视频-文本数据进行增强，我们改进了视频-语言表示，并在常用的视频问答和文本-视频检索数据集上取得了优异的性能。||
|**2024-07-04**|[Visualizing Dialogues: Enhancing Image Selection through Dialogue Understanding with Large Language Models](http://arxiv.org/abs/2407.03615)|**[link](https://github.com/MiuLab/VisualDialog)**|近年来，对话系统的进步凸显了整合多模态响应的重要性，这种响应能够通过多种模态来传达信息，而不仅仅依赖于基于文本的交互。这种丰富性不仅提高了整体的交流效率，还增强了对话体验的质量。然而，现有的对话到图像检索方法由于预训练视觉语言模型 (VLM) 在准确理解复杂对话方面的局限性而面临挑战。为了解决这个问题，我们提出了一种新方法，利用大型语言模型 (LLM) 强大的推理能力来生成精确的对话相关视觉描述符，从而促进与图像的无缝连接。在基准数据上进行的大量实验验证了我们提出的方法在提取简洁准确的视觉描述符方面的有效性，从而显著提高了对话到图像检索的性能。此外，我们的研究结果证明了该方法在不同视觉线索、各种 LLM 和不同数据集上的泛化能力，突出了其在实际应用中的实用性和潜在影响。||
|**2024-07-04**|[Lateralization LoRA: Interleaved Instruction Tuning with Modality-Specialized Adaptations](http://arxiv.org/abs/2407.03604)|null|视觉语言模型 (VLM) 的最新进展导致了能够理解和生成交错图像和文本的视觉语言通用模型 (VLG) 的发展。尽管取得了这些进步，但 VLG 在遵循用户指令进行交错文本和图像生成方面仍然存在困难。为了解决这个问题，我们引入了 LeafInstruct，这是第一个开源的交错指令调整数据，包含跨 10 多个领域的 30,000 多个高质量实例。由于现有 VLG 的规模庞大，我们选择进行参数高效的调整。然而，我们观察到使用标准 LoRA 调整的 VLG 通常在交错文本图像生成中表现出较差的性能。我们将此问题归因于模态干扰和缺乏模态专用适应性设计。因此，我们提出了一种受大脑偏侧化概念启发的新型模态专用适应方法——Lateralization LoRA。Lateralization LoRA 采用混合方法，结合了传统的线性 LoRA 和用于生成文本和图像的卷积 LoRA，通过利用模态特定的结构和参数集来生成高质量的文本和图像。我们使用 LeafInstruct 数据集对 VLG（即 EMU2）进行 Lateralization LoRA 指令调整。大量实验表明，使用 Lateralization LoRA 调整的 EMU2 实现了最先进的性能，在复杂的交错任务中明显优于基线模型。||
|**2024-07-03**|[HEMM: Holistic Evaluation of Multimodal Foundation Models](http://arxiv.org/abs/2407.03418)|**[link](https://github.com/pliang279/hemm)**|能够全面处理文本、图像、视频、音频和其他感官模态的多模态基础模型正越来越多地应用于各种现实应用中。然而，考虑到可能存在的各种建模决策、任务和领域，描述和研究多模态基础模型的进展具有挑战性。在本文中，我们介绍了多模态模型的整体评估 (HEMM)，以系统地评估多模态基础模型在一组 3 个维度上的能力：基本技能、信息流和现实用例。基本的多模态技能是解决问题所需的内部能力，例如学习跨模态的交互、细粒度对齐、多步骤推理以及处理外部知识的能力。信息流研究多模态内容在任务期间如何通过查询、翻译、编辑和融合发生变化。用例涵盖了现实世界多媒体、情感计算、自然科学、医疗保健和人机交互应用中引入的特定领域挑战。通过对 HEMM 中 30 个任务的全面实验，我们 (1) 确定了对当今模型构成挑战的关键数据集维度（例如，基本技能、信息流和用例），以及 (2) 提炼了关于不同建模维度（例如，规模、预训练数据、多模态对齐、预训练和指令微调目标）如何影响性能的性能趋势。我们关于具有挑战性的多模态交互、用例以及需要推理和外部知识的任务、数据和模型规模的好处以及指令微调的影响的结论，为多模态基础模型的未来工作提供了可操作的见解。||
|**2024-07-03**|[Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation](http://arxiv.org/abs/2407.03056)|**[link](https://github.com/miccunifi/kdpl)**|视觉语言模型 (VLM) 在未见过的任务上表现出非凡的零样本泛化能力，但在有限数据下泛化到下游任务的性能不如监督方法。提示学习正在成为一种参数高效的 VLM 自适应方法，但最先进的方法需要带注释的样本。在本文中，我们提出了一种基于无监督知识蒸馏的新型提示学习方法，该方法从更强大的模型中提取知识。我们的方法称为知识蒸馏提示学习 (KDPL)，可以集成到现有的提示学习技术中，并消除了适应过程中对标记示例的需求。我们对十多个标准基准数据集进行的实验表明，KDPL 在提高学习提示的泛化能力方面非常有效，可以解决零样本域泛化、零样本跨数据集泛化和零样本基础到新类泛化问题。KDPL 不需要用于适应的基本事实标签，此外，我们还表明，即使在没有任何训练类名知识的情况下，它也可以用于有效地迁移知识。代码可在 https://github.com/miccunifi/KDPL 公开获取。||
|**2024-07-03**|[SAFT: Towards Out-of-Distribution Generalization in Fine-Tuning](http://arxiv.org/abs/2407.03036)|null|在机器学习领域，处理训练数据中的分布变化，即所谓的分布外 (OOD) 泛化，是一项重大挑战。虽然像 CLIP 这样的预训练视觉语言模型已经展现出卓越的零样本性能，但模型对下游任务的进一步适应会导致 OOD 数据出现不良的性能下降。在这项工作中，我们引入了用于微调的稀疏适应 (SAFT) 方法，该方法可以防止微调过程中遗忘预训练模型中的通用知识。SAFT 仅更新梯度幅度较大的一小部分重要参数，同时保持其他参数冻结。SAFT 易于实现且概念简单。大量实验表明，仅使用 0.1% 的模型参数，SAFT 就可以显著提高 CLIP 的性能。在多个基准测试中，它始终优于基线方法。在 ImageNet 及其变体的少样本学习基准测试中，在 OOD 设置下，SAFT 比传统的微调方法平均提高了 5.15% 的性能。||
|**2024-07-03**|[Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective](http://arxiv.org/abs/2407.02814)|null|在大型数据集上预训练的视觉语言模型 (VLM) 可能会通过将性别信息与特定对象或场景相关联而无意中学习到偏见。当前的方法侧重于修改输入并监控模型输出概率分数的变化，但往往难以从模型组件的角度全面理解偏见。我们提出了一个结合因果中介分析的框架，用于测量和映射 VLM 内偏见产生和传播的路径。这种方法使我们能够确定干预措施对模型偏差的直接影响，以及干预措施通过不同模型组件介导的对偏差的间接影响。我们的结果表明，图像特征是偏见的主要来源，其影响远高于文本特征，具体而言，在 MSCOCO 和 PASCAL-SENTENCE 数据集中分别占偏见的 32.57% 和 12.63%。值得注意的是，图像编码器的贡献超过了文本编码器和深度融合编码器。进一步的实验表明，语言和视觉模态的贡献是一致且不冲突的。因此，专注于模糊图像编码器中对模型偏见贡献最大的性别表征，可以有效地将 MSCOCO 和 PASCAL-SENTENCE 数据集中的偏见分别减少 22.03% 和 9.04%，而性能损失最小，计算量也没有增加。||
|**2024-07-03**|[MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context](http://arxiv.org/abs/2407.02730)|**[link](https://github.com/dongzizhu/medvh)**|大型视觉语言模型 (LVLM) 最近在自然图像和文本数据的各种任务中取得了优异的性能，这激发了大量关于 LVLM 微调和训练的研究。尽管取得了这些进步，但很少有研究关注这些模型在更小的数据集上微调时对幻觉的鲁棒性。在这项研究中，我们引入了一个新的基准数据集，即医学视觉幻觉测试 (MedVH)，用于评估特定领域 LVLM 的幻觉。 MedVH 包含五项任务，用于评估医学环境中 LVLM 的幻觉，其中包括全面理解文本和视觉输入以及生成长文本响应的任务。我们对通用 LVLM 和医学 LVLM 进行的广泛实验表明，尽管医学 LVLM 在标准医学任务中表现出良好的性能，但它们特别容易受到幻觉的影响，通常比通用模型更容易受到影响，这引发了人们对这些特定领域模型可靠性的严重担忧。为了使医学 LVLM 在实际应用中真正发挥价值，它们不仅必须准确地整合医学知识，还必须保持强大的推理能力以防止幻觉。我们的工作为未来对这些研究的评估铺平了道路。||
|**2024-07-02**|[Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models](http://arxiv.org/abs/2407.02716)|null|对预训练的视觉语言模型 (VLM) 进行微调已在医学图像和文本描述协同作用方面展现出卓越的能力。然而，许多预训练数据集受到患者隐私问题的限制，可能包含会对下游性能产生负面影响的噪声。此外，对多模态生成的日益依赖加剧了这个问题，因为它容易受到对抗性攻击。为了研究在对抗性噪声数据上训练的 VLM 如何在下游医学任务中执行，我们首先使用多模态对抗性攻击来制作噪声上游数据集。通过我们的综合分析，我们揭示了适度的噪声增强了模型的鲁棒性和可迁移性，但增加噪声水平会对下游任务性能产生负面影响。为了缓解这个问题，我们提出了校正对抗性噪声 (RAN) 框架，这是一种旨在有效防御对抗性攻击并在微调期间纠正上游噪声影响的方法。||
|**2024-07-02**|[D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data and eXpert model predictions](http://arxiv.org/abs/2407.02604)|null|大型视觉语言模型（VLM）已经从研究阶段发展到适用于通用用例的阶段，取得了令人难以置信的进步。LLaVA-Med 是一种开创性的生物医学大型语言和视觉助手，可以执行多模态生物医学图像和数据分析，为放射科医生提供自然语言界面。虽然它具有高度的通用性，并且可以处理多模态数据，但它目前受到大型语言模型领域现有挑战的限制。回复中的幻觉和不精确性可能导致误诊，这在目前阻碍了 VLM 的临床适应性。为了在医疗保健领域创建精确、用户友好的模型，我们提出了 D-Rax，这是一种特定领域、对话式的放射学辅助工具，可用于获取有关特定放射图像的见解。在这项研究中，我们增强了胸部 X 光（CXR）图像的对话分析，以支持放射学报告，提供来自医学成像的全面见解，并帮助制定准确的诊断。D-Rax 的实现是通过在我们策划的增强型指令跟随数据上微调 LLaVA-Med 架构来实现的，这些数据包括图像、指令以及从 MIMIC-CXR 成像数据、CXR 相关视觉问答 (VQA) 对和多个专家 AI 模型的预测结果中得出的疾病诊断和人口统计学预测。我们观察到，在对开放式和封闭式对话进行评估时，响应在统计学上都有显著改善。D-Rax 利用最先进的诊断模型与 VLM 相结合的力量，使临床医生能够使用自然语言与医学图像进行交互，这有可能简化他们的决策过程，提高诊断准确性并节省他们的时间。||
|**2024-07-02**|[Understanding Alignment in Multimodal LLMs: A Comprehensive Study](http://arxiv.org/abs/2407.02477)|null|偏好对齐已成为提升大型语言模型 (LLM) 性能的关键组成部分，但其对多模态大型语言模型 (MLLM) 的影响仍相对缺乏研究。与语言模型类似，用于图像理解任务的 MLLM 也面临着诸如幻觉之类的挑战。在 MLLM 中，幻觉不仅可以通过陈述错误的事实发生，还可以通过产生与图像内容不一致的响应来发生。MLLM 对齐的主要目标是鼓励这些模型使响应与图像信息更加一致。最近，多项工作引入了 MLLM 的偏好数据集，并研究了不同的对齐方法，包括直接偏好优化 (DPO) 和近端策略优化 (PPO)。然而，由于数据集、基础模型类型和对齐方法的不同，目前尚不清楚哪些具体因素对这些工作中报告的改进贡献最大。在本文中，我们独立分析了 MLLM 中偏好对齐的各个方面。我们首先将对齐算法分为两组，离线（如 DPO）和在线（如在线 DPO），并表明结合离线和在线方法可以在某些情况下提高模型的性能。我们回顾了各种已发布的多模态偏好数据集，并讨论了其构建细节如何影响模型性能。基于这些见解，我们引入了一种创建多模态偏好数据的新方法，称为偏差驱动幻觉采样 (BDHS)，它既不需要额外的注释也不需要外部模型，并表明它可以在各种基准测试中实现与先前发布的多模态模型对齐工作相当的性能。||
|**2024-07-02**|[Conceptual Codebook Learning for Vision-Language Models](http://arxiv.org/abs/2407.02350)|null|在本文中，我们提出了概念码本学习（CoCoLe），这是一种针对视觉语言模型（VLM）的新型微调方法，旨在解决在少量样本情况下对下游任务进行微调时提高VLM泛化能力的挑战。我们认识到，视觉概念（如纹理、形状和颜色）可以自然地跨域迁移，并且在泛化任务中发挥着至关重要的作用。受这一有趣发现的启发，我们学习了一个由视觉概念作为键、概念提示作为值的概念码本，它充当图像编码器输出和文本编码器输入之间的桥梁。具体来说，对于给定的图像，我们利用码本识别与类嵌入相关的最相关的概念提示，以执行分类。此外，我们还结合了一个手工制作的概念缓存作为正则化，以缓解低样本情况下出现的过拟合问题。我们观察到，这种概念码本学习方法能够增强视觉和语言模态之间的对齐。大量的实验结果表明，我们的CoCoLe方法在各种评估设置（包括从基础到新的泛化、跨数据集评估和域泛化任务）中都明显优于现有的最先进方法。详细的消融研究进一步证实了CoCoLe中每个组件的有效性。||
|**2024-07-02**|[Synthetic Multimodal Question Generation](http://arxiv.org/abs/2407.02233)|null|多模态检索增强生成 (MMRAG) 是一种强大的多模态文档问答方法。评估 MMRAG 的一个关键挑战是缺乏与目标问题风格和模态相匹配的高质量数据集。鉴于此，我们提出了 SMMQG，一个合成数据生成框架。SMMQG 利用检索器、大型语言模型 (LLM) 和大型多模态模型 (LMM) 之间的相互作用，直接从多模态文档中生成问答对，并使问题符合指定的风格和模态。我们使用 SMMQG 从维基百科文档中生成了一个包含 1024 个问题的 MMRAG 数据集，并使用该数据集评估了最先进的模型，揭示了只有通过特定风格和模态的评估数据才能获得的模型性能洞察。接下来，我们通过人工研究来衡量 SMMQG 产生的数据的质量。我们发现，我们的合成数据的质量与众包基准 MMQA 的质量相当，并且使用这两个数据集的下游评估结果非常一致。||
|**2024-07-02**|[Multi-Modal Video Dialog State Tracking in the Wild](http://arxiv.org/abs/2407.02218)|null|我们提出了 MST-MIXER，这是一个基于通用多模态状态跟踪方案的新型视频对话模型。目前声称能够执行多模态状态跟踪的模型在两个主要方面存在不足：(1) 它们要么只跟踪一种模态（主要是视觉输入），要么 (2) 它们针对的是不能反映现实世界复杂性的合成数据集。我们的模型解决了这两个限制，试图弥合这一关键的研究差距。具体来说，MST-MIXER 首先跟踪每个输入模态中最重要的成分。然后，它通过使用一种新颖的多模态图结构学习方法学习局部潜在图，从而预测每个模态所选成分缺失的底层结构。随后，将学习到的局部图和特征一起解析，形成一个在所有模态混合上运行的全局图，从而进一步细化其结构和节点嵌入。最后，利用细粒度的图节点特征来增强骨干视觉语言模型 (VLM) 的隐藏状态。MST-MIXER 在五个具有挑战性的基准测试中取得了新的最先进成果。||

<p align=right>(<a href=#updated-on-20240717>back to top</a>)</p>

## 6DOF Object Pose

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-06**|[Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking](http://arxiv.org/abs/2406.04316)|null|6D物体姿态估计是计算机视觉中一项至关重要但极具挑战性的任务，其面临的主要问题是大规模数据集的缺乏。这种稀缺性阻碍了对模型性能的全面评估，限制了研究进展。此外，可用实例或类别的数量有限也限制了其应用。为了解决这些问题，本文提出了Omni6DPose，这是一个以对象类别多样性、规模大和对象材质多样性为特征的大型数据集。Omni6DPose主要分为三个部分：ROPE（真实6D物体姿态估计数据集），包含332K张图像，涵盖149个类别、581个实例的超过150万个标注；SOPE（模拟6D物体姿态估计数据集），包含在混合现实环境中创建的475K张图像，使用深度模拟技术，对149个类别、4162个实例进行了超过500万个标注；以及在ROPE和SOPE中均使用的手动对齐的真实扫描物体。由于存在大量的变化和歧义，Omni6DPose本身就具有挑战性。为了应对这一挑战，我们推出了GenPose++，这是对SOTA类别级姿态估计框架的增强版本，它包含两个关键改进：语义感知特征提取和基于聚类的聚合。此外，我们还提供了全面的基准测试分析，以评估先前方法在这个大规模数据集上在6D物体姿态估计和姿态跟踪方面的性能。|
|**2024-06-05**|[Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices](http://arxiv.org/abs/2406.02977)|null|随着机器人和增强现实应用越来越依赖于精确高效的6D物体姿态估计，边缘设备上的实时性能对于更具交互性和响应能力的系统变得至关重要。我们提出的稀疏颜色编码网络（SCCN）体现了一种清晰简洁的流程设计，可以有效地满足这一需求。SCCN利用基本物体几何特征的稀疏性来加速透视n点（PnP）计算过程，对RGB图像中的目标物体进行像素级预测。此外，它引入了一种新颖的基于像素级几何的物体对称性表示，该表示与初始姿态预测无缝集成，有效地解决了对称物体歧义问题。值得注意的是，SCCN在英伟达Jetson AGX Xavier上分别在基准LINEMOD数据集和遮挡LINEMOD数据集上实现了每秒19帧（FPS）和6 FPS的估计速率，同时在这些速率下始终保持较高的估计精度。|
|**2024-05-31**|[Deep Learning-Based Object Pose Estimation: A Comprehensive Survey](http://arxiv.org/abs/2405.07801)|**[link](https://github.com/cnjianliu/awesome-object-pose-estimation)**|物体姿态估计是计算机视觉中的一个基本问题，在增强现实和机器人技术中有着广泛的应用。在过去的十年中，深度学习模型由于其优越的准确性和鲁棒性，已经逐渐取代了依赖于工程点对特征的传统算法。然而，当代方法仍然存在一些挑战，包括对标记训练数据的依赖、模型紧凑性、在挑战性条件下的鲁棒性以及泛化到新的未见过物体能力。最近缺少一项关于该领域不同方面取得的进展、面临的挑战和未来有希望方向的综述。为了填补这一空白，我们讨论了基于深度学习的物体姿态估计的最新进展，涵盖了该问题的所有三种形式，即实例级、类别级和未见过物体的姿态估计。我们的综述还涵盖了多种输入数据模态、输出姿态的自由度、物体属性和下游任务，为读者提供了对该领域的全面理解。此外，它还讨论了不同领域的训练范式、推理模式、应用领域、评估指标和基准数据集，并报告了当前最先进方法在这些基准上的性能，从而方便读者为其应用选择最合适的方法。最后，该综述指出了关键挑战，回顾了主要趋势及其优缺点，并确定了未来研究的有希望方向。我们还在 https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation 上持续跟踪最新工作。|
|**2024-03-28**|[Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation](http://arxiv.org/abs/2403.19527)|**[link](https://github.com/leeiieeo/ag-pose)**|类别级 6D 物体姿态估计旨在估计特定类别中未见过实例的旋转、平移和尺寸。在这个领域，基于密集对应的算法取得了领先的性能。然而，它们没有明确地考虑不同实例的局部和全局几何信息，导致对形状变化显著的未见过实例的泛化能力较差。为了解决这个问题，我们提出了一种新颖的实例自适应和几何感知关键点学习方法，用于类别级 6D 物体姿态估计 (AG-Pose)，它包括两个关键设计：（1）第一个设计是实例自适应关键点检测模块，它可以自适应地检测一组稀疏关键点，用于表示各种实例的几何结构。(2) 第二个设计是几何感知特征聚合模块，它可以有效地将局部和全局几何信息整合到关键点特征中。这两个模块可以协同工作，为未见过的实例建立鲁棒的关键点级对应关系，从而增强模型的泛化能力。在 CAMERA25 和 REAL275 数据集上的实验结果表明，所提出的 AG-Pose 在没有类别特定形状先验的情况下，大幅度优于现有技术方法。|
|**2024-06-01**|[Object Pose Estimation via the Aggregation of Diffusion Features](http://arxiv.org/abs/2403.18791)|**[link](https://github.com/tianfu18/diff-feats-pose)**|从图像中估计物体姿态是3D场景理解的一项关键任务，最近的方法在非常大的基准测试中显示出良好的结果。然而，这些方法在处理未见过的物体时性能会显著下降。我们认为这是由于图像特征的泛化能力有限造成的。为了解决这个问题，我们深入分析了扩散模型（如Stable Diffusion）的特征，这些特征在对未见过的物体进行建模方面具有巨大潜力。在此分析的基础上，我们创新性地将这些扩散特征引入到物体姿态估计中。为此，我们提出了三种不同的体系结构，可以有效地捕获和聚合不同粒度的扩散特征，从而大大提高了物体姿态估计的泛化能力。我们的方法在三个流行的基准数据集LM、O-LM和T-LESS上，以相当大的优势优于最先进的方法。特别是，我们的方法在未见过的物体上实现了比之前最佳结果更高的准确率：在Unseen LM上为98.2% vs. 93.5%，在Unseen O-LM上为85.9% vs. 76.3%，显示了我们方法强大的泛化能力。我们的代码已发布在https://github.com/Tianfu18/diff-feats-pose。|
|**2024-03-24**|[KITchen: A Real-World Benchmark and Dataset for 6D Object Pose Estimation in Kitchen Environments](http://arxiv.org/abs/2403.16238)|null|尽管最近在用于机器人抓取的 6D 物体姿态估计方法方面取得了进展，但这些方法在现有数据集上的能力与其在现实世界移动操作任务中的功效之间仍然存在很大的性能差距，特别是当机器人仅依赖于其单目以自我为中心的视野 (FOV) 时。现有的现实世界数据集主要集中在桌面抓取场景，其中机械臂放置在固定位置，并且物体集中在固定外部相机视野内。评估此类数据集的性能可能无法准确反映在厨房环境中日常移动操作任务中遇到的挑战，例如从较高的架子、水槽、洗碗机、烤箱、冰箱或微波炉中检索物体。为了解决这一差距，我们提出了 KITchen，这是一个专门为估计位于厨房环境中不同位置的物体的 6D 姿态而设计的新基准。为此，我们记录了一个综合数据集，其中包含约 205k 张真实世界的 RGBD 图像，这些图像用于在两个不同的厨房中捕获的 111 个厨房物体，利用一个具有人类视角的人形机器人。随后，我们开发了一个半自动注释管道，以简化此类数据集的标记过程，从而以最少的人工生成 2D 对象标签、2D 对象分割掩码和 6D 对象姿态。基准、数据集和注释管道可在 https://kitchen-dataset.github.io/KITchen 获取。|
|**2024-03-22**|[DITTO: Demonstration Imitation by Trajectory Transformation](http://arxiv.org/abs/2403.15203)|null|快速便捷地教授机器人新技能对于机器人系统的广泛采用至关重要。在这项工作中，我们提出了一种两阶段方法，用于解决从单个RGB-D视频记录的人类演示中进行一次性模仿的问题。在第一阶段（离线阶段），我们提取演示的轨迹。这需要分割被操纵的物体，并确定它们相对于容器等辅助物体的相对运动。随后，在实时在线轨迹生成阶段，我们首先重新检测所有物体，然后将演示轨迹扭曲到当前场景，最后用机器人跟踪轨迹。为了完成这些步骤，我们的方法利用了几个辅助模型，包括分割模型、相对物体姿态估计模型和抓取预测模型。我们系统地评估了对应和重新检测方法的不同组合，以验证我们在各种任务中的设计决策。具体来说，我们收集了十种不同任务的演示，包括拾放任务以及铰接物体操作。最后，我们在真实的机器人系统上进行了广泛的评估，以证明我们的方法在现实场景中的有效性和实用性。我们在http://ditto.cs.uni-freiburg.de上公开提供了代码。|
|**2024-03-21**|[Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation](http://arxiv.org/abs/2403.14559)|null|在二维图像中定位预定义的三维关键点是建立用于六自由度物体姿态估计的三维-二维对应关系的有效方法。然而，不可见关键点的不可靠定位结果会降低对应关系的质量。在本文中，我们通过定位可见性方面的关键点来解决这个问题。由于关键点可见性信息在当前的数据集收集过程中缺失，我们提出了一种有效的方法，可以从可用的物体级标注中生成二元可见性标签，用于非对称物体和对称物体的关键点。我们进一步基于 PageRank 算法从二元标签中推导出实值可见性感知重要性。利用我们可见性感知重要性的灵活性，我们将可见性感知重要性与最先进的姿态估计算法相结合，并结合额外的positional encoding，构建了 VAPO（可见性感知姿态估计器）。在流行的姿态估计基准上进行了广泛的实验，包括 Linemod、Linemod-Occlusion 和 YCB-V。结果表明，VAPO 改善了关键点对应关系和最终估计的姿态，并明显达到了最先进的性能。|
|**2024-03-18**|[GenFlow: Generalizable Recurrent Flow for 6D Pose Refinement of Novel Objects](http://arxiv.org/abs/2403.11510)|null|尽管基于学习的方法在6D物体姿态估计方面取得了进展，但新物体在精度和可扩展性之间的权衡仍然存在。具体来说，之前针对新物体的方法没有很好地利用目标物体的3D形状信息，因为它们侧重于通过间接处理形状来实现泛化，从而降低了效率。我们提出了GenFlow，这是一种能够在目标物体形状的指导下实现对新物体的精度和泛化能力的方法。我们的方法预测渲染图像和观察图像之间的光流，并迭代地细化6D姿态。它通过3D形状的约束和从端到端可微系统学习到的可泛化几何知识来提高性能。我们通过设计级联网络架构来进一步改进我们的模型，以利用多尺度相关性和从粗到精的细化。GenFlow在RGB和RGB-D情况下均在未见物体姿态估计基准测试中排名第一。它还实现了与现有最先进的已见物体姿态估计方法相媲美的性能，而无需任何微调。|
|**2024-03-14**|[MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion](http://arxiv.org/abs/2403.09309)|null|杂乱的料箱拣选环境对姿态估计模型提出了挑战。尽管深度学习取得了令人瞩目的进步，但单视图 RGB 姿态估计模型在杂乱的动态环境中表现不佳。利用视频场景中包含的丰富时间信息有可能增强模型处理遮挡和环境动态性的不利影响的能力。此外，联合目标检测和姿态估计模型更适合利用任务的相互依赖性来提高两项任务的准确性。为此，我们提出了一种基于注意力的多目标 6D 姿态估计时间融合方法，该方法可以在视频序列的多个帧中积累信息。我们的 MOTPose 方法将一系列图像作为输入，并在一次前向传递中对所有目标执行联合目标检测和姿态估计。它学习使用基于交叉注意力的融合模块在多个时间步长上聚合目标嵌入和目标参数。我们在物理逼真的杂乱料箱拣选数据集 SynPick 和 YCB-Video 数据集上评估了我们的方法，并证明了改进的姿态估计精度以及更好的目标检测精度。|

<p align=right>(<a href=#updated-on-20240717>back to top</a>)</p>

## nerf

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-07-10**|[3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes](http://arxiv.org/abs/2407.07090)|null|基于粒子的辐射场表示法，例如 3D 高斯 splatting，已经在复杂场景的重建和重新渲染方面取得了巨大成功。大多数现有方法通过光栅化渲染粒子，将它们投影到屏幕空间图块中，以便按排序顺序进行处理。这项工作则考虑对粒子进行光线追踪，构建边界体积层次结构，并使用高性能 GPU 光线追踪硬件为每个像素投射光线。为了有效处理大量半透明粒子，我们描述了一种专门的渲染算法，该算法使用边界网格封装粒子，以利用快速的光线三角形相交，并按深度顺序对成批的相交点进行着色。光线追踪在计算机图形学中的优势是众所周知的：处理非相干光线以获得阴影和反射等二级照明效果，从机器人技术中常见的高度失真相机渲染，对光线进行随机采样等等。使用我们的渲染器，与光栅化相比，这种灵活性几乎不需要任何成本。实验结果证明了我们方法的速度和准确性，以及它在计算机图形学和视觉方面的若干应用。我们还提出了对基本高斯表示的相关改进，包括简单使用广义核函数，这可以显著减少粒子命中次数。|
|**2024-07-07**|[GaussReg: Fast 3D Registration with Gaussian Splatting](http://arxiv.org/abs/2407.05254)|null|点云配准是大规模三维场景扫描和重建的基本问题。在深度学习的帮助下，配准方法已经取得了显著的进步，并日趋成熟。随着神经辐射场（NeRF）的引入，它凭借强大的视图合成能力成为最受欢迎的三维场景表示方法。对于 NeRF 表示，大规模场景重建也需要对其进行配准。然而，这个主题还极度缺乏探索。这是因为在用隐式表示对两个场景之间的几何关系进行建模方面存在固有挑战。现有方法通常将隐式表示转换为显式表示以进行进一步配准。最近，引入了高斯散射（GS），它采用显式三维高斯函数。这种方法在保持高质量渲染效果的同时，显著提高了渲染速度。给定两个具有显式 GS 表示的场景，在这项工作中，我们探索了它们之间的三维配准任务。为此，我们提出了 GaussReg，一种快速且准确的由粗到精的新型框架。粗配准阶段遵循现有的点云配准方法，并估计来自 GS 的点云的粗略对齐。我们进一步提出了一种新的图像引导的精配准方法，该方法从 GS 渲染图像，为精确对齐提供更详细的几何信息。为了支持全面评估，我们精心构建了一个名为 ScanNet-GSReg 的场景级数据集，其中包含从 ScanNet 数据集中获得的 1379 个场景，并收集了一个名为 GSReg 的真实世界数据集。实验结果表明，我们的方法在多个数据集上实现了最先进的性能。我们的 GaussReg 比 HLoc（SuperPoint 作为特征提取器，SuperGlue 作为匹配器）快 44 倍，并且具有相当的精度。|
|**2024-07-04**|[CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion Blur Images](http://arxiv.org/abs/2407.03923)|null|神经辐射场 (NeRFs) 因其高质量的新视角渲染能力而备受关注，促使人们对其在各种现实世界案例中的应用进行研究。其中一个关键挑战是相机在曝光时间内移动造成的相机运动模糊，这阻碍了对 3D 场景的准确重建。在本研究中，我们提出了连续刚体运动感知高斯散射 (CRiM-GS)，以实时渲染速度从模糊图像中重建准确的 3D 场景。考虑到实际的相机运动模糊过程包含复杂的运动模式，我们基于神经常微分方程 (ODE) 预测相机的连续运动。具体来说，我们利用刚体变换对相机运动进行建模，并进行适当的正则化，以保持对象的形状和大小。此外，我们在 \textit{SE(3)} 场中引入了连续可变形 3D 变换，通过确保更高的自由度使刚体变换适应现实问题。通过重新审视基本的相机理论并采用先进的神经网络训练技术，我们实现了对连续相机轨迹的精确建模。我们进行了广泛的实验，在基准数据集上定量和定性地证明了其最先进的性能。|
|**2024-06-26**|[Trimming the Fat: Efficient Compression of 3D Gaussian Splats through Pruning](http://arxiv.org/abs/2406.18214)|null|近年来，由于神经辐射场和最近出现的3D高斯散射（3DGS）模型提供了端到端训练的能力，3D模型的使用得到了越来越多的关注。后者具有显著的优势，因为它本身可以简化训练过程中的快速收敛，并提供广泛的可编辑性。然而，尽管取得了快速进展，但关于这些模型可扩展性的文献仍然处于起步阶段。在本研究中，我们针对解决这一差距采取了一些初步措施，展示了一种能够实现此类模型的内存和计算可扩展性的方法。具体来说，我们提出了“Trimming the fat”，这是一种基于梯度的迭代剪枝后处理技术，用于消除模型中编码的冗余信息。我们在广泛认可的基准测试集上的实验结果证明了我们方法的有效性，表明在保持甚至改进基线性能的同时，可以去除高达75%的高斯函数。我们的方法实现了大约50倍的压缩，同时保持了与基线模型相似的性能，并且能够将计算速度提高到600帧/秒。|
|**2024-06-21**|[Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks](http://arxiv.org/abs/2406.15149)|null|模拟器是自主机器人学习的强大工具，因为它们提供了可扩展的数据生成、灵活的设计和轨迹优化。然而，将从模拟数据中学习到的行为转移到现实世界中被证明是困难的，通常需要通过计算量大的域随机化方法或进一步的模型微调来缓解。我们提出了一种改进泛化能力和对模拟到真实视觉四旋翼导航任务中分布变化的鲁棒性的方法。为此，我们首先通过将高斯 splatting 与四旋翼飞行动力学相结合来构建模拟器，然后使用 Liquid 神经网络训练鲁棒的导航策略。通过这种方式，我们获得了一个全栈模仿学习协议，它结合了 3D 高斯 splatting 辐射场渲染的进步、专家演示训练数据的巧妙编程以及 Liquid 网络的任务理解能力。通过一系列定量飞行测试，我们证明了在单个模拟场景中学习到的导航技能可以直接稳健地迁移到现实世界。我们进一步展示了在剧烈的分布和物理环境变化下，在训练环境之外保持性能的能力。我们学习到的 Liquid 策略，仅在从逼真的模拟室内飞行中精选的单目标机动上进行训练，可以泛化到户外真实硬件平台上的多步远足。|
|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|在非结构化的旅游环境中拍摄的照片经常表现出多变的外观和短暂的遮挡，这对精确的场景重建提出了挑战，并在新视角合成中导致了伪影。尽管先前的方法已经将神经辐射场 (NeRF) 与其他可学习模块集成以处理动态外观和消除短暂对象，但其大量的训练需求和缓慢的渲染速度限制了实际部署。最近，3D 高斯 splatting (3DGS) 已成为 NeRF 的一个有前途的替代方案，它提供了卓越的训练和推理效率以及更好的渲染质量。本文介绍了 Wild-GS，这是一种针对不受约束的照片集优化的 3DGS 创新改编，同时保留了其效率优势。Wild-GS 通过每张图像固有的材质属性、全局照明和相机属性，以及逐点反射率的局部方差来确定每个 3D 高斯的外观。与先前在图像空间中模拟参考特征的方法不同，Wild-GS 通过对从参考图像中提取的三平面进行采样，将像素外观特征明确地与相应的局部高斯对齐。这种新颖的设计有效地将参考视图的高频细节外观转移到 3D 空间，并显着加快了训练过程。此外，利用 2D 可见性图和深度正则化分别减轻瞬态效应和约束几何形状。大量实验表明，Wild-GS 在所有现有技术中实现了最先进的渲染性能以及最高的训练和推理效率。|
|**2024-06-06**|[A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation](http://arxiv.org/abs/2406.04253)|null|三维建模一直是计算机视觉和计算机图形学的重要领域。近年来，由于神经表示和生成模型的突破，我们见证了三维建模的快速发展。三维人体建模作为游戏和动画等众多现实应用的核心，受到了广泛关注。在过去的几年里，出现了大量关于创建三维人体化身的工作，为三维人体建模形成了一个新的、丰富的知识库。文献的规模之大，使得个人难以追踪所有的工作。本次综述旨在从重建和生成两个角度，全面概述这些新兴的三维人体化身建模技术。首先，我们回顾了具有代表性的三维人体重建方法，包括基于像素对齐隐函数、神经辐射场和三维高斯散射等方法。然后，我们总结了具有代表性的三维人体生成方法，特别是那些使用大型语言模型（如CLIP）、扩散模型和各种三维表示的方法，这些方法展示了最先进的性能。最后，我们讨论了对现有方法的反思以及三维人体化身建模面临的挑战，为未来的研究指明了方向。|
|**2024-06-13**|[3D-HGS: 3D Half-Gaussian Splatting](http://arxiv.org/abs/2406.02720)|**[link](https://github.com/lihaolin88/3d-half-gaussian-splatting)**|逼真的三维重建是三维计算机视觉中的一个基本问题。由于最近神经渲染技术的出现，该领域取得了长足的进步。这些技术主要旨在专注于学习三维场景的体积表示，并通过渲染得到的损失函数来细化这些表示。其中，三维高斯散射 (3D-GS) 已成为一种重要的方法，其性能超越了神经辐射场 (NeRF)。3D-GS 使用参数化的三维高斯函数来建模空间位置和颜色信息，并结合基于图块的快速渲染技术。尽管其渲染性能和速度都非常出色，但使用三维高斯核在准确表示不连续函数方面存在固有的局限性，特别是在形状不连续的边缘和角落，以及在颜色不连续的不同纹理之间。为了解决这个问题，我们建议采用三维半高斯 (3D-HGS) 核，它可以作为一种即插即用的核函数。我们的实验表明，它们能够提高当前与 3D-GS 相关方法的性能，并在不影响渲染速度的情况下，在各种数据集上实现最先进的渲染性能。|
|**2024-06-04**|[FastLGS: Speeding up Language Embedded Gaussians with Feature Grid Mapping](http://arxiv.org/abs/2406.01916)|null|语义交互式辐射场因其促进用户友好和自动化的现实世界3D场景理解应用的潜力而一直是一项吸引人的任务。然而，在辐射场中同时实现高质量、高效率和零样本能力的语义是一项具有挑战性的任务。在这项工作中，我们提出了FastLGS，这是一种支持高分辨率下3D高斯渲染（3DGS）中实时开放词汇查询的方法。我们提出了语义特征网格来保存基于Segment Anything Model (SAM)掩码提取的多视图CLIP特征，并将网格映射到低维特征，以便通过3DGS进行语义场训练。一旦训练完成，我们就可以通过渲染特征的特征网格恢复像素对齐的CLIP嵌入，用于开放词汇查询。与其他最先进方法的比较证明，FastLGS在速度和精度方面都能达到第一名的性能，其中FastLGS比LERF快98倍，比LangSplat快4倍。同时，实验表明FastLGS具有适应性，并且兼容许多下游任务，例如3D分割和3D对象修复，可以很容易地应用于其他3D操作系统。|
|**2024-05-30**|[ $\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving](http://arxiv.org/abs/2405.20323)|**[link](https://github.com/nnanhuang/s3gaussian)**|逼真的街道场景三维重建是开发自动驾驶真实世界模拟器的关键技术。尽管神经辐射场 (NeRF) 对驾驶场景的重建效果很好，但三维高斯散射 (3DGS) 凭借其更快的速度和更明确的表示，成为一个很有前途的方向。然而，大多数现有的街道 3DGS 方法都需要跟踪的三维车辆边界框来分解静态和动态元素以进行有效重建，这限制了它们在野外场景中的应用。为了在无需昂贵标注的情况下实现高效的三维场景重建，我们提出了一种自监督的街道高斯（$\textit{S}^3$Gaussian）方法，利用四维一致性来分解动态和静态元素。我们使用三维高斯函数来表示每个场景以保持其清晰度，并进一步结合时空场网络来紧凑地建模四维动态。我们在具有挑战性的 Waymo-Open 数据集上进行了大量实验，以评估我们方法的有效性。我们的 $\textit{S}^3$ Gaussian 展示了在不使用三维标注的情况下分解静态和动态场景的能力，并取得了最佳性能。代码可在以下网址获得：https://github.com/nnanhuang/S3Gaussian/。|
|**2024-05-28**|[RT-GS2: Real-Time Generalizable Semantic Segmentation for 3D Gaussian Representations of Radiance Fields](http://arxiv.org/abs/2405.18033)|null|高斯渲染技术通过实现实时的高性能渲染，彻底改变了新视角合成的世界。最近，研究重点集中在为下游任务丰富这些3D表示的语义信息。在本文中，我们介绍了RT-GS2，这是第一个采用高斯渲染技术的可泛化语义分割方法。虽然现有的基于高斯渲染的方法依赖于场景特定的训练，但RT-GS2展示了泛化到未见场景的能力。我们的方法采用了一种新方法，首先以自监督的方式提取视图无关的3D高斯特征，然后进行新颖的视图依赖/视图无关（VDVI）特征融合，以增强不同视图之间的语义一致性。在三个不同数据集上的大量实验表明，RT-GS2在语义分割质量方面优于最先进的方法，例如在Replica数据集上的mIoU提高了8.01%。此外，我们的方法实现了27.03 FPS的实时性能，与现有方法相比实现了惊人的901倍加速。据我们所知，这项工作通过引入第一个用于辐射场3D高斯表示的实时可泛化语义分割方法，代表了该领域的重大进步。||
|**2024-05-29**|[PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting](http://arxiv.org/abs/2405.16829)|null|神经辐射场 (NeRFs) 在合成大规模场景的逼真图像方面表现出了非凡的能力。然而，它们经常受到细节丢失和渲染时间长的困扰。三维高斯 splatting 最近被引入作为一种有效的替代方案，可以实现高保真视觉效果和更快的渲染性能。尽管如此，扩展三维高斯 splatting 仍然充满了挑战。具体来说，大规模场景需要整合来自多个尺度和不同视点的对象，这通常会导致效率下降，因为高斯需要在细节级别之间取得平衡。此外，从大规模数据集中通过 COLMAP 生成初始化点不仅计算量大，而且容易导致重建不完整。为了应对这些挑战，我们提出了采用 NeRF 初始化的金字塔式三维高斯 splatting (PyGS)。我们的方法采用以金字塔形式排列的分层高斯集合来表示场景。金字塔的顶层由一些大的高斯函数组成，而随后的每一层都包含更密集的小高斯函数集合。我们通过以不同的频率对快速训练的基于网格的 NeRF 进行采样，从而有效地初始化这些金字塔高斯函数。我们将这些金字塔高斯函数分组到簇中，并使用紧凑的加权网络在渲染过程中动态确定每个簇中每个金字塔级别的影响，同时考虑相机视点。我们的方法在多个大规模数据集上实现了显著的性能飞跃，渲染速度比当前最先进的方法快 400 多倍。||
|**2024-05-11**|[Direct Learning of Mesh and Appearance via 3D Gaussian Splatting](http://arxiv.org/abs/2405.06945)|null|准确重建包含显式几何信息的3D场景既有吸引力又具有挑战性。几何重建可以受益于结合可微分的表观模型，例如神经辐射场和3D高斯 splatting (3DGS)。在这项工作中，我们提出了一个可学习的场景模型，它将3DGS与显式几何表示（即网格）结合起来。我们的模型以端到端的方式学习网格和外观，我们将3D高斯函数绑定到网格面上，并执行3DGS的可微分渲染以获得光度监督。该模型创建了一个有效的信息通路来监督场景学习，包括网格。实验结果表明，学习到的场景模型不仅实现了最先进的渲染质量，而且还支持使用显式网格进行操作。此外，由于网格和外观的端到端学习，我们的模型在适应场景更新方面具有独特优势。||

<p align=right>(<a href=#updated-on-20240717>back to top</a>)</p>

## 分类/检测/识别/分割

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-07-16**|[Improving Unsupervised Video Object Segmentation via Fake Flow Generation](http://arxiv.org/abs/2407.11714)|**[link](https://github.com/suhwan-cho/FakeFlow)**|无监督视频目标分割（VOS），也称为视频显著目标检测，旨在像素级别检测视频中最突出的目标。近年来，利用 RGB 图像和光流图的两流方法受到了广泛关注。然而，训练数据的有限性仍然是一个巨大的挑战。在本研究中，我们提出了一种新的数据生成方法，可以从单个图像模拟生成伪光流，从而为稳定的网络学习创建大规模训练数据。受光流图高度依赖于深度图的观察结果的启发，我们通过细化和增强每个图像的估计深度图来生成伪光流。通过结合我们模拟的图像-流对，我们在不依赖复杂模块的情况下，在所有公共基准数据集上实现了新的最先进性能。我们相信，我们的数据生成方法代表了未来 VOS 研究的潜在突破。|
|**2024-07-16**|[Relation DETR: Exploring Explicit Position Relation Prior for Object Detection](http://arxiv.org/abs/2407.11699)|**[link](https://github.com/xiuqhou/relation-detr)**|本文提出了一种增强DETR（DEtection TRansformer）收敛性和性能的通用方案。我们从一个新的角度研究了Transformer中的慢收敛问题，认为它源于自注意力机制对输入没有引入结构性偏差。为了解决这个问题，我们在使用所提出的定量宏观相关性（MC）度量验证其统计显著性后，探索将位置关系先验作为注意力偏差来增强目标检测。我们提出的方法称为Relation-DETR，它引入了一个编码器来构建位置关系嵌入，用于渐进式注意力细化，这进一步将DETR的传统流式管道扩展为对比关系管道，以解决非重复预测和正监督之间的冲突。在通用和特定任务数据集上的大量实验表明了我们方法的有效性。在相同的配置下，Relation-DETR取得了显著的改进（与DINO相比，AP提高了+2.0%），达到了最先进的性能（1倍设置下AP为51.7%，2倍设置下AP为52.1%），并且在COCO val2017上实现了比现有DETR检测器快得多的收敛速度（仅用2个训练周期就获得了超过40%的AP）。此外，所提出的关系编码器是一个通用的即插即用组件，可以为理论上任何类似DETR的方法带来明显的改进。此外，我们还引入了一个类别无关的目标检测数据集SA-Det-100k。在该数据集上的实验结果表明，所提出的显式位置关系使AP有了1.3%的明显提高，突出了其在通用目标检测方面的潜力。代码和数据集可在https://github.com/xiuqhou/Relation-DETR获取。|
|**2024-07-16**|[Probing the Efficacy of Federated Parameter-Efficient Fine-Tuning of Vision Transformers for Medical Image Classification](http://arxiv.org/abs/2407.11573)|null|随着大型预训练Transformer模型的出现，针对各种下游任务微调这些模型成为一个关键问题。训练数据的缺乏、数据孤岛的存在以及严格的隐私限制加剧了医学影像领域中的微调问题，因此迫切需要能够协同微调预训练模型的算法。此外，这些模型的庞大规模使得必须使用参数高效微调（PEFT）来减少联邦学习中的通信负担。在这项工作中，我们系统地研究了各种联邦PEFT策略，用于将视觉Transformer（ViT）模型（在大规模自然图像数据集上预训练）应用于医学图像分类。除了评估已知的PEFT技术外，我们还引入了PEFT算法的新联邦变体，例如视觉提示调优（VPT）、视觉提示的低秩分解、随机块注意力微调以及混合PEFT方法（如低秩自适应（LoRA）+VPT）。此外，我们还进行了全面的实证分析，以确定适用于联邦设置的最佳PEFT方法，并了解数据分布对联邦PEFT的影响，特别是在域外（OOD）和非独立同分布（non-IID）数据的情况下。这项研究的关键见解是，虽然大多数联邦PEFT方法在域内迁移中表现良好，但在处理OOD和非IID场景时，精度和效率之间存在巨大的权衡，这在医学影像中很常见。具体来说，微调/交换参数每减少一个数量级，精度就会下降4%。因此，初始模型的选择对于联邦PEFT至关重要。如果可能的话，最好使用从域内医学图像数据中学习到的医学基础模型，而不是通用视觉模型。|
|**2024-07-16**|[Bridge Past and Future: Overcoming Information Asymmetry in Incremental Object Detection](http://arxiv.org/abs/2407.11499)|null|在增量目标检测领域，知识蒸馏已被证明是缓解灾难性遗忘的有效方法。然而，以往的工作侧重于保留旧模型的知识，而忽略了图像可能同时包含来自过去、现在和未来阶段的类别。由于不同阶段对前景目标的定义不同，目标的共现使得优化目标在不同阶段不一致，这极大地限制了模型的性能。为了克服这个问题，我们提出了一种名为“桥接过去与未来”（BPF）的方法，它可以跨阶段对齐模型，确保一致的优化方向。此外，我们还提出了一种新颖的“未来蒸馏”（DwF）损失函数，充分利用背景概率来减轻对旧类别的遗忘，同时确保学习新类别的高度适应性。我们在 Pascal VOC 和 MS COCO 基准测试集上进行了广泛的实验。在没有记忆的情况下，BPF 在各种设置下都优于当前最先进的方法。代码可在 https://github.com/iSEE-Laboratory/BPF 获取。|
|**2024-07-16**|[Crowd-SAM: SAM as a Smart Annotator for Object Detection in Crowded Scenes](http://arxiv.org/abs/2407.11464)|**[link](https://github.com/felixcaae/crowdsam)**|在计算机视觉领域，目标检测是一项重要的任务，在许多场景中都有应用。然而，获取大量的标签可能具有挑战性，尤其是在拥挤的场景中。最近，Segment Anything Model (SAM) 被提出作为一种强大的零样本分割器，为实例分割任务提供了一种新颖的方法。然而，SAM 及其变体在处理拥挤和遮挡场景中的物体时，其准确性和效率往往会受到影响。在本文中，我们介绍了 Crowd-SAM，这是一个基于 SAM 的框架，旨在以少量可学习参数和最小限度标记图像的成本提高 SAM 在拥挤和遮挡场景中的性能。我们引入了高效的提示采样器 (EPS) 和部分-整体判别网络 (PWD-Net)，增强了拥挤场景中的掩码选择和准确性。尽管 Crowd-SAM 结构简单，但它在 CrowdHuman 和 CityPersons 等多个基准测试中，可与最先进的 (SOTA) 全监督目标检测方法相媲美。我们的代码可在 https://github.com/FelixCaae/CrowdSAM 获取。|
|**2024-07-16**|[Leveraging Segment Anything Model in Identifying Buildings within Refugee Camps (SAM4Refugee) from Satellite Imagery for Humanitarian Operations](http://arxiv.org/abs/2407.11381)|**[link](https://github.com/yunyagaotree/sam-adapter-for-refugee-dwelling-extraction)**|利用高分辨率卫星图像更新带有难民营的建筑物覆盖范围可以支持相关的人道主义行动。本研究探讨了利用“分割一切模型”（SAM）及其分支之一 SAM-Adapter 进行语义分割任务，以从卫星图像中提取建筑物。SAM-Adapter 是 SAM 的轻量级改编版本，在不同难民营的提取任务中，它都是一个强大的工具。我们的研究证明，与其他经典（例如 U-Net）或高级语义分割模型（例如 Transformer）相比，SAM-Adapter 在数据可用性有限的情况下表现出色。此外，还强调了放大技术对模型性能的影响，事实证明，超分辨率 (SR) 模型等方法对于提高模型性能非常宝贵。此外，该研究还揭示了一些有趣的现象，包括在使用放大图像数据进行训练时，模型在第一个训练时期快速收敛，这为未来的研究提供了机会。涵盖从数据准备、模型训练、模型推理到预测掩码的 Shapefile 生成的每个步骤的代码都可以在 GitHub 存储库中找到，从而使更广泛的科学界和人道主义行动受益。|
|**2024-07-16**|[Generative AI Driven Task-Oriented Adaptive Semantic Communications](http://arxiv.org/abs/2407.11354)|null|面向任务的语义通信 (TOSC) 被认为是一种很有前途的通信框架，可用于各种人工智能 (AI) 任务驱动型应用。现有的 TOSC 框架侧重于提取源数据的完整语义特征，并学习低维信道输入以在有限的带宽资源内传输它们。 虽然传输完整的语义特征可以保持数据含义的完整性，但这种方法无法达到 TOSC 的性能阈值。在本文中，我们提出了一种面向任务的自适应语义通信 (TasCom) 框架，旨在通过仅发送与任务相关的语义特征来有效地促进 AI 任务的执行。在 TasCom 框架中，我们首先提出了一种基于生成式人工智能 (GAI) 架构的生成式联合信源信道编码 (G-JSCC) 以实现高效的语义传输。然后，提出了一种自适应编码控制器 (ACC)，以找到所提出的 G-JSCC 的最佳编码方案，该方案允许对 AI 任务有重大贡献的语义特征优先占用有限的带宽资源进行无线传输。此外，我们提出了一种生成式训练算法来训练所提出的 TasCom 以获得最佳性能。仿真结果表明，所提出的 TasCom 在所有考虑的信道条件下，在目标检测和实例分割任务上均优于现有的 TOSC 和传统编解码器方案。|
|**2024-07-16**|[LaMI-DETR: Open-Vocabulary Detection with Language Model Instruction](http://arxiv.org/abs/2407.11335)|null|现有的开放词汇目标检测方法利用视觉语言模型 (VLMs)（如 CLIP）强大的开放词汇识别能力来增强性能。然而，出现了两个主要挑战：(1) 概念表示的缺陷，CLIP 文本空间中的类别名称缺乏文本和视觉知识。(2) 对基本类别的过度拟合倾向，在从 VLM 到检测器的迁移过程中，开放词汇知识偏向于基本类别。为了应对这些挑战，我们提出了语言模型指令 (LaMI) 策略，该策略利用视觉概念之间的关系，并在一个简单而有效的类 DETR 检测器（称为 LaMI-DETR）中应用它们。LaMI 利用 GPT 构建视觉概念，并利用 T5 研究类别之间的视觉相似性。这些类别间关系改进了概念表示，并避免了对基本类别的过度拟合。综合实验验证了我们的方法在相同的严格设置下优于现有方法，并且不依赖于外部训练资源。LaMI-DETR 在 OV-LVIS 上实现了 43.4 的罕见框 AP，超过了之前的最佳结果 7.8 个罕见框 AP。|
|**2024-07-16**|[TCFormer: Visual Recognition via Token Clustering Transformer](http://arxiv.org/abs/2407.11321)|**[link](https://github.com/zengwang430521/tcformer)**|Transformer模型在计算机视觉领域得到广泛应用并取得了显著成功。大多数最先进的方法将图像分割成规则网格，并使用视觉标记表示每个网格区域。然而，固定的标记分布忽略了不同图像区域的语义含义，导致性能欠佳。为了解决这个问题，我们提出了标记聚类Transformer（TCFormer），它根据语义含义生成动态视觉标记。我们的动态标记具有两个关键特征：（1）使用相同的视觉标记表示具有相似语义含义的图像区域，即使这些区域不邻接；（2）集中于具有宝贵细节的区域，并使用精细标记表示它们。通过对图像分类、人体姿态估计、语义分割和目标检测等各种应用的广泛实验，我们证明了TCFormer的有效性。这项工作的代码和模型可在https://github.com/zengwang430521/TCFormer获取。|
|**2024-07-16**|[Zero-Shot Adaptation for Approximate Posterior Sampling of Diffusion Models in Inverse Problems](http://arxiv.org/abs/2407.11288)|null|扩散模型已成为解决反问题的强大生成技术。尽管扩散模型已成功应用于各种成像反问题，但这些模型需要许多步骤才能收敛，导致推理时间较长。最近，扩散模型出现了一种趋势，即采用复杂的噪声调度，在较低的噪声水平下更频繁地迭代时间步长，从而改善图像生成和收敛速度。然而，将这些想法应用于解决扩散模型的反问题仍然具有挑战性，因为当使用经验调整来确定正向模型对数似然项权重时，这些噪声调度表现不佳。为了解决这些挑战，我们提出了零样本近似后验采样 (ZAPS) 方法，该方法利用了与零样本物理驱动深度学习的联系。ZAPS 固定了采样步数，并使用零样本训练和物理引导的损失函数来学习每个不规则时间步长的对数似然权重。我们将 ZAPS 应用于最近提出的扩散后验采样方法作为基线，尽管 ZAPS 也可以与其他后验采样扩散模型一起使用。我们进一步使用可学习对角元素的对角化方法来逼近先验对数的 Hessian，以提高计算效率。这些参数在给定计算预算下通过固定次数的 epochs 进行优化。我们对各种噪声反问题的研究结果（包括高斯和运动去模糊、修复和超分辨率）表明，ZAPS 减少了推理时间，提高了对不规则噪声调度的鲁棒性，并提高了重建质量。代码可在 https://github.com/ualcalar17/ZAPS 获取。|
|**2024-07-12**|[Region Attention Transformer for Medical Image Restoration](http://arxiv.org/abs/2407.09268)|**[link](https://github.com/yaziwel/region-attention-transformer-for-medical-image-restoration)**|基于Transformer的方法在医学图像恢复方面表现出令人印象深刻的结果，这归功于其在空间维度上的多头自注意力（MSA）机制。然而，大多数现有的Transformer在固定且粗略划分的区域内进行注意力计算（例如，整幅图像或固定块），导致来自不相关区域的干扰和连续图像内容的碎片化。为了克服这些挑战，我们引入了一种新颖的区域注意力Transformer（RAT），它利用了基于区域的多头自注意力机制（R-MSA）。R-MSA使用强大的Segment Anything Model (SAM)将输入图像动态地划分为非重叠的语义区域，然后在这些区域内执行自注意力。这种区域划分更加灵活和可解释，确保只有来自相似语义区域的像素相互补充，从而消除了来自不相关区域的干扰。此外，我们引入了一个焦点区域损失来引导我们的模型自适应地关注恢复高难度区域。大量实验表明，RAT在各种医学图像恢复任务中是有效的，包括PET图像合成、CT图像去噪和病理图像超分辨率。代码可在\href{https://github.com/Yaziwel/Region-Attention-Transformer-for-Medical-Image-Restoration.git}{https://github.com/RAT}获取。|
|**2024-07-12**|[Open Vocabulary Multi-Label Video Classification](http://arxiv.org/abs/2407.09073)|null|预训练的视觉语言模型 (VLM) 在开放词汇计算机视觉任务（如图像分类、目标检测和图像分割）方面取得了显著进展。最近的一些工作集中于将 VLM 扩展到视频中的开放词汇单标签动作分类。然而，先前的方法在整体视频理解方面存在不足，整体视频理解需要能够在开放词汇环境中同时识别多个动作和实体（例如视频中的对象）。我们将此问题表述为开放词汇多标签视频分类，并提出一种使预训练的 VLM（如 CLIP）适应解决此任务的方法。我们利用大型语言模型 (LLM) 为 VLM 提供有关类别标签的语义指导，通过两个关键贡献来提高其开放词汇性能。首先，我们提出了一种端到端可训练架构，该架构学习提示 LLM 生成 CLIP 文本编码器的软属性，使其能够识别新类别。其次，我们将时间建模模块集成到 CLIP 的视觉编码器中，以有效地对视频概念的时空动态进行建模，并提出一种新颖的正则化微调技术，以确保在视频域中强大的开放词汇分类性能。我们广泛的实验结果证明了我们的方法在多个基准数据集上的有效性。|
|**2024-07-12**|[DroneMOT: Drone-based Multi-Object Tracking Considering Detection Difficulties and Simultaneous Moving of Drones and Objects](http://arxiv.org/abs/2407.09051)|null|静态平台上的多目标跟踪（MOT），例如监控摄像头，已经取得了显著进展，各种范式提供了优异的性能。然而，传统的MOT方法在无人机等动态平台上的有效性会显著降低。这种下降归因于无人机场景下MOT的独特挑战：（1）图像平面中的物体通常较小、模糊且经常被遮挡，难以检测和识别；（2）无人机会从不同角度移动和观察物体，导致物体预测位置和特征嵌入的不可靠性。本文提出了DroneMOT，它首先提出了一种双域集成注意力（DIA）模块，该模块考虑了无人机的快速移动，以增强对小型、模糊和遮挡物体的无人机目标检测和特征嵌入。然后，引入了一种创新的运动驱动关联（MDA）方案，该方案考虑了无人机和物体的并发运动。在MDA中，提出了一种自适应特征同步（AFS）技术来更新从不同角度看到的物体特征。此外，采用了一种基于双运动的预测（DMP）方法来预测物体位置。最后，将改进的特征嵌入和预测位置相结合，以增强物体关联。在VisDrone2019-MOT和UAVDT数据集上的综合评估表明，DroneMOT在无人机MOT领域相较于现有技术提供了显著的性能提升。|
|**2024-07-12**|[CAMP: Continuous and Adaptive Learning Model in Pathology](http://arxiv.org/abs/2407.09030)|null|病理学中存在着大量的诊断任务。传统的计算病理学将这些任务表述为独立的图像分类问题并分别解决，导致计算效率低下和成本高昂。为了应对这些挑战，我们提出了一种通用的、统一的、通用的框架，称为病理学中的持续自适应学习模型 (CAMP)，用于病理图像分类。CAMP 是一种生成式、高效且自适应的分类模型，它可以利用特定于病理学的先验知识和学习特定于任务的知识，以最小的计算成本不断适应任何分类任务，而不会忘记现有任务的知识。我们在 22 个数据集上评估了 CAMP，包括 1,171,526 个图像块和 11,811 张病理切片，涵盖 17 个分类任务。CAMP 在各种数据集和任务的图像块和切片级别上均实现了最先进的分类性能，与传统的分类模型相比，计算时间减少了高达 94%，存储内存减少了 85%。我们的结果表明，CAMP 可以为病理图像分类带来根本性的变革，为全面数字化和计算机化的病理实践铺平道路。|
|**2024-07-11**|[Manipulating a Tetris-Inspired 3D Video Representation](http://arxiv.org/abs/2407.08885)|null|视频摘要是一种以保留视频活动的方式执行视频压缩的技术。这种技术在监控应用中特别有用。尽管它仍然是一个新兴的研究领域，但在过去的二十年中，已经提出了几种不同的方法，这些方法随着应用、优化类型、数据馈送的性质等而变化。这些算法所需的主要数据来自某种对象跟踪方法。在本文中，我们讨论了适用于不同应用的不同时空数据表示。我们还提出了视频摘要算法的正式定义。我们进一步讨论了该定义的假设和修改，以简化问题的版本。我们探索了应用打包算法来解决视频摘要问题。由于数据的性质是三维的，我们在讨论中考虑了 3D 打包问题。本文还对不同的视频摘要方法和打包问题进行了广泛的文献综述。最后，我们研究了该算法的不同应用，以及前面讨论的不同数据表示如何简化问题。我们还讨论了本次讨论后可以探索的未来研究方向。|
|**2024-07-11**|[Local Clustering for Lung Cancer Image Classification via Sparse Solution Technique](http://arxiv.org/abs/2407.08800)|null|在这项工作中，我们建议使用一种基于稀疏解技术的局部聚类方法来研究医学图像，特别是肺癌图像分类任务。我们将图像视为加权图中的顶点，将一对图像之间的相似性视为图中的边。可以假设同一个簇内的顶点共享相似的特征和属性，因此使得图聚类技术的应用对图像分类非常有用。最近，人们发现基于线性系统稀疏解的图聚类方法比传统的聚类方法（如谱聚类）能够更有效地识别聚类。我们建议使用两种新开发的基于线性系统稀疏解的局部聚类方法进行图像分类。此外，我们采用基于箱样条的紧框架小波方法对这些图像进行去噪，并帮助在聚类之前构建更好的邻接矩阵。我们的方法在图像分类方面表现非常有效。与其他最先进的方法相比，我们的方法效率更高，并且优于或等于它们。最后，我们将指出两种图像变形方法来构建更多的人工图像数据以增加标记图像的数量。|
|**2024-07-11**|[Approaching Outside: Scaling Unsupervised 3D Object Detection from 2D Scene](http://arxiv.org/abs/2407.08569)|**[link](https://github.com/ruiyang-061x/lise)**|无监督三维物体检测的目标是在没有明确监督信号的情况下，准确地检测非结构化环境中的物体。考虑到激光雷达点云的稀疏性，这项任务在检测远处或小型物体时，由于其固有的稀疏性和有限的空间分辨率，往往会导致性能下降。在本文中，我们率先尝试将激光雷达数据与二维图像相结合用于无监督三维物体检测，并介绍了一种称为LiDAR-2D自适应学习（LiSe）的新方法。我们认为，RGB图像可以作为激光雷达数据的有价值补充，提供精确的二维定位线索，尤其是在某些物体只有少量激光雷达点的情况下。考虑到两种模式的独特性，我们的框架设计了一个自适应学习流程，该流程结合了自适应采样和弱模型聚合策略。自适应采样策略在训练期间动态调整伪标签的分布，以对抗模型过度拟合易于检测到的样本（例如附近和大型物体）的趋势。这样做可以确保在不同的物体尺度和距离上实现平衡的学习轨迹。弱模型聚合组件整合了在不同伪标签分布下训练的模型的优势，最终形成一个强大而鲁棒的最终模型。实验评估验证了我们提出的LiSe方法的有效性，与现有技术相比，在nuScenes数据集上实现了+7.1% AP $_{BEV}$和+3.4% AP$_{3D}$的显著改进，在Lyft数据集上实现了+8.3% AP$_{BEV}$和+7.4% AP$_{3D}$ 的显著改进。|
|**2024-07-11**|[Projecting Points to Axes: Oriented Object Detection via Point-Axis Representation](http://arxiv.org/abs/2407.08489)|null|本文介绍了一种用于定向目标检测的点轴表示法，强调了其灵活性和几何直观性，它包含两个关键组件：点和轴。1）点描绘了目标的空间范围和轮廓，提供了详细的形状描述。2）轴定义了目标的主要方向，提供了对精确检测至关重要的方向线索。点轴表示法将位置和旋转解耦，解决了传统基于边界框的方法中常见的损失不连续性问题。为了在不引入额外标注的情况下进行有效优化，我们提出了最大投影损失来监督点集学习，以及交叉轴损失来进行鲁棒的轴表示学习。此外，利用这种表示法，我们提出了 Oriented DETR 模型，将 DETR 框架无缝集成，用于精确的点轴预测和端到端检测。实验结果表明，在定向目标检测任务中，该方法的性能有了显著提高。|
|**2024-07-11**|[Global Spatial-Temporal Information-based Residual ConvLSTM for Video Space-Time Super-Resolution](http://arxiv.org/abs/2407.08466)|null|时空视频超分辨率技术可以将低帧率、低分辨率的视频转换为高帧率、高分辨率的视频，从而增强视觉体验并促进更有效的信息传播。我们提出了一种用于时空视频超分辨率的卷积神经网络 (CNN)，称为 GIRNet。为了生成高度准确的特征并提高性能，所提出的网络集成了具有可变形卷积的特征级时间插值模块和基于全局时空信息的残差卷积长短期记忆 (convLSTM) 模块。在特征级时间插值模块中，我们利用可变形卷积来适应不同场景位置的对象的变形和尺度变化。与传统的卷积相比，这为从运动物体中提取特征提供了一种更有效的解决方案。我们的网络有效地利用前向和后向特征信息来确定帧间偏移，从而直接生成插值帧特征。在基于全局时空信息的残差 convLSTM 模块中，第一个 convLSTM 用于从输入特征中导出全局时空信息，第二个 convLSTM 使用先前计算的全局时空信息特征作为其初始单元状态。第二个 convLSTM 采用残差连接来保留空间信息，从而增强输出特征。在 Vimeo90K 数据集上的实验表明，所提出的方法在峰值信噪比（分别比 STARnet、TMNet 和 3DAttGAN 高 1.45 dB、1.14 dB 和 0.02 dB）、结构相似性指数（分别比 STARnet、TMNet 和 3DAttGAN 高 0.027、0.023 和 0.006）和视觉效果方面均优于现有技术。|
|**2024-07-11**|[Semi-Supervised Object Detection: A Survey on Progress from CNN to Transformer](http://arxiv.org/abs/2407.08460)|null|半监督学习技术的显著进步促使研究人员探索其在计算机视觉领域目标检测任务中的潜力。半监督目标检测 (SSOD) 利用少量标记数据集和大量未标记数据集的组合。这种方法有效地减少了对大型标记数据集的依赖，而获取这些数据集通常既昂贵又耗时。最初，SSOD 模型在有效利用未标记数据和管理未标记数据生成的伪标签中的噪声方面遇到了挑战。然而，最近的许多进展已经解决了这些问题，从而大大提高了 SSOD 的性能。本文全面回顾了从卷积神经网络 (CNN) 到 Transformer 的 27 项 SSOD 方法的最新发展。我们深入研究了半监督学习的核心组件及其与目标检测框架的集成，涵盖了数据增强技术、伪标签策略、一致性正则化和对抗训练方法。此外，我们对各种 SSOD 模型进行了比较分析，评估了它们的性能和架构差异。我们的目标是激发进一步的研究兴趣，以克服现有挑战并探索半监督学习在目标检测中的新方向。|
|**2024-07-11**|[PowerYOLO: Mixed Precision Model for Hardware Efficient Object Detection with Event Data](http://arxiv.org/abs/2407.08272)|null|车载解决方案中目标检测系统的性能必须尽可能高，响应时间要短，并且由于通常采用电池供电，因此能耗要低。因此，在设计此类解决方案时，我们面临着嵌入式视觉系统特有的挑战：将内存和计算复杂度高的算法适配到小型低功耗设备中的问题。在本文中，我们提出了 PowerYOLO——一种混合精度解决方案，它针对此类应用的三个基本要素。首先，我们提出了一种基于动态视觉传感器 (DVS) 的系统，这是一种新型传感器，具有低功耗要求，并且在光照条件变化的情况下也能很好地工作。正是这些特性使得事件相机在某些应用中可能优于帧相机。其次，为了确保高精度以及低内存和计算复杂度，我们建议对 YOLO 检测器的卷积权重使用 4 位宽度的二进制幂 (PoT) 量化，并对所有其他参数进行线性量化。最后，我们采用 PoT 方案并用位移代替乘法，通过一种特殊的卷积-批量归一化融合方案来提高此类解决方案的硬件加速效率。与标准方法相比，使用特定传感器、PoT 量化和特殊的批量归一化融合方案，形成了一个独特的系统，该系统将内存复杂度降低了近 8 倍，并大大简化了计算。这个高效的系统在 GEN1 DVS 数据集上实现了 0.301 的高 mAP 精度，标志着这种压缩模型的新水平。||
|**2024-07-11**|[Wind Power Assessment based on Super-Resolution and Downscaling -- A Comparison of Deep Learning Methods](http://arxiv.org/abs/2407.08259)|null|风力涡轮机的有效放置依赖于准确的当地风速预测。气候预测为了解长期风速条件提供了宝贵的见解，但其空间数据分辨率通常不足以进行精确的风力发电预测。深度学习方法，特别是为图像超分辨率开发的模型，为通过提高气候模型的空间分辨率来弥合这种尺度差距提供了一种有前景的解决方案。在本文中，我们比较了各种深度学习模型在两个不同任务上的性能：超分辨率（我们将人工粗化的 ERA5 数据映射到其原始分辨率）和降尺度（我们将原生 ERA5 映射到高分辨率 COSMO-REA6 数据）。我们根据模型在下游应用中预测长期风力的表现对其进行评估，强调空间风速分辨率对风力估计的影响。我们的研究结果强调了将模型和评估指标与其特定下游应用保持一致的重要性。我们证明，扩散模型通过更好地保持风速的分布和物理特性，在估算风能潜力方面优于其他模型。||
|**2024-07-11**|[Knowledge distillation to effectively attain both region-of-interest and global semantics from an image where multiple objects appear](http://arxiv.org/abs/2407.08257)|**[link](https://github.com/seonwhee-genome/rvernet)**|基于卷积神经网络（CNN）和Transformer的模型一直在稳步改进，并已应用于各种计算机视觉下游任务。然而，在目标检测任务中，准确地定位和分类图像中几乎无限种类的食物仍然具有挑战性。为了解决这些问题，我们首先使用Segment Anything模型（SAM）将食物分割为感兴趣区域（ROI），并将ROI以外的区域遮盖为黑色像素。这个过程将问题简化为单一分类，其标注和训练比目标检测简单得多。我们将只保留ROI的图像作为输入，对各种现成的模型进行微调，这些模型编码了它们自己的归纳偏差。其中，数据高效图像Transformer（DeiT）具有最佳的分类性能。然而，当食物的形状和纹理相似时，仅ROI图像的上下文特征不足以进行准确分类。因此，我们引入了一种新型的组合架构RveRNet，它由ROI、额外ROI和集成模块组成，使其能够同时考虑ROI和全局上下文。在对模糊食物图像进行分类时，RveRNet的F1分数比其他单个模型高10%。如果RveRNet的模块是具有CNN知识蒸馏的DeiT，则性能最佳。我们研究了如何使架构对排列和易位引起的输入噪声具有鲁棒性。结果表明，CNN教师的知识有多少可以提炼到DeiT和DeiT的先天优势之间存在权衡。代码公开于：https://github.com/Seonwhee-Genome/RveRNet。||
|**2024-07-11**|[GraphMamba: An Efficient Graph Structure Learning Vision Mamba for Hyperspectral Image Classification](http://arxiv.org/abs/2407.08255)|**[link](https://github.com/ahappyyang/GraphMamba)**|高光谱图像分类中，有效的光谱序列和地理空间信息的提取一直是热门话题。在光谱序列特征捕获方面，RNN和Transformer由于其远程特征捕获能力，已成为主流分类框架。在空间信息聚合方面，CNN通过增强感受野来尽可能多地保留完整的空间信息。然而，光谱特征捕获架构的计算效率较低，并且CNN缺乏感知空间上下文信息的灵活性。为了解决这些问题，本文提出了GraphMamba——一种高效的图结构学习视觉Mamba分类框架，该框架充分考虑了HSI特性，以实现深层次的空间-光谱信息挖掘。具体来说，我们提出了一种新的高光谱视觉GraphMamba处理范式（HVGM），该范式通过构建空间-光谱立方体来保留空间-光谱特征，并利用线性光谱编码来增强后续任务的可操作性。GraphMamba的核心组件包括用于提高计算效率的HyperMamba模块和用于自适应空间上下文感知的SpectralGCN模块。HyperMamba通过采用全局掩码（GM）来减轻杂波干扰，并引入了并行训练推理架构来缓解计算瓶颈。SpatialGCN结合了加权多跳聚合（WMA）空间编码，以关注高度相关的空间结构特征，从而灵活地聚合上下文信息，同时减轻空间噪声干扰。在三个不同尺度的真实HSI数据集上进行了广泛的实验，与最先进的分类框架相比，GraphMamba取得了最佳性能。||
|**2024-07-11**|[DMM: Disparity-guided Multispectral Mamba for Oriented Object Detection in Remote Sensing](http://arxiv.org/abs/2407.08132)|null|多光谱目标检测面临着模态间和模态内差异的挑战。最近的研究经常依赖于基于Transformer的模型来解决这些问题并实现跨模态融合检测。然而，Transformer的平方计算复杂度限制了它们的性能。受Mamba在长序列任务中效率和较低复杂度的启发，我们提出了视差引导的多光谱Mamba（DMM），这是一个多光谱目标检测框架，由视差引导的跨模态融合Mamba（DCFM）模块、多尺度目标感知注意力（MTA）模块和目标先验感知（TPA）辅助任务组成。DCFM模块利用模态之间的视差信息自适应地融合来自RGB和红外图像的特征，从而减轻模态间冲突。MTA模块旨在通过关注RGB模态内的相关目标区域来增强特征表示，解决模态内变化问题。TPA辅助任务利用单模态标签来指导MTA模块的优化，确保其关注目标及其局部上下文。在DroneVehicle和VEDAI数据集上的大量实验表明了我们方法的有效性，它在保持计算效率的同时优于最先进的方法。代码将在https://github.com/Another-0/DMM上提供。||
|**2024-07-10**|[MambaVision: A Hybrid Mamba-Transformer Vision Backbone](http://arxiv.org/abs/2407.08083)|**[link](https://github.com/nvlabs/mambavision)**|我们提出了一种名为 MambaVision 的新型混合 Mamba-Transformer 骨干网络，该网络专为视觉应用而设计。我们的核心贡献包括重新设计 Mamba 公式，以增强其对视觉特征进行高效建模的能力。此外，我们对将视觉Transformer（ViT）与 Mamba 相集成的可行性进行了全面的消融研究。我们的结果表明，在 Mamba 架构的最后几层配备多个自注意力块可以极大地提高模型捕获远程空间依赖关系的能力。基于我们的发现，我们引入了一系列具有层次结构的 MambaVision 模型，以满足各种设计标准。对于 ImageNet-1K 数据集上的图像分类任务，MambaVision 模型变体在 Top-1 准确率和图像吞吐量方面均达到了新的最先进水平 (SOTA)。在下游任务（例如 MS COCO 和 ADE20K 数据集上的目标检测、实例分割和语义分割）中，MambaVision 的性能优于规模相当的骨干网络，并展现出更佳的性能。代码：https://github.com/NVlabs/MambaVision。||
|**2024-07-09**|[CoLA: Conditional Dropout and Language-driven Robust Dual-modal Salient Object Detection](http://arxiv.org/abs/2407.06780)|**[link](https://github.com/ssecv/CoLA)**|深度/热信息有利于利用传统RGB图像检测显著性目标。然而，在双模态显著性目标检测（SOD）模型中，针对噪声输入和模态缺失的鲁棒性至关重要，但很少被研究。为了解决这个问题，我们引入了条件性丢弃和语言驱动（CoLA）框架，该框架包含两个核心组件。1）语言驱动质量评估（LQA）：利用带有提示学习器的预训练视觉语言模型，LQA在不需要额外质量标注的情况下重新校准图像贡献。这种方法有效地减轻了噪声输入的影响。2）条件性丢弃（CD）：一种学习方法，用于增强模型在模态缺失情况下的适应性，同时保持其在完整模态下的性能。CD作为一种插件式训练方案，将模态缺失视为条件，增强了各种双模态SOD模型的整体鲁棒性。大量实验表明，所提出的方法在模态完整和模态缺失两种情况下均优于最先进的双模态SOD模型。我们将开源代码。||
|**2024-07-09**|[Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting Region Captions](http://arxiv.org/abs/2407.06723)|null|人类使用组合性来描述复杂场景，使用带有链接和关系的简单文本描述来丰富描述。虽然视觉语言研究的目标是开发具有组合理解能力的模型，但这还没有反映在现有的数据集中，这些数据集在很大程度上仍然使用纯文本描述图像。在这项工作中，我们提出了一种新的标注策略，即基于图的字幕（GBC），它使用带有各种类型节点的标记图结构来描述图像。GBC 中的节点是通过以下方式创建的：首先，使用对象检测和密集字幕工具递归嵌套以发现和描述实体节点，然后在第二阶段通过使用新型节点、组合和实体之间的关系来突出显示，将它们链接在一起。由于所有 GBC 节点都包含纯文本描述，因此 GBC 保留了自然语言的灵活性，但也可以在其边缘编码分层信息。我们证明了 GBC 可以使用现成的多模态 LLM 和开放词汇检测模型自动生成，方法是构建一个新的数据集 GBC10M，为 CC12M 数据集中的大约 10M 张图像收集 GBC 标注。我们使用 GBC10M 来展示 GBC 发现的大量节点字幕，使用 CLIP 训练进行测量。我们表明，与其他数据集格式相比，使用 GBC 节点的注释（特别是存储在组合和关系节点中的注释）可以显着提高下游模型的性能。为了进一步探索 GBC 提供的机会，我们还提出了一种新的注意力机制，可以利用整个 GBC 图，并获得了令人鼓舞的实验结果，表明了合并图结构的额外好处。我们的数据集发布在 \url{https://huggingface.co/graph-based-captions}。||
|**2024-07-09**|[CTRL-F: Pairing Convolution with Transformer for Image Classification via Multi-Level Feature Cross-Attention and Representation Learning Fusion](http://arxiv.org/abs/2407.06673)|null|Transformer因其强大的容量和全局处理能力，在计算机视觉领域受到越来越多的关注。然而，Transformer是数据密集型的，与卷积神经网络（ConvNets）相比，其泛化能力受到限制，特别是在数据有限的情况下进行训练时，因为它们缺乏ConvNets中存在的内置空间归纳偏差。在本文中，我们致力于将卷积和Transformer的优势结合起来，以完成图像分类任务。为此，我们提出了一种新颖的轻量级混合网络，该网络通过表示学习融合和多级特征交叉注意（CTRL-F）将卷积与Transformer配对。我们的网络包括一个卷积分支和一个名为多级特征交叉注意（MFCA）的新型Transformer模块。MFCA模块对从不同卷积阶段获得的多级特征表示进行操作。它通过两个独立的Transformer分支处理从这些多级特征表示中提取的小块标记和大块标记，其中两个分支通过交叉注意机制进行通信和交换知识。我们使用称为自适应知识融合（AKF）和协作知识融合（CKF）的新型表示融合技术，将从卷积路径获得的局部响应与从MFCA模块获得的全局响应融合在一起。实验表明，我们的CTRL-F变体无论是在大数据上从头开始训练，还是在低数据情况下训练，都能获得最先进的性能。例如，CTRL-F在Oxford-102 Flowers和PlantVillage数据集上从头开始训练时，分别达到了82.24%和99.91%的top-1准确率，超过了最先进的模型，这展示了我们的模型在图像分类任务上的鲁棒性。代码位于：https://github.com/hosamsherif/CTRL-F||
|**2024-07-09**|[NoisyAG-News: A Benchmark for Addressing Instance-Dependent Noise in Text Classification](http://arxiv.org/abs/2407.06579)|null|现有的噪声标签学习研究主要集中在合成标签噪声上。虽然合成噪声具有明确的结构特性，但它往往不能准确地复制现实世界的噪声模式。近年来，人们一直在努力构建用于图像分类的、可泛化和可控的实例相关噪声数据集，这极大地促进了该领域鲁棒噪声学习的发展。然而，关于文本分类中噪声标签学习的研究仍然很少。为了更好地理解现实世界中文本分类环境中的标签噪声，我们通过人工标注构建了基准数据集NoisyAG-News。首先，我们分析了标注数据，以收集关于现实世界噪声的观察结果。我们定性和定量地证明了现实世界的噪声标签遵循实例相关的模式。随后，我们使用预训练语言模型和噪声处理技术，对NoisyAG-News及其相应的合成噪声数据集进行了全面的学习实验。我们的研究结果表明，虽然预训练模型对合成噪声具有鲁棒性，但它们在实例相关噪声面前却表现不佳，不同混淆程度的样本在训练和测试过程中表现出不一致的性能。这些现实世界的噪声模式提出了新的、重大的挑战，促使人们重新评估噪声标签处理方法。我们希望NoisyAG-News能够促进未来噪声标签学习解决方案的开发和评估。||
|**2024-07-09**|[UnmixingSR: Material-aware Network with Unsupervised Unmixing as Auxiliary Task for Hyperspectral Image Super-resolution](http://arxiv.org/abs/2407.06525)|null|基于深度学习 (DL) 的高光谱图像 (HIS) 超分辨率 (SR) 方法取得了显著成果，并在工业界和学术界引起了广泛关注。然而，大多数现有方法都在探索和学习低分辨率 (LR) 和高分辨率 (HR) HSI 之间的映射关系，导致在解决不适定 SR 问题时增加了不可靠性和不合理性。有趣的是，我们发现 LR 成像与混合像元现象相似。传感器阵列中的单个光电探测器接收由多种类别反射的反射信号，导致低空间分辨率和混合像元问题。受此观察的启发，本文提出了一种名为 UnmixingSR 的、组件感知的 HSI SR 网络，其中无监督 HU 作为辅助任务用于感知 HSI 的材料成分。我们将 HU 视为辅助任务，并通过探索 LR 和 HR  丰度之间的约束将其纳入 HSI SR 过程。我们没有仅仅学习 LR 和 HR HSI 之间的映射关系，而是利用 LR 丰度和 HR 丰度之间的联系来提高我们方法在解决 SR 问题时的稳定性。此外，所提出的解混过程可以作为即插即用的辅助任务嵌入到现有的深度 SR 模型中。高光谱实验结果表明，将解混过程作为辅助任务纳入 SR 问题是可行且合理的，并取得了优异的性能。代码可在以下网址获得||
|**2024-07-08**|[Enhancing super-resolution ultrasound localisation through multi-frame deconvolution exploiting spatiotemporal coherence](http://arxiv.org/abs/2407.06373)|null|通过微泡 (MB) 定位和跟踪实现的超分辨率超声成像，也称为超声定位显微镜，可以在动物和人体内对微血管进行非侵入性亚衍射分辨率成像。从获取的对比增强超声 (CEUS) 图像中定位的 MB 数量和定位精度直接影响最终的超分辨率微血管图像的质量。然而，CEUS 图像中不可忽略的噪声会使 MB 定位变得困难。为了提高 MB 定位性能，我们提出了一种多帧反卷积 (MF-Decon) 框架，该框架可以利用 CEUS 数据中固有的时空一致性，并基于总变差 (TV) 和去噪正则化 (RED) 设计新的空间和时间正则化器。基于 MF-Decon 框架，我们引入了两种新方法：具有空间和时间 TV 的 MF-Decon (MF-Decon+3DTV) 和具有空间 RED 和时间 TV 的 MF-Decon (MF-Decon+RED+TV)。计算机模拟结果表明，我们的方法在所有评估指标（包括精度、召回率、 $F_1$ 分数、平均定位误差和标准定位误差）方面均优于两种广泛使用的反卷积或归一化互相关方法。特别是，我们的方法将 MB 定位精度提高了 39%，并将召回率提高了 12%。使用我们的方法在公开可用的体内大鼠大脑数据集上生成的超分辨率微血管图显示出更少的噪声、更好的对比度、更高的分辨率和更多的血管结构。||
|**2024-07-08**|[GeoWATCH for Detecting Heavy Construction in Heterogeneous Time Series of Satellite Images](http://arxiv.org/abs/2407.06337)|null|从多传感器学习是一项挑战，因为存在时空错位以及分辨率和捕获光谱的差异。为此，我们推出了 GeoWATCH，这是一个灵活的框架，用于在来自多个传感器平台的长序列卫星图像上训练模型，该框架旨在处理图像分类、活动识别、物体检测或物体跟踪任务。我们的系统包括一种基于子图同构的新型部分权重加载机制，允许在多个训练周期内持续训练和修改网络。这使我们能够在很长一段时间内训练一系列模型，我们观察到，在我们调整配置的同时保持核心骨干的情况下，性能得到了提高。||
|**2024-07-08**|[Active Label Refinement for Robust Training of Imbalanced Medical Image Classification Tasks in the Presence of High Label Noise](http://arxiv.org/abs/2407.05973)|null|基于监督深度学习的医学图像分类的鲁棒性会被标签噪声显著削弱。为了提高存在噪声标签时的分类性能，目前已经提出了几种方法，但它们面临着一些挑战：1）难以处理类别不平衡的数据集，导致少数类样本经常被误认为是噪声样本；2）仅仅关注于使用噪声数据集最大化性能，而没有结合专家参与主动清理噪声标签。为了应对这些挑战，我们提出了一种结合了噪声标签学习（LNL）和主动学习的两阶段方法。这种方法不仅提高了存在噪声标签时医学图像分类的鲁棒性，而且在有限的标注预算下，通过重新标注重要的错误标签，迭代地提高了数据集的质量。此外，我们在LNL阶段引入了一种新的梯度方差方法，通过对代表性不足的样本进行采样，补充了基于损失的样本选择方法。通过使用两个不平衡的噪声医学分类数据集，我们证明了我们提出的技术在处理类别不平衡方面优于以往的方法，因为它不会将来自少数类的干净样本错误地识别为大部分是噪声样本。||
|**2024-07-08**|[Deform-Mamba Network for MRI Super-Resolution](http://arxiv.org/abs/2407.05969)|null|在本文中，我们提出了一种新的MR图像超分辨率架构，称为Deform-Mamba。不同于传统的CNN或基于Transformer的超分辨率方法，这些方法会遇到与局部感受野或高计算成本相关的挑战，我们的方法旨在有效地探索图像的局部和全局信息。具体来说，我们开发了一个Deform-Mamba编码器，它由两个分支组成：调制变形块和视觉Mamba块。我们还在瓶颈层设计了一个多视图上下文模块，以探索多视图上下文内容。由于编码器提取的特征包括内容自适应的局部信息和高效的全局信息，视觉Mamba解码器最终生成高质量的MR图像。此外，我们引入了一种对比边缘损失来促进边缘和对比度相关内容的重建。在IXI和fastMRI数据集上的定量和定性实验结果表明，我们的方法取得了具有竞争力的性能。||
|**2024-07-08**|[Multi-clue Consistency Learning to Bridge Gaps Between General and Oriented Object in Semi-supervised Detection](http://arxiv.org/abs/2407.05909)|**[link](https://github.com/facias914/sood-mcl)**|虽然现有的半监督目标检测（SSOD）方法在一般场景中表现良好，但它们在处理航空图像中的定向目标时遇到了挑战。我们通过实验发现，在半监督学习中，一般目标检测和定向目标检测之间存在三个差距：1）采样不一致：在从标记数据中选择正标签时，常用的中心采样不适用于长宽比较大的定向目标。2）分配不一致：平衡定向伪框的精度和定位质量带来了更大的挑战，这在从未标记数据中选择正标签时引入了更多噪声。3）置信度不一致：在考虑定向目标时，预测的分类和定位质量之间存在更多不匹配，影响了伪标签的选择。因此，我们提出了一个多线索一致性学习（MCL）框架，以弥合半监督检测中一般目标和定向目标之间的差距。具体来说，考虑到旋转目标的各种形状，我们专门设计了高斯中心分配来从标记数据中选择像素级正标签。然后，我们引入了尺度感知标签分配来选择像素级伪标签而不是不可靠的伪框，这是一种适用于各种尺度目标的分而治之策略。最后采用一致置信度软标签，通过保持预测结果的一致性来进一步提升检测器。在DOTA-v1.5和DOTA-v1.0基准数据集上的综合实验表明，我们提出的MCL方法在半监督定向目标检测任务中可以达到最先进的性能。||
|**2024-07-05**|[SH17: A Dataset for Human Safety and Personal Protective Equipment Detection in Manufacturing Industry](http://arxiv.org/abs/2407.04590)|**[link](https://github.com/ahmadmughees/sh17dataset)**|工作场所事故持续对人类安全构成重大风险，特别是在建筑和制造等行业，因此，有效的个人防护装备 (PPE) 合规性变得越来越重要。我们的研究重点是开发基于目标检测 (OD) 和卷积神经网络 (CNN) 的非侵入性技术，以检测和验证各种类型 PPE 的正确使用，例如安全帽、安全眼镜、口罩和防护服。本研究提出了 SH17 数据集，其中包含从不同工业环境中收集的 8,099 张带注释的图像，这些图像包含 75,994 个 17 个类别的实例，用于训练和验证 OD 模型。我们已经训练了最先进的 OD 模型进行基准测试，初步结果表明，You Only Look Once (YOLO)v9-e 模型变体的 PPE 检测精度超过 70.9%，达到了令人满意的水平。跨域数据集上的模型验证性能表明，集成这些技术可以显着改进安全管理系统，为努力满足人类安全法规和保护员工的行业提供可扩展且高效的解决方案。该数据集可在 https://github.com/ahmadmughees/sh17dataset 获取。||
|**2024-07-05**|[Multi-Branch Auxiliary Fusion YOLO with Re-parameterization Heterogeneous Convolutional for accurate object detection](http://arxiv.org/abs/2407.04381)|**[link](https://github.com/yang-0201/MAF-YOLO)**|由于多尺度特征融合的有效性，路径聚合特征金字塔网络 (PAFPN) 广泛应用于 YOLO 检测器中。然而，它无法同时高效且自适应地融合高级语义信息和低级空间信息。在本文中，我们提出了一个名为 MAF-YOLO 的新模型，它是一个具有名为多分支辅助特征金字塔网络 (MAFPN) 的新型目标检测框架。在 MAFPN 中，浅层辅助融合 (SAF) 模块旨在将主干网络的输出与颈部结合起来，保留最佳级别的浅层信息，以便于后续学习。同时，深度嵌入颈部的高级辅助融合 (AAF) 模块将更多样化的梯度信息传递到输出层。此外，我们提出的重新参数化的异构高效层聚合网络 (RepHELAN) 模块确保了整体模型架构和卷积设计都采用了异构大卷积核。因此，这保证了保留与小目标相关的信息，同时实现了多尺度感受野。最后，以 MAF-YOLO 的纳米版本为例，它在 COCO 数据集上仅用 3.76M 可学习参数和 10.51G FLOPs 即可达到 42.4% 的 AP，性能优于 YOLOv8n 约 5.1%。本研究的源代码可在以下网址获取：https://github.com/yang-0201/MAF-YOLO。||
|**2024-07-05**|[FeatureSORT: Essential Features for Effective Tracking](http://arxiv.org/abs/2407.04249)|null|在这项工作中，我们介绍了一种新颖的跟踪器，该跟踪器专为在线多目标跟踪而设计，其重点在于简单而有效。我们提供了多个特征模块，每个模块代表一个特定的外观信息。通过整合不同的外观特征，包括服装颜色、款式和目标方向，以及用于鲁棒嵌入提取的 ReID 网络，我们的跟踪器显著提高了在线跟踪精度。此外，我们建议结合更强大的检测器，并提供先进的后处理方法，进一步提升跟踪器的性能。在实时操作期间，我们建立测量来跟踪关联距离函数，其中包括 IoU、方向、颜色、样式和 ReID 特征相似性信息，其中每个指标分别计算。通过我们设计的特征相关距离函数，可以跟踪物体更长时间的遮挡，同时保持相对较低的身份切换次数。广泛的实验评估表明，跟踪精度和可靠性显着提高，身份切换减少和遮挡处理增强证明了这一点。这些进步不仅有助于推动目标跟踪领域的最新技术水平，而且为未来需要高精度和可靠性的研究和实际应用开辟了新途径。||
|**2024-07-05**|[AnySR: Realizing Image Super-Resolution as Any-Scale, Any-Resource](http://arxiv.org/abs/2407.04241)|**[link](https://github.com/crispyfeso4/anysr)**|为了提高单图像超分辨率 (SISR) 应用的效率和可扩展性，我们引入了 AnySR，将现有的任意尺度 SR 方法重建为任意尺度、任意资源的实现。与使用相同计算成本解决各种规模的 SR 任务的现成方法相比，我们的 AnySR 创新在于：1) 将任意尺度任务构建为任意资源实现，在不增加额外参数的情况下减少了较小规模的资源需求；2) 以特征交织的方式增强任意尺度性能，将尺度对以规则的间隔插入特征中，并确保正确的特征/尺度处理。我们通过重建大多数现有的任意尺度 SISR 方法并在五个流行的 SISR 测试数据集上进行验证，充分证明了 AnySR 的有效性。结果表明，我们的 AnySR 以计算效率更高的方式实现了 SISR 任务，并且性能与现有的任意尺度 SISR 方法相当。我们首次实现了 SISR 任务，不仅在文献中是任意尺度的，而且是任意资源的。代码可在 https://github.com/CrispyFeSo4/AnySR 获取。||
|**2024-07-05**|[AMD: Automatic Multi-step Distillation of Large-scale Vision Models](http://arxiv.org/abs/2407.04208)|null|基于Transformer的架构因其优越的性能已成为各种视觉任务的标准模型。随着模型规模的不断扩大，模型蒸馏在各种实际应用中变得极其重要，特别是在受计算资源限制的设备上。然而，当教师模型和学生模型之间存在较大的容量差距时，例如10倍的压缩率，现有的知识蒸馏方法效果会下降。在本文中，我们提出了一种名为自动多步蒸馏（AMD）的新方法，用于大规模视觉模型压缩。具体来说，我们的蒸馏过程分为多个步骤。首先，对教师模型进行蒸馏，形成一个中间的助教模型，然后进一步蒸馏到学生模型。我们引入了一个高效且有效的优化框架，来自动识别能够使学生模型性能最大化的最佳助教模型。我们在多个图像分类数据集上进行了广泛的实验，包括CIFAR-10、CIFAR-100和ImageNet。结果一致表明，我们的方法优于几种已建立的基线方法，为未来大规模视觉模型的知识蒸馏方法铺平了道路。||
|**2024-07-04**|[Attention Normalization Impacts Cardinality Generalization in Slot Attention](http://arxiv.org/abs/2407.04170)|null|以对象为中心的场景分解对于计算机视觉和机器人等领域的下游任务非常重要。最近提出的槽位注意力模块已经被一些衍生作品用于图像分割和视频目标跟踪，它是一种深度学习组件，可以在输入图像上执行无监督的以对象为中心的场景分解。它基于一种注意力架构，其中潜在的槽位向量（包含对象的压缩信息）关注来自输入图像的局部感知特征。在本文中，我们发现对注意力架构中聚合值进行归一化的设计决策对槽位注意力泛化到训练期间所见到的更多槽位和对象的能力有相当大的影响。我们认为，原始的槽位注意力归一化方案丢弃了像素先前分配给槽位的概率信息，这损害了其泛化能力。基于这些发现，我们提出并研究了替代的归一化方法，这些方法可以提高槽位注意力对不同槽位和对象数量的泛化能力，从而提高无监督图像分割任务的性能。||
|**2024-07-04**|[Detect Closer Surfaces that can be Seen: New Modeling and Evaluation in Cross-domain 3D Object Detection](http://arxiv.org/abs/2407.04061)|null|目前，域适应技术在自动驾驶三维目标检测领域的性能尚未达到理想水平，这主要是由于车辆尺寸的显著差异以及跨域应用时运行环境的不同。这些因素共同阻碍了从特定数据集中学习到的知识的有效迁移和应用。由于现有的评估指标最初是通过计算预测边界框和真实边界框之间的二维或三维重叠来设计用于单个域上的评估，因此它们经常会遇到由数据集之间的大小差异引起的过拟合问题。这引发了一个与评估三维目标检测模型跨域性能相关的基本问题：我们是否真的需要模型在跨域应用后保持其原始三维边界框的出色性能？从实际应用的角度来看，我们的主要关注点之一实际上是防止车辆与其他障碍物发生碰撞，特别是在跨域场景中，正确预测车辆尺寸要困难得多。换句话说，只要模型能够准确识别出距离自动驾驶车辆最近的表面，就足以有效避开障碍物。在本文中，我们提出了两个指标来衡量三维目标检测模型检测自动驾驶车辆传感器附近表面的能力，这可以用来更全面、更合理地评估其跨域性能。此外，我们提出了一个名为EdgeHead的优化头，用于引导模型更加关注可学习的较近表面，这可以极大地提高现有模型在我们的新指标下以及在原始BEV/3D指标下的跨域性能。||
|**2024-07-04**|[TrackPGD: A White-box Attack using Binary Masks against Robust Transformer Trackers](http://arxiv.org/abs/2407.03946)|null|使用Transformer骨干网络的目标跟踪器在视觉目标跟踪数据集上取得了强大的性能。然而，这些跟踪器的对抗鲁棒性在文献中尚未得到很好的研究。由于骨干网络的差异，为目标跟踪提出的对抗性白盒攻击不能迁移到所有类型的跟踪器。例如，像MixFormerM这样的Transformer跟踪器在黑盒攻击后仍然运行良好，特别是在预测目标二进制掩码方面。我们提出了一种名为TrackPGD的新型白盒攻击，它依靠预测的目标二进制掩码来攻击鲁棒的Transformer跟踪器。这种新攻击通过采用著名的SegPGD分割攻击来关注标注掩码，从而能够成功地对依赖Transformer骨干网络的跟踪器进行白盒攻击。实验结果表明，TrackPGD能够有效攻击基于Transformer的跟踪器，例如MixFormerM、OSTrackSTS和TransT-SEG，并在多个跟踪数据集上取得了成功。||
|**2024-07-04**|[DocXplain: A Novel Model-Agnostic Explainability Method for Document Image Classification](http://arxiv.org/abs/2407.03830)|null|深度学习（DL）彻底改变了文档图像分析领域，在各种任务中展现出超越人类的表现。然而，深度学习模型固有的黑盒性质仍然是其在行业中安全稳健部署的重大挑战。遗憾的是，尽管近年来大量研究致力于开发基于深度学习的文档分析系统，但解决其透明性方面的研究却相对较少。在本文中，我们旨在通过介绍 DocXplain 来弥合这一研究差距，这是一种新颖的模型无关的可解释性方法，专门设计用于为文档图像分类任务生成高可解释性的特征归因图。具体来说，我们的方法涉及将文档的前景和背景特征独立地分割成不同的文档元素，然后消融这些元素以分配特征重要性。我们在文档图像分类的背景下广泛评估了我们提出的方法，利用 4 种不同的评估指标、2 个广泛认可的文档基准数据集和 10 个最先进的文档图像分类模型。通过对 9 种现有的最先进的归因方法进行全面的定量和定性分析，我们证明了我们的方法在保真度和可解释性方面的优越性。据作者所知，这项工作提出了第一个专门针对文档图像量身定制的模型无关的基于归因的可解释性方法。我们预计我们的工作将极大地促进文档图像分类模型的透明度、公平性和鲁棒性研究的进展。||
|**2024-07-04**|[M^3:Manipulation Mask Manufacturer for Arbitrary-Scale Super-Resolution Mask](http://arxiv.org/abs/2407.03695)|null|在图像篡改定位（IML）领域，现有数据集数量少、质量差一直是主要问题。包含各种篡改类型的数据集将极大地提高IML模型的准确性。互联网上的图像（例如百度贴吧PS吧的图像）使用各种技术进行篡改，利用这些图像创建数据集将显著丰富我们数据中的篡改类型。然而，互联网上的图像存在分辨率和清晰度问题，通过简单地从原始图像中减去篡改图像获得的掩码包含各种噪声。这些噪声很难去除，导致掩码无法用于IML模型。受变化检测领域的启发，我们将原始图像和篡改图像视为同一图像随时间的变化，并将数据生成任务视为变化检测任务。然而，由于图像之间的清晰度问题，传统的变化检测模型表现不佳。因此，我们引入了一个超分辨率模块，并提出了篡改掩码生成器（MMM）框架。它增强了原始图像和篡改图像的分辨率，从而改善了图像细节，以便更好地进行比较。同时，该框架将原始图像和篡改图像转换为特征嵌入并进行拼接，有效地对上下文进行建模。此外，我们创建了篡改掩码生成器数据集（MMMD），这是一个涵盖了各种篡改技术的数据集。我们的目标是通过MMM和MMMD提供更真实的篡改数据，为图像取证和篡改检测领域做出贡献。有关MMMD和下载链接的详细信息，请访问：代码和数据集将公开。||
|**2024-07-03**|[Visual Grounding with Attention-Driven Constraint Balancing](http://arxiv.org/abs/2407.03243)|null|不同于目标检测，视觉定位任务需要检测由复杂的自由形式语言描述的对象。为了同时对这种复杂的语义和视觉表示进行建模，最近最先进的研究采用基于 Transformer 的模型来融合来自两种模态的特征，并进一步引入了各种模块来调节视觉特征，使其与语言表达保持一致并消除不相关的冗余信息。然而，它们的损失函数仍然采用常见的目标检测损失，只控制边界框回归输出，无法完全优化上述目标。为了解决这个问题，本文首先分析了基于 Transformer 模型的注意力机制。在此基础上，我们进一步提出了一个名为注意力驱动约束平衡（AttBalance）的新框架，以优化语言相关区域内视觉特征的行为。大量的实验结果表明，我们的方法带来了令人印象深刻的改进。具体来说，我们在四个不同基准上评估的五种不同模型上实现了持续的改进。此外，通过将我们的方法集成到 QRNet 中，我们获得了新的最先进的性能。||
|**2024-07-03**|[Category-Aware Dynamic Label Assignment with High-Quality Oriented Proposal](http://arxiv.org/abs/2407.03205)|null|航拍图像中的物体通常嵌入在复杂的背景中，并呈现任意方向。当使用定向边界框 (OBB) 表示任意方向的物体时，角度的周期性可能导致边界处标签回归值的不连续性，从而导致损失函数出现剧烈波动。为了解决这个问题，在定向检测框架中引入了一种基于复平面的 OBB 表示方法，并提出了一种三角损失函数。此外，利用对复杂背景环境和航拍图像中大型物体显著差异的先验知识，构建了一个 Conformer RPN 头部来预测角度信息。所提出的损失函数和 Conformer RPN 头部共同生成高质量的定向建议。针对仅依靠 IoU 进行建议标签分配的局限性，提出了一种基于预测类别反馈的类别感知动态标签分配方法。该方法使负样本选择更具代表性，确保分类和回归特征之间的一致性。在四个真实的定向检测数据集上进行了实验，结果表明，在参数调整和时间成本最小的情况下，定向目标检测的性能更优。具体而言，在 DOTA-v1.0、DOTA-v1.5、DIOR-R 和 HRSC2016 数据集上分别实现了 82.02%、71.99%、69.87% 和 98.77% 的平均精度均值 (mAP) 分数。||
|**2024-07-03**|[SegVG: Transferring Object Bounding Box to Segmentation for Visual Grounding](http://arxiv.org/abs/2407.03200)|**[link](https://github.com/weitaikang/segvg)**|与目标检测不同，视觉定位（Visual Grounding）旨在为每个文本-图像对检测一个边界框。这种为每个文本-图像数据提供一个边界框的方式提供了稀疏的监督信号。尽管先前的工作取得了令人瞩目的成果，但它们对标注的被动利用，即将边界框标注仅用作回归真值，导致了性能欠佳。在本文中，我们提出了SegVG，这是一种将边界框级标注转换为分割信号的新方法，以便为视觉定位提供额外的像素级监督。具体来说，我们提出了多层多任务编码器-解码器作为目标定位阶段，在该阶段中，我们学习回归查询和多个分割查询，分别通过在每个解码层中对边界框进行回归和分割来定位目标。这种方法使我们能够迭代地利用标注作为边界框级回归和像素级分割的信号。此外，由于骨干网络通常由从单模态任务中学习到的预训练参数初始化，并且用于回归和分割的查询都是静态可学习的嵌入，因此这三种类型的特征之间存在域差异，这会损害后续的目标定位。为了减轻这种差异，我们引入了三重对齐模块，其中查询、文本和视觉标记通过三重注意力机制进行三角更新，以共享相同的空间。在五个广泛使用的数据集上进行的大量实验验证了我们的方法达到了最先进的性能 (SOTA)。||
|**2024-07-03**|[Global Context Modeling in YOLOv8 for Pediatric Wrist Fracture Detection](http://arxiv.org/abs/2407.03163)|**[link](https://github.com/ruiyangju/yolo_global_context_fracture_detection)**|儿童在日常生活中经常遭受腕部损伤，而骨折损伤放射科医生通常需要在外科医生进行手术治疗之前分析和解释 X 光图像。深度学习的发展使神经网络模型能够作为计算机辅助诊断 (CAD) 工具来帮助医生和专家进行诊断。由于 YOLOv8 模型在目标检测任务中取得了令人满意的成功，因此它已被应用于骨折检测。全局上下文 (GC) 模块以轻量级的方式有效地对全局上下文进行建模，将其融入 YOLOv8 可以极大地提高模型性能。本文提出了用于骨折检测的 YOLOv8+GC 模型，它是具有 GC 模块的 YOLOv8 模型的改进版本。实验结果表明，与原始的 YOLOv8 模型相比，所提出的 YOLOv8-GC 模型在 GRAZPEDWRI-DX 数据集上将交并比阈值为 0.5 时的平均精度均值 (mAP 50) 从 63.58% 提高到 66.32%，达到了最先进的水平 (SOTA)。这项工作的实现代码可在 GitHub 上获取：https://github.com/RuiyangJu/YOLOv8_Global_Context_Fracture_Detection。||
|**2024-07-03**|[Applying Extended Object Tracking for Self-Localization of Roadside Radar Sensors](http://arxiv.org/abs/2407.03084)|null|智能交通系统 (ITS) 可以受益于路边 4D 毫米波雷达传感器，用于大规模交通监控，因为它们具有全天候功能、长感应范围和低制造成本。然而，在城市环境中，使用外部测量设备的定位方法存在局限性。此外，如果传感器安装由于环境影响而出现变化，则在仅在安装期间执行测量时无法对其进行校正。在本文中，我们提出了使用扩展目标跟踪 (EOT) 的路边雷达数据自定位方法。该方法分析传感器观察到的车辆跟踪轨迹和城市街道的航空激光扫描，将“直行”、“左转”、“右转”等驾驶行为标签分配给轨迹段和路段，并执行语义迭代最近点 (SICP) 算法来配准点云。该方法利用下游任务（目标跟踪）的结果进行定位。我们展示了亚米范围内的高精度以及非常低的方位误差。该方法还显示出良好的数据效率。评估在仿真和实际测试中均已完成。||
|**2024-07-03**|[YOLOv5, YOLOv8 and YOLOv10: The Go-To Detectors for Real-time Vision](http://arxiv.org/abs/2407.02988)|null|本文全面回顾了YOLO（You Only Look Once）目标检测算法的演进过程，重点关注YOLOv5、YOLOv8和YOLOv10。我们分析了这些版本在架构改进、性能提升以及边缘部署适用性方面的差异。YOLOv5引入了CSPDarknet骨干网络和Mosaic数据增强等重大创新，实现了速度和精度之间的平衡。YOLOv8在此基础上，通过增强特征提取和无锚框检测，提高了算法的通用性和性能。YOLOv10则凭借无NMS训练、空间通道解耦下采样以及大核卷积等技术实现了跨越式发展，以更低的计算开销实现了最先进的性能。我们的研究结果突出了YOLO算法在精度、效率和实时性能方面的逐步提升，特别强调了其在资源受限环境中的适用性。本综述提供了模型复杂度和检测精度之间权衡的见解，为针对特定边缘计算应用选择最合适的YOLO版本提供了指导。||
|**2024-07-03**|[ShiftAddAug: Augment Multiplication-Free Tiny Neural Network with Hybrid Computation](http://arxiv.org/abs/2407.02881)|null|缺乏乘法运算符（例如移位和加法）因其与硬件的兼容性而受到关注。然而，与具有相同结构的传统神经网络 (NN) 相比，采用这些运算符的神经网络 (NN) 通常表现出较低的精度。ShiftAddAug 使用成本高昂的乘法来增强高效但功能较弱的无乘法运算符，从而在没有任何推理开销的情况下提高性能。它将一个 ShiftAdd 小型神经网络放入一个大型乘法模型中，并鼓励将其训练为子模型以获得额外的监督。为了解决混合运算符之间的权重差异问题，提出了一种新的权重共享方法。此外，一种新颖的两阶段神经架构搜索用于为更小但更强的无乘法小型神经网络获得更好的增强效果。ShiftAddAug 的优越性通过图像分类和语义分割实验得到验证，始终如一地提供显着的增强。值得注意的是，与直接训练的对应模型相比，它在 CIFAR100 上的准确率提高了 4.95%，甚至超过了乘法神经网络的性能。||
|**2024-07-03**|[A Pairwise DomMix Attentive Adversarial Network for Unsupervised Domain Adaptive Object Detection](http://arxiv.org/abs/2407.02835)|null|无监督域自适应目标检测 (DAOD) 可以使在一个源域上训练的模型适应未标记的目标域，以进行目标检测。现有的无监督 DAOD 方法通常执行从目标域到源域的特征对齐。单向域迁移会忽略有关目标样本的信息，并在存在较大域差异时导致欠佳的自适应。因此，我们提出了一种具有域混合 (DomMix) 模块的成对注意力对抗网络，以缓解上述挑战。具体来说，采用深度混合来构建一个中间域，允许来自两个域的特征共享它们的差异。然后，应用成对注意力对抗网络，在不同尺度的图像级和实例级特征上进行注意力编码，并通过对抗学习优化域对齐。这使得网络能够专注于具有不同上下文信息的区域，并学习它们在不同域之间的相似性。在几个基准数据集上进行了广泛的实验，证明了我们提出的方法的优越性。||
|**2024-07-03**|[Data Overfitting for On-Device Super-Resolution with Dynamic Algorithm and Compiler Co-Design](http://arxiv.org/abs/2407.02813)|**[link](https://github.com/coulsonlee/dy-dca-ecc)**|深度神经网络 (DNN) 经常被应用于各种计算机视觉应用。如今，当前视频分发系统中一个新兴趋势是利用 DNN 的过拟合特性来执行视频分辨率提升。通过将视频分割成块并应用超分辨率 (SR) 模型对每个块进行过拟合，这种 SR 模型加视频块的方案能够取代传统的视频传输，从而提高视频质量和传输效率。然而，为了保证高性能，需要许多模型和块，这会导致用户端的模型切换和内存占用方面产生巨大的开销。为了解决这些问题，我们提出了一种由内容感知数据处理管道辅助的动态深度神经网络，以将模型数量减少到一个 (Dy-DCA)，这有助于在节省计算资源的同时提高性能。此外，为了在用户端实现真正的加速，我们设计了一个框架来优化 Dy-DCA 中的动态特征（例如，动态形状、大小和控制流），从而实现一系列编译优化，包括融合代码生成、静态执行计划等。通过采用这些技术，我们的方法在现成的手机上实现了更好的 PSNR 和实时性能 (33 FPS)。同时，在我们的编译优化的辅助下，我们实现了 1.7 倍的加速，同时节省了高达 1.61 倍的内存消耗。代码可在 https://github.com/coulsonlee/Dy-DCA-ECCV2024 获取。||
|**2024-07-03**|[Fine-Grained Scene Image Classification with Modality-Agnostic Adapter](http://arxiv.org/abs/2407.02769)|**[link](https://github.com/qunilcs/maa)**|在处理细粒度场景图像分类任务时，以往的大多数工作在进行多模态特征融合时都非常重视全局视觉特征。换句话说，模型的设计是基于对不同模态重要性的先验直觉。在本文中，我们提出了一种新的多模态特征融合方法，称为MAA（模态无关适配器），试图使模型自适应地学习不同模态在不同情况下的重要性，而无需在模型架构中进行先验设置。更具体地说，我们消除了分布中的模态差异，然后使用模态无关的Transformer编码器进行语义级别的特征融合。我们的实验表明，通过应用与以前方法相同的模态，MAA在基准测试中取得了最先进的结果。此外，值得一提的是，在使用MAA时，可以轻松添加新的模态，并进一步提升性能。代码可在https://github.com/quniLcs/MAA获取。||

<p align=right>(<a href=#updated-on-20240717>back to top</a>)</p>

## 生成模型

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-07-16**|[Efficient Training with Denoised Neural Weights](http://arxiv.org/abs/2407.11966)|null|良好的权重初始化是降低深度神经网络 (DNN) 模型训练成本的有效措施。然而，如何初始化参数的选择具有挑战性，可能需要手动调整，这既耗时又容易出现人为错误。为了克服这些限制，这项工作朝着构建权重生成器来合成用于初始化的神经网络权重迈出了新的一步。我们以使用生成对抗网络 (GAN) 的图像到图像转换任务为例，因为它易于收集涵盖广泛范围的模型权重。具体来说，我们首先收集一个包含各种图像编辑概念及其相应训练权重的数据集，这些权重稍后将用于训练权重生成器。为了解决层间不同的特性和要预测的大量权重问题，我们将权重分成大小相等的块，并为每个块分配一个索引。随后，使用扩散模型在该数据集上进行训练，同时使用概念的文本条件和块索引。通过使用我们的扩散模型预测的去噪权重初始化图像转换模型，训练仅需 43.3 秒。与从头开始训练（即 Pix2pix）相比，我们在新概念上实现了 15 倍的训练时间加速，同时获得了更好的图像生成质量。|
|**2024-07-16**|[Context-Guided Diffusion for Out-of-Distribution Molecular and Protein Design](http://arxiv.org/abs/2407.11942)|**[link](https://github.com/leojklarner/context-guided-diffusion)**|生成模型具有加速新型分子疗法和材料发现关键步骤的潜力。扩散模型最近成为一种强大的方法，在无条件样本生成方面表现出色，并且在数据驱动引导下，在其训练领域内进行条件生成也表现出色。 然而，可靠地从训练数据之外的高价值区域进行采样仍然是一个开放的挑战——目前的方法主要集中在修改扩散过程本身。在本文中，我们开发了上下文引导扩散 (CGD)，这是一种简单的即插即用方法，它利用未标记数据和平滑约束来改进引导扩散模型的分布外泛化能力。我们证明，这种方法可以在各种设置中带来实质性的性能提升，包括连续、离散和图结构的扩散过程，并在药物发现、材料科学和蛋白质设计中都有应用。|
|**2024-07-16**|[Data-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model Co-development](http://arxiv.org/abs/2407.11784)|**[link](https://github.com/modelscope/data-juicer)**|大规模多模态生成模型的出现极大地促进了人工智能的发展，带来了前所未有的性能和功能水平。然而，由于以模型为中心和以数据为中心的发展路径在历史上是孤立的，导致结果欠佳和资源利用效率低下，优化这些模型仍然具有挑战性。为了解决这个问题，我们提出了一套全新的沙盒套件，专门用于集成的数据模型协同开发。这个沙盒提供了一个全面的实验平台，能够快速迭代和洞察力驱动的模型和数据改进。我们提出的“探测-分析-改进”工作流程，通过在最先进的类似LLaVA和基于DiT的模型上的应用验证，实现了显著的性能提升，例如登顶VBench排行榜。我们还从详尽的基准测试中获得了富有成效的见解，揭示了数据质量、多样性和模型行为之间至关重要的相互作用。为了促进对多模态数据和生成模型的更深入理解和未来发展，我们的代码、数据集和模型都维护在https://github.com/modelscope/data-juicer/blob/main/docs/Sandbox.md，并且可以访问。|
|**2024-07-16**|[Diffusion-driven self-assembly of emerin nanodomains at the nuclear envelope](http://arxiv.org/abs/2407.11758)|null|Emerin是一种核膜蛋白，在机械传导和核形状适应中起着重要的生物学作用，它在内核膜上自组装成纳米级的结构域。这些纳米域的大小和emerin占据率会随着施加的机械应力和与Emery-Dreifuss肌营养不良症(EDMD)相关的emerin突变而改变。通过理论和实验的结合，我们在这里表明，一个简单的反应扩散模型可以解释emerin纳米域的自组装。我们的模型与野生型emerin和EDMD相关emerin突变（无论是否存在外力）的emerin纳米域的大小和占据率的实验观察结果在数量上一致，并且允许从对emerin纳米域整体性质的观察结果成功预测emerin扩散系数。我们的研究结果从emerin及其核结合伙伴的关键反应和扩散特性的变化方面，为EDMD相关的emerin组织缺陷提供了物理解释。|
|**2024-07-16**|[Generating Multi-Modal and Multi-Attribute Single-Cell Counts with CFGen](http://arxiv.org/abs/2407.11734)|null|单细胞RNA测序数据的生成模型在社区驱动的任务中显示出巨大的潜力，例如轨迹推断、批次效应去除和基因表达生成。然而，大多数最近从噪声生成合成单细胞的深度模型都是在预处理的连续基因表达近似值上运行的，忽略了单细胞数据固有的离散性和过度分散性，这限制了下游应用并阻碍了鲁棒噪声模型的结合。此外，基于深度学习的合成单细胞生成的关键方面仍未得到充分探索，例如可控的多模态和多标签生成及其在下游任务性能增强中的作用。这项工作提出了用于生成的细胞流（CFGen），这是一种基于流的用于多模态单细胞计数的条件生成模型，它明确考虑了数据的离散性。我们的结果表明，在考虑新的生成任务（例如以多个属性为条件和通过数据增强提高稀有细胞类型分类）的同时，可以改进关键生物学数据特征的恢复。通过在一组不同的生物数据集和设置中展示CFGen，我们提供了其对计算生物学和深度生成模型领域价值的证据。|
|**2024-07-16**|[Theoretical Insights into CycleGAN: Analyzing Approximation and Estimation Errors in Unpaired Data Generation](http://arxiv.org/abs/2407.11678)|null|本文重点分析了名为 CycleGAN 的非配对数据生成模型的过度风险。与经典的 GAN 不同，CycleGAN 不仅可以在两个非配对分布之间转换数据，还可以确保映射的一致性，这是由 CycleGAN 独有的循环一致性项所鼓励的。CycleGAN 中模型结构日益复杂以及循环一致性项的加入给误差分析带来了新的挑战。通过考虑模型架构和训练过程的影响，将风险分解为两个方面：逼近误差和估计误差。这两个误差项分别进行分析，并最终通过考虑它们之间的权衡来组合。每个部分都经过严格分析；通过构建最优传输映射的逼近来逼近误差，并通过使用 Rademacher 复杂度建立上限来估计误差。我们的分析不仅隔离了这些误差，还探讨了它们之间的权衡，这为 CycleGAN 的架构和训练过程如何影响其性能提供了理论见解。|
|**2024-07-16**|[Mask-guided cross-image attention for zero-shot in-silico histopathologic image generation with a diffusion model](http://arxiv.org/abs/2407.11664)|null|利用生成式人工智能创建虚拟数据有望成为计算病理学中对整个切片图像进行染色、成像和标注的一种经济高效的替代方案。扩散模型是生成虚拟图像的最先进解决方案，可提供无与伦比的保真度和真实感。使用外观迁移扩散模型可以实现零样本图像生成，从而促进快速应用并无需模型训练。然而，当前的外观迁移扩散模型是为自然图像设计的，其主要任务是将前景物体从源域迁移到目标域，而背景的重要性微不足道。然而，在计算病理学中，特别是在肿瘤学中，直接定义图像中的哪些物体应该被归类为前景和背景并不容易，因为图像中的所有物体都可能对详细了解肿瘤微环境至关重要。我们通过修改外观迁移引导，使用现有的分割掩码在特定类别 AdaIN 特征统计匹配之间交替，从而提高了外观迁移扩散模型对免疫组化染色图像的适用性。所提出的方法的性能在监督上皮分割的下游任务中得到了证明，结果表明，模型训练所需的标注数量可以减少 75%，优于基线方法。此外，我们咨询了 certified 病理学家，以探讨未来的改进方向。我们预计这项工作将激励零样本扩散模型在计算病理学中的应用，为生成具有无与伦比的保真度和真实感的虚拟图像提供一种有效的方法，这些图像对下游任务（如训练现有深度学习模型或微调基础模型）具有重要意义。|
|**2024-07-16**|[Magnetogram-to-Magnetogram: Generative Forecasting of Solar Evolution](http://arxiv.org/abs/2407.11659)|null|研究太阳磁场对于理解太阳内部的物理过程及其对行星际环境的影响至关重要。我们引入了一种新方法，使用基于去噪扩散概率模型 (DDPM) 的图像到图像转换技术来预测太阳视线 (LoS) 磁图的演变。我们的方法结合了图像质量的“计算机科学指标”和物理准确性的“物理指标”来评估模型性能。结果表明，DDPM 在保持太阳磁场的结构完整性、动态范围、磁通量和其他物理特征（如活动区域的大小）方面非常有效，甚至在耀斑情况下也优于传统的持续性模型。我们的目标是利用深度学习不仅用于可视化，而且将其作为望远镜的集成和交互式工具，增强我们对太阳耀斑等意外物理事件的理解。未来的研究将致力于整合更多样化的太阳数据，以提高我们生成模型的准确性和适用性。|
|**2024-07-16**|[CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging](http://arxiv.org/abs/2407.11652)|null|联邦学习 (FL) 提供了一种保护隐私的训练分散数据模型的方法。它在医疗保健领域具有巨大的潜力，但由于医疗图像数据在客户端之间的差异，以及注释有限，因此面临着挑战。本文介绍了跨客户端差异自适应联邦学习 (CCVA-FL) 来解决这些问题。CCVA-FL 旨在通过将图像转换到共同特征空间来最小化客户端之间的差异。它涉及专家对每个客户端的图像子集进行注释，然后选择数据复杂度最低的客户端作为目标客户端。然后，使用基于目标客户端注释图像的可扩展扩散模型和 Transformer (DiT) 生成合成医学图像。这些合成图像捕捉了多样性并代表了原始数据，并与其他客户端共享。然后，每个客户端使用图像到图像的转换将其本地图像转换到目标图像空间。转换后的图像随后用于联邦学习设置中，以开发服务器模型。我们的结果表明，CCVA-FL 通过有效解决客户端之间的数据分布差异，在不损害隐私的情况下，优于传统的联邦平均算法。|
|**2024-07-16**|[Scaling Diffusion Transformers to 16 Billion Parameters](http://arxiv.org/abs/2407.11633)|**[link](https://github.com/feizc/dit-moe)**|本文介绍了DiT-MoE，这是一种稀疏版本的扩散Transformer，它在保持高度优化的推理能力的同时，在规模和竞争力上都与密集网络相当。DiT-MoE包含两种简单的设计：共享专家路由和专家级平衡损失，从而捕获共同知识并减少不同路由专家之间的冗余。当应用于条件图像生成时，对专家专业化的深入分析获得了一些有趣的观察结果：（i）专家选择表现出对空间位置和去噪时间步长的偏好，而对不同的类别条件信息不敏感；（ii）随着MoE层越来越深，专家的选择逐渐从特定的空间位置转变为分散和平衡；（iii）专家专业化倾向于集中在早期时间步长，然后在经过一半时间后逐渐均匀。我们将其归因于扩散过程，该过程首先对低频空间信息进行建模，然后对高频复杂信息进行建模。基于以上指导，一系列DiT-MoE实验实现了与密集网络相当的性能，但在推理过程中需要的计算量要少得多。更令人鼓舞的是，我们展示了DiT-MoE在合成图像数据方面的潜力，将扩散模型扩展到16.5B参数，在512×512分辨率设置下获得了新的SoTA FID-50K分数1.80。项目页面：https://github.com/feizc/DiT-MoE。|
|**2024-07-12**|[Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text](http://arxiv.org/abs/2407.09364)|null|大型语言模型发展的显著进步模糊了人类和人工智能生成文本之间的界限。人工智能生成文本的日益普及及其检测难度给我们的社会带来了新的挑战。在本文中，我们提出了 WhosAI 来解决检测和归因人工智能生成文本的问题，这是一个三元组网络对比学习框架，旨在预测给定输入文本是由人类还是人工智能生成的，并揭示文本的作者。与大多数现有方法不同，我们提出的框架旨在从多个生成器同时学习语义相似性表示，从而平等地处理检测和归因任务。此外，WhosAI 与模型无关，并且可以通过将新的人工智能文本生成模型生成的实例合并到我们框架学习的嵌入空间中来扩展到新模型的发布。在包含 20 万篇新闻文章的 TuringBench 基准测试上的实验结果表明，我们提出的框架在图灵测试和作者归因任务中均取得了优异的结果，优于 TuringBench 基准测试排行榜中列出的所有方法。|
|**2024-07-12**|[Any-Property-Conditional Molecule Generation with Self-Criticism using Spanning Trees](http://arxiv.org/abs/2407.09357)|null|生成新分子极具挑战性，大多数表示方法会导致生成模型产生许多无效分子。基于生成树的图生成 (STGG) 是一种很有前景的方法，可以确保生成有效分子，在无条件生成方面优于最先进的 SMILES 和图扩散模型。在现实世界中，我们希望能够根据一种或多种所需属性（而不是无条件地）生成分子。因此，在这项工作中，我们将 STGG 扩展到多属性条件生成。我们的方法 STGG+ 结合了现代 Transformer 架构、训练期间属性的随机掩蔽（能够以任何属性子集和无分类器引导为条件）、辅助属性预测损失（允许模型对分子进行自我批评并选择最佳分子）和其他改进。我们证明了 STGG+ 在分布内和分布外条件生成以及奖励最大化方面均达到了最先进的性能。|
|**2024-07-12**|[PID: Physics-Informed Diffusion Model for Infrared Image Generation](http://arxiv.org/abs/2407.09299)|**[link](https://github.com/fangyuanmao/pid)**|红外成像技术因其在低能见度条件下的可靠传感能力而受到广泛关注，促使许多研究致力于将丰富的RGB图像转换为红外图像。然而，大多数现有的图像转换方法将红外图像视为一种风格变化，而忽略了潜在的物理规律，这限制了它们的实际应用。为了解决这些问题，我们提出了一种物理信息扩散（PID）模型，用于将RGB图像转换为符合物理规律的红外图像。我们的方法利用了扩散模型的迭代优化，并在训练过程中结合了基于红外规律先验知识的强物理约束。这种方法增强了转换后的红外图像与真实红外域之间的相似性，而无需增加额外的训练参数。实验结果表明，PID明显优于现有的最先进方法。我们的代码可在https://github.com/fangyuanmao/PID获取。|
|**2024-07-12**|[Learning Distances from Data with Normalizing Flows and Score Matching](http://arxiv.org/abs/2407.09297)|null|基于密度的距离 (DBD) 为度量学习问题提供了一种优雅的解决方案。通过定义一个黎曼度量，该度量随着概率密度的降低而增加，最短路径自然遵循数据流形，并且点根据数据的模态进行聚类。我们发现，现有的估计费马距离（一种特定选择的 DBD）的方法，由于 i) 不准确的密度估计和 ii) 依赖于在高维情况下越来越粗糙的基于图的路径，因此在低维和高维情况下都存在收敛性差的问题。为了解决这些问题，我们建议使用归一化流（一种具有易处理密度估计的生成模型）来学习密度，并采用从基于图的提议初始化的评分模型来使用平滑松弛方法。此外，我们引入了一种维度自适应的费马距离，它在扩展到高维时表现出更直观的特性，并提供更好的数值特性。我们的工作为实际应用基于密度的距离铺平了道路，特别是在高维空间中。|
|**2024-07-12**|[Surgical Text-to-Image Generation](http://arxiv.org/abs/2407.09230)|null|获取用于研究和开发的手术数据受到高昂标注成本以及实践和伦理限制的严重阻碍。利用合成生成的图像可以提供一种有价值的替代方案。在这项工作中，我们利用 CholecT50 数据集对文本到图像生成模型在手术领域的适应性进行了深入分析，该数据集提供了用手术动作三元组（器械、动词、目标）标注的手术图像。我们研究了各种语言模型，发现 T5 在基于三元组的文本输入区分手术动作方面提供了更独特的特征。我们的分析表明，长标题和基于三元组的标题之间存在高度一致性，支持使用基于三元组的标签。我们通过发现三元组文本嵌入在潜在空间中以器械为中心，并设计了一种基于器械的类别平衡技术来抵消手术数据中的不平衡和偏差，从而解决了在没有额外输入信号的情况下训练基于三元组标题的文本到图像模型的挑战，改进了训练的收敛性。我们扩展了基于扩散的生成模型 Imagen，开发了 Surgical Imagen，用于从基于三元组的文本提示生成逼真且与活动一致的手术图像。我们使用多种指标评估我们的模型，包括人类专家调查和 FID 和 CLIP 分数等自动化方法。我们从质量、一致性、推理、知识和鲁棒性等关键方面评估模型性能，证明了我们的方法在提供真实数据收集的现实替代方案方面的有效性。|
|**2024-07-12**|[Salt & Pepper Heatmaps: Diffusion-informed Landmark Detection Strategy](http://arxiv.org/abs/2407.09192)|null|解剖标志检测是指识别图像中用于临床测量的关键区域的过程。每个标志都是由临床医生标记的单个真实点。机器学习模型将标志的位置预测为由热图表示的概率区域。扩散模型由于其高质量的采样和模式覆盖范围，在生成建模中越来越受欢迎，导致其在医学图像处理中用于语义分割。扩散模型可以进一步调整以学习标志的分布。扩散模型的随机性捕获了标志预测中的波动，我们通过将其模糊成有意义的概率区域来利用这一点。在本文中，我们将自动解剖标志检测重新表述为一个精确的生成建模任务，生成一个少量热像素热图。我们的方法实现了最先进的MRE性能，并且SDR性能与现有工作相当。|
|**2024-07-12**|[Variational Inference via Smoothed Particle Hydrodynamics](http://arxiv.org/abs/2407.09186)|null|本文提出了一种基于光滑粒子流体动力学 (SPH) 的新型变分推理方法 SPH-ParVI，用于对部分已知密度（例如，直到常数）进行采样或使用梯度进行采样。SPH-ParVI 模拟了在目标密度驱动的外部效应下流体的流动；流体的瞬态或稳态近似于目标密度。连续流体通过 SPH 建模为相互作用的粒子系统 (IPS)，其中每个粒子都携带平滑的属性，并根据纳维-斯托克斯方程进行交互和演化。这种无网格的拉格朗日模拟方法为一类概率模型（例如在贝叶斯推理和生成建模中遇到的模型）提供了快速、灵活、可扩展和确定性的采样和推理。|
|**2024-07-12**|[Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors](http://arxiv.org/abs/2407.09136)|**[link](https://github.com/eth-lre/verify-then-generate)**|大型语言模型 (LLM) 为向所有人提供高质量的个性化教育提供了机会。一种有希望实现这一目标的方法是构建能够支持学生解决问题的对话辅导模型。然而，尽管现有的 LLM 在解决推理问题方面表现出色，但它们难以准确地检测学生的错误并根据这些错误调整反馈。受现实世界教学实践的启发，教师会识别学生的错误并根据错误定制他们的回应，我们专注于验证学生的解决方案，并展示了这种验证的基础如何提高导师回应生成的整体质量。我们收集了一个包含 1000 个逐步数学推理链的数据集，其中第一步错误由教师注释。我们通过实证表明，为当前模型找到学生解决方案中的错误具有挑战性。我们提出并评估了几种用于检测这些错误的验证器。通过自动评估和人工评估，我们表明，与现有基线相比，学生解决方案验证器引导生成模型针对学生错误生成更有针对性的回应，这些回应通常更正确，并且 hallucinations 更少。|
|**2024-07-12**|[Overcoming Catastrophic Forgetting in Tabular Data Classification: A Pseudorehearsal-based approach](http://arxiv.org/abs/2407.09039)|null|持续学习 (CL) 提出了一个重要挑战，即如何在不忘记先前获得的知识的情况下，适应不断变化的数据分布，同时整合新知识。在本文中，我们介绍了一种新的方法，称为基于表格数据排练的增量终身学习框架 (TRIL3)，旨在解决表格数据分类问题中的灾难性遗忘现象。TRIL3 使用基于原型的增量生成模型 XuILVQ 生成合成数据来保留旧知识，并使用 DNDF 算法（该算法已修改为以增量方式运行）来学习表格数据的分类任务，而无需存储旧样本。在经过不同的测试以获得足够的合成数据百分比并将 TRIL3 与其他可用的 CL 方案进行比较后，我们可以得出结论，TRIL3 的性能优于文献中的其他方案，仅使用 50% 的合成数据。|
|**2024-07-12**|[Aligning Diffusion Behaviors with Q-functions for Efficient Continuous Control](http://arxiv.org/abs/2407.09024)|null|基于语言模型对齐的最新进展，我们将离线强化学习制定为一个两阶段优化问题：首先在无奖励行为数据集上预训练表达能力强的生成策略，然后微调这些策略以与特定任务的标注（如 Q 值）对齐。这种策略允许我们利用丰富多样的行为数据来增强泛化能力，并使用最少的标注快速适应下游任务。特别地，我们引入了高效扩散对齐（EDA）来解决连续控制问题。EDA 利用扩散模型进行行为建模。然而，与之前的方法不同，我们将扩散策略表示为标量神经网络相对于动作输入的导数。这种表示至关重要，因为它可以直接计算扩散模型的密度，使其与现有的 LLM 对齐理论兼容。在策略微调期间，我们扩展了基于偏好的对齐方法（如直接偏好优化（DPO））以将扩散行为与连续 Q 函数对齐。我们在 D4RL 基准测试上的评估表明，EDA 在整体性能上优于所有基线方法。值得注意的是，EDA 在仅使用 1% 的 Q 值标记数据进行微调的情况下仍能保持约 95% 的性能，并且仍然优于其他几个基线方法。|
|**2024-07-11**|[Video Diffusion Alignment via Reward Gradients](http://arxiv.org/abs/2407.08737)|**[link](https://github.com/mihirp1998/vader)**|我们在构建基础视频扩散模型方面取得了重大进展。由于这些模型使用大规模无监督数据进行训练，因此使这些模型适应特定的下游任务变得至关重要。通过监督微调来调整这些模型需要收集视频的目标数据集，这既具有挑战性又乏味。在这项工作中，我们利用预先训练的奖励模型来调整视频扩散模型，这些奖励模型是通过基于强大的视觉判别模型的偏好学习的。这些模型包含关于生成的 RGB 像素的密集梯度信息，这对于在复杂搜索空间（例如视频）中的高效学习至关重要。我们表明，将这些奖励模型的梯度反向传播到视频扩散模型可以实现视频扩散模型在计算和样本方面的有效对齐。我们展示了各种奖励模型和视频扩散模型的结果，证明了我们的方法在奖励查询和计算方面比先前的无梯度方法效率更高。我们的代码、模型权重和更多可视化内容可在 https://vader-vid.github.io 获取。||
|**2024-07-11**|[Live2Diff: Live Stream Translation via Uni-directional Attention in Video Diffusion Models](http://arxiv.org/abs/2407.08701)|null|大型语言模型由于其时间上的单向注意机制（该机制对当前标记和先前标记之间的相关性进行建模），在生成文本和音频等流数据方面表现出了显著的效果。然而，尽管对实时视频处理的需求日益增长，但视频流的研究仍然远远落后。最先进的视频扩散模型利用双向时间注意力来建模当前帧和所有周围（即包括未来）帧之间的相关性，这阻碍了它们处理流视频。为了解决这个问题，我们提出了 Live2Diff，这是首次尝试设计具有单向时间注意力的视频扩散模型，专门针对实时流视频翻译。与以前的工作相比，我们的方法通过将当前帧与其前一帧和一些初始预热帧相关联来确保时间一致性和平滑度，而无需任何未来帧。此外，我们使用了一种高效的去噪方案，该方案采用 KV 缓存机制和流水线技术，以促进以交互式帧率进行流视频翻译。大量实验表明，所提出的注意力机制和流水线的有效性，在时间平滑度和/或效率方面优于以前的方法。||
|**2024-07-11**|[Scattering transforms on the sphere, application to large scale structure modelling](http://arxiv.org/abs/2407.08687)|null|散射变换是最近为研究高度非高斯过程而开发的一种新型汇总统计数据，已被证明在天文物理研究中非常有前景。特别是，它们允许人们从有限的数据中构建复杂非线性场的生成模型。在即将进行的宇宙学调查的背景下，有必要将这些工具扩展到球面数据。我们开发了球面上的散射变换，并专注于构建天体物理场最大熵生成模型。生成模型的质量，无论是在统计上还是视觉上，都非常令人满意，因此为未来的宇宙学研究开辟了广泛的新应用。||
|**2024-07-11**|[CAD-Prompted Generative Models: A Pathway to Feasible and Novel Engineering Designs](http://arxiv.org/abs/2407.08675)|null|文本到图像生成模型越来越多地被用于协助各个创意领域的概念生成，例如图形设计、用户界面设计和时装设计。然而，由于模型在生成可行设计概念的图像方面存在挑战，它们在工程设计中的应用仍然有限。为了解决这个问题，本文介绍了一种通过使用可行的 CAD 图像提示生成来提高设计可行性的方法。在这项工作中，通过一个自行车设计任务的案例研究，使用现成的文本到图像模型 Stable Diffusion 2.1，研究了该方法的有效性。在七个不同的生成设置中，使用不同的 CAD 图像提示权重，生成了一组多样化的自行车设计，并根据其感知的可行性和新颖性对这些设计进行了评估。结果表明，CAD 图像提示成功地帮助 Stable Diffusion 2.1 等文本到图像模型创建了明显更可行的设计图像。虽然在可行性和新颖性之间观察到了一般的权衡，但当提示权重保持在 0.35 左右的低水平时，设计可行性得到显着提高，而其新颖性与仅通过文本提示生成的设计保持一致。该案例研究的见解为工程设计过程的不同阶段选择合适的 CAD 图像提示权重提供了一些指导。如果使用得当，我们的 CAD 图像提示方法为文本到图像模型在工程设计中的更广泛应用打开了大门。||
|**2024-07-11**|[Controlling the Fidelity and Diversity of Deep Generative Models via Pseudo Density](http://arxiv.org/abs/2407.08659)|null|我们提出了一种对深度生成模型（如 GAN 和扩散模型）进行偏差的方法，以生成具有更高保真度或更高多样性的数据。我们的方法涉及通过一种新的个体样本度量标准（称为伪密度）来操纵训练数据和生成数据的分布，该度量标准基于来自真实样本的最近邻信息。我们的方法提供了三种不同的技术来调整深度生成模型的保真度和多样性：1）每样本扰动，可以对单个样本进行精确调整，使其具有更常见或更独特的特征；2）模型推理过程中的重要性采样，以增强生成数据的保真度或多样性；3）使用重要性采样进行微调，引导生成模型学习调整后的分布，从而控制保真度和多样性。此外，我们的微调方法展示了在最少迭代次数下改进预训练生成模型的 Frechet Inception Distance (FID) 的能力。||
|**2024-07-11**|[Adaptive Smooth Non-Stationary Bandits](http://arxiv.org/abs/2407.08654)|null|我们研究了一个 $K$臂非平稳老虎机模型，其中奖励随时间平滑变化，可以用奖励作为时间函数的赫尔德类假设来刻画。这种平滑变化由赫尔德指数$\beta$和系数$\lambda$参数化。虽然这个一般模型的各种子情况已经被单独研究，但我们首先对所有$K,\beta,\lambda$都确定了极小极大动态遗憾率。接下来，我们证明了这种最优动态遗憾可以自适应地实现，而无需知道$\beta,\lambda$。相比之下，即使在参数已知的情况下，以前的上界也只在$\beta\leq 1$和$\beta=2$的有限范围内知道(Slivkins, 2014; Krishnamurthy and Gopalan, 2021; Manegueu et al., 2021; Jia et al.,2023)。因此，我们的工作解决了这些不同的文献线索提出的开放性问题。我们还研究了在非平稳老虎机中获得更快的与间隔相关的遗憾率的问题。虽然众所周知，这种速率在一般情况下是不可能实现的(Garivier and Moulines, 2011)，但我们表明，允许安全臂的环境(Suk and Kpotufe, 2022)允许比$\sqrt{T}$ 的最坏情况缩放快得多的速率。虽然之前在这个方向上的工作集中在获得通常的对数遗憾界，如在平稳周期上的总和，但我们新的与间隔相关的速率揭示了非平稳性的新的乐观状态，即使是对数界也是悲观的。我们证明了我们新的与间隔相关的速率是紧的，并且它的可实现性（即，安全臂使之成为可能）在平滑的赫尔德类模型中有一个惊人的简单和干净的特征。||
|**2024-07-11**|[Latent Conditional Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph Mode](http://arxiv.org/abs/2407.08500)|null|连续时间动态图（CTDG）精确地模拟了不断演变的现实世界关系，引起了学术界和工业界对动态图学习的浓厚兴趣。然而，现有的CTDG模型面临着来自噪声和有限历史数据的挑战。图数据增强（GDA）成为了一种关键解决方案，但目前的方法主要集中在静态图上，难以有效解决CTDG固有的动态性问题。此外，这些方法通常需要大量的领域专业知识来进行参数调整，并且缺乏对增强效果的理论保证。为了解决这些问题，我们提出了Conda，这是一种专为CTDG量身定制的基于潜在扩散的新型GDA方法。Conda采用了一种类似三明治的架构，结合了变分自动编码器（VAE）和条件扩散模型，旨在为目标节点生成增强的历史邻居嵌入。与通过预训练在整个图上训练的传统扩散模型不同，Conda需要目标节点的历史邻居序列嵌入进行训练，从而促进更有针对性的增强。我们将Conda集成到CTDG模型中，并采用交替训练策略来优化性能。在六个广泛使用的现实世界数据集上的大量实验表明，我们的方法始终如一地提高了性能，特别是在历史数据有限的情况下。||
|**2024-07-11**|[Natural language is not enough: Benchmarking multi-modal generative AI for Verilog generation](http://arxiv.org/abs/2407.08473)|**[link](https://github.com/aichipdesign/chipgptv)**|自然语言接口在利用大型语言模型实现从高级规范自动生成 Verilog 方面展现出巨大潜力，引发了广泛关注。然而，本文阐述了视觉表示为具有空间复杂性的硬件架构的设计意图提供了至关重要的上下文信息，这可能超越了仅使用自然语言输入的效率。在此基础上，本文介绍了一个开源的多模态生成模型基准测试，该模型专为从视觉-语言输入合成 Verilog 而设计，涵盖了单个和复杂的模块。此外，我们还介绍了一个开源的视觉和自然语言 Verilog 查询语言框架，以促进高效且用户友好的多模态查询。为了评估所提出的多模态硬件生成式人工智能在 Verilog 生成任务中的性能，我们将其与仅依赖自然语言的流行方法进行了比较。结果表明，与仅基于自然语言的查询相比，多模态生成的 Verilog 在准确性方面有显著提高。我们希望在大硬件设计模型时代揭示一种新的硬件设计方法，从而促进更具多样性和效率的硬件设计方法。||
|**2024-07-11**|[A Comprehensive Survey on Human Video Generation: Challenges, Methods, and Insights](http://arxiv.org/abs/2407.08428)|**[link](https://github.com/wentaol86/awesome-human-body-video-generation)**|人体视频生成是一个充满活力且快速发展的领域，其目标是利用生成模型根据文本、音频和姿势等控制条件合成二维人体视频序列。由于其在电影、游戏和虚拟通信等领域具有广泛的应用潜力，因此生成自然逼真的人体视频至关重要。生成模型的最新进展为该领域的兴趣日益浓厚奠定了坚实的基础。尽管取得了重大进展，但由于角色的一致性、人体运动的复杂性以及与环境关系的困难，人体视频生成的任务仍然具有挑战性。本综述全面概述了人体视频生成的现状，据我们所知，这是该领域首个广泛的文献综述。我们首先介绍人体视频生成的基本原理，以及促进该领域发展的生成模型的演变。然后，我们研究了人体视频生成中三个关键子任务所采用的主要方法：文本驱动、音频驱动和姿势驱动的运动生成。这些领域将根据指导生成过程的条件进行探讨。此外，我们还收集了最常用的数据集和评估指标，这些指标对于评估生成视频的质量和真实性至关重要。最后，我们讨论了该领域当前面临的挑战，并提出了未来研究的可能方向。本综述旨在为研究界提供一个清晰而全面的人体视频生成进展概况，重点介绍已取得的里程碑和未来面临的挑战。||
|**2024-07-11**|[Diff-Tracker: Text-to-Image Diffusion Models are Unsupervised Trackers](http://arxiv.org/abs/2407.08394)|null|我们介绍了一种名为 Diff-Tracker 的新方法，用于解决具有挑战性的无监督视觉跟踪任务，该方法利用了预训练的文本到图像扩散模型。我们的主要思想是利用预训练扩散模型中封装的丰富知识，例如对图像语义和结构信息的理解，来解决无监督视觉跟踪问题。为此，我们设计了一个初始提示学习器，通过学习表示目标的提示，使扩散模型能够识别跟踪目标。此外，为了促进提示符动态适应目标的运动，我们提出了一种在线提示符更新器。在五个基准数据集上的大量实验表明，我们提出的方法是有效的，并且达到了最先进的性能。||
|**2024-07-09**|[ConceptExpress: Harnessing Diffusion Models for Single-image Unsupervised Concept Extraction](http://arxiv.org/abs/2407.07077)|**[link](https://github.com/haoosz/conceptexpress)**|虽然个性化文本到图像生成技术已经能够从多张图像中学习单个概念，但更实际但也更具挑战性的场景涉及在单个图像中学习多个概念。然而，现有的解决此场景的工作严重依赖于大量的人工标注。在本文中，我们介绍了一项名为无监督概念提取 (UCE) 的新任务，该任务考虑了没有任何人类概念知识的无监督设置。给定一张包含多个概念的图像，该任务的目标是仅依靠预训练扩散模型的现有知识来提取和重建单个概念。为此，我们提出了 ConceptExpress，它通过从两个方面释放预训练扩散模型的内在能力来解决 UCE。具体来说，概念定位方法通过利用扩散自注意力的空间对应关系来自动定位和分离显著概念；并且基于概念和概念标记之间的查找关联，概念优化过程学习表示每个单独概念的区分标记。最后，我们为 UCE 任务建立了一个定制的评估协议。大量实验表明，ConceptExpress 是 UCE 任务的一个很有前景的解决方案。我们的代码和数据可在以下网址获得：https://github.com/haoosz/ConceptExpress||
|**2024-07-09**|[Latent Space Imaging](http://arxiv.org/abs/2407.07052)|null|传统数字成像系统通常基于对规则网格像素进行暴力测量和处理。另一方面，人类视觉系统对从光感受器到视神经的数据进行了大量压缩，本质上是将图像信息编码成适合人脑处理的低带宽潜在空间表示。在这项工作中，我们建议采用类似的方法来开发人工智能视觉系统。潜在空间成像是一种新范式，它通过光学和软件的结合，将图像信息直接编码到生成模型的语义丰富的潜在空间中，从而大大减少了捕获过程中的带宽和内存需求。我们通过基于单像素相机的初始硬件原型展示了这一新原理。通过设计一种调制幅度以编码到生成模型的潜在空间中的方案，我们在成像过程中实现了 1:100 到 1:1,000 的压缩比，这体现了潜在空间成像在高效成像硬件方面的潜力，使其能够在未来应用于高速成像或硬件复杂度大大降低的特定任务相机。||
|**2024-07-09**|[Generative models of astrophysical fields with scattering transforms on the sphere](http://arxiv.org/abs/2407.07007)|**[link](https://github.com/astro-informatics/s2scat)**|散射变换是一种新类型的概括统计量，最近被开发用于研究高度非高斯过程，并已被证明在天文物理研究中非常有前景。 特别是，它们允许人们从有限的数据中构建复杂非线性场的生成模型，并且还被用作新的统计成分分离算法的基础。 在即将进行的宇宙学巡天观测的背景下，例如针对宇宙微波背景偏振的 LiteBIRD 或用于研究宇宙大尺度结构的 Rubin-LSST 和 Euclid，将这些工具扩展到球面数据是必要的。 我们在球面上开发了散射变换，并专注于构建几个天体物理场的最大熵生成模型。 我们从单个目标场构建均匀的天体物理和宇宙学场的生成模型，使用常见的统计数据（功率谱、像素概率密度函数和 Minkowski 泛函）将样本与目标场进行定量比较。 我们的采样场在统计和视觉上都与目标场非常吻合。 因此，这些生成模型为未来的天体物理学和宇宙学研究开辟了广泛的新应用； 特别是那些几乎没有模拟数据的领域。 我们将代码提供给社区，以便这项工作可以轻松复制和进一步开发。||
|**2024-07-09**|[RodinHD: High-Fidelity 3D Avatar Generation with Diffusion Models](http://arxiv.org/abs/2407.06938)|null|我们提出了 RodinHD，它可以从肖像图像生成高保真 3D 头像。现有方法无法捕捉到我们论文中解决的复杂细节，例如发型。我们首先发现了一个被忽视的问题，即在许多头像上顺序拟合三平面时出现的灾难性遗忘问题，这是由 MLP 解码器共享方案引起的。为了克服这个问题，我们提出了一种新颖的数据调度策略和权重整合正则化项，从而提高了解码器渲染更清晰细节的能力。此外，我们通过计算捕获丰富 2D 纹理线索的更细粒度的层次表示，并通过交叉注意力将它们注入到 3D 扩散模型的多个层中，从而优化了肖像图像的引导效果。当使用针对三平面优化的噪声调度对 46K 头像进行训练时，生成的模型可以生成比以前的方法细节明显更好的 3D 头像，并且可以泛化到野外肖像输入。||
|**2024-07-09**|[HumanRefiner: Benchmarking Abnormal Human Generation and Refining with Coarse-to-fine Pose-Reversible Guidance](http://arxiv.org/abs/2407.06937)|**[link](https://github.com/enderfga/humanrefiner)**|文本到图像扩散模型在条件图像生成方面取得了显著进展。然而，这些模型通常难以准确渲染具有人类特征的图像，导致四肢扭曲和其他异常。这个问题主要源于扩散模型对肢体质量的识别和评估不足。为了解决这个问题，我们引入了AbHuman，这是第一个专注于解剖异常的大规模合成人体基准。该基准包含 56K 张合成的人体图像，每张图像都标注了详细的边界框级别标签，识别了 18 个不同类别中的 147K 个人体异常。在此基础上，可以建立人体异常的识别，进而通过负面提示和引导等传统技术增强图像生成。为了进一步促进改进，我们提出了HumanRefiner，这是一种新颖的即插即用方法，用于在文本到图像生成中对人体异常进行从粗到细的细化。具体来说，HumanRefiner 利用自我诊断程序来检测和纠正与粗粒度异常人体姿势和细粒度异常级别相关的问题，促进姿势可逆的扩散生成。在 AbHuman 基准上的实验结果表明，HumanRefiner 显着减少了生成差异，与最先进的开源生成器 SDXL 相比，肢体质量提高了 2.9 倍，在人类评估中比 DALL-E 3 提高了 1.4 倍。我们的数据和代码可在 https://github.com/Enderfga/HumanRefiner 获取。||
|**2024-07-09**|[Maximum stress minimization via data-driven multifidelity topology design](http://arxiv.org/abs/2407.06746)|null|最大应力最小化问题是结构设计中最重要的课题之一。传统的基于梯度的拓扑优化方法需要通过松弛技术将原始问题转化为伪问题。由于其参数对优化有显著影响，因此，在不使用松弛技术的情况下，准确地解决最大应力最小化问题有望获得最佳性能。本文重点关注这一挑战，并研究与基于梯度的拓扑优化获得的解决方案相比，通过解决没有松弛技术的原始最大应力最小化问题，是否可以获得具有更多避免应力集中的设计。我们采用数据驱动的多保真度拓扑设计（MFTD），这是一种基于进化算法的无梯度拓扑优化。基本框架包括通过解决低保真度优化问题来生成候选解，通过高保真度正向分析评估这些解，并在没有灵敏度分析的情况下使用深度生成模型迭代更新它们。在本研究中，数据驱动的MFTD将通过使用p范数应力度量解决基于梯度的拓扑优化问题获得的优化设计纳入初始解，并基于具有贴体网格的高保真度分析来解决原始最大应力最小化问题。我们通过L型支架的基准测试证明了我们提出的方法的有效性。通过使用数据驱动的MFTD解决原始最大应力最小化问题，与初始解相比，在相同的最大应力值下，体积减少了高达22.6%。||
|**2024-07-09**|[Powerful and Flexible: Personalized Text-to-Image Generation via Reinforcement Learning](http://arxiv.org/abs/2407.06642)|**[link](https://github.com/wfanyue/dpg-t2i-personalization)**|个性化文本到图像模型允许用户为一个对象（由一组参考图像指定）生成不同风格的图像（由句子指定）。虽然基于扩散的生成模型已经取得了显著成果，但在扩散过程中，对象的视觉结构和细节经常发生意外的变化。一个主要原因是，这些基于扩散的方法在训练过程中通常采用简单的重建目标，这很难在生成图像和参考图像之间强制实现适当的结构一致性。为此，本文设计了一种新的强化学习框架，利用确定性策略梯度方法进行个性化文本到图像生成，该框架可以轻松地结合各种目标（可微分甚至不可微分）来监督扩散模型，从而提高生成图像的质量。在个性化文本到图像生成基准数据集上的实验结果表明，我们提出的方法在视觉保真度方面明显优于现有的最先进方法，同时保持了文本对齐。我们的代码可在以下网址获得：\url{https://github.com/wfanyue/DPG-T2I-Personalization}。||
|**2024-07-09**|[Ensembled Cold-Diffusion Restorations for Unsupervised Anomaly Detection](http://arxiv.org/abs/2407.06635)|**[link](https://github.com/snavalm/disyre)**|无监督异常检测 (UAD) 方法旨在通过将测试样本与从已知无异常的数据集中学习到的规范分布进行比较来识别异常。基于生成模型的方法通过生成无异常版本的测试图像来提供可解释性，但通常无法识别细微的异常。或者，使用特征建模或自监督方法（例如依赖于合成生成的异常的方法）不提供开箱即用的可解释性。在这项工作中，我们提出了一种结合了两种策略的优势的新方法：生成式冷扩散管道（即，使用不基于噪声的损坏的类似扩散的管道），该管道经过训练，目标是将合成损坏的图像恢复到正常、原始外观。为了支持我们的管道，我们引入了一种新的合成异常生成程序，称为 DAG，以及一种新的异常评分，它集成了以不同程度的异常为条件的恢复。我们的方法在三个不同的大脑 MRI 数据集中都超过了先前最先进的无监督异常检测水平。||
|**2024-07-09**|[Mobius: An High Efficient Spatial-Temporal Parallel Training Paradigm for Text-to-Video Generation Task](http://arxiv.org/abs/2407.06617)|null|受文本到图像（T2I）生成任务成功的启发，许多研究人员正致力于文本到视频（T2V）生成任务。大多数 T2V 框架通常继承自 T2I 模型，并添加额外的时序层训练以生成动态视频，这可以视为一项微调任务。然而，传统的 3D-Unet 是一种串行模式，时间层跟随空间层，根据其串行特征流，这将导致高 GPU 内存和训练时间消耗。我们认为，这种串行模式将随着大型扩散模型和海量数据集的出现而带来更高的训练成本，这不符合环保要求，也不利于 T2V 的发展。因此，我们提出了一种高效的时空并行训练范式，用于 T2V 任务，称为 Mobius。在我们提出的 3D-Unet 中，时间层和空间层是并行的，这优化了特征流和反向传播。Mobius 将节省 24% 的 GPU 内存和 12% 的训练时间，这可以极大地改善 T2V 微调任务，并为 AIGC 社区提供新的见解。我们将在未来发布我们的代码。||
|**2024-07-09**|[VQA-Diff: Exploiting VQA and Diffusion for Zero-Shot Image-to-3D Vehicle Asset Generation in Autonomous Driving](http://arxiv.org/abs/2407.06516)|null|从野外观察中生成 3D 车辆模型对于自动驾驶至关重要。现有的图像到 3D 方法不能很好地解决这个问题，因为它们仅仅从图像 RGB 信息中学习生成，而没有更深入地理解野外车辆（例如车型、制造商等）。这导致它们对具有遮挡或棘手视角的真实世界观察结果的零样本预测能力较差。为了解决这个问题，在这项工作中，我们提出了 VQA-Diff，这是一个利用野外车辆图像为自动驾驶创建逼真的 3D 车辆模型的新框架。VQA-Diff 利用视觉问答 (VQA) 模型中大型语言模型继承的真实世界知识进行鲁棒的零样本预测，并利用扩散模型中丰富的图像先验知识进行结构和外观生成。具体来说，我们利用多专家扩散模型策略生成结构信息，并采用主题驱动的结构控制生成机制对外观信息进行建模。因此，VQA-Diff 无需从现实世界中收集的大规模图像到 3D 车辆数据集中学习，仍然具有强大的零样本图像到新颖视图生成能力。我们在 Pascal 3D+、Waymo 和 Objaverse 等各种数据集上进行了实验，结果表明 VQA-Diff 在定性和定量上均优于现有的最先进方法。||
|**2024-07-05**|[Structural Constraint Integration in Generative Model for Discovery of Quantum Material Candidates](http://arxiv.org/abs/2407.04557)|null|已知的有机分子数以十亿计，但已发现的功能性无机材料却只占很小一部分，这对寻找新型量子材料的研究群体来说是一个特别突出的问题。基于机器学习的生成模型，尤其是扩散模型的最新进展，为生成新型稳定材料带来了巨大希望。然而，将几何模式融入材料生成仍然是一项挑战。在此，我们介绍了在生成模型中集成结构约束的方法 (SCIGEN)。我们的方法可以通过在每个扩散步骤之前，用扩散约束结构对去噪结构进行策略性掩蔽，从而将生成引导至约束输出，来修改任何经过训练的生成扩散模型。此外，我们从数学上证明了 SCIGEN 可以有效地从原始分布中执行条件采样，这对于生成稳定的约束材料至关重要。我们使用阿基米德格作为原型约束生成了 800 万种化合物，其中超过 10% 通过了多阶段稳定性预筛选。对 26,000 种存活化合物的 DFT（密度泛函理论）高通量计算表明，超过 50% 的化合物通过了 DFT 级别的结构优化。由于量子材料的性质与几何模式密切相关，我们的结果表明 SCIGEN 为生成量子材料候选材料提供了一个通用框架。||
|**2024-07-05**|[Unified continuous-time q-learning for mean-field game and mean-field control problems](http://arxiv.org/abs/2407.04521)|null|本文从代表性智能体的角度研究了平均场跳扩散模型中的连续时间q学习。为了克服无法直接观察到总体分布时的挑战，我们引入了解耦形式的集成q函数（解耦Iq函数），并建立了其与价值函数的鞅表征，这为平均场博弈（MFG）和平均场控制（MFC）问题提供了统一的策略评估规则。此外，根据求解MFG或MFC问题的任务，我们可以通过不同的方式利用解耦Iq函数来分别学习平均场均衡策略或平均场最优策略。因此，我们利用所有源于平均场交互的测试策略，设计了一种适用于MFG和MFC问题的统一q学习算法。对于跳扩散环境下的几个例子，包括LQ框架内外的例子，我们可以获得解耦Iq函数和价值函数的精确参数化，并从代表性智能体的角度说明我们的算法具有令人满意的性能。||
|**2024-07-05**|[Speed-accuracy trade-off for the diffusion models: Wisdom from nonequlibrium thermodynamics and optimal transport](http://arxiv.org/abs/2407.04495)|null|我们探讨了生成模型（称为扩散模型）与非平衡热力学中用于描述福克-普朗克方程的随机热力学之间的联系。基于随机热力学技术，我们推导出了扩散模型的速度-精度权衡，这是扩散模型中数据生成速度和精度之间的权衡关系。我们的结果表明，正向过程中的熵产生率会影响数据生成的误差。从随机热力学的角度来看，我们的结果为如何在扩散模型中最好地生成数据提供了定量见解。最佳学习方案由随机热力学中的保守力和最优传输理论中 2-Wasserstein 距离的空间测地线引入。我们用数值方法说明了具有不同噪声方案（如余弦方案、条件最优传输和最优传输）的扩散模型的速度-精度权衡的有效性。||
|**2024-07-05**|[PROUD: PaRetO-gUided Diffusion Model for Multi-objective Generation](http://arxiv.org/abs/2407.04493)|null|深度生成模型领域的最新进展集中于生成满足多个期望属性的样本。然而，普遍的方法是独立优化这些属性函数，从而忽略了它们之间的权衡。此外，属性优化通常没有被恰当地整合到生成模型中，导致生成质量（即生成样本的质量）的无必要妥协。为了解决这些问题，我们提出了一个约束优化问题。它寻求在确保生成样本位于多个属性目标的帕累托前沿的同时优化生成质量。这样的公式能够生成在相互冲突的属性函数上无法同时进一步改进的样本，并保持生成样本的良好质量。在此公式的基础上，我们引入了帕累托引导扩散模型 (PROUD)，其中去噪过程中的梯度被动态调整以提高生成质量，同时生成样本遵循帕累托最优性。在图像生成和蛋白质生成任务上的实验评估表明，与各种基线相比，我们的 PROUD 在逼近多个属性函数的帕累托最优性的同时，始终保持着卓越的生成质量。||
|**2024-07-05**|[VCD-Texture: Variance Alignment based 3D-2D Co-Denoising for Text-Guided Texturing](http://arxiv.org/abs/2407.04461)|null|最近关于三维形状纹理合成的研究极大地受益于快速发展的二维文本到图像的扩散模型，包括基于修复和基于优化的方案。然而，这些方法忽略了二维扩散模型和三维物体之间的模态差距，它们主要将三维物体渲染成二维图像并分别对每个图像进行纹理处理。在本文中，我们重新审视了纹理合成，并提出了一个基于方差对齐的三维-二维协同去噪框架，称为VCD-Texture，以解决这些问题。具体来说，我们首先在具有重新投影的三维注意力感受野的扩散自注意力模块中统一了二维和三维潜在特征学习。随后，将去噪后的多视图二维潜在特征聚合到三维空间，然后将其光栅化回二维空间，从而形成更一致的二维预测。然而，光栅化过程存在难以处理的方差偏差，我们提出的方差对齐从理论上解决了这个问题，实现了高保真纹理合成。此外，我们还提出了一种修复细化方法，以进一步改善存在冲突区域的细节。值得注意的是，目前还没有公开可用的基准来评估纹理合成，这阻碍了其发展。因此，我们构建了一个基于三个开源三维数据集的新评估集，并建议使用四个指标来全面验证纹理性能。综合实验表明，VCD-Texture相较于其他方法取得了优越的性能。||
|**2024-07-05**|[Benchmarking structure-based three-dimensional molecular generative models using GenBench3D: ligand conformation quality matters](http://arxiv.org/abs/2407.04424)|**[link](https://github.com/bbaillif/genbench3d)**|三维 (3D) 深度分子生成模型提供了基于 3D 依赖属性的目标导向生成的优势，例如结合腔内基于结构的设计的结合亲和力。 为评估 SMILES 或分子图生成器而创建的传统基准，例如 GuacaMol 或 MOSES，由于它们不评估生成的分子构象的质量，因此在评估 3D 生成器方面受到限制。 因此，我们在这项工作中开发了 GenBench3D，它实现了一个用于在结合腔内生成分子的新基准。 我们的主要贡献是 Validity3D 指标，它使用基于剑桥结构数据库中观察到的参考值的键长和价角的可能性来评估构象质量。 对 LiGAN、3D-SBDD、Pocket2Mol、TargetDiff、DiffSBDD 和 ResGen 模型进行了基准测试。 我们发现只有 0% 到 11% 的生成分子具有有效的构象。 对口袋中生成的分子进行局部弛豫，通过至少增加 40% 的 Validity3D，大大提高了所有模型的 Validity3D。 对于 LiGAN、3D-SBDD 或 TargetDiff，有效弛豫分子集显示的平均 Vina 分数（即更差）高于原始生成分子集，表明原始生成分子的结合亲和力可能被高估了。 使用其他评分函数（更重视配体应变）仅在使用有效的弛豫分子时才会产生改进的分数。 使用有效的弛豫分子，TargetDiff 和 Pocket2Mol 显示出比其他模型更好的中位 Vina、Glide 和 Gold PLP 分数。 我们已经在 GitHub 上公开发布了 GenBench3D 以供更广泛地使用：https://github.com/bbaillif/genbench3d||
|**2024-07-05**|[Improving Audio Generation with Visual Enhanced Caption](http://arxiv.org/abs/2407.04416)|null|生成模型在音频生成任务中取得了显著成果，但现有模型难以处理复杂和详细的提示，导致潜在的性能下降。我们假设这个问题源于训练数据的低质量和相对较小的数量。在这项工作中，我们的目标是创建一个带有丰富描述的大规模音频数据集，用于改进音频生成模型。我们开发了一个自动化管道，通过使用大型语言模型 (LLM) 将预测的视觉字幕、音频字幕和标签标签转换为全面的描述，从而为视听数据集生成详细的字幕。我们介绍了 Sound-VECaps，这是一个包含 166 万个高质量音频-字幕对的数据集，其中包含丰富的细节，包括音频事件顺序、发生地点和环境信息。我们证明，使用 Sound-VECaps 进行训练可以显著增强文本到音频生成模型理解和从复杂输入提示生成音频的能力，从而提高整体系统性能。此外，我们对 Sound-VECaps 在多个音频-语言任务中进行了消融研究，表明其在推进音频-文本表示学习方面的潜力。我们的数据集和模型可在网上获得。||
|**2024-07-05**|[Unsupervised Learning of Category-Level 3D Pose from Object-Centric Videos](http://arxiv.org/abs/2407.04384)|**[link](https://github.com/genintel/uns-obj-pose3d)**|类别级三维姿态估计是计算机视觉和机器人技术中的一个基本问题，例如对于具身代理或训练三维生成模型。然而，到目前为止，估计类别级物体姿态的方法要么需要大量的人工标注、CAD模型，要么需要来自RGB-D传感器的输入。相比之下，我们致力于解决仅从随意拍摄的以物体为中心的视频中学习估计类别级三维姿态的问题，无需人工监督。我们提出了一个两步流程：首先，我们引入了一种多视图对齐程序，该程序使用一种新颖且鲁棒的循环距离公式来确定视频之间的规范相机姿态，该公式使用重建的粗网格和DINOv2特征进行几何和外观匹配。其次，规范姿态和重建的网格使我们能够从单个图像中训练用于三维姿态估计的模型。具体来说，我们的模型通过预测二维图像中每个像素的模板网格中对应顶点的特征向量，来学习估计图像和原型三维模板之间的密集对应关系。我们证明，我们的方法在以物体为中心的视频的无监督对齐方面大大优于所有基线，并在实际应用中提供了可靠且鲁棒的预测。我们的代码和数据可在https://github.com/GenIntel/uns-obj-pose3d获取。||
|**2024-07-05**|[A Mapping Strategy for Interacting with Latent Audio Synthesis Using Artistic Materials](http://arxiv.org/abs/2407.04379)|null|本文提出了一种与生成式人工智能模型的潜在空间交互的映射策略。我们的方法涉及使用无监督特征学习对人类控制空间进行编码，并将其映射到音频合成模型的潜在空间。为了演示这种映射策略如何将高维传感器数据转化为深度生成模型的控制机制，我们提出了一个概念验证系统，该系统使用视觉草图来控制音频合成模型。我们借鉴 XAIxArts 中的新兴论述来讨论这种方法如何为艺术和创意环境中的 XAI 做出贡献，我们还讨论了它目前的局限性并提出了未来的研究方向。||
|**2024-07-05**|[MuseBarControl: Enhancing Fine-Grained Control in Symbolic Music Generation through Pre-Training and Counterfactual Loss](http://arxiv.org/abs/2407.04331)|null|自动生成符号音乐——根据特定人类需求量身定制的乐谱——对音乐家和爱好者来说非常有利。最近的研究表明，使用大型数据集和先进的Transformer架构取得了可喜的成果。然而，这些最先进的模型通常只对整首作品的节奏和风格等方面提供基本的控制，缺乏管理更精细细节的能力，例如在单个小节级别的控制。虽然微调预先训练好的符号音乐生成模型似乎是实现这种更精细控制的直接方法，但我们的研究表明这种方法存在挑战。该模型通常无法对新的、细粒度的小节级控制信号做出充分响应。为了解决这个问题，我们提出了两个创新的解决方案。首先，我们引入了一个预训练任务，旨在将控制信号直接与其相应的音乐符号链接起来，这有助于为后续的微调实现更有效的初始化。其次，我们实施了一种新的反事实损失，以促进生成的音乐与控制提示之间更好地保持一致。总的来说，这些技术显著增强了我们在小节级别控制音乐生成的能力，比传统方法提高了13.06%。我们的主观评价也证实，这种增强的控制并没有损害原始预训练生成模型的音乐质量。||
|**2024-07-03**|[DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents](http://arxiv.org/abs/2407.03300)|null|扩散模型 (DM) 为生成式学习带来了革命性的变化。它们利用扩散过程将数据编码为简单的 Gaussian 分布。然而，将复杂且可能具有多模态的数据分布编码为单个连续 Gaussian 分布无疑是一个不必要的具有挑战性的学习问题。我们提出离散-连续潜在变量扩散模型 (DisCo-Diff)，通过引入互补的离散潜在变量来简化此任务。我们使用可学习的离散潜在变量增强 DM，并使用编码器进行推断，并对 DM 和编码器进行端到端训练。DisCo-Diff 不依赖于预先训练的网络，这使得该框架具有普遍适用性。离散潜在变量通过降低 DM 生成 ODE 的曲率，显著简化了学习 DM 复杂噪声到数据映射的过程。一个额外的自回归 Transformer 模型对离散潜在变量的分布进行建模，这是一个简单的步骤，因为 DisCo-Diff 只需要具有少量码本的少量离散变量。我们在玩具数据、多个图像合成任务以及分子对接上验证了 DisCo-Diff，发现引入离散潜在变量始终可以提高模型性能。例如，DisCo-Diff 在使用 ODE 采样器的类条件 ImageNet-64/128 数据集上实现了最先进的 FID 分数。||
|**2024-07-03**|[Improved Noise Schedule for Diffusion Training](http://arxiv.org/abs/2407.03297)|null|扩散模型已成为生成视觉信号的首选方法。然而，训练单个模型来预测不同级别的噪声提出了重大挑战，需要多次迭代并导致巨大的计算成本。为了加快收敛速度，人们引入了各种方法，例如损失加权策略设计和架构改进。在本研究中，我们提出了一种设计噪声调度的新方法，以增强扩散模型的训练。我们的主要见解是，对数信噪比（logSNR）的重要性采样（理论上等效于修改后的噪声调度）对于提高训练效率特别有利，特别是在增加 $\log \text{SNR}=0$ 附近的采样频率时。我们通过经验证明了我们的噪声调度优于标准余弦调度。此外，我们还重点介绍了我们的噪声调度设计在 ImageNet 基准测试中的优势，表明所设计的调度始终有利于不同的预测目标。||
|**2024-07-03**|[Spatio-Temporal Adaptive Diffusion Models for EEG Super-Resolution in Epilepsy Diagnosis](http://arxiv.org/abs/2407.03089)|null|脑电图 (EEG) 技术，特别是高密度脑电图 (HD EEG) 设备，广泛应用于神经科学等领域。HD EEG 设备通过在头皮上放置更多电极来提高 EEG 的空间分辨率，满足癫痫病灶定位等临床诊断应用的要求。然而，该技术面临着采集成本高、使用场景有限等挑战。本文提出了时空自适应扩散模型 (STADM)，率先利用扩散模型实现从低分辨率 (LR, 64 通道或更少) EEG 到高分辨率 (HR, 256 通道) EEG 的空间超分辨率 (SR) 重建。具体而言，设计了一种时空条件模块来提取 LR EEG 的时空特征，然后将其作为条件输入来指导扩散模型的反向去噪过程。此外，构建了一个多尺度 Transformer 去噪模块，利用多尺度卷积块和基于交叉注意力的扩散 Transformer 块进行条件引导，生成自适应于受试者的 SR EEG。实验结果表明，该方法有效提高了 LR EEG 的空间分辨率，并在数量上优于现有方法。此外，STADM 通过将合成的 SR EEG 应用于癫痫患者的分类和源定位任务，证明了其价值，表明其具有显著提高 LR EEG 空间分辨率的潜力。||
|**2024-07-03**|[Artificial Inductive Bias for Synthetic Tabular Data Generation in Data-Scarce Scenarios](http://arxiv.org/abs/2407.03080)|null|虽然使用深度生成模型 (DGM) 生成合成表格数据为数据稀缺和隐私问题提供了一种引人注目的解决方案，但其有效性依赖于大量的训练数据，而这些数据在现实应用中通常不可用。本文提出了一种新颖的方法，用于在有限的真实数据环境中使用 DGM 生成真实可靠的合成表格数据，从而解决了这一挑战。我们的方法提出了几种通过迁移学习和元学习技术在 DGM 中生成人工归纳偏差的方法。我们在该框架内探索并比较了四种不同的方法，证明了预训练和模型平均等迁移学习策略优于模型无关元学习和域随机搜索等元学习方法。我们使用两种最先进的 DGM（变分自动编码器和生成对抗网络）验证了我们的方法，表明我们的人工归纳偏差提高了合成数据的质量（通过 Jensen-Shannon 散度衡量），在使用我们提出的方法时，相对增益高达 50%。这种方法在各种 DGM 和机器学习任务中具有广泛的适用性，特别是在医疗保健和金融等数据稀缺通常是关键问题的领域。||
|**2024-07-03**|[Electromagnetic Property Sensing Based on Diffusion Model in ISAC System](http://arxiv.org/abs/2407.03075)|null|集成传感与通信 (ISAC) 为未来的无线系统开辟了许多颠覆性的机遇。在本文中，我们开发了一种新颖的 ISAC 方案，利用扩散模型来感知预定传感区域中目标的电磁 (EM) 特性。具体来说，我们首先利用从目标反射回来的通信和传感信号来估计传感信道。然后，我们采用扩散模型生成代表目标的点云，从而实现目标电磁特性分布的 3D 可视化。为了最小化真实点云和估计点云之间的平均 Chamfer 距离 (MCD)，我们在最大发射功率和每个用户设备 (UE) 的最小通信可达速率的约束下，进一步设计了通信和传感波束赋形矩阵。仿真结果证明了该方法在实现目标形状、相对介电常数和电导率的高质量重建方面的有效性。此外，该方法可以在传感区域的任何位置有效地感知目标的电磁特性。||
|**2024-07-03**|[Semantic-Aware Power Allocation for Generative Semantic Communications with Foundation Models](http://arxiv.org/abs/2407.03050)|null|扩散模型的最新进展为生成建模带来了重大突破。生成模型与语义通信 (SemCom) 的结合能够以超低速率实现高保真语义信息交换。本文提出了一种用于图像任务的新型生成式 SemCom 框架，其中预训练的基础模型分别充当语义编码器和解码器，用于语义特征提取和图像再生。文章对传输可靠性与再生图像的感知质量以及语义特征的语义值之间的数学关系进行了建模，这些关系是通过对 Kodak 数据集进行数值模拟获得的。我们还研究了语义感知功率分配问题，目标是在保证语义性能的同时最小化总功耗。为了解决这个问题，分别通过约束解耦和二分搜索提出了两种语义感知功率分配方法。数值结果表明，与传统方法相比，所提出的语义感知方法在总功耗方面表现出优越的性能。||
|**2024-07-03**|[SlerpFace: Face Template Protection via Spherical Linear Interpolation](http://arxiv.org/abs/2407.03043)|null|当代人脸识别系统使用从人脸图像中提取的特征模板来识别身份。为了增强隐私性，人脸模板保护技术被广泛用于隐藏存储在模板中的敏感身份和外观信息。本文识别了一种新兴的利用扩散模型的隐私攻击形式，它可以使先前的保护无效，称为反演攻击。这种攻击可以从模板中合成高质量、保留身份的人脸图像，从而暴露人的外貌。基于对扩散模型生成能力的研究，本文提出了一种防御措施来削弱这种攻击，即通过将模板旋转到类似噪声的分布。这是通过在其所在的超球面上对模板进行球面和线性插值（slerp）来有效实现的。为了增强旋转模板的不可逆性，本文进一步提出对模板的特征维度进行分组划分和丢弃。组的划分和每个组内的丢弃以有利于识别的的方式学习。所提出的技术被具体化为一种新的人脸模板保护技术，SlerpFace。大量实验表明，SlerpFace 提供了令人满意的识别精度和全面的隐私保护，可以抵御反演和其他攻击形式，优于现有技术。||
|**2024-07-03**|[An Organism Starts with a Single Pix-Cell: A Neural Cellular Diffusion for High-Resolution Image Synthesis](http://arxiv.org/abs/2407.03018)|null|生成模型旨在逼近真实数据的统计特性，从而能够合成与原始分布非常相似的新数据。生成对抗网络 (GAN) 和去噪扩散概率模型 (DDPM) 代表了生成模型的重大进步，它们分别从博弈论和热力学中汲取灵感。然而，从生物进化的角度探索生成模型在很大程度上仍未得到开发。在本文中，我们介绍了一种称为生成元胞自动机 (GeCA) 的新型模型系列，其灵感来自于生物体从单细胞的进化。GeCA 被评估为一种有效的视网膜疾病分类增强工具，可用于两种成像模式：眼底和光学相干断层扫描 (OCT)。在 OCT 成像中，数据稀缺且类别分布存在固有的偏差，GeCA 显着提高了 11 种不同眼科疾病的表现，与传统基线相比，平均 F1 分数提高了 12%。在类似的参数约束下，GeCA 的性能优于包含 UNet 或基于 Transformer 的最新去噪模型的扩散方法。代码可在以下网址获取：https://github.com/xmed-lab/GeCA。||
|**2024-07-03**|[Frequency-Controlled Diffusion Model for Versatile Text-Guided Image-to-Image Translation](http://arxiv.org/abs/2407.03006)|**[link](https://github.com/xianggao1102/fcdiffusion)**|近年来，大规模文本到图像 (T2I) 扩散模型已成为一种强大的图像到图像转换 (I2I) 工具，允许通过用户提供的文本提示进行开放域图像转换。本文提出了频率控制扩散模型 (FCDiffusion)，这是一个基于扩散的端到端框架，从频域角度为文本引导的 I2I 提供了一种新颖的解决方案。我们框架的核心是一个基于离散余弦变换的特征空间频域滤波模块，它在 DCT 域中滤波源图像的潜在特征，产生具有不同 DCT 谱带的滤波图像特征，作为预训练的潜在扩散模型的不同控制信号。我们发现，不同 DCT 谱带的控制信号在不同的相关性（例如，风格、结构、布局、轮廓等）上桥接了源图像和 T2I 生成的图像，从而使多功能 I2I 应用能够强调不同的 I2I 相关性，包括风格引导的内容创建、图像语义操作、图像场景转换和图像风格转换。与相关方法不同，FCDiffusion 建立了一个统一的文本引导 I2I 框架，只需在推理时切换不同的频率控制分支，即可适用于各种图像转换任务。广泛的定性和定量实验都证明了我们的方法在文本引导 I2I 方面的有效性和优越性。代码公开于：https://github.com/XiangGao1102/FCDiffusion。||
|**2024-07-03**|[Towards a Scalable Reference-Free Evaluation of Generative Models](http://arxiv.org/abs/2407.02961)|**[link](https://github.com/aziksh-ospanov/fkea)**|虽然生成模型的标准评估分数大多是基于参考的，但由于缺乏适用的参考数据集，对生成模型进行依赖参考的评估通常很困难。最近，人们提出了无参考的熵分数 VENDI 和 RKE 来评估生成数据的多样性。然而，从数据中估计这些分数会导致大规模生成模型的计算成本很高。在这项工作中，我们利用随机傅里叶特征框架来降低计算成本，并提出了基于傅里叶的核熵逼近 (FKEA) 方法。我们利用 FKEA 对核矩阵的近似特征谱来有效地估计上述熵分数。此外，我们展示了 FKEA 代理特征向量的应用，以揭示该方法在评估生成样本多样性时识别的模式。我们提供了 FKEA 评估算法的随机实现，其复杂度为 $O(n)$，随样本大小 $n$ 线性增长。我们广泛评估了 FKEA 在标准图像、文本和视频数据集中的数值性能。我们的实验结果表明，该方法应用于大规模生成模型具有可扩展性和可解释性。代码库可在 https://github.com/aziksh-ospanov/FKEA 获取。||

<p align=right>(<a href=#updated-on-20240717>back to top</a>)</p>

## LLM

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-07-13**|[Minimizing PLM-Based Few-Shot Intent Detectors](http://arxiv.org/abs/2407.09943)|null|近期研究表明，基于预训练语言模型（PLM）并在有限的标注数据上训练高效的意图检测器是可行的。然而，由于模型规模庞大，在移动设备等资源受限的环境中部署这些检测器面临挑战。为了解决这个问题，本工作致力于探索在少量样本数据上训练的基于PLM的意图检测器的模型压缩技术。具体来说，我们利用大型语言模型（LLM）进行数据增强，采用先进的模型压缩方法进行知识蒸馏，并设计了一种称为V-Prune的词汇剪枝机制。通过这些方法，我们成功地将模型内存使用量（包括Transformer和词汇表）压缩了21倍，同时在四个真实世界基准测试中保持了几乎相同的性能水平。|
|**2024-07-12**|[Deep Bag-of-Words Model: An Efficient and Interpretable Relevance Architecture for Chinese E-Commerce](http://arxiv.org/abs/2407.09395)|null|查询和产品的文本相关性或文本匹配是电子商务搜索引擎的一项基本技术，其目的是确保展示的产品能够匹配查询的意图。许多研究致力于提高搜索系统中相关性模型的性能。最近，像BERT这样的预训练语言模型在文本相关性任务上取得了可喜的性能。虽然这些模型在离线测试数据集上表现良好，但由于其高延迟，将预训练语言模型部署到在线系统仍然存在障碍。双塔模型因其能够协调性能和计算效率而被广泛应用于工业场景。遗憾的是，此类模型呈现出不透明的“黑盒”性质，这阻碍了开发人员进行专门的优化。在本文中，我们提出了一种深度词袋（DeepBoW）模型，这是一种高效且可解释的中文电子商务相关性架构。我们的方法建议将查询和产品编码为稀疏的词袋表示，这是一组词-权重对。权重表示对应词与原始文本之间的重要性或相关性得分。相关性得分是通过计算查询和产品的稀疏词袋表示之间匹配词的累积来衡量的。与通常存在黑盒缺点的流行的密集分布式表示相比，所提出的表示模型的最大优势是高度可解释和可干预，这对在线搜索引擎的部署和运营来说是一个优越的优势。此外，该模型的在线效率甚至优于最有效的密集表示内积形式...|
|**2024-07-12**|[One Stone, Four Birds: A Comprehensive Solution for QA System Using Supervised Contrastive Learning](http://arxiv.org/abs/2407.09011)|null|本文提出了一种新颖且全面的解决方案，通过监督对比学习 (SCL) 来提高问答 (QA) 系统的鲁棒性和效率。借助预训练语言模型，训练高性能问答系统变得非常简单，只需要少量数据和简单的微调。然而，尽管最近取得了进展，但现有的问答系统在功能性和训练效率方面仍然存在重大缺陷。我们通过定义四个关键任务来解决功能性问题：用户输入意图分类、域外输入检测、新意图发现和持续学习。然后，我们利用基于 SCL 的统一表示学习方法，有效地构建类内紧凑和类间分散的特征空间，促进已知意图分类和未知意图检测和发现。因此，只需对下游任务进行最少的额外调整，我们的方法就能显著提高模型效率，并在所有任务中实现新的最先进性能。|
|**2024-07-12**|[Domain-Hierarchy Adaptation via Chain of Iterative Reasoning for Few-shot Hierarchical Text Classification](http://arxiv.org/abs/2407.08959)|null|近年来，各种预训练语言模型（PLM）被提出，并在各种少样本任务上展现出令人印象深刻的性能。然而，受限于 PLM 中非结构化的先验知识，在复杂结构化场景（如层次文本分类 (HTC)）中难以保持一致的性能，尤其是在下游数据极其稀缺的情况下。主要的挑战是如何将 PLM 中的非结构化语义空间迁移到下游领域层次结构中。与之前关于 HTC 的工作（直接执行多标签分类或使用图神经网络 (GNN) 注入标签层次结构）不同，在这项工作中，我们研究了少样本设置下的 HTC 问题，以使 PLM 中的知识从非结构化方式适应下游层次结构。具体来说，我们设计了一种简单有效的方法，称为层次迭代条件随机场 (HierICRF)，以搜索最具领域挑战性的方向，并将领域层次结构适应巧妙地构建为层次迭代语言建模问题，然后鼓励模型在推理过程中进行层次一致性自我修正，从而实现知识迁移并保持层次一致性。我们在各种架构上执行了 HierICRF，并在两个流行的 HTC 数据集上进行了广泛的实验，结果表明，与之前最先进的 (SOTA) 基线相比，使用 HierICRF 的提示在少样本设置下显着提高了 HTC 性能，平均 Micro-F1 提高了 28.80% 到 1.50%，Macro-F1 提高了 36.29% 到 1.5%，同时保持了 SOTA 层次一致性性能。|
|**2024-07-11**|[Adversarial-MidiBERT: Symbolic Music Understanding Model Based on Unbias Pre-training and Mask Fine-tuning](http://arxiv.org/abs/2407.08306)|**[link](https://github.com/RS2002/Adversarial-MidiBERT)**|作为音乐信息检索 (MIR) 的重要组成部分，符号音乐理解 (SMU)  受到了广泛关注，因为它可以帮助音乐家和业余爱好者学习和创作音乐。近年来，预训练语言模型在 SMU 中得到广泛应用，因为符号音乐与自然语言具有高度相似性，并且预训练方式还有助于充分利用有限的音乐数据。然而，在预训练语言模型中观察到性别歧视、年龄歧视和种族主义等偏见问题，这归因于训练数据分布不均衡。它还对下游任务的性能产生重大影响，这在 SMU 中也会发生。为了应对这一挑战，我们提出了 Adversarial-MidiBERT，一种基于 Transformers 双向编码器表示 (BERT) 的符号音乐理解模型。我们引入了一种基于对抗学习的无偏见预训练方法，以最大程度地减少训练期间导致偏见的标记的参与。此外，我们提出了一种掩码微调方法来缩小预训练和微调之间的数据差距，这可以帮助模型更快地收敛并获得更好的性能。我们在四个音乐理解任务上评估了我们的方法，我们的方法在所有任务中都表现出色。我们模型的代码可在 https://github.com/RS2002/Adversarial-MidiBERT 公开获取。|
|**2024-07-10**|[Deconstructing What Makes a Good Optimizer for Language Models](http://arxiv.org/abs/2407.07972)|null|随着模型规模的扩大，训练语言模型的成本越来越高，这促使人们尝试提高优化效率。尽管付出了这些努力，但 Adam 优化器仍然是使用最广泛的，因为它被普遍认为是最有效的方法。我们的目标是在自回归语言建模的背景下，比较几种优化算法，包括 SGD、Adafactor、Adam 和 Lion，涵盖各种模型大小、超参数和架构变体。我们的研究结果表明，除了 SGD 之外，这些算法在最佳性能和面对各种超参数选择时的表现方面都具有可比性。我们的结果表明，优化器的选择可以根据实际考虑因素进行，例如内存限制和易于实现，因为没有一种算法在性能或对超参数错误指定的稳定性方面明显优于其他算法。鉴于我们的发现，我们进一步剖析了这些方法，检查了 Adam 的两个简化版本：a) 符号动量（Signum），我们发现它恢复了 Adam 的性能和超参数稳定性；b) Adalayer，Adam 的分层变体，我们引入它来研究 Adam 的预处理。对 Adalayer 的研究使我们得出结论，Adam 预处理的最大影响仅限于最后一层和 LayerNorm 参数，而且，也许令人惊讶的是，其余层可以使用 SGD 进行训练。|
|**2024-07-09**|[NoisyAG-News: A Benchmark for Addressing Instance-Dependent Noise in Text Classification](http://arxiv.org/abs/2407.06579)|null|现有的噪声标签学习研究主要集中于合成标签噪声。虽然合成噪声具有明确定义的结构特性，但它往往无法准确地复制现实世界中的噪声模式。近年来，人们一直在努力构建用于图像分类的通用且可控的实例依赖噪声数据集，这极大地促进了该领域抗噪声学习的发展。然而，关于文本分类中噪声标签学习的研究仍然很少。为了更好地了解现实世界文本分类环境中的标签噪声，我们通过人工标注构建了基准数据集 NoisyAG-News。首先，我们分析了标注数据，以收集有关现实世界噪声的观察结果。我们定性和定量地证明了现实世界的噪声标签遵循实例依赖模式。随后，我们使用预训练语言模型和噪声处理技术，对 NoisyAG-News 及其相应的合成噪声数据集进行了全面的学习实验。我们的研究结果表明，虽然预训练模型对合成噪声具有弹性，但它们在实例依赖噪声面前表现不佳，不同混淆程度的样本在训练和测试过程中表现出不一致的性能。这些现实世界的噪声模式带来了新的、重大的挑战，促使人们重新评估噪声标签处理方法。我们希望 NoisyAG-News 将促进未来噪声标签学习解决方案的开发和评估。|
|**2024-07-09**|[Enhancing Low-Resource NMT with a Multilingual Encoder and Knowledge Distillation: A Case Study](http://arxiv.org/abs/2407.06538)|null|神经机器翻译（NMT）仍然是一项艰巨的挑战，尤其是在处理低资源语言时。预训练的序列到序列（seq2seq）多语言模型，例如 mBART-50，已在各种低资源 NMT 任务中表现出令人印象深刻的性能。然而，它们的预训练仅限于 50 种语言，不支持许多低资源语言，尤其是印度次大陆的语言。扩展 mBART-50 的语言支持需要复杂的预训练，由于灾难性遗忘，可能会导致性能下降。考虑到这些不断扩大的挑战，本文探索了一个框架，该框架利用预训练语言模型的优势以及 seq2seq 架构中的知识蒸馏来促进低资源语言的翻译，包括 mBART-50 未涵盖的语言。所提出的框架采用基于多语言编码器的 seq2seq 模型作为基础架构，并随后使用互补的知识蒸馏技术来减轻不平衡训练的影响。我们的框架在四个印度语到印度语方向上的三种低资源印度语上进行了评估，与基线相比，BLEU-4 和 chrF 得到了显着提高。此外，我们进行人工评估以确认我们方法的有效性。我们的代码可在 https://github.com/raypretam/Two-step-low-res-NMT 公开获取。|
|**2024-07-07**|[Advancing Prompt Recovery in NLP: A Deep Dive into the Integration of Gemma-2b-it and Phi2 Models](http://arxiv.org/abs/2407.05233)|null|Prompt recovery, a crucial task in natural language processing, entails the reconstruction of prompts or instructions that language models use to convert input text into a specific output. Although pivotal, the design and effectiveness of prompts represent a challenging and relatively untapped field within NLP research. This paper delves into an exhaustive investigation of prompt recovery methodologies, employing a spectrum of pre-trained language models and strategies. Our study is a comparative analysis aimed at gauging the efficacy of various models on a benchmark dataset, with the goal of pinpointing the most proficient approach for prompt recovery. Through meticulous experimentation and detailed analysis, we elucidate the outstanding performance of the Gemma-2b-it + Phi2 model + Pretrain. This model surpasses its counterparts, showcasing its exceptional capability in accurately reconstructing prompts for text transformation tasks. Our findings offer a significant contribution to the existing knowledge on prompt recovery, shedding light on the intricacies of prompt design and offering insightful perspectives for future innovations in text rewriting and the broader field of natural language processing.|
|**2024-07-06**|[The Solution for the AIGC Inference Performance Optimization Competition](http://arxiv.org/abs/2407.04991)|null|近年来，基于Transformer架构的大规模预训练语言模型的快速发展为自然语言处理任务带来了革命性的变化。其中，ChatGPT因其具备人类水平的对话能力而广受欢迎，到2022年底已吸引超过1亿月活跃用户。与此同时，百度对其文心模型的商业化部署也通过人工智能驱动技术显著提升了营销效果。本文重点研究如何优化文心模型的高性能推理，着重于GPU加速和利用Paddle推理框架。我们采用了一系列技术，例如Faster Transformer以实现高效的模型处理，嵌入层剪枝以减少计算开销，以及FP16半精度推理以提高计算效率。此外，我们的方案还集成了高效的数据处理策略，采用多进程并行处理以最大程度地减少延迟。实验结果表明，与标准方法相比，我们优化的解决方案在推理速度上实现了高达8.96倍的提升，同时保持了具有竞争力的性能。|
|**2024-07-03**|[MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language Models](http://arxiv.org/abs/2407.02775)|null|知识蒸馏是一种有效的预训练语言模型压缩技术。虽然现有的知识蒸馏方法对于最典型的模型BERT表现良好，但它们在两个方面仍有提升空间：可以进一步探索关系级知识以提高模型性能；学生注意力头的设置可以更加灵活，以减少推理时间。因此，我们提出了一种新的知识蒸馏方法MLKD-BERT，用于在师生框架中提取多级知识。在GLUE基准测试和抽取式问答任务上的大量实验表明，我们的方法优于BERT上最先进的知识蒸馏方法。此外，MLKD-BERT可以灵活设置学生注意力头的数量，从而在性能下降很小的情况下大幅减少推理时间。|
|**2024-07-03**|[Supporting Cross-language Cross-project Bug Localization Using Pre-trained Language Models](http://arxiv.org/abs/2407.02732)|null|自动定位大型代码库中的错误对于开发人员来说仍然是一项重大挑战。现有技术由于依赖于应用程序特定的数据和庞大的模型规模，因此在通用性和部署方面常常遇到困难。本文提出了一种新颖的基于预训练语言模型 (PLM) 的错误定位技术，该技术超越了项目和语言的界限。我们的方法利用对比学习来增强错误报告和源代码的表示。然后，它利用一种结合了提交消息和代码段的新型排序方法。此外，我们引入了一种知识蒸馏技术，可以在不影响性能的情况下减小模型规模，以便实际部署。本文提出了几个关键优势。通过将代码段和提交消息分析与传统的代码文件级别检查相结合，我们的技术实现了更高的错误定位精度。此外，我们的模型在通用性方面表现出色——在来自各种项目和语言的代码上进行训练后，它可以有效地识别未见过的代码库中的错误。为了解决计算限制，我们提出了一种兼容 CPU 的解决方案。总而言之，我们提出的工作提出了一种高效、通用且高效的错误定位技术，具有现实部署的潜力。||
|**2024-07-02**|[Ensemble of pre-trained language models and data augmentation for hate speech detection from Arabic tweets](http://arxiv.org/abs/2407.02448)|null|如今，从阿拉伯语推文中识别仇恨言论引起了众多研究者的关注。为了解决这一分类任务，人们已经开发了许多系统和技术。然而，在这方面面临的两大挑战是有限的性能和数据不平衡问题。在本研究中，我们提出了一种新方法，利用集成学习和基于先前手动标记的半监督学习。我们通过将阿拉伯语推文分为5个不同的类别：非仇恨、一般仇恨、种族歧视、宗教歧视或性别歧视，在一个基准数据集上进行了实验。实验结果表明：(1) 基于预训练语言模型的集成学习优于现有的相关工作；(2) 我们提出的数据增强方法提高了阿拉伯语推文中仇恨言论检测的准确率，并优于现有的相关工作。我们的主要贡献是在阿拉伯语仇恨言论检测方面取得了令人鼓舞的结果。||
|**2024-07-02**|[Efficient Nearest Neighbor based Uncertainty Estimation for Natural Language Processing Tasks](http://arxiv.org/abs/2407.02138)|null|深度神经网络 (DNN) 包括预训练语言模型 (PLM) 中的可信预测对于现实世界中安全关键型应用至关重要。然而，DNN 经常面临不确定性估计问题，例如校准错误。特别地，需要多次随机推理的方法可以缓解这个问题，但推理成本高昂，使其不切实际。在本研究中，我们提出了 $k$近邻不确定性估计（$k$NN-UE），这是一种利用来自邻居的距离和邻居的标签存在率进行不确定性估计的方法。在情感分析、自然语言推理和命名实体识别方面的实验表明，我们提出的方法在置信度校准、选择性预测和分布外检测方面优于基线或最近基于密度的方法。此外，我们的分析表明，引入降维或受最近$k$ NN-LM 研究启发的近似最近邻搜索，可以在不显著降低估计性能的情况下，通过适当组合来减少推理开销。||
|**2024-07-01**|[Bridging the Gap: Transfer Learning from English PLMs to Malaysian English](http://arxiv.org/abs/2407.01374)|null|马来西亚英语是一种低资源的混合语，除了标准英语之外，它还包含马来语、汉语和泰米尔语的元素。命名实体识别 (NER) 模型在从马来西亚英语文本中捕获实体时表现不佳，因为它具有独特的形态句法适应、语义特征和语码转换（混合英语和马来语）。考虑到这些差距，我们引入了 MENmBERT 和 MENBERT，这是一种具有上下文理解能力的预训练语言模型，专为马来西亚英语量身定制。我们使用来自马来西亚英语新闻文章 (MEN) 数据集的手动注释实体和关系微调了 MENmBERT 和 MENBERT。这种微调过程使 PLM 能够学习表示，这些表示捕获与 NER 和 RE 任务相关的马来西亚英语的细微差别。与 bert-base-multilingual-cased 模型相比，MENmBERT 在 NER 和 RE 任务上分别实现了 1.52% 和 26.27% 的改进。尽管 NER 的整体性能没有显着提高，但我们进一步的分析表明，按 12 个实体标签进行评估时，性能有显着提高。这些发现表明，在特定语言和地理位置的语料库上预训练语言模型可能是提高低资源环境中 NER 性能的一种很有前景的方法。本文发布的数据集和代码为专注于马来西亚英语的 NLP 研究工作提供了宝贵的资源。||
|**2024-07-01**|[Language Portability Strategies for Open-domain Dialogue with Pre-trained Language Models from High to Low Resource Languages](http://arxiv.org/abs/2407.01315)|null|本文研究用于开放域对话系统的大型预训练语言模型 (PLM) 在高资源语言中的语言可移植性策略。具体来说，目标低资源语言 (L_T) 将使用法语进行模拟，因为它缺乏特定于任务的资源，并且允许我们进行人工评估，而源语言 (L_S) 为英语。出于显而易见的原因，最近使用此类模型进行开放域对话的工作大多是用英语开发的。然而，为每种可能的目标语言构建特定的 PLM 需要收集新的数据集，而且成本高昂。出于这个原因，我们希望尝试利用 L_S 和 L_T 中的所有现有资源（PLM 和数据），评估使用不同方法在 L_T 中可实现的性能。前两种方法评估了神经机器翻译 (NMT) 在不同级别的使用：TrainOnTarget，其中在 L_T 中微调之前翻译 L_S 数据集，以及 TestOnSource，其中 L_S 模型在推理过程中与 NMT 模块耦合。然后，全球第一个开放获取的多语言大型 PLM BLOOM [2] 的出现，使研究人员能够开发新的方法，旨在不仅利用模型的完全可访问性，还利用其多语言性和翻译能力。在这种情况下，首先在 L_S 中学习任务，然后使用 MAD-X 适配器架构 [16] 适应 L_T。在这两组实验中，模型在口语对话条件下与人类进行评估，并且可以根据感知的交互质量比较策略。||
|**2024-07-01**|[A Fingerprint for Large Language Models](http://arxiv.org/abs/2407.01235)|null|近期研究表明，扩展预训练语言模型可以在许多下游任务上实现最先进的性能，这使得大型语言模型（LLM）成为人工智能领域的热门研究课题。然而，由于从头开始训练LLM需要大量的资源，因此保护LLM的知识产权免遭侵权至关重要且紧迫。这促使本文作者提出了一种新颖的LLM黑盒指纹识别技术，该技术既不需要模型训练也不需要模型微调。我们首先证明LLM的输出跨越与每个模型相关的唯一向量空间。我们将所有权认证问题建模为评估受害模型空间与嫌疑模型输出空间之间相似性的任务。为了解决这个问题，我们提出了两种解决方案，其中第一个解决方案涉及验证可疑大型模型的输出是否与受害模型的输出位于相同的空间中，从而能够快速识别模型侵权；第二个解决方案重建LLM输出和受害模型的向量空间的并集，以解决受害模型遭受参数高效微调（PEFT）攻击的情况。实验结果表明，所提出的技术在所有权验证和抵御PEFT攻击方面取得了优异的性能。这项工作揭示了LLM的固有特性，并为黑盒场景下的LLM所有权验证提供了一种有前景的解决方案，确保了效率、通用性和实用性。||
|**2024-07-01**|[Development of Cognitive Intelligence in Pre-trained Language Models](http://arxiv.org/abs/2407.01047)|null|最近的研究表明，大型预训练语言模型 (PLM) 出现了认知能力。这些模型不断提高的认知一致性使其成为认知科学理论的候选者。先前对 PLM 涌现认知能力的研究很大程度上与模型训练路径无关，即侧重于最终的模型权重而不是中间步骤。然而，使用 PLM 构建合理的人类认知模型将受益于考虑其在训练期间的表现与儿童思维轨迹的发展一致性。在人类智力心理测量测试的指导下，我们选择了四组任务来研究十个流行的 PLM 家族的一致性，并评估它们可用的中间和最终训练步骤。这些任务是数字能力、语言能力、概念理解和流体推理。我们发现了一个惊人的规律：无论模型大小如何，PLM 的发展轨迹始终表现出一个与人类认知发展最大程度一致的窗口。在该窗口之前，训练似乎赋予“空白石板”模型以必要的结构，使其能够从经验中快速学习。在该窗口之后，训练似乎服务于降低损失的工程目标，而不是提高与人类认知一致性的科学目标。||
|**2024-07-01**|[Cross-Modal Attention Alignment Network with Auxiliary Text Description for zero-shot sketch-based image retrieval](http://arxiv.org/abs/2407.00979)|null|本文研究了基于零样本草图的图像检索问题 (ZS-SBIR)。先前的方法在只有类别标签甚至没有文本信息的双模态设置中解决该问题。然而，大规模预训练语言模型 (LLM) 的日益普及，展现出从网络规模数据中学习到的丰富知识，为我们提供了一个总结集体文本信息的机会。我们的主要创新在于使用文本数据作为图像的辅助信息，从而利用语言提供的固有的零样本泛化能力。为此，我们提出了一种名为“基于辅助文本描述的跨模态注意力对齐网络”的方法，用于零样本草图图像检索。该网络由三个部分组成：(i) 描述生成模块，通过使用几个疑问句提示LLM，为每个训练类别生成文本描述；(ii) 特征提取模块，包括用于草图和图像数据的两个ViT、用于提取每个训练类别句子标记的转换器；最后 (iii) 跨模态对齐模块，使用交叉注意力机制交换文本-草图和文本-图像的标记特征，并在局部和全局范围内对齐标记。在三个基准数据集上的大量实验表明，我们的方法优于最先进的 ZS-SBIR 方法。||
|**2024-06-30**|[NAIST Simultaneous Speech Translation System for IWSLT 2024](http://arxiv.org/abs/2407.00826)|null|本文描述了NAIST提交给IWSLT 2024评测活动同步赛道的系统：英语到{德语、日语、汉语}的语音到文本翻译和英语到日语的语音到语音翻译。我们开发了一种多语言端到端语音到文本翻译模型，该模型结合了两个预训练语言模型HuBERT和mBART。我们使用两种解码策略训练该模型：局部一致性（LA）和AlignAtt。提交的模型采用LA策略，因为它在之前的模型中优于AlignAtt策略。我们的语音到语音翻译方法是上述语音到文本模型与增量文本到语音（TTS）模块的级联，该模块包含音素估计模型、并行声学模型和并行WaveGAN声码器。我们通过将采用AlignAtt策略的Transformer架构应用于估计模型来改进增量TTS。结果表明，我们升级后的TTS模块有助于提高系统性能。||

<p align=right>(<a href=#updated-on-20240717>back to top</a>)</p>

## Transformer

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-07-16**|[Hierarchical Separable Video Transformer for Snapshot Compressive Imaging](http://arxiv.org/abs/2407.11946)|**[link](https://github.com/pwangcs/hisvit)**|Transformer在解决视频快照压缩成像（SCI）的反问题方面取得了最先进的性能，该问题的病态性源于空间掩蔽和时间混叠的混合退化。然而，以前的Transformer缺乏对这种退化的洞察力，因此性能和效率有限。在这项工作中，我们定制了一个高效的重建架构，在早期层中没有时间聚合，并使用分层可分离视频Transformer（HiSViT）作为构建块。HiSViT由多组跨尺度可分离多头自注意力（CSS-MSA）和门控自调制前馈网络（GSM-FFN）以及密集连接构成，每个网络都在不同尺度的独立通道部分内进行，用于多尺度交互和远程建模。通过将空间操作与时间操作分离，CSS-MSA引入了一种归纳偏差，即在帧内而不是帧之间更加关注，同时节省了计算开销。GSM-FFN旨在通过门控机制和分解的时空卷积来增强局部性。大量实验表明，我们的方法在复杂性和参数相当或更少的情况下，性能优于以前的方法> 0.5 dB。源代码和预训练模型已发布在https://github.com/pwangcs/HiSViT。|
|**2024-07-16**|[Relaxing Graph Transformers for Adversarial Attacks](http://arxiv.org/abs/2407.11764)|null|已有研究表明，图神经网络 (GNN) 容易受到对抗性攻击。尽管图变换器 (GT) 在多个基准测试中优于消息传递 GNN，但其对抗性鲁棒性尚未得到探索。然而，攻击 GT 具有挑战性，因为它们的位置编码 (PE) 和特殊的注意力机制难以区分。我们通过针对基于 (1) 随机游走 PE、(2) 成对最短路径 PE 和 (3) 谱 PE 的三种代表性架构来克服这些挑战，并提出第一个针对 GT 的自适应攻击。我们利用攻击来评估 (a) 节点分类中结构扰动的鲁棒性；以及 (b) （假新闻）图分类的节点注入攻击。我们的评估表明，它们可能非常脆弱，并强调了我们工作的重要性以及自适应攻击的必要性。|
|**2024-07-16**|[Video-Language Alignment Pre-training via Spatio-Temporal Graph Transformer](http://arxiv.org/abs/2407.11677)|null|视频语言对齐是一项重要的多模态任务，有利于各种下游应用，例如视频文本检索和视频问答。现有方法要么利用视频文本对中的多模态信息，要么应用全局和局部对齐技术来提高对齐精度。然而，这些方法往往无法充分探索视频内视觉标记之间以及不同视频文本对之间时空关系。在本文中，我们提出了一种新颖的时空图Transformer模块，用于统一学习视频语言对齐预训练（称为STGT）的时空上下文。具体来说，我们的STGT将时空图结构信息与Transformer块中的注意力相结合，有效地利用了时空上下文。通过这种方式，我们可以对视觉标记之间的关系进行建模，提高视频文本对齐精度，从而有利于下游任务。此外，我们提出了一种自相似性对齐损失，以探索视频和文本中固有的自相似性。通过对比学习实现的初始优化，可以进一步提高视频和文本之间的对齐精度。在包括视频文本检索和视频问答在内的具有挑战性的下游任务上的实验结果证明了我们方法的优越性能。|
|**2024-07-16**|[Probing the Efficacy of Federated Parameter-Efficient Fine-Tuning of Vision Transformers for Medical Image Classification](http://arxiv.org/abs/2407.11573)|null|随着大型预训练 Transformer 模型的出现，针对各种下游任务对这些模型进行微调成为一个关键问题。训练数据的缺乏、数据孤岛的存在以及严格的隐私限制加剧了医学影像领域微调问题的难度，对能够实现预训练模型协同微调的算法提出了强烈需求。此外，由于这些模型的规模庞大，需要使用参数高效微调（PEFT）来减少联邦学习中的通信负担。在这项工作中，我们系统地研究了各种联邦 PEFT 策略，用于使视觉 Transformer（ViT）模型（在大规模自然图像数据集上进行预训练）适应医学图像分类。除了评估已知的 PEFT 技术外，我们还引入了 PEFT 算法的联邦变体，例如视觉提示微调（VPT）、视觉提示的低秩分解、随机块注意力微调以及混合 PEFT 方法（如低秩适应（LoRA）+VPT）。此外，我们进行了全面的实证分析，以确定适用于联邦环境的最佳 PEFT 方法，并了解数据分布对联邦 PEFT 的影响，特别是对于域外（OOD）和非独立同分布（non-IID）数据。本研究的主要见解是，虽然大多数联邦 PEFT 方法在域内迁移中表现良好，但在处理 OOD 和非 IID 场景时，准确性和效率之间存在着巨大的权衡，而这在医学影像中很常见。具体而言，微调/交换参数每减少一个数量级，准确率就会下降 4%。因此，初始模型的选择对于联邦 PEFT 至关重要。如果可能的话，最好使用从域内医学图像数据中学习到的医学基础模型，而不是通用视觉模型。|
|**2024-07-16**|[Understanding Counting in Small Transformers: The Interplay between Attention and Feed-Forward Layers](http://arxiv.org/abs/2407.11542)|null|我们对在直方图任务上训练的简单transformer模型进行了全面分析，该任务的目标是计算来自固定字母表的输入序列中每个项目的出现次数。尽管表面上很简单，但这项任务展现出丰富的现象学，使我们能够描述不同的架构组件如何促进不同算法解决方案的出现。特别是，我们展示了实现解决方案的两种性质不同的机制的存在，即基于关系和基于清单的计数。模型可以实现哪种解决方案，很大程度上取决于注意力机制、激活函数、记忆容量和序列开始标记的存在的精确选择。通过内省在计数任务上学习的模型，我们找到了这两种机制形成的证据。从更广泛的角度来看，我们的分析提供了一个框架，以理解transformer模型的不同架构组件的交互如何塑造不同的算法解决方案和近似值。|
|**2024-07-16**|[Not Another Imputation Method: A Transformer-based Model for Missing Values in Tabular Datasets](http://arxiv.org/abs/2407.11540)|**[link](https://github.com/cosbidev/naim)**|在训练和测试人工智能模型时，处理表格数据集中的缺失值是一项重大挑战，这个问题通常使用插补技术来解决。在此，我们介绍“非另一种插补方法”（NAIM），这是一种基于Transformer的新型模型，专门设计用于解决此问题，而无需传统的插补技术。NAIM 采用特征特定的嵌入和掩蔽自注意力机制，可以有效地从可用数据中学习，从而避免了对缺失值进行插补的必要性。此外，还引入了一种新的正则化技术，以增强模型从不完整数据中泛化的能力。我们在 5 个公开可用的表格数据集上对 NAIM 进行了广泛评估，证明其性能优于 6 种最先进的机器学习模型和 4 种深度学习模型，并在必要时将每种模型与 3 种不同的插补技术配对。结果突出了 NAIM 在存在缺失数据的情况下提高预测性能和鲁棒性的功效。为了促进在没有传统插补方法的情况下处理缺失数据的进一步研究和实际应用，我们在 https://github.com/cosbidev/NAIM 上提供了 NAIM 的代码。|
|**2024-07-16**|[Haze-Aware Attention Network for Single-Image Dehazing](http://arxiv.org/abs/2407.11505)|null|单图像去雾是计算机视觉中的一项关键挑战，旨在去除图像中的雾霾并恢复清晰的背景细节。认识到传统的基于物理模型的方法的局限性和当前基于注意力的解决方案的低效率，我们提出了一种新的去雾网络，它结合了创新的雾霾感知注意力模块 (HAAM) 和多尺度频率增强模块 (MFEM)。HAAM 受大气散射模型的启发，将物理原理巧妙地融入到高维特征中，以实现目标去雾。它在图像恢复过程中提取潜在特征，从而显着提升指标，而 MFEM 有效地增强了高频细节，从而避免了小波或傅里叶变换的复杂性。它采用多尺度场来提取和强调关键频率成分，同时最大限度地减少参数开销。我们的雾霾感知注意力网络 (HAA-Net) 集成了一个简单的 U-Net 框架，用于单图像去雾，在效率和有效性方面明显优于现有的基于注意力和变压器的模型。HAA-Net 在各种公共数据集上进行了测试，树立了新的性能基准。我们的工作不仅推动了图像去雾领域的发展，而且还为更广泛的计算机视觉应用中的注意力机制设计提供了见解。|
|**2024-07-16**|[RIMformer: An End-to-End Transformer for FMCW Radar Interference Mitigation](http://arxiv.org/abs/2407.11459)|null|调频连续波 (FMCW) 雷达在遥感领域发挥着举足轻重的作用。随着 FMCW 雷达部署密度的增加，相互干扰也随之加剧，这削弱了雷达的探测能力，并威胁到系统的可靠性和安全性。本文提出了一种基于Transformer的端到端 FMCW 雷达干扰抑制 (RIM) 新方法，称为 RIMformer。在 RIMformer 中，提出了一种双多头自注意力机制来捕获中频 (IF) 信号不同距离元素之间的相关性。此外，还集成了改进的卷积块，以利用卷积的强大功能来提取局部特征。该架构旨在以端到端的方式处理时域 IF 信号，从而避免了额外的手动数据处理步骤。改进后的解码器结构确保了网络的并行化，从而提高了计算效率。通过仿真和测量实验验证了该方法的准确性和有效性。结果表明，所提出的 RIMformer 可以有效地抑制干扰并恢复目标信号。|
|**2024-07-16**|[PADRe: A Unifying Polynomial Attention Drop-in Replacement for Efficient Vision Transformer](http://arxiv.org/abs/2407.11306)|null|我们提出了多项式注意力即插即用替换 (PADRe)，这是一个新颖且统一的框架，旨在替换 Transformer 模型中传统的自注意力机制。值得注意的是，最近出现的几种替代注意力机制，包括 Hyena、Mamba、SimA、Conv2Former 和 Castling-ViT，都可以被视为我们 PADRe 框架的特定实例。PADRe 利用多项式函数并借鉴了逼近论的既定结果，在不影响准确性的情况下提高了计算效率。PADRe 的关键组件包括乘法非线性，我们使用简单的、硬件友好的操作（例如 Hadamard 积）来实现这些操作，仅产生线性计算和内存成本。PADRe 进一步避免了使用 Softmax 等复杂函数的需要，但与传统的自注意力相比，它保持了相当或更高的准确性。我们评估了 PADRe 作为跨各种计算机视觉任务的自注意力替代品的有效性。这些任务包括图像分类、基于图像的 2D 对象检测和 3D 点云对象检测。实验结果表明，PADRe 的运行速度明显快于传统的自注意力（在服务器 GPU 和移动 NPU 上快 11 倍到 43 倍），同时在 Transformer 模型中替换自注意力时保持了相似的准确性。|
|**2024-07-15**|[Conquering images and the basis of transformative action](http://arxiv.org/abs/2407.11254)|null|我们快速沉浸在网络生活中，这让我们都病了。人工智能技术通过生成、个性化和传播迷人的图像，以令人作呕的精度和规模将大众的思想和心灵商品化。在线网络、人工智能 (AI)、社交媒体和数字新闻推送通过建立将我们的社区和身份细分和极化的叙述，来微调我们的信念和追求。与此同时，那些掌控着这些技术的人征服了我们内心生活、社会关系、地球和宇宙的最后疆域。在注意力经济中，我们的能动性受到限制，我们的活力因其自恋的追求和享乐而枯竭。生成式人工智能增强了那些使生命同质化和根除的力量，这不是通过某种愚蠢的“奇点”事件，而是通过贬低人类的创造力、劳动和社会生活。我们将使用一个破碎的镜头，来审视叙述和网络如何在心理、社会和算法层面影响我们。我们将讨论原子化的图像——疏远而不是激励个人的理想和追求——如何劫持人们的能动性，以维持毁灭他们的力量。我们将发现帝国是如何建立数字网络，以优化社会并鼓励自恋者强化社会二元对立，从而使消费、剥削和等级制度的不断扩张永久化。世界上的结构性等级制度通过我们信念和思想中的等级制度得到强化。只有将图像视为图像，并欣赏对立叙述之间的相似性，我们才能促进变革行动，并摆脱困扰我们生活的军国主义体系。|
|**2024-07-12**|[Region Attention Transformer for Medical Image Restoration](http://arxiv.org/abs/2407.09268)|**[link](https://github.com/yaziwel/region-attention-transformer-for-medical-image-restoration)**|基于Transformer的方法在医学图像恢复方面表现出令人印象深刻的结果，这归功于其在空间维度上的多头自注意力（MSA）机制。然而，大多数现有的Transformer在固定且粗略划分的区域（例如，整个图像或固定块）内进行注意力计算，导致来自不相关区域的干扰和连续图像内容的碎片化。为了克服这些挑战，我们引入了一种新的区域注意力Transformer（RAT），它利用了基于区域的多头自注意力机制（R-MSA）。R-MSA使用鲁棒的Segment Anything Model (SAM)将输入图像动态地划分为不重叠的语义区域，然后在这些区域内执行自注意力。这种区域划分更加灵活和可解释，确保只有来自相似语义区域的像素相互补充，从而消除了来自不相关区域的干扰。此外，我们引入了一个焦点区域损失来引导我们的模型自适应地关注恢复高难度区域。大量实验表明，RAT在各种医学图像恢复任务中是有效的，包括PET图像合成、CT图像去噪和病理图像超分辨率。代码可在https://github.com/Yaziwel/Region-Attention-Transformer-for-Medical-Image-Restoration.git获取。|
|**2024-07-12**|[DANIEL: A fast Document Attention Network for Information Extraction and Labelling of handwritten documents](http://arxiv.org/abs/2407.09103)|**[link](https://github.com/shulk97/daniel)**|从手写文档中提取信息传统上涉及三个不同的步骤：文档布局分析、手写文本识别和命名实体识别。最近的方法试图使用完全端到端的架构将这些步骤集成到一个过程中。尽管如此，当应用于纯文本信息提取时，这些集成方法的性能尚未与语言模型相匹配。在本文中，我们介绍了DANIEL（用于信息提取和标注的文档注意力网络），这是一种完全端到端的架构，集成了语言模型，专为全面理解手写文档而设计。DANIEL对整页文档执行布局识别、手写文本识别和命名实体识别。此外，它可以同时跨多种语言、布局和任务进行学习。对于命名实体识别，可以通过输入提示指定要应用的本体。该架构采用能够处理任何大小图像而无需调整大小的卷积编码器，并与基于transformer的语言模型的自回归解码器配对。DANIEL在四个数据集上取得了具有竞争力的结果，包括在RIMES 2009和M-POPP上手写文本识别和在IAM NER上命名实体识别方面的新SOTA性能。此外，DANIEL比现有方法快得多。我们在\url{https://github.com/Shulk97/daniel}上提供了源代码和训练模型的权重。|
|**2024-07-12**|[Beyond Image Prior: Embedding Noise Prior into Conditional Denoising Transformer](http://arxiv.org/abs/2407.09094)|**[link](https://github.com/yuanfeihuang/condformer)**|现有的基于学习的去噪方法通常训练模型从大规模数据集中泛化图像先验，但在现实世界场景中遇到的噪声分布变化时表现不佳。在这项工作中，我们通过强调噪声和图像先验之间的明显分离，为去噪挑战提出了一个新的视角。这一见解构成了我们开发条件优化框架的基础，旨在克服传统去噪框架的限制。为此，我们引入了一种局部噪声先验估计 (LoNPE) 算法，它可以直接从单个原始噪声图像中准确估计噪声先验。这种估计作为相机传感器成像环境的显式先验表示，与场景的图像先验不同。此外，我们设计了一个辅助可学习的 LoNPE 网络，专为 sRGB 噪声图像的实际应用而定制。利用估计的噪声先验，我们提出了一种新的条件去噪 Transformer (Condformer)，通过将噪声先验纳入条件自注意力机制。这种集成允许 Condformer 将优化过程分割成多个显式子空间，从而显着增强模型的泛化能力和灵活性。对合成数据集和真实世界数据集的广泛实验评估表明，所提出的方法比当前最先进的方法取得了优越的性能。源代码可在 https://github.com/YuanfeiHuang/Condformer 获取。|
|**2024-07-12**|[Global Attention-Guided Dual-Domain Point Cloud Feature Learning for Classification and Segmentation](http://arxiv.org/abs/2407.08994)|null|以往的研究已经证明了基于点的点云分析任务神经网络模型的有效性。然而，如何为原始点坐标生成有效的输入嵌入仍然是一个关键问题。此外，邻域聚合作为网络主干中的一个关键组件，其效率有限也是一个问题。在本文中，我们提出了一个全局注意力引导的双域特征学习网络（GAD）来解决上述问题。我们首先设计了上下文位置增强型Transformer（CPT）模块，该模块配备了改进的全局注意力机制，以生成全局感知的输入嵌入，作为后续聚合的指导。然后，级联双域K近邻特征融合（DKFF）模块，通过新的双域特征学习进行有效的特征聚合，该学习同时考虑了局部几何关系和长距离语义联系。在多个点云分析任务（例如分类、零件分割和场景语义分割）上的大量实验表明，所提出的方法和设计的模块具有优越的性能。|
|**2024-07-12**|[Deep Attention Driven Reinforcement Learning (DAD-RL) for Autonomous Vehicle Decision-Making in Dynamic Environment](http://arxiv.org/abs/2407.08932)|null|由于与周围车辆的动态交互，城市环境中的自动驾驶汽车 (AV) 决策本质上具有挑战性。为了安全规划，自动驾驶汽车必须了解场景中各种时空交互的重要性。当代工作使用巨大的 Transformer 架构来编码交互，主要用于轨迹预测，导致计算复杂性增加。为了在不影响时空理解和性能的情况下解决这个问题，我们提出了简单的深度注意力驱动强化学习 (DADRL) 框架，该框架动态分配并将周围车辆的重要性纳入到自动驾驶汽车的强化学习驱动决策过程中。我们引入了一种以自动驾驶汽车为中心的时空注意力编码 (STAE) 机制，用于学习与不同周围车辆的动态交互。为了理解地图和路线环境，我们采用环境编码器从环境地图中提取特征。时空表示与环境编码相结合，提供了全面的状态表示。由此产生的模型使用软演员评论家 (SAC) 算法进行训练。我们在没有交通信号灯的 SMARTS 城市基准场景中评估了所提出的框架，以证明 DADRL 优于最近的最新方法。此外，消融研究强调了环境编码器和时空注意力编码器在实现卓越性能方面的重要性。|
|**2024-07-11**|[TractGraphFormer: Anatomically Informed Hybrid Graph CNN-Transformer Network for Classification from Diffusion MRI Tractography](http://arxiv.org/abs/2407.08883)|null|利用深度神经网络研究大脑连接和非影像表型之间的关系越来越受到关注。然而，卷积网络设计往往忽略了大脑白质网络的局部和全局特性。我们引入了 TractGraphFormer，这是一个专为弥散磁共振成像纤维束追踪设计的混合图CNN-Transformer深度学习框架。该模型利用了白质结构的局部解剖特征和全局特征依赖性。图CNN模块捕获白质几何形状和灰质连接，以聚合解剖学上相似的白质连接的局部特征，而Transformer模块使用自注意力机制来增强全局信息学习。此外，TractGraphFormer 包含一个注意力模块，用于解释预测性的白质连接。在性别预测测试中，TractGraphFormer 在儿童 (n=9345) 和年轻人 (n=1065) 的大型数据集中表现出强大的性能。总的来说，我们的方法表明，WM 中广泛的连接可以预测个体的性别，并且在两个数据集中都识别出一致的预测性解剖束。所提出的方法强调了整合局部解剖信息和全局特征依赖性以提高基于弥散磁共振成像纤维束追踪的机器学习预测性能的潜力。|
|**2024-07-11**|[Jet Tagging with More-Interaction Particle Transformer](http://arxiv.org/abs/2407.08682)|null|本研究介绍了一种名为 More-Interaction Particle Transformer (MIParT) 的新型深度学习神经网络，用于喷注标记。该框架结合了我们自主设计的 More-Interaction Attention (MIA) 机制，增加了粒子交互嵌入的维度。我们使用顶夸克标记和夸克-胶子数据集测试了 MIParT。结果表明，MIParT 不仅在准确率和 AUC 指标上与 LorentzNet 持平，而且在背景抑制方面显著优于 ParT 模型。具体而言，在顶夸克标记数据集上，当信号效率为 30% 时，其背景抑制率提高了约 25%，在夸克-胶子数据集上提高了 3%。此外，MIParT 仅需要 ParT 30% 的参数量和 47% 的计算复杂度，证明了在降低模型复杂度且无需在大型数据集上进行大量预训练的情况下，也能实现高性能。这些结果表明，MIParT 有潜力提升粒子物理学中喷注标记和事件识别的效率基准。|
|**2024-07-11**|[Latent Spaces Enable Transformer-Based Dose Prediction in Complex Radiotherapy Plans](http://arxiv.org/abs/2407.08650)|**[link](https://github.com/edwardwang1/ldformer)**|越来越多的证据表明，立体定向消融体部放射治疗 (SABR) 可用于治疗肺部多发癌灶。多病灶肺部 SABR 计划十分复杂，创建起来需要耗费大量资源。本研究提出了一种新颖的两阶段潜在变换器框架 (LDFormer)，用于预测不同病灶数量的肺部 SABR 计划的剂量分布。在第一阶段，将患者解剖信息和剂量分布编码到潜在空间中。在第二阶段，变换器学习从解剖潜在特征预测剂量潜在特征。修改因果注意力以适应不同数量的病灶。在病灶内部和周围的剂量适形度方面，LDFormer 优于最先进的生成对抗网络，并且在考虑重叠病灶时，性能差距会更大。LDFormer 可以在 30 秒内在消费级硬件上生成 3D 剂量分布预测，并有可能帮助医生做出临床决策、降低资源成本并加速治疗计划制定。|
|**2024-07-11**|[FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](http://arxiv.org/abs/2407.08608)|null|注意力机制作为Transformer架构的核心层，是大语言模型和长上下文应用的瓶颈。FlashAttention阐述了一种通过最小化内存读/写来加速GPU上注意力计算的方法。然而，它尚未利用最新硬件的新功能，FlashAttention-2在H100 GPU上仅实现了35%的利用率。我们开发了三种主要技术来加速Hopper GPU上的注意力计算：利用Tensor Core和TMA的异步性来(1) 通过warp专业化来重叠整体计算和数据移动，以及(2) 交错块级矩阵乘法和softmax操作，以及(3) 利用硬件对FP8低精度支持的块量化和非相干处理。我们证明，我们的方法FlashAttention-3在使用FP16时，在H100 GPU上实现了1.5-2.0倍的加速，达到高达740 TFLOPs/s（75%的利用率），而在使用FP8时，接近1.2 PFLOPs/s。我们验证了FP8 FlashAttention-3的数值误差比基线FP8注意力机制低2.6倍。|
|**2024-07-12**|[ADMM Based Semi-Structured Pattern Pruning Framework For Transformer](http://arxiv.org/abs/2407.08334)|null|自然语言处理(NLP)通过Transformer模型取得了巨大的成功。然而，该模型拥有数亿甚至数十亿个参数，这对其在个人电脑或小型服务器上的部署造成了巨大的负担。为了解决这个问题，我们可以使模型的权重矩阵更加稀疏，或者压缩注意力层。模式剪枝是最重要的剪枝方法之一，它允许在每个划分的模式块中选择固定数量的参数并将其剪枝。然而，模式剪枝的效果受到每层权重区域内稀疏性的严格限制。在本文中，我们首先介绍了基于交替方向乘子法(ADMM)的模式剪枝框架，以重塑激活映射的分布。具体来说，我们建议将Transformer上的模式剪枝公式化为一个约束优化问题，并使用ADMM来优化该问题。通过这种方式，初始的密集特征图被转换为区域稀疏的特征图。因此，我们可以基于模式剪枝方法在获得更高压缩率的同时获得更好的性能。此外，本文还提供了具有局部稀疏性的ADMM的理论推导。最后，我们还将提出的基于ADMM的框架扩展到量化上，以证明其泛化能力，并使用SR-STE来避免梯度消失问题。我们对GLUE数据集上的分类任务进行了广泛的实验。值得注意的是，我们实现了50%的压缩率，同时在GLUE数据集上保持了80.1的总分。|
|**2024-07-11**|[HDT: Hierarchical Document Transformer](http://arxiv.org/abs/2407.08330)|**[link](https://github.com/autonomousvision/hdt)**|在本文中，我们提出了分层文档Transformer（HDT），这是一种针对结构化分层文档量身定制的新型稀疏Transformer架构。此类文档在众多领域（包括科学、法律或医学）中极为重要。然而，大多数现有解决方案效率低下，并且无法利用文档固有的结构。HDT通过引入辅助锚标记并将注意力机制重新设计为稀疏的多级层次结构来利用文档结构。这种方法促进了不同级别标记之间的信息交换，同时保持了稀疏性，从而在利用文档结构作为归纳偏差的同时提高了计算和内存效率。我们通过开发一种考虑文档层次结构的新型稀疏注意力内核来解决实现HDT依赖于样本的分层注意力模式的技术挑战。正如我们的实验所示，利用文档中存在的结构信息可以加快收敛速度，提高样本效率，并在下游任务中获得更好的性能。||
|**2024-07-11**|[Improving Dental Diagnostics: Enhanced Convolution with Spatial Attention Mechanism](http://arxiv.org/abs/2407.08114)|null|深度学习已经成为医疗保健领域的一种变革性工具，通过分析复杂的影像数据，在牙科诊断方面取得了重大进展。本文提出了一种增强的 ResNet50 架构，并集成了 SimAM 注意力模块，以解决牙科影像中对比度有限的挑战，并在降低计算需求的同时优化深度学习性能。SimAM 模块被添加到第二个 ResNet 模块之后，通过捕获空间依赖性和增强重要特征来改进特征提取。我们的模型在各种特征提取技术中均表现出优异的性能，F1 分数达到 0.676，优于 VGG、EfficientNet、DenseNet 和 AlexNet 等传统架构。这项研究强调了我们的方法在提高牙科影像分析的分类准确性和鲁棒性方面的有效性，突出了深度学习在提高牙科诊断准确性和效率方面的潜力。像我们这样的高级人工智能模型的集成有望彻底改变牙科诊断，为改善患者结果和更广泛地采用人工智能技术做出贡献。||
|**2024-07-10**|[MambaVision: A Hybrid Mamba-Transformer Vision Backbone](http://arxiv.org/abs/2407.08083)|**[link](https://github.com/nvlabs/mambavision)**|我们提出了一种名为 MambaVision 的新型混合 Mamba-Transformer 骨干网络，该网络专为视觉应用而设计。我们的核心贡献包括重新设计 Mamba 公式，以增强其对视觉特征进行有效建模的能力。此外，我们对将视觉 Transformer (ViT) 与 Mamba 集成的可行性进行了全面的消融研究。我们的结果表明，在 Mamba 架构的最后几层配备多个自注意力块可以极大地提高模型捕获远程空间依赖关系的能力。基于我们的发现，我们引入了一系列具有分层架构的 MambaVision 模型，以满足各种设计标准。对于 ImageNet-1K 数据集上的图像分类任务，MambaVision 模型变体在 Top-1 准确率和图像吞吐量方面实现了新的最先进 (SOTA) 性能。在下游任务（如 MS COCO 和 ADE20K 数据集上的目标检测、实例分割和语义分割）中，MambaVision 优于规模相当的骨干网络，并展现出更优越的性能。代码：https://github.com/NVlabs/MambaVision。||
|**2024-07-10**|[Study on Aspect Ratio Variability toward Robustness of Vision Transformer-based Vehicle Re-identification](http://arxiv.org/abs/2407.07842)|null|视觉Transformer（ViT）在车辆重识别（ReID）任务中表现出色。然而，图像或视频输入的非正方形纵横比可能会严重影响重识别的性能。为了解决这个问题，我们在本文中提出了一种新的基于ViT的ReID框架，它融合了在各种纵横比上训练的模型。我们的主要贡献有三方面：（i）我们分析了VeRi-776和VehicleID数据集上的纵横比性能，并根据原始图像的纵横比指导输入设置。(ii)我们在ViT分块过程中引入了分块混合的图像内增强（由空间注意力分数引导），并实现了不均匀步幅以更好地匹配目标纵横比。(iii)我们提出了一种动态特征融合的ReID网络，增强了模型的鲁棒性。我们的ReID方法在VehicleID数据集上实现了91.0%的平均精度均值（mAP），显著优于最接近的现有技术水平（CAL）结果80.9%。||
|**2024-07-10**|[PosFormer: Recognizing Complex Handwritten Mathematical Expression with Position Forest Transformer](http://arxiv.org/abs/2407.07764)|**[link](https://github.com/sjtu-deepvisionlab/posformer)**|手写数学表达式识别（HMER）在数字化教育和自动化办公等人机交互场景中有着广泛的应用。近年来，采用编码器-解码器架构的基于序列的模型已被普遍用于解决这项任务，方法是直接预测表达式图像的LaTeX序列。然而，这些方法只是隐式地学习LaTeX提供的语法规则，由于复杂的结构关系和多样化的书写风格，可能无法描述符号之间的位置和层次关系。为了克服这一挑战，我们提出了一种用于HMER的位置森林转换器（PosFormer），它联合优化了两个任务：表达式识别和位置识别，从而明确地实现位置感知的符号特征表示学习。具体来说，我们首先设计了一个位置森林，将数学表达式建模为森林结构，并解析符号之间的相对位置关系。在不需要额外标注的情况下，每个符号在森林中都被分配了一个位置标识符，以表示其相对空间位置。其次，我们提出了一个隐式注意力校正模块，以便在基于序列的解码器架构中准确地捕捉HMER的注意力。大量实验验证了PosFormer的优越性，它始终优于最先进的方法，在单行CROHME 2014/2016/2019、多行M2E和复杂MNE数据集上分别获得了2.03%/1.22%/2.00%、1.83%和4.62%的提升，而没有增加任何延迟或计算成本。代码可在https://github.com/SJTU-DeepVisionLab/PosFormer获取。||
|**2024-07-10**|[SvANet: A Scale-variant Attention-based Network for Small Medical Object Segmentation](http://arxiv.org/abs/2407.07720)|**[link](https://github.com/anthonyweidai/SvANet)**|早期发现和准确诊断可以预测恶性疾病转化的风险，从而提高有效治疗的可能性。出现轻微症状和小面积感染区域是不祥的征兆，也是疾病早期诊断的首要任务。深度学习算法，如卷积神经网络（CNN），已被用于分割自然或医学物体，并显示出良好的效果。然而，由于 CNN 中的卷积和池化操作会导致信息丢失和压缩缺陷，因此分析图像中小区域的医学物体仍然是一个挑战。这些损失和缺陷随着网络深度的增加而变得越来越严重，特别是对于小型医学物体。为了应对这些挑战，我们提出了一种新的基于尺度变化注意力机制的网络 (SvANet)，用于医学图像中精确的小尺度物体分割。SvANet 由蒙特卡洛注意力机制、尺度变化注意力机制和视觉Transformer组成，它结合了跨尺度特征并减轻了压缩伪影，从而增强了对小型医学物体的辨别能力。定量实验结果表明，SvANet 具有优越的性能，在分割肾脏肿瘤、皮肤病变、肝脏肿瘤、息肉、手术切除细胞、视网膜血管和精子方面，平均 Dice 系数分别达到了 96.12%、96.11%、89.79%、84.15%、80.25%、73.05% 和 72.58%，这些物体在 KiTS23、ISIC 2018、ATLAS、PolypGen、TissueNet、FIVES 和 SpermHealth 数据集中所占的图像面积均不到 1%。||
|**2024-07-09**|[CAPformer: Compression-Aware Pre-trained Transformer for Low-Light Image Enhancement](http://arxiv.org/abs/2407.07056)|null|弱光图像增强(LLIE)随着手机摄影需求的激增而取得了进步，但许多现有方法忽略了压缩，而压缩是资源受限的手机摄影的一个关键问题。大多数LLIE方法忽视了这一点，阻碍了它们的有效性。在这项研究中，我们调查了JPEG压缩对弱光图像的影响，并揭示了由于暗区普遍存在低像素值，JPEG压缩导致大量信息丢失。因此，我们提出了压缩感知预训练Transformer (CAPformer)，采用一种新的预训练策略，从未压缩的弱光图像中学习无损信息。此外，提出的亮度引导自注意力(BGSA)机制增强了合理的信息收集。实验表明，我们的方法在减轻压缩对LLIE的影响方面具有优越性，展示了其在资源受限场景下改进LLIE的潜力。||
|**2024-07-09**|[ERQ: Error Reduction for Post-Training Quantization of Vision Transformers](http://arxiv.org/abs/2407.06794)|null|视觉Transformer（ViT）的训练后量化（PTQ）因其在模型压缩方面的效率而备受关注。然而，现有方法通常忽略了量化权重和激活之间错综复杂的相互依赖性，导致了相当大的量化误差。在本文中，我们提出了ERQ，一种精心设计的两步PTQ方法，用于依次减少由激活和权重量化引起的量化误差。ERQ首先引入了激活量化误差减少（Aqer），该方法策略性地将激活量化误差的最小化制定为岭回归问题，并通过使用全精度更新权重来解决该问题。随后，ERQ引入了权重量化误差减少（Wqer），该方法采用迭代方法来减轻由权重量化引起的量化误差。在每次迭代中，采用经验导出的高效代理来细化量化权重的舍入方向，并结合岭回归求解器来减少权重量化误差。实验结果证明了我们方法的有效性。值得注意的是，对于W3A4 ViT-S，ERQ在准确率方面比最先进的GPTQ高出22.36%。||
|**2024-07-09**|[Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules](http://arxiv.org/abs/2407.06677)|null|在Transformer中，是否总是需要从浅层到深层计算token？Vanilla Transformer及其变体的持续成功似乎给出了毫无疑问的“是”。然而，在这项工作中，我们试图打破这种深度有序的惯例，提出了一种名为模块混合（MoM）的新颖架构，其动机是基于一种直觉，即任何层，无论其位置如何，只要具备所需的处理能力，都可以用来计算token。MoM的构建始于一组有限的模块，这些模块由多头注意力机制和前馈网络定义，每个模块都由其独特的参数化来区分。然后，两个路由器迭代地从集合中选择注意力模块和前馈模块来处理token。这种选择在token的前向传递过程中动态扩展计算图，最终形成模块的组合。我们证明了MoM不仅为Transformer及其众多变体提供了一个统一的框架，而且还提供了一种灵活且可学习的方法来减少Transformer参数化中的冗余。我们使用OpenWebText预训练了各种MoM。实验结果表明，不同参数量的MoM在GLUE和XSUM基准测试中始终优于Vanilla Transformer。更有趣的是，在固定的参数预算下，与GPT-2-large相比，MoM-large能够将计算图的深度增加38%以上，从而在GLUE上获得1.4的绝对增益，在XSUM上获得1的绝对增益。另一方面，MoM-large还可以将深度减少60%以上，同时每层包含更多模块，与GPT-2-large相比，TFLOPs减少了16%，内存使用量减少了43%，同时保持了相当的性能。||
|**2024-07-09**|[CTRL-F: Pairing Convolution with Transformer for Image Classification via Multi-Level Feature Cross-Attention and Representation Learning Fusion](http://arxiv.org/abs/2407.06673)|null|Transformer因其强大的容量和全局处理能力，在计算机视觉领域受到越来越多的关注。然而，Transformer是数据密集型的，与卷积神经网络（ConvNets）相比，其泛化能力受到限制，特别是在数据有限的情况下进行训练时，因为它们缺乏ConvNets中存在的内置空间归纳偏差。在本文中，我们致力于将卷积和Transformer的优势结合起来，以完成图像分类任务。为此，我们提出了一种名为CTRL-F的新型轻量级混合网络，该网络通过表示学习融合和多级特征交叉注意力将卷积与Transformer配对。我们的网络包括一个卷积分支和一个名为多级特征交叉注意力（MFCA）的新型Transformer模块。MFCA模块对在不同卷积阶段获得的多级特征表示进行操作。它通过两个独立的Transformer分支处理从这些多级特征表示中提取的小块标记和大块标记，这两个分支通过交叉注意力机制进行通信和知识交换。我们使用称为自适应知识融合（AKF）和协作知识融合（CKF）的新型表示融合技术，将从卷积路径获得的局部响应与从MFCA模块获得的全局响应融合在一起。实验表明，无论是在大数据集上从头开始训练，还是在数据量少的情况下，我们的CTRL-F变体都能达到最先进的性能。例如，CTRL-F在Oxford-102 Flowers和PlantVillage数据集上从头开始训练时，Top-1准确率分别达到82.24%和99.91%，超过了最先进的模型，这体现了我们模型在图像分类任务上的鲁棒性。代码地址：https://github.com/hosamsherif/CTRL-F||
|**2024-07-09**|[Enhancing spatial auditory attention decoding with neuroscience-inspired prototype training](http://arxiv.org/abs/2407.06498)|null|空间听觉注意解码 (Sp-AAD) 技术旨在通过神经记录来确定多说话者场景中的听觉注意方向。尽管最近的 Sp-AAD 算法取得了成功，但其性能受到脑电图数据中特定于试验的特征的阻碍。本研究旨在针对这些特征提高解码性能。神经科学研究表明，空间听觉注意可以反映在不同频带脑电图能量的拓扑分布中。这一见解促使我们提出原型训练，这是一种受神经科学启发的 Sp-AAD 方法。该方法构建了具有增强的能量分布表示和减少的特定于试验的特征的原型，使模型能够更好地捕获听觉注意特征。为了实施原型训练，进一步提出了一种采用脑电图小波变换的 EEGWaveNet。详细的实验表明，采用原型训练的 EEGWaveNet 在各种数据集上均优于其他竞争模型，并且也验证了所提出方法的有效性。作为一种独立于模型架构的训练方法，原型训练为 Sp-AAD 领域提供了新的见解。||
|**2024-07-08**|[FGA: Fourier-Guided Attention Network for Crowd Count Estimation](http://arxiv.org/abs/2407.06110)|null|人群计数在城市规划、人群管理和公共安全等领域越来越具有社会意义。本文介绍了一种新颖的用于人群计数估计的注意力机制——傅里叶引导注意力（FGA），旨在解决现有基于卷积的注意力网络中全局模式捕获效率低下的问题。FGA 通过利用快速傅里叶变换 (FFT) 以及用于全局特征的空间注意力和用于半全局和局部特征的通道注意力卷积，有效地捕获了包括全尺度全局模式在内的多尺度信息。FGA 的架构涉及一种双路径方法：(1) 通过 FFT 处理全尺度全局特征的路径，允许在频域中高效提取信息，以及 (2) 使用传统卷积和通道注意力处理剩余特征图以获取半全局和局部特征的路径。这种双路径架构使 FGA 能够无缝集成频率和空间信息，增强其捕获不同人群模式的能力。我们将 FGA 应用于两个流行的人群计数工作 CSRNet 和 CANNet 的最后几层，以评估模块在基准数据集（如 ShanghaiTech-A、ShanghaiTech-B、UCF-CC-50 和 JHU++ 人群）上的性能。实验表明，基于均方误差 (MSE) 和平均绝对误差 (MAE) 指标，所有数据集都有显着改进，显示出与最近最先进方法相当的性能。此外，我们利用 Grad-CAM 热图，使用定性分析来说明可解释性，以显示 FGA 在捕获人群模式方面的有效性。||
|**2024-07-08**|[HiT-SR: Hierarchical Transformer for Efficient Image Super-Resolution](http://arxiv.org/abs/2407.05878)|null|Transformer 在包括图像超分辨率 (SR) 在内的计算机视觉任务中表现出了良好的性能。然而，流行的基于 Transformer 的 SR 方法通常采用计算复杂度与窗口大小呈二次关系的窗口自注意力机制，导致采用固定的较小窗口，感受野有限。在本文中，我们提出了一种将基于 Transformer 的 SR 网络转换为分层 Transformer (HiT-SR) 的通用策略，在保持高效设计的同时，利用多尺度特征提升 SR 性能。具体来说，我们首先将常用的固定小窗口替换为扩展的分层窗口，以聚合不同尺度的特征并建立远程依赖关系。考虑到大窗口所需的密集计算，我们进一步设计了一种空间-通道相关性方法，该方法对窗口大小具有线性复杂度，可以有效地从分层窗口中收集空间和通道信息。大量实验验证了我们 HiT-SR 的有效性和效率，我们改进后的 SwinIR-Light、SwinIR-NG 和 SRFormer-Light 版本以更少的参数、FLOPs 和更快的速度（约 7 倍）获得了最先进的 SR 结果。||
|**2024-07-08**|[MSTF: Multiscale Transformer for Incomplete Trajectory Prediction](http://arxiv.org/abs/2407.05671)|null|运动预测在自动驾驶系统中起着至关重要的作用，它使车辆能够根据周围车辆的预测执行碰撞警告和合理的局部路径规划。然而，普遍的方法通常假设观察到的轨迹是完整的，而忽略了物体遮挡、范围限制和传感器故障导致的缺失值可能产生的影响。这种疏忽不可避免地会影响轨迹预测的准确性。为了应对这一挑战，我们提出了一个名为多尺度变换器（MSTF）的端到端框架，该框架专为不完整轨迹预测而精心设计。MSTF集成了多尺度注意力头（MAH）和基于信息增量的模式自适应（IIPA）模块。具体来说，MAH组件利用多头注意力机制，从不同的时间粒度同时捕获轨迹序列的多尺度运动表示。这种方法有助于在不同尺度上对运动中的全局依赖关系进行建模，从而减轻缺失值的不利影响。此外，IIPA模块通过分析数据中的缺失模式，自适应地提取跨时间步长的运动连续性表示。连续性表示在更高层次上描绘了运动趋势，引导MSTF生成与运动连续性一致的预测。我们使用两个大规模真实世界数据集评估了我们提出的MSTF模型。实验结果表明，MSTF在不完整轨迹预测任务中优于最先进（SOTA）模型，展示了其在解决自动驾驶系统运动预测中缺失值挑战方面的有效性。||
|**2024-07-08**|[Graph Attention with Random Rewiring](http://arxiv.org/abs/2407.05649)|null|图神经网络 (GNN) 已成为图结构深度学习的基础。现代 GNN 的关键范式包括消息传递、图重连和图 Transformer。本文介绍了具有随机结构的图重连注意力机制 (GRASS)，这是一种结合了这三种范式优点的新型 GNN 架构。GRASS 通过叠加随机正则图来重新连接输入图，增强了远程信息传播，同时保留了输入图的结构特征。它还采用了一种专为图结构数据量身定制的独特加性注意力机制，在保持计算效率的同时提供了图归纳偏差。我们的实证评估表明，GRASS 在多个基准数据集上实现了最先进的性能，证实了其实用性。||
|**2024-07-08**|[On the Power of Convolution Augmented Transformer](http://arxiv.org/abs/2407.05591)|null|Transformer架构在语言建模方面引发了革命性的进步。然而，最近的架构方法，例如状态空间模型，已经弥合了性能差距。受此启发，我们研究了卷积增强型Transformer（CAT）在召回、复制和长度泛化任务中的优势。CAT在注意力层的K/Q/V嵌入中加入了卷积滤波器。通过CAT，我们展示了卷积的局部性与注意力的全局视图协同作用。与诸如Mamba或Transformer等类似架构不同，CAT可以使用单层可证明地解决关联召回（AR）和复制任务，同时还享有保证的长度泛化能力。我们还通过描述卷积如何通过总结上下文窗口和创建突出的摘要标记来参与注意力，从而减轻对完全注意力的需求，从而建立了卷积和注意力之间的计算权衡。对真实数据集的评估证实了我们的发现，并证明CAT及其变体确实增强了语言建模性能。||
|**2024-07-05**|[Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations](http://arxiv.org/abs/2407.04543)|**[link](https://github.com/namednil/step)**|模型需要适当的归纳偏置才能有效地从少量数据中学习并在训练分布之外进行系统泛化。 虽然 Transformer 具有高度的通用性和强大的功能，但它们仍然可以从增强的结构性归纳偏置中受益，以完成 seq2seq 任务，尤其是那些涉及句法转换的任务，例如将主动语态转换为被动语态或语义解析。在本文中，我们建议通过中间预训练来增强 Transformer 的结构性归纳偏置，以根据转换的描述对依存树执行合成生成的句法转换。我们的实验表明，这有助于对组块等句法任务进行少样本学习，并且还提高了语义解析的结构泛化能力。我们的分析表明，中间预训练会导致注意力头能够跟踪需要对哪些标记应用哪些句法转换，并且模型可以在下游任务中利用这些注意力头。||
|**2024-07-05**|[LayerShuffle: Enhancing Robustness in Vision Transformers by Randomizing Layer Execution Order](http://arxiv.org/abs/2407.04513)|null|由于人工神经网络的架构和训练方式，它们通常对测试时的剪枝、替换或打乱层顺序的操作缺乏鲁棒性。然而，这些特性对于不同的应用场景非常重要，例如在分布式神经网络架构中，执行顺序无法得到保证，或者网络的某些部分在推理过程中可能发生故障。为了解决这些问题，我们提出了一系列针对视觉Transformer的训练方法，其最重要的组成部分是在训练时随机化注意力模块的执行顺序。我们证明，使用我们提出的方法，假设可以容忍在相同模型规模下精度降低（约20%），视觉Transformer确实能够适应测试时任意层的执行顺序。我们还发现，我们训练的模型可以彼此随机合并，生成功能完备的“科学怪人”模型，并且与源模型相比，性能没有损失。最后，我们在测试时对模型进行层剪枝，发现其性能下降平缓。||
|**2024-07-05**|[Hard-Attention Gates with Gradient Routing for Endoscopic Image Computing](http://arxiv.org/abs/2407.04400)|null|为了解决胃肠道息肉大小评估中的过拟合问题并增强模型泛化能力，我们的研究引入了特征选择门（FSG）或硬注意力门（HAG）以及梯度路由（GR）来进行动态特征选择。该技术旨在通过促进稀疏连接来增强卷积神经网络（CNN）和视觉Transformer（ViT），从而减少过拟合并增强泛化能力。HAG 通过使用可学习权重的稀疏化来实现这一点，作为一种正则化策略。GR 通过独立于主模型的两次前向传递优化 HAG 参数，进一步完善了这一过程，以改进特征重新加权。我们的评估涵盖了多个数据集，包括用于广泛影响评估的 CIFAR-100 和专注于息肉大小估计的专业内窥镜数据集（REAL-Colon、Misawa 和 SUN），涵盖超过 370,000 帧图像中的 200 多个息肉。研究结果表明，我们增强的 HAG 网络大大提高了与息肉大小相关的二分类和三分类任务的性能。具体而言，CNN 在二分类中的 F1 分数提高到 87.8%，而在三分类中，ViT-T 模型的 F1 分数达到 76.5%，优于传统的 CNN 和 ViT-T 模型。为了促进进一步的研究，我们发布了代码库，其中包括 CNN、多流 CNN、ViT 和 HAG 增强变体的实现。该资源旨在标准化内窥镜数据集的使用，为胃肠道息肉大小估计提供公开的训练-验证-测试拆分，以进行可靠和可比较的研究。代码库可在 github.com/cosmoimd/feature-selection-gates 获取。||
|**2024-07-05**|[Batch Transformer: Look for Attention in Batch](http://arxiv.org/abs/2407.04218)|null|人脸表情识别 (FER) 在计算机视觉领域受到了广泛关注，尤其是在人机交互等“自然环境”中。然而，FER 图像存在遮挡、低分辨率、姿态变化、光照变化和主观性等不确定因素，其中包括一些与目标标签不匹配的表情。因此，从一张包含噪声的单个图像中获取的信息很少，而且不可信。这可能会显著降低 FER 任务的性能。为了解决这个问题，我们提出了一种批量转换器 (BT)，它包含所提出的类别批量注意力 (CBA) 模块，通过训练一批图像中反映的特征，而不是来自单个图像的信息，来防止噪声数据过拟合并提取可靠信息。我们还提出了多级注意力 (MLA) 机制，通过捕获每个级别之间的相关性来防止过度拟合特定特征。在本文中，我们提出了一个结合上述方案的批量转换器网络 (BTN)。在各种 FER 基准数据集上的实验结果表明，所提出的 BTN 在 FER 数据集中始终优于最先进的方法。代表性结果证明了所提出的 BTN 在 FER 中的前景。||
|**2024-07-04**|[Adaptive Step-size Perception Unfolding Network with Non-local Hybrid Attention for Hyperspectral Image Reconstruction](http://arxiv.org/abs/2407.04024)|null|深度展开方法和Transformer架构最近在高光谱图像（HSI）重建方面展现出良好的结果。然而，仍然存在两个问题：（1）在数据子问题中，大多数方法使用可学习参数表示步长。然而，对于不同的光谱通道，特征和真实值之间的误差是不相等的。(2) Transformer难以平衡感受野大小和像素级细节信息。为了克服上述缺点，我们提出了一种自适应步长感知展开网络（ASPUN），这是一种基于FISTA算法的深度展开网络，它使用自适应步长感知模块来估计每个光谱通道的更新步长。此外，我们设计了一个非局部混合注意力Transformer（NHAT）模块，用于充分利用Transformer的感受野优势。通过将NLHA插入非局部信息聚合（NLIA）模块，展开网络可以获得更好的重建结果。实验结果表明，我们的ASPUN优于现有的SOTA算法，并取得了最佳性能。||
|**2024-07-03**|[Towards Attention-based Contrastive Learning for Audio Spoof Detection](http://arxiv.org/abs/2407.03514)|null|视觉Transformer（ViT）在计算机视觉分类任务中取得了重大进展。最近，Gong等人（2021）将基于注意力的建模方法引入了几项音频任务。然而，使用ViT进行音频欺骗检测任务的研究相对较少。我们弥合了这一差距，并将ViT引入到这项任务中。基于对SSAST（Gong等人，2022）音频ViT模型进行微调的朴素基线模型实现了次优的等错误率（EER）。为了提高性能，我们提出了一种新颖的基于注意力的对比学习框架（SSAST-CL），该框架使用交叉注意力来辅助表示学习。实验表明，我们的框架成功地 disentangled 了真实和欺骗类别，并有助于学习更好的分类器来完成这项任务。通过适当的数据增强策略，在我们的框架上训练的模型在ASVSpoof 2021挑战赛中取得了具有竞争力的性能。我们提供了比较和消融研究来证明我们的观点。||
|**2024-07-03**|[STF: Sentence Transformer Fine-Tuning For Topic Categorization With Limited Data](http://arxiv.org/abs/2407.03253)|null|如今，从推文中进行主题分类引起了相当多的研究关注。由于这些研究工作，人们提出了不同的分类系统。然而，由于标记数据的数量有限，导致性能指标低下，它们面临着重大挑战。我们提出了句子转换器微调 (STF)，这是一个主题检测系统，它利用预训练的句子转换器模型和微调来准确地对推文主题进行分类。此外，我们还进行了广泛的参数敏感性分析，以针对我们的主题分类任务微调 STF 参数，从而获得最佳性能结果。在两个基准数据集上的实验表明：(1) 所提出的 STF 可以有效地用于对推文主题进行分类，并且优于最新的最先进方法，(2) 所提出的 STF 不需要大量的标记推文就能达到良好的准确性，而这是许多最先进方法的局限性。我们的主要贡献是通过应用预训练的句子转换器语言模型在推文主题分类方面取得了可喜的成果。||
|**2024-07-03**|[Visual Grounding with Attention-Driven Constraint Balancing](http://arxiv.org/abs/2407.03243)|null|与目标检测不同，视觉定位任务需要检测由复杂的自由形式语言描述的对象。为了同时对这种复杂的语义和视觉表示进行建模，最近最先进的研究采用基于Transformer的模型来融合来自两种模态的特征，并进一步引入了各种模块来调制视觉特征，使其与语言表达对齐并消除不相关的冗余信息。然而，它们的损失函数仍然采用常见的目标检测损失，仅仅控制边界框回归输出，未能完全优化上述目标。为了解决这个问题，本文首先分析了基于Transformer模型的注意力机制。在此基础上，我们进一步提出了一个名为注意力驱动约束平衡（AttBalance）的新框架，以优化语言相关区域内视觉特征的行为。大量的实验结果表明，我们的方法带来了令人印象深刻的改进。具体来说，我们在四个不同基准上评估的五种不同模型上都取得了持续的改进。此外，通过将我们的方法集成到QRNet中，我们实现了新的最先进的性能。||
|**2024-07-03**|[Learning Disentangled Representation in Object-Centric Models for Visual Dynamics Prediction via Transformers](http://arxiv.org/abs/2407.03216)|null|最近的研究表明，以对象为中心的表示可以极大地提高学习动力学的准确性，同时也增强了可解释性。在这项工作中，我们将这一想法更进一步，提出了以下问题：“学习解耦表示能否进一步提高以对象为中心的模型中视觉动力学预测的准确性？” 虽然之前已经有一些尝试学习静态图像的解耦表示\citep{nsb}，但据我们所知，我们的工作是第一个尝试在视频的通用环境中做到这一点的工作，而没有对对象可能具有的属性类型做出任何特定假设。我们架构的关键构建块是“块”的概念，其中多个块共同构成一个对象。每个块都表示为给定数量的可学习概念向量的线性组合，并在学习过程中迭代优化。我们模型中的块是以无监督的方式发现的，通过类似于发现槽\citep{slot_attention}的方式关注对象掩码，以学习密集的以对象为中心的表示。我们在发现的块上采用 Transformer 进行自注意力机制来预测下一个状态，从而发现视觉动力学。我们在几个二维和三维基准数据集上进行了一系列实验，证明我们的架构 (1) 可以发现语义上有意义的块 (2) 与最先进的以对象为中心的模型相比，有助于提高动力学预测的准确性 (3) 在训练期间未见过特定属性组合的 OOD 设置中表现明显更好。我们的实验强调了发现解耦表示对于视觉动力学预测的重要性。||
|**2024-07-03**|[Relating CNN-Transformer Fusion Network for Change Detection](http://arxiv.org/abs/2407.03178)|**[link](https://github.com/nust-machine-intelligence-laboratory/rctnet)**|虽然深度学习，特别是卷积神经网络（CNN），已经彻底改变了遥感（RS）变化检测（CD），但现有方法由于忽略了全局上下文和不完整的变化学习，经常遗漏关键特征。此外，transformer 网络难以处理低级细节。RCTNet 通过引入以下内容解决了这些限制：\textbf{(1)} 早期融合骨干网络，用于尽早利用空间和时间特征；\textbf{(2)} 跨阶段聚合（CSA）模块，用于增强时间表示；\textbf{(3)} 多尺度特征融合（MSF）模块，用于在解码器中丰富特征提取；\textbf{(4)} 高效自解密注意力（ESA）模块，利用 Transformer 捕获全局信息和细粒度细节，以实现准确的变化检测。大量实验表明，RCTNet 明显优于传统的遥感图像变化检测方法，显示出显著的改进，并在准确性和计算成本之间取得了最佳平衡。||
|**2024-07-03**|[ISWSST: Index-space-wave State Superposition Transformers for Multispectral Remotely Sensed Imagery Semantic Segmentation](http://arxiv.org/abs/2407.03033)|null|目前，多光谱遥感图像(MSRSI) 语义分割任务面临以下问题：1) 通常只考虑单域特征（即空间域或频率域）；2) 编码器中的下采样操作通常会导致边缘提取精度损失；3) 没有充分考虑 MSRSI 的多通道特征；4) 没有充分利用遥感的先验知识。为了解决上述问题，受量子力学的启发，首次提出了一种用于 MSRSI 语义分割的指标-空间-波态叠加Transformer (ISWSST)，其优势如下：1) 通过自适应投票决策（即集成学习思想）叠加或融合指标、空间和波态来模拟量子叠加，从而成为更强大的分类器并提高分割精度；2) 设计了一种无损小波金字塔编码器-解码器模块，基于小波变换和逆小波变换对图像进行无损重建，模拟量子纠缠，避免边缘提取损失；3) 提出结合多光谱特征（即遥感指数和通道注意力机制）从原始分辨率图像中准确提取地面物体；4) 引入量子力学来解释 ISWSST 的潜在优势。实验表明，ISWSST 在 MSRSI 分割任务中得到了验证，并且优于最先进的架构，有效地提高了分割和边缘提取的精度。代码将在我们的论文被接受后公开。||
|**2024-07-03**|[Graph and Skipped Transformer: Exploiting Spatial and Temporal Modeling Capacities for Efficient 3D Human Pose Estimation](http://arxiv.org/abs/2407.02990)|null|近年来，单目三维人体姿态估计 (HPE) 中的 2D 到 3D 姿态提升引起了广泛的研究兴趣。基于 GNN 的方法和基于 Transformer 的方法由于其先进的空间和时间特征学习能力，已成为主流架构。然而，现有方法通常在空间和时间域中构建关节和帧注意力对齐，导致密集连接，从而引入相当大的局部冗余和计算开销。在本文中，我们采用全局方法来利用时空信息，并通过简洁的图和跳跃 Transformer 架构实现高效的 3D HPE。具体来说，在空间编码阶段，部署粗粒度身体部位以构建具有完全数据驱动自适应拓扑的空间图网络，确保模型在各种姿态下的灵活性和泛化能力。在时间编码和解码阶段，提出了一种简单而有效的跳跃 Transformer 来捕获长期时间依赖性并实现分层特征聚合。还开发了一种直接的数据滚动策略，将动态信息引入 2D 姿态序列。在 Human3.6M、MPI-INF-3DHP 和 Human-Eva 基准测试中进行了广泛的实验。G-SFormer 系列方法与以前的最先进技术相比，仅用大约 10% 的参数就实现了卓越的性能，并显着降低了计算复杂度。此外，G-SFormer 还表现出对检测到的 2D 姿态不准确性的出色鲁棒性。||
|**2024-07-03**|[ADFQ-ViT: Activation-Distribution-Friendly Post-Training Quantization for Vision Transformers](http://arxiv.org/abs/2407.02763)|null|视觉Transformer（ViT）在各种计算机视觉任务中都表现出色，但其庞大的参数量导致内存和计算需求显著增加，阻碍了其在资源受限设备上的有效推理。量化已成为缓解这些挑战的一种很有前景的解决方案，但现有方法在低比特情况下仍然存在显著的精度损失。我们将此问题归因于ViT中LayerNorm后和GELU后激活的独特分布，这使得传统的硬件友好型量化器效率低下，尤其是在低比特情况下。为了解决这个问题，我们提出了一种名为Activation-Distribution-Friendly post-training Quantization for Vision Transformers (ADFQ-ViT)的新颖框架。具体来说，我们引入了Per-Patch Outlier-aware Quantizer来处理LayerNorm后激活中的不规则异常值。该量化器在保持阈值以上最小值子集全精度的情况下，将均匀量化器的粒度细化到每个补丁级别。为了处理GELU后激活在正负区域之间的非均匀分布，我们设计了Shift-Log2 Quantizer，它将所有元素移位到正区域，然后应用log2量化。此外，我们提出了Attention-score enhanced Module-wise Optimization，通过重构误差来调整每个量化器的参数，以进一步减轻量化误差。大量实验表明，ADFQ-ViT在4比特图像分类、目标检测和实例分割任务中比各种基线都有显著改进。具体来说，在将ViT-B模型量化到4比特时，我们在ImageNet数据集上的Top-1准确率提高了10.23%。||
|**2024-07-02**|[A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models](http://arxiv.org/abs/2407.02646)|**[link](https://github.com/dakingrai/awesome-mechanistic-interpretability-lm-papers)**|机械可解释性 (MI) 是可解释性的一个新兴子领域，旨在通过逆向工程神经网络模型的内部计算来理解它。近年来，MI 在解释基于 Transformer 的语言模型 (LM) 方面引起了极大的关注，产生了许多新颖的见解，但也带来了新的挑战。然而，目前还没有工作全面回顾这些见解和挑战，特别是作为该领域新人的指南。为了填补这一空白，我们提供了一份全面的综述，概述了 MI 中的基本研究对象、用于其研究的技术、评估 MI 结果的方法，以及使用 MI 理解 LM 所产生的重要发现和应用。特别是，我们为初学者提供了一个路线图，以帮助他们在该领域导航并利用 MI 为自己谋福利。最后，我们还指出了该领域目前的差距，并讨论了未来可能的发展方向。||
|**2024-07-02**|[On the Anatomy of Attention](http://arxiv.org/abs/2407.02423)|null|我们引入一种范畴论图表形式体系，以便系统地关联和推理机器学习模型。我们的图表以直观的方式呈现架构，但不会丢失必要的细节，其中模型之间的自然关系通过图形变换来捕捉，并且重要的差异和相似性可以一目了然地识别出来。在本文中，我们将重点关注注意力机制：将民间传说转化为数学推导，并构建文献中注意力变体的分类法。作为以我们的形式主义为基础的实证研究的第一个例子，我们确定了注意力机制中反复出现的解剖学成分，我们对其进行了详尽的重组，以探索注意力机制的变化空间。||
|**2024-07-02**|[Efficient Sparse Attention needs Adaptive Token Release](http://arxiv.org/abs/2407.02328)|null|近年来，大型语言模型 (LLM) 在各种以文本为中心的的任务中展现出卓越的能力。然而，其“大”规模带来了巨大的计算和存储挑战，尤其是在管理 Transformer 的键值状态方面，这限制了其更广泛的适用性。因此，我们建议自适应地从缓存中释放资源并重建必要的键值状态。特别是，我们通过一个轻量级的控制器模块来近似理想的top- $K$稀疏注意力来实现这一点。该模块保留具有最高 top-$K$ 注意力权重的标记，并同时重建已丢弃但必要的标记，这些标记可能对未来的解码至关重要。自然语言生成和建模方面的综合实验表明，我们的方法不仅在性能方面与完全注意力机制相比具有竞争力，而且还实现了高达 221.8% 的显著吞吐量提升。复制代码可在 https://github.com/WHUIR/ADORE 上获得。||

<p align=right>(<a href=#updated-on-20240717>back to top</a>)</p>

