## Updated on 2024.06.29
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#多模态>多模态</a></li>
    <li><a href=#6dof-object-pose>6DOF Object Pose</a></li>
    <li><a href=#nerf>nerf</a></li>
    <li><a href=#分类/检测/识别/分割>分类/检测/识别/分割</a></li>
    <li><a href=#模型压缩/优化>模型压缩/优化</a></li>
    <li><a href=#ocr>OCR</a></li>
    <li><a href=#生成模型>生成模型</a></li>
    <li><a href=#llm>LLM</a></li>
    <li><a href=#transformer>Transformer</a></li>
    <li><a href=#3dgs>3DGS</a></li>
    <li><a href=#各类学习方式>各类学习方式</a></li>
  </ol>
</details>

## 多模态

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-27**|[OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding](http://arxiv.org/abs/2406.19389)|null|当前的通用分割方法在像素级图像和视频理解方面表现出强大的能力。然而，它们缺乏推理能力，无法通过文本指令进行控制。相比之下，大型视觉语言多模态模型表现出强大的基于视觉的对话和推理能力，但缺乏像素级理解，难以接受视觉提示以实现灵活的用户交互。本文提出了 OMG-LLaVA，这是一个将强大的像素级视觉理解与推理能力相结合的全新优雅框架。它可以接受各种视觉和文本提示，以实现灵活的用户交互。具体来说，我们使用通用分割方法作为视觉编码器，将图像信息、感知先验和视觉提示整合到提供给 LLM 的视觉标记中。LLM 负责理解用户的文本指令，并根据视觉信息提供文本响应和像素级分割结果。我们提出了感知先验嵌入，以更好地将感知先验与图像特征相结合。OMG-LLaVA 在单个模型中实现了图像级、对象级和像素级的推理和理解，在多个基准测试中达到或超过了专门方法的性能。我们的工作目标不是使用 LLM 连接每个专家，而是在一个编码器、一个解码器和一个 LLM 上进行端到端训练。代码和模型已发布，以供进一步研究。||
|**2024-06-27**|[FlowVQA: Mapping Multimodal Logic in Visual Question Answering with Flowcharts](http://arxiv.org/abs/2406.19237)|null|现有的视觉问答基准测试缺乏视觉基础和复杂性，尤其是在评估空间推理能力方面。我们引入了 FlowVQA，这是一个旨在评估视觉问答多模态语言模型在使用流程图作为视觉上下文进行推理方面的能力的新基准。FlowVQA 包含 2,272 张从三个不同内容来源精心生成并经过人工验证的流程图图像，以及 22,413 个不同的问答对，用于测试一系列推理任务，包括信息定位、决策制定和逻辑推理。我们使用各种策略对一套开源和专有多模态语言模型进行了全面的基线评估，然后分析了方向偏差。结果强调了该基准作为推进多模态建模领域的重要工具的潜力，为提高模型在视觉和逻辑推理任务中的性能提供了一个集中且具有挑战性的环境。||
|**2024-06-27**|[RAVEN: Multitask Retrieval Augmented Vision-Language Learning](http://arxiv.org/abs/2406.19150)|null|将大型语言模型扩展到将全世界知识编码到模型参数中是不可持续的，并且加剧了资源障碍。检索增强生成（RAG）提供了一种潜在的解决方案，但其在视觉语言模型（VLM）中的应用尚未得到充分探索。现有方法侧重于为单一任务设计的模型。此外，它们受到资源密集型预训练需求、额外参数需求、未解决的模态优先级以及相对于非检索基线的缺乏明显优势的限制。本文介绍了RAVEN，一个多任务检索增强VLM框架，通过高效、特定于任务的微调来增强基础VLM。通过集成检索增强样本而不需要额外的检索特定参数，我们展示了该模型获得了跨多个任务有效的检索属性。我们在图像描述和VQA任务中对检索模态进行的结果和广泛的消融研究表明，与非检索基线相比，性能有了显著提高：MSCOCO上+1 CIDEr，NoCaps上+4 CIDEr，特定VQA问题类型上接近+3%的准确率。这突出了将RAG方法应用于VLM的有效性，标志着朝着更高效、更易访问的多模态学习迈出了一步。||
|**2024-06-27**|[Evidential Concept Embedding Models: Towards Reliable Concept Explanations for Skin Disease Diagnosis](http://arxiv.org/abs/2406.19130)|**[link](https://github.com/obiyoag/evi-cem)**|由于医疗决策的高风险性，人们迫切需要可解释的深度学习方法来进行医学图像分析。概念瓶颈模型 (CBM) 已成为一种活跃的可解释框架，它将人类可解释的概念纳入决策过程。然而，当应用于临床诊断时，它们的预测结果可能缺乏可靠性，从而影响概念解释的质量。为了解决这个问题，我们提出了一种基于证据的概念嵌入模型 (evi-CEM)，该模型采用证据学习来模拟概念的不确定性。此外，我们建议利用概念不确定性来纠正使用视觉语言模型训练 CBM 时出现的概念错位，而无需完整的概念监督。通过所提出的方法，我们可以提高监督和标签高效设置下概念解释的可靠性。此外，我们引入了概念不确定性，以便在测试时进行有效的干预。我们的评估表明，evi-CEM 在概念预测方面取得了优异的性能，并且所提出的概念校正方法有效地减轻了标签高效训练中的概念错位。我们的代码可在 https://github.com/obiyoag/evi-CEM 获取。||
|**2024-06-27**|[RoboUniView: Visual-Language Model with Unified View Representation for Robotic Manipulaiton](http://arxiv.org/abs/2406.18977)|null|利用视觉语言模型 (VLM) 进行机器人操控是一种新兴范式，旨在增强模型对新物体和指令的泛化能力。然而，由于相机规格和安装位置的差异，现有方法在不同机器人平台上表现出显著的性能差异。为了解决这一挑战，我们在本文中提出了 RoboUniView，这是一种将视觉特征提取与动作学习解耦的创新方法。我们首先通过对易于获取的数据进行预训练，从多视角视图中学习统一的视图表示，然后从该统一视图表示中导出动作以控制机器人操控。这种统一的视图表示更准确地反映了物理世界，并且不受机器人平台相机参数的限制。得益于这种方法，我们在要求苛刻的 CALVIN 基准测试中取得了最先进的性能，将  $D \to D$ 设置下的成功率从 88.7% 提高到 96.2%，在 $ABC \to D$ 设置下的成功率从 82.4% 提高到 94.2%。此外，我们的模型表现出出色的适应性和灵活性：它在未知相机参数下保持高性能，可以使用具有不同相机参数的多个数据集，并且能够跨数据集进行联合跨任务学习。代码已提供用于复现。https://github.com/liufanfanlff/RoboUniview||
|**2024-06-27**|[Advancing Cross-domain Discriminability in Continual Learning of Vison-Language Models](http://arxiv.org/abs/2406.18868)|null|基于视觉语言模型 (VLMs) 的持续学习 (CL) 克服了传统持续学习只关注先前遇到过的类别的限制。在 VLMs 的持续学习过程中，我们不仅需要防止对增量学习知识的灾难性遗忘，还需要保持 VLMs 的零样本学习能力。然而，现有方法需要额外的参考数据集来维持这种零样本学习能力，并且依赖于领域身份提示来对跨不同领域的图像进行分类。在本研究中，我们提出了基于回归的分析增量学习 (RAIL) 方法，该方法利用基于递归岭回归的适配器，以一种不遗忘的方式从一系列领域中学习，并通过将特征投影到更高维空间来解耦跨领域的相关性。结合免训练的融合模块，RAIL 无需任何参考数据即可绝对保留 VLM 对未见领域的零样本学习能力。此外，我们引入了跨领域任务无关增量学习 (X-TAIL) 设置。在这种设置中，持续学习器需要从多个领域增量学习，并在没有任何领域身份提示的情况下对来自已见和未见领域的测试图像进行分类。我们从理论上证明了 RAIL 对增量学习领域的绝对记忆能力。实验结果证实了 RAIL 在 X-TAIL 和现有多领域任务增量学习设置中均达到了最先进的性能。代码将在论文被接收后发布。||
|**2024-06-27**|[MissionGNN: Hierarchical Multimodal GNN-based Weakly Supervised Video Anomaly Recognition with Mission-Specific Knowledge Graph Generation](http://arxiv.org/abs/2406.18815)|null|在不同领域安全问题日益严峻的背景下，视频异常检测 (VAD) 和视频异常识别 (VAR) 任务对于智能监控、证据调查、暴力警报等应用至关重要。这些任务旨在识别和分类视频数据中与正常行为的偏差，但由于异常的罕见性导致数据极度不平衡，以及用于监督学习的广泛帧级数据标注不切实际，因此面临着重大挑战。本文介绍了一种新颖的基于分层图神经网络 (GNN) 的模型 MissionGNN，该模型通过利用最先进的大型语言模型和全面的知识图谱来解决 VAR 中高效弱监督学习的挑战。我们的方法通过避免对大型多模态模型进行繁重的梯度计算并实现无固定视频分割的完全帧级训练，克服了先前方法的局限性。我们的模型利用自动化的、特定于任务的知识图谱生成，为实时视频分析提供了一种实用且高效的解决方案，而不受先前基于分割或多模态方法的限制。在基准数据集上的实验验证证明了我们的模型在 VAD 和 VAR 中的性能，突出了其重新定义视频监控系统中异常检测和识别领域的潜力。||
|**2024-06-26**|[Towards Open-World Grasping with Large Vision-Language Models](http://arxiv.org/abs/2406.18722)|null|从开放式语言指令中抓取野外环境中的物体是机器人技术的一项基本挑战。一个开放世界的抓取系统应该能够结合高级语义推理和低级物理几何推理，以便适用于任意场景。最近的研究利用大型语言模型 (LLM) 中固有的网络规模知识在机器人环境中进行规划和推理，但依赖于外部视觉和动作模型将此类知识接地到环境中并参数化驱动。这种设置存在两个主要瓶颈：a) LLM 的推理能力受到视觉基础质量的限制，以及 b) LLM 不包含对世界的低级空间理解，这对于在接触丰富的场景中进行抓取至关重要。在这项工作中，我们证明了现代视觉语言模型 (VLM) 能够解决这些限制，因为它们隐式地接地，并且可以联合推理语义和几何。我们提出了 OWG，这是一个开放世界抓取管道，它将 VLM 与分割和抓取合成模型相结合，以在三个阶段解锁基础世界理解：开放式参考分割、基于基础的抓取规划和通过接触推理进行抓取排名，所有这些都可以通过合适的视觉提示机制零样本应用。我们在杂乱的室内场景数据集中进行了广泛的评估，以展示 OWG 从开放式语言中进行基础的稳健性，以及在模拟和硬件中进行的开放世界机器人抓取实验，证明了其与以前的有监督和零样本 LLM 方法相比具有优越的性能。||
|**2024-06-26**|[S3: A Simple Strong Sample-effective Multimodal Dialog System](http://arxiv.org/abs/2406.18305)|**[link](https://github.com/s-nlp/s3)**|在这项工作中，我们为多模态对话任务提出了一个概念简单但功能强大的基线模型 S3，该模型在两个引人注目的排行榜 MMMU 和 AI Journey Contest 2023 上均取得了接近最先进水平的结果。该系统基于预训练的大型语言模型、预训练的图像和音频模态编码器以及可训练的模态投影器。提出的用于训练此类架构的有效数据混合表明，基于强大的语言模型并在少量多模态数据上训练的多模态模型可以在多模态对话任务中高效执行。||
|**2024-06-26**|[Speech2UnifiedExpressions: Synchronous Synthesis of Co-Speech Affective Face and Body Expressions from Affordable Inputs](http://arxiv.org/abs/2406.18068)|**[link](https://github.com/UttaranB127/speech2unified_expressions)**|我们提出了一种基于多模态学习的方法，利用商用相机拍摄的RGB视频数据，同时合成数字角色的同步语音面部表情和上半身姿势。我们的方法从稀疏的人脸关键点和上半身关节学习，这些关键点和关节直接从视频数据中估计，以生成逼真的情感角色动作。给定语音音频波形以及从视频中计算出的说话者面部关键点运动和身体关节运动的标记序列，我们的方法合成说话者面部关键点和身体关节的运动序列，以匹配语音的内容和情感。我们设计了一个生成器，它由一组编码器组成，将所有输入转换为捕捉其相关性的多模态嵌入空间，然后是一对解码器，用于合成所需的面部和姿势运动。为了增强合成的合理性，我们使用了一个对抗性鉴别器，它学习根据情感表达来区分从原始视频计算出的面部和姿势运动与我们合成的运动。为了评估我们的方法，我们扩展了TED Gesture数据集，使其除了身体姿势外，还包括视图标准化的、同步语音的人脸关键点。我们通过对多个评估指标的全面定量和定性实验以及用户研究，展示了我们方法的性能。我们观察到，我们的方法产生了较低的重建误差，并为数字角色生成了具有多样面部表情和身体姿势的合成样本。||
|**2024-06-25**|[EXTRACT: Efficient Policy Learning by Extracting Transferrable Robot Skills from Offline Data](http://arxiv.org/abs/2406.17768)|null|大多数强化学习 (RL) 方法侧重于学习低级动作空间上的最优策略。虽然这些方法可以在其训练环境中表现良好，但它们缺乏迁移到新任务的灵活性。相反，能够执行有用的、时间扩展的技能而不是低级动作的 RL 智能体可以更容易地学习新任务。先前在基于技能的 RL 方面的工作要么需要专家监督来定义有用的技能（这难以扩展），要么使用启发式方法从离线数据中学习技能空间，这限制了技能的适应性，使得它们难以在后续 RL 中迁移。我们的方法 EXTRACT 则利用预先训练的视觉语言模型，从离线数据中提取一组离散的语义上有意义的技能，每个技能都由连续的参数进行参数化，无需人工监督。这种技能参数化使机器人只需学习何时选择特定技能以及如何针对特定任务修改其参数，即可学习新任务。我们通过在稀疏奖励、基于图像的机器人操作环境中的实验表明，EXTRACT 比先前的工作可以更快地学习新任务，在样本效率和性能方面比先前的基于技能的 RL 有重大提高。网站：https://www.jessezhang.net/projects/extract/。||
|**2024-06-25**|[DKPROMPT: Domain Knowledge Prompting Vision-Language Models for Open-World Planning](http://arxiv.org/abs/2406.17659)|null|视觉语言模型 (VLM) 已被应用于机器人任务规划问题，其中机器人接收自然语言的任务并根据视觉输入生成计划。虽然当前的 VLM 已经展示出强大的视觉语言理解能力，但它们在规划任务中的表现仍远不能令人满意。同时，虽然经典的任务规划器（例如基于 PDDL 的规划器）擅长规划长期任务，但它们在不可预见的情况很常见的开放世界中表现不佳。在本文中，我们提出了一种新的任务规划和执行框架，称为 DKPROMPT，它使用 PDDL 中的领域知识自动生成 VLM 提示，用于开放世界中的经典规划。定量实验结果表明，DKPROMPT 在任务完成率方面优于经典规划、纯 VLM 方法和其他一些竞争基线。||
|**2024-06-25**|[Towards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights](http://arxiv.org/abs/2406.17430)|null|大型多模态模型 (LMM) 近年来取得了巨大成功，展现出理解多模态信息和与人类用户交互的强大能力。尽管取得了进展，但在多模态环境中，特别是在语音模态中，检测高风险交互的挑战在很大程度上仍未得到探索。传统的语音模态风险研究主要强调内容（例如，转录的内容）。然而，在基于语音的交互中，音频中的副语言线索会显著改变话语背后的预期含义。在这项工作中，我们提出了一个特定于语音的风险分类法，涵盖了敌意（恶意讽刺和威胁）、恶意模仿（年龄、性别、种族）和刻板印象偏见（年龄、性别、种族）下的 8 个风险类别。基于该分类法，我们创建了一个小规模数据集，用于评估当前 LMM 检测这些风险类别的能力。我们观察到即使是最新的模型仍然无法有效地检测语音中各种特定于副语言的风险（例如，Gemini 1.5 Pro 的表现仅略高于随机基线）。警告：本文包含有偏见和冒犯性的示例。||
|**2024-06-24**|[Evaluating the Quality of Hallucination Benchmarks for Large Vision-Language Models](http://arxiv.org/abs/2406.17115)|**[link](https://github.com/hqhbench/hqhbench)**|近年来，尽管大型视觉语言模型（LVLMs）发展迅速且性能优异，但LVLMs一直受到幻觉问题的困扰，即LVLMs倾向于生成与相应视觉输入不一致的响应。为了评估LVLMs中幻觉的程度，以前的工作提出了一系列具有不同类型任务和评估指标的基准测试。然而，我们发现现有幻觉基准测试的质量参差不齐，有些存在问题，例如重复测试下评估结果不一致以及与人工评估不一致。为此，我们提出了一个幻觉基准质量测量框架（HQM），它利用各种指标分别评估现有幻觉基准测试的可靠性和有效性。具体来说，对于可靠性，我们探讨了重测信度和复本信度，而对于有效性，我们检验了标准效度和幻觉类型覆盖率。此外，根据我们的质量测量结果，我们构建了一个用于 LVLMs 的高质量幻觉基准（HQH）。我们对超过 10 个具有代表性的 LVLMs 进行了广泛的评估，包括 GPT-4o 和 Gemini-Vision-Pro，以深入分析现有模型中的幻觉问题。我们的基准测试公开发布于 https://github.com/HQHBench/HQHBench。||
|**2024-06-24**|[At First Sight: Zero-Shot Classification of Astronomical Images with Large Multimodal Models](http://arxiv.org/abs/2406.17057)|null|视觉-语言多模态模型 (VLM) 为天文学中的零样本分类提供了可能性：即通过自然语言提示进行分类，无需训练。我们研究了两种模型，GPT-4o 和 LLaVA-NeXT，用于对低表面亮度星系和伪影进行零样本分类，以及对星系进行形态分类。我们发现，使用自然语言提示，这些模型无需额外训练/微调即可达到很高的准确率（通常高于 80%）。我们讨论了需要改进的地方，特别是对于开源模型 LLaVA-NeXT。我们的研究结果旨在鼓励天文界将 VLM 视为研究和教学的有力工具，并展望未来定制构建或微调的模型可以表现得更好。||
|**2024-06-24**|[Long Context Transfer from Language to Vision](http://arxiv.org/abs/2406.16852)|**[link](https://github.com/evolvinglmms-lab/longva)**|视频序列提供了宝贵的时间信息，但现有的多模态大模型 (LMM) 在理解超长视频方面存在不足。许多研究试图通过使用视觉重采样器来减少视觉标记的数量来解决这个问题。 然而，在本文中，我们从语言模型的角度来解决这个问题。我们仅通过扩展语言主干的上下文长度，就可以使 LMM 理解多一个数量级的视觉标记，而无需任何视频训练。我们将这种现象称为长上下文迁移，并仔细探讨了其特性。为了有效地衡量 LMM 泛化到视觉模态长上下文的能力，我们开发了 V-NIAH（视觉大海捞针），这是一个受语言模型 NIAH 测试启发的纯合成长视觉基准测试。我们提出的长视频助手 (LongVA) 可以处理 2000 帧或超过 20 万个视觉标记，而无需增加额外的复杂性。凭借其扩展的上下文长度，LongVA 通过密集采样更多输入帧，在 7B 级模型中实现了 Video-MME 的最新性能。我们的工作已开源，网址为 https://github.com/EvolvingLMMs-Lab/LongVA。||
|**2024-06-24**|[Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts](http://arxiv.org/abs/2406.16851)|null|我们提出了LoCoVQA，这是一个用于评估视觉语言模型（VLM）长上下文提取推理的动态基准生成器。LoCoVQA使用由分布内和分布外干扰图像组成的越来越长的视觉上下文，增强了数学推理、视觉问答和字符识别任务的测试示例。在这些任务中，随着视觉上下文长度的增加，各种VLM的性能迅速下降，通常呈现出惊人的指数衰减趋势。此测试评估了VLM在回答问题时忽略无关信息的能力——这项任务对于文本领域的语言模型（LM）来说相当容易——这表明当前最先进的VLM缺乏许多长上下文应用的基本能力。||
|**2024-06-24**|[Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark with Human-VLM Collaboration](http://arxiv.org/abs/2406.16469)|null|为了创建具有文化包容性的视觉语言模型 (VLM)，首要任务是开发一个测试基准，用于诊断模型响应反映文化元素问题的能力。本文阐述了此类基准的必要性，并指出现有研究依赖于人工标注，这阻碍了多样性和效率。我们提出了一个用于构建文化 VLM 基准的半自动化流程，以增强多样性和效率。该流程利用人机协作，其中 VLM 根据指南、人工标注的示例和图像相关的知识生成问题，然后由母语人士审查其质量和文化相关性。我们通过一个具体的应用案例证明了该流程的有效性：创建一个针对韩国文化的定制数据集，名为 K-Viscuit。最终的基准测试包含两种类型的问题：类型 1 问题测量视觉识别能力，而类型 2 问题评估细粒度的视觉推理能力。这确保了对 VLM 模型各个方面的全面诊断。我们使用 K-Viscuit 进行的评估表明，开源模型在理解韩国文化方面明显落后于专有模型，突出了需要改进的领域。我们提供了对不同文化方面 VLM 表现的多种分析。此外，我们还探索了结合外部知识检索以增强生成过程的潜力，为未来改进 VLM 的文化解释能力指明了方向。我们的数据集和代码将公开发布。||
|**2024-06-24**|[High-resolution open-vocabulary object 6D pose estimation](http://arxiv.org/abs/2406.16384)|null|在6D姿态估计任务中，对未见过物体的泛化是一个非常具有挑战性的问题。虽然视觉语言模型（VLM）能够使用自然语言描述来支持对未见过物体的6D姿态估计，但与基于模型的方法相比，这些解决方案的性能较差。在这项工作中，我们提出了Horyon，一个基于开放词汇VLM的架构，它解决了仅通过文本提示描述的未见过物体的两个场景之间的相对姿态估计问题。我们使用文本提示来识别场景中的未见过物体，然后获取高分辨率的多尺度特征。这些特征用于提取用于配准的跨场景匹配。我们在一个包含来自四个数据集（REAL275、Toyota-Light、Linemod和YCB-Video）的各种未见过物体的基准上评估了我们的模型。我们的方法在所有数据集上都达到了最先进的性能，平均召回率比之前性能最好的方法高出12.6。||
|**2024-06-24**|[What Do VLMs NOTICE? A Mechanistic Interpretability Pipeline for Noise-free Text-Image Corruption and Evaluation](http://arxiv.org/abs/2406.16320)|**[link](https://github.com/wrudman/NOTICE)**|视觉语言模型 (VLM) 因其能够集成视觉和文本输入以执行复杂任务而 gained gained community-spanning prominence。 尽管取得了成功，但这些模型的内部决策过程仍然是不透明的，这对高风险应用提出了挑战。 为了解决这个问题，我们引入了 NOTICE，这是第一个用于 VLM 机制可解释性的无噪声文本图像损坏和评估流程。 NOTICE 结合了用于图像损坏的语义最小对 (SMP) 框架和用于文本的对称标记替换 (STR)。 这种方法能够对两种模态进行语义上有意义的因果中介分析，从而提供了一种强大的方法来分析 BLIP 等模型中的多模态集成。 我们在 SVO-Probes、MIT-States 和面部表情识别数据集上的实验揭示了对 VLM 决策的关键见解，确定了中间层交叉注意力头的作用。 此外，我们发现了一组“通用交叉注意力头”，它们在不同的任务和模态中始终如一地发挥作用，每个头都执行不同的功能，例如隐式图像分割、对象抑制和异常值抑制。 这项工作为更透明和可解释的多模态系统铺平了道路。||
|**2024-06-21**|[Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning](http://arxiv.org/abs/2406.15334)|null|最近交错式大型多模态模型 (LMM) 在少样本学习方面的成功表明，包含许多示例的上下文学习 (ICL) 对于学习新任务来说很有前景。然而，这种多样本多模态 ICL 设置存在一个关键问题：它从根本上受到预训练时设置的模型上下文长度的限制。这个问题在多模态领域尤为突出，因为它需要处理文本和图像，需要额外的标记。这促使人们需要一种多模态方法，在不进行微调的情况下将许多样本压缩成更少的标记。在这项工作中，我们利用多模态任务向量 (MTV) 使 LMM 能够执行多模态、多样本的上下文学习，MTV 是压缩在模型注意力头中的上下文示例的紧凑隐式表示。具体来说，我们首先证明了 LMM 中存在这种 MTV，然后利用这些提取的 MTV 为各种视觉和语言任务实现多样本上下文学习。我们的实验表明，MTV 的性能可以随着压缩样本数量的增加而扩展，并且可以泛化到类似的域外任务，而无需额外的推理上下文长度。||
|**2024-06-21**|[DiPEx: Dispersing Prompt Expansion for Class-Agnostic Object Detection](http://arxiv.org/abs/2406.14924)|null|类无关目标检测 (OD) 可以是许多下游视觉任务的基石或瓶颈。尽管自下而上和多目标发现方法利用基本视觉线索识别显著目标方面取得了相当大的进步，但由于目标类型的多样性及其上下文复杂性，持续实现高召回率仍然很困难。在这项工作中，我们研究了使用视觉语言模型 (VLM) 通过自监督提示学习策略来增强目标检测。我们的初步研究结果表明，手动制作的文本查询通常会导致目标未被检测到，主要是因为当查询词表现出语义重叠时，检测置信度会降低。为了解决这个问题，我们提出了一种分散提示扩展 (DiPEx) 方法。DiPEx 逐步学习扩展一组不同的、非重叠的超球面提示，以提高召回率，从而提高下游任务（如分布外 OD）的性能。具体来说，DiPEx 通过自训练通用父提示并选择语义不确定性最高的提示以进行进一步扩展来启动该过程。生成的子提示预计将继承其父提示的语义，同时捕获更细粒度的语义。我们应用分散损失来确保子提示之间的高类间差异，同时保持父子提示对之间的语义一致性。为了防止提示集过度增长，我们利用语义空间的最大角度覆盖 (MAC) 作为提前终止的标准。我们通过 MS-COCO 和 LVIS 上广泛的类无关 OD 和 OOD-OD 实验证明了 DiPEx 的有效性，在 AR 中超过其他提示方法高达 20.1%，并在 AP 中比 SAM 实现了 21.3% 的改进。代码可在 https://github.com/jason-lim26/DiPEx 获取。||
|**2024-06-21**|[From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking](http://arxiv.org/abs/2406.14859)|null|大型语言模型（LLM）和多模态大型语言模型（MLLM）的快速发展，也暴露了其面对各种对抗性攻击的脆弱性。本文全面概述了针对LLM和MLLM的越狱研究，重点介绍了评估基准、攻击技术和防御策略方面的最新进展。与较为成熟的单模态越狱相比，多模态领域的探索还远远不够。我们总结了多模态越狱的局限性和潜在研究方向，旨在激发未来的研究，并进一步增强MLLM的稳健性和安全性。||
|**2024-06-21**|[Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models](http://arxiv.org/abs/2406.14852)|null|大型语言模型 (LLM) 和视觉语言模型 (VLM) 在各种任务和领域都表现出卓越的性能。尽管前景广阔，但空间理解和推理——人类认知的基本组成部分——仍然缺乏深入研究。我们开发了涵盖空间推理各个方面的新基准，例如关系理解、导航和计数。我们对具有竞争力的语言和视觉语言模型进行了全面评估。我们的研究结果揭示了文献中被忽视的几个反直觉的见解：(1) 空间推理提出了重大挑战，竞争模型的性能可能落后于随机猜测；(2) 尽管有额外的视觉输入，但 VLM 的性能往往不如 LLM；(3) 当文本和视觉信息都可用时，如果提供足够的文本线索，多模态语言模型对视觉信息的依赖就会降低。此外，我们证明利用视觉和文本之间的冗余可以显着提高模型性能。我们希望我们的研究能够为多模态模型的开发提供信息，以提高空间智能，并进一步缩小与人类智能的差距。||
|**2024-06-20**|[Evaluating vision-capable chatbots in interpreting kinematics graphs: a comparative study of free and subscription-based models](http://arxiv.org/abs/2406.14685)|null|本研究调查了八个基于大型多模态模型 (LMM) 的聊天机器人在运动学图形理解测试 (TUG-K) 中的表现，该测试是一种基于研究的概念清单。图形是 STEM 和医学领域广泛使用的表示形式，这使得它们成为探索基于 LMM 的聊天机器人的视觉解释能力的相关主题。我们评估了免费提供的聊天机器人（Gemini 1.0 Pro、Claude 3 Sonnet、Microsoft Copilot 和 ChatGPT-4o）和基于订阅的聊天机器人（Gemini 1.0 Ultra、Gemini 1.5 Pro API、Claude 3 Opus 和 ChatGPT-4）。我们发现 OpenAI 的聊天机器人的性能优于所有其他聊天机器人，其中 ChatGPT-4o 的整体性能最佳。与预期相反，我们发现免费版和订阅版 Gemini 和 Claude 3 聊天机器人的整体性能没有显着差异，但通过 API 提供的 Gemini 1.5 Pro 除外。此外，我们发现，与需要视觉解释的任务相比，更依赖语言输入的任务通常更容易被聊天机器人理解。该研究为在 STEM 和医学教育中考虑基于 LMM 的聊天机器人应用提供了基础，并为未来的研究提出了方向。||
|**2024-06-20**|[HYPERmotion: Learning Hybrid Behavior Planning for Autonomous Loco-manipulation](http://arxiv.org/abs/2406.14655)|null|使机器人在不同环境中自主执行混合运动能力对于物料搬运、家务和工作辅助等长期任务非常有利。这需要广泛利用内在运动能力，从丰富的环境信息中提取可供性，以及规划物理交互行为。尽管最近的进展已经展示了令人印象深刻的人形全身控制能力，但它们难以在新任务中实现多功能性和适应性。在这项工作中，我们提出了HYPERmotion，这是一个根据不同场景中的任务学习、选择和规划行为的框架。我们将强化学习与全身优化相结合，为38个驱动关节生成运动，并创建一个运动库来存储学习到的技能。我们将大型语言模型（LLM）的规划和推理功能应用于复杂的运动操作任务，构建了一个由一系列基本行为组成的层次任务图，将低级执行与高级规划联系起来。通过利用提取的空间几何形状和二维观察与视觉语言模型（VLM）的交互，将知识融入机器人形态选择器，以便在单臂或双臂、腿式或轮式运动中选择适当的动作。仿真和现实世界的实验表明，学习到的运动可以有效地适应新任务，在非结构化场景中展示了从自由文本命令出发的的高度自主性。视频和网站：hy-motion.github.io/||
|**2024-06-20**|[ICAL: Continual Learning of Multimodal Agents by Transforming Trajectories into Actionable Insights](http://arxiv.org/abs/2406.14596)|null|大规模生成式语言模型和视觉语言模型（LLM 和 VLM）在决策和指令遵循的少样本上下文学习方面表现出色。然而，它们需要在其上下文窗口中包含高质量的示例演示。在这项工作中，我们提出疑问：LLM 和 VLM 是否可以从通用的、次优的演示中生成自己的提示示例？我们提出了上下文抽象学习 (ICAL)，这是一种从次优演示和人类反馈中构建多模态经验见解记忆的方法。在给定新领域中的噪声演示的情况下，VLM 通过修复低效动作并注释认知抽象来将轨迹抽象为通用程序：任务关系、对象状态变化、时间子目标和任务解释。当代理尝试在类似环境中执行轨迹时，这些抽象通过人类反馈进行交互式细化和调整。当在提示中用作示例时，生成的抽象显着提高了检索增强型 LLM 和 VLM 代理中的决策制定。我们的 ICAL 代理在 TEACh 中基于对话的指令遵循、VisualWebArena 中的多模态 Web 代理以及 Ego4D 中的动作预测方面均优于最先进的技术。在 TEACh 中，我们将目标条件成功率提高了 12.6%。在 VisualWebArena 中，我们的任务成功率从 SOTA 的 14.3% 提高到 22.7%。在 Ego4D 动作预测中，我们改进了少样本 GPT-4V，并与监督模型保持竞争力。我们展示了微调我们的检索增强型上下文代理会带来更多改进。我们的方法显着减少了对专家制作示例的依赖，并且始终优于缺乏此类见解的动作计划的上下文学习。||
|**2024-06-20**|[Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities](http://arxiv.org/abs/2406.14562)|null|当面对涉及视觉思维的问题时，人类会自然地切换推理模式，经常形成心理图像或绘制视觉辅助工具。大型语言模型通过将中间推理过程以文本形式表达为思维链，在算术和符号推理方面表现出良好的效果，但即使经过广泛的多模态预训练，它们仍然难以扩展这种能力来回答那些通过视觉推理很容易解决的文本查询。我们引入了一种简单的方法，即思维白板提示，以解锁跨模态多模态大型语言模型的视觉推理能力。思维白板提示为多模态大型语言模型提供了一个隐喻的“白板”，以便将推理步骤绘制成图像，然后将这些图像返回给模型以进行进一步处理。我们发现这可以在没有演示或专门模块的情况下完成，而是利用模型现有的使用 Matplotlib 和 Turtle 等库编写代码的能力。这种简单的方法在涉及视觉和空间推理的四项困难的自然语言任务上显示出最先进的结果。我们确定了 GPT-4o 使用思维链方法会严重失败的多种情况，包括超过一种情况下的准确率为 0%，而思维白板方法在相同情况下能够实现高达 92% 的准确率。我们详细探讨了该技术的成功之处及其错误来源。||
|**2024-06-20**|[Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs](http://arxiv.org/abs/2406.14544)|**[link](https://github.com/sparksjoe/prism)**|视觉语言模型 (VLM) 在解决各种视觉问题方面表现出非凡的能力，这需要强大的感知和推理能力。尽管由于现有 VLM 中视觉和推理的内在联系导致评估这两项能力存在固有困难，但独立评估它们对于模型改进至关重要。为了解决这个问题，我们提出了 Prism，这是一个旨在解耦视觉问答中涉及的感知和推理过程的创新框架。Prism 包含两个不同的阶段：感知阶段利用 VLM 以文本形式提取和表达视觉信息，以及推理阶段使用大型语言模型 (LLM) 根据提取的视觉信息制定响应。这种模块化设计能够系统地比较和评估专有和开源 VLM 的感知和推理优势。我们的分析框架提供了一些有价值的见解，强调了 Prism 作为视觉语言任务的经济高效解决方案的潜力。通过将专注于感知的简化 VLM 与专为推理设计的强大 LLM 相结合，Prism 在一般的视觉语言任务中取得了优异的结果，同时大大降低了训练和运营成本。定量评估表明，Prism 在使用 vanilla 2B LLaVA 和免费访问的 GPT-3.5 进行配置时，在严格的多模态基准 MMStar 上的性能与比其大 10 倍的 VLM 相当。该项目已发布在：https://github.com/SparksJoe/Prism。||
|**2024-06-20**|[MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding](http://arxiv.org/abs/2406.14515)|**[link](https://github.com/open-compass/vlmevalkit)**|大型视觉语言模型 (LVLMs) 的出现促进了对其在多模态环境下应用的研究，特别是在视频理解方面。传统的 VideoQA 基准测试虽然提供了量化指标，但往往无法涵盖视频内容的全部范围，并且无法充分评估模型的时间理解能力。为了解决这些局限性，我们引入了 MMBench-Video，这是一个定量基准测试，旨在严格评估 LVLMs 在视频理解方面的能力。MMBench-Video 收录了来自 YouTube 的长视频，并采用自由形式的问题，以反映实际用例。该基准测试经过精心设计，旨在测试模型的时间推理能力，所有问题都根据精心构建的能力分类法进行了人工标注。我们采用 GPT-4 进行自动评估，证明了其相较于早期基于 LLM 的评估方法具有更高的准确性和鲁棒性。我们利用 MMBench-Video 对图像和视频领域的专有和开源 LVLMs 进行了全面评估。MMBench-Video 是研究界宝贵的资源，有助于改进 LVLMs 的评估，并推动视频理解领域的进步。MMBench-Video 的评估代码将集成到 VLMEvalKit 中：https://github.com/open-compass/VLMEvalKit。||
|**2024-06-20**|[African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification](http://arxiv.org/abs/2406.14496)|**[link](https://github.com/gregor-ge/foci-benchmark)**|近来的大型视觉语言模型 (LVLM) 在众多图像理解和推理任务中展现出惊人的能力。然而，细粒度目标分类任务（例如，区分\textit{动物物种}）尽管具有下游重要性，但尚未得到充分探索。我们通过创建\texttt{FOCI}（\textbf{F}ine-grained \textbf{O}bject \textbf{C}lass\textbf{I}fication，细粒度目标分类）来填补这一评估空白，这是一个从现有目标分类数据集中构建的困难多项选择基准测试：(1) 多项选择避免了将分类视为开放式问答任务所带来的答案模糊性；(2) 我们通过使用 CLIP 模型挖掘负标签来保持分类难度。\texttt{FOCI} 使用来自 ImageNet-21k 的四个特定领域子集对五个流行的分类数据集进行了补充。我们对 \texttt{FOCI} 上的 12 个公共 LVLMs 进行了基准测试，结果表明，它测试了对已建立的图像理解和推理基准的\textit{补充技能}。至关重要的是，CLIP 模型表现出比 LVLMs 好得多的性能。由于 LVLMs 的图像编码器来自这些 CLIP 模型，这表明编码器和 LLM 之间在细粒度目标区分方面存在对齐不足的问题，并且需要具有更细粒度注释的（预）训练数据。我们在 \url{https://github.com/gregor-ge/FOCI-Benchmark} 发布我们的代码。||
|**2024-06-20**|[Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?](http://arxiv.org/abs/2406.14492)|null|大型视觉语言模型 (LVLM) 近期极大地推动了图像描述和许多图像理解任务（例如，视觉问答）的发展。然而，LVLM 经常会“出现幻觉”，生成描述中包含图像中不存在的概念。这些幻觉削弱了 LVLM 的可信度，并且可以说是其广泛应用的主要障碍之一。最近的研究表明，添加 grounding 目标（明确将图像区域或对象与文本范围对齐的目标）可以减少 LVLM 幻觉的发生。虽然直观，但这种说法并没有得到经验证据的支持，因为我们认为，减少效果是通过有缺陷的评估方案建立的，这些方案 (i) 依赖于 LVLM 训练中广泛使用的数据（即 MSCOCO），以及 (ii) 通过问答而不是开放式描述生成来衡量幻觉。相比之下，在这项工作中，我们首次系统地分析了细粒度对象 grounding 对 LVLM 幻觉的影响，该评估方案更真实地反映了开放式生成中的 LVLM 幻觉。我们对三个骨干 LLM 进行的大量实验表明，grounding 目标对开放式描述生成中的对象幻觉几乎没有影响。||
|**2024-06-20**|[Revealing Vision-Language Integration in the Brain with Multimodal Networks](http://arxiv.org/abs/2406.14481)|null|我们使用（多）模态深度神经网络 (DNN) 来探测人脑中多模态整合的位点，方法是预测人类受试者在观看电影时记录的脑电图 (SEEG)。我们将多模态整合的位点定义为：多模态视觉语言模型比单模态语言、单模态视觉或线性整合的语言视觉模型能更好地预测记录的区域。我们的目标 DNN 模型涵盖不同的架构（例如，卷积网络和变换器）和多模态训练技术（例如，交叉注意力和对比学习）。作为一个关键的实现步骤，我们首先证明训练过的视觉和语言模型在预测 SEEG 信号的能力方面系统地优于随机初始化的模型。然后，我们将单模态和多模态模型相互比较。由于我们的目标 DNN 模型通常具有不同的架构、参数数量和训练集（可能会掩盖那些可归因于整合的差异），因此我们对两个模型（SLIP 和 SimCLR）进行了受控比较，这两个模型除了输入模态外，其他所有属性都保持不变。使用这种方法，我们确定了大量的神经位点（平均 1090 个位点中的 141 个，或 12.94%）和似乎发生多模态整合的大脑区域。此外，我们发现在我们评估的多模态训练技术变体中，CLIP 风格的训练最适合在下游预测这些位点的神经活动。||
|**2024-06-20**|[Towards Truthful Multilingual Large Language Models: Benchmarking and Alignment Strategies](http://arxiv.org/abs/2406.14434)|null|在大型语言模型 (LLM) 时代，构建能够为全球用户服务的多语言大型语言模型 (MLLM) 具有重要意义。然而，现有研究很少关注 MLLM 的真实性。同时，当代多语言对齐技术难以平衡大量语言，并且经常表现出不同语言之间严重的真实性差距，尤其是那些与英语差异很大的语言。在我们的工作中，我们构建了一个用于多语言场景下真实性评估的基准，并探索了跨语言对齐事实以增强 MLLM 真实性的方法。此外，我们提出了事实感知多语言选择性协同 (FaMSS) 来优化大量语言和不同数据类型的数据分配。实验结果表明，我们的方法可以有效地减少多语言表示差异并增强 LLM 的多语言能力。||
|**2024-06-20**|[iWISDM: Assessing instruction following in multimodal models at scale](http://arxiv.org/abs/2406.14343)|**[link](https://github.com/bashivanlab/iwisdm)**|能够按照详细指令执行复杂任务是我们人类取得众多非凡成就的关键。作为人类，我们不仅能够执行各种各样的任务，而且还能够执行可能需要数百甚至数千个步骤才能完成的非常复杂的任务。大型语言模型及其最新的多模态模型（集成了文本和视觉输入）在执行复杂任务方面取得了前所未有的成功。然而，大多数现有的基准测试很大程度上局限于单一模态输入（文本或视觉），这缩小了多模态评估的范围，特别是在多模态环境下的指令遵循方面。为了弥合这一差距，我们引入了指令式虚拟视觉决策 (iWISDM) 环境，该环境旨在生成无限量的具有不同复杂程度的视觉语言任务。我们使用 iWISDM 编制了三个不同的指令遵循视觉任务基准测试，涵盖不同的复杂程度，并评估了这些基准测试中几个新开发的多模态模型。我们的研究结果表明，iWISDM 是一个强大的基准测试，用于评估现有和新兴多模态模型的指令遵循性，并突出了这些模型与人类在精确遵循指令的能力方面存在巨大差距。||
|**2024-06-20**|[E-ANT: A Large-Scale Dataset for Efficient Automatic GUI NavigaTion](http://arxiv.org/abs/2406.14250)|null|近年来，由于移动设备上的在线 GUI 导航对许多实际应用都有贡献，因此受到了广泛关注。随着大型语言模型（LLM）的快速发展，多模态大型语言模型（MLLM）在这项任务中具有巨大潜力。然而，现有的 MLLM 需要高质量的数据来提高其根据人类用户输入做出正确导航决策的能力。在本文中，我们开发了一个新颖且非常有价值的数据集，名为 E-ANT，它是第一个包含真实人类行为和高质量带注释屏幕截图的中文 GUI 导航数据集，包含超过 5000 多个不同小程序的近 40,000 条真实人类轨迹。此外，我们在 E-ANT 上评估了各种强大的 MLLM，并通过充分的消融实验展示了它们的实验结果。我们相信，我们提出的数据集将有利于 GUI 导航和 LLM/MLLM 决策能力的评估和发展。||
|**2024-06-20**|[VLBiasBench: A Comprehensive Benchmark for Evaluating Bias in Large Vision-Language Model](http://arxiv.org/abs/2406.14194)|**[link](https://github.com/xiangkui-cao/vlbiasbench)**|大型视觉语言模型 (LVLM) 的出现标志着迈向通用人工智能的重要一步。然而，这些进步受到输出结果经常反映偏差的影响，这个问题尚未得到广泛研究。现有的基准测试由于其数据规模有限、单一的问题格式和狭窄的偏差来源，在评估偏差方面不够全面。为了解决这个问题，我们引入了 VLBiasBench，这是一个旨在全面评估 LVLM 中偏差的基准测试。在 VLBiasBench 中，我们构建了一个包含九种不同社会偏差类别的数据集，包括年龄、残疾状况、性别、国籍、外貌、种族、宗教、职业、社会经济地位以及两个交叉偏差类别（种族 x 性别和种族 x 社会经济地位）。为了创建大规模数据集，我们使用 Stable Diffusion XL 模型生成了 46,848 张高质量图像，并将它们与不同的问题相结合，形成了 128,342 个样本。这些问题分为开放式和封闭式，充分考虑了偏差的来源，并从多个角度全面评估了 LVLM 的偏差。随后，我们对 15 个开源模型以及一个先进的闭源模型进行了广泛的评估，为揭示这些模型的偏差提供了一些新的见解。我们的基准测试可在 https://github.com/Xiangkui-Cao/VLBiasBench 获取。||
|**2024-06-18**|[Synergizing Foundation Models and Federated Learning: A Survey](http://arxiv.org/abs/2406.12844)|null|近年来，以大型语言模型、视觉Transformer和多模态模型为代表的Foundation Models (FMs)发展迅速，对学术界和工业界都产生了重大影响。与小规模模型相比，FMs在预训练阶段对海量数据的需求更大。虽然通用FMs可以使用互联网等公开来源收集的数据进行预训练，但特定领域FMs需要专有数据，由于隐私问题，可用数据量成为一个实际挑战。联邦学习 (FL) 是一种协作学习范式，它打破了不同参与者之间数据可用的障碍。因此，它提供了一个很有前景的解决方案，可以使用分布式数据集定制和调整FMs以适应各种特定领域的任务，同时保护隐私。这篇综述文章讨论了融合FL和FMs的潜力和挑战，并总结了核心技术、未来方向和应用。关于FM-FL的定期更新的论文合集可在https://github.com/lishenghui/awesome-fm-fl获取。||
|**2024-06-18**|[Adversarial Attacks on Multimodal Agents](http://arxiv.org/abs/2406.12814)|**[link](https://github.com/chenwu98/agent-attack)**|具有视觉功能的语言模型 (VLM) 现在被用于构建能够在真实环境中采取行动的自主多模态代理。在本文中，我们展示了多模态代理会引发新的安全风险，尽管由于对环境的访问和了解有限，攻击代理比之前的攻击更具挑战性。我们的攻击使用对抗性文本字符串来指导环境中一个触发图像上的基于梯度的扰动：(1) 我们的字幕攻击器攻击白盒字幕器（如果它们被用于将图像处理成字幕作为 VLM 的附加输入）；(2) 我们的 CLIP 攻击共同攻击一组 CLIP 模型，这些模型可以迁移到专有的 VLM。为了评估攻击，我们策划了 VisualWebArena-Adv，这是一组基于 VisualWebArena 的对抗性任务，VisualWebArena 是一个用于基于 Web 的多模态代理任务的环境。在单个图像上 L-infinity 范数为 $16/256$ 的情况下，字幕攻击器攻击可以让字幕增强型 GPT-4V 代理以 75% 的成功率执行对抗目标。当我们移除字幕生成器或使用 GPT-4V 生成自己的字幕时，CLIP 攻击可以分别达到 21% 和 43% 的成功率。对基于其他 VLM（例如 Gemini-1.5、Claude-3 和 GPT-4o）的代理进行的实验表明，它们的稳健性存在有趣的差异。进一步的分析揭示了攻击成功的几个关键因素，我们还讨论了对防御的影响。项目页面：https://chenwu.io/attack-agent 代码和数据：https://github.com/ChenWu98/agent-attack||
|**2024-06-18**|[OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI](http://arxiv.org/abs/2406.12753)|**[link](https://github.com/gair-nlp/olympicarena)**|人工智能 (AI) 的发展得益于大型语言模型 (LLM) 和大型多模态模型 (LMM) 的进步，逐渐展现出在解决问题和科学发现（即 AI4Science）方面的潜在认知推理能力，而这些能力曾经是人类智能所独有的。为了全面评估当前模型在认知推理能力方面的表现，我们推出了 OlympicArena，其中包含 11,163 道双语问题，涵盖纯文本和交错图文两种形式。这些挑战涵盖了七个领域和 62 个国际奥林匹克竞赛项目的广泛学科，并经过严格的数据泄露检查。我们认为，奥林匹克竞赛问题中的挑战是评估人工智能认知推理的理想选择，因为它们具有复杂性和跨学科性，这对于应对复杂的科学挑战和促进发现至关重要。除了使用仅答案标准评估各个学科的表现外，我们还从多个角度进行了详细的实验和分析。我们深入研究了模型的认知推理能力、它们在不同模态下的表现，以及它们在过程级评估中的结果，这些对于需要复杂推理和冗长解决方案的任务至关重要。我们广泛的评估表明，即使是像 GPT-4o 这样先进的模型，总体准确率也只有 39.97%，这说明了当前人工智能在复杂推理和多模态整合方面的局限性。通过 OlympicArena，我们旨在推动人工智能向超级智能发展，使其能够应对科学及其他领域更复杂的挑战。我们还提供了一套全面的资源来支持人工智能研究，包括一个基准数据集、一个开源标注平台、一个详细的评估工具，以及一个具有自动提交功能的排行榜。||
|**2024-06-18**|[Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning](http://arxiv.org/abs/2406.12742)|**[link](https://github.com/dtennant/mirb_eval)**|大型语言模型 (LLM) 的进步显著拓宽了自然语言处理应用的范围，多模态 LLM 扩展了这些能力，以整合和解释视觉数据。然而，现有的视觉语言模型 (VLM) 基准主要集中在单图像输入上，忽略了多图像理解的关键方面。在本文中，我们介绍了一个多图像关系基准 MIRB，旨在评估 VLM 比较、分析和推理多个图像的能力。我们的基准涵盖四个类别：感知、视觉世界知识、推理和多跳推理。通过对各种开源和闭源模型的全面评估，我们证明，虽然开源 VLM 在单图像任务中表现出接近 GPT-4V 的性能，但在多图像推理任务中仍然存在显著的性能差距。我们的研究结果还表明，即使是最先进的 GPT-4V 模型在我们的基准测试中也遇到了困难，这凸显了在该领域进一步研究和开发的必要性。我们相信，我们对 MIRB 的贡献可以作为开发下一代多模态模型的测试平台。||
|**2024-06-18**|[AGLA: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention](http://arxiv.org/abs/2406.12718)|**[link](https://github.com/lackel/agla)**|尽管大型视觉语言模型 (LVLMs) 在各种多模态任务中取得了巨大成功，但它们面临着一个普遍存在的问题，即对象幻觉，即生成的文本响应与给定图像中的真实对象不一致。本文研究了各种 LVLMs，并指出对判别性局部图像特征的注意力不足是导致对象幻觉的一个根本原因。具体来说，LVLMs 主要关注与提示无关的全局图像特征，而未能捕捉到与提示相关的局部特征，从而破坏了 LVLMs 的视觉基础能力并导致幻觉。为此，我们提出了全局和局部注意力组合 (AGLA) 方法，这是一种无需训练且即插即用的方法，通过探索全局特征集合以生成响应，同时利用局部特征进行视觉区分来减轻对象幻觉。我们的方法展示了一种图像-提示匹配方案，可以从图像中捕获与提示相关的局部特征，从而形成输入图像的增强视图，其中保留了与提示相关的内容，同时屏蔽了不相关的干扰。借助增强视图，可以通过整合来自原始图像的生成性全局特征和来自增强图像的判别性局部特征来获得校准后的解码分布。大量实验表明，AGLA 能够持续减轻各种判别性和生成性基准测试中 LVLMs 的对象幻觉，并增强其一般感知能力。我们的代码将在 https://github.com/Lackel/AGLA 上发布。||
|**2024-06-18**|[Disturbing Image Detection Using LMM-Elicited Emotion Embeddings](http://arxiv.org/abs/2406.12668)|null|本文探讨了利用大型多模态模型 (LMM) 中编码的知识进行令人不安的图像检测 (DID) 的任务。具体来说，我们提出了以两种方式利用 LMM 知识：首先是提取通用的语义描述，其次是提取引发的 emotions。随后，我们使用 CLIP 的文本编码器来获得通用语义描述和 LMM 引发的 emotions 的文本嵌入。最后，我们使用上述文本嵌入以及相应的 CLIP 图像嵌入来执行 DID 任务。所提出的方法显著提高了基线分类精度，在增强后的令人不安的图像检测数据集上实现了最先进的性能。||
|**2024-06-18**|[Do More Details Always Introduce More Hallucinations in LVLM-based Image Captioning?](http://arxiv.org/abs/2406.12663)|null|大型视觉语言模型（LVLM）擅长整合视觉和语言上下文以生成详细的内容，促进了图像描述等应用。然而，使用 LVLM 生成描述时，经常面临对象幻觉（OH）的挑战，即输出文本错误地表示输入图像中的实际对象。虽然之前的研究将 OH 的发生归因于包含更多细节，但我们的研究发现现有指标存在技术缺陷，导致对模型的评估和关于 OH 的结论不可靠。这引发了关于以下问题的争论：在基于 LVLM 的图像描述中，更多细节是否总是会导致更多幻觉？在本文中，我们通过提出一种新颖的解码策略，即差异化波束解码（DBD）以及一组可靠的新评估指标：CLIP-Precision、CLIP-Recall 和 CLIP-F1，来解决这场争论。DBD 将隐藏在视觉输入中的丰富信息并行解码为称为单元事实的不同语言表示。这种解码是通过精心设计的差异分数实现的，该分数指导并行搜索和候选筛选。然后聚合选定的单元事实以生成最终的描述。我们提出的指标通过比较真实图像区域和生成的文本分区的嵌入组，来评估图像描述的全面性和准确性。在视觉基因组数据集上的大量实验验证了我们方法的有效性，证明了它在生成详细描述的同时保持了低幻觉水平。||
|**2024-06-18**|[Efficient and Long-Tailed Generalization for Pre-trained Vision-Language Model](http://arxiv.org/abs/2406.12638)|**[link](https://github.com/shijxcs/candle)**|像CLIP这样的预训练视觉语言模型通过图像-文本匹配展现出强大的零样本推理能力，并被证明是各种下游任务中的强大少样本学习器。然而，在现实场景中，将CLIP应用于下游任务可能会遇到以下挑战：1）数据可能呈现长尾分布，并且可能没有所有类别的充足样本；2）可能出现完全没有样本的新类别的新兴任务。为了克服这些挑战，我们提出了一个名为Candle的新框架来实现高效的长尾泛化。在训练过程中，我们提出了补偿对数调整损失，以鼓励原型的大间隔，并缓解基本类别内部以及基本类别和新类别之间的不平衡。为了实现高效的适应性，我们将CLIP模型视为黑盒，并利用提取的特征来获得用于预测的视觉和文本原型。为了充分利用多模态信息，我们还提出了跨模态注意力机制来丰富来自两种模态的特征。为了实现有效的泛化，我们为新类别引入了虚拟原型，以弥补它们缺乏训练图像的不足。Candle在11个不同数据集上的大量实验中取得了最先进的结果，同时大大减少了训练时间，证明了我们方法的优越性。源代码可在https://github.com/shijxcs/Candle获取。||
|**2024-06-18**|[RS-GPT4V: A Unified Multimodal Instruction-Following Dataset for Remote Sensing Image Understanding](http://arxiv.org/abs/2406.12479)|**[link](https://github.com/geox-lab/rs-gpt4v)**|遥感图像智能理解模型正在经历一场由多模态大语言模型 (MLLM) 推动的深刻范式转变，即从学习领域模型 (LaDM) 的范式转变为学习预训练的通用基础模型，然后再进行领域自适应模型 (LaGD) 的范式。在新的 LaGD 范式下，过去十年推动遥感图像智能理解发展的旧数据集已不再适用于全新的任务。我们认为，必须设计一个新的数据集来简化任务，该数据集应具有以下特征：1）泛化性：训练模型学习任务之间的共享知识并适应不同的任务；2）复杂场景理解：训练模型理解感兴趣目标的细粒度属性，并能够用自然语言描述场景；3）推理能力：训练模型以实现高级视觉推理。在本文中，我们利用 GPT-4V 和现有数据集设计了一个高质量、多样化和统一的多模态指令遵循数据集，用于遥感图像理解，我们称之为 RS-GPT4V。为了实现泛化性，我们使用从 GPT-4V 通过指令遵循推导出的（问题，答案）来统一诸如字幕生成和目标定位等任务；为了实现复杂场景理解，我们提出了一种结合局部策略和全局策略的分层指令描述方法，其中局部策略描述了目标对象的细粒度属性及其空间关系，全局策略将所有局部信息整合起来生成详细的指令描述；为了实现推理能力，我们设计了多轮问答对，为模型提供推理能力。实验结果表明，通过 RS-GPT4V 微调的 MLLM 可以描述细粒度的信息。该数据集可在以下网址获取：https://github.com/GeoX-Lab/RS-GPT4V。||
|**2024-06-18**|[VRSBench: A Versatile Vision-Language Benchmark Dataset for Remote Sensing Image Understanding](http://arxiv.org/abs/2406.12384)|**[link](https://github.com/lx709/vrsbench)**|我们引入了一个新的基准，旨在推进用于遥感图像的通用、大规模视觉语言模型的开发。尽管已经提出了一些遥感领域的视觉语言数据集来实现这一目标，但现有数据集通常针对单一任务，缺乏详细的对象信息，或者质量控制不足。为了探索这些改进机会，我们提出了一个用于遥感图像理解的多功能视觉语言基准，称为VRSBench。该基准包含29,614张图像，其中包括29,614条经过人工验证的详细描述、52,472个对象参考和123,221个问答对。它有助于在广泛的遥感图像理解任务中训练和评估视觉语言模型。我们进一步在这个基准上评估了最先进的模型在三个视觉语言任务上的表现：图像描述、视觉定位和视觉问答。我们的工作旨在为遥感领域先进视觉语言模型的开发做出重大贡献。数据和代码可在https://github.com/lx709/VRSBench获取。||
|**2024-06-17**|[LLaNA: Large Language and NeRF Assistant](http://arxiv.org/abs/2406.11840)|null|多模态大型语言模型 (MLLM) 在理解图像和 3D 数据方面表现出色。然而，这两种模态在整体捕捉物体的外观和几何形状方面都存在缺陷。同时，神经辐射场 (NeRF) 已成为一种日益普遍的模态，它将信息编码在简单多层感知器 (MLP) 的权重中，可以同时编码物体的几何形状和逼真的外观。本文研究了将 NeRF 纳入 MLLM 的可行性和有效性。我们创建了 LLaNA，这是第一个能够执行 NeRF 描述和问答等新任务的通用 NeRF 语言助手。值得注意的是，我们的方法直接处理 NeRF 的 MLP 权重以提取有关所表示对象的信息，而无需渲染图像或实例化 3D 数据结构。此外，我们构建了一个包含 NeRF 数据集以及用于各种 NeRF 语言任务的文本注释，无需人工干预。基于此数据集，我们开发了一个基准来评估我们方法的 NeRF 理解能力。结果表明，处理 NeRF 权重比从 NeRF 中提取 2D 或 3D 表示表现更好。||
|**2024-06-17**|[MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs](http://arxiv.org/abs/2406.11833)|**[link](https://github.com/liuziyu77/mmdu)**|生成自然且有意义的响应以与多模态人类输入进行交流是大视觉语言模型 (LVLM) 的一项基本能力。虽然当前的开源 LVLM 在简化场景（例如单轮单图像输入）中表现出良好的性能，但它们在现实世界的对话场景中却表现不佳，例如在具有多轮和多图像的长上下文历史中遵循指令。现有的 LVLM 基准主要集中在单项选择题或简短回答上，这些并不能充分评估 LVLM 在现实世界人机交互应用中的能力。因此，我们引入了 MMDU，一个综合基准测试，以及 MMDU-45k，一个大规模指令微调数据集，旨在评估和改进 LVLM 在多轮和多图像对话中的能力。我们采用聚类算法从开源维基百科中找到相关的图像和文本描述，并在 GPT-4o 模型的帮助下由人工标注者构建问答对。MMDU 最多包含 18k 个图像+文本标记、20 张图像和 27 轮对话，这比以前的基准测试至少长 5 倍，对当前的 LVLM 构成了挑战。我们使用 MMDU 对 15 个代表性 LVLM 进行的深入分析表明，由于对话指令微调数据有限，开源 LVLM 落后于闭源 LVLM。我们证明，在 MMDU-45k 上对开源 LVLM 进行微调可以显著缩小这一差距，生成更长、更准确的对话，并提高 MMDU 和现有基准测试的得分（MMStar：+1.1%，MathVista：+1.5%，ChartQA：+1.2%）。我们的贡献为弥合当前 LVLM 模型与现实应用需求之间的差距铺平了道路。该项目可在 https://github.com/Liuziyu77/MMDU 获取。||
|**2024-06-17**|[Unveiling Encoder-Free Vision-Language Models](http://arxiv.org/abs/2406.11832)|**[link](https://github.com/baaivision/eve)**|现有的视觉语言模型 (VLM) 主要依赖视觉编码器提取视觉特征，然后利用大型语言模型 (LLM) 执行视觉语言任务。然而，视觉编码器在抽象视觉表示（例如分辨率、纵横比和语义先验）方面设置了强烈的归纳偏差，这可能会阻碍 VLM 的灵活性和效率。训练接受无缝视觉和语言输入（即没有视觉编码器）的纯 VLM 仍然具有挑战性，并且很少被探索。实证观察表明，没有编码器的直接训练会导致收敛缓慢和巨大的性能差距。在这项工作中，我们弥合了基于编码器和无编码器模型之间的差距，并提出了一个简单而有效的训练方法来实现纯 VLM。具体来说，我们通过彻底的实验揭示了有效训练无编码器 VLM 的关键方面：(1) 在一个统一的解码器内桥接视觉语言表示；(2) 通过额外监督增强视觉识别能力。借助这些策略，我们推出了 EVE，这是一种可以高效训练和推理的无编码器视觉语言模型。值得注意的是，仅利用 35M 公开可访问的数据，EVE 就可以在多个视觉语言基准测试中与具有类似能力的基于编码器的 VLM 相媲美。它明显优于具有神秘训练程序和未公开训练数据的对应模型 Fuyu-8B。我们相信 EVE 为开发跨模态的纯解码器架构提供了一条透明且有效的途径。我们的代码和模型可在以下网址公开获取：https://github.com/baaivision/EVE。||
|**2024-06-17**|[On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning](http://arxiv.org/abs/2406.11823)|**[link](https://github.com/naver-ai/elva)**|近年来，语言和视觉助手的进步展现出令人印象深刻的能力，但缺乏透明度，限制了更广泛的研究和可重复性。虽然开源模型可以有效地处理一般的图像任务，但它们在处理复杂的视觉情境文本理解时面临着计算需求高的挑战。此类任务通常需要增加标记输入和大型视觉模块来利用高分辨率信息。如何在模型大小和数据重要性之间取得平衡仍然是一个悬而未决的问题。本研究旨在通过识别关键组件并创建具有受限推理成本的高效模型来重新定义视觉语言模型的设计。通过策略性地制定数据集、优化视觉模块和增强监督技术，我们在保持高性能的同时，显著提高了推理吞吐量。从 1.6 亿到 130 亿参数不等的模型的广泛实验为模型优化提供了见解。我们将在 https://github.com/naver-ai/elva 上完全开源我们的代码库、模型和数据集。||
|**2024-06-17**|[VideoLLM-online: Online Video Large Language Model for Streaming Video](http://arxiv.org/abs/2406.11816)|null|最近的大型语言模型增强了视觉能力，使其能够理解图像、视频和交错的视觉语言内容。然而，这些大型多模态模型的学习方法通常将视频视为预先确定的片段，这使得它们在处理流式视频输入方面效率较低。在本文中，我们提出了一种新颖的视频流学习 (LIVE) 框架，该框架支持在连续视频流中进行时间对齐、长上下文和实时对话。我们的 LIVE 框架包含实现视频流对话的综合方法，包括：（1）旨在对连续流输入执行语言建模的训练目标，（2）将离线时间注释转换为流对话格式的数据生成方案，以及（3）优化的推理管道，以加速模型在现实世界视频流中的响应速度。借助我们的 LIVE 框架，我们在 Llama-2/Llama-3 的基础上构建了 VideoLLM-online 模型，并展示了其在处理流式视频方面的显著优势。例如，平均而言，我们的模型可以在 A100 GPU 上以超过 10 FPS 的速度支持 5 分钟视频片段中的流式对话。此外，它还在公共离线视频基准测试中展示了最先进的性能，例如识别、字幕和预测。代码、模型、数据和演示已在 https://showlab.github.io/videollm-online 上提供。||
|**2024-06-17**|[LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning](http://arxiv.org/abs/2406.11815)|null|近年来，指令微调的大型多模态模型 (LMM) 在图像字幕和视觉问答等多项任务中取得了成功；然而，如何将这些模型应用于机器人领域仍然是一个开放性问题。以往用于机器人应用的 LMM 已经接受了大量语言和动作数据的训练，但它们在不同环境下的泛化能力往往不尽如人意。为了解决这个问题，我们引入了 LLARVA，这是一种使用新型指令微调方法训练的模型，该方法利用结构化提示来统一各种机器人学习任务、场景和环境。此外，我们还表明，预测中间二维表示（我们将其称为“视觉轨迹”）有助于进一步对齐机器人学习的视觉和动作空间。我们从 Open X-Embodiment 数据集中生成了 850 万个图像-视觉轨迹对，用于预训练我们的模型，并在 RLBench 模拟器中的 12 个不同任务以及实体 Franka Emika Panda 7 自由度机器人上进行了评估。我们的实验取得了良好的性能，表明 LLARVA 使用二维和语言表示，与其他几个当代基线模型相比表现出色，并且可以泛化到各种机器人环境和配置中。||
|**2024-06-17**|[See It from My Perspective: Diagnosing the Western Cultural Bias of Large Vision-Language Models in Image Understanding](http://arxiv.org/abs/2406.11665)|**[link](https://github.com/amith-ananthram/see-it-from-my-perspective)**|视觉-语言模型 (VLM) 可以用多种语言回答关于图像的查询。然而，除了语言之外，文化也会影响我们看待事物的方式。例如，来自西方文化的人更关注图像中的中心人物，而来自东方文化的人则更关注场景环境。在这项工作中，我们提出了一项新颖的研究，该研究证明并定位了 VLM 在图像理解中的西方偏见。我们使用文化多样性的图像和注释，在主观和客观视觉任务中评估大型 VLM。我们发现，在每个任务中，VLM 在西方子集上的表现优于东方子集。追踪这种偏见来源的对照实验强调了，即使推理是用英语进行的，在纯文本预训练中使用多样化的语言组合对于构建公平的 VLM 至关重要。此外，虽然以目标文化的语言进行提示可以减少偏见，但这并不能替代构建更能代表世界语言的人工智能。||
|**2024-06-17**|[Multimodal Learning To Improve Segmentation With Intraoperative CBCT & Preoperative CT](http://arxiv.org/abs/2406.11650)|null|术中医学影像，特别是锥束计算机断层扫描 (CBCT)，尽管视觉质量较低，但仍是促进计算机辅助介入治疗的重要工具。虽然这种降级的图像质量会影响下游分割，但高质量术前扫描的可用性为改进提供了潜力。在这里，我们考虑一种可以使用术前 CT 和术中 CBCT 扫描的情况，但是扫描之间的对齐（配准）并不完美。我们提出了一种多模态学习方法，融合了粗略对齐的 CBCT 和 CT 扫描，并研究了 CBCT 质量和错位（促进错位的仿射和弹性变换）对最终分割性能的影响。作为一个应用场景，我们专注于肝脏和肝脏肿瘤语义分割，并评估术中图像质量和错位对分割性能的影响。为此，将高质量、标记的 CT 定义为术前数据，并将其用作模拟术中 CBCT 的基础。我们表明，融合术前 CT 和模拟的术中 CBCT 大多可以提高分割性能，并且即使明显错位的术前数据也有可能提高分割性能。||
|**2024-06-17**|[AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic Manipulation](http://arxiv.org/abs/2406.11548)|null|对于机器人系统与现实世界物体进行稳定交互而言，反思和纠正错误的能力至关重要。观察到多模态大语言模型 (MLLM) 的泛化和推理能力，先前的方法旨在利用这些模型来相应地增强机器人系统。然而，这些方法通常侧重于使用额外的 MLLM 进行高级规划校正，而对失败样本的利用有限，无法校正低级接触姿态。为了解决这一差距，我们提出了一种自主交互校正 (AIC) MLLM，它利用先前的低级交互经验来校正 SE(3) 姿态预测。具体来说，AIC MLLM 最初经过微调，以获得姿态预测和反馈提示理解能力。我们通过与物体的交互精心设计了两种类型的提示指令：1) 视觉掩码，用于突出显示不可移动的部分以进行位置校正；2) 文本描述，用于指示旋转校正的潜在方向。在推理过程中，引入了反馈信息提取模块来识别失败原因，从而允许 AIC MLLM 使用相应的提示自适应地校正姿态预测。为了进一步增强操作稳定性，我们设计了一种测试时适应策略，使 AIC MLLM 能够更好地适应当前场景配置。最后，我们在模拟和现实环境中进行了广泛的实验来评估所提出的方法。结果表明，我们的 AIC MLLM 可以通过利用交互体验提示有效地纠正失败样本。现实世界的演示可以在 https://sites.google.com/view/aic-mllm 找到。||
|**2024-06-17**|[MedThink: Inducing Medical Large-scale Visual Language Models to Hallucinate Less by Thinking More](http://arxiv.org/abs/2406.11451)|null|当大型视觉语言模型（LVLM）应用于多模态医学生成任务时，它们会遇到严重的模型幻觉问题。这严重损害了模型的生成准确性，使得LVLM难以在现实世界的医疗场景中实施以协助医生进行诊断。增强下游医学生成任务的训练数据是解决模型幻觉问题的有效方法。此外，医学领域训练数据的有限可用性和隐私问题极大地阻碍了模型的准确性和泛化能力。在本文中，我们介绍了一种模仿人类认知过程来构建细粒度指令对的方法，并将思维链（CoT）的概念从推理场景应用于训练场景，从而提出了一种称为MedThink的方法。我们对各种LVLM的实验表明，我们专为医学领域量身定制的新型数据构建方法显着提高了模型在医学图像报告生成任务中的性能，并大大减少了幻觉。这项工作的所有资源将很快发布。||

<p align=right>(<a href=#updated-on-20240629>back to top</a>)</p>

## 6DOF Object Pose

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-19**|[NeRF-Feat: 6D Object Pose Estimation using Feature Rendering](http://arxiv.org/abs/2406.13796)|null|物体姿态估计是机器人抓取和增强现实中的一个关键组成部分。基于学习的方法通常需要来自高精度 CAD 模型或使用复杂设置获取的标记训练数据的训练数据。我们通过学习从没有已知 CAD 模型的弱标记数据中估计姿态来解决这个问题。我们建议使用 NeRF 来隐式学习物体形状，然后将其用于使用对比损失与 CNN 一起学习视图不变特征。虽然 NeRF 有助于学习视图一致的特征，但 CNN 确保学习到的特征尊重对称性。在推理过程中，CNN 用于预测视图不变特征，这些特征可用于建立与 NeRF 中隐式 3D 模型的对应关系。然后使用对应关系来估计 NeRF 参考系中的姿态。与使用类似训练设置的其他方法不同，我们的方法还可以处理对称对象。具体来说，我们使用 NeRF 学习视点不变的判别特征，这些特征稍后用于姿态估计。我们在 LM、LM-Occlusion 和 T-Less 数据集上评估了我们的方法，并在使用弱标记数据的情况下实现了基准精度。||
|**2024-06-06**|[Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking](http://arxiv.org/abs/2406.04316)|null|6D物体姿态估计是计算机视觉中一项至关重要但极具挑战性的任务，其面临的主要挑战是缺乏大规模数据集。这种稀缺性阻碍了对模型性能的全面评估，限制了研究进展。此外，可用实例或类别的数量有限也限制了其应用。为了解决这些问题，本文提出了Omni6DPose，这是一个以对象类别多样性、规模大和对象材质多样性为特征的大型数据集。Omni6DPose主要由三个部分组成：ROPE（真实6D物体姿态估计数据集），包含332K张图像，涵盖149个类别、581个实例，标注了超过150万个姿态；SOPE（模拟6D物体姿态估计数据集），包含475K张在混合现实环境中创建的、具有深度模拟的图像，涵盖与ROPE相同的149个类别、4162个实例，标注了超过500万个姿态；以及在ROPE和SOPE中均使用的手动对齐的真实扫描物体模型。由于存在大量的变化和模糊性，Omni6DPose本身就极具挑战性。为了应对这一挑战，我们引入了GenPose++，它是SOTA类别级姿态估计框架的增强版本，它包含两个关键改进：语义感知特征提取和基于聚类的聚合。此外，我们还提供了全面的基准测试分析，以评估先前方法在这个大规模数据集上在6D物体姿态估计和姿态跟踪方面的性能。||
|**2024-06-05**|[Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices](http://arxiv.org/abs/2406.02977)|null|随着机器人和增强现实应用越来越依赖于精确高效的 6D 物体姿态估计，边缘设备上的实时性能对于更具交互性和响应性的系统变得至关重要。我们提出的稀疏颜色代码网络 (SCCN) 体现了一种清晰简洁的管道设计，可以有效地满足这一需求。SCCN 对 RGB 图像中的目标物体执行像素级预测，利用基本物体几何特征的稀疏性来加速透视n点 (PnP) 计算过程。此外，它引入了一种新颖的基于像素级几何的物体对称表示，该表示与初始姿态预测无缝集成，有效地解决了对称物体的歧义性。SCCN 在 NVIDIA Jetson AGX Xavier 上分别在基准 LINEMOD 数据集和遮挡 LINEMOD 数据集上实现了每秒 19 帧 (FPS) 和 6 FPS 的估计速率，同时在这些速率下始终保持较高的估计精度。||
|**2024-05-19**|[Advancing 6-DoF Instrument Pose Estimation in Variable X-Ray Imaging Geometries](http://arxiv.org/abs/2405.11677)|**[link](https://github.com/cviviers/YOLO-6D-Pose)**|在微创手术中，对手术器械进行精确的 6 自由度 (6-DoF) 姿态估计可以显著改善治疗策略并最终提高手术效果。现有的深度学习方法已经取得了准确的结果，但它们需要针对每个对象定制方法，并且需要费力的设置和训练环境（通常扩展到大量的模拟），同时缺乏实时计算能力。我们提出了一种用于 X 射线系统中 6-DoF 姿态估计任务的通用数据采集方法，一种新颖且通用的 YOLOv5-6D 姿态架构，用于准确、快速地进行目标姿态估计，以及一种在单目锥束 X 射线图像采集几何形状考虑下进行手术螺钉姿态估计的完整方法。所提出的 YOLOv5-6D 姿态模型在公共基准测试中取得了具有竞争力的结果，同时在 GPU 上的速度相当快，达到了 42 FPS。此外，该方法可以泛化到不同的 X 射线采集几何形状和语义图像复杂度，从而能够在不同的领域进行准确的姿态估计。最后，所提出的方法在脊柱手术期间对骨螺钉姿态估计进行了测试，用于计算机辅助引导。该模型通过 0.1 ADD-S 指标达到了 92.41% 的精度，证明了其在提高手术精度和患者预后方面具有广阔的应用前景。YOLOv5-6D 的代码已在 https://github.com/cviviers/YOLOv5-6D-Pose 公开发布。||
|**2024-05-18**|[PS6D: Point Cloud Based Symmetry-Aware 6D Object Pose Estimation in Robot Bin-Picking](http://arxiv.org/abs/2405.11257)|null|6D物体姿态估计在许多领域中都扮演着至关重要的角色，尤其是在工业工件抓取方面。针对锈蚀、高反射率和缺乏纹理等挑战，本文介绍了一种基于点云的姿态估计框架（PS6D）。PS6D专注于细长和多对称物体。它通过注意力引导的特征提取模块提取多尺度特征，设计了对称感知旋转损失和中心距离敏感平移损失来回归每个点到实例质心的姿态，然后使用两阶段聚类方法完成实例分割和姿态估计。来自Sil'eane和IPA数据集的对象以及工业实践中的典型工件被用于生成数据和评估算法。与最先进的方法相比，PS6D在F $_{1_{inst}}$ 上提高了11.5%，在召回率上提高了14.8%。PS6D的主要部分已部署到Mech-Mind的软件中，并在分拣实验中取得了91.7%的成功率，标志着其在工业姿态估计任务中的应用。||
|**2024-05-31**|[Deep Learning-Based Object Pose Estimation: A Comprehensive Survey](http://arxiv.org/abs/2405.07801)|**[link](https://github.com/cnjianliu/awesome-object-pose-estimation)**|物体姿态估计是计算机视觉中的一个基本问题，在增强现实和机器人技术中有广泛的应用。在过去的十年中，深度学习模型由于其卓越的准确性和鲁棒性，越来越多地取代了依赖于工程点对特征的传统算法。然而，当代方法仍然存在若干挑战，包括它们对标记训练数据的依赖性、模型紧凑性、在挑战性条件下的鲁棒性以及对新颖的未见过物体的泛化能力。最近缺少一篇综述来讨论该领域不同方面的进展、面临的挑战和未来有希望的方向。为了填补这一空白，我们讨论了基于深度学习的物体姿态估计的最新进展，涵盖了该问题的所有三种形式，即实例级、类别级和未见过物体的姿态估计。我们的综述还涵盖了多种输入数据模态、输出姿态的自由度、物体属性和下游任务，为读者提供了对该领域的全面理解。此外，它还讨论了不同领域的训练范式、推理模式、应用领域、评估指标和基准数据集，并报告了当前最先进方法在这些基准上的性能，从而帮助读者为其应用选择最合适的方法。最后，该综述确定了关键挑战，回顾了当前的趋势及其优缺点，并指出了未来研究的有希望的方向。我们还将继续跟踪https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation上的最新工作。||
|**2024-05-02**|[IntervenGen: Interventional Data Generation for Robust and Data-Efficient Robot Imitation Learning](http://arxiv.org/abs/2405.01472)|null|模仿学习是一种很有前景的机器人控制策略训练范式，但这些策略可能会受到分布偏移的影响，即评估时的条件与训练数据中的条件不同。提高策略对分布偏移鲁棒性的一种流行方法是交互式模仿学习（例如 DAgger 及其变体），其中人类操作员在策略执行期间提供纠正干预。然而，收集足够数量的干预以覆盖策略错误的分布对于人类操作员来说可能是一项繁重的任务。我们提出了 IntervenGen (I-Gen)，这是一种新颖的数据生成系统，可以根据少量的人工干预，自动生成大量覆盖状态空间的纠正干预。我们将 I-Gen 应用于 4 个模拟环境和 1 个具有物体姿态估计误差的物理环境，结果表明，它只需 10 次人工干预即可将策略鲁棒性提高多达 39 倍。视频和更多结果可在 https://sites.google.com/view/intervengen2024 获取。||
|**2024-04-17**|[GeoReF: Geometric Alignment Across Shape Variation for Category-level Object Pose Refinement](http://arxiv.org/abs/2404.11139)|null|物体姿态细化对于鲁棒的物体姿态估计至关重要。先前的工作在实例级物体姿态细化方面取得了重大进展。然而，由于类别内较大的形状变化以及目标物体与形状先验之间的差异，类别级姿态细化是一个更具挑战性的问题。为了解决这些挑战，我们引入了一种用于类别级物体姿态细化的新型架构。我们的方法集成了 HS 层和可学习的仿射变换，旨在增强几何信息的提取和对齐。此外，我们引入了一种跨点云变换机制，可以有效地融合不同的数据源。最后，我们通过结合形状先验信息进行平移和尺寸误差预测，来突破模型的极限。我们进行了广泛的实验来证明所提出框架的有效性。通过大量的定量实验，我们证明了在所有指标上都比基线方法有显著的改进。||
|**2024-04-08**|[Learning a Category-level Object Pose Estimator without Pose Annotations](http://arxiv.org/abs/2404.05626)|null|三维物体姿态估计是一项具有挑战性的任务。以往的工作总是需要数千张带有标注姿态的物体图像来学习三维姿态对应关系，这对于标注来说既费力又耗时。在本文中，我们提出在没有姿态标注的情况下学习类别级别的三维物体姿态估计器。我们没有使用手动标注的图像，而是利用扩散模型（例如 Zero-1-to-3）生成一组在受控姿态差异下的图像，并建议使用这些图像来学习我们的物体姿态估计器。直接使用原始的扩散模型会导致图像出现姿态噪声和伪影。为了解决这个问题，首先，我们利用从专门设计的对比姿态学习中学习到的图像编码器来过滤不合理的细节并提取图像特征图。此外，我们提出了一种新的学习策略，允许模型从那些生成的图像集中学习物体姿态，而无需知道其规范姿态的对齐。实验结果表明，我们的方法能够从单次拍摄设置（作为姿态定义）中进行类别级别的物体姿态估计，同时在少样本类别级别物体姿态估计基准测试中明显优于其他最先进的方法。||
|**2024-03-28**|[Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation](http://arxiv.org/abs/2403.19527)|**[link](https://github.com/leeiieeo/ag-pose)**|类别级 6D 物体姿态估计旨在估计特定类别中未见实例的旋转、平移和尺寸。在这个领域，基于密集对应的方法取得了领先的性能。然而，它们没有明确考虑不同实例的局部和全局几何信息，导致对具有显著形状变化的未见实例的泛化能力较差。为了解决这个问题，我们提出了一种新的用于类别级 6D 物体姿态估计的实例自适应和几何感知关键点学习方法 (AG-Pose)，它包括两个关键设计：(1) 第一个设计是实例自适应关键点检测模块，它可以自适应地检测一组稀疏关键点，用于表示各种实例的几何结构。(2) 第二个设计是几何感知特征聚合模块，它可以有效地将局部和全局几何信息集成到关键点特征中。这两个模块可以协同工作，为未见实例建立稳健的关键点级对应关系，从而增强模型的泛化能力。在 CAMERA25 和 REAL275 数据集上的实验结果表明，所提出的 AG-Pose 在没有类别特定形状先验的情况下，大幅度优于现有技术方法。||
|**2024-06-01**|[Object Pose Estimation via the Aggregation of Diffusion Features](http://arxiv.org/abs/2403.18791)|**[link](https://github.com/tianfu18/diff-feats-pose)**|从图像中估计物体姿态是 3D 场景理解的关键任务，最近的方法在非常大的基准测试中显示出可观的结果。然而，这些方法在处理未见过物体时性能会显著下降。我们认为这是由于图像特征的泛化能力有限造成的。为了解决这个问题，我们深入分析了扩散模型（例如 Stable Diffusion）的特征，这些模型在对未见过物体建模方面具有巨大潜力。基于此分析，我们创新性地将这些扩散特征引入物体姿态估计。为此，我们提出了三种不同的架构，可以有效地捕获和聚合不同粒度的扩散特征，极大地提高了物体姿态估计的泛化能力。我们的方法在三个流行的基准数据集 LM、O-LM 和 T-LESS 上以相当大的优势优于最先进的方法。特别是，我们的方法在未见过物体上实现了比之前最佳结果更高的准确度：在 Unseen LM 上为 98.2% 对比 93.5%，在 Unseen O-LM 上为 85.9% 对比 76.3%，显示了我们方法强大的泛化能力。我们的代码发布在 https://github.com/Tianfu18/diff-feats-pose。||
|**2024-03-24**|[KITchen: A Real-World Benchmark and Dataset for 6D Object Pose Estimation in Kitchen Environments](http://arxiv.org/abs/2403.16238)|null|尽管最近在用于机器人抓取的6D物体姿态估计方法方面取得了进展，但这些方法在现有数据集上的能力与其在现实世界移动操作任务中的效率之间仍然存在巨大的性能差距，特别是当机器人仅仅依靠其单眼以自我为中心的视野（FOV）时。现有的现实世界数据集主要关注桌面抓取场景，其中机械臂被放置在一个固定的位置，并且物体集中在固定外部相机视野的中心。评估此类数据集的性能可能无法准确反映在厨房环境中日常移动操作任务中遇到的挑战，例如从较高的架子、水槽、洗碗机、烤箱、冰箱或微波炉中检索物体。为了解决这一差距，我们提出了KITchen，这是一个专门为估计厨房环境中不同位置物体的6D姿态而设计的新基准。为此，我们录制了一个包含大约205k张真实世界RGBD图像的综合数据集，这些图像来自两个不同厨房中的111个厨房物体，利用一个具有人形机器人以自我为中心的视角。随后，我们开发了一个半自动注释管道，以简化此类数据集的标记过程，从而以最少的人力生成2D物体标签、2D物体分割掩码和6D物体姿态。该基准、数据集和注释管道可在https://kitchen-dataset.github.io/KITchen获取。||
|**2024-03-22**|[DITTO: Demonstration Imitation by Trajectory Transformation](http://arxiv.org/abs/2403.15203)|null|快速便捷地教会机器人新技能对于机器人系统的广泛应用至关重要。在这项工作中，我们提出了一种两阶段方法，用于解决基于单个 RGB-D 视频录制的单样本模仿学习问题。在第一阶段（离线阶段），我们提取演示轨迹。这需要分割被操作物体并确定它们相对于次要物体（例如容器）的相对运动。随后，在实时在线轨迹生成阶段，我们首先重新检测所有物体，然后将演示轨迹变换到当前场景，最后控制机器人跟踪轨迹。为了完成这些步骤，我们的方法利用了几种辅助模型，包括用于分割、相对物体姿态估计和抓取预测的模型。我们系统地评估了对应和重新检测方法的不同组合，以验证我们在各种任务中的设计决策。具体来说，我们收集了十种不同任务的演示，包括拾放任务以及关节物体操作。最后，我们在真实的机器人系统上进行了广泛的评估，以证明我们的方法在实际场景中的有效性和实用性。我们在 http://ditto.cs.uni-freiburg.de 上公开代码。||
|**2024-03-21**|[Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation](http://arxiv.org/abs/2403.14559)|null|在二维图像中定位预定义的三维关键点是建立用于六自由度物体姿态估计的三维-二维对应关系的有效方法。然而，不可见关键点不可靠的定位结果会降低对应关系的质量。在本文中，我们通过定位在可见性方面重要的关键点来解决这个问题。由于关键点可见性信息在当前的数据集收集过程中缺失，我们提出了一种有效的方法，可以从可用的物体级标注中生成二元可见性标签，用于非对称物体和对称物体的关键点。我们进一步基于PageRank算法从二元标签中推导出实值可见性感知重要性。利用我们可见性感知重要性的灵活性，我们通过将可见性感知重要性与最先进的姿态估计算法以及额外的positional encoding相结合，构建了VAPO（可见性感知姿态估计器）。在流行的姿态估计基准上进行了广泛的实验，包括Linemod、Linemod-Occlusion和YCB-V。结果表明，VAPO改进了关键点对应关系和最终估计的姿态，并明显达到了最先进的性能。||
|**2024-03-18**|[GenFlow: Generalizable Recurrent Flow for 6D Pose Refinement of Novel Objects](http://arxiv.org/abs/2403.11510)|null|尽管基于学习的方法在6D物体姿态估计方面取得了进展，但对于新物体，精度和可扩展性之间的权衡仍然存在。具体来说，以前针对新物体的方法没有很好地利用目标物体的3D形状信息，因为它们侧重于通过间接处理形状来实现泛化，从而降低了效率。我们提出了GenFlow，这是一种能够在目标物体形状的指导下实现对新物体的精度和泛化能力的方法。我们的方法预测渲染图像和观察图像之间的光流，并迭代地细化6D姿态。它通过对3D形状的约束和从端到端可微系统学习到的可泛化几何知识来提高性能。我们通过设计级联网络架构来进一步改进我们的模型，以利用多尺度相关性和从粗到精的细化。GenFlow在RGB和RGB-D情况下均在未见物体姿态估计基准测试中排名第一。它还实现了与现有的最先进的已见物体姿态估计方法相当的性能，而无需任何微调。||
|**2024-03-14**|[MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion](http://arxiv.org/abs/2403.09309)|null|杂乱的拾取环境对姿态估计模型提出了挑战。尽管深度学习取得了令人瞩目的进步，但基于单视图 RGB 的姿态估计模型在杂乱的动态环境中表现不佳。利用视频中包含的丰富时间信息有可能增强模型处理遮挡和环境动态性质的不利影响的能力。此外，联合目标检测和姿态估计模型更适合利用任务之间的相互依赖性来提高两项任务的准确性。为此，我们提出了一种基于注意力的时序融合方法，用于多目标 6D 姿态估计，该方法可以在视频序列的多帧中积累信息。我们的 MOTPose 方法将一系列图像作为输入，并在一次前向传递中对所有目标执行联合目标检测和姿态估计。它学习使用基于交叉注意力的融合模块在多个时间步骤中聚合目标嵌入和目标参数。我们在物理逼真的杂乱箱拣选数据集 SynPick 和 YCB-Video 数据集上评估了我们的方法，并证明了改进的姿态估计精度以及更好的目标检测精度。||

<p align=right>(<a href=#updated-on-20240629>back to top</a>)</p>

## nerf

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-26**|[Trimming the Fat: Efficient Compression of 3D Gaussian Splats through Pruning](http://arxiv.org/abs/2406.18214)|null|近年来，由于神经辐射场和最近出现的3D高斯散射（3DGS）模型提供了端到端训练的能力，3D模型的使用得到了越来越多的关注。后者具有显著优势，因为它本身就易于在训练期间快速收敛，并提供了广泛的可编辑性。然而，尽管发展迅速，但关于这些模型可扩展性的文献仍然处于起步阶段。在本研究中，我们针对解决这一差距采取了一些初步措施，展示了一种能够实现此类模型的内存和计算可扩展性的方法。具体来说，我们提出了“Trimming the fat”，这是一种事后梯度信息迭代剪枝技术，用于消除模型中编码的冗余信息。我们在广泛认可的基准测试集上的实验结果证明了我们方法的有效性，表明在保持甚至改进基线性能的同时，可以去除高达75%的高斯函数。我们的方法实现了约50倍的压缩，同时保持了与基线模型相似的性能，并且能够将计算速度提高到600 FPS。||
|**2024-06-21**|[Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks](http://arxiv.org/abs/2406.15149)|null|仿真器是自主机器人学习的强大工具，因为它们可以扩展数据生成、灵活设计和优化轨迹。然而，将从仿真数据中学习到的行为转移到现实世界中被证明是困难的，通常需要通过计算量大的域随机化方法或进一步的模型微调来缓解。我们提出了一种改进泛化能力和对仿真到真实视觉四旋翼导航任务中分布偏移的鲁棒性的方法。为此，我们首先通过将高斯样条插值与四旋翼飞行动力学相结合来构建仿真器，然后使用 Liquid 神经网络训练鲁棒的导航策略。通过这种方式，我们获得了一个全栈模仿学习协议，它结合了 3D 高斯样条辐射场渲染的进步、专家演示训练数据的巧妙编程以及 Liquid 网络的任务理解能力。通过一系列定量飞行测试，我们证明了仅在一个模拟场景中学习到的导航技能可以直接稳健地迁移到现实世界。我们进一步展示了在剧烈的分布和物理环境变化下，在训练环境之外保持性能的能力。我们学习的 Liquid 策略，仅在从真实感室内模拟飞行中精选的单目标机动上训练，可以泛化到户外真实硬件平台上的多步远足。||
|**2024-06-21**|[A3D: Does Diffusion Dream about 3D Alignment?](http://arxiv.org/abs/2406.15020)|null|我们从几何对齐的角度解决了文本驱动的三维生成问题。我们的目标是生成语义和几何上一致的多个对象。最近基于分数蒸馏的方法已经成功地将二维扩散模型的知识提取到由三维神经辐射场表示的高质量对象中。这些方法分别处理多个文本查询，因此生成的对象在对象姿态和结构方面具有很高的可变性。然而，在某些应用中，例如几何编辑，需要获得对齐的对象。为了实现对齐，我们建议通过使用单个 NeRF 表示对文本嵌入的线性成对插值的空間进行建模，来优化对齐对象之间的连续轨迹。我们证明，由语义上对应的部分组成的相似对象可以在三维空间中很好地对齐，而无需对生成过程进行昂贵的修改。我们提供了几个受益于几何对齐的实际场景，包括网格编辑和对象混合，并通过实验证明了我们方法的效率。https://voyleg.github.io/a3d/||
|**2024-06-21**|[E2GS: Event Enhanced Gaussian Splatting](http://arxiv.org/abs/2406.14978)|**[link](https://github.com/deguchihiroyuki/e2gs)**|事件相机以其高动态范围、无运动模糊和低能耗等特点而闻名，近年来由于这些优势，事件相机已广泛应用于各个领域。在过去几年中，基于事件的三维重建领域取得了显著进展，其中基于神经辐射场 (NeRF) 的方法在逼真的视图合成方面展现出巨大潜力。然而，NeRF 的体渲染范式需要大量的训练和渲染时间。在本文中，我们介绍了事件增强型高斯散射 (E2GS)，这是一种将事件数据融入高斯散射的新方法，该方法最近在视图合成领域取得了重大进展。我们的 E2GS 可以有效地利用模糊图像和事件数据，显著改善图像去模糊效果，并生成高质量的新视图。我们在合成数据集和真实世界数据集上进行的全面实验表明，我们的 E2GS 能够生成视觉上吸引人的渲染结果，同时提供更快的训练和渲染速度（140 FPS）。我们的代码可在 https://github.com/deguchihiroyuki/E2GS 获取。||
|**2024-06-20**|[Deblurring Neural Radiance Fields with Event-driven Bundle Adjustment](http://arxiv.org/abs/2406.14360)|null|神经辐射场 (NeRF) 利用高质量的多视图图像作为输入，在 3D 表征学习和新视图合成方面取得了令人瞩目的成果。然而，图像中的运动模糊经常出现在低光照和高速运动场景中，这会显著降低 NeRF 的重建质量。现有的去模糊 NeRF 方法难以估计曝光时间内的信息，无法准确地模拟运动模糊。相比之下，受生物启发的事件相机以高时间分辨率测量强度变化，弥补了这一信息不足。在本文中，我们提出了事件驱动的光束法平差去模糊神经辐射场 (EBAD-NeRF)，通过利用混合事件-RGB 数据联合优化可学习的姿态和 NeRF 参数。我们引入了强度变化度量事件损失和光度模糊损失，以加强相机运动模糊的显式建模。合成数据和真实捕获数据的实验结果表明，与先前的工作相比，EBAD-NeRF 可以在曝光时间内获得准确的相机姿态，并学习到更清晰的 3D 表征。||
|**2024-06-19**|[Freq-Mip-AA : Frequency Mip Representation for Anti-Aliasing Neural Radiance Fields](http://arxiv.org/abs/2406.13251)|**[link](https://github.com/yi0109/freqmipaa)**|神经辐射场 (NeRF) 在表示 3D 场景和生成新视图方面已取得显著成功。然而，它们经常遇到混叠伪影问题，尤其是在渲染与训练视图相机距离不同的图像时。为了解决这个问题，Mip-NeRF 提出使用体积视锥来渲染像素，并建议使用集成位置编码 (IPE)。虽然有效，但这种方法由于依赖 MLP 架构，因此需要较长的训练时间。在这项工作中，我们提出了一种利用基于网格的表示的新型抗锯齿技术，通常可以显著缩短训练时间。此外，我们利用频域表示来处理受采样定理启发的混叠问题。所提出的方法 FreqMipAA 利用特定尺度的低通滤波 (LPF) 和可学习的频率掩码。特定尺度的低通滤波器 (LPF) 可防止混叠并优先考虑重要的图像细节，而可学习的掩码则可有效去除有问题的 高频元素，同时保留必要的信息。通过采用特定尺度的 LPF 和可训练的掩码，FreqMipAA 可以有效消除混叠因子，同时保留重要细节。我们通过将所提出的技术整合到广泛使用的基于网格的方法中来验证其有效性。实验结果表明，FreqMipAA 有效地解决了混叠问题，并在多尺度 Blender 数据集中取得了最先进的结果。我们的代码可在 https://github.com/yi0109/FreqMipAA 获取。||
|**2024-06-18**|[Head Pose Estimation and 3D Neural Surface Reconstruction via Monocular Camera in situ for Navigation and Safe Insertion into Natural Openings](http://arxiv.org/abs/2406.13048)|null|随着模拟在医疗和介入中的重要性不断提高，预计将会出现一个简化且低成本的平台来执行个性化诊断和治疗。3D Slicer 不仅可以进行医学图像分析和可视化，还可以提供手术导航和手术规划功能。在本文中，我们选择 3D Slicer 作为我们的基础平台，并使用单目相机作为传感器。然后，我们使用神经辐射场（NeRF）算法完成了人头的三维模型重建。我们比较了 NeRF 算法在生成三维人头场景中的准确性，并利用 MarchingCube 算法生成了对应的三维网格模型。通过单目视觉获得的个人头部姿态被实时传输到 3D Slicer 中创建的场景中。本文展示的演示包括 3D Slicer 场景中的人头模型与检测到的头部姿态之间变换的实时同步。此外，我们测试了一个场景，其中一个工具标有 ArUco 标记，并由单个摄像头跟踪，该工具同步指向头部姿势的实时变换。这些演示表明，我们的方法可以为鼻咽拭子采集或插管提供可行的实时模拟平台。||
|**2024-06-18**|[Fast Global Localization on Neural Radiance Field](http://arxiv.org/abs/2406.12202)|null|神经辐射场 (NeRF) 提出了一种表示场景的新颖方法，允许从 2D 图像进行高质量的 3D 重建。在其取得显著成就之后，NeRF 地图中的全局定位成为实现广泛应用的一项基本任务。最近，Loc-NeRF 展示了一种将传统的蒙特卡洛定位与 NeRF 相结合的定位方法，展示了使用 NeRF 作为环境地图的良好结果。然而，尽管取得了进步，但 Loc-NeRF 仍面临着光线渲染过程耗时的挑战，这在实际应用中可能是一个很大的限制。为了解决这个问题，我们引入了 Fast Loc-NeRF，它利用从粗到精的方法来实现更高效、更准确的基于 NeRF 地图的全局定位。具体来说，Fast Loc-NeRF 将渲染的像素与观察到的图像从低分辨率到高分辨率的多分辨率上进行匹配。因此，它在保持精确定位结果的同时，加快了代价高昂的粒子更新过程。此外，为了剔除异常粒子，我们提出了粒子剔除加权方法，该方法利用 NeRF 的特性估计粒子的不确定性，并在粒子加权过程中考虑这些不确定性。我们的 Fast Loc-NeRF 在多个基准测试中设定了新的最先进的定位性能，证明了其准确性和效率。||
|**2024-06-17**|[DistillNeRF: Perceiving 3D Scenes from Single-Glance Images by Distilling Neural Fields and Foundation Model Features](http://arxiv.org/abs/2406.12095)|null|我们提出了DistillNeRF，这是一个自监督学习框架，旨在解决自动驾驶中从有限的2D观察中理解3D环境的挑战。我们的方法是一个可泛化的前馈模型，可以从稀疏的单帧多视图相机输入中预测丰富的场景神经表示，并通过可微渲染进行自监督训练，以重建RGB、深度或特征图像。我们的第一个见解是，通过生成密集的深度和虚拟相机目标来训练，从而利用每个场景优化的神经辐射场 (NeRF)，帮助我们的模型从稀疏的非重叠图像输入中学习3D几何。其次，为了学习语义丰富的3D表示，我们建议从预先训练的2D基础模型（如CLIP或DINOv2）中提取特征，从而在不需要昂贵的3D人工标注的情况下实现各种下游任务。为了利用这两个见解，我们引入了一种新颖的模型架构，该架构具有两阶段提升-splat-shoot编码器和参数化的稀疏分层体素表示。在NuScenes数据集上的实验结果表明，DistillNeRF在场景重建、新视图合成和深度估计方面明显优于现有的可比的自监督方法；并且它允许进行有竞争力的零样本3D语义占用预测，以及通过提取的基础模型特征进行开放世界场景理解。演示和代码将在https://distillnerf.github.io/上提供。||
|**2024-06-17**|[Uncertainty modeling for fine-tuned implicit functions](http://arxiv.org/abs/2406.12082)|null|隐函数，如神经辐射场 (NeRFs)、占据网络和符号距离函数 (SDFs)，已成为计算机视觉中从稀疏视图重建详细物体形状的关键。由于输入的极度稀疏性和数据损坏引起的分布偏移，使用这些模型实现最佳性能可能具有挑战性。为此，大型的、无噪声的合成数据集可以作为形状先验，帮助模型填补空白，但必须谨慎对待由此产生的重建结果。不确定性估计对于评估这些重建的质量至关重要，特别是在识别模型对其从先验推断出的部分不确定的区域方面。在本文中，我们介绍了 Dropsembles，这是一种用于调整后的隐函数的不确定性估计的新方法。我们通过一系列实验证明了我们方法的有效性，从玩具示例开始，逐步发展到真实场景。具体来说，我们在合成解剖数据上训练了一个卷积占据网络，并在腰椎的低分辨率 MRI 分割上对其进行了测试。我们的结果表明，Dropsembles 实现了深度集成方法的准确性和校准水平，但计算成本显著降低。||
|**2024-06-17**|[LLaNA: Large Language and NeRF Assistant](http://arxiv.org/abs/2406.11840)|null|多模态大型语言模型 (MLLM) 在理解图像和 3D 数据方面表现出色。然而，这两种模态在全面捕捉物体的外观和几何形状方面都存在缺陷。同时，神经辐射场 (NeRF) 已成为一种日益普遍的模态，它将信息编码在简单多层感知器 (MLP) 的权重中，同时编码了物体的几何形状和逼真的外观。本文研究了将 NeRF 纳入 MLLM 的可行性和有效性。我们创建了 LLaNA，这是第一个能够执行 NeRF 图像描述和问答等新任务的通用 NeRF 语言助手。值得注意的是，我们的方法直接处理 NeRF 的 MLP 权重以提取有关所表示对象的信息，而无需渲染图像或实例化 3D 数据结构。此外，我们构建了一个包含 NeRF 数据集，其中包含用于各种 NeRF 语言任务的文本注释，无需人工干预。基于此数据集，我们开发了一个基准来评估我们方法的 NeRF 理解能力。结果表明，处理 NeRF 权重比从 NeRF 中提取 2D 或 3D 表示表现更好。||
|**2024-06-17**|[InterNeRF: Scaling Radiance Fields via Parameter Interpolation](http://arxiv.org/abs/2406.11737)|null|神经辐射场 (NeRF) 在大型真实场景中具有无与伦比的保真度。扩展 NeRF 的一种常见方法是将场景划分为多个区域，每个区域都有自己的参数。如果采用简单的实现方式，这种方法会受到测试时缩放性差以及外观和几何形状不一致的限制。我们提出了一种名为 InterNeRF 的新颖架构，用于使用模型参数的子集来渲染目标视图。我们的方法支持核外训练和渲染，在仅略微增加训练时间的情况下增加了总模型容量。我们展示了在多房间场景中的显著改进，同时在标准基准测试中保持了竞争力。||
|**2024-06-17**|[Projecting Radiance Fields to Mesh Surfaces](http://arxiv.org/abs/2406.11570)|null|辐射场能够生成高保真度且渲染速度快的图像，但难以操控。我们结合了辐射场和网格表面的优点，有效地实现了不同外观之间的人物化身纹理迁移。我们使用三维高斯散射将源表示为辐射场，然后将高斯投影到目标网格上。我们的流程包括源预处理、目标矢量化和纹理投影。投影在纯CPU计算中只需1.12秒即可完成，而基线技术“逐面纹理投影”和“光线投射”分别需要31秒和4.1分钟。这种方法降低了计算需求，使其适用于从低端移动设备到高端计算机的更广泛设备。||
|**2024-06-16**|[Learning Relighting and Intrinsic Decomposition in Neural Radiance Fields](http://arxiv.org/abs/2406.11077)|null|从神经辐射场中提取内在成分（如反射率和阴影）的任务越来越受到关注。然而，当前的方法主要集中在合成场景和孤立物体上，而忽略了具有背景的真实场景的复杂性。为了解决这一差距，我们的研究引入了一种将重新照明与内在分解相结合的方法。通过利用场景中的光线变化来生成伪标签，我们的方法为内在分解提供了指导，而无需地面真实数据。我们的方法基于物理约束，确保了跨不同场景类型的鲁棒性，并减少了对预训练模型或手工先验的依赖。我们在合成数据集和真实世界数据集上验证了我们的方法，取得了令人信服的结果。此外，我们的方法在图像编辑任务中的适用性也显示出良好的效果。||
|**2024-06-15**|[fNeRF: High Quality Radiance Fields from Practical Cameras](http://arxiv.org/abs/2406.10633)|null|近年来，神经辐射场的开发使得从多视图相机数据中对场景和物体进行逼真的三维重建达到了前所未有的水平。然而，以往的方法使用过于简化的针孔相机模型，导致散焦模糊被“烘焙”到重建的辐射场中。我们提出了一种对射线投射的改进，利用透镜的光学特性来增强存在散焦模糊时的场景重建。这使我们能够利用有限孔径的实际相机的测量结果来提高辐射场重建的质量。我们证明，与针孔模型和其他散焦模糊模型的近似相比，所提出的模型更接近地匹配了实际相机的散焦模糊行为，特别是在存在部分遮挡的情况下。这使我们能够实现更清晰的重建，在合成数据集和真实数据集上，将所有聚焦图像验证的 PSNR 提高了 3 dB。||
|**2024-06-15**|[Federated Neural Radiance Field for Distributed Intelligence](http://arxiv.org/abs/2406.10474)|null|新视角合成（NVS）是许多增强现实（AR）和虚拟现实（VR）应用中的重要技术。最近提出的神经辐射场（NeRF）方法在 NVS 任务中表现出优异的性能，并已应用于其他相关领域。然而，由于严格的法规和隐私问题，某些具有分布式数据存储的应用场景可能会给 NeRF 方法获取训练图像带来挑战。为了克服这一挑战，我们专注于 FedNeRF，这是一种基于联邦学习（FL）的 NeRF 方法，它利用不同数据所有者可用的图像，同时保护数据隐私。在本文中，我们首先构建了一个资源丰富、功能多样的联邦学习测试平台。然后，我们在这样一个实际的联邦学习系统中部署 FedNeRF 算法，并在部分客户端选择的情况下进行 FedNeRF 实验。预计本文提出的 FedNeRF 方法研究将有助于促进 NeRF 方法在分布式数据存储场景中的未来应用。||
|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|在非结构化的旅游环境中拍摄的照片经常呈现出多变的外观和短暂的遮挡，这对精确的场景重建提出了挑战，并在新视角合成中导致了伪影。尽管先前的方法已经将神经辐射场 (NeRF) 与其他可学习模块相结合来处理动态外观和消除瞬态对象，但其大量的训练需求和缓慢的渲染速度限制了实际部署。最近，三维高斯 splatting (3DGS) 已成为 NeRF 的一种很有前途的替代方案，它提供了卓越的训练和推理效率以及更好的渲染质量。本文介绍了 Wild-GS，这是一种针对不受约束的照片集优化的 3DGS 创新改编，同时保留了其效率优势。Wild-GS 通过每个图像的固有材质属性、全局照明和相机属性以及逐点反射率的局部方差来确定每个三维高斯的外观。与先前在图像空间中对参考特征进行建模的方法不同，Wild-GS 通过对从参考图像中提取的三平面进行采样，将像素外观特征显式对齐到相应的局部高斯。这种新颖的设计有效地将参考视图的高频细节外观转移到 3D 空间，并显着加快了训练过程。此外，二维可见性图和深度正则化分别用于减轻瞬态效应和约束几何形状。大量实验表明，Wild-GS 在所有现有技术中实现了最先进的渲染性能以及最高的训练和推理效率。||
|**2024-06-14**|[GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors](http://arxiv.org/abs/2406.10111)|null|从低分辨率输入视图实现高分辨率新视图合成 (HRNVS) 是一项具有挑战性的任务，因为缺乏高分辨率数据。以前的方法从低分辨率输入视图优化高分辨率神经辐射场 (NeRF)，但渲染速度缓慢。在这项工作中，我们基于 3D 高斯渲染 (3DGS) 开发了我们的方法，因为它能够以更快的渲染速度生成高质量的图像。为了缓解高分辨率合成的数据短缺问题，我们建议利用现成的 2D 扩散先验，通过分数蒸馏采样 (SDS) 将 2D 知识提取到 3D。然而，由于生成先验带来的随机性，将 SDS 直接应用于基于高斯的 3D 超分辨率会导致不希望和冗余的 3D 高斯基元。为了缓解这个问题，我们引入了两种简单而有效的技术来减少 SDS 引入的随机扰动。具体来说，我们 1) 使用退火策略缩小 SDS 中扩散时间步长的范围；2) 在密集化过程中随机丢弃冗余的高斯基元。大量实验表明，我们提出的 GaussainSR 可以在合成和真实世界数据集上仅使用低分辨率输入就能获得高质量的 HRNVS 结果。项目页面：https://chchnii.github.io/GaussianSR/||
|**2024-06-14**|[RaNeuS: Ray-adaptive Neural Surface Reconstruction](http://arxiv.org/abs/2406.09801)|**[link](https://github.com/wangyida/ra-neus)**|我们的目标是利用可微分的辐射场（例如 NeRF）来重建详细的 3D 表面，以及生成标准的新颖视图渲染。已经有相关的方法来执行此类任务，通常是利用带符号距离场 (SDF)。然而，最先进的方法仍然无法正确重建小尺度细节，例如树叶、绳索和纺织品表面。考虑到不同的方法通过全局常数 Eikonal 正则化来制定和优化从 SDF 到辐射场的投影，我们通过逐射线加权因子进行改进，以优先考虑渲染和零交叉表面拟合，并在建立完美 SDF 的基础上进行。我们建议自适应地调整带符号距离场上的正则化，以便不令人满意的渲染光线不会强制执行无效的强 Eikonal 正则化，并允许来自具有良好学习辐射的区域的梯度有效地反向传播到 SDF。因此，平衡这两个目标以生成准确和详细的表面。此外，关于 SDF 中的零交叉表面和辐射场中的渲染点之间是否存在几何偏差，投影也会根据优化期间不同的 3D 位置进行调整。我们提出的 RaNeuS 在合成数据集和真实数据集上都进行了广泛的评估，在新颖视图合成和几何重建方面均取得了最先进的结果。||
|**2024-06-13**|[Neural NeRF Compression](http://arxiv.org/abs/2406.08943)|null|神经辐射场 (NeRFs) 已成为通过连续体积表示捕获详细 3D 场景的强大工具。最近的 NeRF 利用特征网格来提高渲染质量和速度；然而，这些表示引入了大量的存储开销。本文提出了一种有效压缩基于网格的 NeRF 模型的新方法，解决了存储开销问题。我们的方法基于非线性变换编码范式，采用神经压缩来压缩模型的特征网格。由于缺乏涉及许多独立同分布场景的训练数据，我们为单个场景设计了一种无编码器、端到端优化的轻量级解码器方法。为了利用潜在特征网格的空间不均匀性，我们引入了重要性加权的率失真目标函数和采用掩蔽机制的稀疏熵模型。我们的实验结果表明，我们提出的方法在基于网格的 NeRF 压缩效率和重建质量方面优于现有工作。||
|**2024-06-06**|[A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation](http://arxiv.org/abs/2406.04253)|null|三维建模一直是计算机视觉和计算机图形学的重要领域。近年来，由于神经表示和生成模型的突破，我们见证了三维建模的快速发展。三维人体建模作为游戏和动画等众多现实应用的核心，备受关注。在过去的几年里，出现了大量关于创建三维人体化身的工作，形成了一个丰富的三维人体建模知识库。文献规模之大，个人难以全面掌握。本次综述旨在从重建和生成两个角度，全面概述这些新兴的三维人体化身建模技术。首先，我们回顾了具有代表性的三维人体重建方法，包括基于像素对齐隐函数、神经辐射场和三维高斯散射等方法。然后，我们总结了具有代表性的三维人体生成方法，特别是那些使用大型语言模型（如CLIP）、扩散模型和各种三维表示的方法，这些方法展示了最先进的性能。最后，我们讨论了对现有方法的反思以及三维人体化身建模面临的挑战，为未来的研究指明了方向。||
|**2024-06-13**|[3D-HGS: 3D Half-Gaussian Splatting](http://arxiv.org/abs/2406.02720)|**[link](https://github.com/lihaolin88/3d-half-gaussian-splatting)**|照片级真实感三维重建是三维计算机视觉中的一个基本问题。由于近期神经渲染技术的出现，该领域取得了长足的进步。这些技术主要旨在专注于学习三维场景的体积表示，并通过渲染得到的损失函数来细化这些表示。其中，三维高斯 splatting (3D-GS) 已成为一种重要方法，其性能超越了神经辐射场 (NeRF)。3D-GS 使用参数化的三维高斯模型来建模空间位置和颜色信息，并结合基于图块的快速渲染技术。尽管其渲染性能和速度都非常出色，但使用三维高斯核在准确表示不连续函数方面存在固有限制，特别是在形状不连续的边缘和角落，以及跨不同纹理的颜色不连续处。为了解决这个问题，我们建议采用三维半高斯 (3D-HGS) 核，它可以作为一种即插即用的核函数。我们的实验表明，它们能够在不影响渲染速度的情况下，提高当前 3D-GS 相关方法的性能，并在各种数据集上实现最先进的渲染性能。||
|**2024-06-04**|[FastLGS: Speeding up Language Embedded Gaussians with Feature Grid Mapping](http://arxiv.org/abs/2406.01916)|null|语义交互式辐射场因其在促进用户友好和自动化的现实世界3D场景理解应用方面的潜力，一直是一项吸引人的任务。然而，要在辐射场中同时实现高质量、高效率和零样本能力的语义化是一项具有挑战性的任务。在这项工作中，我们提出了FastLGS，一种支持在高分辨率的3D高斯样条（3DGS）中进行实时开放词汇查询的方法。我们提出了语义特征网格来保存基于Segment Anything Model (SAM)掩码提取的多视图CLIP特征，并通过3DGS将网格映射到低维特征以进行语义场训练。一旦训练完成，我们就可以通过渲染特征的特征网格恢复像素对齐的CLIP嵌入，用于开放词汇查询。与其他最先进方法的比较证明，FastLGS在速度和精度方面均能达到第一名的性能，其中FastLGS比LERF快98倍，比LangSplat快4倍。同时，实验表明FastLGS具有适应性，并兼容许多下游任务，例如3D分割和3D对象修复，可以轻松应用于其他3D操作系统。||
|**2024-05-30**|[ $\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving](http://arxiv.org/abs/2405.20323)|**[link](https://github.com/nnanhuang/s3gaussian)**|逼真的街道场景 3D 重建是开发用于自动驾驶的真实世界模拟器的关键技术。尽管神经辐射场 (NeRF) 对驾驶场景有效，但由于其速度更快且表示更明确，3D 高斯 splatting (3DGS) 正成为一个有前途的方向。 然而，大多数现有的街道 3DGS 方法都需要跟踪 3D 车辆边界框来分解静态和动态元素以进行有效的重建，从而限制了它们在野外场景中的应用。 为了在没有昂贵注释的情况下促进高效的 3D 场景重建，我们提出了一种自监督的街道高斯（$\textit{S}^3$Gaussian）方法，用于从 4D 一致性中分解动态和静态元素。 我们使用 3D 高斯表示每个场景以保持其明确性，并进一步使用时空场网络来紧凑地模拟 4D 动态。 我们在具有挑战性的 Waymo-Open 数据集上进行了大量实验，以评估我们方法的有效性。 我们的 $\textit{S}^3$ Gaussian 展示了在不使用 3D 标注的情况下分解静态和动态场景的能力，并实现了最佳性能。 代码可在以下网址获得：https://github.com/nnanhuang/S3Gaussian/。||
|**2024-05-28**|[RT-GS2: Real-Time Generalizable Semantic Segmentation for 3D Gaussian Representations of Radiance Fields](http://arxiv.org/abs/2405.18033)|null|高斯渲染 (Gaussian Splatting) 通过实现实时的高渲染性能，彻底改变了新视角合成的世界。最近，研究重点是为下游任务丰富这些 3D 表示的语义信息。在本文中，我们介绍了 RT-GS2，这是第一个采用高斯渲染的通用语义分割方法。虽然现有的基于高斯渲染的方法依赖于特定场景的训练，但 RT-GS2 展示了泛化到未见场景的能力。我们的方法采用了一种新方法，首先以自监督的方式提取与视图无关的 3D 高斯特征，然后进行新颖的视图相关/视图无关 (VDVI) 特征融合，以增强不同视图之间的语义一致性。在三个不同数据集上的大量实验表明，RT-GS2 在语义分割质量方面优于最先进的方法，例如在 Replica 数据集上 mIoU 提高了 8.01%。此外，我们的方法实现了 27.03 FPS 的实时性能，与现有方法相比实现了惊人的 901 倍加速。据我们所知，这项工作通过引入第一个用于辐射场的 3D 高斯表示的实时通用语义分割方法，代表了该领域的重大进步。||
|**2024-05-29**|[PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting](http://arxiv.org/abs/2405.16829)|null|神经辐射场 (NeRFs) 在合成大规模场景的逼真图像方面表现出了非凡的能力。然而，它们经常受到细节丢失和渲染时间长的困扰。三维高斯散射最近被引入作为一种有效的替代方案，实现了高保真视觉效果和加速渲染性能。尽管如此，扩展三维高斯散射充满了挑战。具体来说，大规模场景需要整合跨多个尺度和不同视点的对象，这通常会导致效率下降，因为高斯需要在细节级别之间取得平衡。此外，从大规模数据集中通过 COLMAP 生成初始化点在计算上要求很高，并且容易导致不完整的重建。为了应对这些挑战，我们提出了采用 NeRF 初始化的金字塔式三维高斯散射 (PyGS)。我们的方法使用以金字塔形式排列的分层高斯集合来表示场景。金字塔的顶层由一些大型高斯组成，而每个后续层都包含更密集的较小高斯集合。我们通过以不同的频率对快速训练的基于网格的 NeRF 进行采样，从而有效地初始化这些金字塔高斯。我们将这些金字塔高斯分组到集群中，并使用紧凑的加权网络在渲染期间动态确定每个集群的每个金字塔级别的影响，同时考虑摄像机视点。我们的方法在多个大规模数据集上实现了显着的性能提升，并且渲染时间比当前最先进的方法快 400 多倍。||
|**2024-05-11**|[Direct Learning of Mesh and Appearance via 3D Gaussian Splatting](http://arxiv.org/abs/2405.06945)|null|准确地重建包含显式几何信息的3D场景既具有吸引力又具有挑战性。几何重建可以受益于结合可微分的表观模型，例如神经辐射场和3D高斯散射（3DGS）。在这项工作中，我们提出了一个可学习的场景模型，它将3DGS与一种称为网格的显式几何表示相结合。我们的模型以端到端的方式学习网格和外观，我们将3D高斯绑定到网格面上，并执行3DGS的可微分渲染以获得光度监督。该模型创建了一条有效的信息路径来监督场景学习，包括网格。实验结果表明，学习到的场景模型不仅实现了最先进的渲染质量，而且还支持使用显式网格进行操作。此外，由于网格和外观的端到端学习，我们的模型在适应场景更新方面具有独特的优势。||
|**2024-05-10**|[OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation](http://arxiv.org/abs/2405.06547)|**[link](https://github.com/lin-jinwei/OneTo3D)**|将单张图像转换为可编辑的动态三维模型和视频生成是单图像三维表示或三维重建研究领域的一个新方向和变化。与原始的神经辐射场相比，高斯渲染技术在隐式三维重建方面展现出优势。随着技术和原理的快速发展，人们尝试使用稳定扩散模型通过文本指令生成目标模型。然而，使用常规的隐式机器学习方法难以实现精确的动作和行为控制，并且难以生成内容长且语义连续的三维视频。为了解决这个问题，我们提出了OneTo3D，一种利用单张图像生成可编辑三维模型并生成目标语义连续、时间无限的三维视频的方法和理论。我们使用了一个基本的高斯渲染模型从单张图像生成三维模型，这需要较少的显存和计算能力。随后，我们设计了一种用于物体骨架的自动生成和自适应绑定机制。结合我们提出的可重新编辑的动作和行为分析与控制算法，我们可以在构建三维模型的精确动作和行为控制，以及根据输入文本指令生成稳定的语义连续、时间无限的三维视频方面，取得比SOTA项目更好的性能。本文将分析详细的实现方法和理论分析，并进行相关比较和结论总结。项目代码开源。||

<p align=right>(<a href=#updated-on-20240629>back to top</a>)</p>

## 分类/检测/识别/分割

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-27**|[HUWSOD: Holistic Self-training for Unified Weakly Supervised Object Detection](http://arxiv.org/abs/2406.19394)|**[link](https://github.com/shenyunhang/huwsod)**|大多数弱监督目标检测 (WSOD) 方法依赖于传统的目标建议来生成候选区域，并且面临着训练不稳定的问题，容易陷入局部最优。在本文中，我们介绍了一种名为 HUWSOD 的统一的、高容量的弱监督目标检测 (WSOD) 网络，该网络利用了一个全面的自训练框架，无需外部模块或额外的监督。HUWSOD 创新地结合了自监督建议生成器和自动编码器建议生成器以及多速率重采样金字塔，取代了传统的目标建议，从而实现了端到端的 WSOD 训练和推理。此外，我们实施了一个整体的自训练方案，通过逐步熵最小化和一致性约束正则化来细化检测分数和坐标，确保对同一图像的随机增强进行一致的预测。在 PASCAL VOC 和 MS COCO 上进行的大量实验表明，HUWSOD 与最先进的 WSOD 方法相媲美，无需离线建议和额外数据。HUWSOD 的峰值性能接近于完全监督的 Faster R-CNN。我们的研究结果还表明，随机初始化的边界框虽然与精心设计的离线目标建议有很大不同，但对 WSOD 训练是有效的。||
|**2024-06-27**|[Fibottention: Inceptive Visual Representation Learning with Diverse Attention Across Heads](http://arxiv.org/abs/2406.19391)|null|视觉感知任务主要由视觉Transformer（ViT）架构解决，尽管它们有效，但由于计算自注意力的二次复杂性，遇到了计算瓶颈。这种低效率很大程度上是因为自注意力头捕获了冗余的标记交互，反映了视觉数据中的固有冗余。许多工作旨在降低 ViT 中自注意力的计算复杂度，从而开发高效且稀疏的 Transformer 架构。在本文中，我们从效率的角度出发，意识到在 ViT 中引入任何稀疏的自注意力策略都可以保持较低的计算开销。然而，这些策略并不是最优的，因为它们通常无法捕捉到细粒度的视觉细节。这一观察结果促使我们提出一种通用的、高效的稀疏架构，名为 Fibottention，用于以超线性复杂度逼近自注意力，该架构建立在斐波那契数列的基础上。Fibottention 的关键策略包括：它排除近端标记以减少冗余，通过设计采用结构化稀疏性以降低计算需求，并在注意力头之间纳入类似 Inception 的多样性。这种多样性确保了通过非重叠标记交互捕获互补信息，从而优化了 ViT 在视觉表示学习中的性能和资源利用率。我们将 Fibottention 机制嵌入到多个专用于视觉任务的最先进的 Transformer 架构中。Fibottention 仅利用自注意力头中 2-6% 的元素，与标准 ViT 相比，在图像分类、视频理解和机器人学习任务这三个领域的九个数据集中，Fibottention 与 ViT 及其变体始终实现显著的性能提升。||
|**2024-06-27**|[STAL3D: Unsupervised Domain Adaptation for 3D Object Detection via Collaborating Self-Training and Adversarial Learning](http://arxiv.org/abs/2406.19362)|null|现有的3D目标检测模型由于标注成本高昂，以及由于域差异导致对未知数据的迁移能力差，因此存在局限性。无监督域适应（UDA）旨在将训练在有标签源域上的检测模型泛化，以使其在未探索的目标域上表现稳健，为跨域3D目标检测提供了一种有前景的解决方案。尽管基于自训练（ST）的跨域3D检测方法在伪标签技术的帮助下取得了显著进展，但由于缺乏特征分布对齐的过程，当存在显著的域差异时，它们仍然面临着伪标签质量低下的问题。虽然基于对抗学习（AL）的方法可以有效地对齐源域和目标域的特征分布，但由于无法在目标域中获取标签，因此不得不采用非对称优化损失，从而导致源域偏差这一具有挑战性的问题。为了克服这些限制，我们提出了一种新的基于自训练和对抗学习协作的3D目标检测无监督域适应框架，称为STAL3D，它释放了伪标签和特征分布对齐的互补优势。此外，我们还设计了背景抑制对抗学习（BS-AL）模块和尺度过滤模块（SFM），专门针对3D跨域场景，有效地缓解了背景干扰比例大和源域尺寸偏差的问题。我们的STAL3D在多个跨域任务上实现了最先进的性能，甚至在Waymo $\rightarrow$ KITTI和Waymo $\rightarrow$ KITTI-rain上超过了Oracle结果。||
|**2024-06-27**|[Learning Visual Conditioning Tokens to Correct Domain Shift for Fully Test-time Adaptation](http://arxiv.org/abs/2406.19341)|null|完全测试时域自适应旨在根据推理阶段输入样本的顺序分析来调整网络模型，以解决深度神经网络的跨域性能下降问题。这项工作基于以下有趣的发现：在基于 Transformer 的图像分类中，第一个 Transformer 编码器层中的类别token可以在测试时域自适应期间学习捕获目标样本的特定域特征。当与输入图像块嵌入相结合时，这个学习到的token能够在 Transformer 编码过程中逐渐从输入样本的特征表示中去除特定域的信息，从而显著提高源模型在不同域上的测试时域自适应性能。我们将此类别token称为视觉条件token（VCT）。为了成功学习VCT，我们提出了一种双层学习方法来捕获特定域特征的长期变化，同时适应特定实例特征的局部变化。在基准数据集上的实验结果表明，我们提出的双层视觉条件token学习方法能够显著提高测试时域自适应性能，最高可达1.9%。||
|**2024-06-27**|[Towards Reducing Data Acquisition and Labeling for Defect Detection using Simulated Data](http://arxiv.org/abs/2406.19175)|null|在许多制造环境中，为机器学习和计算机视觉标注数据成本高昂，但生成合成数据的成本要低得多。因此，对于许多需要大量训练数据的机器学习应用来说，用合成数据替代真实数据很有吸引力。然而，仅仅依靠合成数据通常不足以有效地训练在真实数据上表现良好的模型，这主要是由于合成数据和真实数据之间的域差异。我们讨论了在检测铝轮 X 射线扫描缺陷时处理这种域差异的方法。我们使用模拟和真实的 X 射线图像，采用不同的策略训练目标检测模型，以确定在最大限度地减少对带注释的真实训练样本需求的同时，产生最佳检测结果的训练方法。我们的初步研究结果表明，如果可用的带注释样本总数是固定的，那么 sim-2-real 域适应方法比完全监督的 oracle 方法更具成本效益。在一定数量的标记真实样本的情况下，对合成数据和未标记真实数据的混合训练以极低的成本实现了相当甚至更好的检测结果。我们认为，未来对不同训练策略的成本效益进行研究对于更好地理解如何在应用机器学习项目中分配预算是非常重要的。||
|**2024-06-27**|[Super-resolution imaging using super-oscillatory diffractive neural networks](http://arxiv.org/abs/2406.19126)|null|光学超振荡技术能够突破衍射极限，实现远场超分辨率成像。然而，由于缺乏更先进的设计方法和有限的设计自由度，现有的用于空间超分辨率成像系统的超振荡透镜在性能方面仍然面临着关键的限制。在这里，我们提出了一种光学超振荡衍射神经网络，即SODNN，它可以实现超分辨率的空间分辨率，用于超越衍射极限的成像，其性能优于现有方法。SODNN的构建是利用衍射层来实现光学互连，并利用成像样本或生物传感器来实现非线性，从而调制入射光场，在三维空间中产生光学超振荡效应，并产生超分辨率的焦点。通过在入射波长尺寸为 $\lambda$的情况下，利用三维光场约束对衍射层进行优化，我们在超过400$\lambda$的远场距离处实现了半峰全宽为0.407$\lambda$的超振荡光斑，在视场范围内没有旁瓣，景深超过10$\lambda$ 。此外，SODNN实现了多波长和多焦点阵列，有效地避免了色差。我们的研究工作将启发智能光学仪器的发展，以促进成像、传感、感知等方面的应用。||
|**2024-06-27**|[Adaptive Stochastic Weight Averaging](http://arxiv.org/abs/2406.19092)|**[link](https://github.com/dice-group/aswa)**|集成模型通常可以提高挑战性任务的泛化性能。然而，基于预测平均的传统技术存在三个众所周知的缺点：训练多个模型的计算开销、推理延迟增加以及测试时的内存需求。为了解决这些问题，随机权重平均（SWA）技术从特定时期开始维护模型参数的运行平均值。尽管具有潜在的优势，但维护参数的运行平均值可能会阻碍泛化能力，因为潜在的运行模型开始过拟合。相反，与潜在的运行模型相比，选择不当的起点会使 SWA 更容易出现欠拟合。在这项工作中，我们提出了自适应随机权重平均（ASWA）技术，该技术仅当验证数据集上的泛化性能得到改善时才更新模型参数的运行平均值。因此，ASWA 可以看作是 SWA 与早停技术的结合，前者接受参数集成模型上的所有更新，而后者拒绝底层运行模型上的任何更新。我们进行了广泛的实验，从图像分类到知识图谱上的多跳推理。我们对 7 个基线模型的 11 个基准数据集进行的实验表明，ASWA 可以带来跨模型和数据集的统计上更好的泛化性能。||
|**2024-06-27**|[Segment Anything Model for automated image data annotation: empirical studies using text prompts from Grounding DINO](http://arxiv.org/abs/2406.19057)|null|Grounding DINO 和 Segment Anything Model (SAM) 分别在零样本目标检测和图像分割方面取得了令人瞩目的性能。它们一起在彻底改变零样本语义分割或数据标注方面具有巨大潜力。然而，在医学图像分割等专业领域，感兴趣的目标（例如，器官、组织和肿瘤）可能不属于现有的类别名称。为了解决这个问题，Grounding DINO 的指称表达式理解 (REC) 能力被用来通过语言描述检测任意目标。然而，最近的研究表明，由于 REC 框架在目标不存在于给定图像中时倾向于做出假阳性预测，因此在这种应用场景下存在严重限制。 而且，虽然这个瓶颈对于开放集语义分割的前景至关重要，但通过研究预测误差可以实现多少改进仍然很大程度上未知。为此，我们对八个公开可用的数据集进行了实证研究，结果表明，这些错误始终遵循可预测的模式，因此可以通过简单的策略来缓解。具体来说，我们表明，这些具有明显置信度的误报通常占据较大的图像区域，并且通常可以通过它们的相对大小进行过滤。更重要的是，我们希望这些观察结果能够激发未来改进基于 REC 的检测和自动分割的研究。使用这种技术，我们评估了 SAM 在来自各个专业领域的多个数据集上的性能，并报告了与手动方法相比在分割性能和标注时间节省方面的显著改进。||
|**2024-06-27**|[BiCo-Fusion: Bidirectional Complementary LiDAR-Camera Fusion for Semantic- and Spatial-Aware 3D Object Detection](http://arxiv.org/abs/2406.19048)|null|三维目标检测是自动驾驶领域一项至关重要的任务，近年来，融合多模态输入（如激光雷达和相机数据）来完成这项任务已成为一种新趋势。然而，现有的方法要么忽略了激光雷达特征的稀疏性，要么由于模态差异而无法同时保留激光雷达的原始空间结构和相机特征的语义密度。为了解决这些问题，本文提出了一种新颖的双向互补激光雷达-相机融合框架，称为 BiCo-Fusion，它可以实现鲁棒的语义和空间感知三维目标检测。其关键见解是相互融合多模态特征，以增强激光雷达特征的语义和相机特征的空间感知能力，并自适应地从两种模态中选择特征以构建统一的三维表示。具体来说，我们引入了预融合模块（Pre-Fusion），其中包括体素增强模块（VEM）和图像增强模块（IEM），VEM 用于增强二维相机特征对体素特征的语义，IEM 用于增强三维体素特征对相机特征的空间特性。VEM 和 IEM 都进行双向更新，以有效减少模态差异。然后，我们引入了统一融合模块（Unified Fusion），自适应地加权选择增强后的激光雷达和相机特征，以构建统一的三维表示。大量实验表明，我们的 BiCo-Fusion 优于现有技术。项目页面：https://t-ys.github.io/BiCo-Fusion/。||
|**2024-06-26**|[SpY: A Context-Based Approach to Spacecraft Component Detection](http://arxiv.org/abs/2406.18709)|null|本文重点研究如何使用相机馈送自主识别未知常驻空间物体 (RSO) 的组件（例如太阳能电池板、机身面板、天线和推进器），以辅助自主在轨服务 (OOS) 和主动清除碎片。在使用卷积神经网络 (CNN) 的情况下，该领域已经进行了大量研究。虽然 CNN 在学习模式和执行目标检测方面功能强大，但它们在与训练数据不同的环境中难以应对漏检和错误分类，这使得它们在 OOS 等高风险任务中无法保证安全可靠性。此外，CNN 所表现出的故障通常很容易通过人类使用常识推理和上下文知识来纠正。在目标检测器中嵌入此类推理可以提高检测精度。为了验证这一假设，本文提出了一种名为 SpaceYOLOv2 (SpY) 的端到端目标检测器，它利用了 CNN 的泛化能力，同时使用传统计算机视觉技术结合了上下文知识。SpY 由两个主要组件组成：形状检测器和 SpaceYOLO 分类器 (SYC)。形状检测器使用 CNN 来检测 RSO 的原始形状，SYC 将这些形状与上下文知识（例如颜色和纹理）相关联，以将它们分类为航天器组件，如果检测到的形状不确定，则分类为“未知”。SpY 的模块化架构允许自定义使用上下文知识来提高检测性能，或者将 SYC 作为具有现有航天器组件检测器的二级故障安全分类器。对模拟航天器的硬件在环图像进行的性能评估表明，SpY 准确可靠，并且 SpY 与为卫星组件检测而训练的 YOLOv5 的集合将召回率提高了 23.4%，这表明基于视觉的导航任务的安全性得到了增强。||
|**2024-06-25**|[MDHA: Multi-Scale Deformable Transformer with Hybrid Anchors for Multi-View 3D Object Detection](http://arxiv.org/abs/2406.17654)|**[link](https://github.com/naomiex/mdha)**|多视图 3D 对象检测是自动驾驶系统的关键组成部分。当代基于查询的方法主要依赖于特定于数据集的 3D anchors 初始化，这会引入偏差，或者利用密集注意力机制，这种机制计算效率低下且不可扩展。为了克服这些问题，我们提出了 MDHA，一种新颖的基于稀疏查询的框架，它使用来自多视图、多尺度输入的混合 anchors 构建自适应 3D 输出建议。将固定的 2D anchors 与深度预测相结合形成 2.5D anchors，并将其投影以获得 3D 建议。为了确保高效率，我们提出的 Anchor Encoder 执行稀疏细化并选择 top-k anchors 和特征。此外，虽然现有的多视图注意力机制依赖于将参考点投影到多个图像，但我们新颖的循环可变形注意力机制仅投影到单个图像，但允许参考点无缝地关注相邻图像，从而在不影响性能的情况下提高效率。在 nuScenes 验证集上，使用 ResNet101 骨干网络实现了 46.4% 的 mAP 和 55.0% 的 NDS。MDHA 明显优于基线，其中 anchor 建议被建模为可学习的嵌入。||
|**2024-06-25**|[Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP](http://arxiv.org/abs/2406.17639)|null|对比语言-图像预训练 (CLIP) 在零样本分类和跨模态视觉语言任务中表现出了显著的改进。然而，从几何角度来看，CLIP 嵌入空间存在明显的模态差距。这种差距使得嵌入空间过于稀疏和不连贯，不同的模态密集分布在超球体的不同子区域。在这项工作中，我们旨在回答两个主要问题：1. 在多模态编码器之间共享参数空间是否会减少模态差距？2. 能否通过模态内分离来推开单模态嵌入来弥合差距？我们设计了 AlignCLIP 来回答这些问题，并证明这两个问题的答案都是肯定的。通过大量的实验，我们证明 AlignCLIP 在嵌入的跨模态对齐方面取得了显著的增强，从而减少了模态差距，同时在零样本图像分类、零样本多模态检索和零样本语义文本相似度等多个下游评估中保持了性能。||
|**2024-06-25**|[Embedded event based object detection with spiking neural network](http://arxiv.org/abs/2406.17617)|null|基于事件的目标检测 (OD) 的复杂性带来了相当大的挑战。脉冲神经网络 (SNN) 显示出良好的结果，并为高效的基于事件的目标检测铺平了道路。尽管取得了这一成功，但在嵌入式设备上实现高效 SNN 的道路仍然面临挑战。这是由于完成任务所需的网络规模以及设备利用 SNN 优势的能力所致。即使考虑到“边缘”设备，它们通常也使用消耗数十瓦功率的嵌入式 GPU。为了应对这些挑战，我们的研究引入了嵌入式神经形态测试平台，该平台利用了脉冲低功耗基于事件的架构 (SPLEAT) 加速器。使用 Qualia 框架的扩展版本，我们可以在 SPLEAT 的 FPGA 实现上训练、评估、量化和部署脉冲神经网络。我们使用此测试平台加载了最先进的 SNN 解决方案，估计了在专用硬件上部署网络相关的性能损失，并在专门为低功耗脉冲神经网络设计的神经形态硬件上运行了现实世界的基于事件的目标检测。值得注意的是，我们的嵌入式脉冲解决方案（包括具有 108 万个参数的模型）以每个预测 490 毫焦耳的能耗高效运行。||
|**2024-06-25**|[TSynD: Targeted Synthetic Data Generation for Enhanced Medical Image Classification](http://arxiv.org/abs/2406.17473)|null|由于医学图像数据的稀缺性以及数据标注生成成本高昂（通常需要医学专业人员参与），将医学图像数据用于训练大规模机器学习方法尤其具有挑战性。生成模型的快速发展使得我们能够利用大量逼真的合成数据进行训练，从而解决这个问题。然而，随机选择合成样本可能并非最佳策略。在这项工作中，我们研究了针对性生成合成训练数据，以提高图像分类的准确性和鲁棒性。因此，我们的方法旨在引导生成模型合成具有高认知不确定性的数据，因为大量的认知不确定性表明训练集中存在代表性不足的数据点。在图像生成过程中，我们将自动编码器重建的图像输入分类器，并计算类概率分布的互信息作为不确定性的度量。我们通过优化过程改变自动编码器的特征空间，目标是最大化解码图像上的分类器不确定性。通过对这些数据进行训练，我们提高了针对测试时数据增强和对抗性攻击在多个分类任务上的性能和鲁棒性。||
|**2024-06-25**|[Dynamic Scheduling for Vehicle-to-Vehicle Communications Enhanced Federated Learning](http://arxiv.org/abs/2406.17470)|null|利用车辆的计算和感知能力，车联网联邦学习（VFL）已被应用于联网车辆的边缘训练。车联网的动态互联特性为利用直接的车对车 (V2V) 通信提供了独特的机会，从而提高了 VFL 训练效率。在本文中，我们考虑车辆的能量限制和移动性，制定了一个随机优化问题来优化 VFL 训练性能，并提出了一种 V2V 增强的动态调度 (VEDS) 算法来解决该问题。VFL 的模型聚合需求和由于移动性导致的有限传输时间导致了逐步目标函数，这对解决问题提出了挑战。因此，我们提出了一种基于导数的漂移加惩罚方法，将长期随机优化问题转换为在线混合整数非线性规划 (MINLP) 问题，并提供了理论分析来约束在线解和离线最优解之间的性能差距。对调度优先级的进一步分析将原始问题简化为一组凸优化问题，这些问题可以使用内点法有效解决。实验结果表明，与最先进的基准相比，所提出的算法在 CIFAR-10 数据集上的图像分类精度提高了 3.18%，在 Argoverse 轨迹预测数据集上的平均位移误差降低了 10.21%。||
|**2024-06-25**|[Implicit-Zoo: A Large-Scale Dataset of Neural Implicit Functions for 2D Images and 3D Scenes](http://arxiv.org/abs/2406.17438)|**[link](https://github.com/qimaqi/Implicit-Zoo)**|神经隐函数在计算机视觉、图形学等多个领域都具有重要意义。它们的优势包括能够高保真地表示复杂的形状和场景、平滑的插值能力以及连续表示。尽管有这些优点，但由于缺乏全面的数据集以及实现和评估需要大量的计算资源，隐函数的开发和分析受到了限制。为了应对这些挑战，我们推出了“Implicit-Zoo”：一个需要数千个GPU训练天数的大规模数据集，旨在促进该领域的研究和发展。我们的数据集包括各种2D和3D场景，例如用于2D图像任务的CIFAR-10、ImageNet-1K和Cityscapes，以及用于3D视觉任务的OmniObject3D数据集。我们通过严格的检查来确保高质量，优化或过滤掉低质量的数据。使用Implicit-Zoo，我们展示了两个直接的好处：(1)学习transformer模型的token位置；(2)直接回归2D图像相对于NeRF模型的3D相机姿态。这反过来又提高了图像分类、语义分割和3D姿态回归这三项任务的性能，从而为研究开辟了新的途径。||
|**2024-06-25**|[Robustly Optimized Deep Feature Decoupling Network for Fatty Liver Diseases Detection](http://arxiv.org/abs/2406.17338)|**[link](https://github.com/hp-ml/miccai2024)**|当前医学图像分类工作主要目标是更高的平均性能，往往忽略了不同类别之间的平衡。这可能导致类别之间识别精度存在显著差异，并存在明显的识别弱点。在缺乏海量数据支持的情况下，深度学习在脂肪肝的细粒度分类方面面临挑战。本文提出了一种结合特征解耦和自适应对抗训练的创新深度学习框架。首先，我们采用两个迭代压缩的解耦器，对腹部超声图像中与脂肪肝相关的共同特征和特定特征进行监督式解耦。随后，将解耦后的特征与颜色空间转换后的原始图像进行拼接，并输入分类器。在对抗训练过程中，我们根据每个类别的精度自适应地调整扰动，平衡对抗强度。该模型将通过正确分类对抗样本消除识别弱点，从而提高识别鲁棒性。最终，我们的方法将准确率提高了4.16%，达到了82.95%。大量实验表明，我们的方法是一个通用的学习框架，可以直接用于消除任何分类器的识别弱点，同时提高其平均性能。代码可在https://github.com/HP-ML/MICCAI2024获取。||
|**2024-06-25**|[Towards Open-set Camera 3D Object Detection](http://arxiv.org/abs/2406.17297)|null|传统的相机3D物体检测器通常被训练用于识别一组预定义的已知物体类别。在现实场景中，这些检测器可能会遇到训练类别之外的未知物体，并且无法正确识别它们。为了解决这一问题，我们提出了OS-Det3D（开放集相机3D物体检测），这是一个两阶段训练框架，增强了相机3D检测器识别已知和未知物体能力。该框架包含我们提出的3D物体发现网络（ODN3D），该网络经过专门训练，使用几何线索（例如3D边界框的位置和大小）来发现通用的3D物体。ODN3D以类别无关的方式进行训练，并且提供的3D物体区域建议本身就带有数据噪声。为了提高识别未知物体的准确性，我们引入了联合对象性选择（JOS）模块。JOS通过结合ODN3D对象性和相机特征注意力对象性，从ODN3D的3D物体区域建议中选择未知物体的伪ground truth。在nuScenes和KITTI数据集上的实验结果表明，我们的框架能够有效地使相机3D检测器成功识别未知物体，同时提高其对已知物体的识别性能。||
|**2024-06-24**|[Diff3Dformer: Leveraging Slice Sequence Diffusion for Enhanced 3D CT Classification with Transformer Networks](http://arxiv.org/abs/2406.17173)|null|肺部疾病相关症状的表现因患者而异，这凸显了CT扫描中3D信息对医学图像分类的重要性。虽然视觉Transformer在图像分类任务中表现出优于卷积神经网络的性能，但它们的有效性通常在足够大的2D数据集上得到证明，并且在小型医学图像数据集上容易遇到过拟合问题。为了解决这一限制，我们提出了一种基于扩散的3D视觉Transformer（Diff3Dformer），它利用扩散模型的潜在空间来形成用于3D分析的切片序列，并将聚类注意力机制融入ViT中，以聚合3D CT扫描内的重复信息，从而在小型数据集的3D分类任务中利用先进Transformer的强大功能。我们的方法在两个不同规模的小型3D肺部CT扫描数据集上都表现出改进的性能，超过了最先进的3D方法和其他在COVID-19大流行期间出现的基于Transformer的方法，证明了其在不同数据规模上的稳健性和优越性能。实验结果强调了我们提出的方法的优越性，表明了其在现实场景中增强医学图像分类任务的潜力。||
|**2024-06-24**|[Speeding Up Image Classifiers with Little Companions](http://arxiv.org/abs/2406.17117)|null|扩大神经网络规模一直是大型语言和视觉模型取得成功的关键因素。然而，在实践中，扩大规模后的模型在计算成本方面可能不成比例地高，而性能却只有微不足道的提高；例如，EfficientViT-L3-384 在 ImageNet-1K 上的准确率比基础 L1-224 模型提高了不到 2%，而所需的乘积累加运算 (MAC) 却增加了 14 倍。在本文中，我们研究了流行的图像分类神经网络家族的缩放特性，发现扩大规模后的模型主要有助于处理“困难”样本。我们根据难度对样本进行分解，开发了一种简单的模型无关的两阶段 Little-Big 算法，该算法首先使用轻量级的“小”模型对所有样本进行预测，并且只将困难样本传递给“大”模型来解决。优秀的“小”模型可以显著降低各种模型家族和规模的 MAC。在不损失准确率或修改现有模型的情况下，我们的小大模型在 ImageNet-1K 上实现了 EfficientViT-L3-384 的 MAC 减少 76%，EfficientNet-B7-600 的 MAC 减少 81%，DeiT3-L-384 的 MAC 减少 71%。Little-Big 还将 InternImage-G-512 模型的速度提高了 62%，同时实现了 90% 的 ImageNet-1K top-1 准确率，既可以作为强大的基线，也可以作为一种简单实用的大型模型压缩方法。||
|**2024-06-21**|[DiffExplainer: Unveiling Black Box Models Via Counterfactual Generation](http://arxiv.org/abs/2406.15182)|null|在医学影像领域，特别是在与疾病早期检测和预后相关的任务中，理解AI模型预测背后的推理过程对于评估其可靠性至关重要。传统的解释方法在识别医学图像分类中的决定性特征方面面临挑战，尤其是在判别特征微妙或不明显的情况下。为了解决这一局限性，我们提出了一种能够生成反事实图像的代理模型，当将其输入黑盒模型时，可以促使模型做出不同的决策。通过使用这种代理模型，我们可以发现影响黑盒模型最终预测的影响图像模式。通过我们的方法，我们可以有效地识别影响深度黑盒模型决策的特征。我们在医学预后任务的严格领域中验证了我们的方法，展示了其有效性和与现有解释方法相比在提高医学图像分类中深度学习模型可靠性方面的潜力。代码将公开发布在https://github.com/ayanglab/DiffExplainer。||
|**2024-06-21**|[This actually looks like that: Proto-BagNets for local and global interpretability-by-design](http://arxiv.org/abs/2406.15168)|**[link](https://github.com/kdjoumessi/proto-bagnets)**|可解释性是在医疗诊断等高风险应用中使用机器学习模型的关键要求。解释黑盒模型主要依赖于无法忠实反映模型行为的事后方法。作为一种补救措施，已经提出了基于原型的网络，但由于它们已被证明只能提供粗略、不可靠和不精确的解释，因此其可解释性受到限制。在这项工作中，我们介绍了 Proto-BagNets，这是一种可解释的基于原型的模型，它结合了局部特征词袋模型和原型学习的优点，可以提供有意义、连贯和相关的原型部分，这些部分对于准确和可解释的图像分类任务是必需的。我们根据公开可用的视网膜 OCT 数据评估了 Proto-BagNet 用于玻璃疣检测的性能。Proto-BagNet 的性能与最先进的可解释和不可解释模型相当，同时提供了忠实、准确和具有临床意义的局部和全局解释。代码可在 https://github.com/kdjoumessi/Proto-BagNets 获取。||
|**2024-06-21**|[A Generative Machine Learning Approach for Improving Precipitation from Earth System Models](http://arxiv.org/abs/2406.15026)|null|量化人为全球变暖的影响需要精确的地球系统模型 (ESM) 模拟。统计偏差校正和降尺度可用于减少误差并提高 ESM 的分辨率。然而，现有的方法（如分位数映射）不能有效地改善空间格局或时间动态。我们通过结合非配对域转换和超分辨率基础模型的纯生成机器学习方法来解决这个问题。我们的结果表明，在处理后的 ESM 模拟中，空间格局和时间动态更加真实，分布偏差也减少了。||
|**2024-06-21**|[LU2Net: A Lightweight Network for Real-time Underwater Image Enhancement](http://arxiv.org/abs/2406.14973)|null|计算机视觉技术已经赋予了水下机器人有效执行多种任务的能力，包括目标跟踪和路径规划。然而，水下的光学因素，如光的折射和吸收，对水下视觉提出了挑战，导致水下图像质量下降。为了提高水下视觉感知的有效性，人们提出了各种水下图像增强方法。然而，对于水下机器人的实时视觉任务，需要克服与算法效率和实时能力相关的挑战。在本文中，我们介绍了轻量级水下 Unet（LU2Net），这是一种专门为实时增强水下图像而设计的新型 U 形网络。该模型结合了轴向深度卷积和通道注意力模块，能够显著减少计算量和模型参数，从而提高处理速度。在数据集和真实世界水下机器人上进行的大量实验表明，该模型具有优异的性能和速度。它能够以比当前最先进的水下图像增强方法快 8 倍的速度提供高质量的增强水下图像。此外，LU2Net 还能够处理实时水下视频增强。||
|**2024-06-21**|[DiPEx: Dispersing Prompt Expansion for Class-Agnostic Object Detection](http://arxiv.org/abs/2406.14924)|null|类无关目标检测 (OD) 可以是许多下游视觉任务的基石或瓶颈。尽管自底向上和多目标发现方法利用基本视觉线索识别显著目标方面取得了相当大的进步，但由于目标类型的多样性及其上下文复杂性，始终保持较高的召回率仍然很困难。在这项工作中，我们研究了使用视觉语言模型 (VLM) 通过自监督提示学习策略来增强目标检测。我们最初的发现表明，手动构建的文本查询通常会导致目标未被检测到，主要是因为当查询词表现出语义重叠时，检测置信度会降低。为了解决这个问题，我们提出了一种分散提示扩展 (DiPEx) 方法。DiPEx 逐步学习扩展一组不同的、非重叠的超球面提示以提高召回率，从而提高下游任务（如分布外 OD）的性能。具体来说，DiPEx 通过自训练通用父提示并选择语义不确定性最高的提示以进行进一步扩展来启动该过程。生成的子提示预计将从其父提示继承语义，同时捕获更细粒度的语义。我们应用分散损失来确保子提示之间的高类间差异，同时保持父子提示对之间的语义一致性。为了防止提示集过度增长，我们利用语义空间的最大角覆盖 (MAC) 作为提前终止的标准。我们通过 MS-COCO 和 LVIS 上广泛的类无关 OD 和 OOD-OD 实验证明了 DiPEx 的有效性，在 AR 中比其他提示方法高出 20.1%，并在 AP 上比 SAM 提高了 21.3%。代码可在 https://github.com/jason-lim26/DiPEx 获取。||
|**2024-06-21**|[Demonstrating the Efficacy of Kolmogorov-Arnold Networks in Vision Tasks](http://arxiv.org/abs/2406.14916)|**[link](https://github.com/jmj2316/KAN-in-VIsion)**|在深度学习领域，Kolmogorov-Arnold 网络 (KAN) 已成为多层投影 (MLP) 的潜在替代方案。然而，其对视觉任务的适用性尚未得到广泛验证。在我们的研究中，我们通过在 MNIST、CIFAR10 和 CIFAR100 数据集上使用 32 的训练批次大小进行多次试验，证明了 KAN 对视觉任务的有效性。我们的结果表明，虽然 KAN 在 CIFAR10 和 CIFAR100 上的表现优于原始 MLP-Mixer，但在性能上略逊于最先进的 ResNet-18。这些发现表明，KAN 在视觉任务中具有很大的潜力，进一步的改进可以提高其在未来评估中的性能。我们的贡献有三方面：首先，我们展示了基于 KAN 的算法在视觉任务中的效率；其次，我们提供了跨各种视觉基准的广泛实证评估，比较了 KAN 与 MLP-Mixer、CNN 和视觉Transformer (ViT) 的性能；第三，我们开创了在视觉任务中使用自然 KAN 层的先河，解决了先前研究中的空白。本文为未来关于 KAN 的研究奠定了基础，突出了其作为图像分类任务可靠替代方案的潜力。||
|**2024-06-21**|[MOS: Model Synergy for Test-Time Adaptation on LiDAR-Based 3D Object Detection](http://arxiv.org/abs/2406.14878)|null|基于激光雷达的3D目标检测在许多应用中至关重要，但此类检测系统的性能在部署后往往会下降，特别是当面对来自不同位置或遭受损坏的未知测试点云时。在这项工作中，我们介绍了一种新的在线检测器适应框架，称为模型协同（MOS）。具体来说，MOS利用长期知识来指导模型更新而不会忘记，从历史检查点库中为每个测试批次动态组装最合适的超级模型。模型组装由所提出的协同权重（SW）指导，用于所选检查点的加权平均，以最大程度地减少复合超级模型中的冗余。这些权重是通过评估测试数据上预测边界框的相似性和模型库中模型对之间的特征独立性来计算的。为了维护信息丰富且紧凑的模型库，我们弹出平均SW得分最低的检查点，并插入新更新的模型权重。我们的方法在三个数据集和八种类型的损坏情况下，针对先前的测试时域自适应策略进行了严格测试，证明了其对不断变化的场景和条件的出色适应性。值得注意的是，我们的方法在涉及跨数据集不一致和现实世界场景损坏的复杂“交叉损坏”场景中实现了67.3%的性能提升，提供了一个更真实的适应性测试平台。代码可在https://github.com/zhuoxiao-chen/MOS获取。||
|**2024-06-20**|[Visible-Thermal Tiny Object Detection: A Benchmark Dataset and Baselines](http://arxiv.org/abs/2406.14482)|**[link](https://github.com/xinyiying24/rgbt-tiny)**|小目标检测（SOD）几十年来一直是一项长期存在的挑战性任务，已经开发了许多数据集和算法。然而，它们主要集中在可见光或热成像模态，而很少探索可见光-热成像（RGBT）双模态。尽管最近开发了一些RGBT数据集，但数量不足、类别有限、图像未对齐以及目标尺寸过大，这些问题导致无法提供一个公正的基准来评估多类别可见光-热成像小目标检测（RGBT SOD）算法。在本文中，我们为RGBT SOD构建了第一个具有高度多样性的大规模基准（即RGBT-Tiny），包括115个配对序列、93K帧和120万个手动标注。RGBT-Tiny包含丰富的目标（7个类别）和高度多样化的场景（8种类型，涵盖不同的光照和密度变化）。值得注意的是，超过81%的目标小于16x16像素，并且我们提供了带有跟踪ID的配对边界框标注，以提供一个极具挑战性的基准，适用于广泛的应用，例如RGBT融合、检测和跟踪。此外，我们提出了一种尺度自适应适应度（SAFit）度量，该度量在小目标和大目标上均表现出较高的鲁棒性。所提出的SAFit可以提供合理的性能评估并提高检测性能。基于所提出的RGBT-Tiny数据集和SAFit度量，我们对23种最新的算法进行了广泛的评估，涵盖四种不同类型（即可见光通用目标检测、可见光SOD、热成像SOD和RGBT目标检测）。项目地址：https://github.com/XinyiYing24/RGBT-Tiny。||
|**2024-06-20**|[Enhanced Bank Check Security: Introducing a Novel Dataset and Transformer-Based Approach for Detection and Verification](http://arxiv.org/abs/2406.14370)|**[link](https://github.com/saifkhichi96/ssbi-dataset)**|银行支票的自动签名验证对于防止欺诈和确保交易真实性至关重要。由于现实世界文档中签名与其他文本和图形元素共存，这项任务具有挑战性。验证系统必须首先检测签名，然后验证其真实性，这是当前数据集和方法经常忽略的双重挑战，它们只关注验证。为了解决这一差距，我们引入了一个专门为银行支票签名验证设计的新数据集。该数据集包含嵌入在典型支票元素中的各种签名样式，为高级检测方法提供了真实的测试平台。此外，我们提出了一种使用目标检测网络进行独立于书写者的签名验证的新方法。我们基于检测的验证方法将真实和伪造的签名视为目标检测框架内的不同类别，有效地处理了检测和验证。我们采用基于 DINO 的网络，并增加了扩张模块，以同时检测和验证支票图像上的签名。我们的方法实现了 99.2 的真实签名 AP 和 99.4 的伪造签名 AP，相较于 DINO 基线（分别为 93.1 和 89.3）有显著改进。这种改进突出了我们的扩张模块在减少误报和误报方面的有效性。我们的结果证明了基于检测的签名验证技术的实质性进步，为金融文档处理提供了更高的安全性和效率。||
|**2024-06-20**|[HoTPP Benchmark: Are We Good at the Long Horizon Events Forecasting?](http://arxiv.org/abs/2406.14341)|**[link](https://github.com/ivan-chai/hotpp-benchmark)**|在金融、零售、社交网络和医疗保健等领域都有广泛应用的序列事件预测中，一项关键任务是在特定的时间范围内预测多个未来事件。传统上，这通过使用诸如标记时间点过程等下一事件预测模型的自回归生成来解决。然而，自回归方法使用自身的输出来进行未来的预测，随着预测范围的扩大，可能会降低质量。在本文中，我们挑战了传统方法，引入了一个名为 HoTPP 的全新基准测试，专门用于评估模型在预测一段时间内的事件序列方面的能力。该基准测试采用了一种受计算机视觉目标检测启发的新指标，解决了现有指标在评估时间步长预测不精确的模型方面的局限性。我们对使用各种模型的已建立数据集的评估表明，下一事件预测的高精度不一定转化为卓越的范围预测，反之亦然。HoTPP 旨在作为开发更稳健的事件序列预测方法的宝贵工具，最终为该领域的进一步发展铺平道路。||
|**2024-06-20**|[Adaptive Adversarial Cross-Entropy Loss for Sharpness-Aware Minimization](http://arxiv.org/abs/2406.14329)|null|近期的学习算法研究表明，损失曲面的锐度是改善泛化能力的有效指标。基于此概念，人们提出了锐度感知最小化（SAM）算法来增强模型泛化能力，并取得了最先进的结果。SAM 包含两个主要步骤：权重扰动步骤和权重更新步骤。然而，SAM 中的扰动仅由训练损失或交叉熵损失的梯度决定。当模型接近驻点时，该梯度会变得很小并发生震荡，导致扰动方向不一致，并且还有可能导致梯度消失。我们的研究引入了一种创新方法来进一步增强模型泛化能力。我们提出了自适应对抗交叉熵（AACE）损失函数来代替 SAM 扰动中的标准交叉熵损失。AACE 损失及其梯度在模型接近收敛时会独特地增加，确保扰动方向一致，并解决了梯度消失问题。此外，我们还提出了一种利用未经归一化的 AACE 损失生成扰动的新型函数，增强了模型在接近最优阶段的探索能力。实证测试证实了 AACE 的有效性，实验表明，使用 Wide ResNet 和 PyramidNet 在各种数据集上的图像分类任务中，AACE 的性能均有所提高。代码可在网上获取。||
|**2024-06-20**|[Zero-Shot Image Denoising for High-Resolution Electron Microscopy](http://arxiv.org/abs/2406.14264)|**[link](https://github.com/meijitian/zs-denoisier-hrem)**|高分辨率电子显微镜 (HREM) 成像技术是一种强大的工具，可以在真实空间中直接可视化各种材料。然而，由于超低的信噪比 (SNR) 和有限的数据可用性，它在去噪方面面临着挑战。在这项工作中，我们提出了 Noise2SR，一个用于 HREM 的零样本自监督学习 (ZS-SSL) 去噪框架。在我们的框架内，我们提出了一种基于超分辨率 (SR) 的自监督训练策略，并结合了随机子采样器模块。随机子采样器旨在从单个噪声图像生成近似无限的噪声对，作为零样本去噪中的有效数据增强。Noise2SR 使用不同分辨率的成对噪声图像训练网络，这是通过 SR 策略进行的。基于 SR 的训练有助于网络采用更多像素进行监督，随机子采样有助于迫使网络学习连续信号，从而增强鲁棒性。同时，我们通过对去噪结果采用最小均方误差 (MMSE) 估计来减轻随机采样引起的不确定性。通过训练策略和提出的设计的独特集成，Noise2SR 可以使用单个噪声 HREM 图像实现卓越的去噪性能。我们在模拟和真实 HREM 去噪任务中评估了 Noise2SR 的性能。它优于最先进的 ZS-SSL 方法，并实现了与监督方法相当的去噪性能。Noise2SR 的成功表明它在提高材料成像领域图像的 SNR 方面具有潜力。||
|**2024-06-20**|[LeYOLO, New Scalable and Efficient CNN Architecture for Object Detection](http://arxiv.org/abs/2406.14239)|**[link](https://github.com/LilianHollard/LeYOLO)**|深度神经网络中的计算效率对于目标检测至关重要，尤其是当较新的模型优先考虑速度而非高效计算（FLOP）时。这种演变在某种程度上使得面向嵌入式和移动设备的AI目标检测应用落后。在本文中，我们关注于基于FLOP的高效目标检测计算的神经网络架构设计选择，并提出了一些优化措施来提高基于YOLO的模型的效率。首先，我们介绍了一种受倒置瓶颈和信息瓶颈原理的理论见解启发的有效骨干网络缩放方法。其次，我们提出了快速金字塔架构网络（FPAN），旨在促进快速的多尺度特征共享，同时减少计算资源。最后，我们提出了一个解耦网络（DNiN）检测头，该检测头经过精心设计，可以为分类和回归任务提供快速而轻量级的计算。在这些优化的基础上，并利用更高效的骨干网络，本文为目标检测和以YOLO为中心的模型贡献了一种新的缩放范式，称为LeYOLO。我们的贡献在各种资源限制下始终优于现有模型，实现了前所未有的精度和flop比率。值得注意的是，LeYOLO-Small在COCO数据集上仅用4.5 FLOP(G)就实现了38.2%的mAP得分，与最新的YOLOv9-Tiny模型相比，计算量减少了42%，同时实现了相似的精度。我们的新型模型系列实现了前所未有的FLOP-精度比率，提供了从超低神经网络配置（< 1 GFLOP）到高效但要求苛刻的目标检测设置（> 4 GFLOPs）的可扩展性，在0.66、1.47、2.53、4.51、5.8和8.4 FLOP(G)时，mAP分别为25.2、31.3、35.2、38.2、39.3和41。||
|**2024-06-20**|[Seg-LSTM: Performance of xLSTM for Semantic Segmentation of Remotely Sensed Images](http://arxiv.org/abs/2406.14086)|**[link](https://github.com/zhuqinfeng1999/seg-lstm)**|近年来，线性复杂度的自回归网络取得了重大进展，并在大型语言模型中展现出卓越的性能。扩展长短期记忆网络（xLSTM）就是一个典型的例子，它结合了门控机制和记忆结构，在长序列语言任务中的表现可与Transformer架构相媲美。像xLSTM这样的自回归网络可以利用图像序列化将其应用扩展到视觉任务，如分类和分割。尽管现有研究表明Vision-LSTM在图像分类方面取得了令人印象深刻的结果，但其在图像语义分割方面的性能仍未得到验证。我们的研究首次尝试评估Vision-LSTM在遥感图像语义分割中的有效性。此评估基于一个专门设计的编码器-解码器架构，名为Seg-LSTM，并与最先进的分割网络进行了比较。我们的研究发现，Vision-LSTM在语义分割方面的性能有限，并且在大多数比较测试中普遍逊于基于Vision-Transformers和Vision-Mamba的模型。我们还提出了未来增强Vision-LSTM的研究方向。源代码可在https://github.com/zhuqinfeng1999/Seg-LSTM获取。||
|**2024-06-20**|[SSAD: Self-supervised Auxiliary Detection Framework for Panoramic X-ray based Dental Disease Diagnosis](http://arxiv.org/abs/2406.13963)|**[link](https://github.com/dylonsword/ssad)**|全景X射线是一种简单有效的临床牙科疾病诊断工具。在开发深度学习模型以辅助牙医解读全景X射线图像时，大多数模型的性能受到有限的标注数据的限制，因为标注需要牙医的专业知识和大量的时间成本。尽管已经提出了自监督学习（SSL）来应对这一挑战，但预训练和微调的两阶段过程需要更多的训练时间和计算资源。在本文中，我们提出了一种自监督辅助检测（SSAD）框架，该框架即插即用，并与任何检测器兼容。它由重建分支和检测分支组成。两个分支同时训练，共享相同的编码器，无需微调。重建分支学习恢复健康或患病牙齿的牙齿纹理，而检测分支利用这些学习到的特征进行诊断。为了增强编码器捕获细粒度特征的能力，我们结合了SAM的图像编码器来构建纹理一致性（TC）损失，该损失从重建分支的输入和输出中提取图像嵌入，然后将两个嵌入强制到相同的特征空间中。在公共DENTEX数据集上通过三个检测任务进行的大量实验表明，与主流的目标检测方法和SSL方法相比，所提出的SSAD框架实现了最先进的性能。代码可在https://github.com/Dylonsword/SSAD获取。||
|**2024-06-20**|[Towards the in-situ Trunk Identification and Length Measurement of Sea Cucumbers via Bézier Curve Modelling](http://arxiv.org/abs/2406.13951)|**[link](https://github.com/OUCVisionGroup/TISC-Net)**|我们提出了一种基于视觉的新型海参原位躯干识别和长度测量框架，这在海洋牧场资源监测和机械化收获中起着至关重要的作用。为了对具有不同弯曲程度的海参躯干曲线进行建模，我们利用了参数化贝塞尔曲线，因为它计算简单、稳定且具有广泛的变换可能性。然后，我们提出了一个端到端的统一框架，将参数化贝塞尔曲线建模与广泛使用的You-Only-Look-Once (YOLO) 流程（缩写为TISC-Net）相结合，并结合了有效的漏斗激活和高效的多尺度注意力模块，以增强曲线特征的感知和学习。此外，我们建议将躯干端点损失作为额外的约束条件，以有效减轻端点偏差对整体曲线的影响。最后，利用双目相机捕获的沿躯干曲线分布的像素深度信息，我们提出了通过空间曲线积分准确估计海参原位长度的方法。我们为基于曲线的原位海参躯干识别建立了两个具有挑战性的基准数据集。这些数据集包含1000多张真实海洋环境中的海参图像，以及贝塞尔格式的标注。我们在SC-ISTI数据集上进行了评估，我们的方法在目标检测和躯干识别任务中都实现了高于0.9的mAP50。广泛的长度测量实验表明，平均绝对相对误差约为0.15。||
|**2024-06-20**|[Communication-Efficient Adaptive Batch Size Strategies for Distributed Local Gradient Methods](http://arxiv.org/abs/2406.13936)|null|由于现代深度神经网络规模庞大，它们通常需要多个工作节点进行分布式训练。随着工作节点数量的增加，每次迭代梯度同步的数据并行小批量随机梯度下降法中，通信开销成为主要瓶颈。局部梯度方法，如局部随机梯度下降（Local SGD），通过仅在若干个局部步骤后才进行同步来减少通信。尽管我们了解它们在独立同分布和异构设置下的收敛性，并且知道批量大小对效率和泛化性的重要性，但确定最佳局部批量大小仍然很困难。我们为局部梯度方法引入了自适应批量大小策略，该策略自适应地增加批量大小以减少小批量梯度方差。我们在同构数据条件下提供了收敛性保证，并通过图像分类实验支持我们的论点，证明了我们的策略在训练和泛化方面的有效性。||
|**2024-06-18**|[LayerMerge: Neural Network Depth Compression through Layer Pruning and Merging](http://arxiv.org/abs/2406.12837)|**[link](https://github.com/snu-mllab/layermerge)**|最近的研究表明，减少卷积神经网络中的层数可以在保持网络性能的同时提高效率。现有的深度压缩方法移除冗余的非线性激活函数，并将连续的卷积层合并为一层。然而，这些方法存在一个严重的缺陷：合并后的层的内核大小会变大，这大大削弱了减少网络深度所带来的延迟降低。我们发现，这个问题可以通过联合剪枝卷积层和激活函数来解决。为此，我们提出了LayerMerge，这是一种新颖的深度压缩方法，它选择要移除的激活层和卷积层，以在最小化性能损失的同时实现所需的推理速度提升。由于相应的选择问题涉及指数级的搜索空间，我们提出了一个新的代理优化问题，并通过动态规划有效地解决了它。实验结果表明，我们的方法在图像分类和生成任务的各种网络架构上始终优于现有的深度压缩和层剪枝方法。我们在https://github.com/snu-mllab/LayerMerge上发布了代码。||
|**2024-06-18**|[Privacy Preserving Federated Learning in Medical Imaging with Uncertainty Estimation](http://arxiv.org/abs/2406.12815)|**[link](https://github.com/niko-k98/awesome-list-federated-learning-review)**|机器学习 (ML) 和人工智能 (AI) 推动了显著的进步，特别是在医疗保健领域。在医学影像方面，机器学习模型有望改进疾病诊断、治疗计划和治疗后监测。各种计算机视觉任务，如图像分类、目标检测和图像分割，有望成为临床分析的常规手段。然而，围绕患者数据的隐私问题阻碍了构建开发和训练准确、稳健和可泛化模型所需的大型训练数据集。联邦学习 (FL) 作为一种引人注目的解决方案应运而生，它使各机构能够通过共享模型训练信息（梯度）而不是数据（例如医学图像）来协作进行机器学习模型训练。联邦学习的分布式学习框架促进了机构间的协作，同时保护了患者隐私。然而，联邦学习虽然在隐私保护方面很强大，但也面临着一些挑战。在模型训练过程中，从机构之间传递的共享梯度中仍然可以收集到敏感信息。此外，在医学影像中，由于数据中存在噪声和伪影，因此准确量化模型置信度/不确定性至关重要。由于各机构之间的数据异构性，联邦学习中的不确定性估计遇到了独特的障碍。本文全面回顾了联邦学习、隐私保护和不确定性估计，重点关注医学影像。除了对当前研究进行综述外，我们还指出了该领域的差距，并为联邦学习研究提出了未来方向，以增强隐私并应对嘈杂的医学影像数据挑战。||
|**2024-06-18**|[Online Anchor-based Training for Image Classification Tasks](http://arxiv.org/abs/2406.12662)|null|在本文中，我们致力于提升深度学习模型在图像分类任务上的性能，提出了一种名为“在线基于锚点的训练”（OAT）的新型基于锚点的训练方法。OAT 方法受到基于锚点的目标检测方法的启发，建议训练模型学习相对于定义锚点的类别标签的百分比变化，而不是直接学习类别标签。我们将模型输出处的批次中心定义为锚点。然后，在测试阶段，将预测转换回原始类别标签空间，并评估性能。OAT 方法的有效性在四个数据集上得到了验证。||
|**2024-06-18**|[Structured Detection for Simultaneous Super-Resolution and Optical Sectioning in Laser Scanning Microscopy](http://arxiv.org/abs/2406.12542)|**[link](https://github.com/vicidominilab/s2ism)**|快速且灵敏的探测器阵列实现了图像扫描显微镜 (ISM)，克服了共聚焦显微镜中空间分辨率和信噪比 (SNR) 之间的典型权衡问题。然而，目前的 ISM 方法无法提供光学切片，并且在处理厚样本时会失败，除非限制探测器的尺寸。因此，光学切片和 SNR 之间的另一个权衡仍然存在。在这里，我们提出了一种没有缺点的方法，它结合了不折不扣的超分辨率、高信噪比和光学切片。此外，我们的方法可以对图像进行超采样，将奈奎斯特准则放宽了两倍。基于这样的观察，即使用探测器阵列成像本身就嵌入了样本的轴向信息，我们设计了一种简单的重建算法，可以反转 ISM 的物理模型。我们提出了全面的理论框架，并使用配备单光子雪崩二极管 (SPAD) 阵列探测器的定制装置捕获的生物样本的合成和实验图像验证了我们的方法。我们证明了我们的方法在激发线性和非线性状态下的荧光发射方面的可行性。此外，我们将算法推广到荧光寿命成像，充分利用了 SPAD 阵列探测器的单光子计时能力。我们的方法优于传统的 ISM 方法，并且可以扩展到任何 LSM 技术。||
|**2024-06-18**|[ViDSOD-100: A New Dataset and a Baseline Model for RGB-D Video Salient Object Detection](http://arxiv.org/abs/2406.12536)|**[link](https://github.com/jhl-det/rgbd_video_sod)**|随着深度传感器的快速发展，越来越多的RGB-D视频得以获取。识别RGB-D视频中的前景是一项基础且重要的任务。然而，现有的显著性目标检测（SOD）工作只关注静态RGB-D图像或RGB视频，忽略了RGB-D和视频信息的协同作用。在本文中，我们首先收集了一个新的带注释的RGB-D视频SOD（ViDSOD-100）数据集，该数据集包含100个视频，共计9,362帧，这些视频采集自不同的自然场景。每个视频中的所有帧都经过人工标注，具有高质量的显著性标注。此外，我们提出了一个新的基线模型，名为注意力三重融合网络（ATF-Net），用于RGB-D视频显著性目标检测。我们的方法通过设计三个模态特定分支和一个多模态融合分支来聚合来自输入RGB图像的外观信息、来自估计运动图的时空信息以及来自深度图的几何信息。模态特定分支提取不同输入的表示，而多模态融合分支通过引入编码器特征聚合（MEA）模块和解码器特征聚合（MDA）模块来组合多级模态特定特征。在我们新引入的ViDSOD-100数据集和完善的DAVSOD数据集上进行的实验结果突出了所提出的ATF-Net的优越性能。这种性能的提升在定量和定性上都得到了证明，超过了当前最先进技术在各个领域的能力，包括RGB-D显著性检测、视频显著性检测和视频目标分割。我们的数据和代码可在github.com/jhl-Det/RGBD_Video_SOD获取。||
|**2024-06-18**|[LFMamba: Light Field Image Super-Resolution with State Space Model](http://arxiv.org/abs/2406.12463)|null|近年来，由于现代神经网络的进步，光场图像超分辨率（LFSR）取得了显著进展。然而，这些方法在捕获远程依赖关系（基于CNN）或遇到二次计算复杂度（基于Transformer）方面经常面临挑战，这限制了它们的性能。最近，具有选择性扫描机制（S6）的状态空间模型（SSM），以Mamba为例，已成为各种视觉任务中优于传统基于CNN和基于Transformer的方法的替代方案，这得益于其有效的远程序列建模能力和线性时间复杂度。因此，将S6集成到LFSR变得很有吸引力，特别是考虑到4D光场的巨大数据量。然而，主要挑战在于\emph{为4D光场设计一种有效的扫描方法，以有效地对光场特征进行建模}。为了解决这个问题，我们在4D LF的信息丰富的2D切片上采用SSM，以充分探索空间上下文信息、互补的角度信息和结构信息。为此，我们精心设计了一个基本的SSM块，其特征是高效的SS2D机制，有助于在这些2D切片上进行更有效和高效的特征学习。基于以上两种设计，我们进一步介绍了一种用于LFSR的基于SSM的网络，称为LFMamba。在LF基准数据集上的实验结果证明了LFMamba的优越性能。此外，还进行了广泛的消融研究，以验证我们提出的方法的有效性和泛化能力。我们希望我们的LFMamba能够为利用状态空间模型进行LF的有效表示学习提供启示。||
|**2024-06-18**|[SDNIA-YOLO: A Robust Object Detection Model for Extreme Weather Conditions](http://arxiv.org/abs/2406.12395)|null|虽然当前基于深度学习的目标检测模型在许多传统基准数据集上取得了优异的结果，但它们在极端条件下拍摄的真实世界图像上的性能会显著下降。现有方法要么使用基于传统图像处理算法的图像增强，要么应用定制的、场景受限的图像自适应技术来进行鲁棒建模。因此，本研究提出了一种风格化数据驱动的图像自适应神经网络YOLO（SDNIA-YOLO），它通过自适应地提高图像质量和从神经风格迁移（NST）合成的图像中学习与极端天气条件相关的有价值信息来提高模型的鲁棒性。实验表明，与基线模型相比，所开发的SDNIA-YOLOv3在真实世界雾天（RTTS）和低光（ExDark）测试集上的mAP@.5显著提高了至少15%。此外，实验还突出了风格化数据在模拟极端天气条件方面的巨大潜力。所开发的SDNIA-YOLO在很大程度上保留了原生YOLO的优良特性，如端到端的一阶段、数据驱动和快速。||
|**2024-06-18**|[Competitive Learning for Achieving Content-specific Filters in Video Coding for Machines](http://arxiv.org/abs/2406.12367)|null|本文研究了联合优化特定于内容的后处理滤波器以将面向人类的视频/图像编解码器调整为适用于机器视觉任务的编解码器的功效。通过观察视频/图像编解码器产生的伪像是依赖于内容的，我们提出了一种基于竞争学习原理的新型训练策略。该策略以模糊的方式将训练样本动态分配给滤波器，从而进一步优化给定样本上的获胜滤波器。受模拟退火优化技术的启发，我们采用具有温度变量的 softmax 函数作为权重分配函数，以减轻随机初始化的影响。我们在一个在多功能视频编码 (VVC) 编解码器框架内利用多个后处理滤波器的系统上进行的评估表明，使用我们提出的策略训练的特定于内容的滤波器的优越性，特别是在图像分块处理时。使用 VVC 参考软件 VTM 12.0 作为锚点，在 OpenImages 数据集上进行的实验表明，与独立训练的滤波器相比，用于目标检测和实例分割任务的 BD 率降低分别从 -41.3% 和 -44.6% 提高到 -42.3% 和 -44.7%。滤波器使用统计数据与我们的假设一致，并强调了联合优化滤波器的内容和重建质量的重要性。我们的发现为进一步提高视频/图像编解码器的性能铺平了道路。||
|**2024-06-18**|[Certified ML Object Detection for Surveillance Missions](http://arxiv.org/abs/2406.12362)|null|本文介绍了一种无人机探测系统的开发过程，该系统包含一个机器学习目标检测组件。目的是达到可接受的性能目标，并根据ED 324 / ARP 6983标准（即将发布）的建议提供充分的证据，以增强对所设计系统可靠性的信心。||
|**2024-06-18**|[Unleashing the Potential of Open-set Noisy Samples Against Label Noise for Medical Image Classification](http://arxiv.org/abs/2406.12293)|null|在医学图像分类中，解决混合闭集和开集标签噪声的挑战在很大程度上仍未得到探索。与自然图像分类不同，自然图像分类通常将闭集和开集噪声样本与干净样本分开并分别处理，而医学图像分类由于类间相似性高，难以识别开集噪声样本，因此面临着困难。此外，现有方法没有充分利用开集噪声样本的潜力来减轻标签噪声，通常导致它们被排除在外或应用统一的软标签。为了解决这些问题，我们提出了一种扩展的噪声鲁棒对比和开集特征增强（ENCOFA）框架。ENCOFA 包括扩展的噪声鲁棒监督对比（ENSC）损失，它有助于区分不同类别的特征。ENSC 损失将开集噪声样本视为一个扩展类别，并通过用标签可靠性对对比对进行加权来减轻标签噪声。此外，我们开发了一个开集特征增强（OSFeatAug）模块，利用模型的额外容量来丰富开集样本的特征，以防止对噪声数据的过拟合。我们在一个合成噪声数据集和一个真实世界的噪声数据集上进行了实验。我们的结果表明 ENCOFA 的优越性以及利用开集噪声样本对抗标签噪声的有效性。||
|**2024-06-17**|[Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%](http://arxiv.org/abs/2406.11837)|**[link](https://github.com/zh460045050/vqgan-lc)**|在以 VQGAN 为代表的图像量化领域，图像被编码成从预定义大小的码本中提取的离散token。近期的研究进展，特别是 LLAMA 3，表明扩大码本可以显著提高模型性能。然而，VQGAN 及其衍生模型，如 VQGAN-FC（因子化码本）和 VQGAN-EMA，在扩大码本大小和提高码本利用率方面仍然面临挑战。例如，VQGAN-FC 只能学习最大大小为 16,384 的码本，在 ImageNet 上的利用率通常低于 12%。本文提出了一种新的图像量化模型，称为 VQGAN-LC（大型码本），它将码本大小扩展到 100,000，实现了超过 99% 的利用率。与之前优化每个码本条目的方法不同，我们的方法首先使用预训练的视觉编码器提取的 100,000 个特征初始化码本。然后，优化集中于训练一个投影器，使整个码本与 VQGAN-LC 中编码器的特征分布对齐。我们证明了我们的模型在各种任务上的性能优于现有模型，包括图像重建、图像分类、使用 GPT 的自回归图像生成，以及使用基于扩散和基于流的生成模型进行图像创作。代码和模型可在 https://github.com/zh460045050/VQGAN-LC 获取。||
|**2024-06-17**|[V3Det Challenge 2024 on Vast Vocabulary and Open Vocabulary Object Detection: Methods and Results](http://arxiv.org/abs/2406.11739)|null|在现实场景中检测物体是一项复杂的任务，因为它面临着各种挑战，包括物体类别的广泛性以及可能遇到以前未知或未见过的物体。 这些挑战使得开发公共基准和挑战成为必要，以推动物体检测领域的发展。 受之前COCO和LVIS挑战赛成功的启发，我们与第四届开放世界视觉研讨会：开放世界中的视觉感知学习（VPLOW）合作，在2024年美国西雅图举行的CVPR上组织了V3Det挑战赛2024。这项挑战赛旨在推动物体检测研究的边界，并鼓励该领域的创新。V3Det挑战赛2024包括两个赛道：1）海量词汇物体检测：该赛道侧重于从13204个类别的庞大集合中检测物体，测试检测算法识别和定位不同物体能力。 2）开放词汇物体检测：该赛道更进一步，要求算法从开放的类别集中检测物体，包括未知物体。 在接下来的部分中，我们将对参与者提交的解决方案进行全面总结和分析。 通过分析提出的方法和解决方案，我们旨在启发海量词汇和开放词汇物体检测的未来研究方向，推动该领域的进步。 挑战赛主页：https://v3det.openxlab.org.cn/challenge||
|**2024-06-17**|[YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection](http://arxiv.org/abs/2406.11641)|null|目前基于图像的无人机检测方法主要依赖于YOLOv5等通用目标检测算法。虽然这些算法能够有效地在均匀背景下识别无人机，但在复杂、纹理丰富的环境中往往表现不佳。在这种情况下，无人机可以无缝地融入背景，产生伪装效果，从而对检测质量产生负面影响。为了解决这个问题，我们提出了一种名为YOLO-FEDER FusionNet的新型深度学习架构。与传统方法不同，YOLO-FEDER FusionNet将通用目标检测方法与伪装目标检测技术的专业优势相结合，以增强无人机检测能力。对YOLO-FEDER FusionNet的全面评估表明，该模型效率高，并在减少漏检和误报方面都有显著改进。||
|**2024-06-17**|[Cross-domain Open-world Discovery](http://arxiv.org/abs/2406.11422)|**[link](https://github.com/mlbio-epfl/crow)**|在许多实际应用中，测试数据通常会出现类别偏移，其特点是出现新的类别，以及由于特征分布与模型训练时的特征分布不同而产生的分布偏移。然而，现有方法要么在开放世界环境中发现新类别，要么假设存在领域偏移但无法发现新类别。在这项工作中，我们考虑了一个跨领域开放世界发现设置，目标是在领域偏移的情况下将样本分配给已见类别并发现未见类别。为了解决这一挑战性问题，我们提出了 CROW，这是一种基于原型的方法，它引入了由基础模型结构良好的表示空间支持的“先聚类后匹配”策略。通过这种方式，CROW 通过将聚类与先前见过的类别进行稳健匹配来发现新类别，然后使用专为跨领域开放世界发现设计的目标函数对表示空间进行微调。在图像分类基准数据集上的大量实验结果表明，CROW 优于其他基线方法，在 75 个实验设置中平均性能提高了 8%。||
|**2024-06-17**|[A Dictionary Based Approach for Removing Out-of-Focus Blur](http://arxiv.org/abs/2406.11330)|**[link](https://github.com/aurangau/ICIP2024)**|随着深度学习模型的兴起，图像去模糊领域取得了巨大进步。这些模型虽然高效，但计算成本高，能源消耗大。基于字典学习的方法在图像去噪和单图像超分辨率方面已显示出良好的结果。我们提出将 Isidoro、Romano 和 Milanfar 提出的快速准确图像超分辨率 (RAISR) 算法扩展到散焦模糊去除任务。我们定义了一种清晰度质量度量，它与图像的感知质量非常吻合。还提出了一种基于资产配置管理的基于度量的混合策略。与流行的去模糊方法相比，我们的方法平均提高了约 13% (PSNR) 和 10% (SSIM)。此外，我们的混合方案减少了恢复后的振铃伪影。||
|**2024-06-17**|[Low-power Ship Detection in Satellite Images Using Neuromorphic Hardware](http://arxiv.org/abs/2406.11319)|null|将地球观测图像数据从卫星传输到地面站会消耗大量的电力和带宽。对于海上船舶检测，机载数据处理可以识别船舶并减少发送到地面的数据量。然而，大多数在轨捕获的图像只包含水体或陆地，空中客车船舶检测数据集显示只有 22.1% 的图像包含船舶。我们设计了一个低功耗的两阶段系统来优化性能，而不是依赖于单个复杂模型。第一阶段是一个轻量级的二元分类器，充当检测船舶存在的门控机制。此阶段运行在 BrainChip 的 Akida 1.0 上，它利用激活稀疏性来最大程度地减少动态功耗。第二阶段采用 YOLOv5 目标检测模型来识别船舶的位置和大小。这种方法实现了 76.9% 的平均精度 (mAP)，通过减少误报，仅在包含船舶的图像上评估时，该精度提高到 79.3%。此外，我们计算出在 NVIDIA Jetson Nano 设备上评估完整验证集需要 111.4 kJ 的能量。我们的两阶段系统将此能耗降低到 27.3 kJ，不到四分之一，证明了异构计算系统的效率。||
|**2024-06-17**|[Semi-Supervised Domain Adaptation Using Target-Oriented Domain Augmentation for 3D Object Detection](http://arxiv.org/abs/2406.11313)|**[link](https://github.com/rasd3/toda)**|三维目标检测对于自动驾驶和机器人等应用至关重要。然而，在现实环境中，由于传感器升级、天气变化和地理差异导致的传感器数据分布变化会对检测性能产生负面影响。半监督域适应（SSDA）旨在通过将知识从具有丰富标记数据的源域迁移到标记数据稀缺的目标域来应对这些挑战。本文提出了一种新的SSDA方法，称为面向目标的域增强（TODA），该方法专为基于激光雷达的三维目标检测而设计。TODA有效地利用了所有可用数据，包括源域中的标记数据以及目标域中的标记数据和未标记数据，以提高域适应性能。TODA由两个阶段组成：TargetMix和AdvMix。TargetMix采用混合增强技术，结合激光雷达传感器特性，以促进源域和目标域之间的特征对齐。AdvMix将逐点对抗性增强与混合增强相结合，扰动未标记数据，以对齐目标域中标记数据和未标记数据中的特征。我们在具有挑战性的域适应任务上进行的实验表明，TODA的性能明显优于现有的专为三维目标检测设计的域适应技术。代码可在以下网址获取：https://github.com/rasd3/TODA。||
|**2024-06-17**|[Syn-to-Real Unsupervised Domain Adaptation for Indoor 3D Object Detection](http://arxiv.org/abs/2406.11311)|null|在室内3D物体检测中使用合成数据，可以大大减少3D标注所需的人工，并训练出有效的零样本检测器。然而，跨合成到真实室内数据集的复杂域迁移问题仍未得到充分探索。本文提出了一种新的面向对象的层次化域对齐（OHDA）框架，用于室内3D物体检测的合成到真实无监督域自适应。我们的方法包括一种对象感知增强策略，以有效地使源域数据多样化，并且我们引入了一个由对抗训练分支和伪标签分支组成的双分支自适应框架，以便同时实现整体级别和类别级别的域对齐。针对室内无监督域自适应的特点，我们提出了两种方案来进一步优化伪标签。我们从合成数据集3D-FRONT到真实世界数据集ScanNetV2和SUN RGB-D的适应结果表明，与仅使用源数据的基线相比，mAP25分别显著提高了9.7%和9.1%，并且始终优于从2D和3D室外场景改编的方法。代码将在论文被接收后公开。||
|**2024-06-17**|[BaFTA: Backprop-Free Test-Time Adaptation For Zero-Shot Vision-Language Models](http://arxiv.org/abs/2406.11309)|null|像CLIP这样的大规模预训练视觉语言模型在不同领域展现出卓越的零样本图像分类能力。为了在保持零样本范式的情况下提高CLIP的性能，各种测试时提示调优方法被引入，通过无监督学习目标在推理过程中优化类别嵌入。然而，这些方法在选择合适的学习率以防止在测试时适应过程中缺乏验证数据导致的训练崩溃方面经常遇到挑战。在本研究中，我们提出了一种新的免反向传播算法BaFTA，用于视觉语言模型的测试时适应。我们的方法不是微调文本提示来优化类别嵌入，而是在对齐文本和视觉嵌入的投影嵌入空间内使用在线聚类直接估计类别中心点。我们通过使用Rényi熵评估每个预测的可靠性，动态聚合来自估计和原始类别嵌入以及不同增强视图的预测。通过大量实验，我们证明了BaFTA在有效性和效率方面始终优于最先进的测试时适应方法。||
|**2024-06-17**|[VideoVista: A Versatile Benchmark for Video Understanding and Reasoning](http://arxiv.org/abs/2406.11303)|null|尽管在大型多模态模型（LMM）的快速发展的推动下，视频分析取得了重大突破，但仍然缺乏一个通用的评估基准来全面评估这些模型在视频理解和推理方面的性能。为了解决这个问题，我们提出了 VideoVista，这是一个视频问答基准，它整合了跨越不同内容类别、持续时间和能力的挑战。具体来说，VideoVista 包含从 3,400 个视频中提取的 25,000 个问题，涵盖 14 个类别（例如，Howto、电影和娱乐），持续时间从几秒到超过 10 分钟不等。此外，它还包含 19 种理解任务（例如，异常检测、交互理解）和 8 种推理任务（例如，逻辑推理、因果推理）。为了实现这一点，我们提出了一个自动数据构建框架，利用强大的 GPT-4o 以及先进的分析工具（例如，视频分割、对象分割和跟踪）。我们还利用此框架构建训练数据，以增强与视频相关的 LMM（Video-LMM）的能力。通过对尖端模型进行全面和定量的评估，我们发现：1）Video-LMM 在涉及时间定位、对象跟踪和异常检测的细粒度视频任务中面临困难；2）Video-LMM 的逻辑和关系推理能力较差；3）开源 Video-LMM 的性能明显低于 GPT-4o 和 Gemini-1.5，落后 20 个百分点。这突出了 VideoVista 在推进能够准确理解视频并执行精确推理的 LMM 方面将发挥的关键作用。||

<p align=right>(<a href=#updated-on-20240629>back to top</a>)</p>

## 模型压缩/优化

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-21**|[Fair Text to Medical Image Diffusion Model with Subgroup Distribution Aligned Tuning](http://arxiv.org/abs/2406.14847)|null|基于潜在扩散模型的文本到医学图像（T2MedI）模型在缓解医学图像数据稀缺和探索特定患者状态描述中病变的潜在外观分布方面具有巨大潜力。然而，与文本到自然图像模型类似，我们发现T2MedI模型也可能偏向于某些亚组，而忽略训练集中的少数群体。在本研究中，我们首先基于预训练的Imagen模型构建了一个T2MedI模型，该模型具有固定的对比语言-图像预训练（CLIP）文本编码器，而其解码器则在来自ROCO（上下文中的放射学对象）数据集的医学图像上进行了微调。我们定性和定量地分析了其性别偏差。针对这一问题，我们建议根据目标应用数据集微调T2MedI，以调整其敏感亚组的分布概率。具体而言，微调的对齐损失由现成的敏感亚组分类器指导，以匹配生成的图像和预期目标数据集之间的分类概率。此外，图像质量通过遵循知识蒸馏方案的CLIP一致性正则化项来维持。为了进行评估，我们将目标数据集设置为增强的BraST18数据集，并从中训练了一个基于脑磁共振（MR）切片的性别分类器。使用我们的方法，生成的MR图像可以显着减少与BraTS18数据集中性别比例的不一致。||
|**2024-06-20**|[Factual Dialogue Summarization via Learning from Large Language Models](http://arxiv.org/abs/2406.14709)|null|事实一致性是对话摘要中的一个重要质量。与较小的预训练语言模型相比，基于大型语言模型 (LLM) 的自动文本摘要模型生成的摘要在事实上更加一致，但由于隐私或资源限制，它们在实际应用中面临部署挑战。在本文中，我们研究了使用符号知识蒸馏来提高较小的预训练模型在对话摘要中的事实一致性。我们采用零样本学习从 LLM 中提取符号知识，生成事实上一致（正）和不一致（负）的摘要。然后，我们对这些摘要应用两个对比学习目标，以增强较小的摘要模型。使用 BART、PEGASUS 和 Flan-T5 进行的实验表明，我们的方法优于依赖复杂数据增强策略的强基线。我们的方法在保持连贯性、流畅性和相关性的同时实现了更好事实一致性，这一点已通过各种自动评估指标得到证实。我们还提供了对数据和代码的访问权限，以促进未来的研究。||
|**2024-06-20**|[Can LLMs Learn by Teaching? A Preliminary Study](http://arxiv.org/abs/2406.14629)|**[link](https://github.com/imagination-research/lbt)**|在大型语言模型 (LLM) 中，教学以改进学生模型（例如，知识蒸馏）是一种被广泛研究的方法。然而，对于人类来说，教学不仅可以提升学生，也可以提升教师。我们提出疑问：LLM 也能通过教学 (LbT) 来学习吗？如果可以，我们就有可能解锁模型持续进步的可能性，而无需仅仅依赖人类生成的数据或更强大的模型。在本文中，我们对这一雄心勃勃的目标进行了初步探索。我们展示了 LbT 的理念可以融入到现有的 LLM 训练/提示管道中，并带来显著的改进。具体来说，我们设计了三种方法，每一种都模仿了人类 LbT 的三个层次之一：观察学生的反馈、从反馈中学习以及迭代学习，目标是在不进行训练的情况下提高答案准确性，并通过微调提高模型的内在能力。研究结果令人鼓舞。例如，与人类的 LbT 类似，我们观察到：(1) LbT 可以诱导从弱到强的泛化：强大的模型可以通过教授其他较弱的模型来提升自己；(2) 学生的多样性可能会有所帮助：教授多个学生可能比只教授一个学生或教师本身更有效。我们希望这一早期成果能够激发未来对 LbT 的研究，并在更广泛的范围内采用先进的教育技术来改进 LLM。代码可在 https://github.com/imagination-research/lbt 获取。||
|**2024-06-20**|[Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs](http://arxiv.org/abs/2406.14282)|**[link](https://github.com/zjukg/lpkg)**|提升大型语言模型（LLM）在复杂问答（QA）场景中的性能一直是研究的重点。最近的研究试图通过将逐步规划与外部检索相结合来提高LLM的性能。虽然这对GPT-3.5等先进模型有效，但较小的LLM在分解复杂问题方面面临挑战，需要有监督的微调。以前的工作依赖于手动注释和来自教师LLM的知识蒸馏，这些方法既耗时又不准确。在本文中，我们介绍了一种新颖的框架，通过使用从知识图谱（KG）派生的规划数据来增强LLM的规划能力。使用这些数据进行微调的LLM具有改进的规划能力，使其能够更好地处理涉及检索的复杂QA任务。对多个数据集（包括我们新提出的基准测试）的评估突出了我们框架的有效性和KG派生规划数据的优势。||
|**2024-06-20**|[SeCoKD: Aligning Large Language Models for In-Context Learning with Fewer Shots](http://arxiv.org/abs/2406.14208)|null|先前研究表明，演示可以显著帮助大型语言模型（LLM）在给定任务中表现更出色。然而，这种被称为上下文学习（ICL）的能力对呈现的上下文非常敏感，通常需要数十个演示才能奏效。在这项工作中，我们研究了是否可以在保持竞争性能的同时减少样本数量。我们提出了SeCoKD，一个自知识蒸馏（KD）训练框架，它将学生模型与经过大量提示的变体进行对齐，从而提高了单个演示的利用率。我们在三个LLM和六个主要关注推理任务的基准测试中对SeCoKD进行了实验。结果表明，我们的方法优于基础模型和监督微调（SFT），特别是在零样本和单样本设置中，分别提高了30%和10%。此外，SeCoKD在评估新任务时几乎不会带来负面影响，这比监督微调更稳健。||
|**2024-06-20**|[Failure-Resilient Distributed Inference with Model Compression over Heterogeneous Edge Devices](http://arxiv.org/abs/2406.14185)|null|分布式推理范式能够将计算工作负载分布到多个设备上，从而促进在资源极其受限的物联网 (IoT) 场景中实现基于深度学习的智能服务。然而，它也给依靠大量计算/通信能力各不相同且容易发生崩溃或超时故障的物联网设备集群执行复杂的推理任务带来了巨大挑战。在本文中，我们提出了 RoCoIn，这是一种强大的协作推理机制，用于在异构边缘设备上本地分布式执行基于深度神经网络的推理任务。它创建了一组独立且紧凑的学生模型，这些模型是使用知识蒸馏从大型模型中学习而来的，用于分布式部署。特别是，对设备进行策略性分组，以冗余地部署和执行相同的学生模型，从而使推理过程能够抵御任何本地故障，同时设计了联合知识划分和学生模型分配方案，以最大程度地减少存在不同容量设备的情况下分布式推理系统的响应延迟。进行了广泛的仿真，以证实我们的 RoCoIn 在分布式推理方面优于其他几种基线的性能，结果证明了其在及时推理和故障恢复方面的有效性。||
|**2024-06-21**|[Can Low-Rank Knowledge Distillation in LLMs be Useful for Microelectronic Reasoning?](http://arxiv.org/abs/2406.13808)|null|在这项工作中，我们提供了关于在电子设计自动化 (EDA) 中使用离线大型语言模型 (LLM) 的可行性的实证结果。目标是调查和评估当代语言模型 (Llama-2-7B) 作为微电子问答专家以及其在解决微电子相关问题中的推理和生成能力。我们在各种适应方法中测试了 Llama-2-7B，包括引入了一种新颖的低秩知识蒸馏 (LoRA-KD) 方案。我们的实验产生了定性和定量结果。||
|**2024-06-19**|[BiLD: Bi-directional Logits Difference Loss for Large Language Model Distillation](http://arxiv.org/abs/2406.13555)|null|近年来，大型语言模型（LLM）在各种自然语言处理（NLP）任务中展现出卓越的能力。然而，这种令人印象深刻的性能通常伴随着参数规模的增加，这对广泛部署提出了重大挑战。知识蒸馏（KD）通过将知识从大型教师模型迁移到较小的学生模型来提供解决方案。在本文中，我们探讨了在logit级别对LLM进行特定任务的蒸馏。我们的研究表明，经过微调的LLM的logit表现出比视觉模型更极端的“长尾”分布，长尾中的隐藏“噪声”会影响蒸馏性能。此外，现有的logit蒸馏方法往往难以有效利用logit中的内部排序信息。为了解决这些问题，我们提出了双向Logit差异（BiLD）损失。BiLD损失通过仅使用top-k个教师和学生的logit来过滤掉长尾噪声，并通过构建logit差异来利用内部logit排序信息。为了评估BiLD损失，我们使用两种类型的LLM在13个数据集上进行了全面的实验。我们的结果表明，仅使用top-8个logit的BiLD损失优于监督微调（SFT）、vanilla KL损失以及来自NLP和CV领域的五种其他蒸馏方法。||
|**2024-06-19**|[Towards Cyber Threat Intelligence for the IoT](http://arxiv.org/abs/2406.13543)|null|随着数字化在关键领域应用的激增，将网络威胁的发生和评估信息纳入组织的威胁缓解策略至关重要。这种网络威胁情报（CTI）对于国家和工业关键基础设施正变得越来越重要，或者说是不可或缺。当前的CTI解决方案大多是联合式的，不适合共享来自低功耗物联网设备的威胁信息。本文对当今可用的CTI框架和CTI交换平台进行了分类和分析。文章提出了一种新的CTI架构，该架构依赖于定制化的MISP威胁情报共享平台，并专注于物联网环境。文章还介绍了一种定制版的STIX（我们称之为tinySTIX），它是CTI数据建模领域最著名的标准之一，通过使用新的轻量级编码和加密解决方案，该版本针对低功耗物联网设备进行了优化。所提出的CTI架构将对保护物联网网络安全非常有利，尤其是在恶劣和对抗性环境中运行的网络。||
|**2024-06-19**|[Improving Zero-Shot Cross-Lingual Transfer via Progressive Code-Switching](http://arxiv.org/abs/2406.13361)|null|代码切换是一种数据增强方案，它将多种语言的单词混合到源语言文本中。通过对齐跨语言上下文词表示，它在跨语言迁移任务中取得了相当大的泛化性能。然而，不受控制和过度替换的代码切换会将脏样本增加到模型训练中。换句话说，过多的代码切换文本样本将对模型的跨语言迁移能力产生负面影响。为此，我们提出了一种渐进式代码切换 (PCS) 方法，逐渐生成难度适中的代码切换示例，供模型从易到难地进行区分。其思想是逐步结合先前学习到的多语言知识，使用更容易的代码切换数据来指导模型对后续更难的代码切换数据的优化。具体来说，我们首先设计了一个难度测量器，根据词语相关性分数来衡量替换句子中每个词语的影响。然后代码切换器通过可控的温度变量生成难度递增的代码切换数据。此外，训练调度器决定何时对模型训练采样更难的代码切换数据。实验表明，我们的模型在跨越十种语言的三种不同的零样本跨语言迁移任务上取得了最先进的结果。||
|**2024-06-19**|[WaterMono: Teacher-Guided Anomaly Masking and Enhancement Boosting for Robust Underwater Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2406.13344)|**[link](https://github.com/oucvisiongroup/watermono)**|深度信息是陆地或水下各种视觉任务的关键先决条件。近年来，尽管没有深度注释，但自监督方法在多个陆地基准测试中取得了显著成果。然而，在更具挑战性的水下场景中，它们遇到了许多全新的障碍，例如海洋生物的影响和水下图像的退化，这些障碍分别打破了静态场景的假设并带来了低质量的图像。此外，水下图像的相机角度更加多样化。幸运的是，我们发现知识蒸馏为应对这些挑战提供了一种有前景的方法。在本文中，我们提出了 WaterMono，这是一种用于深度估计和图像增强的新型框架。它包含以下关键措施：(1) 我们提出了一种教师引导的异常掩码来识别图像中的动态区域；(2) 我们利用深度信息结合水下图像形成模型生成增强图像，进而有助于深度估计任务；(3) 我们利用旋转蒸馏策略来增强模型的旋转鲁棒性。综合实验表明，我们提出的方法对深度估计和图像增强都是有效的。源代码和预训练模型可在项目主页上找到：https://github.com/OUCVisionGroup/WaterMono。||
|**2024-06-19**|[PathoLM: Identifying pathogenicity from the DNA sequence through the Genome Foundation Model](http://arxiv.org/abs/2406.13133)|null|病原体鉴定对于疾病的诊断、治疗和预防至关重要，这对于控制感染和维护公众健康至关重要。传统的基于比对的方法虽然应用广泛，但计算量大，依赖于庞大的参考数据库，并且由于其敏感性和特异性较低，往往无法检测到新的病原体。同样，传统的机器学习技术虽然很有前景，但需要大量标注的数据集和广泛的特征工程，并且容易出现过拟合。为了应对这些挑战，我们推出了 PathoLM，这是一种尖端的病原体语言模型，经过优化，可以识别细菌和病毒序列中的致病性。PathoLM 利用了核苷酸转换器等预训练 DNA 模型的优势，只需要最少的数据进行微调，从而增强了病原体检测能力。它有效地捕捉了更广泛的基因组背景，显著提高了对新型和变异病原体的识别能力。我们开发了一个包含大约 30 种病毒和细菌的综合数据集，包括 ESKAPEE 病原体和七种对抗生素具有耐药性的著名剧毒细菌菌株。此外，我们还策划了一个专门针对 ESKAPEE 组的物种分类数据集。在比较评估中，PathoLM 的性能远远优于 DciPatho 等现有模型，表现出强大的零样本和小样本能力。此外，我们扩展了 PathoLM-Sp 用于 ESKAPEE 物种分类，尽管任务复杂，但与其他先进的深度学习方法相比，它仍表现出优异的性能。||
|**2024-06-19**|[Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation](http://arxiv.org/abs/2406.13114)|null|大型语言模型（LLM）在各种自然语言处理任务中取得了显著进展，但部署它们仍然计算成本高昂。知识蒸馏（KD）是一种很有前景的解决方案，可以将能力从较大的教师LLM转移到更紧凑的学生模型。特别是，序列级KD蒸馏的是基于推理过程的推理过程，而不仅仅是最终结果，在增强学生的推理能力方面显示出巨大潜力。然而，当前的方法在长尾数据分布下难以进行序列级KD，不利地影响了稀疏表示域的泛化能力。我们引入了多阶段平衡蒸馏（BalDistill）框架，该框架在固定的计算预算内迭代地平衡训练数据。通过动态选择代表性的头部域示例和合成尾部域示例，BalDistill在不同的长尾数据集上实现了最先进的性能，增强了蒸馏模型的效率和效果。||
|**2024-06-18**|[Vernacular? I Barely Know Her: Challenges with Style Control and Stereotyping](http://arxiv.org/abs/2406.12679)|null|大型语言模型 (LLM) 越来越多地应用于教育和学习领域。研究表明，控制语言风格以适应学习者的需求可以促进理解、促进包容并有助于知识蒸馏。为了了解当代 LLM 在风格控制方面的能力和局限性，我们评估了五种最先进的模型：GPT-3.5、GPT-4、GPT-4o、Llama-3 和 Mistral-instruct-7B，评估内容涵盖两项风格控制任务。我们观察到，在第一项任务中存在显著的不一致性，模型性能平均在小学五年级到八年级的阅读水平之间（针对一年级学生的任务），标准差高达 27.6。对于第二项任务，我们观察到性能有统计学意义的显著提高，从 0.02 提高到 0.26。然而，我们发现即使在参考文本中没有刻板印象的情况下，LLM 在执行任务时也经常生成文化上不敏感的内容。我们对结果进行了全面的分析和讨论。||
|**2024-06-18**|[Federated Learning with a Single Shared Image](http://arxiv.org/abs/2406.12658)|**[link](https://github.com/sunnysoni97/single_image_fl)**|联邦学习 (FL) 支持多台机器协同训练机器学习模型，而无需共享私人训练数据。然而，尤其对于异构模型，一个关键瓶颈仍然是如何将每个客户端模型获得的知识传递给服务器。一种流行的方法 FedDF 使用知识蒸馏来解决这个问题，它使用一个通用的共享数据集来交换预测结果。但是，在许多情况下，由于隐私问题，这样的数据集可能难以获取，而且客户端可能不允许存储大型共享数据集。为此，我们在本文中介绍了一种新方法，改进了这种知识蒸馏方法，仅依赖于客户端和服务器之间共享的单个图像。具体来说，我们提出了一种新颖的自适应数据集剪枝算法，该算法从单个图像生成的作物中选择信息量最大的作物。由此，我们证明了在有限的共享数据集预算下，使用单个图像进行知识蒸馏的联邦学习比使用多个单独图像效果更好。最后，我们扩展了我们的方法，通过结合非均匀蒸馏策略和服务器端的客户端模型镜像来允许训练异构客户端架构。||
|**2024-06-18**|[From Instance Training to Instruction Learning: Task Adapters Generation from Instructions](http://arxiv.org/abs/2406.12382)|**[link](https://github.com/Xnhyacinth/TAGI)**|大型语言模型 (LLM) 通过利用指令微调 (IFT) 获得了解决一般任务的能力。然而，IFT 仍然严重依赖于大量任务数据的实例训练，这极大地限制了 LLM 对现实世界场景的适应性，在这些场景中，标记的任务实例稀少，而更广泛的任务泛化能力至关重要。与 LLM 相反，人类获得技能和完成任务不仅仅是通过反复练习，而且是通过理解和遵循指导方针。本文致力于模拟人类学习以解决实例训练的 shortcomings，重点关注指令学习以增强跨任务泛化能力。在此背景下，我们介绍了从指令生成任务适配器 (TAGI)，它可以在给定任务指令的情况下，以参数生成的方式自动构建特定于任务的模型，而无需针对未见过的任务进行重新训练。具体来说，我们利用知识蒸馏来增强通过指令学习开发的 TAGI 与通过实例训练开发的任务特定模型之间的一致性，方法是在它们之间对齐标签、输出 logits 和适配器参数。TAGI 通过包含超网络预训练和微调的两阶段训练过程被赋予了跨任务泛化能力。我们在 Super-Natural Instructions 和 P3 数据集上评估了 TAGI。实验结果表明，TAGI 可以匹配甚至优于传统的元训练模型和其他超网络模型，同时显着降低计算需求。||
|**2024-06-18**|[Enhancing Single-Slice Segmentation with 3D-to-2D Unpaired Scan Distillation](http://arxiv.org/abs/2406.12254)|null|二维单层腹部计算机断层扫描 (CT) 能够以低辐射剂量评估体型和器官健康状况。然而，单层数据需要使用二维网络进行分割，但这些网络通常难以有效捕捉上下文信息。因此，即使在相同数据集上训练，三维网络通常也能获得更好的分割结果。在这项工作中，我们提出了一种新颖的三维到二维的知识蒸馏框架，利用预训练的三维模型来增强二维单层分割。具体来说，我们从三维表示中提取预测分布的中心，通过学习类内和类间相关性来指导二维学生模型。与需要相同数据输入的传统知识蒸馏方法不同，我们的方法采用未配对的、具有任意对比度的三维 CT 扫描来指导二维学生模型。在来自单层巴尔的摩老龄化纵向研究 (BLSA) 数据集的 707 名受试者上进行的实验表明，最先进的二维多器官分割方法可以受益于三维教师模型，在单层多器官分割中实现更高的性能。值得注意的是，我们的方法在低数据情况下表现出相当高的效率，即使仅使用 200 名训练受试者，其性能也优于使用所有可用训练受试者训练的模型。因此，这项工作强调了减轻手动标注负担的潜力。||
|**2024-06-18**|[Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs for Information Retrieval](http://arxiv.org/abs/2406.12169)|null|最近的研究探索了从大型语言模型 (LLM) 中提取知识以优化检索模型，尤其是在检索增强生成 (RAG) 框架内。然而，大多数现有的训练方法依赖于从 LLM 的权重或输出概率中提取监督信号，这不仅资源密集型，而且与黑盒 LLM 不兼容。在本文中，我们介绍了“中间蒸馏”，这是一种数据高效的知识蒸馏训练方案，它将 LLM 视为黑盒，并通过创新的 LLM-排序器-检索器管道提取其知识，仅使用 LLM 的排名生成作为监督信号。大量实验表明，我们提出的方法可以显著提高检索模型的性能，只需 1,000 个训练实例。此外，我们蒸馏的检索模型显著提高了 RAG 框架内问答任务的性能，证明了 LLM 在经济高效地训练小型模型方面的潜力。||
|**2024-06-17**|[Mutual Learning for Finetuning Click-Through Rate Prediction Models](http://arxiv.org/abs/2406.12087)|null|点击率 (CTR) 预测已成为数字行业（如数字广告或在线购物）中的一项基本任务。许多基于深度学习的方法已被实施，并已成为该领域的最新模型。为了进一步提高 CTR 模型的性能，基于知识蒸馏的方法已被广泛使用。然而，目前大多数 CTR 预测模型都没有很复杂的架构，因此很难称其中一个模型“笨重”，而另一个模型“微小”。另一方面，复杂模型和简单模型之间的性能差距也不是很大。因此，将知识从一个模型提炼到另一个模型可能不值得付出努力。在这些考虑下，相互学习可能是一种更好的方法，因为所有模型都可以相互改进。在本文中，我们展示了相互学习算法在平等模型之间使用时的效果。在我们对 Criteo 和 Avazu 数据集的实验中，相互学习算法将模型的性能提高了高达 0.66% 的相对改进。||
|**2024-06-17**|[On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning](http://arxiv.org/abs/2406.11823)|**[link](https://github.com/naver-ai/elva)**|近年来，语言和视觉助手的进步展现出令人印象深刻的能力，但缺乏透明度，限制了更广泛的研究和可重复性。虽然开源模型可以有效地处理一般的图像任务，但它们在处理复杂的视觉定位文本理解任务时面临着计算量大的挑战。这类任务通常需要增加token输入和大型视觉模块来利用高分辨率信息。如何在模型规模和数据重要性之间取得平衡仍然是一个悬而未决的问题。本研究旨在通过识别关键组件并创建具有受限推理成本的高效模型来重新定义视觉语言模型的设计。通过策略性地制定数据集、优化视觉模块和增强监督技术，我们在保持高性能的同时，显著提高了推理吞吐量。从1.6亿到130亿参数不等的模型的广泛实验为模型优化提供了见解。我们将在https://github.com/naver-ai/elva 上完全开源我们的代码库、模型和数据集。||
|**2024-06-17**|[NLDF: Neural Light Dynamic Fields for Efficient 3D Talking Head Generation](http://arxiv.org/abs/2406.11259)|null|基于神经辐射场模型的说话头生成已经展现出良好的视觉效果。然而，由于NeRF需要对数百个采样点进行繁重的计算才能合成一个像素，其渲染速度缓慢严重限制了其应用。为了解决这一问题，本文提出了一种新的神经光动态场模型（NLDF），旨在实现高质量的三维说话人脸生成，并显著提高生成速度。NLDF基于光段表示光场，并使用深度网络一次性学习整个光束的信息。在学习过程中，应用了知识蒸馏技术，并使用基于NeRF的合成结果来指导NLDF中光段的正确着色。此外，本文还提出了一种新的主动池训练策略，以关注高频运动，特别是说话者的嘴巴和眉毛。该方法有效地表示了三维说话视频生成中的面部光线动态，与基于NeRF的最先进方法相比，其速度提高了约30倍，且生成视觉质量相当。||
|**2024-06-17**|[ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking](http://arxiv.org/abs/2406.11257)|**[link](https://github.com/gaffey/excp)**|大型语言模型（LLM）最近在人工智能领域引起了广泛关注。然而，这些模型的训练过程在计算和存储容量方面提出了重大挑战，因此压缩检查点已成为一个迫切问题。在本文中，我们提出了一种新颖的极限检查点压缩（ExCP）框架，该框架可显著减少训练检查点所需的存储空间，同时实现近乎无损的性能。我们首先计算相邻检查点的残差，以获得对更高压缩比至关重要的稀疏信息。为了进一步挖掘检查点中的冗余参数，我们提出了一种权重-动量联合缩减方法，以利用模型优化过程中的另一个重要信息，即动量。具体来说，我们利用模型和优化器的信息来尽可能多地丢弃参数，同时保留关键信息以确保最佳性能。此外，我们利用非均匀量化来进一步压缩检查点的存储空间。我们对从4.1亿到70亿参数不等的多个模型广泛评估了我们提出的ExCP框架，并展示了在保持强大性能的同时显著减少了存储空间。例如，我们对Pythia-410M模型实现了大约70倍的压缩，最终性能在各种下游任务上与原始模型一样准确。代码将在https://github.com/Gaffey/ExCP上提供。||
|**2024-06-17**|[STEVE Series: Step-by-Step Construction of Agent Systems in Minecraft](http://arxiv.org/abs/2406.11247)|null|以大型语言模型 (LLM) 为核心构建具身智能体系统是一个很有前景的方向。由于在现实世界中部署和训练此类智能体的巨大成本和不可控因素，我们决定在 Minecraft 环境中开始探索。我们的 STEVE 系列智能体可以在虚拟环境中完成基本任务以及更具挑战性的任务，例如导航甚至创造性任务，其效率远超以往最先进的方法 2.5 到 7.3 倍。我们首先使用一个原始的大型语言模型，为其增强了视觉编码器和在我们收集的高质量数据集 STEVE-21K 上训练的动作代码库。随后，我们使用 Critic 和记忆模块对其进行增强，将其转变为一个复杂的系统。最后，我们构建了一个分层的多个智能体系统。我们最近的工作探索了如何通过知识蒸馏来简化智能体系统。未来，我们将探索 STEVE 智能体在现实世界中的更多潜在应用。||
|**2024-06-16**|[Outlier Reduction with Gated Attention for Improved Post-training Quantization in Large Sequence-to-sequence Speech Foundation Models](http://arxiv.org/abs/2406.11022)|null|本文探讨了在 Whisper 语音基础模型系列中，知识蒸馏后训练后量化 (PTQ) 的改进。我们解决了权重和激活张量中异常值的问题，众所周知，这些异常值会影响基于 Transformer 的语言和视觉模型的量化质量。将这一观察结果扩展到 Whisper，我们证明了当基于 Transformer 的模型被训练用于执行自动语音识别时，这些异常值也存在，因此需要针对 PTQ 采取缓解策略。我们表明，最近提出的学生模型注意力块中的门控机制可以减少异常值，从而实现有效的 8 位量化，并且与没有采用门控机制的学生模型相比，可以降低词错误率。||
|**2024-06-16**|[Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions](http://arxiv.org/abs/2406.10861)|null|联邦学习 (FL) 是一种分布式且保护隐私的机器学习范式，它协调多个客户端训练模型，同时将原始数据保存在本地。然而，这种传统的联邦学习带来了一些挑战，包括隐私风险、数据异构性、通信瓶颈和系统异构性问题。为了应对这些挑战，知识蒸馏 (KD) 自 2020 年以来已广泛应用于联邦学习。知识蒸馏是一种经过验证且有效的模型压缩和增强算法。知识蒸馏的核心概念是通过在中间层或输出层交换 logits 来促进模型之间的知识转移。这些特性使知识蒸馏成为解决联邦学习中长期挑战的绝佳方案。迄今为止，很少有综述总结和分析知识蒸馏如何有效应用于联邦学习的当前趋势和方法。本文旨在全面概述基于知识蒸馏的联邦学习，重点关注解决上述挑战。首先，我们概述了基于知识蒸馏的联邦学习，包括其动机、基础知识、分类以及与传统联邦学习的比较以及知识蒸馏应该在哪里执行。我们还在附录中分析了基于知识蒸馏的联邦学习中的关键因素，包括教师、知识、数据和方法。我们讨论了知识蒸馏如何应对联邦学习中的挑战，包括隐私保护、数据异构性、通信效率和个性化。最后，我们讨论了基于知识蒸馏的联邦学习算法面临的挑战和未来的研究方向。我们希望本次调查能够为联邦学习领域的研究人员和从业者提供见解和指导。||
|**2024-06-14**|[Self-Knowledge Distillation for Learning Ambiguity](http://arxiv.org/abs/2406.09719)|null|近期的语言模型在自然语言理解（NLU）任务中展现出卓越的性能。然而，当面对可以多重解读的歧义样本时，它们往往表现不佳，过度自信地预测单一标签而未考虑其正确性。为了解决这个问题，我们提出了一种新颖的自我知识蒸馏方法，通过利用从模型底层蒸馏出的知识，使模型能够更准确地学习标签分布。这种方法还包括一个学习阶段，根据蒸馏出的分布知识，重新校准被判定为极其模糊的训练样本的不必要增强的置信度。我们在不同的 NLU 基准数据集上验证了我们的方法，实验结果证明了其在生成更好的标签分布方面的有效性。特别是，通过重新校准高度模糊样本的置信度的过程，显著缓解了未见样本的预测与其真实标签不匹配时的过度自信问题。这已被证明有助于生成比现有最先进方法更好的分布。此外，与现有方法相比，我们的方法在训练模型方面效率更高，因为它不涉及额外的训练过程来优化标签分布。||
|**2024-06-14**|[Frequency-mix Knowledge Distillation for Fake Speech Detection](http://arxiv.org/abs/2406.09664)|null|在电话场景中，用于对抗语音欺骗攻击的假语音检测 (FSD) 任务极具挑战性。数据增强 (DA) 方法被认为是解决电话场景中 FSD 任务的有效手段，通常分为时域和频域两个阶段。虽然每种方法都有其优势，但都可能导致信息丢失。为了解决这个问题，我们提出了一种新的数据增强方法，即频率混合 (Freqmix)，并引入了 Freqmix 知识蒸馏 (FKD) 来增强模型的信息提取和泛化能力。具体来说，我们使用 Freqmix 增强的数据作为教师模型的输入，而学生模型的输入则经过时域数据增强方法处理。我们使用多级特征蒸馏方法来恢复信息并提高模型的泛化能力。我们的方法在 ASVspoof 2021 LA 数据集上取得了最先进的结果，与基线相比提高了 31%，并在 ASVspoof 2021 DF 数据集上表现出竞争力。||
|**2024-06-13**|[RobustSAM: Segment Anything Robustly on Degraded Images](http://arxiv.org/abs/2406.09627)|null|Segment Anything Model (SAM)作为一种变革性的图像分割方法已经出现，它以其强大的零样本分割能力和灵活的提示系统而备受赞誉。然而，它的性能在处理低质量图像时会受到挑战。为了解决这一局限性，我们提出了Robust Segment Anything Model (RobustSAM)，它在保留SAM的可提示性和零样本泛化能力的同时，增强了其在低质量图像上的性能。我们的方法利用了预训练的SAM模型，仅增加了少量参数和计算需求。RobustSAM的额外参数可以在8个GPU上用30小时内完成优化，这证明了其对于典型研究实验室的可行性和实用性。我们还介绍了Robust-Seg数据集，这是一个包含688K图像-掩码对的集合，这些图像-掩码对具有不同的退化程度，旨在优化我们模型的训练和评估。跨多个分割任务和数据集的大量实验结果证实了RobustSAM的优越性能，特别是在零样本条件下，突出了其在广泛现实应用中的潜力。此外，我们的方法已被证明可以有效提高基于SAM的下游任务（如单图像去雾和去模糊）的性能。||
|**2024-06-13**|[Contextual Distillation Model for Diversified Recommendation](http://arxiv.org/abs/2406.09021)|null|推荐的多样性与准确性在改善用户体验方面同样重要。现有研究，例如行列式点过程（DPP）和最大边缘相关性（MMR），采用贪婪范式来迭代地选择同时优化准确性和多样性的项目。然而，先前的方法通常表现出二次复杂度，将其应用限制在重排序阶段，并且不适用于具有更大候选项目池的其他推荐阶段，例如预排序和排序阶段。在本文中，我们提出了上下文蒸馏模型（CDM），这是一种解决多样化的有效推荐模型，适用于工业推荐流程的所有阶段的部署。具体来说，CDM利用同一用户请求中的候选项目作为上下文来增强结果的多样性。我们提出了一种对比上下文编码器，它采用注意力机制来对正面和负面上下文进行建模。对于CDM的训练，我们将每个目标项目与其上下文嵌入进行比较，并利用知识蒸馏框架来学习MMR算法下每个目标项目的获胜概率，其中教师来自MMR输出。在推理过程中，排名是通过推荐模型得分和学生模型得分的线性组合来执行的，从而确保了多样性和效率。我们在两个工业数据集上执行离线评估，并在短视频平台快手上进行CDM的在线A/B测试。正如指标所示，在推荐质量和多样性方面观察到的显着增强，为CDM的有效性提供了强大的优势。||

<p align=right>(<a href=#updated-on-20240629>back to top</a>)</p>

## OCR

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-20**|[Online Matching and Contention Resolution for Edge Arrivals with Vanishing Probabilities](http://arxiv.org/abs/2406.14506)|null|我们研究了在具有消失边概率的随机图上，序列竞争解决和匹配算法的性能。当图的边按对抗性选择的顺序处理时，我们推导出一个新的OCRS，它是0.382-可选的，在消失边概率假设下达到了文献中的“独立性基准”。作为对这一积极结果的补充，我们证明了没有任何OCRS可以超过0.390-可选，这显著改进了文献中0.428的上界。我们还推导出专门针对二部图或OCRS子族的负面结果。同时，当图的边按均匀随机顺序处理时，我们证明了接受所有活跃和可行边的简单贪婪竞争解决方案是1/2-可选的。由于已知的上界，这一结果是紧的。最后，当算法可以选择处理顺序时，我们证明对随机顺序稍作调整——给每个顶点一个随机优先级并按字典顺序处理边——会产生一个严格意义上更好的竞争解决方案，它是1-ln(2-1/e)≈0.510-可选的。我们的积极结果也适用于具有消失（非相同）边概率的1-均匀随机图上的在线匹配，扩展并统一了随机图文献中的一些结果。||
|**2024-06-19**|[GUI Action Narrator: Where and When Did That Action Take Place?](http://arxiv.org/abs/2406.13719)|null|多模态大型语言模型 (LLM) 的出现显著增强了图像 OCR 识别能力，使得 GUI 自动化成为现实，可以有效提高数字化任务的效率。开发 GUI 自动化系统的一个基本方面是理解基本的 GUI 操作。这种理解至关重要，因为它使代理能够从用户演示中学习，而用户演示是自动化的一个基本要素。为了严格评估这些能力，我们开发了一个用于 GUI 操作的视频字幕基准，其中包含 4,189 个不同的视频字幕样本。与自然场景视频字幕相比，这项任务提出了独特的挑战：1) GUI 屏幕截图通常比自然场景包含更密集的信息，以及 2) GUI 中的事件更微妙，发生得更快，需要精确关注适当的时间跨度和空间区域才能准确理解。为了应对这些挑战，我们引入了 GUI 操作数据集 Act2Cap 以及一个简单而有效的框架 GUI Narrator，用于 GUI 视频字幕，它利用光标作为视觉提示来增强对高分辨率屏幕截图的解释。具体来说，在我们的数据集上训练了一个光标检测器，并且具有选择关键帧和关键区域机制的多模态 LLM 模型生成字幕。实验结果表明，即使对于当今最先进的多模态模型（例如 GPT-4o），这项任务仍然极具挑战性。此外，我们的评估表明，我们的策略有效地提高了模型性能，无论是集成到开源模型的微调中还是用作闭源模型中的提示策略。||
|**2024-06-18**|[Sample-Based Matroid Prophet Inequalities](http://arxiv.org/abs/2406.12799)|null|我们研究了当分布未知且只能通过样本访问时的拟阵先知不等式。虽然已知特殊拟阵的单样本先知不等式，但对于一般拟阵，即使样本数量为亚线性，也没有已知的具有恒定竞争比的算法。更重要的是，一般拟阵的单样本问题版本与长期存在的拟阵秘书猜想有着密切的（双向）联系。在这项工作中，我们给出了一个仅需 $O_\varepsilon(\mathrm{poly} \log n)$个样本的$(\frac14 - \varepsilon)$竞争比拟阵先知不等式。我们的算法由两部分组成：(i)  一种基于分位数的新颖的归约方法，它将拟阵先知不等式归约为具有$O_\varepsilon(\log n)$个样本的在线竞争解决方案(OCRS)，以及 (ii)  一种具有$O_\varepsilon(\mathrm{poly} \log n)$个样本的$(\frac14 - \varepsilon)$ 可选拟阵OCRS，它仔细地解决了自适应性挑战。||
|**2024-06-17**|[GUICourse: From General Vision Language Models to Versatile GUI Agents](http://arxiv.org/abs/2406.11317)|**[link](https://github.com/yiye3/guicourse)**|利用图形用户界面 (GUI) 进行人机交互对于访问各种数字工具至关重要。视觉语言模型 (VLM) 的最新进展突出了开发多功能代理以帮助人类完成 GUI 导航任务的巨大潜力。然而，当前的 VLM 在基本能力（OCR 和 grounding）和 GUI 知识（GUI 元素的功能和控制方法）方面面临挑战，这阻碍了它们成为实用的 GUI 代理。为了解决这些挑战，我们贡献了 GUICourse，这是一套用于从通用 VLM 训练基于视觉的 GUI 代理的数据集。首先，我们介绍了 GUIEnv 数据集，以增强 VLM 的 OCR 和 grounding 能力。然后，我们介绍了 GUIAct 和 GUIChat 数据集，以丰富它们对 GUI 组件和交互的知识。实验表明，我们的 GUI 代理在常见 GUI 任务上比其基线 VLM 具有更好的性能。即使是小尺寸的 GUI 代理（具有 31 亿个参数）仍然可以在单步和多步 GUI 任务上良好运行。最后，我们通过消融研究分析了该代理训练阶段的不同变体。我们的源代码和数据集发布在 https://github.com/yiye3/GUICourse。||
|**2024-06-17**|[Unifying Multimodal Retrieval via Document Screenshot Embedding](http://arxiv.org/abs/2406.11251)|null|在现实世界中，文档以不同的格式和多种模态进行组织。传统的检索流程需要定制化的文档解析技术和内容提取模块来准备索引输入。这个过程繁琐且容易出错，并且会导致信息丢失。为此，我们提出了文档截图嵌入（DSE），这是一种新颖的检索范式，将文档截图视为统一的输入格式，不需要任何内容提取预处理，并保留文档中的所有信息（例如，文本、图像和布局）。DSE利用大型视觉语言模型将文档截图直接编码为用于检索的密集表示。为了评估我们的方法，我们首先构建了Wiki-SS数据集，这是一个包含130万个维基百科网页截图的语料库，用于回答自然问题数据集中的问题。在这种文本密集型文档检索环境中，与其他依赖解析的文本检索方法相比，DSE显示出具有竞争力的有效性。例如，在top-1检索准确率方面，DSE比BM25高出17个百分点。此外，在幻灯片检索的混合模态任务中，DSE在nDCG@10指标上明显优于OCR文本检索方法，超过15个百分点。这些实验表明，DSE是一种有效的文档检索范式，适用于各种类型的文档。模型检查点、代码和Wiki-SS集合将被发布。||
|**2024-06-14**|[Enhancing Question Answering on Charts Through Effective Pre-training Tasks](http://arxiv.org/abs/2406.10085)|null|为了完全理解一个文档，仅使用文本信息是不够的。理解视觉线索，例如布局和图表，也是必要的。虽然目前最先进的文档理解方法（包括基于 OCR 和非 OCR 的方法）运行良好，但尚未对其功能和局限性进行全面分析。因此，在这项工作中，我们解决了当前视觉问答模型应用于图表时存在的局限性。为了调查现有模型的缺陷，我们以 ChartQA 为例进行了全面的行为分析。我们的研究结果表明，现有模型在回答与图表结构和视觉上下文以及数值信息相关的问题时表现不佳。为了解决这些问题，我们提出了三个简单的预训练任务，这些任务从结构-视觉知识及其对数值问题的理解两方面增强了现有模型。我们在三个图表数据集（包括提取式和抽象式问题数据集）上评估了我们预训练的模型（称为 MatCha-v2），并观察到它比基线模型平均提高了 1.7%。||
|**2024-06-14**|[OSPC: Detecting Harmful Memes with Large Language Model as a Catalyst](http://arxiv.org/abs/2406.09779)|null|作为网络上个人观点和立场快速传播的媒介，迷因也给社会偏见和歧视的传播带来了重大挑战。本研究提出了一种新方法来检测有害迷因，特别是在新加坡多元文化和多语言的背景下。我们的方法整合了图像描述、光学字符识别 (OCR) 和大型语言模型 (LLM) 分析，以全面理解和分类有害迷因。该系统利用 BLIP 模型进行图像描述，利用 PP-OCR 和 TrOCR 进行多种语言的文本识别，并利用 Qwen LLM 进行细致入微的语言理解，能够识别以英语、中文、马来语和泰米尔语创建的迷因中的有害内容。为了提高系统性能，我们利用 GPT-4V 标记的额外数据对方法进行了微调，旨在将 GPT-4V 对有害迷因的理解能力提炼到我们的系统中。我们的框架在新加坡人工智能举办的网络安全奖挑战赛的公开排行榜上名列前茅，AUROC 为 0.7749，准确率为 0.7087，远远领先于其他团队。值得注意的是，我们的方法优于之前的基准，FLAVA 的 AUROC 为 0.5695，VisualBERT 的 AUROC 为 0.5561。||
|**2024-06-12**|[M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation](http://arxiv.org/abs/2406.08255)|**[link](https://github.com/amazon-science/m3t-multi-modal-translation-bench)**|对于神经机器翻译 (NMT) 系统来说，文档翻译是一项挑战。大多数文档级 NMT 系统依赖于精心整理的句子级平行数据，假设可以完美地从文档中提取文本并确定其精确的阅读顺序。这些系统也倾向于忽略额外的视觉线索，例如文档布局，认为它们无关紧要。然而，现实世界的文档通常具有复杂的文本布局，这与上述假设相矛盾。从光学字符识别 (OCR) 或启发式规则中提取信息可能会导致错误，并且布局（例如，段落、标题）可能会传达文本中相距较远的部分之间的关系。这种复杂性在广泛使用的 PDF 文档中尤为明显，因为 PDF 文档以可视方式呈现信息。为了弥合这一差距，本文介绍了一个名为 M3T 的新型基准数据集，该数据集专为评估 NMT 系统在半结构化文档的全面翻译任务上的性能而设计。该数据集旨在弥合文档级 NMT 系统中的评估差距，承认现实应用中丰富的文本布局所带来的挑战。||
|**2024-06-10**|[VCR: Visual Caption Restoration](http://arxiv.org/abs/2406.06462)|**[link](https://github.com/tianyu-z/vcr)**|我们提出了视觉字幕修复 (VCR)，这是一项新颖的视觉语言任务，它挑战模型使用图像中的像素级提示准确地恢复部分遮挡的文本。这项任务源于这样的观察：嵌入图像中的文本与常见的视觉元素和自然语言本质上不同，因为它需要对视觉、文本和嵌入图像中的文本等模态进行对齐。虽然许多工作已将嵌入图像中的文本整合到视觉问答任务中，但这些任务的方法通常依赖于光学字符识别或掩码语言建模，从而将任务简化为主要基于文本的处理。然而，基于文本的处理在 VCR 中变得无效，因为准确的文本恢复依赖于来自提供的图像、上下文和来自掩码文本的微小暴露区域的微妙线索的组合信息。我们开发了一个管道，使用图像-字幕对生成 VCR 任务的合成图像，并通过可调节的字幕可见性来控制任务难度。利用此管道，我们使用来自维基百科的带有字幕的图像构建了一个名为 VCR-Wiki 的 VCR 数据集，该数据集包含 211 万个英文实体和 34.6 万个中文实体，分为简单和困难两种变体。我们的结果表明，当前的视觉语言模型在 VCR 任务中的表现明显落后于人类，仅仅在我们数据集上微调模型并不能带来显着的改进。我们发布了 VCR-Wiki 和数据构建代码，以促进未来的研究。||
|**2024-06-07**|[Scaling Automatic Extraction of Pseudocode](http://arxiv.org/abs/2406.04635)|null|学术论文中的伪代码提供了一种表达其中实现算法的简洁方法。伪代码也可以被认为是连接编程语言和自然语言之间桥梁的中间表示形式。访问大量的伪代码集合可以带来各种好处，从增强算法理解、促进进一步的算法设计，到支持基于NLP或计算机视觉的模型，用于自动代码生成和光学字符识别（OCR）等任务。我们通过从arXiv论文中提取近320,000个伪代码示例，创建了一个大型伪代码集合。这个过程涉及扫描超过220万篇学术论文，其中1,000篇经过人工检查和标记。鉴于集合固有的异质性，我们的方法包括一个为优化覆盖范围而定制的提取机制，以及一个基于随机抽样的验证机制，以检查其准确性和可靠性。此外，我们还提供了对常见伪代码结构的见解，并辅以聚类和统计分析。值得注意的是，这些分析表明伪代码的使用呈指数级增长，突出了它们日益增长的重要性。||
|**2024-06-06**|[CORU: Comprehensive Post-OCR Parsing and Receipt Understanding Dataset](http://arxiv.org/abs/2406.04493)|**[link](https://github.com/update-for-integrated-business-ai/coru)**|在光学字符识别 (OCR) 和自然语言处理 (NLP) 领域，集成多语言功能仍然是一项关键挑战，尤其是在考虑阿拉伯语等复杂文字时。本文介绍了综合性 OCR 后解析和收据理解数据集 (CORU)，这是一个专门设计用于增强多语言环境（包括阿拉伯语和英语）下收据的 OCR 和信息提取的新数据集。CORU 包含来自不同零售环境（包括超市和服装店）的 20,000 多张带注释的收据，以及 30,000 张用于 OCR 的带注释的图像，这些图像用于识别检测到的每一行，以及 10,000 个为详细信息提取而注释的项目。这些注释捕获了基本细节，例如商家名称、商品描述、总价、收据编号和日期。它们的结构支持三个主要的计算任务：目标检测、OCR 和信息提取。我们在 CORU 上建立了一系列模型的基线性能，以评估传统方法（如 Tesseract OCR）和更先进的基于神经网络的方法的有效性。这些基线对于处理现实世界收据中常见的复杂且嘈杂的文档布局以及推进自动多语言文档处理的现状至关重要。我们的数据集可公开访问 (https://github.com/Update-For-Integrated-Business-AI/CORU)。||
|**2024-06-03**|[Generalized Jersey Number Recognition Using Multi-task Learning With Orientation-guided Weight Refinement](http://arxiv.org/abs/2406.01033)|null|球衣号码识别 (JNR) 一直是体育分析中的一项重要任务。由于图像存在模糊、遮挡、变形和低分辨率等问题，提高识别精度仍然是一项持续的挑战。最近的研究已经使用数字定位和光学字符识别来解决这些问题。一些方法将球员识别方案应用于图像序列，而忽略了人体旋转角度对球衣数字识别的影响。通过使用多任务方案来识别每个数字，可以更准确地预测球衣数字的数量，从而获得更可靠的结果。基于上述考虑，本文提出了一种称为角度数字细化方案 (ADRS) 的多任务学习方法，该方法结合人体方向角度和数字线索来识别运动球衣号码。根据我们的实验结果，我们的方法增加了推理信息，显着提高了预测精度。与只能处理单一类型运动的现有技术方法相比，所提出的方法产生了更加多样化和实用的 JNR 应用。将足球、橄榄球、篮球、排球和棒球等多种类型的团队运动纳入我们的数据集中，极大地促进了体育分析中通用的 JNR。我们的准确率在 Top-1 上达到了 64.07%，在 Top-2 上达到了 89.97%，相应的 F1 分数分别为 67.46% 和 90.64%。||
|**2024-05-30**|[Scaling up archival text analysis with the blockmodeling of n-gram networks -- A case study of Bulgaria's representation in the Osservatore Romano (January-May 1877)](http://arxiv.org/abs/2405.20156)|null|本文旨在通过应用网络聚类方法分析 1877 年 1 月至 5 月期间出版的 123 期《罗马观察报》中对保加利亚的报道，从而弥合档案文本分析与网络分析之间的差距。本研究利用光学字符识别和广义同质性块模型构建相关关键词网络。包括“保加利亚”和“俄罗斯”这两个词集在内的网络结构基本相同，并且与“德国”、“英国”和“战争”的网络结构有很大程度的重叠。从结构上看，这两个网络的块模型呈现出清晰的核心-半边缘-边缘结构，反映了报纸报道中各概念之间的关系。该报的词汇选择有效地消解了保加利亚民族复兴运动的合法性，突出了罗马教廷对该报编辑路线的影响。||

<p align=right>(<a href=#updated-on-20240629>back to top</a>)</p>

## 生成模型

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-27**|[Taming Data and Transformers for Audio Generation](http://arxiv.org/abs/2406.19388)|null|由于数据稀缺且字幕质量通常不足，生成环境声音和效果是一个具有挑战性的问题，这使得很难将大规模生成模型用于此任务。在这项工作中，我们通过引入两个新模型来解决这个问题。首先，我们提出了 AutoCap，一种高质量且高效的自动音频字幕模型。我们表明，通过利用音频模态可用的元数据，我们可以大幅提高字幕的质量。AutoCap 的 CIDEr 得分达到 83.2，与目前最好的字幕模型相比提高了 3.2%，推理速度提高了四倍。然后，我们使用 AutoCap 为现有数据集中的片段添加字幕，获得了 761,000 个带有高质量字幕的音频片段，形成了目前最大的可用音频文本数据集。其次，我们提出了 GenAu，这是一种可扩展的基于 Transformer 的音频生成架构，我们将其扩展到 12.5 亿个参数，并使用我们的新数据集进行训练。与最先进的音频生成器相比，GenAu 的 FAD 得分显著提高了 15.7%，IS 提高了 22.7%，CLAP 得分提高了 13.5%，表明与之前的工作相比，生成的音频质量显著提高。这表明数据的质量通常与其数量同样重要。此外，由于 AutoCap 是全自动的，因此可以将新的音频样本添加到训练数据集中，从而可以训练更大的音频合成生成模型。||
|**2024-06-27**|[Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space](http://arxiv.org/abs/2406.19370)|null|现代生成模型展现出令人印象深刻的能力，这可能源于它们能够识别和操纵训练数据背后的抽象概念。然而，一些基本问题仍然存在：是什么决定了模型学习的概念、学习这些概念的顺序，以及模型操纵这些概念的能力？为了解决这些问题，我们建议通过一个我们称之为“概念空间”的框架来分析模型的学习动态，其中每个轴代表数据生成过程背后的一个独立概念。通过描述这个空间中的学习动态，我们确定了一个概念的学习速度，以及概念学习的顺序是如何由我们称之为“概念信号”的数据属性控制的。此外，我们观察到模型学习动态在概念空间中方向突然转变的时刻。令人惊讶的是，这些点恰好对应于隐藏能力的出现，即潜在干预表明模型具备操纵某个概念的能力，但这些能力还不能通过简单的输入提示来激发。虽然我们的结果集中在人工合成的玩具数据集上，但我们假设关于隐藏能力出现的普遍主张可能成立：生成模型拥有在训练过程中突然且一致地出现的潜在能力，尽管模型可能无法在简单的输入提示下展现出这些能力。||
|**2024-06-27**|[Accelerating Multiphase Flow Simulations with Denoising Diffusion Model Driven Initializations](http://arxiv.org/abs/2406.19333)|null|本研究介绍了一种混合流体模拟方法，该方法将生成扩散模型与基于物理的模拟相结合，旨在在保留所有感兴趣的物理特性的同时降低流动模拟的计算成本。这些模拟增强了我们对应用的理解，例如评估地下储层中氢气和CO $_2$ 储存效率。然而，它们的计算成本很高，并且非唯一解的存在可能需要在单个几何结构内进行多次模拟。为了克服计算成本障碍，我们提出了一种将生成扩散模型与基于物理的建模相结合的混合方法。我们引入了一个系统，可以用感兴趣的几何形状来调节扩散模型，从而允许在相同的几何形状中产生可变的流体饱和度。在训练模型的同时，我们生成初始条件，并使用这些条件执行基于物理的模拟。这种集成方法使我们能够在配备CPU和GPU的单个计算节点上接收实时反馈。通过在一个计算节点内有效地管理这些进程，我们可以持续评估性能，并在满足所需条件时停止训练。为了测试我们的模型，我们在真实的Berea砂岩裂缝中生成了模拟结果，结果表明我们的技术比常用的流动模拟初始化速度快4.4倍。||
|**2024-06-27**|[Subtractive Training for Music Stem Insertion using Latent Diffusion Models](http://arxiv.org/abs/2406.19328)|null|我们提出了“差分训练法”，这是一种简单新颖的方法，用于在给定其他乐器作为上下文的情况下合成单个乐器的音轨。这种方法将完整的音乐混音数据集与 1) 缺少特定音轨的数据集变体，以及 2) 描述如何重新引入缺失音轨的 LLM 生成的指令配对。然后，我们微调预训练的文本到音频扩散模型，以生成缺失的乐器音轨，并以现有音轨和文本指令作为指导。我们的结果证明了“差分训练法”在创建与现有音轨无缝融合的真实鼓音轨方面的功效。我们还展示了可以使用文本指令在节奏、动态和风格方面控制插入音轨的生成，从而允许我们在保持其余乐器不变的情况下修改完整歌曲中单个乐器的风格。最后，我们将此技术扩展到 MIDI 格式，成功地为不完整的编曲生成了兼容的贝斯、鼓和吉他部分。||
|**2024-06-27**|[Efficient World Models with Context-Aware Tokenization](http://arxiv.org/abs/2406.19320)|**[link](https://github.com/vmicheli/delta-iris)**|扩展深度强化学习 (RL) 方法是一项重大挑战。 随着生成建模的发展，基于模型的 RL 将自身定位为强有力的竞争者。 序列建模的最新进展催生了有效的基于 Transformer 的世界模型，尽管由于准确模拟环境所需的标记序列较长，因此计算量很大。 在这项工作中，我们提出了 $\Delta$-IRIS，这是一种具有世界模型架构的新代理，该架构由一个离散自动编码器和一个自回归 Transformer 组成，离散自动编码器对时间步长之间的随机增量进行编码，而自回归 Transformer 通过使用连续标记总结当前世界状态来预测未来增量。 在 Crafter 基准测试中，$\Delta$ -IRIS 在多个帧预算下创造了新的最先进水平，同时比以前基于注意力的方法训练速度快一个数量级。 我们在 https://github.com/vmicheli/delta-iris 上发布了我们的代码和模型。||
|**2024-06-27**|[Compositional Image Decomposition with Diffusion Models](http://arxiv.org/abs/2406.19298)|null|给定一张自然场景的图像，我们能够快速将其分解成一组组件，例如物体、光照、阴影和前景。然后，我们可以想象这样一个场景：我们将某些组件与其他图像中的组件组合在一起，例如，将我们卧室中的一组物体和动物园里的动物，在森林的光照条件下组合在一起，即使我们以前从未遇到过这样的场景。在本文中，我们提出了一种将图像分解成这种组合组件的方法。我们的方法，即分解扩散模型（Decomp Diffusion），是一种无监督方法，当给定一张图像时，它可以推断出图像中的一组不同的组件，每个组件都由一个扩散模型表示。我们演示了组件如何捕获场景的不同因素，从全局场景描述符（如阴影或面部表情）到局部场景描述符（如构成物体的物体）。我们进一步说明了如何灵活地组合推断出的因素，即使是与从其他模型推断出的因素组合，以生成与训练时看到的场景截然不同的各种场景。网站和代码位于https://energy-based-model.github.io/decomp-diffusion。||
|**2024-06-27**|[On Pólya-Young urn models and growth processes](http://arxiv.org/abs/2406.19110)|null|这项工作致力于研究 Pólya-Young 骨灰盒，这是一类在 Young 表分析中具有重要意义的周期性 Pólya 骨灰盒。我们对 Banderier、Marchal 和 Wallner [Ann. Prob. (2020)] 先前关于 Pólya-Young 骨灰盒的结果进行了几项扩展，并对先前研究的模型进行了概括。我们确定了广义模型的极限律，其中涉及噪声增强贝塞尔过程的局部时间。我们还发现了一种鞅结构，它直接导致了目标随机变量的几乎必然收敛性。这使我们能够通过提供鞅尾和的中心极限定理以及迭代对数定律来添加二阶渐近性。我们还研究了随机向量，并获得了具有多种颜色的 Pólya-Young 骨灰盒的极限律。此外，我们介绍了几种增长过程和组合对象，它们与骨灰盒模型密切相关。我们定义了具有周期性迁移的递增树，并将 Pólya-Young 骨灰盒的动力学与此类树族中基于标签的参数相关联。此外，我们讨论了斯特林排列的推广，并获得了与具有周期性迁移的递增树的双射。最后，我们介绍了一种具有竞争机制的中式餐厅过程，并将其与递增树以及 Pólya-Young 骨灰盒相关联。||
|**2024-06-27**|[Using diffusion model as constraint: Empower Image Restoration Network Training with Diffusion Model](http://arxiv.org/abs/2406.19030)|null|随着深度学习的出现，图像修复取得了显著进展。以前的方法通常依赖于设计强大的网络架构来提升性能，然而，修复结果的自然视觉效果受到颜色和纹理失真的限制。除了视觉感知质量外，语义感知恢复是修复图像的一个重要但经常被忽视的方面，这对于在高级任务中的部署至关重要。在本文中，我们通过引入一种面向自然性和语义感知的优化机制，即 DiffLoss，为解决这些问题提出了一个新的视角。具体来说，受扩散模型对自然图像生成的强大分布覆盖能力的启发，我们利用扩散模型的马尔可夫链采样特性，并将现有网络的恢复结果投影到采样空间中。此外，我们发现扩散模型的瓶颈特征（也称为 h 空间特征）是一个天然的高级语义空间。我们深入研究了这一特性，并提出了一种语义感知损失，以进一步释放其语义感知恢复的潜力，这为连接图像修复任务和下游高级识别任务铺平了道路。通过这两种策略，DiffLoss 可以赋予现有修复方法更自然和语义感知的结果。我们在大量常见的图像修复任务和基准测试中验证了我们方法的有效性。代码将在 https://github.com/JosephTiTan/DiffLoss 上提供。||
|**2024-06-27**|[AnyControl: Create Your Artwork with Versatile Control on Text-to-Image Generation](http://arxiv.org/abs/2406.18958)|null|近年来，文本到图像 (T2I) 生成领域取得了重大进展，这在很大程度上得益于扩散模型的进步。语言控制能够有效地创建内容，但在对图像生成的细粒度控制方面却面临挑战。为了应对这一挑战，现有的研究在很大程度上尝试将用户提供的额外空间条件（如深度图和边缘图）通过额外编码的方式融入预训练的 T2I 模型中。然而，多重控制图像合成仍然面临若干挑战。具体而言，当前的方法在处理不同输入控制信号的自由组合方面存在局限性，忽视了多个空间条件之间复杂的相互关系，并且经常无法保持与所提供文本提示的语义一致性，这可能导致用户体验欠佳。为了应对这些挑战，我们提出了 AnyControl，这是一个支持多种控制信号任意组合的多重控制图像合成框架。AnyControl 开发了一种新颖的多重控制编码器，可以提取统一的多模态嵌入来指导生成过程。这种方法能够全面理解用户输入，并在多种控制信号下生成高质量、逼真的结果，这一点已通过大量的定量和定性评估得到证实。我们的项目页面位于\url{https://any-control.github.io}。||
|**2024-06-27**|[Investigating and Defending Shortcut Learning in Personalized Diffusion Models](http://arxiv.org/abs/2406.18944)|null|个性化扩散模型近年来受到广泛关注，它能够仅凭少量图像就可对预训练的文本到图像模型进行微调，生成特定主题的图像。然而，最近的研究发现，这类模型容易受到轻微对抗性扰动的影响，并且在损坏的数据集上，微调性能会大幅下降。这种特性被进一步利用，在敏感图像（如肖像）上制作保护性扰动，以防止未经授权的生成。作为应对措施，研究者提出了基于扩散的净化方法来消除这些扰动并保持生成性能。然而，现有的工作缺乏对个性化扩散模型的基本捷径学习漏洞的详细分析，并且过度净化图像会导致信息丢失。在本文中，我们从捷径学习的角度深入研究了个性化扩散模型的微调过程，并提出了一个假设来解释现有扰动方法的潜在操作机制。具体来说，我们发现受扰动图像在其基于 CLIP 的潜在空间中与其原始配对提示发生了很大偏移。因此，使用这种不匹配的图像-提示对进行训练会导致模型将分布外的噪声模式倾倒到标识符中，从而导致严重的性能下降。基于这一观察结果，我们提出了一种系统性的方法，通过净化来保持训练性能，该方法重新调整了潜在图像及其语义，并引入了带有负面标记的对比学习，以解耦想要的干净身份和不需要的噪声模式的学习，这显示出强大的抵御进一步自适应扰动的能力。||
|**2024-06-25**|[DiffusionPDE: Generative PDE-Solving Under Partial Observation](http://arxiv.org/abs/2406.17763)|null|我们提出了一个使用生成扩散模型求解偏微分方程 (PDE) 的通用框架。 特别是，我们关注于缺乏应用经典求解器所需的完整场景信息的场景。 当对数据或基础系数的观察不完整时，大多数现有的正向或反向 PDE 方法表现不佳，而这在现实世界测量中是一个常见假设。 在这项工作中，我们提出了 DiffusionPDE，它可以通过对解空间和系数空间的联合分布进行建模，同时填充缺失信息并求解 PDE。 我们表明，学习到的生成先验知识为在部分观察下准确求解各种 PDE 提供了一个通用框架，在正向和反向方向上均显着优于最先进的方法。||
|**2024-06-25**|[Extensions of Panjer's recursion for mixed compound distributions](http://arxiv.org/abs/2406.17726)|null|在精算实践中，集体风险模型常用的独立性假设经常被违反，这意味着越来越需要考虑包含依赖性的更通用的模型。为此，本文通过将分布的参数视为随机变量，研究了经典 Panjer 索赔次数分布族的混合对应物及其复合形式。在假设索赔额过程是条件独立同分布且（条件）与索赔次数相互独立的情况下，我们提供了一种递归算法来计算总索赔额的概率质量函数。同时还研究了具有可交换索赔额的复合 Panjer 分布的情况。为了完整性，我们通过各种数值例子来说明我们的结果。||
|**2024-06-25**|[Unified Auto-Encoding with Masked Diffusion](http://arxiv.org/abs/2406.17688)|**[link](https://github.com/philippe-eecs/small-vision)**|在成功的生成式和自监督表征学习模型的核心，都有一个重建目标，它包含某种形式的图像损坏。扩散模型通过预定的高斯损坏过程来实现这种方法，而掩码自动编码器模型则通过掩盖图像块来实现。尽管它们的方法不同，但其方法的潜在相似性为能够同时完成去噪任务的自动编码器提供了一个有希望的途径。我们提出了一个统一的自监督目标，称为统一掩码扩散（UMD），它在单个自动编码框架内结合了基于块和基于噪声的损坏技术。具体来说，UMD 通过在扩散噪声调度中引入一个额外的无噪声、高掩码表示步骤来修改扩散变换器 (DiT) 训练过程，并利用混合掩码和噪声图像进行后续的时间步长。通过整合对扩散建模和预测掩码块标记有用的特征，UMD 在下游生成和表征学习任务（包括线性探测和类别条件生成）中实现了强大的性能。这是在不需要大量数据增强、多视图或额外编码器的情况下实现的。此外，UMD 在总训练时间方面比之前的基于扩散的方法提高了计算效率。我们在 https://github.com/philippe-eecs/small-vision 上发布了我们的代码。||
|**2024-06-25**|[LaTable: Towards Large Tabular Models](http://arxiv.org/abs/2406.17673)|null|表格数据是最普遍存在的数据形式之一，但关于表格生成基础模型的文献却远远落后于文本和视觉领域。创建这样的模型很困难，这是由于不同表格数据集的特征空间异构、表格元数据（例如数据集描述和特征标题）以及表格缺乏先验知识（例如特征顺序）。在这项工作中，我们提出了 LaTable：一种新颖的表格扩散模型，它解决了这些挑战并且可以跨不同的数据集进行训练。通过广泛的实验，我们发现 LaTable 在分布内生成方面优于基线模型，并且微调 LaTable 可以使用更少的样本更好地生成分布外数据集。另一方面，我们探讨了 LaTable 较差的零样本性能，以及它可以教会我们如何构建具有更好零样本和少样本生成能力的表格生成基础模型。||
|**2024-06-25**|[SpecMaskGIT: Masked Generative Modeling of Audio Spectrograms for Efficient Audio Synthesis and Beyond](http://arxiv.org/abs/2406.17672)|null|近年来，迭代合成音频片段的生成模型取得了很大进展，推动了文本到音频合成（TTA）的成功，但同时也带来了合成速度慢和计算量大的问题。 尽管已经有一些尝试加速迭代过程，但由于推理阶段需要数百次迭代以及模型参数量大，高质量的 TTA 系统效率仍然低下。 为了应对这些挑战，我们提出了 SpecMaskGIT，一个基于掩码生成建模的轻量级、高效且有效的 TTA 模型。 首先，SpecMaskGIT 可以通过不到 16 次迭代合成逼真的 10 秒音频片段，比以前的迭代 TTA 方法的数量级要少。 作为一个离散模型，SpecMaskGIT 在 TTA 基准测试中优于更大的 VQ-Diffusion 和自回归模型，同时仅使用 4 个 CPU 内核即可实现实时，使用 GPU 甚至可以快 30 倍。 其次，SpecMaskGIT 建立在梅尔频谱图的潜在空间之上，比建立在潜在波形域上的类似方法具有更广泛的应用范围（例如，零样本带宽扩展）。 此外，我们将 SpecMaskGIT 解释为先前判别性音频掩码 Transformer 的生成扩展，并揭示了其音频表示学习的潜力。我们希望我们的工作能够激发对掩码音频建模的探索，以应对更多样化的场景。||
|**2024-06-25**|[Banishing LLM Hallucinations Requires Rethinking Generalization](http://arxiv.org/abs/2406.17642)|null|尽管大型语言模型 (LLM) 拥有强大的聊天、编码和推理能力，但它们经常会产生幻觉。传统观点认为，幻觉是创造力和事实性之间平衡的结果，可以通过将 LLM 基于外部知识源来缓解，但不能消除。通过广泛的系统性实验，我们表明这些传统方法无法解释为什么 LLM 在实践中会产生幻觉。具体来说，我们证明了增强了大规模记忆专家混合体 (MoME) 的 LLM 可以轻松记忆大型随机数数据集。我们用理论构造证实了这些实验结果，表明训练用于预测下一个标记的简单神经网络会在训练损失超过阈值时产生幻觉，这在使用互联网规模数据进行训练时通常会发生。我们通过与用于缓解幻觉的传统检索方法进行比较来解释我们的发现。我们利用我们的发现设计了第一代去除幻觉的模型——Lamini-1——它将事实存储在数百万个动态检索的记忆专家的巨大混合体中。||
|**2024-06-25**|[Aligning Diffusion Models with Noise-Conditioned Perception](http://arxiv.org/abs/2406.17636)|null|近年来，最初为语言模型 (LM) 开发的人类偏好优化取得了进展，并显示出对文本到图像扩散模型的希望，增强了提示对齐、视觉吸引力和用户偏好。与 LM 不同，扩散模型通常在像素或 VAE 空间中进行优化，这与人类感知并不一致，导致在偏好对齐阶段训练速度更慢且效率更低。我们建议在扩散模型的 U-Net 嵌入空间中使用感知目标来解决这些问题。我们的方法包括使用直接偏好优化 (DPO)、对比偏好优化 (CPO) 和在此嵌入空间内进行监督微调 (SFT) 来微调 Stable Diffusion 1.5 和 XL。这种方法在质量和计算成本等各种指标上都明显优于标准的潜在空间实现。对于 SDXL，我们的方法在 PartiPrompts 数据集上相对于原始开源 SDXL-DPO 提供了 60.8% 的总体偏好、62.2% 的视觉吸引力和 52.1% 的提示跟踪，同时显着减少了计算量。我们的方法不仅提高了扩散模型的人类偏好对齐的效率和质量，而且还可以轻松地与其他优化技术集成。训练代码和 LoRA 权重将在此处提供：https://huggingface.co/alexgambashidze/SDXL\_NCP-DPO\_v0.1||
|**2024-06-25**|[Test-Time Generative Augmentation for Medical Image Segmentation](http://arxiv.org/abs/2406.17608)|**[link](https://github.com/maxiao0234/ttga)**|在本文中，我们提出了一种在测试时增强医学图像分割的新方法。我们不采用在输入测试图像上手动设计变换或函数来创建用于测试时增强的多个视图，而是主张利用先进的域微调生成模型 (GM)，例如稳定扩散 (SD)，来进行测试时增强。鉴于 GM 已经过训练以理解和封装全面的域数据知识，因此它在表示数据特征和分布方面优于分割模型。因此，通过将 GM 集成到测试时增强中，我们可以有效地生成给定测试样本的多个视图，使其与样本的内容和外观特征以及相关的局部数据分布保持一致。与传统的手动变换相比，这种方法使增强过程更具适应性和弹性。在三个医学图像分割任务（九个数据集）中进行的综合实验证明了所提出的 TTGA 在增强分割结果方面的有效性和多功能性。此外，TTGA 显着改善了像素级误差估计，从而促进了更可靠的分割系统的部署。代码将发布在：https://github.com/maxiao0234/TTGA。||
|**2024-06-25**|[Diffusion-based Adversarial Purification for Intrusion Detection](http://arxiv.org/abs/2406.17606)|null|网络攻击的日益复杂化促使机器学习技术被整合到入侵检测系统中，但对抗样本的出现带来了重大挑战。这些精心设计的扰动会误导机器学习模型，使攻击者能够逃避检测或触发错误警报。作为应对措施，对抗净化已成为一种引人注目的解决方案，尤其是扩散模型显示出良好的效果。然而，在入侵检测领域，其净化潜力仍未得到探索。本文论证了扩散模型在网络入侵检测中净化对抗样本的有效性。通过对扩散参数的全面分析，我们确定了在对正常性能影响最小的情况下最大化对抗鲁棒性的最佳配置。重要的是，这项研究揭示了扩散噪声和扩散步骤之间关系的见解，这是对该领域的全新贡献。我们的实验在两个数据集上进行，并针对 5 种对抗攻击。实现代码已公开发布。||
|**2024-06-25**|[Director3D: Real-world Camera Trajectory and 3D Scene Generation from Text](http://arxiv.org/abs/2406.17601)|**[link](https://github.com/imlixinyang/director3d)**|近年来，3D 生成技术的进步利用了具有真实 3D 资源和预定义摄像头的合成数据集。然而，采用真实世界数据集的潜力，虽然可以生成更逼真的 3D 场景，但在很大程度上仍未得到探索。本研究深入探讨了真实世界捕捉中复杂且场景特定的相机轨迹这一关键挑战。我们引入了 Director3D，这是一个强大的开放世界文本到 3D 生成框架，旨在生成真实世界的 3D 场景和自适应相机轨迹。为了实现这一目标，(1) 我们首先利用轨迹扩散 Transformer，充当摄影师的角色，根据文本描述对相机轨迹的分布进行建模。(2) 接下来，以高斯驱动的多视图潜在扩散模型作为装饰器，在给定相机轨迹和文本的情况下对图像序列分布进行建模。该模型从 2D 扩散模型微调而来，直接生成像素对齐的 3D 高斯分布，作为一致去噪的直接 3D 场景表示。(3) 最后，通过新的 SDS++ 损失作为细节增强器对 3D 高斯分布进行细化，该损失结合了 2D 扩散模型的先验知识。大量实验表明，Director3D 优于现有方法，在真实世界 3D 生成方面提供了卓越的性能。||
|**2024-06-21**|[Masked Extended Attention for Zero-Shot Virtual Try-On In The Wild](http://arxiv.org/abs/2406.15331)|null|虚拟试衣 (VTON) 是一个高度活跃的研究领域，需求不断增长。它的目标是用另一件衣服替换图像中的一件衣服，同时保留人物和衣服的特征以及图像的保真度。目前的文献采用监督学习的方法来完成这项任务，这会损害泛化能力并带来沉重的计算负担。在本文中，我们提出了一种新颖的零样本免训练方法，用于参考其他图像来修复服装。我们的方法利用了扩散模型的先验知识，无需额外训练，充分利用了其固有的泛化能力。该方法采用扩展注意力机制将图像信息从参考图像迁移到目标图像，克服了两个重大挑战。我们首先使用深度特征将参考服装扭曲到目标人物身上，从而减轻“纹理粘连”。然后，我们利用带有谨慎掩蔽的扩展注意力机制，消除了参考背景的泄漏和不必要的干扰。通过用户研究、定性和定量比较最先进的方法，我们证明了与未见过的服装或人物相比，我们的方法具有更高的图像质量和服装保真度。||
|**2024-06-21**|[You Only Acquire Sparse-channel (YOAS): A Unified Framework for Dense-channel EEG Generation](http://arxiv.org/abs/2406.15269)|null|获取高精度的密集通道脑电图 (EEG) 信号常常受到设备成本高和便携性差的阻碍。相比之下，从稀疏通道有效生成密集通道脑电图信号显示出良好的前景和经济可行性。然而，稀疏通道脑电图也面临着一些挑战，例如空间分辨率降低、信息丢失、信号混合以及对噪声和干扰的敏感性增加。为了应对这些挑战，我们首先从理论上将密集通道脑电图生成问题表述为通过优化一组跨通道脑电图信号生成问题来解决。然后，我们提出了 YOAS 框架，用于从稀疏通道脑电图信号生成密集通道数据。YOAS 共包含四个顺序阶段：数据准备、数据预处理、偏差脑电图生成和合成脑电图生成。数据准备和预处理阶段仔细考虑了脑电图电极的分布和脑电图信号的低信噪比问题。偏差脑电图生成阶段包括 BiasEEGGanFormer 和 BiasEEGDiffFormer 这两个子模块，它们分别通过注意力机制促进长期特征提取，并通过结合电极位置对齐和扩散模型生成信号。合成脑电图生成阶段采用多通道脑电图生成的演绎范式合成最终信号。大量实验验证了 YOAS 的可行性、效率和理论有效性，甚至显著增强了数据的可辨别性。从稀疏通道数据生成密集通道脑电图信号的这一突破为脑电图信号处理和应用开辟了新的探索途径。||
|**2024-06-21**|[Evaluating Diversity in Automatic Poetry Generation](http://arxiv.org/abs/2406.15267)|null|自然语言生成 (NLG)，更广泛地说，生成式人工智能，是目前最具影响力的研究领域之一。创意性 NLG，例如自动诗歌生成，是该领域一个引人入胜的细分领域。虽然大多数先前的研究在评估自动诗歌生成时都集中在图灵测试的形式上——人类是否能够区分自动生成的诗歌和人类创作的诗歌——但我们通过比较生成诗歌和人类诗歌在结构、词汇、语义和风格维度上的分布来评估自动生成诗歌的多样性，评估不同的模型类型（词级与字符级，通用大型语言模型与诗歌专用模型），包括最近的 LLaMA3，以及微调的类型（条件化与非条件化）。我们发现，当前的自动诗歌系统在多个维度上的多样性明显不足——它们往往押韵不足，语义过于单一，甚至不符合人类诗歌的长度分布。然而，我们的实验表明，风格条件化和字符级建模明显增加了我们探索的几乎所有维度的多样性。我们发现的局限性可以作为未来生成更真正多样化诗歌模型的基础。||
|**2024-06-21**|[Fingerprint Membership and Identity Inference Against Generative Adversarial Networks](http://arxiv.org/abs/2406.15253)|null|生成模型作为新一轮工业革命的潜在催化剂，正受到越来越多的关注。由于自动样本生成有助于解决通常影响学习型生物特征模型的隐私和数据稀缺问题，因此此类技术在该领域得到广泛应用。在本文中，我们通过设计和测试针对由生成对抗网络创建的指纹数据集的身份推理攻击，评估了生成机器学习模型在身份保护方面的漏洞。实验结果表明，所提出的解决方案在不同配置下均被证明是有效的，并且很容易扩展到其他生物特征测量。||
|**2024-06-21**|[MantisScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation](http://arxiv.org/abs/2406.15252)|null|近年来，视频生成技术取得了巨大进步。然而，自动视频指标的发展却严重滞后。现有的指标都不能对生成的视频提供可靠的评分。主要障碍是缺乏大规模的人工标注数据集。在本文中，我们发布了 VideoFeedback，这是第一个包含 37.6 万个合成视频的多方面人工评分的大规模数据集，这些视频来自 11 个现有的视频生成模型。我们基于 VideoFeedback 训练了 MantisScore（从 Mantis 初始化），使其能够进行自动视频质量评估。实验表明，MantisScore 与人类在 VideoFeedback 测试集上的 Spearman 相关性可以达到 77.1，比之前最好的指标高出约 50 个点。在其他未公开的 EvalCrafter、GenAI-Bench 和 VBench 上的进一步结果表明，MantisScore 与人类评估者的一致性始终高于其他指标。基于这些结果，我们认为 MantisScore 可以作为人类评估者的良好代理，用于 (1) 对不同的视频模型进行评分以跟踪进度 (2) 在人类反馈强化学习 (RLHF) 中模拟细粒度的人类反馈，以改进当前的视频生成模型。||
|**2024-06-21**|[Unsupervised Bayesian Generation of Synthetic CT from CBCT Using Patient-Specific Score-Based Prior](http://arxiv.org/abs/2406.15219)|null|背景：锥形束计算机断层扫描 (CBCT) 在图像引导放射治疗 (IGRT) 过程中常被用于患者摆位，例如每天或每周进行扫描，因此它成为实施自适应放射治疗 (ART) 方案的潜在成像方式。然而，CBCT图像中存在的严重伪影和不准确的亨氏单位 (HU) 值阻碍了其在靶区和器官分割以及剂量计算等定量任务中的应用。因此，从 CBCT 扫描中获取 CT 质量的图像对于在临床环境中实施在线 ART 至关重要。目的：本研究旨在开发一种基于患者特定扩散模型的无监督学习方法，用于生成基于 CBCT 的合成 CT (sCT)，以提高 CBCT 图像质量。方法：所提出的方法采用无监督框架，利用患者特定的基于分数的模型作为图像先验，并结合定制的全变分 (TV) 正则化来增强不同横断面切片之间的一致性。该基于分数的模型使用同一患者的计划 CT (pCT) 图像进行无条件训练，以表征 CT 质量图像的流形并捕获特定患者的独特解剖信息。在包括头颈部 (H&N) 癌、胰腺癌和肺癌在内的解剖部位图像上评估了该方法的有效性。使用定量指标评估了所提出的 CBCT 校正方法的性能，包括平均绝对误差 (MAE)、峰值信噪比 (PSNR) 和归一化互相关 (NCC)。此外，将所提出的算法与其他两种基于无监督扩散模型的 CBCT 校正算法进行了基准测试。||
|**2024-06-21**|[Injecting Bias in Text-To-Image Models via Composite-Trigger Backdoors](http://arxiv.org/abs/2406.15213)|null|Recent advances in large text-conditional image generative models such as Stable Diffusion, Midjourney, and DALL-E 3 have revolutionized the field of image generation, allowing users to produce high-quality, realistic images from textual prompts. While these developments have enhanced artistic creation and visual communication, they also present an underexplored attack opportunity: the possibility of inducing biases by an adversary into the generated images for malicious intentions, e.g., to influence society and spread propaganda. In this paper, we demonstrate the possibility of such a bias injection threat by an adversary who backdoors such models with a small number of malicious data samples; the implemented backdoor is activated when special triggers exist in the input prompt of the backdoored models. On the other hand, the model's utility is preserved in the absence of the triggers, making the attack highly undetectable. We present a novel framework that enables efficient generation of poisoning samples with composite (multi-word) triggers for such an attack. Our extensive experiments using over 1 million generated images and against hundreds of fine-tuned models demonstrate the feasibility of the presented backdoor attack. We illustrate how these biases can bypass conventional detection mechanisms, highlighting the challenges in proving the existence of biases within operational constraints. Our cost analysis confirms the low financial barrier to executing such attacks, underscoring the need for robust defensive strategies against such vulnerabilities in text-to-image generation models.||
|**2024-06-21**|[Generative Topological Networks](http://arxiv.org/abs/2406.15152)|null|近年来，生成模型取得了重大进展，但训练和使用通常仍然具有挑战性和成本高昂。我们引入了生成拓扑网络 (GTN)——这是一类解决这些缺点的新型生成模型。GTN 使用基于拓扑理论的简单监督学习方法进行确定性训练。GTN 训练速度快，只需在标准前馈神经网络中进行一次前向传递即可生成样本。我们在多个数据集中展示了 GTN 的优势，包括 MNIST、celebA 以及手和手掌图像数据集。最后，GTN 背后的理论为如何训练生成模型以提高性能提供了见解。||
|**2024-06-21**|[Investigating the impact of 2D gesture representation on co-speech gesture generation](http://arxiv.org/abs/2406.15111)|null|共同语音手势在人类与具身对话代理（ECA）的交互中起着至关重要的作用。最近的深度学习方法能够生成与语音同步的逼真、自然的共同语音手势，但此类方法需要大量的训练数据。 “自然环境”数据集通过人体姿态估计模型从 YouTube 等来源编译视频，提供了一种解决方案，提供与语音配对的 2D 骨架序列。同时，出现了创新的提升模型，能够将这些 2D 姿势序列转换为其 3D 对应物，从而产生大型且多样化的 3D 手势数据集。然而，派生的 3D 姿势估计本质上是一个伪真实值，实际真实值是 2D 运动数据。这种区别引发了关于手势表示维度对生成运动质量的影响的问题，据我们所知，这个主题在很大程度上仍未得到探索。在这项工作中，我们评估了训练数据的维度（2D 或 3D 关节坐标）对多模态语音到手势深度生成模型性能的影响。我们使用提升模型将 2D 生成的身体姿势序列转换为 3D。然后，我们将直接在 3D 中生成的手势序列与在 2D 中生成并提升到 3D 作为后处理的手势进行比较。||
|**2024-06-21**|[Improving Interpretability and Robustness for the Detection of AI-Generated Images](http://arxiv.org/abs/2406.15035)|null|随着生成模型能力的不断增强，人工智能生成内容检测成为一项日益重要且困难的任务。然而，所有流行的解决方法都存在跨领域和生成模型泛化能力差的问题。本研究重点关注人工智能生成图像 (AIGI) 检测器的鲁棒性。我们分析了现有的基于冻结 CLIP 嵌入的 AIGI 检测方法，并展示了如何解释它们，揭示了各种人工智能生成器生成的图像与真实图像之间的差异。接下来，我们提出了两种提高鲁棒性的方法：基于去除嵌入向量中有害成分的方法和基于选择图像编码器模型中性能最佳的注意力头的方法。我们的方法将跨模型迁移的平均分布外 (OOD) 分类得分提高了 6%。我们还提出了一个新的 AIGI 检测数据集，并将其用于评估；我们相信该数据集将有助于推动进一步的研究。数据集和代码作为补充提供。||
|**2024-06-20**|[A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models](http://arxiv.org/abs/2406.14555)|**[link](https://github.com/xinchengshuai/awesome-image-editing)**|图像编辑旨在编辑给定的合成图像或真实图像，以满足用户的特定需求。近年来，作为人工智能生成内容（AIGC）领域中一个充满希望和挑战性的方向，图像编辑得到了广泛的研究。该领域最近取得的重大进展是基于文本到图像 (T2I) 扩散模型的发展，该模型根据文本提示生成图像。这些模型展示了卓越的生成能力，并已成为广泛使用的图像编辑工具。基于 T2I 的图像编辑方法显著提高了编辑性能，并为修改多模态输入引导的内容提供了用户友好的界面。在本次调查中，我们全面回顾了利用 T2I 扩散模型的多模态引导图像编辑技术。首先，我们从整体角度定义图像编辑的范围，并详细介绍各种控制信号和编辑场景。然后，我们提出了一个统一的框架来形式化编辑过程，将其分为两个主要的算法系列。该框架为用户提供了一个设计空间来实现特定目标。随后，我们对该框架内的每个组件进行了深入分析，检查了不同组合的特性和适用场景。鉴于基于训练的方法学会了在用户指导下将源图像直接映射到目标图像，我们分别讨论了它们，并介绍了在不同场景下源图像的注入方案。此外，我们回顾了二维技术在视频编辑中的应用，重点介绍了帧间不一致的解决方案。最后，我们讨论了该领域的开放挑战，并提出了未来可能的研究方向。我们将持续跟踪相关工作，网址为https://github.com/xinchengshuai/Awesome-Image-Editing。||
|**2024-06-20**|[Advancing Fine-Grained Classification by Structure and Subject Preserving Augmentation](http://arxiv.org/abs/2406.14551)|**[link](https://github.com/eyalmichaeli/saspa-aug)**|细粒度视觉分类 (FGVC) 涉及对密切相关的子类别进行分类。由于类别之间的细微差异和较高的类内差异，这项任务非常困难。此外，FGVC 数据集通常很小，而且难以收集，因此迫切需要有效的数据增强方法。文本到图像扩散模型的最新进展为增强分类数据集提供了新的可能性。虽然这些模型已被用于生成分类任务的训练数据，但它们在 FGVC 模型的全数据集训练中的有效性仍未得到充分探索。最近依赖于 Text2Image 生成或 Img2Img 方法的技术通常难以生成既能准确表示类别又能对其进行修改以显著增加数据集多样性的图像。为了应对这些挑战，我们提出了 SaSPA：结构和主题保留增强。与最近的方法相反，我们的方法不使用真实图像作为指导，从而提高了生成灵活性并促进了更大的多样性。为了确保准确的类别表示，我们采用了调节机制，特别是通过调节图像边缘和主题表示。我们进行了广泛的实验，并将 SaSPA 与传统的和最近的生成数据增强方法进行了基准测试。SaSPA 在多种设置（包括全数据集训练、上下文偏差和少样本分类）中始终优于所有已建立的基线。此外，我们的结果揭示了将合成数据用于 FGVC 模型的有趣模式；例如，我们发现了使用的真实数据量与合成数据的最佳比例之间的关系。代码可在 https://github.com/EyalMichaeli/SaSPA-Aug 获取。||
|**2024-06-20**|[Consistency Models Made Easy](http://arxiv.org/abs/2406.14548)|**[link](https://github.com/locuslab/ect)**|一致性模型 (CM) 是一类新兴的生成模型，与传统的扩散模型相比，它们能够提供更快的采样速度。CM 强制要求采样轨迹上的所有点都映射到相同的初始点。但这一目标导致了资源密集型的训练：例如，截至 2024 年，在 CIFAR-10 上训练一个 SoTA CM 需要 8 个 GPU 运行一周。在这项工作中，我们提出了一种训练 CM 的替代方案，极大地提高了构建此类模型的效率。具体来说，通过使用特定的微分方程表示 CM 轨迹，我们认为扩散模型可以被视为具有特定离散化的 CM 的特例。因此，我们可以从预先训练的扩散模型开始微调一致性模型，并在训练过程中逐步将完全一致性条件逼近到更强的程度。我们提出的方法称为简易一致性调整 (ECT)，它极大地缩短了训练时间，同时确实提高了先前方法的质量：例如，ECT 在单个 A100 GPU 上仅用 1 小时即可在 CIFAR10 上实现 2.73 的 2 步 FID，与经过数百 GPU 小时训练的一致性蒸馏相匹配。由于这种计算效率，我们研究了 ECT 下 CM 的缩放规律，表明它们似乎遵循经典的幂律缩放，暗示了它们在更大规模上提高效率和性能的能力。代码 (https://github.com/locuslab/ect) 已开源。||
|**2024-06-20**|[IRASim: Learning Interactive Real-Robot Action Simulators](http://arxiv.org/abs/2406.14540)|null|在现实世界中，可扩展的机器人学习受到真实机器人成本和安全问题的限制。此外，在现实世界中部署机器人轨迹可能非常耗时且劳动强度大。在本文中，我们建议学习一种交互式真实机器人动作模拟器作为替代方案。我们介绍了一种新方法 IRASim，它利用生成模型的力量来生成极其逼真的机器人手臂视频，该视频从给定的初始帧开始执行给定的动作轨迹。为了验证我们方法的有效性，我们基于三个真实机器人数据集创建了一个新的基准测试 IRASim Benchmark，并在该基准测试上进行了广泛的实验。结果表明，IRASim 优于所有基线方法，并且在人类评估中更受欢迎。我们希望 IRASim 可以作为一种有效且可扩展的方法来增强现实世界中的机器人学习。为了促进对生成式真实机器人动作模拟器的研究，我们在 https://gen-irasim.github.io 上开源了代码、基准测试和检查点。||
|**2024-06-20**|[Invertible Consistency Distillation for Text-Guided Image Editing in Around 7 Steps](http://arxiv.org/abs/2406.14539)|null|扩散模型是通过少量采样步骤实现逼真文本到图像生成的很有前途的方向。 然而，尽管最近取得了成功，但现有的蒸馏模型仍然不能提供全面的扩散能力，例如真实图像反演，而反演能力是许多精确图像处理方法的基础。 这项工作旨在丰富蒸馏文本到图像扩散模型，使其能够有效地将真实图像编码到其潜在空间中。 为此，我们引入了可逆一致性蒸馏 (iCD)，这是一种通用的 consistency distillation 框架，可以在仅 3-4 个推理步骤中促进高质量的图像合成和准确的图像编码。 尽管高分类器引导尺度加剧了文本到图像扩散模型的反演问题，但我们注意到动态引导显着减少了重建误差，而生成性能没有明显下降。 因此，我们证明了配备动态引导的 iCD 可以作为零样本文本引导图像编辑的高效工具，与更昂贵的现有最佳方案相比具有竞争力。||
|**2024-06-20**|[Fantastic Copyrighted Beasts and How (Not) to Generate Them](http://arxiv.org/abs/2406.14526)|null|最近的研究表明，图像和视频生成模型可能会被提示从其训练数据中复制受版权保护的内容，引发了围绕版权侵权的严重法律问题。尤其是受版权保护的角色，对图像生成服务提出了艰巨的挑战，至少有一起诉讼已经基于生成这些角色而判给损害赔偿。然而，很少有研究对这个问题进行实证检验。我们进行了系统的评估来填补这一空白。首先，我们构建了 CopyCat，这是一个评估套件，包含各种受版权保护的角色和一个新颖的评估流程。我们的评估同时考虑了与受版权保护角色的相似性检测以及生成的图像与用户输入的一致性。我们的评估系统地表明，即使在提示中没有明确提及角色名称的情况下，图像和视频生成模型仍然可以生成角色，有时只需两个通用关键字（例如，使用“电子游戏、水管工”提示始终会生成任天堂的马里奥角色）。然后，我们介绍了半自动识别此类关键字或描述的技术，这些关键字或描述会触发角色生成。我们使用评估套件研究了运行时缓解策略，包括现有方法和我们提出的新策略。我们的研究结果表明，常用的策略（例如 DALL-E 系统中的提示重写）不足以作为独立的防护措施。这些策略必须与其他方法（如负面提示）相结合，才能有效减少意外生成受版权保护的角色。我们的工作为版权缓解策略的讨论提供了经验证据，并为积极实施这些策略的模型部署者提供了可操作的见解。||
|**2024-06-20**|[V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data](http://arxiv.org/abs/2406.14510)|null|基于扩散的生成模型最近在图像和视频编辑方面展现出非凡的能力。然而，局部视频编辑，特别是去除眼镜等小属性，仍然是一个挑战。现有方法要么过度改变视频，要么产生不真实的伪影，要么无法在整个视频中一致地执行请求的编辑。在这项工作中，我们专注于一致且保留身份的视频眼镜去除，并将其作为视频中一致的局部属性去除的案例研究。由于缺乏配对数据，我们采用弱监督方法，并使用调整后的预训练扩散模型生成合成的非完美数据。我们表明，尽管数据不完美，但通过学习我们生成的数据并利用预训练扩散模型的先验知识，我们的模型能够在保留原始视频内容的同时一致地执行所需的编辑。此外，我们通过将我们的方法成功应用于面部贴纸去除，例证了其对其他局部视频编辑任务的泛化能力。我们的方法证明了相对于现有方法的显着改进，展示了利用合成数据和强大的视频先验知识进行局部视频编辑任务的潜力。||
|**2024-06-20**|[CodeRAG-Bench: Can Retrieval Augment Code Generation?](http://arxiv.org/abs/2406.14497)|**[link](https://github.com/code-rag-bench/code-rag-bench)**|虽然语言模型（LM）在生成代码方面表现出色，但许多程序对于仅使用参数化知识的LM来说难以生成。提供外部上下文（如库文档）可以帮助生成准确且可用的代码。尽管检索增强生成（RAG）在各种面向文本的任务中取得了成功，但其在改进代码生成方面的潜力仍未得到充分探索。在这项工作中，我们进行了系统的、大规模的分析，提出以下问题：在什么情况下检索可以使代码生成模型受益？以及还存在哪些挑战？我们首先策划了一个全面的评估基准，CodeRAG-Bench，涵盖了三类代码生成任务，包括基础编程、开放领域和代码库级别的难题。我们汇总了五个来源的文档供模型检索上下文：竞赛解决方案、在线教程、库文档、StackOverflow 帖子和 GitHub 代码库。我们通过提供从一个或多个来源检索到的上下文，来检查 CodeRAG-Bench 上表现最佳的模型。虽然通过在各种设置中检索高质量的上下文，最终代码生成取得了显著的进步，但我们的分析表明仍有改进的空间——当前的检索器仍然难以获取有用的上下文，尤其是在词汇重叠有限的情况下，而生成器在上下文长度有限或整合额外上下文的能力有限的情况下无法改进。我们希望 CodeRAG-Bench 能够成为一个有效的测试平台，鼓励进一步开发先进的面向代码的 RAG 方法。||
|**2024-06-20**|[SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset](http://arxiv.org/abs/2406.14477)|**[link](https://github.com/pku-alignment/safe-sora)**|为了降低大型视觉模型 (LVM) 产生有害输出的风险，我们引入了 SafeSora 数据集，旨在促进文本到视频生成与人类价值观相一致的研究。该数据集涵盖了文本到视频生成任务中人类偏好的两个主要维度：有用性和无害性。为了深入捕捉人类偏好并促进众包工作者的结构化推理，我们将有用性细分为 4 个子维度，将无害性细分为 12 个子类别，作为试点标注的基础。SafeSora 数据集包含 14,711 个独特的提示、由 4 个不同的 LVM 生成的 57,333 个独特的视频以及 51,691 对由人类标记的偏好注释。我们通过几个应用进一步证明了 SafeSora 数据集的效用，包括训练文本-视频审核模型以及通过微调提示增强模块或扩散模型使 LVM 与人类偏好保持一致。这些应用突出了其作为文本到视频对齐研究基础的潜力，例如人类偏好建模以及对齐算法的开发和验证。||
|**2024-06-20**|[CollaFuse: Collaborative Diffusion Models](http://arxiv.org/abs/2406.14429)|**[link](https://github.com/simeonallmendinger/collafuse)**|在生成式人工智能领域，基于扩散的模型已成为一种很有前景的合成图像生成方法。然而，扩散模型的应用面临着诸多挑战，特别是在数据可用性、计算需求和隐私方面。传统的解决这些缺陷的方法，如联邦学习，通常会给单个客户端带来沉重的计算负担，尤其是那些资源受限的客户端。为了应对这些挑战，我们引入了一种受拆分学习启发的新型分布式协作扩散模型方法。我们的方法促进了扩散模型的协作训练，同时减轻了图像合成过程中客户端的计算负担。这种计算负担的减少是通过将数据和计算成本低廉的流程保留在每个客户端本地，同时将计算成本高昂的流程外包给共享的、更高效的服务器资源来实现的。通过对常见 CelebA 数据集的实验，我们的方法通过减少共享原始数据的必要性来增强隐私。这些功能在各种应用领域，包括边缘计算解决方案的设计，都具有巨大的潜力。因此，我们的工作通过促进协作扩散模型的发展，推动了分布式机器学习的进步。||
|**2024-06-18**|[Evaluating the design space of diffusion-based generative models](http://arxiv.org/abs/2406.12839)|null|大多数现有的关于扩散模型精度理论研究虽然意义重大，但都假设分数函数已被逼近到一定精度，然后使用这个先验界限来控制生成误差。而本文则对整个生成过程，即训练和采样，提供了第一个量化的理解。更准确地说，它对梯度下降下的去噪分数匹配进行了非渐近收敛性分析。此外，还对变异爆炸模型的采样误差进行了改进分析。这两个结果的结合产生了一个完整的误差分析，它阐明了（同样是，但这次是从理论上）如何设计训练和采样过程以实现有效的生成。例如，我们的理论暗示了对噪声分布和损失权重的偏好，这与[Karras et al. 2022]中使用的定性一致。它还提供了一些关于为什么[Karras et al. 2022]中使用的时间和方差调度可以比[Song et al. 2020]中的先驱版本更好地调整的观点。||
|**2024-06-18**|[Neural Approximate Mirror Maps for Constrained Diffusion Models](http://arxiv.org/abs/2406.12816)|null|扩散模型擅长创建视觉上有说服力的图像，但它们往往难以满足训练数据中固有的细微约束。这些约束可以是基于物理的（例如，满足偏微分方程）、几何的（例如，保持对称性）或语义的（例如，包含特定数量的对象）。当训练数据都满足某个约束时，在扩散模型上强制执行此约束不仅可以提高其分布匹配精度，还可以使其在生成有效的合成数据和解决约束逆问题方面更加可靠。然而，现有的约束扩散模型方法对于不同类型的约束缺乏灵活性。最近的研究提出在由镜像映射定义的无约束空间中学习镜像扩散模型 (MDM)，并使用逆镜像映射施加约束，但对于复杂的约束，解析镜像映射难以推导。我们针对一般约束提出了神经近似镜像映射 (NAMM)。我们的方法只需要约束集的可微距离函数。我们学习一个将数据推送到无约束空间的近似镜像映射，以及一个将数据映射回约束集的相应近似逆映射。然后，可以在学习的镜像空间中训练生成模型（例如 MDM），并通过逆映射将其样本恢复到约束集。我们在各种约束条件下验证了我们的方法，结果表明，与无约束扩散模型相比，基于 NAMM 的 MDM 显着提高了约束满足度。我们还演示了如何轻松地将现有的基于扩散的逆问题求解器应用于学习的镜像空间，以解决约束逆问题。||
|**2024-06-18**|[AITTI: Learning Adaptive Inclusive Token for Text-to-Image Generation](http://arxiv.org/abs/2406.12805)|**[link](https://github.com/itsmag11/aitti)**|尽管文本到图像生成取得了高质量的结果，但在生成的内容中发现了刻板印象，这损害了生成模型的公平性。在这项工作中，我们建议学习自适应包容性标记来改变最终生成输出的属性分布。与现有的去偏方法不同，我们的方法既不需要明确的属性规范，也不需要预先了解偏差分布。具体来说，我们方法的核心是一个轻量级的自适应映射网络，它可以为要去偏的概念定制包容性标记，使标记可以推广到未见过的概念，而不管其原始偏差分布如何。这是通过使用锚定损失用少量平衡且包容的样本调整自适应映射网络来实现的。实验结果表明，我们的方法优于以前没有属性规范的偏差缓解方法，同时保留了生成结果和文本描述之间的一致性。此外，我们的方法实现了与需要特定属性或编辑方向才能生成的模型相当的性能。大量实验表明，我们的自适应包容性标记在减轻文本到图像生成中的刻板印象方面非常有效。代码将在 https://github.com/itsmag11/AITTI 上提供。||
|**2024-06-18**|[Extracting Training Data from Unconditional Diffusion Models](http://arxiv.org/abs/2406.12752)|null|随着扩散概率模型 (DPM) 作为生成式人工智能 (AI) 的主流模型得到应用，对其原始训练数据记忆的研究也引起了越来越多的关注。该方向上的现有工作旨在了解 DPM 是否或在多大程度上通过记忆进行学习。这种理解对于识别扩散模型中数据泄露和版权侵权的潜在风险，以及更重要的是，对于更可控地生成和可信地应用人工智能生成内容 (AIGC) 至关重要。尽管先前的工作已经对 DPM 何时容易发生记忆做出了重要观察，但这些发现大多是经验性的，并且开发的数据提取方法仅适用于条件扩散模型。在这项工作中，我们旨在通过以下方面建立对 DPM 记忆的理论理解：1) 用于理论分析的记忆度量，2) 对具有信息标签和随机标签的条件记忆的分析，以及 3) 两个用于衡量记忆的更好评估指标。基于理论分析，我们进一步提出了一种名为“代理条件数据提取 (SIDE)”的新型数据提取方法，该方法利用在生成数据上训练的分类器作为代理条件，直接从无条件扩散模型中提取训练数据。我们的实验结果表明，SIDE 可以从以前方法失败的扩散模型中提取训练数据，并且在不同规模的 CelebA 数据集上平均效率提高了 50% 以上。||
|**2024-06-18**|[SUPER: Selfie Undistortion and Head Pose Editing with Identity Preservation](http://arxiv.org/abs/2406.12700)|null|由于严重的扭曲导致面部特征变形以及头部姿势不当，近距离拍摄的自拍照可能看起来不自然甚至不美观。在本文中，我们提出了 SUPER，这是一种消除近距离面部裁剪中扭曲和调整头部姿势的新方法。我们通过优化相机参数和面部潜在代码对面部图像执行 3D GAN 反演，从而生成图像。此外，我们从获得的潜在代码估计深度，创建深度诱导的 3D 网格，并使用更新的相机参数对其进行渲染以获得扭曲的肖像。最后，我们应用基于可见性的混合，以便重新投影可见区域，并使用生成模型恢复遮挡部分。在面部去扭曲基准数据集和我们自己收集的头部旋转数据集 (HeRo) 上的实验表明，SUPER 在质量和数量上都优于以前的方法，为逼真的自拍编辑开辟了新的可能性。||
|**2024-06-18**|[Speak in the Scene: Diffusion-based Acoustic Scene Transfer toward Immersive Speech Generation](http://arxiv.org/abs/2406.12688)|null|本文介绍了语音生成领域的一项新任务，即声场景迁移（AST），旨在将语音信号的声场景迁移到不同的环境中。AST 通过将语音信号背后的声场景调整到所需的环境，有望在语音感知方面带来沉浸式体验。我们针对 AST 任务提出了 AST-LDM，它可以生成伴随参考提示目标声场景的语音信号。具体来说，AST-LDM 是一种潜在扩散模型，它以描述音频或文本模态目标声场景的 CLAP 嵌入为条件。本文的贡献包括引入 AST 任务并实现其基线模型。对于 AST-LDM，我们强调其核心框架，即保留输入语音并生成与给定语音和目标声环境一致的音频。包括客观和主观测试在内的实验验证了我们方法的可行性和有效性。||
|**2024-06-18**|[GeoBench: Benchmarking and Analyzing Monocular Geometry Estimation Models](http://arxiv.org/abs/2406.12671)|**[link](https://github.com/aim-uofa/geobench)**|近年来，判别性和生成性预训练的进步已经产生了具有强大泛化能力的几何估计模型。虽然判别性单目几何估计方法依赖于大规模微调数据来实现零样本泛化，但一些基于生成的方法通过利用预训练的扩散模型并在小规模合成训练数据上进行微调，显示出在未见场景中实现令人印象深刻的泛化性能的潜力。令人沮丧的是，这些模型在不同的数据集上使用不同的方法进行训练，因此很难找出决定评估性能的关键因素。此外，当前的几何评估基准存在两个主要缺点，可能会阻碍该领域的发展，即场景多样性有限和标签质量不佳。为了解决上述问题，（1）我们在统一的代码库中构建了公平且强大的基线，用于评估和分析几何估计模型；（2）我们在更具挑战性的几何估计任务基准上评估单目几何估计器，这些基准具有多样化的场景和高质量的标注。我们的结果表明，在相同的训练配置下，使用大量数据进行预训练的判别性模型（如 DINOv2）可以优于生成模型，即使后者使用少量高质量的合成数据，这表明微调数据质量比数据规模和模型架构更重要。我们的观察结果还提出了一个问题：如果仅使用少量合成深度数据微调 DINOv2 等通用视觉模型就能产生最先进的结果，那么我们真的需要复杂的生成模型来进行深度估计吗？我们相信这项工作可以推动几何估计任务以及广泛的下游应用的进步。||
|**2024-06-18**|[Research and Implementation of Data Enhancement Techniques for Graph Neural Networks](http://arxiv.org/abs/2406.12640)|null|数据、算法和算力是深度学习在应用领域取得成效的三个基础条件，数据是发展深度学习算法的重点。在实际工程应用中，一些数据受到获取条件的制约，无法获取更多的数据或者获取数据的成本过高，导致数据集规模较小（一般几百到几千个），数据量远小于大数据集的规模（几万到几十万个）。以上两种方法都是基于原始数据集进行生成，在原始数据量不足的情况下，可能无法反映真实环境的所有情况，例如真实环境的光照、轮廓等信息，如果数据量不够，则难以使用简单的变换或神经网络生成模型来生成所需的数据。本文的研究首先分析了图神经网络数据增强技术的关键点，同时深入介绍了图神经网络的组成基础，在此基础上对图神经网络的数据增强技术进行了优化和分析。||
|**2024-06-18**|[Learning Diffusion at Lightspeed](http://arxiv.org/abs/2406.12616)|null|扩散现象调节着大量自然过程以及许多成功生成模型的动力学。现有的从观测数据中学习扩散项的模型依赖于复杂的两级优化问题，并且只能正确地模拟系统的漂移。我们提出了一种新的简单模型JKOnet*，它完全绕过了现有架构的复杂性，同时呈现出显著增强的表示能力：JKOnet*可以恢复潜在的、交互的以及内部能量成分的底层扩散过程。JKOnet*最小化了一个简单的二次损失，以极快的速度运行，并且在实践中大大优于其他基线。此外，对于线性参数化泛函，JKOnet*提供了一个封闭形式的最优解。我们的方法论基于将扩散过程解释为概率空间中通过所谓的JKO方案实现的能量最小化轨迹，我们根据其一阶最优性条件，并根据概率空间优化方面的最新进展对其进行研究。||
|**2024-06-18**|[Unmasking the Veil: An Investigation into Concept Ablation for Privacy and Copyright Protection in Images](http://arxiv.org/abs/2406.12592)|**[link](https://github.com/vlgiitr/unmasking-the-veil)**|在本文中，我们扩展了对预训练模型中概念消融的研究，如 (Kumari et al.,2022) 在“文本到图像扩散模型中的概念消融”一文中所述。我们的工作重点是重现通过预定义指标提出和验证的不同概念消融变体所取得的结果。我们还介绍了一种新的概念消融变体，即“商标消融”。该变体结合了记忆和实例消融的原理，以解决专有或品牌元素对模型输出的细微影响。此外，我们的研究贡献还包括对模型局限性的观察分析。此外，我们还研究了模型对消融泄漏诱导提示的响应行为，该提示旨在间接消融概念，从而揭示模型的弹性和适应性。我们还观察到，对于远离其目标消融概念的概念所生成的图像，模型的性能会下降，这在附录中有所记录。||
|**2024-06-17**|[Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%](http://arxiv.org/abs/2406.11837)|**[link](https://github.com/zh460045050/vqgan-lc)**|在以 VQGAN 为代表的图像量化领域中，该过程将图像编码为从具有预定义大小的码本中提取的离散token。近期的进展，特别是 LLAMA 3，表明扩大码本可以显著提高模型性能。然而，VQGAN 及其衍生模型，如 VQGAN-FC（因子化码本）和 VQGAN-EMA，在扩大码本大小和提高码本利用率方面仍然面临挑战。例如，VQGAN-FC 被限制学习最大大小为 16,384 的码本，在 ImageNet 上的利用率通常低于 12%。在这项工作中，我们提出了一种名为 VQGAN-LC（大型码本）的新型图像量化模型，它将码本大小扩展到 100,000，实现了超过 99% 的利用率。与之前优化每个码本条目的方法不同，我们的方法首先使用由预训练视觉编码器提取的 100,000 个特征初始化码本。然后，优化集中于训练一个投影器，该投影器使整个码本与 VQGAN-LC 中编码器的特征分布对齐。我们证明了我们的模型在各种任务中优于其 counterparts，包括图像重建、图像分类、使用 GPT 的自回归图像生成，以及使用基于扩散和基于流的生成模型进行图像创建。代码和模型可在 https://github.com/zh460045050/VQGAN-LC 获取。||
|**2024-06-17**|[RetinaGS: Scalable Training for Dense Scene Rendering with Billion-Scale 3D Gaussians](http://arxiv.org/abs/2406.11836)|null|在这项工作中，我们探索了在大规模、高分辨率数据集上训练高参数 3D 高斯样条 (3DGS) 模型的可能性。我们为 3DGS 设计了一种通用的模型并行训练方法，称为 RetinaGS，它使用适当的渲染方程，可以应用于任何场景和任意分布的高斯基元。它使我们能够探索 3DGS 在基元数量和训练分辨率方面的缩放行为，这些行为以前难以探索，并超越了先前最先进的重建质量。当使用我们的方法增加基元数量时，我们观察到视觉质量明显提高的积极趋势。我们还首次尝试在完整 MatrixCity 数据集上训练具有超过 10 亿个基元的 3DGS 模型，该模型获得了良好的视觉质量。||
|**2024-06-17**|[Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models](http://arxiv.org/abs/2406.11831)|null|基于解码器-only transformer的大型语言模型（LLM）在文本理解能力方面已经表现出优于CLIP和T5系列模型的能力。然而，在文本到图像扩散模型中利用当前先进LLM的范式仍有待探索。我们观察到一个不寻常的现象：直接使用大型语言模型作为prompt编码器会显著降低图像生成中的prompt遵循能力。我们确定了造成这个问题背后的两个主要障碍。一个是LLM中下一个token预测训练与扩散模型中对判别性prompt特征的要求之间的不匹配。另一个是由解码器-only架构引入的固有位置偏差。为了解决这个问题，我们提出了一个新的框架来充分利用LLM的能力。通过精心设计的使用指南，我们有效地增强了用于prompt编码的文本表示能力，并消除了其固有的位置偏差。这使我们能够灵活地将最先进的LLM集成到文本到图像生成模型中。此外，我们还提供了一种将多个LLM融合到我们框架中的有效方法。考虑到transformer架构所展现出的出色性能和扩展能力，我们进一步设计了一个基于该框架的LLM-Infused Diffusion Transformer（LI-DiT）。我们进行了广泛的实验，以验证LI-DiT在模型规模和数据规模上的有效性。受益于LLM的固有能力和我们的创新设计，LI-DiT的prompt理解性能轻松超越了最先进的开源模型以及包括Stable Diffusion 3、DALL-E 3和Midjourney V6在内的主流闭源商业模型。功能强大的LI-DiT-10B将在进一步优化和安全检查后发布。||
|**2024-06-17**|[MegaScenes: Scene-Level View Synthesis at Scale](http://arxiv.org/abs/2406.11819)|null|场景级新视角合成（NVS）是许多视觉和图形应用的基础。最近，姿势条件扩散模型通过从二维基础模型中提取三维信息取得了显著进展，但这些方法受到缺乏场景级训练数据的限制。常见的数据库选择要么包含孤立的对象（Objaverse），要么包含具有有限姿势分布的以对象为中心的场景（DTU、CO3D）。在本文中，我们利用互联网照片集创建了一个大型场景级数据库，称为MegaScenes，其中包含来自世界各地的超过10万个运动结构（SfM）重建。互联网照片代表了一种可扩展的数据源，但也带来了光照和瞬态对象等挑战。我们解决了这些问题，进一步创建了一个适合NVS任务的子集。此外，我们分析了最先进的NVS方法的失败案例，并显著提高了生成的一致性。通过大量的实验，我们验证了我们的数据库和方法在生成真实场景中的有效性。有关数据库和代码的详细信息，请参见我们的项目页面：https://megascenes.github.io。||
|**2024-06-17**|[DiffMM: Multi-Modal Diffusion Model for Recommendation](http://arxiv.org/abs/2406.11781)|**[link](https://github.com/hkuds/diffmm)**|像抖音和YouTube这样的在线多模态分享平台的兴起使得个性化推荐系统能够将多种模态（如视觉、文本和音频）纳入用户表示中。然而，解决这些系统中的数据稀疏性挑战仍然是一个关键问题。为了解决这一局限性，最近的研究引入了自监督学习技术来增强推荐系统。然而，这些方法通常依赖于简单的随机增强或直观的跨视图信息，这可能会引入不相关的噪声，并且无法准确地将多模态上下文与用户-项目交互建模相一致。为了填补这一研究空白，我们提出了一种新的用于推荐的多模态图扩散模型，称为DiffMM。我们的框架将模态感知图扩散模型与跨模态对比学习范式相结合，以改进模态感知用户表示学习。这种集成促进了多模态特征信息与协同关系建模之间更好的对齐。我们的方法利用扩散模型的生成能力自动生成一个感知不同模态的用户-项目图，从而促进在用户-项目交互建模中纳入有用的多模态知识。我们在三个公开数据集上进行了广泛的实验，一致证明了我们的DiffMM相对于各种竞争基线的优越性。有关开源模型实现的详细信息，您可以访问以下网址获取我们提出的框架的源代码：https://github.com/HKUDS/DiffMM。||
|**2024-06-17**|[Transcendence: Generative Models Can Outperform The Experts That Train Them](http://arxiv.org/abs/2406.11741)|null|生成模型的训练目标很简单，即模仿其训练数据所诱导的条件概率分布。因此，当使用人类生成的数据进行训练时，我们不能期望人工智能模型在其原始目标上超越人类。在这项工作中，我们研究了超越现象：即生成模型实现的能力超过生成其数据的专家的能力。我们通过训练一个自回归Transformer从棋谱中学习下棋来证明超越现象，并表明训练后的模型有时可以取得比数据集中所有棋手都好的表现。我们从理论上证明了低温采样能够实现超越，并在实验上对其进行了严格的评估。最后，我们讨论了超越的其他来源，为在更广泛的背景下进一步研究这一现象奠定了基础。||
|**2024-06-17**|[Latent Denoising Diffusion GAN: Faster sampling, Higher image quality](http://arxiv.org/abs/2406.11713)|**[link](https://github.com/thanhluantrinh/lddgan)**|扩散模型正在成为生成高保真度和多样化图像的强大解决方案，在许多情况下甚至优于GAN。然而，其缓慢的推理速度阻碍了其在实时应用中的潜力。为了解决这个问题，DiffusionGAN利用条件GAN大幅减少了去噪步骤并加快了推理速度。其改进版本Wavelet Diffusion通过将数据转换为小波空间进一步加快了这一过程，从而提高了效率。尽管如此，这些模型在速度和图像质量方面仍落后于GAN。为了弥合这些差距，本文介绍了潜在去噪扩散GAN（Latent Denoising Diffusion GAN），它采用预训练的自编码器将图像压缩到紧凑的潜在空间中，从而显著提高推理速度和图像质量。此外，我们提出了一种加权学习策略来增强图像的多样性和质量。在CIFAR-10、CelebA-HQ和LSUN-Church数据集上的实验结果证明，我们的模型在扩散模型中实现了最先进的运行速度。与之前的DiffusionGAN和Wavelet Diffusion相比，我们的模型在所有评估指标上都显示出显著的改进。代码和预训练模型：\url{https://github.com/thanhluantrinh/LDDGAN.git}||
|**2024-06-17**|[Diffusion Generative Modelling for Divide-and-Conquer MCMC](http://arxiv.org/abs/2406.11664)|**[link](https://github.com/ctrojan/DiffusionDnC)**|分而治之马尔可夫链蒙特卡洛 (MCMC) 是一种并行化马尔可夫链蒙特卡洛采样的策略，它在数据集的不相交子集上运行独立采样器并合并它们的输出。文献中一个持续的挑战是如何有效地执行这种合并，而不必对后验分布做出分布假设。我们建议使用扩散生成模型来拟合子后验分布的密度近似。这种方法在具有挑战性的合并问题上优于现有方法，同时与现有的密度估计方法相比，其计算成本可以更有效地扩展到高维问题。||
|**2024-06-17**|[An approach to non-equilibrium statistical physics using variational Bayesian inference](http://arxiv.org/abs/2406.11630)|null|我们讨论了一种对由耦合在一起的对象组成的系统进行数学建模的方法，该方法使用生成模型来描述构成此类系统的物体状态（或轨迹）之间的依赖关系。这类系统范围很广，包括开放系统或非平衡系统，与自组织系统尤其相关。由此产生的变分自由能原理 (FEP) 与直接使用随机动力系统相比具有一定的优势，特别是它更易于处理，并根据系统组件之间的耦合特性，对联合系统的演化方式提供了一种简洁的解释。使用 FEP，我们可以将一个物体的动力学建模为一个变分推理过程，因为变分自由能（或惊奇）是其动力学的李雅普诺夫函数。简而言之，我们认为使用生成模型来表示和跟踪子系统之间的关系，可以引导我们得到一种关于交互系统的特定统计理论。反过来，该理论使我们能够构建尊重子系统之间已知关系的嵌套模型。我们指出，一个物理对象符合 FEP 并不一定意味着该对象在字面上执行推理；相反，这是一种有用的解释性虚构，它用自由能梯度上的“隐式”流动代替了对象的“显式”动力学——这种虚构可能被对象本身所接受，也可能不被接受。||
|**2024-06-17**|[GECOBench: A Gender-Controlled Text Dataset and Benchmark for Quantifying Biases in Explanations](http://arxiv.org/abs/2406.11547)|**[link](https://github.com/braindatalab/gecobench)**|大型预训练语言模型已在许多应用中流行起来，并构成自然语言处理 (NLP) 中许多下游任务的重要支柱。应用“可解释人工智能”(XAI) 技术来丰富此类模型的输出，对于确保其质量和阐明其内部工作机制至关重要。然而，大型语言模型是在包含各种偏差（例如性别偏差）的大量数据上训练的，这会影响模型权重以及潜在的行为。目前，尚不清楚这种偏差在何种程度上也会以可能不利的方式影响模型解释。我们创建了一个性别控制文本数据集 GECO，其中其他相同的句子以男性和女性形式出现。这产生了用于性别分类任务的基本事实“世界解释”，从而能够客观地评估 XAI 方法的正确性。我们还提供了 GECOBench，这是一个严格的定量评估框架，对流行的 XAI 方法进行基准测试，将它们应用于经过不同程度微调的预训练语言模型。这使我们能够研究预训练如何在模型解释中引发不必要的偏差，以及微调可以在多大程度上减轻这种解释偏差。我们展示了解释性能与微调层数之间的明确依赖关系，其中观察到 XAI 方法特别受益于微调或嵌入层的完整再训练。值得注意的是，这种关系适用于在同一任务上实现相似分类性能的模型。因此，我们强调了所提出的性别控制数据集和新颖的基准测试方法对于新型 XAI 方法的研发具有实用性。所有代码，包括数据集生成、模型训练、评估和可视化，都可以在以下网址获得：https://github.com/braindatalab/gecobench||

<p align=right>(<a href=#updated-on-20240629>back to top</a>)</p>

## LLM

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-26**|[Catching Chameleons: Detecting Evolving Disinformation Generated using Large Language Models](http://arxiv.org/abs/2406.17992)|null|尽管大型语言模型 (LLM) 生成的虚假信息检测技术近期取得了进展，但目前的工作忽略了此类虚假信息不断演变的本质。本研究探讨了一个具有挑战性但又非常实际的研究问题：检测不断演变的 LLM 生成的虚假信息。虚假信息随着 LLM 及其变体的快速发展而不断演变。因此，检测模型面临着重大挑战。首先，为每个虚假信息生成器训练单独的模型效率低下。其次，当按顺序遇到不断演变的 LLM 生成的虚假信息时，性能会下降。为了解决这个问题，我们提出了 DELD（检测不断演变的 LLM 生成的虚假信息），这是一种参数高效的方法，它联合利用了预训练语言模型 (PLM) 的通用事实核查能力和各种 LLM 的独立虚假信息生成特征。具体来说，学习到的特征被顺序连接起来，以促进知识积累和转化。DELD 通过将虚假信息的语义嵌入与可训练的软提示相结合来解决标签稀缺的问题，从而引出模型特定的知识。我们的实验表明，DELD 的性能明显优于最先进的方法。此外，我们的方法为了解不同 LLM 虚假信息生成的独特模式提供了重要的见解，为这一研究方向提供了宝贵的视角。||
|**2024-06-25**|[Find Parent then Label Children: A Two-stage Taxonomy Completion Method with Pre-trained Language Model](http://arxiv.org/abs/2406.17739)|null|分类法将领域概念组织成层次结构，对于构建知识系统和下游应用至关重要。随着领域知识的发展，需要不断更新分类法以纳入新概念。以往的方法主要集中在向现有层次树的叶节点添加概念，这没有充分利用分类法的知识，也无法更新原始分类法结构（通常涉及非叶节点）。在本文中，我们提出了一种称为ATTEMPT的两阶段分类法补全方法。我们的方法通过找到父节点和标记子节点将新概念插入到正确的位置。具体来说，通过将局部节点与提示相结合生成自然语句，我们利用预训练的语言模型进行上位词/下位词识别。在两个公共数据集（包括六个领域）上的实验结果表明，ATTEMPT在分类法补全和扩展任务上均表现最佳，超过了现有方法。||
|**2024-06-25**|[Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?](http://arxiv.org/abs/2406.17274)|null|文本摘要作为一项关键的自然语言生成 (NLG) 任务，在各个领域都至关重要。然而，在风险关键型应用中，尤其是在涉及人在回路决策的应用中，不准确摘要的高昂成本引发了人们对文本摘要不确定性估计 (UE-TS) 评估方法可靠性的担忧。这种担忧源于不确定性模型指标对多样化且可能相互冲突的 NLG 指标的依赖性。为了解决这个问题，我们引入了一个全面的 UE-TS 基准，该基准包含四个维度上的 31 个 NLG 指标。该基准评估了两个大型语言模型和一个预训练语言模型在三个数据集上的不确定性估计能力，并在适用的情况下纳入了人工标注分析。我们还评估了该基准内 14 种常见不确定性估计方法的性能。我们的研究结果强调了考虑多个不相关的 NLG 指标和多样化的不确定性估计方法的重要性，以确保对 UE-TS 技术进行可靠有效的评估。||
|**2024-06-24**|[Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings](http://arxiv.org/abs/2406.16611)|**[link](https://github.com/anpoc/language-models-in-medicine)**|自Transformer架构出现以来，语言模型在巨大潜力的驱动下发展迅速。然而，要将这些模型投入生产应用，需要正确理解它们的行为，尤其是在医学等敏感领域。尽管存在这种需求，但医学文献中仍然缺乏对预训练语言模型的技术评估，而这些模型在计算能力或预算有限的资源受限环境中尤其 valuable。为了弥补这一差距，我们对医学领域的语言模型进行了全面综述。此外，我们还选择了一部分模型进行深入评估，重点关注分类和文本生成任务。我们的子集涵盖了 53 个模型，参数量从 1.1 亿到 130 亿不等，涵盖了 Transformer 模型的三个系列，并来自不同的知识领域。本研究采用了一系列文本分类方法以及零样本提示，而不是模型训练或微调，这与许多语言模型用户所处的资源有限环境非常相似。令人鼓舞的是，我们的研究结果揭示了这些模型在各种任务和数据集上的卓越性能，突出了某些模型在没有领域专业化的情况下也具有包含医学知识的潜在能力。因此，我们的研究主张进一步探索模型在医学环境中的应用，特别是在资源受限的环境中。代码可在 https://github.com/anpoc/Language-models-in-medicine 获取。||
|**2024-06-24**|[Large Vocabulary Size Improves Large Language Models](http://arxiv.org/abs/2406.16508)|null|本文对词表大小与大型语言模型 (LLM) 性能之间的关系进行了实证研究，以提供有关如何定义词表大小的见解。实验结果表明，更大的词表大小会导致 LLM 的性能更好。此外，我们考虑了一种持续训练场景，其中预训练的语言模型使用不同的目标语言进行训练。我们引入了一种简单的方法来使用新词表代替预定义的词表。我们证明，使用新词表的性能优于使用预训练中使用的词表的模型。||
|**2024-06-24**|[KEHRL: Learning Knowledge-Enhanced Language Representations with Hierarchical Reinforcement Learning](http://arxiv.org/abs/2406.16374)|**[link](https://github.com/MatNLP/KEHRL)**|知识增强型预训练语言模型（KEPLM）利用来自知识图谱（KG）的关系三元组，并通过自监督学习将这些外部数据源集成到语言模型中。以前的工作将知识增强视为两个独立的操作，即知识注入和知识整合。在本文中，我们提出了一种使用分层强化学习（KEHRL）来学习知识增强型语言表示的方法，该方法联合解决了检测知识注入位置和将外部知识整合到模型中的问题，以避免注入不准确或不相关的知识。具体来说，一个高级强化学习 (RL) 代理利用内部和先验知识来迭代地检测文本中用于知识注入的关键位置，从而过滤掉意义较小的实体，以避免转移知识学习方向。一旦选择了实体位置，就会触发相关的  三元组过滤模块，以执行低级 RL，通过二进制值操作动态地细化与多义实体相关联的三元组。实验验证了 KEHRL 在探测事实知识和增强模型在各种自然语言理解任务上的性能方面的有效性。||
|**2024-06-23**|[Evaluating the Effectiveness of the Foundational Models for Q&A Classification in Mental Health care](http://arxiv.org/abs/2406.15966)|null|预训练语言模型 (PLM) 有可能通过提供可访问且具有文化敏感性的资源来改变心理健康支持。然而，尽管有这种潜力，但它们在心理保健方面的有效性，特别是对阿拉伯语的有效性尚未得到广泛探索。为了弥合这一差距，本研究评估了基础模型在心理保健领域对问答 (Q＆A) 分类方面的有效性。我们利用了 MentalQA 数据集，这是一个以心理健康相关的问答互动为特色的阿拉伯语集合。在本研究中，我们使用四种不同类型的学习方法进行了实验：传统的特征提取、PLM 作为特征提取器、微调 PLM 以及在零样本和少样本学习设置中提示大型语言模型（GPT-3.5 和 GPT-4）。虽然传统的特征提取器与支持向量机 (SVM) 相结合显示出良好的性能，但 PLM 表现出更好的结果，因为它们能够捕捉语义。例如，MARBERT 在问题分类中取得了最高的性能，Jaccard 得分为 0.80，在答案分类中取得了 0.86 的 Jaccard 得分。我们进一步进行了深入分析，包括检查微调与非微调的影响、不同数据大小的影响以及进行错误分析。我们的分析表明，微调被证明有利于提高 PLM 的性能，并且训练数据的大小在实现高性能方面起着至关重要的作用。我们还探索了提示，其中使用 GPT-3.5 进行的少样本学习产生了可喜的结果。问题分类提高了 12%，答案分类提高了 45%。根据我们的发现，可以得出结论，PLM 和基于提示的方法为阿拉伯语的心理健康支持带来了希望。||
|**2024-06-21**|[Good things come in three: Generating SO Post Titles with Pre-Trained Models, Self Improvement and Post Ranking](http://arxiv.org/abs/2406.15633)|null|Stack Overflow是一个著名的问答论坛，支持开发者在编程相关问题上寻找合适的资源。拥有高质量的问题标题是吸引开发者注意力的有效手段。不幸的是，这一点常常被低估，还有改进的空间。目前已有研究主要利用预训练模型从代码片段和问题描述中生成标题。然而，由于输入数据的质量（例如，包含噪声和歧义）和序列生成模型的固有局限性，获得高质量的标题仍然是一项具有挑战性的任务。在本文中，我们提出了FILLER，它是一个使用经过微调的语言模型生成Stack Overflow帖子标题的解决方案，该模型具有自我改进和帖子排名功能。我们的研究重点是增强预训练语言模型以生成Stack Overflow帖子标题，并对这些模型采用训练和后续微调的范式。为此，我们将模型的预测整合到训练过程中，使其能够从错误中学习，从而减少曝光偏差的影响。此外，我们应用帖子排名方法来生成各种候选样本，随后选择最合适的样本。为了评估FILLER，我们使用基准数据集进行了实验，实证结果表明，我们的模型提供了高质量的建议。此外，它明显优于所有基线，包括Code2Que、SOTitle、CCBERT、M3NSCT5和GPT3.5-turbo。用户研究还表明，FILLER相较于SOTitle和GPT3.5-turbo提供了更相关的标题。||
|**2024-06-21**|[Hybrid Alignment Training for Large Language Models](http://arxiv.org/abs/2406.15178)|**[link](https://github.com/wangclnlp/deepspeed-chat-extension)**|对齐训练对于使大型语言模型 (LLM) 能够满足人类意图和偏好至关重要。它通常基于具有不同目标的两个阶段执行：指令遵循对齐和人类偏好对齐。然而，按顺序将 LLM 与这些目标对齐存在一个固有问题：目标可能存在冲突，并且 LLM 无法保证同时很好地与指令和人类偏好保持一致。为了应对这些问题，在这项工作中，我们提出了一种混合对齐训练 (Hbat) 方法，该方法基于交替对齐和改进的弹性权重合并方法。基本思想是在对齐训练期间交替使用不同的目标，以便在两个对齐任务之间实现更好的协作。我们在摘要和对话任务上试验了 Hbat。实验结果表明，所提出的 Hbat 可以显着优于所有基线。值得注意的是，当同时使用近端策略优化和直接偏好优化时，与传统的两阶段对齐训练相比，Hbat 产生了持续的性能提升。||
|**2024-06-21**|[Enhancing Idiomatic Representation in Multiple Languages via an Adaptive Contrastive Triplet Loss](http://arxiv.org/abs/2406.15175)|null|在自然语言处理 (NLP) 中，准确地对习语或非组合语言进行建模一直是一项长期挑战。部分原因是这些表达的含义并非仅仅来自其组成词，还因为相关数据资源的稀缺，以及它们对机器翻译和简化等下游任务性能的影响。在本文中，我们提出了一种使用三元组损失有效地对习语进行建模的方法，该方法通过使用自适应对比学习和重采样矿工来构建具有习语意识的学习目标，将组成词对习语意义的不对称贡献纳入训练语言模型。我们提出的方法在 SemEval 挑战中进行了评估，并在许多指标上明显优于以前的替代方法。||
|**2024-06-21**|[Harnessing Knowledge Retrieval with Large Language Models for Clinical Report Error Correction](http://arxiv.org/abs/2406.15045)|null|本研究提出了一种利用大型语言模型 (LLM) 和检索增强生成 (RAG) 技术纠正临床放射学报告错误的方法。所提出的框架采用内部和外部检索机制从报告和外部知识源中提取相关的医学实体和关系。引入了一个三阶段推理过程，将任务分解为错误检测、定位和纠正子任务，从而增强了系统的可解释性和性能。该方法的有效性使用一个基准数据集进行评估，该数据集是通过根据领域专家的指导，在现实世界的放射学报告中插入真实的错误而创建的。实验结果证明了所提出方法的优势，内部和外部检索的结合显著提高了各种最先进的 LLM 在错误检测、定位和纠正方面的准确性。这些发现有助于开发更稳健和可靠的临床文档错误纠正系统。||
|**2024-06-21**|[Do Large Language Models Exhibit Cognitive Dissonance? Studying the Difference Between Revealed Beliefs and Stated Answers](http://arxiv.org/abs/2406.14986)|null|由于提示和多项选择题 (MCQ) 易于操作和评估，它们已成为评估大型语言模型 (LLM) 能力的首选方法。此类实验评估指出，LLM 具有执行因果推理或理解不确定性的明显能力。在本文中，我们通过将这些问题重新定义为直接文本补全（LLM 的基础），来研究这些能力是否可以在定制提示和 MCQ 之外进行衡量。为了实现这一目标，我们定义了具有多种可能结果的场景，并将 LLM 通过提示做出的预测（他们的陈述答案）与其在下一个标记预测期间计算的这些结果的概率分布（他们的揭示信念）进行比较。我们的研究结果表明，LLM 的揭示信念与其陈述答案显着不同，并暗示了他们的信念可能在许多场景和结果中产生的多种偏差和误传。由于文本补全是 LLM 的核心，因此这些结果表明，常见的评估方法可能只提供了部分情况，需要更多研究来评估其能力的范围和性质。||
|**2024-06-21**|[ICLEval: Evaluating In-Context Learning Ability of Large Language Models](http://arxiv.org/abs/2406.14955)|**[link](https://github.com/yiye3/icleval)**|上下文学习 (ICL) 是大型语言模型 (LLM) 的一项关键能力，因为它使模型能够理解和推理相互关联的输入。评估 LLM 的 ICL 能力可以增强其利用率，并加深我们对这种能力在训练阶段如何获得的理解。然而，现有的评估框架主要关注语言能力和知识，而往往忽略了对 ICL 能力的评估。在这项工作中，我们引入了 ICLEval 基准来评估 LLM 的 ICL 能力，该基准涵盖了两个关键的子能力：精确复制和规则学习。通过 ICLEval 基准，我们证明了 ICL 能力普遍存在于不同的 LLM 中，模型大小并不是决定 ICL 效率的唯一因素。令人惊讶的是，我们观察到 ICL 能力，尤其是复制能力，在预训练过程的早期就已发展起来，并在之后趋于稳定。我们的源代码和基准测试已发布在 https://github.com/yiye3/ICLEval。||
|**2024-06-21**|[ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models](http://arxiv.org/abs/2406.14952)|**[link](https://github.com/haidequanbu/esc-eval)**|情感支持对话（ESC）是一项至关重要的应用，旨在减轻人类压力、提供情感指导，并最终提升人类身心健康。随着大型语言模型（LLM）的进步，许多研究人员已将LLM用作ESC模型。然而，这些基于LLM的ESC的评估仍然不确定。受角色扮演代理出色发展的启发，我们提出了一个ESC评估框架（ESC-Eval），该框架使用角色扮演代理与ESC模型进行交互，然后对交互式对话进行人工评估。具体来说，我们首先重新组织了来自七个现有数据集的2,801张角色扮演卡，以定义角色扮演代理的角色。其次，我们训练了一个名为ESC-Role的特定角色扮演模型，它的行为更像一个困惑的人，而不是GPT-4。第三，通过ESC-Role和有组织的角色卡，我们系统地使用14个LLM作为ESC模型进行了实验，包括通用人工智能助手LLM（ChatGPT）和面向ESC的LLM（ExTES-Llama）。我们对不同ESC模型的交互式多轮对话进行了全面的人工标注。结果表明，面向ESC的LLM与通用人工智能助手LLM相比表现出更优越的ESC能力，但与人类表现仍有差距。此外，为了自动化未来ESC模型的评分过程，我们开发了ESC-RANK，它在标注数据上进行训练，实现了超过GPT-4 35分的评分性能。我们的数据和代码可在https://github.com/haidequanbu/ESC-Eval获取。||
|**2024-06-21**|[MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression](http://arxiv.org/abs/2406.14909)|**[link](https://github.com/thu-nics/moa)**|稀疏注意力机制可以有效缓解大型语言模型 (LLM) 在长文本中对内存和吞吐量的巨大需求。现有方法通常采用统一的稀疏注意力掩码，在不同的注意力头和输入长度上应用相同的稀疏模式。然而，这种统一的方法无法捕捉LLM中固有的多样化注意力模式，忽略了它们在精度和延迟之间的独特权衡。为了应对这一挑战，我们提出了注意力混合 (MoA) 机制，它可以针对不同的注意力头和层自动定制不同的稀疏注意力配置。MoA构建并导航了一个由各种注意力模式及其相对于输入序列长度的缩放规则组成的搜索空间。它对模型进行分析，评估潜在的配置，并确定最佳的稀疏注意力压缩方案。MoA适应不同的输入大小，揭示了一些注意力头会扩展其关注范围以适应更长的序列，而其他注意力头则始终专注于固定长度的局部上下文。实验表明，在相同的平均注意力跨度下，MoA将有效上下文长度增加了3.9倍，在Vicuna-7B、Vicuna-13B和Llama3-8B模型上，检索精度比统一注意力基线提高了1.5到7.1倍。此外，MoA缩小了稀疏模型和密集模型之间的性能差距，在两个长文本理解基准测试中，将最大相对性能下降从9%-36%降低到5%以内。MoA在单个GPU上实现了1.2到1.4倍的GPU内存减少，并将7B和13B密集模型的解码吞吐量提高了5.5到6.7倍，而对性能的影响微乎其微。||
|**2024-06-21**|[GIEBench: Towards Holistic Evaluation of Group Indentity-based Empathy for Large Language Models](http://arxiv.org/abs/2406.14903)|**[link](https://github.com/giebench/giebench)**|随着大型语言模型 (LLM) 不断发展并得到广泛应用，LLM 对不同群体身份表现出同理心并理解其观点的能力日益被认为至关重要。大多数现有的 LLM 同理心评估基准主要关注普遍的人类情绪，如悲伤和痛苦，而往往忽视个体群体身份的背景。为了弥补这一差距，我们引入了 GIEBench，这是一个综合基准，包含 11 个身份维度，涵盖 97 个群体身份，共有 999 个与特定群体身份相关的单项选择题。GIEBench 旨在评估 LLM 在面对特定群体身份（如性别、年龄、职业和种族）时表现出的同理心，强调其从已识别群体的立场做出回应的能力。这支持了针对不同身份用户量身定制的同理心 LLM 应用的持续开发。我们对 23 个 LLM 的评估表明，虽然这些 LLM 理解不同的身份立场，但如果没有明确指示它们采用这些观点，它们就无法在这些身份之间始终如一地表现出同等的同理心。这凸显了将 LLM 与不同的价值观更好地结合起来以更好地适应人类身份的多方面性的必要性。我们的数据集可在 https://github.com/GIEBench/GIEBench 获取。||
|**2024-06-21**|[Safely Learning with Private Data: A Federated Learning Framework for Large Language Model](http://arxiv.org/abs/2406.14898)|null|私有数据规模更大、质量更高，可以极大地改进大型语言模型 (LLM)。然而，出于隐私考虑，这些数据通常分散在多个孤岛中，这使得将其安全地用于 LLM 训练成为一项挑战。联邦学习 (FL) 是使用分布式私有数据训练模型的理想解决方案，但 FedAvg 等传统框架由于对客户端的计算需求较高，因此不适用于 LLM。另一种方法是拆分学习，将大部分训练参数卸载到服务器，同时在本地训练嵌入层和输出层，使其更适合 LLM。尽管如此，它在安全性和效率方面仍面临着重大挑战。首先，嵌入的梯度容易受到攻击，导致可能对私有数据进行逆向工程。此外，服务器一次只能处理一个客户端的训练请求，这限制了并行训练，严重影响了训练效率。在本文中，我们提出了一个用于 LLM 的联邦学习框架，名为 FL-GLM，它可以防止服务器端和客户端攻击造成的数据泄露，同时提高训练效率。具体来说，我们首先将输入块和输出块放置在本地客户端，以防止来自服务器的嵌入梯度攻击。其次，我们在客户端-服务器通信期间采用密钥加密，以防止来自其他客户端的逆向工程攻击。最后，我们采用客户端批处理或服务器分层等优化方法，根据服务器的实际计算能力采用不同的加速方法。NLU 和生成任务的实验结果表明，FL-GLM 实现了与集中式 chatGLM 模型相当的指标，验证了我们联邦学习框架的有效性。||
|**2024-06-21**|[Talking the Talk Does Not Entail Walking the Walk: On the Limits of Large Language Models in Lexical Entailment Recognition](http://arxiv.org/abs/2406.14894)|null|动词是语言的支柱，为句子提供了结构和意义。然而，它们复杂的语义差别构成了一个长期挑战。通过词汇蕴含关系理解动词关系对于理解句子意义和掌握动词动态至关重要。本研究调查了八种大型语言模型在识别动词之间词汇蕴含关系方面的能力，这些模型采用不同的提示策略和零样本/少样本设置，并使用来自两个词汇数据库（WordNet 和 HyperLex）的动词对进行评估。我们的研究结果表明，这些模型可以以中等程度的性能完成词汇蕴含识别任务，尽管有效性和条件有所不同。此外，利用少样本提示可以提高模型的性能。然而，完美解决该任务对所有被测试的大型语言模型来说都是一个尚未解决的挑战，这引发了对该主题进一步研究的需要。||
|**2024-06-24**|[Relation Extraction with Fine-Tuned Large Language Models in Retrieval Augmented Generation Frameworks](http://arxiv.org/abs/2406.14745)|null|信息提取 (IE) 对于将非结构化数据转换为知识图谱 (KGs) 等结构化格式至关重要。关系提取 (RE) 是 IE 中的一项关键任务，它识别文本中实体之间的关系。RE 方法多种多样，包括监督、无监督、弱监督和基于规则的方法。最近利用预训练语言模型 (PLM) 的研究在该领域取得了重大成功。在当前以大型语言模型 (LLM) 为主导的时代，微调这些模型可以克服与零样本 LLM 基于提示的 RE 方法相关的局限性，特别是在领域适应挑战和识别句子中实体之间的隐性关系方面。这些隐性关系无法从句子的依存树中轻易提取，需要逻辑推理才能准确识别。这项工作探讨了微调 LLM 的性能及其与检索增强型 (RAG) RE 方法的集成，以解决在句子级别识别隐性关系的挑战，特别是在 LLM 作为 RAG 框架内的生成器时。对 TACRED、TACRED-Revisited (TACREV)、Re-TACRED 和 SemEVAL 数据集的实证评估表明，使用微调的 LLM（包括 Llama2-7B、Mistral-7B 和 T5 (Large)）可以显着提高性能。值得注意的是，我们的方法在隐性关系很常见的 SemEVAL 上取得了实质性进展，超过了该数据集上的先前结果。此外，我们的方法在 TACRED、TACREV 和 Re-TACRED 上的性能优于先前的工作，证明了其在各种评估场景中的出色性能。||
|**2024-06-20**|[Asynchronous Large Language Model Enhanced Planner for Autonomous Driving](http://arxiv.org/abs/2406.14556)|null|尽管实时规划器在自动驾驶领域表现出色，但大型语言模型（LLM）的不断发展为增强运动规划的可解释性和可控性开辟了道路。然而，基于LLM的规划器仍然面临着重大挑战，包括资源消耗高和推理时间长，这对其在实际应用中构成了巨大障碍。鉴于这些挑战，我们引入了AsyncDriver，这是一个新的异步LLM增强闭环框架，旨在利用由LLM生成的场景相关指令特征来指导实时规划器进行精确且可控的轨迹预测。一方面，我们的方法突出了LLM在理解和推理矢量化场景数据以及一系列路线指令方面的能力，展示了其对实时规划器的有效辅助作用。另一方面，所提出的框架将LLM和实时规划器的推理过程解耦。通过利用其推理频率的异步特性，我们的方法成功地降低了LLM引入的计算成本，同时保持了相当的性能。实验表明，我们的方法在nuPlan的挑战性场景中实现了优越的闭环评估性能。||
|**2024-06-20**|[GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models](http://arxiv.org/abs/2406.14550)|null|长上下文能力对于大型语言模型 (LLM) 处理复杂和长输入的任务至关重要。尽管为优化 LLM 以适应长上下文做出了许多努力，但在稳健地处理长输入方面仍然存在挑战。在本文中，我们介绍了 GraphReader，一种基于图的代理系统，旨在通过将长文本构建成图并利用代理自主探索该图来处理长文本。在收到问题后，代理首先进行逐步分析并制定合理的计划。然后，它调用一组预定义的函数来读取节点内容及其邻居，从而促进对图的从粗到细的探索。在整个探索过程中，代理不断记录新的见解并反思当前情况以优化过程，直到它收集到足够的信息来生成答案。在 LV-Eval 数据集上的实验结果表明，GraphReader 使用 4k 上下文窗口，在 16k 到 256k 的上下文长度范围内，始终大幅优于 GPT-4-128k。此外，我们的方法在四个具有挑战性的单跳和多跳基准测试中均表现出优异的性能。||
|**2024-06-20**|[Uncovering Latent Memories: Assessing Data Leakage and Memorization Patterns in Large Language Models](http://arxiv.org/abs/2406.14549)|null|大型语言模型的兴起彻底改变了自然语言处理任务，但也引发了人们对数据隐私和安全的深刻担忧。语言模型在包含潜在敏感或专有信息的海量语料库上进行训练，而数据泄露的风险（即模型响应泄露此类信息的片段）仍然未得到充分理解。本研究通过量化机器学习模型中的记忆现象来检验其对数据泄露的敏感性，重点关注训练过程中记忆模式的演变。我们通过评估重复如何影响记忆来研究训练数据的统计特征如何影响编码在模型中的记忆。我们重现了以下发现：记忆序列的概率与其在数据中出现的次数成对数关系。此外，我们发现，即使在第一次遇到后没有明显记住的序列，也可以在整个训练过程中被发现，即使没有后续的遇到。这些潜在记忆序列的存在对数据隐私提出了挑战，因为它们可能隐藏在模型的最终检查点中。为此，我们开发了一种诊断测试，通过考虑其交叉熵损失来揭示这些潜在的记忆序列。||
|**2024-06-20**|[PostMark: A Robust Blackbox Watermark for Large Language Models](http://arxiv.org/abs/2406.14517)|**[link](https://github.com/lilakk/postmark)**|目前，检测LLM生成文本最有效的方法是在模型解码过程中插入可检测的签名（或水印）。大多数现有的水印方法都需要访问底层LLM的logits，而LLM API提供商出于对模型蒸馏的担忧，不愿分享这些信息。因此，这些水印必须由每个LLM提供商独立实施。在本文中，我们开发了PostMark，一种模块化的后置水印程序，它在解码过程完成后将一组依赖于输入的单词（通过语义嵌入确定）插入到文本中。重要的是，PostMark不需要访问logit，这意味着它可以由第三方实现。我们还表明，PostMark比现有的水印方法更能抵抗释义攻击：我们的实验涵盖了8种基线算法、5种基础LLM和3个数据集。最后，我们使用自动评估和人工评估来评估PostMark对文本质量的影响，突出了质量和抵抗释义攻击之间的权衡。我们在https://github.com/lilakk/PostMark上发布了我们的代码、输出和注释。||
|**2024-06-20**|[Evidence of a log scaling law for political persuasion with large language models](http://arxiv.org/abs/2406.14508)|**[link](https://github.com/kobihackenburg/scaling-llm-persuasion)**|大型语言模型现在可以生成与人类撰写的政治信息一样具有说服力的信息，这引发了人们对这种说服力可能随着模型规模的增加而持续增强的担忧。在这里，我们使用 24 个规模跨越多个数量级的语言模型，生成了 720 条关于 10 个美国政治议题的说服性信息。然后，我们在一个大规模随机调查实验 (N = 25,982) 中部署了这些信息，以估计每个模型的说服能力。我们的发现有两方面。首先，我们发现了对数比例定律的证据：模型说服力的特点是收益递减，因此当前的前沿模型的说服力仅略高于规模小一个或多个数量级的模型。其次，仅仅完成任务（连贯性、保持主题）似乎是大型模型具有说服力优势的原因。这些发现表明，进一步扩大模型规模不会大幅提高静态 LLM 生成信息的说服力。||
|**2024-06-20**|[Improving Expert Radiology Report Summarization by Prompting Large Language Models with a Layperson Summary](http://arxiv.org/abs/2406.14500)|null|放射学报告总结 (RRS) 对于患者护理至关重要，需要从详细的“结果”中得出简洁的“印象”。 本文介绍了一种新的提示策略，通过首先生成外行摘要来增强 RRS。这种方法使用受医患互动启发的非专家交流技巧，对关键观察结果进行标准化并简化复杂信息。结合少样本上下文学习，这种方法提高了模型将通用术语与特定发现联系起来的能力。我们在 MIMIC-CXR、CheXpert 和 MIMIC-III 数据集上评估了这种方法，将其与 7B/8B 参数最先进的开源大型语言模型 (LLM)（如 Meta-Llama-3-8B-Instruct）进行基准测试。我们的结果表明，总结的准确性和可访问性有所提高，尤其是在域外测试中，某些指标的改进高达 5%。||
|**2024-06-20**|[Data-Centric AI in the Age of Large Language Models](http://arxiv.org/abs/2406.14473)|null|这篇立场论文提出了以数据为中心的AI研究视角，重点关注大型语言模型（LLM）。我们首先注意到一个关键观察结果：数据在LLM的发展阶段（例如，预训练和微调）和推理阶段（例如，上下文学习）中都起着重要作用，但它受到研究界的关注却少之又少。我们确定了四个以数据为中心的具体场景，涵盖以数据为中心的基准和数据整理、数据溯源、知识迁移和推理语境化。在每个场景中，我们都强调数据的重要性，突出有前景的研究方向，并阐明对研究界以及在适用的情况下对整个社会的潜在影响。例如，我们倡导一套针对LLM的数据规模和复杂性量身定制的以数据为中心的基准。这些基准可用于开发新的数据整理方法和记录研究工作和结果，这有助于促进AI和LLM研究的开放性和透明度。||
|**2024-06-20**|[Explicit and Implicit Large Language Model Personas Generate Opinions but Fail to Replicate Deeper Perceptions and Biases](http://arxiv.org/abs/2406.14462)|null|大型语言模型 (LLM) 正越来越多地应用于以人为中心的社会科学任务中，例如数据标注、合成数据创建以及参与对话。然而，这些任务具有高度的主观性，并依赖于环境、态度、信仰和生活经历等人类因素。因此，在这些任务中使用缺乏此类人类因素的 LLM 可能会导致数据缺乏变化，无法反映人类经验的多样性。在本文中，我们探讨了使用类人角色提示 LLM 并要求模型像特定的人一样回答的作用。这是通过明确的、具有确切人口统计、政治信仰和生活经历的角色，或通过特定人群中普遍存在的姓名隐含地完成的。然后，通过 (1) 主观标注任务（例如，检测毒性）和 (2) 信念生成任务来评估 LLM 角色，这两个任务都因人类因素而异。我们检查了显式和隐式角色的影响，并调查了 LLM 识别和响应的人类因素。结果表明，LLM 角色在再现已知的人类偏见方面表现出混合的结果，但通常无法表现出隐性偏见。我们得出结论，LLM 缺乏人类思维的内在认知机制，而只是捕捉了人们说话的统计模式，这可能会限制其在复杂社会科学应用中的有效性。||
|**2024-06-20**|[APEER: Automatic Prompt Engineering Enhances Large Language Model Reranking](http://arxiv.org/abs/2406.14449)|**[link](https://github.com/jincan333/apeer)**|大型语言模型（LLM）显著增强了信息检索（IR）各个模块的能力，例如重排序。尽管性能出色，但当前使用 LLM 进行的零样本相关性排序严重依赖于人工提示工程。现有的自动提示工程算法主要集中在语言建模和分类任务上，而信息检索领域，特别是重排序领域，尚未得到充分探索。由于输入中集成了查询和长段落对，将当前的提示工程算法直接应用于相关性排序具有挑战性，因为其排序复杂性超过了分类任务。为了减少人力投入并释放提示优化在重排序中的潜力，我们引入了一种名为 APEER 的新型自动提示工程算法。APEER 通过反馈和偏好优化迭代地生成改进的提示。对四个 LLM 和十个数据集进行的大量实验表明，APEER 相比现有的最先进（SoTA）人工提示，性能有了显著提高。此外，我们发现 APEER 生成的提示在不同任务和 LLM 之间表现出更好的可迁移性。代码可在 https://github.com/jincan333/APEER 获取。||
|**2024-06-20**|[LLM4CP: Adapting Large Language Models for Channel Prediction](http://arxiv.org/abs/2406.14440)|null|信道预测是降低大规模多输入多输出 (m-MIMO) 系统反馈或估计开销的有效方法。然而，现有的信道预测方法由于模型失配误差或网络泛化问题而缺乏精度。大型语言模型 (LLM) 已展现出强大的建模和泛化能力，并已成功应用于跨模态任务，包括时间序列分析。利用 LLM的表达能力，我们提出了一种基于预训练LLM的信道预测方法 (LLM4CP)，用于根据历史上下行信道状态信息 (CSI) 序列预测未来下行CSI序列。我们对网络进行微调，同时冻结预训练 LLM 的大部分参数，以实现更好的跨模态知识迁移。为了弥合信道数据与 LLM 特征空间之间的差距，我们专门针对独特的信道特性定制了预处理器、嵌入模块和输出模块。仿真结果表明，所提出的方法在全样本、小样本和泛化测试中均实现了最先进的预测性能，同时训练和推理成本较低。||
|**2024-06-19**|[Knowledge Tagging System on Math Questions via LLMs with Flexible Demonstration Retriever](http://arxiv.org/abs/2406.13885)|null|知识标注在当代智能教育应用中发挥着至关重要的作用，包括学习进度诊断、练习题推荐和课程内容组织。传统上，这些标注工作一直由教学专家完成，因为这项任务不仅需要对题干和知识定义有很强的语义理解，还需要深入了解如何将解题逻辑与相应的知识概念联系起来。随着预训练语言模型等先进文本编码算法的出现，许多研究人员开发了基于计算知识和问题嵌入之间语义相似度的自动知识标注系统。在本文中，针对先前基于编码的方法无法处理涉及强领域知识和复杂概念定义的难题，我们探索了使用大型语言模型 (LLM) 来自动化完成这项任务。通过展示零样本和小样本学习在数学问题知识标注任务上的强大性能，我们证明了 LLM 在克服先前方法所面临的挑战方面具有巨大潜力。此外，通过提出一种基于强化学习的演示检索器，我们成功地利用了不同规模 LLM 的巨大潜力，在保持上下文内演示使用效率高的同时实现了更好的性能结果。||
|**2024-06-18**|[Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?](http://arxiv.org/abs/2406.12822)|null|大型语言模型，特别是多语言模型，其设计、宣传和预期目标都是服务于各种语言的母语使用者。我们假设，由于严重依赖翻译，这些模型目前的微调和评估实践可能与这一目标不符，因为翻译可能会引入翻译错误和缺陷。目前尚不清楚指令数据的性质是否会对模型输出产生影响；另一方面，翻译后的测试集是否能够捕捉到这些细微差别也值得怀疑。由于在这两个阶段经常使用翻译数据的做法，这种缺陷可能被忽视了。这项工作通过在指令微调和评估阶段使用受控的母语或翻译数据并观察模型结果来研究这些问题。对8个基础模型和8个不同基准的实验表明，母语或生成基准在母语和翻译指令数据之间表现出显著差异，尤其是在模型性能较高时，而其他类型的测试集则没有这种差异。最后，我们证明了正则化有利于弥合结构化任务（而非生成任务）上的差距。||
|**2024-06-18**|[Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?](http://arxiv.org/abs/2406.12809)|null|大型语言模型 (LLM) 表现出令人印象深刻的能力，但仍然存在不一致的问题（例如，LLM 对重新措辞或无关紧要的顺序更改等干扰的反应可能不同）。除了这些不一致之外，我们还观察到，尽管 LLM 能够解决难题，但反常的是，它们却可能在更简单的难题上失败。为了评估这种由难到易的不一致性，我们开发了 ConsisEval 基准测试，其中每个条目包含一对难度严格排序的问题。此外，我们引入了“一致性得分”的概念，以定量衡量这种不一致性，并通过“相对一致性得分”分析一致性改进的潜力。基于对各种现有模型的综合实验，我们发现：(1) GPT-4 取得了 92.2% 的最高一致性得分，但由于冗余信息的干扰、对问题的误解等原因，在特定问题上仍然存在不一致性；(2) 能力越强的模型通常表现出更高的一致性，但也存在例外情况；(3) 硬数据增强了微调和上下文学习的一致性。我们的数据和代码将在 GitHub 上公开。||
|**2024-06-18**|[Supporting Human Raters with the Detection of Harmful Content using Large Language Models](http://arxiv.org/abs/2406.12800)|null|本文探讨了利用大型语言模型 (LLM) 自动化或辅助人工评估员识别有害内容（包括仇恨言论、骚扰、暴力极端主义和选举虚假信息）的可行性。我们使用包含 50,000 条评论的数据集证明，与人工判决相比，LLM 可以达到 90% 的准确率。我们探索了如何最好地利用这些能力，提出了五种将 LLM 与人工评估相结合的设计模式，例如预先过滤非违规内容、检测人工评估中的潜在错误或显示关键上下文以支持人工评估。我们概述了如何使用单一、优化的提示来支持所有这些设计模式。除了这些合成实验之外，我们还分享了在现实世界审核队列中试行我们提出的技术如何使可用人工评估员的能力优化了 41.5%，并将识别违规内容的精确率和召回率提高了 9% 到 11%（绝对值）。||
|**2024-06-18**|[ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools](http://arxiv.org/abs/2406.12793)|**[link](https://github.com/thudm/chatglm-6b)**|我们介绍 ChatGLM，这是一个随着时间的推移而不断发展的系列大型语言模型。本报告主要关注 GLM-4 语言系列，其中包括 GLM-4、GLM-4-Air 和 GLM-4-9B。它们代表了我们功能最强大的模型，这些模型利用了前三代 ChatGLM 的所有见解和经验教训进行训练。迄今为止，GLM-4 模型已经接受了 10 万亿个标记的预训练，这些标记主要来自中文和英文，以及来自 24 种语言的一小部分语料库，并且主要针对中文和英文的使用进行了调整。高质量的对齐是通过多阶段的训练后过程实现的，该过程包括监督微调和从人类反馈中学习。评估表明，GLM-4 1) 在 MMLU、GSM8K、MATH、BBH、GPQA 和 HumanEval 等一般指标方面与 GPT-4 相当或优于 GPT-4，2) 在 IFEval 衡量的指令遵循方面接近 GPT-4-Turbo，3) 在长上下文任务方面与 GPT-4 Turbo (128K) 和 Claude 3 相当，以及 4) 在 AlignBench 衡量的中文对齐方面优于 GPT-4。GLM-4 All Tools 模型经过进一步调整，可以理解用户意图并自主决定何时以及使用哪些工具（包括网络浏览器、Python 解释器、文本到图像模型和用户定义的函数）来有效完成复杂的任务。在实际应用中，它在通过网络浏览访问在线信息和使用 Python 解释器解决数学问题等任务中，与 GPT-4 All Tools 相当甚至超越了它。在此过程中，我们开源了一系列模型，包括 ChatGLM-6B（三代）、GLM-4-9B（128K、1M）、GLM-4V-9B、WebGLM 和 CodeGeeX，仅在 2023 年就吸引了 Hugging face 上超过 1000 万次的下载量。开源模型可以通过 https://github.com/THUDM 和 https://huggingface.co/THUDM 访问。||
|**2024-06-18**|[UBENCH: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions](http://arxiv.org/abs/2406.12784)|**[link](https://github.com/Cyno2232/UBENCH)**|大型语言模型（LLM）的快速发展已经展现出可观的应用前景。然而，它们的可解释性较低，这常常导致在不可预见的情况下出现错误，限制了它们的实用性。许多研究致力于创建全面的评估体系，但以往的基准测试主要评估解决问题的能力，而忽略了响应的不确定性，这可能导致不可靠的结果。最近测量LLM可靠性的方法非常消耗资源，并且无法测试黑盒模型。为了解决这个问题，我们提出了UBENCH，这是一个用于评估LLM可靠性的综合基准测试。UBENCH包含3,978道多项选择题，涵盖知识、语言、理解和推理能力。实验结果表明，UBENCH取得了最先进的性能，同时其单次采样方法与需要多次采样的基线方法相比，显著节省了计算资源。此外，我们基于UBENCH评估了15种流行LLM的可靠性，发现GLM4表现最为出色，紧随其后的是GPT-4。我们还探讨了思维链提示、角色扮演提示、选项顺序和温度对LLM可靠性的影响，分析了它们对不同LLM的不同影响。||
|**2024-06-18**|[Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries](http://arxiv.org/abs/2406.12775)|**[link](https://github.com/edenbiran/HoppingTooLate)**|大型语言模型 (LLM) 可以解决复杂的多步骤问题，但人们对其内部计算机制知之甚少。基于此，我们研究了 LLM 如何回答多跳查询，例如“Imagine 的演唱者的配偶是”。这些查询需要两个信息提取步骤：一个潜在步骤，用于将第一个跳跃（“Imagine 的演唱者”）解析为桥接实体（约翰·列侬），另一个步骤用于将第二个跳跃（“约翰·列侬的配偶”）解析为目标实体（小野洋子）。理解潜在步骤如何在内部计算是理解整体计算的关键。通过仔细分析基于 Transformer 的 LLM 的内部计算，我们发现桥接实体是在模型的早期层中解析的。然后，只有在这个解析之后，两跳查询才会在后面的层中得到解决。由于第二个跳跃始于后面的层，因此在某些情况下，这些层可能不再编码正确预测答案所需的知识。基于此，我们提出了一种新颖的“反向补丁”分析方法，将来自后面层的隐藏表示“补丁”回较早的层。我们发现在高达 57% 的先前错误案例中，存在一个反向补丁，可以导致正确生成答案，这表明后面的层确实有时缺乏必要的功能。总的来说，我们的方法和发现为理解和改进基于 Transformer 的 LLM 中的潜在推理提供了进一步的机会。||
|**2024-06-18**|[Large Language Model as a Universal Clinical Multi-task Decoder](http://arxiv.org/abs/2406.12738)|null|开发有效的机器学习方法来提高临床系统的效率和准确性至关重要。尽管进行了大量的研究，但管理大量多样化的临床任务和适应新出现的任务仍然是重大挑战。本文提出了一种新颖的范式，该范式采用预训练的大型语言模型作为通用的临床多任务解码器。这种方法利用语言表达的灵活性和多样性来处理任务主题变化和相关参数。引入新任务只需添加新的指令模板。我们在数百项任务中验证了此框架，证明了其在促进多任务预测方面的稳健性，其性能与传统的  多任务学习和单任务学习方法相当。此外，它还表现出对新任务的出色适应能力，在某些情况下具有令人印象深刻的零样本性能，并且在少样本场景中具有卓越的数据效率。这种新方法为管理临床应用中的大量新兴任务提供了统一的解决方案。||
|**2024-06-18**|[Can Large Language Models Code Like a Linguist?: A Case Study in Low Resource Sound Law Induction](http://arxiv.org/abs/2406.12725)|null|长期以来，历史语言学家一直在编写一种未完全形式化的“程序”，用于将祖先语言中的重建词转换为其已证实的后代语言中的词，该程序由一系列有序的字符串重写函数（称为语音规律）组成。他们通过观察重建语言（原始形式）和后代语言（反射形式）中的词对，并构建一个将原始形式转换为反射形式的程序来做到这一点。然而，编写这些程序容易出错且耗时。先前的工作已经成功地用计算方法为这个过程搭建了脚手架，但很少有研究人员处理语音规律归纳（SLI）问题，我们在本文中将其视为示例编程来处理。我们提出了一种语言无关的解决方案，该解决方案利用大型语言模型（LLM）的编程能力，从语音变化示例中生成 Python 语音规律程序。我们评估了我们的方法对各种 LLM 的有效性，提出了生成额外的语言无关合成数据的有效方法，以微调用于 SLI 的 LLM，并将我们的方法与现有的自动 SLI 方法进行了比较，结果表明，虽然 LLM 落后于它们，但它们可以弥补它们的一些弱点。||
|**2024-06-18**|[Estimating Knowledge in Large Language Models Without Generating a Single Token](http://arxiv.org/abs/2406.12673)|null|为了评估大型语言模型 (LLM) 的知识，当前的方法是查询模型，然后评估其生成的响应。在这项工作中，我们询问是否可以在模型生成任何文本之前完成评估。具体来说，是否可以仅根据模型的内部计算来估计模型对某个实体的了解程度？我们通过两个任务来研究这个问题：给定一个主题实体，目标是预测 (a) 模型回答有关该实体的常见问题的能力，以及 (b) 模型生成的有关该实体的响应的事实性。对各种 LLM 进行的实验表明，KEEN 是一种基于内部主题表示训练的简单探针，在这两项任务中都取得了成功——与每个主题的模型问答准确率和 FActScore（最近在开放式生成中的事实性指标）密切相关。此外，KEEN 自然地与模型的 hedging 行为相一致，并忠实地反映了微调后模型知识的变化。最后，我们展示了一个更具解释性但性能相当的 KEEN 变体，它突出显示了一小组与模型缺乏知识相关的标记。KEEN 简单轻便，可用于识别 LLM 中实体知识的差距和集群，并指导决策，例如使用检索增强查询。||
|**2024-06-18**|[Stealth edits for provably fixing or attacking large language models](http://arxiv.org/abs/2406.12670)|**[link](https://github.com/qinghua-zhou/stealth-edits)**|We reveal new methods and the theoretical foundations of techniques for editing large language models. We also show how the new theory can be used to assess the editability of models and to expose their susceptibility to previously unknown malicious attacks. Our theoretical approach shows that a single metric (a specific measure of the intrinsic dimensionality of the model's features) is fundamental to predicting the success of popular editing approaches, and reveals new bridges between disparate families of editing methods. We collectively refer to these approaches as stealth editing methods, because they aim to directly and inexpensively update a model's weights to correct the model's responses to known hallucinating prompts without otherwise affecting the model's behaviour, without requiring retraining. By carefully applying the insight gleaned from our theoretical investigation, we are able to introduce a new network block -- named a jet-pack block -- which is optimised for highly selective model editing, uses only standard network operations, and can be inserted into existing networks. The intrinsic dimensionality metric also determines the vulnerability of a language model to a stealth attack: a small change to a model's weights which changes its response to a single attacker-chosen prompt. Stealth attacks do not require access to or knowledge of the model's training data, therefore representing a potent yet previously unrecognised threat to redistributed foundation models. They are computationally simple enough to be implemented in malware in many cases. Extensive experimental results illustrate and support the method and its theoretical underpinnings. Demos and source code for editing language models are available at https://github.com/qinghua-zhou/stealth-edits.||
|**2024-06-17**|[mDPO: Conditional Preference Optimization for Multimodal Large Language Models](http://arxiv.org/abs/2406.11839)|null|直接偏好优化 (DPO) 已被证明是一种有效的大语言模型 (LLM) 对齐方法。最近的研究尝试将 DPO 应用于多模态场景，但发现难以实现一致的改进。通过对比实验，我们发现了多模态偏好优化中的无条件偏好问题，即模型忽略了图像条件。为了解决这个问题，我们提出了 mDPO，这是一种多模态 DPO 目标，通过同时优化图像偏好来防止过度优先考虑仅语言偏好。此外，我们引入了一个奖励锚点，强制选择响应的奖励为正，从而避免了其可能性降低——这是相对偏好优化固有的问题。在两个不同规模的多模态 LLM 和三个广泛使用的基准测试上的实验表明，mDPO 有效地解决了多模态偏好优化中的无条件偏好问题，并显着提高了模型性能，特别是在减少幻觉方面。||
|**2024-06-17**|[Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models](http://arxiv.org/abs/2406.11831)|null|基于解码器Transformer的大语言模型（LLM）在文本理解能力方面已经展现出优于CLIP和T5系列模型的优势。然而，如何将当前先进的LLM应用于文本到图像的扩散模型仍然是一个有待探索的领域。我们观察到一个不寻常的现象：直接使用大型语言模型作为提示编码器会显著降低图像生成中的提示遵循能力。我们确定了造成这个问题背后的两个主要障碍。一个是LLM中预测下一个词的训练目标与扩散模型中对判别性提示特征的要求之间存在偏差。另一个是由仅解码器架构引入的固有位置偏差。为了解决这个问题，我们提出了一个全新的框架来充分利用LLM的能力。通过精心设计的使用指南，我们有效地增强了用于提示编码的文本表示能力，并消除了其固有的位置偏差。这使我们能够灵活地将最先进的LLM集成到文本到图像的生成模型中。此外，我们还提供了一种将多个LLM融合到我们框架中的有效方法。考虑到Transformer架构所展现出的出色性能和扩展能力，我们进一步设计了一种基于该框架的LLM注入式扩散Transformer（LI-DiT）。我们进行了广泛的实验，以验证LI-DiT在模型规模和数据规模上的有效性。得益于LLM的固有能力和我们的创新设计，LI-DiT的提示理解性能轻松超越了最先进的开源模型以及包括Stable Diffusion 3、DALL-E 3和Midjourney V6在内的主流闭源商业模型。功能强大的LI-DiT-10B将在经过进一步优化和安全检查后发布。||
|**2024-06-17**|[VideoLLM-online: Online Video Large Language Model for Streaming Video](http://arxiv.org/abs/2406.11816)|null|近年来，大型语言模型已经具备了视觉能力，能够理解图像、视频和交错的视觉语言内容。然而，这些大型多模态模型的学习方法通常将视频视为预先确定的片段，这使得它们在处理流式视频输入方面效率较低。在本文中，我们提出了一种新颖的视频流学习（LIVE）框架，该框架支持在连续视频流中进行时间对齐、长上下文和实时的对话。我们的LIVE框架包含实现视频流对话的综合方法，包括：（1）旨在对连续流输入执行语言建模的训练目标，（2）将离线时间注释转换为流对话格式的数据生成方案，以及（3）优化的推理流程，以加快模型在现实世界视频流中的响应速度。借助我们的LIVE框架，我们在Llama-2/Llama-3的基础上构建了VideoLLM-online模型，并展示了其在处理流媒体视频方面的显著优势。例如，平均而言，我们的模型可以在A100 GPU上以超过10 FPS的速度支持5分钟视频片段中的流媒体对话。此外，它还在公共离线视频基准测试中展示了最先进的性能，例如识别、字幕和预测。代码、模型、数据和演示已在https://showlab.github.io/videollm-online上提供。||
|**2024-06-17**|[How Do Large Language Models Acquire Factual Knowledge During Pretraining?](http://arxiv.org/abs/2406.11813)|null|尽管近期观察到大型语言模型 (LLM) 可以存储大量的 factual knowledge，但对于它们如何通过预训练获取 factual knowledge 的机制，人们的理解仍然有限。这项工作通过研究 LLM 在预训练期间如何获取 factual knowledge 来解决这一差距。研究结果揭示了关于预训练期间 factual knowledge 获取动态的几个重要见解。首先，与直觉相反，我们观察到，对更多数据的预训练并没有显著提高模型获取和维护 factual knowledge 的能力。其次，训练步数与 factual knowledge 的记忆遗忘和泛化之间存在幂律关系，并且使用重复训练数据训练的 LLM 表现出更快的遗忘速度。第三，使用更大的批大小训练 LLM 可以增强模型对遗忘的鲁棒性。总的来说，我们的观察表明 LLM 预训练中的 factual knowledge 获取是通过逐步增加在每个步骤中预训练数据中出现的 factual knowledge 的概率来实现的。然而，这种增加会被随后的遗忘所稀释。基于这种解释，我们可以对最近观察到的 LLM 行为提供合理的解释，例如 LLM 在长尾知识上的糟糕表现以及对预训练语料库进行重复数据删除的好处。||
|**2024-06-17**|[DataComp-LM: In search of the next generation of training sets for language models](http://arxiv.org/abs/2406.11794)|null|我们推出了面向语言模型的数据比较测试平台 (DCLM)，这是一个用于控制数据集实验以改进语言模型的测试平台。作为 DCLM 的一部分，我们提供了一个从 Common Crawl 中提取的包含 240 万亿个标记的标准化语料库、基于 OpenLM 框架的有效预训练方案以及一套包含 53 个下游评估的广泛套件。DCLM 基准测试的参与者可以尝试各种数据整理策略，例如去重、过滤和数据混合，模型规模从 4.12 亿到 70 亿个参数不等。作为 DCLM 的基线，我们进行了广泛的实验，发现基于模型的过滤是组装高质量训练集的关键。生成的数据集 DCLM-Baseline 使从头开始训练一个包含 70 亿个参数的语言模型成为可能，该模型在 MMLU 上的 5-shot 准确率达到 64%，训练使用了 2.6 万亿个标记。与之前最先进的开放数据语言模型 MAP-Neo 相比，DCLM-Baseline 在 MMLU 上实现了 6.6 个百分点的改进，而计算量减少了 40%。我们的基线模型在 MMLU 上也与 Mistral-7B-v0.3 和 Llama 3 8B 相当（63% 和 66%），并且在平均 53 个自然语言理解任务上的表现相似，而训练使用的计算量比 Llama 3 8B 少 6.6 倍。我们的结果突出了数据集设计对训练语言模型的重要性，并为进一步研究数据整理提供了一个起点。||
|**2024-06-17**|[CELL your Model: Contrastive Explanation Methods for Large Language Models](http://arxiv.org/abs/2406.11785)|null|黑盒深度神经网络分类模型的出现引发了对其决策进行解释的需求。然而，对于生成式人工智能（如大型语言模型（LLM））来说，没有类别预测需要解释。相反，人们可以询问LLM为何对给定提示输出特定响应。在本文中，我们通过提出据我们所知第一个仅需要黑盒/查询访问的对比解释方法来回答这个问题。我们的解释表明，LLM对给定提示输出回复的原因是，如果稍微修改提示，LLM会给出不同的回复，这些回复要么不太可取，要么与原始回复相矛盾。关键见解是，对比解释只需要对用户有意义的距离函数，而不是特定响应（即类别标签）的实际值表示。我们提供了两种寻找对比解释的算法：i）一种短视算法，虽然在创建对比方面有效，但需要多次模型调用；ii）一种预算算法，这是我们的主要算法贡献，它可以智能地创建符合查询预算的对比，这对于较长的上下文是必要的。我们展示了这些方法在各种自然语言任务上的有效性，例如开放文本生成、自动红队和解释对话降级。||
|**2024-06-17**|[Multi-Layer Ranking with Large Language Models for News Source Recommendation](http://arxiv.org/abs/2406.11745)|null|为了寻找新闻事件的可靠信息来源，我们引入了一项新的专家推荐任务，旨在根据专家先前引用的陈述来识别可信赖的来源。为此，我们构建了一个名为 NewsQuote 的新数据集，该数据集包含从新闻文章集合中提取的 23,571 对引言-发言者对。我们将推荐任务制定为根据专家与给定查询相关的可能性来检索专家。我们还提出了一个采用大型语言模型的多层排名框架，以提高推荐性能。我们的结果表明，采用基于上下文学习的 LLM 排名器和基于多层排名的过滤器可以显着提高推荐系统的预测质量和行为质量。||
|**2024-06-17**|[Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models](http://arxiv.org/abs/2406.11736)|**[link](https://github.com/xufangzhi/envisions)**|大型语言模型 (LLM) 性能优越的主要驱动力之一是用于对齐微调的大量人工标注自然语言数据的可用性。这促使研究人员探索自训练方法，以减少对人工标注的过度依赖。然而，目前自训练的成功主要是在自然语言场景中观察到的，而不是在越来越重要的神经符号场景中。为此，我们提出了一个名为 ENVISIONS 的环境引导神经符号自训练框架。它旨在克服两个主要挑战：(1) 符号数据的稀缺性，以及 (2) LLM 在处理符号语言方面的能力有限。在三个不同领域进行的广泛评估证明了我们方法的有效性。此外，我们还进行了全面分析，以揭示 ENVISIONS 成功的影响因素，从而为该领域的未来研究提供宝贵见解。代码将在 \url{https://github.com/xufangzhi/ENVISIONS} 提供。||
|**2024-06-17**|[Meta Reasoning for Large Language Models](http://arxiv.org/abs/2406.11698)|null|我们介绍了元推理提示 (MRP)，这是一种受人类元推理启发，为大型语言模型 (LLM) 设计的、新颖且高效的系统提示方法。传统的基于上下文学习的推理技术，例如思维树，虽然很有前景，但由于其专业性，缺乏跨不同任务的一致最先进性能。MRP 通过引导 LLM 根据每个任务的特定要求动态选择和应用不同的推理方法来解决这一限制，从而优化性能和计算效率。使用 MRP，LLM 推理分两个阶段运行。最初，LLM 使用任务输入线索和可用方法的目标描述来确定最合适的推理方法。随后，它应用所选方法来完成任务。这种动态策略反映了人类的元推理，使模型能够在广泛的问题领域中脱颖而出。我们通过综合基准评估了 MRP 的有效性。结果表明，MRP 在不同任务中均达到或接近最先进的性能。MRP 代表了使 LLM 能够识别跨问题的认知挑战并利用不同推理方法的优势的重大进步，从而增强了它们高效处理多样化和复杂问题领域的能力。每个 LLM 都应该有一个元推理提示，以充分发挥其潜力，并确保在不断变化的挑战和应用环境中的适应性。||
|**2024-06-17**|[HoLLMwood: Unleashing the Creativity of Large Language Models in Screenwriting via Role Playing](http://arxiv.org/abs/2406.11683)|null|生成式人工智能在计算机视觉领域展现出了前所未有的创造力，但在自然语言处理领域尚未观察到类似现象。特别是，由于文学创作的高度复杂性，大型语言模型（LLM）难以创作出达到人类专家水平的文学作品。在本文中，我们提出了 HoLLMwood，这是一个自动化框架，旨在释放 LLM 的创造力并探索其在剧本创作方面的潜力，而剧本创作是一项要求极高的任务。模仿人类创作过程，我们将 LLM 分配给现实场景中的不同角色。除了将 LLM 视为“编剧”的常见做法外，我们还将 LLM 用作“编辑”，负责向“编剧”提供反馈和修改建议。此外，为了丰富角色和深化情节，我们引入了角色扮演机制，并将 LLM 用作可以相互交流和互动的“演员”。对自动生成剧本的评估表明，HoLLMwood 在连贯性、相关性、趣味性和整体质量方面明显优于强大的基线模型。||

<p align=right>(<a href=#updated-on-20240629>back to top</a>)</p>

## Transformer

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-25**|[Transformer Normalisation Layers and the Independence of Semantic Subspaces](http://arxiv.org/abs/2406.17837)|null|最近的研究表明，transformer可以通过在内部执行称为电路的计算图来解决上下文推理任务。电路通常使用注意力机制从表示的子空间中逻辑匹配信息，例如使用序列中的位置来识别前一个标记。在这项工作中，我们将语义子空间视为潜在表示的任何独立子空间，它可以完全确定注意力分布。我们发现，除非模型学习正交球体的严格表示结构，否则最先进的transformer使用的规范化层位置Pre-Norm会破坏这种能力。这是因为它会导致线性子空间通过其共同的规范化因子发生干扰。理论上，我们通过将这种干扰建模为查询/键/值向量的 $L_2$-范数上的随机噪声来分析电路稳定性，预测当稀疏注意力转移到不同标记时会出现电路崩溃现象。根据经验，我们研究了针对数学加法训练的现实世界模型的敏感性，观察到当范数被人为扰动$\lesssim$ 10%时，电路崩溃率为1%。我们将Pre-Norm与QKV-Norm进行对比，后者将规范化放在注意力头的线性算子之后。从理论上讲，这放宽了表示约束。根据经验，我们观察到分布内性能相当，但分布外性能较差。||
|**2024-06-25**|[Brain Tumor Classification using Vision Transformer with Selective Cross-Attention Mechanism and Feature Calibration](http://arxiv.org/abs/2406.17670)|null|脑肿瘤分类是医学图像分析中一项具有挑战性的任务。在本文中，我们提出了一种使用具有新型交叉注意力机制的视觉Transformer进行脑肿瘤分类的新方法。我们的方法利用了Transformer在建模远程依赖和多尺度特征融合方面的优势。我们引入了两种新机制来提高交叉注意力融合模块的性能：特征校准机制（FCM）和选择性交叉注意力（SCA）。FCM校准来自不同分支的特征，使其更加兼容，而SCA选择性地关注最具信息量的特征。我们的实验表明，所提出的方法在脑肿瘤分类方面优于其他最先进的方法，实现了更高的准确性和效率。所提出的FCM和SCA机制可以很容易地集成到其他视觉Transformer架构中，使其成为未来医学图像分析研究的一个有希望的方向。实验结果证实，我们的方法优于现有方法，在脑肿瘤分类任务中实现了最先进的性能。||
|**2024-06-24**|[Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers](http://arxiv.org/abs/2406.16747)|null|在自回归 Transformer 中有效地处理长序列，尤其是在扩展的上下文窗口内，由于自注意力机制固有的二次计算复杂性和大量的 KV 内存需求，带来了重大挑战。在这项工作中，我们介绍了 SPARSEK 注意力，这是一种新颖的稀疏注意力机制，旨在克服这些计算和内存障碍，同时保持性能。我们的方法集成了一个评分网络和一个可微分的 top-k 掩码算子 SPARSEK，为每个查询选择恒定数量的 KV 对，从而实现基于梯度的优化。因此，SPARSEK 注意力在生成过程中提供了线性时间复杂度和恒定的内存占用。实验结果表明，SPARSEK 注意力优于以前的稀疏注意力方法，并在训练和推理过程中显著提高了速度，特别是在语言建模和下游任务中。此外，我们的方法可以无缝集成到预训练的大型语言模型 (LLM) 中，只需最少的微调，为有效管理各种应用中的长期依赖性提供了一种实用的解决方案。||
|**2024-06-24**|[Multi-Modal Vision Transformers for Crop Mapping from Satellite Image Time Series](http://arxiv.org/abs/2406.16513)|null|利用不同卫星传感器获取的图像已被证明可以提高基于卫星图像时间序列 (SITS) 的作物分类性能。现有的最先进架构使用自注意力机制来处理时间维度，并使用卷积来处理 SITS 的空间维度。受基于纯注意力的架构在单模态 SITS 作物分类中的成功的启发，我们引入了几种基于多模态多时相 Transformer 的架构。具体来说，我们研究了时空调 Vision Transformer (TSViT) 中早期融合、交叉注意力融合和同步类标记融合的有效性。实验结果表明，与同时包含卷积和自注意力组件的最先进架构相比，我们的方法具有显著的改进。||
|**2024-06-21**|[GeoLRM: Geometry-Aware Large Reconstruction Model for High-Quality 3D Gaussian Generation](http://arxiv.org/abs/2406.15333)|**[link](https://github.com/alibaba-yuanjing-aigclab/geolrm)**|在这项工作中，我们介绍了几何感知大型重建模型 (GeoLRM)，这是一种仅需 11 GB GPU 内存即可通过 512k 个高斯函数和 21 张输入图像预测高质量资产的方法。以前的工作忽略了 3D 结构的固有稀疏性，并且没有利用 3D 和 2D 图像之间的显式几何关系。这将这些方法限制在低分辨率表示，并且难以扩展到密集视图以获得更好的质量。GeoLRM 通过结合一种新颖的 3D 感知transformer结构来解决这些问题，该结构直接处理 3D 点并使用可变形交叉注意机制将图像特征有效地集成到 3D 表示中。我们通过一个两阶段流程来实现此解决方案：首先，轻量级proposal网络从姿态图像输入生成一组稀疏的 3D 锚点；随后，专门的重建transformer细化几何形状并检索纹理细节。广泛的实验结果表明，GeoLRM 明显优于现有模型，尤其是在密集视图输入方面。我们还通过 3D 生成任务展示了我们模型的实际适用性，展示了其多功能性和在现实世界应用中更广泛采用的潜力。||
|**2024-06-21**|[MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression](http://arxiv.org/abs/2406.14909)|**[link](https://github.com/thu-nics/moa)**|稀疏注意力能够有效缓解大型语言模型 (LLM) 在长上下文中显著的内存和吞吐量需求。现有方法通常采用统一的稀疏注意力掩码，对不同的注意力头和输入长度应用相同的稀疏模式。然而，这种统一的方法无法捕捉到 LLM 中固有的多样化注意力模式，忽略了它们不同的精度-延迟权衡。为了应对这一挑战，我们提出了注意力混合 (MoA)，它可以针对不同的注意力头和层自动定制不同的稀疏注意力配置。MoA 构建并导航一个由各种注意力模式及其相对于输入序列长度的缩放规则组成的搜索空间。它对模型进行分析，评估潜在的配置，并确定最佳的稀疏注意力压缩方案。MoA 适应不同的输入大小，揭示了一些注意力头会扩展它们的关注范围以适应更长的序列，而其他注意力头则始终专注于固定长度的局部上下文。实验表明，在相同的平均注意力跨度下，MoA 将有效上下文长度增加了 3.9 倍，在 Vicuna-7B、Vicuna-13B 和 Llama3-8B 模型中，检索精度比统一注意力基线提高了 1.5-7.1 倍。此外，MoA 缩小了稀疏模型和密集模型之间的性能差距，在两个长上下文理解基准测试中，将最大相对性能下降从 9%-36% 降低到 5% 以内。MoA 在单个 GPU 上将 7B 和 13B 密集模型的 GPU 内存减少了 1.2-1.4 倍，并将解码吞吐量提高了 5.5-6.7 倍，而对性能的影响微乎其微。||
|**2024-06-20**|[Revealing Vision-Language Integration in the Brain with Multimodal Networks](http://arxiv.org/abs/2406.14481)|null|我们使用（多）模态深度神经网络 (DNN) 来探测人脑中多模态整合的位点，方法是预测人类受试者观看电影时记录的脑电图 (SEEG)。我们将多模态整合的位点定义为：多模态视觉语言模型比单模态语言、单模态视觉或线性整合的语言视觉模型能更好地预测记录的区域。我们的目标 DNN 模型涵盖不同的架构（例如，卷积网络和变换器）和多模态训练技术（例如，交叉注意力和对比学习）。作为一个关键的启动步骤，我们首先证明经过训练的视觉和语言模型在预测 SEEG 信号的能力方面系统地优于随机初始化的模型。然后，我们将单模态和多模态模型相互比较。由于我们的目标 DNN 模型通常具有不同的架构、参数数量和训练集（可能会掩盖那些归因于整合的差异），因此我们对两个模型（SLIP 和 SimCLR）进行了受控比较，这两个模型除了输入模态外，所有这些属性都保持不变。使用这种方法，我们确定了大量的神经位点（平均 1090 个位点中的 141 个，或 12.94%）和似乎发生多模态整合的大脑区域。此外，我们发现在我们评估的多模态训练技术变体中，CLIP 风格的训练最适合在下游预测这些位点的神经活动。||
|**2024-06-20**|[SimulSeamless: FBK at IWSLT 2024 Simultaneous Speech Translation](http://arxiv.org/abs/2406.14177)|**[link](https://github.com/hlt-mt/fbk-fairseq)**|本文介绍了FBK参加IWSLT 2024同步翻译评测活动的情况。在今年的语音到文本翻译 (ST) 子赛道中，我们提出了SimulSeamless，它结合了AlignAtt和中等配置的SeamlessM4T模型。SeamlessM4T模型是“开箱即用”的，它的同步推理是通过采用AlignAtt实现的，AlignAtt是一种基于交叉注意力的SimulST策略，无需对底层模型进行任何再训练或针对同步任务的调整即可应用。我们参与了所有共享任务语言（英语到德语、日语、汉语，以及捷克语到英语），取得了与去年提交的结果相当甚至更好的结果。SimulSeamless涵盖了143多种源语言和200多种目标语言，已发布在：https://github.com/hlt-mt/FBK-fairseq/。||
|**2024-06-20**|[Feature Fusion Based on Mutual-Cross-Attention Mechanism for EEG Emotion Recognition](http://arxiv.org/abs/2406.14014)|**[link](https://github.com/ztony0712/MCA)**|对于心理学家来说，客观准确的情绪诊断参考至关重要，尤其是在面对因病理原因难以沟通的患者时。然而，目前基于脑电图（EEG）数据的情绪识别系统存在一些问题，包括模型过于复杂、准确率平庸以及可解释性有限。因此，我们提出了一种新颖有效的特征融合机制，称为互交叉注意机制（MCA）。这种纯数学机制结合专门定制的三维卷积神经网络（3D-CNN），能够巧妙地发现脑电图数据中时域和频域特征之间的互补关系。此外，新设计的Channel-PSD-DE 3D特征也有助于实现高性能。最终，该方法在DEAP数据集上实现了99.49%（效价）和99.30%（唤醒度）的准确率。||
|**2024-06-19**|[StableSemantics: A Synthetic Language-Vision Dataset of Semantic Representations in Naturalistic Images](http://arxiv.org/abs/2406.13735)|null|理解视觉场景的语义是计算机视觉中的一个基本挑战。这项挑战的一个关键方面是，具有相似语义含义或功能的对象可能表现出显著的视觉差异，这使得准确识别和分类变得困难。文本到图像框架的最新进展催生了能够隐式捕捉自然场景统计数据的模型。这些框架考虑了对象的视觉可变性，以及复杂的对象共现和噪声源，例如不同的光照条件。通过利用大规模数据集和交叉注意力调节，这些模型可以生成详细且上下文丰富的场景表示。这种能力为改进复杂环境中的对象识别和场景理解开辟了新途径。我们的工作提出了 StableSemantics，这是一个包含 224,000 个人工策划的提示、处理后的自然语言标题、超过 200 万张合成图像以及对应于各个名词块的 1000 万个注意力图的数据集。我们明确利用了与视觉上有趣的稳定扩散生成相对应的人工生成的提示，每个短语提供 10 个生成，并提取每个图像的交叉注意力图。我们探索了生成图像的语义分布，检查了图像中对象的分布，并在我们的数据上对字幕和开放词汇分割方法进行了基准测试。据我们所知，我们是第一个发布具有语义属性的扩散数据集。我们期望我们提出的数据集能够促进视觉语义理解的进步，并为开发更复杂、更有效的视觉模型奠定基础。网站：https://stablesemantics.github.io/StableSemantics||
|**2024-06-19**|[In-Context Former: Lightning-fast Compressing Context for Large Language Model](http://arxiv.org/abs/2406.13618)|null|随着基于Transformer的大型语言模型（LLM）的日益普及，降低其高昂的推理成本已成为一个重要的研究方向。一种有效的方法是压缩较长的输入上下文。现有方法通常利用LLM本身的自注意力机制进行上下文压缩。虽然这些方法已经取得了显著成果，但压缩过程仍涉及二次时间复杂度，这限制了它们的适用性。为了减轻这种限制，我们提出了上下文内编码器（IC-Former）。与之前的方法不同，IC-Former不依赖于目标LLM。相反，它利用交叉注意力机制和少量可学习的摘要标记来直接从上下文词嵌入中提取信息。这种方法显著减少了推理时间，在压缩范围内实现了时间复杂度的线性增长。实验结果表明，我们的方法在压缩过程中仅需要基线1/32的浮点运算，并在保持90%以上基线性能指标的同时，将处理速度提高了68到112倍。总的来说，我们的模型有效地降低了压缩成本，并使实时压缩场景成为可能。||
|**2024-06-19**|[ECAFormer: Low-light Image Enhancement using Cross Attention](http://arxiv.org/abs/2406.13281)|null|弱光图像增强（LLIE）对于自动驾驶至关重要。尽管非常重要，但现有的 LLIE 方法通常优先考虑整体亮度调整的鲁棒性，但这可能会牺牲细节保留。为了克服这一限制，我们提出了通过交叉注意Transformer进行分层相互增强的ECAFormer，这是一种利用双多头自注意力（DMSA）来增强跨尺度的视觉和语义特征的新型网络，并在过程中显著保留细节。ECAFormer 中的交叉注意机制不仅改进了传统的增强技术，而且在保持全局亮度调整和局部细节保留之间的平衡方面表现出色。我们在著名的低照度数据集（包括 SID 和 LOL）上进行了广泛的实验验证，并在黑暗道路场景中进行了额外的测试。与现有方法相比，我们的方法在照度增强和噪声减少方面表现更出色，同时还优化了计算复杂度和参数数量，进一步提升了 SSIM 和 PSNR 指标。我们的项目可在 https://github.com/ruanyudi/ECAFormer 获取。||
|**2024-06-19**|[Diffusion Model-based FOD Restoration from High Distortion in dMRI](http://arxiv.org/abs/2406.13209)|null|纤维方向分布（FOD）是表示弥散磁共振成像（dMRI）数据的常用模型。然而，dMRI 中的成像伪影（如磁化率引起的畸变）会导致信号丢失，并导致 FOD 重建损坏，从而阻碍在受影响的大脑区域（如脑干）中成功进行纤维追踪和连接分析。生成模型，如扩散模型，已成功应用于各种图像恢复任务。然而，它们在 FOD 图像上的应用提出了独特的挑战，因为 FOD 是由球谐函数（SPHARM）表示的 4 维数据，其第 4 维表现出与阶数相关的依赖性。在本文中，我们提出了一种新的用于 FOD 恢复的扩散模型，它可以恢复由畸变伪影引起的信号丢失。我们使用体积-阶数编码来增强扩散模型生成所有 SPHARM 阶数的单个 FOD 体积的能力。此外，我们在生成每个 FOD 体积时，添加了跨所有 SPHARM 阶数提取的交叉注意特征，以捕获 FOD 体积之间与阶数相关的依赖关系。我们还使用高畸变区域周围的低畸变 FOD 对扩散模型进行条件化，以保持生成的 FOD 的几何一致性。我们使用来自英国生物银行的数据（n = 1315）训练和测试了我们的模型。在一个具有真实值的测试集（n = 43）上，我们证明了生成的 FOD 在 FOD 体积的均方根误差和 FOD 峰值的角误差方面的高精度。我们还将我们的方法应用于脑干区域具有较大畸变的测试集（n = 1172），并证明了我们的方法在恢复 FOD 完整性方面的有效性，从而大大提高了受影响大脑区域的纤维束成像性能。||
|**2024-06-19**|[High-Fidelity Facial Albedo Estimation via Texture Quantization](http://arxiv.org/abs/2406.13149)|null|近年来，三维人脸重建方法在形状估计方面取得了显著进展，但高保真面部反照率重建仍然具有挑战性。现有方法依赖于昂贵的光场采集数据来学习面部反照率图。然而，受试者缺乏多样性限制了其恢复高保真结果的能力。在本文中，我们提出了一种新颖的面部反照率重建模型 HiFiAlbedo，它可以直接从单个图像中恢复反照率图，而无需采集反照率数据。我们的主要见解是，反照率图是不受光照影响的纹理图，这使我们能够使用廉价的纹理数据通过消除光照来推导出反照率估计。为此，我们首先收集了大规模的超高分辨率人脸图像，并训练了一个高保真人脸纹理码本。通过使用 FFHQ 数据集和有限的 UV 纹理，我们随后微调编码器，以便在图像和 UV 空间中使用对抗性监督从输入图像重建纹理。最后，我们训练了一个交叉注意力模块并利用组身份损失来学习从面部纹理到反照率域的适应。大量实验表明，我们的方法具有出色的泛化能力，并且能够实现野外人脸反照率恢复的高保真结果。我们的代码、预训练权重和训练数据将在 https://hifialbedo.github.io/ 上公开。||
|**2024-06-18**|[The Wisdom of a Crowd of Brains: A Universal Brain Encoder](http://arxiv.org/abs/2406.12179)|null|图像到功能性核磁共振 (fMRI) 编码对于神经科学研究和实际应用都很重要。然而，这种“大脑编码器”通常是针对每个受试者和每个 fMRI 数据集进行训练的，因此仅限于非常有限的训练数据。在本文中，我们提出了一种通用大脑编码器，它可以利用来自许多不同受试者/数据集/机器的数据进行联合训练。 使这成为可能的是我们新的以体素为中心的编码器架构，该架构可以为每个大脑体素学习一个独特的“体素嵌入”。我们的编码器通过直接计算大脑体素嵌入和多级深度图像特征之间的交叉注意，来训练预测每个大脑体素对每个图像的响应。这种以体素为中心的架构允许每个大脑体素的功能作用从体素图像交叉注意中自然而然地出现。我们展示了这种方法的强大功能，可以 (i) 结合来自多个不同受试者（“大脑人群”）的数据，以改善每个个体的大脑编码，(ii) 快速有效地跨受试者、数据集和机器（例如，3 特斯拉、7 特斯拉）进行迁移学习，只需少量训练样本，以及 (iii) 使用学习到的体素嵌入作为探索大脑功能的强大工具（例如，大脑中哪些区域编码了哪些信息）。||
|**2024-06-17**|[Crossfusor: A Cross-Attention Transformer Enhanced Conditional Diffusion Model for Car-Following Trajectory Prediction](http://arxiv.org/abs/2406.11941)|null|车辆轨迹预测对于推进自动驾驶和高级驾驶辅助系统 (ADAS) 至关重要，可提高道路安全和交通效率。虽然传统方法奠定了基础，但现代深度学习技术，特别是基于 Transformer 的模型和生成方法，通过捕捉车辆运动和交通交互中复杂和非线性的模式，显著提高了预测精度。然而，这些模型往往忽略了现实驾驶场景中必不可少的详细跟车行为和车辆间交互。本研究介绍了一种专为跟车轨迹预测设计的交叉注意力 Transformer 增强型条件扩散模型 (Crossfusor)。Crossfusor 将详细的车辆间交互和跟车动力学集成到一个强大的扩散框架中，提高了预测轨迹的准确性和真实性。该模型利用一种结合了 GRU、基于位置的注意力机制和傅里叶嵌入的新型时间特征编码框架来捕捉历史车辆动力学。它在正向扩散过程中采用由这些编码的历史特征缩放的噪声，并在反向去噪过程中使用交叉注意力 Transformer 对复杂的车辆间依赖关系进行建模。在 NGSIM 数据集上的实验结果表明，Crossfusor 优于最先进的模型，特别是在长期预测方面，展示了其增强自动驾驶系统预测能力的潜力。||
|**2024-06-17**|[Composing Object Relations and Attributes for Image-Text Matching](http://arxiv.org/abs/2406.11820)|null|我们研究了用于图像-文本匹配的视觉语义嵌入问题。大多数现有工作利用定制的交叉注意力机制来执行跨图像和文本两种模态的局部对齐。尽管这种方法比单模态双编码器方法更强大，但计算成本很高。本工作介绍了一种双编码器图像-文本匹配模型，利用场景图来表示字幕，其中节点表示对象和属性，并通过关系边相互连接。我们的模型利用图注意力网络，有效地编码了对象-属性和对象-对象语义关系，从而形成了一个鲁棒且快速的系统。将字幕表示为场景图，可以利用图神经网络强大的关系归纳偏差来有效地学习对象-属性和对象-对象关系。为了训练模型，我们提出了在整体级别（图像-字幕）和局部级别（图像-对象实体）上对齐图像和字幕的损失函数，我们证明这是模型成功的关键。我们的模型被称为对象关系和属性组合模型，简称CORA。在两个著名的图像-文本检索基准数据集Flickr30K和MSCOCO上的实验结果表明，CORA在召回率方面优于现有的计算成本高昂的交叉注意力方法，同时实现了双编码器的快速计算速度。||
|**2024-06-17**|[AnyMaker: Zero-shot General Object Customization via Decoupled Dual-Level ID Injection](http://arxiv.org/abs/2406.11643)|**[link](https://github.com/lingjiekong-fdu/anymaker)**|基于文本到图像的对象定制旨在根据文本提示和参考图像生成具有相同身份（ID）的图像，这一领域已取得重大进展。然而，最近的定制研究主要集中在特定任务上，例如人物定制或虚拟试穿，而忽略了通用的对象定制。为此，我们推出了 AnyMaker，这是一个创新的零样本对象定制框架，能够生成具有高度ID保真度和灵活文本可编辑性的通用对象。AnyMaker 的功效源于其新颖的通用 ID 提取、双层 ID 注入和 ID 感知解耦。具体来说，通用 ID 提取模块使用一组自监督模型提取足够的 ID 信息，以处理针对通用对象的各种定制任务。然后，为了在不损害生成过程中文本可编辑性的情况下，为扩散 UNet 提供尽可能多的提取 ID 信息，我们设计了一个全局-局部双层 ID 注入模块，其中全局语义 ID 被注入文本描述中，而局部 ID 细节则通过新添加的交叉注意模块直接注入模型。此外，我们提出了一个 ID 感知解耦模块，用于从提取的表示中分离与 ID 相关的信息和非 ID 元素，以实现身份和文本描述的高保真生成。为了验证我们的方法并促进通用对象定制的研究，我们创建了第一个大规模通用 ID 数据集，即多类别 ID 一致性（MC-IDC）数据集，包含 31.5 万个文本图像样本和 1 万个类别。实验表明，AnyMaker 在通用对象定制方面表现出色，并在相应任务中优于专门方法。代码和数据集将很快发布。||
|**2024-06-17**|[Simple Yet Efficient: Towards Self-Supervised FG-SBIR with Unified Sample Feature Alignment](http://arxiv.org/abs/2406.11551)|**[link](https://github.com/exponentiai/effnet)**|细粒度草图图像检索 (FG-SBIR) 旨在最小化嵌入空间中草图和对应图像之间的距离。然而，解决方案日益复杂，主要是因为细粒度草图的抽象性，阻碍了可扩展性。在本文中，我们提出了一种简单而有效的方法来缩小两种模式之间的差距。它主要促进了样本内和样本间统一的互信息共享，而不是将它们视为模态之间的单一特征对齐问题。具体来说，我们的方法包括：(i) 采用双权重共享网络来优化草图和图像域内的对齐，这也有效地缓解了模型学习饱和问题。(ii) 引入基于对比损失的目标优化函数，以增强模型在样本内和样本间对齐特征的能力。(iii) 提出了一种由自注意力和交叉注意力相结合的可学习 TRSM，以促进标记之间的特征表示，进一步增强嵌入空间中的样本对齐。我们的框架在基于 CNN 和 ViT 的骨干网络上取得了优异的结果。大量实验表明，它优于现有方法。我们还介绍了 Cloths-V1，这是第一个专业的服装草图和图像数据集，用于验证我们的方法，并将有利于其他应用。||
|**2024-06-17**|[Learning from Exemplars for Interactive Image Segmentation](http://arxiv.org/abs/2406.11472)|null|交互式图像分割使用户能够以最少的交互与机器进行交互，从而逐步细化目标对象的分割掩码。先前的研究已经证明了通过交互式分割提取单个目标掩码的出色性能。然而，现有方法忽略了先前交互对象的语义线索，这些线索可以被进一步探索以加速对同一类别中多个目标的交互式分割。为此，我们针对单一对象和同一类别中的多个对象引入了新颖的交互式分割框架。具体来说，我们的模型利用 Transformer 骨干网络从图像和交互中提取以交互为中心的视觉特征，以获得令人满意的目标掩码作为样本。对于多个对象，我们提出了一个样本信息模块，以增强对目标类别对象之间相似性的学习。为了组合来自不同模块的注意力特征，我们结合了交叉注意力块和特征融合模块。在主流基准数据集上进行的实验表明，与以前的方法相比，我们的模型取得了优越的性能。特别是，我们的模型将用户的劳动量减少了约 15%，需要少点击两次才能达到目标 IoU 85% 和 90%。结果突出了我们的模型作为灵活实用的标注工具的潜力。源代码将在发表后发布。||
|**2024-06-17**|[Analysing the Behaviour of Tree-Based Neural Networks in Regression Tasks](http://arxiv.org/abs/2406.11437)|**[link](https://github.com/petersamoaa/tree_based_nn_error_analysis)**|深度学习领域，特别是利用抽象语法树 (AST) 等结构化表示，极大地扩展了源代码分析的边界。虽然这些方法已在分类任务中表现出有效性，但它们在回归应用（例如根据源代码预测执行时间）中的效果仍有待探索。本文致力于解码基于树的神经网络模型在此类回归挑战中的行为。我们将已建立的模型（基于树的卷积神经网络 (CNN)、Code2Vec 和基于 Transformer 的方法）的应用扩展到通过将源代码解析为 AST 来预测其执行时间。我们的比较分析表明，虽然这些模型是代码表示的基准，但在执行回归任务时表现出局限性。为了解决这些缺陷，我们提出了一种新颖的双 Transformer 方法，该方法同时对源代码标记和 AST 表示进行操作，并采用交叉注意力机制来增强两个域之间的可解释性。此外，我们还探索了图神经网络 (GNN) 对此基于树的问题的适应性，并从理论上论证了由于 AST 的图形性质而导致的 inherent 兼容性。对真实世界数据集的实证评估表明，我们的双 Transformer 模型优于所有其他基于树的神经网络和基于 GNN 的模型。此外，我们提出的双 Transformer 在不同的数据集上表现出卓越的适应性和鲁棒性能。||
|**2024-06-17**|[DiTTo-TTS: Efficient and Scalable Zero-Shot Text-to-Speech with Diffusion Transformer](http://arxiv.org/abs/2406.11427)|null|大规模扩散模型在图像、视频和音频等多种模态中展现出卓越的生成能力。然而，文本到语音（TTS）系统通常涉及特定领域的建模因素（例如，音素和音素级时长）以确保文本和语音之间精确的时间对齐，这阻碍了扩散模型在 TTS 中的效率和可扩展性。在这项工作中，我们提出了一种高效且可扩展的扩散Transformer（DiT），它利用现成的预训练文本和语音编码器。我们的方法通过交叉注意力机制和语音表示总长度的预测来解决文本语音对齐的挑战。为此，我们增强了 DiT 架构以适应 TTS，并通过将语义指导纳入语音的潜在空间来改进对齐。我们将训练数据集和模型大小分别扩展到 82K 小时和 7.9 亿个参数。我们广泛的实验表明，这种无需特定领域建模的大规模 TTS 扩散模型不仅简化了训练流程，而且在自然度、清晰度和说话人相似度方面，其零样本性能优于或可与最先进的 TTS 模型相媲美。我们的语音样本可在 https://ditto-tts.github.io 获取。||
|**2024-06-16**|[STAR: Scale-wise Text-to-image generation via Auto-Regressive representations](http://arxiv.org/abs/2406.10797)|**[link](https://github.com/krennic999/STAR)**|我们提出了STAR，这是一个采用尺度自回归范式的文本到图像模型。与仅限于在预定类别集合内进行类别条件合成的VAR不同，我们的STAR通过三个关键设计实现了文本驱动的开放式生成：为了提高具有未见对象和概念组合的多样性和泛化能力，我们引入了一个预训练的文本编码器来提取文本约束的表示，然后将其用作指导。为了改善生成的图像与细粒度文本指导之间的交互，使结果更可控，我们在每个尺度上都加入了额外的交叉注意力层。鉴于不同尺度之间存在自然的结构关联，我们利用二维旋转位置编码（RoPE）并将其调整为归一化版本。这确保了对不同尺度标记图中相对位置的一致解释，并稳定了训练过程。大量实验表明，STAR在保真度、图像文本一致性和美学质量方面都超过了现有基准。我们的研究结果强调了自回归方法在高质量图像合成领域的潜力，为目前以扩散方法为主导的T2I领域提供了新的方向。||
|**2024-06-15**|[CrossFuse: A Novel Cross Attention Mechanism based Infrared and Visible Image Fusion Approach](http://arxiv.org/abs/2406.10581)|**[link](https://github.com/hli1221/crossfuse)**|多模态视觉信息融合旨在将多传感器数据集成到包含更多互补信息和更少冗余特征的单个图像中。然而，互补信息很难提取，特别是对于红外和可见光图像，这两种模态之间存在很大的相似性差距。常见的交叉注意力模块只考虑相关性，相反，图像融合任务需要关注互补性（不相关性）。因此，本文提出了一种新的交叉注意机制（CAM）来增强互补信息。此外，提出了一种基于两阶段训练策略的融合方案来生成融合图像。在第一阶段，针对每种模态训练两个具有相同架构的自动编码器网络。然后，在固定编码器的情况下，在第二阶段训练CAM和解码器。利用训练好的CAM，将从两种模态中提取的特征融合成一个融合特征，其中增强了互补信息，减少了冗余特征。最后，可以通过训练好的解码器生成融合图像。实验结果表明，与现有的融合网络相比，我们提出的融合方法获得了最先进的融合性能。代码可在https://github.com/hli1221/CrossFuse获取。||
|**2024-06-14**|[Whisper-Flamingo: Integrating Visual Features into Whisper for Audio-Visual Speech Recognition and Translation](http://arxiv.org/abs/2406.10082)|**[link](https://github.com/roudimit/whisper-flamingo)**|音频-视觉语音识别 (AVSR) 利用基于嘴唇的视频来提高在噪声环境下的性能。由于视频比音频更难获取，AVSR 模型的视频训练数据通常只有几千小时。相比之下，Whisper 等语音模型使用数十万小时的数据进行训练，因此可以学习到更好的语音到文本解码器。巨大的训练数据差异促使我们调整 Whisper 以处理视频输入。受 Flamingo 将视觉特征注入语言模型的启发，我们提出了 Whisper-Flamingo，它使用门控交叉注意力将视觉特征集成到 Whisper 语音识别和翻译模型中。在嘈杂条件下，我们的音频-视觉 Whisper-Flamingo 在 6 种语言的英语语音识别和英语-X 翻译方面优于纯音频 Whisper。此外，Whisper-Flamingo 是一个多功能模型，可以使用一组参数执行所有这些任务，而先前的方法是在每种语言上单独训练的。||
|**2024-06-14**|[Simul-Whisper: Attention-Guided Streaming Whisper with Truncation Detection](http://arxiv.org/abs/2406.10052)|**[link](https://github.com/backspacetg/simul_whisper)**|作为一种强大且规模庞大的多语言语音识别模型，Whisper 在许多低资源和 out-of-distribution 场景中表现出色。然而，其编码器-解码器结构阻碍了其在流式语音识别中的应用。在本文中，我们介绍了 Simul-Whisper，它利用 Whisper 交叉注意力机制中嵌入的时间对齐信息来指导自回归解码，并在无需对预训练模型进行任何微调的情况下实现基于块的流式自动语音识别。此外，我们观察到块边界处截断词对解码结果的负面影响，并提出了一种基于脉冲神经元的截断检测模型来解决这个问题。在多种语言和 Whisper 架构上的实验表明，Simul-Whisper 在 1 秒的块大小下平均绝对词错误率仅下降 1.46%，明显优于当前最先进的基线模型。||
|**2024-06-14**|[HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning](http://arxiv.org/abs/2406.09827)|null|在现代大型语言模型（LLM）中，增加序列长度是提高其在处理多模态问答等复杂任务时的理解能力和连贯性的关键挑战。然而，由于传统的注意力机制具有二次时间和空间复杂度，因此使用 LLM 处理长上下文序列的成本非常高，而且上下文窗口大小受限于 GPU 内存。尽管最近的研究已经提出了线性和稀疏注意力机制来解决这个问题，但由于需要重新训练预训练模型，它们的实际适用性往往受到限制。为了解决这个问题，我们提出了一种新方法，即分层剪枝注意力（HiP），它可以同时将训练和推理时间复杂度从 $O(T^2)$ 降低到 $O(T \log T)$，并将空间复杂度从 $O(T^2)$ 降低到 $O(T)$ 。为此，我们设计了一种动态稀疏注意力机制，该机制通过一种新颖的类似树搜索的算法，针对给定的查询动态生成注意力掩码。HiP 无需训练，因为它只利用预训练的注意力分数来找出每个查询中最重要的前 k 个元素的位置。此外，它确保不会遗漏任何标记，这与基于滑动窗口的次二次注意力方法（如 StreamingLLM）不同。在各种真实世界基准测试上的大量实验表明，HiP 可以显著减少提示（即预填充）和解码延迟以及内存使用量，同时保持较高的生成性能，几乎没有或根本没有退化。由于 HiP 易于即插即用部署，因此它允许预训练的 LLM 在商用 GPU 上扩展到数百万个标记，而无需额外的工程工作，我们相信我们的工作将产生巨大的实际影响，为许多以前不可行的长上下文 LLM 应用开辟了可能性。||
|**2024-06-13**|[Memory-Efficient Sparse Pyramid Attention Networks for Whole Slide Image Analysis](http://arxiv.org/abs/2406.09333)|null|全切片图像（WSI）对于现代病理诊断至关重要，但其千兆像素级的分辨率和稀疏的信息区域带来了巨大的计算挑战。传统的密集注意力机制广泛应用于计算机视觉和自然语言处理，但由于数据规模庞大和对无信息区域的冗余处理，因此不适用于WSI分析。为了应对这些挑战，我们从其他领域中最先进的稀疏注意力技术中汲取灵感，提出了具有移位窗口的内存高效型稀疏金字塔注意力网络（SPAN）。SPAN引入了一种稀疏金字塔注意力架构，该架构分层地关注WSI内的信息区域，旨在在保留关键特征的同时减少内存开销。此外，移位窗口的结合使模型能够捕获对准确分类至关重要的远程上下文依赖关系。我们在多个公共WSI数据集上评估了SPAN，观察到其具有竞争力的性能。与由于内存限制而经常难以对空间和上下文信息进行建模的现有方法不同，我们的方法能够对这些关键特征进行准确建模。我们的研究还强调了注意力机制中关键设计元素的重要性，例如移位窗口方案和层次结构，它们对SPAN在WSI分析中的有效性做出了重大贡献。因此，证明了SPAN在内存高效且有效地分析WSI数据方面的潜力，并且代码将在本工作发表后公开。||

<p align=right>(<a href=#updated-on-20240629>back to top</a>)</p>

## 3DGS

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-21**|[Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks](http://arxiv.org/abs/2406.15149)|null|仿真器是自主机器人学习的强大工具，因为它们可以提供可扩展的数据生成、灵活的设计和轨迹优化。然而，将从仿真数据中学习到的行为迁移到现实世界中被证明是困难的，通常需要通过计算量大的域随机化方法或进一步的模型微调来缓解。我们提出了一种改进泛化能力和对模拟到真实视觉四旋翼导航任务中分布偏移的鲁棒性的方法。为此，我们首先通过将高斯 splatting 与四旋翼飞行动力学相结合来构建模拟器，然后使用 Liquid 神经网络训练鲁棒的导航策略。通过这种方式，我们获得了一个全栈模仿学习协议，它结合了 3D 高斯 splatting 辐射场渲染的进步、专家演示训练数据的巧妙编程以及 Liquid 网络的任务理解能力。通过一系列定量飞行测试，我们证明了在单个模拟场景中学习到的导航技能可以直接稳健地迁移到现实世界。我们进一步展示了在剧烈的分布和物理环境变化下，在训练环境之外保持性能的能力。我们学习的 Liquid 策略，仅在从逼真的模拟室内飞行中精选的单个目标操作上进行训练，可以泛化到户外真实硬件平台上的多步远足。||
|**2024-06-21**|[E2GS: Event Enhanced Gaussian Splatting](http://arxiv.org/abs/2406.14978)|**[link](https://github.com/deguchihiroyuki/e2gs)**|事件相机以其高动态范围、无运动模糊和低能耗等特点而闻名，近年来由于这些特性，事件相机得到了广泛的应用。在过去几年中，基于事件的三维重建领域取得了显著进展，其中基于神经辐射场 (NeRF) 的方法展示了逼真的视图合成结果。然而，NeRF 的体渲染范式需要大量的训练和渲染时间。在本文中，我们介绍了事件增强型高斯散射 (E2GS)，这是一种将事件数据融入高斯散射的新方法，该方法最近在视图合成领域取得了重大进展。我们的 E2GS 有效地利用了模糊图像和事件数据，显著改善了图像去模糊效果，并产生了高质量的视图合成。我们在合成数据集和真实世界数据集上进行的全面实验表明，我们的 E2GS 可以生成视觉上吸引人的渲染结果，同时提供更快的训练和渲染速度 (140 FPS)。 我们的代码可在 https://github.com/deguchihiroyuki/E2GS 获取。||
|**2024-06-18**|[Sampling 3D Gaussian Scenes in Seconds with Latent Diffusion Models](http://arxiv.org/abs/2406.13099)|null|我们提出了一种针对3D场景的潜在扩散模型，该模型可以使用仅2D图像数据进行训练。为了实现这一点，我们首先设计了一个自动编码器，它将多视图图像映射到3D高斯斑点，并同时构建这些斑点的压缩潜在表示。然后，我们在潜在空间上训练一个多视图扩散模型，以学习一个高效的生成模型。该流程不需要对象掩码或深度信息，适用于具有任意相机位置的复杂场景。我们在两个大型复杂真实场景数据集（MVImgNet 和 RealEstate10K）上进行了仔细的实验。我们证明，我们的方法能够在短短 0.2 秒内生成 3D 场景，无论是从头开始生成，从单个输入视图生成，还是从稀疏输入视图生成。它可以生成多样化的高质量结果，同时运行速度比非潜在扩散模型和早期的基于 NeRF 的生成模型快一个数量级。||
|**2024-06-18**|[HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors](http://arxiv.org/abs/2406.12459)|**[link](https://github.com/humansplat/humansplat.github.io)**|尽管高保真人体重建技术取得了最新进展，但对密集捕获图像或耗时的每实例优化的要求极大地阻碍了其在更广泛场景中的应用。为了解决这些问题，我们提出了HumanSplat，它可以从单个输入图像中以可泛化的方式预测任何人的3D高斯样条属性。具体来说，HumanSplat包含一个2D多视图扩散模型和一个具有人体结构先验的潜在重建Transformer，它能够将几何先验和语义特征巧妙地集成到一个统一的框架中。此外，我们还设计了一个包含人体语义信息的层次化损失函数，以实现高保真纹理建模并更好地约束估计的多个视图。在标准基准数据集和真实世界图像上的综合实验表明，HumanSplat在实现逼真的新视图合成方面优于现有的最先进方法。||
|**2024-06-17**|[A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets](http://arxiv.org/abs/2406.12080)|null|近年来，新视角合成技术取得了重大进展，其中 3D 高斯渲染技术提供了出色的视觉质量、快速的训练速度和实时渲染能力。然而，训练和渲染所需的资源不可避免地限制了能够以良好视觉质量表示的捕获场景的大小。我们引入了一种 3D 高斯层次结构，它可以在保留超大场景视觉质量的同时，为高效渲染远处内容提供一种高效的细节级别 (LOD) 解决方案，并在不同级别之间实现有效的级别选择和平滑过渡。我们引入了一种分治法，使我们能够以独立块的形式训练超大场景。我们将这些块合并成一个层次结构，通过优化该层次结构可以进一步提高合并到中间节点的高斯的视觉质量。超大规模的捕获通常对场景的覆盖范围有限，这对原始 3D 高斯渲染训练方法提出了许多挑战；我们对训练进行了调整和规范化，以解决这些问题。我们提出了一个完整的解决方案，该方案能够实时渲染超大场景，并且由于采用了我们的 LOD 方法，可以适应可用的资源。我们展示了使用简单且价格合理的装备捕获的场景的结果，这些场景包含多达数万张图像，覆盖了长达数公里、持续时间长达一小时的轨迹。项目页面：https://repo-sam.inria.fr/fungraph/hierarchical-3d-gaussians/||
|**2024-06-17**|[RetinaGS: Scalable Training for Dense Scene Rendering with Billion-Scale 3D Gaussians](http://arxiv.org/abs/2406.11836)|null|在这项工作中，我们探索了在大规模、高分辨率数据集上训练高参数 3D 高斯 splatting (3DGS) 模型的可能性。我们为 3DGS 设计了一种通用的模型并行训练方法，名为 RetinaGS，它使用适当的渲染方程，可以应用于任何场景和任意分布的高斯图元。它使我们能够探索 3DGS 在图元数量和训练分辨率方面的缩放行为，而这在以前是难以探索的，并超越了以前最先进的重建质量。当使用我们的方法增加图元数量时，我们观察到视觉质量有明显的积极趋势。我们还首次尝试在完整的 MatrixCity 数据集上训练具有超过 10 亿个图元的 3DGS 模型，该模型获得了非常有希望的视觉质量。||
|**2024-06-18**|[Effective Rank Analysis and Regularization for Enhanced 3D Gaussian Splatting](http://arxiv.org/abs/2406.11672)|null|从多视图图像进行三维重建是计算机视觉和图形学中的基本挑战之一。近年来，三维高斯 splatting  (3DGS) 已成为一种很有前途的技术，能够进行高质量的实时三维重建。该方法利用三维高斯表示和平铺 splatting 技术，绕过了昂贵的神经场查询。尽管具有潜力，但由于高斯函数会收敛成具有一个主导方差的各向异性高斯函数，3DGS 仍面临着挑战，包括针状伪影、次优几何形状和不准确的法线。我们建议使用有效秩分析来检查三维高斯基元的形状统计数据，并确定高斯函数确实会收敛成有效秩为 1 的针状形状。为了解决这个问题，我们引入有效秩作为正则化，它限制了高斯函数的结构。我们新的正则化方法增强了法线和几何重建，同时减少了针状伪影。该方法可以作为附加模块集成到其他 3DGS 变体中，在不影响视觉保真度的情况下提高其质量。||
|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|在非结构化的旅游环境中拍摄的照片经常呈现出多变的外观和短暂的遮挡，这对精确的场景重建提出了挑战，并在新颖的视图合成中导致了伪影。尽管先前的方法已经将神经辐射场 (NeRF) 与其他可学习模块集成以处理动态外观并消除瞬态对象，但其大量的训练需求和缓慢的渲染速度限制了实际部署。最近，3D 高斯 splatting (3DGS) 已成为 NeRF 的一个有希望的替代方案，它提供了卓越的训练和推理效率以及更好的渲染质量。本文介绍了 Wild-GS，这是 3DGS 的一项创新改编，针对不受约束的照片集进行了优化，同时保留了其效率优势。Wild-GS 通过每个 3D 高斯的固有材质属性、每张图像的全局照明和相机属性以及逐点反射率的局部变化来确定其外观。与先前在图像空间中对参考特征建模的方法不同，Wild-GS 通过对从参考图像中提取的三平面进行采样，将像素外观特征显式地与相应的局部高斯对齐。这种新颖的设计有效地将参考视图的高频细节外观转移到 3D 空间，并显着加快了训练过程。此外，利用 2D 可见性图和深度正则化分别减轻瞬态效应和约束几何形状。大量实验表明，Wild-GS 在所有现有技术中实现了最先进的渲染性能以及最高的训练和推理效率。||
|**2024-06-14**|[L4GM: Large 4D Gaussian Reconstruction Model](http://arxiv.org/abs/2406.10324)|null|我们提出了L4GM，这是第一个能够从单视角视频输入生成动画对象的4D大型重建模型，并且只需一次几秒钟的前馈传递即可完成。我们成功的关键在于一个新颖的多视角视频数据集，其中包含从Objaverse精心策划和渲染的动画对象。该数据集描绘了4.4万个不同的对象，包含11万个以48个视角渲染的动画，产生了1200万个视频，总计3亿帧。为了实现可扩展性，我们保持L4GM的简洁性，并直接构建在LGM的基础上，LGM是一个预训练的3D大型重建模型，可以从多视角图像输入中输出3D高斯椭球体。L4GM从以低帧率采样的视频帧中输出每帧3D高斯 splatting 表示，然后将表示上采样到更高的帧率以实现时间平滑度。我们在基础LGM中添加了时间自注意力层，以帮助它学习跨时间的一致性，并利用每个时间步的多视角渲染损失来训练模型。通过训练一个插值模型将表示上采样到更高的帧率，该模型生成中间3D高斯表示。我们展示了仅在合成数据上训练的L4GM在实际视频中具有极好的泛化能力，可以生成高质量的动画3D资产。||
|**2024-06-14**|[PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting](http://arxiv.org/abs/2406.10219)|null|近年来，新颖视图合成的进步实现了实时渲染速度和高重建精度。三维高斯拟合 (3D-GS) 是一种基础的基于点的参数化三维场景表示方法，将场景建模为大量三维高斯函数的集合。复杂场景可能包含数百万个高斯函数，导致大量的存储和内存需求，限制了 3D-GS 在资源有限的设备上的可行性。目前通过修剪高斯函数来压缩这些预训练模型的技术依赖于结合启发式方法来确定要移除哪些高斯函数。在本文中，我们提出了一种有原则的空间敏感性修剪评分，其性能优于这些方法。它被计算为训练视图上的重建误差相对于每个高斯函数的空间参数的二阶近似。此外，我们提出了一个多轮修剪-优化流程，可以应用于任何预训练的 3D-GS 模型，而无需改变训练流程。在修剪了 88.44% 的高斯函数后，我们观察到我们的 PUP 3D-GS 流程将 3D-GS 的平均渲染速度提高了 2.65 倍，同时保留了更多显著的前景信息，并在 Mip-NeRF 360、Tanks & Temples 和 Deep Blending 数据集的场景上实现了比先前修剪技术更高的图像质量指标。||
|**2024-06-14**|[GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors](http://arxiv.org/abs/2406.10111)|null|从低分辨率输入视图实现高分辨率新视角合成（HRNVS）是一项具有挑战性的任务，因为缺乏高分辨率数据。以前的方法从低分辨率输入视图优化高分辨率神经辐射场（NeRF），但渲染速度慢。在这项工作中，我们基于 3D 高斯 splatting（3DGS）开发了我们的方法，因为它能够以更快的渲染速度生成高质量的图像。为了缓解高分辨率合成的数据短缺问题，我们建议利用现成的 2D 扩散先验，通过分数蒸馏采样（SDS）将 2D 知识提取到 3D 中。然而，由于生成先验带来的随机性，将 SDS 直接应用于基于高斯的 3D 超分辨率会导致不希望出现且冗余的 3D 高斯基元。为了缓解这个问题，我们引入了两种简单而有效的技术来减少 SDS 引入的随机扰动。具体来说，我们 1）使用退火策略缩小 SDS 中扩散时间步长的范围；2）在密集化过程中随机丢弃冗余的高斯基元。大量实验表明，我们提出的 GaussainSR 可以在合成和真实世界数据集上仅使用低分辨率输入就能获得高质量的 HRNVS 结果。项目页面：https://chchnii.github.io/GaussianSR/||
|**2024-06-14**|[GradeADreamer: Enhanced Text-to-3D Generation Using Gaussian Splatting and Multi-View Diffusion](http://arxiv.org/abs/2406.09850)|**[link](https://github.com/trapoom555/gradeadreamer)**|文本到3D生成已经展现出可观的结果，但仍然存在一些普遍的挑战，例如多面 Janus 问题以及生成高质量资源所需的时间过长。在本文中，我们通过引入一种名为 GradeADreamer 的新型三阶段训练流程来解决这些问题。该流程能够仅使用单个 RTX 3090 GPU 在 30 分钟内生成高质量的资源。我们提出的方法采用多视图扩散模型 MVDream 生成高斯点云作为先验，然后使用 StableDiffusion 优化几何形状和纹理。实验结果表明，与之前最先进的方法相比，我们的方法显著减轻了多面 Janus 问题，并实现了最高的平均用户偏好排名。项目代码可在 https://github.com/trapoom555/GradeADreamer 获取。||
|**2024-06-14**|[Unified Gaussian Primitives for Scene Representation and Rendering](http://arxiv.org/abs/2406.09733)|null|在计算机图形学中，寻找一种统一的场景表示方法仍然是一个挑战。传统的基于网格的表示方法不适用于密集、模糊的元素，并且在过滤和可微渲染方面引入了额外的复杂性。相反，基于体素的表示方法难以模拟坚硬的表面，并且存在内存需求大的问题。我们提出了一种基于三维高斯分布的通用渲染基元，用于统一场景表示，其特点是具有从光滑表面到模糊元素的多样化外观，以及基于物理的散射，以实现精确的全局照明。我们基于非指数传输制定了基元的渲染理论，并推导了与蒙特卡洛路径追踪兼容的高效渲染操作。这种新的表示方法可以从不同的来源转换而来，包括网格和三维高斯 splatting，并且由于其可微性，可以通过透射率优化进一步细化。我们展示了该表示方法在全局照明和外观编辑等各种渲染应用中的多功能性，同时本质上支持任意照明条件。此外，我们将我们的表示方法与现有的体积表示方法进行了比较，突出了其在细节再现方面的效率。||
|**2024-06-13**|[Modeling Ambient Scene Dynamics for Free-view Synthesis](http://arxiv.org/abs/2406.09395)|null|我们提出了一种新颖的动态自由视点合成方法，可以从单目捕捉中合成环境场景，为观看体验带来沉浸式的质量。我们的方法建立在3D高斯渲染(3DGS)的最新进展之上，该技术可以忠实地重建复杂的静态场景。先前将3DGS扩展到表示动态的尝试仅限于有界场景或需要多相机捕捉，并且通常无法泛化到未见过的运动，从而限制了它们的实际应用。我们的方法通过利用环境运动的周期性来学习运动轨迹模型，并结合仔细的正则化来克服这些限制。我们还提出了一些重要的实用策略，以提高基线3DGS静态重建的视觉质量，并提高对GPU内存密集型学习至关重要的内存效率。我们展示了几个具有复杂纹理和精细结构元素的周围自然场景的高质量逼真新颖视图合成。||
|**2024-06-13**|[GGHead: Fast and Generalizable 3D Gaussian Heads](http://arxiv.org/abs/2406.09377)|null|从大型二维图像集中学习三维头部先验信息是实现高质量三维感知人体建模的重要步骤。其核心要求是构建一种能够很好地扩展到大型数据集和高分辨率图像的有效架构。遗憾的是，现有的三维生成对抗网络（GAN）由于其训练和渲染速度相对较慢，难以扩展到生成高分辨率样本，并且通常不得不依赖二维超分辨率网络，但这是以牺牲全局三维一致性为代价的。为了应对这些挑战，我们提出了生成式高斯头部（GGHead）方法，该方法在三维 GAN 框架内采用了最新的三维高斯渲染表示。为了生成三维表示，我们采用了一个强大的二维卷积神经网络（CNN）生成器来预测模板头部网格的 UV 空间中的高斯属性。通过这种方式，GGHead 利用了模板 UV 布局的规律性，极大地促进了预测非结构化三维高斯集这一具有挑战性的任务。我们进一步通过一种新颖的渲染 UV 坐标上的总变差损失来提高生成的三维表示的几何保真度。直观地说，这种正则化鼓励相邻渲染像素应该来自模板 UV 空间中的相邻高斯。综上所述，我们的流程可以有效地生成仅从单视图二维图像观察中训练得到的三维头部。我们提出的框架在 FFHQ 数据集上的质量与现有的三维头部 GAN 相当，同时速度明显更快，并且完全保持了三维一致性。因此，我们首次展示了以 1024² 分辨率实时生成和渲染高质量、三维一致的头部。||

<p align=right>(<a href=#updated-on-20240629>back to top</a>)</p>

## 各类学习方式

|Publish Date|Title|Code|Abstract|
|---|---|---|--------------------------------------------------|
|**2024-06-27**|[STAL3D: Unsupervised Domain Adaptation for 3D Object Detection via Collaborating Self-Training and Adversarial Learning](http://arxiv.org/abs/2406.19362)|null|现有的3D目标检测模型由于标注成本高昂且由于域差异导致对未知数据的可迁移性较差，因此存在一些问题。无监督域适应（UDA）旨在将训练在有标签源域的检测模型泛化，使其在未探索的目标域上表现稳健，为跨域3D目标检测提供了一种很有前景的解决方案。尽管基于自训练（ST）的跨域3D检测方法在伪标签技术的辅助下取得了显著进展，但由于缺乏特征分布对齐的过程，当存在显著的域差异时，它们仍然面临着伪标签质量低下的问题。虽然基于对抗学习（AL）的方法可以有效地对齐源域和目标域的特征分布，但由于无法在目标域中获取标签，因此不得不采用非对称优化损失，从而导致了源域偏差这一具有挑战性的问题。为了克服这些限制，我们提出了一种新的基于自训练和对抗学习协作的3D目标检测无监督域适应框架，称为STAL3D，它释放了伪标签和特征分布对齐的互补优势。此外，我们还设计了背景抑制对抗学习（BS-AL）模块和尺度过滤模块（SFM），专门针对3D跨域场景，有效地缓解了背景干扰比例大和源域尺寸偏差的问题。我们的STAL3D在多个跨域任务上实现了最先进的性能，甚至在Waymo $\rightarrow$ KITTI和Waymo $\rightarrow$ KITTI-rain上超过了Oracle结果。||
|**2024-06-27**|[Compositional Image Decomposition with Diffusion Models](http://arxiv.org/abs/2406.19298)|null|给定一幅自然场景图像，我们能够快速将其分解成一组组成部分，例如物体、光照、阴影和前景。然后，我们可以想象这样一个场景：我们将某些组成部分与来自其他图像的组成部分组合起来，例如，将我们卧室里的一组物体和动物园里的动物在森林的光照条件下组合起来，即使我们以前从未遇到过这样的场景。在本文中，我们提出了一种将图像分解成这种组成部分的方法。我们的方法称为“分解扩散”，它是一种无监督方法，当给定单个图像时，它可以推断出图像中的一组不同的组成部分，每个组成部分都由一个扩散模型表示。我们演示了组件如何捕获场景的不同因素，从阴影或面部表情等全局场景描述符到组成对象等局部场景描述符。我们进一步说明了如何灵活地组合推断出的因素，即使是与从其他模型推断出的因素组合，以生成与训练期间看到的场景截然不同的各种场景。网站和代码位于https://energy-based-model.github.io/decomp-diffusion。||
|**2024-06-27**|[Enhancing Continual Learning in Visual Question Answering with Modality-Aware Feature Distillation](http://arxiv.org/abs/2406.19297)|null|持续学习致力于在任务序列上增量训练模型，目标是在学习新任务的同时最大程度地减少对先前任务的性能下降。现有的持续学习和视觉问答 (VQA) 交叉研究方法没有研究输入的多模态性质如何影响模型的学习动态。在本文中，我们证明了每种模态在连续任务中的演化速度不同，并且这种行为发生在已建立的仅编码器模型以及用于开发视觉和语言 (VL) 模型的现代方法中。基于这一观察结果，我们提出了一种模态感知特征蒸馏 (MAFED) 方法，该方法在三种多模态持续学习设置中优于现有的基线模型，涵盖了不同规模的模型。此外，我们提供的消融研究表明，模态感知蒸馏是对经验回放的补充。总的来说，我们的结果强调了在多模态持续学习中解决模态特定动态以防止遗忘的重要性。||
|**2024-06-27**|[Leveraging Contrastive Learning for Enhanced Node Representations in Tokenized Graph Transformers](http://arxiv.org/abs/2406.19258)|null|虽然基于token的图Transformer在节点分类任务中表现出了强大的性能，但它们依赖于具有高相似性分数的节点的有限子集来构建token序列，忽略了来自其他节点的有价值信息，阻碍了它们充分利用图信息来学习最佳节点表示的能力。为了解决这一限制，我们提出了一种名为GCFormer的新型图Transformer。与以往的方法不同，GCFormer开发了一种混合token生成器来创建两种类型的token序列（正向和负向），以捕获不同的图信息。并采用了一种定制的基于Transformer的骨干网络，从这些生成的token序列中学习有意义的节点表示。此外，GCFormer引入了对比学习，从正向和负向token序列中提取有价值的信息，从而提高了学习到的节点表示的质量。在包括同质图和异质图在内的各种数据集上的大量实验结果表明，与具有代表性的图神经网络（GNN）和图Transformer相比，GCFormer在节点分类方面具有优越性。||
|**2024-06-27**|[Empirical Investigation of the Relationship Between Design Smells and Role Stereotypes](http://arxiv.org/abs/2406.19254)|null|在软件开发过程中，糟糕的设计和实现选择会严重影响软件的可维护性。设计异味，即设计不良片段的重复模式，表明了这些问题。角色刻板印象表示类在系统设计中承担的通用职责。尽管角色刻板印象和设计异味的概念不同，但两者都对软件系统的设计和维护做出了重大贡献。了解这些方面之间的关系对于增强软件可维护性、代码质量、高效代码审查、指导重构和特定于角色的指标设计至关重要。本文采用了一种探索性的方法，结合统计分析和无监督学习方法，来了解桌面和移动应用程序中设计异味与角色刻板印象之间的关系。通过分析来自 30 个 GitHub 存储库的 11,350 个类，我们发现了一些在某些角色刻板印象中频繁同时出现的设计异味。具体来说，在我们研究的六 (6) 种角色刻板印象中，有三种 (3) 种更容易出现设计异味。我们还研究了两种生态系统中设计异味的差异，这是由它们底层架构的显著差异所驱动的。研究结果表明，桌面应用程序中的设计异味比移动应用程序更普遍，尤其是在服务提供者和信息持有者角色刻板印象中。此外，无监督学习方法表明，某些角色刻板印象对或组容易出现类似类型的设计异味。我们认为，这些关系与角色刻板印象之间的特征和协作属性有关。这项研究的见解为软件团队实施设计异味预防和纠正机制提供了宝贵的指导，确保了设计和维护阶段的概念完整性。||
|**2024-06-27**|[Local Manifold Learning for No-Reference Image Quality Assessment](http://arxiv.org/abs/2406.19247)|null|对比学习作为一种广泛采用的技术，极大地推动了图像质量评价（IQA）领域的发展。对比学习的核心机制涉及最小化质量相似（正）样本之间的距离，同时最大化质量不相似（负）样本之间的距离。尽管取得了成功，但目前的对比学习方法往往忽略了保持局部流形结构的重要性。这种疏忽可能导致特征空间内困难样本之间的高度相似性，从而阻碍有效的区分和评估。为了解决这个问题，我们提出了一个创新的框架，将局部流形学习与对比学习相结合，用于无参考图像质量评价（NR-IQA）。我们的方法首先从给定图像中采样多个裁剪图像，识别出视觉上最显著的裁剪图像。然后，使用该裁剪图像将来自同一图像的其他裁剪图像聚类为正类，而将来自不同图像的裁剪图像视为负类，以增加类间距离。独特的是，我们的方法还将来自同一图像的非显著性裁剪图像视为类内负类，以保持它们的区别性。此外，我们采用了相互学习框架，进一步增强了模型自适应学习和识别视觉显著区域的能力。我们的方法在7个标准数据集上与最先进的方法相比表现出更好的性能，在TID2013和LIVEC数据集上分别取得了0.942（相比之下为0.908）和0.914（相比之下为0.894）的PLCC值。||
|**2024-06-27**|[ProtoGMM: Multi-prototype Gaussian-Mixture-based Domain Adaptation Model for Semantic Segmentation](http://arxiv.org/abs/2406.19225)|null|领域自适应语义分割旨在利用在标记的源域上训练的监督模型，为未标记的目标域生成准确和密集的预测。流行的自训练方法涉及使用来自目标域的伪标签重新训练 $p(class|pixel feature)$的密集判别分类器。虽然许多方法侧重于缓解噪声伪标签的问题，但它们往往忽略了源域和目标域中潜在的数据分布$ p(pixel feature|
|**2024-06-27**|[Towards Learning Abductive Reasoning using VSA Distributed Representations](http://arxiv.org/abs/2406.19121)|**[link](https://github.com/ibm/abductive-rule-learner-with-context-awareness)**|我们介绍了具有上下文感知能力的溯因规则学习器 (ARLC)，这是一个基于 Learn-VRF 解决抽象推理任务的模型。ARLC 采用了一种新颖且更广泛适用的溯因推理训练目标，在解决瑞文推理测验 (RPM) 时具有更好的可解释性和更高的准确性。ARLC 允许编程领域知识和学习数据分布背后的规则。我们在 I-RAVEN 数据集上评估了 ARLC，展示了其在分布内和分布外（未见过的属性-规则对）测试中的最新准确性。尽管参数数量少几个数量级，但 ARLC 优于神经符号和连接主义基线，包括大型语言模型。我们通过在编程知识的基础上逐步学习示例，展示了 ARLC 对编程后训练的鲁棒性，这只会提高其性能，而不会导致对编程解决方案的灾难性遗忘。我们验证了 ARLC 从 2x2 RPM 星座到未见星座的无缝迁移学习能力。我们的代码可在 https://github.com/IBM/abductive-rule-learner-with-context-awareness 获取。||
|**2024-06-27**|[Unsupervised Latent Stain Adaption for Digital Pathology](http://arxiv.org/abs/2406.19081)|null|在数字病理学中，用于分割或组织分类等任务的深度学习 (DL) 模型已知会因不同的染色技术而受到域偏移的影响。染色适应旨在通过在泛化到目标染色的源染色上训练模型来减少不同染色之间的泛化误差。尽管目标染色数据丰富，但一个关键挑战是缺乏注释。为了解决这个问题，我们提出了一种对包括所有可用染色图像在内的人工标记和未标记数据进行联合训练的方法，称为无监督潜在染色适应 (ULSA)。我们的方法使用染色转换来用合成目标图像丰富标记的源图像，以增加监督信号。此外，我们利用染色不变特征一致性学习来利用未标记的目标染色图像。借助 ULSA，我们提出了一种半监督策略，可在无法访问带注释的目标染色数据的情况下实现高效的染色适应。值得注意的是，ULSA 在整个切片图像 (WSI) 的补丁级别分析中与任务无关。通过对外部数据集的广泛评估，我们证明了 ULSA 在跨染色光谱的肾脏组织分割和乳腺癌分类方面实现了最先进的 (SOTA) 性能。我们的研究结果表明，ULSA 是数字病理学中染色适应的重要框架。||
|**2024-06-27**|[Zero-shot domain adaptation based on dual-level mix and contrast](http://arxiv.org/abs/2406.18996)|null|零样本域适应（ZSDA）是一种域适应问题，在这种情况下，目标任务（感兴趣的任务）的标记样本在训练时仅来自源域，而对于与感兴趣的任务不同的任务（不相关任务），标记样本来自源域和目标域。在这种情况下，经典的域适应技术只能学习不相关任务中的域不变特征。然而，由于两个任务之间样本分布的差异，在不相关任务中学习到的域不变特征是有偏差的，并且在感兴趣的任务中不一定是不变的。为了解决这个问题，本文提出了一种新的ZSDA方法来学习具有低任务偏差的域不变特征。为此，我们提出了（1）在任务和域中使用双层混合进行数据增强，以填补目标感兴趣任务数据的缺失，（2）扩展域对抗性学习以学习任务偏差较小的域不变特征，以及（3）一种新的双层对比学习方法，增强了特征的域不变性和较小的任务偏差。实验结果表明，我们的建议在几个基准测试中取得了良好的性能。||
|**2024-06-25**|[Data curation via joint example selection further accelerates multimodal learning](http://arxiv.org/abs/2406.17711)|null|数据筛选是大规模预训练的重要组成部分。本研究表明，联合选择数据批次比独立选择样本更能有效地学习。多模态对比目标揭示了数据之间的依赖关系，从而自然地为衡量批次的联合可学习性提供了标准。我们推导出一种简单且易于处理的算法来选择此类批次，该算法可以显著加速训练，超越了单独排序的数据点。由于从更大的超级批次中选择数据可以提高性能，因此我们还利用模型逼近的最新进展来减少相关的计算开销。因此，我们的方法——多模态对比学习与联合样本选择（JEST）——在迭代次数减少13倍、计算量减少10倍的情况下，性能优于最先进的模型。JEST 性能的关键在于能够通过预训练的参考模型将数据选择过程引导至更小、精心筛选的数据集的分布，从而将数据筛选级别作为神经网络缩放定律的新维度。||
|**2024-06-25**|[End-to-End Autonomous Driving without Costly Modularization and 3D Manual Annotation](http://arxiv.org/abs/2406.17680)|null|我们提出了UAD，一种基于视觉的端到端自动驾驶（E2EAD）方法，在nuScenes中实现了最佳的开环评估性能，同时在CARLA中展现出强大的闭环驾驶质量。我们的动机源于观察到当前的E2EAD模型仍然模仿典型驾驶堆栈中的模块化架构，通过精心设计的监督感知和预测子任务来为定向规划提供环境信息。尽管取得了突破性进展，但这种设计存在一定的缺陷：1）前面的子任务需要大量高质量的3D注释作为监督，这对扩展训练数据构成了重大障碍； 2）每个子模块在训练和推理中都需要大量的计算开销。为此，我们提出了UAD，这是一个带有无监督代理的E2EAD框架，可以解决所有这些问题。首先，我们设计了一种新颖的角度感知借口任务，以消除对注释的要求。该借口任务通过预测角度方向上的空间对象性和时间动态来对驾驶场景进行建模，而无需手动注释。其次，提出了一种自监督训练策略，该策略学习不同增强视图下预测轨迹的一致性，以增强转向场景中的规划鲁棒性。我们的UAD在nuScenes的平均碰撞率上比UniAD实现了38.7%的相对改进，并且在CARLA的Town05 Long基准测试中驾驶得分超过VAD 41.32分。此外，该方法仅消耗UniAD 44.3%的训练资源，并且推理速度提高了3.4倍。我们的创新设计不仅首次展示了相对于监督方法无可争议的性能优势，而且在数据、训练和推理方面享有前所未有的效率。代码和模型将在https://github.com/KargoBot_Research/UAD发布。||
|**2024-06-25**|[Video Inpainting Localization with Contrastive Learning](http://arxiv.org/abs/2406.17628)|null|深度视频修复通常被用作恶意篡改，去除重要对象以创建虚假视频。盲目地识别修复区域非常重要。这封信提出了一种简单而有效的取证方案，用于使用对比学习进行视频修复定位（ViLocal）。具体来说，将 3D Uniformer 编码器应用于视频噪声残差，以学习有效的时空取证特征。为了增强判别能力，采用监督对比学习来捕捉修复视频的局部不一致性，方法是吸引/排斥正/负原始像素对和伪造像素对。通过采用专门的两阶段训练策略的轻量级卷积解码器生成像素级修复定位图。为了准备足够的训练样本，我们构建了一个包含 2500 个视频的视频对象分割数据集，每帧都有像素级标注。大量的实验结果验证了 ViLocal 相对于现有技术的优越性。代码和数据集将在 https://github.com/multimediaFor/ViLocal 上提供。||
|**2024-06-25**|[Toward Universal Medical Image Registration via Sharpness-Aware Meta-Continual Learning](http://arxiv.org/abs/2406.17575)|**[link](https://github.com/xzluo97/continual-reg)**|当前的医学图像配准深度学习方法通常面临分布偏移和数据收集的挑战，阻碍了其在现实世界中的部署。相比之下，通用医学图像配准旨在同时对各种临床相关任务执行配准，因此在临床应用中具有巨大潜力。在本文中，我们首次尝试通过提出一种持续学习方法来实现顺序学习场景中通用3D医学图像配准的目标。具体来说，我们利用元学习和经验回放来缓解灾难性遗忘问题。为了提高元持续学习的泛化能力，我们进一步提出了锐度感知元持续学习（SAMCL）。我们在持续学习设置的四个数据集上验证了我们方法的有效性，包括脑部MR、腹部CT、肺部CT和腹部MR-CT图像对。结果表明，SAMCL在实现通用图像配准方面具有潜力，其性能优于或与传统的顺序或集中式多任务训练策略相当。源代码可在https://github.com/xzluo97/Continual-Reg获取。||
|**2024-06-25**|[Retrieval-style In-Context Learning for Few-shot Hierarchical Text Classification](http://arxiv.org/abs/2406.17534)|**[link](https://github.com/DreamH1gh/TACL2024)**|层次文本分类 (HTC) 是一项具有广泛应用的重要任务，而少样本 HTC 近来受到越来越多的关注。虽然基于大型语言模型 (LLM) 的上下文学习 (ICL) 在少样本学习中取得了显著成功，但由于其庞大的层次标签集和极其模糊的标签，它对 HTC 的效果并不理想。在这项工作中，我们介绍了第一个基于 ICL 的少样本 HTC 的 LLM 框架。我们利用检索数据库来识别相关的演示，并使用迭代策略来管理多层级联标签。特别是，我们为检索数据库配备了针对输入文本的 HTC 标签感知表示，这是通过在预训练语言模型上使用掩码语言建模 (MLM)、逐层分类（CLS，专门针对 HTC）和一种新颖的差异对比学习（DCL，主要针对相邻语义相似标签）目标进行持续训练来实现的。在三个基准数据集上的实验结果证明了我们方法的优越性能，并且我们可以在少样本 HTC 中达到最先进的结果。||
|**2024-06-25**|[Investigating Self-Supervised Methods for Label-Efficient Learning](http://arxiv.org/abs/2406.17460)|null|结合自监督学习的视觉Transformer使得模型能够扩展到大型数据集，用于分类、分割和检测等多个下游任务。这些模型在多个少样本下游任务中的少样本学习能力在很大程度上尚未得到探索。我们对不同的自监督 pretext 任务（即对比学习、聚类和掩码图像建模）进行了系统级研究，通过比较预训练模型来了解它们的少样本能力。此外，我们还研究了避免崩溃方法（即中心化、ME-MAX、Sinkhorn）对这些下游任务的影响。基于我们的详细分析，我们引入了一个框架，该框架涉及掩码图像建模和聚类作为pretext 任务，该框架在所有少样本下游任务（包括多类别分类、多标签分类和语义分割）中表现更佳。此外，在完整数据集上测试模型时，我们发现多类别分类、多标签分类和语义分割的性能有所提高。||
|**2024-06-25**|[SE-VGAE: Unsupervised Disentangled Representation Learning for Interpretable Architectural Layout Design Graph Generation](http://arxiv.org/abs/2406.17418)|**[link](https://github.com/JanineCHEN/SE-VGAE)**|尽管图结构适用于捕捉建筑布局设计中固有的关系结构，但利用基于图的表示学习来解释建筑设计空间以及探索建筑设计图生成的研宄还相当匮乏。同时，图生成中的解耦表示学习面临着节点排列不变性和表示表达能力等挑战。为了应对这些挑战，我们引入了一种无监督解耦表示学习框架，即基于风格的边增强变分图自动编码器 (SE-VGAE)，旨在以属性邻接多图的形式生成建筑布局，同时优先考虑表示解耦。该框架设计了三种可选方案，每种方案都集成了一个基于 Transformer 的边增强编码器、一个潜在空间解耦模块和一个基于风格的解码器。这些组件共同促进了影响建筑布局图生成潜在因素的分解，增强了生成保真度和多样性。我们还通过系统地探索图特征增强方案，并通过大量实验评估其对解耦建筑布局表示的有效性，从而为优化框架提供了见解。此外，我们提供了一个从真实平面图图像中提取的新的基准大规模建筑布局图数据集，以促进对基于图数据的建筑设计表示空间解释的探索。这项研究开创了用于建筑布局图生成的解耦表示学习的先河。本研究的代码和数据集将开源。||
|**2024-06-25**|[Less can be more: representational vs. stereotypical gender bias in facial expression recognition](http://arxiv.org/abs/2406.17405)|null|机器学习模型可能会从其训练数据中继承偏差，导致预测结果出现歧视或不准确。随着越来越多的使用大型无监督数据集来训练基础模型，这一点尤其令人担忧。传统上，人们对这些数据集中的人口统计学偏差缺乏了解，这限制了我们理解这些偏差如何传播到模型本身的能力。为了解决这个问题，本文研究了人口统计学偏差从数据集到机器学习模型的传播。我们重点关注性别人口统计学组成部分，分析了两种类型的偏差：代表性偏差和刻板印象偏差。在我们的分析中，我们考虑了面部表情识别 (FER) 领域，该领域已知在大多数流行数据集中都存在偏差。我们使用 Affectnet（最大的 FER 数据集之一）作为基线，仔细设计和生成了包含不同程度的代表性偏差和刻板印象偏差的子集。随后，我们在这些有偏差的子集上训练了多个模型，并在一个通用测试集上评估了它们的性能，以评估偏差传播到模型预测中的情况。我们的结果表明，代表性偏差的影响比预期的要弱。即使在训练数据集中缺少一个性别的情况下，模型也表现出良好的泛化能力。相反，刻板印象偏差的影响要大得多，主要集中在有偏差的类别上，尽管它也会影响对无偏差类别的预测。这些结果强调了对偏差分析的需求，该分析要区分偏差类型，这对于制定有效的偏差缓解策略至关重要。||
|**2024-06-25**|[Forget but Recall: Incremental Latent Rectification in Continual Learning](http://arxiv.org/abs/2406.17381)|null|深度神经网络 (DNN) 非常需要具备持续学习不断变化的数据流的内在能力。然而，当前的 DNN 存在灾难性遗忘问题，这阻碍了对过去知识的记忆。为了缓解这个问题，现有的持续学习 (CL) 方法要么保留样本以供重放，要么规范学习，要么为新任务分配专用容量。本文研究了一种尚未开发的增量学习持续学习方向，称为增量潜在校正 (ILR)。简而言之，ILR 学习将当前训练的 DNN 的表示向后传播（或校正）到旧任务的表示空间，在那里执行预测决策更容易。这种校正过程仅采用一系列称为校正器单元的小型表示映射网络。在几个持续学习基准（包括 CIFAR10、CIFAR100 和 Tiny ImageNet）上的实证实验表明，与现有代表性 CL 方法相比，这种新颖的 CL 方向的有效性和潜力。||
|**2024-06-25**|[A review of unsupervised learning in astronomy](http://arxiv.org/abs/2406.17316)|null|这篇综述总结了流行的无监督学习方法，并概述了它们在过去、现在和未来在天文学中的应用。无监督学习旨在组织数据集的信息内容，以便提取知识。传统上，这是通过降维技术实现的，这些技术有助于对数据集进行排序，例如通过主成分分析或使用自动编码器，或者通过使用自组织映射等方法对高维空间进行更简单的可视化。无监督学习的其他理想特性包括识别聚类，即相似对象的组，传统上这是通过k均值算法实现的，而最近则是通过基于密度的聚类（如HDBSCAN）实现的。最近，出现了将降维和聚类方法链接在一起的复杂框架。然而，没有一个数据集是完全未知的。因此，现在的很多研究都转向了自监督和半监督方法，这些方法可以从监督学习和无监督学习中获益。||
|**2024-06-21**|[FT-AED: Benchmark Dataset for Early Freeway Traffic Anomalous Event Detection](http://arxiv.org/abs/2406.15283)|null|高速公路上异常事件（如事故）的早期准确检测可以改善应急响应和道路疏通。然而，事件识别和报告中存在的延迟和错误使其成为一个难以解决的问题。当前的大规模高速公路交通数据集并非为异常检测而设计，并且忽略了这些挑战。在本文中，我们介绍了第一个用于异常检测的大规模车道级高速公路交通数据集。我们的数据集包含在一个月的工作日中，沿着田纳西州纳什维尔市区方向 18 英里长的 24 号州际公路的 4 条车道上收集的雷达探测传感器数据，其中包含超过 370 万条传感器测量值。我们还从纳什维尔交通管理中心收集了官方事故报告，并手动标记了数据集中所有其他潜在的异常情况。为了展示我们的数据集在未来机器学习和交通研究中的应用潜力，我们在数据集上对众多深度学习异常检测模型进行了基准测试。我们发现，无监督图神经网络自动编码器是解决此问题的有效方案，并且忽略空间关系会导致性能下降。我们证明，我们的方法可以将报告延迟平均缩短 10 分钟以上，同时检测到 75% 的事故。我们的数据集和所有开始所需的预处理代码均已在 https://vu.edu/ft-aed/ 上公开发布，以促进未来的研究。||
|**2024-06-21**|[Unsupervised Bayesian Generation of Synthetic CT from CBCT Using Patient-Specific Score-Based Prior](http://arxiv.org/abs/2406.15219)|null|背景：锥形束计算机断层扫描 (CBCT) 通常在图像引导放射治疗 (IGRT) 过程中用于患者摆位，例如每天或每周进行，因此使其成为实施自适应放射治疗 (ART) 方案的潜在成像方式。然而，CBCT图像中存在的严重伪影和不正确的Hounsfield单位 (HU) 值阻碍了其在靶区和器官分割以及剂量计算等定量任务中的应用。因此，从CBCT扫描中获取CT质量的图像对于在临床环境中实施在线ART至关重要。目的：本研究旨在开发一种基于患者特定扩散模型的无监督学习方法，用于基于CBCT的合成CT (sCT) 生成，以提高CBCT的图像质量。方法：所提出的方法采用无监督框架，利用患者特定的基于分数的模型作为图像先验，并结合定制的全变分 (TV) 正则化来增强不同横断面切片之间的一致性。使用同一患者的计划CT (pCT) 图像对基于分数的模型进行无条件训练，以表征CT质量图像的流形并捕获特定患者的独特解剖信息。在包括头颈部 (H&N) 癌、胰腺癌和肺癌在内的解剖部位的图像上评估了该方法的有效性。使用定量指标评估了所提出的CBCT校正方法的性能，包括平均绝对误差 (MAE)、峰值信噪比 (PSNR) 和归一化互相关 (NCC)。此外，将所提出的算法与其他两种基于无监督扩散模型的CBCT校正算法进行了基准测试。||
|**2024-06-21**|[Enhancing Idiomatic Representation in Multiple Languages via an Adaptive Contrastive Triplet Loss](http://arxiv.org/abs/2406.15175)|null|准确地对习语或非组合语言进行建模一直是自然语言处理 (NLP) 中的一个长期挑战。这在一定程度上是因为这些表达的含义并非仅仅来自其组成词，还因为相关数据资源的稀缺及其对机器翻译和简化等下游任务性能的影响。在本文中，我们提出了一种使用三元组损失有效地对习语进行建模的方法，该方法结合了组成词对习语意义的不对称贡献，通过使用自适应对比学习和重采样挖掘器来构建一个习语感知学习目标来训练语言模型。我们提出的方法在 SemEval 挑战中进行了评估，并在许多指标上明显优于以前的替代方案。||
|**2024-06-21**|[From Overfitting to Robustness: Quantity, Quality, and Variety Oriented Negative Sample Selection in Graph Contrastive Learning](http://arxiv.org/abs/2406.15044)|null|图对比学习 (GCL) 旨在对比正负样本以学习节点嵌入，而图数据增强方法则用于生成这些正负样本。与正样本相比，负样本的变化、数量和质量在学习用于下游节点分类任务的有意义嵌入中起着至关重要的作用。与正样本相比，变化少、数量过多和质量低的负样本会导致模型对特定节点过拟合，从而导致模型鲁棒性较差。为了解决 GCL 范式中的过拟合问题，本研究综合考虑负样本的质量、变化和数量，提出了一种新的累积样本选择 (CSS) 算法。首先，构建三个负样本池：简单、中等和困难负样本，分别包含总可用负样本的 25%、50% 和 25%。然后，从这三个负样本池中各选择 10% 的负样本用于训练模型。之后，决策代理模块评估模型训练结果，并决定是通过增加比例从三个负样本池中探索更多负样本，还是继续利用当前的采样比例。所提出的算法被集成到一个名为 NegAmplify 的图对比学习框架中。在九个图节点分类数据集上，将 NegAmplify 与 SOTA 方法进行了比较，其中七个数据集的节点分类精度提高了高达 2.86%。||
|**2024-06-21**|[Behaviour Distillation](http://arxiv.org/abs/2406.15042)|**[link](https://github.com/flairox/behaviour-distillation)**|数据集蒸馏旨在将大型数据集浓缩成少量合成样本，这些样本可以用作训练新模型时的直接替代品。它在可解释性、神经架构搜索、隐私和持续学习方面都有应用。尽管在监督领域取得了巨大成功，但此类方法尚未扩展到强化学习，因为缺乏固定数据集使得大多数蒸馏方法无法使用。为了填补这一空白，我们将行为蒸馏形式化，该设置旨在发现并将训练专家策略所需的信息浓缩到状态-动作对的合成数据集中，而无需访问专家数据。然后，我们介绍了使用进化策略 (HaDES) 生成幻觉数据集，这是一种行为蒸馏方法，可以发现仅包含四个状态-动作对的数据集，这些数据集在监督学习下可以训练代理在连续控制任务中达到具有竞争力的性能水平。我们证明了这些数据集可以泛化到训练具有各种架构和超参数的策略。我们还展示了其在下游任务中的应用，即以零样本的方式训练多任务代理。除了行为蒸馏之外，HaDES 还比以前的方法显着改进了强化学习中的神经进化，并在一个标准的监督数据集蒸馏任务上取得了最先进的结果。最后，我们展示了可视化合成数据集可以提供对人类可解释的任务洞察力。||
|**2024-06-21**|[Skip and Skip: Segmenting Medical Images with Prompts](http://arxiv.org/abs/2406.14958)|null|大多数医学图像病灶分割方法依赖于原始图像的手动精确标注进行监督学习。近年来，人们提出了一系列弱监督或无监督方法来减少对像素级标注的依赖。然而，这些方法本质上仍然基于像素级标注，忽略了当前海量医学图像中包含的图像级诊断结果。在本文中，我们提出了一个双U型两阶段框架，利用图像级标签来提示分割。在第一阶段，我们使用图像级标签预训练一个分类网络，用于获取层次金字塔特征并指导下游分支的学习。在第二阶段，我们将从分类分支获得的层次特征通过短跳跃连接和长跳跃连接输入到下游分支，并在像素级标签的监督学习下获得病灶掩码。实验表明，我们的框架比仅使用像素级标注的网络取得了更好的结果。||
|**2024-06-21**|[DN-CL: Deep Symbolic Regression against Noise via Contrastive Learning](http://arxiv.org/abs/2406.14844)|null|由于物理、电子和环境等多种因素的影响，信号中普遍存在噪声。传统的符号回归方法，如遗传编程或深度学习模型，旨在寻找最适合这些信号的表达式。然而，这些方法往往忽略了现实数据中存在的噪声，导致拟合精度降低。为了解决这个问题，我们提出了\textit{\textbf{D}eep Symbolic Regression against \textbf{N}oise via \textbf{C}ontrastive \textbf{L}earning (DN-CL)}。DN-CL 采用两个参数共享的编码器，将来自各种数据变换的数据点嵌入到针对噪声的特征屏蔽中。该模型将噪声数据和干净数据视为真实数学表达式的不同视图。这些特征之间的距离被最小化，利用对比学习来区分“正”的噪声校正对和“负”的对比对。我们的实验表明，DN-CL 在处理噪声和干净数据方面表现出优越的性能，是一种很有前景的符号回归方法。||
|**2024-06-21**|[Is this a bad table? A Closer Look at the Evaluation of Table Generation from Text](http://arxiv.org/abs/2406.14829)|null|理解生成的表格质量对于能否使用自动方法创建或编辑文档至关重要。在这项工作中，我们强调现有的表格质量评估方法无法捕捉表格的整体语义，并且有时会不公平地惩罚好的表格而奖励坏的表格。我们提出了 TabEval，这是一种新颖的表格评估策略，它通过首先将表格分解成一系列自然语言原子语句，然后使用基于蕴涵的度量将它们与真实语句进行比较来捕捉表格语义。为了验证我们的方法，我们整理了一个包含 1,250 个不同维基百科表格的文本描述的数据集，涵盖了广泛的主题和结构，这与现有数据集的有限范围形成了对比。我们使用无监督和有监督的文本到表格生成方法将 TabEval 与现有指标进行了比较，证明了它与人类对四个数据集的表格质量判断具有更强的相关性。||
|**2024-06-21**|[TemPrompt: Multi-Task Prompt Learning for Temporal Relation Extraction in RAG-based Crowdsourcing Systems](http://arxiv.org/abs/2406.14825)|null|时间关系抽取 (TRE) 旨在掌握事件或动作的演变，从而塑造相关任务的工作流程，因此在帮助理解众包系统中请求者发起的任务请求方面很有前景。然而，现有方法仍然难以应对有限且分布不均的标注数据。因此，受预训练语言模型 (PLM) 中存储的丰富全局知识的启发，我们提出了一个用于 TRE 的多任务提示学习框架 (TemPrompt)，它结合了提示调整和对比学习来解决这些问题。为了为 PLM 引出更有效的提示，我们引入了一种面向任务的提示构建方法，该方法充分考虑了 TRE 的多种因素来自动生成提示。此外，我们提出了时间事件推理作为补充，以加强模型对事件和时间线索的关注。实验结果表明，在标准设置和少样本设置下，TemPrompt 在大多数指标上都优于所有比较基线。案例研究验证了其在众包场景中的有效性。||
|**2024-06-20**|[Compliance Cards: Computational Artifacts for Automated AI Regulation Compliance](http://arxiv.org/abs/2406.14758)|null|随着人工智能 (AI) 供应链变得越来越复杂，人工智能系统和模型越来越有可能纳入外部来源的成分，例如数据集和其他模型。在这种情况下，确定人工智能系统或模型是否符合欧盟人工智能法案将需要收集有关整个系统或模型以及外部供应成分的合规性相关元数据。然后，必须对所有这些元数据进行分析，以预测整体人工智能系统或模型的合规性。到目前为止，这个过程还没有实现自动化。因此，在实时合规性判定有利的情况下（例如当今人工智能开发人员的迭代工作流程、在 Hugging Face 等社区上搜索和获取人工智能成分、联邦学习和持续学习等），还不可能进行实时合规性判定。为了解决这个缺点，我们引入了一个高度自动化的欧盟人工智能法案合规性分析系统。该系统有两个关键要素。首先是一组互锁的计算工件，它们捕获以下两者的合规性相关元数据：(1) 整个人工智能系统或模型；(2) 任何组成成分，例如数据集和模型。其次是一个自动分析算法，它跨这些计算工件运行，以实时预测整个人工智能系统或模型是否符合人工智能法案。这些要素协同工作，有望增强和加速人工智能法案的合规性评估。||
|**2024-06-20**|[Instruction Pre-Training: Language Models are Supervised Multitask Learners](http://arxiv.org/abs/2406.14491)|**[link](https://github.com/microsoft/lmops)**|无监督多任务预训练是近年来语言模型（LM）取得成功的关键方法。然而，监督多任务学习仍然具有巨大的潜力，因为在后训练阶段对其进行扩展往往会带来更好的泛化能力。在本文中，我们通过提出指令预训练来探索监督多任务预训练，该框架使用指令-响应对可扩展地扩充大量原始语料库，以预训练LM。指令-响应对由一个基于开源模型构建的高效指令合成器生成。在我们的实验中，我们合成了2亿个指令-响应对，涵盖40多个任务类别，以验证指令预训练的有效性。在从头开始的预训练中，指令预训练不仅持续增强了预训练的基础模型，而且从进一步的指令微调中获益更多。在持续预训练中，指令预训练使Llama3-8B能够与Llama3-70B相媲美，甚至超越后者。我们的模型、代码和数据可在https://github.com/microsoft/LMOps获取。||
|**2024-06-20**|[Revealing Vision-Language Integration in the Brain with Multimodal Networks](http://arxiv.org/abs/2406.14481)|null|我们使用（多）模态深度神经网络 (DNN) 来探测人脑中多模态整合的位点，方法是预测人类受试者观看电影时记录的脑电图 (SEEG)。我们将多模态整合的位点定义为：多模态视觉语言模型比单模态语言、单模态视觉或线性整合的语言视觉模型能更好地预测记录的区域。我们的目标 DNN 模型涵盖不同的架构（例如，卷积网络和变换器）和多模态训练技术（例如，交叉注意力和对比学习）。作为一个关键的启动步骤，我们首先证明经过训练的视觉和语言模型在预测 SEEG 信号的能力方面系统地优于随机初始化的模型。然后，我们将单模态和多模态模型相互比较。因为我们的目标 DNN 模型通常具有不同的架构、参数数量和训练集（可能会掩盖那些可归因于整合的差异），所以我们对两种模型（SLIP 和 SimCLR）进行了受控比较，这两种模型除了输入模态外，所有这些属性都保持不变。使用这种方法，我们确定了大量的神经位点（平均 1090 个位点中的 141 个，或 12.94%）和似乎发生多模态整合的大脑区域。此外，我们发现，在我们评估的各种多模态训练技术中，CLIP 风格的训练最适合下游预测这些位点的神经活动。||
|**2024-06-20**|[Podcast Outcasts: Understanding Rumble's Podcast Dynamics](http://arxiv.org/abs/2406.14460)|null|Rumble是一个替代性的视频分享平台，其播客吸引了以传播分裂性和经常误导性的内容而闻名的争议性人物，这与YouTube更加规范的环境形成了鲜明对比。鉴于播客对政治话语的影响越来越大，例如乔·罗根和安德鲁·泰特等人物，本文探讨了这些平台使用的政治偏见和内容策略。在本文中，我们对来自 YouTube 和 Rumble 的 1.3 万多个播客视频进行了全面分析，重点关注其政治内容和受众动态。使用先进的语音转文本转录、主题建模和对比学习技术，我们探索了三个关键方面：播客频道中政治偏见的存在、推动播客观看的内容的性质以及这些播客中视觉元素的使用。我们的研究结果表明，Rumble 的播客具有明显的右翼倾向，而 YouTube 的内容则更加多样化且非政治化。||
|**2024-06-20**|[Capturing Temporal Components for Time Series Classification](http://arxiv.org/abs/2406.14456)|null|在许多领域，分析序列数据至关重要，特别是考虑到物联网范式收集的大量数据。时间序列分类，即对序列数据进行分类的任务，已经变得越来越重要，机器学习方法在公共基准数据集上表现出卓越的性能。然而，目前的进展主要集中在设计架构，以便从固定（或理想）时间尺度的原始数据中学习表示，而这种架构可能无法推广到更长的序列。这项工作介绍了一种\textit{组合表示学习}方法，该方法在从序列数据中提取的统计一致的组件上进行训练。基于多尺度变化空间，提出了一种无监督方法，将序列数据分割成具有相似统计特性的块。在多任务环境下训练基于序列的编码器模型，以从这些时间组件中学习用于时间序列分类的组合表示。我们通过对公开可用的时间序列分类基准进行的大量实验，证明了其有效性。评估分段组件的一致性表明，它在无监督分段任务中具有竞争力。||
|**2024-06-20**|[Maintenance Required: Updating and Extending Bootstrapped Human Activity Recognition Systems for Smart Homes](http://arxiv.org/abs/2406.14446)|null|由于住宅布局、个性化设置以及居民特异行为的不同，为智能家居开发人类活动识别（HAR）系统并非易事。因此，现成的人类活动识别系统在单个家庭中的有效性有限，而且人类活动识别系统通常需要“从头开始”构建，这需要付出巨大的努力，而且往往会给居民带来负担。先前的工作已经成功地针对了初始阶段。在初始阶段结束时，我们确定了种子点。我们在自举人类活动识别系统的基础上，引入了一种有效的更新和扩展程序，用于持续改进人类活动识别系统，旨在跟上不断变化的生活环境。我们的方法利用了初始自举阶段结束时确定的种子点。我们使用这些种子点和为其获得的标签训练对比学习框架。然后，该模型用于提高识别的突出活动的分割精度。通过此过程改进活动识别系统有助于对智能家居中的大多数日常活动进行建模。我们通过 CASAS 数据集上的实验展示了我们程序的有效性，这些实验表明了我们方法的实用价值。||
|**2024-06-20**|[ATAC-Net: Zoomed view works better for Anomaly Detection](http://arxiv.org/abs/2406.14398)|null|由于深度学习在质量控制和制造业中的潜在用途，其在视觉异常检测中的应用已获得广泛普及。当前的标准方法是无监督的，它利用干净的数据集来检测偏差并在测试期间标记异常。然而，如果事先知道异常类型，则加入一些样本可以显著提高性能。因此，我们提出了 ATAC-Net，这是一个训练从最少的一组已知先验异常中检测异常的框架。此外，我们引入了注意力引导裁剪，它可以在训练阶段更近距离地观察可疑区域。我们的框架是一个可靠且易于理解的异常检测系统，并且我们证实了它在类似环境下优于当前一些最先进技术的优越性。||
|**2024-06-20**|[LARP: Language Audio Relational Pre-training for Cold-Start Playlist Continuation](http://arxiv.org/abs/2406.14333)|**[link](https://github.com/rsalganik1123/larp)**|随着在线音乐消费越来越倾向于基于播放列表的收听方式，播放列表延续的任务，即算法以个性化和音乐衔接的方式推荐歌曲以扩展播放列表，已经成为音乐流媒体成功的关键。目前，许多现有的播放列表延续方法依赖于协同过滤方法来执行推荐。然而，此类方法难以推荐缺乏交互数据的歌曲，这个问题被称为冷启动问题。目前应对这一挑战的方法是设计复杂的机制，从稀疏的协同数据中提取关系信号，并将其整合到内容表示中。然而，这些方法忽略了内容表示学习，并利用了预先训练好的、可能与特定音乐环境的分布或格式不一致的冻结内容模型。此外，即使是最先进的音乐内容模块也存在以下问题：(1) 与冷启动设置不兼容，或 (2) 无法有效地整合跨模态和关系信号。在本文中，我们介绍了 LARP，一个多模态冷启动播放列表延续模型，以有效地克服这些限制。LARP 是一个三阶段对比学习框架，它将多模态和关系信号整合到其学习的表示中。我们的框架使用递增的任务特定抽象阶段：曲目内（语言-音频）对比损失、曲目间对比损失和曲目-播放列表对比损失。在两个公开数据集上的实验结果表明，在冷启动设置下，LARP 在播放列表延续方面优于单模态和多模态模型。代码和数据集发布在：https://github.com/Rsalganik1123/LARP。||
|**2024-06-20**|[Revisiting Modularity Maximization for Graph Clustering: A Contrastive Learning Perspective](http://arxiv.org/abs/2406.14288)|**[link](https://github.com/edisonleeeee/magi)**|图聚类是图挖掘中一项基本且具有挑战性的任务，旨在将图中的节点分类为几个不相交的簇。近年来，图对比学习 (GCL) 已成为图聚类研究的主流方向，并推动了新的技术发展。然而，基于 GCL 的方法严重依赖于图增强和对比方案，这可能会带来语义漂移和可扩展性问题等挑战。另一个有前途的研究方向是采用模块化最大化作为聚类任务的指导原则，模块化最大化是一种流行且有效的社区检测方法。尽管最近取得了一些进展，但模块化最大化的潜在机制仍不清楚。在这项工作中，我们深入研究了模块化最大化在图聚类中的隐藏成功之处。我们的分析揭示了模块化最大化与图对比学习之间的密切联系，其中正例和负例由模块化自然定义。根据我们的结果，我们提出了一个社区感知图聚类框架，称为 MAGI，它利用模块化最大化作为对比性预训练任务，有效地揭示图中社区的底层信息，同时避免了语义漂移问题。在多个图数据集上的大量实验验证了 MAGI 在可扩展性和聚类性能方面优于最先进的图聚类方法。值得注意的是，MAGI 可以轻松扩展到具有 1 亿个节点的大型图，同时优于强大的基线方法。||
|**2024-06-20**|[Unleashing the Potential of Tracklets for Unsupervised Video Person Re-Identification](http://arxiv.org/abs/2406.14261)|null|基于视频的人员重识别方法具有丰富的时空信息，展现出广阔的应用前景。虽然可以使用现成的跟踪模型轻松获得轨迹，但对身份进行标注仍然成本高昂且不切实际。因此，一些基于视频的方法建议仅使用少量身份标注或摄像头标签来促进特征学习。这些方法通常简单地平均每个轨迹的帧特征，而忽略了轨迹内出现的意外变化和固有的身份一致性。在本文中，我们提出了自监督精细化聚类（SSR-C）框架，该框架不依赖任何标注或辅助信息来促进无监督视频人员重识别。具体来说，我们首先提出了噪声过滤轨迹划分（NFTP）模块，以减少由噪声跟踪结果引起的轨迹特征偏差，并将噪声过滤后的轨迹依次划分为“子轨迹”。然后，我们使用来自轨迹划分的自监督信号对子轨迹进行聚类和进一步合并，并通过渐进式策略增强该信号以生成可靠的伪标签，从而促进类内跨轨迹聚合。此外，我们提出了类别平滑分类（CSC）损失来有效地促进模型学习。在 MARS 和 DukeMTMC-VideoReID 数据集上的大量实验表明，我们提出的用于无监督视频人员重识别的 SSR-C 框架取得了最先进的结果，并且与先进的有监督方法相媲美。||
|**2024-06-20**|[Self-Supervised Pretext Tasks for Alzheimer's Disease Classification using 3D Convolutional Neural Networks on Large-Scale Synthetic Neuroimaging Dataset](http://arxiv.org/abs/2406.14210)|null|结构磁共振成像 (MRI) 研究表明，阿尔茨海默病 (AD) 会导致整个大脑发生局部和广泛的神经退行性改变。然而，缺乏突出大脑退行性改变的分割对以监督方式训练基于 CNN 的分类器提出了独特的挑战。在这项工作中，我们评估了几种无监督方法来训练特征提取器，用于下游 AD 与 CN 分类。使用来自合成神经影像 LDM100K 数据集的认知正常 (CN) 受试者的 3D T1 加权 MRI 数据，训练轻量级 3D 基于 CNN 的模型进行脑龄预测、脑图像旋转分类、脑图像重建以及将所有三个任务组合成一个的多头任务。在 LDM100K 合成数据集上训练的特征提取器与使用真实世界数据的相同模型相比取得了类似的性能。这支持了利用大规模合成数据进行预训练任务的可行性。所有训练和测试拆分均在受试者级别上执行，以防止数据泄漏问题。除了简单的预处理步骤外，随机裁剪数据增强技术在所有实验中都显示出一致的改进。||
|**2024-06-18**|[GroPrompt: Efficient Grounded Prompting and Adaptation for Referring Video Object Segmentation](http://arxiv.org/abs/2406.12834)|null|指代视频目标分割 (RVOS) 旨在分割整个视频中查询语句所指代的目标。大多数现有方法需要使用密集掩码注释进行端到端训练，这可能非常耗时且可扩展性较差。在这项工作中，我们旨在利用所提出的 Grounded Prompting (GroPrompt) 框架，有效地调整基础分割模型，以便从弱监督中解决 RVOS 问题。更具体地说，我们提出了文本感知提示对比学习 (TAP-CL)，仅使用边界框监督来增强位置提示和指代表达式之间的关联，包括分别在帧级别和视频级别的文本对比提示学习 (TextCon) 和模态对比提示学习 (ModalCon)。借助所提出的 TAP-CL，我们的 GroPrompt 框架可以生成描述视频中所指对象的位置和移动的时间一致且文本感知的位置提示。在标准 RVOS 基准测试（Ref-YouTube-VOS、Ref-DAVIS17、A2D-Sentences 和 JHMDB-Sentences）中的实验结果表明，在仅给出边界框弱监督的情况下，我们提出的 GroPrompt 框架具有竞争力的性能。||
|**2024-06-18**|[Unsupervised explainable activity prediction in competitive Nordic Walking from experimental data](http://arxiv.org/abs/2406.12762)|null|人工智能 (AI) 已应用于竞技体育中的人体活动识别 (HAR)。迄今为止，大多数用于 HAR 的机器学习 (ML) 方法都依赖于离线（批处理）训练，与在线处理无监督方法相比，这带来了更高的计算和标记负担。此外，传统 ML 预测器背后的决策是不透明的，需要人工解释。在这项工作中，我们应用了一种基于低成本可穿戴惯性测量单元 (IMU) 的在线处理无监督聚类方法。系统生成的结果允许在这些集群内自动扩展有限的可用标记（例如，由裁判标记），从而为可解释分类阶段生成相关信息。具体来说，我们的工作重点是实现与运动员活动相关的预测的自动可解释性，区分北欧式健走中的正确、错误和作弊行为。所提出的解决方案实现了接近 100% 的平均性能指标。||
|**2024-06-18**|[BIOSCAN-5M: A Multimodal Dataset for Insect Biodiversity](http://arxiv.org/abs/2406.12723)|**[link](https://github.com/zahrag/BIOSCAN-5M)**|作为一项持续进行的理解和监测昆虫生物多样性的全球努力的一部分，本文向机器学习社区推出了 BIOSCAN-5M 昆虫数据集，并建立了多个基准任务。BIOSCAN-5M 是一个综合数据集，包含超过 500 万个昆虫样本的多模态信息，它通过包含分类标签、原始核苷酸条形码序列、分配的条形码索引号和地理信息，显著扩展了现有的基于图像的生物数据集。我们提出了三个基准实验，以证明多模态数据类型对分类和聚类准确性的影响。首先，我们在 BIOSCAN-5M 数据集的 DNA 条形码序列上预训练了一个掩码语言模型，并证明了使用这个大型参考库对物种和属级别分类性能的影响。其次，我们提出了一个应用于图像和 DNA 条形码的零样本迁移学习任务，以对从自监督学习中获得的特征嵌入进行聚类，以研究是否可以从这些表示嵌入中导出有意义的聚类。第三，我们通过对 DNA 条形码、图像数据和分类信息进行对比学习来对多模态进行基准测试。这产生了一个通用的共享嵌入空间，可以使用多种类型的信息和模态进行分类。BIOSCAN-5M 昆虫数据集的代码库可在以下网址获得：{\url{https://github.com/zahrag/BIOSCAN-5M}}||
|**2024-06-18**|[Reproducibility of predictive networks for mouse visual cortex](http://arxiv.org/abs/2406.12625)|null|近年来，神经元活动的深度预测模型使得关于视觉皮层神经元选择性和不变性的若干新发现成为可能。这些模型学习一组共享的非线性基函数，这些基函数通过学习的权重向量进行线性组合，以表示神经元的函数。这种权重向量可以被认为是神经元功能的嵌入，已被提议通过无监督聚类来定义功能细胞类型。然而，由于深度模型通常高度过参数化，学习问题不太可能有唯一的解决方案，这就引发了一个问题，即这种嵌入是否可以以有意义的方式用于下游分析。在本文中，我们研究了神经元嵌入相对于模型架构和初始化变化的稳定性。我们发现 $L_1$ 正则是结构化嵌入的重要因素，并开发了一种自适应正则化，可以根据每个神经元调整正则化的强度。与统一正则化相比，这种正则化提高了预测性能，以及神经元嵌入在模型拟合中聚类的稳定性。为了克服过参数化，我们提出了一种迭代特征剪枝策略，可以在不损失性能的情况下将性能优化模型的维数减少一半，并提高神经元嵌入在聚类神经元方面的一致性。这一结果表明，为了实现细胞类型的客观分类或功能景观的紧凑表示，我们需要新的架构或学习技术来提高可识别性。我们将在发表时提供代码。||
|**2024-06-18**|[PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental Learning for Document Retrieval](http://arxiv.org/abs/2406.12593)|null|可微搜索索引 (DSI) 利用预训练语言模型 (PLM) 实现高效的文档检索，而无需依赖外部索引。然而，DSI 需要完全重新训练才能处理动态语料库中的更新，这会导致严重的计算效率低下问题。我们引入了 PromptDSI，这是一种基于提示的免排练实例级增量学习方法，用于文档检索。PromptDSI 将提示附加到 DSI 冻结的 PLM 编码器上，利用其强大的表示能力有效地索引新语料库，同时保持稳定性和可塑性之间的平衡。我们消除了基于提示的持续学习方法的初始前向传递，该传递会使训练和推理时间加倍。此外，我们提出了一个主题感知提示池，它使用神经主题嵌入作为固定键。该策略确保了多样化和有效的提示使用，解决了由于查询键匹配机制崩溃导致的参数利用不足的挑战。我们的实证评估表明，PromptDSI 在管理遗忘方面与 IncDSI 相匹配，同时在新语料库上显着提高了 4% 以上的召回率。||
|**2024-06-18**|[Unsupervised Online Continual Learning for Automatic Speech Recognition](http://arxiv.org/abs/2406.12503)|**[link](https://github.com/stevenvdeeckt/unsupervised-ocl-for-asr)**|将自动语音识别 (ASR) 模型适应新领域会导致对先前学习信息的灾难性遗忘 (CF)。本文解决了在线持续学习 (OCL) 挑战性背景下的 CF 问题，其中任务以具有未知边界的连续数据流的形式呈现。我们通过利用自训练 (ST) 来促进无监督适应，将 ASR 的 OCL 扩展到无监督领域，使模型能够在不依赖标签且不忘记先前知识的情况下不断适应。通过对两个领域适应实验中各种 OCL 和 ST 方法的比较分析，我们发现与监督 OCL 相比，无监督 OCL 遭受的遗忘要少得多，这使得 UOCL 方法能够接近监督 OCL 的性能水平。我们提出的 UOCL 扩展进一步提高了 UOCL 的效率。我们的研究结果代表着向持续适应性 ASR 系统迈出的重要一步，该系统能够利用跨不同领域的未标记数据。||
|**2024-06-18**|[RIGL: A Unified Reciprocal Approach for Tracing the Independent and Group Learning Processes](http://arxiv.org/abs/2406.12465)|null|在教育领域，自主学习和协作学习都被视为最经典的范式。前者允许学习者自主指导学习，而后者通常以教师指导为特征。近年来，智能教育领域的研究利用深度时间模型来追踪学习过程，捕捉学生知识状态的动态变化，并取得了显著成果。然而，现有方法主要集中于对自主学习过程的建模，而对协作学习范式的关注较少。此外，两种学习过程之间的相互影响，特别是它们在促进学生全面发展方面的综合潜力，仍未得到充分探索。为此，在本文中，我们提出了RIGL，一个统一的交互模型，用于在个体和群体层面追踪知识状态，该模型借鉴了自主学习和协作学习过程。具体而言，我们首先引入了一个时间帧感知的交互嵌入模块，用于同时对不同时间帧中学生和群体响应交互进行建模。随后，我们采用交互增强的学习模型来充分利用两种行为之间全面和互补的信息。此外，我们设计了一个关系引导的时间注意力网络，该网络由动态图建模和时间自注意力机制组成，用于深入研究个体和群体交互在整个学习过程中的动态影响。最后，我们引入了一个偏差感知的对比学习模块，以增强模型训练的稳定性。在四个真实世界教育数据集上的大量实验清楚地证明了所提出的RIGL模型的有效性。||
|**2024-06-18**|[Effective Generation of Feasible Solutions for Integer Programming via Guided Diffusion](http://arxiv.org/abs/2406.12349)|null|可行解对于整数规划 (IP) 至关重要，因为它们可以大大加快求解过程。在许多应用中，类似的 IP 实例通常表现出相似的结构和共享的解分布，这可以使用深度学习方法进行建模。不幸的是，现有的基于深度学习的算法，例如神经潜水和预测搜索框架，仅限于生成部分可行解，并且它们必须依赖 SCIP 和 Gurobi 等求解器来完成给定 IP 问题的解。在本文中，我们提出了一个新的框架，可以端到端地生成完整的可行解。我们的框架利用对比学习来表征 IP 实例和解之间的关系，并学习 IP 实例及其解的潜在嵌入。此外，该框架采用扩散模型来学习以 IP 表示为条件的解嵌入分布，并采用专门的引导采样策略来考虑约束和目标。我们根据 IP 问题的四个典型数据集对我们的框架进行了实证评估，结果表明，它可以有效地生成完整的可行解，并且概率很高（> 89.7%），而无需依赖求解器，并且解的质量可与 Gurobi 最好的启发式解相媲美。此外，通过将我们方法采样的部分解与 SCIP 的 CompleteSol 启发式算法相结合，所得的可行解优于所有数据集中的最新方法，与最优值的差距提高了 3.7% 到 33.7%，并在所有数据集中保持了超过 99.7% 的可行比率。||
|**2024-06-18**|[Toward Exploring the Code Understanding Capabilities of Pre-trained Code Generation Models](http://arxiv.org/abs/2406.12326)|null|近年来，以自监督方式在大量未标记编程语言数据上训练的大型代码生成模型取得了显著成功。虽然这些模型获得了大量的代码知识，但由于它们专门针对生成进行了训练，因此在代码理解任务（如代码搜索和克隆检测）中表现不佳。从头开始在海量代码数据上预训练一个更大的仅编码器架构模型可以提高理解性能。然而，这种方法成本高昂且耗时，因此并非最佳选择。在本文中，我们率先将知识从预训练的代码生成模型迁移到代码理解任务，显著降低了训练成本。我们研究了使仅解码器模型能够获得稳健代码表示的有效策略。此外，我们还引入了CL4D，这是一种对比学习方法，旨在增强仅解码器模型的表示能力。综合实验表明，我们的方法在代码搜索和克隆检测等理解任务中实现了最先进的性能。我们的分析表明，我们的方法有效地减少了表示空间中语义相同样本之间的距离。这些发现表明，可以使用仅解码器结构模型来统一代码理解和生成任务的潜力。||
|**2024-06-18**|[COT: A Generative Approach for Hate Speech Counter-Narratives via Contrastive Optimal Transport](http://arxiv.org/abs/2406.12304)|null|反叙事，即由基于事实的非攻击性论点组成的直接回应，已成为打击仇恨言论扩散的一种非常有效的方法。以前的方法主要侧重于微调和后期编辑技术，以确保生成内容的流畅性，而忽略了与特定仇恨目标（如LGBT群体、移民等）相关的个性化和相关性等关键方面。本研究论文介绍了一种基于对比最优传输的新框架，该框架有效地解决了在生成反叙事时维护目标交互和促进多样化的挑战。首先，利用最优传输核（OTK）模块将仇恨目标信息纳入到词元表示中，其中在原始特征和传输特征之间提取比较对。其次，采用自对比学习模块来解决模型退化的问题。该模块通过生成词元表示的各向异性分布来实现这一点。最后，集成了面向目标的搜索方法作为一种改进的解码策略，以在推理过程中明确促进领域相关性和多样性。该策略通过同时考虑词元相似性和目标相关性来修改模型的置信度得分。在两个基准数据集上进行了定量和定性实验，结果表明，我们提出的模型在多个方面的指标评估中明显优于当前的方法。||
|**2024-06-17**|[Mix-Domain Contrastive Learning for Unpaired H&E-to-IHC Stain Translation](http://arxiv.org/abs/2406.11799)|null|苏木精-伊红 (H&E) 染色到免疫组化 (IHC) 染色的图像转换技术为精确的癌症诊断提供了一种很有前景的解决方案，特别是在缺乏专业医疗人员和昂贵设备资源匮乏的地区。考虑到 H&E-IHC 图像对之间存在像素级别的错位，当前的研究探索了图像对相同位置的图像块之间的病理学一致性。然而，大多数方法过度强调域或图像块之间的对应关系，而忽略了非对应目标提供的辅助信息。在本文中，我们提出了一种混合域对比学习 (MDCL) 方法，以利用非配对 H&E 到 IHC 染色转换中的监督信息。具体来说，所提出的 MDCL 方法通过估计锚点图像块与匹配图像中所有图像块之间的相关性来聚合域间和域内的病理学信息，鼓励网络从混合域中学习额外的对比知识。通过混合域病理信息聚合，MDCL 增强了对应图像块之间的病理学一致性，以及生成的 IHC 图像中不同位置的图像块之间的成分差异。在两个 H&E 到 IHC 染色转换数据集（即 MIST 和 BCI）上的大量实验表明，所提出的方法在多个指标上均达到了最先进的性能。||
|**2024-06-17**|[A Brief Survey on Leveraging Large Scale Vision Models for Enhanced Robot Grasping](http://arxiv.org/abs/2406.11786)|null|在现实世界场景中，机器人抓取是一项艰巨的运动任务，构成了在各行各业部署高性能机器人的主要障碍。值得注意的是，数据的稀缺使得学习模型的抓取尤其具有挑战性。近年来，计算机视觉领域的进步见证了基于互联网海量数据的成功无监督训练机制的发展，现在几乎所有杰出的模型都利用了预训练的骨干网络。在此背景下，我们开始研究大规模视觉预训练在提高机器人抓取性能方面的潜在优势。这篇初步的文献综述阐明了关键挑战，并为机器人操作的视觉预训练的未来研究方向进行了展望。||
|**2024-06-17**|[DiffMM: Multi-Modal Diffusion Model for Recommendation](http://arxiv.org/abs/2406.11781)|**[link](https://github.com/hkuds/diffmm)**|像抖音和YouTube这样的在线多模态分享平台的兴起使得个性化推荐系统能够将多种模态（如视觉、文本和音频）纳入用户表示中。然而，解决这些系统中数据稀疏性的挑战仍然是一个关键问题。为了解决这一局限性，最近的研究引入了自监督学习技术来增强推荐系统。然而，这些方法通常依赖于简单的随机增强或直观的跨视图信息，这可能会引入不相关的噪声，并且无法准确地将多模态上下文与用户-项目交互建模对齐。为了填补这一研究空白，我们提出了一种新的用于推荐的多模态图扩散模型，称为DiffMM。我们的框架将模态感知图扩散模型与跨模态对比学习范式相结合，以改进模态感知用户表示学习。这种整合促进了多模态特征信息与协同关系建模之间更好的对齐。我们的方法利用扩散模型的生成能力自动生成一个感知不同模态的用户-项目图，从而促进在用户-项目交互建模中纳入有用的多模态知识。我们在三个公共数据集上进行了广泛的实验，一致证明了我们的DiffMM相对于各种竞争基线的优越性。有关开源模型实现的详细信息，您可以访问以下地址获取我们提出的框架的源代码：https://github.com/HKUDS/DiffMM。||
|**2024-06-17**|[Zero-Shot Generalization during Instruction Tuning: Insights from Similarity and Granularity](http://arxiv.org/abs/2406.11721)|**[link](https://github.com/hbx-hbx/dynamics_of_zero-shot_generalization)**|理解对齐技术始于理解指令微调带来的零样本泛化能力，但人们对其机制知之甚少。现有的研究主要局限于任务层面，而没有考虑到任务是人为定义的，对于大型语言模型来说，仅仅是由token和表示组成的。这条研究路线仅限于从任务对的角度研究任务之间的迁移，很少有研究从数据本身的角度理解零样本泛化。为了弥合这一差距，我们首先通过多个指标证明，指令微调过程中的零样本泛化发生在非常早期的阶段。接下来，我们从数据相似性和粒度两个角度研究了零样本泛化的促进因素，证实了在指令微调的早期阶段遇到高度相似和细粒度的训练数据，而不受限于定义的“任务”，能够实现更好的泛化。最后，我们提出了一种更合理的训练数据编排方法，即以测试为中心的多轮编排，并展示了其在促进持续学习和进一步降低损失方面的有效性。我们首次证明，指令微调过程中的零样本泛化是训练数据和测试数据之间在实例级别的基于相似性的泛化形式。我们希望我们的分析能够促进对指令微调过程中零样本泛化的理解，并有助于开发更对齐的大型语言模型。我们的代码发布在https://github.com/HBX-hbx/dynamics_of_zero-shot_generalization。||
|**2024-06-17**|[Multiple Descents in Unsupervised Learning: The Role of Noise, Domain Shift and Anomalies](http://arxiv.org/abs/2406.11703)|null|双下降现象最近在监督学习中受到关注。它挑战了偏差-方差权衡的传统观点，展示了一种令人惊讶的行为。随着模型复杂度的增加，测试误差最初会下降，直到达到模型开始过拟合训练集的某个点，导致测试误差上升。然而，与经典理论不同的是，当超过一定程度的过参数化时，误差会再次下降。我们研究了双下降现象在无监督学习中的存在，这是一个很少受到关注且尚未完全理解的领域。我们使用欠完备自动编码器 (AE) 对各种应用进行了广泛的实验，例如处理噪声数据、域偏移和异常。我们使用合成数据和真实数据，并确定了上述所有应用中模型、时期和样本方面的双下降现象。最后，我们评估了自动编码器在检测异常和减轻数据集之间域偏移方面的可用性。我们的研究结果表明，过参数化模型不仅可以提高重建性能，还可以增强下游任务的能力。||
|**2024-06-17**|[Making Old Things New: A Unified Algorithm for Differentially Private Clustering](http://arxiv.org/abs/2406.11649)|null|作为数据分析和无监督学习的支柱，私有聚类问题已在各种隐私模型下得到了广泛研究。集中式差分隐私是其中最早的模型，并且该问题也已针对本地和随机变换进行了研究。在每种情况下，目标都是设计一种算法，以尽可能小的误差私下计算聚类。对每种变体的研究都催生了新的算法：因此，私有聚类算法的图景相当复杂。在本文中，我们展示了一种已有 20 年历史的算法，只需稍作修改即可适用于任何这些模型。这提供了一个统一的视角：在匹配几乎所有先前已知结果的同时，它允许我们改进其中的一些结果，并将其扩展到一个新的隐私模型，即持续观察设置，其中输入随时间而变化，并且算法必须在每个时间步输出一个新的解决方案。||
|**2024-06-17**|[Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!](http://arxiv.org/abs/2406.11629)|null|利用大型语言模型 (LLM) 作为评判者来评估 LLM 的性能最近受到了关注。然而，这种方法同时引入了来自 LLM 的潜在偏差，引发了对评估结果可靠性的担忧。为了缓解这个问题，我们提出并研究了两种版本的少样本上下文提示，即强化和无监督 ICL，用于帮助 GPT-4o 作为评判者进行单答案评分。基于设计的提示，我们研究了扩展上下文示例数量对评估的一致性和质量的影响。此外，我们首先揭示了 GPT-4o 作为评判者在成对比较中的符号偏差，然后提出了一种简单而有效的方法来减轻它。实验结果表明，先进的长上下文 LLM，例如 GPT-4o，在少样本机制中的表现优于零样本机制。同时，实验结果进一步验证了符号偏差缓解方法的有效性。||
|**2024-06-17**|[Wide Area VISTA Extra-galactic Survey (WAVES): Unsupervised star-galaxy separation on the WAVES-Wide photometric input catalogue using UMAP and ${\rm{\scriptsize HDBSCAN}}$](http://arxiv.org/abs/2406.11611)|null|星系分离是创建河外光谱巡天目标星表的关键步骤。倾向于包容性的分类器可能会将伪星包含在内，浪费光纤时间，而更保守的分类器可能会忽略星系，从而损害完整性，进而影响巡天目标。为了避免监督方法中训练集引入的偏差，我们采用了一种无监督机器学习方法。我们利用广域 VISTA 河外星系巡天 (WAVES)-Wide 星表的光度数据，该星表包含 9 波段 $u-K_s$ 数据，并利用 ${\rm P{\scriptsize RO} F{\scriptsize OUND}}$ 提取颜色、流量和视尺寸信息，创建了一个特征空间。我们应用非线性降维方法 UMAP（均匀流形近似和投影）结合分类器 ${\rm{\scriptsize HDBSCAN}}$ 对恒星和星系进行分类。我们使用来自 Gaia、SDSS、GAMA 和 DESI 的真实星表，根据基线颜色和形态方法验证了我们的方法。在 AB 星等限制为 $Z = 21.2$ 的情况下，我们正确识别了 99.72% 的星系，在整个真实样本中，F1 得分为 0.9970，而基线方法的 F1 得分为 0.9871。与基线方法 (0.9780) 相比，我们的方法具有更高的纯度 (0.9966)，从而提高了效率，识别出的星系或不明来源减少了 11%，在 4MOST 仪器上节省了大约 70,000 个光纤小时。我们获得了具有挑战性的来源（包括类星体、致密星系和低表面亮度星系）的可靠分类统计数据，分别检索到其中的 95.1%、84.6% 和 99.5%。角聚类分析验证了我们的分类，表明无论基线分类如何，都与预期的星系聚类一致。||
|**2024-06-17**|[CM2-Net: Continual Cross-Modal Mapping Network for Driver Action Recognition](http://arxiv.org/abs/2406.11340)|null|驾驶员动作识别通过整合红外和深度等多种模态，在增强驾驶员与车辆的交互和确保驾驶安全方面取得了显著进展。然而，与仅使用RGB模态相比，在车厢环境中为所有类型的非RGB模态收集大量数据始终是一项费力且昂贵的工作。因此，以前的工作建议通过微调在RGB视频上预训练的模型来独立学习每个非RGB模态，但这些方法在面对新出现的模态时，由于较大的域差异，在提取信息特征方面效果较差。相比之下，我们提出了一种连续跨模态映射网络（CM2-Net），利用先前学习的模态的指导性提示，持续学习每个新出现的模态。具体来说，我们开发了累积跨模态映射提示（ACMP），将从先前模态中学习到的判别性和信息性特征映射到新出现的模态的特征空间中。然后，当面对新出现的模态时，这些映射的特征能够为应该提取和优先考虑哪些特征提供有效的提示。这些提示在整个持续学习过程中不断积累，从而进一步提高识别性能。在Drive&Act数据集上进行的大量实验表明，CM2-Net在单模态和多模态驾驶员动作识别方面均具有优越的性能。||
|**2024-06-17**|[Syn-to-Real Unsupervised Domain Adaptation for Indoor 3D Object Detection](http://arxiv.org/abs/2406.11311)|null|在室内3D目标检测中使用合成数据，可以大大减少3D标注所需的人工，并训练有效的零样本检测器。然而，跨越合成到真实室内数据集的复杂域偏移问题仍未得到充分探索。本文提出了一种新的面向对象的层次化域对齐（OHDA）框架，用于室内3D目标检测中的合成到真实无监督域自适应。我们的方法包括一种对象感知增强策略，以有效地使源域数据多样化，并且我们引入了一个由对抗训练分支和伪标签分支组成的双分支自适应框架，以便同时实现整体级别和类别级别的域对齐。针对室内无监督域自适应，我们提出了两种专门设计的方案，进一步改进了伪标签。我们从合成数据集3D-FRONT到真实世界数据集ScanNetV2和SUN RGB-D的适应结果表明，与仅使用源数据的基线相比，mAP25分别显著提高了9.7%和9.1%，并且始终优于从2D和3D室外场景适应的方法。代码将在论文被接收后公开。||

<p align=right>(<a href=#updated-on-20240629>back to top</a>)</p>

