{"\u591a\u6a21\u6001": {"2409.02914": "|**2024-09-04**|[Can LVLMs Obtain a Driver's License? A Benchmark Towards Reliable AGI for Autonomous Driving](http://arxiv.org/abs/2409.02914)|null|\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u8fd1\u671f\u5907\u53d7\u5173\u6ce8\uff0c\u8bb8\u591a\u7814\u7a76\u81f4\u529b\u4e8e\u5229\u7528\u5176\u901a\u7528\u77e5\u8bc6\u6765\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002\u7136\u800c\uff0cLVLM \u901a\u5e38\u4f9d\u8d56\u4e8e\u5927\u578b\u901a\u7528\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u4e13\u4e1a\u5b89\u5168\u9a7e\u9a76\u6240\u9700\u7684\u4e13\u4e1a\u77e5\u8bc6\u3002\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u9a7e\u9a76\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u573a\u666f\u7406\u89e3\u548c\u51b3\u7b56\uff0c\u6ca1\u6709\u63d0\u4f9b\u4e0e\u9a7e\u9a76\u5b89\u5168\u76f4\u63a5\u76f8\u5173\u7684\u4ea4\u901a\u89c4\u5219\u548c\u9a7e\u9a76\u6280\u80fd\u65b9\u9762\u7684\u660e\u786e\u6307\u5bfc\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 IDKB\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b\u8d85\u8fc7\u4e00\u767e\u4e07\u6761\u6570\u636e\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u6570\u636e\u6765\u81ea\u591a\u4e2a\u56fd\u5bb6\uff0c\u5305\u62ec\u9a7e\u9a76\u624b\u518c\u3001\u7406\u8bba\u6d4b\u8bd5\u6570\u636e\u548c\u6a21\u62df\u9053\u8def\u6d4b\u8bd5\u6570\u636e\u3002\u5c31\u50cf\u83b7\u5f97\u9a7e\u9a76\u6267\u7167\u7684\u8fc7\u7a0b\u4e00\u6837\uff0cIDKB \u6db5\u76d6\u4e86\u4ece\u7406\u8bba\u5230\u5b9e\u8df5\u9a7e\u9a76\u6240\u9700\u7684\u51e0\u4e4e\u6240\u6709\u663e\u6027\u77e5\u8bc6\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u4f7f\u7528 IDKB \u5bf9 15 \u4e2a LVLM \u8fdb\u884c\u4e86\u5168\u9762\u6d4b\u8bd5\uff0c\u4ee5\u8bc4\u4f30\u5b83\u4eec\u5728\u81ea\u52a8\u9a7e\u9a76\u73af\u5883\u4e0b\u7684\u53ef\u9760\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u7684\u5206\u6790\u3002\u6211\u4eec\u8fd8\u5fae\u8c03\u4e86\u6d41\u884c\u7684\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8fd9\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6211\u4eec\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027\u3002\u9879\u76ee\u9875\u9762\uff1a\\url{https://4dvlab.github.io/project_page/idkb.html}|\n", "2409.02882": "|**2024-09-04**|[Benchmarking Spurious Bias in Few-Shot Image Classifiers](http://arxiv.org/abs/2409.02882)|**[link](https://github.com/gtzheng/fewstab)**|\u5c11\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u5668\u65e8\u5728\u4ee5\u6700\u5c11\u7684\u76d1\u7763\u548c\u6709\u9650\u7684\u6570\u636e\u8bc6\u522b\u548c\u5206\u7c7b\u65b0\u6570\u636e\uff0c\u4f46\u901a\u5e38\u8868\u73b0\u51fa\u5bf9\u7c7b\u522b\u548c\u865a\u5047\u5c5e\u6027\u4e4b\u95f4\u865a\u5047\u76f8\u5173\u6027\u7684\u4f9d\u8d56\uff0c\u79f0\u4e3a\u865a\u5047\u504f\u5dee\u3002\u865a\u5047\u76f8\u5173\u6027\u901a\u5e38\u5b58\u5728\u4e8e\u67d0\u4e9b\u6837\u672c\u4e2d\uff0c\u5c11\u6837\u672c\u5206\u7c7b\u5668\u53ef\u80fd\u4f1a\u53d7\u5230\u7531\u5b83\u4eec\u5f15\u8d77\u7684\u865a\u5047\u504f\u5dee\u7684\u5f71\u54cd\u3002\u76ee\u524d\u7f3a\u4e4f\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u7cfb\u7edf\u6765\u8bc4\u4f30\u5c11\u6837\u672c\u5206\u7c7b\u5668\u9488\u5bf9\u865a\u5047\u504f\u5dee\u7684\u9c81\u68d2\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u4e14\u4e25\u683c\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u79f0\u4e3aFewSTAB\uff0c\u4ee5\u516c\u5e73\u5730\u5c55\u793a\u548c\u91cf\u5316\u5c11\u6837\u672c\u5206\u7c7b\u5668\u5bf9\u865a\u5047\u504f\u5dee\u7684\u4e0d\u540c\u7a0b\u5ea6\u7684\u9c81\u68d2\u6027\u3002FewSTAB \u521b\u5efa\u4e86\u5177\u6709\u504f\u5dee\u5c5e\u6027\u7684\u5c11\u6837\u672c\u8bc4\u4f30\u4efb\u52a1\uff0c\u4ee5\u4fbf\u4f7f\u7528\u5b83\u4eec\u8fdb\u884c\u9884\u6d4b\u53ef\u4ee5\u663e\u793a\u51fa\u8f83\u5dee\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u6784\u5efa\u8fd9\u4e9b\u4efb\u52a1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u4e8e\u5c5e\u6027\u7684\u6837\u672c\u9009\u62e9\u7b56\u7565\uff0c\u4ece\u800c\u65e0\u9700\u624b\u52a8\u8fdb\u884c\u6570\u636e\u96c6\u7ba1\u7406\u3002\u8fd9\u4f7f\u5f97 FewSTAB \u53ef\u4ee5\u4f7f\u7528\u4efb\u4f55\u73b0\u6709\u7684\u6d4b\u8bd5\u6570\u636e\u81ea\u52a8\u5bf9\u865a\u5047\u504f\u5dee\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002FewSTAB \u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7ef4\u5ea6\u7684\u8bc4\u4f30\u7ed3\u679c\u4ee5\u53ca\u6784\u5efa\u9c81\u68d2\u5206\u7c7b\u5668\u7684\u65b0\u8bbe\u8ba1\u6307\u5357\u3002\u6b64\u5916\uff0c\u5b83\u53ef\u4ee5\u5bf9\u4e0d\u540c\u7a0b\u5ea6\u7684\u865a\u5047\u504f\u5dee\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u652f\u6301\u4e0d\u540c\u7a0b\u5ea6\u7684\u9c81\u68d2\u6027\u8bbe\u8ba1\u3002\u901a\u8fc7\u5bf9\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5341\u79cd\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002\u6211\u4eec\u5e0c\u671b\u6211\u4eec\u7684\u6846\u67b6\u80fd\u591f\u542f\u53d1\u9c81\u68d2\u7684\u5c11\u6837\u672c\u5206\u7c7b\u5668\u7684\u65b0\u8bbe\u8ba1\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728https://github.com/gtzheng/FewSTAB\u83b7\u53d6\u3002|\n", "2409.02834": "|**2024-09-04**|[CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of Large Multimodal Models](http://arxiv.org/abs/2409.02834)|null|\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6570\u5b66\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u53ef\u559c\u7684\u6210\u679c\uff0c\u800c\u6570\u5b66\u63a8\u7406\u662f\u4eba\u7c7b\u667a\u80fd\u7684\u4e00\u9879\u57fa\u672c\u6280\u80fd\u3002\u4ee5\u5f80\u7684\u5927\u591a\u6570\u7814\u7a76\u90fd\u96c6\u4e2d\u5728\u57fa\u4e8e\u6587\u672c\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\uff08\u4f8b\u5982MATH\u3001GSM8K\uff09\u6765\u6539\u8fdb\u548c\u8861\u91cfLLM\u7684\u6027\u80fd\u3002\u6700\u8fd1\uff0c\u4e00\u4e9b\u7814\u7a76\u4eba\u5458\u53d1\u5e03\u4e86\u82f1\u6587\u591a\u6a21\u6001\u6570\u5b66\u6570\u636e\u96c6\uff08\u4f8b\u5982MATHVISTA\u548cMATH-V\uff09\uff0c\u4ee5\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u7684\u6709\u6548\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u53d1\u5e03\u4e86\u4e00\u4e2a\u4e2d\u6587\u591a\u6a21\u6001\u6570\u5b66\uff08CMM-Math\uff09\u6570\u636e\u96c6\uff0c\u5305\u62ec\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bad\u7ec3\u90e8\u5206\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u589e\u5f3aLMM\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002CMM-Math\u5305\u542b\u8d85\u8fc728,000\u4e2a\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u6db5\u76d6\u4e86\u4ece\u5c0f\u5b66\u5230\u9ad8\u4e2d12\u4e2a\u5e74\u7ea7\u7684\u5404\u79cd\u95ee\u9898\u7c7b\u578b\uff08\u4f8b\u5982\uff0c\u591a\u9879\u9009\u62e9\u3001\u586b\u7a7a\u7b49\uff09\uff0c\u5e76\u63d0\u4f9b\u8be6\u7ec6\u7684\u89e3\u7b54\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u89c6\u89c9\u4e0a\u4e0b\u6587\u53ef\u80fd\u51fa\u73b0\u5728\u95ee\u9898\u6216\u9009\u9879\u4e2d\uff0c\u8fd9\u4f7f\u5f97\u8be5\u6570\u636e\u96c6\u66f4\u5177\u6311\u6218\u6027\u3002\u901a\u8fc7\u5168\u9762\u5206\u6790\uff0c\u6211\u4eec\u53d1\u73b0\u6700\u5148\u8fdb\u7684LMM\u5728CMM-Math\u6570\u636e\u96c6\u4e0a\u9762\u4e34\u6311\u6218\uff0c\u8fd9\u51f8\u663e\u4e86\u8fdb\u4e00\u6b65\u6539\u8fdbLMM\u5f00\u53d1\u7684\u5fc5\u8981\u6027\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6570\u5b66LMM\uff08Math-LMM\uff09\u6765\u5904\u7406\u5305\u542b\u591a\u4e2a\u56fe\u50cf\u548c\u6587\u672c\u7247\u6bb5\u7684\u6df7\u5408\u8f93\u5165\u95ee\u9898\u3002\u6211\u4eec\u4f7f\u7528\u4e09\u4e2a\u9636\u6bb5\u8bad\u7ec3\u6211\u4eec\u7684\u6a21\u578b\uff0c\u5305\u62ec\u57fa\u7840\u9884\u8bad\u7ec3\u3001\u57fa\u7840\u5fae\u8c03\u548c\u6570\u5b66\u5fae\u8c03\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u4e0e\u4e09\u4e2a\u591a\u6a21\u6001\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u7684SOTA LMM\u8fdb\u884c\u6bd4\u8f83\uff0c\u6211\u4eec\u7684\u6a21\u578b\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u6570\u5b66\u63a8\u7406\u6027\u80fd\u3002|\n", "2409.02813": "|**2024-09-04**|[MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark](http://arxiv.org/abs/2409.02813)|null|\u672c\u6587\u4ecb\u7ecd\u4e86MMMU-Pro\uff0c\u5b83\u662f\u5927\u89c4\u6a21\u591a\u5b66\u79d1\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u63a8\u7406\uff08MMMU\uff09\u57fa\u51c6\u6d4b\u8bd5\u7684\u589e\u5f3a\u7248\u672c\u3002MMMU-Pro\u901a\u8fc7\u57fa\u4e8eMMMU\u7684\u4e09\u6b65\u6d41\u7a0b\u4e25\u683c\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u771f\u6b63\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff1a\uff081\uff09\u8fc7\u6ee4\u6389\u4ec5\u51ed\u6587\u672c\u6a21\u578b\u5373\u53ef\u56de\u7b54\u7684\u95ee\u9898\uff0c\uff082\uff09\u6269\u5145\u5019\u9009\u9009\u9879\uff0c\u4ee5\u53ca\uff083\uff09\u5f15\u5165\u7eaf\u89c6\u89c9\u8f93\u5165\u8bbe\u7f6e\uff0c\u5176\u4e2d\u95ee\u9898\u5d4c\u5165\u5728\u56fe\u50cf\u4e2d\u3002\u8fd9\u79cd\u8bbe\u7f6e\u6311\u6218\u4eba\u5de5\u667a\u80fd\u771f\u6b63\u505a\u5230\u540c\u65f6\u201c\u770b\u201d\u548c\u201c\u8bfb\u201d\uff0c\u6d4b\u8bd5\u4eba\u7c7b\u65e0\u7f1d\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u7684\u57fa\u672c\u8ba4\u77e5\u80fd\u529b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u5728MMMU-Pro\u4e0a\u7684\u6027\u80fd\u660e\u663e\u4f4e\u4e8eMMMU\uff0c\u4e0d\u540c\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u572816.8%\u523026.9%\u4e4b\u95f4\u3002\u6211\u4eec\u63a2\u8ba8\u4e86OCR\u63d0\u793a\u548c\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u7684\u5f71\u54cd\uff0c\u53d1\u73b0OCR\u63d0\u793a\u7684\u5f71\u54cd\u5f88\u5c0f\uff0c\u800cCoT\u901a\u5e38\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002MMMU-Pro\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u66f4\u8d34\u8fd1\u771f\u5b9e\u573a\u666f\uff0c\u5e76\u4e3a\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u65b9\u5411\u3002|\n", "2409.02664": "|**2024-09-04**|[Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection](http://arxiv.org/abs/2409.02664)|null|\u6df1\u5ea6\u4f2a\u9020\u4eba\u8138\u7684\u6cdb\u6ee5\u5bf9\u6211\u4eec\u7684\u65e5\u5e38\u751f\u6d3b\u6784\u6210\u5de8\u5927\u7684\u6f5c\u5728\u8d1f\u9762\u5f71\u54cd\u3002\u5c3d\u7ba1\u8fd1\u5e74\u6765\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u53d6\u5f97\u4e86\u5b9e\u8d28\u6027\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5bf9\u6765\u81ea\u672a\u89c1\u6570\u636e\u96c6\u6216\u7531\u65b0\u5174\u751f\u6210\u6a21\u578b\u521b\u5efa\u7684\u4f2a\u9020\u54c1\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u7136\u6709\u9650\u3002\u5728\u672c\u6587\u4e2d\uff0c\u53d7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u96f6\u6837\u672c\u4f18\u52bf\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5c06\u8bad\u7ec3\u6709\u7d20\u7684 VLM \u7528\u4e8e\u901a\u7528\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u3002\u53d7\u901a\u8fc7\u6570\u636e\u6270\u52a8\u64cd\u7eb5\u6a21\u578b\u9884\u6d4b\u7684\u6a21\u578b\u91cd\u7f16\u7a0b\u8303\u5f0f\u7684\u542f\u53d1\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u4ec5\u57fa\u4e8e\u64cd\u7eb5\u5176\u8f93\u5165\u6765\u91cd\u65b0\u7f16\u7a0b\u9884\u8bad\u7ec3\u7684 VLM \u6a21\u578b\uff08\u4f8b\u5982 CLIP\uff09\uff0c\u800c\u65e0\u9700\u8c03\u6574\u5185\u90e8\u53c2\u6570\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728\u6587\u672c\u63d0\u793a\u4e2d\u63d2\u5165\u4e86\u4e00\u4e2a\u7531\u9762\u90e8\u8eab\u4efd\u5f15\u5bfc\u7684\u4f2a\u8bcd\u3002\u5728\u51e0\u4e2a\u6d41\u884c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff1a(1) \u4f7f\u7528\u6211\u4eec\u63d0\u51fa\u7684\u91cd\u7f16\u7a0b\u65b9\u6cd5\uff0c\u9884\u8bad\u7ec3\u7684 CLIP \u6a21\u578b\u53ef\u4ee5\u663e\u8457\u4e14\u4e00\u81f4\u5730\u63d0\u9ad8\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u8de8\u6570\u636e\u96c6\u548c\u8de8\u64cd\u4f5c\u6027\u80fd\uff08\u4f8b\u5982\uff0c\u4ece FF++ \u5230 WildDeepfake \u7684\u8de8\u6570\u636e\u96c6\u8bbe\u7f6e\u4e2d\u8d85\u8fc7 88% \u7684 AUC\uff09\uff1b(2) \u6211\u4eec\u7684\u4f18\u8d8a\u6027\u80fd\u4ee5\u66f4\u5c11\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u4e3a\u4ee3\u4ef7\uff0c\u4f7f\u5176\u6210\u4e3a\u73b0\u5b9e\u5e94\u7528\u4e2d\u5f88\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002|\n", "2409.02530": "|**2024-09-04**|[Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models](http://arxiv.org/abs/2409.02530)|null|\u80be\u5c0f\u7403\u6ee4\u8fc7\u7387\uff08eGFR\uff09\u662f\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u80be\u529f\u80fd\u7684\u91cd\u8981\u6307\u6807\u3002\u867d\u7136\u4f20\u7edf\u7684\u516c\u5f0f\u548c\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u6a21\u578b\u53ef\u4ee5\u4f7f\u7528\u4e34\u5e8a\u548c\u5b9e\u9a8c\u5ba4\u6570\u636e\u4f30\u7b97eGFR\uff0c\u4f46\u51c6\u786e\u9884\u6d4b\u672a\u6765eGFR\u6c34\u5e73\u5bf9\u80be\u75c5\u5b66\u5bb6\u548cML\u7814\u7a76\u4eba\u5458\u6765\u8bf4\u4ecd\u7136\u662f\u4e00\u9879\u91cd\u5927\u6311\u6218\u3002\u6700\u8fd1\u7684\u8fdb\u5c55\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u53ef\u4ee5\u4f5c\u4e3a\u5404\u79cd\u5e94\u7528\u7684\u5f3a\u5927\u57fa\u7840\u6a21\u578b\u3002\u672c\u7814\u7a76\u5229\u7528\u4e00\u4e2a\u5305\u542b50\u540d\u60a3\u8005\u5b9e\u9a8c\u5ba4\u548c\u4e34\u5e8a\u503c\u7684\u6570\u636e\u96c6\uff0c\u63a2\u8ba8\u4e86LMM\u9884\u6d4b\u672a\u6765eGFR\u6c34\u5e73\u7684\u6f5c\u529b\u3002\u901a\u8fc7\u6574\u5408\u5404\u79cd\u63d0\u793a\u6280\u672f\u548cLMM\u96c6\u6210\uff0c\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u7ed3\u5408\u7cbe\u786e\u63d0\u793a\u548ceGFR\u8f68\u8ff9\u7684\u53ef\u89c6\u5316\u8868\u793a\u65f6\uff0c\u5176\u9884\u6d4b\u6027\u80fd\u53ef\u4e0e\u73b0\u6709ML\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002\u8fd9\u9879\u7814\u7a76\u6269\u5c55\u4e86\u57fa\u7840\u6a21\u578b\u7684\u5e94\u7528\uff0c\u5e76\u4e3a\u672a\u6765\u5229\u7528\u8fd9\u4e9b\u6a21\u578b\u89e3\u51b3\u590d\u6742\u533b\u5b66\u9884\u6d4b\u6311\u6218\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u9014\u5f84\u3002|\n", "2409.02278": "|**2024-09-03**|[Evaluation and Comparison of Visual Language Models for Transportation Engineering Problems](http://arxiv.org/abs/2409.02278)|null|\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u6700\u65b0\u53d1\u5c55\u663e\u793a\u51fa\u5176\u5728\u56fe\u50cf\u7406\u89e3\u76f8\u5173\u5e94\u7528\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u4ea4\u901a\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4f8b\u5982\u56fe\u50cf\u5206\u7c7b\u548c\u76ee\u6807\u68c0\u6d4b\u3002\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u5305\u62ec\u62e5\u5835\u68c0\u6d4b\u548c\u88c2\u7f1d\u8bc6\u522b\uff0c\u800c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u5219\u7528\u4e8e\u8bc6\u522b\u672a\u4f69\u6234\u5934\u76d4\u7684\u884c\u4e3a\u3002\u6211\u4eec\u5e94\u7528\u4e86CLIP\u3001BLIP\u3001OWL-ViT\u3001Llava-Next\u7b49\u5f00\u6e90\u6a21\u578b\u548c\u95ed\u6e90\u6a21\u578bGPT-4o\uff0c\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4ee5\u5229\u7528\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u6765\u5b8c\u6210\u57fa\u4e8e\u89c6\u89c9\u7684\u4ea4\u901a\u4efb\u52a1\u3002\u8fd9\u4e9b\u4efb\u52a1\u662f\u901a\u8fc7\u5bf9VLM\u6a21\u578b\u5e94\u7528\u96f6\u6837\u672c\u63d0\u793a\u6765\u6267\u884c\u7684\uff0c\u56e0\u4e3a\u96f6\u6837\u672c\u63d0\u793a\u5141\u8bb8\u5728\u4e0d\u5bf9\u4efb\u52a1\u8fdb\u884c\u4efb\u4f55\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u4efb\u52a1\u3002\u5b83\u6d88\u9664\u4e86\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u6807\u6ce8\u6570\u636e\u96c6\u6216\u5fae\u8c03\u7684\u9700\u6c42\u3002\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4e0e\u57fa\u51c6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u4f46\u5728\u76ee\u6807\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4ecd\u6709\u6539\u8fdb\u7684\u7a7a\u95f4\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u5bf9\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u7a81\u51fa\u4e86\u8fd9\u4e9b\u6a21\u578b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u672a\u6765\u6539\u8fdb\u548c\u5927\u89c4\u6a21\u5b9e\u65bd\u7684\u57fa\u7ebf\u3002|\n", "2409.02253": "|**2024-09-03**|[How to Determine the Preferred Image Distribution of a Black-Box Vision-Language Model?](http://arxiv.org/abs/2409.02253)|null|\u5927\u578b\u57fa\u7840\u6a21\u578b\u5f7b\u5e95\u6539\u53d8\u4e86\u8be5\u9886\u57df\uff0c\u4f46\u9488\u5bf9\u7279\u5b9a\u89c6\u89c9\u4efb\u52a1\u4f18\u5316\u591a\u6a21\u6001\u6a21\u578b\u4ecd\u7136\u5b58\u5728\u6311\u6218\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u4e0d\u540c\u8f93\u5165\u63d0\u793a\u4e0b\u8f93\u51fa\u7684\u4e00\u81f4\u6027\uff0c\u6765\u786e\u5b9a\u9ed1\u76d2\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u9996\u9009\u56fe\u50cf\u5206\u5e03\u3002\u6211\u4eec\u5c06\u5176\u5e94\u7528\u4e8e 3D \u5bf9\u8c61\u7684\u4e0d\u540c\u6e32\u67d3\u7c7b\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9700\u8981\u7cbe\u786e\u89e3\u91ca\u590d\u6742\u7ed3\u6784\u7684\u5404\u4e2a\u9886\u57df\u7684\u6709\u6548\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u8ba1\u7b97\u673a\u8f85\u52a9\u8bbe\u8ba1 (CAD) \u4f5c\u4e3a\u793a\u4f8b\u9886\u57df\u3002\u6211\u4eec\u4f7f\u7528\u4eba\u7c7b\u53cd\u9988\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u8fdb\u4e00\u6b65\u5b8c\u5584\u4e86 VLM \u8f93\u51fa\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u91ca\u8d28\u91cf\u3002\u4e3a\u4e86\u89e3\u51b3\u4e13\u4e1a\u9886\u57df\u7f3a\u4e4f\u57fa\u51c6\u7684\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 CAD-VQA\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30 VLM \u5728 CAD \u76f8\u5173\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u65b0\u6570\u636e\u96c6\u3002\u6211\u4eec\u5bf9 CAD-VQA \u4e0a\u6700\u5148\u8fdb\u7684 VLM \u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5efa\u7acb\u4e86\u57fa\u7ebf\u6027\u80fd\u6c34\u5e73\uff0c\u4e3a\u5728\u9700\u8981\u4e13\u5bb6\u7ea7\u89c6\u89c9\u89e3\u91ca\u7684\u5404\u4e2a\u9886\u57df\u63a8\u8fdb VLM \u5728\u590d\u6742\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\u3002\u6211\u4eec\u5728 \\url{https://github.com/asgsaeid/cad_vqa} \u4e0a\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u4ee3\u7801\u3002|\n", "2409.02101": "|**2024-09-03**|[Towards Real-World Adverse Weather Image Restoration: Enhancing Clearness and Semantics with Vision-Language Models](http://arxiv.org/abs/2409.02101)|**[link](https://github.com/jiaqixuac/WResVLM)**|\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6076\u52a3\u5929\u6c14\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u5e94\u7528\u4e8e\u73b0\u5b9e\u573a\u666f\u65f6\u7684\u5c40\u9650\u6027\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6765\u589e\u5f3a\u73b0\u5b9e\u73af\u5883\u4e2d\u4e0d\u540c\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u6062\u590d\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u56fe\u50cf\u6e05\u6670\u5ea6\u8bc4\u4f30\u548c\u8bed\u4e49\u63d0\u4f9b\uff0c\u4f5c\u4e3a\u8bad\u7ec3\u6062\u590d\u6a21\u578b\u7684\u76d1\u7763\u4fe1\u53f7\u3002\u5bf9\u4e8e\u6e05\u6670\u5ea6\u589e\u5f3a\uff0c\u6211\u4eec\u4f7f\u7528\u771f\u5b9e\u6570\u636e\uff0c\u91c7\u7528\u53cc\u91cd\u7b56\u7565\uff0c\u5373\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u7684\u4f2a\u6807\u7b7e\u548c\u5929\u6c14\u63d0\u793a\u5b66\u4e60\u3002\u5bf9\u4e8e\u8bed\u4e49\u589e\u5f3a\uff0c\u6211\u4eec\u901a\u8fc7\u8c03\u6574\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63cf\u8ff0\u4e2d\u7684\u5929\u6c14\u6761\u4ef6\uff0c\u540c\u65f6\u4fdd\u7559\u8bed\u4e49\uff0c\u6765\u6574\u5408\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\u6765\u63d0\u5347\u6062\u590d\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u7684\u6076\u52a3\u5929\u6c14\u56fe\u50cf\u6062\u590d\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7ed3\u679c\uff0c\u901a\u8fc7\u4e0e\u73b0\u6709\u6700\u4f73\u5de5\u4f5c\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u6bd4\u8f83\u8bc1\u660e\u4e86\u8fd9\u4e00\u70b9\u3002|\n", "2409.02084": "|**2024-09-03**|[GraspSplats: Efficient Manipulation with 3D Feature Splatting](http://arxiv.org/abs/2409.02084)|null|The ability for robots to perform efficient and zero-shot grasping of object parts is crucial for practical applications and is becoming prevalent with recent advances in Vision-Language Models (VLMs). To bridge the 2D-to-3D gap for representations to support such a capability, existing methods rely on neural fields (NeRFs) via differentiable rendering or point-based projection methods. However, we demonstrate that NeRFs are inappropriate for scene changes due to their implicitness and point-based methods are inaccurate for part localization without rendering-based optimization. To amend these issues, we propose GraspSplats. Using depth supervision and a novel reference feature computation method, GraspSplats generates high-quality scene representations in under 60 seconds. We further validate the advantages of Gaussian-based representation by showing that the explicit and optimized geometry in GraspSplats is sufficient to natively support (1) real-time grasp sampling and (2) dynamic and articulated object manipulation with point trackers. With extensive experiments on a Franka robot, we demonstrate that GraspSplats significantly outperforms existing methods under diverse task settings. In particular, GraspSplats outperforms NeRF-based methods like F3RM and LERF-TOGO, and 2D detection methods.|\n", "2409.03521": "|**2024-09-05**|[Have Large Vision-Language Models Mastered Art History?](http://arxiv.org/abs/2409.03521)|null|\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u51fa\u73b0\u6700\u8fd1\u5728\u8de8\u591a\u4e2a\u9886\u57df\u7684\u56fe\u50cf\u5206\u7c7b\u65b9\u9762\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002\u7136\u800c\uff0cVLM \u5728\u827a\u672f\u54c1\u5206\u7c7b\u8fd9\u4e00\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u7ed8\u753b\u827a\u672f\u98ce\u683c\u5206\u7c7b\u2014\u2014\u4f20\u7edf\u4e0a\u7531\u827a\u672f\u53f2\u5b66\u5bb6\u638c\u63e1\u7684\u9886\u57df\u2014\u2014\u5c1a\u672a\u5f97\u5230\u63a2\u7d22\u3002\u4e0e\u81ea\u7136\u56fe\u50cf\u76f8\u6bd4\uff0c\u827a\u672f\u54c1\u7531\u4e8e\u5176\u56fa\u6709\u7684\u590d\u6742\u548c\u591a\u6837\u5316\u7684\u7ed3\u6784\uff0c\u4ee5\u591a\u53d8\u7684\u6784\u56fe\u548c\u98ce\u683c\u4e3a\u7279\u5f81\uff0c\u56e0\u6b64\u6784\u6210\u4e86\u72ec\u7279\u7684\u6311\u6218\u3002\u827a\u672f\u53f2\u5b66\u5bb6\u957f\u671f\u4ee5\u6765\u4e00\u76f4\u5728\u7814\u7a76\u827a\u672f\u54c1\u7684\u72ec\u7279\u65b9\u9762\uff0c\u98ce\u683c\u9884\u6d4b\u662f\u5176\u5b66\u79d1\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002\u672c\u6587\u7814\u7a76\u4e86\u96c6\u6210\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u7684\u5927\u578b VLM \u662f\u5426\u53ef\u4ee5\u6709\u6548\u5730\u9884\u6d4b\u7ed8\u753b\u7684\u827a\u672f\u5386\u53f2\u5c5e\u6027\u3002\u6211\u4eec\u5bf9\u56db\u79cd VLM\uff0c\u5373 CLIP\u3001LLaVA\u3001OpenFlamingo \u548c GPT-4o \u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u91cd\u70b9\u5173\u6ce8\u4f7f\u7528\u4e24\u4e2a\u516c\u5171\u827a\u672f\u54c1\u57fa\u51c6\u6570\u636e\u96c6\u5bf9\u827a\u672f\u98ce\u683c\u3001\u4f5c\u8005\u548c\u65f6\u95f4\u6bb5\u8fdb\u884c\u96f6\u6837\u672c\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86 ArTest\uff0c\u8fd9\u662f\u4e00\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u827a\u672f\u54c1\u6d4b\u8bd5\u96c6\uff0c\u5176\u4e2d\u5305\u62ec\u827a\u672f\u53f2\u5b66\u5bb6\u7814\u7a76\u7684\u5173\u952e\u7ed8\u753b\u4f5c\u54c1\u3002|\n"}, "6DOF Object Pose": {"2409.02581": "|**2024-09-04**|[Object Gaussian for Monocular 6D Pose Estimation from Sparse Views](http://arxiv.org/abs/2409.02581)|null|\u5355\u76ee\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u4eba\u6280\u672f\u4e2d\u7684\u4e00\u9879\u5173\u952e\u4efb\u52a1\uff0c\u4e25\u91cd\u4f9d\u8d56\u4e8e\u7cbe\u786e\u7684 2D-3D \u5bf9\u5e94\u5173\u7cfb\uff0c\u800c\u8fd9\u901a\u5e38\u9700\u8981\u53ef\u80fd\u4e0d\u6613\u83b7\u5f97\u7684\u6602\u8d35 CAD \u6a21\u578b\u3002\u7269\u4f53\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u5176\u4e2d\u6700\u8fd1\u5728\u4e09\u7ef4\u9ad8\u65af\u6563\u5c04 (3DGS) \u65b9\u9762\u7684\u8fdb\u5c55\u63d0\u4f9b\u4e86\u5f15\u4eba\u6ce8\u76ee\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u5b83\u7684\u6027\u80fd\u4ecd\u7136\u53d7\u5230\u5f71\u54cd\uff0c\u5e76\u4e14\u5728\u8f93\u5165\u89c6\u56fe\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u5f80\u5f80\u4f1a\u51fa\u73b0\u8fc7\u62df\u5408\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86 SGPose\uff0c\u8fd9\u662f\u4e00\u4e2a\u4f7f\u7528\u57fa\u4e8e\u9ad8\u65af\u65b9\u6cd5\u8fdb\u884c\u7a00\u758f\u89c6\u56fe\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u65b0\u6846\u67b6\u3002SGPose \u4ec5\u9700\u5341\u4e2a\u89c6\u56fe\u5373\u53ef\u751f\u6210\u51e0\u4f55\u611f\u77e5\u8868\u793a\uff0c\u5b83\u4ece\u968f\u673a\u957f\u65b9\u4f53\u521d\u59cb\u5316\u5f00\u59cb\uff0c\u907f\u514d\u4e86\u4f20\u7edf 3DGS \u65b9\u6cd5\u6240\u9700\u7684\u57fa\u4e8e\u8fd0\u52a8\u6062\u590d\u7ed3\u6784 (SfM) \u7ba1\u9053\u51e0\u4f55\u7ed3\u6784\u7684\u4f9d\u8d56\u3002SGPose \u901a\u8fc7\u56de\u5f52\u7a00\u758f\u8f93\u5165\u548c\u968f\u673a\u521d\u59cb\u5316\u7684\u56fe\u50cf\u4e0e\u91cd\u5efa\u6a21\u578b\u4e4b\u95f4\u7684\u5bc6\u96c6 2D-3D \u5bf9\u5e94\u5173\u7cfb\uff0c\u6d88\u9664\u4e86\u5bf9 CAD \u6a21\u578b\u7684\u4f9d\u8d56\uff0c\u800c\u51e0\u4f55\u4e00\u81f4\u7684\u6df1\u5ea6\u76d1\u7763\u548c\u5728\u7ebf\u5408\u6210\u89c6\u56fe\u626d\u66f2\u662f\u6210\u529f\u7684\u5173\u952e\u3002\u5728\u5178\u578b\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5c24\u5176\u662f\u5728\u906e\u6321 LM-O \u6570\u636e\u96c6\u4e0a\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u7a00\u758f\u89c6\u56fe\u7ea6\u675f\u4e0b\uff0cSGPose \u4e5f\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd9\u7a81\u51fa\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002|\n", "2408.16547": "|**2024-08-29**|[OP-Align: Object-level and Part-level Alignment for Self-supervised Category-level Articulated Object Pose Estimation](http://arxiv.org/abs/2408.16547)|**[link](https://github.com/yc-che/op-align)**|\u7c7b\u522b\u7ea7\u94f0\u63a5\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4fa7\u91cd\u4e8e\u4f30\u8ba1\u5df2\u77e5\u7c7b\u522b\u4e2d\u672a\u77e5\u94f0\u63a5\u7269\u4f53\u7684\u59ff\u6001\u3002\u5c3d\u7ba1\u610f\u4e49\u91cd\u5927\uff0c\u4f46\u7531\u4e8e\u7269\u4f53\u5f62\u72b6\u548c\u59ff\u6001\u7684\u591a\u6837\u6027\u3001\u6570\u636e\u96c6\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u4ee5\u53ca\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u7684\u590d\u6742\u6027\uff0c\u8fd9\u9879\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u5229\u7528\u5355\u5e27\u70b9\u4e91\u6765\u89e3\u51b3\u6b64\u4efb\u52a1\u3002\u6211\u4eec\u7684\u6a21\u578b\u4e00\u81f4\u5730\u751f\u6210\u5177\u6709\u89c4\u8303\u59ff\u6001\u548c\u6574\u4e2a\u8f93\u5165\u5bf9\u8c61\u7684\u5173\u8282\u72b6\u6001\u7684\u91cd\u5efa\uff0c\u5e76\u4f30\u8ba1\u5bf9\u8c61\u7ea7\u59ff\u6001\uff08\u51cf\u5c11\u6574\u4f53\u59ff\u6001\u5dee\u5f02\uff09\u548c\u96f6\u4ef6\u7ea7\u59ff\u6001\uff08\u5c06\u8f93\u5165\u7684\u6bcf\u4e2a\u96f6\u4ef6\u4e0e\u5176\u5bf9\u5e94\u96f6\u4ef6\u5bf9\u9f50\uff09\u91cd\u5efa\uff09\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u4ee5\u524d\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u4e14\u4e0e\u6700\u5148\u8fdb\u7684\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u3002\u4e3a\u4e86\u8bc4\u4f30\u6211\u4eec\u6a21\u578b\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u73b0\u5b9e\u4e16\u754c\u94f0\u63a5\u7269\u4f53\u57fa\u51c6\u6570\u636e\u96c6\u3002|\n", "2408.10450": "|**2024-08-19**|[RUMI: Rummaging Using Mutual Information](http://arxiv.org/abs/2408.10450)|null|\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u641c\u7d22 (RUMI) \u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7528\u4e8e\u5728\u7ebf\u751f\u6210\u673a\u5668\u4eba\u5728\u89c6\u89c9\u906e\u6321\u73af\u5883\u4e2d\u6536\u96c6\u6709\u5173\u5df2\u77e5\u53ef\u79fb\u52a8\u7269\u4f53\u59ff\u6001\u4fe1\u606f\u7684\u52a8\u4f5c\u5e8f\u5217\u3002 \u6211\u4eec\u7684\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u63a5\u89e6\u4e30\u5bcc\u7684\u641c\u7d22\uff0c\u5229\u7528\u7269\u4f53\u59ff\u6001\u5206\u5e03\u548c\u673a\u5668\u4eba\u8f68\u8ff9\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u8fdb\u884c\u52a8\u4f5c\u89c4\u5212\u3002 \u4ece\u89c2\u5bdf\u5230\u7684\u90e8\u5206\u70b9\u4e91\uff0cRUMI \u63a8\u65ad\u51fa\u517c\u5bb9\u7684\u7269\u4f53\u59ff\u6001\u5206\u5e03\uff0c\u5e76\u5b9e\u65f6\u8fd1\u4f3c\u5176\u4e0e\u5de5\u4f5c\u7a7a\u95f4\u5360\u7528\u7387\u7684\u4e92\u4fe1\u606f\u3002 \u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u4fe1\u606f\u589e\u76ca\u6210\u672c\u51fd\u6570\u548c\u4e00\u4e2a\u53ef\u8fbe\u6027\u6210\u672c\u51fd\u6570\uff0c\u4ee5\u5c06\u7269\u4f53\u4fdd\u6301\u5728\u673a\u5668\u4eba\u7684\u89e6\u53ca\u8303\u56f4\u5185\u3002 \u8fd9\u4e9b\u4e0e\u968f\u673a\u52a8\u529b\u5b66\u6a21\u578b\u96c6\u6210\u5230\u6a21\u578b\u9884\u6d4b\u63a7\u5236 (MPC) \u6846\u67b6\u4e2d\uff0c\u5728\u95ed\u73af\u4e2d\u66f4\u65b0\u59ff\u6001\u5206\u5e03\u3002 \u4e3b\u8981\u8d21\u732e\u5305\u62ec\u4e00\u4e2a\u65b0\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7f6e\u4fe1\u6846\u67b6\u3001\u4e00\u79cd\u6709\u6548\u7684\u4fe1\u606f\u589e\u76ca\u8ba1\u7b97\u7b56\u7565\u548c\u4e00\u4e2a\u9c81\u68d2\u7684\u57fa\u4e8e MPC \u7684\u63a7\u5236\u65b9\u6848\u3002 \u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cRUMI \u5728\u6a21\u62df\u548c\u771f\u5b9e\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002|\n", "2408.08234": "|**2024-08-15**|[Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation](http://arxiv.org/abs/2408.08234)|**[link](https://github.com/varunburde/reconstruction_pose_benchmark)**|\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u5bf9\u4e8e\u8bb8\u591a\u6d89\u53ca\u673a\u5668\u4eba\u64cd\u4f5c\u3001\u5bfc\u822a\u548c\u589e\u5f3a\u73b0\u5b9e\u7684\u5de5\u4e1a\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u901a\u7528\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u5668\uff0c\u5373\u4e0d\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u7269\u4f53\u8fdb\u884c\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u4f9d\u8d56\u4e8e\u7cbe\u786e\u7684 3D \u6a21\u578b\u3002\u4e3b\u8981\u4f7f\u7528 CAD \u6a21\u578b\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u53ef\u80fd\u96be\u4ee5\u83b7\u53d6\u3002\u540c\u65f6\uff0c\u83b7\u53d6\u7269\u4f53\u7684\u56fe\u50cf\u901a\u5e38\u662f\u53ef\u884c\u7684\u3002\u81ea\u7136\u800c\u7136\u5730\uff0c\u8fd9\u5c31\u5f15\u51fa\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u4ece\u56fe\u50cf\u91cd\u5efa\u7684 3D \u6a21\u578b\u662f\u5426\u8db3\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff1f\u4e3a\u4e86\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8861\u91cf 3D \u91cd\u5efa\u8d28\u91cf\u5bf9\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u7528\u4e8e\u7269\u4f53\u91cd\u5efa\u7684\u6821\u51c6\u56fe\u50cf\uff0c\u8fd9\u4e9b\u56fe\u50cf\u4e0e YCB-V \u6570\u636e\u96c6\u7684\u6d4b\u8bd5\u56fe\u50cf\u914d\u51c6\uff0c\u7528\u4e8e\u5728 BOP \u57fa\u51c6\u6d4b\u8bd5\u683c\u5f0f\u4e0b\u8fdb\u884c\u59ff\u6001\u8bc4\u4f30\u3002\u4f7f\u7528\u591a\u79cd\u6700\u5148\u8fdb\u7684 3D \u91cd\u5efa\u548c\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u8fdb\u884c\u7684\u8be6\u7ec6\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u4ee3\u91cd\u5efa\u65b9\u6cd5\u751f\u6210\u7684\u51e0\u4f55\u5f62\u72b6\u901a\u5e38\u8db3\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u59ff\u6001\u4f30\u8ba1\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u5f97\u51fa\u4e86\u4e00\u4e9b\u6709\u8da3\u7684\u89c2\u5bdf\u7ed3\u679c\uff1a(1) \u7528\u4e8e\u8861\u91cf 3D \u91cd\u5efa\u8d28\u91cf\u7684\u6807\u51c6\u6307\u6807\u4e0d\u4e00\u5b9a\u80fd\u53cd\u6620\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u8fd9\u8868\u660e\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f8b\u5982\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002(2) \u4f20\u7edf\u7684\u3001\u975e\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u53ef\u4ee5\u4e0e\u73b0\u4ee3\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u91cd\u5efa\u6280\u672f\u76f8\u5ab2\u7f8e\uff0c\u751a\u81f3\u53ef\u4ee5\u63d0\u4f9b\u66f4\u597d\u7684\u91cd\u5efa\u65f6\u95f4-\u59ff\u6001\u7cbe\u5ea6\u6743\u8861\u3002(3) \u4f7f\u7528\u91cd\u5efa\u6a21\u578b\u548c CAD \u6a21\u578b\u7684\u6027\u80fd\u4e4b\u95f4\u4ecd\u7136\u5b58\u5728\u76f8\u5f53\u5927\u7684\u5dee\u8ddd\u3002\u4e3a\u4e86\u4fc3\u8fdb\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u7684\u7814\u7a76\uff0c\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u5728 https://github.com/VarunBurde/reconstruction_pose_benchmark \u4e0a\u516c\u5f00\u53ef\u7528\u3002|\n", "2407.12207": "|**2024-07-16**|[NeuSurfEmb: A Complete Pipeline for Dense Correspondence-based 6D Object Pose Estimation without CAD Models](http://arxiv.org/abs/2407.12207)|**[link](https://github.com/ethz-asl/neusurfemb)**|\u7528\u4e8e 6D \u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u5047\u8bbe\u53ef\u4ee5\u4f7f\u7528 CAD \u6a21\u578b\uff0c\u5e76\u4e14\u9700\u8981\u7528\u6237\u624b\u52a8\u8bbe\u7f6e\u57fa\u4e8e\u7269\u7406\u7684\u6e32\u67d3 (PBR) \u7ba1\u9053\u6765\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\u3002\u8fd9\u4e24\u4e2a\u56e0\u7d20\u90fd\u9650\u5236\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u9700\u8981 CAD \u6a21\u578b\u7684\u6d41\u7a0b\uff0c\u5e76\u4e14\u53ea\u9700\u4e00\u5c0f\u7ec4\u771f\u5b9e\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u5373\u53ef\u8bad\u7ec3\u51fa\u6700\u5148\u8fdb\u7684\u59ff\u6001\u4f30\u8ba1\u5668\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u57fa\u4e8e NeuS2 \u5bf9\u8c61\u8868\u793a\uff0c\u6211\u4eec\u901a\u8fc7\u57fa\u4e8e\u8fd0\u52a8\u6062\u590d\u7ed3\u6784 (SfM) \u548c\u4e0e\u5bf9\u8c61\u65e0\u5173\u7684\u5206\u5272\u7684\u534a\u81ea\u52a8\u5316\u7a0b\u5e8f\u6765\u5b66\u4e60\u8be5\u8868\u793a\u3002\u6211\u4eec\u5229\u7528 NeuS2 \u7684\u65b0\u89c6\u56fe\u5408\u6210\u80fd\u529b\u548c\u7b80\u5355\u7684\u526a\u5207\u7c98\u8d34\u589e\u5f3a\u529f\u80fd\u6765\u81ea\u52a8\u751f\u6210\u903c\u771f\u7684\u7269\u4f53\u6e32\u67d3\u56fe\uff0c\u6211\u4eec\u4f7f\u7528\u8fd9\u4e9b\u6e32\u67d3\u56fe\u6765\u8bad\u7ec3\u57fa\u4e8e\u5bf9\u5e94\u7684 SurfEmb \u59ff\u6001\u4f30\u8ba1\u5668\u3002\u6211\u4eec\u5728 LINEMOD-Occlusion \u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e7f\u6cdb\u7814\u7a76\u4e86\u5176\u5404\u4e2a\u7ec4\u4ef6\u7684\u5f71\u54cd\uff0c\u5e76\u5c55\u793a\u4e86\u76f8\u5bf9\u4e8e\u57fa\u4e8e CAD \u6a21\u578b\u548c PBR \u6570\u636e\u7684\u65b9\u6cd5\u7684\u7ade\u4e89\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u6211\u4eec\u6d41\u7a0b\u5728\u81ea\u6536\u96c6\u7684\u73b0\u5b9e\u4e16\u754c\u7269\u4f53\u4e0a\u7684\u6613\u7528\u6027\u548c\u6709\u6548\u6027\uff0c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65e0 CAD \u6a21\u578b\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u597d\u7684\u7cbe\u5ea6\u548c\u5bf9\u8f7b\u5fae\u906e\u6321\u7684\u9c81\u68d2\u6027\u3002\u4e3a\u4e86\u8ba9\u673a\u5668\u4eba\u793e\u533a\u4ece\u8be5\u7cfb\u7edf\u4e2d\u53d7\u76ca\uff0c\u6211\u4eec\u5c06\u5728 https://www.github.com/ethz-asl/neusurfemb \u4e0a\u516c\u5f00\u53d1\u5e03\u5b83\u3002|\n", "2406.04316": "|**2024-06-06**|[Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking](http://arxiv.org/abs/2406.04316)|null|6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u4e00\u9879\u81f3\u5173\u91cd\u8981\u4f46\u6781\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5176\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002\u8fd9\u79cd\u7a00\u7f3a\u6027\u963b\u788d\u4e86\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u9650\u5236\u4e86\u7814\u7a76\u8fdb\u5c55\u3002\u6b64\u5916\uff0c\u53ef\u7528\u5b9e\u4f8b\u6216\u7c7b\u522b\u7684\u6570\u91cf\u6709\u9650\u4e5f\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86Omni6DPose\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ee5\u5bf9\u8c61\u7c7b\u522b\u591a\u6837\u6027\u3001\u89c4\u6a21\u5927\u548c\u5bf9\u8c61\u6750\u8d28\u591a\u6837\u6027\u4e3a\u7279\u5f81\u7684\u5927\u578b\u6570\u636e\u96c6\u3002Omni6DPose\u4e3b\u8981\u7531\u4e09\u4e2a\u90e8\u5206\u7ec4\u6210\uff1aROPE\uff08\u771f\u5b9e6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u96c6\uff09\uff0c\u5305\u542b332K\u5f20\u56fe\u50cf\uff0c\u6db5\u76d6149\u4e2a\u7c7b\u522b\u3001581\u4e2a\u5b9e\u4f8b\uff0c\u8d85\u8fc7150\u4e07\u4e2a\u6807\u6ce8\uff1bSOPE\uff08\u6a21\u62df6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u96c6\uff09\uff0c\u5305\u542b475K\u5f20\u56fe\u50cf\uff0c\u8fd9\u4e9b\u56fe\u50cf\u662f\u5728\u6df7\u5408\u73b0\u5b9e\u73af\u5883\u4e2d\u521b\u5efa\u7684\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u6a21\u62df\u751f\u6210\uff0c\u6db5\u76d6\u4e0eROPE\u76f8\u540c\u7684149\u4e2a\u7c7b\u522b\u30014162\u4e2a\u5b9e\u4f8b\uff0c\u8d85\u8fc7500\u4e07\u4e2a\u6807\u6ce8\uff1b\u4ee5\u53ca\u5728ROPE\u548cSOPE\u4e2d\u5747\u4f7f\u7528\u7684\u624b\u52a8\u5bf9\u9f50\u7684\u771f\u5b9e\u626b\u63cf\u7269\u4f53\u3002\u7531\u4e8e\u5b58\u5728\u5927\u91cf\u53d8\u5316\u548c\u6b67\u4e49\uff0cOmni6DPose\u672c\u8eab\u5c31\u6781\u5177\u6311\u6218\u6027\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86GenPose++\uff0c\u5b83\u662fSOTA\u7c7b\u522b\u7ea7\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\u7684\u589e\u5f3a\u7248\u672c\uff0c\u5b83\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6539\u8fdb\uff1a\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u63d0\u53d6\u548c\u57fa\u4e8e\u805a\u7c7b\u7684\u805a\u5408\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u5206\u6790\uff0c\u4ee5\u8bc4\u4f30\u5148\u524d\u65b9\u6cd5\u5728\u8fd9\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u57286D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u59ff\u6001\u8ddf\u8e2a\u65b9\u9762\u7684\u6027\u80fd\u3002|\n", "2406.02977": "|**2024-06-05**|[Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices](http://arxiv.org/abs/2406.02977)|null|\u968f\u7740\u673a\u5668\u4eba\u548c\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u8d8a\u6765\u8d8a\u4f9d\u8d56\u4e8e\u7cbe\u786e\u9ad8\u6548\u7684 6D \u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u6027\u80fd\u5bf9\u4e8e\u5b9e\u73b0\u66f4\u5177\u4ea4\u4e92\u6027\u548c\u54cd\u5e94\u80fd\u529b\u7684\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u63d0\u51fa\u7684\u7a00\u758f\u989c\u8272\u4ee3\u7801\u7f51\u7edc (SCCN) \u4f53\u73b0\u4e86\u4e00\u79cd\u6e05\u6670\u7b80\u6d01\u7684\u6d41\u6c34\u7ebf\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u3002SCCN \u5229\u7528\u57fa\u672c\u7269\u4f53\u51e0\u4f55\u7279\u5f81\u7684\u7a00\u758f\u6027\u6765\u52a0\u901f\u900f\u89c6 n \u70b9 (PnP) \u8ba1\u7b97\u8fc7\u7a0b\uff0c\u5bf9 RGB \u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u7269\u4f53\u8fdb\u884c\u50cf\u7d20\u7ea7\u9884\u6d4b\u3002\u6b64\u5916\uff0c\u5b83\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u50cf\u7d20\u7ea7\u51e0\u4f55\u7684\u7269\u4f53\u5bf9\u79f0\u8868\u793a\uff0c\u8be5\u8868\u793a\u4e0e\u521d\u59cb\u59ff\u6001\u9884\u6d4b\u65e0\u7f1d\u96c6\u6210\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5bf9\u79f0\u7269\u4f53\u7684\u6b67\u4e49\u95ee\u9898\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSCCN \u5728 NVIDIA Jetson AGX Xavier \u4e0a\u5206\u522b\u5728\u57fa\u51c6 LINEMOD \u6570\u636e\u96c6\u548c\u906e\u6321 LINEMOD \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6bcf\u79d2 19 \u5e27 (FPS) \u548c 6 FPS \u7684\u4f30\u8ba1\u901f\u7387\uff0c\u540c\u65f6\u5728\u8fd9\u4e9b\u901f\u7387\u4e0b\u59cb\u7ec8\u4fdd\u6301\u8f83\u9ad8\u7684\u4f30\u8ba1\u7cbe\u5ea6\u3002|\n", "2405.07801": "|**2024-05-31**|[Deep Learning-Based Object Pose Estimation: A Comprehensive Survey](http://arxiv.org/abs/2405.07801)|**[link](https://github.com/cnjianliu/awesome-object-pose-estimation)**|\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff0c\u5728\u589e\u5f3a\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u6280\u672f\u4e2d\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u5728\u8fc7\u53bb\u7684\u5341\u5e74\u4e2d\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7531\u4e8e\u5176\u5353\u8d8a\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u8d8a\u6765\u8d8a\u591a\u5730\u53d6\u4ee3\u4e86\u4f9d\u8d56\u4e8e\u5de5\u7a0b\u70b9\u5bf9\u7279\u5f81\u7684\u4f20\u7edf\u7b97\u6cd5\u3002\u7136\u800c\uff0c\u5f53\u4ee3\u65b9\u6cd5\u4ecd\u7136\u5b58\u5728\u82e5\u5e72\u6311\u6218\uff0c\u5305\u62ec\u5b83\u4eec\u5bf9\u6807\u8bb0\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\u6027\u3001\u6a21\u578b\u7d27\u51d1\u6027\u3001\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u6cdb\u5316\u5230\u65b0\u9896\u7684\u672a\u89c1\u8fc7\u5bf9\u8c61\u7684\u80fd\u529b\u3002\u6700\u8fd1\u7f3a\u4e4f\u4e00\u7bc7\u7efc\u8ff0\u6765\u8ba8\u8bba\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3001\u5b58\u5728\u7684\u6311\u6218\u548c\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6db5\u76d6\u4e86\u8be5\u95ee\u9898\u7684\u6240\u6709\u4e09\u79cd\u5f62\u5f0f\uff0c\u5373\u5b9e\u4f8b\u7ea7\u3001\u7c7b\u522b\u7ea7\u548c\u672a\u89c1\u8fc7\u7269\u4f53\u7684\u59ff\u6001\u4f30\u8ba1\u3002\u6211\u4eec\u7684\u7efc\u8ff0\u8fd8\u6db5\u76d6\u4e86\u591a\u79cd\u8f93\u5165\u6570\u636e\u6a21\u6001\u3001\u8f93\u51fa\u59ff\u6001\u7684\u81ea\u7531\u5ea6\u3001\u7269\u4f53\u5c5e\u6027\u548c\u4e0b\u6e38\u4efb\u52a1\uff0c\u4e3a\u8bfb\u8005\u63d0\u4f9b\u4e86\u5bf9\u8be5\u9886\u57df\u7684\u5168\u9762\u7406\u89e3\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u8ba8\u8bba\u4e86\u4e0d\u540c\u9886\u57df\u7684\u8bad\u7ec3\u8303\u5f0f\u3001\u63a8\u7406\u6a21\u5f0f\u3001\u5e94\u7528\u9886\u57df\u3001\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u62a5\u544a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u5728\u8fd9\u4e9b\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\uff0c\u4ece\u800c\u65b9\u4fbf\u8bfb\u8005\u4e3a\u5176\u5e94\u7528\u9009\u62e9\u6700\u5408\u9002\u7684\u65b9\u6cd5\u3002\u6700\u540e\uff0c\u8be5\u7efc\u8ff0\u6307\u51fa\u4e86\u5173\u952e\u6311\u6218\uff0c\u56de\u987e\u4e86\u5f53\u524d\u7684\u8d8b\u52bf\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002\u6211\u4eec\u8fd8\u5c06\u7ee7\u7eed\u8ddf\u8e2ahttps://github.com/CNJianLiu/Awesome-Object-Pose-Estimation\u4e0a\u7684\u6700\u65b0\u5de5\u4f5c\u3002|\n", "2403.19527": "|**2024-03-28**|[Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation](http://arxiv.org/abs/2403.19527)|**[link](https://github.com/leeiieeo/ag-pose)**|\u7c7b\u522b\u7ea7 6D \u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65e8\u5728\u4f30\u8ba1\u7279\u5b9a\u7c7b\u522b\u4e2d\u672a\u89c1\u5b9e\u4f8b\u7684\u65cb\u8f6c\u3001\u5e73\u79fb\u548c\u5c3a\u5bf8\u3002\u5728\u8fd9\u4e2a\u9886\u57df\uff0c\u57fa\u4e8e\u5bc6\u96c6\u5bf9\u5e94\u7684\u65b9\u6cd5\u5df2\u7ecf\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5b83\u4eec\u6ca1\u6709\u660e\u786e\u8003\u8651\u4e0d\u540c\u5b9e\u4f8b\u7684\u5c40\u90e8\u548c\u5168\u5c40\u51e0\u4f55\u4fe1\u606f\uff0c\u5bfc\u81f4\u5bf9\u5177\u6709\u663e\u8457\u5f62\u72b6\u53d8\u5316\u7684\u672a\u89c1\u5b9e\u4f8b\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5b9e\u4f8b\u81ea\u9002\u5e94\u548c\u51e0\u4f55\u611f\u77e5\u5173\u952e\u70b9\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u7c7b\u522b\u7ea7 6D \u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff08AG-Pose\uff09\uff0c\u5b83\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a\uff081\uff09\u7b2c\u4e00\u4e2a\u8bbe\u8ba1\u662f\u5b9e\u4f8b\u81ea\u9002\u5e94\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u5757\uff0c\u5b83\u53ef\u4ee5\u81ea\u9002\u5e94\u5730\u68c0\u6d4b\u4e00\u7ec4\u7a00\u758f\u5173\u952e\u70b9\uff0c\u7528\u4e8e\u8868\u793a\u5404\u79cd\u5b9e\u4f8b\u7684\u51e0\u4f55\u7ed3\u6784\u3002(2) \u7b2c\u4e8c\u4e2a\u8bbe\u8ba1\u662f\u51e0\u4f55\u611f\u77e5\u7279\u5f81\u805a\u5408\u6a21\u5757\uff0c\u5b83\u53ef\u4ee5\u6709\u6548\u5730\u5c06\u5c40\u90e8\u548c\u5168\u5c40\u51e0\u4f55\u4fe1\u606f\u6574\u5408\u5230\u5173\u952e\u70b9\u7279\u5f81\u4e2d\u3002\u8fd9\u4e24\u4e2a\u6a21\u5757\u53ef\u4ee5\u534f\u540c\u5de5\u4f5c\uff0c\u4e3a\u672a\u89c1\u5b9e\u4f8b\u5efa\u7acb\u9c81\u68d2\u7684\u5173\u952e\u70b9\u7ea7\u5bf9\u5e94\u5173\u7cfb\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728 CAMERA25 \u548c REAL275 \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 AG-Pose \u5728\u6ca1\u6709\u7c7b\u522b\u7279\u5b9a\u5f62\u72b6\u5148\u9a8c\u7684\u60c5\u51b5\u4e0b\uff0c\u5927\u5e45\u5ea6\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u65b9\u6cd5\u3002|\n", "2403.18791": "|**2024-06-01**|[Object Pose Estimation via the Aggregation of Diffusion Features](http://arxiv.org/abs/2403.18791)|**[link](https://github.com/tianfu18/diff-feats-pose)**|\u4ece\u56fe\u50cf\u4e2d\u4f30\u8ba1\u7269\u4f53\u59ff\u6001\u662f 3D \u573a\u666f\u7406\u89e3\u7684\u4e00\u9879\u5173\u952e\u4efb\u52a1\uff0c\u6700\u8fd1\u7684\u65b9\u6cd5\u5728\u975e\u5e38\u5927\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u53ef\u559c\u7684\u7ed3\u679c\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5904\u7406\u672a\u89c1\u8fc7\u7269\u4f53\u65f6\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\u3002\u6211\u4eec\u8ba4\u4e3a\u8fd9\u662f\u7531\u4e8e\u56fe\u50cf\u7279\u5f81\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u9020\u6210\u7684\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u6df1\u5165\u5206\u6790\u4e86\u6269\u6563\u6a21\u578b\uff08\u4f8b\u5982 Stable Diffusion\uff09\u7684\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u5728\u5bf9\u672a\u89c1\u8fc7\u7269\u4f53\u5efa\u6a21\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002\u5728\u6b64\u5206\u6790\u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u521b\u65b0\u6027\u5730\u5c06\u8fd9\u4e9b\u6269\u6563\u7279\u5f81\u5f15\u5165\u5230\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4e2d\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u67b6\u6784\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u6355\u6349\u548c\u805a\u5408\u4e0d\u540c\u7c92\u5ea6\u7684\u6269\u6563\u7279\u5f81\uff0c\u5927\u5927\u63d0\u9ad8\u4e86\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e09\u4e2a\u6d41\u884c\u7684\u57fa\u51c6\u6570\u636e\u96c6 LM\u3001O-LM \u548c T-LESS \u4e0a\u4ee5\u76f8\u5f53\u5927\u7684\u4f18\u52bf\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u672a\u89c1\u8fc7\u7269\u4f53\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u5148\u524d\u6700\u4f73\u65b9\u6cd5\u66f4\u9ad8\u7684\u51c6\u786e\u7387\uff1a\u5728 Unseen LM \u4e0a\u4e3a 98.2% \u5bf9 93.5%\uff0c\u5728 Unseen O-LM \u4e0a\u4e3a 85.9% \u5bf9 76.3%\uff0c\u663e\u793a\u4e86\u6211\u4eec\u65b9\u6cd5\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u53d1\u5e03\u5728 https://github.com/Tianfu18/diff-feats-pose\u3002|\n"}, "nerf": {"2408.09130": "|**2024-08-20**|[Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark Images Using Gaussian Splatting](http://arxiv.org/abs/2408.09130)|**[link](https://github.com/yec22/Gaussian-DK)**|\u4e09\u7ef4\u9ad8\u65af\u4f53\u79ef\u6e32\u67d3\u6280\u672f\u8fd1\u5e74\u6765\u53d1\u5c55\u8fc5\u901f\uff0c\u5b83\u80fd\u591f\u5229\u7528\u4e00\u81f4\u7684\u591a\u89c6\u56fe\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u5408\u6210\u51fa\u975e\u51e1\u7684\u65b0\u89c6\u89d2\u3002\u7136\u800c\uff0c\u6211\u4eec\u6ce8\u610f\u5230\uff0c\u5728\u5149\u7ebf\u4e0d\u8db3\u7684\u73af\u5883\u4e2d\u62cd\u6444\u7684\u56fe\u50cf\uff0c\u7531\u4e8e\u573a\u666f\u672a\u88ab\u5b8c\u5168\u7167\u4eae\uff0c\u53ef\u80fd\u4f1a\u51fa\u73b0\u660e\u663e\u7684\u4eae\u5ea6\u53d8\u5316\u548c\u591a\u89c6\u56fe\u4e0d\u4e00\u81f4\u6027\uff0c\u8fd9\u5bf9\u4e09\u7ef4\u9ad8\u65af\u4f53\u79ef\u6e32\u67d3\u6280\u672f\u63d0\u51fa\u4e86\u5de8\u5927\u7684\u6311\u6218\uff0c\u5e76\u4e25\u91cd\u5f71\u54cd\u4e86\u5176\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Gaussian-DK \u65b9\u6cd5\u3002\u89c2\u5bdf\u5230\u4e0d\u4e00\u81f4\u6027\u4e3b\u8981\u662f\u7531\u76f8\u673a\u6210\u50cf\u5f15\u8d77\u7684\uff0c\u6211\u4eec\u4f7f\u7528\u4e00\u7ec4\u5404\u5411\u5f02\u6027\u7684\u4e09\u7ef4\u9ad8\u65af\u51fd\u6570\u6765\u8868\u793a\u7269\u7406\u4e16\u754c\u7684\u8f90\u5c04\u573a\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u76f8\u673a\u54cd\u5e94\u6a21\u5757\u6765\u8865\u507f\u591a\u89c6\u56fe\u7684\u4e0d\u4e00\u81f4\u6027\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b65\u957f\u7684\u68af\u5ea6\u7f29\u653e\u7b56\u7565\uff0c\u4ee5\u7ea6\u675f\u9760\u8fd1\u76f8\u673a\u7684\u9ad8\u65af\u51fd\u6570\uff08\u5373\u6f02\u6d6e\u7269\uff09\u7684\u5206\u5272\u548c\u514b\u9686\u3002\u5728\u6211\u4eec\u63d0\u51fa\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGaussian-DK \u65b9\u6cd5\u53ef\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\u7ed3\u679c\uff0c\u6ca1\u6709\u91cd\u5f71\u548c\u6f02\u6d6e\u7269\u4f2a\u5f71\uff0c\u5e76\u4e14\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u53ef\u4ee5\u901a\u8fc7\u63a7\u5236\u66dd\u5149\u7ea7\u522b\u6765\u5408\u6210\u589e\u4eae\u56fe\u50cf\uff0c\u6e05\u6670\u5730\u663e\u793a\u9634\u5f71\u533a\u57df\u7684\u7ec6\u8282\u3002|\n", "2407.13520": "|**2024-09-05**|[EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian Splatting](http://arxiv.org/abs/2407.13520)|null|\u8fd1\u5e74\u6765\uff0c\u968f\u7740\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u548c3D\u9ad8\u65af\u6563\u5c04\uff083DGS\uff09\u7684\u53d1\u5c55\uff0c3D\u53bb\u6a21\u7cca\u91cd\u5efa\u6280\u672f\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002 \u5c3d\u7ba1\u8fd9\u4e9b\u6280\u672f\u53ef\u4ee5\u4ece\u6a21\u7cca\u7684\u56fe\u50cf\u8f93\u5165\u4e2d\u6062\u590d\u76f8\u5bf9\u6e05\u6670\u76843D\u91cd\u5efa\uff0c\u4f46\u5b83\u4eec\u5728\u5904\u7406\u4e25\u91cd\u6a21\u7cca\u548c\u590d\u6742\u76f8\u673a\u8fd0\u52a8\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u5c40\u9650\u6027\u3002 \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e8b\u4ef6\u8f85\u52a93D\u9ad8\u65af\u6563\u5c04\u53bb\u6a21\u7cca\u91cd\u5efa\uff08EaDeblur-GS\uff09\uff0c\u5b83\u96c6\u6210\u4e86\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u4ee5\u589e\u5f3a3DGS\u5bf9\u8fd0\u52a8\u6a21\u7cca\u7684\u9c81\u68d2\u6027\u3002 \u901a\u8fc7\u91c7\u7528\u81ea\u9002\u5e94\u504f\u5dee\u4f30\u8ba1\u5668\uff08ADE\uff09\u7f51\u7edc\u4f30\u8ba1\u9ad8\u65af\u4e2d\u5fc3\u504f\u5dee\u5e76\u4f7f\u7528\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0cEaDeblur-GS\u53ef\u4ee5\u5b9e\u65f6\u5b9e\u73b0\u6e05\u6670\u76843D\u91cd\u5efa\uff0c\u5176\u6027\u80fd\u53ef\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002|\n", "2407.07090": "|**2024-07-10**|[3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes](http://arxiv.org/abs/2407.07090)|null|\u57fa\u4e8e\u7c92\u5b50\u7684\u8f90\u5c04\u573a\u8868\u793a\u6cd5\uff08\u5982 3D \u9ad8\u65af splatting\uff09\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u590d\u6742\u573a\u666f\u7684\u91cd\u5efa\u548c\u91cd\u65b0\u6e32\u67d3\u3002\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5149\u6805\u5316\u6e32\u67d3\u7c92\u5b50\uff0c\u5c06\u5b83\u4eec\u6295\u5f71\u5230\u5c4f\u5e55\u7a7a\u95f4\u56fe\u5757\u4e2d\uff0c\u5e76\u6309\u6392\u5e8f\u987a\u5e8f\u8fdb\u884c\u5904\u7406\u3002\u800c\u8fd9\u9879\u5de5\u4f5c\u5219\u8003\u8651\u5bf9\u7c92\u5b50\u8fdb\u884c\u5149\u7ebf\u8ffd\u8e2a\uff0c\u6784\u5efa\u8fb9\u754c\u4f53\u79ef\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u4f7f\u7528\u9ad8\u6027\u80fd GPU \u5149\u7ebf\u8ffd\u8e2a\u786c\u4ef6\u4e3a\u6bcf\u4e2a\u50cf\u7d20\u6295\u5c04\u5149\u7ebf\u3002\u4e3a\u4e86\u6709\u6548\u5904\u7406\u5927\u91cf\u534a\u900f\u660e\u7c92\u5b50\uff0c\u6211\u4eec\u63cf\u8ff0\u4e86\u4e00\u79cd\u4e13\u95e8\u7684\u6e32\u67d3\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4f7f\u7528\u8fb9\u754c\u7f51\u683c\u5c01\u88c5\u7c92\u5b50\u4ee5\u5229\u7528\u5feb\u901f\u7684\u5149\u7ebf\u4e09\u89d2\u5f62\u76f8\u4ea4\uff0c\u5e76\u6309\u6df1\u5ea6\u987a\u5e8f\u5bf9\u6210\u6279\u7684\u76f8\u4ea4\u8fdb\u884c\u7740\u8272\u3002\u5149\u7ebf\u8ffd\u8e2a\u5728\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\u7684\u4f18\u52bf\u4f17\u6240\u5468\u77e5\uff1a\u5904\u7406\u975e\u76f8\u5e72\u5149\u7ebf\u4ee5\u83b7\u5f97\u9634\u5f71\u548c\u53cd\u5c04\u7b49\u4e8c\u6b21\u7167\u660e\u6548\u679c\uff0c\u4ece\u673a\u5668\u4eba\u6280\u672f\u4e2d\u5e38\u89c1\u7684\u9ad8\u5ea6\u626d\u66f2\u7684\u76f8\u673a\u6e32\u67d3\uff0c\u968f\u673a\u91c7\u6837\u5149\u7ebf\u7b49\u7b49\u3002\u4f7f\u7528\u6211\u4eec\u7684\u6e32\u67d3\u5668\uff0c\u4e0e\u5149\u6805\u5316\u76f8\u6bd4\uff0c\u8fd9\u79cd\u7075\u6d3b\u6027\u51e0\u4e4e\u4e0d\u9700\u8981\u4efb\u4f55\u6210\u672c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u901f\u5ea6\u548c\u51c6\u786e\u6027\uff0c\u4ee5\u53ca\u5728\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u548c\u89c6\u89c9\u65b9\u9762\u7684\u82e5\u5e72\u5e94\u7528\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u5bf9\u57fa\u672c\u9ad8\u65af\u8868\u793a\u7684\u76f8\u5173\u6539\u8fdb\uff0c\u5305\u62ec\u7b80\u5355\u4f7f\u7528\u5e7f\u4e49\u6838\u51fd\u6570\uff0c\u8fd9\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u7c92\u5b50\u547d\u4e2d\u6b21\u6570\u3002|\n", "2407.05254": "|**2024-07-07**|[GaussReg: Fast 3D Registration with Gaussian Splatting](http://arxiv.org/abs/2407.05254)|null|\u70b9\u4e91\u914d\u51c6\u662f\u5927\u89c4\u6a21\u4e09\u7ef4\u573a\u666f\u626b\u63cf\u548c\u91cd\u5efa\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\u3002\u5728\u6df1\u5ea6\u5b66\u4e60\u7684\u5e2e\u52a9\u4e0b\uff0c\u914d\u51c6\u65b9\u6cd5\u5f97\u5230\u4e86\u663e\u8457\u53d1\u5c55\uff0c\u5df2\u63a5\u8fd1\u6210\u719f\u9636\u6bb5\u3002\u968f\u7740\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u7684\u5f15\u5165\uff0c\u5b83\u51ed\u501f\u5f3a\u5927\u7684\u89c6\u56fe\u5408\u6210\u80fd\u529b\u6210\u4e3a\u4e86\u6700\u6d41\u884c\u7684\u4e09\u7ef4\u573a\u666f\u8868\u793a\u65b9\u6cd5\u3002\u5bf9\u4e8e NeRF \u8868\u793a\uff0c\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u540c\u6837\u9700\u8981\u5bf9\u5176\u8fdb\u884c\u914d\u51c6\u3002\u7136\u800c\uff0c\u8be5\u4e3b\u9898\u8fd8\u5904\u4e8e\u6781\u5ea6\u7f3a\u4e4f\u63a2\u7d22\u7684\u72b6\u6001\u3002\u8fd9\u662f\u56e0\u4e3a\u4f7f\u7528\u9690\u5f0f\u8868\u793a\u5bf9\u4e24\u4e2a\u573a\u666f\u4e4b\u95f4\u7684\u51e0\u4f55\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u5b58\u5728\u56fa\u6709\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u9690\u5f0f\u8868\u793a\u8f6c\u6362\u4e3a\u663e\u5f0f\u8868\u793a\u4ee5\u8fdb\u884c\u8fdb\u4e00\u6b65\u914d\u51c6\u3002\u6700\u8fd1\uff0c\u9ad8\u65af splatting (GS) \u88ab\u5f15\u5165\uff0c\u5b83\u91c7\u7528\u4e86\u663e\u5f0f\u4e09\u7ef4\u9ad8\u65af\u51fd\u6570\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\u6548\u679c\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6e32\u67d3\u901f\u5ea6\u3002\u7ed9\u5b9a\u4e24\u4e2a\u5177\u6709\u663e\u5f0f GS \u8868\u793a\u7684\u573a\u666f\uff0c\u6211\u4eec\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\u63a2\u7d22\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u4e09\u7ef4\u914d\u51c6\u4efb\u52a1\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 GaussReg\uff0c\u4e00\u4e2a\u5feb\u901f\u4e14\u51c6\u786e\u7684\u4ece\u7c97\u5230\u7cbe\u7684\u65b0\u6846\u67b6\u3002\u7c97\u914d\u51c6\u9636\u6bb5\u6cbf\u7528\u73b0\u6709\u7684\u70b9\u4e91\u914d\u51c6\u65b9\u6cd5\uff0c\u5e76\u4f30\u8ba1\u6765\u81ea GS \u7684\u70b9\u4e91\u7684\u7c97\u7565\u5bf9\u9f50\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u56fe\u50cf\u5f15\u5bfc\u7684\u7cbe\u914d\u51c6\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ece GS \u6e32\u67d3\u56fe\u50cf\uff0c\u4e3a\u7cbe\u786e\u5bf9\u9f50\u63d0\u4f9b\u66f4\u8be6\u7ec6\u7684\u51e0\u4f55\u4fe1\u606f\u3002\u4e3a\u4e86\u652f\u6301\u5168\u9762\u8bc4\u4f30\uff0c\u6211\u4eec\u4ed4\u7ec6\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a ScanNet-GSReg \u7684\u573a\u666f\u7ea7\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4ece ScanNet \u6570\u636e\u96c6\u4e2d\u83b7\u5f97\u7684 1379 \u4e2a\u573a\u666f\uff0c\u5e76\u6536\u96c6\u4e86\u4e00\u4e2a\u540d\u4e3a GSReg \u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684 GaussReg \u6bd4 HLoc\uff08SuperPoint \u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0cSuperGlue \u4f5c\u4e3a\u5339\u914d\u5668\uff09\u5feb 44 \u500d\uff0c\u5e76\u4e14\u7cbe\u5ea6\u76f8\u5f53\u3002|\n", "2407.03923": "|**2024-07-04**|[CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion Blur Images](http://arxiv.org/abs/2407.03923)|null|\u7531\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u5177\u6709\u9ad8\u8d28\u91cf\u7684\u65b0\u9896\u89c6\u56fe\u6e32\u67d3\u80fd\u529b\uff0c\u5176\u53d7\u5230\u4e86\u5e7f\u6cdb\u5173\u6ce8\uff0c\u5e76\u4fc3\u4f7f\u7814\u7a76\u4eba\u5458\u81f4\u529b\u4e8e\u89e3\u51b3\u5404\u79cd\u73b0\u5b9e\u4e16\u754c\u6848\u4f8b\u3002\u5176\u4e2d\u4e00\u4e2a\u5173\u952e\u6311\u6218\u662f\u76f8\u673a\u5728\u66dd\u5149\u65f6\u95f4\u5185\u79fb\u52a8\u5bfc\u81f4\u7684\u8fd0\u52a8\u6a21\u7cca\uff0c\u8fd9\u963b\u788d\u4e86\u5bf9 3D \u573a\u666f\u8fdb\u884c\u7cbe\u786e\u91cd\u5efa\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8fde\u7eed\u521a\u4f53\u8fd0\u52a8\u611f\u77e5\u9ad8\u65af\u6563\u5c04 (CRiM-GS)\uff0c\u4ee5\u5b9e\u65f6\u6e32\u67d3\u901f\u5ea6\u4ece\u6a21\u7cca\u56fe\u50cf\u91cd\u5efa\u7cbe\u786e\u7684 3D \u573a\u666f\u3002\u8003\u8651\u5230\u5b9e\u9645\u76f8\u673a\u8fd0\u52a8\u6a21\u7cca\u8fc7\u7a0b\u5305\u542b\u590d\u6742\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u6211\u4eec\u57fa\u4e8e\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b (ODE) \u9884\u6d4b\u76f8\u673a\u7684\u8fde\u7eed\u8fd0\u52a8\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5229\u7528\u521a\u4f53\u53d8\u6362\u5bf9\u76f8\u673a\u8fd0\u52a8\u8fdb\u884c\u5efa\u6a21\u5e76\u8fdb\u884c\u9002\u5f53\u7684\u6b63\u5219\u5316\uff0c\u4ee5\u4fdd\u6301\u7269\u4f53\u7684\u5f62\u72b6\u548c\u5927\u5c0f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728 \\textit{SE(3)} \u573a\u4e2d\u5f15\u5165\u4e86\u8fde\u7eed\u53ef\u53d8\u5f62 3D \u53d8\u6362\uff0c\u901a\u8fc7\u786e\u4fdd\u66f4\u9ad8\u7684\u81ea\u7531\u5ea6\u4f7f\u521a\u4f53\u53d8\u6362\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\u3002\u901a\u8fc7\u91cd\u65b0\u5ba1\u89c6\u57fa\u672c\u76f8\u673a\u7406\u8bba\u5e76\u91c7\u7528\u5148\u8fdb\u7684\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6280\u672f\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u5bf9\u8fde\u7eed\u76f8\u673a\u8f68\u8ff9\u7684\u7cbe\u786e\u5efa\u6a21\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u65e0\u8bba\u662f\u5728\u5b9a\u91cf\u8fd8\u662f\u5b9a\u6027\u65b9\u9762\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|\n", "2406.18214": "|**2024-07-29**|[Trimming the Fat: Efficient Compression of 3D Gaussian Splats through Pruning](http://arxiv.org/abs/2406.18214)|**[link](https://github.com/salmanali96/trimming-the-fat)**|\u8fd1\u5e74\u6765\uff0c\u7531\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\u548c\u6700\u8fd1\u51fa\u73b0\u76843D\u9ad8\u65af splatting (3DGS) \u6a21\u578b\u63d0\u4f9b\u4e86\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u80fd\u529b\uff0c3D\u6a21\u578b\u7684\u4f7f\u7528\u5f97\u5230\u4e86\u63a8\u5e7f\u3002\u540e\u8005\u5177\u6709\u660e\u663e\u7684\u4f18\u52bf\uff0c\u56e0\u4e3a\u5b83\u672c\u8eab\u53ef\u4ee5\u7b80\u5316\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5feb\u901f\u6536\u655b\uff0c\u5e76\u63d0\u4f9b\u5e7f\u6cdb\u7684\u53ef\u7f16\u8f91\u6027\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5173\u4e8e\u8fd9\u4e9b\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u7684\u6587\u732e\u4ecd\u7136\u5904\u4e8e\u8d77\u6b65\u9636\u6bb5\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u91c7\u53d6\u4e86\u4e00\u4e9b\u521d\u6b65\u63aa\u65bd\u6765\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5c55\u793a\u4e86\u4e00\u79cd\u80fd\u591f\u5b9e\u73b0\u6b64\u7c7b\u6a21\u578b\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u7684\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u201cTrimming the fat\u201d\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u8fed\u4ee3\u5f0f\u540e\u526a\u679d\u6280\u672f\uff0c\u7528\u4e8e\u6d88\u9664\u7f16\u7801\u5728\u6a21\u578b\u4e2d\u7684\u5197\u4f59\u4fe1\u606f\u3002\u6211\u4eec\u5728\u5e7f\u6cdb\u8ba4\u53ef\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8\u57fa\u7ebf\u6027\u80fd\u7684\u540c\u65f6\uff0c\u53ef\u4ee5\u53bb\u9664\u9ad8\u8fbe75%\u7684\u9ad8\u65af\u51fd\u6570\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5927\u7ea650\u500d\u7684\u538b\u7f29\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u4f3c\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u5c06\u8ba1\u7b97\u901f\u5ea6\u63d0\u9ad8\u5230600 FPS\u3002|\n", "2406.15149": "|**2024-06-21**|[Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks](http://arxiv.org/abs/2406.15149)|null|\u4eff\u771f\u5668\u662f\u81ea\u4e3b\u673a\u5668\u4eba\u5b66\u4e60\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ef\u4ee5\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u6570\u636e\u751f\u6210\u3001\u7075\u6d3b\u7684\u8bbe\u8ba1\u548c\u8f68\u8ff9\u4f18\u5316\u3002\u7136\u800c\uff0c\u5c06\u4ece\u4eff\u771f\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u7684\u884c\u4e3a\u8fc1\u79fb\u5230\u73b0\u5b9e\u4e16\u754c\u4e2d\u88ab\u8bc1\u660e\u662f\u56f0\u96be\u7684\uff0c\u901a\u5e38\u9700\u8981\u901a\u8fc7\u8ba1\u7b97\u91cf\u5927\u7684\u57df\u968f\u673a\u5316\u65b9\u6cd5\u6216\u8fdb\u4e00\u6b65\u7684\u6a21\u578b\u5fae\u8c03\u6765\u7f13\u89e3\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u4eff\u771f\u5230\u771f\u5b9e\u89c6\u89c9\u56db\u65cb\u7ffc\u5bfc\u822a\u4efb\u52a1\u4e2d\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u5c06\u9ad8\u65af\u6837\u6761\u51fd\u6570\u4e0e\u56db\u65cb\u7ffc\u98de\u884c\u52a8\u529b\u5b66\u76f8\u7ed3\u5408\u6765\u6784\u5efa\u4eff\u771f\u5668\uff0c\u7136\u540e\u4f7f\u7528 Liquid \u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u9c81\u68d2\u7684\u5bfc\u822a\u7b56\u7565\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u6211\u4eec\u83b7\u5f97\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u6a21\u4eff\u5b66\u4e60\u534f\u8bae\uff0c\u5b83\u7ed3\u5408\u4e86 3D \u9ad8\u65af\u6837\u6761\u51fd\u6570\u8f90\u5c04\u573a\u6e32\u67d3\u7684\u8fdb\u6b65\u3001\u4e13\u5bb6\u6f14\u793a\u8bad\u7ec3\u6570\u636e\u7684\u7cbe\u5fc3\u7f16\u7a0b\u4ee5\u53ca Liquid \u7f51\u7edc\u7684\u4efb\u52a1\u7406\u89e3\u80fd\u529b\u3002\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9a\u91cf\u98de\u884c\u6d4b\u8bd5\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5728\u5355\u4e2a\u4eff\u771f\u573a\u666f\u4e2d\u5b66\u4e60\u5230\u7684\u5bfc\u822a\u6280\u80fd\u53ef\u4ee5\u76f4\u63a5\u7a33\u5065\u5730\u8fc1\u79fb\u5230\u73b0\u5b9e\u4e16\u754c\u4e2d\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5c55\u793a\u4e86\u5728\u5267\u70c8\u7684\u5206\u5e03\u548c\u7269\u7406\u73af\u5883\u53d8\u5316\u4e0b\uff0c\u5728\u8bad\u7ec3\u73af\u5883\u4e4b\u5916\u4fdd\u6301\u6027\u80fd\u7684\u80fd\u529b\u3002\u6211\u4eec\u5b66\u4e60\u5230\u7684 Liquid \u7b56\u7565\uff0c\u4ec5\u5728\u4ece\u903c\u771f\u7684\u6a21\u62df\u5ba4\u5185\u98de\u884c\u4e2d\u7cbe\u9009\u7684\u5355\u76ee\u6807\u673a\u52a8\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u53ef\u4ee5\u6cdb\u5316\u5230\u6237\u5916\u771f\u5b9e\u786c\u4ef6\u5e73\u53f0\u4e0a\u7684\u591a\u6b65\u8fdc\u8db3\u3002|\n", "2406.10373": "|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|\u5728\u975e\u7ed3\u6784\u5316\u7684\u65c5\u6e38\u73af\u5883\u4e2d\u62cd\u6444\u7684\u7167\u7247\u7ecf\u5e38\u5448\u73b0\u51fa\u591a\u53d8\u7684\u5916\u89c2\u548c\u77ed\u6682\u7684\u906e\u6321\uff0c\u8fd9\u5bf9\u7cbe\u786e\u7684\u573a\u666f\u91cd\u5efa\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u5e76\u5728\u65b0\u89c6\u56fe\u5408\u6210\u4e2d\u5bfc\u81f4\u4e86\u4f2a\u5f71\u3002\u5c3d\u7ba1\u5148\u524d\u7684\u65b9\u6cd5\u5df2\u7ecf\u5c06\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u4e0e\u5176\u4ed6\u53ef\u5b66\u4e60\u6a21\u5757\u76f8\u7ed3\u5408\u6765\u5904\u7406\u52a8\u6001\u5916\u89c2\u5e76\u6d88\u9664\u77ac\u6001\u5bf9\u8c61\uff0c\u4f46\u5176\u5927\u91cf\u7684\u8bad\u7ec3\u9700\u6c42\u548c\u7f13\u6162\u7684\u6e32\u67d3\u901f\u5ea6\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u6700\u8fd1\uff0c3D \u9ad8\u65af splatting (3DGS) \u5df2\u6210\u4e3a NeRF \u7684\u4e00\u79cd\u5f88\u6709\u524d\u9014\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5b83\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u4ee5\u53ca\u66f4\u597d\u7684\u6e32\u67d3\u8d28\u91cf\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 Wild-GS\uff0c\u8fd9\u662f\u4e00\u79cd\u9488\u5bf9\u4e0d\u53d7\u7ea6\u675f\u7684\u7167\u7247\u96c6\u4f18\u5316\u7684 3DGS \u521b\u65b0\u6539\u7f16\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5176\u6548\u7387\u4f18\u52bf\u3002Wild-GS \u901a\u8fc7\u6bcf\u5f20\u56fe\u50cf\u7684\u56fa\u6709\u6750\u8d28\u5c5e\u6027\u3001\u5168\u5c40\u7167\u660e\u548c\u76f8\u673a\u5c5e\u6027\u4ee5\u53ca\u9010\u70b9\u53cd\u5c04\u7387\u7684\u5c40\u90e8\u53d8\u5316\u6765\u786e\u5b9a\u6bcf\u4e2a 3D \u9ad8\u65af\u7684\u5916\u89c2\u3002\u4e0e\u5148\u524d\u5728\u56fe\u50cf\u7a7a\u95f4\u4e2d\u5bf9\u53c2\u8003\u7279\u5f81\u8fdb\u884c\u5efa\u6a21\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cWild-GS \u901a\u8fc7\u5bf9\u4ece\u53c2\u8003\u56fe\u50cf\u4e2d\u63d0\u53d6\u7684\u4e09\u5e73\u9762\u8fdb\u884c\u91c7\u6837\uff0c\u5c06\u50cf\u7d20\u5916\u89c2\u7279\u5f81\u660e\u786e\u5730\u4e0e\u76f8\u5e94\u7684\u5c40\u90e8\u9ad8\u65af\u5bf9\u9f50\u3002\u8fd9\u79cd\u65b0\u9896\u7684\u8bbe\u8ba1\u6709\u6548\u5730\u5c06\u53c2\u8003\u89c6\u56fe\u7684\u9ad8\u9891\u7ec6\u8282\u5916\u89c2\u8f6c\u79fb\u5230 3D \u7a7a\u95f4\uff0c\u5e76\u663e\u7740\u52a0\u5feb\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c2D \u53ef\u89c1\u6027\u56fe\u548c\u6df1\u5ea6\u6b63\u5219\u5316\u5206\u522b\u7528\u4e8e\u51cf\u8f7b\u77ac\u6001\u6548\u5e94\u548c\u7ea6\u675f\u51e0\u4f55\u5f62\u72b6\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cWild-GS \u5728\u6240\u6709\u73b0\u6709\u6280\u672f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6e32\u67d3\u6027\u80fd\u4ee5\u53ca\u6700\u9ad8\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002|\n", "2406.04253": "|**2024-06-06**|[A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation](http://arxiv.org/abs/2406.04253)|null|\u4e09\u7ef4\u5efa\u6a21\u4e00\u76f4\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u7684\u91cd\u8981\u9886\u57df\u3002\u8fd1\u5e74\u6765\uff0c\u7531\u4e8e\u795e\u7ecf\u8868\u793a\u548c\u751f\u6210\u6a21\u578b\u7684\u7a81\u7834\uff0c\u6211\u4eec\u76ee\u7779\u4e86\u4e09\u7ef4\u5efa\u6a21\u7684\u5feb\u901f\u53d1\u5c55\u3002\u4e09\u7ef4\u4eba\u4f53\u5efa\u6a21\u4f5c\u4e3a\u6e38\u620f\u548c\u52a8\u753b\u7b49\u8bb8\u591a\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u7684\u6838\u5fc3\uff0c\u5df2\u7ecf\u5f15\u8d77\u4e86\u4eba\u4eec\u7684\u5e7f\u6cdb\u5173\u6ce8\u3002\u5728\u8fc7\u53bb\u51e0\u5e74\u4e2d\uff0c\u51fa\u73b0\u4e86\u5927\u91cf\u5173\u4e8e\u521b\u5efa\u4e09\u7ef4\u4eba\u4f53\u5316\u8eab\u7684\u8457\u4f5c\uff0c\u4e3a\u4e09\u7ef4\u4eba\u4f53\u5efa\u6a21\u5f62\u6210\u4e86\u4e00\u4e2a\u65b0\u7684\u3001\u4e30\u5bcc\u7684\u77e5\u8bc6\u5e93\u3002\u6587\u732e\u7684\u89c4\u6a21\u4e4b\u5927\uff0c\u4f7f\u5f97\u4e2a\u4eba\u96be\u4ee5\u8ddf\u8e2a\u6240\u6709\u7684\u5de5\u4f5c\u3002\u672c\u7efc\u8ff0\u65e8\u5728\u4ece\u91cd\u5efa\u548c\u751f\u6210\u7684\u89d2\u5ea6\uff0c\u5168\u9762\u6982\u8ff0\u8fd9\u4e9b\u65b0\u5174\u7684\u4e09\u7ef4\u4eba\u4f53\u5316\u8eab\u5efa\u6a21\u6280\u672f\u3002\u9996\u5148\uff0c\u6211\u4eec\u56de\u987e\u4e86\u5177\u6709\u4ee3\u8868\u6027\u7684\u4e09\u7ef4\u4eba\u4f53\u91cd\u5efa\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u50cf\u7d20\u5bf9\u9f50\u9690\u51fd\u6570\u3001\u795e\u7ecf\u8f90\u5c04\u573a\u548c\u4e09\u7ef4\u9ad8\u65af\u6563\u5c04\u7b49\u65b9\u6cd5\u3002\u7136\u540e\uff0c\u6211\u4eec\u603b\u7ed3\u4e86\u5177\u6709\u4ee3\u8868\u6027\u7684\u4e09\u7ef4\u4eba\u4f53\u751f\u6210\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u3001\u6269\u6563\u6a21\u578b\u548c\u5404\u79cd\u4e09\u7ef4\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6700\u540e\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u6211\u4eec\u5bf9\u73b0\u6709\u65b9\u6cd5\u7684\u53cd\u601d\u4ee5\u53ca\u4e09\u7ef4\u4eba\u4f53\u5316\u8eab\u5efa\u6a21\u9762\u4e34\u7684\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002|\n", "2406.02720": "|**2024-06-13**|[3D-HGS: 3D Half-Gaussian Splatting](http://arxiv.org/abs/2406.02720)|**[link](https://github.com/lihaolin88/3d-half-gaussian-splatting)**|\u7167\u7247\u7ea7\u903c\u771f\u7684\u4e09\u7ef4\u91cd\u5efa\u662f\u4e09\u7ef4\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\u3002\u7531\u4e8e\u6700\u8fd1\u795e\u7ecf\u6e32\u67d3\u6280\u672f\u7684\u51fa\u73b0\uff0c\u8be5\u9886\u57df\u53d6\u5f97\u4e86\u76f8\u5f53\u5927\u7684\u8fdb\u6b65\u3002\u8fd9\u4e9b\u6280\u672f\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5b66\u4e60\u4e09\u7ef4\u573a\u666f\u7684\u4f53\u79ef\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u6e32\u67d3\u5f97\u5230\u7684\u635f\u5931\u51fd\u6570\u6765\u7ec6\u5316\u8fd9\u4e9b\u8868\u793a\u3002\u5176\u4e2d\uff0c\u4e09\u7ef4\u9ad8\u65af\u6563\u5c04\uff083D-GS\uff09\u5df2\u6210\u4e3a\u4e00\u79cd\u91cd\u8981\u7684\u65b9\u6cd5\uff0c\u5176\u6027\u80fd\u8d85\u8fc7\u4e86\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRFs\uff09\u30023D-GS\u4f7f\u7528\u53c2\u6570\u5316\u7684\u4e09\u7ef4\u9ad8\u65af\u51fd\u6570\u6765\u5efa\u6a21\u7a7a\u95f4\u4f4d\u7f6e\u548c\u989c\u8272\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u56fe\u5757\u7684\u5feb\u901f\u6e32\u67d3\u6280\u672f\u3002\u5c3d\u7ba1\u5176\u6e32\u67d3\u6027\u80fd\u548c\u901f\u5ea6\u90fd\u5f88\u51fa\u8272\uff0c\u4f46\u4f7f\u7528\u4e09\u7ef4\u9ad8\u65af\u6838\u51fd\u6570\u5728\u51c6\u786e\u8868\u793a\u4e0d\u8fde\u7eed\u51fd\u6570\u65b9\u9762\u5b58\u5728\u56fa\u6709\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u5f62\u72b6\u4e0d\u8fde\u7eed\u7684\u8fb9\u7f18\u548c\u89d2\u843d\uff0c\u4ee5\u53ca\u5728\u989c\u8272\u4e0d\u8fde\u7eed\u7684\u4e0d\u540c\u7eb9\u7406\u4e4b\u95f4\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u8bae\u91c7\u7528\u4e09\u7ef4\u534a\u9ad8\u65af\uff083D-HGS\uff09\u6838\u51fd\u6570\uff0c\u5b83\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u6838\u51fd\u6570\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u4eec\u80fd\u591f\u63d0\u9ad8\u5f53\u524d\u4e0e3D-GS\u76f8\u5173\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u5728\u4e0d\u5f71\u54cd\u6e32\u67d3\u901f\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6e32\u67d3\u6027\u80fd\u3002|\n", "2409.03213": "|**2024-09-05**|[Optimizing 3D Gaussian Splatting for Sparse Viewpoint Scene Reconstruction](http://arxiv.org/abs/2409.03213)|null|\u4e09\u7ef4\u9ad8\u65af splatting (3DGS) \u4f5c\u4e3a\u4e00\u79cd\u5f88\u6709\u524d\u666f\u7684\u4e09\u7ef4\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u4e0e\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u76f8\u6bd4\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002\u7136\u800c\uff0c3DGS \u5bb9\u6613\u51fa\u73b0\u9ad8\u9891\u4f2a\u5f71\uff0c\u5e76\u4e14\u5728\u7a00\u758f\u89c6\u70b9\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u4eba\u6280\u672f\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u9002\u7528\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86 SVS-GS\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u7a00\u758f\u89c6\u70b9\u573a\u666f\u91cd\u5efa\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u96c6\u6210\u4e86\u4e09\u7ef4\u9ad8\u65af\u5e73\u6ed1\u6ee4\u6ce2\u5668\u6765\u6291\u5236\u4f2a\u5f71\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u7ed3\u5408\u4e86\u6df1\u5ea6\u68af\u5ea6\u5256\u9762\u5148\u9a8c (DGPP) \u635f\u5931\u548c\u52a8\u6001\u6df1\u5ea6\u63a9\u7801\u6765\u9510\u5316\u8fb9\u7f18\uff0c\u5e76\u7ed3\u5408\u4e86\u5206\u6570\u84b8\u998f\u91c7\u6837 (SDS) \u635f\u5931\u7684\u4e8c\u7ef4\u6269\u6563\u6765\u589e\u5f3a\u65b0\u89c6\u56fe\u5408\u6210\u4e2d\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u5728 MipNeRF-360 \u548c SeaThru-NeRF \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cSVS-GS \u663e\u7740\u6539\u5584\u4e86\u7a00\u758f\u89c6\u70b9\u4e0b\u7684\u4e09\u7ef4\u91cd\u5efa\uff0c\u4e3a\u673a\u5668\u4eba\u6280\u672f\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u4e2d\u7684\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5065\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2409.02838": "|**2024-09-04**|[iConFormer: Dynamic Parameter-Efficient Tuning with Input-Conditioned Adaptation](http://arxiv.org/abs/2409.02838)|null|\u57fa\u4e8e\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u5b8c\u6574\u5fae\u8c03\uff08FFT\uff09\u548c\u4efb\u52a1\u7279\u5b9a\u89e3\u7801\u5668\u7684\u8fc1\u79fb\u5b66\u4e60\u968f\u7740\u6df1\u5ea6\u6a21\u578b\u7684\u6307\u6570\u7ea7\u589e\u957f\u800c\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\u3002\u4f7f\u7528\u7531\u5c0f\u578b\u53ef\u5b66\u4e60\u5c42\u7ec4\u6210\u7684\u9002\u914d\u5668\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u5df2\u6210\u4e3a FFT \u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u8bad\u7ec3\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u53ef\u6bd4\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u9002\u914d\u5668\u5bf9\u8f93\u5165\u5b9e\u4f8b\u7684\u4e0d\u7075\u6d3b\u9650\u5236\u4e86\u5176\u5728\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u4fe1\u606f\u7684\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 PEFT \u65b9\u6cd5\uff0c\u5373\u8f93\u5165\u6761\u4ef6\u5316\u7684 Transformer\uff0c\u79f0\u4e3a iConFormer\uff0c\u5b83\u5229\u7528\u4e86\u4ee5\u8f93\u5165\u5b9e\u4f8b\u4e3a\u6761\u4ef6\u7684\u52a8\u6001\u9002\u914d\u5668\u3002\u4e3a\u4e86\u786e\u4fdd\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5bf9\u8f93\u5165\u5b9e\u4f8b\u7684\u7075\u6d3b\u5b66\u4e60\u80fd\u529b\uff0c\u6211\u4eec\u5728\u52a8\u6001\u9002\u914d\u5668\u4e2d\u5f15\u5165\u4e86\u8f93\u5165\u6761\u4ef6\u5316\u7f51\u7edc\uff08iCoN\uff09\uff0c\u4ece\u800c\u5b9e\u73b0\u5b9e\u4f8b\u7ea7\u7279\u5f81\u8f6c\u6362\u3002\u5177\u4f53\u6765\u8bf4\uff0ciCoN \u4e3a\u6bcf\u4e2a\u7279\u5f81\u751f\u6210\u901a\u9053\u7ea7\u7684\u5377\u79ef\u6838\uff0c\u5e76\u4f7f\u7528\u81ea\u9002\u5e94\u5377\u79ef\u8fc7\u7a0b\u5bf9\u5176\u8fdb\u884c\u8f6c\u6362\uff0c\u4ee5\u6709\u6548\u6355\u83b7\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u4efb\u52a1\u7279\u5b9a\u548c\u7ec6\u7c92\u5ea6\u7ec6\u8282\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u4ec5\u8c03\u6574 Transformer \u4e3b\u5e72\u53c2\u6570\u7684 1.6% \u5230 2.8%\uff0ciConFormer \u5728\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u548c\u8bed\u4e49\u5206\u5272\u65b9\u9762\u5b9e\u73b0\u4e86\u4e0e FFT \u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u5b9e\u4f8b\u5206\u5272\u65b9\u9762\u4f18\u4e8e FFT\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6240\u6709\u4e0a\u8ff0\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u8fd1\u7684 PEFT \u65b9\u6cd5\u3002|\n", "2409.02546": "|**2024-09-04**|[Real-Time Dynamic Scale-Aware Fusion Detection Network: Take Road Damage Detection as an example](http://arxiv.org/abs/2409.02546)|null|\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u9053\u8def\u635f\u574f\u68c0\u6d4b (RDD) \u5bf9\u57ce\u5e02\u7684\u65e5\u5e38\u7ef4\u62a4\u548c\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u663e\u8457\u964d\u4f4e\u52b3\u52a8\u529b\u6210\u672c\u65b9\u9762\u3002\u7136\u800c\uff0c\u5f53\u524d\u57fa\u4e8e\u65e0\u4eba\u673a\u7684 RDD \u7814\u7a76\u4ecd\u9762\u4e34\u8bb8\u591a\u6311\u6218\u3002\u4f8b\u5982\uff0c\u5f62\u72b6\u548c\u65b9\u5411\u4e0d\u89c4\u5219\u7684\u635f\u574f\u3001\u80cc\u666f\u5bf9\u635f\u574f\u7684\u906e\u6321\u4ee5\u53ca\u96be\u4ee5\u533a\u5206\u635f\u574f\u548c\u80cc\u666f\uff0c\u8fd9\u4e9b\u56e0\u7d20\u90fd\u663e\u8457\u5f71\u54cd\u4e86\u65e0\u4eba\u673a\u5728\u65e5\u5e38\u5de1\u68c0\u4e2d\u68c0\u6d4b\u9053\u8def\u635f\u574f\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u63d0\u9ad8\u65e0\u4eba\u673a\u5b9e\u65f6\u9053\u8def\u635f\u574f\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u6211\u4eec\u8bbe\u8ba1\u5e76\u63d0\u51fa\u4e86\u4e09\u4e2a\u76f8\u5e94\u7684\u6a21\u5757\uff1a\u4e00\u4e2a\u80fd\u591f\u7075\u6d3b\u9002\u5e94\u5f62\u72b6\u548c\u80cc\u666f\u7684\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff1b\u4e00\u4e2a\u878d\u5408\u591a\u5c3a\u5ea6\u611f\u77e5\u5e76\u9002\u5e94\u5f62\u72b6\u548c\u80cc\u666f\u7684\u6a21\u5757\uff1b\u4e00\u4e2a\u9ad8\u6548\u7684\u4e0b\u91c7\u6837\u6a21\u5757\u3002 \u57fa\u4e8e\u8fd9\u4e9b\u6a21\u5757\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5177\u6709\u81ea\u52a8\u53bb\u9664\u80cc\u666f\u5e72\u6270\u80fd\u529b\u7684\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u9053\u8def\u635f\u574f\u68c0\u6d4b\u6a21\u578b\uff0c\u79f0\u4e3a\u52a8\u6001\u5c3a\u5ea6\u611f\u77e5\u878d\u5408\u68c0\u6d4b\u6a21\u578b (RT-DSAFDet)\u3002\u5728 UAV-PDD2023 \u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b RT-DSAFDet \u7684 mAP50 \u8fbe\u5230\u4e86 54.2%\uff0c\u6bd4\u6700\u65b0\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u6a21\u578b YOLOv10 \u7684\u9ad8\u6548\u53d8\u4f53 YOLOv10-m \u9ad8 11.1%\uff0c\u800c\u53c2\u6570\u91cf\u51cf\u5c11\u5230 1.8M\uff0cFLOPs \u51cf\u5c11\u5230 4.6G\uff0c\u5206\u522b\u964d\u4f4e\u4e86 88% \u548c 93%\u3002\u6b64\u5916\uff0c\u5728\u5927\u578b\u901a\u7528\u76ee\u6807\u68c0\u6d4b\u516c\u5f00\u6570\u636e\u96c6 MS COCO2017 \u4e0a\u4e5f\u5c55\u73b0\u4e86\u6211\u4eec\u6a21\u578b\u7684\u4f18\u8d8a\u6027\uff0c\u5176 mAP50-95 \u4e0e YOLOv9-t \u76f8\u540c\uff0c\u4f46 mAP50 \u9ad8\u51fa 0.5%\uff0c\u53c2\u6570\u91cf\u51cf\u5c11 10%\uff0cFLOPs \u51cf\u5c11 40%\u3002|\n", "2409.02486": "|**2024-09-04**|[Boosting Generalizability towards Zero-Shot Cross-Dataset Single-Image Indoor Depth by Meta-Initialization](http://arxiv.org/abs/2409.02486)|null|\u5ba4\u5185\u673a\u5668\u4eba\u7684\u5bfc\u822a\u6216\u969c\u788d\u7269\u68c0\u6d4b\u7b49\u4efb\u52a1\u4f9d\u8d56\u4e8e\u6df1\u5ea6\u4fe1\u606f\uff0c\u800c\u5355\u56fe\u50cf\u6df1\u5ea6\u4f30\u8ba1\u88ab\u5e7f\u6cdb\u7528\u4e8e\u8f85\u52a9\u611f\u77e5\u3002\u5927\u591a\u6570\u5ba4\u5185\u5355\u56fe\u50cf\u6df1\u5ea6\u9884\u6d4b\u8f83\u5c11\u5173\u6ce8\u6a21\u578b\u5bf9\u672a\u89c1\u6570\u636e\u96c6\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u66f4\u5173\u6ce8\u7cfb\u7edf\u90e8\u7f72\u7684\u91ce\u5916\u9c81\u68d2\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u5229\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u5143\u5b66\u4e60\u5728\u96f6\u6837\u672c\u8de8\u6570\u636e\u96c6\u63a8\u7406\u4e2d\u83b7\u5f97\u66f4\u9ad8\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4e0e\u7814\u7a76\u6700\u591a\u7684\u3001\u4e0e\u663e\u5f0f\u7c7b\u522b\u6807\u7b7e\u76f8\u5173\u7684\u56fe\u50cf\u5206\u7c7b\u5143\u5b66\u4e60\u4e0d\u540c\uff0c\u5bf9\u4e8e\u4e0e\u7269\u4f53\u6392\u5217\u548c\u573a\u666f\u6784\u6210\u65b9\u9762\u9ad8\u5ea6\u53d8\u5316\u7684\u5ba4\u5185\u73af\u5883\u76f8\u5173\u7684\u8fde\u7eed\u6df1\u5ea6\u503c\uff0c\u4e0d\u5b58\u5728\u660e\u786e\u7684\u4efb\u52a1\u8fb9\u754c\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u7ec6\u7c92\u5ea6\u4efb\u52a1\uff0c\u5728\u6211\u4eec\u7684\u5143\u5b66\u4e60\u516c\u5f0f\u4e2d\u5c06\u6bcf\u4e2aRGB-D\u5c0f\u6279\u91cf\u89c6\u4e3a\u4e00\u4e2a\u4efb\u52a1\u3002\u6211\u4eec\u9996\u5148\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6709\u9650\u6570\u636e\u4e0a\u8bf1\u5bfc\u51fa\u66f4\u597d\u7684\u5148\u9a8c\uff08RMSE \u6700\u9ad8\u964d\u4f4e 27.8%\uff09\u3002\u7136\u540e\uff0c\u5728\u5143\u5b66\u4e60\u521d\u59cb\u5316\u4e0a\u8fdb\u884c\u5fae\u8c03\u59cb\u7ec8\u4f18\u4e8e\u6ca1\u6709\u5143\u65b9\u6cd5\u7684\u57fa\u7ebf\u3002\u4e3a\u4e86\u5b9e\u73b0\u6cdb\u5316\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u96f6\u6837\u672c\u8de8\u6570\u636e\u96c6\u534f\u8bae\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7531\u6211\u4eec\u7684\u5143\u521d\u59cb\u5316\u8bf1\u5bfc\u7684\u66f4\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u4f5c\u4e3a\u8bb8\u591a\u73b0\u6709\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u7684\u7b80\u5355\u800c\u6709\u7528\u7684\u63d2\u4ef6\u3002\u6df1\u5ea6\u548c\u5143\u5b66\u4e60\u4ea4\u53c9\u9886\u57df\u7684\u5de5\u4f5c\u6709\u53ef\u80fd\u63a8\u52a8\u8fd9\u4e24\u9879\u7814\u7a76\u66f4\u63a5\u8fd1\u5b9e\u9645\u7684\u673a\u5668\u4eba\u548c\u673a\u5668\u611f\u77e5\u5e94\u7528\u3002|\n", "2409.02329": "|**2024-09-03**|[Site Selection for the Second Flyeye Telescope: A Simulation Study for Optimizing Near-Earth Object Discovery](http://arxiv.org/abs/2409.02329)|null|\u6b27\u6d32\u822a\u5929\u5c40 (ESA) \u6b63\u5728\u5f00\u53d1\u4e00\u4e2a\u540d\u4e3a Flyeye \u7684\u5e7f\u57df\u5de1\u5929\u671b\u8fdc\u955c\u7f51\u7edc\uff0c\u4ee5\u6539\u8fdb\u8fd1\u5730\u5929\u4f53 (NEO) \u7684\u53d1\u73b0\u3002\u8be5\u7f51\u7edc\u4e2d\u7684\u7b2c\u4e00\u4e2a\u671b\u8fdc\u955c\u5c06\u4f4d\u4e8e\u5317\u534a\u7403\u7684\u7a46\u6cd5\u62c9\u5c71\uff08\u610f\u5927\u5229\uff09\uff0c\u800c\u7b2c\u4e8c\u4e2a\u5177\u6709\u589e\u5f3a\u63a2\u6d4b\u80fd\u529b\u7684 Flyeye \u671b\u8fdc\u955c\u521a\u521a\u5f00\u59cb\u5173\u952e\u8bbe\u8ba1\u9636\u6bb5\u3002\u901a\u8fc7\u5bf9\u649e\u51fb\u8f68\u8ff9\u4e0a\u7684\u8fd1\u5730\u5929\u4f53\u8fdb\u884c\u6a21\u62df\uff0c\u7814\u7a76\u4e86\u7b2c\u4e8c\u4e2a Flyeye \u671b\u8fdc\u955c\u7684\u6f5c\u5728\u4f4d\u7f6e\u3002\u5bf9\u5927\u7ea6 3000 \u4e2a\u649e\u51fb\u5c0f\u884c\u661f\uff08\u7edd\u5bf9\u661f\u7b49\u4e3a H=25 \u548c H=28\uff09\u8fdb\u884c\u4e86\u4f20\u64ad\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e3b\u8981\u73b0\u6709\u5de1\u5929\u9879\u76ee\uff08Catalina\u3001Pan-STARRS\u3001ATLAS\uff09\u3001\u5373\u5c06\u6295\u5165\u4f7f\u7528\u7684\u8587\u62c9\u00b7\u9c81\u5bbe\u5929\u6587\u53f0 (LSST) \u4ee5\u53ca Flyeye \u53ef\u80fd\u9009\u5740\u7684\u53ef\u63a2\u6d4b\u6027\u3002 \u8003\u8651\u4e86\u667a\u5229\u3001\u5357\u975e\u548c\u5317\u534a\u7403\u7684\u7b2c\u4e8c\u4e2a\u8bbe\u65bd\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u5929\u6587\u53f0\uff0c\u5728\u6a21\u62df\u4e2d\u90fd\u8003\u8651\u4e86\u5b83\u4eec\u8fc7\u53bb\u6216\u8ba1\u5212\u7684\u6307\u5411\u7b56\u7565\u3002\u5728 LSST \u90e8\u7f72\u4e4b\u524d\uff0c\u5357\u534a\u7403\u7684\u4e00\u4e2a Flyeye \u7684\u6027\u80fd\u4e0e\u5317\u534a\u7403\u7684\u4e00\u4e2a\u671b\u8fdc\u955c\u76f8\u4f3c\u3002\u7ed3\u5408\u8d77\u6765\uff0c\u5728\u5317\u65b9\u548c\u5357\u65b9\u5404\u653e\u7f6e\u4e00\u53f0\u671b\u8fdc\u955c\u53ef\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u63a2\u6d4b\u7387\u548c\u63a2\u6d4b\u5230\u7684\u72ec\u7279\u7269\u4f53\u7684\u6570\u91cf\u3002LSST \u4e4b\u540e\uff0c\u5357\u90e8\u548c\u5317\u90e8\u7684 Flyeye \u671b\u8fdc\u955c\u4ecd\u7136\u662f\u4e92\u8865\u7684\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6a21\u62df\u8868\u660e\uff0c\u65e0\u8bba\u662f\u5728 LSST \u4e4b\u524d\u8fd8\u662f\u4e4b\u540e\uff0c\u4f4d\u4e8e\u5357\u90e8\u7684\u7b2c\u4e8c\u4e2a Flyeye \u90fd\u53ef\u4ee5\u8865\u5145\u4f4d\u4e8e\u5317\u90e8\u7684 Flyeye \u671b\u8fdc\u955c\u3002\u4f4d\u4e8e\u62c9\u897f\u62c9\u7684 Flyeye \u5c06\u5229\u7528\u5176\u4f18\u8d8a\u7684\u5927\u6c14\u6761\u4ef6\uff0c\u540c\u65f6\u5e73\u8861\u5357\u5317\u534a\u7403\u7684\u8d44\u4ea7\u3002|\n", "2409.02281": "|**2024-09-03**|[K-Origins: Better Colour Quantification for Neural Networks](http://arxiv.org/abs/2409.02281)|null|K-Origins\u662f\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u5c42\uff0c\u65e8\u5728\u5728\u5b66\u4e60\u989c\u8272\u6216\u5f3a\u5ea6\u6709\u5229\u65f6\u63d0\u9ad8\u57fa\u4e8e\u56fe\u50cf\u7684\u7f51\u7edc\u6027\u80fd\u3002 \u8d85\u8fc7 250 \u4e2a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5377\u79ef\u7f51\u7edc\u5728 16 \u4f4d\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0cK-Origins \u63d0\u9ad8\u4e86\u8bed\u4e49\u5206\u5272\u7cbe\u5ea6\uff1a\u4f4e\u4fe1\u566a\u6bd4\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u4ee5\u53ca\u5206\u5272\u5f62\u72b6\u76f8\u540c\u4f46\u989c\u8272\u4e0d\u540c\u7684\u591a\u4e2a\u76ee\u6807\u3002 \u5bf9\u4e8e\u6bcf\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570$w_k$\uff0cK-Origins \u901a\u8fc7\u516c\u5f0f $\\textbf{Y}_k = \\textbf{X}-\\textbf{J}\\cdot w_k$ \u4ece\u8f93\u5165\u7279\u5f81 $\\textbf{X}$ \u751f\u6210\u8f93\u51fa\u7279\u5f81\uff0c\u5176\u4e2d $\\textbf{J}$ \u662f\u4e00\u4e2a\u5168 1 \u77e9\u9635\u3002 \u6b64\u5916\uff0c\u8fd8\u8bad\u7ec3\u4e86\u5177\u6709\u4e0d\u540c\u611f\u53d7\u91ce\u7684\u7f51\u7edc\uff0c\u4ee5\u6839\u636e\u76ee\u6807\u7c7b\u522b\u7684\u7ef4\u5ea6\u786e\u5b9a\u6700\u4f73\u7f51\u7edc\u6df1\u5ea6\uff0c\u8fd9\u8868\u660e\u611f\u53d7\u91ce\u957f\u5ea6\u5e94\u8d85\u8fc7\u76ee\u6807\u5927\u5c0f\u3002 \u901a\u8fc7\u786e\u4fdd\u8db3\u591f\u7684\u611f\u53d7\u91ce\u957f\u5ea6\u5e76\u7ed3\u5408 K-Origins\uff0c\u6211\u4eec\u53ef\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u8bed\u4e49\u7f51\u7edc\u6027\u80fd\u3002|\n", "2409.02278": "|**2024-09-03**|[Evaluation and Comparison of Visual Language Models for Transportation Engineering Problems](http://arxiv.org/abs/2409.02278)|null|\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\u5c55\u73b0\u51fa\u5176\u5728\u56fe\u50cf\u7406\u89e3\u76f8\u5173\u5e94\u7528\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u4ea4\u901a\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4f8b\u5982\u56fe\u50cf\u5206\u7c7b\u548c\u76ee\u6807\u68c0\u6d4b\u3002\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u5305\u62ec\u62e5\u5835\u68c0\u6d4b\u548c\u88c2\u7f1d\u8bc6\u522b\uff0c\u800c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u5219\u7528\u4e8e\u8bc6\u522b\u672a\u4f69\u6234\u5934\u76d4\u7684\u884c\u4e3a\u3002\u6211\u4eec\u5e94\u7528\u4e86\u5f00\u6e90\u6a21\u578b\uff08\u5982CLIP\u3001BLIP\u3001OWL-ViT\u3001Llava-Next\uff09\u548c\u95ed\u6e90\u6a21\u578bGPT-4o\uff0c\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4ee5\u5229\u7528\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u6765\u5b8c\u6210\u57fa\u4e8e\u89c6\u89c9\u7684\u4ea4\u901a\u4efb\u52a1\u3002\u8fd9\u4e9b\u4efb\u52a1\u901a\u8fc7\u5bf9VLM\u6a21\u578b\u5e94\u7528\u96f6\u6837\u672c\u63d0\u793a\u6765\u5b8c\u6210\uff0c\u56e0\u4e3a\u96f6\u6837\u672c\u63d0\u793a\u53ef\u4ee5\u5728\u4e0d\u5bf9\u4efb\u52a1\u8fdb\u884c\u4efb\u4f55\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u4efb\u52a1\u3002\u8fd9\u6d88\u9664\u4e86\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u6807\u6ce8\u6570\u636e\u96c6\u6216\u5fae\u8c03\u7684\u9700\u6c42\u3002\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4e0e\u57fa\u51c6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u4f46\u5728\u76ee\u6807\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4ecd\u6709\u6539\u8fdb\u7684\u7a7a\u95f4\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u5bf9\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u7a81\u51fa\u4e86\u8fd9\u4e9b\u6a21\u578b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u672a\u6765\u6539\u8fdb\u548c\u5e7f\u6cdb\u5b9e\u65bd\u7684\u57fa\u51c6\u3002|\n", "2409.02035": "|**2024-09-03**|[A Modern Take on Visual Relationship Reasoning for Grasp Planning](http://arxiv.org/abs/2409.02035)|null|\u4e0e\u73b0\u5b9e\u4e16\u754c\u6742\u4e71\u573a\u666f\u4ea4\u4e92\u5bf9\u673a\u5668\u4eba\u4ee3\u7406\u63d0\u51fa\u4e86\u82e5\u5e72\u6311\u6218\uff0c\u8fd9\u4e9b\u4ee3\u7406\u9700\u8981\u7406\u89e3\u89c2\u5bdf\u5230\u7684\u7269\u4f53\u4e4b\u95f4\u590d\u6742\u7684\u7684\u7a7a\u95f4\u4f9d\u8d56\u6027\uff0c\u4ee5\u786e\u5b9a\u6700\u4f73\u62fe\u53d6\u987a\u5e8f\u6216\u6709\u6548\u7684\u7269\u4f53\u68c0\u7d22\u7b56\u7565\u3002 \u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u7ba1\u7406\u7b80\u5316\u7684\u573a\u666f\uff0c\u5e76\u4fa7\u91cd\u4e8e\u5728\u521d\u59cb\u7269\u4f53\u68c0\u6d4b\u9636\u6bb5\u4e4b\u540e\u9884\u6d4b\u6210\u5bf9\u7269\u4f53\u5173\u7cfb\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u5168\u5c40\u4e0a\u4e0b\u6587\u6216\u96be\u4ee5\u5904\u7406\u5197\u4f59\u548c\u7f3a\u5931\u7684\u7269\u4f53\u5173\u7cfb\u3002 \u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6293\u53d6\u89c4\u5212\u7684\u89c6\u89c9\u5173\u7cfb\u63a8\u7406\u7684\u73b0\u4ee3\u65b9\u6cd5\u3002 \u6211\u4eec\u4ecb\u7ecd\u4e86 D3GD\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5176\u4e2d\u5305\u62ec\u5305\u542b\u6765\u81ea 97 \u4e2a\u4e0d\u540c\u7c7b\u522b\u7684\u591a\u8fbe 35 \u4e2a\u7269\u4f53\u7684\u5206\u62e3\u573a\u666f\u3002 \u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86 D3G\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u7aef\u5230\u7aef transformer \u7684\u4f9d\u8d56\u56fe\u751f\u6210\u6a21\u578b\uff0c\u5b83\u53ef\u4ee5\u540c\u65f6\u68c0\u6d4b\u7269\u4f53\u5e76\u751f\u6210\u8868\u793a\u5176\u7a7a\u95f4\u5173\u7cfb\u7684\u90bb\u63a5\u77e9\u9635\u3002 \u8ba4\u8bc6\u5230\u6807\u51c6\u6307\u6807\u7684\u5c40\u9650\u6027\uff0c\u6211\u4eec\u9996\u6b21\u91c7\u7528\u5173\u7cfb\u5e73\u5747\u7cbe\u5ea6\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u57fa\u51c6\u6d4b\u8bd5\u3002 \u83b7\u5f97\u7684\u7ed3\u679c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u8fd9\u9879\u4efb\u52a1\u7684\u6700\u65b0\u6280\u672f\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002 \u6211\u4eec\u5728 https://paolotron.github.io/d3g.github.io \u4e0a\u516c\u5f00\u53d1\u5e03\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002|\n", "2409.01988": "|**2024-09-03**|[Compressed learning based onboard semantic compression for remote sensing platforms](http://arxiv.org/abs/2409.01988)|null|\u5730\u7403\u89c2\u6d4b (EO) \u5728\u521b\u5efa\u548c\u7ef4\u6301\u4e00\u4e2a\u5177\u6709\u5f39\u6027\u548c\u7e41\u8363\u7684\u793e\u4f1a\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u8fd9\u5bf9\u6240\u6709\u751f\u547d\u548c\u5730\u7403\u672c\u8eab\u90fd\u5177\u6709\u6df1\u8fdc\u7684\u5f71\u54cd\u3002\u536b\u661f\u3001\u822a\u7a7a\u5e73\u53f0\u4ee5\u53ca\u6700\u8fd1\u7684\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u9a7e\u9a76\u98de\u884c\u5668\u7b49\u9065\u611f\u5e73\u53f0\u90fd\u7528\u4e8e EO\u3002\u5b83\u4eec\u6536\u96c6\u5927\u91cf\u6570\u636e\uff0c\u9700\u8981\u5c06\u5176\u4e0b\u4f20\u5230\u5730\u7403\u8fdb\u884c\u8fdb\u4e00\u6b65\u5904\u7406\u548c\u5206\u6790\u3002\u8fd9\u79cd\u9ad8\u541e\u5410\u91cf\u91c7\u96c6\u7684\u74f6\u9888\u662f\u4e0b\u884c\u94fe\u8def\u5e26\u5bbd\u3002\u9700\u8981\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u56fe\u50cf\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u6765\u5e94\u5bf9\u8fd9\u79cd\u6d77\u91cf\u6570\u636e\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u901a\u8fc7\u538b\u7f29\u5b66\u4e60\u6846\u67b6\u7814\u7a76\u4e86\u8bed\u4e49\u538b\u7f29\uff0c\u8be5\u6846\u67b6\u4ec5\u5229\u7528\u5feb\u901f\u548c\u7a00\u758f\u7684\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\u6765\u7f16\u7801\u6570\u636e\u3002\u76f8\u673a\u566a\u58f0\u548c\u901a\u4fe1\u4fe1\u9053\u662f\u9020\u6210\u5931\u771f\u7684\u4e3b\u8981\u6765\u6e90\u3002\u7136\u540e\uff0c\u5b8c\u6574\u7684\u8bed\u4e49\u901a\u4fe1\u7ba1\u9053\u7531\u4e00\u4e2a\u5b66\u4e60\u5230\u7684\u4f4e\u590d\u6742\u5ea6\u538b\u7f29\u77e9\u9635\u7ec4\u6210\uff0c\u8be5\u77e9\u9635\u4f5c\u7528\u4e8e\u566a\u58f0\u76f8\u673a\u8f93\u51fa\uff0c\u4ee5\u5728\u673a\u8f7d\u751f\u6210\u4e00\u4e2a\u89c2\u6d4b\u5411\u91cf\uff0c\u8be5\u5411\u91cf\u901a\u8fc7\u901a\u4fe1\u4fe1\u9053\u4e0b\u884c\u94fe\u8def\u4f20\u8f93\uff0c\u901a\u8fc7\u5c55\u5f00\u7f51\u7edc\u5904\u7406\uff0c\u7136\u540e\u9988\u9001\u5230\u6267\u884c\u5fc5\u8981\u4e0b\u6e38\u4efb\u52a1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1b\u7814\u7a76\u4e86\u56fe\u50cf\u5206\u7c7b\u3002\u901a\u8fc7\u4f7f\u7528\u5c0f\u6ce2\u7a00\u758f\u5148\u9a8c\u5c55\u5f00 NA-ALISTA \u7684\u5c42\u6765\u8865\u507f\u5931\u771f\u3002\u56e0\u6b64\uff0c\u89e3\u7801\u662f\u4e00\u79cd\u6839\u636e\u76f8\u673a/\u73af\u5883\u4fe1\u606f\u548c\u4e0b\u6e38\u4efb\u52a1\u8bbe\u8ba1\u7684\u5373\u63d2\u5373\u7528\u65b9\u6cd5\u3002\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u901a\u8fc7\u7aef\u5230\u7aef\u65b9\u5f0f\u7684\u635f\u5931\u51fd\u6570\u4e0e\u538b\u7f29\u77e9\u9635\u548c\u5c55\u5f00\u7f51\u7edc\u8054\u5408\u5fae\u8c03\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4f4e\u538b\u7f29\u6bd4\u7684\u566a\u58f0\u73af\u5883\u4e2d\uff0c\u6dfb\u52a0\u6062\u590d\u635f\u5931\u4ee5\u53ca\u4efb\u52a1\u76f8\u5173\u635f\u5931\u53ef\u4ee5\u63d0\u9ad8\u4e0b\u6e38\u6027\u80fd\u3002|\n", "2409.01872": "|**2024-09-03**|[Latent Distillation for Continual Object Detection at the Edge](http://arxiv.org/abs/2409.01872)|**[link](https://github.com/pastifra/Continual_Nanodet)**|\u867d\u7136\u5728\u76ee\u6807\u68c0\u6d4b\u6587\u732e\u4e2d\u5b58\u5728\u8bb8\u591a\u6027\u80fd\u5353\u8d8a\u7684\u65b9\u6cd5\uff0c\u4f46\u89e3\u51b3\u6570\u636e\u5206\u5e03\u504f\u79fb\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u4e3a\u8fd9\u4e2a\u95ee\u9898\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u9002\u5e94\u65b0\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5148\u524d\u6570\u636e\u7684\u6027\u80fd\u3002\u8fd9\u5bf9\u4e8e\u8fb9\u7f18\u8bbe\u5907\u5c24\u5176\u91cd\u8981\uff0c\u8fd9\u4e9b\u8bbe\u5907\u5728\u6c7d\u8f66\u548c\u673a\u5668\u4eba\u7b49\u52a8\u6001\u73af\u5883\u4e2d\u5f88\u5e38\u89c1\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u76ee\u6807\u68c0\u6d4b\u6301\u7eed\u5b66\u4e60\uff08CLOD\uff09\u573a\u666f\u4e2d\u8fb9\u7f18\u8bbe\u5907\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u9650\u5236\u3002\u5177\u4f53\u6765\u8bf4\uff0c\uff08i\uff09\u6211\u4eec\u7814\u7a76\u4e86\u4e00\u79cd\u5f00\u6e90\u3001\u8f7b\u91cf\u7ea7\u548c\u5feb\u901f\u7684\u68c0\u6d4b\u5668 NanoDet \u5bf9\u8fb9\u7f18\u8bbe\u5907\u4e0a CLOD \u7684\u9002\u7528\u6027\uff0c\u6539\u8fdb\u4e86\u6587\u732e\u4e2d\u4f7f\u7528\u7684\u8f83\u5927\u67b6\u6784\u3002\u6b64\u5916\uff0c\uff08ii\uff09\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6f5c\u5728\u84b8\u998f\uff08LD\uff09\u7684\u65b0\u578b CL \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u663e\u7740\u5f71\u54cd\u68c0\u6d4b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u4e86\u6700\u5148\u8fdb\u7684 CL \u65b9\u6cd5\u6240\u9700\u7684\u8fd0\u7b97\u6b21\u6570\u548c\u5185\u5b58\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u8457\u540d\u7684 VOC \u548c COCO \u57fa\u51c6\u6d4b\u8bd5\u96c6\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u4e0e\u5176\u4ed6\u84b8\u998f\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6bcf\u6b21\u6a21\u578b\u66f4\u65b0\u53ef\u5c06\u84b8\u998f\u53c2\u6570\u5f00\u9500\u51cf\u5c11 74%\uff0c\u5c06\u6d6e\u70b9\u8fd0\u7b97\uff08FLOPs\uff09\u51cf\u5c11 56%\u3002|\n", "2409.01816": "|**2024-09-03**|[GeoBEV: Learning Geometric BEV Representation for Multi-view 3D Object Detection](http://arxiv.org/abs/2409.01816)|null|\u9e1f\u77b0\u56fe (BEV) \u8868\u793a\u5df2\u6210\u4e3a\u591a\u89c6\u56fe 3D \u5bf9\u8c61\u68c0\u6d4b\u7684\u4e3b\u6d41\u8303\u5f0f\uff0c\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u611f\u77e5\u80fd\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86 BEV \u8868\u793a\u7684\u51e0\u4f55\u8d28\u91cf\uff0c\u4f7f\u5176\u5904\u4e8e\u4f4e\u5206\u8fa8\u7387\u72b6\u6001\uff0c\u65e0\u6cd5\u6062\u590d\u573a\u666f\u771f\u5b9e\u7684\u51e0\u4f55\u4fe1\u606f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u5148\u524d\u65b9\u6cd5\u53d7\u9650\u4e8e\u4f4e BEV \u8868\u793a\u5206\u8fa8\u7387\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u5f84\u5411-\u7b1b\u5361\u5c14 BEV \u91c7\u6837 (RC-Sampling)\uff0c\u4ece\u800c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u5bc6\u96c6 BEV \u8868\u793a\uff0c\u800c\u65e0\u9700\u590d\u6742\u7684\u7b97\u5b50\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u76d2\u5185\u6807\u7b7e\u6765\u66ff\u4ee3\u4ece\u6fc0\u5149\u96f7\u8fbe\u70b9\u751f\u6210\u7684\u4f20\u7edf\u6df1\u5ea6\u6807\u7b7e\u3002\u6b64\u6807\u7b7e\u53cd\u6620\u4e86\u5bf9\u8c61\u7684\u5b9e\u9645\u51e0\u4f55\u7ed3\u6784\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5b83\u4eec\u7684\u8868\u9762\uff0c\u5c06\u73b0\u5b9e\u4e16\u754c\u7684\u51e0\u4f55\u4fe1\u606f\u6ce8\u5165 BEV \u8868\u793a\u4e2d\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u76d2\u5185\u6807\u7b7e\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u8d28\u5fc3\u611f\u77e5\u5185\u90e8\u635f\u5931 (CAI \u635f\u5931) \u6765\u6355\u6349\u5bf9\u8c61\u7684\u7ec6\u7c92\u5ea6\u5185\u90e8\u51e0\u4f55\u7ed3\u6784\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c06\u4e0a\u8ff0\u6a21\u5757\u96c6\u6210\u5230\u4e00\u4e2a\u540d\u4e3a GeoBEV \u7684\u65b0\u578b\u591a\u89c6\u56fe 3D \u5bf9\u8c61\u68c0\u6d4b\u6846\u67b6\u4e2d\u3002\u5728 nuScenes \u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGeoBEV \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7a81\u51fa\u4e86\u5176\u6709\u6548\u6027\u3002|\n", "2409.03530": "|**2024-09-05**|[Use of triplet loss for facial restoration in low-resolution images](http://arxiv.org/abs/2409.03530)|null|\u8fd1\u5e74\u6765\uff0c\u4eba\u8138\u8bc6\u522b (FR) \u6a21\u578b\u5df2\u6210\u4e3a\u5e94\u7528\u6700\u5e7f\u6cdb\u7684\u751f\u7269\u8bc6\u522b\u5de5\u5177\uff0c\u5728\u4f17\u591a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u6210\u679c\u3002\u7136\u800c\uff0c\u786c\u4ef6\u7684\u56fa\u6709\u6311\u6218\u6216\u62cd\u6444\u8ddd\u79bb often \u5bfc\u81f4\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u8fd9\u4f1a\u4e25\u91cd\u5f71\u54cd\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4eba\u4eec\u63d0\u51fa\u4e86\u51e0\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u751f\u6210\u9ad8\u5ea6\u903c\u771f\u7684\u4eba\u8138\u7684\u8d85\u5206\u8fa8\u7387 (SR) \u6a21\u578b\u3002\u5c3d\u7ba1\u505a\u51fa\u4e86\u8fd9\u4e9b\u52aa\u529b\uff0c\u4f46\u4eba\u8138\u8bc6\u522b\u7b97\u6cd5\u5e76\u672a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8d85\u5206\u8fa8\u7387\u6a21\u578b FTLGAN\uff0c\u5b83\u4fa7\u91cd\u4e8e\u751f\u6210\u4fdd\u7559\u4e2a\u4eba\u8eab\u4efd\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\uff0c\u4ece\u800c\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u7684\u6027\u80fd\u3002\u7ed3\u679c\u4ee4\u4eba\u4fe1\u670d\uff0c\u8868\u660e d' \u7684\u5e73\u5747\u503c\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u9ad8\u51fa 21%\uff0c\u5177\u4f53\u800c\u8a00\uff0c14x14 \u50cf\u7d20\u65f6 d' = 1.099\uff0cAUC = 0.78\uff0c28x28 \u50cf\u7d20\u65f6 d' = 2.112\uff0cAUC = 0.92\uff0c56x56 \u50cf\u7d20\u65f6 d' = 3.049\uff0cAUC = 0.98\u3002\u8fd9\u9879\u7814\u7a76\u7684\u8d21\u732e\u5728\u51e0\u4e2a\u5173\u952e\u9886\u57df\u610f\u4e49\u91cd\u5927\u3002\u9996\u5148\uff0c\u5728\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff08\u7279\u522b\u662f 14x14\u300128x28 \u548c 56x56 \u50cf\u7d20\u7684\u5206\u8fa8\u7387\uff09\u4e2d\uff0c\u4eba\u8138\u8bc6\u522b\u6027\u80fd\u53d6\u5f97\u4e86\u663e\u7740\u63d0\u9ad8\u3002\u5176\u6b21\uff0cFTLGAN \u6240\u5c55\u793a\u7684\u589e\u5f3a\u529f\u80fd\u5728\u6240\u6709\u5206\u8fa8\u7387\u4e0b\u90fd\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u54cd\u5e94\uff0c\u4e0e\u5176\u4ed6\u6bd4\u8f83\u6a21\u578b\u4e0d\u540c\uff0c\u5b83\u59cb\u7ec8\u5982\u4e00\u5730\u63d0\u4f9b\u51fa\u8272\u7684\u6027\u80fd\u3002\u7b2c\u4e09\uff0c\u4f7f\u7528\u4e09\u5143\u7ec4\u635f\u5931\u903b\u8f91\u5b9e\u65bd\u4e86\u4e00\u79cd\u521b\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u4ec5\u4f7f\u7528\u771f\u5b9e\u56fe\u50cf\u8bad\u7ec3\u8d85\u5206\u8fa8\u7387\u6a21\u578b\uff0c\u8fd9\u4e0e\u5f53\u524d\u6a21\u578b\u5f62\u6210\u5bf9\u6bd4\uff0c\u5e76\u6269\u5c55\u4e86\u6f5c\u5728\u7684\u73b0\u5b9e\u5e94\u7528\u3002\u6700\u540e\uff0c\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5728\u6a21\u578b\u8bad\u7ec3\u671f\u95f4\u5c06\u4eba\u8138\u8bc6\u522b\u8d28\u91cf\u4f5c\u4e3a\u635f\u5931\u7eb3\u5165\u5176\u4e2d\uff0c\u4e13\u95e8\u89e3\u51b3\u4e86\u63d0\u9ad8\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5206\u7c7b\u6027\u80fd\u7684\u6311\u6218\u3002|\n", "2409.03521": "|**2024-09-05**|[Have Large Vision-Language Models Mastered Art History?](http://arxiv.org/abs/2409.03521)|null|\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u51fa\u73b0\u6700\u8fd1\u5728\u8de8\u591a\u4e2a\u9886\u57df\u7684\u56fe\u50cf\u5206\u7c7b\u65b9\u9762\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002\u7136\u800c\uff0cVLM \u5728\u827a\u672f\u54c1\u5206\u7c7b\u8fd9\u4e00\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u7ed8\u753b\u827a\u672f\u98ce\u683c\u5206\u7c7b\u2014\u2014\u4f20\u7edf\u4e0a\u7531\u827a\u672f\u53f2\u5b66\u5bb6\u638c\u63e1\u7684\u9886\u57df\u2014\u2014\u5c1a\u672a\u5f97\u5230\u63a2\u7d22\u3002\u4e0e\u81ea\u7136\u56fe\u50cf\u76f8\u6bd4\uff0c\u827a\u672f\u54c1\u7531\u4e8e\u5176\u56fa\u6709\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u7ed3\u6784\uff08\u4ee5\u591a\u53d8\u7684\u6784\u56fe\u548c\u98ce\u683c\u4e3a\u7279\u5f81\uff09\u800c\u6784\u6210\u4e86\u72ec\u7279\u7684\u6311\u6218\u3002\u827a\u672f\u53f2\u5b66\u5bb6\u957f\u671f\u4ee5\u6765\u4e00\u76f4\u5728\u7814\u7a76\u827a\u672f\u54c1\u7684\u72ec\u7279\u65b9\u9762\uff0c\u800c\u98ce\u683c\u9884\u6d4b\u662f\u5176\u5b66\u79d1\u7684\u4e00\u4e2a\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u3002\u672c\u6587\u7814\u7a76\u4e86\u96c6\u6210\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u7684\u5927\u578b VLM \u662f\u5426\u53ef\u4ee5\u6709\u6548\u5730\u9884\u6d4b\u7ed8\u753b\u7684\u827a\u672f\u53f2\u5c5e\u6027\u3002\u6211\u4eec\u5bf9\u56db\u79cd VLM\uff08\u5373 CLIP\u3001LLaVA\u3001OpenFlamingo \u548c GPT-4o\uff09\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u91cd\u70b9\u5173\u6ce8\u4f7f\u7528\u4e24\u4e2a\u516c\u5171\u827a\u672f\u54c1\u57fa\u51c6\u5bf9\u827a\u672f\u98ce\u683c\u3001\u4f5c\u8005\u548c\u65f6\u95f4\u6bb5\u8fdb\u884c\u96f6\u6837\u672c\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86 ArTest\uff0c\u8fd9\u662f\u4e00\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u827a\u672f\u54c1\u6d4b\u8bd5\u96c6\uff0c\u5176\u4e2d\u5305\u62ec\u827a\u672f\u53f2\u5b66\u5bb6\u7814\u7a76\u7684\u5173\u952e\u7ed8\u753b\u4f5c\u54c1\u3002|\n", "2409.03516": "|**2024-09-05**|[LMLT: Low-to-high Multi-Level Vision Transformer for Image Super-Resolution](http://arxiv.org/abs/2409.03516)|**[link](https://github.com/jwgdmkj/lmlt)**|\u8fd1\u5e74\u6765\uff0c\u57fa\u4e8e\u89c6\u89c9Transformer (ViT) \u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5b58\u5728\u590d\u6742\u6027\u9ad8\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u63a8\u7406\u65f6\u95f4\u548c\u5185\u5b58\u4f7f\u7528\u91cf\u5927\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u673a\u5236(WSA) \u7684ViT\u6a21\u578b\u5728\u5904\u7406\u7a97\u53e3\u533a\u57df\u5916\u7684\u4fe1\u606f\u65f6\u9762\u4e34\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4f4e\u5230\u9ad8\u591a\u7ea7Transformer (LMLT)\uff0c\u5b83\u5bf9\u6bcf\u4e2a\u5934\u91c7\u7528\u4e0d\u540c\u7279\u5f81\u5927\u5c0f\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002LMLT \u6cbf\u901a\u9053\u7ef4\u5ea6\u5212\u5206\u56fe\u50cf\u7279\u5f81\uff0c\u9010\u6e10\u51cf\u5c0f\u4f4e\u5c42\u5934\u7684\u7a7a\u95f4\u5927\u5c0f\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u5934\u5e94\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3002\u8fd9\u79cd\u65b9\u6cd5\u6709\u6548\u5730\u6355\u83b7\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u3002\u901a\u8fc7\u5c06\u4f4e\u5c42\u5934\u7684\u7ed3\u679c\u6574\u5408\u5230\u9ad8\u5c42\u5934\u4e2d\uff0cLMLT \u514b\u670d\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u7a97\u53e3\u8fb9\u754c\u95ee\u9898\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u4fdd\u6301\u751a\u81f3\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u57fa\u4e8e ViT \u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u7684\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\u548c GPU \u5185\u5b58\u4f7f\u7528\u91cf\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/jwgdmkj/LMLT \u83b7\u53d6\u3002|\n", "2409.03458": "|**2024-09-05**|[Non-Uniform Illumination Attack for Fooling Convolutional Neural Networks](http://arxiv.org/abs/2409.03458)|**[link](https://github.com/Akshayjain97/Non-Uniform_Illumination)**|\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u867d\u7136\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u653b\u51fb\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u4eba\u7c7b\u5bb9\u6613\u8bc6\u522b\u7684\u5fae\u5c0f\u56fe\u50cf\u6270\u52a8\u65f6\u3002\u8fd9\u79cd\u5f31\u70b9\u901a\u5e38\u88ab\u79f0\u4e3a\u201c\u653b\u51fb\u201d\uff0c\u7a81\u663e\u4e86CNN\u7684\u9c81\u68d2\u6027\u6709\u9650\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u589e\u5f3a\u5176\u62b5\u6297\u6b64\u7c7b\u64cd\u7eb5\u7684\u80fd\u529b\u3002\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u975e\u5747\u5300\u7167\u660e\uff08NUI\uff09\u653b\u51fb\u6280\u672f\uff0c\u8be5\u6280\u672f\u4f7f\u7528\u4e0d\u540c\u7684NUI\u63a9\u7801\u5bf9\u56fe\u50cf\u8fdb\u884c\u7ec6\u5fae alteration\u3002\u6211\u4eec\u5728\u5e7f\u6cdb\u63a5\u53d7\u7684\u6570\u636e\u96c6\uff08\u5305\u62ecCIFAR10\u3001TinyImageNet\u548cCalTech256\uff09\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u91cd\u70b9\u5173\u6ce812\u79cd\u4e0d\u540cNUI\u653b\u51fb\u6a21\u578b\u7684\u56fe\u50cf\u5206\u7c7b\u3002\u8bc4\u4f30\u4e86VGG\u3001ResNet\u3001MobilenetV3-small\u548cInceptionV3\u6a21\u578b\u5bf9NUI\u653b\u51fb\u7684\u62b5\u6297\u529b\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cCNN\u6a21\u578b\u5728\u906d\u53d7NUI\u653b\u51fb\u65f6\uff0c\u5206\u7c7b\u7cbe\u5ea6\u5927\u5e45\u4e0b\u964d\uff0c\u8868\u660e\u5b83\u4eec\u5728\u975e\u5747\u5300\u7167\u660e\u4e0b\u7684\u8106\u5f31\u6027\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u79cd\u60c5\u51b5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u5fa1\u7b56\u7565\uff0c\u5c06\u901a\u8fc7\u65b0\u7684NUI\u53d8\u6362\u751f\u6210\u7684NUI\u653b\u51fb\u56fe\u50cf\u5305\u542b\u5230\u8bad\u7ec3\u96c6\u4e2d\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5f53CNN\u6a21\u578b\u9762\u5bf9\u53d7NUI\u653b\u51fb\u5f71\u54cd\u7684\u6270\u52a8\u56fe\u50cf\u65f6\uff0c\u5176\u6027\u80fd\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002\u8be5\u7b56\u7565\u65e8\u5728\u589e\u5f3aCNN\u6a21\u578b\u5bf9NUI\u653b\u51fb\u7684\u62b5\u6297\u529b\u3002|\n", "2409.03377": "|**2024-09-05**|[Raw Speech Enhancement with Deep State Space Modeling](http://arxiv.org/abs/2409.03377)|**[link](https://github.com/Brainchip-Inc/aTENNuate)**|\u6211\u4eec\u63d0\u51fa\u4e86 aTENNuate\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u7684\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u81ea\u7f16\u7801\u5668\uff0c\u4e13\u4e3a\u9ad8\u6548\u7684\u5728\u7ebf\u539f\u59cb\u8bed\u97f3\u589e\u5f3a\u800c\u914d\u7f6e\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u3002\u8be5\u7f51\u7edc\u7684\u6027\u80fd\u4e3b\u8981\u5728\u539f\u59cb\u8bed\u97f3\u53bb\u566a\u65b9\u9762\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5728\u8d85\u5206\u8fa8\u7387\u548c\u53bb\u91cf\u5316\u7b49\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u989d\u5916\u8bc4\u4f30\u3002\u6211\u4eec\u5728 VoiceBank + DEMAND \u548c Microsoft DNS1 \u5408\u6210\u6d4b\u8bd5\u96c6\u4e0a\u5bf9 aTENNuate \u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u8be5\u7f51\u7edc\u5728 PESQ \u5206\u6570\u3001\u53c2\u6570\u6570\u91cf\u3001MAC \u548c\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8e\u4ee5\u524d\u7684\u5b9e\u65f6\u53bb\u566a\u6a21\u578b\u3002\u5373\u4f7f\u4f5c\u4e3a\u539f\u59cb\u6ce2\u5f62\u5904\u7406\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4e5f\u80fd\u4fdd\u6301\u5bf9\u5e72\u51c0\u4fe1\u53f7\u7684\u9ad8\u4fdd\u771f\u5ea6\uff0c\u5e76\u4e14\u53ef\u542c\u89c1\u7684\u4f2a\u5f71\u6781\u5c11\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u5c06\u566a\u58f0\u8f93\u5165\u538b\u7f29\u81f3 4000Hz \u548c 4 \u4f4d\uff0c\u8be5\u6a21\u578b\u4ecd\u80fd\u4fdd\u6301\u826f\u597d\u7684\u6027\u80fd\uff0c\u8fd9\u8868\u660e\u5b83\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u5177\u6709\u4e00\u822c\u7684\u8bed\u97f3\u589e\u5f3a\u80fd\u529b\u3002|\n", "2409.03368": "|**2024-09-05**|[Training-free Conversion of Pretrained ANNs to SNNs for Low-Power and High-Performance Applications](http://arxiv.org/abs/2409.03368)|null|\u8109\u51b2\u795e\u7ecf\u7f51\u7edc (SNN) \u7531\u4e8e\u5176\u63a8\u7406\u901f\u5ea6\u5feb\u3001\u529f\u8017\u4f4e\u7b49\u4f18\u52bf\uff0c\u5df2\u6210\u4e3a\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc (ANN) \u7684\u4e00\u79cd\u5f88\u6709\u524d\u9014\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u8bad\u7ec3\u7b97\u6cd5\u963b\u788d\u4e86\u5b83\u4eec\u7684\u5e7f\u6cdb\u5e94\u7528\u3002\u73b0\u6709\u7684 SNN \u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\u6bd4 ANN \u9700\u8981\u66f4\u591a\u7684\u5185\u5b58\u548c\u65f6\u95f4\u3002\u5373\u4f7f\u662f\u5e38\u7528\u7684 ANN-SNN \u8f6c\u6362\u65b9\u6cd5\u4e5f\u9700\u8981\u91cd\u65b0\u8bad\u7ec3 ANN \u4ee5\u63d0\u9ad8\u8f6c\u6362\u6548\u7387\uff0c\u4ece\u800c\u4ea7\u751f\u989d\u5916\u7684\u8ba1\u7b97\u6210\u672c\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u514d\u8bad\u7ec3 ANN-SNN \u8f6c\u6362\u6d41\u7a0b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u9884\u5148\u8bad\u7ec3\u597d\u7684 ANN \u6a21\u578b\u76f4\u63a5\u8f6c\u6362\u4e3a\u9ad8\u6027\u80fd SNN\uff0c\u65e0\u9700\u989d\u5916\u7684\u8bad\u7ec3\u3002\u8be5\u8f6c\u6362\u6d41\u7a0b\u5305\u62ec\u4e00\u4e2a\u57fa\u4e8e\u5c40\u90e8\u5b66\u4e60\u7684\u9608\u503c\u5e73\u8861\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u8ba1\u7b97\u6700\u4f73\u9608\u503c\u5e76\u901a\u8fc7\u901a\u9053\u7f29\u653e\u5bf9\u9608\u503c\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8c03\u6574\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u6846\u67b6\u5728\u4e09\u4e2a\u5178\u578b\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u53ef\u6269\u5c55\u6027\uff1a\u56fe\u50cf\u5206\u7c7b\u3001\u8bed\u4e49\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\u3002\u8fd9\u5c55\u793a\u4e86\u5176\u5bf9\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u7684\u9002\u7528\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u8f6c\u6362\u540e\u7684 SNN \u7684\u80fd\u8017\uff0c\u8bc1\u660e\u4e86\u5b83\u4eec\u4e0e\u4f20\u7edf ANN \u76f8\u6bd4\u5177\u6709\u4f18\u8d8a\u7684\u4f4e\u529f\u8017\u4f18\u52bf\u3002\u6211\u4eec\u7684\u514d\u8bad\u7ec3\u7b97\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7a81\u51fa\u4e86\u5176\u5b9e\u7528\u6027\u548c\u6548\u7387\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u5f00\u6e90\u9884\u8bad\u7ec3 ANN \u6a21\u578b\u548c\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u7b80\u5316\u4e86 SNN \u7684\u90e8\u7f72\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u4f4e\u529f\u8017\u7684\u63a8\u7406\uff0c\u5e76\u4e14\u6027\u80fd\u635f\u5931\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u3002|\n", "2409.03320": "|**2024-09-05**|[YOLO-PPA based Efficient Traffic Sign Detection for Cruise Control in Autonomous Driving](http://arxiv.org/abs/2409.03320)|null|\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u9ad8\u6548\u3001\u51c6\u786e\u5730\u68c0\u6d4b\u4ea4\u901a\u6807\u5fd7\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8ddd\u79bb\u8d8a\u8fdc\uff0c\u4ea4\u901a\u6807\u5fd7\u8d8a\u5c0f\u3002\u73b0\u6709\u7684\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u5f88\u96be\u68c0\u6d4b\u5230\u8fd9\u4e9b\u5c0f\u5c3a\u5bf8\u7684\u6807\u5fd7\u3002\u6b64\u5916\uff0c\u8f66\u8f7d\u5d4c\u5165\u5f0f\u8bbe\u5907\u7684\u6027\u80fd\u9650\u5236\u4e86\u68c0\u6d4b\u6a21\u578b\u7684\u89c4\u6a21\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e YOLO PPA \u7684\u4ea4\u901a\u6807\u5fd7\u68c0\u6d4b\u7b97\u6cd5\u3002\u5728 GTSDB \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u539f\u59cb YOLO \u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5c06\u63a8\u7406\u6548\u7387\u63d0\u9ad8\u4e86 11.2%\uff0cmAP 50 \u4e5f\u63d0\u9ad8\u4e86 93.2%\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684 YOLO PPA \u7684\u6709\u6548\u6027\u3002|\n", "2409.03192": "|**2024-09-05**|[PEPL: Precision-Enhanced Pseudo-Labeling for Fine-Grained Image Classification in Semi-Supervised Learning](http://arxiv.org/abs/2409.03192)|null|\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u7684\u51fa\u73b0\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u6b65\u3002\u7136\u800c\uff0c\u8be6\u7ec6\u6807\u6ce8\u7684\u7f3a\u4e4f\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u83b7\u53d6\u9ad8\u8d28\u91cf\u6807\u8bb0\u6570\u636e\u7684\u6210\u672c\u9ad8\u6602\u6216\u8017\u65f6\u7684\u60c5\u51b5\u4e0b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e13\u4e3a\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u5185\u7684\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u8bbe\u8ba1\u7684\u7cbe\u5ea6\u589e\u5f3a\u578b\u4f2a\u6807\u7b7e\uff08PEPL\uff09\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4f2a\u6807\u7b7e\u6765\u5229\u7528\u4e30\u5bcc\u7684\u672a\u6807\u8bb0\u6570\u636e\uff0c\u8fd9\u4e9b\u4f2a\u6807\u7b7e\u901a\u8fc7\u4e24\u4e2a\u5173\u952e\u9636\u6bb5\u9010\u6b65\u7ec6\u5316\uff1a\u521d\u59cb\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u8bed\u4e49\u6df7\u5408\u4f2a\u6807\u7b7e\u751f\u6210\u3002\u8fd9\u4e9b\u9636\u6bb5\u5229\u7528\u7c7b\u6fc0\u6d3b\u56fe\uff08CAM\uff09\u6765\u51c6\u786e\u4f30\u8ba1\u8bed\u4e49\u5185\u5bb9\u5e76\u751f\u6210\u7ec6\u5316\u6807\u7b7e\uff0c\u8fd9\u4e9b\u6807\u7b7e\u6355\u83b7\u4e86\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6240\u9700\u7684\u57fa\u672c\u7ec6\u8282\u3002\u901a\u8fc7\u5173\u6ce8\u8bed\u4e49\u7ea7\u4fe1\u606f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u6807\u51c6\u6570\u636e\u589e\u5f3a\u548c\u56fe\u50cf\u6df7\u5408\u6280\u672f\u5728\u4fdd\u7559\u5173\u952e\u7ec6\u7c92\u5ea6\u7279\u5f81\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002\u6211\u4eec\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u76f8\u5bf9\u4e8e\u73b0\u6709\u534a\u76d1\u7763\u7b56\u7565\u7684\u663e\u8457\u6539\u8fdb\uff0c\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u5728https://github.com/TianSuya/SemiFG\u5f00\u6e90\u3002|\n", "2409.03137": "|**2024-09-05**|[The AdEMAMix Optimizer: Better, Faster, Older](http://arxiv.org/abs/2409.03137)|null|\u57fa\u4e8e\u52a8\u91cf\u7684\u4f18\u5316\u5668\u662f\u4f17\u591a\u673a\u5668\u5b66\u4e60\u5e94\u7528\u7684\u6838\u5fc3\u3002\u8fd9\u4e9b\u4f18\u5316\u5668\u901a\u5e38\u4f9d\u8d56\u4e8e\u68af\u5ea6\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747 (EMA)\uff0c\u5b83\u4f1a\u4ee5\u6307\u6570\u65b9\u5f0f\u8870\u51cf\u65e7\u68af\u5ea6\u5bf9\u5f53\u524d\u68af\u5ea6\u7684\u8d21\u732e\u3002\u8fd9\u662f\u56e0\u4e3a\u68af\u5ea6\u662f\u5c40\u90e8\u7684\u7ebf\u6027\u8fd1\u4f3c\uff0c\u5f53\u8fed\u4ee3\u70b9\u5728\u635f\u5931\u51fd\u6570\u66f2\u9762\u4e0a\u79fb\u52a8\u65f6\uff0c\u65e7\u68af\u5ea6\u7684\u76f8\u5173\u6027\u4f1a\u964d\u4f4e\u3002\u8fd9\u9879\u5de5\u4f5c\u5bf9\u4f7f\u7528\u5355\u4e2a EMA \u6765\u7d2f\u79ef\u8fc7\u53bb\u68af\u5ea6\u7684\u505a\u6cd5\u63d0\u51fa\u4e86\u8d28\u7591\uff0c\u5e76\u901a\u8fc7\u7ecf\u9a8c\u8bc1\u660e\u4e86\u8fd9\u79cd\u9009\u62e9\u53ef\u80fd\u662f\u6b21\u4f18\u7684\uff1a\u5355\u4e2a EMA \u65e0\u6cd5\u540c\u65f6\u5bf9\u6700\u8fd1\u7684\u68af\u5ea6\u8d4b\u4e88\u9ad8\u6743\u91cd\uff0c\u5e76\u5bf9\u8f83\u65e7\u7684\u68af\u5ea6\u8d4b\u4e88\u4e0d\u53ef\u5ffd\u7565\u7684\u6743\u91cd\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c2\u5bdf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 AdEMAMix\uff0c\u5b83\u662f\u5bf9 Adam \u4f18\u5316\u5668\u7684\u4e00\u79cd\u7b80\u5355\u4fee\u6539\uff0c\u5b83\u6df7\u5408\u4e86\u4e24\u4e2a EMA\uff0c\u4ee5\u66f4\u597d\u5730\u5229\u7528\u8fc7\u53bb\u7684\u68af\u5ea6\u3002\u6211\u4eec\u5728\u8bed\u8a00\u5efa\u6a21\u548c\u56fe\u50cf\u5206\u7c7b\u65b9\u9762\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u68af\u5ea6\u5728\u6570\u4e07\u6b65\u5185\u4ecd\u7136\u5177\u6709\u76f8\u5173\u6027\u3002\u5b83\u4eec\u6709\u52a9\u4e8e\u66f4\u5feb\u5730\u6536\u655b\uff0c\u5e76\u4e14\u901a\u5e38\u6536\u655b\u5230\u66f4\u4f4e\u7684\u6700\u5c0f\u503c\uff1a\u4f8b\u5982\uff0c\u4e00\u4e2a\u5728 1010 \u4ebf\u4e2a\u8bcd\u7b26\u4e0a\u8bad\u7ec3\u7684\u5177\u6709 13 \u4ebf\u4e2a\u53c2\u6570\u7684 AdEMAMix LLM \u7684\u6027\u80fd\u4e0e\u5728\u4e00\u4e2a 1970 \u4ebf\u4e2a\u8bcd\u7b26\u4e0a\u8bad\u7ec3\u7684 AdamW \u6a21\u578b\u76f8\u5f53\uff08+95%\uff09\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u51cf\u7f13\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6a21\u578b\u9057\u5fd8\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u9f13\u52b1\u8fdb\u4e00\u6b65\u63a2\u7d22\u5229\u7528\u8fc7\u53bb\u68af\u5ea6\u7684\u4e0d\u540c\u7c7b\u578b\u7684\u51fd\u6570\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f EMA\u3002|\n", "2409.03022": "|**2024-09-04**|[Boundless: Generating Photorealistic Synthetic Data for Object Detection in Urban Streetscapes](http://arxiv.org/abs/2409.03022)|**[link](https://github.com/zk2172-columbia/boundless)**|\u6211\u4eec\u4ecb\u7ecdBoundless\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u5bc6\u96c6\u7684\u57ce\u5e02\u8857\u666f\u4e2d\u5b9e\u73b0\u9ad8\u5ea6\u51c6\u786e\u7684\u76ee\u6807\u68c0\u6d4b\u7684\u903c\u771f\u5408\u6210\u6570\u636e\u751f\u6210\u7cfb\u7edf\u3002Boundless\u53ef\u4ee5\u7528\u81ea\u52a8\u5316\u548c\u53ef\u914d\u7f6e\u7684\u8fc7\u7a0b\u53d6\u4ee3\u5927\u89c4\u6a21\u7684\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u6536\u96c6\u548c\u624b\u52a8\u5730\u9762\u5b9e\u51b5\u76ee\u6807\u6ce8\u91ca\uff08\u6807\u8bb0\uff09\u3002Boundless\u57fa\u4e8e\u865a\u5e7b\u5f15\u64ce5 (UE5) \u57ce\u5e02\u793a\u4f8b\u9879\u76ee\uff0c\u5e76\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u7684\u7167\u660e\u548c\u573a\u666f\u53d8\u5316\u6761\u4ef6\u4e0b\u51c6\u786e\u6536\u96c63D\u8fb9\u754c\u6846\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u5728Boundless\u751f\u6210\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u4ece\u4e2d\u7a7a\u76f8\u673a\u83b7\u53d6\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u63a8\u7406\u65f6\u7684\u6027\u80fd\u3002\u6211\u4eec\u5c06Boundless\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u4e0eCARLA\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u89c2\u5bdf\u52307.8 mAP\u7684\u6539\u8fdb\u3002\u6211\u4eec\u53d6\u5f97\u7684\u7ed3\u679c\u652f\u6301\u4e86\u5408\u6210\u6570\u636e\u751f\u6210\u662f\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u7528\u4e8e\u8bad\u7ec3/\u5fae\u8c03\u7528\u4e8e\u57ce\u5e02\u573a\u666f\u7684\u53ef\u6269\u5c55\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u3002|\n"}, "\u751f\u6210\u6a21\u578b": {"2409.02919": "|**2024-09-04**|[HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts](http://arxiv.org/abs/2409.02919)|**[link](https://github.com/Liuxinyv/HiPrompt)**|\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u751f\u6210\u66f4\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5728\u5904\u7406\u7269\u4f53\u91cd\u590d\u548c\u7ed3\u6784\u4f2a\u5f71\u65b9\u9762\u5e38\u5e38\u9047\u5230\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u6269\u5c55\u5230 4K \u53ca\u66f4\u9ad8\u5206\u8fa8\u7387\u65f6\u3002\u6211\u4eec\u53d1\u73b0\u95ee\u9898\u5728\u4e8e\uff0c\u5355\u4e2a\u63d0\u793a\u751f\u6210\u591a\u4e2a\u5c3a\u5ea6\u7684\u65b9\u5f0f\u6548\u7387\u4f4e\u4e0b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 HiPrompt\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u987b\u5fae\u8c03\u7684\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u901a\u8fc7\u5f15\u5165\u5206\u5c42\u63d0\u793a\u6765\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002\u5206\u5c42\u63d0\u793a\u63d0\u4f9b\u5168\u5c40\u548c\u5c40\u90e8\u6307\u5bfc\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5168\u5c40\u6307\u5bfc\u6765\u81ea\u63cf\u8ff0\u6574\u4f53\u5185\u5bb9\u7684\u7528\u6237\u8f93\u5165\uff0c\u800c\u5c40\u90e8\u6307\u5bfc\u5219\u5229\u7528\u6765\u81ea MLLM \u7684\u9010\u5757\u63cf\u8ff0\u6765\u7cbe\u5fc3\u6307\u5bfc\u5c40\u90e8\u7ed3\u6784\u548c\u7eb9\u7406\u7684\u751f\u6210\u3002\u6b64\u5916\uff0c\u5728\u9006\u5411\u53bb\u566a\u8fc7\u7a0b\u4e2d\uff0c\u751f\u6210\u7684\u566a\u58f0\u88ab\u5206\u89e3\u4e3a\u4f4e\u9891\u548c\u9ad8\u9891\u7a7a\u95f4\u5206\u91cf\u3002\u8fd9\u4e9b\u5206\u91cf\u4ee5\u591a\u4e2a\u63d0\u793a\u7ea7\u522b\u4e3a\u6761\u4ef6\uff0c\u5305\u62ec\u8be6\u7ec6\u7684\u9010\u5757\u63cf\u8ff0\u548c\u66f4\u5e7f\u6cdb\u7684\u56fe\u50cf\u7ea7\u63d0\u793a\uff0c\u4ece\u800c\u4fc3\u8fdb\u5728\u5206\u5c42\u8bed\u4e49\u6307\u5bfc\u4e0b\u7684\u63d0\u793a\u5f15\u5bfc\u53bb\u566a\u3002\u5b83\u8fdb\u4e00\u6b65\u5141\u8bb8\u751f\u6210\u8fc7\u7a0b\u66f4\u591a\u5730\u5173\u6ce8\u5c40\u90e8\u7a7a\u95f4\u533a\u57df\uff0c\u5e76\u786e\u4fdd\u751f\u6210\u7684\u56fe\u50cf\u5728\u9ad8\u6e05\u6670\u5ea6\u4e0b\u4fdd\u6301\u4e00\u81f4\u7684\u5c40\u90e8\u548c\u5168\u5c40\u8bed\u4e49\u3001\u7ed3\u6784\u548c\u7eb9\u7406\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cHiPrompt \u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u7269\u4f53\u91cd\u590d\u5e76\u63d0\u9ad8\u4e86\u7ed3\u6784\u8d28\u91cf\u3002|\n", "2409.02915": "|**2024-09-04**|[Latent Watermarking of Audio Generative Models](http://arxiv.org/abs/2409.02915)|null|\u97f3\u9891\u751f\u6210\u6a21\u578b\u7684\u8fdb\u6b65\u7ed9\u5176\u8d1f\u8d23\u4efb\u7684\u62ab\u9732\u548c\u6ee5\u7528\u68c0\u6d4b\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u901a\u8fc7\u5bf9\u5176\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u7279\u5b9a\u6c34\u5370\u6765\u6807\u8bb0\u6f5c\u5728\u751f\u6210\u6a21\u578b\u7684\u65b9\u6cd5\u3002\u7531\u6b64\u4ea7\u751f\u7684\u6c34\u5370\u6a21\u578b\u751f\u6210\u7684\u6f5c\u5728\u8868\u793a\uff0c\u5176\u89e3\u7801\u8f93\u51fa\u53ef\u4ee5\u88ab\u9ad8\u7f6e\u4fe1\u5ea6\u5730\u68c0\u6d4b\u5230\uff0c\u800c\u65e0\u8bba\u4f7f\u7528\u4f55\u79cd\u89e3\u7801\u65b9\u6cd5\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e0\u9700\u8fdb\u884c\u4e8b\u540e\u6c34\u5370\u6b65\u9aa4\u5373\u53ef\u68c0\u6d4b\u751f\u6210\u7684\u5185\u5bb9\u3002\u5b83\u4e3a\u5f00\u6e90\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6709\u52a9\u4e8e\u8bc6\u522b\u90a3\u4e9b\u5728\u672a\u9075\u5b88\u8bb8\u53ef\u6761\u6b3e\u7684\u60c5\u51b5\u4e0b\u5bf9\u8fd9\u4e9b\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u6216\u4f7f\u7528\u7684\u884d\u751f\u4f5c\u54c1\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u5bf9\u6f5c\u5728\u751f\u6210\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u540e\uff0c\u751f\u6210\u8f93\u51fa\u7684\u68c0\u6d4b\u7cbe\u5ea6\u4e5f\u80fd\u5728\u5047\u9633\u6027\u7387\u4e3a$10^{-3}$\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230 75% \u4ee5\u4e0a\u3002|\n", "2409.02908": "|**2024-09-04**|[Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling](http://arxiv.org/abs/2409.02908)|null|\u63a9\u7801\u6269\u6563\u6a21\u578b (MDM) \u7531\u4e8e\u5176\u76f8\u8f83\u4e8e\u5176\u4ed6\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5df2\u6210\u4e3a\u79bb\u6563\u6570\u636e\u751f\u6210\u5efa\u6a21\u7684\u70ed\u95e8\u7814\u7a76\u8bfe\u9898\uff0c\u5e76\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u4e0e\u81ea\u56de\u5f52\u6a21\u578b (ARM) \u5c55\u5f00\u7ade\u4e89\u3002\u6700\u8fd1\u7b80\u5316\u63a9\u7801\u6269\u6563\u6846\u67b6\u7684\u52aa\u529b\u8fdb\u4e00\u6b65\u4f7f\u5176\u4e0e\u8fde\u7eed\u7a7a\u95f4\u6269\u6563\u6a21\u578b\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u83b7\u5f97\u4e86\u66f4\u6709\u539f\u5219\u7684\u8bad\u7ec3\u548c\u91c7\u6837\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63ed\u793a\u4e86 MDM \u7684\u8bad\u7ec3\u548c\u91c7\u6837\u5728\u7406\u8bba\u4e0a\u90fd\u53ef\u4ee5\u6446\u8131\u65f6\u95f4\u53d8\u91cf\uff08\u53ef\u4ee5\u8bf4\u662f\u6269\u6563\u6a21\u578b\u7684\u5173\u952e\u7279\u5f81\uff09\uff0c\u5e76\u4e14\u7b49\u6548\u4e8e\u63a9\u7801\u6a21\u578b\u3002\u6211\u4eec\u5728\u91c7\u6837\u65b9\u9762\u7684\u8054\u7cfb\u662f\u901a\u8fc7\u6211\u4eec\u63d0\u51fa\u7684\u9996\u6b21\u547d\u4e2d\u91c7\u6837\u5668 (FHS) \u5efa\u7acb\u7684\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 FHS \u5728\u7406\u8bba\u4e0a\u7b49\u6548\u4e8e MDM \u7684\u539f\u59cb\u751f\u6210\u8fc7\u7a0b\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8017\u65f6\u7684\u5206\u7c7b\u91c7\u6837\uff0c\u5e76\u5b9e\u73b0\u4e86 20 \u500d\u7684\u52a0\u901f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u7814\u7a76\u5bf9\u5148\u524d\u5173\u4e8e MDM \u5728\u751f\u6210\u56f0\u60d1\u5ea6\u65b9\u9762\u53ef\u4ee5\u8d85\u8d8a ARM \u7684\u8bf4\u6cd5\u63d0\u51fa\u4e86\u8d28\u7591\u3002\u6211\u4eec\u9996\u6b21\u53d1\u73b0\u4e86\u4e00\u4e2a\u6f5c\u5728\u7684\u6570\u503c\u95ee\u9898\uff0c\u5373\u4f7f\u4f7f\u7528 32 \u4f4d\u6d6e\u70b9\u7cbe\u5ea6\uff0c\u4e5f\u4f1a\u5bfc\u81f4\u4e0d\u51c6\u786e\u7684\u5206\u7c7b\u91c7\u6837\u3002\u6211\u4eec\u8868\u660e\uff0c\u8be5\u6570\u503c\u95ee\u9898\u5728\u7406\u8bba\u4e0a\u548c\u7ecf\u9a8c\u4e0a\u90fd\u964d\u4f4e\u4e86\u6709\u6548\u6e29\u5ea6\uff0c\u5bfc\u81f4\u5148\u524d\u6587\u732e\u4e2d\u5bf9 MDM \u751f\u6210\u7ed3\u679c\u7684\u8bc4\u4f30\u4e0d\u516c\u5e73\u3002|\n", "2409.02851": "|**2024-09-04**|[Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video Diffusion Models](http://arxiv.org/abs/2409.02851)|null|\u4ece\u5355\u5f20RGB\u56fe\u50cf\u751f\u6210\u903c\u771f3D\u4eba\u4f53\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u7cbe\u786e\u7684\u51e0\u4f55\u5efa\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u7eb9\u7406\u548c\u5408\u7406\u7684\u4e0d\u53ef\u89c1\u90e8\u5206\u751f\u6210\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u8fdb\u884c3D\u4eba\u4f53\u751f\u6210\uff0c\u4f46\u5b83\u4eec\u7ecf\u5e38\u9762\u4e34\u89c6\u89d2\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u8fd9\u963b\u788d\u4e86\u9ad8\u8d28\u91cf3D\u4eba\u4f53\u7684\u751f\u6210\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Human-VDM\uff0c\u4e00\u79cd\u4f7f\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u4ece\u5355\u5f20RGB\u56fe\u50cf\u751f\u62103D\u4eba\u4f53\u7684\u65b0\u65b9\u6cd5\u3002Human-VDM\u4f7f\u7528\u9ad8\u65af\u6e32\u67d3\u4e3a3D\u4eba\u4f53\u751f\u6210\u63d0\u4f9b\u4e86\u65f6\u95f4\u4e0a\u4e00\u81f4\u7684\u89c6\u56fe\u3002\u5b83\u7531\u4e09\u4e2a\u6a21\u5757\u7ec4\u6210\uff1a\u89c6\u56fe\u4e00\u81f4\u7684\u4eba\u4f53\u89c6\u9891\u6269\u6563\u6a21\u5757\u3001\u89c6\u9891\u589e\u5f3a\u6a21\u5757\u548c\u9ad8\u65af\u6e32\u67d3\u6a21\u5757\u3002\u9996\u5148\uff0c\u5c06\u5355\u5f20\u56fe\u50cf\u8f93\u5165\u4eba\u4f53\u89c6\u9891\u6269\u6563\u6a21\u5757\u4ee5\u751f\u6210\u8fde\u8d2f\u7684\u4eba\u4f53\u89c6\u9891\u3002\u63a5\u4e0b\u6765\uff0c\u89c6\u9891\u589e\u5f3a\u6a21\u5757\u5e94\u7528\u8d85\u5206\u8fa8\u7387\u548c\u89c6\u9891\u63d2\u503c\u6765\u589e\u5f3a\u751f\u6210\u89c6\u9891\u7684\u7eb9\u7406\u548c\u51e0\u4f55\u5e73\u6ed1\u5ea6\u3002\u6700\u540e\uff0c3D\u4eba\u4f53\u9ad8\u65af\u6e32\u67d3\u6a21\u5757\u5728\u8fd9\u4e9b\u9ad8\u5206\u8fa8\u7387\u548c\u89c6\u89d2\u4e00\u81f4\u7684\u56fe\u50cf\u7684\u6307\u5bfc\u4e0b\u5b66\u4e60\u903c\u771f\u7684\u4eba\u4f53\u3002\u5b9e\u9a8c\u8868\u660e\uff0cHuman-VDM\u53ef\u4ee5\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u4eba\u4f53\uff0c\u5728\u751f\u6210\u8d28\u91cf\u548c\u6570\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://human-vdm.github.io/Human-VDM/|\n", "2409.02845": "|**2024-09-04**|[Multi-Track MusicLDM: Towards Versatile Music Generation with Latent Diffusion Model](http://arxiv.org/abs/2409.02845)|null|\u6269\u6563\u6a21\u578b\u5728\u6d89\u53ca\u97f3\u9891\u548c\u97f3\u4e50\u7684\u8de8\u6a21\u6001\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f8b\u5982\u6587\u672c\u5230\u58f0\u97f3\u548c\u6587\u672c\u5230\u97f3\u4e50\u7684\u751f\u6210\u3002\u8fd9\u4e9b\u6587\u672c\u63a7\u5236\u7684\u97f3\u4e50\u751f\u6210\u6a21\u578b\u901a\u5e38\u4fa7\u91cd\u4e8e\u901a\u8fc7\u6355\u6349\u5168\u5c40\u97f3\u4e50\u5c5e\u6027\uff08\u5982\u6d41\u6d3e\u548c\u60c5\u7eea\uff09\u6765\u751f\u6210\u97f3\u4e50\u3002\u7136\u800c\uff0c\u97f3\u4e50\u521b\u4f5c\u662f\u4e00\u9879\u590d\u6742\u7684\u591a\u5c42\u6b21\u4efb\u52a1\uff0c\u901a\u5e38\u5c06\u97f3\u4e50\u7f16\u6392\u4f5c\u4e3a\u521b\u4f5c\u8fc7\u7a0b\u7684\u4e00\u4e2a\u7ec4\u6210\u90e8\u5206\u3002\u6b64\u8fc7\u7a0b\u6d89\u53ca\u521b\u4f5c\u6bcf\u4e2a\u4e50\u5668\u90e8\u5206\uff0c\u4f7f\u5176\u5728\u8282\u594f\u3001\u529b\u5ea6\u3001\u548c\u58f0\u548c\u65cb\u5f8b\u65b9\u9762\u4e0e\u73b0\u6709\u90e8\u5206\u4fdd\u6301\u4e00\u81f4\uff0c\u8fd9\u9700\u8981\u6bd4\u6587\u672c\u63d0\u793a\u901a\u5e38\u63d0\u4f9b\u7684\u66f4\u7cbe\u786e\u7684\u97f3\u8f68\u63a7\u5236\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5c06 MusicLDM\uff08\u4e00\u79cd\u7528\u4e8e\u97f3\u4e50\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff09\u6269\u5c55\u4e3a\u591a\u8f68\u751f\u6210\u6a21\u578b\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002\u901a\u8fc7\u5b66\u4e60\u5171\u4eab\u4e0a\u4e0b\u6587\u7684\u97f3\u8f68\u7684\u8054\u5408\u6982\u7387\uff0c\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u8de8\u591a\u4e2a\u97f3\u8f68\u751f\u6210\u5f7c\u6b64\u826f\u597d\u5bf9\u5e94\u7684\u97f3\u4e50\uff0c\u65e0\u8bba\u662f\u6709\u6761\u4ef6\u5730\u8fd8\u662f\u65e0\u6761\u4ef6\u5730\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6a21\u578b\u8fd8\u80fd\u591f\u8fdb\u884c\u7f16\u66f2\u751f\u6210\uff0c\u5176\u4e2d\u6a21\u578b\u53ef\u4ee5\u5728\u7ed9\u5b9a\u5176\u4ed6\u97f3\u8f68\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u4efb\u4f55\u97f3\u8f68\u5b50\u96c6\uff08\u4f8b\u5982\uff0c\u751f\u6210\u4e0e\u7ed9\u5b9a\u8d1d\u65af\u548c\u9f13\u97f3\u8f68\u4e92\u8865\u7684\u94a2\u7434\u97f3\u8f68\uff09\u3002\u6211\u4eec\u5c06\u6211\u4eec\u7684\u6a21\u578b\u4e0e\u73b0\u6709\u7684\u591a\u8f68\u751f\u6210\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u603b\u751f\u6210\u4efb\u52a1\u548c\u7f16\u66f2\u751f\u6210\u4efb\u52a1\u7684\u5ba2\u89c2\u6307\u6807\u4e0a\u90fd\u53d6\u5f97\u4e86\u76f8\u5f53\u5927\u7684\u6539\u8fdb\u3002|\n", "2409.02683": "|**2024-09-04**|[Rethinking HTG Evaluation: Bridging Generation and Recognition](http://arxiv.org/abs/2409.02683)|null|\u751f\u6210\u6a21\u578b\u5728\u81ea\u7136\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u8bc4\u4f30\u5df2\u5f97\u5230\u5e7f\u6cdb\u7814\u7a76\u3002\u5373\u4f7f\u5728\u8bf8\u5982\u624b\u5199\u751f\u6210\uff08HTG\uff09\u7b49\u5177\u6709\u72ec\u7279\u7279\u6b8a\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u4f7f\u7528\u4e86\u7c7b\u4f3c\u7684\u534f\u8bae\u548c\u6307\u6807\uff0c\u5373\u4f7f\u5b83\u4eec\u53ef\u80fd\u5e76\u975e\u5b8c\u5168\u5408\u9002\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e09\u79cd\u4e13\u4e3a HTG \u8bc4\u4f30\u91cf\u8eab\u5b9a\u5236\u7684\u5ea6\u91cf\u6307\u6807\uff1a$ \\text{HTG}_{\\text{HTR}} $\u3001$ \\text{HTG}_{\\text{style}} $ \u548c $ \\text{HTG}_{\\text{OOV}} $\uff0c\u5e76\u8ba4\u4e3a\u5b83\u4eec\u66f4\u4fbf\u4e8e\u8bc4\u4f30\u751f\u6210\u624b\u5199\u56fe\u50cf\u7684\u8d28\u91cf\u3002\u8fd9\u4e9b\u6307\u6807\u4f9d\u8d56\u4e8e\u624b\u5199\u6587\u672c\u8bc6\u522b\u548c\u4e66\u5199\u8005\u8bc6\u522b\u6a21\u578b\u7684\u8bc6\u522b\u9519\u8bef/\u51c6\u786e\u7387\uff0c\u5e76\u5f3a\u8c03\u4e66\u5199\u98ce\u683c\u3001\u6587\u672c\u5185\u5bb9\u548c\u591a\u6837\u6027\u662f\u7b26\u5408\u624b\u5199\u56fe\u50cf\u5185\u5bb9\u7684\u4e3b\u8981\u65b9\u9762\u3002\u6211\u4eec\u5728 IAM \u624b\u5199\u6570\u636e\u5e93\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u8bf8\u5982 FID \u4e4b\u7c7b\u7684\u5e7f\u6cdb\u4f7f\u7528\u7684\u6307\u6807\u65e0\u6cd5\u6b63\u786e\u91cf\u5316\u751f\u6210\u624b\u5199\u6837\u672c\u7684\u591a\u6837\u6027\u548c\u5b9e\u7528\u6027\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6307\u6807\u4fe1\u606f\u66f4\u4e30\u5bcc\uff0c\u5e76\u5f3a\u8c03\u4e86 HTG \u4e2d\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\u7684\u5fc5\u8981\u6027\u3002\u6240\u63d0\u51fa\u7684\u6307\u6807\u4e3a\u8bc4\u4f30 HTG \u8d28\u91cf\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u3001\u4fe1\u606f\u66f4\u4e30\u5bcc\u7684\u534f\u8bae\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8 HTR \u7684\u6027\u80fd\u3002\u8bc4\u4f30\u534f\u8bae\u7684\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u5f97\uff1ahttps://github.com/koninik/HTG_evaluation\u3002|\n", "2409.02668": "|**2024-09-04**|[Introduction to Machine Learning](http://arxiv.org/abs/2409.02668)|null|\u672c\u4e66\u4ecb\u7ecd\u4e86\u673a\u5668\u5b66\u4e60\u4e2d\u8bb8\u591a\u7b97\u6cd5\u7684\u5f00\u53d1\u548c\u5206\u6790\u6240\u4f9d\u8d56\u7684\u6570\u5b66\u57fa\u7840\u548c\u6280\u672f\u3002\u672c\u4e66\u9996\u5148\u4ecb\u7ecd\u4e86\u8d2f\u7a7f\u5168\u4e66\u7684\u7b26\u53f7\u8868\u793a\uff0c\u5e76\u56de\u987e\u4e86\u5fae\u79ef\u5206\u3001\u7ebf\u6027\u4ee3\u6570\u548c\u6982\u7387\u8bba\u7684\u57fa\u672c\u6982\u5ff5\uff0c\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6d4b\u5ea6\u8bba\u672f\u8bed\uff0c\u53ef\u4f5c\u4e3a\u4f7f\u7528\u8fd9\u4e9b\u5de5\u5177\u7684\u90e8\u5206\u7684\u9605\u8bfb\u6307\u5357\u3002\u5bfc\u8bba\u7ae0\u8282\u8fd8\u63d0\u4f9b\u4e86\u77e9\u9635\u5206\u6790\u548c\u4f18\u5316\u7684\u80cc\u666f\u77e5\u8bc6\u3002\u540e\u9762\u7684\u7ae0\u8282\u4e3a\u672c\u4e66\u4e2d\u4f7f\u7528\u7684\u8bb8\u591a\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5305\u62ec\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u3001\u8fd1\u4f3c\u65b9\u6cd5\u7b49\u3002\u5728\u8ba8\u8bba\u4e86\u7edf\u8ba1\u9884\u6d4b\u7684\u57fa\u672c\u6982\u5ff5\u4e4b\u540e\uff0c\u672c\u4e66\u4ecb\u7ecd\u4e86\u518d\u751f\u6838\u7406\u8bba\u548c\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u6280\u672f\uff0c\u8fd9\u4e9b\u6280\u672f\u5728\u8bb8\u591a\u5730\u65b9\u90fd\u6709\u5e94\u7528\uff0c\u7136\u540e\u4ecb\u7ecd\u4e86\u5404\u79cd\u76d1\u7763\u7edf\u8ba1\u5b66\u4e60\u7b97\u6cd5\uff0c\u5305\u62ec\u7ebf\u6027\u65b9\u6cd5\u3001\u652f\u6301\u5411\u91cf\u673a\u3001\u51b3\u7b56\u6811\u3001boosting\u548c\u795e\u7ecf\u7f51\u7edc\u3002\u63a5\u4e0b\u6765\u8f6c\u5411\u751f\u6210\u65b9\u6cd5\uff0c\u9996\u5148\u4ecb\u7ecd\u4e86\u91c7\u6837\u65b9\u6cd5\u548c\u9a6c\u5c14\u53ef\u592b\u94fe\u7406\u8bba\u3002\u63a5\u4e0b\u6765\u7684\u7ae0\u8282\u63cf\u8ff0\u4e86\u56fe\u6a21\u578b\u7406\u8bba\uff0c\u4ecb\u7ecd\u4e86\u6f5c\u53d8\u91cf\u6a21\u578b\u7684\u53d8\u5206\u65b9\u6cd5\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u751f\u6210\u6a21\u578b\u3002\u63a5\u4e0b\u6765\u7684\u7ae0\u8282\u91cd\u70b9\u4ecb\u7ecd\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ec\u805a\u7c7b\u3001\u56e0\u5b50\u5206\u6790\u548c\u6d41\u5f62\u5b66\u4e60\u3002\u672c\u4e66\u7684\u6700\u540e\u4e00\u7ae0\u504f\u5411\u7406\u8bba\uff0c\u8ba8\u8bba\u4e86\u96c6\u4e2d\u4e0d\u7b49\u5f0f\u548c\u6cdb\u5316\u754c\u3002|\n", "2409.02664": "|**2024-09-04**|[Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection](http://arxiv.org/abs/2409.02664)|null|The proliferation of deepfake faces poses huge potential negative impacts on our daily lives. Despite substantial advancements in deepfake detection over these years, the generalizability of existing methods against forgeries from unseen datasets or created by emerging generative models remains constrained. In this paper, inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach that repurposes a well-trained VLM for general deepfake detection. Motivated by the model reprogramming paradigm that manipulates the model prediction via data perturbations, our method can reprogram a pretrained VLM model (e.g., CLIP) solely based on manipulating its input without tuning the inner parameters. Furthermore, we insert a pseudo-word guided by facial identity into the text prompt. Extensive experiments on several popular benchmarks demonstrate that (1) the cross-dataset and cross-manipulation performances of deepfake detection can be significantly and consistently improved (e.g., over 88% AUC in cross-dataset setting from FF++ to WildDeepfake) using a pre-trained CLIP model with our proposed reprogramming method; (2) our superior performances are at less cost of trainable parameters, making it a promising approach for real-world applications.|\n", "2409.02657": "|**2024-09-04**|[PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for One-Shot Talking Head Generation](http://arxiv.org/abs/2409.02657)|null|While previous audio-driven talking head generation (THG) methods generate head poses from driving audio, the generated poses or lips cannot match the audio well or are not editable. In this study, we propose \\textbf{PoseTalk}, a THG system that can freely generate lip-synchronized talking head videos with free head poses conditioned on text prompts and audio. The core insight of our method is using head pose to connect visual, linguistic, and audio signals. First, we propose to generate poses from both audio and text prompts, where the audio offers short-term variations and rhythm correspondence of the head movements and the text prompts describe the long-term semantics of head motions. To achieve this goal, we devise a Pose Latent Diffusion (PLD) model to generate motion latent from text prompts and audio cues in a pose latent space. Second, we observe a loss-imbalance problem: the loss for the lip region contributes less than 4\\% of the total reconstruction loss caused by both pose and lip, making optimization lean towards head movements rather than lip shapes. To address this issue, we propose a refinement-based learning strategy to synthesize natural talking videos using two cascaded networks, i.e., CoarseNet, and RefineNet. The CoarseNet estimates coarse motions to produce animated images in novel poses and the RefineNet focuses on learning finer lip motions by progressively estimating lip motions from low-to-high resolutions, yielding improved lip-synchronization performance. Experiments demonstrate our pose prediction strategy achieves better pose diversity and realness compared to text-only or audio-only, and our video generator model outperforms state-of-the-art methods in synthesizing talking videos with natural head motions. Project: https://junleen.github.io/projects/posetalk.|\n", "2409.02653": "|**2024-09-04**|[Skip-and-Play: Depth-Driven Pose-Preserved Image Generation for Any Objects](http://arxiv.org/abs/2409.02653)|null|The emergence of diffusion models has enabled the generation of diverse high-quality images solely from text, prompting subsequent efforts to enhance the controllability of these models. Despite the improvement in controllability, pose control remains limited to specific objects (e.g., humans) or poses (e.g., frontal view) due to the fact that pose is generally controlled via camera parameters (e.g., rotation angle) or keypoints (e.g., eyes, nose). Specifically, camera parameters-conditional pose control models generate unrealistic images depending on the object, owing to the small size of 3D datasets for training. Also, keypoint-based approaches encounter challenges in acquiring reliable keypoints for various objects (e.g., church) or poses (e.g., back view). To address these limitations, we propose depth-based pose control, as depth maps are easily obtainable from a single depth estimation model regardless of objects and poses, unlike camera parameters and keypoints. However, depth-based pose control confronts issues of shape dependency, as depth maps influence not only the pose but also the shape of the generated images. To tackle this issue, we propose Skip-and-Play (SnP), designed via analysis of the impact of three components of depth-conditional ControlNet on the pose and the shape of the generated images. To be specific, based on the analysis, we selectively skip parts of the components to mitigate shape dependency on the depth map while preserving the pose. Through various experiments, we demonstrate the superiority of SnP over baselines and showcase the ability of SnP to generate images of diverse objects and poses. Remarkably, SnP exhibits the ability to generate images even when the objects in the condition (e.g., a horse) and the prompt (e.g., a hedgehog) differ from each other.|\n", "2409.03757": "|**2024-09-05**|[Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding](http://arxiv.org/abs/2409.03757)|**[link](https://github.com/yunzeman/lexicon3d)**|\u590d\u6742\u4e09\u7ef4\u573a\u666f\u7406\u89e3\u8fd1\u5e74\u6765\u5907\u53d7\u5173\u6ce8\uff0c\u573a\u666f\u7f16\u7801\u7b56\u7565\u5728\u5176\u4e2d\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u9488\u5bf9\u4e0d\u540c\u573a\u666f\u7684\u6700\u4f73\u573a\u666f\u7f16\u7801\u7b56\u7565\u4ecd\u4e0d\u660e\u786e\uff0c\u7279\u522b\u662f\u4e0e\u57fa\u4e8e\u56fe\u50cf\u7684\u7f16\u7801\u7b56\u7565\u76f8\u6bd4\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5bf9\u7528\u4e8e\u4e09\u7ef4\u573a\u666f\u7406\u89e3\u7684\u5404\u79cd\u89c6\u89c9\u7f16\u7801\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u7814\u7a76\uff0c\u786e\u5b9a\u4e86\u6bcf\u4e2a\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u6db5\u76d6\u4e86\u4e03\u79cd\u89c6\u89c9\u57fa\u7840\u7f16\u7801\u5668\uff0c\u5305\u62ec\u57fa\u4e8e\u56fe\u50cf\u3001\u57fa\u4e8e\u89c6\u9891\u548c\u4e09\u7ef4\u57fa\u7840\u6a21\u578b\u3002\u6211\u4eec\u5728\u56db\u4e2a\u4efb\u52a1\u4e2d\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\uff1a\u89c6\u89c9\u8bed\u8a00\u573a\u666f\u63a8\u7406\u3001\u89c6\u89c9\u5b9a\u4f4d\u3001\u5206\u5272\u548c\u914d\u51c6\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u4fa7\u91cd\u4e8e\u573a\u666f\u7406\u89e3\u7684\u4e0d\u540c\u65b9\u9762\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u5f97\u51fa\u4e86\u4ee5\u4e0b\u4e3b\u8981\u53d1\u73b0\uff1aDINOv2 \u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u89c6\u9891\u6a21\u578b\u5728\u5bf9\u8c61\u7ea7\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6269\u6563\u6a21\u578b\u6709\u5229\u4e8e\u51e0\u4f55\u4efb\u52a1\uff0c\u800c\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u8bed\u8a00\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u610f\u60f3\u4e0d\u5230\u7684\u5c40\u9650\u6027\u3002\u8fd9\u4e9b\u89c1\u89e3\u6311\u6218\u4e86\u4e00\u4e9b\u4f20\u7edf\u8ba4\u77e5\uff0c\u4e3a\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u672a\u6765\u7684\u89c6\u89c9\u8bed\u8a00\u548c\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e2d\u9700\u8981\u66f4\u7075\u6d3b\u7684\u7f16\u7801\u5668\u9009\u62e9\u3002|\n", "2409.03745": "|**2024-09-05**|[ArtiFade: Learning to Generate High-quality Subject from Blemished Images](http://arxiv.org/abs/2409.03745)|null|\u4ee5\u4e3b\u9898\u4e3a\u4e3b\u5bfc\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6280\u672f\u5728\u5b66\u4e60\u548c\u6355\u6349\u4e3b\u9898\u7279\u5f81\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u5373\u4f7f\u53ea\u4f7f\u7528\u6709\u9650\u6570\u91cf\u7684\u56fe\u50cf\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u5f53\u8f93\u5165\u56fe\u50cf\u5b58\u5728\u7455\u75b5\u65f6\uff0c\u53ef\u80fd\u96be\u4ee5\u751f\u6210\u5408\u7406\u7684\u56fe\u50cf\u3002\u8fd9\u4e3b\u8981\u5f52\u56e0\u4e8e\u5f53\u524d\u6280\u672f\u5728\u533a\u5206\u4e3b\u9898\u76f8\u5173\u7279\u5f81\u548c\u5e72\u6270\u6027\u7455\u75b5\u65b9\u9762\u7684\u80fd\u529b\u4e0d\u8db3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86ArtiFade\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u6210\u529f\u5730\u4ece\u6709\u7455\u75b5\u7684\u6570\u636e\u96c6\u4e2d\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u7684\u65e0\u7455\u75b5\u56fe\u50cf\u3002\u5177\u4f53\u6765\u8bf4\uff0cArtiFade\u5229\u7528\u9884\u5148\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5fae\u8c03\u6765\u6d88\u9664\u7455\u75b5\u3002\u901a\u8fc7\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u5305\u542b\u65e0\u7455\u75b5\u56fe\u50cf\u53ca\u5176\u5bf9\u5e94\u7684\u6709\u7455\u75b5\u56fe\u50cf\u7684\u4e13\u95e8\u6570\u636e\u96c6\u6765\u5b9e\u73b0\u7455\u75b5\u7684\u6d88\u9664\u3002ArtiFade\u8fd8\u786e\u4fdd\u4e86\u4fdd\u7559\u6269\u6563\u6a21\u578b\u4e2d\u56fa\u6709\u7684\u539f\u59cb\u751f\u6210\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u4e3b\u9898\u9a71\u52a8\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u548c\u65e0\u7455\u75b5\u56fe\u50cf\u65b9\u9762\u7684\u6574\u4f53\u6027\u80fd\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u4e3a\u8fd9\u9879\u4efb\u52a1\u8bbe\u8ba1\u4e86\u8bc4\u4f30\u57fa\u51c6\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86ArtiFade\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u60c5\u51b5\u4e0b\u90fd\u80fd\u6709\u6548\u53bb\u9664\u7455\u75b5\u7684\u6cdb\u5316\u80fd\u529b\u3002|\n", "2409.03644": "|**2024-09-05**|[RealisHuman: A Two-Stage Approach for Refining Malformed Human Parts in Generated Images](http://arxiv.org/abs/2409.03644)|null|\u8fd1\u5e74\u6765\uff0c\u6269\u6563\u6a21\u578b\u5f7b\u5e95\u6539\u53d8\u4e86\u89c6\u89c9\u751f\u6210\u9886\u57df\uff0c\u5176\u6027\u80fd\u8d85\u8d8a\u4e86\u751f\u6210\u5bf9\u6297\u7f51\u7edc (GANs) \u7b49\u4f20\u7edf\u6846\u67b6\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4eba\u7c7b\u53ca\u5176\u8bed\u4e49\u90e8\u5206\uff08\u5982\u624b\u548c\u8138\uff09\u590d\u6742\u7684\u7ed3\u6784\uff0c\u751f\u6210\u5177\u6709\u771f\u5b9e\u611f\u7684\u4eba\u7c7b\u56fe\u50cf\u4ecd\u7136\u662f\u4e00\u9879\u91cd\u5927\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a RealisHuman \u7684\u65b0\u578b\u540e\u5904\u7406\u89e3\u51b3\u65b9\u6848\u3002RealisHuman \u6846\u67b6\u5206\u4e24\u4e2a\u9636\u6bb5\u8fd0\u884c\u3002\u9996\u5148\uff0c\u5b83\u4f7f\u7528\u539f\u59cb\u7684\u7578\u5f62\u90e8\u5206\u4f5c\u4e3a\u53c2\u8003\uff0c\u751f\u6210\u903c\u771f\u7684\u4eba\u4f53\u90e8\u4f4d\uff08\u5982\u624b\u6216\u8138\uff09\uff0c\u786e\u4fdd\u7ec6\u8282\u4e0e\u539f\u59cb\u56fe\u50cf\u4e00\u81f4\u3002\u5176\u6b21\uff0c\u5b83\u901a\u8fc7\u91cd\u65b0\u7ed8\u5236\u5468\u56f4\u533a\u57df\u5c06\u6821\u6b63\u540e\u7684\u4eba\u4f53\u90e8\u4f4d\u65e0\u7f1d\u5730\u878d\u5165\u5230\u5176\u5bf9\u5e94\u7684\u4f4d\u7f6e\uff0c\u4ee5\u786e\u4fdd\u5e73\u6ed1\u903c\u771f\u7684\u878d\u5408\u3002RealisHuman \u6846\u67b6\u663e\u8457\u589e\u5f3a\u4e86\u4eba\u7c7b\u751f\u6210\u7684\u771f\u5b9e\u611f\uff0c\u8fd9\u53ef\u4ee5\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u6307\u6807\u7684\u663e\u8457\u6539\u8fdb\u5f97\u5230\u8bc1\u660e\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/Wangbenzhi/RealisHuman \u83b7\u53d6\u3002|\n", "2409.03636": "|**2024-09-05**|[DiffEVC: Any-to-Any Emotion Voice Conversion with Expressive Guidance](http://arxiv.org/abs/2409.03636)|null|\u60c5\u611f\u8bed\u97f3\u8f6c\u6362 (EVC) \u901a\u8fc7\u653e\u5927\u79ef\u6781\u7ebf\u7d22\u548c\u51cf\u5c11\u6d88\u6781\u7ebf\u7d22\u6765\u6539\u53d8\u8bed\u97f3\u60c5\u611f\uff0c\u4ece\u800c\u589e\u5f3a\u6c9f\u901a\u3002\u8fd9\u9879\u590d\u6742\u7684\u4efb\u52a1\u6d89\u53ca\u8bed\u97f3\u8d28\u91cf\u3001\u8bf4\u8bdd\u8005\u7279\u5f81\u548c\u5185\u5bb9\u7b49\u7ea0\u7f20\u4e0d\u6e05\u7684\u56e0\u7d20\u3002\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982 GAN \u548c\u81ea\u52a8\u7f16\u7801\u5668\uff09\u901a\u8fc7\u5b66\u4e60\u6620\u5c04\u6216\u89e3\u8026\u7279\u5f81\u5728 EVC \u4e2d\u53d6\u5f97\u4e86\u4e00\u5b9a\u7684\u6210\u529f\uff0c\u4f46\u9762\u4e34\u7740\u4e0d\u7a33\u5b9a\u6027\u548c\u8bed\u97f3\u8d28\u91cf\u4e0b\u964d\u7b49\u6311\u6218\u3002\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u7a33\u5b9a\u7684\u8bad\u7ec3\u548c\u9ad8\u8d28\u91cf\u7684\u751f\u6210\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684 EVC \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u4e92\u4fe1\u606f\u635f\u5931\u548c\u8f85\u52a9\u6a21\u578b\u6765\u89e3\u8026\u60c5\u611f\u548c\u8bf4\u8bdd\u8005\u8eab\u4efd\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u8868\u8fbe\u6027\u5f15\u5bfc\u673a\u5236\uff0c\u4ee5\u6539\u5584\u60c5\u611f\u8f6c\u6362\uff0c\u540c\u65f6\u4fdd\u6301\u8bf4\u8bdd\u8005\u7279\u5f81\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5bf9\u4e8e\u672a\u77e5\u8bf4\u8bdd\u8005\u548c\u60c5\u611f\u7684\u6709\u6548\u6027\uff0c\u5728 EVC \u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|\n", "2409.03600": "|**2024-09-05**|[TCDiff: Triple Condition Diffusion Model with 3D Constraints for Stylizing Synthetic Faces](http://arxiv.org/abs/2409.03600)|**[link](https://github.com/bovifocr/tcdiff)**|\u4e00\u4e2a\u9c81\u68d2\u7684\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u9700\u8981\u4f7f\u7528\u5305\u542b\u5927\u91cf\u4e2a\u4f53\u4ee5\u53ca\u6bcf\u4e2a\u4e2a\u4f53\u5728\u4e0d\u540c\u6761\u4ef6\uff08\u4f8b\u5982\u59ff\u6001\u3001\u8868\u60c5\u3001\u5e74\u9f84\u3001\u566a\u58f0\u548c\u906e\u6321\uff09\u4e0b\u7684\u5927\u91cf\u6837\u672c\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002\u7531\u4e8e\u4f26\u7406\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5927\u578b\u771f\u5b9e\u4eba\u8138\u6570\u636e\u96c6\uff08\u4f8b\u5982 MS1MV3\uff09\u5df2\u88ab\u505c\u7528\uff0c\u5e76\u4e14\u5df2\u7ecf\u63d0\u51fa\u4e86\u5229\u7528 GAN \u548c\u6269\u6563\u6a21\u578b\u7684\u5408\u6210\u4eba\u8138\u751f\u6210\u5668\uff0c\u4f8b\u5982 SYNFace\u3001SFace\u3001DigiFace-1M\u3001IDiff-Face\u3001DCFace \u548c GANDiffFace\uff0c\u65e8\u5728\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u3002\u5176\u4e2d\u4e00\u4e9b\u65b9\u6cd5\u53ef\u4ee5\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u771f\u5b9e\u4eba\u8138\uff0c\u4f46\u7c7b\u5185\u5dee\u5f02\u8f83\u4f4e\uff0c\u800c\u53e6\u4e00\u4e9b\u65b9\u6cd5\u5219\u751f\u6210\u5177\u6709\u9ad8\u5dee\u5f02\u6027\u4f46\u8eab\u4efd\u4e00\u81f4\u6027\u8f83\u4f4e\u7684\u4eba\u8138\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u91cd\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff08TCDiff\uff09\uff0c\u901a\u8fc7 2D \u548c 3D \u4eba\u8138\u7ea6\u675f\u6765\u6539\u8fdb\u4ece\u771f\u5b9e\u4eba\u8138\u5230\u5408\u6210\u4eba\u8138\u7684\u4eba\u8138\u98ce\u683c\u8fc1\u79fb\uff0c\u5728\u4fdd\u6301\u5fc5\u8981\u7684\u7c7b\u5185\u9ad8\u5dee\u5f02\u6027\u7684\u540c\u65f6\u589e\u5f3a\u4eba\u8138\u8eab\u4efd\u4e00\u81f4\u6027\u3002\u4f7f\u7528\u6211\u4eec\u65b0\u7684\u6570\u636e\u96c6\u7684 1k\u30012k \u548c 5k \u7c7b\u8fdb\u884c\u8bad\u7ec3\u7684\u4eba\u8138\u8bc6\u522b\u5b9e\u9a8c\u5728 LFW\u3001CFP-FP\u3001AgeDB \u548c BUPT \u7b49\u771f\u5b9e\u4eba\u8138\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5408\u6210\u6570\u636e\u96c6\u3002\u6211\u4eec\u7684\u6e90\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u5f97\uff1ahttps://github.com/BOVIFOCR/tcdiff\u3002|\n", "2409.03550": "|**2024-09-05**|[DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any Architecture](http://arxiv.org/abs/2409.03550)|null|\u6269\u6563\u6a21\u578b (DM) \u5728\u5404\u4e2a\u9886\u57df\u90fd\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5176\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u7684\u63a8\u7406\u901f\u5ea6\u6162\u548c\u8ba1\u7b97\u9700\u6c42\u9ad8\u5374\u963b\u788d\u4e86\u5176\u53d1\u5c55\u3002\u52a0\u901fDM\u6700\u5e38\u7528\u7684\u65b9\u6cd5\u662f\u51cf\u5c11\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u53bb\u566a\u6b65\u9aa4\uff0c\u8fd9\u53ef\u4ee5\u901a\u8fc7\u66f4\u5feb\u7684\u91c7\u6837\u6c42\u89e3\u5668\u6216\u77e5\u8bc6\u84b8\u998f (KD) \u6765\u5b9e\u73b0\u3002\u4e0e\u5148\u524d\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5c06\u5927\u578b\u9884\u8bad\u7ec3DM\u7684\u529f\u80fd\u8fc1\u79fb\u5230\u66f4\u5feb\u7684\u67b6\u6784\u4e2d\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u4ee5\u72ec\u7279\u7684\u65b9\u5f0f\u4f7f\u7528KD\uff0c\u901a\u8fc7\u5c06\u751f\u6210\u80fd\u529b\u63d0\u70bc\u5230\u66f4\u5feb\u7684\u53d8\u4f53\u4e2d\u6765\u538b\u7f29DM\u3002\u6b64\u5916\uff0c\u8003\u8651\u5230\u6e90\u6570\u636e\u4e0d\u53ef\u8bbf\u95ee\u6216\u5bf9\u4e8e\u5f53\u524d\u7684\u751f\u6210\u6a21\u578b\u6765\u8bf4\u5b58\u50a8\u91cf\u592a\u5927\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u6e90\u6570\u636e\u84b8\u998f\u8303\u5f0f\uff0c\u79f0\u4e3a\u6269\u6563\u6a21\u578b\u7684\u65e0\u6570\u636e\u77e5\u8bc6\u84b8\u998f (DKDM)\u3002\u901a\u5e38\uff0c\u6211\u4eec\u5efa\u7acb\u7684DKDM\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a1) DKDM\u76ee\u6807\u51fd\u6570\uff0c\u5b83\u4f7f\u7528\u9884\u8bad\u7ec3DM\u751f\u6210\u7684\u5408\u6210\u53bb\u566a\u6570\u636e\u6765\u4f18\u5316\u66f4\u5feb\u7684DM\uff0c\u800c\u65e0\u9700\u6e90\u6570\u636e\uff1b2) \u52a8\u6001\u8fed\u4ee3\u84b8\u998f\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u7075\u6d3b\u5730\u7ec4\u7ec7\u53bb\u566a\u6570\u636e\u7684\u5408\u6210\uff0c\u9632\u6b62\u7531\u4e8e\u751f\u6210\u901f\u5ea6\u6162\u800c\u51cf\u6162\u4f18\u5316\u8fc7\u7a0b\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u6b21\u5c1d\u8bd5\u4f7f\u7528KD\u4ee5\u65e0\u6570\u636e\u7684\u65b9\u5f0f\u5c06DM\u63d0\u70bc\u5230\u4efb\u4f55\u67b6\u6784\u4e2d\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u7684DKDM\u4e0e\u5927\u591a\u6570\u73b0\u6709\u7684\u52a0\u901f\u65b9\u6cd5\uff08\u4f8b\u5982\u51cf\u5c11\u53bb\u566a\u6b65\u9aa4\u3001\u91cf\u5316\u548c\u526a\u679d\uff09\u662f\u6b63\u4ea4\u7684\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684DKDM\u80fd\u591f\u63a8\u5bfc\u51fa\u901f\u5ea6\u63d0\u9ad82\u500d\u7684DM\uff0c\u5176\u6027\u80fd\u4e0e\u57fa\u7ebf\u4fdd\u6301\u4e00\u81f4\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684DKDM\u4f7f\u9884\u8bad\u7ec3\u7684DM\u80fd\u591f\u4f5c\u4e3a\u201c\u6570\u636e\u96c6\u201d\u6765\u8bad\u7ec3\u65b0\u7684DM\u3002|\n", "2409.03514": "|**2024-09-05**|[Blended Latent Diffusion under Attention Control for Real-World Video Editing](http://arxiv.org/abs/2409.03514)|null|\u7531\u4e8e\u7f3a\u4e4f\u5b8c\u5168\u516c\u5f00\u53ef\u7528\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\uff0c\u76ee\u524d\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u503e\u5411\u4e8e\u5efa\u7acb\u5728\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e4b\u4e0a\uff0c\u7136\u800c\uff0c\u5728\u5904\u7406\u5177\u6709\u65f6\u95f4\u4fe1\u606f\u7684\u89c6\u9891\u5c40\u90e8\u7f16\u8f91\u65b9\u9762\uff0c\u5b83\u4eec\u4ecd\u7136\u9762\u4e34\u7740\u5de8\u5927\u7684\u6311\u6218\u3002\u9996\u5148\uff0c\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u8bd5\u56fe\u901a\u8fc7\u9884\u5148\u5b9a\u4e49\u7684\u63a9\u7801\u6765\u5173\u6ce8\u5c40\u90e8\u533a\u57df\u7f16\u8f91\uff0c\u4f46\u7531\u4e8e\u6bcf\u4e00\u5e27\u7684\u7a7a\u95f4\u6574\u4f53\u751f\u6210\uff0c\u5916\u90e8\u533a\u57df\u80cc\u666f\u7684\u4fdd\u7559\u5e76\u4e0d\u7406\u60f3\u3002\u6b64\u5916\uff0c\u7531\u7528\u6237\u4e13\u95e8\u63d0\u4f9b\u63a9\u7801\u662f\u4e00\u9879\u989d\u5916\u7684\u6602\u8d35\u5de5\u4f5c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u96c6\u6210\u5230\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u7684\u81ea\u4e3b\u63a9\u7801\u7b56\u7565\u3002\u6700\u540e\u4f46\u540c\u6837\u91cd\u8981\u7684\u662f\uff0c\u56fe\u50cf\u7ea7\u9884\u8bad\u7ec3\u6a21\u578b\u6ca1\u6709\u5b66\u4e60\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u800c\u8fd9\u5bf9\u4e8e\u8868\u8fbe\u8fd0\u52a8\u548c\u52a8\u6001\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u91c7\u7528\u56fe\u50cf\u7ea7\u6df7\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6765\u6267\u884c\u5c40\u90e8\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5229\u7528 DDIM \u53cd\u6f14\u6765\u83b7\u53d6\u6f5c\u5728\u5411\u91cf\u4f5c\u4e3a\u80cc\u666f\u6f5c\u5728\u5411\u91cf\uff0c\u800c\u4e0d\u662f\u968f\u673a\u566a\u58f0\u7684\u6f5c\u5728\u5411\u91cf\uff0c\u4ee5\u66f4\u597d\u5730\u4fdd\u7559\u8f93\u5165\u89c6\u9891\u7684\u80cc\u666f\u4fe1\u606f\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4ece\u6269\u6563\u6b65\u9aa4\u4e2d\u7684\u4ea4\u53c9\u6ce8\u610f\u56fe\u884d\u751f\u7684\u81ea\u4e3b\u63a9\u7801\u5236\u9020\u673a\u5236\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u5c06 U-Net \u7684\u81ea\u6ce8\u610f\u529b\u5757\u8f6c\u6362\u4e3a\u65f6\u7a7a\u5757\u6765\u589e\u5f3a\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002|\n", "2409.03455": "|**2024-09-05**|[Data-free Distillation with Degradation-prompt Diffusion for Multi-weather Image Restoration](http://arxiv.org/abs/2409.03455)|null|\u591a\u5929\u6c14\u56fe\u50cf\u590d\u539f\u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u8fdb\u5c55\uff0c\u4f46\u6a21\u578b\u5bb9\u91cf\u7684\u589e\u52a0\u548c\u6602\u8d35\u7684\u6570\u636e\u83b7\u53d6\u9650\u5236\u4e86\u5176\u5728\u5185\u5b58\u6709\u9650\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u3002\u65e0\u6570\u636e\u84b8\u998f\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u5141\u8bb8\u4ece\u9884\u8bad\u7ec3\u7684\u6559\u5e08\u6a21\u578b\u4e2d\u5b66\u4e60\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u3002\u73b0\u6709\u7684\u65e0\u6570\u636e\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5229\u7528GAN\u751f\u6210\u7684\u4f2a\u6570\u636e\u6216\u4ece\u4e92\u8054\u7f51\u6536\u96c6\u7684\u771f\u5b9e\u6570\u636e\u6765\u4f18\u5316\u6a21\u578b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u9047\u5230\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6216\u4e0e\u539f\u59cb\u6570\u636e\u5b58\u5728\u57df\u504f\u79fb\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u9000\u5316\u63d0\u793a\u6269\u6563\u7684\u65e0\u6570\u636e\u84b8\u998f\u591a\u5929\u6c14\u56fe\u50cf\u590d\u539f\u6846\u67b6\uff08D4IR\uff09\u3002\u5b83\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u4ee3\u66ffGAN\u4ee5\u907f\u514d\u6a21\u578b\u5d29\u6e83\uff0c\u5e76\u7ed3\u5408\u4e86\u9000\u5316\u611f\u77e5\u63d0\u793a\u9002\u914d\u5668\uff0c\u4ee5\u4fc3\u8fdb\u5185\u5bb9\u9a71\u52a8\u7684\u6761\u4ef6\u6269\u6563\uff0c\u4ece\u800c\u751f\u6210\u4e0e\u57df\u76f8\u5173\u7684\u56fe\u50cf\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u9996\u5148\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u7684\u9000\u5316\u63d0\u793a\u9002\u914d\u5668\uff0c\u7528\u4e8e\u4ece\u7f51\u7edc\u6536\u96c6\u7684\u9000\u5316\u56fe\u50cf\u4e2d\u6355\u83b7\u9000\u5316\u611f\u77e5\u63d0\u793a\u3002\u7136\u540e\uff0c\u5c06\u6536\u96c6\u5230\u7684\u672a\u914d\u5bf9\u7684\u5e72\u51c0\u56fe\u50cf\u6270\u52a8\u5230\u7a33\u5b9a\u6269\u6563\u7684\u6f5c\u5728\u7279\u5f81\u4e2d\uff0c\u5e76\u4ee5\u9000\u5316\u611f\u77e5\u63d0\u793a\u4e3a\u6761\u4ef6\uff0c\u5408\u6210\u65b0\u7684\u57df\u76f8\u5173\u9000\u5316\u56fe\u50cf\uff0c\u7528\u4e8e\u77e5\u8bc6\u84b8\u998f\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u4e0e\u4f7f\u7528\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u84b8\u998f\u7684\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u751a\u81f3\u4f18\u4e8e\u5176\u4ed6\u4e3b\u6d41\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u3002|\n", "2409.03417": "|**2024-09-05**|[Convergence Rates for the Maximum A Posteriori Estimator in PDE-Regression Models with Random Design](http://arxiv.org/abs/2409.03417)|null|\u6211\u4eec\u8003\u8651\u4ece\u9ad8\u65af\u56de\u5f52\u95ee\u9898$Y = \\mathscr{G}(\\theta)(Z)+\\varepsilon$\u4ea7\u751f\u7684\u6570\u636e\u4e2d\u6062\u590d\u53c2\u6570$\\theta\\in H^\\alpha$\u7684\u7edf\u8ba1\u9006\u95ee\u9898\uff0c\u5176\u4e2d$\\mathscr{G}:\\mathbb{L}^2\\to\\mathbb{L}^2$\u662f\u975e\u7ebf\u6027\u6b63\u5411\u6620\u5c04\uff0c$Z$\u662f\u968f\u673a\u8bbe\u8ba1\u70b9\uff0c$\\varepsilon$\u662f\u9ad8\u65af\u566a\u58f0\u3002\u4f30\u8ba1\u7b56\u7565\u57fa\u4e8e$\\Vert\\cdot\\Vert_{H^\\alpha}$-\u7ea6\u675f\u4e0b\u7684\u6700\u5c0f\u4e8c\u4e58\u6cd5\u3002\u6211\u4eec\u5728\u6b63\u5411\u6620\u5c04$\\mathscr{G}$\u6ee1\u8db3Lipschitz\u7c7b\u578b\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\uff0c\u5efa\u7acb\u4e86\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\u91cf$\\hat{\\theta}$\u4f5c\u4e3a\u7ed9\u5b9a\u6cdb\u51fd\u7684\u6700\u5927\u503c\u7684\u5b58\u5728\u6027\u3002\u8bc1\u660e\u4e86\u4e00\u4e2a\u4e00\u822c\u7684\u6d53\u5ea6\u7ed3\u679c\uff0c\u5e76\u7528\u5b83\u6765\u8bc1\u660e\u9884\u6d4b\u8bef\u5dee\u7684\u4e00\u81f4\u6027\u548c\u4e0a\u754c\u3002\u76f8\u5e94\u7684\u6536\u655b\u901f\u5ea6\u4e0d\u4ec5\u53cd\u6620\u4e86\u76ee\u6807\u53c2\u6570\u7684\u5e73\u6ed1\u6027\uff0c\u8fd8\u53cd\u6620\u4e86\u6f5c\u5728\u9006\u95ee\u9898\u7684\u9002\u5b9a\u6027\u3002\u6211\u4eec\u5c06\u4e00\u822c\u6a21\u578b\u5e94\u7528\u4e8e\u8fbe\u897f\u95ee\u9898\uff0c\u5176\u4e2dPDE\u7684\u672a\u77e5\u7cfb\u6570\u51fd\u6570$f$\u7684\u6062\u590d\u662f\u4ee4\u4eba\u611f\u5174\u8da3\u7684\u3002\u5bf9\u4e8e\u8fd9\u4e2a\u4f8b\u5b50\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u9884\u6d4b\u8bef\u5dee\u548c\u4f30\u8ba1\u8bef\u5dee\u7684\u76f8\u5e94\u6536\u655b\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u7b80\u8981\u8ba8\u8bba\u4e86\u8be5\u4e00\u822c\u6a21\u578b\u5bf9\u5176\u4ed6\u95ee\u9898\u7684\u9002\u7528\u6027\u3002|\n", "2409.03403": "|**2024-09-05**|[RoVi-Aug: Robot and Viewpoint Augmentation for Cross-Embodiment Robot Learning](http://arxiv.org/abs/2409.03403)|null|\u6269\u5927\u673a\u5668\u4eba\u5b66\u4e60\u89c4\u6a21\u9700\u8981\u5e9e\u5927\u800c\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\uff0c\u5982\u4f55\u6709\u6548\u5730\u91cd\u590d\u4f7f\u7528\u6536\u96c6\u5230\u7684\u6570\u636e\u5e76\u5c06\u7b56\u7565\u8fc1\u79fb\u5230\u65b0\u7684\u673a\u5668\u4eba\u5e73\u53f0\u4ecd\u7136\u662f\u4e00\u4e2a\u60ac\u800c\u672a\u51b3\u7684\u95ee\u9898\u3002\u8bf8\u5982Open-X Embodiment (OXE) \u9879\u76ee\u7b49\u65b0\u5174\u7814\u7a76\u5df2\u7ecf\u8868\u660e\uff0c\u901a\u8fc7\u7ec4\u5408\u5305\u542b\u4e0d\u540c\u673a\u5668\u4eba\u7684\u6570\u636e\u96c6\u6765\u5229\u7528\u6280\u80fd\u662f\u6709\u5e0c\u671b\u7684\u3002\u7136\u800c\uff0c\u8bb8\u591a\u6570\u636e\u96c6\u4e2d\u673a\u5668\u4eba\u7c7b\u578b\u548c\u76f8\u673a\u89d2\u5ea6\u5206\u5e03\u7684\u4e0d\u5e73\u8861\u4f7f\u5f97\u7b56\u7565\u5bb9\u6613\u8fc7\u62df\u5408\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86RoVi-Aug\uff0c\u5b83\u5229\u7528\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u5408\u6210\u5177\u6709\u4e0d\u540c\u673a\u5668\u4eba\u548c\u76f8\u673a\u89c6\u89d2\u7684\u6f14\u793a\u6765\u589e\u5f3a\u673a\u5668\u4eba\u6570\u636e\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u7269\u7406\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u901a\u8fc7\u5728\u673a\u5668\u4eba\u548c\u89c6\u70b9\u589e\u5f3a\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0cRoVi-Aug \u53ef\u4ee5\u5728\u5177\u6709\u663e\u8457\u4e0d\u540c\u76f8\u673a\u89d2\u5ea6\u7684\u672a\u77e5\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u90e8\u7f72\u3002\u4e0e Mirage \u7b49\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u7b97\u6cd5\u76f8\u6bd4\uff0cRoVi-Aug \u5728\u6d4b\u8bd5\u65f6\u4e0d\u9700\u8981\u989d\u5916\u7684\u5904\u7406\uff0c\u4e0d\u5047\u8bbe\u5df2\u77e5\u76f8\u673a\u89d2\u5ea6\uff0c\u5e76\u4e14\u5141\u8bb8\u7b56\u7565\u5fae\u8c03\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5728\u539f\u59cb\u673a\u5668\u4eba\u6570\u636e\u96c6\u548c\u589e\u5f3a\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\uff0cRoVi-Aug \u53ef\u4ee5\u5b66\u4e60\u591a\u673a\u5668\u4eba\u548c\u591a\u4efb\u52a1\u7b56\u7565\uff0c\u4ece\u800c\u5b9e\u73b0\u673a\u5668\u4eba\u548c\u6280\u80fd\u4e4b\u95f4\u66f4\u6709\u6548\u7684\u8fc1\u79fb\uff0c\u5e76\u5c06\u6210\u529f\u7387\u63d0\u9ad8\u9ad8\u8fbe 30%\u3002|\n"}, "LLM": {"2409.01909": "|**2024-09-03**|[LUK: Empowering Log Understanding with Expert Knowledge from Large Language Models](http://arxiv.org/abs/2409.01909)|**[link](https://github.com/LeaperOvO/LUK)**|Logs play a critical role in providing essential information for system monitoring and troubleshooting. Recently, with the success of pre-trained language models (PLMs) and large language models (LLMs) in natural language processing (NLP), smaller PLMs (such as BERT) and LLMs (like ChatGPT) have become the current mainstream approaches for log analysis. While LLMs possess rich knowledge, their high computational costs and unstable performance make LLMs impractical for analyzing logs directly. In contrast, smaller PLMs can be fine-tuned for specific tasks even with limited computational resources, making them more practical. However, these smaller PLMs face challenges in understanding logs comprehensively due to their limited expert knowledge. To better utilize the knowledge embedded within LLMs for log understanding, this paper introduces a novel knowledge enhancement framework, called LUK, which acquires expert knowledge from LLMs to empower log understanding on a smaller PLM. Specifically, we design a multi-expert collaboration framework based on LLMs consisting of different roles to acquire expert knowledge. In addition, we propose two novel pre-training tasks to enhance the log pre-training with expert knowledge. LUK achieves state-of-the-art results on different log analysis tasks and extensive experiments demonstrate expert knowledge from LLMs can be utilized more effectively to understand logs.|\n", "2409.00702": "|**2024-09-04**|[MARS: Matching Attribute-aware Representations for Text-based Sequential Recommendation](http://arxiv.org/abs/2409.00702)|null|Sequential recommendation aims to predict the next item a user is likely to prefer based on their sequential interaction history. Recently, text-based sequential recommendation has emerged as a promising paradigm that uses pre-trained language models to exploit textual item features to enhance performance and facilitate knowledge transfer to unseen datasets. However, existing text-based recommender models still struggle with two key challenges: (i) representing users and items with multiple attributes, and (ii) matching items with complex user interests. To address these challenges, we propose a novel model, Matching Attribute-aware Representations for Text-based Sequential Recommendation (MARS). MARS extracts detailed user and item representations through attribute-aware text encoding, capturing diverse user intents with multiple attribute-aware representations. It then computes user-item scores via attribute-wise interaction matching, effectively capturing attribute-level user preferences. Our extensive experiments demonstrate that MARS significantly outperforms existing sequential models, achieving improvements of up to 24.43% and 29.26% in Recall@10 and NDCG@10 across five benchmark datasets. Code is available at https://github.com/junieberry/MARS|\n", "2409.00323": "|**2024-08-31**|[From Prediction to Application: Language Model-based Code Knowledge Tracing with Domain Adaptive Pre-Training and Automatic Feedback System with Pedagogical Prompting for Comprehensive Programming Education](http://arxiv.org/abs/2409.00323)|null|Knowledge Tracing (KT) is a critical component in online learning, but traditional approaches face limitations in interpretability and cross-domain adaptability. This paper introduces Language Model-based Code Knowledge Tracing (CodeLKT), an innovative application of Language model-based Knowledge Tracing (LKT) to programming education. CodeLKT leverages pre-trained language models to process learning data, demonstrating superior performance over existing KT and Code KT models. We explore Domain Adaptive Pre-Training (DAPT) and Task Adaptive Pre-Training (TAPT), showing enhanced performance in the coding domain and investigating cross-domain transfer between mathematics and coding. Additionally, we present an theoretically-informed integrated system combining CodeLKT with large language models to generate personalized, in-depth feedback to support students' programming learning. This work advances the field of Code Knowledge Tracing by expanding the knowledge base with language model-based approach and offering practical implications for programming education through data-informed feedback.|\n", "2408.17354": "|**2024-08-30**|[Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage](http://arxiv.org/abs/2408.17354)|null|\u5728\u79c1\u6709\u6570\u636e\u4e0a\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee5\u7528\u4e8e\u4e0b\u6e38\u5e94\u7528\u7a0b\u5e8f\u4f1a\u5728\u6f5c\u5728\u66b4\u9732\u654f\u611f\u4fe1\u606f\u65b9\u9762\u5e26\u6765\u91cd\u5927\u7684\u9690\u79c1\u98ce\u9669\u3002\u4e00\u4e9b\u6d41\u884c\u7684\u793e\u533a\u5e73\u53f0\u73b0\u5728\u63d0\u4f9b\u5404\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4fbf\u6377\u5206\u53d1\uff0c\u5141\u8bb8\u4efb\u4f55\u4eba\u53d1\u5e03\u800c\u65e0\u9700\u4e25\u683c\u9a8c\u8bc1\u3002\u8fd9\u79cd\u60c5\u51b5\u4f1a\u9020\u6210\u9690\u79c1\u5a01\u80c1\uff0c\u56e0\u4e3a\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u80fd\u4f1a\u88ab\u6545\u610f\u8bbe\u8ba1\u7528\u4e8e\u6cc4\u9732\u5fae\u8c03\u6570\u636e\u96c6\u7684\u9690\u79c1\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u6295\u6bd2\u6280\u672f\uff0c\u8be5\u6280\u672f\u4f7f\u7528\u6a21\u578b\u9057\u5fd8\u4f5c\u4e3a\u653b\u51fb\u5de5\u5177\u3002\u8fd9\u79cd\u65b9\u6cd5\u64cd\u7eb5\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u589e\u52a0\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u79c1\u6709\u6570\u636e\u7684\u6cc4\u9732\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u589e\u5f3a\u4e86\u6210\u5458\u63a8\u7406\u548c\u6570\u636e\u63d0\u53d6\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u6a21\u578b\u6548\u7528\u3002\u8de8\u4e0d\u540c\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u5fae\u8c03\u8bbe\u7f6e\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u653b\u51fb\u660e\u663e\u4f18\u4e8e\u57fa\u7ebf\u6027\u80fd\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4ece\u672a\u7ecf\u9a8c\u8bc1\u7684\u6765\u6e90\u4e0b\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u7528\u6237\u6572\u54cd\u4e86\u8b66\u949f\uff0c\u7a81\u51fa\u4e86\u5176\u4e2d\u6d89\u53ca\u7684\u6f5c\u5728\u98ce\u9669\u3002|\n", "2408.14505": "|**2024-08-24**|[Empowering Pre-Trained Language Models for Spatio-Temporal Forecasting via Decoupling Enhanced Discrete Reprogramming](http://arxiv.org/abs/2408.14505)|null|Spatio-temporal time series forecasting plays a critical role in various real-world applications, such as transportation optimization, energy management, and climate analysis. The recent advancements in Pre-trained Language Models (PLMs) have inspired efforts to reprogram these models for time series forecasting tasks, by leveraging their superior reasoning and generalization capabilities. However, existing approaches fall short in handling complex spatial inter-series dependencies and intrinsic intra-series frequency components, limiting their spatio-temporal forecasting performance. Moreover, the linear mapping of continuous time series to a compressed subset vocabulary in reprogramming constrains the spatio-temporal semantic expressivity of PLMs and may lead to potential information bottleneck. To overcome the above limitations, we propose \\textsc{RePST}, a tailored PLM reprogramming framework for spatio-temporal forecasting. The key insight of \\textsc{RePST} is to decouple the spatio-temporal dynamics in the frequency domain, allowing better alignment with the PLM text space. Specifically, we first decouple spatio-temporal data in Fourier space and devise a structural diffusion operator to obtain temporal intrinsic and spatial diffusion signals, making the dynamics more comprehensible and predictable for PLMs. To avoid information bottleneck from a limited vocabulary, we further propose a discrete reprogramming strategy that selects relevant discrete textual information from an expanded vocabulary space in a differentiable manner. Extensive experiments on four real-world datasets show that our proposed approach significantly outperforms state-of-the-art spatio-temporal forecasting models, particularly in data-scarce scenarios.|\n", "2408.13040": "|**2024-08-23**|[SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks](http://arxiv.org/abs/2408.13040)|null|Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both storage and computation. Additionally, prompting modifies only the LM's inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. This significantly reduces the need for human labor in designing task-specific models. These advantages become even more evident as the number of tasks served by the LM scales up. Motivated by the strengths of prompting, we are the first to explore the potential of prompting speech LMs in the domain of speech processing. Recently, there has been a growing interest in converting speech into discrete units for language modeling. Our pioneer research demonstrates that these quantized speech units are highly versatile within our unified prompting framework. Not only can they serve as class labels, but they also contain rich phonetic information that can be re-synthesized back into speech signals for speech generation tasks. Specifically, we reformulate speech processing tasks into speech-to-unit generation tasks. As a result, we can seamlessly integrate tasks such as speech classification, sequence generation, and speech generation within a single, unified prompting framework. The experiment results show that the prompting method can achieve competitive performance compared to the strong fine-tuning method based on self-supervised learning models with a similar number of trainable parameters. The prompting method also shows promising results in the few-shot setting. Moreover, with the advanced speech LMs coming into the stage, the proposed prompting framework attains great potential.|\n", "2408.12779": "|**2024-08-23**|[Investigating LLM Applications in E-Commerce](http://arxiv.org/abs/2408.12779)|null|The emergence of Large Language Models (LLMs) has revolutionized natural language processing in various applications especially in e-commerce. One crucial step before the application of such LLMs in these fields is to understand and compare the performance in different use cases in such tasks. This paper explored the efficacy of LLMs in the e-commerce domain, focusing on instruction-tuning an open source LLM model with public e-commerce datasets of varying sizes and comparing the performance with the conventional models prevalent in industrial applications. We conducted a comprehensive comparison between LLMs and traditional pre-trained language models across specific tasks intrinsic to the e-commerce domain, namely classification, generation, summarization, and named entity recognition (NER). Furthermore, we examined the effectiveness of the current niche industrial application of very large LLM, using in-context learning, in e-commerce specific tasks. Our findings indicate that few-shot inference with very large LLMs often does not outperform fine-tuning smaller pre-trained models, underscoring the importance of task-specific model optimization.Additionally, we investigated different training methodologies such as single-task training, mixed-task training, and LoRA merging both within domain/tasks and between different tasks. Through rigorous experimentation and analysis, this paper offers valuable insights into the potential effectiveness of LLMs to advance natural language processing capabilities within the e-commerce industry.|\n", "2408.12125": "|**2024-08-22**|[AutoTest: Evolutionary Code Solution Selection with Test Cases](http://arxiv.org/abs/2408.12125)|null|With the development of code generation techniques, selecting the correct code solution from multiple candidate solutions has become a crucial task. This study proposes AutoTest, a novel technique that combines automated test case generation with code solution execution to optimize the selection process using an evolutionary genetic algorithm. Firstly, AutoTest utilizes large pre-trained language models such as codegen-16B, code-davinci-002, and incoder-6B to provide code solutions and their corresponding test cases. Then, by executing the code solutions and evaluating their performance on the test cases, a consensus set is formed. Fine-grained ranking is achieved through the selection, mutation, and crossover mechanisms based on the evolutionary genetic algorithm, with the adjustment of alpha and beta parameters. Finally, the best code solution is chosen. AutoTest demonstrates significant performance improvements on the HumanEval benchmark test. The HumanEval dataset consists of 164 programming problems, and AutoTest achieves approximately a 10% improvement over the baseline method in terms of pass@1 score.|\n", "2408.11319": "|**2024-08-24**|[SarcasmBench: Towards Evaluating Large Language Models on Sarcasm Understanding](http://arxiv.org/abs/2408.11319)|null|In the era of large language models (LLMs), the task of ``System I''~-~the fast, unconscious, and intuitive tasks, e.g., sentiment analysis, text classification, etc., have been argued to be successfully solved. However, sarcasm, as a subtle linguistic phenomenon, often employs rhetorical devices like hyperbole and figuration to convey true sentiments and intentions, involving a higher level of abstraction than sentiment analysis. There is growing concern that the argument about LLMs' success may not be fully tenable when considering sarcasm understanding. To address this question, we select eleven SOTA LLMs and eight SOTA pre-trained language models (PLMs) and present comprehensive evaluations on six widely used benchmark datasets through different prompting approaches, i.e., zero-shot input/output (IO) prompting, few-shot IO prompting, chain of thought (CoT) prompting. Our results highlight three key findings: (1) current LLMs underperform supervised PLMs based sarcasm detection baselines across six sarcasm benchmarks. This suggests that significant efforts are still required to improve LLMs' understanding of human sarcasm. (2) GPT-4 consistently and significantly outperforms other LLMs across various prompting methods, with an average improvement of 14.0\\%$\\uparrow$. Claude 3 and ChatGPT demonstrate the next best performance after GPT-4. (3) Few-shot IO prompting method outperforms the other two methods: zero-shot IO and few-shot CoT. The reason is that sarcasm detection, being a holistic, intuitive, and non-rational cognitive process, is argued not to adhere to step-by-step logical reasoning, making CoT less effective in understanding sarcasm compared to its effectiveness in mathematical reasoning tasks.|\n", "2408.10548": "|**2024-08-20**|[Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution](http://arxiv.org/abs/2408.10548)|**[link](https://github.com/lanxiang1017/language-modeling-on-tabular-data-survey)**|Tabular data, a prevalent data type across various domains, presents unique challenges due to its heterogeneous nature and complex structural relationships. Achieving high predictive performance and robustness in tabular data analysis holds significant promise for numerous applications. Influenced by recent advancements in natural language processing, particularly transformer architectures, new methods for tabular data modeling have emerged. Early techniques concentrated on pre-training transformers from scratch, often encountering scalability issues. Subsequently, methods leveraging pre-trained language models like BERT have been developed, which require less data and yield enhanced performance. The recent advent of large language models, such as GPT and LLaMA, has further revolutionized the field, facilitating more advanced and diverse applications with minimal fine-tuning. Despite the growing interest, a comprehensive survey of language modeling techniques for tabular data remains absent. This paper fills this gap by providing a systematic review of the development of language modeling for tabular data, encompassing: (1) a categorization of different tabular data structures and data types; (2) a review of key datasets used in model training and tasks used for evaluation; (3) a summary of modeling techniques including widely-adopted data processing methods, popular architectures, and training objectives; (4) the evolution from adapting traditional Pre-training/Pre-trained language models to the utilization of large language models; (5) an identification of persistent challenges and potential future research directions in language modeling for tabular data analysis. GitHub page associated with this survey is available at: https://github.com/lanxiang1017/Language-Modeling-on-Tabular-Data-Survey.git.|\n"}, "Transformer": {"2409.02727": "|**2024-09-05**|[Pooling And Attention: What Are Effective Designs For LLM-Based Embedding Models?](http://arxiv.org/abs/2409.02727)|**[link](https://github.com/yixuantt/poolingandattn)**|The significant advancements of Large Language Models (LLMs) in generative tasks have led to a growing body of work exploring LLM-based embedding models. While these models, employing different pooling and attention strategies, have achieved state-of-the-art performance on public embedding benchmarks, questions still arise about what constitutes an effective design for LLM-based embedding models. However, these models are often trained on different datasets, using different LLM base models or training settings. Moreover, evaluations on public embedding benchmarks often fail to report statistical significance, making it difficult to determine which designs truly contribute to final performance. This complicates the process for practitioners seeking optimal training recipes for LLM-based embedding models. In this study, we conduct a large-scale experiment by training a series of LLM-based embedding models using the same training data and base model but differing in their pooling and attention strategies. The results show that there is no one-size-fits-all solution: while bidirectional attention and an additional trainable pooling layer outperform in text similarity and information retrieval tasks, they do not significantly surpass simpler designs like EOS-last token pooling and default causal attention in clustering and classification tasks. Furthermore, we propose a new pooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs of all hidden layers, rather than just the last layer, using a cross-attention network. This method proves to be statistically superior in text similarity and retrieval tasks compared to existing pooling methods. Overall, this paper sheds light on effective training strategies for LLM-based embedding models.|\n", "2409.02545": "|**2024-09-04**|[UniTT-Stereo: Unified Training of Transformer for Enhanced Stereo Matching](http://arxiv.org/abs/2409.02545)|null|Unlike other vision tasks where Transformer-based approaches are becoming increasingly common, stereo depth estimation is still dominated by convolution-based approaches. This is mainly due to the limited availability of real-world ground truth for stereo matching, which is a limiting factor in improving the performance of Transformer-based stereo approaches. In this paper, we propose UniTT-Stereo, a method to maximize the potential of Transformer-based stereo architectures by unifying self-supervised learning used for pre-training with stereo matching framework based on supervised learning. To be specific, we explore the effectiveness of reconstructing features of masked portions in an input image and at the same time predicting corresponding points in another image from the perspective of locality inductive bias, which is crucial in training models with limited training data. Moreover, to address these challenging tasks of reconstruction-and-prediction, we present a new strategy to vary a masking ratio when training the stereo model with stereo-tailored losses. State-of-the-art performance of UniTT-Stereo is validated on various benchmarks such as ETH3D, KITTI 2012, and KITTI 2015 datasets. Lastly, to investigate the advantages of the proposed approach, we provide a frequency analysis of feature maps and the analysis of locality inductive bias based on attention maps.|\n", "2409.02056": "|**2024-09-03**|[F2former: When Fractional Fourier Meets Deep Wiener Deconvolution and Selective Frequency Transformer for Image Deblurring](http://arxiv.org/abs/2409.02056)|null|Recent progress in image deblurring techniques focuses mainly on operating in both frequency and spatial domains using the Fourier transform (FT) properties. However, their performance is limited due to the dependency of FT on stationary signals and its lack of capability to extract spatial-frequency properties. In this paper, we propose a novel approach based on the Fractional Fourier Transform (FRFT), a unified spatial-frequency representation leveraging both spatial and frequency components simultaneously, making it ideal for processing non-stationary signals like images. Specifically, we introduce a Fractional Fourier Transformer (F2former), where we combine the classical fractional Fourier based Wiener deconvolution (F2WD) as well as a multi-branch encoder-decoder transformer based on a new fractional frequency aware transformer block (F2TB). We design F2TB consisting of a fractional frequency aware self-attention (F2SA) to estimate element-wise product attention based on important frequency components and a novel feed-forward network based on frequency division multiplexing (FM-FFN) to refine high and low frequency features separately for efficient latent clear image restoration. Experimental results for the cases of both motion deblurring as well as defocus deblurring show that the performance of our proposed method is superior to other state-of-the-art (SOTA) approaches.|\n", "2409.02018": "|**2024-09-03**|[TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation](http://arxiv.org/abs/2409.02018)|null|In healthcare, medical image segmentation is crucial for accurate disease diagnosis and the development of effective treatment strategies. Early detection can significantly aid in managing diseases and potentially prevent their progression. Machine learning, particularly deep convolutional neural networks, has emerged as a promising approach to addressing segmentation challenges. Traditional methods like U-Net use encoding blocks for local representation modeling and decoding blocks to uncover semantic relationships. However, these models often struggle with multi-scale objects exhibiting significant variations in texture and shape, and they frequently fail to capture long-range dependencies in the input data. Transformers designed for sequence-to-sequence predictions have been proposed as alternatives, utilizing global self-attention mechanisms. Yet, they can sometimes lack precise localization due to insufficient granular details. To overcome these limitations, we introduce TransDAE: a novel approach that reimagines the self-attention mechanism to include both spatial and channel-wise associations across the entire feature space, while maintaining computational efficiency. Additionally, TransDAE enhances the skip connection pathway with an inter-scale interaction module, promoting feature reuse and improving localization accuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on the Synaps multi-organ dataset, even without relying on pre-trained weights.|\n", "2409.01557": "|**2024-09-03**|[TASL-Net: Tri-Attention Selective Learning Network for Intelligent Diagnosis of Bimodal Ultrasound Video](http://arxiv.org/abs/2409.01557)|null|In the intelligent diagnosis of bimodal (gray-scale and contrast-enhanced) ultrasound videos, medical domain knowledge such as the way sonographers browse videos, the particular areas they emphasize, and the features they pay special attention to, plays a decisive role in facilitating precise diagnosis. Embedding medical knowledge into the deep learning network can not only enhance performance but also boost clinical confidence and reliability of the network. However, it is an intractable challenge to automatically focus on these person- and disease-specific features in videos and to enable networks to encode bimodal information comprehensively and efficiently. This paper proposes a novel Tri-Attention Selective Learning Network (TASL-Net) to tackle this challenge and automatically embed three types of diagnostic attention of sonographers into a mutual transformer framework for intelligent diagnosis of bimodal ultrasound videos. Firstly, a time-intensity-curve-based video selector is designed to mimic the temporal attention of sonographers, thus removing a large amount of redundant information while improving computational efficiency of TASL-Net. Then, to introduce the spatial attention of the sonographers for contrast-enhanced video analysis, we propose the earliest-enhanced position detector based on structural similarity variation, on which the TASL-Net is made to focus on the differences of perfusion variation inside and outside the lesion. Finally, by proposing a mutual encoding strategy that combines convolution and transformer, TASL-Net possesses bimodal attention to structure features on gray-scale videos and to perfusion variations on contrast-enhanced videos. These modules work collaboratively and contribute to superior performance. We conduct a detailed experimental validation of TASL-Net's performance on three datasets, including lung, breast, and liver.|\n", "2409.01352": "|**2024-09-02**|[Spectron: Target Speaker Extraction using Conditional Transformer with Adversarial Refinement](http://arxiv.org/abs/2409.01352)|null|Recently, attention-based transformers have become a de facto standard in many deep learning applications including natural language processing, computer vision, signal processing, etc.. In this paper, we propose a transformer-based end-to-end model to extract a target speaker's speech from a monaural multi-speaker mixed audio signal. Unlike existing speaker extraction methods, we introduce two additional objectives to impose speaker embedding consistency and waveform encoder invertibility and jointly train both speaker encoder and speech separator to better capture the speaker conditional embedding. Furthermore, we leverage a multi-scale discriminator to refine the perceptual quality of the extracted speech. Our experiments show that the use of a dual path transformer in the separator backbone along with proposed training paradigm improves the CNN baseline by $3.12$ dB points. Finally, we compare our approach with recent state-of-the-arts and show that our model outperforms existing methods by $4.1$ dB points on an average without creating additional data dependency.|\n", "2409.01193": "|**2024-09-02**|[CLIBE: Detecting Dynamic Backdoors in Transformer-based NLP Models](http://arxiv.org/abs/2409.01193)|null|Backdoors can be injected into NLP models to induce misbehavior when the input text contains a specific feature, known as a trigger, which the attacker secretly selects. Unlike fixed words, phrases, or sentences used in the static text trigger, NLP dynamic backdoor attacks design triggers associated with abstract and latent text features, making them considerably stealthier than traditional static backdoor attacks. However, existing research on NLP backdoor detection primarily focuses on defending against static backdoor attacks, while detecting dynamic backdoors in NLP models remains largely unexplored. This paper presents CLIBE, the first framework to detect dynamic backdoors in Transformer-based NLP models. CLIBE injects a \"few-shot perturbation\" into the suspect Transformer model by crafting optimized weight perturbation in the attention layers to make the perturbed model classify a limited number of reference samples as a target label. Subsequently, CLIBE leverages the generalization ability of this few-shot perturbation to determine whether the original model contains a dynamic backdoor. Extensive evaluation on three advanced NLP dynamic backdoor attacks, two widely-used Transformer frameworks, and four real-world classification tasks strongly validates the effectiveness of CLIBE. We also demonstrate the robustness of CLIBE against various adaptive attacks. Furthermore, we employ CLIBE to scrutinize 49 popular Transformer models on Hugging Face and discover one exhibiting a high probability of containing a dynamic backdoor. We have contacted Hugging Face and provided detailed evidence of this model's backdoor behavior. Moreover, we extend CLIBE to detect backdoor text generation models modified to exhibit toxic behavior. To the best of our knowledge, CLIBE is the first framework capable of detecting backdoors in text generation models without access to trigger input test samples.|\n", "2409.01068": "|**2024-09-02**|[Progressive Retinal Image Registration via Global and Local Deformable Transformations](http://arxiv.org/abs/2409.01068)|**[link](https://github.com/lyp-deeplearning/awesome-retinal-registration)**|Retinal image registration plays an important role in the ophthalmological diagnosis process. Since there exist variances in viewing angles and anatomical structures across different retinal images, keypoint-based approaches become the mainstream methods for retinal image registration thanks to their robustness and low latency. These methods typically assume the retinal surfaces are planar, and adopt feature matching to obtain the homography matrix that represents the global transformation between images. Yet, such a planar hypothesis inevitably introduces registration errors since retinal surface is approximately curved. This limitation is more prominent when registering image pairs with significant differences in viewing angles. To address this problem, we propose a hybrid registration framework called HybridRetina, which progressively registers retinal images with global and local deformable transformations. For that, we use a keypoint detector and a deformation network called GAMorph to estimate the global transformation and local deformable transformation, respectively. Specifically, we integrate multi-level pixel relation knowledge to guide the training of GAMorph. Additionally, we utilize an edge attention module that includes the geometric priors of the images, ensuring the deformation field focuses more on the vascular regions of clinical interest. Experiments on two widely-used datasets, FIRE and FLoRI21, show that our proposed HybridRetina significantly outperforms some state-of-the-art methods. The code is available at https://github.com/lyp-deeplearning/awesome-retinal-registration.|\n", "2409.00904": "|**2024-09-02**|[Multi-scale Temporal Fusion Transformer for Incomplete Vehicle Trajectory Prediction](http://arxiv.org/abs/2409.00904)|null|Motion prediction plays an essential role in autonomous driving systems, enabling autonomous vehicles to achieve more accurate local-path planning and driving decisions based on predictions of the surrounding vehicles. However, existing methods neglect the potential missing values caused by object occlusion, perception failures, etc., which inevitably degrades the trajectory prediction performance in real traffic scenarios. To address this limitation, we propose a novel end-to-end framework for incomplete vehicle trajectory prediction, named Multi-scale Temporal Fusion Transformer (MTFT), which consists of the Multi-scale Attention Head (MAH) and the Continuity Representation-guided Multi-scale Fusion (CRMF) module. Specifically, the MAH leverages the multi-head attention mechanism to parallelly capture multi-scale motion representation of trajectory from different temporal granularities, thus mitigating the adverse effect of missing values on prediction. Furthermore, the multi-scale motion representation is input into the CRMF module for multi-scale fusion to obtain the robust temporal feature of the vehicle. During the fusion process, the continuity representation of vehicle motion is first extracted across time steps to guide the fusion, ensuring that the resulting temporal feature incorporates both detailed information and the overall trend of vehicle motion, which facilitates the accurate decoding of future trajectory that is consistent with the vehicle's motion trend. We evaluate the proposed model on four datasets derived from highway and urban traffic scenarios. The experimental results demonstrate its superior performance in the incomplete vehicle trajectory prediction task compared with state-of-the-art models, e.g., a comprehensive performance improvement of more than 39% on the HighD dataset.|\n", "2409.00591": "|**2024-09-01**|[Attention-Guided Multi-scale Interaction Network for Face Super-Resolution](http://arxiv.org/abs/2409.00591)|null|Recently, CNN and Transformer hybrid networks demonstrated excellent performance in face super-resolution (FSR) tasks. Since numerous features at different scales in hybrid networks, how to fuse these multi-scale features and promote their complementarity is crucial for enhancing FSR. However, existing hybrid network-based FSR methods ignore this, only simply combining the Transformer and CNN. To address this issue, we propose an attention-guided Multi-scale interaction network (AMINet), which contains local and global feature interactions as well as encoder-decoder phases feature interactions. Specifically, we propose a Local and Global Feature Interaction Module (LGFI) to promote fusions of global features and different receptive fields' local features extracted by our Residual Depth Feature Extraction Module (RDFE). Additionally, we propose a Selective Kernel Attention Fusion Module (SKAF) to adaptively select fusions of different features within LGFI and encoder-decoder phases. Our above design allows the free flow of multi-scale features from within modules and between encoder and decoder, which can promote the complementarity of different scale features to enhance FSR. Comprehensive experiments confirm that our method consistently performs well with less computational consumption and faster inference.|\n", "2409.03621": "|**2024-09-05**|[Attend First, Consolidate Later: On the Importance of Attention in Different LLM Layers](http://arxiv.org/abs/2409.03621)|null|In decoder-based LLMs, the representation of a given layer serves two purposes: as input to the next layer during the computation of the current token; and as input to the attention mechanism of future tokens. In this work, we show that the importance of the latter role might be overestimated. To show that, we start by manipulating the representations of previous tokens; e.g. by replacing the hidden states at some layer k with random vectors. Our experimenting with four LLMs and four tasks show that this operation often leads to small to negligible drop in performance. Importantly, this happens if the manipulation occurs in the top part of the model-k is in the final 30-50% of the layers. In contrast, doing the same manipulation in earlier layers might lead to chance level performance. We continue by switching the hidden state of certain tokens with hidden states of other tokens from another prompt; e.g., replacing the word \"Italy\" with \"France\" in \"What is the capital of Italy?\". We find that when applying this switch in the top 1/3 of the model, the model ignores it (answering \"Rome\"). However if we apply it before, the model conforms to the switch (\"Paris\"). Our results hint at a two stage process in transformer-based LLMs: the first part gathers input from previous tokens, while the second mainly processes that information internally.|\n", "2409.03516": "|**2024-09-05**|[LMLT: Low-to-high Multi-Level Vision Transformer for Image Super-Resolution](http://arxiv.org/abs/2409.03516)|**[link](https://github.com/jwgdmkj/lmlt)**|Recent Vision Transformer (ViT)-based methods for Image Super-Resolution have demonstrated impressive performance. However, they suffer from significant complexity, resulting in high inference times and memory usage. Additionally, ViT models using Window Self-Attention (WSA) face challenges in processing regions outside their windows. To address these issues, we propose the Low-to-high Multi-Level Transformer (LMLT), which employs attention with varying feature sizes for each head. LMLT divides image features along the channel dimension, gradually reduces spatial size for lower heads, and applies self-attention to each head. This approach effectively captures both local and global information. By integrating the results from lower heads into higher heads, LMLT overcomes the window boundary issues in self-attention. Extensive experiments show that our model significantly reduces inference time and GPU memory usage while maintaining or even surpassing the performance of state-of-the-art ViT-based Image Super-Resolution methods. Our codes are availiable at https://github.com/jwgdmkj/LMLT.|\n", "2409.03514": "|**2024-09-05**|[Blended Latent Diffusion under Attention Control for Real-World Video Editing](http://arxiv.org/abs/2409.03514)|null|\u7531\u4e8e\u7f3a\u4e4f\u5b8c\u5168\u516c\u5f00\u53ef\u7528\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\uff0c\u5f53\u524d\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u503e\u5411\u4e8e\u5efa\u7acb\u5728\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e4b\u4e0a\uff0c\u7136\u800c\uff0c\u5b83\u4eec\u5728\u5904\u7406\u5177\u6709\u65f6\u95f4\u4fe1\u606f\u7684\u89c6\u9891\u5c40\u90e8\u7f16\u8f91\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u9996\u5148\uff0c\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u8bd5\u56fe\u901a\u8fc7\u9884\u5b9a\u4e49\u7684\u63a9\u7801\u4e13\u6ce8\u4e8e\u5c40\u90e8\u533a\u57df\u7f16\u8f91\uff0c\u4f46\u7531\u4e8e\u6bcf\u4e00\u5e27\u7684\u7a7a\u95f4\u6574\u4f53\u751f\u6210\uff0c\u533a\u57df\u5916\u80cc\u666f\u7684\u4fdd\u7559\u5e76\u4e0d\u7406\u60f3\u3002\u6b64\u5916\uff0c\u7528\u6237\u4e13\u95e8\u63d0\u4f9b\u63a9\u7801\u662f\u4e00\u9879\u989d\u5916\u7684\u6602\u8d35\u5de5\u4f5c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u96c6\u6210\u5230\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u7684\u81ea\u4e3b\u63a9\u7801\u7b56\u7565\u3002\u6700\u540e\u4f46\u540c\u6837\u91cd\u8981\u7684\u662f\uff0c\u56fe\u50cf\u7ea7\u9884\u8bad\u7ec3\u6a21\u578b\u6ca1\u6709\u5b66\u4e60\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u800c\u8fd9\u5bf9\u4e8e\u8868\u8fbe\u8fd0\u52a8\u548c\u52a8\u6001\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u91c7\u7528\u56fe\u50cf\u7ea7\u6df7\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6765\u6267\u884c\u5c40\u90e8\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5229\u7528 DDIM \u53cd\u6f14\u6765\u83b7\u53d6\u6f5c\u5728\u4ee3\u7801\u4f5c\u4e3a\u80cc\u666f\u6f5c\u5728\u4ee3\u7801\uff0c\u800c\u4e0d\u662f\u968f\u673a\u566a\u58f0\u7684\u6f5c\u5728\u4ee3\u7801\uff0c\u4ee5\u66f4\u597d\u5730\u4fdd\u7559\u8f93\u5165\u89c6\u9891\u7684\u80cc\u666f\u4fe1\u606f\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4ece\u6269\u6563\u6b65\u9aa4\u4e2d\u7684\u4ea4\u53c9\u6ce8\u610f\u56fe\u6d3e\u751f\u7684\u81ea\u4e3b\u63a9\u7801\u5236\u9020\u673a\u5236\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u5c06 U-Net \u7684\u81ea\u6ce8\u610f\u529b\u5757\u8f6c\u6362\u4e3a\u65f6\u7a7a\u5757\u6765\u589e\u5f3a\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u6709\u6548\u6027\u3002|\n", "2409.03463": "|**2024-09-05**|[Characterizing Massive Activations of Attention Mechanism in Graph Neural Networks](http://arxiv.org/abs/2409.03463)|**[link](https://github.com/msorbi/gnn-ma)**|Graph Neural Networks (GNNs) have become increasingly popular for effectively modeling data with graph structures. Recently, attention mechanisms have been integrated into GNNs to improve their ability to capture complex patterns. This paper presents the first comprehensive study revealing a critical, unexplored consequence of this integration: the emergence of Massive Activations (MAs) within attention layers. We introduce a novel method for detecting and analyzing MAs, focusing on edge features in different graph transformer architectures. Our study assesses various GNN models using benchmark datasets, including ZINC, TOX21, and PROTEINS. Key contributions include (1) establishing the direct link between attention mechanisms and MAs generation in GNNs, (2) developing a robust definition and detection method for MAs based on activation ratio distributions, (3) introducing the Explicit Bias Term (EBT) as a potential countermeasure and exploring it as an adversarial framework to assess models robustness based on the presence or absence of MAs. Our findings highlight the prevalence and impact of attention-induced MAs across different architectures, such as GraphTransformer, GraphiT, and SAN. The study reveals the complex interplay between attention mechanisms, model architecture, dataset characteristics, and MAs emergence, providing crucial insights for developing more robust and reliable graph models.|\n", "2409.03460": "|**2024-09-05**|[LowFormer: Hardware Efficient Design for Convolutional Transformer Backbones](http://arxiv.org/abs/2409.03460)|**[link](https://github.com/altair199797/lowformer)**|Research in efficient vision backbones is evolving into models that are a mixture of convolutions and transformer blocks. A smart combination of both, architecture-wise and component-wise is mandatory to excel in the speedaccuracy trade-off. Most publications focus on maximizing accuracy and utilize MACs (multiply accumulate operations) as an efficiency metric. The latter however often do not measure accurately how fast a model actually is due to factors like memory access cost and degree of parallelism. We analyzed common modules and architectural design choices for backbones not in terms of MACs, but rather in actual throughput and latency, as the combination of the latter two is a better representation of the efficiency of models in real applications. We applied the conclusions taken from that analysis to create a recipe for increasing hardware-efficiency in macro design. Additionally we introduce a simple slimmed-down version of MultiHead Self-Attention, that aligns with our analysis. We combine both macro and micro design to create a new family of hardware-efficient backbone networks called LowFormer. LowFormer achieves a remarkable speedup in terms of throughput and latency, while achieving similar or better accuracy than current state-of-the-art efficient backbones. In order to prove the generalizability of our hardware-efficient design, we evaluate our method on GPU, mobile GPU and ARM CPU. We further show that the downstream tasks object detection and semantic segmentation profit from our hardware-efficient architecture. Code and models are available at https://github.com/ altair199797/LowFormer.|\n", "2409.03332": "|**2024-09-05**|[Masked Sensory-Temporal Attention for Sensor Generalization in Quadruped Locomotion](http://arxiv.org/abs/2409.03332)|null|With the rising focus on quadrupeds, a generalized policy capable of handling different robot models and sensory inputs will be highly beneficial. Although several methods have been proposed to address different morphologies, it remains a challenge for learning-based policies to manage various combinations of proprioceptive information. This paper presents Masked Sensory-Temporal Attention (MSTA), a novel transformer-based model with masking for quadruped locomotion. It employs direct sensor-level attention to enhance sensory-temporal understanding and handle different combinations of sensor data, serving as a foundation for incorporating unseen information. This model can effectively understand its states even with a large portion of missing information, and is flexible enough to be deployed on a physical system despite the long input sequence.|\n", "2409.03223": "|**2024-09-05**|[Why mamba is effective? Exploit Linear Transformer-Mamba Network for Multi-Modality Image Fusion](http://arxiv.org/abs/2409.03223)|null|Multi-modality image fusion aims to integrate the merits of images from different sources and render high-quality fusion images. However, existing feature extraction and fusion methods are either constrained by inherent local reduction bias and static parameters during inference (CNN) or limited by quadratic computational complexity (Transformers), and cannot effectively extract and fuse features. To solve this problem, we propose a dual-branch image fusion network called Tmamba. It consists of linear Transformer and Mamba, which has global modeling capabilities while maintaining linear complexity. Due to the difference between the Transformer and Mamba structures, the features extracted by the two branches carry channel and position information respectively. T-M interaction structure is designed between the two branches, using global learnable parameters and convolutional layers to transfer position and channel information respectively. We further propose cross-modal interaction at the attention level to obtain cross-modal attention. Experiments show that our Tmamba achieves promising results in multiple fusion tasks, including infrared-visible image fusion and medical image fusion. Code with checkpoints will be available after the peer-review process.|\n", "2409.03115": "|**2024-09-04**|[Probing self-attention in self-supervised speech models for cross-linguistic differences](http://arxiv.org/abs/2409.03115)|null|Speech models have gained traction thanks to increase in accuracy from novel transformer architectures. While this impressive increase in performance across automatic speech recognition (ASR) benchmarks is noteworthy, there is still much that is unknown about the use of attention mechanisms for speech-related tasks. For example, while it is assumed that these models are learning language-independent (i.e., universal) speech representations, there has not yet been an in-depth exploration of what it would mean for the models to be language-independent. In the current paper, we explore this question within the realm of self-attention mechanisms of one small self-supervised speech transformer model (TERA). We find that even with a small model, the attention heads learned are diverse ranging from almost entirely diagonal to almost entirely global regardless of the training language. We highlight some notable differences in attention patterns between Turkish and English and demonstrate that the models do learn important phonological information during pretraining. We also present a head ablation study which shows that models across languages primarily rely on diagonal heads to classify phonemes.|\n", "2409.03103": "|**2024-09-04**|[Leveraging Interpretability in the Transformer to Automate the Proactive Scaling of Cloud Resources](http://arxiv.org/abs/2409.03103)|null|\u73b0\u4ee3Web\u670d\u52a1\u91c7\u7528\u4e91\u539f\u751f\u539f\u5219\u6765\u5229\u7528\u5fae\u670d\u52a1\u7684\u4f18\u52bf\u3002\u4e3a\u4e86\u6839\u636e\u670d\u52a1\u7b49\u7ea7\u534f\u8bae\uff08SLA\uff09\u6301\u7eed\u4fdd\u8bc1\u9ad8\u8d28\u91cf\u7684\u670d\u52a1\uff08QoS\uff09\uff0c\u786e\u4fdd\u4ee4\u4eba\u6ee1\u610f\u7684\u7528\u6237\u4f53\u9a8c\u5e76\u6700\u5927\u7a0b\u5ea6\u5730\u964d\u4f4e\u8fd0\u8425\u6210\u672c\uff0c\u5fc5\u987b\u4e3a\u6bcf\u4e2a\u5fae\u670d\u52a1\u914d\u7f6e\u9002\u91cf\u7684\u8d44\u6e90\u3002\u7136\u800c\uff0c\u51c6\u786e\u5730\u4e3a\u5fae\u670d\u52a1\u914d\u7f6e\u5145\u8db3\u7684\u8d44\u6e90\u975e\u5e38\u590d\u6742\uff0c\u5e76\u4e14\u53d6\u51b3\u4e8e\u8bb8\u591a\u56e0\u7d20\uff0c\u5305\u62ec\u5de5\u4f5c\u8d1f\u8f7d\u5f3a\u5ea6\u548c\u5fae\u670d\u52a1\u4e4b\u95f4\u590d\u6742\u7684\u4e92\u8fde\u5173\u7cfb\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u6355\u83b7\u4e86\u7aef\u5230\u7aef\u5ef6\u8fdf\u3001\u524d\u7aef\u7ea7\u522b\u7684\u8bf7\u6c42\u548c\u8d44\u6e90\u5229\u7528\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u5f00\u53d1\u7684\u6a21\u578b\u6765\u9884\u6d4b\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002\u6211\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u5229\u7528\u4e86\u65f6\u95f4\u878d\u5408Transformer\uff08TFT\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5177\u6709\u53ef\u89e3\u91ca\u6027\u7279\u5f81\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u67b6\u6784\u3002\u5f53\u9884\u6d4b\u7ed3\u679c\u8868\u660e\u4e0d\u7b26\u5408SLA\u65f6\uff0c\u6211\u4eec\u4f7f\u7528TFT\u63d0\u4f9b\u7684\u7279\u5f81\u91cd\u8981\u6027\u4f5c\u4e3a\u6838\u5cad\u56de\u5f52\uff08KRR\uff09\u4e2d\u7684\u534f\u53d8\u91cf\uff0c\u5e76\u5c06\u54cd\u5e94\u53d8\u91cf\u8bbe\u7f6e\u4e3a\u671f\u671b\u5ef6\u8fdf\uff0c\u4ee5\u5b66\u4e60\u4e0e\u7279\u5f81\u91cd\u8981\u6027\u76f8\u5173\u7684\u53c2\u6570\u3002\u8fd9\u4e9b\u5b66\u4e60\u5230\u7684\u53c2\u6570\u53cd\u6620\u4e86\u4e3a\u786e\u4fdd\u7b26\u5408SLA\u800c\u9700\u8981\u5bf9\u7279\u5f81\u8fdb\u884c\u7684\u8c03\u6574\u3002\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u57fa\u4e8e\u5fae\u670d\u52a1\u7684\u5e94\u7528\u7a0b\u5e8f\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u90e8\u7f72\u8def\u7ebf\u56fe\u3002|\n"}}