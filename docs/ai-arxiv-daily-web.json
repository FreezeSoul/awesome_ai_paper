{"\u591a\u6a21\u6001": {"2409.02914": "|**2024-09-04**|[Can LVLMs Obtain a Driver's License? A Benchmark Towards Reliable AGI for Autonomous Driving](http://arxiv.org/abs/2409.02914)|null|\u8fd1\u5e74\u6765\uff0c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\uff0c\u8bb8\u591a\u7814\u7a76\u81f4\u529b\u4e8e\u5229\u7528\u5176\u901a\u7528\u77e5\u8bc6\u6765\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002\u7136\u800c\uff0cLVLM \u901a\u5e38\u4f9d\u8d56\u4e8e\u5927\u578b\u901a\u7528\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u4e13\u4e1a\u5b89\u5168\u9a7e\u9a76\u6240\u9700\u7684\u4e13\u4e1a\u77e5\u8bc6\u3002\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u9a7e\u9a76\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u573a\u666f\u7406\u89e3\u548c\u51b3\u7b56\uff0c\u6ca1\u6709\u63d0\u4f9b\u4e0e\u9a7e\u9a76\u5b89\u5168\u76f4\u63a5\u76f8\u5173\u7684\u4ea4\u901a\u89c4\u5219\u548c\u9a7e\u9a76\u6280\u80fd\u65b9\u9762\u7684\u660e\u786e\u6307\u5bfc\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 IDKB\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b\u6765\u81ea\u591a\u4e2a\u56fd\u5bb6\u7684\u4e00\u767e\u591a\u4e07\u4e2a\u6570\u636e\u9879\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u62ec\u9a7e\u9a76\u624b\u518c\u3001\u7406\u8bba\u6d4b\u8bd5\u6570\u636e\u548c\u6a21\u62df\u9053\u8def\u6d4b\u8bd5\u6570\u636e\u3002\u5c31\u50cf\u83b7\u5f97\u9a7e\u9a76\u6267\u7167\u7684\u8fc7\u7a0b\u4e00\u6837\uff0cIDKB \u6db5\u76d6\u4e86\u4ece\u7406\u8bba\u5230\u5b9e\u8df5\u9a7e\u9a76\u6240\u9700\u7684\u51e0\u4e4e\u6240\u6709\u663e\u6027\u77e5\u8bc6\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u4f7f\u7528 IDKB \u5bf9 15 \u4e2a LVLM \u8fdb\u884c\u4e86\u5168\u9762\u6d4b\u8bd5\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u7684\u5206\u6790\u3002\u6211\u4eec\u8fd8\u5fae\u8c03\u4e86\u6d41\u884c\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8fd9\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6211\u4eec\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027\u3002\u9879\u76ee\u9875\u9762\u53ef\u4ee5\u5728\u4ee5\u4e0b\u7f51\u5740\u627e\u5230\uff1a\\url{https://4dvlab.github.io/project_page/idkb.html}|\n", "2409.02882": "|**2024-09-04**|[Benchmarking Spurious Bias in Few-Shot Image Classifiers](http://arxiv.org/abs/2409.02882)|**[link](https://github.com/gtzheng/fewstab)**|\u5c11\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u5668\u65e8\u5728\u7528\u6700\u5c11\u7684\u76d1\u7763\u548c\u6709\u9650\u7684\u6570\u636e\u8bc6\u522b\u548c\u5206\u7c7b\u65b0\u6570\u636e\uff0c\u4f46\u901a\u5e38\u8868\u73b0\u51fa\u5bf9\u7c7b\u548c\u865a\u5047\u5c5e\u6027\u4e4b\u95f4\u865a\u5047\u76f8\u5173\u6027\u7684\u4f9d\u8d56\uff0c\u8fd9\u88ab\u79f0\u4e3a\u865a\u5047\u504f\u5dee\u3002\u865a\u5047\u76f8\u5173\u6027\u901a\u5e38\u5b58\u5728\u4e8e\u67d0\u4e9b\u6837\u672c\u4e2d\uff0c\u800c\u5c11\u6837\u672c\u5206\u7c7b\u5668\u53ef\u80fd\u4f1a\u53d7\u5230\u7531\u5b83\u4eec\u5f15\u8d77\u7684\u865a\u5047\u504f\u5dee\u7684\u5f71\u54cd\u3002\u76ee\u524d\u7f3a\u4e4f\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u7cfb\u7edf\u6765\u8bc4\u4f30\u5c11\u6837\u672c\u5206\u7c7b\u5668\u9488\u5bf9\u865a\u5047\u504f\u5dee\u7684\u9c81\u68d2\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u4e14\u4e25\u683c\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u79f0\u4e3a FewSTAB\uff0c\u4ee5\u516c\u5e73\u5730\u5c55\u793a\u548c\u91cf\u5316\u5c11\u6837\u672c\u5206\u7c7b\u5668\u5bf9\u865a\u5047\u504f\u5dee\u7684\u4e0d\u540c\u7a0b\u5ea6\u7684\u9c81\u68d2\u6027\u3002FewSTAB \u521b\u5efa\u4e86\u5177\u6709\u504f\u5dee\u5c5e\u6027\u7684\u5c11\u6837\u672c\u8bc4\u4f30\u4efb\u52a1\uff0c\u56e0\u6b64\u4f7f\u7528\u5b83\u4eec\u8fdb\u884c\u9884\u6d4b\u53ef\u4ee5\u8bc1\u660e\u6027\u80fd\u4e0d\u4f73\u3002\u4e3a\u4e86\u6784\u5efa\u8fd9\u4e9b\u4efb\u52a1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u4e8e\u5c5e\u6027\u7684\u6837\u672c\u9009\u62e9\u7b56\u7565\uff0c\u4ece\u800c\u65e0\u9700\u624b\u52a8\u8fdb\u884c\u6570\u636e\u96c6\u7ba1\u7406\u3002\u8fd9\u4f7f\u5f97 FewSTAB \u53ef\u4ee5\u4f7f\u7528\u4efb\u4f55\u73b0\u6709\u7684\u6d4b\u8bd5\u6570\u636e\u81ea\u52a8\u5bf9\u865a\u5047\u504f\u5dee\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002FewSTAB \u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7ef4\u5ea6\u7684\u8bc4\u4f30\u7ed3\u679c\uff0c\u4ee5\u53ca\u6784\u5efa\u9c81\u68d2\u5206\u7c7b\u5668\u7684\u65b0\u8bbe\u8ba1\u6307\u5357\u3002\u6b64\u5916\uff0c\u5b83\u53ef\u4ee5\u5bf9\u4e0d\u540c\u7a0b\u5ea6\u7684\u865a\u5047\u504f\u5dee\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u652f\u6301\u9488\u5bf9\u4e0d\u540c\u7a0b\u5ea6\u7684\u9c81\u68d2\u6027\u8fdb\u884c\u8bbe\u8ba1\u3002\u901a\u8fc7\u5bf9\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5341\u79cd\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002\u6211\u4eec\u5e0c\u671b\u6211\u4eec\u7684\u6846\u67b6\u80fd\u591f\u542f\u53d1\u9c81\u68d2\u7684\u5c11\u6837\u672c\u5206\u7c7b\u5668\u7684\u65b0\u8bbe\u8ba1\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/gtzheng/FewSTAB \u83b7\u53d6\u3002|\n", "2409.02834": "|**2024-09-04**|[CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of Large Multimodal Models](http://arxiv.org/abs/2409.02834)|null|Large language models (LLMs) have obtained promising results in mathematical reasoning, which is a foundational skill for human intelligence. Most previous studies focus on improving and measuring the performance of LLMs based on textual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few researchers have released English multimodal math datasets (e.g., MATHVISTA and MATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In this paper, we release a Chinese multimodal math (CMM-Math) dataset, including benchmark and training parts, to evaluate and enhance the mathematical reasoning of LMMs. CMM-Math contains over 28,000 high-quality samples, featuring a variety of problem types (e.g., multiple-choice, fill-in-the-blank, and so on) with detailed solutions across 12 grade levels from elementary to high school in China. Specifically, the visual context may be present in the questions or opinions, which makes this dataset more challenging. Through comprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math dataset face challenges, emphasizing the necessity for further improvements in LMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to handle the problems with mixed input of multiple images and text segments. We train our model using three stages, including foundational pre-training, foundational fine-tuning, and mathematical fine-tuning. The extensive experiments indicate that our model effectively improves math reasoning performance by comparing it with the SOTA LMMs over three multimodal mathematical datasets.|\n", "2409.02813": "|**2024-09-04**|[MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark](http://arxiv.org/abs/2409.02813)|null|This paper introduces MMMU-Pro, a robust version of the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark. MMMU-Pro rigorously assesses multimodal models' true understanding and reasoning capabilities through a three-step process based on MMMU: (1) filtering out questions answerable by text-only models, (2) augmenting candidate options, and (3) introducing a vision-only input setting where questions are embedded within images. This setting challenges AI to truly \"see\" and \"read\" simultaneously, testing a fundamental human cognitive skill of seamlessly integrating visual and textual information. Results show that model performance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8% to 26.9% across models. We explore the impact of OCR prompts and Chain of Thought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT generally improves performance. MMMU-Pro provides a more rigorous evaluation tool, closely mimicking real-world scenarios and offering valuable directions for future research in multimodal AI.|\n", "2409.02664": "|**2024-09-04**|[Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection](http://arxiv.org/abs/2409.02664)|null|The proliferation of deepfake faces poses huge potential negative impacts on our daily lives. Despite substantial advancements in deepfake detection over these years, the generalizability of existing methods against forgeries from unseen datasets or created by emerging generative models remains constrained. In this paper, inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach that repurposes a well-trained VLM for general deepfake detection. Motivated by the model reprogramming paradigm that manipulates the model prediction via data perturbations, our method can reprogram a pretrained VLM model (e.g., CLIP) solely based on manipulating its input without tuning the inner parameters. Furthermore, we insert a pseudo-word guided by facial identity into the text prompt. Extensive experiments on several popular benchmarks demonstrate that (1) the cross-dataset and cross-manipulation performances of deepfake detection can be significantly and consistently improved (e.g., over 88% AUC in cross-dataset setting from FF++ to WildDeepfake) using a pre-trained CLIP model with our proposed reprogramming method; (2) our superior performances are at less cost of trainable parameters, making it a promising approach for real-world applications.|\n", "2409.02530": "|**2024-09-04**|[Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models](http://arxiv.org/abs/2409.02530)|null|The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of kidney function in clinical practice. Although traditional equations and Machine Learning (ML) models using clinical and laboratory data can estimate eGFR, accurately predicting future eGFR levels remains a significant challenge for nephrologists and ML researchers. Recent advances demonstrate that Large Language Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust foundation models for diverse applications. This study investigates the potential of LMMs to predict future eGFR levels with a dataset consisting of laboratory and clinical values from 50 patients. By integrating various prompting techniques and ensembles of LMMs, our findings suggest that these models, when combined with precise prompts and visual representations of eGFR trajectories, offer predictive performance comparable to existing ML models. This research extends the application of foundation models and suggests avenues for future studies to harness these models in addressing complex medical forecasting challenges.|\n", "2409.02278": "|**2024-09-03**|[Evaluation and Comparison of Visual Language Models for Transportation Engineering Problems](http://arxiv.org/abs/2409.02278)|null|Recent developments in vision language models (VLM) have shown great potential for diverse applications related to image understanding. In this study, we have explored state-of-the-art VLM models for vision-based transportation engineering tasks such as image classification and object detection. The image classification task involves congestion detection and crack identification, whereas, for object detection, helmet violations were identified. We have applied open-source models such as CLIP, BLIP, OWL-ViT, Llava-Next, and closed-source GPT-4o to evaluate the performance of these state-of-the-art VLM models to harness the capabilities of language understanding for vision-based transportation tasks. These tasks were performed by applying zero-shot prompting to the VLM models, as zero-shot prompting involves performing tasks without any training on those tasks. It eliminates the need for annotated datasets or fine-tuning for specific tasks. Though these models gave comparative results with benchmark Convolutional Neural Networks (CNN) models in the image classification tasks, for object localization tasks, it still needs improvement. Therefore, this study provides a comprehensive evaluation of the state-of-the-art VLM models highlighting the advantages and limitations of the models, which can be taken as the baseline for future improvement and wide-scale implementation.|\n", "2409.02253": "|**2024-09-03**|[How to Determine the Preferred Image Distribution of a Black-Box Vision-Language Model?](http://arxiv.org/abs/2409.02253)|null|Large foundation models have revolutionized the field, yet challenges remain in optimizing multi-modal models for specialized visual tasks. We propose a novel, generalizable methodology to identify preferred image distributions for black-box Vision-Language Models (VLMs) by measuring output consistency across varied input prompts. Applying this to different rendering types of 3D objects, we demonstrate its efficacy across various domains requiring precise interpretation of complex structures, with a focus on Computer-Aided Design (CAD) as an exemplar field. We further refine VLM outputs using in-context learning with human feedback, significantly enhancing explanation quality. To address the lack of benchmarks in specialized domains, we introduce CAD-VQA, a new dataset for evaluating VLMs on CAD-related visual question answering tasks. Our evaluation of state-of-the-art VLMs on CAD-VQA establishes baseline performance levels, providing a framework for advancing VLM capabilities in complex visual reasoning tasks across various fields requiring expert-level visual interpretation. We release the dataset and evaluation codes at \\url{https://github.com/asgsaeid/cad_vqa}.|\n", "2409.02101": "|**2024-09-03**|[Towards Real-World Adverse Weather Image Restoration: Enhancing Clearness and Semantics with Vision-Language Models](http://arxiv.org/abs/2409.02101)|null|This paper addresses the limitations of adverse weather image restoration approaches trained on synthetic data when applied to real-world scenarios. We formulate a semi-supervised learning framework employing vision-language models to enhance restoration performance across diverse adverse weather conditions in real-world settings. Our approach involves assessing image clearness and providing semantics using vision-language models on real data, serving as supervision signals for training restoration models. For clearness enhancement, we use real-world data, utilizing a dual-step strategy with pseudo-labels assessed by vision-language models and weather prompt learning. For semantic enhancement, we integrate real-world data by adjusting weather conditions in vision-language model descriptions while preserving semantic meaning. Additionally, we introduce an effective training strategy to bootstrap restoration performance. Our approach achieves superior results in real-world adverse weather image restoration, demonstrated through qualitative and quantitative comparisons with state-of-the-art works.|\n", "2409.02084": "|**2024-09-03**|[GraspSplats: Efficient Manipulation with 3D Feature Splatting](http://arxiv.org/abs/2409.02084)|null|The ability for robots to perform efficient and zero-shot grasping of object parts is crucial for practical applications and is becoming prevalent with recent advances in Vision-Language Models (VLMs). To bridge the 2D-to-3D gap for representations to support such a capability, existing methods rely on neural fields (NeRFs) via differentiable rendering or point-based projection methods. However, we demonstrate that NeRFs are inappropriate for scene changes due to their implicitness and point-based methods are inaccurate for part localization without rendering-based optimization. To amend these issues, we propose GraspSplats. Using depth supervision and a novel reference feature computation method, GraspSplats generates high-quality scene representations in under 60 seconds. We further validate the advantages of Gaussian-based representation by showing that the explicit and optimized geometry in GraspSplats is sufficient to natively support (1) real-time grasp sampling and (2) dynamic and articulated object manipulation with point trackers. With extensive experiments on a Franka robot, we demonstrate that GraspSplats significantly outperforms existing methods under diverse task settings. In particular, GraspSplats outperforms NeRF-based methods like F3RM and LERF-TOGO, and 2D detection methods.|\n"}, "6DOF Object Pose": {"2409.02581": "|**2024-09-04**|[Object Gaussian for Monocular 6D Pose Estimation from Sparse Views](http://arxiv.org/abs/2409.02581)|null|Monocular object pose estimation, as a pivotal task in computer vision and robotics, heavily depends on accurate 2D-3D correspondences, which often demand costly CAD models that may not be readily available. Object 3D reconstruction methods offer an alternative, among which recent advancements in 3D Gaussian Splatting (3DGS) afford a compelling potential. Yet its performance still suffers and tends to overfit with fewer input views. Embracing this challenge, we introduce SGPose, a novel framework for sparse view object pose estimation using Gaussian-based methods. Given as few as ten views, SGPose generates a geometric-aware representation by starting with a random cuboid initialization, eschewing reliance on Structure-from-Motion (SfM) pipeline-derived geometry as required by traditional 3DGS methods. SGPose removes the dependence on CAD models by regressing dense 2D-3D correspondences between images and the reconstructed model from sparse input and random initialization, while the geometric-consistent depth supervision and online synthetic view warping are key to the success. Experiments on typical benchmarks, especially on the Occlusion LM-O dataset, demonstrate that SGPose outperforms existing methods even under sparse view constraints, under-scoring its potential in real-world applications.|\n", "2408.16547": "|**2024-08-29**|[OP-Align: Object-level and Part-level Alignment for Self-supervised Category-level Articulated Object Pose Estimation](http://arxiv.org/abs/2408.16547)|**[link](https://github.com/yc-che/op-align)**|\u7c7b\u522b\u7ea7\u94f0\u63a5\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4fa7\u91cd\u4e8e\u4f30\u8ba1\u5df2\u77e5\u7c7b\u522b\u4e2d\u672a\u77e5\u94f0\u63a5\u7269\u4f53\u7684\u59ff\u6001\u3002\u5c3d\u7ba1\u610f\u4e49\u91cd\u5927\uff0c\u4f46\u7531\u4e8e\u7269\u4f53\u7684\u5f62\u72b6\u548c\u59ff\u6001\u5404\u4e0d\u76f8\u540c\u3001\u6570\u636e\u96c6\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u4ee5\u53ca\u73b0\u5b9e\u73af\u5883\u590d\u6742\uff0c\u8fd9\u9879\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u5229\u7528\u5355\u5e27\u70b9\u4e91\u6765\u89e3\u51b3\u6b64\u4efb\u52a1\u3002\u6211\u4eec\u7684\u6a21\u578b\u59cb\u7ec8\u5982\u4e00\u5730\u751f\u6210\u5177\u6709\u89c4\u8303\u59ff\u6001\u548c\u5173\u8282\u72b6\u6001\u7684\u5b8c\u6574\u8f93\u5165\u5bf9\u8c61\u7684\u91cd\u5efa\uff0c\u5e76\u4f30\u8ba1\u5bf9\u8c61\u7ea7\u59ff\u6001\uff08\u51cf\u5c11\u6574\u4f53\u59ff\u6001\u5dee\u5f02\uff09\u548c\u96f6\u4ef6\u7ea7\u59ff\u6001\uff08\u5c06\u8f93\u5165\u7684\u6bcf\u4e2a\u96f6\u4ef6\u4e0e\u5176\u5bf9\u5e94\u96f6\u4ef6\u5bf9\u9f50\uff09\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u4ee5\u5f80\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u4e14\u4e0e\u6700\u5148\u8fdb\u7684\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u3002\u4e3a\u4e86\u8bc4\u4f30\u6211\u4eec\u7684\u6a21\u578b\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u73b0\u5b9e\u4e16\u754c\u94f0\u63a5\u7269\u4f53\u57fa\u51c6\u6570\u636e\u96c6\u3002|\n", "2408.10450": "|**2024-08-19**|[RUMI: Rummaging Using Mutual Information](http://arxiv.org/abs/2408.10450)|null|\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u7ffb\u627e\u65b9\u6cd5 (RUMI)\uff0c\u8be5\u65b9\u6cd5\u7528\u4e8e\u5728\u7ebf\u751f\u6210\u673a\u5668\u4eba\u5728\u89c6\u89c9\u906e\u6321\u73af\u5883\u4e2d\u6536\u96c6\u5df2\u77e5\u53ef\u79fb\u52a8\u7269\u4f53\u59ff\u6001\u4fe1\u606f\u7684\u52a8\u4f5c\u5e8f\u5217\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u5bcc\u63a5\u89e6\u5f0f\u7ffb\u627e\uff0c\u5229\u7528\u7269\u4f53\u59ff\u6001\u5206\u5e03\u548c\u673a\u5668\u4eba\u8f68\u8ff9\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u8fdb\u884c\u52a8\u4f5c\u89c4\u5212\u3002RUMI \u4ece\u89c2\u5bdf\u5230\u7684\u90e8\u5206\u70b9\u4e91\u63a8\u65ad\u51fa\u517c\u5bb9\u7684\u7269\u4f53\u59ff\u6001\u5206\u5e03\uff0c\u5e76\u5b9e\u65f6\u8ba1\u7b97\u5176\u4e0e\u5de5\u4f5c\u7a7a\u95f4\u5360\u7528\u7387\u7684\u4e92\u4fe1\u606f\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u4fe1\u606f\u589e\u76ca\u6210\u672c\u51fd\u6570\u548c\u53ef\u8fbe\u6027\u6210\u672c\u51fd\u6570\uff0c\u4ee5\u5c06\u7269\u4f53\u4fdd\u6301\u5728\u673a\u5668\u4eba\u7684\u53ef\u53ca\u8303\u56f4\u5185\u3002\u8fd9\u4e9b\u51fd\u6570\u88ab\u96c6\u6210\u5230\u5177\u6709\u968f\u673a\u52a8\u529b\u5b66\u6a21\u578b\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236 (MPC) \u6846\u67b6\u4e2d\uff0c\u5728\u95ed\u73af\u4e2d\u66f4\u65b0\u59ff\u6001\u5206\u5e03\u3002\u4e3b\u8981\u8d21\u732e\u5305\u62ec\u4e00\u79cd\u65b0\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7f6e\u4fe1\u6846\u67b6\u3001\u4e00\u79cd\u9ad8\u6548\u7684\u4fe1\u606f\u589e\u76ca\u8ba1\u7b97\u7b56\u7565\u4ee5\u53ca\u4e00\u79cd\u9c81\u68d2\u7684\u57fa\u4e8e MPC \u7684\u63a7\u5236\u65b9\u6848\u3002\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cRUMI \u5728\u6a21\u62df\u548c\u771f\u5b9e\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002|\n", "2408.08234": "|**2024-08-15**|[Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation](http://arxiv.org/abs/2408.08234)|**[link](https://github.com/varunburde/reconstruction_pose_benchmark)**|\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u5bf9\u4e8e\u8bb8\u591a\u6d89\u53ca\u673a\u5668\u4eba\u64cd\u4f5c\u3001\u5bfc\u822a\u548c\u589e\u5f3a\u73b0\u5b9e\u7684\u5de5\u4e1a\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u901a\u7528\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u5668\uff0c\u5373\u4e0d\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u7269\u4f53\u8fdb\u884c\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u4f9d\u8d56\u4e8e\u7cbe\u786e\u7684 3D \u6a21\u578b\u3002\u76ee\u524d\u4e3b\u8981\u4f7f\u7528 CAD \u6a21\u578b\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u5f88\u96be\u83b7\u53d6\u3002\u540c\u65f6\uff0c\u901a\u5e38\u53ef\u4ee5\u83b7\u53d6\u7269\u4f53\u7684\u56fe\u50cf\u3002\u81ea\u7136\u800c\u7136\u5730\uff0c\u8fd9\u5c31\u5f15\u51fa\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u4ece\u56fe\u50cf\u91cd\u5efa\u7684 3D \u6a21\u578b\u662f\u5426\u8db3\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff1f\u4e3a\u4e86\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u6d4b\u91cf 3D \u91cd\u5efa\u8d28\u91cf\u5bf9\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u7528\u4e8e\u7269\u4f53\u91cd\u5efa\u7684\u6821\u51c6\u56fe\u50cf\uff0c\u8fd9\u4e9b\u56fe\u50cf\u4e0e YCB-V \u6570\u636e\u96c6\u7684\u6d4b\u8bd5\u56fe\u50cf\u8fdb\u884c\u4e86\u914d\u51c6\uff0c\u7528\u4e8e\u5728 BOP \u57fa\u51c6\u6d4b\u8bd5\u683c\u5f0f\u4e0b\u8fdb\u884c\u59ff\u6001\u8bc4\u4f30\u3002\u5bf9\u591a\u79cd\u6700\u5148\u8fdb\u7684 3D \u91cd\u5efa\u548c\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u8fdb\u884c\u7684\u8be6\u7ec6\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u4ee3\u91cd\u5efa\u65b9\u6cd5\u751f\u6210\u7684\u51e0\u4f55\u7ed3\u6784\u901a\u5e38\u8db3\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u59ff\u6001\u4f30\u8ba1\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u5f97\u51fa\u4e86\u4e00\u4e9b\u6709\u8da3\u7684\u89c2\u5bdf\u7ed3\u679c\uff1a(1) \u7528\u4e8e\u6d4b\u91cf 3D \u91cd\u5efa\u8d28\u91cf\u7684\u6807\u51c6\u6307\u6807\u4e0d\u4e00\u5b9a\u80fd\u53cd\u6620\u59ff\u6001\u4f30\u8ba1\u7684\u7cbe\u5ea6\uff0c\u8fd9\u8868\u660e\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f8b\u5982\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002(2) \u4f20\u7edf\u7684\u3001\u975e\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u53ef\u4ee5\u4e0e\u73b0\u4ee3\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u91cd\u5efa\u6280\u672f\u76f8\u5ab2\u7f8e\uff0c\u751a\u81f3\u53ef\u4ee5\u63d0\u4f9b\u66f4\u597d\u7684\u91cd\u5efa\u65f6\u95f4-\u59ff\u6001\u7cbe\u5ea6\u6743\u8861\u3002(3) \u4f7f\u7528\u91cd\u5efa\u6a21\u578b\u548c\u4f7f\u7528 CAD \u6a21\u578b\u7684\u6027\u80fd\u4e4b\u95f4\u4ecd\u7136\u5b58\u5728\u76f8\u5f53\u5927\u7684\u5dee\u8ddd\u3002\u4e3a\u4e86\u4fc3\u8fdb\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u7684\u7814\u7a76\uff0c\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u5df2\u5728 https://github.com/VarunBurde/reconstruction_pose_benchmark \u4e0a\u516c\u5f00\u53d1\u5e03\u3002|\n", "2407.12207": "|**2024-07-16**|[NeuSurfEmb: A Complete Pipeline for Dense Correspondence-based 6D Object Pose Estimation without CAD Models](http://arxiv.org/abs/2407.12207)|**[link](https://github.com/ethz-asl/neusurfemb)**|\u76ee\u524d\u6700\u5148\u8fdb\u7684 6D \u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5047\u8bbe\u53ef\u4ee5\u4f7f\u7528 CAD \u6a21\u578b\uff0c\u5e76\u4e14\u9700\u8981\u7528\u6237\u624b\u52a8\u8bbe\u7f6e\u57fa\u4e8e\u7269\u7406\u7684\u6e32\u67d3 (PBR) \u7ba1\u9053\u6765\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\u3002\u8fd9\u4e24\u4e2a\u56e0\u7d20\u90fd\u9650\u5236\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0d\u9700\u8981 CAD \u6a21\u578b\u7684\u6d41\u7a0b\uff0c\u5e76\u4e14\u53ea\u9700\u8981\u4e00\u5c0f\u7ec4\u771f\u5b9e\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u5373\u53ef\u8bad\u7ec3\u6700\u5148\u8fdb\u7684\u59ff\u6001\u4f30\u8ba1\u5668\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u57fa\u4e8e NeuS2 \u5bf9\u8c61\u8868\u793a\uff0c\u6211\u4eec\u901a\u8fc7\u57fa\u4e8e\u8fd0\u52a8\u6062\u590d\u7ed3\u6784 (SfM) \u548c\u5bf9\u8c61\u65e0\u5173\u5206\u5272\u7684\u534a\u81ea\u52a8\u5316\u7a0b\u5e8f\u6765\u5b66\u4e60\u8be5\u8868\u793a\u3002\u6211\u4eec\u5229\u7528 NeuS2 \u7684\u65b0\u89c6\u56fe\u5408\u6210\u80fd\u529b\u548c\u7b80\u5355\u7684\u526a\u5207\u7c98\u8d34\u589e\u5f3a\u529f\u80fd\u6765\u81ea\u52a8\u751f\u6210\u903c\u771f\u7684\u5bf9\u8c61\u6e32\u67d3\uff0c\u6211\u4eec\u4f7f\u7528\u8fd9\u4e9b\u6e32\u67d3\u6765\u8bad\u7ec3\u57fa\u4e8e\u5bf9\u5e94\u7684 SurfEmb \u59ff\u6001\u4f30\u8ba1\u5668\u3002\u6211\u4eec\u5728 LINEMOD-Occlusion \u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e7f\u6cdb\u7814\u7a76\u4e86\u5176\u5404\u4e2a\u7ec4\u4ef6\u7684\u5f71\u54cd\uff0c\u5e76\u663e\u793a\u4e86\u76f8\u5bf9\u4e8e\u57fa\u4e8e CAD \u6a21\u578b\u548c PBR \u6570\u636e\u7684\u65b9\u6cd5\u7684\u7ade\u4e89\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u6211\u4eec\u7684\u6d41\u7a0b\u5728\u81ea\u6536\u96c6\u7684\u73b0\u5b9e\u4e16\u754c\u5bf9\u8c61\u4e0a\u7684\u6613\u7528\u6027\u548c\u6709\u6548\u6027\uff0c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65e0 CAD \u6a21\u578b\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u597d\u7684\u7cbe\u5ea6\u548c\u5bf9\u8f7b\u5ea6\u906e\u6321\u7684\u9c81\u68d2\u6027\u3002\u4e3a\u4e86\u8ba9\u673a\u5668\u4eba\u793e\u533a\u80fd\u591f\u4ece\u8be5\u7cfb\u7edf\u4e2d\u53d7\u76ca\uff0c\u6211\u4eec\u5c06\u5728 https://www.github.com/ethz-asl/neusurfemb \u4e0a\u516c\u5f00\u53d1\u5e03\u5b83\u3002|\n", "2406.04316": "|**2024-06-06**|[Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking](http://arxiv.org/abs/2406.04316)|null|6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u4e00\u9879\u81f3\u5173\u91cd\u8981\u4f46\u6781\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5176\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u7f3a\u4e4f\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002\u8fd9\u79cd\u6570\u636e\u7a00\u7f3a\u6027\u963b\u788d\u4e86\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u9650\u5236\u4e86\u7814\u7a76\u8fdb\u5c55\u3002\u6b64\u5916\uff0c\u53ef\u7528\u5b9e\u4f8b\u6216\u7c7b\u522b\u7684\u6570\u91cf\u6709\u9650\u4e5f\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86Omni6DPose\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ee5\u5bf9\u8c61\u7c7b\u522b\u591a\u6837\u6027\u3001\u89c4\u6a21\u5927\u548c\u5bf9\u8c61\u6750\u8d28\u591a\u6837\u6027\u4e3a\u7279\u5f81\u7684\u5927\u578b\u6570\u636e\u96c6\u3002Omni6DPose\u4e3b\u8981\u7531\u4e09\u4e2a\u90e8\u5206\u7ec4\u6210\uff1aROPE\uff08\u771f\u5b9e6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u96c6\uff09\uff0c\u5305\u542b332K\u5f20\u56fe\u50cf\uff0c\u6db5\u76d6149\u4e2a\u7c7b\u522b\u3001581\u4e2a\u5b9e\u4f8b\uff0c\u8d85\u8fc7150\u4e07\u4e2a\u6807\u6ce8\uff1bSOPE\uff08\u6a21\u62df6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u96c6\uff09\uff0c\u5305\u542b475K\u5f20\u56fe\u50cf\uff0c\u8fd9\u4e9b\u56fe\u50cf\u662f\u5229\u7528\u6df1\u5ea6\u6a21\u62df\u6280\u672f\u5728\u6df7\u5408\u73b0\u5b9e\u73af\u5883\u4e2d\u521b\u5efa\u7684\uff0c\u6db5\u76d6\u4e0eROPE\u76f8\u540c\u7684149\u4e2a\u7c7b\u522b\uff0c4162\u4e2a\u5b9e\u4f8b\uff0c\u8d85\u8fc7500\u4e07\u4e2a\u6807\u6ce8\uff1b\u4ee5\u53ca\u5728ROPE\u548cSOPE\u4e2d\u5747\u4f7f\u7528\u7684\u624b\u52a8\u5bf9\u9f50\u7684\u771f\u5b9e\u626b\u63cf\u7269\u4f53\u3002\u7531\u4e8e\u5b58\u5728\u5927\u91cf\u7684\u53d8\u5316\u548c\u6b67\u4e49\uff0cOmni6DPose\u672c\u8eab\u5c31\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86GenPose++\uff0c\u5b83\u662fSOTA\u7c7b\u522b\u7ea7\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\u7684\u589e\u5f3a\u7248\u672c\uff0c\u5b83\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6539\u8fdb\uff1a\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u63d0\u53d6\u548c\u57fa\u4e8e\u805a\u7c7b\u7684\u805a\u5408\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u57fa\u51c6\u5206\u6790\uff0c\u4ee5\u8bc4\u4f30\u5148\u524d\u65b9\u6cd5\u5728\u8fd9\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u57286D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u59ff\u6001\u8ddf\u8e2a\u65b9\u9762\u7684\u6027\u80fd\u3002|\n", "2406.02977": "|**2024-06-05**|[Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices](http://arxiv.org/abs/2406.02977)|null|\u968f\u7740\u673a\u5668\u4eba\u548c\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u8d8a\u6765\u8d8a\u4f9d\u8d56\u4e8e\u7cbe\u786e\u9ad8\u6548\u76846D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u6027\u80fd\u5bf9\u4e8e\u66f4\u5177\u4ea4\u4e92\u6027\u548c\u54cd\u5e94\u6027\u7684\u7cfb\u7edf\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u63d0\u51fa\u7684\u7a00\u758f\u989c\u8272\u4ee3\u7801\u7f51\u7edc\uff08SCCN\uff09\u4f53\u73b0\u4e86\u4e00\u79cd\u6e05\u6670\u7b80\u6d01\u7684\u6d41\u7a0b\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u3002SCCN\u5229\u7528\u76ee\u6807\u7269\u4f53\u57fa\u672c\u51e0\u4f55\u7279\u5f81\u7684\u7a00\u758f\u6027\uff0c\u5bf9RGB\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u7269\u4f53\u8fdb\u884c\u50cf\u7d20\u7ea7\u9884\u6d4b\uff0c\u4ece\u800c\u52a0\u5feb\u900f\u89c6n\u70b9\uff08PnP\uff09\u8ba1\u7b97\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u5b83\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u50cf\u7d20\u7ea7\u51e0\u4f55\u7684\u7269\u4f53\u5bf9\u79f0\u6027\u8868\u793a\uff0c\u8be5\u8868\u793a\u4e0e\u521d\u59cb\u59ff\u6001\u9884\u6d4b\u65e0\u7f1d\u96c6\u6210\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5bf9\u79f0\u7269\u4f53\u6b67\u4e49\u95ee\u9898\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSCCN\u5728\u82f1\u4f1f\u8fbeJetson AGX Xavier\u4e0a\u5206\u522b\u5728\u57fa\u51c6LINEMOD\u6570\u636e\u96c6\u548c\u906e\u6321LINEMOD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6bcf\u79d219\u5e27\uff08FPS\uff09\u548c6 FPS\u7684\u4f30\u8ba1\u901f\u7387\uff0c\u540c\u65f6\u5728\u8fd9\u4e9b\u901f\u7387\u4e0b\u59cb\u7ec8\u4fdd\u6301\u8f83\u9ad8\u7684\u4f30\u8ba1\u7cbe\u5ea6\u3002|\n", "2405.07801": "|**2024-05-31**|[Deep Learning-Based Object Pose Estimation: A Comprehensive Survey](http://arxiv.org/abs/2405.07801)|**[link](https://github.com/cnjianliu/awesome-object-pose-estimation)**|\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff0c\u5728\u589e\u5f3a\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u6280\u672f\u4e2d\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u5728\u8fc7\u53bb\u7684\u5341\u5e74\u4e2d\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7531\u4e8e\u5176\u4f18\u8d8a\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u8d8a\u6765\u8d8a\u591a\u5730\u53d6\u4ee3\u4e86\u4f9d\u8d56\u4e8e\u5de5\u7a0b\u70b9\u5bf9\u7279\u5f81\u7684\u4f20\u7edf\u7b97\u6cd5\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u5f53\u4ee3\u65b9\u6cd5\u4ecd\u7136\u5b58\u5728\u4e00\u4e9b\u6311\u6218\uff0c\u5305\u62ec\u5b83\u4eec\u5bf9\u6807\u8bb0\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\u6027\u3001\u6a21\u578b\u7d27\u51d1\u6027\u3001\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u65b0\u7269\u4f53\u80fd\u529b\u3002\u6700\u8fd1\u7f3a\u5c11\u4e00\u9879\u8c03\u67e5\u6765\u8ba8\u8bba\u8be5\u9886\u57df\u5404\u4e2a\u65b9\u9762\u7684\u8fdb\u5c55\u3001\u672a\u89e3\u51b3\u7684\u6311\u6218\u548c\u6709\u5e0c\u671b\u7684\u672a\u6765\u65b9\u5411\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6db5\u76d6\u4e86\u8be5\u95ee\u9898\u7684\u6240\u6709\u4e09\u79cd\u5f62\u5f0f\uff0c\u5373\u5b9e\u4f8b\u7ea7\u3001\u7c7b\u522b\u7ea7\u548c\u672a\u89c1\u8fc7\u7269\u4f53\u7684\u59ff\u6001\u4f30\u8ba1\u3002\u6211\u4eec\u7684\u7efc\u8ff0\u8fd8\u6db5\u76d6\u4e86\u591a\u79cd\u8f93\u5165\u6570\u636e\u6a21\u6001\u3001\u8f93\u51fa\u59ff\u6001\u7684\u81ea\u7531\u5ea6\u3001\u7269\u4f53\u5c5e\u6027\u548c\u4e0b\u6e38\u4efb\u52a1\uff0c\u4e3a\u8bfb\u8005\u63d0\u4f9b\u4e86\u5bf9\u8be5\u9886\u57df\u7684\u5168\u9762\u7406\u89e3\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u8ba8\u8bba\u4e86\u4e0d\u540c\u9886\u57df\u7684\u8bad\u7ec3\u8303\u5f0f\u3001\u63a8\u7406\u6a21\u5f0f\u3001\u5e94\u7528\u9886\u57df\u3001\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u62a5\u544a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u5728\u8fd9\u4e9b\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\uff0c\u4ece\u800c\u5e2e\u52a9\u8bfb\u8005\u4e3a\u5176\u5e94\u7528\u9009\u62e9\u6700\u5408\u9002\u7684\u65b9\u6cd5\u3002\u6700\u540e\uff0c\u8be5\u7efc\u8ff0\u6307\u51fa\u4e86\u5173\u952e\u6311\u6218\uff0c\u56de\u987e\u4e86\u5f53\u524d\u7684\u8d8b\u52bf\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002\u6211\u4eec\u8fd8\u5c06\u7ee7\u7eed\u8ddf\u8e2ahttps://github.com/CNJianLiu/Awesome-Object-Pose-Estimation \u4e0a\u7684\u6700\u65b0\u5de5\u4f5c\u3002|\n", "2403.19527": "|**2024-03-28**|[Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation](http://arxiv.org/abs/2403.19527)|**[link](https://github.com/leeiieeo/ag-pose)**|\u7c7b\u522b\u7ea7 6D \u5bf9\u8c61\u59ff\u6001\u4f30\u8ba1\u65e8\u5728\u4f30\u8ba1\u7279\u5b9a\u7c7b\u522b\u4e2d\u672a\u89c1\u8fc7\u5b9e\u4f8b\u7684\u65cb\u8f6c\u3001\u5e73\u79fb\u548c\u5927\u5c0f\u3002\u5728\u8fd9\u4e2a\u9886\u57df\uff0c\u57fa\u4e8e\u5bc6\u96c6\u5bf9\u5e94\u7684\u65b9\u6cd5\u5df2\u7ecf\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5b83\u4eec\u6ca1\u6709\u660e\u786e\u8003\u8651\u4e0d\u540c\u5b9e\u4f8b\u7684\u5c40\u90e8\u548c\u5168\u5c40\u51e0\u4f55\u4fe1\u606f\uff0c\u5bfc\u81f4\u5bf9\u5177\u6709\u663e\u8457\u5f62\u72b6\u53d8\u5316\u7684\u672a\u89c1\u8fc7\u5b9e\u4f8b\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b9e\u4f8b\u81ea\u9002\u5e94\u548c\u51e0\u4f55\u611f\u77e5\u5173\u952e\u70b9\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u7c7b\u522b\u7ea7 6D \u5bf9\u8c61\u59ff\u6001\u4f30\u8ba1 (AG-Pose)\uff0c\u5b83\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a(1) \u7b2c\u4e00\u4e2a\u8bbe\u8ba1\u662f\u5b9e\u4f8b\u81ea\u9002\u5e94\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u5757\uff0c\u5b83\u53ef\u4ee5\u81ea\u9002\u5e94\u5730\u68c0\u6d4b\u4e00\u7ec4\u7a00\u758f\u5173\u952e\u70b9\uff0c\u7528\u4e8e\u8868\u793a\u5404\u79cd\u5b9e\u4f8b\u7684\u51e0\u4f55\u7ed3\u6784\u3002(2) \u7b2c\u4e8c\u4e2a\u8bbe\u8ba1\u662f\u51e0\u4f55\u611f\u77e5\u7279\u5f81\u805a\u5408\u6a21\u5757\uff0c\u5b83\u53ef\u4ee5\u6709\u6548\u5730\u5c06\u5c40\u90e8\u548c\u5168\u5c40\u51e0\u4f55\u4fe1\u606f\u6574\u5408\u5230\u5173\u952e\u70b9\u7279\u5f81\u4e2d\u3002\u8fd9\u4e24\u4e2a\u6a21\u5757\u53ef\u4ee5\u534f\u540c\u5de5\u4f5c\uff0c\u4e3a\u672a\u89c1\u8fc7\u7684\u5b9e\u4f8b\u5efa\u7acb\u9c81\u68d2\u7684\u5173\u952e\u70b9\u7ea7\u5bf9\u5e94\u5173\u7cfb\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728 CAMERA25 \u548c REAL275 \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 AG-Pose \u5728\u6ca1\u6709\u7c7b\u522b\u7279\u5b9a\u5f62\u72b6\u5148\u9a8c\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u65b9\u6cd5\u3002|\n", "2403.18791": "|**2024-06-01**|[Object Pose Estimation via the Aggregation of Diffusion Features](http://arxiv.org/abs/2403.18791)|**[link](https://github.com/tianfu18/diff-feats-pose)**|\u4ece\u56fe\u50cf\u4e2d\u4f30\u8ba1\u7269\u4f53\u59ff\u6001\u662f 3D \u573a\u666f\u7406\u89e3\u7684\u4e00\u9879\u5173\u952e\u4efb\u52a1\uff0c\u6700\u8fd1\u7684\u65b9\u6cd5\u5728\u975e\u5e38\u5927\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u826f\u597d\u7684\u7ed3\u679c\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5904\u7406\u672a\u89c1\u8fc7\u7269\u4f53\u65f6\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\u3002\u6211\u4eec\u8ba4\u4e3a\u8fd9\u662f\u7531\u4e8e\u56fe\u50cf\u7279\u5f81\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u9020\u6210\u7684\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u6df1\u5165\u5206\u6790\u4e86\u6269\u6563\u6a21\u578b\uff08\u4f8b\u5982 Stable Diffusion\uff09\u7684\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u5728\u5bf9\u672a\u89c1\u8fc7\u7269\u4f53\u8fdb\u884c\u5efa\u6a21\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002\u57fa\u4e8e\u8fd9\u4e00\u5206\u6790\uff0c\u6211\u4eec\u521b\u65b0\u6027\u5730\u5c06\u8fd9\u4e9b\u6269\u6563\u7279\u5f81\u5f15\u5165\u5230\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4e2d\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u67b6\u6784\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u6355\u83b7\u548c\u805a\u5408\u4e0d\u540c\u7c92\u5ea6\u7684\u6269\u6563\u7279\u5f81\uff0c\u6781\u5927\u5730\u63d0\u9ad8\u4e86\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728\u4e09\u4e2a\u6d41\u884c\u7684\u57fa\u51c6\u6570\u636e\u96c6 LM\u3001O-LM \u548c T-LESS \u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u672a\u89c1\u8fc7\u7269\u4f53\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u5148\u524d\u6700\u4f73\u65b9\u6cd5\u66f4\u9ad8\u7684\u51c6\u786e\u7387\uff1a\u5728 Unseen LM \u4e0a\u4e3a 98.2% \u5bf9\u6bd4 93.5%\uff0c\u5728 Unseen O-LM \u4e0a\u4e3a 85.9% \u5bf9\u6bd4 76.3%\uff0c\u663e\u793a\u4e86\u6211\u4eec\u65b9\u6cd5\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u53d1\u5e03\u5728 https://github.com/Tianfu18/diff-feats-pose\u3002|\n"}, "nerf": {"2408.09130": "|**2024-08-20**|[Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark Images Using Gaussian Splatting](http://arxiv.org/abs/2408.09130)|**[link](https://github.com/yec22/Gaussian-DK)**|\u4e09\u7ef4\u9ad8\u65af\u4f53\u6e32\u67d3\u6280\u672f\u8fd1\u5e74\u6765\u6210\u4e3a\u4e00\u79cd\u5f3a\u5927\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e00\u81f4\u7684\u591a\u89c6\u56fe\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u5408\u6210\u975e\u51e1\u7684\u65b0\u89c6\u56fe\u3002\u7136\u800c\uff0c\u6211\u4eec\u6ce8\u610f\u5230\u5728\u573a\u666f\u672a\u88ab\u5b8c\u5168\u7167\u4eae\u7684\u9ed1\u6697\u73af\u5883\u4e2d\u62cd\u6444\u7684\u56fe\u50cf\u53ef\u80fd\u8868\u73b0\u51fa\u76f8\u5f53\u5927\u7684\u4eae\u5ea6\u53d8\u5316\u548c\u591a\u89c6\u56fe\u4e0d\u4e00\u81f4\u6027\uff0c\u8fd9\u5bf9\u4e09\u7ef4\u9ad8\u65af\u4f53\u6e32\u67d3\u6280\u672f\u63d0\u51fa\u4e86\u5de8\u5927\u7684\u6311\u6218\uff0c\u5e76\u4e25\u91cd\u964d\u4f4e\u4e86\u5176\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Gaussian-DK\u3002\u89c2\u5bdf\u5230\u4e0d\u4e00\u81f4\u6027\u4e3b\u8981\u662f\u7531\u76f8\u673a\u6210\u50cf\u5f15\u8d77\u7684\uff0c\u6211\u4eec\u4f7f\u7528\u4e00\u7ec4\u5404\u5411\u5f02\u6027\u4e09\u7ef4\u9ad8\u65af\u51fd\u6570\u6765\u8868\u793a\u7269\u7406\u4e16\u754c\u7684\u4e00\u81f4\u8f90\u5c04\u573a\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u76f8\u673a\u54cd\u5e94\u6a21\u5757\u6765\u8865\u507f\u591a\u89c6\u56fe\u4e0d\u4e00\u81f4\u6027\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b65\u957f\u7684\u68af\u5ea6\u7f29\u653e\u7b56\u7565\u6765\u7ea6\u675f\u9760\u8fd1\u76f8\u673a\u7684\u9ad8\u65af\u51fd\u6570\uff08\u7ed3\u679c\u8bc1\u660e\u662f\u6f02\u6d6e\u7269\uff09\u7684\u62c6\u5206\u548c\u514b\u9686\u3002\u5728\u6211\u4eec\u63d0\u51fa\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGaussian-DK \u53ef\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\u7ed3\u679c\uff0c\u6ca1\u6709\u91cd\u5f71\u548c\u6f02\u6d6e\u7269\u4f2a\u5f71\uff0c\u5e76\u4e14\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u53ef\u4ee5\u901a\u8fc7\u63a7\u5236\u66dd\u5149\u7ea7\u522b\u6765\u5408\u6210\u4eae\u5ea6\u589e\u5f3a\u7684\u56fe\u50cf\uff0c\u4ece\u800c\u6e05\u6670\u5730\u663e\u793a\u9634\u5f71\u533a\u57df\u7684\u7ec6\u8282\u3002|\n", "2407.13520": "|**2024-08-29**|[EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian Splatting](http://arxiv.org/abs/2407.13520)|null|\u8fd1\u5e74\u6765\uff0c\u968f\u7740\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u548c 3D \u9ad8\u65af\u6563\u5c04 (3DGS) \u7684\u53d1\u5c55\uff0c3D \u53bb\u6a21\u7cca\u91cd\u5efa\u6280\u672f\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002 \u5c3d\u7ba1\u8fd9\u4e9b\u6280\u672f\u53ef\u4ee5\u4ece\u6a21\u7cca\u7684\u56fe\u50cf\u8f93\u5165\u4e2d\u6062\u590d\u76f8\u5bf9\u6e05\u6670\u7684 3D \u91cd\u5efa\uff0c\u4f46\u5b83\u4eec\u5728\u5904\u7406\u4e25\u91cd\u6a21\u7cca\u548c\u590d\u6742\u76f8\u673a\u8fd0\u52a8\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u5c40\u9650\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e8b\u4ef6\u8f85\u52a9\u7684 3D \u9ad8\u65af\u6563\u5c04\u53bb\u6a21\u7cca\u91cd\u5efa (EaDeblur-GS)\uff0c\u5b83\u96c6\u6210\u4e86\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u4ee5\u589e\u5f3a 3DGS \u5bf9\u8fd0\u52a8\u6a21\u7cca\u7684\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u91c7\u7528\u81ea\u9002\u5e94\u504f\u5dee\u4f30\u8ba1\u5668 (ADE) \u7f51\u7edc\u6765\u4f30\u8ba1\u9ad8\u65af\u4e2d\u5fc3\u504f\u5dee\u5e76\u4f7f\u7528\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0cEaDeblur-GS \u53ef\u4ee5\u5b9e\u65f6\u5b9e\u73b0\u6e05\u6670\u7684 3D \u91cd\u5efa\uff0c\u5176\u6027\u80fd\u53ef\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002|\n", "2407.07090": "|**2024-07-10**|[3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes](http://arxiv.org/abs/2407.07090)|null|\u57fa\u4e8e\u7c92\u5b50\u7684\u8f90\u5c04\u573a\u8868\u793a\u65b9\u6cd5\uff0c\u4f8b\u5982\u4e09\u7ef4\u9ad8\u65af splatting\uff0c\u5df2\u7ecf\u5728\u590d\u6742\u573a\u666f\u7684\u91cd\u5efa\u548c\u91cd\u65b0\u6e32\u67d3\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\u3002\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5149\u6805\u5316\u6e32\u67d3\u7c92\u5b50\uff0c\u5c06\u5b83\u4eec\u6295\u5f71\u5230\u5c4f\u5e55\u7a7a\u95f4\u56fe\u5757\u4e2d\uff0c\u5e76\u6309\u6392\u5e8f\u987a\u5e8f\u8fdb\u884c\u5904\u7406\u3002\u800c\u8fd9\u9879\u5de5\u4f5c\u5219\u8003\u8651\u5bf9\u7c92\u5b50\u8fdb\u884c\u5149\u7ebf\u8ffd\u8e2a\uff0c\u6784\u5efa\u8fb9\u754c\u4f53\u79ef\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u4f7f\u7528\u9ad8\u6027\u80fd GPU \u5149\u7ebf\u8ffd\u8e2a\u786c\u4ef6\u4e3a\u6bcf\u4e2a\u50cf\u7d20\u6295\u5c04\u5149\u7ebf\u3002\u4e3a\u4e86\u6709\u6548\u5904\u7406\u5927\u91cf\u7684\u534a\u900f\u660e\u7c92\u5b50\uff0c\u6211\u4eec\u63cf\u8ff0\u4e86\u4e00\u79cd\u4e13\u95e8\u7684\u6e32\u67d3\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4f7f\u7528\u8fb9\u754c\u7f51\u683c\u5c01\u88c5\u7c92\u5b50\uff0c\u4ee5\u5229\u7528\u5feb\u901f\u7684\u5149\u7ebf\u4e09\u89d2\u5f62\u76f8\u4ea4\u6d4b\u8bd5\uff0c\u5e76\u6309\u6df1\u5ea6\u987a\u5e8f\u5bf9\u6210\u6279\u7684\u76f8\u4ea4\u70b9\u8fdb\u884c\u7740\u8272\u3002\u5149\u7ebf\u8ffd\u8e2a\u5728\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\u7684\u4f18\u52bf\u662f\u4f17\u6240\u5468\u77e5\u7684\uff1a\u5904\u7406\u975e\u76f8\u5e72\u5149\u7ebf\u4ee5\u83b7\u5f97\u9634\u5f71\u548c\u53cd\u5c04\u7b49\u4e8c\u6b21\u7167\u660e\u6548\u679c\u3001\u4ece\u673a\u5668\u4eba\u6280\u672f\u4e2d\u5e38\u89c1\u7684\u9ad8\u5ea6\u626d\u66f2\u7684\u76f8\u673a\u8fdb\u884c\u6e32\u67d3\u3001\u5bf9\u5149\u7ebf\u8fdb\u884c\u968f\u673a\u91c7\u6837\u7b49\u7b49\u3002\u4e0e\u5149\u6805\u5316\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u6e32\u67d3\u5668\u4ee5\u5f88\u5c0f\u7684\u6210\u672c\u5b9e\u73b0\u4e86\u8fd9\u79cd\u7075\u6d3b\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u901f\u5ea6\u548c\u51c6\u786e\u6027\uff0c\u4ee5\u53ca\u5176\u5728\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u548c\u89c6\u89c9\u9886\u57df\u7684\u591a\u79cd\u5e94\u7528\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u5bf9\u57fa\u672c\u9ad8\u65af\u8868\u793a\u7684\u76f8\u5173\u6539\u8fdb\uff0c\u5305\u62ec\u7b80\u5355\u4f7f\u7528\u5e7f\u4e49\u6838\u51fd\u6570\uff0c\u8fd9\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u7c92\u5b50\u547d\u4e2d\u6b21\u6570\u3002|\n", "2407.05254": "|**2024-07-07**|[GaussReg: Fast 3D Registration with Gaussian Splatting](http://arxiv.org/abs/2407.05254)|null|\u70b9\u4e91\u914d\u51c6\u662f\u5927\u89c4\u6a21\u4e09\u7ef4\u573a\u666f\u626b\u63cf\u548c\u91cd\u5efa\u7684\u57fa\u672c\u95ee\u9898\u3002\u5728\u6df1\u5ea6\u5b66\u4e60\u7684\u5e2e\u52a9\u4e0b\uff0c\u914d\u51c6\u65b9\u6cd5\u5f97\u5230\u4e86\u663e\u8457\u53d1\u5c55\uff0c\u5df2\u63a5\u8fd1\u6210\u719f\u9636\u6bb5\u3002\u968f\u7740\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u7684\u5f15\u5165\uff0c\u5b83\u56e0\u5176\u5f3a\u5927\u7684\u89c6\u56fe\u5408\u6210\u80fd\u529b\u800c\u6210\u4e3a\u6700\u6d41\u884c\u7684\u4e09\u7ef4\u573a\u666f\u8868\u793a\u65b9\u6cd5\u3002\u5bf9\u4e8e NeRF \u8868\u793a\uff0c\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u4e5f\u9700\u8981\u5bf9\u5176\u8fdb\u884c\u914d\u51c6\u3002\u7136\u800c\uff0c\u8fd9\u4e2a\u4e3b\u9898\u6781\u5ea6\u7f3a\u4e4f\u63a2\u7d22\u3002\u8fd9\u662f\u56e0\u4e3a\u5728\u7528\u9690\u5f0f\u8868\u793a\u5bf9\u4e24\u4e2a\u573a\u666f\u4e4b\u95f4\u7684\u51e0\u4f55\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u65b9\u9762\u5b58\u5728\u56fa\u6709\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u9690\u5f0f\u8868\u793a\u8f6c\u6362\u4e3a\u663e\u5f0f\u8868\u793a\uff0c\u4ee5\u4fbf\u8fdb\u4e00\u6b65\u914d\u51c6\u3002\u6700\u8fd1\uff0c\u5f15\u5165\u4e86\u9ad8\u65af splatting (GS)\uff0c\u5b83\u91c7\u7528\u663e\u5f0f\u4e09\u7ef4\u9ad8\u65af\u51fd\u6570\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6e32\u67d3\u901f\u5ea6\u3002\u7ed9\u5b9a\u4e24\u4e2a\u5177\u6709\u663e\u5f0f GS \u8868\u793a\u7684\u573a\u666f\uff0c\u6211\u4eec\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\u63a2\u7d22\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u4e09\u7ef4\u914d\u51c6\u4efb\u52a1\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 GaussReg\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u7531\u7c97\u5230\u7cbe\u7684\u6846\u67b6\uff0c\u65e2\u5feb\u901f\u53c8\u51c6\u786e\u3002\u7c97\u7565\u9636\u6bb5\u9075\u5faa\u73b0\u6709\u7684\u70b9\u4e91\u914d\u51c6\u65b9\u6cd5\uff0c\u5e76\u4f30\u8ba1\u6765\u81ea GS \u7684\u70b9\u4e91\u7684\u7c97\u7565\u5bf9\u9f50\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u5f15\u5bfc\u7684\u7cbe\u7ec6\u914d\u51c6\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ece GS \u6e32\u67d3\u56fe\u50cf\uff0c\u4e3a\u7cbe\u786e\u5bf9\u9f50\u63d0\u4f9b\u66f4\u8be6\u7ec6\u7684\u51e0\u4f55\u4fe1\u606f\u3002\u4e3a\u4e86\u652f\u6301\u5168\u9762\u8bc4\u4f30\uff0c\u6211\u4eec\u7cbe\u5fc3\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a ScanNet-GSReg \u7684\u573a\u666f\u7ea7\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4ece ScanNet \u6570\u636e\u96c6\u4e2d\u83b7\u5f97\u7684 1379 \u4e2a\u573a\u666f\uff0c\u5e76\u6536\u96c6\u4e86\u4e00\u4e2a\u540d\u4e3a GSReg \u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684 GaussReg \u6bd4 HLoc\uff08SuperPoint \u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0cSuperGlue \u4f5c\u4e3a\u5339\u914d\u5668\uff09\u5feb 44 \u500d\uff0c\u5e76\u4e14\u5177\u6709\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002|\n", "2407.03923": "|**2024-07-04**|[CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion Blur Images](http://arxiv.org/abs/2407.03923)|null|\u795e\u7ecf\u8f90\u5c04\u573a (NeRFs) \u56e0\u5176\u9ad8\u8d28\u91cf\u7684\u65b0\u9896\u89c6\u56fe\u6e32\u67d3\u80fd\u529b\u800c\u5907\u53d7\u5173\u6ce8\uff0c\u4fc3\u4f7f\u4eba\u4eec\u5bf9\u5176\u5728\u5404\u79cd\u73b0\u5b9e\u4e16\u754c\u6848\u4f8b\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u7814\u7a76\u3002\u5176\u4e2d\u4e00\u4e2a\u5173\u952e\u6311\u6218\u662f\u76f8\u673a\u5728\u66dd\u5149\u65f6\u95f4\u5185\u7684\u8fd0\u52a8\u5bfc\u81f4\u7684\u8fd0\u52a8\u6a21\u7cca\uff0c\u8fd9\u963b\u788d\u4e86\u5bf9 3D \u573a\u666f\u7684\u51c6\u786e\u91cd\u5efa\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8fde\u7eed\u521a\u4f53\u8fd0\u52a8\u611f\u77e5\u9ad8\u65af\u6563\u5c04 (CRiM-GS)\uff0c\u4ee5\u5b9e\u65f6\u6e32\u67d3\u901f\u5ea6\u4ece\u6a21\u7cca\u56fe\u50cf\u91cd\u5efa\u7cbe\u786e\u7684 3D \u573a\u666f\u3002\u8003\u8651\u5230\u5b9e\u9645\u76f8\u673a\u8fd0\u52a8\u6a21\u7cca\u8fc7\u7a0b\u5305\u542b\u590d\u6742\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u6211\u4eec\u57fa\u4e8e\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b (ODE) \u9884\u6d4b\u76f8\u673a\u7684\u8fde\u7eed\u8fd0\u52a8\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5229\u7528\u521a\u4f53\u53d8\u6362\u5bf9\u76f8\u673a\u8fd0\u52a8\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u8fdb\u884c\u9002\u5f53\u7684\u6b63\u5219\u5316\uff0c\u4ee5\u4fdd\u6301\u7269\u4f53\u7684\u5f62\u72b6\u548c\u5927\u5c0f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728 \\textit{SE(3)} \u573a\u4e2d\u5f15\u5165\u4e86\u8fde\u7eed\u53ef\u53d8\u5f62 3D \u53d8\u6362\uff0c\u901a\u8fc7\u786e\u4fdd\u66f4\u9ad8\u7684\u81ea\u7531\u5ea6\u4f7f\u521a\u4f53\u53d8\u6362\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u7684\u95ee\u9898\u3002\u901a\u8fc7\u56de\u987e\u57fa\u672c\u76f8\u673a\u7406\u8bba\u5e76\u91c7\u7528\u5148\u8fdb\u7684\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6280\u672f\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u5bf9\u8fde\u7eed\u76f8\u673a\u8f68\u8ff9\u7684\u7cbe\u786e\u5efa\u6a21\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u6027\u80fd\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002|\n", "2406.18214": "|**2024-07-29**|[Trimming the Fat: Efficient Compression of 3D Gaussian Splats through Pruning](http://arxiv.org/abs/2406.18214)|**[link](https://github.com/salmanali96/trimming-the-fat)**|In recent times, the utilization of 3D models has gained traction, owing to the capacity for end-to-end training initially offered by Neural Radiance Fields and more recently by 3D Gaussian Splatting (3DGS) models. The latter holds a significant advantage by inherently easing rapid convergence during training and offering extensive editability. However, despite rapid advancements, the literature still lives in its infancy regarding the scalability of these models. In this study, we take some initial steps in addressing this gap, showing an approach that enables both the memory and computational scalability of such models. Specifically, we propose \"Trimming the fat\", a post-hoc gradient-informed iterative pruning technique to eliminate redundant information encoded in the model. Our experimental findings on widely acknowledged benchmarks attest to the effectiveness of our approach, revealing that up to 75% of the Gaussians can be removed while maintaining or even improving upon baseline performance. Our approach achieves around 50$\\times$ compression while preserving performance similar to the baseline model, and is able to speed-up computation up to 600 FPS.|\n", "2406.15149": "|**2024-06-21**|[Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks](http://arxiv.org/abs/2406.15149)|null|\u4eff\u771f\u5668\u662f\u81ea\u52a8\u673a\u5668\u4eba\u5b66\u4e60\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u6570\u636e\u751f\u6210\u3001\u7075\u6d3b\u7684\u8bbe\u8ba1\u548c\u8f68\u8ff9\u4f18\u5316\u3002\u7136\u800c\uff0c\u5c06\u4ece\u4eff\u771f\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u7684\u884c\u4e3a\u8fc1\u79fb\u5230\u73b0\u5b9e\u4e16\u754c\u4e2d\u88ab\u8bc1\u660e\u662f\u56f0\u96be\u7684\uff0c\u901a\u5e38\u9700\u8981\u901a\u8fc7\u8ba1\u7b97\u91cf\u5927\u7684\u57df\u968f\u673a\u5316\u65b9\u6cd5\u6216\u8fdb\u4e00\u6b65\u7684\u6a21\u578b\u5fae\u8c03\u6765\u7f13\u89e3\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\u6765\u63d0\u9ad8\u73b0\u5b9e\u4e16\u754c\u4e2d\u89c6\u89c9\u56db\u65cb\u7ffc\u98de\u884c\u5668\u5bfc\u822a\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u5206\u5e03\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u5c06\u9ad8\u65af\u6837\u6761\u51fd\u6570\u4e0e\u56db\u65cb\u7ffc\u98de\u884c\u5668\u98de\u884c\u52a8\u529b\u5b66\u76f8\u7ed3\u5408\u6765\u6784\u5efa\u4eff\u771f\u5668\uff0c\u7136\u540e\u4f7f\u7528 Liquid \u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u9c81\u68d2\u7684\u5bfc\u822a\u7b56\u7565\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u6211\u4eec\u83b7\u5f97\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u6a21\u4eff\u5b66\u4e60\u534f\u8bae\uff0c\u5b83\u7ed3\u5408\u4e86 3D \u9ad8\u65af\u6837\u6761\u51fd\u6570\u8f90\u5c04\u573a\u6e32\u67d3\u3001\u4e13\u5bb6\u6f14\u793a\u8bad\u7ec3\u6570\u636e\u7684\u5de7\u5999\u7f16\u7a0b\u4ee5\u53ca Liquid \u7f51\u7edc\u7684\u4efb\u52a1\u7406\u89e3\u80fd\u529b\u65b9\u9762\u7684\u8fdb\u6b65\u3002\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9a\u91cf\u98de\u884c\u6d4b\u8bd5\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5728\u5355\u4e2a\u4eff\u771f\u573a\u666f\u4e2d\u5b66\u4e60\u5230\u7684\u5bfc\u822a\u6280\u80fd\u53ef\u4ee5\u76f4\u63a5\u7a33\u5065\u5730\u8fc1\u79fb\u5230\u73b0\u5b9e\u4e16\u754c\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5c55\u793a\u4e86\u5728\u5267\u70c8\u7684\u5206\u5e03\u548c\u7269\u7406\u73af\u5883\u53d8\u5316\u4e0b\uff0c\u5728\u8bad\u7ec3\u73af\u5883\u4e4b\u5916\u4fdd\u6301\u6027\u80fd\u7684\u80fd\u529b\u3002\u6211\u4eec\u5b66\u4e60\u7684 Liquid \u7b56\u7565\uff0c\u4ec5\u5728\u4ece\u771f\u5b9e\u611f\u5ba4\u5185\u98de\u884c\u6a21\u62df\u4e2d\u63d0\u53d6\u7684\u5355\u76ee\u6807\u673a\u52a8\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u53ef\u4ee5\u6cdb\u5316\u5230\u6237\u5916\u771f\u5b9e\u786c\u4ef6\u5e73\u53f0\u4e0a\u7684\u591a\u6b65\u8fdc\u8db3\u3002|\n", "2406.10373": "|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|\u5728\u975e\u7ed3\u6784\u5316\u7684\u65c5\u6e38\u73af\u5883\u4e2d\u62cd\u6444\u7684\u7167\u7247\u7ecf\u5e38\u5448\u73b0\u51fa\u591a\u53d8\u7684\u5916\u89c2\u548c\u77ed\u6682\u7684\u906e\u6321\uff0c\u8fd9\u5bf9\u7cbe\u786e\u7684\u573a\u666f\u91cd\u5efa\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u5e76\u5728\u65b0\u9896\u89c6\u56fe\u5408\u6210\u4e2d\u5bfc\u81f4\u4e86\u4f2a\u5f71\u3002\u5c3d\u7ba1\u5148\u524d\u7684\u65b9\u6cd5\u5df2\u7ecf\u5c06\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u4e0e\u5176\u4ed6\u53ef\u5b66\u4e60\u6a21\u5757\u76f8\u7ed3\u5408\u6765\u5904\u7406\u52a8\u6001\u5916\u89c2\u5e76\u6d88\u9664\u77ac\u6001\u5bf9\u8c61\uff0c\u4f46\u5176\u5927\u91cf\u7684\u8bad\u7ec3\u9700\u6c42\u548c\u7f13\u6162\u7684\u6e32\u67d3\u901f\u5ea6\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u6700\u8fd1\uff0c3D \u9ad8\u65af splatting (3DGS) \u5df2\u6210\u4e3a NeRF \u7684\u4e00\u79cd\u5f88\u6709\u524d\u9014\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5b83\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u4ee5\u53ca\u66f4\u597d\u7684\u6e32\u67d3\u8d28\u91cf\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 Wild-GS\uff0c\u8fd9\u662f\u4e00\u79cd\u9488\u5bf9\u4e0d\u53d7\u7ea6\u675f\u7684\u7167\u7247\u96c6\u4f18\u5316\u7684 3DGS \u521b\u65b0\u6539\u7f16\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5176\u6548\u7387\u4f18\u52bf\u3002Wild-GS \u901a\u8fc7\u6bcf\u4e2a 3D \u9ad8\u65af\u7684\u56fa\u6709\u6750\u8d28\u5c5e\u6027\u3001\u6bcf\u5f20\u56fe\u50cf\u7684\u5168\u5c40\u7167\u660e\u548c\u76f8\u673a\u5c5e\u6027\u4ee5\u53ca\u9010\u70b9\u53cd\u5c04\u7387\u7684\u5c40\u90e8\u65b9\u5dee\u6765\u786e\u5b9a\u5176\u5916\u89c2\u3002\u4e0e\u5148\u524d\u5728\u56fe\u50cf\u7a7a\u95f4\u4e2d\u5bf9\u53c2\u8003\u7279\u5f81\u8fdb\u884c\u5efa\u6a21\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cWild-GS \u901a\u8fc7\u5bf9\u4ece\u53c2\u8003\u56fe\u50cf\u4e2d\u63d0\u53d6\u7684\u4e09\u5e73\u9762\u8fdb\u884c\u91c7\u6837\uff0c\u5c06\u50cf\u7d20\u5916\u89c2\u7279\u5f81\u660e\u786e\u5730\u4e0e\u76f8\u5e94\u7684\u5c40\u90e8\u9ad8\u65af\u5bf9\u9f50\u3002\u8fd9\u79cd\u65b0\u9896\u7684\u8bbe\u8ba1\u6709\u6548\u5730\u5c06\u53c2\u8003\u89c6\u56fe\u7684\u9ad8\u9891\u7ec6\u8282\u5916\u89c2\u8f6c\u79fb\u5230 3D \u7a7a\u95f4\uff0c\u5e76\u663e\u7740\u52a0\u5feb\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u5229\u7528 2D \u53ef\u89c1\u6027\u56fe\u548c\u6df1\u5ea6\u6b63\u5219\u5316\u5206\u522b\u51cf\u8f7b\u77ac\u6001\u6548\u5e94\u548c\u7ea6\u675f\u51e0\u4f55\u5f62\u72b6\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cWild-GS \u5728\u6240\u6709\u73b0\u6709\u6280\u672f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6e32\u67d3\u6027\u80fd\u4ee5\u53ca\u6700\u9ad8\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002|\n", "2406.04253": "|**2024-06-06**|[A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation](http://arxiv.org/abs/2406.04253)|null|\u4e09\u7ef4\u5efa\u6a21\u4e00\u76f4\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u7684\u91cd\u8981\u9886\u57df\u3002\u8fd1\u5e74\u6765\uff0c\u7531\u4e8e\u795e\u7ecf\u8868\u793a\u548c\u751f\u6210\u6a21\u578b\u7684\u7a81\u7834\uff0c\u6211\u4eec\u89c1\u8bc1\u4e86\u4e09\u7ef4\u5efa\u6a21\u7684\u5feb\u901f\u53d1\u5c55\u3002\u4e09\u7ef4\u4eba\u4f53\u5efa\u6a21\u4f5c\u4e3a\u6e38\u620f\u548c\u52a8\u753b\u7b49\u4f17\u591a\u73b0\u5b9e\u5e94\u7528\u7684\u6838\u5fc3\uff0c\u5f15\u8d77\u4e86\u4eba\u4eec\u7684\u5e7f\u6cdb\u5173\u6ce8\u3002\u5728\u8fc7\u53bb\u7684\u51e0\u5e74\u91cc\uff0c\u51fa\u73b0\u4e86\u5927\u91cf\u5173\u4e8e\u521b\u5efa\u4e09\u7ef4\u4eba\u4f53\u5316\u8eab\u7684\u5de5\u4f5c\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u65b0\u7684\u3001\u4e30\u5bcc\u7684\u4e09\u7ef4\u4eba\u4f53\u5efa\u6a21\u77e5\u8bc6\u5e93\u3002\u6587\u732e\u7684\u89c4\u6a21\u4e4b\u5927\uff0c\u4f7f\u5f97\u4e2a\u4eba\u96be\u4ee5\u8ddf\u8e2a\u6240\u6709\u7684\u5de5\u4f5c\u3002\u672c\u6b21\u7efc\u8ff0\u65e8\u5728\u4ece\u91cd\u5efa\u548c\u751f\u6210\u4e24\u4e2a\u89d2\u5ea6\uff0c\u5168\u9762\u6982\u8ff0\u8fd9\u4e9b\u65b0\u5174\u7684\u4e09\u7ef4\u4eba\u4f53\u5316\u8eab\u5efa\u6a21\u6280\u672f\u3002\u9996\u5148\uff0c\u6211\u4eec\u56de\u987e\u4e86\u5177\u6709\u4ee3\u8868\u6027\u7684\u4e09\u7ef4\u4eba\u4f53\u91cd\u5efa\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u50cf\u7d20\u5bf9\u9f50\u9690\u51fd\u6570\u3001\u795e\u7ecf\u8f90\u5c04\u573a\u548c\u4e09\u7ef4\u9ad8\u65af\u6563\u5c04\u7b49\u65b9\u6cd5\u3002\u7136\u540e\uff0c\u6211\u4eec\u603b\u7ed3\u4e86\u5177\u6709\u4ee3\u8868\u6027\u7684\u4e09\u7ef4\u4eba\u4f53\u751f\u6210\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982 CLIP\uff09\u3001\u6269\u6563\u6a21\u578b\u548c\u5404\u79cd\u4e09\u7ef4\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6700\u540e\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u6211\u4eec\u5bf9\u73b0\u6709\u65b9\u6cd5\u7684\u53cd\u601d\u4ee5\u53ca\u4e09\u7ef4\u4eba\u4f53\u5316\u8eab\u5efa\u6a21\u9762\u4e34\u7684\u5f00\u653e\u6027\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002|\n", "2406.02720": "|**2024-06-13**|[3D-HGS: 3D Half-Gaussian Splatting](http://arxiv.org/abs/2406.02720)|**[link](https://github.com/lihaolin88/3d-half-gaussian-splatting)**|\u7167\u7247\u7ea7\u903c\u771f\u7684\u4e09\u7ef4\u91cd\u5efa\u662f\u4e09\u7ef4\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\u3002\u7531\u4e8e\u6700\u8fd1\u795e\u7ecf\u6e32\u67d3\u6280\u672f\u7684\u51fa\u73b0\uff0c\u8be5\u9886\u57df\u53d6\u5f97\u4e86\u76f8\u5f53\u5927\u7684\u8fdb\u6b65\u3002\u8fd9\u4e9b\u6280\u672f\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5b66\u4e60\u4e09\u7ef4\u573a\u666f\u7684\u4f53\u79ef\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u6e32\u67d3\u5f97\u5230\u7684\u635f\u5931\u51fd\u6570\u6765\u7ec6\u5316\u8fd9\u4e9b\u8868\u793a\u3002\u5176\u4e2d\uff0c\u4e09\u7ef4\u9ad8\u65af\u6563\u5c04\uff083D-GS\uff09\u5df2\u6210\u4e3a\u4e00\u79cd\u91cd\u8981\u7684\u65b9\u6cd5\uff0c\u5176\u6027\u80fd\u8d85\u8fc7\u4e86\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRFs\uff09\u30023D-GS\u4f7f\u7528\u53c2\u6570\u5316\u7684\u4e09\u7ef4\u9ad8\u65af\u51fd\u6570\u6765\u5efa\u6a21\u7a7a\u95f4\u4f4d\u7f6e\u548c\u989c\u8272\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u56fe\u5757\u7684\u5feb\u901f\u6e32\u67d3\u6280\u672f\u3002\u5c3d\u7ba1\u5176\u6e32\u67d3\u6027\u80fd\u548c\u901f\u5ea6\u90fd\u5f88\u51fa\u8272\uff0c\u4f46\u4f7f\u7528\u4e09\u7ef4\u9ad8\u65af\u6838\u51fd\u6570\u5728\u51c6\u786e\u8868\u793a\u4e0d\u8fde\u7eed\u51fd\u6570\u65b9\u9762\u5b58\u5728\u56fa\u6709\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u5f62\u72b6\u4e0d\u8fde\u7eed\u7684\u8fb9\u7f18\u548c\u89d2\u843d\uff0c\u4ee5\u53ca\u5728\u989c\u8272\u4e0d\u8fde\u7eed\u7684\u4e0d\u540c\u7eb9\u7406\u4e4b\u95f4\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u8bae\u91c7\u7528\u4e09\u7ef4\u534a\u9ad8\u65af\uff083D-HGS\uff09\u6838\u51fd\u6570\uff0c\u5b83\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u6838\u51fd\u6570\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u4eec\u80fd\u591f\u63d0\u9ad8\u5f53\u524d\u4e0e3D-GS\u76f8\u5173\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u5728\u4e0d\u5f71\u54cd\u6e32\u67d3\u901f\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6e32\u67d3\u6027\u80fd\u3002|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2409.02838": "|**2024-09-04**|[iConFormer: Dynamic Parameter-Efficient Tuning with Input-Conditioned Adaptation](http://arxiv.org/abs/2409.02838)|null|\u57fa\u4e8e\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u5b8c\u6574\u5fae\u8c03\uff08FFT\uff09\u548c\u4efb\u52a1\u7279\u5b9a\u89e3\u7801\u5668\u7684\u8fc1\u79fb\u5b66\u4e60\u968f\u7740\u6df1\u5ea6\u6a21\u578b\u7684\u6307\u6570\u7ea7\u589e\u957f\u800c\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\u3002\u4f7f\u7528\u7531\u5c0f\u578b\u53ef\u5b66\u4e60\u5c42\u7ec4\u6210\u7684\u9002\u914d\u5668\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u5df2\u6210\u4e3a FFT \u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u8bad\u7ec3\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u53ef\u6bd4\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u9002\u914d\u5668\u5bf9\u8f93\u5165\u5b9e\u4f8b\u7684\u4e0d\u7075\u6d3b\u9650\u5236\u4e86\u5176\u5728\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u4fe1\u606f\u7684\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 PEFT \u65b9\u6cd5\uff0c\u5373\u8f93\u5165\u6761\u4ef6\u5316\u7684 Transformer\uff0c\u79f0\u4e3a iConFormer\uff0c\u5b83\u5229\u7528\u4e86\u4ee5\u8f93\u5165\u5b9e\u4f8b\u4e3a\u6761\u4ef6\u7684\u52a8\u6001\u9002\u914d\u5668\u3002\u4e3a\u4e86\u786e\u4fdd\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5bf9\u8f93\u5165\u5b9e\u4f8b\u7684\u7075\u6d3b\u5b66\u4e60\u80fd\u529b\uff0c\u6211\u4eec\u5728\u52a8\u6001\u9002\u914d\u5668\u4e2d\u5f15\u5165\u4e86\u8f93\u5165\u6761\u4ef6\u5316\u7f51\u7edc\uff08iCoN\uff09\uff0c\u4ece\u800c\u5b9e\u73b0\u5b9e\u4f8b\u7ea7\u7279\u5f81\u8f6c\u6362\u3002\u5177\u4f53\u6765\u8bf4\uff0ciCoN \u4e3a\u6bcf\u4e2a\u7279\u5f81\u751f\u6210\u901a\u9053\u7ea7\u7684\u5377\u79ef\u6838\uff0c\u5e76\u4f7f\u7528\u81ea\u9002\u5e94\u5377\u79ef\u8fc7\u7a0b\u5bf9\u5176\u8fdb\u884c\u8f6c\u6362\uff0c\u4ee5\u6709\u6548\u6355\u83b7\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u4efb\u52a1\u7279\u5b9a\u548c\u7ec6\u7c92\u5ea6\u7ec6\u8282\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u4ec5\u8c03\u6574 Transformer \u4e3b\u5e72\u53c2\u6570\u7684 1.6% \u5230 2.8%\uff0ciConFormer \u5728\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u548c\u8bed\u4e49\u5206\u5272\u65b9\u9762\u5b9e\u73b0\u4e86\u4e0e FFT \u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u5b9e\u4f8b\u5206\u5272\u65b9\u9762\u4f18\u4e8e FFT\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6240\u6709\u4e0a\u8ff0\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u8fd1\u7684 PEFT \u65b9\u6cd5\u3002|\n", "2409.02546": "|**2024-09-04**|[Real-Time Dynamic Scale-Aware Fusion Detection Network: Take Road Damage Detection as an example](http://arxiv.org/abs/2409.02546)|null|\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u9053\u8def\u635f\u574f\u68c0\u6d4b (RDD) \u5bf9\u57ce\u5e02\u7684\u65e5\u5e38\u7ef4\u62a4\u548c\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u663e\u8457\u964d\u4f4e\u52b3\u52a8\u529b\u6210\u672c\u65b9\u9762\u3002\u7136\u800c\uff0c\u5f53\u524d\u57fa\u4e8e\u65e0\u4eba\u673a\u7684 RDD \u7814\u7a76\u4ecd\u9762\u4e34\u8bb8\u591a\u6311\u6218\u3002\u4f8b\u5982\uff0c\u5f62\u72b6\u548c\u65b9\u5411\u4e0d\u89c4\u5219\u7684\u635f\u574f\u3001\u80cc\u666f\u5bf9\u635f\u574f\u7684\u906e\u6321\u4ee5\u53ca\u96be\u4ee5\u533a\u5206\u635f\u574f\u548c\u80cc\u666f\uff0c\u8fd9\u4e9b\u56e0\u7d20\u90fd\u663e\u8457\u5f71\u54cd\u4e86\u65e0\u4eba\u673a\u5728\u65e5\u5e38\u5de1\u68c0\u4e2d\u68c0\u6d4b\u9053\u8def\u635f\u574f\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u63d0\u9ad8\u65e0\u4eba\u673a\u5b9e\u65f6\u9053\u8def\u635f\u574f\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u6211\u4eec\u8bbe\u8ba1\u5e76\u63d0\u51fa\u4e86\u4e09\u4e2a\u76f8\u5e94\u7684\u6a21\u5757\uff1a\u4e00\u4e2a\u80fd\u591f\u7075\u6d3b\u9002\u5e94\u5f62\u72b6\u548c\u80cc\u666f\u7684\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff1b\u4e00\u4e2a\u878d\u5408\u591a\u5c3a\u5ea6\u611f\u77e5\u5e76\u9002\u5e94\u5f62\u72b6\u548c\u80cc\u666f\u7684\u6a21\u5757\uff1b\u4e00\u4e2a\u9ad8\u6548\u7684\u4e0b\u91c7\u6837\u6a21\u5757\u3002 \u57fa\u4e8e\u8fd9\u4e9b\u6a21\u5757\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5177\u6709\u81ea\u52a8\u53bb\u9664\u80cc\u666f\u5e72\u6270\u80fd\u529b\u7684\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u9053\u8def\u635f\u574f\u68c0\u6d4b\u6a21\u578b\uff0c\u79f0\u4e3a\u52a8\u6001\u5c3a\u5ea6\u611f\u77e5\u878d\u5408\u68c0\u6d4b\u6a21\u578b (RT-DSAFDet)\u3002\u5728 UAV-PDD2023 \u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b RT-DSAFDet \u7684 mAP50 \u8fbe\u5230\u4e86 54.2%\uff0c\u6bd4\u6700\u65b0\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u6a21\u578b YOLOv10 \u7684\u9ad8\u6548\u53d8\u4f53 YOLOv10-m \u9ad8 11.1%\uff0c\u800c\u53c2\u6570\u91cf\u51cf\u5c11\u5230 1.8M\uff0cFLOPs \u51cf\u5c11\u5230 4.6G\uff0c\u5206\u522b\u964d\u4f4e\u4e86 88% \u548c 93%\u3002\u6b64\u5916\uff0c\u5728\u5927\u578b\u901a\u7528\u76ee\u6807\u68c0\u6d4b\u516c\u5f00\u6570\u636e\u96c6 MS COCO2017 \u4e0a\u4e5f\u5c55\u73b0\u4e86\u6211\u4eec\u6a21\u578b\u7684\u4f18\u8d8a\u6027\uff0c\u5176 mAP50-95 \u4e0e YOLOv9-t \u76f8\u540c\uff0c\u4f46 mAP50 \u9ad8\u51fa 0.5%\uff0c\u53c2\u6570\u91cf\u51cf\u5c11 10%\uff0cFLOPs \u51cf\u5c11 40%\u3002|\n", "2409.02486": "|**2024-09-04**|[Boosting Generalizability towards Zero-Shot Cross-Dataset Single-Image Indoor Depth by Meta-Initialization](http://arxiv.org/abs/2409.02486)|null|\u5ba4\u5185\u673a\u5668\u4eba\u7684\u5bfc\u822a\u6216\u969c\u788d\u7269\u68c0\u6d4b\u7b49\u4efb\u52a1\u4f9d\u8d56\u4e8e\u6df1\u5ea6\u4fe1\u606f\uff0c\u800c\u5355\u56fe\u50cf\u6df1\u5ea6\u4f30\u8ba1\u88ab\u5e7f\u6cdb\u7528\u4e8e\u8f85\u52a9\u611f\u77e5\u3002\u5927\u591a\u6570\u5ba4\u5185\u5355\u56fe\u50cf\u6df1\u5ea6\u9884\u6d4b\u8f83\u5c11\u5173\u6ce8\u6a21\u578b\u5bf9\u672a\u89c1\u6570\u636e\u96c6\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u66f4\u5173\u6ce8\u7cfb\u7edf\u90e8\u7f72\u7684\u91ce\u5916\u9c81\u68d2\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u5229\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u5143\u5b66\u4e60\u5728\u96f6\u6837\u672c\u8de8\u6570\u636e\u96c6\u63a8\u7406\u4e2d\u83b7\u5f97\u66f4\u9ad8\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4e0e\u7814\u7a76\u6700\u591a\u7684\u3001\u4e0e\u663e\u5f0f\u7c7b\u522b\u6807\u7b7e\u76f8\u5173\u7684\u56fe\u50cf\u5206\u7c7b\u5143\u5b66\u4e60\u4e0d\u540c\uff0c\u5bf9\u4e8e\u4e0e\u7269\u4f53\u6392\u5217\u548c\u573a\u666f\u6784\u6210\u65b9\u9762\u9ad8\u5ea6\u53d8\u5316\u7684\u5ba4\u5185\u73af\u5883\u76f8\u5173\u7684\u8fde\u7eed\u6df1\u5ea6\u503c\uff0c\u4e0d\u5b58\u5728\u660e\u786e\u7684\u4efb\u52a1\u8fb9\u754c\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u7ec6\u7c92\u5ea6\u4efb\u52a1\uff0c\u5728\u6211\u4eec\u7684\u5143\u5b66\u4e60\u516c\u5f0f\u4e2d\u5c06\u6bcf\u4e2aRGB-D\u5c0f\u6279\u91cf\u89c6\u4e3a\u4e00\u4e2a\u4efb\u52a1\u3002\u6211\u4eec\u9996\u5148\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6709\u9650\u6570\u636e\u4e0a\u8bf1\u5bfc\u51fa\u66f4\u597d\u7684\u5148\u9a8c\uff08RMSE \u6700\u9ad8\u964d\u4f4e 27.8%\uff09\u3002\u7136\u540e\uff0c\u5728\u5143\u5b66\u4e60\u521d\u59cb\u5316\u4e0a\u8fdb\u884c\u5fae\u8c03\u59cb\u7ec8\u4f18\u4e8e\u6ca1\u6709\u5143\u65b9\u6cd5\u7684\u57fa\u7ebf\u3002\u4e3a\u4e86\u5b9e\u73b0\u6cdb\u5316\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u96f6\u6837\u672c\u8de8\u6570\u636e\u96c6\u534f\u8bae\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7531\u6211\u4eec\u7684\u5143\u521d\u59cb\u5316\u8bf1\u5bfc\u7684\u66f4\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u4f5c\u4e3a\u8bb8\u591a\u73b0\u6709\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u7684\u7b80\u5355\u800c\u6709\u7528\u7684\u63d2\u4ef6\u3002\u6df1\u5ea6\u548c\u5143\u5b66\u4e60\u4ea4\u53c9\u9886\u57df\u7684\u5de5\u4f5c\u6709\u53ef\u80fd\u63a8\u52a8\u8fd9\u4e24\u9879\u7814\u7a76\u66f4\u63a5\u8fd1\u5b9e\u9645\u7684\u673a\u5668\u4eba\u548c\u673a\u5668\u611f\u77e5\u5e94\u7528\u3002|\n", "2409.02329": "|**2024-09-03**|[Site Selection for the Second Flyeye Telescope: A Simulation Study for Optimizing Near-Earth Object Discovery](http://arxiv.org/abs/2409.02329)|null|\u6b27\u6d32\u822a\u5929\u5c40 (ESA) \u6b63\u5728\u5f00\u53d1\u4e00\u4e2a\u540d\u4e3a Flyeye \u7684\u5e7f\u57df\u5de1\u5929\u671b\u8fdc\u955c\u7f51\u7edc\uff0c\u4ee5\u6539\u8fdb\u8fd1\u5730\u5929\u4f53 (NEO) \u7684\u53d1\u73b0\u3002\u8be5\u7f51\u7edc\u4e2d\u7684\u7b2c\u4e00\u4e2a\u671b\u8fdc\u955c\u5c06\u4f4d\u4e8e\u5317\u534a\u7403\u7684\u7a46\u6cd5\u62c9\u5c71\uff08\u610f\u5927\u5229\uff09\uff0c\u800c\u7b2c\u4e8c\u4e2a\u5177\u6709\u589e\u5f3a\u63a2\u6d4b\u80fd\u529b\u7684 Flyeye \u671b\u8fdc\u955c\u521a\u521a\u5f00\u59cb\u5173\u952e\u8bbe\u8ba1\u9636\u6bb5\u3002\u901a\u8fc7\u5bf9\u649e\u51fb\u8f68\u8ff9\u4e0a\u7684\u8fd1\u5730\u5929\u4f53\u8fdb\u884c\u6a21\u62df\uff0c\u7814\u7a76\u4e86\u7b2c\u4e8c\u4e2a Flyeye \u671b\u8fdc\u955c\u7684\u6f5c\u5728\u4f4d\u7f6e\u3002\u5bf9\u5927\u7ea6 3000 \u4e2a\u649e\u51fb\u5c0f\u884c\u661f\uff08\u7edd\u5bf9\u661f\u7b49\u4e3a H=25 \u548c H=28\uff09\u8fdb\u884c\u4e86\u4f20\u64ad\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e3b\u8981\u73b0\u6709\u5de1\u5929\u9879\u76ee\uff08Catalina\u3001Pan-STARRS\u3001ATLAS\uff09\u3001\u5373\u5c06\u6295\u5165\u4f7f\u7528\u7684\u8587\u62c9\u00b7\u9c81\u5bbe\u5929\u6587\u53f0 (LSST) \u4ee5\u53ca Flyeye \u53ef\u80fd\u9009\u5740\u7684\u53ef\u63a2\u6d4b\u6027\u3002 \u8003\u8651\u4e86\u667a\u5229\u3001\u5357\u975e\u548c\u5317\u534a\u7403\u7684\u7b2c\u4e8c\u4e2a\u8bbe\u65bd\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u5929\u6587\u53f0\uff0c\u5728\u6a21\u62df\u4e2d\u90fd\u8003\u8651\u4e86\u5b83\u4eec\u8fc7\u53bb\u6216\u8ba1\u5212\u7684\u6307\u5411\u7b56\u7565\u3002\u5728 LSST \u90e8\u7f72\u4e4b\u524d\uff0c\u5357\u534a\u7403\u7684\u4e00\u4e2a Flyeye \u7684\u6027\u80fd\u4e0e\u5317\u534a\u7403\u7684\u4e00\u4e2a\u671b\u8fdc\u955c\u76f8\u4f3c\u3002\u7ed3\u5408\u8d77\u6765\uff0c\u5728\u5317\u65b9\u548c\u5357\u65b9\u5404\u653e\u7f6e\u4e00\u53f0\u671b\u8fdc\u955c\u53ef\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u63a2\u6d4b\u7387\u548c\u63a2\u6d4b\u5230\u7684\u72ec\u7279\u7269\u4f53\u7684\u6570\u91cf\u3002LSST \u4e4b\u540e\uff0c\u5357\u90e8\u548c\u5317\u90e8\u7684 Flyeye \u671b\u8fdc\u955c\u4ecd\u7136\u662f\u4e92\u8865\u7684\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6a21\u62df\u8868\u660e\uff0c\u65e0\u8bba\u662f\u5728 LSST \u4e4b\u524d\u8fd8\u662f\u4e4b\u540e\uff0c\u4f4d\u4e8e\u5357\u90e8\u7684\u7b2c\u4e8c\u4e2a Flyeye \u90fd\u53ef\u4ee5\u8865\u5145\u4f4d\u4e8e\u5317\u90e8\u7684 Flyeye \u671b\u8fdc\u955c\u3002\u4f4d\u4e8e\u62c9\u897f\u62c9\u7684 Flyeye \u5c06\u5229\u7528\u5176\u4f18\u8d8a\u7684\u5927\u6c14\u6761\u4ef6\uff0c\u540c\u65f6\u5e73\u8861\u5357\u5317\u534a\u7403\u7684\u8d44\u4ea7\u3002|\n", "2409.02281": "|**2024-09-03**|[K-Origins: Better Colour Quantification for Neural Networks](http://arxiv.org/abs/2409.02281)|null|K-Origins\u662f\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u5c42\uff0c\u65e8\u5728\u5728\u5b66\u4e60\u989c\u8272\u6216\u5f3a\u5ea6\u6709\u5229\u65f6\u63d0\u9ad8\u57fa\u4e8e\u56fe\u50cf\u7684\u7f51\u7edc\u6027\u80fd\u3002 \u8d85\u8fc7 250 \u4e2a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5377\u79ef\u7f51\u7edc\u5728 16 \u4f4d\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0cK-Origins \u63d0\u9ad8\u4e86\u8bed\u4e49\u5206\u5272\u7cbe\u5ea6\uff1a\u4f4e\u4fe1\u566a\u6bd4\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u4ee5\u53ca\u5206\u5272\u5f62\u72b6\u76f8\u540c\u4f46\u989c\u8272\u4e0d\u540c\u7684\u591a\u4e2a\u76ee\u6807\u3002 \u5bf9\u4e8e\u6bcf\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570$w_k$\uff0cK-Origins \u901a\u8fc7\u516c\u5f0f $\\textbf{Y}_k = \\textbf{X}-\\textbf{J}\\cdot w_k$ \u4ece\u8f93\u5165\u7279\u5f81 $\\textbf{X}$ \u751f\u6210\u8f93\u51fa\u7279\u5f81\uff0c\u5176\u4e2d $\\textbf{J}$ \u662f\u4e00\u4e2a\u5168 1 \u77e9\u9635\u3002 \u6b64\u5916\uff0c\u8fd8\u8bad\u7ec3\u4e86\u5177\u6709\u4e0d\u540c\u611f\u53d7\u91ce\u7684\u7f51\u7edc\uff0c\u4ee5\u6839\u636e\u76ee\u6807\u7c7b\u522b\u7684\u7ef4\u5ea6\u786e\u5b9a\u6700\u4f73\u7f51\u7edc\u6df1\u5ea6\uff0c\u8fd9\u8868\u660e\u611f\u53d7\u91ce\u957f\u5ea6\u5e94\u8d85\u8fc7\u76ee\u6807\u5927\u5c0f\u3002 \u901a\u8fc7\u786e\u4fdd\u8db3\u591f\u7684\u611f\u53d7\u91ce\u957f\u5ea6\u5e76\u7ed3\u5408 K-Origins\uff0c\u6211\u4eec\u53ef\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u8bed\u4e49\u7f51\u7edc\u6027\u80fd\u3002|\n", "2409.02278": "|**2024-09-03**|[Evaluation and Comparison of Visual Language Models for Transportation Engineering Problems](http://arxiv.org/abs/2409.02278)|null|\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\u5c55\u73b0\u51fa\u5176\u5728\u56fe\u50cf\u7406\u89e3\u76f8\u5173\u5e94\u7528\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u4ea4\u901a\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4f8b\u5982\u56fe\u50cf\u5206\u7c7b\u548c\u76ee\u6807\u68c0\u6d4b\u3002\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u5305\u62ec\u62e5\u5835\u68c0\u6d4b\u548c\u88c2\u7f1d\u8bc6\u522b\uff0c\u800c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u5219\u7528\u4e8e\u8bc6\u522b\u672a\u4f69\u6234\u5934\u76d4\u7684\u884c\u4e3a\u3002\u6211\u4eec\u5e94\u7528\u4e86\u5f00\u6e90\u6a21\u578b\uff08\u5982CLIP\u3001BLIP\u3001OWL-ViT\u3001Llava-Next\uff09\u548c\u95ed\u6e90\u6a21\u578bGPT-4o\uff0c\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4ee5\u5229\u7528\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u6765\u5b8c\u6210\u57fa\u4e8e\u89c6\u89c9\u7684\u4ea4\u901a\u4efb\u52a1\u3002\u8fd9\u4e9b\u4efb\u52a1\u901a\u8fc7\u5bf9VLM\u6a21\u578b\u5e94\u7528\u96f6\u6837\u672c\u63d0\u793a\u6765\u5b8c\u6210\uff0c\u56e0\u4e3a\u96f6\u6837\u672c\u63d0\u793a\u53ef\u4ee5\u5728\u4e0d\u5bf9\u4efb\u52a1\u8fdb\u884c\u4efb\u4f55\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u4efb\u52a1\u3002\u8fd9\u6d88\u9664\u4e86\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u6807\u6ce8\u6570\u636e\u96c6\u6216\u5fae\u8c03\u7684\u9700\u6c42\u3002\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4e0e\u57fa\u51c6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u4f46\u5728\u76ee\u6807\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4ecd\u6709\u6539\u8fdb\u7684\u7a7a\u95f4\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u5bf9\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u7a81\u51fa\u4e86\u8fd9\u4e9b\u6a21\u578b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u672a\u6765\u6539\u8fdb\u548c\u5e7f\u6cdb\u5b9e\u65bd\u7684\u57fa\u51c6\u3002|\n", "2409.02035": "|**2024-09-03**|[A Modern Take on Visual Relationship Reasoning for Grasp Planning](http://arxiv.org/abs/2409.02035)|null|\u4e0e\u73b0\u5b9e\u4e16\u754c\u6742\u4e71\u573a\u666f\u4ea4\u4e92\u5bf9\u673a\u5668\u4eba\u4ee3\u7406\u63d0\u51fa\u4e86\u82e5\u5e72\u6311\u6218\uff0c\u8fd9\u4e9b\u4ee3\u7406\u9700\u8981\u7406\u89e3\u89c2\u5bdf\u5230\u7684\u7269\u4f53\u4e4b\u95f4\u590d\u6742\u7684\u7684\u7a7a\u95f4\u4f9d\u8d56\u6027\uff0c\u4ee5\u786e\u5b9a\u6700\u4f73\u62fe\u53d6\u987a\u5e8f\u6216\u6709\u6548\u7684\u7269\u4f53\u68c0\u7d22\u7b56\u7565\u3002 \u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u7ba1\u7406\u7b80\u5316\u7684\u573a\u666f\uff0c\u5e76\u4fa7\u91cd\u4e8e\u5728\u521d\u59cb\u7269\u4f53\u68c0\u6d4b\u9636\u6bb5\u4e4b\u540e\u9884\u6d4b\u6210\u5bf9\u7269\u4f53\u5173\u7cfb\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u5168\u5c40\u4e0a\u4e0b\u6587\u6216\u96be\u4ee5\u5904\u7406\u5197\u4f59\u548c\u7f3a\u5931\u7684\u7269\u4f53\u5173\u7cfb\u3002 \u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6293\u53d6\u89c4\u5212\u7684\u89c6\u89c9\u5173\u7cfb\u63a8\u7406\u7684\u73b0\u4ee3\u65b9\u6cd5\u3002 \u6211\u4eec\u4ecb\u7ecd\u4e86 D3GD\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5176\u4e2d\u5305\u62ec\u5305\u542b\u6765\u81ea 97 \u4e2a\u4e0d\u540c\u7c7b\u522b\u7684\u591a\u8fbe 35 \u4e2a\u7269\u4f53\u7684\u5206\u62e3\u573a\u666f\u3002 \u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86 D3G\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u7aef\u5230\u7aef transformer \u7684\u4f9d\u8d56\u56fe\u751f\u6210\u6a21\u578b\uff0c\u5b83\u53ef\u4ee5\u540c\u65f6\u68c0\u6d4b\u7269\u4f53\u5e76\u751f\u6210\u8868\u793a\u5176\u7a7a\u95f4\u5173\u7cfb\u7684\u90bb\u63a5\u77e9\u9635\u3002 \u8ba4\u8bc6\u5230\u6807\u51c6\u6307\u6807\u7684\u5c40\u9650\u6027\uff0c\u6211\u4eec\u9996\u6b21\u91c7\u7528\u5173\u7cfb\u5e73\u5747\u7cbe\u5ea6\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u57fa\u51c6\u6d4b\u8bd5\u3002 \u83b7\u5f97\u7684\u7ed3\u679c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u8fd9\u9879\u4efb\u52a1\u7684\u6700\u65b0\u6280\u672f\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002 \u6211\u4eec\u5728 https://paolotron.github.io/d3g.github.io \u4e0a\u516c\u5f00\u53d1\u5e03\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002|\n", "2409.01988": "|**2024-09-03**|[Compressed learning based onboard semantic compression for remote sensing platforms](http://arxiv.org/abs/2409.01988)|null|\u5730\u7403\u89c2\u6d4b (EO) \u5728\u521b\u5efa\u548c\u7ef4\u6301\u4e00\u4e2a\u5177\u6709\u5f39\u6027\u548c\u7e41\u8363\u7684\u793e\u4f1a\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u8fd9\u5bf9\u6240\u6709\u751f\u547d\u548c\u5730\u7403\u672c\u8eab\u90fd\u5177\u6709\u6df1\u8fdc\u7684\u5f71\u54cd\u3002\u536b\u661f\u3001\u822a\u7a7a\u5e73\u53f0\u4ee5\u53ca\u6700\u8fd1\u7684\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u9a7e\u9a76\u98de\u884c\u5668\u7b49\u9065\u611f\u5e73\u53f0\u90fd\u7528\u4e8e EO\u3002\u5b83\u4eec\u6536\u96c6\u5927\u91cf\u6570\u636e\uff0c\u9700\u8981\u5c06\u5176\u4e0b\u4f20\u5230\u5730\u7403\u8fdb\u884c\u8fdb\u4e00\u6b65\u5904\u7406\u548c\u5206\u6790\u3002\u8fd9\u79cd\u9ad8\u541e\u5410\u91cf\u91c7\u96c6\u7684\u74f6\u9888\u662f\u4e0b\u884c\u94fe\u8def\u5e26\u5bbd\u3002\u9700\u8981\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u56fe\u50cf\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u6765\u5e94\u5bf9\u8fd9\u79cd\u6d77\u91cf\u6570\u636e\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u901a\u8fc7\u538b\u7f29\u5b66\u4e60\u6846\u67b6\u7814\u7a76\u4e86\u8bed\u4e49\u538b\u7f29\uff0c\u8be5\u6846\u67b6\u4ec5\u5229\u7528\u5feb\u901f\u548c\u7a00\u758f\u7684\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\u6765\u7f16\u7801\u6570\u636e\u3002\u76f8\u673a\u566a\u58f0\u548c\u901a\u4fe1\u4fe1\u9053\u662f\u9020\u6210\u5931\u771f\u7684\u4e3b\u8981\u6765\u6e90\u3002\u7136\u540e\uff0c\u5b8c\u6574\u7684\u8bed\u4e49\u901a\u4fe1\u7ba1\u9053\u7531\u4e00\u4e2a\u5b66\u4e60\u5230\u7684\u4f4e\u590d\u6742\u5ea6\u538b\u7f29\u77e9\u9635\u7ec4\u6210\uff0c\u8be5\u77e9\u9635\u4f5c\u7528\u4e8e\u566a\u58f0\u76f8\u673a\u8f93\u51fa\uff0c\u4ee5\u5728\u673a\u8f7d\u751f\u6210\u4e00\u4e2a\u89c2\u6d4b\u5411\u91cf\uff0c\u8be5\u5411\u91cf\u901a\u8fc7\u901a\u4fe1\u4fe1\u9053\u4e0b\u884c\u94fe\u8def\u4f20\u8f93\uff0c\u901a\u8fc7\u5c55\u5f00\u7f51\u7edc\u5904\u7406\uff0c\u7136\u540e\u9988\u9001\u5230\u6267\u884c\u5fc5\u8981\u4e0b\u6e38\u4efb\u52a1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1b\u7814\u7a76\u4e86\u56fe\u50cf\u5206\u7c7b\u3002\u901a\u8fc7\u4f7f\u7528\u5c0f\u6ce2\u7a00\u758f\u5148\u9a8c\u5c55\u5f00 NA-ALISTA \u7684\u5c42\u6765\u8865\u507f\u5931\u771f\u3002\u56e0\u6b64\uff0c\u89e3\u7801\u662f\u4e00\u79cd\u6839\u636e\u76f8\u673a/\u73af\u5883\u4fe1\u606f\u548c\u4e0b\u6e38\u4efb\u52a1\u8bbe\u8ba1\u7684\u5373\u63d2\u5373\u7528\u65b9\u6cd5\u3002\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u901a\u8fc7\u7aef\u5230\u7aef\u65b9\u5f0f\u7684\u635f\u5931\u51fd\u6570\u4e0e\u538b\u7f29\u77e9\u9635\u548c\u5c55\u5f00\u7f51\u7edc\u8054\u5408\u5fae\u8c03\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4f4e\u538b\u7f29\u6bd4\u7684\u566a\u58f0\u73af\u5883\u4e2d\uff0c\u6dfb\u52a0\u6062\u590d\u635f\u5931\u4ee5\u53ca\u4efb\u52a1\u76f8\u5173\u635f\u5931\u53ef\u4ee5\u63d0\u9ad8\u4e0b\u6e38\u6027\u80fd\u3002|\n", "2409.01872": "|**2024-09-03**|[Latent Distillation for Continual Object Detection at the Edge](http://arxiv.org/abs/2409.01872)|**[link](https://github.com/pastifra/Continual_Nanodet)**|\u867d\u7136\u5728\u76ee\u6807\u68c0\u6d4b\u6587\u732e\u4e2d\u5b58\u5728\u8bb8\u591a\u6027\u80fd\u5353\u8d8a\u7684\u65b9\u6cd5\uff0c\u4f46\u89e3\u51b3\u6570\u636e\u5206\u5e03\u504f\u79fb\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u4e3a\u8fd9\u4e2a\u95ee\u9898\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u9002\u5e94\u65b0\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5148\u524d\u6570\u636e\u7684\u6027\u80fd\u3002\u8fd9\u5bf9\u4e8e\u8fb9\u7f18\u8bbe\u5907\u5c24\u5176\u91cd\u8981\uff0c\u8fd9\u4e9b\u8bbe\u5907\u5728\u6c7d\u8f66\u548c\u673a\u5668\u4eba\u7b49\u52a8\u6001\u73af\u5883\u4e2d\u5f88\u5e38\u89c1\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u76ee\u6807\u68c0\u6d4b\u6301\u7eed\u5b66\u4e60\uff08CLOD\uff09\u573a\u666f\u4e2d\u8fb9\u7f18\u8bbe\u5907\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u9650\u5236\u3002\u5177\u4f53\u6765\u8bf4\uff0c\uff08i\uff09\u6211\u4eec\u7814\u7a76\u4e86\u4e00\u79cd\u5f00\u6e90\u3001\u8f7b\u91cf\u7ea7\u548c\u5feb\u901f\u7684\u68c0\u6d4b\u5668 NanoDet \u5bf9\u8fb9\u7f18\u8bbe\u5907\u4e0a CLOD \u7684\u9002\u7528\u6027\uff0c\u6539\u8fdb\u4e86\u6587\u732e\u4e2d\u4f7f\u7528\u7684\u8f83\u5927\u67b6\u6784\u3002\u6b64\u5916\uff0c\uff08ii\uff09\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6f5c\u5728\u84b8\u998f\uff08LD\uff09\u7684\u65b0\u578b CL \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u663e\u7740\u5f71\u54cd\u68c0\u6d4b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u4e86\u6700\u5148\u8fdb\u7684 CL \u65b9\u6cd5\u6240\u9700\u7684\u8fd0\u7b97\u6b21\u6570\u548c\u5185\u5b58\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u8457\u540d\u7684 VOC \u548c COCO \u57fa\u51c6\u6d4b\u8bd5\u96c6\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u4e0e\u5176\u4ed6\u84b8\u998f\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6bcf\u6b21\u6a21\u578b\u66f4\u65b0\u53ef\u5c06\u84b8\u998f\u53c2\u6570\u5f00\u9500\u51cf\u5c11 74%\uff0c\u5c06\u6d6e\u70b9\u8fd0\u7b97\uff08FLOPs\uff09\u51cf\u5c11 56%\u3002|\n", "2409.01816": "|**2024-09-03**|[GeoBEV: Learning Geometric BEV Representation for Multi-view 3D Object Detection](http://arxiv.org/abs/2409.01816)|null|\u9e1f\u77b0\u56fe (BEV) \u8868\u793a\u5df2\u6210\u4e3a\u591a\u89c6\u56fe 3D \u5bf9\u8c61\u68c0\u6d4b\u7684\u4e3b\u6d41\u8303\u5f0f\uff0c\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u611f\u77e5\u80fd\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86 BEV \u8868\u793a\u7684\u51e0\u4f55\u8d28\u91cf\uff0c\u4f7f\u5176\u5904\u4e8e\u4f4e\u5206\u8fa8\u7387\u72b6\u6001\uff0c\u65e0\u6cd5\u6062\u590d\u573a\u666f\u771f\u5b9e\u7684\u51e0\u4f55\u4fe1\u606f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u5148\u524d\u65b9\u6cd5\u53d7\u9650\u4e8e\u4f4e BEV \u8868\u793a\u5206\u8fa8\u7387\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u5f84\u5411-\u7b1b\u5361\u5c14 BEV \u91c7\u6837 (RC-Sampling)\uff0c\u4ece\u800c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u5bc6\u96c6 BEV \u8868\u793a\uff0c\u800c\u65e0\u9700\u590d\u6742\u7684\u7b97\u5b50\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u76d2\u5185\u6807\u7b7e\u6765\u66ff\u4ee3\u4ece\u6fc0\u5149\u96f7\u8fbe\u70b9\u751f\u6210\u7684\u4f20\u7edf\u6df1\u5ea6\u6807\u7b7e\u3002\u6b64\u6807\u7b7e\u53cd\u6620\u4e86\u5bf9\u8c61\u7684\u5b9e\u9645\u51e0\u4f55\u7ed3\u6784\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5b83\u4eec\u7684\u8868\u9762\uff0c\u5c06\u73b0\u5b9e\u4e16\u754c\u7684\u51e0\u4f55\u4fe1\u606f\u6ce8\u5165 BEV \u8868\u793a\u4e2d\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u76d2\u5185\u6807\u7b7e\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u8d28\u5fc3\u611f\u77e5\u5185\u90e8\u635f\u5931 (CAI \u635f\u5931) \u6765\u6355\u6349\u5bf9\u8c61\u7684\u7ec6\u7c92\u5ea6\u5185\u90e8\u51e0\u4f55\u7ed3\u6784\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c06\u4e0a\u8ff0\u6a21\u5757\u96c6\u6210\u5230\u4e00\u4e2a\u540d\u4e3a GeoBEV \u7684\u65b0\u578b\u591a\u89c6\u56fe 3D \u5bf9\u8c61\u68c0\u6d4b\u6846\u67b6\u4e2d\u3002\u5728 nuScenes \u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGeoBEV \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7a81\u51fa\u4e86\u5176\u6709\u6548\u6027\u3002|\n"}, "\u751f\u6210\u6a21\u578b": {"2409.02919": "|**2024-09-04**|[HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts](http://arxiv.org/abs/2409.02919)|**[link](https://github.com/Liuxinyv/HiPrompt)**|\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u751f\u6210\u66f4\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5728\u5904\u7406\u7269\u4f53\u91cd\u590d\u548c\u7ed3\u6784\u4f2a\u5f71\u65b9\u9762\u5e38\u5e38\u9047\u5230\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u6269\u5c55\u5230 4K \u53ca\u66f4\u9ad8\u5206\u8fa8\u7387\u65f6\u3002\u6211\u4eec\u53d1\u73b0\u95ee\u9898\u5728\u4e8e\uff0c\u5355\u4e2a\u63d0\u793a\u751f\u6210\u591a\u4e2a\u5c3a\u5ea6\u7684\u65b9\u5f0f\u6548\u7387\u4f4e\u4e0b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 HiPrompt\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u987b\u5fae\u8c03\u7684\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u901a\u8fc7\u5f15\u5165\u5206\u5c42\u63d0\u793a\u6765\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002\u5206\u5c42\u63d0\u793a\u63d0\u4f9b\u5168\u5c40\u548c\u5c40\u90e8\u6307\u5bfc\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5168\u5c40\u6307\u5bfc\u6765\u81ea\u63cf\u8ff0\u6574\u4f53\u5185\u5bb9\u7684\u7528\u6237\u8f93\u5165\uff0c\u800c\u5c40\u90e8\u6307\u5bfc\u5219\u5229\u7528\u6765\u81ea MLLM \u7684\u9010\u5757\u63cf\u8ff0\u6765\u7cbe\u5fc3\u6307\u5bfc\u5c40\u90e8\u7ed3\u6784\u548c\u7eb9\u7406\u7684\u751f\u6210\u3002\u6b64\u5916\uff0c\u5728\u9006\u5411\u53bb\u566a\u8fc7\u7a0b\u4e2d\uff0c\u751f\u6210\u7684\u566a\u58f0\u88ab\u5206\u89e3\u4e3a\u4f4e\u9891\u548c\u9ad8\u9891\u7a7a\u95f4\u5206\u91cf\u3002\u8fd9\u4e9b\u5206\u91cf\u4ee5\u591a\u4e2a\u63d0\u793a\u7ea7\u522b\u4e3a\u6761\u4ef6\uff0c\u5305\u62ec\u8be6\u7ec6\u7684\u9010\u5757\u63cf\u8ff0\u548c\u66f4\u5e7f\u6cdb\u7684\u56fe\u50cf\u7ea7\u63d0\u793a\uff0c\u4ece\u800c\u4fc3\u8fdb\u5728\u5206\u5c42\u8bed\u4e49\u6307\u5bfc\u4e0b\u7684\u63d0\u793a\u5f15\u5bfc\u53bb\u566a\u3002\u5b83\u8fdb\u4e00\u6b65\u5141\u8bb8\u751f\u6210\u8fc7\u7a0b\u66f4\u591a\u5730\u5173\u6ce8\u5c40\u90e8\u7a7a\u95f4\u533a\u57df\uff0c\u5e76\u786e\u4fdd\u751f\u6210\u7684\u56fe\u50cf\u5728\u9ad8\u6e05\u6670\u5ea6\u4e0b\u4fdd\u6301\u4e00\u81f4\u7684\u5c40\u90e8\u548c\u5168\u5c40\u8bed\u4e49\u3001\u7ed3\u6784\u548c\u7eb9\u7406\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cHiPrompt \u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u7269\u4f53\u91cd\u590d\u5e76\u63d0\u9ad8\u4e86\u7ed3\u6784\u8d28\u91cf\u3002|\n", "2409.02915": "|**2024-09-04**|[Latent Watermarking of Audio Generative Models](http://arxiv.org/abs/2409.02915)|null|\u97f3\u9891\u751f\u6210\u6a21\u578b\u7684\u8fdb\u6b65\u7ed9\u5176\u8d1f\u8d23\u4efb\u7684\u62ab\u9732\u548c\u6ee5\u7528\u68c0\u6d4b\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u901a\u8fc7\u5bf9\u5176\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u7279\u5b9a\u6c34\u5370\u6765\u6807\u8bb0\u6f5c\u5728\u751f\u6210\u6a21\u578b\u7684\u65b9\u6cd5\u3002\u7531\u6b64\u4ea7\u751f\u7684\u6c34\u5370\u6a21\u578b\u751f\u6210\u7684\u6f5c\u5728\u8868\u793a\uff0c\u5176\u89e3\u7801\u8f93\u51fa\u53ef\u4ee5\u88ab\u9ad8\u7f6e\u4fe1\u5ea6\u5730\u68c0\u6d4b\u5230\uff0c\u800c\u65e0\u8bba\u4f7f\u7528\u4f55\u79cd\u89e3\u7801\u65b9\u6cd5\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e0\u9700\u8fdb\u884c\u4e8b\u540e\u6c34\u5370\u6b65\u9aa4\u5373\u53ef\u68c0\u6d4b\u751f\u6210\u7684\u5185\u5bb9\u3002\u5b83\u4e3a\u5f00\u6e90\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6709\u52a9\u4e8e\u8bc6\u522b\u90a3\u4e9b\u5728\u672a\u9075\u5b88\u8bb8\u53ef\u6761\u6b3e\u7684\u60c5\u51b5\u4e0b\u5bf9\u8fd9\u4e9b\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u6216\u4f7f\u7528\u7684\u884d\u751f\u4f5c\u54c1\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u5bf9\u6f5c\u5728\u751f\u6210\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u540e\uff0c\u751f\u6210\u8f93\u51fa\u7684\u68c0\u6d4b\u7cbe\u5ea6\u4e5f\u80fd\u5728\u5047\u9633\u6027\u7387\u4e3a$10^{-3}$\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230 75% \u4ee5\u4e0a\u3002|\n", "2409.02908": "|**2024-09-04**|[Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling](http://arxiv.org/abs/2409.02908)|null|\u63a9\u7801\u6269\u6563\u6a21\u578b (MDM) \u7531\u4e8e\u5176\u76f8\u8f83\u4e8e\u5176\u4ed6\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5df2\u6210\u4e3a\u79bb\u6563\u6570\u636e\u751f\u6210\u5efa\u6a21\u7684\u70ed\u95e8\u7814\u7a76\u8bfe\u9898\uff0c\u5e76\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u4e0e\u81ea\u56de\u5f52\u6a21\u578b (ARM) \u5c55\u5f00\u7ade\u4e89\u3002\u6700\u8fd1\u7b80\u5316\u63a9\u7801\u6269\u6563\u6846\u67b6\u7684\u52aa\u529b\u8fdb\u4e00\u6b65\u4f7f\u5176\u4e0e\u8fde\u7eed\u7a7a\u95f4\u6269\u6563\u6a21\u578b\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u83b7\u5f97\u4e86\u66f4\u6709\u539f\u5219\u7684\u8bad\u7ec3\u548c\u91c7\u6837\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63ed\u793a\u4e86 MDM \u7684\u8bad\u7ec3\u548c\u91c7\u6837\u5728\u7406\u8bba\u4e0a\u90fd\u53ef\u4ee5\u6446\u8131\u65f6\u95f4\u53d8\u91cf\uff08\u53ef\u4ee5\u8bf4\u662f\u6269\u6563\u6a21\u578b\u7684\u5173\u952e\u7279\u5f81\uff09\uff0c\u5e76\u4e14\u7b49\u6548\u4e8e\u63a9\u7801\u6a21\u578b\u3002\u6211\u4eec\u5728\u91c7\u6837\u65b9\u9762\u7684\u8054\u7cfb\u662f\u901a\u8fc7\u6211\u4eec\u63d0\u51fa\u7684\u9996\u6b21\u547d\u4e2d\u91c7\u6837\u5668 (FHS) \u5efa\u7acb\u7684\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 FHS \u5728\u7406\u8bba\u4e0a\u7b49\u6548\u4e8e MDM \u7684\u539f\u59cb\u751f\u6210\u8fc7\u7a0b\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8017\u65f6\u7684\u5206\u7c7b\u91c7\u6837\uff0c\u5e76\u5b9e\u73b0\u4e86 20 \u500d\u7684\u52a0\u901f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u7814\u7a76\u5bf9\u5148\u524d\u5173\u4e8e MDM \u5728\u751f\u6210\u56f0\u60d1\u5ea6\u65b9\u9762\u53ef\u4ee5\u8d85\u8d8a ARM \u7684\u8bf4\u6cd5\u63d0\u51fa\u4e86\u8d28\u7591\u3002\u6211\u4eec\u9996\u6b21\u53d1\u73b0\u4e86\u4e00\u4e2a\u6f5c\u5728\u7684\u6570\u503c\u95ee\u9898\uff0c\u5373\u4f7f\u4f7f\u7528 32 \u4f4d\u6d6e\u70b9\u7cbe\u5ea6\uff0c\u4e5f\u4f1a\u5bfc\u81f4\u4e0d\u51c6\u786e\u7684\u5206\u7c7b\u91c7\u6837\u3002\u6211\u4eec\u8868\u660e\uff0c\u8be5\u6570\u503c\u95ee\u9898\u5728\u7406\u8bba\u4e0a\u548c\u7ecf\u9a8c\u4e0a\u90fd\u964d\u4f4e\u4e86\u6709\u6548\u6e29\u5ea6\uff0c\u5bfc\u81f4\u5148\u524d\u6587\u732e\u4e2d\u5bf9 MDM \u751f\u6210\u7ed3\u679c\u7684\u8bc4\u4f30\u4e0d\u516c\u5e73\u3002|\n", "2409.02851": "|**2024-09-04**|[Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video Diffusion Models](http://arxiv.org/abs/2409.02851)|null|\u4ece\u5355\u5f20RGB\u56fe\u50cf\u751f\u6210\u903c\u771f3D\u4eba\u4f53\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u7cbe\u786e\u7684\u51e0\u4f55\u5efa\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u7eb9\u7406\u548c\u5408\u7406\u7684\u4e0d\u53ef\u89c1\u90e8\u5206\u751f\u6210\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u8fdb\u884c3D\u4eba\u4f53\u751f\u6210\uff0c\u4f46\u5b83\u4eec\u7ecf\u5e38\u9762\u4e34\u89c6\u89d2\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u8fd9\u963b\u788d\u4e86\u9ad8\u8d28\u91cf3D\u4eba\u4f53\u7684\u751f\u6210\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Human-VDM\uff0c\u4e00\u79cd\u4f7f\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u4ece\u5355\u5f20RGB\u56fe\u50cf\u751f\u62103D\u4eba\u4f53\u7684\u65b0\u65b9\u6cd5\u3002Human-VDM\u4f7f\u7528\u9ad8\u65af\u6e32\u67d3\u4e3a3D\u4eba\u4f53\u751f\u6210\u63d0\u4f9b\u4e86\u65f6\u95f4\u4e0a\u4e00\u81f4\u7684\u89c6\u56fe\u3002\u5b83\u7531\u4e09\u4e2a\u6a21\u5757\u7ec4\u6210\uff1a\u89c6\u56fe\u4e00\u81f4\u7684\u4eba\u4f53\u89c6\u9891\u6269\u6563\u6a21\u5757\u3001\u89c6\u9891\u589e\u5f3a\u6a21\u5757\u548c\u9ad8\u65af\u6e32\u67d3\u6a21\u5757\u3002\u9996\u5148\uff0c\u5c06\u5355\u5f20\u56fe\u50cf\u8f93\u5165\u4eba\u4f53\u89c6\u9891\u6269\u6563\u6a21\u5757\u4ee5\u751f\u6210\u8fde\u8d2f\u7684\u4eba\u4f53\u89c6\u9891\u3002\u63a5\u4e0b\u6765\uff0c\u89c6\u9891\u589e\u5f3a\u6a21\u5757\u5e94\u7528\u8d85\u5206\u8fa8\u7387\u548c\u89c6\u9891\u63d2\u503c\u6765\u589e\u5f3a\u751f\u6210\u89c6\u9891\u7684\u7eb9\u7406\u548c\u51e0\u4f55\u5e73\u6ed1\u5ea6\u3002\u6700\u540e\uff0c3D\u4eba\u4f53\u9ad8\u65af\u6e32\u67d3\u6a21\u5757\u5728\u8fd9\u4e9b\u9ad8\u5206\u8fa8\u7387\u548c\u89c6\u89d2\u4e00\u81f4\u7684\u56fe\u50cf\u7684\u6307\u5bfc\u4e0b\u5b66\u4e60\u903c\u771f\u7684\u4eba\u4f53\u3002\u5b9e\u9a8c\u8868\u660e\uff0cHuman-VDM\u53ef\u4ee5\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u4eba\u4f53\uff0c\u5728\u751f\u6210\u8d28\u91cf\u548c\u6570\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://human-vdm.github.io/Human-VDM/|\n", "2409.02845": "|**2024-09-04**|[Multi-Track MusicLDM: Towards Versatile Music Generation with Latent Diffusion Model](http://arxiv.org/abs/2409.02845)|null|\u6269\u6563\u6a21\u578b\u5728\u6d89\u53ca\u97f3\u9891\u548c\u97f3\u4e50\u7684\u8de8\u6a21\u6001\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f8b\u5982\u6587\u672c\u5230\u58f0\u97f3\u548c\u6587\u672c\u5230\u97f3\u4e50\u7684\u751f\u6210\u3002\u8fd9\u4e9b\u6587\u672c\u63a7\u5236\u7684\u97f3\u4e50\u751f\u6210\u6a21\u578b\u901a\u5e38\u4fa7\u91cd\u4e8e\u901a\u8fc7\u6355\u6349\u5168\u5c40\u97f3\u4e50\u5c5e\u6027\uff08\u5982\u6d41\u6d3e\u548c\u60c5\u7eea\uff09\u6765\u751f\u6210\u97f3\u4e50\u3002\u7136\u800c\uff0c\u97f3\u4e50\u521b\u4f5c\u662f\u4e00\u9879\u590d\u6742\u7684\u591a\u5c42\u6b21\u4efb\u52a1\uff0c\u901a\u5e38\u5c06\u97f3\u4e50\u7f16\u6392\u4f5c\u4e3a\u521b\u4f5c\u8fc7\u7a0b\u7684\u4e00\u4e2a\u7ec4\u6210\u90e8\u5206\u3002\u6b64\u8fc7\u7a0b\u6d89\u53ca\u521b\u4f5c\u6bcf\u4e2a\u4e50\u5668\u90e8\u5206\uff0c\u4f7f\u5176\u5728\u8282\u594f\u3001\u529b\u5ea6\u3001\u548c\u58f0\u548c\u65cb\u5f8b\u65b9\u9762\u4e0e\u73b0\u6709\u90e8\u5206\u4fdd\u6301\u4e00\u81f4\uff0c\u8fd9\u9700\u8981\u6bd4\u6587\u672c\u63d0\u793a\u901a\u5e38\u63d0\u4f9b\u7684\u66f4\u7cbe\u786e\u7684\u97f3\u8f68\u63a7\u5236\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5c06 MusicLDM\uff08\u4e00\u79cd\u7528\u4e8e\u97f3\u4e50\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff09\u6269\u5c55\u4e3a\u591a\u8f68\u751f\u6210\u6a21\u578b\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002\u901a\u8fc7\u5b66\u4e60\u5171\u4eab\u4e0a\u4e0b\u6587\u7684\u97f3\u8f68\u7684\u8054\u5408\u6982\u7387\uff0c\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u8de8\u591a\u4e2a\u97f3\u8f68\u751f\u6210\u5f7c\u6b64\u826f\u597d\u5bf9\u5e94\u7684\u97f3\u4e50\uff0c\u65e0\u8bba\u662f\u6709\u6761\u4ef6\u5730\u8fd8\u662f\u65e0\u6761\u4ef6\u5730\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6a21\u578b\u8fd8\u80fd\u591f\u8fdb\u884c\u7f16\u66f2\u751f\u6210\uff0c\u5176\u4e2d\u6a21\u578b\u53ef\u4ee5\u5728\u7ed9\u5b9a\u5176\u4ed6\u97f3\u8f68\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u4efb\u4f55\u97f3\u8f68\u5b50\u96c6\uff08\u4f8b\u5982\uff0c\u751f\u6210\u4e0e\u7ed9\u5b9a\u8d1d\u65af\u548c\u9f13\u97f3\u8f68\u4e92\u8865\u7684\u94a2\u7434\u97f3\u8f68\uff09\u3002\u6211\u4eec\u5c06\u6211\u4eec\u7684\u6a21\u578b\u4e0e\u73b0\u6709\u7684\u591a\u8f68\u751f\u6210\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u603b\u751f\u6210\u4efb\u52a1\u548c\u7f16\u66f2\u751f\u6210\u4efb\u52a1\u7684\u5ba2\u89c2\u6307\u6807\u4e0a\u90fd\u53d6\u5f97\u4e86\u76f8\u5f53\u5927\u7684\u6539\u8fdb\u3002|\n", "2409.02683": "|**2024-09-04**|[Rethinking HTG Evaluation: Bridging Generation and Recognition](http://arxiv.org/abs/2409.02683)|null|\u751f\u6210\u6a21\u578b\u5728\u81ea\u7136\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u8bc4\u4f30\u5df2\u5f97\u5230\u5e7f\u6cdb\u7814\u7a76\u3002\u5373\u4f7f\u5728\u8bf8\u5982\u624b\u5199\u751f\u6210\uff08HTG\uff09\u7b49\u5177\u6709\u72ec\u7279\u7279\u6b8a\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u4f7f\u7528\u4e86\u7c7b\u4f3c\u7684\u534f\u8bae\u548c\u6307\u6807\uff0c\u5373\u4f7f\u5b83\u4eec\u53ef\u80fd\u5e76\u975e\u5b8c\u5168\u5408\u9002\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e09\u79cd\u4e13\u4e3a HTG \u8bc4\u4f30\u91cf\u8eab\u5b9a\u5236\u7684\u5ea6\u91cf\u6307\u6807\uff1a$ \\text{HTG}_{\\text{HTR}} $\u3001$ \\text{HTG}_{\\text{style}} $ \u548c $ \\text{HTG}_{\\text{OOV}} $\uff0c\u5e76\u8ba4\u4e3a\u5b83\u4eec\u66f4\u4fbf\u4e8e\u8bc4\u4f30\u751f\u6210\u624b\u5199\u56fe\u50cf\u7684\u8d28\u91cf\u3002\u8fd9\u4e9b\u6307\u6807\u4f9d\u8d56\u4e8e\u624b\u5199\u6587\u672c\u8bc6\u522b\u548c\u4e66\u5199\u8005\u8bc6\u522b\u6a21\u578b\u7684\u8bc6\u522b\u9519\u8bef/\u51c6\u786e\u7387\uff0c\u5e76\u5f3a\u8c03\u4e66\u5199\u98ce\u683c\u3001\u6587\u672c\u5185\u5bb9\u548c\u591a\u6837\u6027\u662f\u7b26\u5408\u624b\u5199\u56fe\u50cf\u5185\u5bb9\u7684\u4e3b\u8981\u65b9\u9762\u3002\u6211\u4eec\u5728 IAM \u624b\u5199\u6570\u636e\u5e93\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u8bf8\u5982 FID \u4e4b\u7c7b\u7684\u5e7f\u6cdb\u4f7f\u7528\u7684\u6307\u6807\u65e0\u6cd5\u6b63\u786e\u91cf\u5316\u751f\u6210\u624b\u5199\u6837\u672c\u7684\u591a\u6837\u6027\u548c\u5b9e\u7528\u6027\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6307\u6807\u4fe1\u606f\u66f4\u4e30\u5bcc\uff0c\u5e76\u5f3a\u8c03\u4e86 HTG \u4e2d\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\u7684\u5fc5\u8981\u6027\u3002\u6240\u63d0\u51fa\u7684\u6307\u6807\u4e3a\u8bc4\u4f30 HTG \u8d28\u91cf\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u3001\u4fe1\u606f\u66f4\u4e30\u5bcc\u7684\u534f\u8bae\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8 HTR \u7684\u6027\u80fd\u3002\u8bc4\u4f30\u534f\u8bae\u7684\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u5f97\uff1ahttps://github.com/koninik/HTG_evaluation\u3002|\n", "2409.02668": "|**2024-09-04**|[Introduction to Machine Learning](http://arxiv.org/abs/2409.02668)|null|\u672c\u4e66\u4ecb\u7ecd\u4e86\u673a\u5668\u5b66\u4e60\u4e2d\u8bb8\u591a\u7b97\u6cd5\u7684\u5f00\u53d1\u548c\u5206\u6790\u6240\u4f9d\u8d56\u7684\u6570\u5b66\u57fa\u7840\u548c\u6280\u672f\u3002\u672c\u4e66\u9996\u5148\u4ecb\u7ecd\u4e86\u8d2f\u7a7f\u5168\u4e66\u7684\u7b26\u53f7\u8868\u793a\uff0c\u5e76\u56de\u987e\u4e86\u5fae\u79ef\u5206\u3001\u7ebf\u6027\u4ee3\u6570\u548c\u6982\u7387\u8bba\u7684\u57fa\u672c\u6982\u5ff5\uff0c\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6d4b\u5ea6\u8bba\u672f\u8bed\uff0c\u53ef\u4f5c\u4e3a\u4f7f\u7528\u8fd9\u4e9b\u5de5\u5177\u7684\u90e8\u5206\u7684\u9605\u8bfb\u6307\u5357\u3002\u5bfc\u8bba\u7ae0\u8282\u8fd8\u63d0\u4f9b\u4e86\u77e9\u9635\u5206\u6790\u548c\u4f18\u5316\u7684\u80cc\u666f\u77e5\u8bc6\u3002\u540e\u9762\u7684\u7ae0\u8282\u4e3a\u672c\u4e66\u4e2d\u4f7f\u7528\u7684\u8bb8\u591a\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5305\u62ec\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u3001\u8fd1\u4f3c\u65b9\u6cd5\u7b49\u3002\u5728\u8ba8\u8bba\u4e86\u7edf\u8ba1\u9884\u6d4b\u7684\u57fa\u672c\u6982\u5ff5\u4e4b\u540e\uff0c\u672c\u4e66\u4ecb\u7ecd\u4e86\u518d\u751f\u6838\u7406\u8bba\u548c\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u6280\u672f\uff0c\u8fd9\u4e9b\u6280\u672f\u5728\u8bb8\u591a\u5730\u65b9\u90fd\u6709\u5e94\u7528\uff0c\u7136\u540e\u4ecb\u7ecd\u4e86\u5404\u79cd\u76d1\u7763\u7edf\u8ba1\u5b66\u4e60\u7b97\u6cd5\uff0c\u5305\u62ec\u7ebf\u6027\u65b9\u6cd5\u3001\u652f\u6301\u5411\u91cf\u673a\u3001\u51b3\u7b56\u6811\u3001boosting\u548c\u795e\u7ecf\u7f51\u7edc\u3002\u63a5\u4e0b\u6765\u8f6c\u5411\u751f\u6210\u65b9\u6cd5\uff0c\u9996\u5148\u4ecb\u7ecd\u4e86\u91c7\u6837\u65b9\u6cd5\u548c\u9a6c\u5c14\u53ef\u592b\u94fe\u7406\u8bba\u3002\u63a5\u4e0b\u6765\u7684\u7ae0\u8282\u63cf\u8ff0\u4e86\u56fe\u6a21\u578b\u7406\u8bba\uff0c\u4ecb\u7ecd\u4e86\u6f5c\u53d8\u91cf\u6a21\u578b\u7684\u53d8\u5206\u65b9\u6cd5\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u751f\u6210\u6a21\u578b\u3002\u63a5\u4e0b\u6765\u7684\u7ae0\u8282\u91cd\u70b9\u4ecb\u7ecd\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ec\u805a\u7c7b\u3001\u56e0\u5b50\u5206\u6790\u548c\u6d41\u5f62\u5b66\u4e60\u3002\u672c\u4e66\u7684\u6700\u540e\u4e00\u7ae0\u504f\u5411\u7406\u8bba\uff0c\u8ba8\u8bba\u4e86\u96c6\u4e2d\u4e0d\u7b49\u5f0f\u548c\u6cdb\u5316\u754c\u3002|\n", "2409.02664": "|**2024-09-04**|[Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection](http://arxiv.org/abs/2409.02664)|null|The proliferation of deepfake faces poses huge potential negative impacts on our daily lives. Despite substantial advancements in deepfake detection over these years, the generalizability of existing methods against forgeries from unseen datasets or created by emerging generative models remains constrained. In this paper, inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach that repurposes a well-trained VLM for general deepfake detection. Motivated by the model reprogramming paradigm that manipulates the model prediction via data perturbations, our method can reprogram a pretrained VLM model (e.g., CLIP) solely based on manipulating its input without tuning the inner parameters. Furthermore, we insert a pseudo-word guided by facial identity into the text prompt. Extensive experiments on several popular benchmarks demonstrate that (1) the cross-dataset and cross-manipulation performances of deepfake detection can be significantly and consistently improved (e.g., over 88% AUC in cross-dataset setting from FF++ to WildDeepfake) using a pre-trained CLIP model with our proposed reprogramming method; (2) our superior performances are at less cost of trainable parameters, making it a promising approach for real-world applications.|\n", "2409.02657": "|**2024-09-04**|[PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for One-Shot Talking Head Generation](http://arxiv.org/abs/2409.02657)|null|While previous audio-driven talking head generation (THG) methods generate head poses from driving audio, the generated poses or lips cannot match the audio well or are not editable. In this study, we propose \\textbf{PoseTalk}, a THG system that can freely generate lip-synchronized talking head videos with free head poses conditioned on text prompts and audio. The core insight of our method is using head pose to connect visual, linguistic, and audio signals. First, we propose to generate poses from both audio and text prompts, where the audio offers short-term variations and rhythm correspondence of the head movements and the text prompts describe the long-term semantics of head motions. To achieve this goal, we devise a Pose Latent Diffusion (PLD) model to generate motion latent from text prompts and audio cues in a pose latent space. Second, we observe a loss-imbalance problem: the loss for the lip region contributes less than 4\\% of the total reconstruction loss caused by both pose and lip, making optimization lean towards head movements rather than lip shapes. To address this issue, we propose a refinement-based learning strategy to synthesize natural talking videos using two cascaded networks, i.e., CoarseNet, and RefineNet. The CoarseNet estimates coarse motions to produce animated images in novel poses and the RefineNet focuses on learning finer lip motions by progressively estimating lip motions from low-to-high resolutions, yielding improved lip-synchronization performance. Experiments demonstrate our pose prediction strategy achieves better pose diversity and realness compared to text-only or audio-only, and our video generator model outperforms state-of-the-art methods in synthesizing talking videos with natural head motions. Project: https://junleen.github.io/projects/posetalk.|\n", "2409.02653": "|**2024-09-04**|[Skip-and-Play: Depth-Driven Pose-Preserved Image Generation for Any Objects](http://arxiv.org/abs/2409.02653)|null|The emergence of diffusion models has enabled the generation of diverse high-quality images solely from text, prompting subsequent efforts to enhance the controllability of these models. Despite the improvement in controllability, pose control remains limited to specific objects (e.g., humans) or poses (e.g., frontal view) due to the fact that pose is generally controlled via camera parameters (e.g., rotation angle) or keypoints (e.g., eyes, nose). Specifically, camera parameters-conditional pose control models generate unrealistic images depending on the object, owing to the small size of 3D datasets for training. Also, keypoint-based approaches encounter challenges in acquiring reliable keypoints for various objects (e.g., church) or poses (e.g., back view). To address these limitations, we propose depth-based pose control, as depth maps are easily obtainable from a single depth estimation model regardless of objects and poses, unlike camera parameters and keypoints. However, depth-based pose control confronts issues of shape dependency, as depth maps influence not only the pose but also the shape of the generated images. To tackle this issue, we propose Skip-and-Play (SnP), designed via analysis of the impact of three components of depth-conditional ControlNet on the pose and the shape of the generated images. To be specific, based on the analysis, we selectively skip parts of the components to mitigate shape dependency on the depth map while preserving the pose. Through various experiments, we demonstrate the superiority of SnP over baselines and showcase the ability of SnP to generate images of diverse objects and poses. Remarkably, SnP exhibits the ability to generate images even when the objects in the condition (e.g., a horse) and the prompt (e.g., a hedgehog) differ from each other.|\n"}, "LLM": {"2409.01909": "|**2024-09-03**|[LUK: Empowering Log Understanding with Expert Knowledge from Large Language Models](http://arxiv.org/abs/2409.01909)|null|Logs play a critical role in providing essential information for system monitoring and troubleshooting. Recently, with the success of pre-trained language models (PLMs) and large language models (LLMs) in natural language processing (NLP), smaller PLMs (such as BERT) and LLMs (like ChatGPT) have become the current mainstream approaches for log analysis. While LLMs possess rich knowledge, their high computational costs and unstable performance make LLMs impractical for analyzing logs directly. In contrast, smaller PLMs can be fine-tuned for specific tasks even with limited computational resources, making them more practical. However, these smaller PLMs face challenges in understanding logs comprehensively due to their limited expert knowledge. To better utilize the knowledge embedded within LLMs for log understanding, this paper introduces a novel knowledge enhancement framework, called LUK, which acquires expert knowledge from LLMs to empower log understanding on a smaller PLM. Specifically, we design a multi-expert collaboration framework based on LLMs consisting of different roles to acquire expert knowledge. In addition, we propose two novel pre-training tasks to enhance the log pre-training with expert knowledge. LUK achieves state-of-the-art results on different log analysis tasks and extensive experiments demonstrate expert knowledge from LLMs can be utilized more effectively to understand logs.|\n", "2409.00702": "|**2024-09-04**|[MARS: Matching Attribute-aware Representations for Text-based Sequential Recommendation](http://arxiv.org/abs/2409.00702)|null|Sequential recommendation aims to predict the next item a user is likely to prefer based on their sequential interaction history. Recently, text-based sequential recommendation has emerged as a promising paradigm that uses pre-trained language models to exploit textual item features to enhance performance and facilitate knowledge transfer to unseen datasets. However, existing text-based recommender models still struggle with two key challenges: (i) representing users and items with multiple attributes, and (ii) matching items with complex user interests. To address these challenges, we propose a novel model, Matching Attribute-aware Representations for Text-based Sequential Recommendation (MARS). MARS extracts detailed user and item representations through attribute-aware text encoding, capturing diverse user intents with multiple attribute-aware representations. It then computes user-item scores via attribute-wise interaction matching, effectively capturing attribute-level user preferences. Our extensive experiments demonstrate that MARS significantly outperforms existing sequential models, achieving improvements of up to 24.43% and 29.26% in Recall@10 and NDCG@10 across five benchmark datasets. Code is available at https://github.com/junieberry/MARS|\n", "2409.00323": "|**2024-08-31**|[From Prediction to Application: Language Model-based Code Knowledge Tracing with Domain Adaptive Pre-Training and Automatic Feedback System with Pedagogical Prompting for Comprehensive Programming Education](http://arxiv.org/abs/2409.00323)|null|Knowledge Tracing (KT) is a critical component in online learning, but traditional approaches face limitations in interpretability and cross-domain adaptability. This paper introduces Language Model-based Code Knowledge Tracing (CodeLKT), an innovative application of Language model-based Knowledge Tracing (LKT) to programming education. CodeLKT leverages pre-trained language models to process learning data, demonstrating superior performance over existing KT and Code KT models. We explore Domain Adaptive Pre-Training (DAPT) and Task Adaptive Pre-Training (TAPT), showing enhanced performance in the coding domain and investigating cross-domain transfer between mathematics and coding. Additionally, we present an theoretically-informed integrated system combining CodeLKT with large language models to generate personalized, in-depth feedback to support students' programming learning. This work advances the field of Code Knowledge Tracing by expanding the knowledge base with language model-based approach and offering practical implications for programming education through data-informed feedback.|\n", "2408.17354": "|**2024-08-30**|[Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage](http://arxiv.org/abs/2408.17354)|null|Fine-tuning large language models on private data for downstream applications poses significant privacy risks in potentially exposing sensitive information. Several popular community platforms now offer convenient distribution of a large variety of pre-trained models, allowing anyone to publish without rigorous verification. This scenario creates a privacy threat, as pre-trained models can be intentionally crafted to compromise the privacy of fine-tuning datasets. In this study, we introduce a novel poisoning technique that uses model-unlearning as an attack tool. This approach manipulates a pre-trained language model to increase the leakage of private data during the fine-tuning process. Our method enhances both membership inference and data extraction attacks while preserving model utility. Experimental results across different models, datasets, and fine-tuning setups demonstrate that our attacks significantly surpass baseline performance. This work serves as a cautionary note for users who download pre-trained models from unverified sources, highlighting the potential risks involved.|\n", "2408.14505": "|**2024-08-24**|[Empowering Pre-Trained Language Models for Spatio-Temporal Forecasting via Decoupling Enhanced Discrete Reprogramming](http://arxiv.org/abs/2408.14505)|null|Spatio-temporal time series forecasting plays a critical role in various real-world applications, such as transportation optimization, energy management, and climate analysis. The recent advancements in Pre-trained Language Models (PLMs) have inspired efforts to reprogram these models for time series forecasting tasks, by leveraging their superior reasoning and generalization capabilities. However, existing approaches fall short in handling complex spatial inter-series dependencies and intrinsic intra-series frequency components, limiting their spatio-temporal forecasting performance. Moreover, the linear mapping of continuous time series to a compressed subset vocabulary in reprogramming constrains the spatio-temporal semantic expressivity of PLMs and may lead to potential information bottleneck. To overcome the above limitations, we propose \\textsc{RePST}, a tailored PLM reprogramming framework for spatio-temporal forecasting. The key insight of \\textsc{RePST} is to decouple the spatio-temporal dynamics in the frequency domain, allowing better alignment with the PLM text space. Specifically, we first decouple spatio-temporal data in Fourier space and devise a structural diffusion operator to obtain temporal intrinsic and spatial diffusion signals, making the dynamics more comprehensible and predictable for PLMs. To avoid information bottleneck from a limited vocabulary, we further propose a discrete reprogramming strategy that selects relevant discrete textual information from an expanded vocabulary space in a differentiable manner. Extensive experiments on four real-world datasets show that our proposed approach significantly outperforms state-of-the-art spatio-temporal forecasting models, particularly in data-scarce scenarios.|\n", "2408.13040": "|**2024-08-23**|[SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks](http://arxiv.org/abs/2408.13040)|null|Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both storage and computation. Additionally, prompting modifies only the LM's inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. This significantly reduces the need for human labor in designing task-specific models. These advantages become even more evident as the number of tasks served by the LM scales up. Motivated by the strengths of prompting, we are the first to explore the potential of prompting speech LMs in the domain of speech processing. Recently, there has been a growing interest in converting speech into discrete units for language modeling. Our pioneer research demonstrates that these quantized speech units are highly versatile within our unified prompting framework. Not only can they serve as class labels, but they also contain rich phonetic information that can be re-synthesized back into speech signals for speech generation tasks. Specifically, we reformulate speech processing tasks into speech-to-unit generation tasks. As a result, we can seamlessly integrate tasks such as speech classification, sequence generation, and speech generation within a single, unified prompting framework. The experiment results show that the prompting method can achieve competitive performance compared to the strong fine-tuning method based on self-supervised learning models with a similar number of trainable parameters. The prompting method also shows promising results in the few-shot setting. Moreover, with the advanced speech LMs coming into the stage, the proposed prompting framework attains great potential.|\n", "2408.12779": "|**2024-08-23**|[Investigating LLM Applications in E-Commerce](http://arxiv.org/abs/2408.12779)|null|The emergence of Large Language Models (LLMs) has revolutionized natural language processing in various applications especially in e-commerce. One crucial step before the application of such LLMs in these fields is to understand and compare the performance in different use cases in such tasks. This paper explored the efficacy of LLMs in the e-commerce domain, focusing on instruction-tuning an open source LLM model with public e-commerce datasets of varying sizes and comparing the performance with the conventional models prevalent in industrial applications. We conducted a comprehensive comparison between LLMs and traditional pre-trained language models across specific tasks intrinsic to the e-commerce domain, namely classification, generation, summarization, and named entity recognition (NER). Furthermore, we examined the effectiveness of the current niche industrial application of very large LLM, using in-context learning, in e-commerce specific tasks. Our findings indicate that few-shot inference with very large LLMs often does not outperform fine-tuning smaller pre-trained models, underscoring the importance of task-specific model optimization.Additionally, we investigated different training methodologies such as single-task training, mixed-task training, and LoRA merging both within domain/tasks and between different tasks. Through rigorous experimentation and analysis, this paper offers valuable insights into the potential effectiveness of LLMs to advance natural language processing capabilities within the e-commerce industry.|\n", "2408.12125": "|**2024-08-22**|[AutoTest: Evolutionary Code Solution Selection with Test Cases](http://arxiv.org/abs/2408.12125)|null|With the development of code generation techniques, selecting the correct code solution from multiple candidate solutions has become a crucial task. This study proposes AutoTest, a novel technique that combines automated test case generation with code solution execution to optimize the selection process using an evolutionary genetic algorithm. Firstly, AutoTest utilizes large pre-trained language models such as codegen-16B, code-davinci-002, and incoder-6B to provide code solutions and their corresponding test cases. Then, by executing the code solutions and evaluating their performance on the test cases, a consensus set is formed. Fine-grained ranking is achieved through the selection, mutation, and crossover mechanisms based on the evolutionary genetic algorithm, with the adjustment of alpha and beta parameters. Finally, the best code solution is chosen. AutoTest demonstrates significant performance improvements on the HumanEval benchmark test. The HumanEval dataset consists of 164 programming problems, and AutoTest achieves approximately a 10% improvement over the baseline method in terms of pass@1 score.|\n", "2408.11319": "|**2024-08-24**|[SarcasmBench: Towards Evaluating Large Language Models on Sarcasm Understanding](http://arxiv.org/abs/2408.11319)|null|In the era of large language models (LLMs), the task of ``System I''~-~the fast, unconscious, and intuitive tasks, e.g., sentiment analysis, text classification, etc., have been argued to be successfully solved. However, sarcasm, as a subtle linguistic phenomenon, often employs rhetorical devices like hyperbole and figuration to convey true sentiments and intentions, involving a higher level of abstraction than sentiment analysis. There is growing concern that the argument about LLMs' success may not be fully tenable when considering sarcasm understanding. To address this question, we select eleven SOTA LLMs and eight SOTA pre-trained language models (PLMs) and present comprehensive evaluations on six widely used benchmark datasets through different prompting approaches, i.e., zero-shot input/output (IO) prompting, few-shot IO prompting, chain of thought (CoT) prompting. Our results highlight three key findings: (1) current LLMs underperform supervised PLMs based sarcasm detection baselines across six sarcasm benchmarks. This suggests that significant efforts are still required to improve LLMs' understanding of human sarcasm. (2) GPT-4 consistently and significantly outperforms other LLMs across various prompting methods, with an average improvement of 14.0\\%$\\uparrow$. Claude 3 and ChatGPT demonstrate the next best performance after GPT-4. (3) Few-shot IO prompting method outperforms the other two methods: zero-shot IO and few-shot CoT. The reason is that sarcasm detection, being a holistic, intuitive, and non-rational cognitive process, is argued not to adhere to step-by-step logical reasoning, making CoT less effective in understanding sarcasm compared to its effectiveness in mathematical reasoning tasks.|\n", "2408.10548": "|**2024-08-20**|[Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution](http://arxiv.org/abs/2408.10548)|**[link](https://github.com/lanxiang1017/language-modeling-on-tabular-data-survey)**|Tabular data, a prevalent data type across various domains, presents unique challenges due to its heterogeneous nature and complex structural relationships. Achieving high predictive performance and robustness in tabular data analysis holds significant promise for numerous applications. Influenced by recent advancements in natural language processing, particularly transformer architectures, new methods for tabular data modeling have emerged. Early techniques concentrated on pre-training transformers from scratch, often encountering scalability issues. Subsequently, methods leveraging pre-trained language models like BERT have been developed, which require less data and yield enhanced performance. The recent advent of large language models, such as GPT and LLaMA, has further revolutionized the field, facilitating more advanced and diverse applications with minimal fine-tuning. Despite the growing interest, a comprehensive survey of language modeling techniques for tabular data remains absent. This paper fills this gap by providing a systematic review of the development of language modeling for tabular data, encompassing: (1) a categorization of different tabular data structures and data types; (2) a review of key datasets used in model training and tasks used for evaluation; (3) a summary of modeling techniques including widely-adopted data processing methods, popular architectures, and training objectives; (4) the evolution from adapting traditional Pre-training/Pre-trained language models to the utilization of large language models; (5) an identification of persistent challenges and potential future research directions in language modeling for tabular data analysis. GitHub page associated with this survey is available at: https://github.com/lanxiang1017/Language-Modeling-on-Tabular-Data-Survey.git.|\n"}, "Transformer": {"2409.02727": "|**2024-09-04**|[Pooling And Attention: What Are Effective Designs For LLm-Based Embedding Models?](http://arxiv.org/abs/2409.02727)|**[link](https://github.com/yixuantt/poolingandattn)**|The significant advancements of Large Language Models (LLMs) in generative tasks have led to a growing body of work exploring LLM-based embedding models. While these models, employing different pooling and attention strategies, have achieved state-of-the-art performance on public embedding benchmarks, questions still arise about what constitutes an effective design for LLM-based embedding models. However, these models are often trained on different datasets, using different LLM base models or training settings. Moreover, evaluations on public embedding benchmarks often fail to report statistical significance, making it difficult to determine which designs truly contribute to final performance. This complicates the process for practitioners seeking optimal training recipes for LLM-based embedding models. In this study, we conduct a large-scale experiment by training a series of LLM-based embedding models using the same training data and base model but differing in their pooling and attention strategies. The results show that there is no one-size-fits-all solution: while bidirectional attention and an additional trainable pooling layer outperform in text similarity and information retrieval tasks, they do not significantly surpass simpler designs like EOS-last token pooling and default causal attention in clustering and classification tasks. Furthermore, we propose a new pooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs of all hidden layers, rather than just the last layer, using a cross-attention network. This method proves to be statistically superior in text similarity and retrieval tasks compared to existing pooling methods. Overall, this paper sheds light on effective training strategies for LLM-based embedding models.|\n", "2409.02545": "|**2024-09-04**|[UniTT-Stereo: Unified Training of Transformer for Enhanced Stereo Matching](http://arxiv.org/abs/2409.02545)|null|Unlike other vision tasks where Transformer-based approaches are becoming increasingly common, stereo depth estimation is still dominated by convolution-based approaches. This is mainly due to the limited availability of real-world ground truth for stereo matching, which is a limiting factor in improving the performance of Transformer-based stereo approaches. In this paper, we propose UniTT-Stereo, a method to maximize the potential of Transformer-based stereo architectures by unifying self-supervised learning used for pre-training with stereo matching framework based on supervised learning. To be specific, we explore the effectiveness of reconstructing features of masked portions in an input image and at the same time predicting corresponding points in another image from the perspective of locality inductive bias, which is crucial in training models with limited training data. Moreover, to address these challenging tasks of reconstruction-and-prediction, we present a new strategy to vary a masking ratio when training the stereo model with stereo-tailored losses. State-of-the-art performance of UniTT-Stereo is validated on various benchmarks such as ETH3D, KITTI 2012, and KITTI 2015 datasets. Lastly, to investigate the advantages of the proposed approach, we provide a frequency analysis of feature maps and the analysis of locality inductive bias based on attention maps.|\n", "2409.02056": "|**2024-09-03**|[F2former: When Fractional Fourier Meets Deep Wiener Deconvolution and Selective Frequency Transformer for Image Deblurring](http://arxiv.org/abs/2409.02056)|null|Recent progress in image deblurring techniques focuses mainly on operating in both frequency and spatial domains using the Fourier transform (FT) properties. However, their performance is limited due to the dependency of FT on stationary signals and its lack of capability to extract spatial-frequency properties. In this paper, we propose a novel approach based on the Fractional Fourier Transform (FRFT), a unified spatial-frequency representation leveraging both spatial and frequency components simultaneously, making it ideal for processing non-stationary signals like images. Specifically, we introduce a Fractional Fourier Transformer (F2former), where we combine the classical fractional Fourier based Wiener deconvolution (F2WD) as well as a multi-branch encoder-decoder transformer based on a new fractional frequency aware transformer block (F2TB). We design F2TB consisting of a fractional frequency aware self-attention (F2SA) to estimate element-wise product attention based on important frequency components and a novel feed-forward network based on frequency division multiplexing (FM-FFN) to refine high and low frequency features separately for efficient latent clear image restoration. Experimental results for the cases of both motion deblurring as well as defocus deblurring show that the performance of our proposed method is superior to other state-of-the-art (SOTA) approaches.|\n", "2409.02018": "|**2024-09-03**|[TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation](http://arxiv.org/abs/2409.02018)|null|In healthcare, medical image segmentation is crucial for accurate disease diagnosis and the development of effective treatment strategies. Early detection can significantly aid in managing diseases and potentially prevent their progression. Machine learning, particularly deep convolutional neural networks, has emerged as a promising approach to addressing segmentation challenges. Traditional methods like U-Net use encoding blocks for local representation modeling and decoding blocks to uncover semantic relationships. However, these models often struggle with multi-scale objects exhibiting significant variations in texture and shape, and they frequently fail to capture long-range dependencies in the input data. Transformers designed for sequence-to-sequence predictions have been proposed as alternatives, utilizing global self-attention mechanisms. Yet, they can sometimes lack precise localization due to insufficient granular details. To overcome these limitations, we introduce TransDAE: a novel approach that reimagines the self-attention mechanism to include both spatial and channel-wise associations across the entire feature space, while maintaining computational efficiency. Additionally, TransDAE enhances the skip connection pathway with an inter-scale interaction module, promoting feature reuse and improving localization accuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on the Synaps multi-organ dataset, even without relying on pre-trained weights.|\n", "2409.01557": "|**2024-09-03**|[TASL-Net: Tri-Attention Selective Learning Network for Intelligent Diagnosis of Bimodal Ultrasound Video](http://arxiv.org/abs/2409.01557)|null|In the intelligent diagnosis of bimodal (gray-scale and contrast-enhanced) ultrasound videos, medical domain knowledge such as the way sonographers browse videos, the particular areas they emphasize, and the features they pay special attention to, plays a decisive role in facilitating precise diagnosis. Embedding medical knowledge into the deep learning network can not only enhance performance but also boost clinical confidence and reliability of the network. However, it is an intractable challenge to automatically focus on these person- and disease-specific features in videos and to enable networks to encode bimodal information comprehensively and efficiently. This paper proposes a novel Tri-Attention Selective Learning Network (TASL-Net) to tackle this challenge and automatically embed three types of diagnostic attention of sonographers into a mutual transformer framework for intelligent diagnosis of bimodal ultrasound videos. Firstly, a time-intensity-curve-based video selector is designed to mimic the temporal attention of sonographers, thus removing a large amount of redundant information while improving computational efficiency of TASL-Net. Then, to introduce the spatial attention of the sonographers for contrast-enhanced video analysis, we propose the earliest-enhanced position detector based on structural similarity variation, on which the TASL-Net is made to focus on the differences of perfusion variation inside and outside the lesion. Finally, by proposing a mutual encoding strategy that combines convolution and transformer, TASL-Net possesses bimodal attention to structure features on gray-scale videos and to perfusion variations on contrast-enhanced videos. These modules work collaboratively and contribute to superior performance. We conduct a detailed experimental validation of TASL-Net's performance on three datasets, including lung, breast, and liver.|\n", "2409.01352": "|**2024-09-02**|[Spectron: Target Speaker Extraction using Conditional Transformer with Adversarial Refinement](http://arxiv.org/abs/2409.01352)|null|Recently, attention-based transformers have become a de facto standard in many deep learning applications including natural language processing, computer vision, signal processing, etc.. In this paper, we propose a transformer-based end-to-end model to extract a target speaker's speech from a monaural multi-speaker mixed audio signal. Unlike existing speaker extraction methods, we introduce two additional objectives to impose speaker embedding consistency and waveform encoder invertibility and jointly train both speaker encoder and speech separator to better capture the speaker conditional embedding. Furthermore, we leverage a multi-scale discriminator to refine the perceptual quality of the extracted speech. Our experiments show that the use of a dual path transformer in the separator backbone along with proposed training paradigm improves the CNN baseline by $3.12$ dB points. Finally, we compare our approach with recent state-of-the-arts and show that our model outperforms existing methods by $4.1$ dB points on an average without creating additional data dependency.|\n", "2409.01193": "|**2024-09-02**|[CLIBE: Detecting Dynamic Backdoors in Transformer-based NLP Models](http://arxiv.org/abs/2409.01193)|null|Backdoors can be injected into NLP models to induce misbehavior when the input text contains a specific feature, known as a trigger, which the attacker secretly selects. Unlike fixed words, phrases, or sentences used in the static text trigger, NLP dynamic backdoor attacks design triggers associated with abstract and latent text features, making them considerably stealthier than traditional static backdoor attacks. However, existing research on NLP backdoor detection primarily focuses on defending against static backdoor attacks, while detecting dynamic backdoors in NLP models remains largely unexplored. This paper presents CLIBE, the first framework to detect dynamic backdoors in Transformer-based NLP models. CLIBE injects a \"few-shot perturbation\" into the suspect Transformer model by crafting optimized weight perturbation in the attention layers to make the perturbed model classify a limited number of reference samples as a target label. Subsequently, CLIBE leverages the generalization ability of this few-shot perturbation to determine whether the original model contains a dynamic backdoor. Extensive evaluation on three advanced NLP dynamic backdoor attacks, two widely-used Transformer frameworks, and four real-world classification tasks strongly validates the effectiveness of CLIBE. We also demonstrate the robustness of CLIBE against various adaptive attacks. Furthermore, we employ CLIBE to scrutinize 49 popular Transformer models on Hugging Face and discover one exhibiting a high probability of containing a dynamic backdoor. We have contacted Hugging Face and provided detailed evidence of this model's backdoor behavior. Moreover, we extend CLIBE to detect backdoor text generation models modified to exhibit toxic behavior. To the best of our knowledge, CLIBE is the first framework capable of detecting backdoors in text generation models without access to trigger input test samples.|\n", "2409.01068": "|**2024-09-02**|[Progressive Retinal Image Registration via Global and Local Deformable Transformations](http://arxiv.org/abs/2409.01068)|**[link](https://github.com/lyp-deeplearning/awesome-retinal-registration)**|Retinal image registration plays an important role in the ophthalmological diagnosis process. Since there exist variances in viewing angles and anatomical structures across different retinal images, keypoint-based approaches become the mainstream methods for retinal image registration thanks to their robustness and low latency. These methods typically assume the retinal surfaces are planar, and adopt feature matching to obtain the homography matrix that represents the global transformation between images. Yet, such a planar hypothesis inevitably introduces registration errors since retinal surface is approximately curved. This limitation is more prominent when registering image pairs with significant differences in viewing angles. To address this problem, we propose a hybrid registration framework called HybridRetina, which progressively registers retinal images with global and local deformable transformations. For that, we use a keypoint detector and a deformation network called GAMorph to estimate the global transformation and local deformable transformation, respectively. Specifically, we integrate multi-level pixel relation knowledge to guide the training of GAMorph. Additionally, we utilize an edge attention module that includes the geometric priors of the images, ensuring the deformation field focuses more on the vascular regions of clinical interest. Experiments on two widely-used datasets, FIRE and FLoRI21, show that our proposed HybridRetina significantly outperforms some state-of-the-art methods. The code is available at https://github.com/lyp-deeplearning/awesome-retinal-registration.|\n", "2409.00904": "|**2024-09-02**|[Multi-scale Temporal Fusion Transformer for Incomplete Vehicle Trajectory Prediction](http://arxiv.org/abs/2409.00904)|null|Motion prediction plays an essential role in autonomous driving systems, enabling autonomous vehicles to achieve more accurate local-path planning and driving decisions based on predictions of the surrounding vehicles. However, existing methods neglect the potential missing values caused by object occlusion, perception failures, etc., which inevitably degrades the trajectory prediction performance in real traffic scenarios. To address this limitation, we propose a novel end-to-end framework for incomplete vehicle trajectory prediction, named Multi-scale Temporal Fusion Transformer (MTFT), which consists of the Multi-scale Attention Head (MAH) and the Continuity Representation-guided Multi-scale Fusion (CRMF) module. Specifically, the MAH leverages the multi-head attention mechanism to parallelly capture multi-scale motion representation of trajectory from different temporal granularities, thus mitigating the adverse effect of missing values on prediction. Furthermore, the multi-scale motion representation is input into the CRMF module for multi-scale fusion to obtain the robust temporal feature of the vehicle. During the fusion process, the continuity representation of vehicle motion is first extracted across time steps to guide the fusion, ensuring that the resulting temporal feature incorporates both detailed information and the overall trend of vehicle motion, which facilitates the accurate decoding of future trajectory that is consistent with the vehicle's motion trend. We evaluate the proposed model on four datasets derived from highway and urban traffic scenarios. The experimental results demonstrate its superior performance in the incomplete vehicle trajectory prediction task compared with state-of-the-art models, e.g., a comprehensive performance improvement of more than 39% on the HighD dataset.|\n", "2409.00591": "|**2024-09-01**|[Attention-Guided Multi-scale Interaction Network for Face Super-Resolution](http://arxiv.org/abs/2409.00591)|null|Recently, CNN and Transformer hybrid networks demonstrated excellent performance in face super-resolution (FSR) tasks. Since numerous features at different scales in hybrid networks, how to fuse these multi-scale features and promote their complementarity is crucial for enhancing FSR. However, existing hybrid network-based FSR methods ignore this, only simply combining the Transformer and CNN. To address this issue, we propose an attention-guided Multi-scale interaction network (AMINet), which contains local and global feature interactions as well as encoder-decoder phases feature interactions. Specifically, we propose a Local and Global Feature Interaction Module (LGFI) to promote fusions of global features and different receptive fields' local features extracted by our Residual Depth Feature Extraction Module (RDFE). Additionally, we propose a Selective Kernel Attention Fusion Module (SKAF) to adaptively select fusions of different features within LGFI and encoder-decoder phases. Our above design allows the free flow of multi-scale features from within modules and between encoder and decoder, which can promote the complementarity of different scale features to enhance FSR. Comprehensive experiments confirm that our method consistently performs well with less computational consumption and faster inference.|\n"}}