{"\u591a\u6a21\u6001": {"2409.02914": "|**2024-09-04**|[Can LVLMs Obtain a Driver's License? A Benchmark Towards Reliable AGI for Autonomous Driving](http://arxiv.org/abs/2409.02914)|null|Large Vision-Language Models (LVLMs) have recently garnered significant attention, with many efforts aimed at harnessing their general knowledge to enhance the interpretability and robustness of autonomous driving models. However, LVLMs typically rely on large, general-purpose datasets and lack the specialized expertise required for professional and safe driving. Existing vision-language driving datasets focus primarily on scene understanding and decision-making, without providing explicit guidance on traffic rules and driving skills, which are critical aspects directly related to driving safety. To bridge this gap, we propose IDKB, a large-scale dataset containing over one million data items collected from various countries, including driving handbooks, theory test data, and simulated road test data. Much like the process of obtaining a driver's license, IDKB encompasses nearly all the explicit knowledge needed for driving from theory to practice. In particular, we conducted comprehensive tests on 15 LVLMs using IDKB to assess their reliability in the context of autonomous driving and provided extensive analysis. We also fine-tuned popular models, achieving notable performance improvements, which further validate the significance of our dataset. The project page can be found at: \\url{https://4dvlab.github.io/project_page/idkb.html}||\n", "2409.02882": "|**2024-09-04**|[Benchmarking Spurious Bias in Few-Shot Image Classifiers](http://arxiv.org/abs/2409.02882)|**[link](https://github.com/gtzheng/fewstab)**|Few-shot image classifiers are designed to recognize and classify new data with minimal supervision and limited data but often show reliance on spurious correlations between classes and spurious attributes, known as spurious bias. Spurious correlations commonly hold in certain samples and few-shot classifiers can suffer from spurious bias induced from them. There is an absence of an automatic benchmarking system to assess the robustness of few-shot classifiers against spurious bias. In this paper, we propose a systematic and rigorous benchmark framework, termed FewSTAB, to fairly demonstrate and quantify varied degrees of robustness of few-shot classifiers to spurious bias. FewSTAB creates few-shot evaluation tasks with biased attributes so that using them for predictions can demonstrate poor performance. To construct these tasks, we propose attribute-based sample selection strategies based on a pre-trained vision-language model, eliminating the need for manual dataset curation. This allows FewSTAB to automatically benchmark spurious bias using any existing test data. FewSTAB offers evaluation results in a new dimension along with a new design guideline for building robust classifiers. Moreover, it can benchmark spurious bias in varied degrees and enable designs for varied degrees of robustness. Its effectiveness is demonstrated through experiments on ten few-shot learning methods across three datasets. We hope our framework can inspire new designs of robust few-shot classifiers. Our code is available at https://github.com/gtzheng/FewSTAB.||\n", "2409.02834": "|**2024-09-06**|[CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of Large Multimodal Models](http://arxiv.org/abs/2409.02834)|null|Large language models (LLMs) have obtained promising results in mathematical reasoning, which is a foundational skill for human intelligence. Most previous studies focus on improving and measuring the performance of LLMs based on textual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few researchers have released English multimodal math datasets (e.g., MATHVISTA and MATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In this paper, we release a Chinese multimodal math (CMM-Math) dataset, including benchmark and training parts, to evaluate and enhance the mathematical reasoning of LMMs. CMM-Math contains over 28,000 high-quality samples, featuring a variety of problem types (e.g., multiple-choice, fill-in-the-blank, and so on) with detailed solutions across 12 grade levels from elementary to high school in China. Specifically, the visual context may be present in the questions or opinions, which makes this dataset more challenging. Through comprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math dataset face challenges, emphasizing the necessity for further improvements in LMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to handle the problems with mixed input of multiple images and text segments. We train our model using three stages, including foundational pre-training, foundational fine-tuning, and mathematical fine-tuning. The extensive experiments indicate that our model effectively improves math reasoning performance by comparing it with the SOTA LMMs over three multimodal mathematical datasets.||\n", "2409.02813": "|**2024-09-04**|[MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark](http://arxiv.org/abs/2409.02813)|null|This paper introduces MMMU-Pro, a robust version of the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark. MMMU-Pro rigorously assesses multimodal models' true understanding and reasoning capabilities through a three-step process based on MMMU: (1) filtering out questions answerable by text-only models, (2) augmenting candidate options, and (3) introducing a vision-only input setting where questions are embedded within images. This setting challenges AI to truly \"see\" and \"read\" simultaneously, testing a fundamental human cognitive skill of seamlessly integrating visual and textual information. Results show that model performance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8% to 26.9% across models. We explore the impact of OCR prompts and Chain of Thought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT generally improves performance. MMMU-Pro provides a more rigorous evaluation tool, closely mimicking real-world scenarios and offering valuable directions for future research in multimodal AI.||\n", "2409.02664": "|**2024-09-04**|[Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection](http://arxiv.org/abs/2409.02664)|null|The proliferation of deepfake faces poses huge potential negative impacts on our daily lives. Despite substantial advancements in deepfake detection over these years, the generalizability of existing methods against forgeries from unseen datasets or created by emerging generative models remains constrained. In this paper, inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach that repurposes a well-trained VLM for general deepfake detection. Motivated by the model reprogramming paradigm that manipulates the model prediction via data perturbations, our method can reprogram a pretrained VLM model (e.g., CLIP) solely based on manipulating its input without tuning the inner parameters. Furthermore, we insert a pseudo-word guided by facial identity into the text prompt. Extensive experiments on several popular benchmarks demonstrate that (1) the cross-dataset and cross-manipulation performances of deepfake detection can be significantly and consistently improved (e.g., over 88% AUC in cross-dataset setting from FF++ to WildDeepfake) using a pre-trained CLIP model with our proposed reprogramming method; (2) our superior performances are at less cost of trainable parameters, making it a promising approach for real-world applications.||\n", "2409.02530": "|**2024-09-04**|[Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models](http://arxiv.org/abs/2409.02530)|null|The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of kidney function in clinical practice. Although traditional equations and Machine Learning (ML) models using clinical and laboratory data can estimate eGFR, accurately predicting future eGFR levels remains a significant challenge for nephrologists and ML researchers. Recent advances demonstrate that Large Language Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust foundation models for diverse applications. This study investigates the potential of LMMs to predict future eGFR levels with a dataset consisting of laboratory and clinical values from 50 patients. By integrating various prompting techniques and ensembles of LMMs, our findings suggest that these models, when combined with precise prompts and visual representations of eGFR trajectories, offer predictive performance comparable to existing ML models. This research extends the application of foundation models and suggests avenues for future studies to harness these models in addressing complex medical forecasting challenges.||\n", "2409.02278": "|**2024-09-03**|[Evaluation and Comparison of Visual Language Models for Transportation Engineering Problems](http://arxiv.org/abs/2409.02278)|null|\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u6700\u65b0\u53d1\u5c55\u663e\u793a\u51fa\u5176\u5728\u56fe\u50cf\u7406\u89e3\u76f8\u5173\u5e94\u7528\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u4ea4\u901a\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4f8b\u5982\u56fe\u50cf\u5206\u7c7b\u548c\u76ee\u6807\u68c0\u6d4b\u3002\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u5305\u62ec\u62e5\u5835\u68c0\u6d4b\u548c\u88c2\u7f1d\u8bc6\u522b\uff0c\u800c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u5219\u7528\u4e8e\u8bc6\u522b\u672a\u4f69\u6234\u5934\u76d4\u7684\u884c\u4e3a\u3002\u6211\u4eec\u5e94\u7528\u4e86CLIP\u3001BLIP\u3001OWL-ViT\u3001Llava-Next\u7b49\u5f00\u6e90\u6a21\u578b\u548c\u95ed\u6e90\u6a21\u578bGPT-4o\uff0c\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4ee5\u5229\u7528\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u6765\u5b8c\u6210\u57fa\u4e8e\u89c6\u89c9\u7684\u4ea4\u901a\u4efb\u52a1\u3002\u8fd9\u4e9b\u4efb\u52a1\u662f\u901a\u8fc7\u5bf9VLM\u6a21\u578b\u5e94\u7528\u96f6\u6837\u672c\u63d0\u793a\u6765\u6267\u884c\u7684\uff0c\u56e0\u4e3a\u96f6\u6837\u672c\u63d0\u793a\u5141\u8bb8\u5728\u4e0d\u5bf9\u4efb\u52a1\u8fdb\u884c\u4efb\u4f55\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u4efb\u52a1\u3002\u5b83\u6d88\u9664\u4e86\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u6807\u6ce8\u6570\u636e\u96c6\u6216\u5fae\u8c03\u7684\u9700\u6c42\u3002\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4e0e\u57fa\u51c6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u4f46\u5728\u76ee\u6807\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4ecd\u6709\u6539\u8fdb\u7684\u7a7a\u95f4\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u5bf9\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u7a81\u51fa\u4e86\u8fd9\u4e9b\u6a21\u578b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u672a\u6765\u6539\u8fdb\u548c\u5927\u89c4\u6a21\u5b9e\u65bd\u7684\u57fa\u7ebf\u3002||\n", "2409.02253": "|**2024-09-03**|[How to Determine the Preferred Image Distribution of a Black-Box Vision-Language Model?](http://arxiv.org/abs/2409.02253)|null|\u5927\u578b\u57fa\u7840\u6a21\u578b\u5f7b\u5e95\u6539\u53d8\u4e86\u8be5\u9886\u57df\uff0c\u4f46\u9488\u5bf9\u7279\u5b9a\u89c6\u89c9\u4efb\u52a1\u4f18\u5316\u591a\u6a21\u6001\u6a21\u578b\u4ecd\u7136\u5b58\u5728\u6311\u6218\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u4e0d\u540c\u8f93\u5165\u63d0\u793a\u4e0b\u8f93\u51fa\u7684\u4e00\u81f4\u6027\uff0c\u6765\u786e\u5b9a\u9ed1\u76d2\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u9996\u9009\u56fe\u50cf\u5206\u5e03\u3002\u6211\u4eec\u5c06\u5176\u5e94\u7528\u4e8e 3D \u5bf9\u8c61\u7684\u4e0d\u540c\u6e32\u67d3\u7c7b\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9700\u8981\u7cbe\u786e\u89e3\u91ca\u590d\u6742\u7ed3\u6784\u7684\u5404\u4e2a\u9886\u57df\u7684\u6709\u6548\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u8ba1\u7b97\u673a\u8f85\u52a9\u8bbe\u8ba1 (CAD) \u4f5c\u4e3a\u793a\u4f8b\u9886\u57df\u3002\u6211\u4eec\u4f7f\u7528\u4eba\u7c7b\u53cd\u9988\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u8fdb\u4e00\u6b65\u5b8c\u5584\u4e86 VLM \u8f93\u51fa\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u91ca\u8d28\u91cf\u3002\u4e3a\u4e86\u89e3\u51b3\u4e13\u4e1a\u9886\u57df\u7f3a\u4e4f\u57fa\u51c6\u7684\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 CAD-VQA\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30 VLM \u5728 CAD \u76f8\u5173\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u65b0\u6570\u636e\u96c6\u3002\u6211\u4eec\u5bf9 CAD-VQA \u4e0a\u6700\u5148\u8fdb\u7684 VLM \u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5efa\u7acb\u4e86\u57fa\u7ebf\u6027\u80fd\u6c34\u5e73\uff0c\u4e3a\u5728\u9700\u8981\u4e13\u5bb6\u7ea7\u89c6\u89c9\u89e3\u91ca\u7684\u5404\u4e2a\u9886\u57df\u63a8\u8fdb VLM \u5728\u590d\u6742\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\u3002\u6211\u4eec\u5728 \\url{https://github.com/asgsaeid/cad_vqa} \u4e0a\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u4ee3\u7801\u3002||\n", "2409.02101": "|**2024-09-03**|[Towards Real-World Adverse Weather Image Restoration: Enhancing Clearness and Semantics with Vision-Language Models](http://arxiv.org/abs/2409.02101)|**[link](https://github.com/jiaqixuac/WResVLM)**|\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6076\u52a3\u5929\u6c14\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u5e94\u7528\u4e8e\u73b0\u5b9e\u573a\u666f\u65f6\u7684\u5c40\u9650\u6027\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6765\u589e\u5f3a\u73b0\u5b9e\u73af\u5883\u4e2d\u4e0d\u540c\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u6062\u590d\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u56fe\u50cf\u6e05\u6670\u5ea6\u8bc4\u4f30\u548c\u8bed\u4e49\u63d0\u4f9b\uff0c\u4f5c\u4e3a\u8bad\u7ec3\u6062\u590d\u6a21\u578b\u7684\u76d1\u7763\u4fe1\u53f7\u3002\u5bf9\u4e8e\u6e05\u6670\u5ea6\u589e\u5f3a\uff0c\u6211\u4eec\u4f7f\u7528\u771f\u5b9e\u6570\u636e\uff0c\u91c7\u7528\u53cc\u91cd\u7b56\u7565\uff0c\u5373\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u7684\u4f2a\u6807\u7b7e\u548c\u5929\u6c14\u63d0\u793a\u5b66\u4e60\u3002\u5bf9\u4e8e\u8bed\u4e49\u589e\u5f3a\uff0c\u6211\u4eec\u901a\u8fc7\u8c03\u6574\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63cf\u8ff0\u4e2d\u7684\u5929\u6c14\u6761\u4ef6\uff0c\u540c\u65f6\u4fdd\u7559\u8bed\u4e49\uff0c\u6765\u6574\u5408\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\u6765\u63d0\u5347\u6062\u590d\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u7684\u6076\u52a3\u5929\u6c14\u56fe\u50cf\u6062\u590d\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7ed3\u679c\uff0c\u901a\u8fc7\u4e0e\u73b0\u6709\u6700\u4f73\u5de5\u4f5c\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u6bd4\u8f83\u8bc1\u660e\u4e86\u8fd9\u4e00\u70b9\u3002||\n", "2409.02084": "|**2024-09-03**|[GraspSplats: Efficient Manipulation with 3D Feature Splatting](http://arxiv.org/abs/2409.02084)|null|The ability for robots to perform efficient and zero-shot grasping of object parts is crucial for practical applications and is becoming prevalent with recent advances in Vision-Language Models (VLMs). To bridge the 2D-to-3D gap for representations to support such a capability, existing methods rely on neural fields (NeRFs) via differentiable rendering or point-based projection methods. However, we demonstrate that NeRFs are inappropriate for scene changes due to their implicitness and point-based methods are inaccurate for part localization without rendering-based optimization. To amend these issues, we propose GraspSplats. Using depth supervision and a novel reference feature computation method, GraspSplats generates high-quality scene representations in under 60 seconds. We further validate the advantages of Gaussian-based representation by showing that the explicit and optimized geometry in GraspSplats is sufficient to natively support (1) real-time grasp sampling and (2) dynamic and articulated object manipulation with point trackers. With extensive experiments on a Franka robot, we demonstrate that GraspSplats significantly outperforms existing methods under diverse task settings. In particular, GraspSplats outperforms NeRF-based methods like F3RM and LERF-TOGO, and 2D detection methods.||\n", "2409.03521": "|**2024-09-05**|[Have Large Vision-Language Models Mastered Art History?](http://arxiv.org/abs/2409.03521)|null|The emergence of large Vision-Language Models (VLMs) has recently established new baselines in image classification across multiple domains. However, the performance of VLMs in the specific task of artwork classification, particularly art style classification of paintings - a domain traditionally mastered by art historians - has not been explored yet. Artworks pose a unique challenge compared to natural images due to their inherently complex and diverse structures, characterized by variable compositions and styles. Art historians have long studied the unique aspects of artworks, with style prediction being a crucial component of their discipline. This paper investigates whether large VLMs, which integrate visual and textual data, can effectively predict the art historical attributes of paintings. We conduct an in-depth analysis of four VLMs, namely CLIP, LLaVA, OpenFlamingo, and GPT-4o, focusing on zero-shot classification of art style, author and time period using two public benchmarks of artworks. Additionally, we present ArTest, a well-curated test set of artworks, including pivotal paintings studied by art historians.||\n", "2409.04053": "|**2024-09-06**|[COLUMBUS: Evaluating COgnitive Lateral Understanding through Multiple-choice reBUSes](http://arxiv.org/abs/2409.04053)|null|\u867d\u7136\u89c6\u89c9\u95ee\u7b54 (VQA) \u57fa\u51c6\u6d4b\u8bd5\u63a8\u52a8\u4e86\u63a8\u7406\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4f46\u5b83\u4eec\u4e00\u76f4\u4e13\u6ce8\u4e8e\u5782\u76f4\u601d\u7ef4\u3002\u6709\u6548\u7684\u89e3\u51b3\u95ee\u9898\u8fd8\u9700\u8981\u6a2a\u5411\u601d\u7ef4\uff0c\u800c\u6a2a\u5411\u601d\u7ef4\u5728\u4eba\u5de5\u667a\u80fd\u9886\u57df\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u4e5f\u6ca1\u6709\u7528\u4e8e\u6d4b\u8bd5\u89c6\u89c9\u611f\u77e5\u7cfb\u7edf\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5c06\u89c6\u89c9\u6a2a\u5411\u601d\u7ef4\u5f62\u5f0f\u5316\u4e3a\u4e00\u4e2a\u591a\u9879\u9009\u62e9\u9898\u95ee\u7b54\u4efb\u52a1\uff0c\u5e76\u63cf\u8ff0\u4e86\u4e00\u4e2a\u7531\u5206\u7c7b\u6cd5\u9a71\u52a8\u7684\u4e09\u6b65\u6cd5\u6765\u5b9e\u4f8b\u5316\u4efb\u52a1\u793a\u4f8b\u3002\u7136\u540e\uff0c\u6211\u4eec\u5f00\u53d1\u4e86 COLUMBUS\uff0c\u8fd9\u662f\u4e00\u4e2a\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b83\u5e94\u7528\u4efb\u52a1\u7ba1\u9053\uff0c\u6839\u636e\u516c\u5f00\u53ef\u7528\u7684\u5316\u5408\u7269\u548c\u5e38\u7528\u77ed\u8bed\u96c6\u5408\uff0c\u521b\u5efa\u5e26\u6709\u6587\u672c\u548c\u56fe\u6807\u5b57\u8c1c\u7684 QA \u96c6\u3002COLUMBUS \u5305\u542b\u8d85\u8fc7 1,000 \u4e2a\u8c1c\u9898\uff0c\u6bcf\u4e2a\u8c1c\u9898\u6709\u56db\u4e2a\u5019\u9009\u7b54\u6848\u3002\u867d\u7136\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u53d6\u5f97\u4e86\u4e0d\u9519\u7684\u6027\u80fd\uff0c\u4f46\u6211\u4eec\u7684\u8bc4\u4f30\u8868\u660e\u4eba\u7c7b\u548c\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u5de8\u5927\u5dee\u8ddd\u3002VLM \u53d7\u76ca\u4e8e\u4eba\u5de5\u7b56\u5212\u7684\u63cf\u8ff0\uff0c\u4f46\u5728\u6b63\u786e\u7684\u62bd\u8c61\u7ea7\u522b\u4e0a\u96be\u4ee5\u81ea\u884c\u751f\u6210\u6b64\u7c7b\u8868\u793a\u3002||\n", "2409.03961": "|**2024-09-06**|[Generating Faithful and Salient Text from Multimodal Data](http://arxiv.org/abs/2409.03961)|**[link](https://github.com/TahsinaHashem/FaithD2T)**|\u867d\u7136\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u5728\u8bb8\u591a\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u5728\u751f\u6210\u6587\u672c\u65f6\u4ecd\u53ef\u80fd\u4f1a\u51fa\u73b0\u5e7b\u89c9\u3002\u5b83\u4eec\u5728\u4ece\u89c6\u89c9\u6570\u636e\u4e2d\u68c0\u6d4b\u663e\u8457\u7279\u5f81\u65b9\u9762\u7684\u6027\u80fd\u4e5f\u4e0d\u6e05\u695a\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6df7\u5408\u6a21\u6001\u6570\u636e\uff08\u5305\u62ec\u56fe\u50cf\u548c\u7ed3\u6784\u5316\u6570\u636e\uff08\u4ee5\u77e5\u8bc6\u56fe\u8c31\u6216\u8868\u683c\u8868\u793a\uff09\uff09\u751f\u6210\u5fe0\u5b9e\u4e14\u663e\u8457\u7684\u6587\u672c\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5c0f\u578b\u89c6\u89c9\u8bc4\u8bba\u5bb6\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u56fe\u50cf\u6a21\u6001\u4e2d\u8bc6\u522b\u5e7b\u89c9\u548c\u975e\u663e\u8457\u7279\u5f81\u3002\u8bc4\u8bba\u5bb6\u6a21\u578b\u8fd8\u4f1a\u751f\u6210\u663e\u8457\u56fe\u50cf\u7279\u5f81\u5217\u8868\u3002\u6b64\u4fe1\u606f\u7528\u4e8e\u540e\u671f\u7f16\u8f91\u6b65\u9aa4\uff0c\u4ee5\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\u3002\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u63d0\u9ad8\u4e86 LMM \u5728\u5fe0\u5b9e\u5ea6\u548c\u663e\u8457\u6027\u65b9\u9762\u7684\u751f\u6210\u8d28\u91cf\uff0c\u4f18\u4e8e\u6700\u8fd1\u65e8\u5728\u51cf\u5c11\u5e7b\u89c9\u7684\u6280\u672f\u3002||\n", "2409.03868": "|**2024-09-05**|[Few-shot Adaptation of Medical Vision-Language Models](http://arxiv.org/abs/2409.03868)|**[link](https://github.com/fereshteshakeri/few-shot-medvlms)**|Integrating image and text data through multi-modal learning has emerged as a new approach in medical imaging research, following its successful deployment in computer vision. While considerable efforts have been dedicated to establishing medical foundation models and their zero-shot transfer to downstream tasks, the popular few-shot setting remains relatively unexplored. Following on from the currently strong emergence of this setting in computer vision, we introduce the first structured benchmark for adapting medical vision-language models (VLMs) in a strict few-shot regime and investigate various adaptation strategies commonly used in the context of natural images. Furthermore, we evaluate a simple generalization of the linear-probe adaptation baseline, which seeks an optimal blending of the visual prototypes and text embeddings via learnable class-wise multipliers. Surprisingly, such a text-informed linear probe yields competitive performances in comparison to convoluted prompt-learning and adapter-based strategies, while running considerably faster and accommodating the black-box setting. Our extensive experiments span three different medical modalities and specialized foundation models, nine downstream tasks, and several state-of-the-art few-shot adaptation methods. We made our benchmark and code publicly available to trigger further developments in this emergent subject: \\url{https://github.com/FereshteShakeri/few-shot-MedVLMs}.||\n", "2409.06351": "|**2024-09-10**|[MAGDA: Multi-agent guideline-driven diagnostic assistance](http://arxiv.org/abs/2409.06351)|null|\u5728\u6025\u8bca\u79d1\u3001\u4e61\u6751\u533b\u9662\u6216\u6b20\u53d1\u8fbe\u5730\u533a\u7684\u8bca\u6240\uff0c\u4e34\u5e8a\u533b\u751f\u5f80\u5f80\u7f3a\u4e4f\u8bad\u7ec3\u6709\u7d20\u7684\u653e\u5c04\u79d1\u533b\u751f\u8fdb\u884c\u5feb\u901f\u56fe\u50cf\u5206\u6790\uff0c\u8fd9\u53ef\u80fd\u5bf9\u60a3\u8005\u7684\u533b\u7597\u4fdd\u5065\u4ea7\u751f\u4e0d\u5229\u5f71\u54cd\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6709\u53ef\u80fd\u901a\u8fc7\u63d0\u4f9b\u6709\u52a9\u4e8e\u4e34\u5e8a\u533b\u751f\u505a\u51fa\u51b3\u7b56\u7684\u89c1\u89e3\uff0c\u4ece\u800c\u51cf\u8f7b\u4ed6\u4eec\u7684\u4e00\u4e9b\u538b\u529b\u3002\u867d\u7136\u8fd9\u4e9b LLM \u5728\u533b\u5b66\u8003\u8bd5\u4e2d\u53d6\u5f97\u4e86\u5f88\u9ad8\u7684\u6d4b\u8bd5\u6210\u7ee9\uff0c\u5c55\u793a\u4e86\u5176\u4e30\u5bcc\u7684\u7406\u8bba\u533b\u5b66\u77e5\u8bc6\uff0c\u4f46\u5b83\u4eec\u5f80\u5f80\u4e0d\u9075\u5faa\u533b\u5b66\u6307\u5357\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u96f6\u6837\u672c\u6307\u5357\u9a71\u52a8\u51b3\u7b56\u652f\u6301\u65b9\u6cd5\u3002\u6211\u4eec\u6a21\u62df\u4e86\u4e00\u4e2a\u7531\u591a\u4e2a LLM \u4ee3\u7406\u7ec4\u6210\u7684\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u589e\u5f3a\u4e86\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u8fd9\u4e9b\u4ee3\u7406\u534f\u4f5c\u4ee5\u8fbe\u6210\u60a3\u8005\u8bca\u65ad\u3002\u5728\u5411\u4ee3\u7406\u63d0\u4f9b\u7b80\u5355\u7684\u8bca\u65ad\u6307\u5357\u540e\uff0c\u4ed6\u4eec\u5c06\u6839\u636e\u8fd9\u4e9b\u6307\u5357\u5408\u6210\u63d0\u793a\u5e76\u7b5b\u9009\u56fe\u50cf\u4ee5\u67e5\u627e\u7ed3\u679c\u3002\u6700\u540e\uff0c\u4ed6\u4eec\u4e3a\u81ea\u5df1\u7684\u8bca\u65ad\u63d0\u4f9b\u6613\u4e8e\u7406\u89e3\u7684\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u7136\u540e\u5bf9\u5176\u8fdb\u884c\u81ea\u6211\u5b8c\u5584\uff0c\u4ee5\u8003\u8651\u75be\u75c5\u4e4b\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\u3002\u7531\u4e8e\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u96f6\u6837\u672c\u7684\uff0c\u56e0\u6b64\u5b83\u9002\u7528\u4e8e\u7f55\u89c1\u75be\u75c5\u7684\u8bbe\u7f6e\uff0c\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0c\u8bad\u7ec3\u6570\u636e\u6709\u9650\uff0c\u4f46\u53ef\u4ee5\u4f7f\u7528\u4e13\u5bb6\u5236\u5b9a\u7684\u75be\u75c5\u63cf\u8ff0\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u80f8\u90e8 X \u5149\u6570\u636e\u96c6 CheXpert \u548c ChestX-ray 14 Longtail \u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u76f8\u5bf9\u4e8e\u73b0\u6709\u96f6\u6837\u672c\u65b9\u6cd5\u7684\u6027\u80fd\u6539\u8fdb\u4ee5\u53ca\u5bf9\u7f55\u89c1\u75be\u75c5\u7684\u6cdb\u5316\u80fd\u529b\u3002||\n", "2409.06210": "|**2024-09-10**|[INTRA: Interaction Relationship-aware Weakly Supervised Affordance Grounding](http://arxiv.org/abs/2409.06210)|null|\u53ef\u4f9b\u6027\u662f\u6307\u7269\u4f53\u56fa\u6709\u7684\u6f5c\u5728\u4ea4\u4e92\u65b9\u5f0f\u3002\u5bf9\u53ef\u4f9b\u6027\u7684\u611f\u77e5\u53ef\u4ee5\u8ba9\u667a\u80fd\u4f53\u9ad8\u6548\u5730\u5728\u65b0\u73af\u5883\u4e2d\u5bfc\u822a\u548c\u4ea4\u4e92\u3002\u5f31\u76d1\u7763\u53ef\u4f9b\u6027\u57fa\u7840\u53ef\u4ee5\u8ba9\u667a\u80fd\u4f53\u5728\u6ca1\u6709\u6602\u8d35\u7684\u50cf\u7d20\u7ea7\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u53ef\u4f9b\u6027\u7684\u6982\u5ff5\uff0c\u4f46\u9700\u8981\u4f7f\u7528\u4ee5\u73af\u5883\u4e3a\u4e2d\u5fc3\u7684\u56fe\u50cf\u3002\u5c3d\u7ba1\u5f31\u76d1\u7763\u53ef\u4f9b\u6027\u57fa\u7840\u7684\u6700\u65b0\u8fdb\u5c55\u5df2\u7ecf\u53d6\u5f97\u4e86\u53ef\u559c\u7684\u6210\u679c\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u4e00\u4e9b\u6311\u6218\uff0c\u4f8b\u5982\u9700\u8981\u914d\u5bf9\u7684\u4ee5\u73af\u5883\u4e3a\u4e2d\u5fc3\u548c\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u4e3a\u5355\u4e2a\u7269\u4f53\u57fa\u7840\u591a\u79cd\u53ef\u4f9b\u6027\u7684\u590d\u6742\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4ea4\u4e92\u5173\u7cfb\u611f\u77e5\u7684\u5f31\u76d1\u7763\u53ef\u4f9b\u6027\u57fa\u7840 (INTRA)\u3002\u4e0e\u73b0\u6709\u6280\u672f\u4e0d\u540c\uff0cINTRA \u5c06\u8fd9\u4e2a\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8868\u5f81\u5b66\u4e60\uff0c\u901a\u8fc7\u4ec5\u4f7f\u7528\u4ee5\u73af\u5883\u4e3a\u4e2d\u5fc3\u7684\u56fe\u50cf\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\u6765\u8bc6\u522b\u4ea4\u4e92\u7684\u72ec\u7279\u7279\u5f81\uff0c\u4ece\u800c\u6d88\u9664\u4e86\u5bf9\u914d\u5bf9\u6570\u636e\u96c6\u7684\u9700\u6c42\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u6765\u7075\u6d3b\u5730\u4f7f\u7528\u4efb\u4f55\u6587\u672c\u8fdb\u884c\u53ef\u4f9b\u6027\u57fa\u7840\uff0c\u8bbe\u8ba1\u4e86\u4ee5\u6587\u672c\u4e3a\u6761\u4ef6\u7684\u53ef\u4f9b\u6027\u6620\u5c04\u751f\u6210\uff0c\u4ee5\u53cd\u6620\u4ea4\u4e92\u5173\u7cfb\u4ee5\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u6211\u4eec\u7684\u6587\u672c\u540c\u4e49\u8bcd\u589e\u5f3a\u6765\u589e\u5f3a\u9c81\u68d2\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728 AGD20K\u3001IIT-AFF\u3001CAD \u548c UMD \u7b49\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002\u6b64\u5916\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5bf9\u5408\u6210\u56fe\u50cf/\u63d2\u56fe\u5177\u6709\u663e\u8457\u7684\u9886\u57df\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4e14\u80fd\u591f\u5bf9\u65b0\u7684\u4ea4\u4e92\u548c\u7269\u4f53\u8fdb\u884c\u53ef\u4f9b\u6027\u57fa\u7840\u3002||\n", "2409.06166": "|**2024-09-10**|[Revisiting Prompt Pretraining of Vision-Language Models](http://arxiv.org/abs/2409.06166)|null|\u63d0\u793a\u5b66\u4e60\u662f\u4e00\u79cd\u6709\u6548\u7684\u5b9a\u5236\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u4ee5\u9002\u5e94\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u5b83\u4ec5\u9700\u5fae\u8c03\u8f93\u5165\u63d0\u793a\u8bcd\u7b26\u7684\u5c11\u91cf\u53c2\u6570\u3002\u8fd1\u5e74\u6765\uff0c\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08\u4f8b\u5982 ImageNet-21K\uff09\u4e0a\u8fdb\u884c\u63d0\u793a\u9884\u8bad\u7ec3\u5df2\u6210\u4e3a\u901a\u7528\u89c6\u89c9\u8bc6\u522b\u63d0\u793a\u5b66\u4e60\u7684\u5173\u952e\u3002\u7136\u800c\uff0c\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u5e76\u89c2\u5bdf\u5230\uff0c\u5728\u63d0\u793a\u9884\u8bad\u7ec3\u671f\u95f4\uff0c\u9274\u4e8e\u56fe\u50cf\u6570\u91cf\u5e9e\u5927\uff0c\u6709\u9650\u7684\u53ef\u5b66\u4e60\u63d0\u793a\u53ef\u80fd\u4f1a\u9762\u4e34\u6b20\u62df\u5408\u7684\u98ce\u9669\uff0c\u540c\u65f6\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u91cd\u65b0\u5ba1\u89c6\u63d0\u793a\u9884\u8bad\u7ec3\u201d\uff08RPP\uff09\u7684\u901a\u7528\u6846\u67b6\uff0c\u65e8\u5728\u4ece\u63d0\u793a\u7ed3\u6784\u548c\u63d0\u793a\u76d1\u7763\u4e24\u4e2a\u65b9\u9762\u63d0\u9ad8\u62df\u5408\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5bf9\u4e8e\u63d0\u793a\u7ed3\u6784\uff0c\u6211\u4eec\u6253\u7834\u4e86\u67e5\u8be2\u3001\u952e\u548c\u503c\u5411\u91cf\u5747\u6765\u81ea\u5171\u4eab\u7684\u53ef\u5b66\u4e60\u63d0\u793a\u8bcd\u7b26\u7684\u5e38\u89c1\u505a\u6cd5\u7684\u9650\u5236\u3002\u76f8\u53cd\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u975e\u5171\u4eab\u7684\u72ec\u7acb\u67e5\u8be2\u3001\u952e\u548c\u503c\u53ef\u5b66\u4e60\u63d0\u793a\uff0c\u4ece\u800c\u901a\u8fc7\u589e\u52a0\u53c2\u6570\u591a\u6837\u6027\u6765\u589e\u5f3a\u6a21\u578b\u7684\u62df\u5408\u80fd\u529b\u3002\u5bf9\u4e8e\u63d0\u793a\u76d1\u7763\uff0c\u6211\u4eec\u8fd8\u5229\u7528\u4e86\u7531\u9884\u8bad\u7ec3\u7684\u5bf9\u6bd4\u8bed\u8a00\u56fe\u50cf\u9884\u8bad\u7ec3 (CLIP) \u6559\u5e08\u6a21\u578b\u63d0\u4f9b\u7684\u96f6\u6837\u672c\u6982\u7387\u9884\u6d4b\u5f97\u5230\u7684\u8f6f\u6807\u7b7e\u3002\u8fd9\u4e9b\u8f6f\u6807\u7b7e\u53ef\u4ee5\u66f4\u7ec6\u81f4\u3001\u66f4\u5168\u9762\u5730\u6d1e\u5bdf\u7c7b\u95f4\u5173\u7cfb\uff0c\u4ece\u800c\u8d4b\u4e88\u9884\u8bad\u7ec3\u8fc7\u7a0b\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002RPP \u4ea7\u751f\u66f4\u7a33\u5065\u7684\u63d0\u793a\u521d\u59cb\u5316\uff0c\u589e\u5f3a\u5176\u5728\u5404\u79cd\u89c6\u89c9\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u8fc1\u79fb\u80fd\u529b\u3002\u8de8\u591a\u4e2a\u57fa\u51c6\u7684\u5b9e\u9a8c\u4e00\u81f4\u8bc1\u5b9e\u4e86\u6211\u4eec\u9884\u8bad\u7ec3\u63d0\u793a\u7684\u6700\u65b0\u6027\u80fd\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5f88\u5feb\u53d1\u5e03\u3002||\n", "2409.06078": "|**2024-09-09**|[PEERNet: An End-to-End Profiling Tool for Real-Time Networked Robotic Systems](http://arxiv.org/abs/2409.06078)|null|\u7f51\u7edc\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u3001\u65e0\u4eba\u673a\u7fa4\u548c\u8fdc\u7a0b\u624b\u672f\u7b49\u5e94\u7528\u4e2d\u9700\u8981\u5e73\u8861\u8ba1\u7b97\u3001\u529f\u8017\u548c\u5ef6\u8fdf\u7ea6\u675f\u3002\u8be5\u9886\u57df\u7684\u6838\u5fc3\u95ee\u9898\u662f\u4f55\u65f6\u5c06\u8ba1\u7b97\u91cf\u5927\u7684\u4efb\u52a1\u5378\u8f7d\u5230\u4e91\u7aef\uff08\u8fdc\u7a0b\u670d\u52a1\u5668\uff09\u4ee5\u6362\u53d6\u901a\u4fe1\u5ef6\u8fdf\u3002\u4efb\u52a1\u5378\u8f7d\u7b97\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5bf9\u7cfb\u7edf\u7279\u5b9a\u6027\u80fd\u6307\u6807\u7684\u7cbe\u786e\u4e86\u89e3\uff0c\u4f8b\u5982\u4f20\u611f\u5668\u6570\u636e\u901f\u7387\u3001\u7f51\u7edc\u5e26\u5bbd\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5ef6\u8fdf\u3002\u867d\u7136\u8fd9\u4e9b\u6307\u6807\u53ef\u4ee5\u5728\u7cfb\u7edf\u8bbe\u8ba1\u671f\u95f4\u8fdb\u884c\u5efa\u6a21\uff0c\u4f46\u8fde\u63a5\u8d28\u91cf\u3001\u670d\u52a1\u5668\u8d1f\u8f7d\u548c\u786c\u4ef6\u6761\u4ef6\u7684\u4e0d\u786e\u5b9a\u6027\u4f1a\u5bfc\u81f4\u5b9e\u65f6\u6027\u80fd\u53d8\u5316\uff0c\u4ece\u800c\u5f71\u54cd\u6574\u4f53\u6027\u80fd\u3002\u6211\u4eec\u63a8\u51fa\u4e86 PEERNet\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u4e91\u673a\u5668\u4eba\u7684\u7aef\u5230\u7aef\u5b9e\u65f6\u5206\u6790\u5de5\u5177\u3002PEERNet \u901a\u8fc7\u5bf9\u4f20\u611f\u5668\u3001\u7f51\u7edc\u3001\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\u548c\u8bbe\u5907\u7b49\u7cfb\u7edf\u7ec4\u4ef6\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u4f46\u81ea\u9002\u5e94\u7684\u5206\u6790\uff0c\u4ece\u800c\u80fd\u591f\u5728\u5f02\u6784\u786c\u4ef6\u4e0a\u8fdb\u884c\u6027\u80fd\u76d1\u63a7\u3002\u6211\u4eec\u901a\u8fc7\u7f51\u7edc\u673a\u5668\u4eba\u4efb\u52a1\u5c55\u793a\u4e86 PEERNet \u7684\u529f\u80fd\uff0c\u4f8b\u5982\u57fa\u4e8e\u56fe\u50cf\u7684 Franka Emika Panda \u673a\u68b0\u81c2\u8fdc\u7a0b\u64cd\u4f5c\u548c\u4f7f\u7528 Nvidia Jetson Orin \u67e5\u8be2\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002PEERNet \u63ed\u793a\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u975e\u76f4\u89c2\u7684\u7684\u884c\u4e3a\uff0c\u4f8b\u5982\u975e\u5bf9\u79f0\u7f51\u7edc\u4f20\u8f93\u548c\u53cc\u5cf0\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u5f3a\u8c03\u4e86\u7f51\u7edc\u673a\u5668\u4eba\u4e2d\u57fa\u51c6\u6d4b\u8bd5\u7684\u6709\u6548\u6027\u548c\u91cd\u8981\u6027\uff0c\u8bc1\u660e\u4e86 PEERNet \u7684\u9002\u5e94\u6027\u3002\u6211\u4eec\u7684\u4ee3\u7801\u662f\u5f00\u6e90\u7684\uff0c\u53ef\u5728 github.com/UTAustin-SwarmLab/PEERNet \u83b7\u53d6\u3002||\n", "2409.05493": "|**2024-09-09**|[DexDiff: Towards Extrinsic Dexterity Manipulation of Ungraspable Objects in Unrestricted Environments](http://arxiv.org/abs/2409.05493)|null|\u6293\u53d6\u53c8\u5927\u53c8\u5e73\u7684\u7269\u4f53\uff08\u4f8b\u5982\u4e66\u6216\u5e73\u5e95\u9505\uff09\u901a\u5e38\u88ab\u8ba4\u4e3a\u662f\u4e00\u9879\u65e0\u6cd5\u5b8c\u6210\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u6293\u53d6\u59ff\u52bf\u65e0\u6cd5\u4f01\u53ca\uff0c\u8fd9\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u4ee5\u524d\u7684\u5de5\u4f5c\u5229\u7528\u5899\u58c1\u6216\u684c\u5b50\u8fb9\u7f18\u7b49\u5916\u90e8\u7075\u6d3b\u6027\u6765\u6293\u53d6\u6b64\u7c7b\u7269\u4f53\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4ec5\u9650\u4e8e\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u7b56\u7565\uff0c\u5e76\u4e14\u7f3a\u4e4f\u5bfb\u627e\u9884\u6293\u53d6\u6761\u4ef6\u7684\u4efb\u52a1\u89c4\u5212\u3002\u8fd9\u4f7f\u5f97\u9002\u5e94\u5404\u79cd\u73af\u5883\u548c\u5916\u90e8\u7075\u6d3b\u6027\u7ea6\u675f\u53d8\u5f97\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DexDiff\uff0c\u4e00\u79cd\u7528\u4e8e\u5177\u6709\u5916\u90e8\u7075\u6d3b\u6027\u7684\u957f\u89c6\u91ce\u89c4\u5212\u7684\u7a33\u5065\u673a\u5668\u4eba\u64cd\u4f5c\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u6765\u611f\u77e5\u73af\u5883\u72b6\u6001\u5e76\u751f\u6210\u9ad8\u7ea7\u4efb\u52a1\u8ba1\u5212\uff0c\u7136\u540e\u4f7f\u7528\u76ee\u6807\u6761\u4ef6\u52a8\u4f5c\u6269\u6563 (GCAD) \u6a21\u578b\u6765\u9884\u6d4b\u4f4e\u7ea7\u52a8\u4f5c\u5e8f\u5217\u3002\u8be5\u6a21\u578b\u4ece\u79bb\u7ebf\u6570\u636e\u4e2d\u5b66\u4e60\u4f4e\u7ea7\u7b56\u7565\uff0c\u5e76\u5c06\u9ad8\u7ea7\u89c4\u5212\u5f15\u5bfc\u7684\u7d2f\u79ef\u5956\u52b1\u4f5c\u4e3a\u76ee\u6807\u6761\u4ef6\uff0c\u4ece\u800c\u53ef\u4ee5\u6539\u8fdb\u5bf9\u673a\u5668\u4eba\u52a8\u4f5c\u7684\u9884\u6d4b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u53ef\u4ee5\u6709\u6548\u5730\u6267\u884c\u65e0\u6cd5\u5b8c\u6210\u7684\u4efb\u52a1\uff0c\u800c\u4e14\u53ef\u4ee5\u6cdb\u5316\u5230\u4ee5\u524d\u4ece\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u3002\u5b83\u5728\u6a21\u62df\u4e2d\u7684\u6210\u529f\u7387\u6bd4\u57fa\u7ebf\u9ad8 47%\uff0c\u5e76\u6709\u52a9\u4e8e\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u9ad8\u6548\u90e8\u7f72\u548c\u64cd\u4f5c\u3002||\n", "2409.05076": "|**2024-09-08**|[PIP: Detecting Adversarial Examples in Large Vision-Language Models via Attention Patterns of Irrelevant Probe Questions](http://arxiv.org/abs/2409.05076)|null|\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u5df2\u7ecf\u5c55\u793a\u51fa\u5f3a\u5927\u7684\u591a\u6a21\u6001\u80fd\u529b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4e5f\u9762\u4e34\u7740\u4e25\u91cd\u7684\u5b89\u5168\u95ee\u9898\uff0c\u56e0\u4e3a\u653b\u51fb\u8005\u53ef\u4ee5\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5bf9\u6297\u6837\u672c\u5728 LVLM \u4e2d\u5f15\u53d1\u9c81\u68d2\u6027\u95ee\u9898\u3002\u56e0\u6b64\uff0cLVLM \u8feb\u5207\u9700\u8981\u9488\u5bf9\u5bf9\u6297\u6837\u672c\u7684\u68c0\u6d4b\u5de5\u5177\uff0c\u4ee5\u9632\u6b62\u51fa\u73b0\u9519\u8bef\u54cd\u5e94\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u53d1\u73b0\uff0c\u5f53\u4f7f\u7528\u63a2\u6d4b\u95ee\u9898\u65f6\uff0cLVLM \u5bf9\u5e72\u51c0\u56fe\u50cf\u8868\u73b0\u51fa\u89c4\u5f8b\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a PIP \u7684\u975e\u5e38\u89c4\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u4e00\u4e2a\u968f\u673a\u9009\u62e9\u7684\u65e0\u5173\u63a2\u6d4b\u95ee\u9898\uff08\u4f8b\u5982\uff0c\u201c\u6709\u949f\u8868\u5417\uff1f\u201d\uff09\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u6765\u533a\u5206\u5bf9\u6297\u6837\u672c\u548c\u5e72\u51c0\u6837\u672c\u3002\u65e0\u8bba\u5f85\u6d4b\u56fe\u50cf\u53ca\u5176\u5bf9\u5e94\u7684\u95ee\u9898\u662f\u4ec0\u4e48\uff0cPIP \u53ea\u9700\u8981\u5bf9\u5f85\u6d4b\u56fe\u50cf\u548c\u63a2\u6d4b\u95ee\u9898\u8fdb\u884c\u4e00\u6b21\u989d\u5916\u7684\u63a8\u7406\uff0c\u5373\u53ef\u6210\u529f\u68c0\u6d4b\u5bf9\u6297\u6837\u672c\u3002\u5373\u4f7f\u5728\u9ed1\u76d2\u653b\u51fb\u548c\u5f00\u653e\u6570\u636e\u96c6\u573a\u666f\u4e0b\uff0c\u6211\u4eec\u7684 PIP \u4e0e\u7b80\u5355\u7684 SVM \u76f8\u7ed3\u5408\uff0c\u4ecd\u7136\u53ef\u4ee5\u5b9e\u73b0\u8d85\u8fc7 98% \u7684\u53ec\u56de\u7387\u548c\u8d85\u8fc7 90% \u7684\u7cbe\u786e\u7387\u3002\u6211\u4eec\u7684 PIP \u662f\u9996\u6b21\u5c1d\u8bd5\u901a\u8fc7\u7b80\u5355\u7684\u65e0\u5173\u63a2\u6d4b\u95ee\u9898\u6765\u68c0\u6d4b\u9488\u5bf9 LVLM \u7684\u5bf9\u6297\u653b\u51fb\uff0c\u4e3a\u66f4\u6df1\u5165\u5730\u7406\u89e3\u548c\u53cd\u601d LVLM \u63d0\u4f9b\u4e86\u601d\u8def\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/btzyd/pip \u83b7\u53d6\u3002||\n", "2409.05916": "|**2024-09-07**|[Unlocking Potential Binders: Multimodal Pretraining DEL-Fusion for Denoising DNA-Encoded Libraries](http://arxiv.org/abs/2409.05916)|null|\u5728\u836f\u7269\u53d1\u73b0\u9886\u57df\uff0cDNA \u7f16\u7801\u5316\u5408\u7269\u5e93 (DEL) \u7b5b\u9009\u6280\u672f\u5df2\u6210\u4e3a\u8bc6\u522b\u9ad8\u4eb2\u548c\u529b\u5316\u5408\u7269\u7684\u6709\u6548\u65b9\u6cd5\u3002\u7136\u800c\uff0cDEL \u7b5b\u9009\u9762\u4e34\u7740\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff1a\u590d\u6742\u751f\u7269\u7cfb\u7edf\u4e2d\u975e\u7279\u5f02\u6027\u76f8\u4e92\u4f5c\u7528\u4ea7\u751f\u7684\u566a\u58f0\u3002\u5728 DEL \u5e93\u4e0a\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u5df2\u88ab\u7528\u4e8e\u63d0\u53d6\u5316\u5408\u7269\u7279\u5f81\uff0c\u65e8\u5728\u5bf9\u6570\u636e\u8fdb\u884c\u53bb\u566a\u5e76\u53d1\u73b0\u6f5c\u5728\u7684\u6cbb\u7597\u9776\u70b9\u7ed3\u5408\u5242\u3002\u7136\u800c\uff0cDEL \u7684\u56fa\u6709\u7ed3\u6784\u53d7\u9650\u4e8e\u7ed3\u6784\u5355\u5143\u7684\u6709\u9650\u591a\u6837\u6027\uff0c\u8fd9\u5f71\u54cd\u4e86\u5316\u5408\u7269\u7f16\u7801\u5668\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u5728\u5355\u4e00\u7ea7\u522b\u6355\u83b7\u5316\u5408\u7269\u7279\u5f81\uff0c\u8fdb\u4e00\u6b65\u9650\u5236\u4e86\u53bb\u566a\u7b56\u7565\u7684\u6709\u6548\u6027\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u9884\u8bad\u7ec3 DEL-Fusion \u6a21\u578b (MPDF)\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u9884\u8bad\u7ec3\u589e\u5f3a\u7f16\u7801\u5668\u80fd\u529b\uff0c\u5e76\u5728\u4e0d\u540c\u5c3a\u5ea6\u4e0a\u6574\u5408\u5316\u5408\u7269\u7279\u5f81\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u5728\u4e0d\u540c\u5316\u5408\u7269\u8868\u793a\u53ca\u5176\u6587\u672c\u63cf\u8ff0\u4e4b\u95f4\u5e94\u7528\u5bf9\u6bd4\u76ee\u6807\u7684\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u589e\u5f3a\u4e86\u5316\u5408\u7269\u7f16\u7801\u5668\u83b7\u53d6\u901a\u7528\u7279\u5f81\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 DEL-fusion \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u878d\u5408\u4e86\u539f\u5b50\u3001\u4e9a\u5206\u5b50\u548c\u5206\u5b50\u6c34\u5e73\u7684\u5316\u5408\u7269\u4fe1\u606f\uff0c\u8fd9\u4e9b\u4fe1\u606f\u7531\u5404\u79cd\u5316\u5408\u7269\u7f16\u7801\u5668\u6355\u83b7\u3002\u8fd9\u4e9b\u521b\u65b0\u7684\u534f\u540c\u4f5c\u7528\u4f7f MPDF \u5177\u5907\u4e30\u5bcc\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u4ece\u800c\u5b9e\u73b0\u5168\u9762\u7684\u4e0b\u6e38\u53bb\u566a\u3002\u5728\u4e09\u4e2a DEL \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u8bc4\u4f30\u8868\u660e\uff0cMPDF \u5728\u9a8c\u8bc1\u4efb\u52a1\u7684\u6570\u636e\u5904\u7406\u548c\u5206\u6790\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cMPDF \u4e3a\u8bc6\u522b\u9ad8\u4eb2\u548c\u529b\u5206\u5b50\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u4e3a\u6539\u8fdb DEL \u5728\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002||\n", "2409.04828": "|**2024-09-07**|[POINTS: Improving Your Vision-language Model with Affordable Strategies](http://arxiv.org/abs/2409.04828)|null|\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u5728\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\u548c\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0c\u4ecd\u7136\u5b58\u5728\u51e0\u4e2a\u5173\u952e\u95ee\u9898\uff1a1\uff09\u4e13\u6709\u6a21\u578b\u7684\u67b6\u6784\u5f80\u5f80\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u800c\u5f00\u6e90\u6a21\u578b\u9700\u8981\u5bf9\u5176\u8bad\u7ec3\u7b56\u7565\u8fdb\u884c\u66f4\u8be6\u7ec6\u7684\u6d88\u878d\u7814\u7a76\u30022\uff09\u5f00\u6e90\u5de5\u4f5c\u4e2d\u7684\u9884\u8bad\u7ec3\u6570\u636e\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u6570\u636e\u96c6\u662f\u6839\u636e\u7ecf\u9a8c\u6dfb\u52a0\u7684\uff0c\u8fd9\u4f7f\u5f97\u8fc7\u7a0b\u53d8\u5f97\u7e41\u7410\u30023\uff09\u5fae\u8c03\u901a\u5e38\u4fa7\u91cd\u4e8e\u6dfb\u52a0\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u6536\u76ca\u9012\u51cf\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4ee5\u4e0b\u8d21\u732e\uff1a1\uff09\u6211\u4eec\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5f15\u5165\u4e86\u6709\u6548\u7684\u6539\u8fdb\uff0c\u5e76\u5bf9\u6bcf\u79cd\u6280\u672f\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6d88\u878d\u548c\u9a8c\u8bc1\u30022\uff09\u53d7\u8fd1\u671f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5de5\u4f5c\u7684\u542f\u53d1\uff0c\u6211\u4eec\u4f7f\u7528\u56f0\u60d1\u5ea6\u5bf9\u9884\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u8fc7\u6ee4\uff0c\u9009\u62e9\u56f0\u60d1\u5ea6\u6700\u4f4e\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u6211\u4eec\u80fd\u591f\u5728\u7cbe\u9009\u7684 1M \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u30023\uff09\u5728\u89c6\u89c9\u6307\u4ee4\u5fae\u8c03\u671f\u95f4\uff0c\u5f53\u6dfb\u52a0\u66f4\u591a\u6570\u636e\u96c6\u7684\u6536\u76ca\u5fae\u4e4e\u5176\u5fae\u65f6\uff0c\u6211\u4eec\u5bf9\u4e0d\u540c\u6570\u636e\u96c6\u4f7f\u7528\u4e86\u6a21\u578b\u878d\u5408\u3002\u8fd9\u4e9b\u521b\u65b0\u4ea7\u751f\u4e86\u4e00\u4e2a 9B \u53c2\u6570\u7684\u6a21\u578b\uff0c\u5176\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u6a21\u578b\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002\u6211\u4eec\u7684\u7b56\u7565\u9ad8\u6548\u4e14\u8f7b\u91cf\u7ea7\uff0c\u56e0\u6b64\u793e\u533a\u5f88\u5bb9\u6613\u91c7\u7528\u3002||\n", "2409.04796": "|**2024-09-07**|[Enhancing Outlier Knowledge for Few-Shot Out-of-Distribution Detection with Extensible Local Prompts](http://arxiv.org/abs/2409.04796)|null|\u5206\u5e03\u5916 (OOD) \u68c0\u6d4b\u65e8\u5728\u533a\u5206\u5df2\u77e5\u7c7b\u522b\u4e4b\u5916\u7684\u5f02\u5e38\u503c\uff0c\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5df2\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u51fa\u73b0\u6fc0\u53d1\u4e86\u4eba\u4eec\u5bf9\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u5fae\u8c03\u6765\u589e\u5f3a VLM \u7684 OOD \u68c0\u6d4b\u7684\u5174\u8da3\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4fa7\u91cd\u4e8e\u4f18\u5316\u5168\u5c40\u63d0\u793a\uff0c\u800c\u5ffd\u7565\u4e86\u5bf9\u5f02\u5e38\u503c\u7684\u5c40\u90e8\u4fe1\u606f\u7684\u7cbe\u7ec6\u5229\u7528\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u51bb\u7ed3\u5168\u5c40\u63d0\u793a\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ece\u7c97\u5230\u7cbe\u7684\u5fae\u8c03\u8303\u5f0f\uff0c\u4ee5\u5f3a\u8c03\u4f7f\u7528\u5c40\u90e8\u63d0\u793a\u8fdb\u884c\u533a\u57df\u589e\u5f3a\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u7ec4\u6210\u90e8\u5206\uff1a\u5168\u5c40\u63d0\u793a\u5f15\u5bfc\u7684\u8d1f\u589e\u5f3a\u548c\u5c40\u90e8\u63d0\u793a\u589e\u5f3a\u7684\u533a\u57df\u6b63\u5219\u5316\u3002\u524d\u8005\u5229\u7528\u51bb\u7ed3\u7684\u3001\u7c97\u7565\u7684\u5168\u5c40\u63d0\u793a\u4f5c\u4e3a\u6307\u5bfc\u7ebf\u7d22\u6765\u5408\u5e76\u8d1f\u589e\u5f3a\uff0c\u4ece\u800c\u5229\u7528\u5c40\u90e8\u5f02\u5e38\u503c\u77e5\u8bc6\u3002\u540e\u8005\u91c7\u7528\u53ef\u8bad\u7ec3\u7684\u5c40\u90e8\u63d0\u793a\u548c\u533a\u57df\u6b63\u5219\u5316\u6765\u6709\u6548\u5730\u6355\u83b7\u5c40\u90e8\u4fe1\u606f\uff0c\u4ece\u800c\u5e2e\u52a9\u8bc6\u522b\u5f02\u5e38\u503c\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u533a\u57df\u76f8\u5173\u6307\u6807\uff0c\u4ee5\u589e\u5f3a OOD \u68c0\u6d4b\u7684\u4e30\u5bcc\u6027\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u6211\u4eec\u7684\u65b9\u6cd5\u4ec5\u63a2\u7d22\u589e\u5f3a\u5c40\u90e8\u63d0\u793a\uff0c\u56e0\u6b64\u53ef\u4ee5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4e0e\u8bad\u7ec3\u597d\u7684\u5168\u5c40\u63d0\u793a\u65e0\u7f1d\u96c6\u6210\uff0c\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002\u7efc\u5408\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728 ImageNet-1k \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684 4 \u6b21\u6837\u672c\u5fae\u8c03\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5c06\u5e73\u5747 FPR95 \u964d\u4f4e\u4e86 5.17%\uff0c\u751a\u81f3\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u7684 16 \u6b21\u6837\u672c\u5fae\u8c03\u7ed3\u679c\u3002||\n", "2409.08202": "|**2024-09-12**|[What Makes a Maze Look Like a Maze?](http://arxiv.org/abs/2409.08202)|null|\u4eba\u7c7b\u89c6\u89c9\u7406\u89e3\u7684\u4e00\u4e2a\u72ec\u7279\u4e4b\u5904\u5728\u4e8e\u80fd\u591f\u7075\u6d3b\u5730\u89e3\u91ca\u62bd\u8c61\u6982\u5ff5\uff1a\u83b7\u53d6\u89e3\u91ca\u5176\u8c61\u5f81\u610f\u4e49\u7684\u63d0\u5347\u89c4\u5219\uff0c\u5c06\u5b83\u4eec\u5e94\u7528\u4e8e\u719f\u6089\u548c\u4e0d\u719f\u6089\u7684\u8bed\u5883\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u9884\u6d4b\u6216\u63a8\u7406\u3002\u867d\u7136\u73b0\u6210\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u64c5\u957f\u5bf9\u56fe\u50cf\u8fdb\u884c\u5b57\u9762\u89e3\u91ca\uff08\u4f8b\u5982\uff0c\u8bc6\u522b\u6811\u679d\u7b49\u7269\u4f53\u7c7b\u522b\uff09\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u96be\u4ee5\u7406\u89e3\u6b64\u7c7b\u89c6\u89c9\u62bd\u8c61\u6982\u5ff5\uff08\u4f8b\u5982\uff0c\u6811\u679d\u7684\u6392\u5217\u65b9\u5f0f\u5982\u4f55\u5f62\u6210\u8ff7\u5bab\u7684\u5899\u58c1\uff09\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6df1\u5ea6\u6a21\u5f0f\u57fa\u7840\uff08DSG\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u5229\u7528\u89c6\u89c9\u62bd\u8c61\u7684\u663e\u5f0f\u7ed3\u6784\u5316\u8868\u793a\u8fdb\u884c\u57fa\u7840\u5316\u548c\u63a8\u7406\u7684\u6846\u67b6\u3002DSG \u7684\u6838\u5fc3\u662f\u6a21\u5f0f\u2014\u2014\u62bd\u8c61\u6982\u5ff5\u7684\u4f9d\u8d56\u56fe\u63cf\u8ff0\uff0c\u5c06\u5b83\u4eec\u5206\u89e3\u6210\u66f4\u539f\u59cb\u7ea7\u522b\u7684\u7b26\u53f7\u3002DSG \u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u63d0\u53d6\u6a21\u5f0f\uff0c\u7136\u540e\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c06\u6a21\u5f0f\u7684\u5177\u4f53\u7ec4\u4ef6\u5230\u62bd\u8c61\u7ec4\u4ef6\u5206\u5c42\u5730\u57fa\u7840\u5316\u5230\u56fe\u50cf\u4e0a\u3002\u57fa\u7840\u5316\u7684\u6a21\u5f0f\u7528\u4e8e\u589e\u5f3a\u89c6\u89c9\u62bd\u8c61\u7406\u89e3\u3002\u6211\u4eec\u5728\u65b0\u7684\u89c6\u89c9\u62bd\u8c61\u6570\u636e\u96c6\u4e0a\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86 DSG \u548c\u4e0d\u540c\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u5404\u79cd\u73b0\u5b9e\u4e16\u754c\u4e2d\u62bd\u8c61\u6982\u5ff5\u7684\u56fe\u50cf\u4ee5\u53ca\u7531\u4eba\u7c7b\u6807\u8bb0\u7684\u76f8\u5e94\u95ee\u7b54\u5bf9\u3002\u6211\u4eec\u8868\u660e\uff0cDSG \u663e\u7740\u63d0\u9ad8\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u4e14\u662f\u671d\u7740\u4eba\u7c7b\u4e00\u81f4\u7684\u89c6\u89c9\u62bd\u8c61\u7406\u89e3\u8fc8\u51fa\u7684\u4e00\u6b65\u3002|\n", "2409.07825": "|**2024-09-13**|[A Comprehensive Survey on Deep Multimodal Learning with Missing Modality](http://arxiv.org/abs/2409.07825)|null|\u5728\u591a\u6a21\u6001\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u7531\u4e8e\u4f20\u611f\u5668\u9650\u5236\u3001\u6210\u672c\u9650\u5236\u3001\u9690\u79c1\u95ee\u9898\u3001\u6570\u636e\u4e22\u5931\u4ee5\u53ca\u65f6\u95f4\u548c\u7a7a\u95f4\u56e0\u7d20\uff0c\u6570\u636e\u6837\u672c\u53ef\u80fd\u4f1a\u7f3a\u5c11\u67d0\u4e9b\u6a21\u6001\uff0c\u4ece\u800c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002\u672c\u7efc\u8ff0\u6982\u8ff0\u4e86\u7f3a\u5931\u6a21\u6001\u7684\u591a\u6a21\u6001\u5b66\u4e60 (MLMM) \u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u3002\u5b83\u662f\u7b2c\u4e00\u4e2a\u6db5\u76d6\u5386\u53f2\u80cc\u666f\u548c MLMM \u4e0e\u6807\u51c6\u591a\u6a21\u6001\u5b66\u4e60\u8bbe\u7f6e\u4e4b\u95f4\u533a\u522b\u7684\u7efc\u5408\u6027\u7efc\u8ff0\uff0c\u7136\u540e\u8be6\u7ec6\u5206\u6790\u4e86\u5f53\u524d\u7684 MLMM \u65b9\u6cd5\u3001\u5e94\u7528\u548c\u6570\u636e\u96c6\uff0c\u6700\u540e\u8ba8\u8bba\u4e86\u8be5\u9886\u57df\u7684\u6311\u6218\u548c\u6f5c\u5728\u7684\u672a\u6765\u65b9\u5411\u3002|\n", "2409.07748": "|**2024-09-12**|[Top-down Activity Representation Learning for Video Question Answering](http://arxiv.org/abs/2409.07748)|null|\u4ece\u539f\u5b50\u52a8\u4f5c\uff08\u4f8b\u5982\uff0c\u62ff\u8d77\u4e00\u4e2a\u793c\u7269\uff0c\u79fb\u52a8\u5230\u6c99\u53d1\uff0c\u6253\u5f00\u793c\u7269\uff09\u5230\u4e0a\u4e0b\u6587\u4e8b\u4ef6\uff08\u4f8b\u5982\uff0c\u5e86\u795d\u5723\u8bde\u8282\uff09\u6355\u6349\u590d\u6742\u7684\u5206\u5c42\u4eba\u7c7b\u6d3b\u52a8\u5bf9\u4e8e\u5b9e\u73b0\u9ad8\u6027\u80fd\u89c6\u9891\u95ee\u7b54 (VideoQA) \u81f3\u5173\u91cd\u8981\u3002 \u6700\u8fd1\u7684\u5de5\u4f5c\u5df2\u7ecf\u6269\u5c55\u4e86\u591a\u6a21\u6001\u6a21\u578b\uff08\u4f8b\u5982\uff0cCLIP\uff0cLLaVA\uff09\u6765\u5904\u7406\u8fde\u7eed\u89c6\u9891\u5e8f\u5217\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002 \u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u65e0\u6cd5\u6355\u6349\u53ef\u4ee5\u5206\u89e3\u4e3a\u591a\u4e2a\u539f\u5b50\u52a8\u4f5c\u7684\u4e0a\u4e0b\u6587\u4e8b\u4ef6\uff0c\u8fd9\u4e9b\u52a8\u4f5c\u975e\u8fde\u7eed\u5730\u5206\u5e03\u5728\u76f8\u5bf9\u957f\u671f\u7684\u5e8f\u5217\u4e2d\u3002 \u5728\u672c\u6587\u4e2d\uff0c\u4e3a\u4e86\u5229\u7528 CLIP \u6a21\u578b\u7684\u7a7a\u95f4\u89c6\u89c9\u4e0a\u4e0b\u6587\u8868\u793a\u80fd\u529b\u6765\u83b7\u5f97\u89c6\u9891\u4e2d\u4e0a\u4e0b\u6587\u4e8b\u4ef6\u65b9\u9762\u7684\u975e\u8fde\u7eed\u89c6\u89c9\u8868\u793a\uff0c\u6211\u4eec\u5c06\u957f\u671f\u89c6\u9891\u5e8f\u5217\u8f6c\u6362\u4e3a\u7a7a\u95f4\u56fe\u50cf\u57df\uff0c\u5e76\u9488\u5bf9 VideoQA \u4efb\u52a1\u5fae\u8c03\u591a\u6a21\u6001\u6a21\u578b LLaVA\u3002 \u6211\u4eec\u7684\u65b9\u6cd5\u5728 STAR \u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728 NExTQA \u4efb\u52a1\u4e0a\uff0c\u83b7\u5f97\u4e86 78.4% \u7684\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5f97\u5206 2.8 \u4e2a\u767e\u5206\u70b9\u3002|\n", "2409.07703": "|**2024-09-12**|[DSBench: How Far Are Data Science Agents to Becoming Data Science Experts?](http://arxiv.org/abs/2409.07703)|**[link](https://github.com/liqiangjing/dsbench)**|\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5df2\u7ecf\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u8bed\u8a00/\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u5f15\u53d1\u4e86\u6784\u5efa\u9488\u5bf9\u7279\u5b9a\u5e94\u7528\uff08\u5982\u8d2d\u7269\u52a9\u624b\u6216AI\u8f6f\u4ef6\u5de5\u7a0b\u5e08\uff09\u7684\u4ee3\u7406\u7684\u6700\u65b0\u8d8b\u52bf\u3002\u6700\u8fd1\uff0c\u8bb8\u591a\u6570\u636e\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u88ab\u63d0\u51fa\uff0c\u4ee5\u7814\u7a76\u5176\u5728\u6570\u636e\u79d1\u5b66\u9886\u57df\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u6570\u636e\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u79d1\u5b66\u5e94\u7528\u76f8\u6bd4\u4ecd\u7136\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u4e3a\u5b83\u4eec\u7684\u8bbe\u7f6e\u8fc7\u4e8e\u7b80\u5316\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86 DSBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u5177\u6709\u73b0\u5b9e\u4efb\u52a1\u7684\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u5305\u62ec 466 \u4e2a\u6570\u636e\u5206\u6790\u4efb\u52a1\u548c 74 \u4e2a\u6570\u636e\u5efa\u6a21\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u6765\u81ea Eloquence \u548c Kaggle \u7ade\u8d5b\u3002DSBench \u901a\u8fc7\u5305\u542b\u957f\u4e0a\u4e0b\u6587\u3001\u591a\u6a21\u6001\u4efb\u52a1\u80cc\u666f\u3001\u5bf9\u5927\u578b\u6570\u636e\u6587\u4ef6\u548c\u591a\u8868\u7ed3\u6784\u8fdb\u884c\u63a8\u7406\u4ee5\u53ca\u6267\u884c\u7aef\u5230\u7aef\u6570\u636e\u5efa\u6a21\u4efb\u52a1\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u771f\u5b9e\u7684\u8bbe\u7f6e\u3002\u6211\u4eec\u5bf9\u6700\u5148\u8fdb\u7684 LLM\u3001LVLM \u548c\u4ee3\u7406\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5b83\u4eec\u96be\u4ee5\u5b8c\u6210\u5927\u591a\u6570\u4efb\u52a1\uff0c\u6700\u597d\u7684\u4ee3\u7406\u4ec5\u80fd\u89e3\u51b3 34.12% \u7684\u6570\u636e\u5206\u6790\u4efb\u52a1\uff0c\u5e76\u5b9e\u73b0\u4e86 34.74% \u7684\u76f8\u5bf9\u6027\u80fd\u5dee\u8ddd (RPG)\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u8fdb\u4e00\u6b65\u53d1\u5c55\u66f4\u5b9e\u7528\u3001\u66f4\u667a\u80fd\u3001\u66f4\u81ea\u4e3b\u7684\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u7684\u5fc5\u8981\u6027\u3002|\n", "2409.07683": "|**2024-09-12**|[Open-Vocabulary Remote Sensing Image Semantic Segmentation](http://arxiv.org/abs/2409.07683)|null|\u5f00\u653e\u8bcd\u6c47\u56fe\u50cf\u8bed\u4e49\u5206\u5272 (OVS) \u65e8\u5728\u5c06\u56fe\u50cf\u5206\u5272\u6210\u8de8\u5f00\u653e\u7c7b\u522b\u96c6\u7684\u8bed\u4e49\u533a\u57df\u3002\u73b0\u6709\u7684 OVS \u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u57fa\u7840\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5229\u7528\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u6765\u5904\u7406 OVS \u4efb\u52a1\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u81ea\u7136\u56fe\u50cf\u91cf\u8eab\u5b9a\u5236\uff0c\u96be\u4ee5\u5e94\u5bf9\u9065\u611f\u56fe\u50cf\u7684\u72ec\u7279\u7279\u5f81\uff0c\u4f8b\u5982\u5feb\u901f\u53d8\u5316\u7684\u65b9\u5411\u548c\u663e\u8457\u7684\u5c3a\u5ea6\u53d8\u5316\u3002\u8fd9\u4e9b\u6311\u6218\u4f7f\u5730\u7403\u89c6\u89c9\u4e2d\u7684 OVS \u4efb\u52a1\u53d8\u5f97\u590d\u6742\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u96be\u9898\uff0c\u6211\u4eec\u501f\u9274\u4e86\u72ec\u7279\u7684\u9065\u611f\u7279\u5f81\uff0c\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u4e13\u95e8\u4e3a\u9065\u611f\u56fe\u50cf\u8bbe\u8ba1\u7684 OVS \u6846\u67b6\u3002\u7279\u522b\u662f\uff0c\u4e3a\u4e86\u89e3\u51b3\u4e0d\u540c\u7684\u65b9\u5411\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65cb\u8f6c\u805a\u5408\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u751f\u6210\u65b9\u5411\u81ea\u9002\u5e94\u76f8\u4f3c\u5ea6\u56fe\u4f5c\u4e3a\u521d\u59cb\u8bed\u4e49\u56fe\u3002\u968f\u540e\uff0c\u8fd9\u4e9b\u56fe\u4f1a\u5728\u7a7a\u95f4\u548c\u7c7b\u522b\u7ea7\u522b\u8fdb\u884c\u7ec6\u5316\uff0c\u4ee5\u751f\u6210\u66f4\u51c6\u786e\u7684\u8bed\u4e49\u56fe\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u7ba1\u7406\u663e\u8457\u7684\u5c3a\u5ea6\u53d8\u5316\uff0c\u6211\u4eec\u5c06\u591a\u5c3a\u5ea6\u56fe\u50cf\u7279\u5f81\u96c6\u6210\u5230\u4e0a\u91c7\u6837\u8fc7\u7a0b\u4e2d\uff0c\u4ece\u800c\u5f97\u5230\u6700\u7ec8\u7684\u5c3a\u5ea6\u611f\u77e5\u8bed\u4e49\u63a9\u7801\u3002\u4e3a\u4e86\u63a8\u8fdb\u5730\u7403\u89c6\u89c9\u4e2d\u7684 OVS \u5e76\u9f13\u52b1\u53ef\u91cd\u590d\u7814\u7a76\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u7b2c\u4e00\u4e2a\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u7684\u5f00\u6e90 OVS \u57fa\u51c6\uff0c\u5305\u62ec\u56db\u4e2a\u516c\u5171\u9065\u611f\u6570\u636e\u96c6\u3002\u5728\u8fd9\u4e2a\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u96c6\u90fd\u53ef\u4ee5\u5728 https://github.com/caoql98/OVRS \u83b7\u53d6\u3002|\n", "2409.07353": "|**2024-09-11**|[Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks](http://arxiv.org/abs/2409.07353)|**[link](https://github.com/speedlab-git/robust-encoder-against-jailbreak-attack)**|\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6781\u5927\u5730\u63a8\u8fdb\u4e86\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u5c24\u5176\u662f\u8d8a\u72f1\u653b\u51fb\uff0c\u8fd9\u4e9b\u653b\u51fb\u4f1a\u7ed5\u8fc7\u5b89\u5168\u534f\u8bae\uff0c\u5bfc\u81f4\u6a21\u578b\u751f\u6210\u8bef\u5bfc\u6027\u6216\u6709\u5bb3\u7684\u54cd\u5e94\u3002\u8fd9\u79cd\u8106\u5f31\u6027\u6e90\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u56fa\u6709\u7684\u654f\u611f\u6027\u4ee5\u53ca\u89c6\u89c9\u6a21\u6001\u5f15\u5165\u7684\u6269\u5927\u653b\u51fb\u9762\u3002\u6211\u4eec\u63d0\u51fa\u4e86 Sim-CLIP+\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u9632\u5fa1\u673a\u5236\uff0c\u5b83\u5229\u7528 Siamese \u67b6\u6784\u901a\u8fc7\u5bf9\u6297\u6027\u5fae\u8c03 CLIP \u89c6\u89c9\u7f16\u7801\u5668\u3002\u8fd9\u79cd\u65b9\u6cd5\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u4e86\u6270\u52a8\u6837\u672c\u548c\u5e72\u51c0\u6837\u672c\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0c\u589e\u5f3a\u4e86\u5bf9\u5bf9\u6297\u6027\u64cd\u4f5c\u7684\u62b5\u6297\u529b\u3002Sim-CLIP+ \u63d0\u4f9b\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5141\u8bb8\u4f5c\u4e3a\u5f3a\u5927\u7684\u89c6\u89c9\u7f16\u7801\u5668\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7684 LVLM \u67b6\u6784\u4e2d\u3002\u4e0e\u4ee5\u524d\u7684\u9632\u5fa1\u63aa\u65bd\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u9700\u8981\u5bf9 LVLM \u8fdb\u884c\u7ed3\u6784\u4fee\u6539\uff0c\u5e76\u4e14\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\u3002Sim-CLIP+ \u8bc1\u660e\u4e86\u5176\u5bf9\u57fa\u4e8e\u68af\u5ea6\u7684\u5bf9\u6297\u6027\u653b\u51fb\u548c\u5404\u79cd\u8d8a\u72f1\u6280\u672f\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u9488\u5bf9\u4e09\u79cd\u4e0d\u540c\u7684\u8d8a\u72f1\u653b\u51fb\u7b56\u7565\u8bc4\u4f30\u4e86 Sim-CLIP+\uff0c\u5e76\u4f7f\u7528\u6807\u51c6\u4e0b\u6e38\u6570\u636e\u96c6\uff08\u5305\u62ec\u7528\u4e8e\u56fe\u50cf\u5b57\u5e55\u7684 COCO \u548c\u7528\u4e8e\u89c6\u89c9\u95ee\u7b54\u7684 OKVQA\uff09\u6267\u884c\u4e86\u5e72\u51c0\u8bc4\u4f30\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSim-CLIP+ \u5728\u4fdd\u6301\u9ad8\u6e05\u6d01\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u7740\u63d0\u9ad8\u4e86\u5bf9\u57fa\u4e8e\u68af\u5ea6\u7684\u5bf9\u6297\u6027\u653b\u51fb\u548c\u8d8a\u72f1\u6280\u672f\u7684\u9c81\u68d2\u6027\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u5f3a\u5927\u7684\u89c6\u89c9\u7f16\u7801\u5668\u53ef\u5728 https://github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git \u83b7\u53d6\u3002||\n", "2409.07267": "|**2024-09-11**|[MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D Features as Text Tokens for Autonomous Driving](http://arxiv.org/abs/2409.07267)|**[link](https://github.com/emzucas/minidrive)**|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u4f5c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u901a\u7528\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u901a\u8fc7\u95ee\u7b54\u4ea4\u4e92\u6267\u884c\u9884\u6d4b\u3001\u89c4\u5212\u548c\u611f\u77e5\u7b49\u5b50\u4efb\u52a1\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u89c6\u89c9\u7f16\u7801\u5668\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM)\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u96be\u4ee5\u90e8\u7f72\u5728\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u548c\u5b9e\u65f6\u5e94\u7528\u7a0b\u5e8f\u4e2d\u3002\u540c\u65f6\uff0c\u5927\u591a\u6570\u73b0\u6709 VLM \u7f3a\u4e4f\u5904\u7406\u591a\u56fe\u50cf\u7684\u80fd\u529b\uff0c\u96be\u4ee5\u9002\u5e94\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u591a\u6444\u50cf\u5934\u611f\u77e5\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a MiniDrive \u7684\u65b0\u578b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u7279\u5f81\u5de5\u7a0b\u6df7\u5408\u4e13\u5bb6 (FE-MoE) \u6a21\u5757\u548c\u52a8\u6001\u6307\u4ee4\u9002\u914d\u5668 (DI-Adapter)\u3002FE-MoE \u5728\u8f93\u5165\u8bed\u8a00\u6a21\u578b\u4e4b\u524d\uff0c\u5c06 2D \u7279\u5f81\u6709\u6548\u5730\u6620\u5c04\u5230\u89c6\u89c9\u6807\u8bb0\u5d4c\u5165\u4e2d\u3002DI-Adapter \u4f7f\u89c6\u89c9\u6807\u8bb0\u5d4c\u5165\u80fd\u591f\u968f\u6307\u4ee4\u6587\u672c\u5d4c\u5165\u52a8\u6001\u53d8\u5316\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u65b9\u6cd5\u4e2d\u540c\u4e00\u56fe\u50cf\u7684\u9759\u6001\u89c6\u89c9\u6807\u8bb0\u5d4c\u5165\u95ee\u9898\u3002\u4e0e\u4e4b\u524d\u7684\u5de5\u4f5c\u76f8\u6bd4\uff0cMiniDrive \u5728\u53c2\u6570\u5927\u5c0f\u3001\u6d6e\u70b9\u8fd0\u7b97\u548c\u54cd\u5e94\u6548\u7387\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6700\u5c0f\u7248\u672c\u4ec5\u5305\u542b 83M \u53c2\u6570\u3002||\n", "2409.07129": "|**2024-09-11**|[MVLLaVA: An Intelligent Agent for Unified and Flexible Novel View Synthesis](http://arxiv.org/abs/2409.07129)|null|\u672c\u6587\u4ecb\u7ecd\u4e86MVLLaVA\uff0c\u8fd9\u662f\u4e00\u79cd\u4e13\u4e3a\u65b0\u89c6\u89d2\u5408\u6210\u4efb\u52a1\u8bbe\u8ba1\u7684\u667a\u80fd\u4ee3\u7406\u3002MVLLaVA\u5c06\u591a\u4e2a\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b\u4e0e\u5927\u578b\u591a\u6a21\u6001\u6a21\u578bLLaVA\u76f8\u7ed3\u5408\uff0c\u4f7f\u5176\u80fd\u591f\u9ad8\u6548\u5730\u5904\u7406\u5404\u79cd\u4efb\u52a1\u3002MVLLaVA\u4ee3\u8868\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u7edf\u4e00\u7684\u5e73\u53f0\uff0c\u53ef\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u8f93\u5165\u7c7b\u578b\uff0c\u5305\u62ec\u5355\u4e2a\u56fe\u50cf\u3001\u63cf\u8ff0\u6027\u6807\u9898\u6216\u89c6\u89d2\u65b9\u4f4d\u89d2\u7684\u7279\u5b9a\u53d8\u5316\uff0c\u5e76\u4ee5\u8bed\u8a00\u6307\u4ee4\u6307\u5bfc\u89c6\u89d2\u751f\u6210\u3002\u6211\u4eec\u7cbe\u5fc3\u8bbe\u8ba1\u4e86\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u6307\u4ee4\u6a21\u677f\uff0c\u968f\u540e\u7528\u4e8e\u5fae\u8c03LLaVA\u3002\u56e0\u6b64\uff0cMVLLaVA\u83b7\u5f97\u4e86\u6839\u636e\u7528\u6237\u6307\u4ee4\u751f\u6210\u65b0\u89c6\u89d2\u56fe\u50cf\u7684\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u7075\u6d3b\u6027\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MVLLaVA\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5e94\u5bf9\u5404\u79cd\u65b0\u89c6\u89d2\u5408\u6210\u6311\u6218\u65f6\u7684\u5f3a\u5927\u6027\u80fd\u548c\u591a\u529f\u80fd\u6027\u3002||\n", "2409.06945": "|**2024-09-11**|[FSMDet: Vision-guided feature diffusion for fully sparse 3D detector](http://arxiv.org/abs/2409.06945)|null|\u8fd1\u5e74\u6765\uff0c\u5168\u7a00\u758f\u4e09\u7ef4\u76ee\u6807\u68c0\u6d4b\u5f15\u8d77\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6846\u67b6\u4e2d\u7279\u5f81\u7684\u7a00\u758f\u6027\u7531\u4e8e\u6269\u6563\u8fc7\u7a0b\u6709\u9650\uff0c\u5bf9\u5019\u9009\u6846\u7684\u751f\u6210\u63d0\u51fa\u4e86\u6311\u6218\u3002\u6b64\u5916\uff0c\u5bf9\u6548\u7387\u7684\u8ffd\u6c42\u5bfc\u81f4\u5bf9\u89c6\u89c9\u8f85\u52a9\u7684\u5168\u7a00\u758f\u6a21\u578b\u7684\u7814\u7a76\u5f88\u5c11\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86FSMDet\uff08\u5168\u7a00\u758f\u591a\u6a21\u6001\u68c0\u6d4b\uff09\uff0c\u5b83\u4f7f\u7528\u89c6\u89c9\u4fe1\u606f\u6765\u6307\u5bfc\u6fc0\u5149\u96f7\u8fbe\u7279\u5f81\u6269\u6563\u8fc7\u7a0b\uff0c\u540c\u65f6\u4ecd\u7136\u4fdd\u6301\u7ba1\u9053\u7684\u6548\u7387\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5927\u591a\u6570\u5168\u7a00\u758f\u5de5\u4f5c\u90fd\u96c6\u4e2d\u5728\u590d\u6742\u7684\u5b9a\u5236\u4e2d\u5fc3\u878d\u5408\u6269\u6563/\u56de\u5f52\u7b97\u5b50\u4e0a\u3002\u7136\u800c\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u5982\u679c\u6267\u884c\u4e86\u9002\u5f53\u7684\u76ee\u6807\u8865\u5168\uff0c\u5373\u4f7f\u662f\u6700\u7b80\u5355\u7684\u63d2\u503c\u7b97\u5b50\u4e5f\u80fd\u5f97\u5230\u4ee4\u4eba\u6ee1\u610f\u7684\u7ed3\u679c\u3002\u53d7\u6b64\u89c2\u5bdf\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5c06\u89c6\u89c9\u5f15\u5bfc\u7684\u6269\u6563\u8fc7\u7a0b\u5206\u4e3a\u4e24\u4e2a\u6a21\u5757\uff1a\u5f62\u72b6\u6062\u590d\u5c42\uff08SRLayer\uff09\u548c\u81ea\u6269\u6563\u5c42\uff08SDLayer\uff09\u3002\u524d\u8005\u4f7f\u7528RGB\u4fe1\u606f\u6765\u6062\u590d\u7269\u4f53\u53ef\u89c1\u90e8\u5206\u7684\u5f62\u72b6\uff0c\u540e\u8005\u4f7f\u7528\u89c6\u89c9\u5148\u9a8c\u5c06\u7279\u5f81\u8fdb\u4e00\u6b65\u6269\u6563\u5230\u4e2d\u5fc3\u533a\u57df\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6210\u529f\u5730\u63d0\u9ad8\u4e86\u4ee5\u5f80\u4ec5\u4f7f\u7528\u6fc0\u5149\u96f7\u8fbe\u7684\u5168\u7a00\u758f\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u8fbe\u5230\u4e86SOTA\u6027\u80fd\u3002\u540c\u65f6\uff0c\u7531\u4e8e\u91c7\u7528\u4e86\u7a00\u758f\u67b6\u6784\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6bd4\u4ee5\u5f80\u7684SOTA\u65b9\u6cd5\u6548\u7387\u6700\u9ad8\u53ef\u63d0\u9ad85\u500d\u3002||\n", "2409.06853": "|**2024-09-10**|[ExIQA: Explainable Image Quality Assessment Using Distortion Attributes](http://arxiv.org/abs/2409.06853)|null|\u76f2\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30 (BIQA) \u65e8\u5728\u5f00\u53d1\u65e0\u9700\u53c2\u8003\u56fe\u50cf\u5373\u53ef\u4f30\u8ba1\u56fe\u50cf\u8d28\u91cf\u5206\u6570\u7684\u65b9\u6cd5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ece\u5931\u771f\u8bc6\u522b\u89d2\u5ea6\u63a2\u8ba8 BIQA\uff0c\u4e3b\u8981\u76ee\u6807\u662f\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM)\uff08\u5982 CLIP\uff09\u9884\u6d4b\u5931\u771f\u7c7b\u578b\u548c\u5f3a\u5ea6\uff0c\u56e0\u4e3a\u5b83\u4eec\u5177\u6709\u5e7f\u6cdb\u7684\u77e5\u8bc6\u548c\u6cdb\u5316\u80fd\u529b\u3002\u57fa\u4e8e\u8fd9\u4e9b\u9884\u6d4b\u7684\u5931\u771f\uff0c\u6211\u4eec\u7136\u540e\u4f30\u8ba1\u56fe\u50cf\u7684\u8d28\u91cf\u5206\u6570\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c5e\u6027\u5b66\u4e60\u7684\u53ef\u89e3\u91ca\u5931\u771f\u8bc6\u522b\u65b9\u6cd5\u3002\u6211\u4eec\u6ca1\u6709\u4f7f\u7528\u5931\u771f\u540d\u79f0\u63d0\u793a VLM\uff0c\u800c\u662f\u4f7f\u7528\u5931\u771f\u7684\u5c5e\u6027\u6216\u5f71\u54cd\u63d0\u793a\u5b83\u4eec\uff0c\u5e76\u6c47\u603b\u8fd9\u4e9b\u4fe1\u606f\u4ee5\u63a8\u65ad\u5931\u771f\u5f3a\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4e3a\u6bcf\u5f20\u56fe\u50cf\u8003\u8651\u4e86\u591a\u79cd\u5931\u771f\uff0c\u4f7f\u6211\u4eec\u7684\u65b9\u6cd5\u66f4\u5177\u53ef\u6269\u5c55\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u751f\u6210\u4e86\u4e00\u4e2a\u5305\u542b 100,000 \u5f20\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u9ad8\u6548\u8bad\u7ec3\u3002\u6700\u540e\uff0c\u68c0\u7d22\u5c5e\u6027\u6982\u7387\u5e76\u5c06\u5176\u8f93\u5165\u56de\u5f52\u5668\u4ee5\u9884\u6d4b\u56fe\u50cf\u8d28\u91cf\u5206\u6570\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u9664\u4e86\u5177\u6709\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u5916\uff0c\u8fd8\u5728\u591a\u4e2a\u6570\u636e\u96c6\u7684 PLCC \u548c SRCC \u6307\u6807\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb (SOTA) \u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u96f6\u6837\u672c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u3002||\n", "2409.08885": "|**2024-09-13**|[Interactive Masked Image Modeling for Multimodal Object Detection in Remote Sensing](http://arxiv.org/abs/2409.08885)|null|\u9065\u611f\u5f71\u50cf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u5728\u5730\u7403\u89c2\u6d4b\u7684\u5404\u79cd\u5e94\u7528\u4e2d\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u4e0e\u81ea\u7136\u573a\u666f\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u4e0d\u540c\uff0c\u8fd9\u9879\u4efb\u52a1\u7279\u522b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5728\u4e0d\u540c\u7684\u5730\u5f62\u4e2d\u5b58\u5728\u5927\u91cf\u7684\u5c0f\u578b\u4e14\u901a\u5e38\u96be\u4ee5\u5bdf\u89c9\u7684\u76ee\u6807\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u53ef\u4ee5\u4f7f\u7528\u591a\u6a21\u6001\u5b66\u4e60\u6765\u6574\u5408\u6765\u81ea\u4e0d\u540c\u6570\u636e\u6a21\u6001\u7684\u7279\u5f81\uff0c\u4ece\u800c\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002\u7136\u800c\uff0c\u591a\u6a21\u6001\u5b66\u4e60\u7684\u6027\u80fd\u5f80\u5f80\u53d7\u5230\u6807\u8bb0\u6570\u636e\u96c6\u5927\u5c0f\u7684\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528\u63a9\u853d\u56fe\u50cf\u5efa\u6a21\uff08MIM\uff09\u4f5c\u4e3a\u4e00\u79cd\u9884\u8bad\u7ec3\u6280\u672f\uff0c\u5229\u7528\u65e0\u6807\u8bb0\u6570\u636e\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6765\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684MIM\u65b9\u6cd5\uff08\u5982MAE\uff09\u4f7f\u7528\u6ca1\u6709\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u63a9\u853d\u6807\u8bb0\uff0c\u7531\u4e8e\u7f3a\u4e4f\u4e0e\u56fe\u50cf\u5176\u4ed6\u90e8\u5206\u7684\u4ea4\u4e92\uff0c\u96be\u4ee5\u6355\u6349\u5230\u7ec6\u7c92\u5ea6\u7684\u7ec6\u8282\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ea4\u4e92\u5f0fMIM\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u6807\u8bb0\u4e4b\u95f4\u5efa\u7acb\u4ea4\u4e92\uff0c\u8fd9\u5bf9\u4e8e\u9065\u611f\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u7279\u522b\u6709\u5229\u3002\u5927\u91cf\u7684\u6d88\u878d\u7814\u7a76\u548c\u8bc4\u4f30\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002|\n", "2409.08790": "|**2024-09-13**|[A Multimodal Approach for Fluid Overload Prediction: Integrating Lung Ultrasound and Clinical Data](http://arxiv.org/abs/2409.08790)|null|\u7ef4\u6301\u900f\u6790\u60a3\u8005\u7684\u4f53\u6db2\u5e73\u8861\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u7ba1\u7406\u4e0d\u5f53\u4f1a\u5bfc\u81f4\u4e25\u91cd\u5e76\u53d1\u75c7\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u80ba\u90e8\u8d85\u58f0\u56fe\u50cf\u7684\u89c6\u89c9\u7279\u5f81\u548c\u4e34\u5e8a\u6570\u636e\uff0c\u4ee5\u589e\u5f3a\u5bf9\u4f53\u5185\u591a\u4f59\u6db2\u4f53\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u6211\u4eec\u7684\u6846\u67b6\u91c7\u7528\u72ec\u7acb\u7684\u7f16\u7801\u5668\u6765\u63d0\u53d6\u6bcf\u79cd\u6a21\u6001\u7684\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u8de8\u57df\u6ce8\u610f\u529b\u673a\u5236\u5c06\u5b83\u4eec\u7ec4\u5408\u8d77\u6765\uff0c\u4ee5\u6355\u83b7\u4e92\u8865\u4fe1\u606f\u3002\u901a\u8fc7\u5c06\u9884\u6d4b\u6784\u5efa\u4e3a\u5206\u7c7b\u4efb\u52a1\uff0c\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86\u6bd4\u56de\u5f52\u6a21\u578b\u66f4\u597d\u7684\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u591a\u6a21\u6001\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u5355\u6a21\u6001\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5148\u8003\u8651\u8868\u683c\u6570\u636e\u65f6\u3002\u4f2a\u6837\u672c\u751f\u6210\u8fdb\u4e00\u6b65\u6709\u52a9\u4e8e\u7f13\u89e3\u5206\u7c7b\u95ee\u9898\u4e2d\u7684\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86 88.31% \u7684\u6700\u9ad8\u51c6\u786e\u7387\u3002\u8fd9\u9879\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u5bf9\u900f\u6790\u60a3\u8005\u6db2\u4f53\u8d85\u8d1f\u8377\u7ba1\u7406\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6539\u5584\u4e34\u5e8a\u7ed3\u679c\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002|\n", "2409.08582": "|**2024-09-13**|[ChangeChat: An Interactive Model for Remote Sensing Change Analysis via Multimodal Instruction Tuning](http://arxiv.org/abs/2409.08582)|null|\u9065\u611f (RS) \u53d8\u5316\u5206\u6790\u901a\u8fc7\u68c0\u6d4b\u56fe\u50cf\u968f\u65f6\u95f4\u7684\u53d8\u5316\u6765\u76d1\u6d4b\u5730\u7403\u52a8\u6001\u8fc7\u7a0b\uff0c\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u53d8\u70b9\u68c0\u6d4b\u64c5\u957f\u8bc6\u522b\u50cf\u7d20\u7ea7\u7684\u53d8\u5316\uff0c\u4f46\u7f3a\u4e4f\u5c06\u8fd9\u4e9b\u53d8\u5316\u7f6e\u4e8e\u80cc\u666f\u4e2d\u7684\u80fd\u529b\u3002\u867d\u7136\u6700\u8fd1\u5728\u53d8\u5316\u63cf\u8ff0\u65b9\u9762\u7684\u8fdb\u5c55\u63d0\u4f9b\u4e86\u5bf9\u53d8\u5316\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u4f46\u5b83\u4eec\u4e0d\u652f\u6301\u4ea4\u4e92\u5f0f\u7684\u3001\u7528\u6237\u7279\u5b9a\u7684\u67e5\u8be2\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86 ChangeChat\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4e13\u4e3a RS \u53d8\u5316\u5206\u6790\u8bbe\u8ba1\u7684\u53cc\u65f6\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM)\u3002ChangeChat \u5229\u7528\u591a\u6a21\u6001\u6307\u4ee4\u5fae\u8c03\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u67e5\u8be2\uff0c\u4f8b\u5982\u53d8\u5316\u63cf\u8ff0\u3001\u7279\u5b9a\u7c7b\u522b\u7684\u91cf\u5316\u548c\u53d8\u5316\u5b9a\u4f4d\u3002\u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\uff0c\u6211\u4eec\u5f00\u53d1\u4e86 ChangeChat-87k \u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u662f\u4f7f\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u548c GPT \u8f85\u52a9\u6280\u672f\u76f8\u7ed3\u5408\u751f\u6210\u7684\u3002\u5b9e\u9a8c\u8868\u660e\uff0cChangeChat \u4e3a RS \u53d8\u5316\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u3001\u4ea4\u4e92\u5f0f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8fbe\u5230\u751a\u81f3\u4f18\u4e8e\u6700\u5148\u8fdb (SOTA) \u65b9\u6cd5\uff0c\u5e76\u663e\u7740\u8d85\u8fc7\u4e86\u6700\u65b0\u7684\u901a\u7528\u6a21\u578b GPT-4\u3002\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6743\u91cd\u53ef\u5728 https://github.com/hanlinwu/ChangeChat \u83b7\u53d6\u3002|\n", "2409.08468": "|**2024-09-13**|[Generalization Boosted Adapter for Open-Vocabulary Segmentation](http://arxiv.org/abs/2409.08468)|null|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5df2\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u8bc6\u522b\u80fd\u529b\uff0c\u8fd9\u4fc3\u4f7f\u5b83\u4eec\u88ab\u5e94\u7528\u4e8e\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\uff0c\u4f8b\u5982\u5206\u5272\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7f3a\u4e4f\u50cf\u7d20\u7ea7\u7c92\u5ea6\u4ee5\u53ca\u53ef\u7528\u4e8e\u5fae\u8c03\u7684\u6570\u636e\u6709\u9650\uff0c\u76f4\u63a5\u5c06 VLM \u5e94\u7528\u4e8e\u6b64\u7c7b\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u62df\u5408\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6cdb\u5316\u589e\u5f3a\u9002\u914d\u5668 (GBA)\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u9002\u914d\u5668\u7b56\u7565\uff0c\u53ef\u4ee5\u589e\u5f3a VLM \u5bf9\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002GBA \u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u98ce\u683c\u591a\u6837\u5316\u9002\u914d\u5668 (SDA)\uff0c\u5b83\u5c06\u7279\u5f81\u89e3\u8026\u4e3a\u5e45\u5ea6\u548c\u76f8\u4f4d\u5206\u91cf\uff0c\u4ec5\u5bf9\u5e45\u5ea6\u8fdb\u884c\u64cd\u4f5c\u4ee5\u4e30\u5bcc\u7279\u5f81\u7a7a\u95f4\u8868\u793a\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\uff1b(2) \u76f8\u5173\u6027\u7ea6\u675f\u9002\u914d\u5668 (CCA)\uff0c\u5b83\u91c7\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5728\u6587\u672c\u7c7b\u522b\u548c\u76ee\u6807\u533a\u57df\u4e4b\u95f4\u5efa\u7acb\u66f4\u7d27\u5bc6\u7684\u8bed\u4e49\u5173\u8054\uff0c\u6291\u5236\u4e0d\u76f8\u5173\u7684\u4f4e\u9891\u201c\u566a\u58f0\u201d\u4fe1\u606f\u5e76\u907f\u514d\u9519\u8bef\u5173\u8054\u3002\u901a\u8fc7\u6d45\u5c42 SDA \u548c\u6df1\u5c42 CCA \u7684\u534f\u540c\u6548\u5e94\uff0cGBA \u6709\u6548\u5730\u7f13\u89e3\u4e86\u8fc7\u5ea6\u62df\u5408\u95ee\u9898\uff0c\u5e76\u589e\u5f3a\u4e86\u7279\u5f81\u8868\u793a\u7684\u8bed\u4e49\u76f8\u5173\u6027\u3002\u4f5c\u4e3a\u4e00\u4e2a\u7b80\u5355\u3001\u9ad8\u6548\u3001\u5373\u63d2\u5373\u7528\u7684\u7ec4\u4ef6\uff0cGBA \u53ef\u4ee5\u7075\u6d3b\u5730\u96c6\u6210\u5230\u5404\u79cd\u57fa\u4e8e CLIP \u7684\u65b9\u6cd5\u4e2d\uff0c\u5c55\u73b0\u51fa\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|\n", "2409.08381": "|**2024-09-12**|[Rethinking Prompting Strategies for Multi-Label Recognition with Partial Annotations](http://arxiv.org/abs/2409.08381)|null|\u50cf CLIP \u8fd9\u6837\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5df2\u88ab\u5e94\u7528\u4e8e\u90e8\u5206\u6807\u6ce8\u7684\u591a\u6807\u7b7e\u8bc6\u522b (MLR)\uff0c\u5176\u65b9\u6cd5\u662f\u5229\u7528\u63d0\u793a\u5b66\u4e60\uff0c\u4e3a\u6bcf\u4e2a\u7c7b\u522b\u5b66\u4e60\u6b63\u8d1f\u63d0\u793a\uff0c\u4ee5\u4fbf\u5c06\u5b83\u4eec\u7684\u5d4c\u5165\u4e0e\u5171\u4eab\u89c6\u89c9\u6587\u672c\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u7c7b\u522b\u5b58\u5728\u6216\u4e0d\u5b58\u5728\u76f8\u5173\u8054\u3002\u867d\u7136\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u4f9d\u8d56 VLM \u5148\u9a8c\u4fe1\u606f\u63d0\u9ad8\u4e86 MLR \u6027\u80fd\uff0c\u4f46\u6211\u4eec\u5047\u8bbe\u5b66\u4e60\u8d1f\u9762\u63d0\u793a\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u56e0\u4e3a\u7528\u4e8e\u8bad\u7ec3 VLM \u7684\u6570\u636e\u96c6\u7f3a\u4e4f\u660e\u786e\u5173\u6ce8\u7c7b\u522b\u7f3a\u5931\u7684\u56fe\u50cf-\u6807\u9898\u5bf9\u3002\u4e3a\u4e86\u5206\u6790\u6b63\u8d1f\u63d0\u793a\u5b66\u4e60\u5bf9 MLR \u7684\u5f71\u54cd\uff0c\u6211\u4eec\u5f15\u5165\u4e86 PositiveCoOp \u548c NegativeCoOp\uff0c\u5176\u4e2d\u53ea\u6709\u4e00\u4e2a\u63d0\u793a\u662f\u5728 VLM \u6307\u5bfc\u4e0b\u5b66\u4e60\u7684\uff0c\u800c\u53e6\u4e00\u4e2a\u63d0\u793a\u5219\u88ab\u76f4\u63a5\u5728\u5171\u4eab\u7279\u5f81\u7a7a\u95f4\u4e2d\u5b66\u4e60\u7684\u5d4c\u5165\u5411\u91cf\u6240\u53d6\u4ee3\uff0c\u800c\u4e0d\u4f9d\u8d56\u4e8e\u6587\u672c\u7f16\u7801\u5668\u3002\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u8d1f\u9762\u63d0\u793a\u4f1a\u964d\u4f4e MLR \u6027\u80fd\uff0c\u5e76\u4e14\u4ec5\u5b66\u4e60\u6b63\u9762\u63d0\u793a\u5e76\u7ed3\u5408\u5b66\u4e60\u5230\u7684\u8d1f\u9762\u5d4c\u5165\uff08PositiveCoOp\uff09\u4f18\u4e8e\u53cc\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u91cf\u5316\u4e86\u63d0\u793a\u5b66\u4e60\u76f8\u5bf9\u4e8e\u4ec5\u4f7f\u7528\u89c6\u89c9\u7279\u5f81\u7684\u7b80\u5355\u57fa\u7ebf\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u89c2\u5bdf\u5230\u5f53\u7f3a\u5931\u6807\u7b7e\u7684\u6bd4\u4f8b\u8f83\u4f4e\u65f6\uff0c\u57fa\u7ebf\u8868\u73b0\u51fa\u4e0e\u53cc\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5 (DualCoOp) \u76f8\u5f53\u7684\u5f3a\u52b2\u6027\u80fd\uff0c\u540c\u65f6\u6240\u9700\u7684\u8bad\u7ec3\u8ba1\u7b97\u91cf\u51cf\u5c11\u4e00\u534a\uff0c\u53c2\u6570\u6570\u91cf\u51cf\u5c11 16 \u500d\u3002|\n", "2409.11402": "|**2024-09-17**|[NVLM: Open Frontier-Class Multimodal LLMs](http://arxiv.org/abs/2409.11402)|null|\u6211\u4eec\u63a8\u51fa\u4e86 NVLM 1.0\uff0c\u8fd9\u662f\u4e00\u7cfb\u5217\u524d\u6cbf\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM)\uff0c\u5b83\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u53ef\u4e0e\u9886\u5148\u7684\u4e13\u6709\u6a21\u578b\uff08\u4f8b\u5982 GPT-4o\uff09\u548c\u5f00\u653e\u8bbf\u95ee\u6a21\u578b\uff08\u4f8b\u5982 Llama 3-V 405B \u548c InternVL 2\uff09\u76f8\u5ab2\u7f8e\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cNVLM 1.0 \u5728\u591a\u6a21\u6001\u8bad\u7ec3\u540e\uff0c\u5176\u7eaf\u6587\u672c\u6027\u80fd\u4f18\u4e8e\u5176 LLM \u9aa8\u5e72\u7f51\u7edc\u3002\u5728\u6a21\u578b\u8bbe\u8ba1\u65b9\u9762\uff0c\u6211\u4eec\u5bf9\u4ec5\u89e3\u7801\u5668\u591a\u6a21\u6001 LLM\uff08\u4f8b\u5982 LLaVA\uff09\u548c\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u6a21\u578b\uff08\u4f8b\u5982 Flamingo\uff09\u8fdb\u884c\u4e86\u5168\u9762\u6bd4\u8f83\u3002\u57fa\u4e8e\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\uff0c\u53ef\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4e3a\u57fa\u4e8e\u56fe\u5757\u7684\u52a8\u6001\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5f15\u5165\u4e86 1-D \u56fe\u5757\u6807\u8bb0\u8bbe\u8ba1\uff0c\u8fd9\u663e\u7740\u63d0\u9ad8\u4e86\u591a\u6a21\u6001\u63a8\u7406\u548c OCR \u76f8\u5173\u4efb\u52a1\u7684\u6027\u80fd\u3002\u5173\u4e8e\u8bad\u7ec3\u6570\u636e\uff0c\u6211\u4eec\u7cbe\u5fc3\u7b56\u5212\u5e76\u63d0\u4f9b\u6709\u5173\u6211\u4eec\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\u6570\u636e\u96c6\u7684\u8be6\u7ec6\u4fe1\u606f\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u6570\u636e\u96c6\u8d28\u91cf\u548c\u4efb\u52a1\u591a\u6837\u6027\u4e5f\u6bd4\u89c4\u6a21\u66f4\u91cd\u8981\uff0c\u8fd9\u5728\u6240\u6709\u67b6\u6784\u4e2d\u90fd\u662f\u5982\u6b64\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u4e3a NVLM-1.0 \u6a21\u578b\u5f00\u53d1\u4e86\u751f\u4ea7\u7ea7\u591a\u6a21\u6001\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u6539\u8fdb\u4e0e\u5176 LLM \u9aa8\u5e72\u7f51\u7edc\u76f8\u6bd4\u7684\u7eaf\u6587\u672c\u6027\u80fd\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u5c06\u9ad8\u8d28\u91cf\u7684\u7eaf\u6587\u672c\u6570\u636e\u96c6\u4e0e\u5927\u91cf\u7684\u591a\u6a21\u6001\u6570\u5b66\u548c\u63a8\u7406\u6570\u636e\u4e00\u8d77\u7cbe\u5fc3\u5236\u4f5c\u5e76\u96c6\u6210\u5230\u591a\u6a21\u6001\u8bad\u7ec3\u4e2d\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u8de8\u6a21\u6001\u7684\u6570\u5b66\u548c\u7f16\u7801\u80fd\u529b\u3002\u4e3a\u4e86\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u53d1\uff0c\u6211\u4eec\u6b63\u5728\u53d1\u5e03\u6a21\u578b\u6743\u91cd\uff0c\u5e76\u5c06\u5f00\u6e90\u4ee3\u7801\u4f9b\u793e\u533a\u4f7f\u7528\uff1ahttps://nvlm-project.github.io/\u3002|\n", "2409.11007": "|**2024-09-17**|[CAST: Cross-modal Alignment Similarity Test for Vision Language Models](http://arxiv.org/abs/2409.11007)|**[link](https://github.com/gautierdag/cast)**|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u901a\u5e38\u901a\u8fc7\u89c6\u89c9\u95ee\u7b54 (VQA) \u4efb\u52a1\u8fdb\u884c\u8bc4\u4f30\uff0c\u8fd9\u4e9b\u4efb\u52a1\u8bc4\u4f30\u6a21\u578b\u5bf9\u573a\u666f\u7684\u7406\u89e3\u3002\u826f\u597d\u7684 VQA \u6027\u80fd\u88ab\u89c6\u4e3a\u8be5\u6a21\u578b\u5728\u9700\u8981\u89c6\u89c9\u548c\u8bed\u8a00\u8f93\u5165\u7684\u66f4\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u7684\u8bc1\u636e\u3002\u7136\u800c\uff0c\u573a\u666f\u611f\u77e5 VQA \u5e76\u4e0d\u80fd\u5b8c\u5168\u6355\u6349\u8f93\u5165\u504f\u5dee\u6216\u8bc4\u4f30\u7531\u6a21\u6001\u4e4b\u95f4\u9519\u4f4d\u5f15\u8d77\u7684\u5e7b\u89c9\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u5bf9\u9f50\u76f8\u4f3c\u6027\u6d4b\u8bd5 (CAST) \u6765\u63a2\u6d4b VLM \u5728\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7684\u81ea\u6d3d\u6027\u3002\u8be5\u6d4b\u8bd5\u5305\u62ec\u8981\u6c42\u6a21\u578b\u4ec5\u901a\u8fc7\u6587\u672c\u3001\u4ec5\u901a\u8fc7\u56fe\u50cf\u6216\u540c\u65f6\u901a\u8fc7\u6587\u672c\u548c\u56fe\u50cf\u6765\u8bc6\u522b\u4e24\u4e2a\u573a\u666f\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u7136\u540e\u8bc4\u4f30\u5b83\u4eec\u751f\u6210\u7684\u76f8\u4f3c\u6027\u7684\u771f\u5b9e\u6027\u3002\u7531\u4e8e\u6ca1\u6709\u53ef\u4f9b\u6bd4\u8f83\u7684\u57fa\u672c\u4e8b\u5b9e\uff0c\u56e0\u6b64\u8be5\u8bc4\u4f30\u5e76\u4e0d\u5173\u6ce8\u5ba2\u89c2\u51c6\u786e\u6027\uff0c\u800c\u662f\u5173\u6ce8 VLM \u7684\u8f93\u51fa\u662f\u5426\u5185\u90e8\u4e00\u81f4\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u867d\u7136\u5e76\u975e\u6240\u6709\u81ea\u6d3d\u6a21\u578b\u90fd\u5177\u6709\u80fd\u529b\u6216\u51c6\u786e\u6027\uff0c\u4f46\u6240\u6709\u6709\u80fd\u529b\u7684 VLM \u90fd\u5fc5\u987b\u662f\u81ea\u6d3d\u7684\u3002|\n", "2409.10921": "|**2024-09-17**|[KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph](http://arxiv.org/abs/2409.10921)|**[link](https://github.com/yanbei-jiang/artwork-interpretation)**|\u63a2\u7d22\u7f8e\u672f\u7ed8\u753b\u6240\u4f20\u8fbe\u7684\u53d9\u4e8b\u662f\u56fe\u50cf\u63cf\u8ff0\u9886\u57df\u7684\u4e00\u9879\u6311\u6218\uff0c\u5176\u76ee\u6807\u662f\u751f\u6210\u4e0d\u4ec5\u51c6\u786e\u8868\u73b0\u89c6\u89c9\u5185\u5bb9\uff0c\u8fd8\u80fd\u6df1\u5165\u89e3\u8bfb\u827a\u672f\u54c1\u542b\u4e49\u7684\u63cf\u8ff0\u3002\u7531\u4e8e\u827a\u672f\u54c1\u7684\u591a\u79cd\u89e3\u8bfb\u65b9\u5f0f\u4ee5\u53ca\u4e0d\u540c\u827a\u672f\u6d41\u6d3e\u548c\u98ce\u683c\u7684\u591a\u6837\u5316\u5ba1\u7f8e\u539f\u5219\uff0c\u8fd9\u9879\u4efb\u52a1\u5bf9\u4e8e\u827a\u672f\u56fe\u50cf\u6765\u8bf4\u5c24\u5176\u590d\u6742\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86KALE\uff08\u77e5\u8bc6\u589e\u5f3a\u7684\u89c6\u89c9\u8bed\u8a00\u827a\u672f\u54c1\u9610\u91ca\u6a21\u578b\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u8fc7\u6574\u5408\u827a\u672f\u54c1\u5143\u6570\u636e\u4f5c\u4e3a\u989d\u5916\u77e5\u8bc6\u6765\u589e\u5f3a\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\u3002KALE\u4ee5\u4e24\u79cd\u65b9\u5f0f\u878d\u5165\u5143\u6570\u636e\uff1a\u9996\u5148\u662f\u4f5c\u4e3a\u76f4\u63a5\u6587\u672c\u8f93\u5165\uff0c\u5176\u6b21\u662f\u901a\u8fc7\u591a\u6a21\u6001\u5f02\u6784\u77e5\u8bc6\u56fe\u8c31\u3002\u4e3a\u4e86\u4f18\u5316\u56fe\u8868\u793a\u7684\u5b66\u4e60\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u635f\u5931\uff0c\u5b83\u53ef\u4ee5\u6700\u5927\u5316\u56fe\u50cf\u4e0e\u5176\u5bf9\u5e94\u5143\u6570\u636e\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u591a\u4e2a\u827a\u672f\u54c1\u6570\u636e\u96c6\u4e0a\uff0cKALE\u76f8\u8f83\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u5de5\u4f5c\u53d6\u5f97\u4e86\u5f3a\u5927\u7684\u6027\u80fd\uff08\u7279\u522b\u662f\u5728\u4f7f\u7528CIDEr\u6307\u6807\u8bc4\u4f30\u65f6\uff09\u3002\u9879\u76ee\u7684\u6e90\u4ee3\u7801\u53ef\u5728https://github.com/Yanbei-Jiang/Artwork-Interpretation\u83b7\u53d6\u3002|\n", "2409.10488": "|**2024-09-16**|[Do Pre-trained Vision-Language Models Encode Object States?](http://arxiv.org/abs/2409.10488)|null|\u4e3a\u4e86\u8ba9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7406\u89e3\u7269\u7406\u4e16\u754c\uff08\u4f8b\u5982\u56e0\u679c\u5173\u7cfb\uff09\uff0c\u9996\u5148\u9700\u8981\u6355\u6349\u89c6\u89c9\u4e16\u754c\u7684\u65f6\u95f4\u52a8\u6001\uff0c\u4f8b\u5982\u7269\u4f53\u7269\u7406\u72b6\u6001\u5982\u4f55\u968f\u65f6\u95f4\u6f14\u53d8\uff08\u4f8b\u5982\uff0c\u4e00\u4e2a\u5b8c\u6574\u7684\u82f9\u679c\u53d8\u6210\u4e00\u4e2a\u5207\u7247\u7684\u82f9\u679c\uff09\u3002\u6211\u4eec\u7684\u8bba\u6587\u65e8\u5728\u7814\u7a76\u5728\u7f51\u7edc\u89c4\u6a21\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684 VLM \u662f\u5426\u5b66\u4f1a\u4e86\u7f16\u7801\u5bf9\u8c61\u72b6\u6001\uff0c\u800c\u8fd9\u4e9b\u72b6\u6001\u53ef\u4ee5\u901a\u8fc7\u96f6\u6837\u672c\u6587\u672c\u63d0\u793a\u63d0\u53d6\u3002\u6211\u4eec\u6574\u7406\u4e86\u4e00\u4e2a\u5bf9\u8c61\u72b6\u6001\u8bc6\u522b\u6570\u636e\u96c6 ChangeIt-Frames\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e5d\u4e2a\u5f00\u6e90 VLM\uff0c\u5305\u62ec\u4f7f\u7528\u5bf9\u6bd4\u548c\u751f\u6210\u76ee\u6807\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u867d\u7136\u8fd9\u4e9b\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u53ef\u9760\u5730\u6267\u884c\u5bf9\u8c61\u8bc6\u522b\uff0c\u4f46\u5b83\u4eec\u59cb\u7ec8\u65e0\u6cd5\u51c6\u786e\u533a\u5206\u5bf9\u8c61\u7684\u7269\u7406\u72b6\u6001\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u6211\u4eec\u786e\u5b9a\u4e86 VLM \u66f4\u597d\u5730\u7f16\u7801\u5bf9\u8c61\u72b6\u6001\u9700\u8981\u6539\u8fdb\u7684\u4e09\u4e2a\u65b9\u9762\uff0c\u5373\u5bf9\u8c61\u5b9a\u4f4d\u7684\u8d28\u91cf\u3001\u5c06\u6982\u5ff5\u7ed1\u5b9a\u5230\u5bf9\u8c61\u7684\u67b6\u6784\uff0c\u4ee5\u53ca\u5b66\u4e60\u5bf9\u8c61\u72b6\u6001\u7684\u5224\u522b\u6027\u89c6\u89c9\u548c\u8bed\u8a00\u7f16\u7801\u5668\u7684\u76ee\u6807\u3002\u6570\u636e\u548c\u4ee3\u7801\u5df2\u53d1\u5e03\u3002|\n", "2409.10441": "|**2024-09-16**|[CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera](http://arxiv.org/abs/2409.10441)|null|\u76f8\u673a\u5230\u673a\u5668\u4eba\u7684\u6807\u5b9a\u5bf9\u4e8e\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u4eba\u63a7\u5236\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e14\u9700\u8981\u52aa\u529b\u4f7f\u5176\u7cbe\u786e\u3002\u6700\u8fd1\u5728\u65e0\u6807\u8bb0\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\u65b9\u9762\u7684\u8fdb\u6b65\u6d88\u9664\u4e86\u5bf9\u76f8\u673a\u5230\u673a\u5668\u4eba\u6807\u5b9a\u8fdb\u884c\u8017\u65f6\u7684\u7269\u7406\u8bbe\u7f6e\u7684\u9700\u8981\u3002\u867d\u7136\u73b0\u6709\u7684\u65e0\u6807\u8bb0\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\u5df2\u7ecf\u8bc1\u660e\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7cbe\u5ea6\uff0c\u800c\u4e0d\u9700\u8981\u7e41\u7410\u7684\u8bbe\u7f6e\uff0c\u4f46\u5b83\u4eec\u4f9d\u8d56\u4e8e\u6240\u6709\u673a\u5668\u4eba\u5173\u8282\u90fd\u5728\u76f8\u673a\u89c6\u91ce\u5185\u53ef\u89c1\u7684\u5047\u8bbe\u3002\u7136\u800c\uff0c\u5728\u5b9e\u8df5\u4e2d\uff0c\u673a\u5668\u4eba\u901a\u5e38\u4f1a\u79fb\u5165\u548c\u79fb\u51fa\u89c6\u91ce\uff0c\u5e76\u4e14\u7531\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u9650\u5236\uff0c\u673a\u5668\u4eba\u7684\u67d0\u4e9b\u90e8\u5206\u53ef\u80fd\u4f1a\u5728\u6574\u4e2a\u64cd\u4f5c\u4efb\u52a1\u671f\u95f4\u4fdd\u6301\u5728\u753b\u9762\u5916\uff0c\u5bfc\u81f4\u7f3a\u4e4f\u8db3\u591f\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u4ece\u800c\u5bfc\u81f4\u8fd9\u4e9b\u65b9\u6cd5\u5931\u8d25\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u5e76\u589e\u5f3a\u5bf9\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u4eba\u63a7\u5236\u7684\u9002\u7528\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u4f30\u8ba1\u5177\u6709\u90e8\u5206\u53ef\u89c1\u673a\u5668\u4eba\u673a\u68b0\u81c2\u7684\u673a\u5668\u4eba\u4f4d\u59ff\u7684\u65b0\u6846\u67b6\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684\u673a\u5668\u4eba\u7ec4\u4ef6\u68c0\u6d4b\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u4f4d\u59ff\u4f30\u8ba1\u7f51\u7edc\u4e2d\uff0c\u4ece\u800c\u5728\u4e0d\u540c\u7684\u64cd\u4f5c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u6027\u80fd\u3002\u8be5\u6846\u67b6\u5728\u516c\u5171\u673a\u5668\u4eba\u6570\u636e\u96c6\u548c\u81ea\u6536\u96c6\u7684\u90e8\u5206\u89c6\u56fe\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u4ee5\u8bc1\u660e\u6211\u4eec\u7684\u7a33\u5065\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u8be5\u65b9\u6cd5\u5728\u66f4\u5e7f\u6cdb\u7684\u73b0\u5b9e\u4e16\u754c\u64cd\u4f5c\u573a\u666f\u4e2d\u5bf9\u673a\u5668\u4eba\u4f4d\u59ff\u4f30\u8ba1\u662f\u6709\u6548\u7684\u3002|\n", "2409.10419": "|**2024-09-16**|[HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping Using Vision-Language Models](http://arxiv.org/abs/2409.10419)|null|\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4e0e\u4eba\u7c7b\u4ea4\u4e92\u7684\u673a\u5668\u4eba\u53ef\u4ee5\u89e3\u9501\u8bf8\u5982\u6307\u793a\u6027\u6293\u53d6\u5408\u6210 (RGS) \u7b49\u4f17\u591a\u5e94\u7528\u3002\u7ed9\u5b9a\u4e00\u4e2a\u6587\u672c\u67e5\u8be2\uff0cRGS \u786e\u5b9a\u4e00\u4e2a\u7a33\u5b9a\u7684\u6293\u53d6\u59ff\u6001\u6765\u64cd\u7eb5\u673a\u5668\u4eba\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u6240\u6307\u793a\u7684\u5bf9\u8c61\u3002RGS \u5305\u62ec\u4e24\u4e2a\u6b65\u9aa4\uff1a\u89c6\u89c9\u5b9a\u4f4d\u548c\u6293\u53d6\u59ff\u6001\u4f30\u8ba1\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5229\u7528\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5728\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u6267\u884c\u4e2d\u5bf9\u81ea\u7531\u6d41\u52a8\u7684\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u89c6\u89c9\u5b9a\u4f4d\u3002\u7136\u800c\uff0c\u7f3a\u4e4f\u5728\u5177\u6709\u540c\u4e00\u5bf9\u8c61\u7684\u591a\u4e2a\u5b9e\u4f8b\u7684\u590d\u6742\u3001\u6742\u4e71\u73af\u5883\u4e2d\u7684\u6bd4\u8f83\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 HiFi-CS\uff0c\u5b83\u91c7\u7528\u7279\u5f81\u7ebf\u6027\u8c03\u5236 (FiLM) \u7684\u5206\u5c42\u5e94\u7528\u6765\u878d\u5408\u56fe\u50cf\u548c\u6587\u672c\u5d4c\u5165\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u673a\u5668\u4eba\u6293\u53d6\u4e2d\u9047\u5230\u7684\u590d\u6742\u5c5e\u6027\u4e30\u5bcc\u7684\u6587\u672c\u67e5\u8be2\u7684\u89c6\u89c9\u5b9a\u4f4d\u3002\u89c6\u89c9\u5b9a\u4f4d\u5c06 2D/3D \u7a7a\u95f4\u4e2d\u7684\u5bf9\u8c61\u4e0e\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u76f8\u5173\u8054\uff0c\u5e76\u5728\u4e24\u79cd\u60c5\u51b5\u4e0b\u8fdb\u884c\u7814\u7a76\uff1a\u5c01\u95ed\u8bcd\u6c47\u548c\u5f00\u653e\u8bcd\u6c47\u3002HiFi-CS \u5177\u6709\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\uff0c\u5e76\u7ed3\u5408\u4e86\u51bb\u7ed3\u7684 VLM\uff0c\u5728\u5c01\u95ed\u8bcd\u6c47\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\uff0c\u540c\u65f6\u5c3a\u5bf8\u7f29\u5c0f\u4e86 100 \u500d\u3002\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u6307\u5bfc GroundedSAM \u7b49\u5f00\u653e\u96c6\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u5f00\u653e\u8bcd\u6c47\u6027\u80fd\u3002\u6211\u4eec\u4f7f\u7528 7 \u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u901a\u8fc7\u73b0\u5b9e\u4e16\u754c RGS \u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5728 15 \u4e2a\u684c\u9762\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86 90.33% \u7684\u89c6\u89c9\u5b9a\u4f4d\u51c6\u786e\u7387\u3002\u6211\u4eec\u5728\u8865\u5145\u6750\u6599\u4e2d\u5305\u542b\u4e86\u6211\u4eec\u7684\u4ee3\u7801\u5e93\u3002|\n", "2409.10078": "|**2024-09-19**|[IRIS: Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis](http://arxiv.org/abs/2409.10078)|null|\u8fd1\u5e74\u6765\uff0c\u5927\u578b\u8bed\u8a00\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\u663e\u8457\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u7136\u800c\u5c06\u9ad8\u7ea7\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u4e3a\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u7684\u7cbe\u786e\u673a\u5668\u4eba\u52a8\u4f5c\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u4ecb\u7ecd\u4e86IRIS\uff08\u4ea4\u4e92\u5f0f\u54cd\u5e94\u667a\u80fd\u5206\u5272\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u4e09\u7ef4\u529f\u80fd\u5206\u5272\u7684\u5168\u65b0\u514d\u8bad\u7ec3\u591a\u6a21\u6001\u7cfb\u7edf\uff0c\u540c\u65f6\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u65e5\u5e38\u73af\u5883\u4e2d\u4ea4\u4e92\u5f0f\u8bed\u8a00\u5f15\u5bfc\u529f\u80fd\u7684\u57fa\u51c6\u3002IRIS\u5c06\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u4e0e\u4e13\u95e8\u7684\u4e09\u7ef4\u89c6\u89c9\u7f51\u7edc\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u4e8c\u7ef4\u548c\u4e09\u7ef4\u89c6\u89c9\u7406\u89e3\u4e0e\u8bed\u8a00\u7406\u89e3\u7684\u65e0\u7f1d\u878d\u5408\u3002\u4e3a\u4e86\u4fbf\u4e8e\u8bc4\u4f30\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b10\u4e2a\u5178\u578b\u5ba4\u5185\u73af\u5883\u7684\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u73af\u5883\u5305\u542b50\u5f20\u56fe\u50cf\uff0c\u6807\u6ce8\u4e86\u7269\u4f53\u52a8\u4f5c\u548c\u4e09\u7ef4\u529f\u80fd\u5206\u5272\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cIRIS\u80fd\u591f\u5904\u7406\u5404\u79cd\u73af\u5883\u4e0b\u7684\u4ea4\u4e92\u5f0f\u4e09\u7ef4\u529f\u80fd\u5206\u5272\u4efb\u52a1\uff0c\u5e76\u5728\u5404\u79cd\u6307\u6807\u4e0a\u5747\u5c55\u73b0\u51fa\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u7ed3\u679c\u7a81\u51fa\u4e86IRIS\u5728\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\u57fa\u4e8e\u529f\u80fd\u7406\u89e3\u589e\u5f3a\u4eba\u673a\u4ea4\u4e92\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u9762\u5411\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u7684\u66f4\u76f4\u89c2\u3001\u66f4\u9ad8\u6548\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u53d1\u5c55\u3002|\n", "2409.09845": "|**2024-09-15**|[FSL-LVLM: Friction-Aware Safety Locomotion using Large Vision Language Model in Wheeled Robots](http://arxiv.org/abs/2409.09845)|null|\u8f6e\u817f\u5f0f\u673a\u5668\u4eba\u5728\u79fb\u52a8\u6027\u548c\u591a\u529f\u80fd\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u5728\u6e7f\u6ed1\u5730\u5f62\u4e0a\u8fd0\u884c\u65f6\u9762\u4e34\u7740\u5de8\u5927\u6311\u6218\u3002\u8fd9\u4e9b\u673a\u5668\u4eba\u7684\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\u5668\u5047\u8bbe\u6ca1\u6709\u6ed1\u52a8\u3002\u867d\u7136\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u53ef\u4ee5\u5e2e\u52a9\u56db\u8db3\u673a\u5668\u4eba\u9002\u5e94\u4e0d\u540c\u7684\u8868\u9762\uff0c\u4f46\u4ece\u6ed1\u52a8\u4e2d\u6062\u590d\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u63a5\u89e6\u70b9\u8f83\u5c11\u7684\u7cfb\u7edf\u3002\u4f30\u8ba1\u5730\u9762\u6469\u64e6\u7cfb\u6570\u662f\u53e6\u4e00\u4e2a\u5f00\u653e\u7684\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6469\u64e6\u611f\u77e5\u5b89\u5168\u8fd0\u52a8\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0eRL\u7b56\u7565\u76f8\u7ed3\u5408\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u4f30\u8ba1\u7684\u6469\u64e6\u7cfb\u6570\u660e\u786e\u7eb3\u5165RL\u7b56\u7565\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5728\u5230\u8fbe\u8868\u9762\u4e4b\u524d\u6839\u636e\u8868\u9762\u7c7b\u578b\u63d0\u524d\u8c03\u6574\u5176\u884c\u4e3a\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u201c\u89c6\u89c9\u6469\u64e6\u201d\uff08FFV\uff09\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5229\u7528LLM\u4f30\u8ba1\u5730\u9762\u6469\u64e6\u7cfb\u6570\uff0c\u4ece\u800c\u65e0\u9700\u5927\u578b\u6570\u636e\u96c6\u548c\u5927\u91cf\u8bad\u7ec3\u3002\u8be5\u6846\u67b6\u5728\u5b9a\u5236\u7684\u8f6e\u5f0f\u5012\u7acb\u6446\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u901a\u8fc7\u6839\u636e\u5730\u5f62\u7c7b\u578b\u8c03\u6574\u901f\u5ea6\u6765\u63d0\u9ad8\u5b8c\u6210\u9a7e\u9a76\u4efb\u52a1\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8ddf\u8e2a\u6027\u80fd\u3002\u6211\u4eec\u7684\u6846\u67b6\u53ef\u4ee5\u8f7b\u677e\u5730\u4e0e\u4efb\u4f55\u5176\u4ed6RL\u7b56\u7565\u96c6\u6210\u3002|\n", "2409.09788": "|**2024-09-15**|[Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models](http://arxiv.org/abs/2409.09788)|null|\u5c3d\u7ba1\u8fd1\u671f\u7814\u7a76\u8868\u660e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u80fd\u591f\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u56fe\u50cf\u4e2d\u7684\u590d\u6742\u5173\u7cfb\uff0c\u4f46\u5176\u5bf9\u7269\u4f53\u5927\u5c0f\u548c\u8ddd\u79bb\u8fdb\u884c\u5b9a\u91cf\u63a8\u7406\u7684\u80fd\u529b\u4ecd\u6709\u5f85\u63a2\u7d22\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6 Q-Spatial Bench\uff0c\u5176\u4e2d\u5305\u542b 271 \u4e2a\u8de8\u8d8a\u4e94\u4e2a\u7c7b\u522b\u7684\u3001\u4e13\u4e3a\u5b9a\u91cf\u7a7a\u95f4\u63a8\u7406\u8bbe\u8ba1\u7684\u95ee\u9898\uff0c\u5e76\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u6700\u5148\u8fdb\u7684 VLM \u5728\u8fd9\u9879\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0c\u63a8\u7406\u7269\u4f53\u4e4b\u95f4\u7684\u8ddd\u79bb\u5bf9 SoTA VLM \u6765\u8bf4\u5c24\u5176\u5177\u6709\u6311\u6218\u6027\uff1b\u7136\u800c\uff0c\u4e00\u4e9b VLM \u7684\u6027\u80fd\u660e\u663e\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u8868\u73b0\u6700\u597d\u7684\u4e24\u4e2a\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u8d85\u8fc7 40 \u4e2a\u767e\u5206\u70b9\u7684\u5dee\u8ddd\u3002\u6211\u4eec\u8fd8\u60ca\u5947\u5730\u89c2\u5bdf\u5230\uff0c\u5f53\u54cd\u5e94\u4e2d\u81ea\u7136\u51fa\u73b0\u4f7f\u7528\u53c2\u8003\u5bf9\u8c61\u7684\u63a8\u7406\u8def\u5f84\u65f6\uff0c\u6027\u80fd\u6700\u4f73\u7684 VLM \u7684\u6210\u529f\u7387\u63d0\u9ad8\u4e86 19 \u4e2a\u767e\u5206\u70b9\u3002\u53d7\u6b64\u89c2\u5bdf\u7ed3\u679c\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u63d0\u793a\u6280\u672f SpatialPrompt\uff0c\u8be5\u6280\u672f\u9f13\u52b1 VLM \u4f7f\u7528\u53c2\u8003\u5bf9\u8c61\u4f5c\u4e3a\u89c6\u89c9\u7ebf\u7d22\u6765\u56de\u7b54\u5b9a\u91cf\u7a7a\u95f4\u95ee\u9898\u3002\u901a\u8fc7 SpatialPrompt \u6307\u5bfc VLM \u5728\u5176\u63a8\u7406\u8def\u5f84\u4e2d\u4f7f\u7528\u53c2\u8003\u5bf9\u8c61\uff0cGemini 1.5 Pro\u3001Gemini 1.5 Flash \u548c GPT-4V \u7684\u6210\u529f\u7387\u5206\u522b\u63d0\u9ad8\u4e86 40\u300120 \u548c 30 \u4e2a\u767e\u5206\u70b9\u4ee5\u4e0a\u3002\u6211\u4eec\u5f3a\u8c03\uff0c\u8fd9\u4e9b\u663e\u8457\u7684\u6539\u8fdb\u65e0\u9700\u66f4\u591a\u6570\u636e\u3001\u6a21\u578b\u67b6\u6784\u4fee\u6539\u6216\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u3002|\n", "2409.09721": "|**2024-09-15**|[Finetuning CLIP to Reason about Pairwise Differences](http://arxiv.org/abs/2409.09721)|null|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5982 CLIP \u662f\u901a\u8fc7\u6587\u672c\u548c\u56fe\u50cf\u5bf9\u4e4b\u95f4\u7684\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\u7684\uff0c\u4ece\u800c\u4ea7\u751f\u5bf9\u9f50\u7684\u56fe\u50cf\u548c\u6587\u672c\u5d4c\u5165\uff0c\u8fd9\u5bf9\u8bb8\u591a\u4e0b\u6e38\u4efb\u52a1\u975e\u5e38\u6709\u7528\u3002\u7136\u800c\uff0cCLIP \u7684\u4e00\u4e2a\u663e\u8457\u7f3a\u70b9\u662f\uff0c\u7531\u6b64\u4ea7\u751f\u7684\u5d4c\u5165\u7a7a\u95f4\u4f3c\u4e4e\u7f3a\u4e4f\u5176\u7eaf\u6587\u672c\u66ff\u4ee3\u65b9\u6848\u6240\u5177\u6709\u7684\u4e00\u4e9b\u7ed3\u6784\u3002\u4f8b\u5982\uff0c\u957f\u671f\u4ee5\u6765\uff0c\u4eba\u4eec\u4e00\u76f4\u6ce8\u610f\u5230\u6587\u672c\u5d4c\u5165\u53ef\u4ee5\u4f7f\u7528\u5411\u91cf\u7b97\u672f\u6765\u6ee1\u8db3\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\\emph{\u7c7b\u6bd4}\uff0c\u800c CLIP \u5219\u6ca1\u6709\u8fd9\u79cd\u7279\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u5bf9\u6bd4\u65b9\u5f0f\u539f\u751f\u8bad\u7ec3 CLIP \u7684\u65b9\u6cd5\uff0c\u4ee5\u4fbf\u63a8\u7406\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5dee\u5f02\u3002\u6211\u4eec\u5bf9 CLIP \u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u4ee5\u4fbf\u56fe\u50cf\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5dee\u5f02\u5bf9\u5e94\u4e8e\\emph{\u56fe\u50cf\u5dee\u5f02\u7684\u6587\u672c\u63cf\u8ff0}\uff0c\u6211\u4eec\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf-\u6807\u9898\u914d\u5bf9\u6570\u636e\u96c6\u4e0a\u5408\u6210\u5730\u751f\u6210\u4e86\u8fd9\u4e9b\u63cf\u8ff0\u3002\u6211\u4eec\u9996\u5148\u8bc1\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6309\u7279\u5b9a\u5c5e\u6027\u5bf9\u56fe\u50cf\u8fdb\u884c\u6392\u5e8f\uff08\u4f8b\u5982\uff0c\u5927\u8c61\u6bd4\u732b\u5927\uff09\u65b9\u9762\u4ea7\u751f\u4e86\u663e\u8457\u6539\u8fdb\u7684\u80fd\u529b\uff0c\u8fd9\u5728\u68c0\u7d22\u6216\u6784\u5efa\u57fa\u4e8e\u5c5e\u6027\u7684\u5206\u7c7b\u5668\u4e2d\u975e\u5e38\u6709\u7528\uff0c\u5e76\u4e14\u63d0\u9ad8\u4e86\u8bb8\u591a\u4e0b\u6e38\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u96f6\u6837\u672c\u5206\u7c7b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8fd8\u5b9e\u73b0\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u673a\u5236\uff0c\u6211\u4eec\u5c06\u5176\u79f0\u4e3a\u6bd4\u8f83\u63d0\u793a\uff0c\u5176\u4e2d\u6211\u4eec\u5229\u7528\u5bf9\u611f\u5174\u8da3\u7c7b\u522b\u4e4b\u95f4\u5dee\u5f02\u7684\u6587\u672c\u63cf\u8ff0\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5728\u5206\u7c7b\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5927\u7684\u6027\u80fd\u63d0\u5347\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bf4\u660e\u4e86\u751f\u6210\u7684\u5d4c\u5165\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u9075\u5faa\u66f4\u5927\u7a0b\u5ea6\u7684\u51e0\u4f55\u7279\u6027\uff0c\u4f8b\u5982\u5728\u6587\u672c\u5230\u56fe\u50cf\u7684\u751f\u6210\u4e2d\u3002|\n", "2409.12191": "|**2024-09-18**|[Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution](http://arxiv.org/abs/2409.12191)|**[link](https://github.com/qwenlm/qwen2-vl)**|\u6211\u4eec\u63a8\u51fa Qwen2-VL \u7cfb\u5217\uff0c\u8fd9\u662f\u5bf9\u4e4b\u524d Qwen-VL \u6a21\u578b\u7684\u5347\u7ea7\uff0c\u5b83\u91cd\u65b0\u5b9a\u4e49\u4e86\u89c6\u89c9\u5904\u7406\u4e2d\u4f20\u7edf\u7684\u9884\u5148\u786e\u5b9a\u5206\u8fa8\u7387\u7684\u65b9\u6cd5\u3002Qwen2-VL \u5f15\u5165\u4e86 Naive \u52a8\u6001\u5206\u8fa8\u7387\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5c06\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u52a8\u6001\u5904\u7406\u6210\u4e0d\u540c\u6570\u91cf\u7684\u89c6\u89c9\u6807\u8bb0\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u751f\u6210\u66f4\u9ad8\u6548\u3001\u66f4\u51c6\u786e\u7684\u89c6\u89c9\u8868\u793a\uff0c\u4e0e\u4eba\u7c7b\u7684\u611f\u77e5\u8fc7\u7a0b\u5bc6\u5207\u4e00\u81f4\u3002\u8be5\u6a21\u578b\u8fd8\u96c6\u6210\u4e86\u591a\u6a21\u6001\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801 (M-RoPE)\uff0c\u4fc3\u8fdb\u4e86\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u4e2d\u4f4d\u7f6e\u4fe1\u606f\u7684\u6709\u6548\u878d\u5408\u3002\u6211\u4eec\u91c7\u7528\u7edf\u4e00\u7684\u8303\u5f0f\u6765\u5904\u7406\u56fe\u50cf\u548c\u89c6\u9891\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u3002\u4e3a\u4e86\u63a2\u7d22\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u6f5c\u529b\uff0cQwen2-VL \u7814\u7a76\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u7684\u89c4\u6a21\u6cd5\u5219\u3002\u901a\u8fc7\u6269\u5c55\u6a21\u578b\u89c4\u6a21\uff08\u5305\u62ec 2B\u30018B \u548c 72B \u53c2\u6570\u7684\u7248\u672c\uff09\u548c\u8bad\u7ec3\u6570\u636e\u91cf\uff0cQwen2-VL \u7cfb\u5217\u5b9e\u73b0\u4e86\u6781\u5177\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cQwen2-VL-72B \u6a21\u578b\u5728\u5404\u79cd\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4e0e GPT-4o \u548c Claude3.5-Sonnet \u7b49\u9886\u5148\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u4f18\u4e8e\u5176\u4ed6\u901a\u7528\u6a21\u578b\u3002\u4ee3\u7801\u53ef\u5728 \\url{https://github.com/QwenLM/Qwen2-VL} \u83b7\u53d6\u3002|\n", "2409.11941": "|**2024-09-18**|[GauTOAO: Gaussian-based Task-Oriented Affordance of Objects](http://arxiv.org/abs/2409.11941)|null|\u5f53\u60a8\u7684\u673a\u5668\u4eba\u4f7f\u7528\u7075\u5de7\u7684\u624b\u6216\u6293\u624b\u6293\u53d6\u7269\u4f53\u65f6\uff0c\u5b83\u5e94\u8be5\u7406\u89e3\u7269\u4f53\u7684\u9762\u5411\u4efb\u52a1\u7684\u53ef\u4f9b\u6027 (TOAO)\uff0c\u56e0\u4e3a\u4e0d\u540c\u7684\u4efb\u52a1\u901a\u5e38\u9700\u8981\u5173\u6ce8\u7269\u4f53\u7684\u7279\u5b9a\u90e8\u5206\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 GauTOAO\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u9ad8\u65af\u7684\u7269\u4f53\u9762\u5411\u4efb\u52a1\u53ef\u4f9b\u6027\u6846\u67b6\uff0c\u5b83\u4ee5\u96f6\u6837\u672c\u7684\u65b9\u5f0f\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u7ed9\u5b9a\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7684\u60c5\u51b5\u4e0b\u9884\u6d4b\u7269\u4f53\u4e0a\u4e0e\u53ef\u4f9b\u6027\u76f8\u5173\u7684\u533a\u57df\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\uff1a\u201c\u9759\u6001\u76f8\u673a\uff0c\u79fb\u52a8\u7269\u4f53\u201d\uff0c\u5141\u8bb8\u673a\u5668\u4eba\u5728\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\u66f4\u597d\u5730\u89c2\u5bdf\u548c\u7406\u89e3\u624b\u4e2d\u7684\u7269\u4f53\u3002GauTOAO \u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u7f3a\u4e4f\u6709\u6548\u7684\u7a7a\u95f4\u5206\u7ec4\uff0c\u5b83\u4f7f\u7528 DINO \u7279\u5f81\u63d0\u53d6\u5b8c\u6574\u7684 3D \u7269\u4f53\u63a9\u7801\u3002\u7136\u540e\uff0c\u8be5\u63a9\u7801\u7528\u4e8e\u6709\u6761\u4ef6\u5730\u67e5\u8be2\u9ad8\u65af\u5206\u5e03\uff0c\u4ece\u800c\u4e3a\u6307\u5b9a\u4efb\u52a1\u751f\u6210\u66f4\u7cbe\u7ec6\u7684\u7269\u4f53\u8bed\u4e49\u5206\u5e03\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u63d0\u53d6 TOAO\uff0c\u589e\u5f3a\u673a\u5668\u4eba\u5bf9\u7269\u4f53\u7684\u7406\u89e3\u5e76\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\u3002\u6211\u4eec\u901a\u8fc7\u73b0\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86 GauTOAO \u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5b83\u80fd\u591f\u6cdb\u5316\u5230\u5404\u79cd\u4efb\u52a1\u3002|\n", "2409.11919": "|**2024-09-18**|[LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Foundation Models](http://arxiv.org/abs/2409.11919)|null|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5728\u4f17\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4e0e\u5176\u4e13\u7528\u6216\u5fae\u8c03\u6a21\u578b\u76f8\u6bd4\uff0c\u5176\u96f6\u6837\u672c\u80fd\u529b\u53ef\u80fd\u6709\u9650\u3002\u7136\u800c\uff0c\u5fae\u8c03 VLM \u5177\u6709\u5c40\u9650\u6027\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u5bf9\u6a21\u578b\u67b6\u6784\u548c\u6743\u91cd\u7684\u201c\u767d\u76d2\u201d\u8bbf\u95ee\u6743\u9650\uff0c\u4ee5\u53ca\u8bbe\u8ba1\u5fae\u8c03\u76ee\u6807\u548c\u4f18\u5316\u8d85\u53c2\u6570\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u800c\u8fd9\u4e9b\u90fd\u56e0\u6bcf\u4e2a VLM \u548c\u4e0b\u6e38\u4efb\u52a1\u800c\u5f02\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 LLM-wrapper\uff0c\u4e00\u79cd\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6765\u63a8\u7406\u5176\u8f93\u51fa\uff0c\u4ece\u800c\u4ee5\u201c\u9ed1\u76d2\u201d\u65b9\u5f0f\u8c03\u6574 VLM \u7684\u65b0\u65b9\u6cd5\u3002\u6211\u4eec\u901a\u8fc7\u6307\u79f0\u8868\u8fbe\u5f0f\u7406\u89e3 (REC) \u8bc1\u660e\u4e86 LLM-wrapper \u7684\u6709\u6548\u6027\uff0c\u8fd9\u662f\u4e00\u9879\u9700\u8981\u7a7a\u95f4\u548c\u8bed\u4e49\u63a8\u7406\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u5f00\u653e\u8bcd\u6c47\u4efb\u52a1\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u73b0\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e0e\u7ecf\u5178\u5fae\u8c03\u76f8\u6bd4\u83b7\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002|\n"}, "6DOF Object Pose": {"2409.02581": "|**2024-09-04**|[Object Gaussian for Monocular 6D Pose Estimation from Sparse Views](http://arxiv.org/abs/2409.02581)|null|\u5355\u76ee\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u4eba\u6280\u672f\u4e2d\u7684\u4e00\u9879\u5173\u952e\u4efb\u52a1\uff0c\u4e25\u91cd\u4f9d\u8d56\u4e8e\u7cbe\u786e\u76842D-3D\u5bf9\u5e94\u5173\u7cfb\uff0c\u800c\u8fd9\u901a\u5e38\u9700\u8981\u6602\u8d35\u7684CAD\u6a21\u578b\uff0c\u800c\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u5e76\u4e0d\u5bb9\u6613\u83b7\u5f97\u3002\u7269\u4f53\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u5176\u4e2d\u6700\u8fd1\u5728\u4e09\u7ef4\u9ad8\u65af splatting (3DGS) \u65b9\u9762\u7684\u8fdb\u5c55\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f15\u4eba\u6ce8\u76ee\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u5b83\u7684\u6027\u80fd\u4ecd\u7136\u53d7\u5230\u5f71\u54cd\uff0c\u5e76\u4e14\u5728\u8f93\u5165\u89c6\u56fe\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u5bb9\u6613\u51fa\u73b0\u8fc7\u62df\u5408\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86SGPose\uff0c\u8fd9\u662f\u4e00\u4e2a\u4f7f\u7528\u57fa\u4e8e\u9ad8\u65af\u65b9\u6cd5\u7684\u7a00\u758f\u89c6\u56fe\u5bf9\u8c61\u59ff\u6001\u4f30\u8ba1\u7684\u65b0\u6846\u67b6\u3002SGPose\u4ec5\u9700\u5341\u4e2a\u89c6\u56fe\uff0c\u5c31\u53ef\u4ee5\u901a\u8fc7\u4ece\u968f\u673a\u957f\u65b9\u4f53\u521d\u59cb\u5316\u5f00\u59cb\u751f\u6210\u51e0\u4f55\u611f\u77e5\u8868\u793a\uff0c\u4ece\u800c\u907f\u514d\u4e86\u4f20\u7edf3DGS\u65b9\u6cd5\u6240\u8981\u6c42\u7684\u5bf9\u8fd0\u52a8\u6062\u590d\u7ed3\u6784(SfM)\u7ba1\u9053\u6d3e\u751f\u51e0\u4f55\u7684\u4f9d\u8d56\u3002SGPose\u901a\u8fc7\u56de\u5f52\u7a00\u758f\u8f93\u5165\u548c\u968f\u673a\u521d\u59cb\u5316\u7684\u56fe\u50cf\u548c\u91cd\u5efa\u6a21\u578b\u4e4b\u95f4\u7684\u5bc6\u96c62D-3D\u5bf9\u5e94\u5173\u7cfb\uff0c\u6d88\u9664\u4e86\u5bf9CAD\u6a21\u578b\u7684\u4f9d\u8d56\uff0c\u800c\u51e0\u4f55\u4e00\u81f4\u7684\u6df1\u5ea6\u76d1\u7763\u548c\u5728\u7ebf\u5408\u6210\u89c6\u56fe\u626d\u66f2\u662f\u6210\u529f\u7684\u5173\u952e\u3002\u5728\u5178\u578b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\uff0c\u7279\u522b\u662f\u5728\u906e\u6321LM-O\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u7a00\u758f\u89c6\u56fe\u7ea6\u675f\u4e0b\uff0cSGPose\u7684\u6027\u80fd\u4e5f\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd9\u8868\u660e\u5176\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002|\n", "2408.16547": "|**2024-08-29**|[OP-Align: Object-level and Part-level Alignment for Self-supervised Category-level Articulated Object Pose Estimation](http://arxiv.org/abs/2408.16547)|**[link](https://github.com/yc-che/op-align)**|\u7c7b\u522b\u7ea7\u94f0\u63a5\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4fa7\u91cd\u4e8e\u4f30\u8ba1\u5df2\u77e5\u7c7b\u522b\u4e2d\u672a\u77e5\u94f0\u63a5\u7269\u4f53\u7684\u59ff\u6001\u3002\u5c3d\u7ba1\u610f\u4e49\u91cd\u5927\uff0c\u4f46\u7531\u4e8e\u7269\u4f53\u7684\u5f62\u72b6\u548c\u59ff\u6001\u5404\u4e0d\u76f8\u540c\u3001\u6570\u636e\u96c6\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u4ee5\u53ca\u73b0\u5b9e\u73af\u5883\u590d\u6742\uff0c\u8fd9\u9879\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u5229\u7528\u5355\u5e27\u70b9\u4e91\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u6211\u4eec\u7684\u6a21\u578b\u4e00\u81f4\u5730\u751f\u6210\u5177\u6709\u89c4\u8303\u59ff\u6001\u548c\u6574\u4e2a\u8f93\u5165\u5bf9\u8c61\u7684\u5173\u8282\u72b6\u6001\u7684\u91cd\u5efa\uff0c\u5e76\u4f30\u8ba1\u7269\u4f53\u7ea7\u59ff\u6001\uff08\u51cf\u5c11\u6574\u4f53\u59ff\u6001\u5dee\u5f02\uff09\u548c\u96f6\u4ef6\u7ea7\u59ff\u6001\uff08\u5c06\u8f93\u5165\u7684\u6bcf\u4e2a\u96f6\u4ef6\u4e0e\u5176\u5bf9\u5e94\u7684\u91cd\u5efa\u96f6\u4ef6\u5bf9\u9f50\uff09\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u4ee5\u5f80\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u4e14\u4e0e\u6700\u5148\u8fdb\u7684\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u3002\u4e3a\u4e86\u8bc4\u4f30\u6211\u4eec\u7684\u6a21\u578b\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u73b0\u5b9e\u4e16\u754c\u94f0\u63a5\u7269\u4f53\u57fa\u51c6\u6570\u636e\u96c6\u3002|\n", "2408.10450": "|**2024-08-19**|[RUMI: Rummaging Using Mutual Information](http://arxiv.org/abs/2408.10450)|null|\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u7ffb\u52a8\u7269\u4f53\u65b9\u6cd5 (RUMI)\uff0c\u8be5\u65b9\u6cd5\u7528\u4e8e\u5728\u7ebf\u751f\u6210\u673a\u5668\u4eba\u5728\u89c6\u89c9\u906e\u6321\u73af\u5883\u4e2d\u6536\u96c6\u5df2\u77e5\u53ef\u52a8\u7269\u4f53\u59ff\u6001\u4fe1\u606f\u7684\u52a8\u4f5c\u5e8f\u5217\u3002\u8be5\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u63a5\u89e6\u4e30\u5bcc\u7684\u7ffb\u52a8\u7269\u4f53\u4efb\u52a1\uff0c\u5229\u7528\u7269\u4f53\u59ff\u6001\u5206\u5e03\u4e0e\u673a\u5668\u4eba\u8f68\u8ff9\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u8fdb\u884c\u52a8\u4f5c\u89c4\u5212\u3002RUMI \u4ece\u89c2\u5bdf\u5230\u7684\u90e8\u5206\u70b9\u4e91\u4e2d\u63a8\u65ad\u51fa\u517c\u5bb9\u7684\u7269\u4f53\u59ff\u6001\u5206\u5e03\uff0c\u5e76\u5b9e\u65f6\u8ba1\u7b97\u5176\u4e0e\u5de5\u4f5c\u7a7a\u95f4\u5360\u7528\u7387\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u4fe1\u606f\u589e\u76ca\u6210\u672c\u51fd\u6570\u548c\u53ef\u8fbe\u6027\u6210\u672c\u51fd\u6570\uff0c\u4ee5\u786e\u4fdd\u7269\u4f53\u59cb\u7ec8\u5904\u4e8e\u673a\u5668\u4eba\u7684\u53ef\u8fbe\u8303\u56f4\u5185\u3002\u8fd9\u4e9b\u51fd\u6570\u88ab\u96c6\u6210\u5230\u5177\u6709\u968f\u673a\u52a8\u529b\u5b66\u6a21\u578b\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236 (MPC) \u6846\u67b6\u4e2d\uff0c\u4ee5\u95ed\u73af\u65b9\u5f0f\u66f4\u65b0\u59ff\u6001\u5206\u5e03\u3002\u4e3b\u8981\u8d21\u732e\u5305\u62ec\u4e00\u79cd\u65b0\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7f6e\u4fe1\u5ea6\u6846\u67b6\u3001\u4e00\u79cd\u9ad8\u6548\u7684\u4fe1\u606f\u589e\u76ca\u8ba1\u7b97\u7b56\u7565\u4ee5\u53ca\u4e00\u79cd\u9c81\u68d2\u7684\u57fa\u4e8eMPC\u7684\u63a7\u5236\u65b9\u6848\u3002\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cRUMI \u5728\u4eff\u771f\u548c\u5b9e\u9645\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u3002|\n", "2408.08234": "|**2024-08-15**|[Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation](http://arxiv.org/abs/2408.08234)|**[link](https://github.com/varunburde/reconstruction_pose_benchmark)**|\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u5bf9\u4e8e\u8bb8\u591a\u6d89\u53ca\u673a\u5668\u4eba\u64cd\u4f5c\u3001\u5bfc\u822a\u548c\u589e\u5f3a\u73b0\u5b9e\u7684\u5de5\u4e1a\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u901a\u7528\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u5668\uff0c\u5373\u4e0d\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u7269\u4f53\u8fdb\u884c\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u4f9d\u8d56\u4e8e\u7cbe\u786e\u7684 3D \u6a21\u578b\u3002\u76ee\u524d\u4e3b\u8981\u4f7f\u7528 CAD \u6a21\u578b\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u53ef\u80fd\u96be\u4ee5\u83b7\u53d6\u3002\u540c\u65f6\uff0c\u83b7\u53d6\u7269\u4f53\u7684\u56fe\u50cf\u901a\u5e38\u662f\u53ef\u884c\u7684\u3002\u81ea\u7136\u800c\u7136\u5730\uff0c\u8fd9\u5c31\u5f15\u51fa\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u4ece\u56fe\u50cf\u91cd\u5efa\u7684 3D \u6a21\u578b\u662f\u5426\u8db3\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff1f\u4e3a\u4e86\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8861\u91cf 3D \u91cd\u5efa\u8d28\u91cf\u5bf9\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u7528\u4e8e\u7269\u4f53\u91cd\u5efa\u7684\u6821\u51c6\u56fe\u50cf\uff0c\u8fd9\u4e9b\u56fe\u50cf\u4e0e YCB-V \u6570\u636e\u96c6\u7684\u6d4b\u8bd5\u56fe\u50cf\u914d\u51c6\uff0c\u7528\u4e8e\u5728 BOP \u57fa\u51c6\u6d4b\u8bd5\u683c\u5f0f\u4e0b\u8fdb\u884c\u59ff\u6001\u8bc4\u4f30\u3002\u4f7f\u7528\u591a\u79cd\u6700\u5148\u8fdb\u7684 3D \u91cd\u5efa\u548c\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u8fdb\u884c\u7684\u8be6\u7ec6\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u4ee3\u91cd\u5efa\u65b9\u6cd5\u751f\u6210\u7684\u51e0\u4f55\u5f62\u72b6\u901a\u5e38\u8db3\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u59ff\u6001\u4f30\u8ba1\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u5f97\u51fa\u4e86\u4e00\u4e9b\u6709\u8da3\u7684\u89c2\u5bdf\u7ed3\u679c\uff1a(1) \u7528\u4e8e\u8861\u91cf 3D \u91cd\u5efa\u8d28\u91cf\u7684\u6807\u51c6\u6307\u6807\u4e0d\u4e00\u5b9a\u80fd\u6307\u793a\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u8fd9\u8868\u660e\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f8b\u5982\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002(2) \u4f20\u7edf\u7684\u3001\u975e\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u53ef\u4ee5\u4e0e\u73b0\u4ee3\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u91cd\u5efa\u6280\u672f\u76f8\u5ab2\u7f8e\uff0c\u751a\u81f3\u53ef\u4ee5\u63d0\u4f9b\u66f4\u597d\u7684\u91cd\u5efa\u65f6\u95f4-\u59ff\u6001\u7cbe\u5ea6\u6743\u8861\u3002(3) \u4f7f\u7528\u91cd\u5efa\u6a21\u578b\u548c CAD \u6a21\u578b\u7684\u6027\u80fd\u4e4b\u95f4\u4ecd\u7136\u5b58\u5728\u76f8\u5f53\u5927\u7684\u5dee\u8ddd\u3002\u4e3a\u4e86\u4fc3\u8fdb\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u7684\u7814\u7a76\uff0c\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u5728 https://github.com/VarunBurde/reconstruction_pose_benchmark \u4e0a\u516c\u5f00\u53ef\u7528\u3002|\n", "2407.12207": "|**2024-07-16**|[NeuSurfEmb: A Complete Pipeline for Dense Correspondence-based 6D Object Pose Estimation without CAD Models](http://arxiv.org/abs/2407.12207)|**[link](https://github.com/ethz-asl/neusurfemb)**|\u76ee\u524d\u6700\u5148\u8fdb\u76846D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u90fd\u5047\u8bbe\u53ef\u4ee5\u4f7f\u7528CAD\u6a21\u578b\uff0c\u5e76\u4e14\u9700\u8981\u7528\u6237\u624b\u52a8\u8bbe\u7f6e\u57fa\u4e8e\u7269\u7406\u7684\u6e32\u67d3(PBR)\u6d41\u7a0b\u6765\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\u3002\u8fd9\u4e24\u4e2a\u56e0\u7d20\u90fd\u9650\u5236\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0d\u9700\u8981CAD\u6a21\u578b\u7684\u6d41\u7a0b\uff0c\u5e76\u4e14\u53ea\u9700\u8f93\u5165\u4e00\u5c0f\u90e8\u5206\u771f\u5b9e\u56fe\u50cf\u5373\u53ef\u8bad\u7ec3\u51fa\u6700\u5148\u8fdb\u7684\u59ff\u6001\u4f30\u8ba1\u5668\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u57fa\u4e8eNeuS2\u7269\u4f53\u8868\u793a\uff0c\u6211\u4eec\u901a\u8fc7\u57fa\u4e8e\u8fd0\u52a8\u6062\u590d\u7ed3\u6784(SfM)\u548c\u7269\u4f53\u65e0\u5173\u5206\u5272\u7684\u534a\u81ea\u52a8\u7a0b\u5e8f\u6765\u5b66\u4e60\u8fd9\u79cd\u8868\u793a\u3002\u6211\u4eec\u5229\u7528NeuS2\u7684\u65b0\u9896\u89c6\u56fe\u5408\u6210\u80fd\u529b\u548c\u7b80\u5355\u7684\u526a\u5207\u7c98\u8d34\u589e\u5f3a\u6280\u672f\u6765\u81ea\u52a8\u751f\u6210\u903c\u771f\u7684\u7269\u4f53\u6e32\u67d3\u56fe\uff0c\u6211\u4eec\u4f7f\u7528\u8fd9\u4e9b\u6e32\u67d3\u56fe\u6765\u8bad\u7ec3\u57fa\u4e8e\u5bf9\u5e94\u7684SurfEmb\u59ff\u6001\u4f30\u8ba1\u5668\u3002\u6211\u4eec\u5728LINEMOD-Occlusion\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e7f\u6cdb\u7814\u7a76\u4e86\u5176\u5404\u4e2a\u7ec4\u4ef6\u7684\u5f71\u54cd\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u76f8\u5bf9\u4e8e\u57fa\u4e8eCAD\u6a21\u578b\u548cPBR\u6570\u636e\u7684\u65b9\u6cd5\u7684\u7ade\u4e89\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u6211\u4eec\u7684\u6d41\u7a0b\u5728\u81ea\u6536\u96c6\u7684\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u4e0a\u7684\u6613\u7528\u6027\u548c\u6709\u6548\u6027\uff0c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65e0CAD\u6a21\u578b\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u597d\u7684\u7cbe\u5ea6\u548c\u5bf9\u8f7b\u5fae\u906e\u6321\u7684\u9c81\u68d2\u6027\u3002\u4e3a\u4e86\u8ba9\u673a\u5668\u4eba\u793e\u533a\u80fd\u591f\u4ece\u8fd9\u4e2a\u7cfb\u7edf\u4e2d\u53d7\u76ca\uff0c\u6211\u4eec\u5c06\u5728https://www.github.com/ethz-asl/neusurfemb\u516c\u5f00\u53d1\u5e03\u5b83\u3002|\n", "2406.04316": "|**2024-06-06**|[Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking](http://arxiv.org/abs/2406.04316)|null|6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u4e00\u9879\u81f3\u5173\u91cd\u8981\u4f46\u6781\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5176\u9762\u4e34\u7684\u4e3b\u8981\u95ee\u9898\u662f\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u4e25\u91cd\u7f3a\u4e4f\u3002\u8fd9\u79cd\u7a00\u7f3a\u6027\u963b\u788d\u4e86\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u9650\u5236\u4e86\u7814\u7a76\u8fdb\u5c55\u3002\u6b64\u5916\uff0c\u53ef\u7528\u5b9e\u4f8b\u6216\u7c7b\u522b\u7684\u6570\u91cf\u6709\u9650\u4e5f\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86 Omni6DPose\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ee5\u5bf9\u8c61\u7c7b\u522b\u591a\u6837\u6027\u3001\u89c4\u6a21\u5927\u548c\u5bf9\u8c61\u6750\u6599\u79cd\u7c7b\u7e41\u591a\u4e3a\u7279\u5f81\u7684\u5927\u578b\u6570\u636e\u96c6\u3002Omni6DPose \u4e3b\u8981\u5206\u4e3a\u4e09\u4e2a\u90e8\u5206\uff1aROPE\uff08\u771f\u5b9e 6D \u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u96c6\uff09\uff0c\u5305\u542b 332K \u5f20\u56fe\u50cf\uff0c\u6db5\u76d6 149 \u4e2a\u7c7b\u522b\u3001581 \u4e2a\u5b9e\u4f8b\u7684\u8d85\u8fc7 150 \u4e07\u4e2a\u6807\u6ce8\uff1bSOPE\uff08\u6a21\u62df 6D \u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u96c6\uff09\uff0c\u7531\u5728\u6df7\u5408\u73b0\u5b9e\u73af\u5883\u4e2d\u521b\u5efa\u7684 475K \u5f20\u56fe\u50cf\u7ec4\u6210\uff0c\u91c7\u7528\u6df1\u5ea6\u6a21\u62df\u6280\u672f\uff0c\u5305\u542b 149 \u4e2a\u7c7b\u522b\u30014162 \u4e2a\u5b9e\u4f8b\u7684\u8d85\u8fc7 500 \u4e07\u4e2a\u6807\u6ce8\uff1b\u4ee5\u53ca\u5728 ROPE \u548c SOPE \u4e2d\u5747\u4f7f\u7528\u7684\u624b\u52a8\u5bf9\u9f50\u7684\u771f\u5b9e\u626b\u63cf\u7269\u4f53\u3002\u7531\u4e8e\u5b58\u5728\u5927\u91cf\u7684\u53d8\u5316\u548c\u6b67\u4e49\uff0cOmni6DPose \u672c\u8eab\u5c31\u6781\u5177\u6311\u6218\u6027\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86 GenPose++\uff0c\u5b83\u662f SOTA \u7c7b\u522b\u7ea7\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\u7684\u589e\u5f3a\u7248\u672c\uff0c\u5b83\u5305\u542b\u4e24\u9879\u5173\u952e\u6539\u8fdb\uff1a\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u63d0\u53d6\u548c\u57fa\u4e8e\u805a\u7c7b\u7684\u805a\u5408\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u5206\u6790\uff0c\u4ee5\u8bc4\u4f30\u5148\u524d\u65b9\u6cd5\u5728\u8fd9\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5728 6D \u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u59ff\u6001\u8ddf\u8e2a\u65b9\u9762\u7684\u6027\u80fd\u3002|\n", "2406.02977": "|**2024-06-05**|[Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices](http://arxiv.org/abs/2406.02977)|null|\u968f\u7740\u673a\u5668\u4eba\u548c\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u8d8a\u6765\u8d8a\u4f9d\u8d56\u4e8e\u7cbe\u786e\u9ad8\u6548\u76846D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u6027\u80fd\u5bf9\u4e8e\u66f4\u5177\u4ea4\u4e92\u6027\u548c\u54cd\u5e94\u6027\u7684\u7cfb\u7edf\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u63d0\u51fa\u7684\u7a00\u758f\u989c\u8272\u4ee3\u7801\u7f51\u7edc(SCCN)\u4f53\u73b0\u4e86\u4e00\u79cd\u6e05\u6670\u7b80\u6d01\u7684\u6d41\u6c34\u7ebf\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u3002SCCN\u5bf9RGB\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u7269\u4f53\u8fdb\u884c\u50cf\u7d20\u7ea7\u9884\u6d4b\uff0c\u5229\u7528\u57fa\u672c\u7269\u4f53\u51e0\u4f55\u7279\u5f81\u7684\u7a00\u758f\u6027\u6765\u52a0\u901f\u900f\u89c6n\u70b9(PnP)\u8ba1\u7b97\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u5b83\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u50cf\u7d20\u7ea7\u51e0\u4f55\u7684\u7269\u4f53\u5bf9\u79f0\u8868\u793a\uff0c\u8be5\u8868\u793a\u4e0e\u521d\u59cb\u59ff\u6001\u9884\u6d4b\u65e0\u7f1d\u96c6\u6210\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5bf9\u79f0\u7269\u4f53\u6b67\u4e49\u95ee\u9898\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSCCN\u5728\u82f1\u4f1f\u8fbeJetson AGX Xavier\u4e0a\u5206\u522b\u5728\u57fa\u51c6LINEMOD\u6570\u636e\u96c6\u548c\u906e\u6321LINEMOD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6bcf\u79d219\u5e27(FPS)\u548c6\u5e27\u7684\u4f30\u8ba1\u901f\u7387\uff0c\u540c\u65f6\u5728\u8fd9\u4e9b\u901f\u7387\u4e0b\u59cb\u7ec8\u4fdd\u6301\u8f83\u9ad8\u7684\u4f30\u8ba1\u7cbe\u5ea6\u3002|\n", "2405.07801": "|**2024-05-31**|[Deep Learning-Based Object Pose Estimation: A Comprehensive Survey](http://arxiv.org/abs/2405.07801)|**[link](https://github.com/cnjianliu/awesome-object-pose-estimation)**|\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff0c\u5728\u589e\u5f3a\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u6280\u672f\u4e2d\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u5728\u8fc7\u53bb\u7684\u5341\u5e74\u4e2d\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7531\u4e8e\u5176\u5353\u8d8a\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5df2\u7ecf\u9010\u6e10\u53d6\u4ee3\u4e86\u4f9d\u8d56\u4e8e\u5de5\u7a0b\u70b9\u5bf9\u7279\u5f81\u7684\u4f20\u7edf\u7b97\u6cd5\u3002\u7136\u800c\uff0c\u5f53\u4ee3\u65b9\u6cd5\u4ecd\u7136\u5b58\u5728\u4e00\u4e9b\u6311\u6218\uff0c\u5305\u62ec\u5b83\u4eec\u5bf9\u6807\u8bb0\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\u6027\u3001\u6a21\u578b\u7d27\u51d1\u6027\u3001\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u5bf9\u65b0\u9896\u7684\u672a\u89c1\u8fc7\u7269\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u3002\u76ee\u524d\u7f3a\u5c11\u4e00\u7bc7\u7efc\u8ff0\u6765\u8ba8\u8bba\u8be5\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u3001\u9762\u4e34\u7684\u6311\u6218\u4ee5\u53ca\u672a\u6765\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6db5\u76d6\u4e86\u8be5\u95ee\u9898\u7684\u4e09\u79cd\u5f62\u5f0f\uff0c\u5373\u5b9e\u4f8b\u7ea7\u3001\u7c7b\u522b\u7ea7\u548c\u672a\u89c1\u8fc7\u7269\u4f53\u7684\u59ff\u6001\u4f30\u8ba1\u3002\u6211\u4eec\u7684\u7efc\u8ff0\u8fd8\u6db5\u76d6\u4e86\u591a\u79cd\u8f93\u5165\u6570\u636e\u6a21\u6001\u3001\u8f93\u51fa\u59ff\u6001\u7684\u81ea\u7531\u5ea6\u3001\u7269\u4f53\u5c5e\u6027\u548c\u4e0b\u6e38\u4efb\u52a1\uff0c\u4e3a\u8bfb\u8005\u63d0\u4f9b\u4e86\u5bf9\u8be5\u9886\u57df\u7684\u5168\u9762\u7406\u89e3\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u8ba8\u8bba\u4e86\u4e0d\u540c\u9886\u57df\u7684\u8bad\u7ec3\u8303\u5f0f\u3001\u63a8\u7406\u6a21\u5f0f\u3001\u5e94\u7528\u9886\u57df\u3001\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u62a5\u544a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u5728\u8fd9\u4e9b\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\uff0c\u4ece\u800c\u65b9\u4fbf\u8bfb\u8005\u6839\u636e\u81ea\u5df1\u7684\u5e94\u7528\u9009\u62e9\u6700\u5408\u9002\u7684\u65b9\u6cd5\u3002\u6700\u540e\uff0c\u8be5\u7efc\u8ff0\u6307\u51fa\u4e86\u5173\u952e\u6311\u6218\uff0c\u56de\u987e\u4e86\u5f53\u524d\u7684\u8d8b\u52bf\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002\u6211\u4eec\u8fd8\u5728 https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation \u4e0a\u6301\u7eed\u8ddf\u8e2a\u6700\u65b0\u7684\u5de5\u4f5c\u3002|\n", "2403.19527": "|**2024-03-28**|[Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation](http://arxiv.org/abs/2403.19527)|**[link](https://github.com/leeiieeo/ag-pose)**|\u7c7b\u522b\u7ea7 6D \u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65e8\u5728\u4f30\u8ba1\u7279\u5b9a\u7c7b\u522b\u4e2d\u672a\u89c1\u8fc7\u5b9e\u4f8b\u7684\u65cb\u8f6c\u3001\u5e73\u79fb\u548c\u5927\u5c0f\u3002\u5728\u8fd9\u4e2a\u9886\u57df\uff0c\u57fa\u4e8e\u5bc6\u96c6\u5bf9\u5e94\u7684\u7b97\u6cd5\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5b83\u4eec\u6ca1\u6709\u660e\u786e\u8003\u8651\u4e0d\u540c\u5b9e\u4f8b\u7684\u5c40\u90e8\u548c\u5168\u5c40\u51e0\u4f55\u4fe1\u606f\uff0c\u5bfc\u81f4\u5bf9\u5f62\u72b6\u53d8\u5316\u663e\u8457\u7684\u672a\u89c1\u8fc7\u5b9e\u4f8b\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b9e\u4f8b\u81ea\u9002\u5e94\u548c\u51e0\u4f55\u611f\u77e5\u5173\u952e\u70b9\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u7c7b\u522b\u7ea7 6D \u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff08AG-Pose\uff09\uff0c\u5b83\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a\uff081\uff09\u7b2c\u4e00\u4e2a\u8bbe\u8ba1\u662f\u5b9e\u4f8b\u81ea\u9002\u5e94\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u5757\uff0c\u5b83\u53ef\u4ee5\u81ea\u9002\u5e94\u5730\u68c0\u6d4b\u4e00\u7ec4\u7a00\u758f\u5173\u952e\u70b9\uff0c\u7528\u4e8e\u8868\u793a\u5404\u79cd\u5b9e\u4f8b\u7684\u51e0\u4f55\u7ed3\u6784\u3002\uff082\uff09\u7b2c\u4e8c\u4e2a\u8bbe\u8ba1\u662f\u51e0\u4f55\u611f\u77e5\u7279\u5f81\u805a\u5408\u6a21\u5757\uff0c\u5b83\u53ef\u4ee5\u6709\u6548\u5730\u5c06\u5c40\u90e8\u548c\u5168\u5c40\u51e0\u4f55\u4fe1\u606f\u6574\u5408\u5230\u5173\u952e\u70b9\u7279\u5f81\u4e2d\u3002\u8fd9\u4e24\u4e2a\u6a21\u5757\u53ef\u4ee5\u534f\u540c\u5de5\u4f5c\uff0c\u4e3a\u672a\u89c1\u8fc7\u7684\u5b9e\u4f8b\u5efa\u7acb\u9c81\u68d2\u7684\u5173\u952e\u70b9\u7ea7\u5bf9\u5e94\u5173\u7cfb\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728 CAMERA25 \u548c REAL275 \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 AG-Pose \u5728\u6ca1\u6709\u7c7b\u522b\u7279\u5b9a\u5f62\u72b6\u5148\u9a8c\u7684\u60c5\u51b5\u4e0b\uff0c\u5927\u5927\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002|\n", "2403.18791": "|**2024-06-01**|[Object Pose Estimation via the Aggregation of Diffusion Features](http://arxiv.org/abs/2403.18791)|**[link](https://github.com/tianfu18/diff-feats-pose)**|\u4ece\u56fe\u50cf\u4e2d\u4f30\u8ba1\u7269\u4f53\u59ff\u6001\u662f3D\u573a\u666f\u7406\u89e3\u7684\u5173\u952e\u4efb\u52a1\uff0c\u6700\u8fd1\u7684\u65b9\u6cd5\u5728\u975e\u5e38\u5927\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u53ef\u559c\u7684\u7ed3\u679c\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5904\u7406\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u65f6\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\u3002\u6211\u4eec\u8ba4\u4e3a\u8fd9\u662f\u7531\u4e8e\u56fe\u50cf\u7279\u5f81\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u9020\u6210\u7684\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5bf9\u6269\u6563\u6a21\u578b\uff08\u4f8b\u5982Stable Diffusion\uff09\u7684\u7279\u5f81\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u5bf9\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u5efa\u6a21\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002\u5728\u6b64\u5206\u6790\u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u521b\u65b0\u6027\u5730\u5c06\u8fd9\u4e9b\u6269\u6563\u7279\u5f81\u5f15\u5165\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u67b6\u6784\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u6355\u83b7\u548c\u805a\u5408\u4e0d\u540c\u7c92\u5ea6\u7684\u6269\u6563\u7279\u5f81\uff0c\u6781\u5927\u5730\u63d0\u9ad8\u4e86\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e09\u4e2a\u6d41\u884c\u7684\u57fa\u51c6\u6570\u636e\u96c6LM\u3001O-LM\u548cT-LESS\u4e0a\uff0c\u4ee5\u76f8\u5f53\u5927\u7684\u4f18\u52bf\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u4e0a\u53d6\u5f97\u4e86\u6bd4\u5148\u524d\u6700\u4f73\u7ed3\u679c\u66f4\u9ad8\u7684\u7cbe\u5ea6\uff1a\u5728Unseen LM\u4e0a\u4e3a98.2%\u5bf993.5%\uff0c\u5728Unseen O-LM\u4e0a\u4e3a85.9%\u5bf976.3%\uff0c\u663e\u793a\u4e86\u6211\u4eec\u65b9\u6cd5\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53d1\u5e03\u5728https://github.com/Tianfu18/diff-feats-pose\u3002||\n", "2409.08269": "|**2024-09-12**|[Touch2Touch: Cross-Modal Tactile Generation for Object Manipulation](http://arxiv.org/abs/2409.08269)|null|\u73b0\u4eca\u7684\u89e6\u89c9\u4f20\u611f\u5668\u5f62\u6001\u5404\u5f02\uff0c\u5c3a\u5bf8\u4e0d\u4e00\u3002\u7531\u4e8e\u6a21\u578b\u901a\u5e38\u4e0e\u7279\u5b9a\u7684\u4f20\u611f\u5668\u8bbe\u8ba1\u7ed1\u5b9a\uff0c\u56e0\u6b64\u5f00\u53d1\u901a\u7528\u7684\u89e6\u89c9\u5904\u7406\u65b9\u6cd5\u53d8\u5f97\u6781\u5177\u6311\u6218\u6027\u3002\u6211\u4eec\u901a\u8fc7\u5728\u89e6\u89c9\u4f20\u611f\u5668\u4e4b\u95f4\u8fdb\u884c\u8de8\u6a21\u6001\u9884\u6d4b\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff1a\u7ed9\u5b9a\u6765\u81ea\u4e00\u4e2a\u4f20\u611f\u5668\u7684\u89e6\u89c9\u4fe1\u53f7\uff0c\u6211\u4eec\u4f7f\u7528\u751f\u6210\u6a21\u578b\u6765\u4f30\u8ba1\u53e6\u4e00\u4e2a\u4f20\u611f\u5668\u5982\u4f55\u611f\u77e5\u76f8\u540c\u7684\u7269\u7406\u63a5\u89e6\u3002\u8fd9\u4f7f\u5f97\u6211\u4eec\u53ef\u4ee5\u5c06\u7279\u5b9a\u4e8e\u4f20\u611f\u5668\u7684\u5904\u7406\u65b9\u6cd5\u5e94\u7528\u4e8e\u751f\u6210\u7684\u4fe1\u53f7\u3002\u6211\u4eec\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u6269\u6563\u6a21\u578b\u6765\u5b9e\u73b0\u8fd9\u4e00\u60f3\u6cd5\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u5728\u6d41\u884c\u7684 GelSlim \u548c Soft Bubble \u4f20\u611f\u5668\u4e4b\u95f4\u8fdb\u884c\u8f6c\u6362\u3002\u4f5c\u4e3a\u4e0b\u6e38\u4efb\u52a1\uff0c\u6211\u4eec\u4f7f\u7528 GelSlim \u4f20\u611f\u5668\u8fdb\u884c\u624b\u6301\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u540c\u65f6\u4f7f\u7528\u4ec5\u5bf9 Soft Bubble \u4fe1\u53f7\u8fdb\u884c\u64cd\u4f5c\u7684\u7b97\u6cd5\u3002\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u53ef\u4ee5\u5728 https://www.mmintlab.com/research/touch2touch/ \u4e0a\u627e\u5230\u3002|\n"}, "nerf": {"2408.09130": "|**2024-08-20**|[Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark Images Using Gaussian Splatting](http://arxiv.org/abs/2408.09130)|**[link](https://github.com/yec22/Gaussian-DK)**|\u4e09\u7ef4\u9ad8\u65af\u4f53\u79ef\u6e32\u67d3\u6280\u672f\u8fd1\u5e74\u6765\u53d1\u5c55\u8fc5\u901f\uff0c\u5b83\u80fd\u591f\u5229\u7528\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u5408\u6210\u51fa\u975e\u51e1\u7684\u65b0\u89c6\u89d2\u3002\u7136\u800c\uff0c\u6211\u4eec\u6ce8\u610f\u5230\uff0c\u5728\u5149\u7ebf\u660f\u6697\u3001\u573a\u666f\u672a\u88ab\u5b8c\u5168\u7167\u4eae\u7684\u73af\u5883\u4e2d\u62cd\u6444\u7684\u56fe\u50cf\uff0c\u53ef\u80fd\u4f1a\u5448\u73b0\u51fa\u660e\u663e\u7684\u4eae\u5ea6\u53d8\u5316\u548c\u591a\u89c6\u89d2\u4e0d\u4e00\u81f4\u6027\uff0c\u8fd9\u5bf9\u4e09\u7ef4\u9ad8\u65af\u4f53\u79ef\u6e32\u67d3\u6280\u672f\u63d0\u51fa\u4e86\u5de8\u5927\u7684\u6311\u6218\uff0c\u5e76\u4e25\u91cd\u964d\u4f4e\u4e86\u5176\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Gaussian-DK \u65b9\u6cd5\u3002\u89c2\u5bdf\u5230\u4e0d\u4e00\u81f4\u6027\u4e3b\u8981\u662f\u7531\u76f8\u673a\u6210\u50cf\u5f15\u8d77\u7684\uff0c\u6211\u4eec\u4f7f\u7528\u4e00\u7ec4\u5404\u5411\u5f02\u6027\u7684\u4e09\u7ef4\u9ad8\u65af\u51fd\u6570\u6765\u8868\u793a\u771f\u5b9e\u4e16\u754c\u4e2d\u4e00\u81f4\u7684\u8f90\u5c04\u573a\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u76f8\u673a\u54cd\u5e94\u6a21\u5757\u6765\u8865\u507f\u591a\u89c6\u89d2\u4e0d\u4e00\u81f4\u6027\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b65\u957f\u7684\u68af\u5ea6\u7f29\u653e\u7b56\u7565\uff0c\u4ee5\u7ea6\u675f\u9760\u8fd1\u76f8\u673a\u7684\u9ad8\u65af\u51fd\u6570\uff08\u8fd9\u4e9b\u51fd\u6570\u5bb9\u6613\u53d8\u6210\u6f02\u6d6e\u7269\uff09\u7684\u5206\u5272\u548c\u514b\u9686\u3002\u5728\u6211\u4eec\u63d0\u51fa\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGaussian-DK \u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\u7ed3\u679c\uff0c\u6ca1\u6709\u91cd\u5f71\u548c\u6f02\u6d6e\u7269\u4f2a\u5f71\uff0c\u5e76\u4e14\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u53ef\u4ee5\u901a\u8fc7\u63a7\u5236\u66dd\u5149\u6c34\u5e73\u6765\u5408\u6210\u4eae\u5ea6\u66f4\u9ad8\u7684\u56fe\u50cf\uff0c\u4ece\u800c\u6e05\u6670\u5730\u663e\u793a\u9634\u5f71\u533a\u57df\u7684\u7ec6\u8282\u3002|\n", "2407.13520": "|**2024-09-05**|[EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian Splatting](http://arxiv.org/abs/2407.13520)|null|\u8fd1\u5e74\u6765\uff0c\u968f\u7740\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u548c 3D \u9ad8\u65af\u6563\u5c04 (3DGS) \u7684\u53d1\u5c55\uff0c3D \u53bb\u6a21\u7cca\u91cd\u5efa\u6280\u672f\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002 \u5c3d\u7ba1\u8fd9\u4e9b\u6280\u672f\u53ef\u4ee5\u4ece\u6a21\u7cca\u7684\u56fe\u50cf\u8f93\u5165\u4e2d\u6062\u590d\u76f8\u5bf9\u6e05\u6670\u7684 3D \u91cd\u5efa\uff0c\u4f46\u5b83\u4eec\u5728\u5904\u7406\u4e25\u91cd\u6a21\u7cca\u548c\u590d\u6742\u76f8\u673a\u8fd0\u52a8\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u5c40\u9650\u6027\u3002 \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e8b\u4ef6\u8f85\u52a9\u7684 3D \u9ad8\u65af\u6563\u5c04\u53bb\u6a21\u7cca\u91cd\u5efa (EaDeblur-GS)\uff0c\u5b83\u96c6\u6210\u4e86\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u4ee5\u589e\u5f3a 3DGS \u5bf9\u8fd0\u52a8\u6a21\u7cca\u7684\u9c81\u68d2\u6027\u3002 \u901a\u8fc7\u4f7f\u7528\u81ea\u9002\u5e94\u504f\u5dee\u4f30\u8ba1\u5668 (ADE) \u7f51\u7edc\u6765\u4f30\u8ba1\u9ad8\u65af\u4e2d\u5fc3\u504f\u5dee\u5e76\u4f7f\u7528\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0cEaDeblur-GS \u53ef\u4ee5\u5b9e\u65f6\u5b9e\u73b0\u6e05\u6670\u7684 3D \u91cd\u5efa\uff0c\u5176\u6027\u80fd\u53ef\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002|\n", "2407.07090": "|**2024-07-10**|[3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes](http://arxiv.org/abs/2407.07090)|null|\u57fa\u4e8e\u7c92\u5b50\u7684\u8f90\u5c04\u573a\u8868\u793a\u6cd5\uff0c\u4f8b\u5982 3D \u9ad8\u65af splatting\uff0c\u5df2\u7ecf\u5728\u590d\u6742\u573a\u666f\u7684\u91cd\u5efa\u548c\u91cd\u65b0\u6e32\u67d3\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\u3002\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5149\u6805\u5316\u6e32\u67d3\u7c92\u5b50\uff0c\u5c06\u5b83\u4eec\u6295\u5f71\u5230\u5c4f\u5e55\u7a7a\u95f4\u56fe\u5757\u4e2d\uff0c\u4ee5\u4fbf\u6309\u6392\u5e8f\u987a\u5e8f\u8fdb\u884c\u5904\u7406\u3002\u8fd9\u9879\u5de5\u4f5c\u5219\u8003\u8651\u5bf9\u7c92\u5b50\u8fdb\u884c\u5149\u7ebf\u8ffd\u8e2a\uff0c\u6784\u5efa\u8fb9\u754c\u4f53\u79ef\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u4f7f\u7528\u9ad8\u6027\u80fd GPU \u5149\u7ebf\u8ffd\u8e2a\u786c\u4ef6\u4e3a\u6bcf\u4e2a\u50cf\u7d20\u6295\u5c04\u5149\u7ebf\u3002\u4e3a\u4e86\u6709\u6548\u5904\u7406\u5927\u91cf\u534a\u900f\u660e\u7c92\u5b50\uff0c\u6211\u4eec\u63cf\u8ff0\u4e86\u4e00\u79cd\u4e13\u95e8\u7684\u6e32\u67d3\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4f7f\u7528\u8fb9\u754c\u7f51\u683c\u5c01\u88c5\u7c92\u5b50\u4ee5\u5229\u7528\u5feb\u901f\u7684\u5149\u7ebf\u4e09\u89d2\u5f62\u76f8\u4ea4\uff0c\u5e76\u6309\u6df1\u5ea6\u987a\u5e8f\u5bf9\u6210\u6279\u7684\u76f8\u4ea4\u8fdb\u884c\u7740\u8272\u3002\u5149\u7ebf\u8ffd\u8e2a\u7684\u4f18\u52bf\u5728\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\u662f\u4f17\u6240\u5468\u77e5\u7684\uff1a\u5904\u7406\u975e\u76f8\u5e72\u5149\u7ebf\u4ee5\u83b7\u5f97\u9634\u5f71\u548c\u53cd\u5c04\u7b49\u4e8c\u6b21\u7167\u660e\u6548\u679c\u3001\u4ece\u673a\u5668\u4eba\u6280\u672f\u4e2d\u5e38\u89c1\u7684\u9ad8\u5ea6\u626d\u66f2\u7684\u76f8\u673a\u8fdb\u884c\u6e32\u67d3\u3001\u968f\u673a\u91c7\u6837\u5149\u7ebf\u7b49\u7b49\u3002\u4f7f\u7528\u6211\u4eec\u7684\u6e32\u67d3\u5668\uff0c\u4e0e\u5149\u6805\u5316\u76f8\u6bd4\uff0c\u8fd9\u79cd\u7075\u6d3b\u6027\u51e0\u4e4e\u6ca1\u6709\u6210\u672c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u901f\u5ea6\u548c\u51c6\u786e\u6027\uff0c\u4ee5\u53ca\u5728\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u548c\u89c6\u89c9\u65b9\u9762\u7684\u51e0\u79cd\u5e94\u7528\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u5bf9\u57fa\u672c\u9ad8\u65af\u8868\u793a\u7684\u76f8\u5173\u6539\u8fdb\uff0c\u5305\u62ec\u7b80\u5355\u5730\u4f7f\u7528\u5e7f\u4e49\u6838\u51fd\u6570\uff0c\u8fd9\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u7c92\u5b50\u547d\u4e2d\u6b21\u6570\u3002|\n", "2407.05254": "|**2024-07-07**|[GaussReg: Fast 3D Registration with Gaussian Splatting](http://arxiv.org/abs/2407.05254)|null|\u70b9\u4e91\u914d\u51c6\u662f\u5927\u89c4\u6a213D\u573a\u666f\u626b\u63cf\u548c\u91cd\u5efa\u7684\u57fa\u672c\u95ee\u9898\u3002\u5728\u6df1\u5ea6\u5b66\u4e60\u7684\u5e2e\u52a9\u4e0b\uff0c\u914d\u51c6\u65b9\u6cd5\u5f97\u5230\u4e86\u663e\u8457\u53d1\u5c55\uff0c\u5df2\u63a5\u8fd1\u6210\u719f\u9636\u6bb5\u3002\u968f\u7740\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u7684\u5f15\u5165\uff0c\u5b83\u5f3a\u5927\u7684\u89c6\u56fe\u5408\u6210\u80fd\u529b\u4f7f\u5176\u6210\u4e3a\u6700\u53d7\u6b22\u8fce\u76843D\u573a\u666f\u8868\u793a\u65b9\u6cd5\u3002\u5bf9\u4e8eNeRF\u8868\u793a\uff0c\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u4e5f\u9700\u8981\u5bf9\u5176\u8fdb\u884c\u914d\u51c6\u3002\u7136\u800c\uff0c\u8fd9\u65b9\u9762\u8fd8\u7f3a\u4e4f\u6df1\u5165\u7684\u7814\u7a76\u3002\u8fd9\u662f\u56e0\u4e3a\u5bf9\u5177\u6709\u9690\u5f0f\u8868\u793a\u7684\u4e24\u4e2a\u573a\u666f\u4e4b\u95f4\u7684\u51e0\u4f55\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u5b58\u5728\u56fa\u6709\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u9690\u5f0f\u8868\u793a\u8f6c\u6362\u4e3a\u663e\u5f0f\u8868\u793a\u4ee5\u8fdb\u884c\u8fdb\u4e00\u6b65\u914d\u51c6\u3002\u6700\u8fd1\uff0c\u5f15\u5165\u4e86\u9ad8\u65af\u6e32\u67d3\uff08GS\uff09\uff0c\u5b83\u91c7\u7528\u663e\u5f0f3D\u9ad8\u65af\u51fd\u6570\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6e32\u67d3\u901f\u5ea6\u3002\u7ed9\u5b9a\u4e24\u4e2a\u5177\u6709\u663e\u5f0fGS\u8868\u793a\u7684\u573a\u666f\uff0c\u6211\u4eec\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\u63a2\u7d22\u4e86\u5b83\u4eec\u4e4b\u95f4\u76843D\u914d\u51c6\u4efb\u52a1\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86GaussReg\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u7531\u7c97\u5230\u7cbe\u7684\u6846\u67b6\uff0c\u5b83\u65e2\u5feb\u901f\u53c8\u51c6\u786e\u3002\u7c97\u914d\u51c6\u9636\u6bb5\u9075\u5faa\u73b0\u6709\u7684\u70b9\u4e91\u914d\u51c6\u65b9\u6cd5\uff0c\u5e76\u4f30\u8ba1\u6765\u81eaGS\u7684\u70b9\u4e91\u7684\u7c97\u7565\u5bf9\u9f50\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u5f15\u5bfc\u7684\u7cbe\u914d\u51c6\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4eceGS\u6e32\u67d3\u56fe\u50cf\uff0c\u4e3a\u7cbe\u786e\u5bf9\u9f50\u63d0\u4f9b\u66f4\u8be6\u7ec6\u7684\u51e0\u4f55\u4fe1\u606f\u3002\u4e3a\u4e86\u652f\u6301\u5168\u9762\u8bc4\u4f30\uff0c\u6211\u4eec\u4ed4\u7ec6\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aScanNet-GSReg\u7684\u573a\u666f\u7ea7\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4eceScanNet\u6570\u636e\u96c6\u4e2d\u83b7\u5f97\u76841379\u4e2a\u573a\u666f\uff0c\u5e76\u6536\u96c6\u4e86\u4e00\u4e2a\u540d\u4e3aGSReg\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684GaussReg\u6bd4HLoc\uff08SuperPoint\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0cSuperGlue\u4f5c\u4e3a\u5339\u914d\u5668\uff09\u5feb44\u500d\uff0c\u5e76\u4e14\u5177\u6709\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002|\n", "2407.03923": "|**2024-07-04**|[CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion Blur Images](http://arxiv.org/abs/2407.03923)|null|\u795e\u7ecf\u8f90\u5c04\u573a (NeRFs) \u56e0\u5176\u9ad8\u8d28\u91cf\u7684\u65b0\u89c6\u89d2\u6e32\u67d3\u80fd\u529b\u800c\u5907\u53d7\u5173\u6ce8\uff0c\u4fc3\u4f7f\u7814\u7a76\u4eba\u5458\u81f4\u529b\u4e8e\u89e3\u51b3\u5404\u79cd\u73b0\u5b9e\u4e16\u754c\u6848\u4f8b\u3002\u5176\u4e2d\u4e00\u4e2a\u5173\u952e\u6311\u6218\u662f\u5728\u66dd\u5149\u65f6\u95f4\u5185\u7531\u76f8\u673a\u79fb\u52a8\u5f15\u8d77\u7684\u76f8\u673a\u8fd0\u52a8\u6a21\u7cca\uff0c\u8fd9\u963b\u788d\u4e86\u7cbe\u786e\u7684 3D \u573a\u666f\u91cd\u5efa\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8fde\u7eed\u521a\u6027\u8fd0\u52a8\u611f\u77e5\u9ad8\u65af splatting (CRiM-GS)\uff0c\u4ee5\u5b9e\u65f6\u6e32\u67d3\u901f\u5ea6\u4ece\u6a21\u7cca\u56fe\u50cf\u91cd\u5efa\u7cbe\u786e\u7684 3D \u573a\u666f\u3002\u8003\u8651\u5230\u5b9e\u9645\u7684\u76f8\u673a\u8fd0\u52a8\u6a21\u7cca\u8fc7\u7a0b\u5305\u542b\u590d\u6742\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u6211\u4eec\u57fa\u4e8e\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b (ODE) \u9884\u6d4b\u76f8\u673a\u7684\u8fde\u7eed\u8fd0\u52a8\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5229\u7528\u521a\u4f53\u53d8\u6362\u5bf9\u76f8\u673a\u8fd0\u52a8\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u8fdb\u884c\u9002\u5f53\u7684\u6b63\u5219\u5316\uff0c\u4ee5\u4fdd\u6301\u5bf9\u8c61\u7684\u5f62\u72b6\u548c\u5927\u5c0f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728\\textit{SE(3)}  \u57df\u4e2d\u5f15\u5165\u4e86\u8fde\u7eed\u53ef\u53d8\u5f62 3D \u53d8\u6362\uff0c\u901a\u8fc7\u786e\u4fdd\u66f4\u9ad8\u7684\u81ea\u7531\u5ea6\u4f7f\u521a\u4f53\u53d8\u6362\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u7684\u95ee\u9898\u3002\u901a\u8fc7\u91cd\u65b0\u5ba1\u89c6\u57fa\u672c\u7684\u76f8\u673a\u7406\u8bba\u5e76\u91c7\u7528\u5148\u8fdb\u7684\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6280\u672f\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u5bf9\u8fde\u7eed\u76f8\u673a\u8f68\u8ff9\u7684\u7cbe\u786e\u5efa\u6a21\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9a\u91cf\u548c\u5b9a\u6027\u5730\u8bc1\u660e\u4e86\u5176\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002|\n", "2406.18214": "|**2024-07-29**|[Trimming the Fat: Efficient Compression of 3D Gaussian Splats through Pruning](http://arxiv.org/abs/2406.18214)|**[link](https://github.com/salmanali96/trimming-the-fat)**|\u8fd1\u5e74\u6765\uff0c\u7531\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\u548c\u6700\u8fd1\u51fa\u73b0\u76843D\u9ad8\u65af\u6e32\u67d3\uff083DGS\uff09\u6a21\u578b\u63d0\u4f9b\u4e86\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u80fd\u529b\uff0c3D\u6a21\u578b\u7684\u4f7f\u7528\u5f97\u5230\u4e86\u63a8\u5e7f\u3002\u540e\u8005\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u56e0\u4e3a\u5b83\u672c\u8eab\u5c31\u6613\u4e8e\u5728\u8bad\u7ec3\u671f\u95f4\u5feb\u901f\u6536\u655b\u5e76\u63d0\u4f9b\u5e7f\u6cdb\u7684\u53ef\u7f16\u8f91\u6027\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5173\u4e8e\u8fd9\u4e9b\u6a21\u578b\u53ef\u6269\u5c55\u6027\u7684\u6587\u732e\u4ecd\u7136\u5904\u4e8e\u8d77\u6b65\u9636\u6bb5\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u91c7\u53d6\u4e86\u4e00\u4e9b\u521d\u6b65\u6b65\u9aa4\u6765\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5c55\u793a\u4e86\u4e00\u79cd\u80fd\u591f\u5b9e\u73b0\u6b64\u7c7b\u6a21\u578b\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u7684\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u201cTrimming the fat\u201d\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u540e\u9a8c\u8fed\u4ee3\u526a\u679d\u6280\u672f\uff0c\u7528\u4e8e\u6d88\u9664\u6a21\u578b\u4e2d\u7f16\u7801\u7684\u5197\u4f59\u4fe1\u606f\u3002\u6211\u4eec\u5728\u5e7f\u6cdb\u8ba4\u53ef\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u5728\u4fdd\u6301\u751a\u81f3\u6539\u8fdb\u57fa\u7ebf\u6027\u80fd\u7684\u540c\u65f6\uff0c\u53ef\u4ee5\u5220\u9664\u9ad8\u8fbe75%\u7684\u9ad8\u65af\u51fd\u6570\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7ea650\u500d\u7684\u538b\u7f29\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u4f3c\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u5c06\u8ba1\u7b97\u901f\u5ea6\u63d0\u9ad8\u5230600 FPS\u3002|\n", "2406.15149": "|**2024-06-21**|[Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks](http://arxiv.org/abs/2406.15149)|null|\u6a21\u62df\u5668\u662f\u81ea\u4e3b\u673a\u5668\u4eba\u5b66\u4e60\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u56e0\u4e3a\u5b83\u4eec\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6570\u636e\u751f\u6210\u3001\u7075\u6d3b\u7684\u8bbe\u8ba1\u548c\u8f68\u8ff9\u4f18\u5316\u3002\u7136\u800c\uff0c\u5c06\u4ece\u4eff\u771f\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u7684\u884c\u4e3a\u8fc1\u79fb\u5230\u73b0\u5b9e\u4e16\u754c\u4e2d\u88ab\u8bc1\u660e\u662f\u56f0\u96be\u7684\uff0c\u901a\u5e38\u9700\u8981\u901a\u8fc7\u8ba1\u7b97\u91cf\u5927\u7684\u57df\u968f\u673a\u5316\u65b9\u6cd5\u6216\u8fdb\u4e00\u6b65\u7684\u6a21\u578b\u5fae\u8c03\u6765\u7f13\u89e3\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\u6765\u63d0\u9ad8\u4eff\u771f\u5230\u771f\u5b9e\u89c6\u89c9\u56db\u65cb\u7ffc\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u5206\u5e03\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u5c06\u9ad8\u65af splatting \u4e0e\u56db\u65cb\u7ffc\u98de\u884c\u52a8\u529b\u5b66\u76f8\u7ed3\u5408\u6765\u6784\u5efa\u6a21\u62df\u5668\uff0c\u7136\u540e\u4f7f\u7528 Liquid \u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u9c81\u68d2\u7684\u5bfc\u822a\u7b56\u7565\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u6211\u4eec\u83b7\u5f97\u4e86\u5b8c\u6574\u7684\u6a21\u4eff\u5b66\u4e60\u534f\u8bae\uff0c\u8be5\u534f\u8bae\u7ed3\u5408\u4e86 3D \u9ad8\u65af splatting \u8f90\u5c04\u573a\u6e32\u67d3\u7684\u8fdb\u6b65\u3001\u4e13\u5bb6\u6f14\u793a\u8bad\u7ec3\u6570\u636e\u7684\u5de7\u5999\u7f16\u7a0b\u4ee5\u53ca Liquid \u7f51\u7edc\u7684\u4efb\u52a1\u7406\u89e3\u80fd\u529b\u3002\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9a\u91cf\u98de\u884c\u6d4b\u8bd5\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5728\u5355\u4e2a\u6a21\u62df\u573a\u666f\u4e2d\u5b66\u4e60\u5230\u7684\u5bfc\u822a\u6280\u80fd\u53ef\u4ee5\u76f4\u63a5\u7a33\u5065\u5730\u8fc1\u79fb\u5230\u73b0\u5b9e\u4e16\u754c\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5c55\u793a\u4e86\u5728\u5267\u70c8\u7684\u5206\u5e03\u548c\u7269\u7406\u73af\u5883\u53d8\u5316\u4e0b\u4fdd\u6301\u8bad\u7ec3\u73af\u5883\u4e4b\u5916\u6027\u80fd\u7684\u80fd\u529b\u3002\u6211\u4eec\u5b66\u4e60\u7684 Liquid \u7b56\u7565\uff0c\u4ec5\u5728\u4ece\u903c\u771f\u7684\u6a21\u62df\u5ba4\u5185\u98de\u884c\u4e2d\u7cbe\u9009\u7684\u5355\u76ee\u6807\u673a\u52a8\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u53ef\u4ee5\u63a8\u5e7f\u5230\u6237\u5916\u771f\u5b9e\u786c\u4ef6\u5e73\u53f0\u4e0a\u7684\u591a\u6b65\u8fdc\u8db3\u3002|\n", "2406.10373": "|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|\u5728\u975e\u7ed3\u6784\u5316\u7684\u65c5\u6e38\u73af\u5883\u4e2d\u62cd\u6444\u7684\u7167\u7247\u7ecf\u5e38\u8868\u73b0\u51fa\u591a\u53d8\u7684\u5916\u89c2\u548c\u77ed\u6682\u7684\u906e\u6321\uff0c\u8fd9\u5bf9\u7cbe\u786e\u7684\u573a\u666f\u91cd\u5efa\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u5e76\u5728\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u5bfc\u81f4\u4e86\u4f2a\u5f71\u3002\u867d\u7136\u5148\u524d\u7684\u65b9\u6cd5\u5df2\u7ecf\u5c06\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u4e0e\u5176\u4ed6\u53ef\u5b66\u4e60\u6a21\u5757\u76f8\u7ed3\u5408\u6765\u5904\u7406\u52a8\u6001\u5916\u89c2\u5e76\u6d88\u9664\u77ac\u6001\u5bf9\u8c61\uff0c\u4f46\u5176\u5927\u91cf\u7684\u8bad\u7ec3\u9700\u6c42\u548c\u7f13\u6162\u7684\u6e32\u67d3\u901f\u5ea6\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u6700\u8fd1\uff0c3D \u9ad8\u65af splatting (3DGS) \u5df2\u6210\u4e3a NeRF \u7684\u4e00\u79cd\u5f88\u6709\u524d\u9014\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5b83\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u4ee5\u53ca\u66f4\u597d\u7684\u6e32\u67d3\u8d28\u91cf\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 Wild-GS\uff0c\u8fd9\u662f\u4e00\u79cd\u9488\u5bf9\u4e0d\u53d7\u7ea6\u675f\u7684\u7167\u7247\u96c6\u5408\u800c\u4f18\u5316\u7684 3DGS \u521b\u65b0\u6539\u7f16\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5176\u6548\u7387\u4f18\u52bf\u3002Wild-GS \u901a\u8fc7\u6bcf\u4e2a 3D \u9ad8\u65af\u7684\u56fa\u6709\u6750\u8d28\u5c5e\u6027\u3001\u6bcf\u5f20\u56fe\u50cf\u7684\u5168\u5c40\u7167\u660e\u548c\u76f8\u673a\u5c5e\u6027\u4ee5\u53ca\u9010\u70b9\u53cd\u5c04\u7387\u7684\u5c40\u90e8\u65b9\u5dee\u6765\u786e\u5b9a\u5176\u5916\u89c2\u3002\u4e0e\u5148\u524d\u5728\u56fe\u50cf\u7a7a\u95f4\u4e2d\u5bf9\u53c2\u8003\u7279\u5f81\u8fdb\u884c\u5efa\u6a21\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cWild-GS \u901a\u8fc7\u5bf9\u4ece\u53c2\u8003\u56fe\u50cf\u4e2d\u63d0\u53d6\u7684\u4e09\u5e73\u9762\u8fdb\u884c\u91c7\u6837\uff0c\u5c06\u50cf\u7d20\u5916\u89c2\u7279\u5f81\u663e\u5f0f\u5bf9\u9f50\u5230\u76f8\u5e94\u7684\u5c40\u90e8\u9ad8\u65af\u3002\u8fd9\u79cd\u65b0\u9896\u7684\u8bbe\u8ba1\u6709\u6548\u5730\u5c06\u53c2\u8003\u89c6\u56fe\u7684\u9ad8\u9891\u7ec6\u8282\u5916\u89c2\u8f6c\u79fb\u5230 3D \u7a7a\u95f4\uff0c\u5e76\u663e\u7740\u52a0\u5feb\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u5229\u7528 2D \u53ef\u89c1\u6027\u56fe\u548c\u6df1\u5ea6\u6b63\u5219\u5316\u5206\u522b\u51cf\u8f7b\u77ac\u6001\u6548\u5e94\u5e76\u7ea6\u675f\u51e0\u4f55\u5f62\u72b6\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cWild-GS \u5728\u6240\u6709\u73b0\u6709\u6280\u672f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6e32\u67d3\u6027\u80fd\u4ee5\u53ca\u6700\u9ad8\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002|\n", "2406.04253": "|**2024-06-06**|[A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation](http://arxiv.org/abs/2406.04253)|null|3D modeling has long been an important area in computer vision and computer graphics. Recently, thanks to the breakthroughs in neural representations and generative models, we witnessed a rapid development of 3D modeling. 3D human modeling, lying at the core of many real-world applications, such as gaming and animation, has attracted significant attention. Over the past few years, a large body of work on creating 3D human avatars has been introduced, forming a new and abundant knowledge base for 3D human modeling. The scale of the literature makes it difficult for individuals to keep track of all the works. This survey aims to provide a comprehensive overview of these emerging techniques for 3D human avatar modeling, from both reconstruction and generation perspectives. Firstly, we review representative methods for 3D human reconstruction, including methods based on pixel-aligned implicit function, neural radiance field, and 3D Gaussian Splatting, etc. We then summarize representative methods for 3D human generation, especially those using large language models like CLIP, diffusion models, and various 3D representations, which demonstrate state-of-the-art performance. Finally, we discuss our reflection on existing methods and open challenges for 3D human avatar modeling, shedding light on future research.||\n", "2406.02720": "|**2024-06-13**|[3D-HGS: 3D Half-Gaussian Splatting](http://arxiv.org/abs/2406.02720)|**[link](https://github.com/lihaolin88/3d-half-gaussian-splatting)**|\u7167\u7247\u7ea7\u903c\u771f\u7684\u4e09\u7ef4\u91cd\u5efa\u662f\u4e09\u7ef4\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\u3002\u7531\u4e8e\u6700\u8fd1\u795e\u7ecf\u6e32\u67d3\u6280\u672f\u7684\u51fa\u73b0\uff0c\u8be5\u9886\u57df\u53d6\u5f97\u4e86\u76f8\u5f53\u5927\u7684\u8fdb\u6b65\u3002\u8fd9\u4e9b\u6280\u672f\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5b66\u4e60\u4e09\u7ef4\u573a\u666f\u7684\u4f53\u79ef\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u6e32\u67d3\u5f97\u5230\u7684\u635f\u5931\u51fd\u6570\u6765\u7ec6\u5316\u8fd9\u4e9b\u8868\u793a\u3002\u5176\u4e2d\uff0c\u4e09\u7ef4\u9ad8\u65af\u6563\u5c04\uff083D-GS\uff09\u5df2\u6210\u4e3a\u4e00\u79cd\u91cd\u8981\u7684\u65b9\u6cd5\uff0c\u5176\u6027\u80fd\u8d85\u8fc7\u4e86\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRFs\uff09\u30023D-GS\u4f7f\u7528\u53c2\u6570\u5316\u7684\u4e09\u7ef4\u9ad8\u65af\u51fd\u6570\u6765\u5efa\u6a21\u7a7a\u95f4\u4f4d\u7f6e\u548c\u989c\u8272\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u56fe\u5757\u7684\u5feb\u901f\u6e32\u67d3\u6280\u672f\u3002\u5c3d\u7ba1\u5176\u6e32\u67d3\u6027\u80fd\u548c\u901f\u5ea6\u90fd\u5f88\u51fa\u8272\uff0c\u4f46\u4f7f\u7528\u4e09\u7ef4\u9ad8\u65af\u6838\u51fd\u6570\u5728\u51c6\u786e\u8868\u793a\u4e0d\u8fde\u7eed\u51fd\u6570\u65b9\u9762\u5b58\u5728\u56fa\u6709\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u5f62\u72b6\u4e0d\u8fde\u7eed\u7684\u8fb9\u7f18\u548c\u89d2\u843d\uff0c\u4ee5\u53ca\u5728\u989c\u8272\u4e0d\u8fde\u7eed\u7684\u4e0d\u540c\u7eb9\u7406\u4e4b\u95f4\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u8bae\u91c7\u7528\u4e09\u7ef4\u534a\u9ad8\u65af\uff083D-HGS\uff09\u6838\u51fd\u6570\uff0c\u5b83\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u6838\u51fd\u6570\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u4eec\u80fd\u591f\u63d0\u9ad8\u5f53\u524d\u4e0e3D-GS\u76f8\u5173\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u5728\u4e0d\u5f71\u54cd\u6e32\u67d3\u901f\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6e32\u67d3\u6027\u80fd\u3002||\n", "2409.03213": "|**2024-09-05**|[Optimizing 3D Gaussian Splatting for Sparse Viewpoint Scene Reconstruction](http://arxiv.org/abs/2409.03213)|null|\u4e09\u7ef4\u9ad8\u65af\u6e32\u67d3 (3DGS) \u5df2\u6210\u4e3a\u4e00\u79cd\u5f88\u6709\u524d\u666f\u7684\u4e09\u7ef4\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u4e0e\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u76f8\u6bd4\uff0c\u5b83\u53ef\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002\u7136\u800c\uff0c3DGS \u5bb9\u6613\u53d7\u5230\u9ad8\u9891\u4f2a\u5f71\u7684\u5f71\u54cd\uff0c\u5e76\u4e14\u5728\u7a00\u758f\u89c6\u70b9\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5e94\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86 SVS-GS\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u7a00\u758f\u89c6\u70b9\u573a\u666f\u91cd\u5efa\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u96c6\u6210\u4e86\u4e09\u7ef4\u9ad8\u65af\u5e73\u6ed1\u6ee4\u6ce2\u5668\u6765\u6291\u5236\u4f2a\u5f71\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u7ed3\u5408\u4e86\u6df1\u5ea6\u68af\u5ea6\u5256\u9762\u5148\u9a8c (DGPP) \u635f\u5931\u548c\u52a8\u6001\u6df1\u5ea6\u63a9\u7801\u6765\u9510\u5316\u8fb9\u7f18\uff0c\u5e76\u7ed3\u5408\u4e86\u5206\u6570\u84b8\u998f\u91c7\u6837 (SDS) \u635f\u5931\u7684\u4e8c\u7ef4\u6269\u6563\u6765\u589e\u5f3a\u65b0\u89c6\u56fe\u5408\u6210\u4e2d\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u5728 MipNeRF-360 \u548c SeaThru-NeRF \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cSVS-GS \u663e\u7740\u6539\u5584\u4e86\u7a00\u758f\u89c6\u70b9\u4e0b\u7684\u4e09\u7ef4\u91cd\u5efa\uff0c\u4e3a\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u4e2d\u7684\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5065\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002|\n", "2409.06407": "|**2024-09-10**|[Sources of Uncertainty in 3D Scene Reconstruction](http://arxiv.org/abs/2409.06407)|**[link](https://github.com/aaltoml/uncertainty-nerf-gs)**|\u4e09\u7ef4\u573a\u666f\u91cd\u5efa\u8fc7\u7a0b\u4f1a\u53d7\u5230\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u4f17\u591a\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u7684\u5f71\u54cd\u3002\u867d\u7136\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u548c\u4e09\u7ef4\u9ad8\u65af splatting (GS) \u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u6e32\u67d3\uff0c\u4f46\u5b83\u4eec\u7f3a\u4e4f\u5185\u7f6e\u673a\u5236\u6765\u76f4\u63a5\u89e3\u51b3\u6216\u91cf\u5316\u7531\u566a\u58f0\u3001\u906e\u6321\u3001\u6df7\u6dc6\u5f02\u5e38\u503c\u548c\u4e0d\u7cbe\u786e\u7684\u76f8\u673a\u59ff\u6001\u8f93\u5165\u5f15\u8d77\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u5206\u7c7b\u6cd5\uff0c\u5bf9\u8fd9\u4e9b\u65b9\u6cd5\u4e2d\u56fa\u6709\u7684\u4e0d\u540c\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u8fdb\u884c\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4f7f\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6280\u672f\u6269\u5c55\u4e86\u57fa\u4e8e NeRF \u548c GS \u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u5b66\u4e60\u4e0d\u786e\u5b9a\u6027\u8f93\u51fa\u548c\u96c6\u6210\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\u4ee5\u8bc4\u4f30\u5b83\u4eec\u6355\u6349\u91cd\u5efa\u654f\u611f\u6027\u7684\u80fd\u529b\u3002\u6211\u4eec\u7684\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u8bbe\u8ba1\u57fa\u4e8e NeRF/GS \u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\u65f6\uff0c\u9700\u8981\u89e3\u51b3\u5404\u79cd\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u7684\u9700\u6c42\u3002|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2409.02838": "|**2024-09-04**|[iConFormer: Dynamic Parameter-Efficient Tuning with Input-Conditioned Adaptation](http://arxiv.org/abs/2409.02838)|null|\u57fa\u4e8e\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u5b8c\u6574\u5fae\u8c03\uff08FFT\uff09\u548c\u4efb\u52a1\u7279\u5b9a\u89e3\u7801\u5668\u7684\u8fc1\u79fb\u5b66\u4e60\u968f\u7740\u6df1\u5ea6\u6a21\u578b\u7684\u6307\u6570\u7ea7\u589e\u957f\u800c\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\u3002\u4f7f\u7528\u7531\u5c0f\u578b\u53ef\u5b66\u4e60\u5c42\u7ec4\u6210\u7684\u9002\u914d\u5668\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u5df2\u6210\u4e3a FFT \u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u8bad\u7ec3\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u53ef\u6bd4\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u9002\u914d\u5668\u5bf9\u8f93\u5165\u5b9e\u4f8b\u7684\u4e0d\u7075\u6d3b\u9650\u5236\u4e86\u5176\u5728\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u4fe1\u606f\u7684\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 PEFT \u65b9\u6cd5\uff0c\u5373\u8f93\u5165\u6761\u4ef6\u5316\u7684 Transformer\uff0c\u79f0\u4e3a iConFormer\uff0c\u5b83\u5229\u7528\u4e86\u4ee5\u8f93\u5165\u5b9e\u4f8b\u4e3a\u6761\u4ef6\u7684\u52a8\u6001\u9002\u914d\u5668\u3002\u4e3a\u4e86\u786e\u4fdd\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5bf9\u8f93\u5165\u5b9e\u4f8b\u7684\u7075\u6d3b\u5b66\u4e60\u80fd\u529b\uff0c\u6211\u4eec\u5728\u52a8\u6001\u9002\u914d\u5668\u4e2d\u5f15\u5165\u4e86\u8f93\u5165\u6761\u4ef6\u5316\u7f51\u7edc\uff08iCoN\uff09\uff0c\u4ece\u800c\u5b9e\u73b0\u5b9e\u4f8b\u7ea7\u7279\u5f81\u8f6c\u6362\u3002\u5177\u4f53\u6765\u8bf4\uff0ciCoN \u4e3a\u6bcf\u4e2a\u7279\u5f81\u751f\u6210\u901a\u9053\u7ea7\u7684\u5377\u79ef\u6838\uff0c\u5e76\u4f7f\u7528\u81ea\u9002\u5e94\u5377\u79ef\u8fc7\u7a0b\u5bf9\u5176\u8fdb\u884c\u8f6c\u6362\uff0c\u4ee5\u6709\u6548\u6355\u83b7\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u4efb\u52a1\u7279\u5b9a\u548c\u7ec6\u7c92\u5ea6\u7ec6\u8282\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u4ec5\u8c03\u6574 Transformer \u4e3b\u5e72\u53c2\u6570\u7684 1.6% \u5230 2.8%\uff0ciConFormer \u5728\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u548c\u8bed\u4e49\u5206\u5272\u65b9\u9762\u5b9e\u73b0\u4e86\u4e0e FFT \u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u5b9e\u4f8b\u5206\u5272\u65b9\u9762\u4f18\u4e8e FFT\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6240\u6709\u4e0a\u8ff0\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u8fd1\u7684 PEFT \u65b9\u6cd5\u3002||\n", "2409.02546": "|**2024-09-04**|[Real-Time Dynamic Scale-Aware Fusion Detection Network: Take Road Damage Detection as an example](http://arxiv.org/abs/2409.02546)|null|\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u9053\u8def\u635f\u574f\u68c0\u6d4b (RDD) \u5bf9\u57ce\u5e02\u7684\u65e5\u5e38\u7ef4\u62a4\u548c\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u663e\u8457\u964d\u4f4e\u52b3\u52a8\u529b\u6210\u672c\u65b9\u9762\u3002\u7136\u800c\uff0c\u5f53\u524d\u57fa\u4e8e\u65e0\u4eba\u673a\u7684 RDD \u7814\u7a76\u4ecd\u9762\u4e34\u8bb8\u591a\u6311\u6218\u3002\u4f8b\u5982\uff0c\u5f62\u72b6\u548c\u65b9\u5411\u4e0d\u89c4\u5219\u7684\u635f\u574f\u3001\u80cc\u666f\u5bf9\u635f\u574f\u7684\u906e\u6321\u4ee5\u53ca\u96be\u4ee5\u533a\u5206\u635f\u574f\u548c\u80cc\u666f\uff0c\u8fd9\u4e9b\u56e0\u7d20\u90fd\u663e\u8457\u5f71\u54cd\u4e86\u65e0\u4eba\u673a\u5728\u65e5\u5e38\u5de1\u68c0\u4e2d\u68c0\u6d4b\u9053\u8def\u635f\u574f\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u63d0\u9ad8\u65e0\u4eba\u673a\u5b9e\u65f6\u9053\u8def\u635f\u574f\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u6211\u4eec\u8bbe\u8ba1\u5e76\u63d0\u51fa\u4e86\u4e09\u4e2a\u76f8\u5e94\u7684\u6a21\u5757\uff1a\u4e00\u4e2a\u80fd\u591f\u7075\u6d3b\u9002\u5e94\u5f62\u72b6\u548c\u80cc\u666f\u7684\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff1b\u4e00\u4e2a\u878d\u5408\u591a\u5c3a\u5ea6\u611f\u77e5\u5e76\u9002\u5e94\u5f62\u72b6\u548c\u80cc\u666f\u7684\u6a21\u5757\uff1b\u4e00\u4e2a\u9ad8\u6548\u7684\u4e0b\u91c7\u6837\u6a21\u5757\u3002 \u57fa\u4e8e\u8fd9\u4e9b\u6a21\u5757\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5177\u6709\u81ea\u52a8\u53bb\u9664\u80cc\u666f\u5e72\u6270\u80fd\u529b\u7684\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u9053\u8def\u635f\u574f\u68c0\u6d4b\u6a21\u578b\uff0c\u79f0\u4e3a\u52a8\u6001\u5c3a\u5ea6\u611f\u77e5\u878d\u5408\u68c0\u6d4b\u6a21\u578b (RT-DSAFDet)\u3002\u5728 UAV-PDD2023 \u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b RT-DSAFDet \u7684 mAP50 \u8fbe\u5230\u4e86 54.2%\uff0c\u6bd4\u6700\u65b0\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u6a21\u578b YOLOv10 \u7684\u9ad8\u6548\u53d8\u4f53 YOLOv10-m \u9ad8 11.1%\uff0c\u800c\u53c2\u6570\u91cf\u51cf\u5c11\u5230 1.8M\uff0cFLOPs \u51cf\u5c11\u5230 4.6G\uff0c\u5206\u522b\u964d\u4f4e\u4e86 88% \u548c 93%\u3002\u6b64\u5916\uff0c\u5728\u5927\u578b\u901a\u7528\u76ee\u6807\u68c0\u6d4b\u516c\u5f00\u6570\u636e\u96c6 MS COCO2017 \u4e0a\u4e5f\u5c55\u73b0\u4e86\u6211\u4eec\u6a21\u578b\u7684\u4f18\u8d8a\u6027\uff0c\u5176 mAP50-95 \u4e0e YOLOv9-t \u76f8\u540c\uff0c\u4f46 mAP50 \u9ad8\u51fa 0.5%\uff0c\u53c2\u6570\u91cf\u51cf\u5c11 10%\uff0cFLOPs \u51cf\u5c11 40%\u3002||\n", "2409.02486": "|**2024-09-04**|[Boosting Generalizability towards Zero-Shot Cross-Dataset Single-Image Indoor Depth by Meta-Initialization](http://arxiv.org/abs/2409.02486)|null|\u5ba4\u5185\u673a\u5668\u4eba\u7684\u5bfc\u822a\u6216\u969c\u788d\u7269\u68c0\u6d4b\u7b49\u4efb\u52a1\u4f9d\u8d56\u4e8e\u6df1\u5ea6\u4fe1\u606f\uff0c\u800c\u5355\u56fe\u50cf\u6df1\u5ea6\u4f30\u8ba1\u88ab\u5e7f\u6cdb\u7528\u4e8e\u8f85\u52a9\u611f\u77e5\u3002\u5927\u591a\u6570\u5ba4\u5185\u5355\u56fe\u50cf\u6df1\u5ea6\u9884\u6d4b\u8f83\u5c11\u5173\u6ce8\u6a21\u578b\u5bf9\u672a\u89c1\u6570\u636e\u96c6\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u66f4\u5173\u6ce8\u7cfb\u7edf\u90e8\u7f72\u7684\u91ce\u5916\u9c81\u68d2\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u5229\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u5143\u5b66\u4e60\u5728\u96f6\u6837\u672c\u8de8\u6570\u636e\u96c6\u63a8\u7406\u4e2d\u83b7\u5f97\u66f4\u9ad8\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4e0e\u7814\u7a76\u6700\u591a\u7684\u3001\u4e0e\u663e\u5f0f\u7c7b\u522b\u6807\u7b7e\u76f8\u5173\u7684\u56fe\u50cf\u5206\u7c7b\u5143\u5b66\u4e60\u4e0d\u540c\uff0c\u5bf9\u4e8e\u4e0e\u7269\u4f53\u6392\u5217\u548c\u573a\u666f\u6784\u6210\u65b9\u9762\u9ad8\u5ea6\u53d8\u5316\u7684\u5ba4\u5185\u73af\u5883\u76f8\u5173\u7684\u8fde\u7eed\u6df1\u5ea6\u503c\uff0c\u4e0d\u5b58\u5728\u660e\u786e\u7684\u4efb\u52a1\u8fb9\u754c\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u7ec6\u7c92\u5ea6\u4efb\u52a1\uff0c\u5728\u6211\u4eec\u7684\u5143\u5b66\u4e60\u516c\u5f0f\u4e2d\u5c06\u6bcf\u4e2aRGB-D\u5c0f\u6279\u91cf\u89c6\u4e3a\u4e00\u4e2a\u4efb\u52a1\u3002\u6211\u4eec\u9996\u5148\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6709\u9650\u6570\u636e\u4e0a\u8bf1\u5bfc\u51fa\u66f4\u597d\u7684\u5148\u9a8c\uff08RMSE \u6700\u9ad8\u964d\u4f4e 27.8%\uff09\u3002\u7136\u540e\uff0c\u5728\u5143\u5b66\u4e60\u521d\u59cb\u5316\u4e0a\u8fdb\u884c\u5fae\u8c03\u59cb\u7ec8\u4f18\u4e8e\u6ca1\u6709\u5143\u65b9\u6cd5\u7684\u57fa\u7ebf\u3002\u4e3a\u4e86\u5b9e\u73b0\u6cdb\u5316\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u96f6\u6837\u672c\u8de8\u6570\u636e\u96c6\u534f\u8bae\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7531\u6211\u4eec\u7684\u5143\u521d\u59cb\u5316\u8bf1\u5bfc\u7684\u66f4\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u4f5c\u4e3a\u8bb8\u591a\u73b0\u6709\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u7684\u7b80\u5355\u800c\u6709\u7528\u7684\u63d2\u4ef6\u3002\u6df1\u5ea6\u548c\u5143\u5b66\u4e60\u4ea4\u53c9\u9886\u57df\u7684\u5de5\u4f5c\u6709\u53ef\u80fd\u63a8\u52a8\u8fd9\u4e24\u9879\u7814\u7a76\u66f4\u63a5\u8fd1\u5b9e\u9645\u7684\u673a\u5668\u4eba\u548c\u673a\u5668\u611f\u77e5\u5e94\u7528\u3002||\n", "2409.02329": "|**2024-09-03**|[Site Selection for the Second Flyeye Telescope: A Simulation Study for Optimizing Near-Earth Object Discovery](http://arxiv.org/abs/2409.02329)|null|\u6b27\u6d32\u822a\u5929\u5c40 (ESA) \u6b63\u5728\u5f00\u53d1\u4e00\u4e2a\u540d\u4e3a Flyeye \u7684\u5e7f\u57df\u5de1\u5929\u671b\u8fdc\u955c\u7f51\u7edc\uff0c\u4ee5\u6539\u8fdb\u8fd1\u5730\u5929\u4f53 (NEO) \u7684\u53d1\u73b0\u3002\u8be5\u7f51\u7edc\u4e2d\u7684\u7b2c\u4e00\u4e2a\u671b\u8fdc\u955c\u5c06\u4f4d\u4e8e\u5317\u534a\u7403\u7684\u7a46\u6cd5\u62c9\u5c71\uff08\u610f\u5927\u5229\uff09\uff0c\u800c\u7b2c\u4e8c\u4e2a\u5177\u6709\u589e\u5f3a\u63a2\u6d4b\u80fd\u529b\u7684 Flyeye \u671b\u8fdc\u955c\u521a\u521a\u5f00\u59cb\u5173\u952e\u8bbe\u8ba1\u9636\u6bb5\u3002\u901a\u8fc7\u5bf9\u649e\u51fb\u8f68\u8ff9\u4e0a\u7684\u8fd1\u5730\u5929\u4f53\u8fdb\u884c\u6a21\u62df\uff0c\u7814\u7a76\u4e86\u7b2c\u4e8c\u4e2a Flyeye \u671b\u8fdc\u955c\u7684\u6f5c\u5728\u4f4d\u7f6e\u3002\u5bf9\u5927\u7ea6 3000 \u4e2a\u649e\u51fb\u5c0f\u884c\u661f\uff08\u7edd\u5bf9\u661f\u7b49\u4e3a H=25 \u548c H=28\uff09\u8fdb\u884c\u4e86\u4f20\u64ad\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e3b\u8981\u73b0\u6709\u5de1\u5929\u9879\u76ee\uff08Catalina\u3001Pan-STARRS\u3001ATLAS\uff09\u3001\u5373\u5c06\u6295\u5165\u4f7f\u7528\u7684\u8587\u62c9\u00b7\u9c81\u5bbe\u5929\u6587\u53f0 (LSST) \u4ee5\u53ca Flyeye \u53ef\u80fd\u9009\u5740\u7684\u53ef\u63a2\u6d4b\u6027\u3002 \u8003\u8651\u4e86\u667a\u5229\u3001\u5357\u975e\u548c\u5317\u534a\u7403\u7684\u7b2c\u4e8c\u4e2a\u8bbe\u65bd\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u5929\u6587\u53f0\uff0c\u5728\u6a21\u62df\u4e2d\u90fd\u8003\u8651\u4e86\u5b83\u4eec\u8fc7\u53bb\u6216\u8ba1\u5212\u7684\u6307\u5411\u7b56\u7565\u3002\u5728 LSST \u90e8\u7f72\u4e4b\u524d\uff0c\u5357\u534a\u7403\u7684\u4e00\u4e2a Flyeye \u7684\u6027\u80fd\u4e0e\u5317\u534a\u7403\u7684\u4e00\u4e2a\u671b\u8fdc\u955c\u76f8\u4f3c\u3002\u7ed3\u5408\u8d77\u6765\uff0c\u5728\u5317\u65b9\u548c\u5357\u65b9\u5404\u653e\u7f6e\u4e00\u53f0\u671b\u8fdc\u955c\u53ef\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u63a2\u6d4b\u7387\u548c\u63a2\u6d4b\u5230\u7684\u72ec\u7279\u7269\u4f53\u7684\u6570\u91cf\u3002LSST \u4e4b\u540e\uff0c\u5357\u90e8\u548c\u5317\u90e8\u7684 Flyeye \u671b\u8fdc\u955c\u4ecd\u7136\u662f\u4e92\u8865\u7684\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6a21\u62df\u8868\u660e\uff0c\u65e0\u8bba\u662f\u5728 LSST \u4e4b\u524d\u8fd8\u662f\u4e4b\u540e\uff0c\u4f4d\u4e8e\u5357\u90e8\u7684\u7b2c\u4e8c\u4e2a Flyeye \u90fd\u53ef\u4ee5\u8865\u5145\u4f4d\u4e8e\u5317\u90e8\u7684 Flyeye \u671b\u8fdc\u955c\u3002\u4f4d\u4e8e\u62c9\u897f\u62c9\u7684 Flyeye \u5c06\u5229\u7528\u5176\u4f18\u8d8a\u7684\u5927\u6c14\u6761\u4ef6\uff0c\u540c\u65f6\u5e73\u8861\u5357\u5317\u534a\u7403\u7684\u8d44\u4ea7\u3002||\n", "2409.02281": "|**2024-09-03**|[K-Origins: Better Colour Quantification for Neural Networks](http://arxiv.org/abs/2409.02281)|**[link](https://github.com/lewismmason/Thesis-Public)**|K-Origins\u662f\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u5c42\uff0c\u65e8\u5728\u5728\u5b66\u4e60\u989c\u8272\u6216\u5f3a\u5ea6\u6709\u5229\u65f6\u63d0\u9ad8\u57fa\u4e8e\u56fe\u50cf\u7684\u7f51\u7edc\u6027\u80fd\u3002 \u8d85\u8fc7 250 \u4e2a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5377\u79ef\u7f51\u7edc\u5728 16 \u4f4d\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0cK-Origins \u63d0\u9ad8\u4e86\u8bed\u4e49\u5206\u5272\u7cbe\u5ea6\uff1a\u4f4e\u4fe1\u566a\u6bd4\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u4ee5\u53ca\u5206\u5272\u5f62\u72b6\u76f8\u540c\u4f46\u989c\u8272\u4e0d\u540c\u7684\u591a\u4e2a\u76ee\u6807\u3002 \u5bf9\u4e8e\u6bcf\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570$w_k$\uff0cK-Origins \u901a\u8fc7\u516c\u5f0f $\\textbf{Y}_k = \\textbf{X}-\\textbf{J}\\cdot w_k$ \u4ece\u8f93\u5165\u7279\u5f81 $\\textbf{X}$ \u751f\u6210\u8f93\u51fa\u7279\u5f81\uff0c\u5176\u4e2d $\\textbf{J}$ \u662f\u4e00\u4e2a\u5168 1 \u77e9\u9635\u3002 \u6b64\u5916\uff0c\u8fd8\u8bad\u7ec3\u4e86\u5177\u6709\u4e0d\u540c\u611f\u53d7\u91ce\u7684\u7f51\u7edc\uff0c\u4ee5\u6839\u636e\u76ee\u6807\u7c7b\u522b\u7684\u7ef4\u5ea6\u786e\u5b9a\u6700\u4f73\u7f51\u7edc\u6df1\u5ea6\uff0c\u8fd9\u8868\u660e\u611f\u53d7\u91ce\u957f\u5ea6\u5e94\u8d85\u8fc7\u76ee\u6807\u5927\u5c0f\u3002 \u901a\u8fc7\u786e\u4fdd\u8db3\u591f\u7684\u611f\u53d7\u91ce\u957f\u5ea6\u5e76\u7ed3\u5408 K-Origins\uff0c\u6211\u4eec\u53ef\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u8bed\u4e49\u7f51\u7edc\u6027\u80fd\u3002||\n", "2409.02278": "|**2024-09-03**|[Evaluation and Comparison of Visual Language Models for Transportation Engineering Problems](http://arxiv.org/abs/2409.02278)|null|\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\u5c55\u73b0\u51fa\u5176\u5728\u56fe\u50cf\u7406\u89e3\u76f8\u5173\u5e94\u7528\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u4ea4\u901a\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4f8b\u5982\u56fe\u50cf\u5206\u7c7b\u548c\u76ee\u6807\u68c0\u6d4b\u3002\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u5305\u62ec\u62e5\u5835\u68c0\u6d4b\u548c\u88c2\u7f1d\u8bc6\u522b\uff0c\u800c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u5219\u7528\u4e8e\u8bc6\u522b\u672a\u4f69\u6234\u5934\u76d4\u7684\u884c\u4e3a\u3002\u6211\u4eec\u5e94\u7528\u4e86\u5f00\u6e90\u6a21\u578b\uff08\u5982CLIP\u3001BLIP\u3001OWL-ViT\u3001Llava-Next\uff09\u548c\u95ed\u6e90\u6a21\u578bGPT-4o\uff0c\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4ee5\u5229\u7528\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u6765\u5b8c\u6210\u57fa\u4e8e\u89c6\u89c9\u7684\u4ea4\u901a\u4efb\u52a1\u3002\u8fd9\u4e9b\u4efb\u52a1\u901a\u8fc7\u5bf9VLM\u6a21\u578b\u5e94\u7528\u96f6\u6837\u672c\u63d0\u793a\u6765\u5b8c\u6210\uff0c\u56e0\u4e3a\u96f6\u6837\u672c\u63d0\u793a\u53ef\u4ee5\u5728\u4e0d\u5bf9\u4efb\u52a1\u8fdb\u884c\u4efb\u4f55\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u4efb\u52a1\u3002\u8fd9\u6d88\u9664\u4e86\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u6807\u6ce8\u6570\u636e\u96c6\u6216\u5fae\u8c03\u7684\u9700\u6c42\u3002\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4e0e\u57fa\u51c6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u4f46\u5728\u76ee\u6807\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4ecd\u6709\u6539\u8fdb\u7684\u7a7a\u95f4\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u5bf9\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u7a81\u51fa\u4e86\u8fd9\u4e9b\u6a21\u578b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u672a\u6765\u6539\u8fdb\u548c\u5e7f\u6cdb\u5b9e\u65bd\u7684\u57fa\u51c6\u3002||\n", "2409.02035": "|**2024-09-03**|[A Modern Take on Visual Relationship Reasoning for Grasp Planning](http://arxiv.org/abs/2409.02035)|null|\u4e0e\u73b0\u5b9e\u4e16\u754c\u6742\u4e71\u573a\u666f\u4ea4\u4e92\u5bf9\u673a\u5668\u4eba\u4ee3\u7406\u63d0\u51fa\u4e86\u82e5\u5e72\u6311\u6218\uff0c\u8fd9\u4e9b\u4ee3\u7406\u9700\u8981\u7406\u89e3\u89c2\u5bdf\u5230\u7684\u7269\u4f53\u4e4b\u95f4\u590d\u6742\u7684\u7684\u7a7a\u95f4\u4f9d\u8d56\u6027\uff0c\u4ee5\u786e\u5b9a\u6700\u4f73\u62fe\u53d6\u987a\u5e8f\u6216\u6709\u6548\u7684\u7269\u4f53\u68c0\u7d22\u7b56\u7565\u3002 \u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u7ba1\u7406\u7b80\u5316\u7684\u573a\u666f\uff0c\u5e76\u4fa7\u91cd\u4e8e\u5728\u521d\u59cb\u7269\u4f53\u68c0\u6d4b\u9636\u6bb5\u4e4b\u540e\u9884\u6d4b\u6210\u5bf9\u7269\u4f53\u5173\u7cfb\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u5168\u5c40\u4e0a\u4e0b\u6587\u6216\u96be\u4ee5\u5904\u7406\u5197\u4f59\u548c\u7f3a\u5931\u7684\u7269\u4f53\u5173\u7cfb\u3002 \u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6293\u53d6\u89c4\u5212\u7684\u89c6\u89c9\u5173\u7cfb\u63a8\u7406\u7684\u73b0\u4ee3\u65b9\u6cd5\u3002 \u6211\u4eec\u4ecb\u7ecd\u4e86 D3GD\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5176\u4e2d\u5305\u62ec\u5305\u542b\u6765\u81ea 97 \u4e2a\u4e0d\u540c\u7c7b\u522b\u7684\u591a\u8fbe 35 \u4e2a\u7269\u4f53\u7684\u5206\u62e3\u573a\u666f\u3002 \u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86 D3G\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u7aef\u5230\u7aef transformer \u7684\u4f9d\u8d56\u56fe\u751f\u6210\u6a21\u578b\uff0c\u5b83\u53ef\u4ee5\u540c\u65f6\u68c0\u6d4b\u7269\u4f53\u5e76\u751f\u6210\u8868\u793a\u5176\u7a7a\u95f4\u5173\u7cfb\u7684\u90bb\u63a5\u77e9\u9635\u3002 \u8ba4\u8bc6\u5230\u6807\u51c6\u6307\u6807\u7684\u5c40\u9650\u6027\uff0c\u6211\u4eec\u9996\u6b21\u91c7\u7528\u5173\u7cfb\u5e73\u5747\u7cbe\u5ea6\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u57fa\u51c6\u6d4b\u8bd5\u3002 \u83b7\u5f97\u7684\u7ed3\u679c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u8fd9\u9879\u4efb\u52a1\u7684\u6700\u65b0\u6280\u672f\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002 \u6211\u4eec\u5728 https://paolotron.github.io/d3g.github.io \u4e0a\u516c\u5f00\u53d1\u5e03\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002||\n", "2409.01988": "|**2024-09-03**|[Compressed learning based onboard semantic compression for remote sensing platforms](http://arxiv.org/abs/2409.01988)|null|\u5730\u7403\u89c2\u6d4b (EO) \u5728\u521b\u5efa\u548c\u7ef4\u6301\u4e00\u4e2a\u5177\u6709\u5f39\u6027\u548c\u7e41\u8363\u7684\u793e\u4f1a\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u8fd9\u5bf9\u6240\u6709\u751f\u547d\u548c\u5730\u7403\u672c\u8eab\u90fd\u5177\u6709\u6df1\u8fdc\u7684\u5f71\u54cd\u3002\u536b\u661f\u3001\u822a\u7a7a\u5e73\u53f0\u4ee5\u53ca\u6700\u8fd1\u7684\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u9a7e\u9a76\u98de\u884c\u5668\u7b49\u9065\u611f\u5e73\u53f0\u90fd\u7528\u4e8e EO\u3002\u5b83\u4eec\u6536\u96c6\u5927\u91cf\u6570\u636e\uff0c\u9700\u8981\u5c06\u5176\u4e0b\u4f20\u5230\u5730\u7403\u8fdb\u884c\u8fdb\u4e00\u6b65\u5904\u7406\u548c\u5206\u6790\u3002\u8fd9\u79cd\u9ad8\u541e\u5410\u91cf\u91c7\u96c6\u7684\u74f6\u9888\u662f\u4e0b\u884c\u94fe\u8def\u5e26\u5bbd\u3002\u9700\u8981\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u56fe\u50cf\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u6765\u5e94\u5bf9\u8fd9\u79cd\u6d77\u91cf\u6570\u636e\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u901a\u8fc7\u538b\u7f29\u5b66\u4e60\u6846\u67b6\u7814\u7a76\u4e86\u8bed\u4e49\u538b\u7f29\uff0c\u8be5\u6846\u67b6\u4ec5\u5229\u7528\u5feb\u901f\u548c\u7a00\u758f\u7684\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\u6765\u7f16\u7801\u6570\u636e\u3002\u76f8\u673a\u566a\u58f0\u548c\u901a\u4fe1\u4fe1\u9053\u662f\u9020\u6210\u5931\u771f\u7684\u4e3b\u8981\u6765\u6e90\u3002\u7136\u540e\uff0c\u5b8c\u6574\u7684\u8bed\u4e49\u901a\u4fe1\u7ba1\u9053\u7531\u4e00\u4e2a\u5b66\u4e60\u5230\u7684\u4f4e\u590d\u6742\u5ea6\u538b\u7f29\u77e9\u9635\u7ec4\u6210\uff0c\u8be5\u77e9\u9635\u4f5c\u7528\u4e8e\u566a\u58f0\u76f8\u673a\u8f93\u51fa\uff0c\u4ee5\u5728\u673a\u8f7d\u751f\u6210\u4e00\u4e2a\u89c2\u6d4b\u5411\u91cf\uff0c\u8be5\u5411\u91cf\u901a\u8fc7\u901a\u4fe1\u4fe1\u9053\u4e0b\u884c\u94fe\u8def\u4f20\u8f93\uff0c\u901a\u8fc7\u5c55\u5f00\u7f51\u7edc\u5904\u7406\uff0c\u7136\u540e\u9988\u9001\u5230\u6267\u884c\u5fc5\u8981\u4e0b\u6e38\u4efb\u52a1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1b\u7814\u7a76\u4e86\u56fe\u50cf\u5206\u7c7b\u3002\u901a\u8fc7\u4f7f\u7528\u5c0f\u6ce2\u7a00\u758f\u5148\u9a8c\u5c55\u5f00 NA-ALISTA \u7684\u5c42\u6765\u8865\u507f\u5931\u771f\u3002\u56e0\u6b64\uff0c\u89e3\u7801\u662f\u4e00\u79cd\u6839\u636e\u76f8\u673a/\u73af\u5883\u4fe1\u606f\u548c\u4e0b\u6e38\u4efb\u52a1\u8bbe\u8ba1\u7684\u5373\u63d2\u5373\u7528\u65b9\u6cd5\u3002\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u901a\u8fc7\u7aef\u5230\u7aef\u65b9\u5f0f\u7684\u635f\u5931\u51fd\u6570\u4e0e\u538b\u7f29\u77e9\u9635\u548c\u5c55\u5f00\u7f51\u7edc\u8054\u5408\u5fae\u8c03\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4f4e\u538b\u7f29\u6bd4\u7684\u566a\u58f0\u73af\u5883\u4e2d\uff0c\u6dfb\u52a0\u6062\u590d\u635f\u5931\u4ee5\u53ca\u4efb\u52a1\u76f8\u5173\u635f\u5931\u53ef\u4ee5\u63d0\u9ad8\u4e0b\u6e38\u6027\u80fd\u3002||\n", "2409.01872": "|**2024-09-03**|[Latent Distillation for Continual Object Detection at the Edge](http://arxiv.org/abs/2409.01872)|**[link](https://github.com/pastifra/Continual_Nanodet)**|\u867d\u7136\u5728\u76ee\u6807\u68c0\u6d4b\u6587\u732e\u4e2d\u5b58\u5728\u8bb8\u591a\u6027\u80fd\u5353\u8d8a\u7684\u65b9\u6cd5\uff0c\u4f46\u89e3\u51b3\u6570\u636e\u5206\u5e03\u504f\u79fb\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u4e3a\u8fd9\u4e2a\u95ee\u9898\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u9002\u5e94\u65b0\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5148\u524d\u6570\u636e\u7684\u6027\u80fd\u3002\u8fd9\u5bf9\u4e8e\u8fb9\u7f18\u8bbe\u5907\u5c24\u5176\u91cd\u8981\uff0c\u8fd9\u4e9b\u8bbe\u5907\u5728\u6c7d\u8f66\u548c\u673a\u5668\u4eba\u7b49\u52a8\u6001\u73af\u5883\u4e2d\u5f88\u5e38\u89c1\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u76ee\u6807\u68c0\u6d4b\u6301\u7eed\u5b66\u4e60\uff08CLOD\uff09\u573a\u666f\u4e2d\u8fb9\u7f18\u8bbe\u5907\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u9650\u5236\u3002\u5177\u4f53\u6765\u8bf4\uff0c\uff08i\uff09\u6211\u4eec\u7814\u7a76\u4e86\u4e00\u79cd\u5f00\u6e90\u3001\u8f7b\u91cf\u7ea7\u548c\u5feb\u901f\u7684\u68c0\u6d4b\u5668 NanoDet \u5bf9\u8fb9\u7f18\u8bbe\u5907\u4e0a CLOD \u7684\u9002\u7528\u6027\uff0c\u6539\u8fdb\u4e86\u6587\u732e\u4e2d\u4f7f\u7528\u7684\u8f83\u5927\u67b6\u6784\u3002\u6b64\u5916\uff0c\uff08ii\uff09\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6f5c\u5728\u84b8\u998f\uff08LD\uff09\u7684\u65b0\u578b CL \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u663e\u7740\u5f71\u54cd\u68c0\u6d4b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u4e86\u6700\u5148\u8fdb\u7684 CL \u65b9\u6cd5\u6240\u9700\u7684\u8fd0\u7b97\u6b21\u6570\u548c\u5185\u5b58\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u8457\u540d\u7684 VOC \u548c COCO \u57fa\u51c6\u6d4b\u8bd5\u96c6\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u4e0e\u5176\u4ed6\u84b8\u998f\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6bcf\u6b21\u6a21\u578b\u66f4\u65b0\u53ef\u5c06\u84b8\u998f\u53c2\u6570\u5f00\u9500\u51cf\u5c11 74%\uff0c\u5c06\u6d6e\u70b9\u8fd0\u7b97\uff08FLOPs\uff09\u51cf\u5c11 56%\u3002||\n", "2409.01816": "|**2024-09-03**|[GeoBEV: Learning Geometric BEV Representation for Multi-view 3D Object Detection](http://arxiv.org/abs/2409.01816)|null|\u9e1f\u77b0\u56fe (BEV) \u8868\u793a\u5df2\u6210\u4e3a\u591a\u89c6\u56fe 3D \u5bf9\u8c61\u68c0\u6d4b\u7684\u4e3b\u6d41\u8303\u5f0f\uff0c\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u611f\u77e5\u80fd\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86 BEV \u8868\u793a\u7684\u51e0\u4f55\u8d28\u91cf\uff0c\u4f7f\u5176\u5904\u4e8e\u4f4e\u5206\u8fa8\u7387\u72b6\u6001\uff0c\u65e0\u6cd5\u6062\u590d\u573a\u666f\u771f\u5b9e\u7684\u51e0\u4f55\u4fe1\u606f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u5148\u524d\u65b9\u6cd5\u53d7\u9650\u4e8e\u4f4e BEV \u8868\u793a\u5206\u8fa8\u7387\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u5f84\u5411-\u7b1b\u5361\u5c14 BEV \u91c7\u6837 (RC-Sampling)\uff0c\u4ece\u800c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u5bc6\u96c6 BEV \u8868\u793a\uff0c\u800c\u65e0\u9700\u590d\u6742\u7684\u7b97\u5b50\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u76d2\u5185\u6807\u7b7e\u6765\u66ff\u4ee3\u4ece\u6fc0\u5149\u96f7\u8fbe\u70b9\u751f\u6210\u7684\u4f20\u7edf\u6df1\u5ea6\u6807\u7b7e\u3002\u6b64\u6807\u7b7e\u53cd\u6620\u4e86\u5bf9\u8c61\u7684\u5b9e\u9645\u51e0\u4f55\u7ed3\u6784\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5b83\u4eec\u7684\u8868\u9762\uff0c\u5c06\u73b0\u5b9e\u4e16\u754c\u7684\u51e0\u4f55\u4fe1\u606f\u6ce8\u5165 BEV \u8868\u793a\u4e2d\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u76d2\u5185\u6807\u7b7e\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u8d28\u5fc3\u611f\u77e5\u5185\u90e8\u635f\u5931 (CAI \u635f\u5931) \u6765\u6355\u6349\u5bf9\u8c61\u7684\u7ec6\u7c92\u5ea6\u5185\u90e8\u51e0\u4f55\u7ed3\u6784\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c06\u4e0a\u8ff0\u6a21\u5757\u96c6\u6210\u5230\u4e00\u4e2a\u540d\u4e3a GeoBEV \u7684\u65b0\u578b\u591a\u89c6\u56fe 3D \u5bf9\u8c61\u68c0\u6d4b\u6846\u67b6\u4e2d\u3002\u5728 nuScenes \u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGeoBEV \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7a81\u51fa\u4e86\u5176\u6709\u6548\u6027\u3002||\n", "2409.03530": "|**2024-09-05**|[Use of triplet loss for facial restoration in low-resolution images](http://arxiv.org/abs/2409.03530)|null|\u8fd1\u5e74\u6765\uff0c\u4eba\u8138\u8bc6\u522b (FR) \u6a21\u578b\u5df2\u6210\u4e3a\u5e94\u7528\u6700\u5e7f\u6cdb\u7684\u751f\u7269\u8bc6\u522b\u5de5\u5177\uff0c\u5728\u4f17\u591a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u6210\u679c\u3002\u7136\u800c\uff0c\u786c\u4ef6\u7684\u56fa\u6709\u6311\u6218\u6216\u62cd\u6444\u8ddd\u79bb often \u5bfc\u81f4\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u8fd9\u4f1a\u4e25\u91cd\u5f71\u54cd\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4eba\u4eec\u63d0\u51fa\u4e86\u51e0\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u751f\u6210\u9ad8\u5ea6\u903c\u771f\u7684\u4eba\u8138\u7684\u8d85\u5206\u8fa8\u7387 (SR) \u6a21\u578b\u3002\u5c3d\u7ba1\u505a\u51fa\u4e86\u8fd9\u4e9b\u52aa\u529b\uff0c\u4f46\u4eba\u8138\u8bc6\u522b\u7b97\u6cd5\u5e76\u672a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8d85\u5206\u8fa8\u7387\u6a21\u578b FTLGAN\uff0c\u5b83\u4fa7\u91cd\u4e8e\u751f\u6210\u4fdd\u7559\u4e2a\u4eba\u8eab\u4efd\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\uff0c\u4ece\u800c\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u7684\u6027\u80fd\u3002\u7ed3\u679c\u4ee4\u4eba\u4fe1\u670d\uff0c\u8868\u660e d' \u7684\u5e73\u5747\u503c\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u9ad8\u51fa 21%\uff0c\u5177\u4f53\u800c\u8a00\uff0c14x14 \u50cf\u7d20\u65f6 d' = 1.099\uff0cAUC = 0.78\uff0c28x28 \u50cf\u7d20\u65f6 d' = 2.112\uff0cAUC = 0.92\uff0c56x56 \u50cf\u7d20\u65f6 d' = 3.049\uff0cAUC = 0.98\u3002\u8fd9\u9879\u7814\u7a76\u7684\u8d21\u732e\u5728\u51e0\u4e2a\u5173\u952e\u9886\u57df\u610f\u4e49\u91cd\u5927\u3002\u9996\u5148\uff0c\u5728\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff08\u7279\u522b\u662f 14x14\u300128x28 \u548c 56x56 \u50cf\u7d20\u7684\u5206\u8fa8\u7387\uff09\u4e2d\uff0c\u4eba\u8138\u8bc6\u522b\u6027\u80fd\u53d6\u5f97\u4e86\u663e\u7740\u63d0\u9ad8\u3002\u5176\u6b21\uff0cFTLGAN \u6240\u5c55\u793a\u7684\u589e\u5f3a\u529f\u80fd\u5728\u6240\u6709\u5206\u8fa8\u7387\u4e0b\u90fd\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u54cd\u5e94\uff0c\u4e0e\u5176\u4ed6\u6bd4\u8f83\u6a21\u578b\u4e0d\u540c\uff0c\u5b83\u59cb\u7ec8\u5982\u4e00\u5730\u63d0\u4f9b\u51fa\u8272\u7684\u6027\u80fd\u3002\u7b2c\u4e09\uff0c\u4f7f\u7528\u4e09\u5143\u7ec4\u635f\u5931\u903b\u8f91\u5b9e\u65bd\u4e86\u4e00\u79cd\u521b\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u4ec5\u4f7f\u7528\u771f\u5b9e\u56fe\u50cf\u8bad\u7ec3\u8d85\u5206\u8fa8\u7387\u6a21\u578b\uff0c\u8fd9\u4e0e\u5f53\u524d\u6a21\u578b\u5f62\u6210\u5bf9\u6bd4\uff0c\u5e76\u6269\u5c55\u4e86\u6f5c\u5728\u7684\u73b0\u5b9e\u5e94\u7528\u3002\u6700\u540e\uff0c\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5728\u6a21\u578b\u8bad\u7ec3\u671f\u95f4\u5c06\u4eba\u8138\u8bc6\u522b\u8d28\u91cf\u4f5c\u4e3a\u635f\u5931\u7eb3\u5165\u5176\u4e2d\uff0c\u4e13\u95e8\u89e3\u51b3\u4e86\u63d0\u9ad8\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5206\u7c7b\u6027\u80fd\u7684\u6311\u6218\u3002||\n", "2409.03521": "|**2024-09-05**|[Have Large Vision-Language Models Mastered Art History?](http://arxiv.org/abs/2409.03521)|null|\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u51fa\u73b0\u6700\u8fd1\u5728\u8de8\u591a\u4e2a\u9886\u57df\u7684\u56fe\u50cf\u5206\u7c7b\u65b9\u9762\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002\u7136\u800c\uff0cVLM \u5728\u827a\u672f\u54c1\u5206\u7c7b\u8fd9\u4e00\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u7ed8\u753b\u827a\u672f\u98ce\u683c\u5206\u7c7b\u2014\u2014\u4f20\u7edf\u4e0a\u7531\u827a\u672f\u53f2\u5b66\u5bb6\u638c\u63e1\u7684\u9886\u57df\u2014\u2014\u5c1a\u672a\u5f97\u5230\u63a2\u7d22\u3002\u4e0e\u81ea\u7136\u56fe\u50cf\u76f8\u6bd4\uff0c\u827a\u672f\u54c1\u7531\u4e8e\u5176\u56fa\u6709\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u7ed3\u6784\uff08\u4ee5\u591a\u53d8\u7684\u6784\u56fe\u548c\u98ce\u683c\u4e3a\u7279\u5f81\uff09\u800c\u6784\u6210\u4e86\u72ec\u7279\u7684\u6311\u6218\u3002\u827a\u672f\u53f2\u5b66\u5bb6\u957f\u671f\u4ee5\u6765\u4e00\u76f4\u5728\u7814\u7a76\u827a\u672f\u54c1\u7684\u72ec\u7279\u65b9\u9762\uff0c\u800c\u98ce\u683c\u9884\u6d4b\u662f\u5176\u5b66\u79d1\u7684\u4e00\u4e2a\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u3002\u672c\u6587\u7814\u7a76\u4e86\u96c6\u6210\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u7684\u5927\u578b VLM \u662f\u5426\u53ef\u4ee5\u6709\u6548\u5730\u9884\u6d4b\u7ed8\u753b\u7684\u827a\u672f\u53f2\u5c5e\u6027\u3002\u6211\u4eec\u5bf9\u56db\u79cd VLM\uff08\u5373 CLIP\u3001LLaVA\u3001OpenFlamingo \u548c GPT-4o\uff09\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u91cd\u70b9\u5173\u6ce8\u4f7f\u7528\u4e24\u4e2a\u516c\u5171\u827a\u672f\u54c1\u57fa\u51c6\u5bf9\u827a\u672f\u98ce\u683c\u3001\u4f5c\u8005\u548c\u65f6\u95f4\u6bb5\u8fdb\u884c\u96f6\u6837\u672c\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86 ArTest\uff0c\u8fd9\u662f\u4e00\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u827a\u672f\u54c1\u6d4b\u8bd5\u96c6\uff0c\u5176\u4e2d\u5305\u62ec\u827a\u672f\u53f2\u5b66\u5bb6\u7814\u7a76\u7684\u5173\u952e\u7ed8\u753b\u4f5c\u54c1\u3002||\n", "2409.03516": "|**2024-09-05**|[LMLT: Low-to-high Multi-Level Vision Transformer for Image Super-Resolution](http://arxiv.org/abs/2409.03516)|**[link](https://github.com/jwgdmkj/lmlt)**|\u8fd1\u5e74\u6765\uff0c\u57fa\u4e8e\u89c6\u89c9Transformer (ViT) \u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5b58\u5728\u590d\u6742\u6027\u9ad8\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u63a8\u7406\u65f6\u95f4\u548c\u5185\u5b58\u4f7f\u7528\u91cf\u5927\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u673a\u5236(WSA) \u7684ViT\u6a21\u578b\u5728\u5904\u7406\u7a97\u53e3\u533a\u57df\u5916\u7684\u4fe1\u606f\u65f6\u9762\u4e34\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4f4e\u5230\u9ad8\u591a\u7ea7Transformer (LMLT)\uff0c\u5b83\u5bf9\u6bcf\u4e2a\u5934\u91c7\u7528\u4e0d\u540c\u7279\u5f81\u5927\u5c0f\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002LMLT \u6cbf\u901a\u9053\u7ef4\u5ea6\u5212\u5206\u56fe\u50cf\u7279\u5f81\uff0c\u9010\u6e10\u51cf\u5c0f\u4f4e\u5c42\u5934\u7684\u7a7a\u95f4\u5927\u5c0f\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u5934\u5e94\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3002\u8fd9\u79cd\u65b9\u6cd5\u6709\u6548\u5730\u6355\u83b7\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u3002\u901a\u8fc7\u5c06\u4f4e\u5c42\u5934\u7684\u7ed3\u679c\u6574\u5408\u5230\u9ad8\u5c42\u5934\u4e2d\uff0cLMLT \u514b\u670d\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u7a97\u53e3\u8fb9\u754c\u95ee\u9898\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u4fdd\u6301\u751a\u81f3\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u57fa\u4e8e ViT \u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u7684\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\u548c GPU \u5185\u5b58\u4f7f\u7528\u91cf\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/jwgdmkj/LMLT \u83b7\u53d6\u3002||\n", "2409.03458": "|**2024-09-05**|[Non-Uniform Illumination Attack for Fooling Convolutional Neural Networks](http://arxiv.org/abs/2409.03458)|**[link](https://github.com/Akshayjain97/Non-Uniform_Illumination)**|\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u867d\u7136\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u653b\u51fb\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u4eba\u7c7b\u5bb9\u6613\u8bc6\u522b\u7684\u5fae\u5c0f\u56fe\u50cf\u6270\u52a8\u65f6\u3002\u8fd9\u79cd\u5f31\u70b9\u901a\u5e38\u88ab\u79f0\u4e3a\u201c\u653b\u51fb\u201d\uff0c\u7a81\u663e\u4e86CNN\u7684\u9c81\u68d2\u6027\u6709\u9650\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u589e\u5f3a\u5176\u62b5\u6297\u6b64\u7c7b\u64cd\u7eb5\u7684\u80fd\u529b\u3002\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u975e\u5747\u5300\u7167\u660e\uff08NUI\uff09\u653b\u51fb\u6280\u672f\uff0c\u8be5\u6280\u672f\u4f7f\u7528\u4e0d\u540c\u7684NUI\u63a9\u7801\u5bf9\u56fe\u50cf\u8fdb\u884c\u7ec6\u5fae alteration\u3002\u6211\u4eec\u5728\u5e7f\u6cdb\u63a5\u53d7\u7684\u6570\u636e\u96c6\uff08\u5305\u62ecCIFAR10\u3001TinyImageNet\u548cCalTech256\uff09\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u91cd\u70b9\u5173\u6ce812\u79cd\u4e0d\u540cNUI\u653b\u51fb\u6a21\u578b\u7684\u56fe\u50cf\u5206\u7c7b\u3002\u8bc4\u4f30\u4e86VGG\u3001ResNet\u3001MobilenetV3-small\u548cInceptionV3\u6a21\u578b\u5bf9NUI\u653b\u51fb\u7684\u62b5\u6297\u529b\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cCNN\u6a21\u578b\u5728\u906d\u53d7NUI\u653b\u51fb\u65f6\uff0c\u5206\u7c7b\u7cbe\u5ea6\u5927\u5e45\u4e0b\u964d\uff0c\u8868\u660e\u5b83\u4eec\u5728\u975e\u5747\u5300\u7167\u660e\u4e0b\u7684\u8106\u5f31\u6027\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u79cd\u60c5\u51b5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u5fa1\u7b56\u7565\uff0c\u5c06\u901a\u8fc7\u65b0\u7684NUI\u53d8\u6362\u751f\u6210\u7684NUI\u653b\u51fb\u56fe\u50cf\u5305\u542b\u5230\u8bad\u7ec3\u96c6\u4e2d\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5f53CNN\u6a21\u578b\u9762\u5bf9\u53d7NUI\u653b\u51fb\u5f71\u54cd\u7684\u6270\u52a8\u56fe\u50cf\u65f6\uff0c\u5176\u6027\u80fd\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002\u8be5\u7b56\u7565\u65e8\u5728\u589e\u5f3aCNN\u6a21\u578b\u5bf9NUI\u653b\u51fb\u7684\u62b5\u6297\u529b\u3002||\n", "2409.03377": "|**2024-09-05**|[Raw Speech Enhancement with Deep State Space Modeling](http://arxiv.org/abs/2409.03377)|**[link](https://github.com/Brainchip-Inc/aTENNuate)**|\u6211\u4eec\u63d0\u51fa\u4e86 aTENNuate\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u7684\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u81ea\u7f16\u7801\u5668\uff0c\u4e13\u4e3a\u9ad8\u6548\u7684\u5728\u7ebf\u539f\u59cb\u8bed\u97f3\u589e\u5f3a\u800c\u914d\u7f6e\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u3002\u8be5\u7f51\u7edc\u7684\u6027\u80fd\u4e3b\u8981\u5728\u539f\u59cb\u8bed\u97f3\u53bb\u566a\u65b9\u9762\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5728\u8d85\u5206\u8fa8\u7387\u548c\u53bb\u91cf\u5316\u7b49\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u989d\u5916\u8bc4\u4f30\u3002\u6211\u4eec\u5728 VoiceBank + DEMAND \u548c Microsoft DNS1 \u5408\u6210\u6d4b\u8bd5\u96c6\u4e0a\u5bf9 aTENNuate \u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u8be5\u7f51\u7edc\u5728 PESQ \u5206\u6570\u3001\u53c2\u6570\u6570\u91cf\u3001MAC \u548c\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8e\u4ee5\u524d\u7684\u5b9e\u65f6\u53bb\u566a\u6a21\u578b\u3002\u5373\u4f7f\u4f5c\u4e3a\u539f\u59cb\u6ce2\u5f62\u5904\u7406\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4e5f\u80fd\u4fdd\u6301\u5bf9\u5e72\u51c0\u4fe1\u53f7\u7684\u9ad8\u4fdd\u771f\u5ea6\uff0c\u5e76\u4e14\u53ef\u542c\u89c1\u7684\u4f2a\u5f71\u6781\u5c11\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u5c06\u566a\u58f0\u8f93\u5165\u538b\u7f29\u81f3 4000Hz \u548c 4 \u4f4d\uff0c\u8be5\u6a21\u578b\u4ecd\u80fd\u4fdd\u6301\u826f\u597d\u7684\u6027\u80fd\uff0c\u8fd9\u8868\u660e\u5b83\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u5177\u6709\u4e00\u822c\u7684\u8bed\u97f3\u589e\u5f3a\u80fd\u529b\u3002||\n", "2409.03368": "|**2024-09-05**|[Training-free Conversion of Pretrained ANNs to SNNs for Low-Power and High-Performance Applications](http://arxiv.org/abs/2409.03368)|null|\u8109\u51b2\u795e\u7ecf\u7f51\u7edc (SNN) \u7531\u4e8e\u5176\u63a8\u7406\u901f\u5ea6\u5feb\u3001\u529f\u8017\u4f4e\u7b49\u4f18\u52bf\uff0c\u5df2\u6210\u4e3a\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc (ANN) \u7684\u4e00\u79cd\u5f88\u6709\u524d\u9014\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u8bad\u7ec3\u7b97\u6cd5\u963b\u788d\u4e86\u5b83\u4eec\u7684\u5e7f\u6cdb\u5e94\u7528\u3002\u73b0\u6709\u7684 SNN \u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\u6bd4 ANN \u9700\u8981\u66f4\u591a\u7684\u5185\u5b58\u548c\u65f6\u95f4\u3002\u5373\u4f7f\u662f\u5e38\u7528\u7684 ANN-SNN \u8f6c\u6362\u65b9\u6cd5\u4e5f\u9700\u8981\u91cd\u65b0\u8bad\u7ec3 ANN \u4ee5\u63d0\u9ad8\u8f6c\u6362\u6548\u7387\uff0c\u4ece\u800c\u4ea7\u751f\u989d\u5916\u7684\u8ba1\u7b97\u6210\u672c\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u514d\u8bad\u7ec3 ANN-SNN \u8f6c\u6362\u6d41\u7a0b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u9884\u5148\u8bad\u7ec3\u597d\u7684 ANN \u6a21\u578b\u76f4\u63a5\u8f6c\u6362\u4e3a\u9ad8\u6027\u80fd SNN\uff0c\u65e0\u9700\u989d\u5916\u7684\u8bad\u7ec3\u3002\u8be5\u8f6c\u6362\u6d41\u7a0b\u5305\u62ec\u4e00\u4e2a\u57fa\u4e8e\u5c40\u90e8\u5b66\u4e60\u7684\u9608\u503c\u5e73\u8861\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u8ba1\u7b97\u6700\u4f73\u9608\u503c\u5e76\u901a\u8fc7\u901a\u9053\u7f29\u653e\u5bf9\u9608\u503c\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8c03\u6574\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u6846\u67b6\u5728\u4e09\u4e2a\u5178\u578b\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u53ef\u6269\u5c55\u6027\uff1a\u56fe\u50cf\u5206\u7c7b\u3001\u8bed\u4e49\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\u3002\u8fd9\u5c55\u793a\u4e86\u5176\u5bf9\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u7684\u9002\u7528\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u8f6c\u6362\u540e\u7684 SNN \u7684\u80fd\u8017\uff0c\u8bc1\u660e\u4e86\u5b83\u4eec\u4e0e\u4f20\u7edf ANN \u76f8\u6bd4\u5177\u6709\u4f18\u8d8a\u7684\u4f4e\u529f\u8017\u4f18\u52bf\u3002\u6211\u4eec\u7684\u514d\u8bad\u7ec3\u7b97\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7a81\u51fa\u4e86\u5176\u5b9e\u7528\u6027\u548c\u6548\u7387\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u5f00\u6e90\u9884\u8bad\u7ec3 ANN \u6a21\u578b\u548c\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u7b80\u5316\u4e86 SNN \u7684\u90e8\u7f72\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u4f4e\u529f\u8017\u7684\u63a8\u7406\uff0c\u5e76\u4e14\u6027\u80fd\u635f\u5931\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u3002||\n", "2409.03320": "|**2024-09-05**|[YOLO-PPA based Efficient Traffic Sign Detection for Cruise Control in Autonomous Driving](http://arxiv.org/abs/2409.03320)|null|\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u9ad8\u6548\u3001\u51c6\u786e\u5730\u68c0\u6d4b\u4ea4\u901a\u6807\u5fd7\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8ddd\u79bb\u8d8a\u8fdc\uff0c\u4ea4\u901a\u6807\u5fd7\u8d8a\u5c0f\u3002\u73b0\u6709\u7684\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u5f88\u96be\u68c0\u6d4b\u5230\u8fd9\u4e9b\u5c0f\u5c3a\u5bf8\u7684\u6807\u5fd7\u3002\u6b64\u5916\uff0c\u8f66\u8f7d\u5d4c\u5165\u5f0f\u8bbe\u5907\u7684\u6027\u80fd\u9650\u5236\u4e86\u68c0\u6d4b\u6a21\u578b\u7684\u89c4\u6a21\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e YOLO PPA \u7684\u4ea4\u901a\u6807\u5fd7\u68c0\u6d4b\u7b97\u6cd5\u3002\u5728 GTSDB \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u539f\u59cb YOLO \u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5c06\u63a8\u7406\u6548\u7387\u63d0\u9ad8\u4e86 11.2%\uff0cmAP 50 \u4e5f\u63d0\u9ad8\u4e86 93.2%\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684 YOLO PPA \u7684\u6709\u6548\u6027\u3002||\n", "2409.03192": "|**2024-09-05**|[PEPL: Precision-Enhanced Pseudo-Labeling for Fine-Grained Image Classification in Semi-Supervised Learning](http://arxiv.org/abs/2409.03192)|null|\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u7684\u51fa\u73b0\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u6b65\u3002\u7136\u800c\uff0c\u8be6\u7ec6\u6807\u6ce8\u7684\u7f3a\u4e4f\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u83b7\u53d6\u9ad8\u8d28\u91cf\u6807\u8bb0\u6570\u636e\u7684\u6210\u672c\u9ad8\u6602\u6216\u8017\u65f6\u7684\u60c5\u51b5\u4e0b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e13\u4e3a\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u5185\u7684\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u8bbe\u8ba1\u7684\u7cbe\u5ea6\u589e\u5f3a\u578b\u4f2a\u6807\u7b7e\uff08PEPL\uff09\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4f2a\u6807\u7b7e\u6765\u5229\u7528\u4e30\u5bcc\u7684\u672a\u6807\u8bb0\u6570\u636e\uff0c\u8fd9\u4e9b\u4f2a\u6807\u7b7e\u901a\u8fc7\u4e24\u4e2a\u5173\u952e\u9636\u6bb5\u9010\u6b65\u7ec6\u5316\uff1a\u521d\u59cb\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u8bed\u4e49\u6df7\u5408\u4f2a\u6807\u7b7e\u751f\u6210\u3002\u8fd9\u4e9b\u9636\u6bb5\u5229\u7528\u7c7b\u6fc0\u6d3b\u56fe\uff08CAM\uff09\u6765\u51c6\u786e\u4f30\u8ba1\u8bed\u4e49\u5185\u5bb9\u5e76\u751f\u6210\u7ec6\u5316\u6807\u7b7e\uff0c\u8fd9\u4e9b\u6807\u7b7e\u6355\u83b7\u4e86\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6240\u9700\u7684\u57fa\u672c\u7ec6\u8282\u3002\u901a\u8fc7\u5173\u6ce8\u8bed\u4e49\u7ea7\u4fe1\u606f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u6807\u51c6\u6570\u636e\u589e\u5f3a\u548c\u56fe\u50cf\u6df7\u5408\u6280\u672f\u5728\u4fdd\u7559\u5173\u952e\u7ec6\u7c92\u5ea6\u7279\u5f81\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002\u6211\u4eec\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u76f8\u5bf9\u4e8e\u73b0\u6709\u534a\u76d1\u7763\u7b56\u7565\u7684\u663e\u8457\u6539\u8fdb\uff0c\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u5728https://github.com/TianSuya/SemiFG\u5f00\u6e90\u3002||\n", "2409.03137": "|**2024-09-05**|[The AdEMAMix Optimizer: Better, Faster, Older](http://arxiv.org/abs/2409.03137)|null|\u57fa\u4e8e\u52a8\u91cf\u7684\u4f18\u5316\u5668\u662f\u4f17\u591a\u673a\u5668\u5b66\u4e60\u5e94\u7528\u7684\u6838\u5fc3\u3002\u8fd9\u4e9b\u4f18\u5316\u5668\u901a\u5e38\u4f9d\u8d56\u4e8e\u68af\u5ea6\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747 (EMA)\uff0c\u5b83\u4f1a\u4ee5\u6307\u6570\u65b9\u5f0f\u8870\u51cf\u65e7\u68af\u5ea6\u5bf9\u5f53\u524d\u68af\u5ea6\u7684\u8d21\u732e\u3002\u8fd9\u662f\u56e0\u4e3a\u68af\u5ea6\u662f\u5c40\u90e8\u7684\u7ebf\u6027\u8fd1\u4f3c\uff0c\u5f53\u8fed\u4ee3\u70b9\u5728\u635f\u5931\u51fd\u6570\u66f2\u9762\u4e0a\u79fb\u52a8\u65f6\uff0c\u65e7\u68af\u5ea6\u7684\u76f8\u5173\u6027\u4f1a\u964d\u4f4e\u3002\u8fd9\u9879\u5de5\u4f5c\u5bf9\u4f7f\u7528\u5355\u4e2a EMA \u6765\u7d2f\u79ef\u8fc7\u53bb\u68af\u5ea6\u7684\u505a\u6cd5\u63d0\u51fa\u4e86\u8d28\u7591\uff0c\u5e76\u901a\u8fc7\u7ecf\u9a8c\u8bc1\u660e\u4e86\u8fd9\u79cd\u9009\u62e9\u53ef\u80fd\u662f\u6b21\u4f18\u7684\uff1a\u5355\u4e2a EMA \u65e0\u6cd5\u540c\u65f6\u5bf9\u6700\u8fd1\u7684\u68af\u5ea6\u8d4b\u4e88\u9ad8\u6743\u91cd\uff0c\u5e76\u5bf9\u8f83\u65e7\u7684\u68af\u5ea6\u8d4b\u4e88\u4e0d\u53ef\u5ffd\u7565\u7684\u6743\u91cd\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c2\u5bdf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 AdEMAMix\uff0c\u5b83\u662f\u5bf9 Adam \u4f18\u5316\u5668\u7684\u4e00\u79cd\u7b80\u5355\u4fee\u6539\uff0c\u5b83\u6df7\u5408\u4e86\u4e24\u4e2a EMA\uff0c\u4ee5\u66f4\u597d\u5730\u5229\u7528\u8fc7\u53bb\u7684\u68af\u5ea6\u3002\u6211\u4eec\u5728\u8bed\u8a00\u5efa\u6a21\u548c\u56fe\u50cf\u5206\u7c7b\u65b9\u9762\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u68af\u5ea6\u5728\u6570\u4e07\u6b65\u5185\u4ecd\u7136\u5177\u6709\u76f8\u5173\u6027\u3002\u5b83\u4eec\u6709\u52a9\u4e8e\u66f4\u5feb\u5730\u6536\u655b\uff0c\u5e76\u4e14\u901a\u5e38\u6536\u655b\u5230\u66f4\u4f4e\u7684\u6700\u5c0f\u503c\uff1a\u4f8b\u5982\uff0c\u4e00\u4e2a\u5728 1010 \u4ebf\u4e2a\u8bcd\u7b26\u4e0a\u8bad\u7ec3\u7684\u5177\u6709 13 \u4ebf\u4e2a\u53c2\u6570\u7684 AdEMAMix LLM \u7684\u6027\u80fd\u4e0e\u5728\u4e00\u4e2a 1970 \u4ebf\u4e2a\u8bcd\u7b26\u4e0a\u8bad\u7ec3\u7684 AdamW \u6a21\u578b\u76f8\u5f53\uff08+95%\uff09\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u51cf\u7f13\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6a21\u578b\u9057\u5fd8\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u9f13\u52b1\u8fdb\u4e00\u6b65\u63a2\u7d22\u5229\u7528\u8fc7\u53bb\u68af\u5ea6\u7684\u4e0d\u540c\u7c7b\u578b\u7684\u51fd\u6570\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f EMA\u3002||\n", "2409.03022": "|**2024-09-04**|[Boundless: Generating Photorealistic Synthetic Data for Object Detection in Urban Streetscapes](http://arxiv.org/abs/2409.03022)|**[link](https://github.com/zk2172-columbia/boundless)**|\u6211\u4eec\u4ecb\u7ecdBoundless\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u5bc6\u96c6\u7684\u57ce\u5e02\u8857\u666f\u4e2d\u5b9e\u73b0\u9ad8\u5ea6\u51c6\u786e\u7684\u76ee\u6807\u68c0\u6d4b\u7684\u903c\u771f\u5408\u6210\u6570\u636e\u751f\u6210\u7cfb\u7edf\u3002Boundless\u53ef\u4ee5\u7528\u81ea\u52a8\u5316\u548c\u53ef\u914d\u7f6e\u7684\u8fc7\u7a0b\u53d6\u4ee3\u5927\u89c4\u6a21\u7684\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u6536\u96c6\u548c\u624b\u52a8\u5730\u9762\u5b9e\u51b5\u76ee\u6807\u6ce8\u91ca\uff08\u6807\u8bb0\uff09\u3002Boundless\u57fa\u4e8e\u865a\u5e7b\u5f15\u64ce5 (UE5) \u57ce\u5e02\u793a\u4f8b\u9879\u76ee\uff0c\u5e76\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u7684\u7167\u660e\u548c\u573a\u666f\u53d8\u5316\u6761\u4ef6\u4e0b\u51c6\u786e\u6536\u96c63D\u8fb9\u754c\u6846\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u5728Boundless\u751f\u6210\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u4ece\u4e2d\u7a7a\u76f8\u673a\u83b7\u53d6\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u63a8\u7406\u65f6\u7684\u6027\u80fd\u3002\u6211\u4eec\u5c06Boundless\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u4e0eCARLA\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u89c2\u5bdf\u52307.8 mAP\u7684\u6539\u8fdb\u3002\u6211\u4eec\u53d6\u5f97\u7684\u7ed3\u679c\u652f\u6301\u4e86\u5408\u6210\u6570\u636e\u751f\u6210\u662f\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u7528\u4e8e\u8bad\u7ec3/\u5fae\u8c03\u7528\u4e8e\u57ce\u5e02\u573a\u666f\u7684\u53ef\u6269\u5c55\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u3002||\n", "2409.05650": "|**2024-09-09**|[Replay Consolidation with Label Propagation for Continual Object Detection](http://arxiv.org/abs/2409.05650)|null|\u76ee\u6807\u68c0\u6d4b\u662f\u4e00\u4e2a\u4e0e\u673a\u5668\u4eba\u6280\u672f\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u8bb8\u591a\u5e94\u7528\u9ad8\u5ea6\u76f8\u5173\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u95ee\u9898\u3002\u6301\u7eed\u5b66\u4e60 (CL) \u8003\u8651\u7684\u662f\u6a21\u578b\u5728\u4fdd\u7559\u5148\u524d\u83b7\u5f97\u7684\u77e5\u8bc6\u7684\u540c\u65f6\u9010\u6b65\u5b66\u4e60\u65b0\u4fe1\u606f\u7684\u8bbe\u7f6e\u3002\u8fd9\u5c24\u5176\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8bad\u7ec3\u65b0\u6570\u636e\u65f6\u5f80\u5f80\u4f1a\u707e\u96be\u6027\u5730\u5fd8\u8bb0\u65e7\u77e5\u8bc6\u3002\u7279\u522b\u662f\uff0c\u4e0e\u7528\u4e8e\u5206\u7c7b\u7684\u6301\u7eed\u5b66\u4e60\u76f8\u6bd4\uff0c\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\u7684\u6301\u7eed\u5b66\u4e60 (CLOD) \u5e26\u6765\u4e86\u989d\u5916\u7684\u56f0\u96be\u3002\u5728 CLOD \u4e2d\uff0c\u6765\u81ea\u5148\u524d\u4efb\u52a1\u7684\u56fe\u50cf\u53ef\u80fd\u5305\u542b\u672a\u77e5\u7684\u7c7b\u522b\uff0c\u8fd9\u4e9b\u7c7b\u522b\u53ef\u80fd\u4f1a\u5728\u672a\u6765\u7684\u4efb\u52a1\u4e2d\u91cd\u65b0\u51fa\u73b0\u5e76\u88ab\u6807\u8bb0\u3002\u8fd9\u4e9b\u7f3a\u5931\u7684\u6ce8\u91ca\u4f1a\u5bfc\u81f4\u57fa\u4e8e\u91cd\u653e\u7684\u65b9\u6cd5\u51fa\u73b0\u4efb\u52a1\u5e72\u6270\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u6587\u732e\u4e2d\u7684\u5927\u591a\u6570\u5de5\u4f5c\u90fd\u96c6\u4e2d\u5728\u57fa\u4e8e\u84b8\u998f\u7684\u65b9\u6cd5\u4e0a\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u53ea\u6709\u5728\u4e0d\u540c\u4efb\u52a1\u4e4b\u95f4\u5b58\u5728\u5f3a\u5927\u7684\u7c7b\u522b\u91cd\u53e0\u65f6\u624d\u6709\u6548\u3002\u4e3a\u4e86\u89e3\u51b3\u5f53\u524d\u65b9\u6cd5\u7684\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3 CLOD \u7684\u65b0\u6280\u672f\uff0c\u79f0\u4e3a\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\u7684\u6807\u7b7e\u4f20\u64ad\u91cd\u653e\u6574\u5408 (RCLPOD)\u3002\u57fa\u4e8e\u91cd\u653e\u65b9\u6cd5\uff0c\u6211\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u901a\u8fc7\u589e\u5f3a\u7f13\u51b2\u533a\u5185\u5b58\u6837\u672c\u6765\u907f\u514d\u4efb\u52a1\u5e72\u6270\u95ee\u9898\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728 CLOD \u6587\u732e\u4e2d\u7684\u73b0\u6709\u6280\u672f\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5176\u5728 VOC \u548c COCO \u7b49\u65e2\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002||\n", "2409.05564": "|**2024-09-09**|[LEROjD: Lidar Extended Radar-Only Object Detection](http://arxiv.org/abs/2409.05564)|**[link](https://github.com/rst-tu-dortmund/lerojd)**|\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u800c\u8a00\uff0c\u7cbe\u786e\u7684\u4e09\u7ef4\u7269\u4f53\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u6fc0\u5149\u96f7\u8fbe\u4f20\u611f\u5668\u975e\u5e38\u9002\u5408\u8fd9\u9879\u4efb\u52a1\uff0c\u4f46\u5b83\u4eec\u4ef7\u683c\u6602\u8d35\uff0c\u5e76\u4e14\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u5b58\u5728\u5c40\u9650\u6027\u30023+1D \u6210\u50cf\u96f7\u8fbe\u4f20\u611f\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u7531\u4e8e\u5176\u5206\u8fa8\u7387\u4f4e\u548c\u6d4b\u91cf\u566a\u58f0\u9ad8\u800c\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u7684 3+1D \u6210\u50cf\u96f7\u8fbe\u6570\u636e\u96c6\u5305\u62ec\u96f7\u8fbe\u548c\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\uff0c\u53ef\u4ee5\u6539\u8fdb\u8de8\u6a21\u6001\u6a21\u578b\u3002\u5c3d\u7ba1\u4e0d\u5e94\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u6fc0\u5149\u96f7\u8fbe\uff0c\u4f46\u5b83\u53ef\u4ee5\u5e2e\u52a9\u8bad\u7ec3\u4ec5\u4f7f\u7528\u96f7\u8fbe\u7684\u7269\u4f53\u68c0\u6d4b\u5668\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u4e24\u79cd\u5c06\u77e5\u8bc6\u4ece\u6fc0\u5149\u96f7\u8fbe\u57df\u8fc1\u79fb\u5230\u96f7\u8fbe\u57df\u548c\u4ec5\u4f7f\u7528\u96f7\u8fbe\u7684\u7269\u4f53\u68c0\u6d4b\u5668\u7684\u7b56\u7565\uff1a1. \u4f7f\u7528\u987a\u5e8f\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u7ec6\u5316\u7684\u591a\u9636\u6bb5\u8bad\u7ec3\uff0c\u4ee5\u53ca 2. \u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u3002\u5728\u591a\u9636\u6bb5\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u4e09\u79cd\u7ec6\u5316\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\uff0c\u5e73\u5747\u7cbe\u5ea6 (mAP) \u663e\u7740\u63d0\u9ad8\u4e86 4.2 \u4e2a\u767e\u5206\u70b9\uff0c\u901a\u8fc7\u4f7f\u7528\u6559\u5e08\u6a21\u578b\u7684\u6743\u91cd\u521d\u59cb\u5316\u5b66\u751f\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\uff0c\u5e73\u5747\u7cbe\u5ea6\u63d0\u9ad8\u4e86 3.9 \u4e2a\u767e\u5206\u70b9\u3002\u8fd9\u4e9b\u65b9\u6cd5\u7684\u4e3b\u8981\u4f18\u70b9\u662f\u5b83\u4eec\u9002\u7528\u4e8e\u5176\u4ed6 3D \u7269\u4f53\u68c0\u6d4b\u7f51\u7edc\uff0c\u800c\u65e0\u9700\u6539\u53d8\u5176\u67b6\u6784\uff0c\u6b63\u5982\u6211\u4eec\u901a\u8fc7\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u7269\u4f53\u68c0\u6d4b\u5668\u4e0a\u8fdb\u884c\u5206\u6790\u6240\u5c55\u793a\u7684\u90a3\u6837\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/rst-tu-dortmund/lerojd \u83b7\u53d6\u3002||\n", "2409.05162": "|**2024-09-08**|[Can OOD Object Detectors Learn from Foundation Models?](http://arxiv.org/abs/2409.05162)|**[link](https://github.com/cvmi-lab/syncood)**|Out-of-distribution (OOD) object detection is a challenging task due to the absence of open-set OOD data. Inspired by recent advancements in text-to-image generative models, such as Stable Diffusion, we study the potential of generative models trained on large-scale open-set data to synthesize OOD samples, thereby enhancing OOD object detection. We introduce SyncOOD, a simple data curation method that capitalizes on the capabilities of large foundation models to automatically extract meaningful OOD data from text-to-image generative models. This offers the model access to open-world knowledge encapsulated within off-the-shelf foundation models. The synthetic OOD samples are then employed to augment the training of a lightweight, plug-and-play OOD detector, thus effectively optimizing the in-distribution (ID)/OOD decision boundaries. Extensive experiments across multiple benchmarks demonstrate that SyncOOD significantly outperforms existing methods, establishing new state-of-the-art performance with minimal synthetic data usage.||\n", "2409.04999": "|**2024-09-08**|[Visual Grounding with Multi-modal Conditional Adaptation](http://arxiv.org/abs/2409.04999)|**[link](https://github.com/mr-bigworth/mmca)**|Visual grounding is the task of locating objects specified by natural language expressions. Existing methods extend generic object detection frameworks to tackle this task. They typically extract visual and textual features separately using independent visual and textual encoders, then fuse these features in a multi-modal decoder for final prediction. However, visual grounding presents unique challenges. It often involves locating objects with different text descriptions within the same image. Existing methods struggle with this task because the independent visual encoder produces identical visual features for the same image, limiting detection performance. Some recently approaches propose various language-guided visual encoders to address this issue, but they mostly rely solely on textual information and require sophisticated designs. In this paper, we introduce Multi-modal Conditional Adaptation (MMCA), which enables the visual encoder to adaptively update weights, directing its focus towards text-relevant regions. Specifically, we first integrate information from different modalities to obtain multi-modal embeddings. Then we utilize a set of weighting coefficients, which generated from the multimodal embeddings, to reorganize the weight update matrices and apply them to the visual encoder of the visual grounding model. Extensive experiments on four widely used datasets demonstrate that MMCA achieves significant improvements and state-of-the-art results. Ablation experiments further demonstrate the lightweight and efficiency of our method. Our source code is available at: https://github.com/Mr-Bigworth/MMCA.||\n", "2409.04979": "|**2024-09-08**|[RCBEVDet++: Toward High-accuracy Radar-Camera Fusion 3D Perception Network](http://arxiv.org/abs/2409.04979)|null|Perceiving the surrounding environment is a fundamental task in autonomous driving. To obtain highly accurate perception results, modern autonomous driving systems typically employ multi-modal sensors to collect comprehensive environmental data. Among these, the radar-camera multi-modal perception system is especially favored for its excellent sensing capabilities and cost-effectiveness. However, the substantial modality differences between radar and camera sensors pose challenges in fusing information. To address this problem, this paper presents RCBEVDet, a radar-camera fusion 3D object detection framework. Specifically, RCBEVDet is developed from an existing camera-based 3D object detector, supplemented by a specially designed radar feature extractor, RadarBEVNet, and a Cross-Attention Multi-layer Fusion (CAMF) module. Firstly, RadarBEVNet encodes sparse radar points into a dense bird's-eye-view (BEV) feature using a dual-stream radar backbone and a Radar Cross Section aware BEV encoder. Secondly, the CAMF module utilizes a deformable attention mechanism to align radar and camera BEV features and adopts channel and spatial fusion layers to fuse them. To further enhance RCBEVDet's capabilities, we introduce RCBEVDet++, which advances the CAMF through sparse fusion, supports query-based multi-view camera perception models, and adapts to a broader range of perception tasks. Extensive experiments on the nuScenes show that our method integrates seamlessly with existing camera-based 3D perception models and improves their performance across various perception tasks. Furthermore, our method achieves state-of-the-art radar-camera fusion results in 3D object detection, BEV semantic segmentation, and 3D multi-object tracking tasks. Notably, with ViT-L as the image backbone, RCBEVDet++ achieves 72.73 NDS and 67.34 mAP in 3D object detection without test-time augmentation or model ensembling.||\n", "2409.04975": "|**2024-09-08**|[PatchAlign:Fair and Accurate Skin Disease Image Classification by Alignment with Clinical Labels](http://arxiv.org/abs/2409.04975)|**[link](https://github.com/aayushmanace/patchalign24)**|\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u76ae\u80a4\u75c5\u53d8\u8bca\u65ad\u81ea\u52a8\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\u3002\u7136\u800c\uff0c\u5728\u90e8\u7f72\u8fd9\u4e9b\u6a21\u578b\u4e4b\u524d\uff0c\u9700\u8981\u89e3\u51b3\u5176\u9884\u6d4b\u4e2d\u5b58\u5728\u7684\u79cd\u65cf\u5dee\u5f02\u95ee\u9898\u3002\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a PatchAlign \u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0e\u76ae\u80a4\u75c5\u4e34\u5e8a\u6587\u672c\u8868\u5f81\u5bf9\u9f50\u6765\u63d0\u9ad8\u76ae\u80a4\u75c5\u56fe\u50cf\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u3002PatchAlign \u4f7f\u7528\u56fe\u6700\u4f18\u4f20\u8f93 (GOT) \u635f\u5931\u4f5c\u4e3a\u6b63\u5219\u5316\u5668\u6765\u6267\u884c\u8de8\u57df\u5bf9\u9f50\u3002\u5373\u4f7f\u5728\u8bad\u7ec3\u6837\u672c\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u83b7\u5f97\u7684\u8868\u5f81\u4e5f\u662f\u7a33\u5065\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u5f88\u597d\u5730\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u80a4\u8272\u3002\u4e3a\u4e86\u51cf\u5c11\u4e34\u5e8a\u76ae\u80a4\u75c5\u56fe\u50cf\u4e2d\u566a\u58f0\u548c\u4f2a\u5f71\u7684\u5f71\u54cd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u63a9\u7801\u56fe\u6700\u4f18\u4f20\u8f93\uff0c\u7528\u4e8e\u8de8\u57df\u5bf9\u9f50\uff0c\u8fdb\u4e00\u6b65\u6539\u5584\u4e86\u516c\u5e73\u6027\u6307\u6807\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u5177\u6709\u4e0d\u540c\u76ae\u80a4\u7c7b\u578b\u7684\u76ae\u80a4\u75c5\u53d8\u6570\u636e\u96c6\u4e0a\u5c06\u6211\u4eec\u7684\u6a21\u578b\u4e0e\u6700\u5148\u8fdb\u7684 FairDisCo \u8fdb\u884c\u4e86\u6bd4\u8f83\uff1aFitzpatrick17k \u548c Diverse Dermatology Images (DDI)\u3002\u4e0e FairDisCo \u76f8\u6bd4\uff0cPatchAlign \u5728 Fitzpatrick17k \u4e0a\u5c06\u76ae\u80a4\u75c5\u56fe\u50cf\u5206\u7c7b\u7684\u51c6\u786e\u6027\u63d0\u9ad8\u4e86 2.8%\uff08\u57df\u5185\uff09\u548c 6.2%\uff08\u8de8\u57df\uff09\uff0c\u5728 DDI \u4e0a\u63d0\u9ad8\u4e86 4.2%\uff08\u57df\u5185\uff09\u3002\u6b64\u5916\uff0c\u5b83\u6301\u7eed\u6539\u5584\u4e86\u4e0d\u540c\u80a4\u8272\u771f\u5b9e\u9633\u6027\u7387\u7684\u516c\u5e73\u6027\u3002\u7528\u4e8e\u5b9e\u73b0\u7684\u6e90\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b GitHub \u5b58\u50a8\u5e93\u4e2d\u83b7\u53d6\uff1ahttps://github.com/aayushmanace/PatchAlign24\uff0c\u53ef\u4ee5\u8f7b\u677e\u590d\u73b0\u548c\u8fdb\u4e00\u6b65\u8bd5\u9a8c\u3002||\n", "2409.04915": "|**2024-09-07**|[Activation Function Optimization Scheme for Image Classification](http://arxiv.org/abs/2409.04915)|null|Activation function has a significant impact on the dynamics, convergence, and performance of deep neural networks. The search for a consistent and high-performing activation function has always been a pursuit during deep learning model development. Existing state-of-the-art activation functions are manually designed with human expertise except for Swish. Swish was developed using a reinforcement learning-based search strategy. In this study, we propose an evolutionary approach for optimizing activation functions specifically for image classification tasks, aiming to discover functions that outperform current state-of-the-art options. Through this optimization framework, we obtain a series of high-performing activation functions denoted as Exponential Error Linear Unit (EELU). The developed activation functions are evaluated for image classification tasks from two perspectives: (1) five state-of-the-art neural network architectures, such as ResNet50, AlexNet, VGG16, MobileNet, and Compact Convolutional Transformer which cover computationally heavy to light neural networks, and (2) eight standard datasets, including CIFAR10, Imagenette, MNIST, Fashion MNIST, Beans, Colorectal Histology, CottonWeedID15, and TinyImageNet which cover from typical machine vision benchmark, agricultural image applications to medical image applications. Finally, we statistically investigate the generalization of the resultant activation functions developed through the optimization scheme. With a Friedman test, we conclude that the optimization scheme is able to generate activation functions that outperform the existing standard ones in 92.8% cases among 28 different cases studied, and $-x\\cdot erf(e^{-x})$ is found to be the best activation function for image classification generated by the optimization scheme.||\n", "2409.04817": "|**2024-09-07**|[SSFam: Scribble Supervised Salient Object Detection Family](http://arxiv.org/abs/2409.04817)|**[link](https://github.com/liuzywen/ssfam)**|Scribble supervised salient object detection (SSSOD) constructs segmentation ability of attractive objects from surroundings under the supervision of sparse scribble labels. For the better segmentation, depth and thermal infrared modalities serve as the supplement to RGB images in the complex scenes. Existing methods specifically design various feature extraction and multi-modal fusion strategies for RGB, RGB-Depth, RGB-Thermal, and Visual-Depth-Thermal image input respectively, leading to similar model flood. As the recently proposed Segment Anything Model (SAM) possesses extraordinary segmentation and prompt interactive capability, we propose an SSSOD family based on SAM, named SSFam, for the combination input with different modalities. Firstly, different modal-aware modulators are designed to attain modal-specific knowledge which cooperates with modal-agnostic information extracted from the frozen SAM encoder for the better feature ensemble. Secondly, a siamese decoder is tailored to bridge the gap between the training with scribble prompt and the testing with no prompt for the stronger decoding ability. Our model demonstrates the remarkable performance among combinations of different modalities and refreshes the highest level of scribble supervised methods and comes close to the ones of fully supervised methods. https://github.com/liuzywen/SSFam||\n", "2409.04801": "|**2024-09-07**|[SpotActor: Training-Free Layout-Controlled Consistent Image Generation](http://arxiv.org/abs/2409.04801)|null|Text-to-image diffusion models significantly enhance the efficiency of artistic creation with high-fidelity image generation. However, in typical application scenarios like comic book production, they can neither place each subject into its expected spot nor maintain the consistent appearance of each subject across images. For these issues, we pioneer a novel task, Layout-to-Consistent-Image (L2CI) generation, which produces consistent and compositional images in accordance with the given layout conditions and text prompts. To accomplish this challenging task, we present a new formalization of dual energy guidance with optimization in a dual semantic-latent space and thus propose a training-free pipeline, SpotActor, which features a layout-conditioned backward update stage and a consistent forward sampling stage. In the backward stage, we innovate a nuanced layout energy function to mimic the attention activations with a sigmoid-like objective. While in the forward stage, we design Regional Interconnection Self-Attention (RISA) and Semantic Fusion Cross-Attention (SFCA) mechanisms that allow mutual interactions across images. To evaluate the performance, we present ActorBench, a specified benchmark with hundreds of reasonable prompt-box pairs stemming from object detection datasets. Comprehensive experiments are conducted to demonstrate the effectiveness of our method. The results prove that SpotActor fulfills the expectations of this task and showcases the potential for practical applications with superior layout alignment, subject consistency, prompt conformity and background diversity.||\n", "2409.04778": "|**2024-09-07**|[LoCa: Logit Calibration for Knowledge Distillation](http://arxiv.org/abs/2409.04778)|null|Knowledge Distillation (KD), aiming to train a better student model by mimicking the teacher model, plays an important role in model compression. One typical way is to align the output logits. However, we find a common issue named mis-instruction, that the student would be misled when the predictions based on teacher logits do not follow the labels. Meanwhile, there is other useful dark knowledge in the logits such as the class discriminability, which is vital for distillation. In this paper, we propose a simple yet effective Logit Calibration (LoCa) method, which calibrates the logits from the teacher model based on the ground-truth labels. The key insight is to correct the prediction (to address the mis-instruction issue) and maintain useful dark knowledge simultaneously. Our proposed LoCa does not require any additional parameters. Empirical results on image classification and text generation tasks demonstrate that LoCa can effectively improve the performance of baselines.||\n", "2409.06689": "|**2024-09-10**|[A comprehensive study on Blood Cancer detection and classification using Convolutional Neural Network](http://arxiv.org/abs/2409.06689)|null|\u591a\u5e74\u6765\uff0c\u5728\u76ee\u6807\u68c0\u6d4b\u9886\u57df\uff0c\u4e00\u4e9b\u9ad8\u6548\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN)\uff0c\u5982 DenseNet201\u3001InceptionV3\u3001ResNet152v2\u3001SEresNet152\u3001VGG19\u3001Xception \u56e0\u5176\u6027\u80fd\u800c\u5907\u53d7\u5173\u6ce8\u3002\u6b64\u5916\uff0cCNN \u8303\u5f0f\u5df2\u7ecf\u6269\u5c55\u5230\u4ece\u539f\u59cb CNN \u67b6\u6784\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u548c\u96c6\u6210\u6a21\u578b\u3002\u7814\u7a76\u8868\u660e\uff0c\u8fc1\u79fb\u5b66\u4e60\u548c\u96c6\u6210\u6a21\u578b\u80fd\u591f\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60 (DL) \u6a21\u578b\u7684\u51c6\u786e\u6027\u3002\u7136\u800c\uff0c\u5f88\u5c11\u6709\u7814\u7a76\u5229\u7528\u8fd9\u4e9b\u6280\u672f\u5bf9\u8840\u6db2\u6076\u6027\u80bf\u7624\u8fdb\u884c\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7684\u7efc\u5408\u5b9e\u9a8c\u3002\u610f\u8bc6\u5230\u8fd9\u4e00\u5dee\u8ddd\uff0c\u672c\u7814\u7a76\u8fdb\u884c\u4e86\u4e09\u4e2a\u5b9e\u9a8c\uff1b\u5728\u7b2c\u4e00\u4e2a\u5b9e\u9a8c\u4e2d\uff0c\u4f7f\u7528\u4e86\u516d\u4e2a\u539f\u59cb CNN\uff0c\u5728\u7b2c\u4e8c\u4e2a\u5b9e\u9a8c\u4e2d\uff0c\u4f7f\u7528\u4e86\u8fc1\u79fb\u5b66\u4e60\uff0c\u5728\u7b2c\u4e09\u4e2a\u5b9e\u9a8c\u4e2d\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684\u96c6\u6210\u6a21\u578b DIX\uff08DenseNet201\u3001InceptionV3 \u548c Xception\uff09\u6765\u68c0\u6d4b\u548c\u5206\u7c7b\u8840\u764c\u3002\u7edf\u8ba1\u7ed3\u679c\u8868\u660e\uff0cDIX \u7684\u6027\u80fd\u4f18\u4e8e\u539f\u59cb\u6a21\u578b\u548c\u8fc1\u79fb\u5b66\u4e60\uff0c\u51c6\u786e\u7387\u8fbe\u5230 99.12%\u3002\u7136\u800c\uff0c\u8fd9\u9879\u7814\u7a76\u4e5f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u8d1f\u9762\u7ed3\u679c\uff0c\u56e0\u4e3a\u8fc1\u79fb\u5b66\u4e60\u5e76\u6ca1\u6709\u63d0\u9ad8\u539f\u59cb CNN \u7684\u51c6\u786e\u6027\u3002\u4e0e\u8bb8\u591a\u5176\u4ed6\u764c\u75c7\u4e00\u6837\uff0c\u8840\u764c\u75be\u75c5\u9700\u8981\u53ca\u65f6\u8bc6\u522b\uff0c\u624d\u80fd\u5236\u5b9a\u6709\u6548\u7684\u6cbb\u7597\u65b9\u6848\u5e76\u63d0\u9ad8\u751f\u5b58\u673a\u4f1a\u3002\u4f7f\u7528 CNN \u68c0\u6d4b\u548c\u5206\u7c7b\u8840\u764c\u7684\u9ad8\u7cbe\u5ea6\u8868\u660e\uff0cCNN \u6a21\u578b\u5728\u8840\u764c\u68c0\u6d4b\u4e2d\u5f88\u6709\u524d\u666f\u3002\u8fd9\u9879\u7814\u7a76\u5728\u751f\u7269\u533b\u5b66\u5de5\u7a0b\u3001\u8ba1\u7b97\u673a\u8f85\u52a9\u75be\u75c5\u8bca\u65ad\u548c\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u75be\u75c5\u68c0\u6d4b\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002||\n", "2409.06590": "|**2024-09-10**|[Lightweight Multiscale Feature Fusion Super-Resolution Network Based on Two-branch Convolution and Transformer](http://arxiv.org/abs/2409.06590)|null|\u76ee\u524d\uff0c\u6df1\u5ea6\u5b66\u4e60\u4e0b\u7684\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387(SISR)\u7b97\u6cd5\u4e3b\u8981\u6709\u4e24\u5927\u6a21\u578b\uff0c\u4e00\u79cd\u662f\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u578b\uff0c\u53e6\u4e00\u79cd\u662f\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u3002\u524d\u8005\u91c7\u7528\u4e0d\u540c\u5377\u79ef\u6838\u5927\u5c0f\u7684\u5377\u79ef\u5c42\u5806\u53e0\u7684\u65b9\u5f0f\u6765\u8bbe\u8ba1\u6a21\u578b\uff0c\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u63d0\u53d6\u56fe\u50cf\u7684\u5c40\u90e8\u7279\u5f81\uff1b\u540e\u8005\u91c7\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u8bbe\u8ba1\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u53ef\u4ee5\u8ba9\u6a21\u578b\u5efa\u7acb\u56fe\u50cf\u50cf\u7d20\u70b9\u4e4b\u95f4\u7684\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\uff0c\u8fdb\u800c\u66f4\u597d\u5730\u63d0\u53d6\u56fe\u50cf\u7684\u5168\u5c40\u7279\u5f81\u3002\u7136\u800c\uff0c\u4e0a\u8ff0\u4e24\u79cd\u65b9\u6cd5\u90fd\u9762\u4e34\u7740\u81ea\u5df1\u7684\u95ee\u9898\u3002\u57fa\u4e8e\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u5411\u4e92\u8865\u5377\u79ef\u548cTransformer\u7684\u65b0\u578b\u8f7b\u91cf\u7ea7\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u7f51\u7edc\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u53cc\u5206\u652f\u7f51\u7edc\u67b6\u6784\uff0c\u878d\u5408Transformer\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5404\u81ea\u7684\u7279\u70b9\uff0c\u5b9e\u73b0\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\u7684\u76f8\u4e92\u878d\u5408\u3002\u540c\u65f6\uff0c\u8003\u8651\u5230\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u4f4e\u50cf\u7d20\u56fe\u50cf\u9020\u6210\u7684\u5c40\u90e8\u4fe1\u606f\u4e22\u5931\uff0c\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u7279\u5f81\u8865\u5145\u7684\u6a21\u5757\u5316\u8fde\u63a5\u65b9\u5f0f\uff0c\u5c06\u6a21\u578b\u6d45\u5c42\u9636\u6bb5\u63d0\u53d6\u7684\u7279\u5f81\u56fe\u4e0e\u6a21\u578b\u6df1\u5c42\u9636\u6bb5\u63d0\u53d6\u7684\u7279\u5f81\u56fe\u8fdb\u884c\u878d\u5408\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u7279\u5f81\u56fe\u50cf\u4e2d\u4fe1\u606f\u7684\u4e22\u5931\uff0c\u6709\u5229\u4e8e\u56fe\u50cf\u7684\u590d\u539f\uff0c\u4fbf\u4e8e\u83b7\u5f97\u66f4\u9ad8\u8d28\u91cf\u7684\u590d\u539f\u56fe\u50cf\u3002\u6700\u7ec8\u7684\u5b9e\u8df5\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u53c2\u6570\u91cf\u76f8\u540c\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u76f8\u6bd4\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6a21\u578b\u5728\u56fe\u50cf\u6062\u590d\u6027\u80fd\u65b9\u9762\u662f\u6700\u4f18\u7684\u3002||\n", "2409.06584": "|**2024-09-10**|[Transtreaming: Adaptive Delay-aware Transformer for Real-time Streaming Perception](http://arxiv.org/abs/2409.06584)|null|\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u5bf9\u4e8e\u8bb8\u591a\u73b0\u5b9e\u5e94\u7528\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u9632\u649e\u548c\u8def\u5f84\u89c4\u5212\uff09\u7684\u51b3\u7b56\u8fc7\u7a0b\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u5b9e\u65f6\u6d41\u611f\u77e5\u65b9\u6cd5 Transtreaming\uff0c\u5b83\u89e3\u51b3\u4e86\u5177\u6709\u52a8\u6001\u8ba1\u7b97\u5ef6\u8fdf\u7684\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u6311\u6218\u3002Transtreaming \u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u81ea\u9002\u5e94\u5ef6\u8fdf\u611f\u77e5\u8f6c\u6362\u5668\uff0c\u5b83\u53ef\u4ee5\u540c\u65f6\u9884\u6d4b\u591a\u4e2a\u672a\u6765\u5e27\u5e76\u9009\u62e9\u4e0e\u73b0\u5b9e\u4e16\u754c\u5f53\u524d\u65f6\u95f4\u6700\u5339\u914d\u7684\u8f93\u51fa\uff0c\u4ece\u800c\u8865\u507f\u4efb\u4f55\u7cfb\u7edf\u5f15\u8d77\u7684\u8ba1\u7b97\u5ef6\u8fdf\u3002\u5373\u4f7f\u5728\u5355\u5e27\u68c0\u6d4b\u573a\u666f\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u4e5f\u901a\u8fc7\u5229\u7528\u57fa\u4e8e\u8f6c\u6362\u5668\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u5b83\u5728\u4ece\u5f3a\u5927\u7684 V100 \u5230\u9002\u5ea6\u7684 2080Ti \u7684\u5404\u79cd\u8bbe\u5907\u4e0a\u5747\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5728\u6240\u6709\u5e73\u53f0\u4e0a\u90fd\u5b9e\u73b0\u4e86\u6700\u9ad8\u6c34\u5e73\u7684\u611f\u77e5\u7cbe\u5ea6\u3002\u4e0e\u5927\u591a\u6570\u96be\u4ee5\u5728\u529f\u80fd\u8f83\u5f31\u7684\u8bbe\u5907\u4e0a\u5728\u4e00\u5e27\u5185\u5b8c\u6210\u8ba1\u7b97\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u4e0d\u540c\uff0cTranstreaming \u53ef\u4ee5\u6ee1\u8db3\u5404\u79cd\u8bbe\u5907\u4e0a\u7684\u4e25\u683c\u5b9e\u65f6\u5904\u7406\u8981\u6c42\u3002\u5b9e\u9a8c\u7ed3\u679c\u5f3a\u8c03\u4e86\u8be5\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u5176\u663e\u7740\u63d0\u9ad8\u8bb8\u591a\u73b0\u5b9e\u7cfb\u7edf\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\uff09\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u7684\u6f5c\u529b\u3002||\n", "2409.06583": "|**2024-09-10**|[Semi-Supervised 3D Object Detection with Chanel Augmentation using Transformation Equivariance](http://arxiv.org/abs/2409.06583)|null|\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u548c\u673a\u5668\u4eba\u6765\u8bf4\uff0c\u7cbe\u786e\u7684\u4e09\u7ef4\u7269\u4f53\u68c0\u6d4b\u5bf9\u4e8e\u5176\u5b89\u5168\u6709\u6548\u5730\u5bfc\u822a\u548c\u4e0e\u73af\u5883\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002\u540c\u65f6\uff0c\u4e09\u7ef4\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u4f9d\u8d56\u4e8e\u6570\u636e\u89c4\u6a21\u548c\u6807\u6ce8\uff0c\u800c\u8fd9\u901a\u5e38\u6210\u672c\u9ad8\u6602\u3002\u56e0\u6b64\uff0c\u4f7f\u7528\u6709\u9650\u7684\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u672c\u6587\u63a2\u7d22\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5e08\u751f\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u901a\u9053\u589e\u5f3a\u6280\u672f\u8fdb\u884c\u4e09\u7ef4\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u3002\u5e08\u751fSSL\u901a\u5e38\u5bf9\u6559\u5e08\u548c\u5b66\u751f\u5206\u522b\u91c7\u7528\u5f31\u589e\u5f3a\u548c\u5f3a\u589e\u5f3a\u3002\u5728\u672c\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u53d8\u6362\u7b49\u53d8\u68c0\u6d4b\u5668\uff08TED\uff09\u5bf9\u4e24\u4e2a\u7f51\u7edc\u5e94\u7528\u4e86\u591a\u901a\u9053\u589e\u5f3a\u3002TED\u4f7f\u6211\u4eec\u80fd\u591f\u63a2\u7d22\u70b9\u4e91\u4e0a\u589e\u5f3a\u7684\u4e0d\u540c\u7ec4\u5408\uff0c\u5e76\u6709\u6548\u5730\u805a\u5408\u591a\u901a\u9053\u53d8\u6362\u7b49\u53d8\u7279\u5f81\u3002\u539f\u5219\u4e0a\uff0c\u901a\u8fc7\u5bf9\u6559\u5e08\u7f51\u7edc\u91c7\u7528\u56fa\u5b9a\u7684\u901a\u9053\u589e\u5f3a\uff0c\u5b66\u751f\u53ef\u4ee5\u5728\u53ef\u9760\u7684\u4f2a\u6807\u7b7e\u4e0a\u7a33\u5b9a\u5730\u8bad\u7ec3\u3002\u91c7\u7528\u5f3a\u901a\u9053\u589e\u5f3a\u53ef\u4ee5\u4e30\u5bcc\u6570\u636e\u7684\u591a\u6837\u6027\uff0c\u589e\u5f3a\u5bf9\u53d8\u6362\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u9ad8\u5b66\u751f\u7f51\u7edc\u7684\u6cdb\u5316\u6027\u80fd\u3002\u6211\u4eec\u4f7f\u7528SOTA\u5c42\u6b21\u76d1\u7763\u4f5c\u4e3a\u57fa\u7ebf\uff0c\u5e76\u5c06\u5176\u53cc\u9608\u503c\u8c03\u6574\u5230TED\uff0c\u79f0\u4e3a\u901a\u9053IoU\u4e00\u81f4\u6027\u3002\u6211\u4eec\u4f7f\u7528KITTI\u6570\u636e\u96c6\u5bf9\u6211\u4eec\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8d8a\u4e86SOTA\u4e09\u7ef4\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u3002||\n", "2409.06542": "|**2024-09-10**|[Dynamic Decoupling of Placid Terminal Attractor-based Gradient Descent Algorithm](http://arxiv.org/abs/2409.06542)|null|\u68af\u5ea6\u4e0b\u964d (GD) \u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d (SGD) \u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4f17\u591a\u5e94\u7528\u9886\u57df\u3002\u56e0\u6b64\uff0c\u7406\u89e3 GD \u7684\u52a8\u529b\u5b66\u5e76\u63d0\u9ad8\u5176\u6536\u655b\u901f\u5ea6\u4ecd\u7136\u975e\u5e38\u91cd\u8981\u3002\u672c\u6587\u6839\u636e\u68af\u5ea6\u6d41\u4e0d\u540c\u9636\u6bb5\u7684\u7ec8\u7aef\u5438\u5f15\u5b50\uff0c\u4ed4\u7ec6\u5206\u6790\u4e86 GD \u7684\u52a8\u529b\u5b66\u3002\u57fa\u4e8e\u7ec8\u7aef\u6ed1\u6a21\u7406\u8bba\u548c\u7ec8\u7aef\u5438\u5f15\u5b50\u7406\u8bba\uff0c\u8bbe\u8ba1\u4e86\u56db\u79cd\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u3002\u5e76\u901a\u8fc7\u8be6\u7ec6\u7684\u7406\u8bba\u7814\u7a76\u8003\u5bdf\u4e86\u5b83\u4eec\u7684\u6027\u80fd\uff0c\u5e76\u5bf9\u5b66\u4e60\u8fc7\u7a0b\u7684\u8fd0\u884c\u65f6\u95f4\u8fdb\u884c\u4e86\u8bc4\u4f30\u548c\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u8fd8\u8be6\u7ec6\u7814\u7a76\u4e86\u5b83\u4eec\u5b66\u4e60\u8fc7\u7a0b\u7684\u603b\u65f6\u95f4\u3002\u4e3a\u4e86\u8bc4\u4f30\u5176\u6709\u6548\u6027\uff0c\u5728\u51fd\u6570\u903c\u8fd1\u95ee\u9898\u548c\u56fe\u50cf\u5206\u7c7b\u95ee\u9898\u4e0a\u5bf9\u5404\u79cd\u4eff\u771f\u7ed3\u679c\u8fdb\u884c\u4e86\u7814\u7a76\u3002||\n", "2409.06443": "|**2024-09-10**|[Knowledge Distillation via Query Selection for Detection Transformer](http://arxiv.org/abs/2409.06443)|null|Transformer \u901a\u8fc7\u5f15\u5165 DETR \u4e3a\u76ee\u6807\u68c0\u6d4b\u9886\u57df\u5e26\u6765\u4e86\u9769\u547d\u6027\u7684\u53d8\u5316\uff0cDETR \u4ee5\u5176\u7b80\u6d01\u6027\u548c\u6709\u6548\u6027\u800c\u5907\u53d7\u8d5e\u8a89\u3002\u5c3d\u7ba1\u6709\u8fd9\u4e9b\u4f18\u52bf\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u7684\u5e9e\u5927\u89c4\u6a21\u5bf9\u5176\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\uff0c\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u672c\u6587\u5229\u7528\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u89e3\u51b3\u4e86\u538b\u7f29 DETR \u7684\u6311\u6218\uff0c\u8be5\u6280\u672f\u6709\u671b\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c0f\u6a21\u578b\u89c4\u6a21\u3002DETR \u6027\u80fd\u7684\u4e00\u4e2a\u5173\u952e\u65b9\u9762\u662f\u5b83\u4eec\u4f9d\u8d56\u67e5\u8be2\u6765\u51c6\u786e\u89e3\u91ca\u5bf9\u8c61\u8868\u793a\u3002\u4f20\u7edf\u7684\u84b8\u998f\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u901a\u8fc7\u4e8c\u5206\u5339\u914d\u8bc6\u522b\u7684\u6b63\u67e5\u8be2\uff0c\u800c\u5ffd\u7565\u4e86\u786c\u8d1f\u67e5\u8be2\u4e2d\u5b58\u5728\u7684\u4fe1\u606f\u3002\u6211\u4eec\u7684\u89c6\u89c9\u5206\u6790\u8868\u660e\uff0c\u5173\u6ce8\u524d\u666f\u5143\u7d20\u7684\u786c\u8d1f\u67e5\u8be2\u5bf9\u4e8e\u589e\u5f3a\u84b8\u998f\u7ed3\u679c\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7ec4\u67e5\u8be2\u9009\u62e9\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u901a\u8fc7\u6839\u636e\u67e5\u8be2\u4e0e\u771f\u5b9e\u5bf9\u8c61\u7684\u5e7f\u4e49\u4ea4\u5e76\u6bd4 (GIoU) \u5bf9\u67e5\u8be2\u8fdb\u884c\u5206\u6bb5\uff0c\u4ece\u800c\u53d1\u73b0\u6709\u4ef7\u503c\u7684\u786c\u8d1f\u67e5\u8be2\u7528\u4e8e\u84b8\u998f\uff0c\u8fd9\u4e0e DETR \u84b8\u998f\u4e2d\u7684\u4f20\u7edf\u67e5\u8be2\u9009\u62e9\u4e0d\u540c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u67e5\u8be2\u9009\u62e9\u7684 DETR \u77e5\u8bc6\u84b8\u998f (QSKD) \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u6ce8\u610f\u529b\u5f15\u5bfc\u7279\u5f81\u84b8\u998f (AGFD) \u548c\u5c40\u90e8\u5bf9\u9f50\u9884\u6d4b\u84b8\u998f (LAPD)\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u901a\u8fc7\u5173\u6ce8\u6559\u5e08\u6a21\u578b\u4e2d\u95f4\u7279\u5f81\u548c\u8f93\u51fa\u4e2d\u6700\u6709\u4fe1\u606f\u7684\u90e8\u5206\u6765\u4f18\u5316\u84b8\u998f\u8fc7\u7a0b\u3002\u6211\u4eec\u5bf9 MS-COCO \u6570\u636e\u96c6\u7684\u7efc\u5408\u5b9e\u9a8c\u8bc4\u4f30\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u4e0d\u589e\u52a0\u5927\u91cf\u8ba1\u7b97\u6210\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u7740\u63d0\u9ad8\u4e86\u5404\u79cd DETR \u67b6\u6784\u7684\u5e73\u5747\u7cbe\u5ea6 (AP)\u3002\u5177\u4f53\u6765\u8bf4\uff0cConditional DETR ResNet-18 \u7684 AP \u4ece 35.8 \u63d0\u9ad8\u5230 39.9\u3002||\n", "2409.06311": "|**2024-09-10**|[Seam Carving as Feature Pooling in CNN](http://arxiv.org/abs/2409.06311)|null|\u8fd9\u9879\u5de5\u4f5c\u7814\u7a76\u4e86\u5c06\u63a5\u7f1d\u88c1\u526a\u4f5c\u4e3a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u4e2d\u7684\u4e00\u79cd\u7279\u5f81\u6c60\u5316\u6280\u672f\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u6f5c\u529b\u3002\u6211\u4eec\u5efa\u8bae\u7528\u63a5\u7f1d\u88c1\u526a\u64cd\u4f5c\u66ff\u6362\u4f20\u7edf\u7684\u6700\u5927\u6c60\u5316\u5c42\u3002\u6211\u4eec\u5728 Caltech-UCSD Birds 200-2011 \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u63a5\u7f1d\u88c1\u526a\u7684 CNN \u4e0e\u91c7\u7528\u6700\u5927\u6c60\u5316\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548c F1 \u5206\u6570\u7b49\u6307\u6807\u4e0a\u5747\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002\u6211\u4eec\u901a\u8fc7\u7279\u5f81\u56fe\u53ef\u89c6\u5316\u8fdb\u4e00\u6b65\u5206\u6790\u4e86\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u884c\u4e3a\uff0c\u8868\u660e\u63a5\u7f1d\u88c1\u526a\u5728\u6c60\u5316\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u4fdd\u7559\u4e86\u66f4\u591a\u7ed3\u6784\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8ba8\u8bba\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6f5c\u5728\u65b9\u5411\u3002||\n", "2409.06300": "|**2024-09-10**|[An Attribute-Enriched Dataset and Auto-Annotated Pipeline for Open Detection](http://arxiv.org/abs/2409.06300)|null|\u901a\u8fc7\u8bed\u8a00\u68c0\u6d4b\u611f\u5174\u8da3\u7684\u5bf9\u8c61\u7ecf\u5e38\u4f1a\u9047\u5230\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u90a3\u4e9b\u4e0d\u5e38\u89c1\u6216\u96be\u4ee5\u63cf\u8ff0\u7684\u5bf9\u8c61\uff0c\u56e0\u4e3a\u81ea\u52a8\u5316\u6a21\u578b\u548c\u4eba\u7c7b\u6807\u6ce8\u8005\u4e4b\u95f4\u5b58\u5728\u611f\u77e5\u5dee\u5f02\u3002\u8fd9\u4e9b\u6311\u6218\u51f8\u663e\u4e86\u5bf9\u7efc\u5408\u6570\u636e\u96c6\u7684\u9700\u6c42\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u9700\u8981\u8d85\u8d8a\u6807\u51c6\u7684\u5bf9\u8c61\u6807\u7b7e\uff0c\u5e76\u7ed3\u5408\u8be6\u7ec6\u7684\u5c5e\u6027\u63cf\u8ff0\u3002\u4e3a\u4e86\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\uff0c\u6211\u4eec\u5f15\u5165\u4e86 Objects365-Attr \u6570\u636e\u96c6\uff0c\u5b83\u662f\u5bf9\u73b0\u6709 Objects365 \u6570\u636e\u96c6\u7684\u6269\u5c55\uff0c\u5176\u7279\u70b9\u662f\u5177\u6709\u5c5e\u6027\u6807\u6ce8\u3002\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u6574\u5408\u5e7f\u6cdb\u7684\u5c5e\u6027\uff08\u5305\u62ec\u989c\u8272\u3001\u6750\u8d28\u3001\u72b6\u6001\u3001\u7eb9\u7406\u548c\u8272\u8c03\uff09\u6765\u51cf\u5c11\u5bf9\u8c61\u68c0\u6d4b\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u3002\u5b83\u5305\u542b 560 \u4e07\u4e2a\u5bf9\u8c61\u7ea7\u5c5e\u6027\u63cf\u8ff0\u7684\u6269\u5c55\u96c6\u5408\uff0c\u8fd9\u4e9b\u63cf\u8ff0\u5728 140 \u4e07\u4e2a\u8fb9\u754c\u6846\u4e2d\u8fdb\u884c\u4e86\u7cbe\u5fc3\u6807\u6ce8\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5bf9\u4e0d\u540c\u89c4\u6a21\u7684 YOLO-World \u8fdb\u884c\u4e86\u4e25\u683c\u7684\u8bc4\u4f30\uff0c\u6d4b\u91cf\u4e86\u5b83\u4eec\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u6570\u636e\u96c6\u5bf9\u63a8\u8fdb\u5bf9\u8c61\u68c0\u6d4b\u7684\u8d21\u732e\u3002||\n", "2409.07904": "|**2024-09-12**|[FACT: Feature Adaptive Continual-learning Tracker for Multiple Object Tracking](http://arxiv.org/abs/2409.07904)|null|\u591a\u76ee\u6807\u8ddf\u8e2a (MOT) \u6d89\u53ca\u8bc6\u522b\u89c6\u9891\u5e8f\u5217\u4e2d\u7684\u591a\u4e2a\u76ee\u6807\u5e76\u4e3a\u5176\u5206\u914d\u76f8\u5e94\u7684 ID\uff0c\u5176\u4e2d\u7ecf\u5e38\u9047\u5230\u906e\u6321\u3002\u6700\u8fd1\u7684\u65b9\u6cd5\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u6280\u672f\u89e3\u51b3\u906e\u6321\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u9002\u5e94\u6027\uff0c\u6216\u901a\u8fc7\u79bb\u7ebf\u5b66\u4e60\u6280\u672f\u5229\u7528\u89c6\u9891\u4e2d\u7684\u65f6\u95f4\u4fe1\u606f\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u7684\u57fa\u4e8e\u5728\u7ebf\u5b66\u4e60\u7684 MOT \u65b9\u6cd5\u65e0\u6cd5\u4ece\u6240\u6709\u8fc7\u53bb\u7684\u8ddf\u8e2a\u4fe1\u606f\u4e2d\u5b66\u4e60\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u5b9e\u65f6\u8ddf\u8e2a\u901f\u5ea6\u7684\u540c\u65f6\u63d0\u9ad8\u5bf9\u957f\u671f\u906e\u6321\u7684\u9002\u5e94\u6027\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u57fa\u4e8e\u65f6\u95f4\u4fe1\u606f\u7684\u79bb\u7ebf\u5b66\u4e60\u65b9\u6cd5\u7ef4\u62a4\u4e00\u4e2a\u957f\u671f\u8bb0\u5fc6\u6765\u5b58\u50a8\u8fc7\u53bb\u7684\u8ddf\u8e2a\u4fe1\u606f\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u9650\u5236\u4e86\u5b83\u4eec\u5728\u8ddf\u8e2a\u8fc7\u7a0b\u4e2d\u53ea\u80fd\u4f7f\u7528\u5c40\u90e8\u7684\u8fc7\u53bb\u4fe1\u606f\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 MOT \u6846\u67b6\uff0c\u79f0\u4e3a\u7279\u5f81\u81ea\u9002\u5e94\u6301\u7eed\u5b66\u4e60\u8ddf\u8e2a\u5668 (FACT)\uff0c\u5b83\u901a\u8fc7\u5229\u7528\u6240\u6709\u8fc7\u53bb\u7684\u8ddf\u8e2a\u4fe1\u606f\u5b9e\u73b0\u76ee\u6807\u7684\u5b9e\u65f6\u8ddf\u8e2a\u548c\u7279\u5f81\u5b66\u4e60\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u53ef\u4ee5\u4e0e\u5404\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u7279\u5f81\u7684\u8ddf\u8e2a\u5668\u96c6\u6210\uff0c\u4ece\u800c\u63d0\u9ad8\u5b83\u4eec\u7684\u8ddf\u8e2a\u80fd\u529b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u7279\u5f81\u81ea\u9002\u5e94\u6301\u7eed\u5b66\u4e60 (FAC) \u6a21\u5757\uff0c\u8fd9\u662f\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\uff0c\u53ef\u4ee5\u5728\u7ebf\u8bad\u7ec3\u4ee5\u81ea\u9002\u5e94\u5730\u5b66\u4e60\u7279\u5f81\uff0c\u5e76\u5728\u8ddf\u8e2a\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u6240\u6709\u8fc7\u53bb\u7684\u8ddf\u8e2a\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u4e13\u4e3a\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u6301\u7eed\u5b66\u4e60\u7684\u8ddf\u8e2a\u800c\u8bbe\u8ba1\u7684\u4e24\u9636\u6bb5\u5173\u8054\u6a21\u5757\u3002\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728 MOT17 \u548c MOT20 \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5728\u7ebf\u8ddf\u8e2a\u6027\u80fd\u3002\u4ee3\u7801\u5c06\u5728\u63a5\u6536\u540e\u53d1\u5e03\u3002|\n", "2409.07896": "|**2024-09-12**|[Microscopic-Mamba: Revealing the Secrets of Microscopic Images with Just 4M Parameters](http://arxiv.org/abs/2409.07896)|**[link](https://github.com/zs1314/microscopic-mamba)**|\u5728\u533b\u5b66\u663e\u5fae\u56fe\u50cf\u5206\u7c7b (MIC) \u9886\u57df\uff0c\u57fa\u4e8e CNN \u548c Transformer \u7684\u6a21\u578b\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\u3002\u7136\u800c\uff0cCNN \u96be\u4ee5\u5efa\u6a21\u8fdc\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u5176\u5145\u5206\u5229\u7528\u56fe\u50cf\u8bed\u4e49\u4fe1\u606f\u7684\u80fd\u529b\u3002\u76f8\u53cd\uff0cTransformer \u5219\u53d7\u5230\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u6027\u7684\u963b\u788d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e Mamba \u67b6\u6784\u7684\u6a21\u578b\uff1aMicroscopic-Mamba\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u90e8\u5206\u9009\u62e9\u524d\u9988\u7f51\u7edc\uff08PSFFN\uff09\u6765\u66ff\u6362\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\uff08VSSM\uff09\u7684\u6700\u540e\u4e00\u4e2a\u7ebf\u6027\u5c42\uff0c\u589e\u5f3a\u4e86 Mamba \u7684\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u8c03\u5236\u4ea4\u4e92\u7279\u5f81\u805a\u5408\uff08MIFA\uff09\u6a21\u5757\uff0c\u4ee5\u6709\u6548\u5730\u8c03\u5236\u548c\u52a8\u6001\u805a\u5408\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u3002\u6211\u4eec\u8fd8\u7ed3\u5408\u4e86\u5e76\u884c VSSM \u673a\u5236\uff0c\u4ee5\u6539\u5584\u901a\u9053\u95f4\u7684\u4fe1\u606f\u4ea4\u4e92\uff0c\u540c\u65f6\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e94\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/zs1314/Microscopic-Mamba \u83b7\u53d6\u3002||\n", "2409.07813": "|**2024-09-12**|[What is YOLOv9: An In-Depth Exploration of the Internal Features of the Next-Generation Object Detector](http://arxiv.org/abs/2409.07813)|null|\u672c\u7814\u7a76\u5168\u9762\u5206\u6790\u4e86 YOLOv9 \u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u67b6\u6784\u521b\u65b0\u3001\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u53ca\u76f8\u8f83\u4e8e\u5148\u524d\u7248\u672c\u7684\u6027\u80fd\u6539\u8fdb\u3002\u5173\u952e\u7684\u6539\u8fdb\uff0c\u4f8b\u5982\u5e7f\u4e49\u9ad8\u6548\u5c42\u805a\u5408\u7f51\u7edc (GELAN) \u548c\u53ef\u7f16\u7a0b\u68af\u5ea6\u4fe1\u606f (PGI)\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u7279\u5f81\u63d0\u53d6\u548c\u68af\u5ea6\u6d41\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u5377\u79ef\u548c\u8f7b\u91cf\u7ea7 C3Ghost \u67b6\u6784\uff0cYOLOv9 \u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u5728 Microsoft COCO \u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u5b83\u5177\u6709\u4f18\u8d8a\u7684\u5e73\u5747\u7cbe\u5ea6\u5747\u503c (mAP) \u548c\u66f4\u5feb\u7684\u63a8\u7406\u65f6\u95f4\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e YOLOv8\u3002\u8be5\u6a21\u578b\u7684\u591a\u529f\u80fd\u6027\u4f53\u73b0\u5728\u5b83\u53ef\u4ee5\u65e0\u7f1d\u90e8\u7f72\u5230\u4ece\u8fb9\u7f18\u8bbe\u5907\u5230\u9ad8\u6027\u80fd GPU \u7684\u5404\u79cd\u786c\u4ef6\u5e73\u53f0\u4e0a\uff0c\u5e76\u5185\u7f6e\u652f\u6301 PyTorch \u548c TensorRT \u96c6\u6210\u3002\u672c\u6587\u9996\u6b21\u6df1\u5165\u63a2\u8ba8\u4e86 YOLOv9 \u7684\u5185\u90e8\u7279\u5f81\u53ca\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5c06\u5176\u786e\u7acb\u4e3a\u8de8\u884c\u4e1a\u7684\u5b9e\u65f6\u5bf9\u8c61\u68c0\u6d4b\u7684\u6700\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u7269\u8054\u7f51\u8bbe\u5907\u5230\u5927\u578b\u5de5\u4e1a\u5e94\u7528\u3002||\n", "2409.07769": "|**2024-09-12**|[Mesh-based Super-Resolution of Fluid Flows with Multiscale Graph Neural Networks](http://arxiv.org/abs/2409.07769)|null|\u8fd9\u9879\u5de5\u4f5c\u4ecb\u7ecd\u4e86\u4e00\u79cd\u56fe\u795e\u7ecf\u7f51\u7edc (GNN) \u65b9\u6cd5\uff0c\u80fd\u591f\u5bf9\u6d41\u4f53\u6d41\u52a8\u8fdb\u884c\u57fa\u4e8e\u7f51\u683c\u7684\u4e09\u7ef4\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u3002\u5728\u6b64\u6846\u67b6\u4e2d\uff0cGNN \u7684\u8bbe\u8ba1\u4e0d\u662f\u4e00\u6b21\u6027\u5728\u6574\u4e2a\u57fa\u4e8e\u7f51\u683c\u7684\u573a\u4e0a\u8fd0\u884c\uff0c\u800c\u662f\u76f4\u63a5\u5728\u5c40\u90e8\u5143\u7d20\uff08\u6216\u5355\u5143\uff09\u7f51\u683c\u4e0a\u8fd0\u884c\u3002\u4e3a\u4e86\u4ee5\u7c7b\u4f3c\u4e8e\u8c31\uff08\u6216\u6709\u9650\uff09\u5143\u7d20\u79bb\u6563\u5316\u7684\u65b9\u5f0f\u4fc3\u8fdb\u57fa\u4e8e\u7f51\u683c\u7684 GNN \u8868\u793a\uff0c\u4fee\u6539\u4e86\u57fa\u7ebf GNN \u5c42\uff08\u79f0\u4e3a\u6d88\u606f\u4f20\u9012\u5c42\uff0c\u7528\u4e8e\u66f4\u65b0\u5c40\u90e8\u8282\u70b9\u5c5e\u6027\uff09\u4ee5\u8003\u8651\u91cd\u5408\u56fe\u8282\u70b9\u7684\u540c\u6b65\uff0c\u4ece\u800c\u4f7f\u5176\u4e0e\u5e38\u7528\u7684\u57fa\u4e8e\u5143\u7d20\u7684\u7f51\u683c\u8fde\u63a5\u517c\u5bb9\u3002\u8be5\u67b6\u6784\u672c\u8d28\u4e0a\u662f\u591a\u5c3a\u5ea6\u7684\uff0c\u7531\u7c97\u5c3a\u5ea6\u548c\u7ec6\u5c3a\u5ea6\u6d88\u606f\u4f20\u9012\u5c42\u5e8f\u5217\uff08\u79f0\u4e3a\u5904\u7406\u5668\uff09\u7ec4\u5408\u800c\u6210\uff0c\u8fd9\u4e9b\u5e8f\u5217\u4e4b\u95f4\u901a\u8fc7\u56fe\u89e3\u6c60\u5c42\u8fdb\u884c\u5206\u79bb\u3002\u7c97\u5c3a\u5ea6\u5904\u7406\u5668\u4f7f\u7528\u7c97\u5c3a\u5ea6\u540c\u6b65\u6d88\u606f\u4f20\u9012\u5728\u5143\u7d20\u90bb\u57df\u4e0a\u5c06\u67e5\u8be2\u5143\u7d20\uff08\u4ee5\u53ca\u4e00\u7ec4\u76f8\u90bb\u7684\u7c97\u5143\u7d20\uff09\u5d4c\u5165\u5230\u5355\u4e2a\u6f5c\u5728\u56fe\u8868\u793a\u4e2d\uff0c\u800c\u7ec6\u5c3a\u5ea6\u5904\u7406\u5668\u5229\u7528\u6b64\u6f5c\u5728\u56fe\u4e0a\u7684\u5176\u4ed6\u6d88\u606f\u4f20\u9012\u64cd\u4f5c\u6765\u6821\u6b63\u63d2\u503c\u8bef\u5dee\u3002\u4f7f\u7528\u6765\u81ea\u96f7\u8bfa\u6570\u4e3a 1600 \u548c 3200 \u7684\u6cf0\u52d2-\u683c\u6797\u6da1\u6d41\u6a21\u62df\u7684\u516d\u9762\u4f53\u7f51\u683c\u6570\u636e\u8fdb\u884c\u6f14\u793a\u7814\u7a76\u3002\u901a\u8fc7\u5206\u6790\u5168\u5c40\u548c\u5c40\u90e8\u8bef\u5dee\uff0c\u7ed3\u679c\u6700\u7ec8\u8868\u660e\uff0c\u4e0e\u7c97\u5c3a\u5ea6\u548c\u591a\u5c3a\u5ea6\u6a21\u578b\u914d\u7f6e\u4e2d\u7684\u76ee\u6807\u76f8\u6bd4\uff0cGNN \u5982\u4f55\u80fd\u591f\u751f\u6210\u51c6\u786e\u7684\u8d85\u5206\u8fa8\u7387\u573a\u3002\u53d1\u73b0\u56fa\u5b9a\u67b6\u6784\u7684\u91cd\u5efa\u8bef\u5dee\u4e0e\u96f7\u8bfa\u6570\u6210\u6b63\u6bd4\uff0c\u800c\u5305\u542b\u5468\u56f4\u7c97\u5143\u7d20\u90bb\u5c45\u88ab\u53d1\u73b0\u53ef\u4ee5\u6539\u5584 Re=1600 \u65f6\u7684\u9884\u6d4b\uff0c\u4f46\u5728 Re=3200 \u65f6\u5219\u4e0d\u7136\u3002||\n", "2409.07734": "|**2024-09-12**|[DFDG: Data-Free Dual-Generator Adversarial Distillation for One-Shot Federated Learning](http://arxiv.org/abs/2409.07734)|null|\u8054\u90a6\u5b66\u4e60 (FL) \u662f\u4e00\u79cd\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u65b9\u6848\uff0c\u5176\u4e2d\u5ba2\u6237\u7aef\u901a\u8fc7\u5171\u4eab\u6a21\u578b\u4fe1\u606f\u800c\u4e0d\u662f\u5176\u79c1\u6709\u6570\u636e\u96c6\u6765\u5171\u540c\u53c2\u4e0e\u5168\u5c40\u6a21\u578b\u7684\u534f\u4f5c\u8bad\u7ec3\u3002\u8003\u8651\u5230\u4e0e\u901a\u4fe1\u548c\u9690\u79c1\u76f8\u5173\u7684\u62c5\u5fe7\uff0c\u5177\u6709\u4e00\u8f6e\u901a\u4fe1\u7684\u5355\u6b21\u8054\u90a6\u5b66\u4e60\u5df2\u6210\u4e3a\u4e8b\u5b9e\u4e0a\u7684\u6709\u5e0c\u671b\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5355\u6b21\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u516c\u5171\u6570\u636e\u96c6\uff0c\u8981\u4e48\u4fa7\u91cd\u4e8e\u6a21\u578b\u540c\u6784\u8bbe\u7f6e\uff0c\u8981\u4e48\u4ece\u672c\u5730\u6a21\u578b\u4e2d\u63d0\u53d6\u7684\u77e5\u8bc6\u6709\u9650\uff0c\u8fd9\u4f7f\u5f97\u8bad\u7ec3\u9c81\u68d2\u7684\u5168\u5c40\u6a21\u578b\u53d8\u5f97\u56f0\u96be\u751a\u81f3\u4e0d\u5207\u5b9e\u9645\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7528\u4e8e\u5355\u6b21\u8054\u90a6\u5b66\u4e60\u7684\u65e0\u6570\u636e\u53cc\u751f\u6210\u5668\u5bf9\u6297\u84b8\u998f\u65b9\u6cd5 (\u5373 DFDG)\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u8bad\u7ec3\u53cc\u751f\u6210\u5668\u6765\u63a2\u7d22\u66f4\u5e7f\u6cdb\u7684\u672c\u5730\u6a21\u578b\u8bad\u7ec3\u7a7a\u95f4\u3002DFDG \u4ee5\u5bf9\u6297\u65b9\u5f0f\u6267\u884c\uff0c\u5305\u62ec\u4e24\u90e8\u5206\uff1a\u53cc\u751f\u6210\u5668\u8bad\u7ec3\u548c\u53cc\u6a21\u578b\u84b8\u998f\u3002\u5728\u53cc\u751f\u6210\u5668\u8bad\u7ec3\u4e2d\uff0c\u6211\u4eec\u6df1\u5165\u7814\u7a76\u4e86\u6bcf\u4e2a\u751f\u6210\u5668\u5728\u4fdd\u771f\u5ea6\u3001\u53ef\u8fc1\u79fb\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u7684\u5185\u5bb9\uff0c\u4ee5\u786e\u4fdd\u5176\u6548\u7528\uff0c\u5e76\u989d\u5916\u5b9a\u5236\u4e86\u4ea4\u53c9\u6563\u5ea6\u635f\u5931\u4ee5\u51cf\u5c11\u53cc\u751f\u6210\u5668\u8f93\u51fa\u7a7a\u95f4\u7684\u91cd\u53e0\u3002\u5728\u53cc\u6a21\u578b\u84b8\u998f\u4e2d\uff0c\u8bad\u7ec3\u597d\u7684\u53cc\u751f\u6210\u5668\u534f\u540c\u5de5\u4f5c\uff0c\u4e3a\u5168\u5c40\u6a21\u578b\u7684\u66f4\u65b0\u63d0\u4f9b\u8bad\u7ec3\u6570\u636e\u3002\u6700\u540e\uff0c\u6211\u4eec\u5bf9\u5404\u79cd\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e SOTA \u57fa\u7ebf\u76f8\u6bd4\uff0cDFDG \u5728\u51c6\u786e\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u7684\u6027\u80fd\u63d0\u5347\u3002||\n", "2409.07693": "|**2024-09-12**|[Cooperative Inference with Interleaved Operator Partitioning for CNNs](http://arxiv.org/abs/2409.07693)|null|\u5c06\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u90e8\u7f72\u5728\u7269\u8054\u7f51\uff08IoT\uff09\u8bbe\u5907\u4e0a\u901a\u5e38\u4f1a\u9762\u4e34\u5185\u5b58\u8d44\u6e90\u548c\u8ba1\u7b97\u80fd\u529b\u6709\u9650\u7684\u6311\u6218\u3002\u534f\u540c\u63a8\u7406\u662f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u91cd\u8981\u65b9\u6cd5\uff0c\u9700\u8981\u5bf9\u667a\u80fd\u6a21\u578b\u8fdb\u884c\u5206\u533a\u548c\u5206\u5e03\u5f0f\u90e8\u7f72\u3002\u4e3a\u4e86\u6267\u884c\u6c34\u5e73\u5206\u533a\uff0c\u73b0\u6709\u7684\u534f\u540c\u63a8\u7406\u65b9\u6cd5\u8981\u4e48\u91c7\u7528\u7b97\u5b50\u7684\u8f93\u51fa\u901a\u9053\uff0c\u8981\u4e48\u91c7\u7528\u7279\u5f81\u56fe\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u4f5c\u4e3a\u5206\u533a\u7ef4\u5ea6\u3002\u5728\u8fd9\u79cd\u65b9\u5f0f\u4e0b\uff0c\u7531\u4e8e\u7b97\u5b50\u7684\u6fc0\u6d3b\u662f\u5206\u5e03\u5f0f\u7684\uff0c\u56e0\u6b64\u5fc5\u987b\u5c06\u5b83\u4eec\u8fde\u63a5\u5728\u4e00\u8d77\uff0c\u7136\u540e\u624d\u80fd\u5c06\u5176\u9988\u9001\u5230\u4e0b\u4e00\u4e2a\u7b97\u5b50\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u534f\u540c\u63a8\u7406\u7684\u5ef6\u8fdf\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4e3aCNN\u6a21\u578b\u63d0\u51fa\u4e86\u4ea4\u9519\u7b97\u5b50\u5206\u533a\uff08IOP\uff09\u7b56\u7565\u3002\u901a\u8fc7\u57fa\u4e8e\u8f93\u51fa\u901a\u9053\u7ef4\u5ea6\u5bf9\u4e00\u4e2a\u7b97\u5b50\u8fdb\u884c\u5206\u533a\uff0c\u5e76\u57fa\u4e8e\u8f93\u5165\u901a\u9053\u7ef4\u5ea6\u5bf9\u5176\u540e\u7eed\u7b97\u5b50\u8fdb\u884c\u5206\u533a\uff0c\u53ef\u4ee5\u907f\u514d\u6fc0\u6d3b\u8fde\u63a5\uff0c\u4ece\u800c\u51cf\u5c11\u901a\u4fe1\u8fde\u63a5\u7684\u6570\u91cf\uff0c\u4ece\u800c\u51cf\u5c11\u534f\u540c\u63a8\u7406\u5ef6\u8fdf\u3002\u57fa\u4e8eIOP\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u5206\u5272\u7b97\u6cd5\uff0c\u7528\u4e8e\u6700\u5c0f\u5316\u534f\u540c\u63a8\u7406\u65f6\u95f4\uff0c\u8be5\u7b97\u6cd5\u6839\u636e\u83b7\u5f97\u7684\u63a8\u7406\u5ef6\u8fdf\u6536\u76ca\uff0c\u8d2a\u5a6a\u5730\u9009\u62e9\u7528\u4e8eIOP\u914d\u5bf9\u7684\u7b97\u5b50\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0eCoEdge\u4e2d\u4f7f\u7528\u7684\u6700\u5148\u8fdb\u7684\u5206\u533a\u65b9\u6cd5\u76f8\u6bd4\uff0cIOP\u7b56\u7565\u5728\u4e09\u4e2a\u7ecf\u5178\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e866.39%~16.83%\u7684\u52a0\u901f\uff0c\u5e76\u5c06\u5cf0\u503c\u5185\u5b58\u5360\u7528\u51cf\u5c11\u4e8621.22%~49.98%\u3002||\n", "2409.07582": "|**2024-09-11**|[Minimizing Embedding Distortion for Robust Out-of-Distribution Performance](http://arxiv.org/abs/2409.07582)|null|\u57fa\u4e8e\u5e9e\u5927\u4e14\u591a\u6837\u5316\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u5728\u5404\u79cd\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u8de8\u4e0d\u540c\u9886\u57df\u548c\u5206\u5e03\u6cdb\u5316\u7684\u975e\u51e1\u80fd\u529b\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u89e3\u51b3\u4e86\u5728\u901a\u8fc7\u5fae\u8c03\u4f7f\u57fa\u7840\u6a21\u578b\u9002\u5e94\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\u65f6\uff0c\u5982\u4f55\u4fdd\u7559\u8fd9\u4e9b\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u7684\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u76f8\u4f3c\u6027\u635f\u5931\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u878d\u5165\u5230\u4efb\u4f55\u4efb\u52a1\u7684\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u3002\u901a\u8fc7\u6700\u5c0f\u5316\u5fae\u8c03\u5d4c\u5165\u4e0e\u9884\u8bad\u7ec3\u5d4c\u5165\u4e4b\u95f4\u7684\u626d\u66f2\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u7279\u5b9a\u4efb\u52a1\u9002\u5e94\u548c\u4fdd\u6301\u5e7f\u6cdb\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff1a\u536b\u661f\u56fe\u50cf\u7684\u56fe\u50cf\u5206\u7c7b\u548c\u4eba\u8138\u8bc6\u522b\uff0c\u91cd\u70b9\u5173\u6ce8\u5f00\u653e\u7c7b\u522b\u548c\u9886\u57df\u8fc1\u79fb\u573a\u666f\uff0c\u4ee5\u8bc4\u4f30\u5206\u5e03\u5916 (OOD) \u6027\u80fd\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u5f3a\u5927\u7684\u5206\u5e03\u5185 (ID) \u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86 OOD \u6027\u80fd\u3002||\n", "2409.07541": "|**2024-09-11**|[ENACT: Entropy-based Clustering of Attention Input for Improving the Computational Performance of Object Detection Transformers](http://arxiv.org/abs/2409.07541)|null|Transformer\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u5177\u6709\u7ade\u4e89\u529b\u7684\u7cbe\u5ea6\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6ce8\u610f\u529b\u6743\u91cd\u7684\u5e73\u65b9\u5927\u5c0f\uff0c\u5b83\u4eec\u9700\u8981\u76f8\u5f53\u5927\u7684\u8ba1\u7b97\u8d44\u6e90\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u6839\u636e\u8f93\u5165\u4fe1\u606f\u71b5\u5bf9transformer\u8f93\u5165\u8fdb\u884c\u805a\u7c7b\u3002\u8fd9\u6837\u505a\u7684\u539f\u56e0\u662f\uff0c\u6bcf\u4e2a\u50cf\u7d20\u7684\u81ea\u4fe1\u606f\uff08\u5176\u603b\u548c\u4e3a\u71b5\uff09\u5728\u5bf9\u5e94\u4e8e\u540c\u4e00\u5bf9\u8c61\u7684\u50cf\u7d20\u4e4b\u95f4\u53ef\u80fd\u662f\u76f8\u4f3c\u7684\u3002\u805a\u7c7b\u51cf\u5c11\u4e86\u4f5c\u4e3atransformer\u8f93\u5165\u7684\u6570\u636e\u91cf\uff0c\u56e0\u6b64\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u548cGPU\u5185\u5b58\u4f7f\u7528\u91cf\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u8981\u4f20\u9012\u5230\u7f51\u7edc\u5176\u4f59\u90e8\u5206\u7684\u6709\u610f\u4e49\u4fe1\u606f\u3002\u5efa\u8bae\u7684\u8fc7\u7a0b\u7ec4\u7ec7\u5728\u4e00\u4e2a\u540d\u4e3aENACT\u7684\u6a21\u5757\u4e2d\uff0c\u8be5\u6a21\u5757\u53ef\u4ee5\u63d2\u5165\u4efb\u4f55\u5728\u5176\u7f16\u7801\u5668\u4e2d\u5305\u542b\u591a\u5934\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u7684transformer\u67b6\u6784\u3002\u6211\u4eec\u4f7f\u7528COCO\u76ee\u6807\u68c0\u6d4b\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u68c0\u6d4btransformer\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u83b7\u5f97\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6240\u6709\u6d4b\u8bd5\u6848\u4f8b\u4e2d\uff0c\u6240\u9700\u7684\u8ba1\u7b97\u8d44\u6e90\u90fd\u6301\u7eed\u51cf\u5c11\uff0c\u800c\u68c0\u6d4b\u4efb\u52a1\u7684\u7cbe\u5ea6\u4ec5\u7565\u6709\u4e0b\u964d\u3002ENACT\u6a21\u5757\u7684\u4ee3\u7801\u5c06\u5728https://github.com/GSavathrakis/ENACT\u4e0a\u63d0\u4f9b\u3002||\n", "2409.07387": "|**2024-09-11**|[A Contrastive Symmetric Forward-Forward Algorithm (SFFA) for Continual Learning Tasks](http://arxiv.org/abs/2409.07387)|null|\u6240\u8c13\u7684\u201c\u6b63\u5411-\u6b63\u5411\u7b97\u6cd5\u201d(FFA) \u8fd1\u671f\u4f5c\u4e3a\u4e00\u79cd\u66ff\u4ee3\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u4e2d\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u7684\u65b0\u65b9\u6cd5\u83b7\u5f97\u4e86\u5173\u6ce8\uff0c\u5728\u5404\u79cd\u5efa\u6a21\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u901a\u8fc7\u7528\u4e24\u6b21\u5bf9\u6bd4\u6b63\u5411\u4f20\u9012\u4ee3\u66ff\u68af\u5ea6\u53cd\u5411\u4f20\u64ad\u7684\u53cd\u5411\u4f20\u9012\uff0cFFA \u901a\u8fc7\u542f\u7528\u9010\u5c42\u8bad\u7ec3\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u5176\u524d\u8eab\u6240\u7ecf\u5386\u7684\u51e0\u4e2a\u7f3a\u70b9\uff08\u4f8b\u5982\u68af\u5ea6\u6d88\u5931/\u7206\u70b8\uff09\u3002\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u8fd9\u79cd\u5bf9\u6bd4\u65b9\u6cd5\u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u6709\u6548\u5730\u521b\u5efa\u8f93\u5165\u6570\u636e\u7684\u6f5c\u5728\u7a00\u758f\u8868\u793a\uff0c\u6700\u7ec8\u6709\u5229\u4e8e\u533a\u5206\u6027\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6b63\u8d1f\u6570\u636e\u4e4b\u95f4\u635f\u5931\u51fd\u6570\u7684\u4e0d\u5e73\u8861\uff0cFFA \u8868\u73b0\u51fa\u56fa\u6709\u7684\u4e0d\u5bf9\u79f0\u68af\u5ea6\u884c\u4e3a\uff0c\u8fd9\u4f1a\u5bf9\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u5e76\u5bfc\u81f4\u51c6\u786e\u6027\u4e0b\u964d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u5bf9\u79f0\u6b63\u5411-\u6b63\u5411\u7b97\u6cd5 (SFFA)\uff0c\u8fd9\u662f\u5bf9\u539f\u59cb FFA \u7684\u4e00\u79cd\u65b0\u9896\u6539\u8fdb\uff0c\u5b83\u5c06\u6bcf\u4e00\u5c42\u5212\u5206\u4e3a\u6b63\u795e\u7ecf\u5143\u548c\u8d1f\u795e\u7ecf\u5143\u3002\u8fd9\u5141\u8bb8\u5c06\u5c40\u90e8\u9002\u5e94\u5ea6\u51fd\u6570\u5b9a\u4e49\u4e3a\u6b63\u795e\u7ecf\u5143\u6fc0\u6d3b\u4e0e\u6574\u4f53\u5c42\u6d3b\u52a8\u4e4b\u95f4\u7684\u6bd4\u7387\uff0c\u4ece\u800c\u5728\u8bad\u7ec3\u9636\u6bb5\u4ea7\u751f\u5bf9\u79f0\u7684\u635f\u5931\u60c5\u51b5\u3002\u4e3a\u4e86\u8bc4\u4f30\u6211\u4eec\u65b9\u6cd5\u589e\u5f3a\u7684\u6536\u655b\u6027\uff0c\u6211\u4eec\u4f7f\u7528\u591a\u4e2a\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u8fdb\u884c\u4e86\u591a\u9879\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u4f7f\u7528 SFFA \u8bad\u7ec3\u7684\u6a21\u578b\u4e0e\u5176\u4f7f\u7528 FFA \u8bad\u7ec3\u7684\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002\u4f5c\u4e3a\u8fd9\u79cd\u91cd\u65b0\u8868\u8ff0\u7684\u526f\u4ea7\u54c1\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u5c06\u9010\u5c42\u8bad\u7ec3\u7b97\u6cd5\u7528\u4e8e\u6301\u7eed\u5b66\u4e60 (CL) \u4efb\u52a1\u7684\u4f18\u52bf\u3002\u9010\u5c42\u8bad\u7ec3\u7b97\u6cd5\u5f15\u8d77\u7684\u795e\u7ecf\u5143\u7279\u5316\u53ca\u5176\u6fc0\u6d3b\u7684\u7a00\u758f\u6027\u4f7f\u5f97\u80fd\u591f\u5b9e\u73b0\u6709\u6548\u7684 CL \u7b56\u7565\uff0c\u5c06\u65b0\u77e5\u8bc6\uff08\u7c7b\u522b\uff09\u6574\u5408\u5230\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u540c\u65f6\u9632\u6b62\u707e\u96be\u6027\u5730\u9057\u5fd8\u5148\u524d...||\n", "2409.07322": "|**2024-09-11**|[Three-Dimensional, Multimodal Synchrotron Data for Machine Learning Applications](http://arxiv.org/abs/2409.07322)|null|Machine learning techniques are being increasingly applied in medical and physical sciences across a variety of imaging modalities; however, an important issue when developing these tools is the availability of good quality training data. Here we present a unique, multimodal synchrotron dataset of a bespoke zinc-doped Zeolite 13X sample that can be used to develop advanced deep learning and data fusion pipelines. Multi-resolution micro X-ray computed tomography was performed on a zinc-doped Zeolite 13X fragment to characterise its pores and features, before spatially resolved X-ray diffraction computed tomography was carried out to characterise the homogeneous distribution of sodium and zinc phases. Zinc absorption was controlled to create a simple, spatially isolated, two-phase material. Both raw and processed data is available as a series of Zenodo entries. Altogether we present a spatially resolved, three-dimensional, multimodal, multi-resolution dataset that can be used for the development of machine learning techniques. Such techniques include development of super-resolution, multimodal data fusion, and 3D reconstruction algorithm development.||\n", "2409.09031": "|**2024-09-13**|[Optically-Validated Microvascular Phantom for Super-Resolution Ultrasound Imaging](http://arxiv.org/abs/2409.09031)|null|\u8d85\u5206\u8fa8\u7387\u8d85\u58f0 (SRUS) \u901a\u8fc7\u5b9a\u4f4d\u548c\u8ddf\u8e2a\u7a7a\u95f4\u9694\u79bb\u7684\u5fae\u6ce1\u9020\u5f71\u5242\uff0c\u53ef\u89c6\u5316\u8d85\u58f0\u884d\u5c04\u6781\u9650\uff08\u6ce2\u957f ($\u03bb$)/2\uff09\u4ee5\u5916\u7684\u5fae\u8840\u7ba1\u7ed3\u6784\u3002SRUS \u6a21\u578b\u901a\u5e38\u7531\u7b80\u5355\u7684\u7ba1\u72b6\u7ed3\u6784\u7ec4\u6210\uff0c\u5176\u4e2d\u76f4\u5f84\u5c0f\u4e8e 100 \u5fae\u7c73\u7684\u901a\u9053\u4e0d\u53ef\u7528\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u6613\u788e\u4e14\u4e0d\u7a33\u5b9a\uff0c\u771f\u503c\u9a8c\u8bc1\u6709\u9650\uff0c\u5e76\u4e14\u5176\u7b80\u5355\u7684\u7ed3\u6784\u9650\u5236\u4e86 SRUS \u7b97\u6cd5\u7684\u8bc4\u4f30\u3002\u4e3a\u4e86\u5e2e\u52a9 SRUS \u7684\u5f00\u53d1\uff0c\u9700\u8981\u5177\u6709\u5df2\u77e5\u4e14\u751f\u7406\u76f8\u5173\u7684\u5fae\u8840\u7ba1\u7ed3\u6784\u7684\u575a\u56fa\u8010\u7528\u7684\u6a21\u578b\uff0c\u4ee5\u4fbf\u8fdb\u884c\u53ef\u91cd\u590d\u7684 SRUS \u6d4b\u8bd5\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u5236\u9020\u8010\u7528\u5fae\u8840\u7ba1\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u8be5\u6a21\u578b\u5141\u8bb8\u8fdb\u884c\u5149\u5b66\u6d4b\u91cf\u4ee5\u8fdb\u884c SRUS \u9a8c\u8bc1\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u5d4c\u5165\u805a\u4e8c\u7532\u57fa\u7845\u6c27\u70f7\u4e2d\u7684\u5fae\u8840\u7ba1\u9634\u6a21\u6765\u5236\u9020\u5fae\u8840\u7ba1\u6a21\u578b\u3002\u5c55\u793a\u4e86\u5177\u6709\u53ef\u53d8\u5fae\u8840\u7ba1\u5bc6\u5ea6\u7684\u5206\u652f\u5fae\u8840\u7ba1\u6a21\u578b\uff0c\u5176\u5149\u5b66\u9a8c\u8bc1\u7684\u8840\u7ba1\u76f4\u5f84\u4f4e\u81f3\u7ea6 60 \u5fae\u7c73\uff08\u03bb/5.8\uff1b\u03bb = \u7ea6 350 \u5fae\u7c73\uff09\u3002\u8fdb\u884c\u4e86 SRUS \u6210\u50cf\u5e76\u901a\u8fc7\u5149\u5b66\u6d4b\u91cf\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u5e73\u5747 SRUS \u8bef\u5dee\u4e3a 15.61 \u5fae\u7c73\uff08\u03bb/22\uff09\uff0c\u6807\u51c6\u504f\u5dee\u8bef\u5dee\u4e3a 11.44 \u5fae\u7c73\u3002\u4e00\u65e6\u5b9a\u4f4d\u7684\u5fae\u6ce1\u6570\u91cf\u8d85\u8fc7\u6bcf\u4e2a\u4f30\u8ba1\u76f4\u5f84 1000 \u4e2a\uff0c\u5e73\u5747\u8bef\u5dee\u964d\u4f4e\u81f3 7.93 \u5fae\u7c73\uff08\u03bb/44\uff09\u3002\u6b64\u5916\uff0c\u5236\u9020\u4e00\u5e74\u540e\u6d4b\u5f97\u7684\u58f0\u5b66\u548c\u5149\u5b66\u7279\u6027\u53d8\u5316\u5c0f\u4e8e 10% \u4ee5\u53ca\u6a21\u578b\u7684\u673a\u68b0\u97e7\u6027\u8bc1\u660e\u4e86\u5176\u957f\u671f\u8010\u7528\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u5236\u9020\u8010\u7528\u4e14\u7ecf\u8fc7\u5149\u5b66\u9a8c\u8bc1\u7684\u590d\u6742\u5fae\u8840\u7ba1\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u8be5\u6a21\u578b\u53ef\u7528\u4e8e\u91cf\u5316 SRUS \u6027\u80fd\u5e76\u4fc3\u8fdb\u5176\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002|\n", "2409.08943": "|**2024-09-13**|[Pushing Joint Image Denoising and Classification to the Edge](http://arxiv.org/abs/2409.08943)|null|\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06\u56fe\u50cf\u5206\u7c7b\u548c\u56fe\u50cf\u53bb\u566a\u76f8\u7ed3\u5408\uff0c\u65e8\u5728\u589e\u5f3a\u4eba\u7c7b\u5bf9\u8fb9\u7f18\u8bbe\u5907\uff08\u5982\u4f4e\u7167\u5ea6\u76d1\u63a7\u6444\u50cf\u5934\uff09\u6240\u62cd\u6444\u566a\u58f0\u56fe\u50cf\u7684\u611f\u77e5\u80fd\u529b\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u91cd\u8981\u7684\u662f\u8981\u4fdd\u7559\u4eba\u7c7b\u9a8c\u8bc1\u81ea\u52a8\u5206\u7c7b\u51b3\u7b56\u7684\u80fd\u529b\uff0c\u4ece\u800c\u8054\u5408\u5bf9\u56fe\u50cf\u8fdb\u884c\u53bb\u566a\u4ee5\u589e\u5f3a\u4eba\u7c7b\u611f\u77e5\u3002\u7531\u4e8e\u8fb9\u7f18\u8bbe\u5907\u8ba1\u7b97\u80fd\u529b\u6709\u9650\uff0c\u6211\u4eec\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u96c6\u6210\u8fd9\u4e24\u9879\u4efb\u52a1\u7684\u65b0\u578b\u67b6\u6784\u6765\u660e\u786e\u4f18\u5316\u6548\u7387\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4fee\u6539\u4e86\u4e00\u79cd\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u641c\u7d22\u5206\u7c7b\u5668\u4ee5\u641c\u7d22\u96c6\u6210\u6a21\u578b\uff0c\u540c\u65f6\u4f18\u5316\u76ee\u6807\u5ef6\u8fdf\u3001\u5206\u7c7b\u7cbe\u5ea6\u548c\u53bb\u566a\u6027\u80fd\u3002NAS \u67b6\u6784\u5728\u53bb\u566a\u548c\u5206\u7c7b\u65b9\u9762\u5747\u4f18\u4e8e\u6211\u4eec\u624b\u52a8\u8bbe\u8ba1\u7684\u65b9\u6848\uff0c\u53ef\u663e\u8457\u6539\u5584\u4eba\u7c7b\u611f\u77e5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u6237\u80fd\u591f\u6784\u5efa\u9488\u5bf9\u533b\u7597\u6210\u50cf\u3001\u76d1\u63a7\u7cfb\u7edf\u548c\u5de5\u4e1a\u68c0\u6d4b\u7b49\u9886\u57df\u7684\u5b9a\u5236\u67b6\u6784\u3002|\n", "2409.08885": "|**2024-09-13**|[Interactive Masked Image Modeling for Multimodal Object Detection in Remote Sensing](http://arxiv.org/abs/2409.08885)|null|\u9065\u611f\u5f71\u50cf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u5728\u5730\u7403\u89c2\u6d4b\u7684\u5404\u4e2a\u5e94\u7528\u4e2d\u90fd\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u4e0e\u81ea\u7136\u573a\u666f\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u4e0d\u540c\uff0c\u7531\u4e8e\u4e0d\u540c\u5730\u5f62\u4e2d\u5b58\u5728\u5927\u91cf\u7684\u5c0f\u578b\u4e14\u901a\u5e38\u96be\u4ee5\u5bdf\u89c9\u7684\u76ee\u6807\uff0c\u8fd9\u9879\u4efb\u52a1\u5c24\u5176\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u53ef\u4ee5\u4f7f\u7528\u591a\u6a21\u6001\u5b66\u4e60\u6765\u6574\u5408\u6765\u81ea\u4e0d\u540c\u6570\u636e\u6a21\u6001\u7684\u7279\u5f81\uff0c\u4ece\u800c\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002\u7136\u800c\uff0c\u591a\u6a21\u6001\u5b66\u4e60\u7684\u6027\u80fd\u5f80\u5f80\u53d7\u5230\u6807\u8bb0\u6570\u636e\u96c6\u5927\u5c0f\u6709\u9650\u7684\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528\u63a9\u853d\u56fe\u50cf\u5efa\u6a21\uff08MIM\uff09\u4f5c\u4e3a\u9884\u8bad\u7ec3\u6280\u672f\uff0c\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6765\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684 MIM \u65b9\u6cd5\uff08\u5982 MAE\uff09\u4f7f\u7528\u4e0d\u5305\u542b\u4efb\u4f55\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u63a9\u7801\u6807\u8bb0\uff0c\u7531\u4e8e\u7f3a\u4e4f\u4e0e\u56fe\u50cf\u5176\u4ed6\u90e8\u5206\u7684\u4ea4\u4e92\uff0c\u96be\u4ee5\u6355\u6349\u5230\u7ec6\u7c92\u5ea6\u7684\u7ec6\u8282\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ea4\u4e92\u5f0f MIM \u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u540c\u6807\u8bb0\u4e4b\u95f4\u5efa\u7acb\u4ea4\u4e92\uff0c\u8fd9\u5bf9\u4e8e\u9065\u611f\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u7279\u522b\u6709\u5229\u3002\u5927\u91cf\u7684\u6d88\u878d\u7814\u7a76\u548c\u8bc4\u4f30\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002|\n", "2409.08840": "|**2024-09-13**|[Direct-CP: Directed Collaborative Perception for Connected and Autonomous Vehicles via Proactive Attention](http://arxiv.org/abs/2409.08840)|null|\u534f\u540c\u611f\u77e5 (CP) \u5229\u7528\u6765\u81ea\u8054\u7f51\u548c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86 (CAV) \u7684\u89c6\u89c9\u6570\u636e\u6765\u589e\u5f3a\u81ea\u8f66\u89c6\u91ce (FoV)\u3002\u5c3d\u7ba1\u6700\u8fd1\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u76ee\u524d\u7684 CP \u65b9\u6cd5\u51e0\u4e4e\u5e73\u7b49\u5730\u6269\u5c55\u4e86\u81ea\u8f66\u7684 360 \u5ea6\u611f\u77e5\u8303\u56f4\uff0c\u8fd9\u9762\u4e34\u7740\u4e24\u4e2a\u5173\u952e\u6311\u6218\u3002\u9996\u5148\uff0c\u5728\u4ea4\u901a\u5206\u5e03\u4e0d\u5747\u5300\u7684\u5730\u533a\uff0c\u5173\u6ce8\u4ea4\u901a\u6d41\u91cf\u5c0f\u7684\u65b9\u5411\u5e26\u6765\u7684\u597d\u5904\u6709\u9650\u3002\u5176\u6b21\uff0c\u5728\u6709\u9650\u7684\u901a\u4fe1\u9884\u7b97\u4e0b\uff0c\u4e3a\u4e0d\u592a\u91cd\u8981\u7684\u65b9\u5411\u5206\u914d\u8fc7\u591a\u7684\u5e26\u5bbd\u4f1a\u964d\u4f4e\u66f4\u91cd\u8981\u533a\u57df\u7684\u611f\u77e5\u7cbe\u5ea6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Direct-CP\uff0c\u4e00\u79cd\u4e3b\u52a8\u4e14\u65b9\u5411\u611f\u77e5\u7684 CP \u7cfb\u7edf\uff0c\u65e8\u5728\u6539\u5584\u7279\u5b9a\u65b9\u5411\u7684 CP\u3002\u6211\u4eec\u7684\u6838\u5fc3\u7406\u5ff5\u662f\u4f7f\u81ea\u8f66\u80fd\u591f\u4e3b\u52a8\u53d1\u51fa\u5176\u611f\u5174\u8da3\u65b9\u5411\u7684\u4fe1\u53f7\uff0c\u5e76\u91cd\u65b0\u8c03\u6574\u5176\u6ce8\u610f\u529b\u4ee5\u589e\u5f3a\u5c40\u90e8\u65b9\u5411\u6027 CP \u6027\u80fd\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u79cd RSU \u8f85\u52a9\u65b9\u5411\u63a9\u853d\u673a\u5236\uff0c\u4ee5\u5e2e\u52a9\u81ea\u8f66\u8bc6\u522b\u91cd\u8981\u65b9\u5411\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b9\u5411\u611f\u77e5\u7684\u9009\u62e9\u6027\u6ce8\u610f\u6a21\u5757\uff0c\u6839\u636e\u81ea\u8f66\u7684\u65b9\u5411\u4f18\u5148\u7ea7\u3001\u901a\u4fe1\u9884\u7b97\u548c CAV \u7684\u4f4d\u7f6e\u6570\u636e\uff0c\u660e\u667a\u5730\u805a\u5408\u76f8\u5173\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u65b9\u5411\u52a0\u6743\u68c0\u6d4b\u635f\u5931 (DWLoss) \u6765\u6355\u6349\u65b9\u5411\u6027 CP \u7ed3\u679c\u4e0e\u771f\u5b9e\u60c5\u51b5\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4ece\u800c\u4fc3\u8fdb\u6709\u6548\u7684\u6a21\u578b\u8bad\u7ec3\u3002\u5728 V2X-Sim 2.0 \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u534f\u4f5c 3D \u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u611f\u5174\u8da3\u65b9\u5411\u7684\u5c40\u90e8\u611f\u77e5\u7cbe\u5ea6\u63d0\u9ad8\u4e86 19.8%\uff0c\u6574\u4f53\u611f\u77e5\u7cbe\u5ea6\u63d0\u9ad8\u4e86 2.5%\u3002|\n", "2409.08667": "|**2024-09-13**|[Test-time Training for Hyperspectral Image Super-resolution](http://arxiv.org/abs/2409.08667)|null|\u9ad8\u5149\u8c31\u56fe\u50cf (HSI) \u8d85\u5206\u8fa8\u7387 (SR) \u7684\u7814\u7a76\u8fdb\u5c55\u4ecd\u7136\u843d\u540e\u4e8e RGB \u56fe\u50cf SR \u7684\u7814\u7a76\u3002HSI \u901a\u5e38\u5177\u6709\u5927\u91cf\u7684\u6ce2\u6bb5\uff0c\u56e0\u6b64\u51c6\u786e\u5730\u6a21\u62df HSI SR \u7684\u6ce2\u6bb5\u95f4\u4ea4\u4e92\u975e\u5e38\u56f0\u96be\u3002\u6b64\u5916\uff0cHSI SR \u7684\u8bad\u7ec3\u6570\u636e\u96be\u4ee5\u83b7\u53d6\uff0c\u56e0\u6b64\u6570\u636e\u96c6\u901a\u5e38\u5f88\u5c0f\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684\u81ea\u8bad\u7ec3\u6846\u67b6\uff0c\u53ef\u4ee5\u751f\u6210\u66f4\u51c6\u786e\u7684\u4f2a\u6807\u7b7e\u548c\u66f4\u51c6\u786e\u7684 LR-HR \u5173\u7cfb\uff0c\u4ee5\u4fbf\u6a21\u578b\u53ef\u4ee5\u4f7f\u7528\u5b83\u4eec\u8fdb\u884c\u8fdb\u4e00\u6b65\u8bad\u7ec3\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u652f\u6301\u6211\u4eec\u7684\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7f51\u7edc\u67b6\u6784\u6765\u5b66\u4e60 HSI SR\uff0c\u800c\u65e0\u9700\u5bf9\u6ce2\u6bb5\u95f4\u4ea4\u4e92\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5 Spectral Mixup\uff0c\u4ee5\u589e\u52a0\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u6570\u636e\u7684\u7684\u591a\u6837\u6027\u3002\u6211\u4eec\u8fd8\u6536\u96c6\u4e86\u4e00\u4e2a\u65b0\u7684 HSI \u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4ece\u98df\u7269\u5230\u690d\u88ab\u3001\u6750\u6599\u548c\u4e00\u822c\u573a\u666f\u7b49\u5404\u79cd\u6709\u8da3\u5bf9\u8c61\u7684\u56fe\u50cf\u3002\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u540e\u663e\u7740\u63d0\u9ad8\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5728 HSI SR \u65b9\u9762\u663e\u7740\u4f18\u4e8e\u7ade\u4e89\u65b9\u6cd5\u3002|\n", "2409.08650": "|**2024-09-13**|[Low Complexity DoA-ToA Signature Estimation for Multi-Antenna Multi-Carrier Systems](http://arxiv.org/abs/2409.08650)|null|\u51c6\u786e\u7684\u65b9\u5411\u4f30\u8ba1 (DoA) \u548c\u5230\u8fbe\u65f6\u95f4 (ToA) \u4f30\u8ba1\u662f\u58f0\u7eb3\u3001\u96f7\u8fbe\u3001\u901a\u4fe1\u548c\u53cc\u529f\u80fd\u96f7\u8fbe\u901a\u4fe1 (DFRC) \u7b49\u591a\u79cd\u65e0\u7ebf\u7cfb\u7edf\u7684\u4e25\u683c\u8981\u6c42\u3002\u7531\u4e8e\u4f7f\u7528\u9ad8\u8f7d\u6ce2\u9891\u7387\u548c\u5e26\u5bbd\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5927\u591a\u6570\u8bbe\u8ba1\u6709\u591a\u4e2a\u5929\u7ebf\u548c\u5b50\u8f7d\u6ce2\u3002\u5c3d\u7ba1\u5927\u9635\u5217\u673a\u5236\u4e0b\u7684\u5206\u8fa8\u7387\u5f88\u9ad8\uff0c\u4f46\u7531\u4e8e\u9891\u8c31\u6cc4\u6f0f\u6548\u5e94\uff0c\u5b9e\u9645\u7684\u7f51\u683c\u4f30\u8ba1\u65b9\u6cd5\u7684 DoA-ToA \u4f30\u8ba1\u7cbe\u5ea6\u4ecd\u7136\u5b58\u5728\u4f30\u8ba1\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u9488\u5bf9\u5177\u6709\u6b63\u4ea4\u9891\u5206\u590d\u7528 (OFDM) \u4fe1\u53f7\u7684\u591a\u5929\u7ebf\u591a\u8f7d\u6ce2\u7cfb\u7edf\u7684 DoA-ToA \u4f30\u8ba1\u65b9\u6cd5\u3002\u5728\u7b2c\u4e00\u79cd\u65b9\u6cd5\u4e2d\uff0c\u6211\u4eec\u5e94\u7528\u4e86\u57fa\u4e8e\u79bb\u6563\u5085\u7acb\u53f6\u53d8\u6362 (DFT) \u7684\u7c97\u7565\u7279\u5f81\u4f30\u8ba1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7684\u591a\u7ea7\u5fae\u8c03\u65b9\u6cd5\uff0c\u4ee5\u6781\u5927\u5730\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u3002\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u57fa\u4e8e\u538b\u7f29\u611f\u77e5\uff0c\u5176\u4e2d\u6211\u4eec\u901a\u8fc7\u91c7\u7528\u6bd4\u5929\u7ebf\u548c\u5b50\u8f7d\u6ce2\u57fa\u6570\u5b9e\u9645\u6570\u91cf\u66f4\u591a\u7684\u4e8c\u7ef4\u8fc7\u5b8c\u5907\u89d2\u5ea6\u5ef6\u8fdf\u5b57\u5178\u6765\u5b9e\u73b0\u8d85\u5206\u8fa8\u7387\u3002\u4e0e\u5411\u91cf\u5316\u4e00\u7ef4\u6b63\u4ea4\u5339\u914d\u8ffd\u8e2a (OMP) \u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u5c06\u4f4e\u590d\u6742\u5ea6\u7684\u4e8c\u7ef4 OMP \u65b9\u6cd5\u5e94\u7528\u4e8e\u77e9\u9635\u6570\u636e\u6a21\u578b\uff0c\u8fd9\u4f7f\u5f97\u5728\u5927\u578b\u9635\u5217\u673a\u5236\u4e2d\u4f7f\u7528\u538b\u7f29\u611f\u77e5\u65b9\u6cd5\u53d8\u5f97\u5207\u5b9e\u53ef\u884c\u3002\u901a\u8fc7\u6570\u503c\u4eff\u771f\uff0c\u6211\u4eec\u8868\u660e\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e0e\u57fa\u4e8e\u5b50\u7a7a\u95f4\u7684\u4e8c\u7ef4\u591a\u91cd\u4fe1\u53f7\u5206\u7c7b (MUSIC) \u65b9\u6cd5\u76f8\u4f3c\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u663e\u7740\u964d\u4f4e\u3002|\n", "2409.08640": "|**2024-09-13**|[Byzantine-Robust and Communication-Efficient Distributed Learning via Compressed Momentum Filtering](http://arxiv.org/abs/2409.08640)|null|\u5206\u5e03\u5f0f\u5b66\u4e60\u5df2\u6210\u4e3a\u8de8\u79c1\u6709\u6570\u636e\u5b64\u5c9b\u8bad\u7ec3\u5927\u89c4\u6a21\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6807\u51c6\u65b9\u6cd5\u3002\u867d\u7136\u5206\u5e03\u5f0f\u5b66\u4e60\u589e\u5f3a\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u4f46\u5b83\u4e5f\u9762\u4e34\u7740\u4e0e\u62dc\u5360\u5ead\u9c81\u68d2\u6027\u548c\u901a\u4fe1\u51cf\u5c11\u76f8\u5173\u7684\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7684\u62dc\u5360\u5ead\u9c81\u68d2\u4e14\u9ad8\u6548\u901a\u4fe1\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6bcf\u6b21\u8fed\u4ee3\u6216\u4ee5\u4e00\u5b9a\u6982\u7387\u5728\u67d0\u4e9b\u8fed\u4ee3\u4e2d\u83b7\u5f97\u5b8c\u6574\u7684\u68af\u5ea6\u4fe1\u606f\uff0c\u5e76\u4e14\u5b83\u4eec\u4ec5\u6536\u655b\u5230\u89e3\u5468\u56f4\u4e00\u4e2a\u4e0d\u5fc5\u8981\u7684\u5927\u7684\u90bb\u57df\u3002\u57fa\u4e8e\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u62dc\u5360\u5ead\u9c81\u68d2\u4e14\u9ad8\u6548\u901a\u4fe1\u7684\u968f\u673a\u5206\u5e03\u5f0f\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5bf9\u6279\u91cf\u5927\u5c0f\u6ca1\u6709\u4efb\u4f55\u8981\u6c42\uff0c\u5e76\u4e14\u6536\u655b\u5230\u6bd4\u6240\u6709\u73b0\u6709\u65b9\u6cd5\u90fd\u66f4\u63a5\u8fd1\u6700\u4f18\u89e3\u7684\u5c0f\u90bb\u57df\uff0c\u4e0e\u7406\u8bba\u4e0b\u754c\u4e00\u81f4\u3002\u6211\u4eec\u7684\u5173\u952e\u521b\u65b0\u662f\u5229\u7528 Polyak \u52a8\u91cf\u6765\u51cf\u8f7b\u7531\u6709\u504f\u538b\u7f29\u5668\u548c\u968f\u673a\u68af\u5ea6\u5f15\u8d77\u7684\u566a\u58f0\uff0c\u4ece\u800c\u5728\u4fe1\u606f\u538b\u7f29\u7684\u60c5\u51b5\u4e0b\u9632\u5fa1\u62dc\u5360\u5ead\u5de5\u4f5c\u8005\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u5728\u975e\u51f8\u5e73\u6ed1\u635f\u5931\u51fd\u6570\u7684\u80cc\u666f\u4e0b\uff0c\u6211\u4eec\u7b97\u6cd5\u7684\u7d27\u590d\u6742\u5ea6\u754c\u9650\u7684\u8bc1\u660e\uff0c\u8bc1\u660e\u8fd9\u4e9b\u754c\u9650\u4e0e\u65e0\u62dc\u5360\u5ead\u573a\u666f\u4e2d\u7684\u4e0b\u754c\u76f8\u5339\u914d\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u4e00\u7cfb\u5217\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7b97\u6cd5\u7684\u5b9e\u9645\u610f\u4e49\uff0c\u5bf9\u4e8c\u8fdb\u5236\u5206\u7c7b\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002|\n", "2409.08551": "|**2024-09-13**|[Think Twice Before You Act: Improving Inverse Problem Solving With MCMC](http://arxiv.org/abs/2409.08551)|null|\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u89e3\u51b3\u9006\u95ee\u9898\u7684\u5f3a\u6709\u529b\u5148\u9a8c\u3002\u4e00\u4e2a\u7a81\u51fa\u7684\u4f8b\u5b50\u662f\u6269\u6563\u540e\u9a8c\u91c7\u6837\uff08DPS\uff09\uff0c\u5b83\u4f7f\u7528Tweedie\u516c\u5f0f\u6765\u8fd1\u4f3c\u7ed9\u5b9a\u6d4b\u91cf\u503c\u7684\u6570\u636e\u540e\u9a8c\u5206\u5e03\u3002\u5c3d\u7ba1DPS\u5728\u89e3\u51b3\u5404\u79cd\u9006\u95ee\u9898\u65f6\u5177\u6709\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u4f18\u70b9\uff0c\u4f46\u7531\u4e8e\u8fd9\u79cd\u540e\u9a8c\u8fd1\u4f3c\u53ef\u80fd\u4e0d\u51c6\u786e\uff0c\u7279\u522b\u662f\u5728\u9ad8\u566a\u58f0\u6c34\u5e73\u4e0b\uff0c\u56e0\u6b64\u5176\u6027\u80fd\u53d7\u5230\u9650\u5236\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6269\u6563\u540e\u9a8cMCMC\uff08DPMC\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u9000\u706bMCMC\u7684\u65b0\u578b\u63a8\u7406\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4f7f\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u9006\u95ee\u9898\u3002\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u7cfb\u5217\u4e2d\u95f4\u5206\u5e03\uff0c\u5176\u7075\u611f\u6765\u81eaDPS\u4f7f\u7528\u7684\u8fd1\u4f3c\u6761\u4ef6\u5206\u5e03\u3002\u901a\u8fc7\u9000\u706bMCMC\u91c7\u6837\uff0c\u6211\u4eec\u9f13\u52b1\u6837\u672c\u5728\u79fb\u52a8\u5230\u566a\u58f0\u6c34\u5e73\u8f83\u4f4e\u7684\u4e0b\u4e00\u4e2a\u5206\u5e03\u4e4b\u524d\uff0c\u66f4\u7d27\u5bc6\u5730\u9075\u5faa\u6bcf\u4e2a\u4e2d\u95f4\u5206\u5e03\uff0c\u4ece\u800c\u51cf\u5c11\u6cbf\u8def\u5f84\u7d2f\u79ef\u7684\u8bef\u5dee\u3002\u6211\u4eec\u5728\u5404\u79cd\u9006\u95ee\u9898\u4e2d\u6d4b\u8bd5\u4e86\u6211\u4eec\u7684\u7b97\u6cd5\uff0c\u5305\u62ec\u8d85\u5206\u8fa8\u7387\u3001\u9ad8\u65af\u53bb\u6a21\u7cca\u3001\u8fd0\u52a8\u53bb\u6a21\u7cca\u3001\u4fee\u590d\u548c\u76f8\u4f4d\u68c0\u7d22\u3002\u6211\u4eec\u7684\u7b97\u6cd5\u5728\u51e0\u4e4e\u6240\u6709\u4efb\u52a1\u4e2d\u90fd\u4f18\u4e8eDPS\uff0c\u5e76\u4e14\u8bc4\u4f30\u6b21\u6570\u66f4\u5c11\uff0c\u5e76\u4e14\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002|\n", "2409.08376": "|**2024-09-12**|[Learned Compression for Images and Point Clouds](http://arxiv.org/abs/2409.08376)|**[link](https://github.com/multimedialabsfu/learned-point-cloud-compression-for-classification)**|\u5728\u8fc7\u53bb\u5341\u5e74\u4e2d\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u6267\u884c\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08\u5305\u62ec\u5206\u7c7b\u3001\u8d85\u5206\u8fa8\u7387\u548c\u98ce\u683c\u8fc1\u79fb\uff09\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u73b0\u5728\uff0c\u6211\u4eec\u5c06\u5176\u5e94\u7528\u4e8e\u6570\u636e\u538b\u7f29\uff0c\u4ee5\u5e2e\u52a9\u6784\u5efa\u4e0b\u4e00\u4ee3\u591a\u5a92\u4f53\u7f16\u89e3\u7801\u5668\u3002\u672c\u8bba\u6587\u5bf9\u8fd9\u4e00\u65b0\u5174\u7684\u5b66\u4e60\u538b\u7f29\u9886\u57df\u505a\u51fa\u4e86\u4e09\u4e2a\u4e3b\u8981\u8d21\u732e\u3002\u9996\u5148\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4f4e\u590d\u6742\u5ea6\u71b5\u6a21\u578b\uff0c\u5b83\u901a\u8fc7\u5c06\u7f16\u7801\u5206\u5e03\u672c\u8eab\u4f5c\u4e3a\u8fb9\u4fe1\u606f\u8fdb\u884c\u538b\u7f29\u548c\u4f20\u8f93\uff0c\u4ece\u800c\u52a8\u6001\u5730\u4f7f\u7f16\u7801\u5206\u5e03\u9002\u5e94\u7279\u5b9a\u7684\u8f93\u5165\u3002\u5176\u6b21\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8f7b\u91cf\u7ea7\u4f4e\u590d\u6742\u5ea6\u70b9\u4e91\u7f16\u89e3\u7801\u5668\uff0c\u8be5\u7f16\u89e3\u7801\u5668\u4e13\u95e8\u9488\u5bf9\u5206\u7c7b\u8fdb\u884c\u4e86\u9ad8\u5ea6\u4f18\u5316\uff0c\u4e0e\u975e\u4e13\u95e8\u7f16\u89e3\u7801\u5668\u76f8\u6bd4\uff0c\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u6bd4\u7279\u7387\u3002\u6700\u540e\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86\u8fde\u7eed\u89c6\u9891\u5e27\u4e4b\u95f4\u8f93\u5165\u57df\u5185\u7684\u8fd0\u52a8\u662f\u5982\u4f55\u4f53\u73b0\u5728\u76f8\u5e94\u7684\u5377\u79ef\u5bfc\u51fa\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u3002|\n", "2409.11235": "|**2024-09-17**|[SLAck: Semantic, Location, and Appearance Aware Open-Vocabulary Tracking](http://arxiv.org/abs/2409.11235)|**[link](https://github.com/siyuanliii/slack)**|\u5f00\u653e\u8bcd\u6c47\u591a\u76ee\u6807\u8ddf\u8e2a (MOT) \u65e8\u5728\u5c06\u8ddf\u8e2a\u5668\u6cdb\u5316\u5230\u8bad\u7ec3\u96c6\u4e2d\u6ca1\u6709\u7684\u65b0\u7c7b\u522b\u3002\u76ee\u524d\uff0c\u6027\u80fd\u6700\u597d\u7684\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u7eaf\u7cb9\u7684\u5916\u89c2\u5339\u914d\u3002\u7531\u4e8e\u5927\u8bcd\u6c47\u573a\u666f\u4e2d\u8fd0\u52a8\u6a21\u5f0f\u7684\u590d\u6742\u6027\u548c\u65b0\u5bf9\u8c61\u7684\u5206\u7c7b\u4e0d\u7a33\u5b9a\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u8fd0\u52a8\u548c\u8bed\u4e49\u7ebf\u7d22\uff0c\u8981\u4e48\u5728\u6700\u7ec8\u5339\u914d\u6b65\u9aa4\u4e2d\u57fa\u4e8e\u542f\u53d1\u5f0f\u65b9\u6cd5\u5e94\u7528\u5b83\u4eec\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6 SLAck\uff0c\u8be5\u6846\u67b6\u5728\u5173\u8054\u7684\u65e9\u671f\u6b65\u9aa4\u4e2d\u8054\u5408\u8003\u8651\u8bed\u4e49\u3001\u4f4d\u7f6e\u548c\u5916\u89c2\u5148\u9a8c\uff0c\u5e76\u5b66\u4e60\u5982\u4f55\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u65f6\u7a7a\u5bf9\u8c61\u56fe\u6574\u5408\u6240\u6709\u6709\u4ef7\u503c\u7684\u4fe1\u606f\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6d88\u9664\u4e86\u7528\u4e8e\u878d\u5408\u4e0d\u540c\u7ebf\u7d22\u7684\u590d\u6742\u540e\u5904\u7406\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u663e\u7740\u63d0\u9ad8\u4e86\u5927\u89c4\u6a21\u5f00\u653e\u8bcd\u6c47\u8ddf\u8e2a\u7684\u5173\u8054\u6027\u80fd\u3002\u5728\u6ca1\u6709\u8fc7\u591a\u88c5\u9970\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u5728\u5f00\u653e\u8bcd\u6c47 MOT \u548c TAO TETA \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u65b0\u7c7b\u522b\u8ddf\u8e2a\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 \\href{https://github.com/siyuanliii/SLAck}{github.com/siyuanliii/SLAck} \u83b7\u53d6\u3002|\n", "2409.11234": "|**2024-09-17**|[STCMOT: Spatio-Temporal Cohesion Learning for UAV-Based Multiple Object Tracking](http://arxiv.org/abs/2409.11234)|**[link](https://github.com/ydhcg-bobo/stcmot)**|Multiple object tracking (MOT) in Unmanned Aerial Vehicle (UAV) videos is important for diverse applications in computer vision. Current MOT trackers rely on accurate object detection results and precise matching of target reidentification (ReID). These methods focus on optimizing target spatial attributes while overlooking temporal cues in modelling object relationships, especially for challenging tracking conditions such as object deformation and blurring, etc. To address the above-mentioned issues, we propose a novel Spatio-Temporal Cohesion Multiple Object Tracking framework (STCMOT), which utilizes historical embedding features to model the representation of ReID and detection features in a sequential order. Concretely, a temporal embedding boosting module is introduced to enhance the discriminability of individual embedding based on adjacent frame cooperation. While the trajectory embedding is then propagated by a temporal detection refinement module to mine salient target locations in the temporal field. Extensive experiments on the VisDrone2019 and UAVDT datasets demonstrate our STCMOT sets a new state-of-the-art performance in MOTA and IDF1 metrics. The source codes are released at https://github.com/ydhcg-BoBo/STCMOT.|\n", "2409.11175": "|**2024-09-17**|[Vision foundation models: can they be applied to astrophysics data?](http://arxiv.org/abs/2409.11175)|null|Vision foundation models, which have demonstrated significant potential in many multimedia applications, are often underutilized in the natural sciences. This is primarily due to mismatches between the nature of domain-specific scientific data and the typical training data used for foundation models, leading to distribution shifts. Scientific data often differ substantially in structure and characteristics; researchers frequently face the challenge of optimizing model performance with limited labeled data of only a few hundred or thousand images. To adapt foundation models effectively requires customized approaches in preprocessing, data augmentation, and training techniques. Additionally, each vision foundation model exhibits unique strengths and limitations, influenced by differences in architecture, training procedures, and the datasets used for training. In this work, we evaluate the application of various vision foundation models to astrophysics data, specifically images from optical and radio astronomy. Our results show that using features extracted by specific foundation models improves the classification accuracy of optical galaxy images compared to conventional supervised training. Similarly, these models achieve equivalent or better performance in object detection tasks with radio images. However, their performance in classifying radio galaxy images is generally poor and often inferior to traditional supervised training results. These findings suggest that selecting suitable vision foundation models for astrophysics applications requires careful consideration of the model characteristics and alignment with the specific requirements of the downstream tasks.|\n", "2409.11018": "|**2024-09-17**|[Unleashing the Potential of Mamba: Boosting a LiDAR 3D Sparse Detector by Using Cross-Model Knowledge Distillation](http://arxiv.org/abs/2409.11018)|null|The LiDAR-based 3D object detector that strikes a balance between accuracy and speed is crucial for achieving real-time perception in autonomous driving and robotic navigation systems. To enhance the accuracy of point cloud detection, integrating global context for visual understanding improves the point clouds ability to grasp overall spatial information. However, many existing LiDAR detection models depend on intricate feature transformation and extraction processes, leading to poor real-time performance and high resource consumption, which limits their practical effectiveness. In this work, we propose a Faster LiDAR 3D object detection framework, called FASD, which implements heterogeneous model distillation by adaptively uniform cross-model voxel features. We aim to distill the transformer's capacity for high-performance sequence modeling into Mamba models with low FLOPs, achieving a significant improvement in accuracy through knowledge transfer. Specifically, Dynamic Voxel Group and Adaptive Attention strategies are integrated into the sparse backbone, creating a robust teacher model with scale-adaptive attention for effective global visual context modeling. Following feature alignment with the Adapter, we transfer knowledge from the Transformer to the Mamba through latent space feature supervision and span-head distillation, resulting in improved performance and an efficient student model. We evaluated the framework on the Waymo and nuScenes datasets, achieving a 4x reduction in resource consumption and a 1-2\\% performance improvement over the current SoTA methods.|\n", "2409.10901": "|**2024-09-17**|[TrajSSL: Trajectory-Enhanced Semi-Supervised 3D Object Detection](http://arxiv.org/abs/2409.10901)|null|Semi-supervised 3D object detection is a common strategy employed to circumvent the challenge of manually labeling large-scale autonomous driving perception datasets. Pseudo-labeling approaches to semi-supervised learning adopt a teacher-student framework in which machine-generated pseudo-labels on a large unlabeled dataset are used in combination with a small manually-labeled dataset for training. In this work, we address the problem of improving pseudo-label quality through leveraging long-term temporal information captured in driving scenes. More specifically, we leverage pre-trained motion-forecasting models to generate object trajectories on pseudo-labeled data to further enhance the student model training. Our approach improves pseudo-label quality in two distinct manners: first, we suppress false positive pseudo-labels through establishing consistency across multiple frames of motion forecasting outputs. Second, we compensate for false negative detections by directly inserting predicted object tracks into the pseudo-labeled scene. Experiments on the nuScenes dataset demonstrate the effectiveness of our approach, improving the performance of standard semi-supervised approaches in a variety of settings.|\n", "2409.10836": "|**2024-09-17**|[Single-Layer Learnable Activation for Implicit Neural Representation (SL$^{2}$A-INR)](http://arxiv.org/abs/2409.10836)|null|\u9690\u5f0f\u795e\u7ecf\u8868\u793a (INR) \u5229\u7528\u795e\u7ecf\u7f51\u7edc\u5c06\u5750\u6807\u8f93\u5165\u8f6c\u6362\u4e3a\u76f8\u5e94\u7684\u5c5e\u6027\uff0c\u8fd1\u5e74\u6765\u5728\u591a\u4e2a\u89c6\u89c9\u76f8\u5173\u9886\u57df\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\u3002\u7136\u800c\uff0cINR \u7684\u6027\u80fd\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d7\u5176\u591a\u5c42\u611f\u77e5\u5668 (MLP) \u67b6\u6784\u4e2d\u4f7f\u7528\u7684\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u9009\u62e9\u7684\u5f71\u54cd\u3002\u76ee\u524d\u5df2\u7ecf\u7814\u7a76\u4e86\u591a\u79cd\u975e\u7ebf\u6027\u65b9\u6cd5\uff1b\u7136\u800c\uff0c\u5f53\u524d\u7684 INR \u5728\u6355\u83b7\u9ad8\u9891\u5206\u91cf\u3001\u591a\u6837\u4fe1\u53f7\u7c7b\u578b\u548c\u5904\u7406\u9006\u95ee\u9898\u65b9\u9762\u9762\u4e34\u5c40\u9650\u6027\u3002\u6211\u4eec\u5df2\u7ecf\u786e\u5b9a\uff0c\u901a\u8fc7\u5f15\u5165 INR \u7684\u8303\u5f0f\u8f6c\u53d8\u53ef\u4ee5\u5927\u5927\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u5728\u521d\u59cb\u5c42\u5177\u6709\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u7684\u67b6\u6784\u53ef\u4ee5\u8868\u793a\u5e95\u5c42\u4fe1\u53f7\u4e2d\u7684\u7cbe\u7ec6\u7ec6\u8282\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 SL$^{2}$A-INR\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e INR \u7684\u6df7\u5408\u7f51\u7edc\uff0c\u5177\u6709\u5355\u5c42\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u4f20\u7edf\u57fa\u4e8e ReLU \u7684 MLP \u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u56fe\u50cf\u8868\u793a\u30013D \u5f62\u72b6\u91cd\u5efa\u3001\u56fe\u50cf\u4fee\u590d\u3001\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u3001CT \u91cd\u5efa\u548c\u65b0\u89c6\u56fe\u5408\u6210\u3002\u901a\u8fc7\u7efc\u5408\u5b9e\u9a8c\uff0cSL$^{2}$A-INR \u5728 INR \u7684\u51c6\u786e\u6027\u3001\u8d28\u91cf\u548c\u6536\u655b\u901f\u5ea6\u65b9\u9762\u6811\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002|\n", "2409.10811": "|**2024-09-17**|[Context-Dependent Interactable Graphical User Interface Element Detection for VR Applications](http://arxiv.org/abs/2409.10811)|null|In recent years, Virtual Reality (VR) has emerged as a transformative technology, offering users immersive and interactive experiences across diversified virtual environments. Users can interact with VR apps through interactable GUI elements (IGEs) on the stereoscopic three-dimensional (3D) graphical user interface (GUI). The accurate recognition of these IGEs is instrumental, serving as the foundation of many software engineering tasks, including automated testing and effective GUI search. The most recent IGE detection approaches for 2D mobile apps typically train a supervised object detection model based on a large-scale manually-labeled GUI dataset, usually with a pre-defined set of clickable GUI element categories like buttons and spinners. Such approaches can hardly be applied to IGE detection in VR apps, due to a multitude of challenges including complexities posed by open-vocabulary and heterogeneous IGE categories, intricacies of context-sensitive interactability, and the necessities of precise spatial perception and visual-semantic alignment for accurate IGE detection results. Thus, it is necessary to embark on the IGE research tailored to VR apps. In this paper, we propose the first zero-shot cOntext-sensitive inteRactable GUI ElemeNT dEtection framework for virtual Reality apps, named Orienter. By imitating human behaviors, Orienter observes and understands the semantic contexts of VR app scenes first, before performing the detection. The detection process is iterated within a feedback-directed validation and reflection loop. Specifically, Orienter contains three components, including (1) Semantic context comprehension, (2) Reflection-directed IGE candidate detection, and (3) Context-sensitive interactability classification. Extensive experiments on the dataset demonstrate that Orienter is more effective than the state-of-the-art GUI element detection approaches.|\n", "2409.10775": "|**2024-09-16**|[Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?](http://arxiv.org/abs/2409.10775)|null|\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\uff0c\u5305\u62ec\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\uff0c\u5728\u5404\u79cd\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u90e8\u5206\u906e\u6321\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4f8b\u5982\uff0c\u7269\u4f53\u88ab\u90e8\u5206\u906e\u6321\u5728\u76f8\u673a\u89c6\u91ce\u4e4b\u5916\u7684\u60c5\u51b5\u3002\u5df2\u7ecf\u51fa\u73b0\u4e86\u4e00\u4e9b\u65b9\u6cd5\u6765\u63d0\u9ad8\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\uff0c\u5305\u62ec\u6570\u636e\u589e\u5f3a\u3001\u57fa\u4e8e\u90e8\u5206\u7684\u805a\u7c7b\uff0c\u4ee5\u53ca\u66f4\u5f3a\u5927\u7684\u67b6\u6784\uff0c\u5305\u62ec\u89c6\u89c9Transformer\uff08ViT\uff09\u6a21\u578b\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u5df2\u7ecf\u6839\u636e\u5176\u5728\u90e8\u5206\u906e\u6321\u4e0b\u5bf9\u7269\u4f53\u8fdb\u884c\u5206\u7c7b\u7684\u80fd\u529b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7136\u800c\uff0c\u5bf9\u8fd9\u4e9b\u65b9\u6cd5\u7684\u8bc4\u4f30\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u5305\u542b\u4eba\u5de5\u906e\u6321\u7684\u56fe\u50cf\uff0c\u8fd9\u4e9b\u56fe\u50cf\u901a\u5e38\u662f\u8ba1\u7b97\u673a\u751f\u6210\u7684\uff0c\u56e0\u6b64\u6807\u6ce8\u6210\u672c\u4f4e\u5ec9\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5f88\u5c11\u76f8\u4e92\u6bd4\u8f83\uff0c\u8bb8\u591a\u65b9\u6cd5\u662f\u4e0e\u65e9\u671f\u3001\u73b0\u5728\u5df2\u7ecf\u8fc7\u65f6\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u7684\u3002\u6211\u4eec\u8d21\u732e\u4e86\u906e\u6321\u4e0b\u56fe\u50cf\u8bc6\u522b\uff08IRUO\uff09\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u57fa\u4e8e\u6700\u8fd1\u5f00\u53d1\u7684\u906e\u6321\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\uff08OVIS\uff09\u6570\u636e\u96c6\uff08arXiv:2102.01558\uff09\u3002IRUO\u5229\u7528\u771f\u5b9e\u4e16\u754c\u548c\u4eba\u5de5\u906e\u6321\u7684\u56fe\u50cf\u6765\u6d4b\u8bd5\u548c\u6bd4\u8f83\u9886\u5148\u65b9\u6cd5\u5728\u89c6\u89c9\u8bc6\u522b\u4efb\u52a1\u4e2d\u5bf9\u90e8\u5206\u906e\u6321\u7684\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8d21\u732e\u4e86\u4f7f\u7528IRUO\u56fe\u50cf\u8fdb\u884c\u7684\u4eba\u7c7b\u7814\u7a76\u7684\u8bbe\u8ba1\u548c\u7ed3\u679c\uff0c\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u4eba\u7c7b\u5728\u591a\u4e2a\u7ea7\u522b\u548c\u7c7b\u578b\u7684\u906e\u6321\u4e0b\u7684\u5206\u7c7b\u6027\u80fd\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u4e0e\u65e9\u671f\u7684\u57fa\u4e8eCNN\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u73b0\u4ee3\u57fa\u4e8eCNN\u7684\u6a21\u578b\u5728\u906e\u6321\u56fe\u50cf\u4e0a\u7684\u8bc6\u522b\u7cbe\u5ea6\u6709\u6240\u63d0\u9ad8\uff0c\u5e76\u4e14\u57fa\u4e8eViT\u7684\u6a21\u578b\u5728\u906e\u6321\u56fe\u50cf\u4e0a\u7684\u7cbe\u5ea6\u9ad8\u4e8e\u57fa\u4e8eCNN\u7684\u6a21\u578b\uff0c\u5176\u6027\u80fd\u4ec5\u7565\u4f4e\u4e8e\u4eba\u7c7b\u7cbe\u5ea6\u3002\u6211\u4eec\u8fd8\u53d1\u73b0\uff0c\u67d0\u4e9b\u7c7b\u578b\u7684\u906e\u6321\uff0c\u5305\u62ec\u6f2b\u5c04\u906e\u6321\uff0c\u5373\u76f8\u5173\u7269\u4f53\u901a\u8fc7\u6805\u680f\u548c\u6811\u53f6\u7b49\u906e\u6321\u7269\u4e0a\u7684\u201c\u5b54\u6d1e\u201d\u53ef\u89c1\uff0c\u4e0e\u4eba\u7c7b\u76f8\u6bd4\uff0c\u8fd9\u79cd\u906e\u6321\u4f1a\u5927\u5927\u964d\u4f4e\u6df1\u5ea6\u8bc6\u522b\u6a21\u578b\u7684\u7cbe\u5ea6\uff0c\u5c24\u5176\u662f\u90a3\u4e9b\u5177\u6709CNN\u9aa8\u5e72\u7684\u6a21\u578b\u3002|\n", "2409.10699": "|**2024-09-16**|[CoMamba: Real-time Cooperative Perception Unlocked with State Space Models](http://arxiv.org/abs/2409.10699)|null|Cooperative perception systems play a vital role in enhancing the safety and efficiency of vehicular autonomy. Although recent studies have highlighted the efficacy of vehicle-to-everything (V2X) communication techniques in autonomous driving, a significant challenge persists: how to efficiently integrate multiple high-bandwidth features across an expanding network of connected agents such as vehicles and infrastructure. In this paper, we introduce CoMamba, a novel cooperative 3D detection framework designed to leverage state-space models for real-time onboard vehicle perception. Compared to prior state-of-the-art transformer-based models, CoMamba enjoys being a more scalable 3D model using bidirectional state space models, bypassing the quadratic complexity pain-point of attention mechanisms. Through extensive experimentation on V2X/V2V datasets, CoMamba achieves superior performance compared to existing methods while maintaining real-time processing capabilities. The proposed framework not only enhances object detection accuracy but also significantly reduces processing time, making it a promising solution for next-generation cooperative perception systems in intelligent transportation networks.|\n", "2409.10362": "|**2024-09-16**|[Frequency-Guided Masking for Enhanced Vision Self-Supervised Learning](http://arxiv.org/abs/2409.10362)|null|We present a novel frequency-based Self-Supervised Learning (SSL) approach that significantly enhances its efficacy for pre-training. Prior work in this direction masks out pre-defined frequencies in the input image and employs a reconstruction loss to pre-train the model. While achieving promising results, such an implementation has two fundamental limitations as identified in our paper. First, using pre-defined frequencies overlooks the variability of image frequency responses. Second, pre-trained with frequency-filtered images, the resulting model needs relatively more data to adapt to naturally looking images during fine-tuning. To address these drawbacks, we propose FOurier transform compression with seLf-Knowledge distillation (FOLK), integrating two dedicated ideas. First, inspired by image compression, we adaptively select the masked-out frequencies based on image frequency responses, creating more suitable SSL tasks for pre-training. Second, we employ a two-branch framework empowered by knowledge distillation, enabling the model to take both the filtered and original images as input, largely reducing the burden of downstream tasks. Our experimental results demonstrate the effectiveness of FOLK in achieving competitive performance to many state-of-the-art SSL methods across various downstream tasks, including image classification, few-shot learning, and semantic segmentation.|\n", "2409.12111": "|**2024-09-18**|[Applications of Knowledge Distillation in Remote Sensing: A Survey](http://arxiv.org/abs/2409.12111)|null|\u968f\u7740\u9065\u611f (RS) \u9886\u57df\u6a21\u578b\u590d\u6742\u6027\u7684\u4e0d\u65ad\u63d0\u9ad8\uff0c\u5bf9\u5e73\u8861\u6a21\u578b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u7684\u9700\u6c42\u4e5f\u65e5\u76ca\u589e\u957f\u3002\u77e5\u8bc6\u84b8\u998f (KD) \u5df2\u6210\u4e3a\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u80fd\u591f\u5728\u4e0d\u663e\u8457\u964d\u4f4e\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5c06\u77e5\u8bc6\u4ece\u5927\u578b\u590d\u6742\u6a21\u578b\u8f6c\u79fb\u5230\u66f4\u5c0f\u3001\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u3002\u8fd9\u7bc7\u7efc\u8ff0\u6587\u7ae0\u5168\u9762\u8003\u5bdf\u4e86 KD \u53ca\u5176\u5728\u9065\u611f\u9886\u57df\u7684\u521b\u65b0\u5e94\u7528\u3002KD \u662f\u4e00\u79cd\u5c06\u77e5\u8bc6\u4ece\u590d\u6742\u3001\u901a\u5e38\u5f88\u7b28\u91cd\u7684\u6a21\u578b\uff08\u6559\u5e08\uff09\u8f6c\u79fb\u5230\u66f4\u7d27\u51d1\u3001\u66f4\u9ad8\u6548\u7684\u6a21\u578b\uff08\u5b66\u751f\uff09\u7684\u6280\u672f\uff0c\u5df2\u7ecf\u5728\u5404\u4e2a\u9886\u57df\u5f97\u5230\u4e86\u663e\u8457\u7684\u53d1\u5c55\u548c\u5e94\u7528\u3002\u9996\u5148\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 KD \u65b9\u6cd5\u7684\u57fa\u672c\u6982\u5ff5\u548c\u5386\u53f2\u6f14\u8fdb\u3002\u6587\u7ae0\u91cd\u70b9\u5f3a\u8c03\u4e86\u91c7\u7528 KD \u7684\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u6a21\u578b\u538b\u7f29\u3001\u8ba1\u7b97\u6548\u7387\u63d0\u9ad8\u548c\u6027\u80fd\u6539\u8fdb\u65b9\u9762\uff0c\u8fd9\u4e9b\u4f18\u52bf\u5bf9\u4e8e RS \u573a\u666f\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u6587\u7ae0\u63d0\u4f9b\u4e86 KD \u6280\u672f\u7684\u5168\u9762\u5206\u7c7b\uff0c\u5bf9\u6bcf\u4e00\u7c7b\u90fd\u8fdb\u884c\u4e86\u6279\u5224\u6027\u5206\u6790\uff0c\u4ee5\u5c55\u793a\u66ff\u4ee3\u65b9\u6848\u7684\u5e7f\u5ea6\u548c\u6df1\u5ea6\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\u7b49 RS \u4efb\u52a1\u7684\u5177\u4f53\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86 KD \u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u3002\u6b64\u5916\uff0c\u8be5\u7efc\u8ff0\u8fd8\u8ba8\u8bba\u4e86 KD \u5728\u9065\u611f\u9886\u57df\u9762\u4e34\u7684\u6311\u6218\u548c\u5c40\u9650\u6027\uff0c\u5305\u62ec\u5b9e\u9645\u7ea6\u675f\u548c\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\uff0c\u4e3a\u9065\u611f\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6982\u8ff0\u3002\u901a\u8fc7\u8fd9\u79cd\u7ec4\u7ec7\u65b9\u5f0f\uff0c\u672c\u6587\u4e0d\u4ec5\u9610\u660e\u4e86 KD \u7814\u7a76\u7684\u73b0\u72b6\uff0c\u8fd8\u4e3a\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4ece\u800c\u5bf9\u5b66\u672f\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u505a\u51fa\u4e86\u91cd\u5927\u8d21\u732e\u3002|\n", "2409.11995": "|**2024-09-18**|[Unraveling the Hessian: A Key to Smooth Convergence in Loss Function Landscapes](http://arxiv.org/abs/2409.11995)|**[link](https://github.com/kisnikser/landscape-hessian)**|\u795e\u7ecf\u7f51\u7edc\u7684\u635f\u5931\u666f\u89c2\u662f\u5176\u8bad\u7ec3\u7684\u5173\u952e\u65b9\u9762\uff0c\u4e86\u89e3\u5176\u5c5e\u6027\u5bf9\u4e8e\u63d0\u9ad8\u5176\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5f53\u6837\u672c\u91cf\u589e\u52a0\u65f6\u635f\u5931\u66f2\u9762\u5982\u4f55\u53d8\u5316\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ee5\u524d\u672a\u63a2\u8ba8\u8fc7\u7684\u95ee\u9898\u3002\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4e86\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u4e2d\u635f\u5931\u666f\u89c2\u7684\u6536\u655b\u6027\uff0c\u5e76\u63a8\u5bfc\u51fa\u6dfb\u52a0\u65b0\u5bf9\u8c61\u5230\u6837\u672c\u65f6\u635f\u5931\u51fd\u6570\u503c\u5dee\u5f02\u7684\u4e0a\u9650\u3002\u6211\u4eec\u7684\u5b9e\u8bc1\u7814\u7a76\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u8bc1\u5b9e\u4e86\u8fd9\u4e9b\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u635f\u5931\u51fd\u6570\u66f2\u9762\u7684\u6536\u655b\u6027\u3002\u6211\u4eec\u7684\u53d1\u73b0\u4e3a\u795e\u7ecf\u635f\u5931\u666f\u89c2\u7684\u5c40\u90e8\u51e0\u4f55\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e76\u5bf9\u6837\u672c\u91cf\u786e\u5b9a\u6280\u672f\u7684\u53d1\u5c55\u5177\u6709\u542f\u793a\u610f\u4e49\u3002|\n", "2409.11923": "|**2024-09-18**|[Agglomerative Token Clustering](http://arxiv.org/abs/2409.11923)|null|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684token\u5408\u5e76\u65b9\u6cd5\u2014\u2014\u805a\u5408token\u805a\u7c7b\uff08ATC\uff09\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u56fe\u50cf\u5408\u6210\u4ee5\u53ca\u76ee\u6807\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u4ee5\u524d\u7684token\u5408\u5e76\u548c\u526a\u679d\u65b9\u6cd5\u3002ATC\u901a\u8fc7\u81ea\u4e0b\u800c\u4e0a\u7684\u5c42\u6b21\u805a\u7c7b\u5408\u5e76\u805a\u7c7b\uff0c\u65e0\u9700\u5f15\u5165\u989d\u5916\u7684\u53ef\u5b66\u4e60\u53c2\u6570\u3002\u6211\u4eec\u53d1\u73b0ATC\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u90fd\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751a\u81f3\u5728\u5e94\u7528\u4e8e\u73b0\u6210\u6a21\u578b\u65f6\uff0c\u5373\u65e0\u9700\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u4e0e\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002\u5f53\u5e94\u7528\u4e8e\u4f4e\u4fdd\u7559\u7387\u65f6\uff0cATC\u5c24\u5176\u6709\u6548\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u53ea\u6709\u4e00\u5c0f\u90e8\u5206token\u88ab\u4fdd\u7559\uff0c\u5e76\u4e14\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7279\u522b\u56f0\u96be\u3002|\n", "2409.11867": "|**2024-09-18**|[Distillation-free Scaling of Large SSMs for Images and Videos](http://arxiv.org/abs/2409.11867)|null|\u72b6\u6001\u7a7a\u95f4\u6a21\u578b (SSM)\uff0c\u4f8b\u5982 S4\uff0c\u901a\u8fc7\u5c06\u72b6\u6001\u7a7a\u95f4\u6280\u672f\u96c6\u6210\u5230\u6df1\u5ea6\u5b66\u4e60\u4e2d\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u5efa\u6a21\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5176\u6570\u636e\u65e0\u5173\u77e9\u9635\uff0c\u5b83\u4eec\u96be\u4ee5\u8fdb\u884c\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u3002Mamba \u6a21\u578b\u901a\u8fc7 S6 \u9009\u62e9\u6027\u626b\u63cf\u7b97\u6cd5\u7684\u6570\u636e\u76f8\u5173\u53d8\u4f53\u89e3\u51b3\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u589e\u5f3a\u4e86\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u957f\u5e8f\u5217\u3002\u7136\u800c\uff0c\u57fa\u4e8e Mamba \u7684\u67b6\u6784\u96be\u4ee5\u5728\u53c2\u6570\u6570\u91cf\u65b9\u9762\u6269\u5c55\uff0c\u8fd9\u5bf9\u4e8e\u89c6\u89c9\u5e94\u7528\u6765\u8bf4\u662f\u4e00\u4e2a\u4e3b\u8981\u9650\u5236\u3002\u672c\u6587\u89e3\u51b3\u4e86\u5927\u578b SSM \u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u548c\u52a8\u4f5c\u8bc6\u522b\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u800c\u65e0\u9700\u989d\u5916\u7684\u6280\u672f\uff0c\u5982\u77e5\u8bc6\u84b8\u998f\u3002\u6211\u4eec\u5206\u6790\u4e86\u57fa\u4e8e Mamba \u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\u7684\u4e0d\u540c\u7279\u5f81\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd Mamba-Attention \u4ea4\u9519\u67b6\u6784\uff0c\u4ee5\u589e\u5f3a\u53ef\u6269\u5c55\u6027\u3001\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u7a33\u5b9a\u9ad8\u6548\u7684\u4ea4\u9519\u67b6\u6784\u89e3\u51b3\u4e86\u57fa\u4e8e Mamba \u7684\u67b6\u6784\u5728\u56fe\u50cf\u548c\u89c6\u9891\u65b9\u9762\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u4e86\u5bf9 JPEG \u538b\u7f29\u7b49\u5e38\u89c1\u4f2a\u5f71\u7684\u9c81\u68d2\u6027\u3002\u6211\u4eec\u5728 ImageNet-1K\u3001Kinetics-400 \u548c Something-Something-v2 \u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5168\u9762\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u6700\u5148\u8fdb\u7684\u57fa\u4e8e Mamba \u7684\u67b6\u6784\u7684\u51c6\u786e\u6027\u63d0\u9ad8\u4e86 +1.7\u3002|\n", "2409.11749": "|**2024-09-18**|[RockTrack: A 3D Robust Multi-Camera-Ken Multi-Object Tracking Framework](http://arxiv.org/abs/2409.11749)|null|\u968f\u77403D\u76ee\u6807\u68c0\u6d4b\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5c24\u5176\u662f\u5728\u7ecf\u6d4e\u9ad8\u6548\u7684\u591a\u6444\u50cf\u5934\u8bbe\u7f6e\u4e2d\uff0c3D\u591a\u76ee\u6807\u8ddf\u8e2a\uff08MOT\uff09\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u7136\u800c\uff0c\u76ee\u524d\u6d41\u884c\u7684\u591a\u6444\u50cf\u5934\u8ddf\u8e2a\u5668\u7aef\u5230\u7aef\u8bad\u7ec3\u65b9\u6cd5\u5bfc\u81f4\u6a21\u578b\u4f9d\u8d56\u4e8e\u7279\u5b9a\u68c0\u6d4b\u5668\uff0c\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7684\u901a\u7528\u8ddf\u8e2a\u5668\u5ffd\u7565\u4e86\u591a\u6444\u50cf\u5934\u68c0\u6d4b\u5668\u7684\u72ec\u7279\u7279\u5f81\uff0c\u5373\u8fd0\u52a8\u89c2\u6d4b\u7684\u4e0d\u53ef\u9760\u6027\u548c\u89c6\u89c9\u4fe1\u606f\u7684\u53ef\u7528\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86RockTrack\uff0c\u4e00\u79cd\u9762\u5411\u591a\u6444\u50cf\u5934\u68c0\u6d4b\u5668\u76843D MOT\u65b9\u6cd5\u3002RockTrack\u9075\u5faa\u201c\u68c0\u6d4b\u8ddf\u8e2a\u201d\u6846\u67b6\uff0c\u517c\u5bb9\u5404\u79cd\u73b0\u6210\u7684\u68c0\u6d4b\u5668\u3002RockTrack\u5305\u542b\u4e00\u4e2a\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u9884\u5904\u7406\u6a21\u5757\uff0c\u7528\u4e8e\u4ece\u5355\u4e2a\u68c0\u6d4b\u5668\u7684\u4e0d\u540c\u8868\u793a\u7a7a\u95f4\u4e2d\u63d0\u53d6\u53ef\u9760\u7684\u8fd0\u52a8\u548c\u56fe\u50cf\u89c2\u6d4b\u7ed3\u679c\u3002\u7136\u540e\uff0c\u8fd9\u4e9b\u89c2\u6d4b\u7ed3\u679c\u5728\u4e00\u4e2a\u5173\u8054\u6a21\u5757\u4e2d\u878d\u5408\uff0c\u8be5\u6a21\u5757\u5229\u7528\u51e0\u4f55\u548c\u5916\u89c2\u7ebf\u7d22\u6765\u6700\u5927\u7a0b\u5ea6\u5730\u51cf\u5c11\u9519\u8bef\u5339\u914d\u3002\u6700\u7ec8\u7684\u5339\u914d\u7ed3\u679c\u901a\u8fc7\u4e00\u4e2a\u5206\u9636\u6bb5\u7684\u4f30\u8ba1\u8fc7\u7a0b\u8fdb\u884c\u4f20\u64ad\uff0c\u4e3a\u542f\u53d1\u5f0f\u566a\u58f0\u5efa\u6a21\u5960\u5b9a\u57fa\u7840\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5916\u89c2\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u591a\u6444\u50cf\u5934\u8bbe\u7f6e\u4e2d\u660e\u786e\u8868\u5f81\u76ee\u6807\u7684\u5173\u8054\u6027\u3002RockTrack\u5728nuScenes\u4ec5\u89c6\u89c9\u8ddf\u8e2a\u6392\u884c\u699c\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0cAMOTA\u8fbe\u523059.1%\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u8ba1\u7b97\u6548\u7387\u3002|\n", "2409.11644": "|**2024-09-18**|[Few-Shot Learning Approach on Tuberculosis Classification Based on Chest X-Ray Images](http://arxiv.org/abs/2409.11644)|null|\u7ed3\u6838\u75c5 (TB) \u662f\u7531\u7ed3\u6838\u5206\u679d\u6746\u83cc\u5f15\u8d77\u7684\u75be\u75c5\uff0c\u4e3b\u8981\u5f71\u54cd\u80ba\u90e8\u3002\u65e9\u671f\u68c0\u6d4b\u5bf9\u4e8e\u63d0\u9ad8\u6cbb\u7597\u6548\u679c\u548c\u964d\u4f4e\u4f20\u64ad\u98ce\u9669\u81f3\u5173\u91cd\u8981\u3002\u4eba\u5de5\u667a\u80fd (AI)\uff0c\u7279\u522b\u662f\u901a\u8fc7\u80f8\u90e8 X \u5149\u7247\u7684\u56fe\u50cf\u5206\u7c7b\uff0c\u53ef\u4ee5\u5e2e\u52a9\u68c0\u6d4b\u7ed3\u6838\u75c5\u3002\u7136\u800c\uff0c\u7ed3\u6838\u75c5\u80f8\u90e8 X \u5149\u7247\u6570\u636e\u96c6\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u5bf9\u51c6\u786e\u5206\u7c7b\u63d0\u51fa\u4e86\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u539f\u578b\u7f51\u7edc\u7b97\u6cd5\u7684\u5c0f\u6837\u672c\u5b66\u4e60 (FSL) \u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u6211\u4eec\u6bd4\u8f83\u4e86 ResNet-18\u3001ResNet-50 \u548c VGG16 \u5728\u4ece TBX11K \u80f8\u90e8 X \u5149\u7247\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u7279\u5f81\u65b9\u9762\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cResNet-18 \u7684\u5206\u7c7b\u51c6\u786e\u7387\u4e3a 98.93%\uff0cResNet-50 \u4e3a 98.60%\uff0cVGG16 \u4e3a 33.33%\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7f13\u89e3\u6570\u636e\u4e0d\u5e73\u8861\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u8fd9\u5bf9\u4e8e\u75be\u75c5\u5206\u7c7b\u5e94\u7528\u7279\u522b\u6709\u5229\u3002|\n", "2409.11542": "|**2024-09-17**|[VALO: A Versatile Anytime Framework for LiDAR-based Object Detection Deep Neural Networks](http://arxiv.org/abs/2409.11542)|**[link](https://github.com/csl-ku/valo)**|\u8fd9\u9879\u5de5\u4f5c\u89e3\u51b3\u4e86\u4e3a\u6fc0\u5149\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (DNN) \u8c03\u6574\u52a8\u6001\u622a\u6b62\u65e5\u671f\u8981\u6c42\u7684\u6311\u6218\u3002\u76ee\u6807\u68c0\u6d4b\u7684\u8ba1\u7b97\u5ef6\u8fdf\u5bf9\u4e8e\u786e\u4fdd\u5b89\u5168\u9ad8\u6548\u7684\u5bfc\u822a\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u6700\u5148\u8fdb\u7684\u6fc0\u5149\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b DNN \u901a\u5e38\u8868\u73b0\u51fa\u663e\u8457\u7684\u5ef6\u8fdf\uff0c\u963b\u788d\u4e86\u5b83\u4eec\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u5e73\u53f0\u4e0a\u7684\u5b9e\u65f6\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u5e94\u8be5\u5728\u8fd0\u884c\u65f6\u52a8\u6001\u7ba1\u7406\u68c0\u6d4b\u7cbe\u5ea6\u548c\u5ef6\u8fdf\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4ee5\u5b9e\u73b0\u6700\u4f73\u7ed3\u679c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 VALO\uff08\u7528\u4e8e\u6fc0\u5149\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u7684\u901a\u7528\u968f\u65f6\u7b97\u6cd5\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u968f\u65f6\u8ba1\u7b97 3D \u6fc0\u5149\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b DNN\u3002VALO \u91c7\u7528\u622a\u6b62\u65e5\u671f\u611f\u77e5\u8c03\u5ea6\u7a0b\u5e8f\u6765\u9009\u62e9\u6027\u5730\u5904\u7406\u8f93\u5165\u533a\u57df\uff0c\u4ece\u800c\u5728\u4e0d\u4fee\u6539\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u6743\u8861\u6267\u884c\u65f6\u95f4\u548c\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u5b83\u5229\u7528\u5bf9\u8fc7\u53bb\u68c0\u6d4b\u7ed3\u679c\u7684\u6709\u6548\u9884\u6d4b\u6765\u51cf\u8f7b\u7531\u4e8e\u90e8\u5206\u5904\u7406\u8f93\u5165\u53ef\u80fd\u5bfc\u81f4\u7684\u7cbe\u5ea6\u635f\u5931\u3002\u6700\u540e\uff0c\u5b83\u5728\u5176\u68c0\u6d4b\u5934\u4e2d\u5229\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8f93\u5165\u51cf\u5c11\u6280\u672f\uff0c\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u663e\u7740\u52a0\u5feb\u6267\u884c\u901f\u5ea6\u3002\u6211\u4eec\u5728\u6700\u5148\u8fdb\u7684 3D \u6fc0\u5149\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\uff08\u5373 CenterPoint \u548c VoxelNext\uff09\u4e0a\u5b9e\u73b0\u4e86 VALO\uff0c\u5e76\u5c55\u793a\u4e86\u5b83\u5bf9\u5404\u79cd\u65f6\u95f4\u9650\u5236\u7684\u52a8\u6001\u9002\u5e94\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u6280\u672f\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002\u4ee3\u7801\u53ef\u5728https://github.com/CSL-KU/VALO\u83b7\u53d6\u3002|\n", "2409.11532": "|**2024-09-17**|[Enhancing the Reliability of LiDAR Point Cloud Sampling: A Colorization and Super-Resolution Approach Based on LiDAR-Generated Images](http://arxiv.org/abs/2409.11532)|null|\u8fd1\u5e74\u6765\uff0c\u5149\u63a2\u6d4b\u548c\u6d4b\u8ddd\uff08LiDAR\uff09\u6280\u672f\u4f5c\u4e3a\u673a\u5668\u4eba\u548c\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u4f20\u611f\u5668\uff0c\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u6b65\u3002\u8fd9\u4e9b\u8fdb\u6b65\u5305\u62ec\u63d0\u9ad8\u4e86\u70b9\u4e91\u7684\u5206\u8fa8\u7387\uff0c\u5e76\u80fd\u591f\u63d0\u4f9b360\u5ea6\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u3002\u8fd9\u4e9b\u56fe\u50cf\u5728\u50cf\u7d20\u4e2d\u7f16\u7801\u4e86\u5404\u79cd\u6570\u636e\uff0c\u4f8b\u5982\u6df1\u5ea6\u3001\u53cd\u5c04\u7387\u548c\u8fd1\u7ea2\u5916\u5149\u3002\u7136\u800c\uff0c\u8fc7\u9ad8\u7684\u70b9\u5bc6\u5ea6\u548c\u4f20\u7edf\u7684\u70b9\u4e91\u91c7\u6837\u53ef\u80fd\u9002\u5f97\u5176\u53cd\uff0c\u7279\u522b\u662f\u5728LiDAR\u91cc\u7a0b\u8ba1\u7b49\u5e94\u7528\u4e2d\uff0c\u8bef\u5bfc\u70b9\u548c\u9000\u5316\u7684\u51e0\u4f55\u4fe1\u606f\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6f02\u79fb\u8bef\u5dee\u3002\u76ee\u524d\uff0c\u5e7f\u6cdb\u7684\u7814\u7a76\u5de5\u4f5c\u81f4\u529b\u4e8e\u5229\u7528LiDAR\u751f\u6210\u7684\u56fe\u50cf\u6765\u63d0\u9ad8\u6001\u52bf\u611f\u77e5\u80fd\u529b\u3002\u672c\u6587\u5168\u9762\u56de\u987e\u4e86\u5f53\u524d\u7684\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u6280\u672f\uff0c\u5305\u62ec\u4f20\u7edf\u4e0a\u7528\u4e8e\u4f20\u7edf\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u7740\u8272\u548c\u8d85\u5206\u8fa8\u7387\u6280\u672f\u3002\u8fd9\u4e9b\u6280\u672f\u5e94\u7528\u4e8eLiDAR\u751f\u6210\u7684\u56fe\u50cf\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9a\u6027\u5206\u6790\u3002\u5728\u6b64\u5206\u6790\u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5c06\u6700\u5408\u9002\u7684\u7740\u8272\u548c\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4e0eLiDAR\u56fe\u50cf\u9009\u62e9\u6027\u5730\u96c6\u6210\uff0c\u4ee5\u4eceLiDAR\u70b9\u4e91\u4e2d\u91c7\u6837\u53ef\u9760\u7684\u70b9\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u65e8\u5728\u63d0\u9ad8\u70b9\u4e91\u914d\u51c6\u7684\u7cbe\u5ea6\uff0c\u800c\u4e14\u8fd8\u907f\u514d\u4e86\u7531\u4e8e\u7f3a\u4e4f\u51e0\u4f55\u4fe1\u606f\u800c\u5bfc\u81f4\u7684\u9519\u914d\uff0c\u4ece\u800c\u589e\u5f3a\u4e86LiDAR\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u7528\u548c\u7cbe\u5ea6\u3002\u5728\u6211\u4eec\u7684\u8bc4\u4f30\u4e2d\uff0c\u4e0e\u6211\u4eec\u4e4b\u524d\u7684\u5de5\u4f5c\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5728\u51cf\u5c11\u70b9\u6570\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u5e73\u79fb\u548c\u65cb\u8f6c\u8bef\u5dee\u3002|\n", "2409.11502": "|**2024-09-19**|[Super Resolution On Global Weather Forecasts](http://arxiv.org/abs/2409.11502)|null|\u5929\u6c14\u9884\u62a5\u662f\u4e00\u9879\u81f3\u5173\u91cd\u8981\u7684\u5de5\u5177\uff0c\u5e94\u7528\u8303\u56f4\u4ece\u65e5\u5e38\u6d3b\u52a8\u89c4\u5212\u5230\u707e\u5bb3\u5e94\u5bf9\u89c4\u5212\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5929\u6c14\u5177\u6709\u6df7\u6c8c\u548c\u4e0d\u53ef\u9884\u6d4b\u7684\u6027\u8d28\uff0c\u5bf9\u5929\u6c14\u8fdb\u884c\u5efa\u6a21\u4e00\u76f4\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u4ece\u6e29\u5ea6\u5230\u964d\u6c34\u518d\u5230\u98ce\uff0c\u6bcf\u4e2a\u53d8\u91cf\u90fd\u4f1a\u5f71\u54cd\u73af\u5883\u7684\u53d8\u5316\u8f68\u8ff9\u3002\u56e0\u6b64\uff0c\u968f\u7740\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u7684\u589e\u52a0\uff0c\u6240\u6709\u6a21\u578b\u7684\u51c6\u786e\u6027\u90fd\u4f1a\u8fc5\u901f\u4e0b\u964d\u3002\u4f20\u7edf\u7684\u9884\u6d4b\u65b9\u6cd5\u4f7f\u7528\u5404\u79cd\u57fa\u4e8e\u7269\u7406\u3001\u6570\u503c\u548c\u968f\u673a\u7684\u6280\u672f\u6765\u9884\u6d4b\u5929\u6c14\u53d8\u91cf\u968f\u65f6\u95f4\u7684\u53d8\u5316\u3002\u7136\u800c\uff0c\u6b64\u7c7b\u9884\u6d4b\u901a\u5e38\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u5e76\u4e14\u8ba1\u7b97\u6210\u672c\u6781\u9ad8\u3002\u6b64\u5916\uff0c\u968f\u7740\u6c14\u5019\u548c\u5168\u7403\u5929\u6c14\u6a21\u5f0f\u7684\u53d8\u5316\uff0c\u4f20\u7edf\u6a21\u578b\u66f4\u96be\u4ee5\u66f4\u65b0\u4ee5\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u73af\u5883\uff0c\u5e76\u4e14\u66f4\u65b0\u8fc7\u7a0b\u4e5f\u66f4\u52a0\u8017\u65f6\u3002\u5e78\u8fd0\u7684\u662f\uff0c\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u7684\u6700\u65b0\u8fdb\u5c55\u4ee5\u53ca\u516c\u5f00\u53ef\u7528\u7684\u9ad8\u8d28\u91cf\u5929\u6c14\u6570\u636e\u96c6\u7684\u51fa\u73b0\uff0c\u90e8\u7f72\u5b66\u4e60\u65b9\u6cd5\u6765\u4f30\u8ba1\u8fd9\u4e9b\u590d\u6742\u7cfb\u7edf\u5df2\u7ecf\u53d8\u5f97\u53ef\u884c\u3002\u76ee\u524d\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u51c6\u786e\u6027\u4e0e\u884c\u4e1a\u6807\u51c6\u6570\u503c\u6a21\u578b\u76f8\u5f53\uff0c\u5e76\u4e14\u7531\u4e8e\u5176\u9002\u5e94\u6027\u5f3a\uff0c\u5728\u5b9e\u8df5\u4e2d\u8d8a\u6765\u8d8a\u666e\u904d\u3002\u6211\u4eec\u5c0f\u7ec4\u81f4\u529b\u4e8e\u6539\u8fdb\u73b0\u6709\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9884\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5168\u7403\u5929\u6c14\u9884\u62a5\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5e0c\u671b\u5bf9 GraphCast \u6e29\u5ea6\u9884\u6d4b\u6267\u884c\u8d85\u5206\u8fa8\u7387 (SR)\uff0c\u5c06\u5168\u7403\u7cbe\u5ea6\u4ece 1 \u5ea6\u63d0\u9ad8\u5230 0.5 \u5ea6\uff0c\u5206\u522b\u7ea6\u4e3a 111 \u516c\u91cc\u548c 55 \u516c\u91cc\u3002|\n"}, "\u751f\u6210\u6a21\u578b": {"2409.02919": "|**2024-09-04**|[HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts](http://arxiv.org/abs/2409.02919)|**[link](https://github.com/Liuxinyv/HiPrompt)**|\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u751f\u6210\u66f4\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5728\u5904\u7406\u7269\u4f53\u91cd\u590d\u548c\u7ed3\u6784\u4f2a\u5f71\u65b9\u9762\u5e38\u5e38\u9047\u5230\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u6269\u5c55\u5230 4K \u53ca\u66f4\u9ad8\u5206\u8fa8\u7387\u65f6\u3002\u6211\u4eec\u53d1\u73b0\u95ee\u9898\u5728\u4e8e\uff0c\u5355\u4e2a\u63d0\u793a\u751f\u6210\u591a\u4e2a\u5c3a\u5ea6\u7684\u65b9\u5f0f\u6548\u7387\u4f4e\u4e0b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 HiPrompt\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u987b\u5fae\u8c03\u7684\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u901a\u8fc7\u5f15\u5165\u5206\u5c42\u63d0\u793a\u6765\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002\u5206\u5c42\u63d0\u793a\u63d0\u4f9b\u5168\u5c40\u548c\u5c40\u90e8\u6307\u5bfc\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5168\u5c40\u6307\u5bfc\u6765\u81ea\u63cf\u8ff0\u6574\u4f53\u5185\u5bb9\u7684\u7528\u6237\u8f93\u5165\uff0c\u800c\u5c40\u90e8\u6307\u5bfc\u5219\u5229\u7528\u6765\u81ea MLLM \u7684\u9010\u5757\u63cf\u8ff0\u6765\u7cbe\u5fc3\u6307\u5bfc\u5c40\u90e8\u7ed3\u6784\u548c\u7eb9\u7406\u7684\u751f\u6210\u3002\u6b64\u5916\uff0c\u5728\u9006\u5411\u53bb\u566a\u8fc7\u7a0b\u4e2d\uff0c\u751f\u6210\u7684\u566a\u58f0\u88ab\u5206\u89e3\u4e3a\u4f4e\u9891\u548c\u9ad8\u9891\u7a7a\u95f4\u5206\u91cf\u3002\u8fd9\u4e9b\u5206\u91cf\u4ee5\u591a\u4e2a\u63d0\u793a\u7ea7\u522b\u4e3a\u6761\u4ef6\uff0c\u5305\u62ec\u8be6\u7ec6\u7684\u9010\u5757\u63cf\u8ff0\u548c\u66f4\u5e7f\u6cdb\u7684\u56fe\u50cf\u7ea7\u63d0\u793a\uff0c\u4ece\u800c\u4fc3\u8fdb\u5728\u5206\u5c42\u8bed\u4e49\u6307\u5bfc\u4e0b\u7684\u63d0\u793a\u5f15\u5bfc\u53bb\u566a\u3002\u5b83\u8fdb\u4e00\u6b65\u5141\u8bb8\u751f\u6210\u8fc7\u7a0b\u66f4\u591a\u5730\u5173\u6ce8\u5c40\u90e8\u7a7a\u95f4\u533a\u57df\uff0c\u5e76\u786e\u4fdd\u751f\u6210\u7684\u56fe\u50cf\u5728\u9ad8\u6e05\u6670\u5ea6\u4e0b\u4fdd\u6301\u4e00\u81f4\u7684\u5c40\u90e8\u548c\u5168\u5c40\u8bed\u4e49\u3001\u7ed3\u6784\u548c\u7eb9\u7406\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cHiPrompt \u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u7269\u4f53\u91cd\u590d\u5e76\u63d0\u9ad8\u4e86\u7ed3\u6784\u8d28\u91cf\u3002||\n", "2409.02915": "|**2024-09-04**|[Latent Watermarking of Audio Generative Models](http://arxiv.org/abs/2409.02915)|null|\u97f3\u9891\u751f\u6210\u6a21\u578b\u7684\u8fdb\u6b65\u7ed9\u5176\u8d1f\u8d23\u4efb\u7684\u62ab\u9732\u548c\u6ee5\u7528\u68c0\u6d4b\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u901a\u8fc7\u5bf9\u5176\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u7279\u5b9a\u6c34\u5370\u6765\u6807\u8bb0\u6f5c\u5728\u751f\u6210\u6a21\u578b\u7684\u65b9\u6cd5\u3002\u7531\u6b64\u4ea7\u751f\u7684\u6c34\u5370\u6a21\u578b\u751f\u6210\u7684\u6f5c\u5728\u8868\u793a\uff0c\u5176\u89e3\u7801\u8f93\u51fa\u53ef\u4ee5\u88ab\u9ad8\u7f6e\u4fe1\u5ea6\u5730\u68c0\u6d4b\u5230\uff0c\u800c\u65e0\u8bba\u4f7f\u7528\u4f55\u79cd\u89e3\u7801\u65b9\u6cd5\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e0\u9700\u8fdb\u884c\u4e8b\u540e\u6c34\u5370\u6b65\u9aa4\u5373\u53ef\u68c0\u6d4b\u751f\u6210\u7684\u5185\u5bb9\u3002\u5b83\u4e3a\u5f00\u6e90\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6709\u52a9\u4e8e\u8bc6\u522b\u90a3\u4e9b\u5728\u672a\u9075\u5b88\u8bb8\u53ef\u6761\u6b3e\u7684\u60c5\u51b5\u4e0b\u5bf9\u8fd9\u4e9b\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u6216\u4f7f\u7528\u7684\u884d\u751f\u4f5c\u54c1\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u5bf9\u6f5c\u5728\u751f\u6210\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u540e\uff0c\u751f\u6210\u8f93\u51fa\u7684\u68c0\u6d4b\u7cbe\u5ea6\u4e5f\u80fd\u5728\u5047\u9633\u6027\u7387\u4e3a$10^{-3}$\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230 75% \u4ee5\u4e0a\u3002||\n", "2409.02908": "|**2024-09-04**|[Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling](http://arxiv.org/abs/2409.02908)|null|\u63a9\u7801\u6269\u6563\u6a21\u578b (MDM) \u7531\u4e8e\u5176\u76f8\u8f83\u4e8e\u5176\u4ed6\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5df2\u6210\u4e3a\u79bb\u6563\u6570\u636e\u751f\u6210\u5efa\u6a21\u7684\u70ed\u95e8\u7814\u7a76\u8bfe\u9898\uff0c\u5e76\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u4e0e\u81ea\u56de\u5f52\u6a21\u578b (ARM) \u5c55\u5f00\u7ade\u4e89\u3002\u6700\u8fd1\u7b80\u5316\u63a9\u7801\u6269\u6563\u6846\u67b6\u7684\u52aa\u529b\u8fdb\u4e00\u6b65\u4f7f\u5176\u4e0e\u8fde\u7eed\u7a7a\u95f4\u6269\u6563\u6a21\u578b\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u83b7\u5f97\u4e86\u66f4\u6709\u539f\u5219\u7684\u8bad\u7ec3\u548c\u91c7\u6837\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63ed\u793a\u4e86 MDM \u7684\u8bad\u7ec3\u548c\u91c7\u6837\u5728\u7406\u8bba\u4e0a\u90fd\u53ef\u4ee5\u6446\u8131\u65f6\u95f4\u53d8\u91cf\uff08\u53ef\u4ee5\u8bf4\u662f\u6269\u6563\u6a21\u578b\u7684\u5173\u952e\u7279\u5f81\uff09\uff0c\u5e76\u4e14\u7b49\u6548\u4e8e\u63a9\u7801\u6a21\u578b\u3002\u6211\u4eec\u5728\u91c7\u6837\u65b9\u9762\u7684\u8054\u7cfb\u662f\u901a\u8fc7\u6211\u4eec\u63d0\u51fa\u7684\u9996\u6b21\u547d\u4e2d\u91c7\u6837\u5668 (FHS) \u5efa\u7acb\u7684\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 FHS \u5728\u7406\u8bba\u4e0a\u7b49\u6548\u4e8e MDM \u7684\u539f\u59cb\u751f\u6210\u8fc7\u7a0b\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8017\u65f6\u7684\u5206\u7c7b\u91c7\u6837\uff0c\u5e76\u5b9e\u73b0\u4e86 20 \u500d\u7684\u52a0\u901f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u7814\u7a76\u5bf9\u5148\u524d\u5173\u4e8e MDM \u5728\u751f\u6210\u56f0\u60d1\u5ea6\u65b9\u9762\u53ef\u4ee5\u8d85\u8d8a ARM \u7684\u8bf4\u6cd5\u63d0\u51fa\u4e86\u8d28\u7591\u3002\u6211\u4eec\u9996\u6b21\u53d1\u73b0\u4e86\u4e00\u4e2a\u6f5c\u5728\u7684\u6570\u503c\u95ee\u9898\uff0c\u5373\u4f7f\u4f7f\u7528 32 \u4f4d\u6d6e\u70b9\u7cbe\u5ea6\uff0c\u4e5f\u4f1a\u5bfc\u81f4\u4e0d\u51c6\u786e\u7684\u5206\u7c7b\u91c7\u6837\u3002\u6211\u4eec\u8868\u660e\uff0c\u8be5\u6570\u503c\u95ee\u9898\u5728\u7406\u8bba\u4e0a\u548c\u7ecf\u9a8c\u4e0a\u90fd\u964d\u4f4e\u4e86\u6709\u6548\u6e29\u5ea6\uff0c\u5bfc\u81f4\u5148\u524d\u6587\u732e\u4e2d\u5bf9 MDM \u751f\u6210\u7ed3\u679c\u7684\u8bc4\u4f30\u4e0d\u516c\u5e73\u3002||\n", "2409.02851": "|**2024-09-04**|[Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video Diffusion Models](http://arxiv.org/abs/2409.02851)|**[link](https://github.com/Human-VDM/Human-VDM)**|\u4ece\u5355\u5f20RGB\u56fe\u50cf\u751f\u6210\u903c\u771f3D\u4eba\u4f53\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u7cbe\u786e\u7684\u51e0\u4f55\u5efa\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u7eb9\u7406\u548c\u5408\u7406\u7684\u4e0d\u53ef\u89c1\u90e8\u5206\u751f\u6210\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u8fdb\u884c3D\u4eba\u4f53\u751f\u6210\uff0c\u4f46\u5b83\u4eec\u7ecf\u5e38\u9762\u4e34\u89c6\u89d2\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u8fd9\u963b\u788d\u4e86\u9ad8\u8d28\u91cf3D\u4eba\u4f53\u7684\u751f\u6210\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Human-VDM\uff0c\u4e00\u79cd\u4f7f\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u4ece\u5355\u5f20RGB\u56fe\u50cf\u751f\u62103D\u4eba\u4f53\u7684\u65b0\u65b9\u6cd5\u3002Human-VDM\u4f7f\u7528\u9ad8\u65af\u6e32\u67d3\u4e3a3D\u4eba\u4f53\u751f\u6210\u63d0\u4f9b\u4e86\u65f6\u95f4\u4e0a\u4e00\u81f4\u7684\u89c6\u56fe\u3002\u5b83\u7531\u4e09\u4e2a\u6a21\u5757\u7ec4\u6210\uff1a\u89c6\u56fe\u4e00\u81f4\u7684\u4eba\u4f53\u89c6\u9891\u6269\u6563\u6a21\u5757\u3001\u89c6\u9891\u589e\u5f3a\u6a21\u5757\u548c\u9ad8\u65af\u6e32\u67d3\u6a21\u5757\u3002\u9996\u5148\uff0c\u5c06\u5355\u5f20\u56fe\u50cf\u8f93\u5165\u4eba\u4f53\u89c6\u9891\u6269\u6563\u6a21\u5757\u4ee5\u751f\u6210\u8fde\u8d2f\u7684\u4eba\u4f53\u89c6\u9891\u3002\u63a5\u4e0b\u6765\uff0c\u89c6\u9891\u589e\u5f3a\u6a21\u5757\u5e94\u7528\u8d85\u5206\u8fa8\u7387\u548c\u89c6\u9891\u63d2\u503c\u6765\u589e\u5f3a\u751f\u6210\u89c6\u9891\u7684\u7eb9\u7406\u548c\u51e0\u4f55\u5e73\u6ed1\u5ea6\u3002\u6700\u540e\uff0c3D\u4eba\u4f53\u9ad8\u65af\u6e32\u67d3\u6a21\u5757\u5728\u8fd9\u4e9b\u9ad8\u5206\u8fa8\u7387\u548c\u89c6\u89d2\u4e00\u81f4\u7684\u56fe\u50cf\u7684\u6307\u5bfc\u4e0b\u5b66\u4e60\u903c\u771f\u7684\u4eba\u4f53\u3002\u5b9e\u9a8c\u8868\u660e\uff0cHuman-VDM\u53ef\u4ee5\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u4eba\u4f53\uff0c\u5728\u751f\u6210\u8d28\u91cf\u548c\u6570\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://human-vdm.github.io/Human-VDM/||\n", "2409.02845": "|**2024-09-04**|[Multi-Track MusicLDM: Towards Versatile Music Generation with Latent Diffusion Model](http://arxiv.org/abs/2409.02845)|null|\u6269\u6563\u6a21\u578b\u5728\u6d89\u53ca\u97f3\u9891\u548c\u97f3\u4e50\u7684\u8de8\u6a21\u6001\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f8b\u5982\u6587\u672c\u5230\u58f0\u97f3\u548c\u6587\u672c\u5230\u97f3\u4e50\u7684\u751f\u6210\u3002\u8fd9\u4e9b\u6587\u672c\u63a7\u5236\u7684\u97f3\u4e50\u751f\u6210\u6a21\u578b\u901a\u5e38\u4fa7\u91cd\u4e8e\u901a\u8fc7\u6355\u6349\u5168\u5c40\u97f3\u4e50\u5c5e\u6027\uff08\u5982\u6d41\u6d3e\u548c\u60c5\u7eea\uff09\u6765\u751f\u6210\u97f3\u4e50\u3002\u7136\u800c\uff0c\u97f3\u4e50\u521b\u4f5c\u662f\u4e00\u9879\u590d\u6742\u7684\u591a\u5c42\u6b21\u4efb\u52a1\uff0c\u901a\u5e38\u5c06\u97f3\u4e50\u7f16\u6392\u4f5c\u4e3a\u521b\u4f5c\u8fc7\u7a0b\u7684\u4e00\u4e2a\u7ec4\u6210\u90e8\u5206\u3002\u6b64\u8fc7\u7a0b\u6d89\u53ca\u521b\u4f5c\u6bcf\u4e2a\u4e50\u5668\u90e8\u5206\uff0c\u4f7f\u5176\u5728\u8282\u594f\u3001\u529b\u5ea6\u3001\u548c\u58f0\u548c\u65cb\u5f8b\u65b9\u9762\u4e0e\u73b0\u6709\u90e8\u5206\u4fdd\u6301\u4e00\u81f4\uff0c\u8fd9\u9700\u8981\u6bd4\u6587\u672c\u63d0\u793a\u901a\u5e38\u63d0\u4f9b\u7684\u66f4\u7cbe\u786e\u7684\u97f3\u8f68\u63a7\u5236\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5c06 MusicLDM\uff08\u4e00\u79cd\u7528\u4e8e\u97f3\u4e50\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff09\u6269\u5c55\u4e3a\u591a\u8f68\u751f\u6210\u6a21\u578b\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002\u901a\u8fc7\u5b66\u4e60\u5171\u4eab\u4e0a\u4e0b\u6587\u7684\u97f3\u8f68\u7684\u8054\u5408\u6982\u7387\uff0c\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u8de8\u591a\u4e2a\u97f3\u8f68\u751f\u6210\u5f7c\u6b64\u826f\u597d\u5bf9\u5e94\u7684\u97f3\u4e50\uff0c\u65e0\u8bba\u662f\u6709\u6761\u4ef6\u5730\u8fd8\u662f\u65e0\u6761\u4ef6\u5730\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6a21\u578b\u8fd8\u80fd\u591f\u8fdb\u884c\u7f16\u66f2\u751f\u6210\uff0c\u5176\u4e2d\u6a21\u578b\u53ef\u4ee5\u5728\u7ed9\u5b9a\u5176\u4ed6\u97f3\u8f68\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u4efb\u4f55\u97f3\u8f68\u5b50\u96c6\uff08\u4f8b\u5982\uff0c\u751f\u6210\u4e0e\u7ed9\u5b9a\u8d1d\u65af\u548c\u9f13\u97f3\u8f68\u4e92\u8865\u7684\u94a2\u7434\u97f3\u8f68\uff09\u3002\u6211\u4eec\u5c06\u6211\u4eec\u7684\u6a21\u578b\u4e0e\u73b0\u6709\u7684\u591a\u8f68\u751f\u6210\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u603b\u751f\u6210\u4efb\u52a1\u548c\u7f16\u66f2\u751f\u6210\u4efb\u52a1\u7684\u5ba2\u89c2\u6307\u6807\u4e0a\u90fd\u53d6\u5f97\u4e86\u76f8\u5f53\u5927\u7684\u6539\u8fdb\u3002||\n", "2409.02683": "|**2024-09-04**|[Rethinking HTG Evaluation: Bridging Generation and Recognition](http://arxiv.org/abs/2409.02683)|null|\u751f\u6210\u6a21\u578b\u5728\u81ea\u7136\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u8bc4\u4f30\u5df2\u5f97\u5230\u5e7f\u6cdb\u7814\u7a76\u3002\u5373\u4f7f\u5728\u8bf8\u5982\u624b\u5199\u751f\u6210\uff08HTG\uff09\u7b49\u5177\u6709\u72ec\u7279\u7279\u6b8a\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u4f7f\u7528\u4e86\u7c7b\u4f3c\u7684\u534f\u8bae\u548c\u6307\u6807\uff0c\u5373\u4f7f\u5b83\u4eec\u53ef\u80fd\u5e76\u975e\u5b8c\u5168\u5408\u9002\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e09\u79cd\u4e13\u4e3a HTG \u8bc4\u4f30\u91cf\u8eab\u5b9a\u5236\u7684\u5ea6\u91cf\u6307\u6807\uff1a$ \\text{HTG}_{\\text{HTR}} $\u3001$ \\text{HTG}_{\\text{style}} $ \u548c $ \\text{HTG}_{\\text{OOV}} $\uff0c\u5e76\u8ba4\u4e3a\u5b83\u4eec\u66f4\u4fbf\u4e8e\u8bc4\u4f30\u751f\u6210\u624b\u5199\u56fe\u50cf\u7684\u8d28\u91cf\u3002\u8fd9\u4e9b\u6307\u6807\u4f9d\u8d56\u4e8e\u624b\u5199\u6587\u672c\u8bc6\u522b\u548c\u4e66\u5199\u8005\u8bc6\u522b\u6a21\u578b\u7684\u8bc6\u522b\u9519\u8bef/\u51c6\u786e\u7387\uff0c\u5e76\u5f3a\u8c03\u4e66\u5199\u98ce\u683c\u3001\u6587\u672c\u5185\u5bb9\u548c\u591a\u6837\u6027\u662f\u7b26\u5408\u624b\u5199\u56fe\u50cf\u5185\u5bb9\u7684\u4e3b\u8981\u65b9\u9762\u3002\u6211\u4eec\u5728 IAM \u624b\u5199\u6570\u636e\u5e93\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u8bf8\u5982 FID \u4e4b\u7c7b\u7684\u5e7f\u6cdb\u4f7f\u7528\u7684\u6307\u6807\u65e0\u6cd5\u6b63\u786e\u91cf\u5316\u751f\u6210\u624b\u5199\u6837\u672c\u7684\u591a\u6837\u6027\u548c\u5b9e\u7528\u6027\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6307\u6807\u4fe1\u606f\u66f4\u4e30\u5bcc\uff0c\u5e76\u5f3a\u8c03\u4e86 HTG \u4e2d\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\u7684\u5fc5\u8981\u6027\u3002\u6240\u63d0\u51fa\u7684\u6307\u6807\u4e3a\u8bc4\u4f30 HTG \u8d28\u91cf\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u3001\u4fe1\u606f\u66f4\u4e30\u5bcc\u7684\u534f\u8bae\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8 HTR \u7684\u6027\u80fd\u3002\u8bc4\u4f30\u534f\u8bae\u7684\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u5f97\uff1ahttps://github.com/koninik/HTG_evaluation\u3002||\n", "2409.02668": "|**2024-09-04**|[Introduction to Machine Learning](http://arxiv.org/abs/2409.02668)|null|\u672c\u4e66\u4ecb\u7ecd\u4e86\u673a\u5668\u5b66\u4e60\u4e2d\u8bb8\u591a\u7b97\u6cd5\u7684\u5f00\u53d1\u548c\u5206\u6790\u6240\u4f9d\u8d56\u7684\u6570\u5b66\u57fa\u7840\u548c\u6280\u672f\u3002\u672c\u4e66\u9996\u5148\u4ecb\u7ecd\u4e86\u8d2f\u7a7f\u5168\u4e66\u7684\u7b26\u53f7\u8868\u793a\uff0c\u5e76\u56de\u987e\u4e86\u5fae\u79ef\u5206\u3001\u7ebf\u6027\u4ee3\u6570\u548c\u6982\u7387\u8bba\u7684\u57fa\u672c\u6982\u5ff5\uff0c\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6d4b\u5ea6\u8bba\u672f\u8bed\uff0c\u53ef\u4f5c\u4e3a\u4f7f\u7528\u8fd9\u4e9b\u5de5\u5177\u7684\u90e8\u5206\u7684\u9605\u8bfb\u6307\u5357\u3002\u5bfc\u8bba\u7ae0\u8282\u8fd8\u63d0\u4f9b\u4e86\u77e9\u9635\u5206\u6790\u548c\u4f18\u5316\u7684\u80cc\u666f\u77e5\u8bc6\u3002\u540e\u9762\u7684\u7ae0\u8282\u4e3a\u672c\u4e66\u4e2d\u4f7f\u7528\u7684\u8bb8\u591a\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5305\u62ec\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u3001\u8fd1\u4f3c\u65b9\u6cd5\u7b49\u3002\u5728\u8ba8\u8bba\u4e86\u7edf\u8ba1\u9884\u6d4b\u7684\u57fa\u672c\u6982\u5ff5\u4e4b\u540e\uff0c\u672c\u4e66\u4ecb\u7ecd\u4e86\u518d\u751f\u6838\u7406\u8bba\u548c\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u6280\u672f\uff0c\u8fd9\u4e9b\u6280\u672f\u5728\u8bb8\u591a\u5730\u65b9\u90fd\u6709\u5e94\u7528\uff0c\u7136\u540e\u4ecb\u7ecd\u4e86\u5404\u79cd\u76d1\u7763\u7edf\u8ba1\u5b66\u4e60\u7b97\u6cd5\uff0c\u5305\u62ec\u7ebf\u6027\u65b9\u6cd5\u3001\u652f\u6301\u5411\u91cf\u673a\u3001\u51b3\u7b56\u6811\u3001boosting\u548c\u795e\u7ecf\u7f51\u7edc\u3002\u63a5\u4e0b\u6765\u8f6c\u5411\u751f\u6210\u65b9\u6cd5\uff0c\u9996\u5148\u4ecb\u7ecd\u4e86\u91c7\u6837\u65b9\u6cd5\u548c\u9a6c\u5c14\u53ef\u592b\u94fe\u7406\u8bba\u3002\u63a5\u4e0b\u6765\u7684\u7ae0\u8282\u63cf\u8ff0\u4e86\u56fe\u6a21\u578b\u7406\u8bba\uff0c\u4ecb\u7ecd\u4e86\u6f5c\u53d8\u91cf\u6a21\u578b\u7684\u53d8\u5206\u65b9\u6cd5\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u751f\u6210\u6a21\u578b\u3002\u63a5\u4e0b\u6765\u7684\u7ae0\u8282\u91cd\u70b9\u4ecb\u7ecd\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ec\u805a\u7c7b\u3001\u56e0\u5b50\u5206\u6790\u548c\u6d41\u5f62\u5b66\u4e60\u3002\u672c\u4e66\u7684\u6700\u540e\u4e00\u7ae0\u504f\u5411\u7406\u8bba\uff0c\u8ba8\u8bba\u4e86\u96c6\u4e2d\u4e0d\u7b49\u5f0f\u548c\u6cdb\u5316\u754c\u3002||\n", "2409.02664": "|**2024-09-04**|[Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection](http://arxiv.org/abs/2409.02664)|null|The proliferation of deepfake faces poses huge potential negative impacts on our daily lives. Despite substantial advancements in deepfake detection over these years, the generalizability of existing methods against forgeries from unseen datasets or created by emerging generative models remains constrained. In this paper, inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach that repurposes a well-trained VLM for general deepfake detection. Motivated by the model reprogramming paradigm that manipulates the model prediction via data perturbations, our method can reprogram a pretrained VLM model (e.g., CLIP) solely based on manipulating its input without tuning the inner parameters. Furthermore, we insert a pseudo-word guided by facial identity into the text prompt. Extensive experiments on several popular benchmarks demonstrate that (1) the cross-dataset and cross-manipulation performances of deepfake detection can be significantly and consistently improved (e.g., over 88% AUC in cross-dataset setting from FF++ to WildDeepfake) using a pre-trained CLIP model with our proposed reprogramming method; (2) our superior performances are at less cost of trainable parameters, making it a promising approach for real-world applications.||\n", "2409.02657": "|**2024-09-04**|[PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for One-Shot Talking Head Generation](http://arxiv.org/abs/2409.02657)|null|While previous audio-driven talking head generation (THG) methods generate head poses from driving audio, the generated poses or lips cannot match the audio well or are not editable. In this study, we propose \\textbf{PoseTalk}, a THG system that can freely generate lip-synchronized talking head videos with free head poses conditioned on text prompts and audio. The core insight of our method is using head pose to connect visual, linguistic, and audio signals. First, we propose to generate poses from both audio and text prompts, where the audio offers short-term variations and rhythm correspondence of the head movements and the text prompts describe the long-term semantics of head motions. To achieve this goal, we devise a Pose Latent Diffusion (PLD) model to generate motion latent from text prompts and audio cues in a pose latent space. Second, we observe a loss-imbalance problem: the loss for the lip region contributes less than 4\\% of the total reconstruction loss caused by both pose and lip, making optimization lean towards head movements rather than lip shapes. To address this issue, we propose a refinement-based learning strategy to synthesize natural talking videos using two cascaded networks, i.e., CoarseNet, and RefineNet. The CoarseNet estimates coarse motions to produce animated images in novel poses and the RefineNet focuses on learning finer lip motions by progressively estimating lip motions from low-to-high resolutions, yielding improved lip-synchronization performance. Experiments demonstrate our pose prediction strategy achieves better pose diversity and realness compared to text-only or audio-only, and our video generator model outperforms state-of-the-art methods in synthesizing talking videos with natural head motions. Project: https://junleen.github.io/projects/posetalk.||\n", "2409.02653": "|**2024-09-04**|[Skip-and-Play: Depth-Driven Pose-Preserved Image Generation for Any Objects](http://arxiv.org/abs/2409.02653)|null|The emergence of diffusion models has enabled the generation of diverse high-quality images solely from text, prompting subsequent efforts to enhance the controllability of these models. Despite the improvement in controllability, pose control remains limited to specific objects (e.g., humans) or poses (e.g., frontal view) due to the fact that pose is generally controlled via camera parameters (e.g., rotation angle) or keypoints (e.g., eyes, nose). Specifically, camera parameters-conditional pose control models generate unrealistic images depending on the object, owing to the small size of 3D datasets for training. Also, keypoint-based approaches encounter challenges in acquiring reliable keypoints for various objects (e.g., church) or poses (e.g., back view). To address these limitations, we propose depth-based pose control, as depth maps are easily obtainable from a single depth estimation model regardless of objects and poses, unlike camera parameters and keypoints. However, depth-based pose control confronts issues of shape dependency, as depth maps influence not only the pose but also the shape of the generated images. To tackle this issue, we propose Skip-and-Play (SnP), designed via analysis of the impact of three components of depth-conditional ControlNet on the pose and the shape of the generated images. To be specific, based on the analysis, we selectively skip parts of the components to mitigate shape dependency on the depth map while preserving the pose. Through various experiments, we demonstrate the superiority of SnP over baselines and showcase the ability of SnP to generate images of diverse objects and poses. Remarkably, SnP exhibits the ability to generate images even when the objects in the condition (e.g., a horse) and the prompt (e.g., a hedgehog) differ from each other.||\n", "2409.03757": "|**2024-09-05**|[Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding](http://arxiv.org/abs/2409.03757)|**[link](https://github.com/yunzeman/lexicon3d)**|\u590d\u6742\u4e09\u7ef4\u573a\u666f\u7406\u89e3\u8fd1\u5e74\u6765\u5907\u53d7\u5173\u6ce8\uff0c\u573a\u666f\u7f16\u7801\u7b56\u7565\u5728\u5176\u4e2d\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u9488\u5bf9\u4e0d\u540c\u573a\u666f\u7684\u6700\u4f73\u573a\u666f\u7f16\u7801\u7b56\u7565\u4ecd\u4e0d\u660e\u786e\uff0c\u7279\u522b\u662f\u4e0e\u57fa\u4e8e\u56fe\u50cf\u7684\u7f16\u7801\u7b56\u7565\u76f8\u6bd4\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5bf9\u7528\u4e8e\u4e09\u7ef4\u573a\u666f\u7406\u89e3\u7684\u5404\u79cd\u89c6\u89c9\u7f16\u7801\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u7814\u7a76\uff0c\u786e\u5b9a\u4e86\u6bcf\u4e2a\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u6db5\u76d6\u4e86\u4e03\u79cd\u89c6\u89c9\u57fa\u7840\u7f16\u7801\u5668\uff0c\u5305\u62ec\u57fa\u4e8e\u56fe\u50cf\u3001\u57fa\u4e8e\u89c6\u9891\u548c\u4e09\u7ef4\u57fa\u7840\u6a21\u578b\u3002\u6211\u4eec\u5728\u56db\u4e2a\u4efb\u52a1\u4e2d\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\uff1a\u89c6\u89c9\u8bed\u8a00\u573a\u666f\u63a8\u7406\u3001\u89c6\u89c9\u5b9a\u4f4d\u3001\u5206\u5272\u548c\u914d\u51c6\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u4fa7\u91cd\u4e8e\u573a\u666f\u7406\u89e3\u7684\u4e0d\u540c\u65b9\u9762\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u5f97\u51fa\u4e86\u4ee5\u4e0b\u4e3b\u8981\u53d1\u73b0\uff1aDINOv2 \u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u89c6\u9891\u6a21\u578b\u5728\u5bf9\u8c61\u7ea7\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6269\u6563\u6a21\u578b\u6709\u5229\u4e8e\u51e0\u4f55\u4efb\u52a1\uff0c\u800c\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u8bed\u8a00\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u610f\u60f3\u4e0d\u5230\u7684\u5c40\u9650\u6027\u3002\u8fd9\u4e9b\u89c1\u89e3\u6311\u6218\u4e86\u4e00\u4e9b\u4f20\u7edf\u8ba4\u77e5\uff0c\u4e3a\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u672a\u6765\u7684\u89c6\u89c9\u8bed\u8a00\u548c\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e2d\u9700\u8981\u66f4\u7075\u6d3b\u7684\u7f16\u7801\u5668\u9009\u62e9\u3002||\n", "2409.03745": "|**2024-09-05**|[ArtiFade: Learning to Generate High-quality Subject from Blemished Images](http://arxiv.org/abs/2409.03745)|null|\u4ee5\u4e3b\u9898\u4e3a\u4e3b\u5bfc\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6280\u672f\u5728\u5b66\u4e60\u548c\u6355\u6349\u4e3b\u9898\u7279\u5f81\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u5373\u4f7f\u53ea\u4f7f\u7528\u6709\u9650\u6570\u91cf\u7684\u56fe\u50cf\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u5f53\u8f93\u5165\u56fe\u50cf\u5b58\u5728\u7455\u75b5\u65f6\uff0c\u53ef\u80fd\u96be\u4ee5\u751f\u6210\u5408\u7406\u7684\u56fe\u50cf\u3002\u8fd9\u4e3b\u8981\u5f52\u56e0\u4e8e\u5f53\u524d\u6280\u672f\u5728\u533a\u5206\u4e3b\u9898\u76f8\u5173\u7279\u5f81\u548c\u5e72\u6270\u6027\u7455\u75b5\u65b9\u9762\u7684\u80fd\u529b\u4e0d\u8db3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86ArtiFade\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u6210\u529f\u5730\u4ece\u6709\u7455\u75b5\u7684\u6570\u636e\u96c6\u4e2d\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u7684\u65e0\u7455\u75b5\u56fe\u50cf\u3002\u5177\u4f53\u6765\u8bf4\uff0cArtiFade\u5229\u7528\u9884\u5148\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5fae\u8c03\u6765\u6d88\u9664\u7455\u75b5\u3002\u901a\u8fc7\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u5305\u542b\u65e0\u7455\u75b5\u56fe\u50cf\u53ca\u5176\u5bf9\u5e94\u7684\u6709\u7455\u75b5\u56fe\u50cf\u7684\u4e13\u95e8\u6570\u636e\u96c6\u6765\u5b9e\u73b0\u7455\u75b5\u7684\u6d88\u9664\u3002ArtiFade\u8fd8\u786e\u4fdd\u4e86\u4fdd\u7559\u6269\u6563\u6a21\u578b\u4e2d\u56fa\u6709\u7684\u539f\u59cb\u751f\u6210\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u4e3b\u9898\u9a71\u52a8\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u548c\u65e0\u7455\u75b5\u56fe\u50cf\u65b9\u9762\u7684\u6574\u4f53\u6027\u80fd\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u4e3a\u8fd9\u9879\u4efb\u52a1\u8bbe\u8ba1\u4e86\u8bc4\u4f30\u57fa\u51c6\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86ArtiFade\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u60c5\u51b5\u4e0b\u90fd\u80fd\u6709\u6548\u53bb\u9664\u7455\u75b5\u7684\u6cdb\u5316\u80fd\u529b\u3002||\n", "2409.03644": "|**2024-09-05**|[RealisHuman: A Two-Stage Approach for Refining Malformed Human Parts in Generated Images](http://arxiv.org/abs/2409.03644)|null|\u8fd1\u5e74\u6765\uff0c\u6269\u6563\u6a21\u578b\u5f7b\u5e95\u6539\u53d8\u4e86\u89c6\u89c9\u751f\u6210\u9886\u57df\uff0c\u5176\u6027\u80fd\u8d85\u8d8a\u4e86\u751f\u6210\u5bf9\u6297\u7f51\u7edc (GANs) \u7b49\u4f20\u7edf\u6846\u67b6\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4eba\u7c7b\u53ca\u5176\u8bed\u4e49\u90e8\u5206\uff08\u5982\u624b\u548c\u8138\uff09\u590d\u6742\u7684\u7ed3\u6784\uff0c\u751f\u6210\u5177\u6709\u771f\u5b9e\u611f\u7684\u4eba\u7c7b\u56fe\u50cf\u4ecd\u7136\u662f\u4e00\u9879\u91cd\u5927\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a RealisHuman \u7684\u65b0\u578b\u540e\u5904\u7406\u89e3\u51b3\u65b9\u6848\u3002RealisHuman \u6846\u67b6\u5206\u4e24\u4e2a\u9636\u6bb5\u8fd0\u884c\u3002\u9996\u5148\uff0c\u5b83\u4f7f\u7528\u539f\u59cb\u7684\u7578\u5f62\u90e8\u5206\u4f5c\u4e3a\u53c2\u8003\uff0c\u751f\u6210\u903c\u771f\u7684\u4eba\u4f53\u90e8\u4f4d\uff08\u5982\u624b\u6216\u8138\uff09\uff0c\u786e\u4fdd\u7ec6\u8282\u4e0e\u539f\u59cb\u56fe\u50cf\u4e00\u81f4\u3002\u5176\u6b21\uff0c\u5b83\u901a\u8fc7\u91cd\u65b0\u7ed8\u5236\u5468\u56f4\u533a\u57df\u5c06\u6821\u6b63\u540e\u7684\u4eba\u4f53\u90e8\u4f4d\u65e0\u7f1d\u5730\u878d\u5165\u5230\u5176\u5bf9\u5e94\u7684\u4f4d\u7f6e\uff0c\u4ee5\u786e\u4fdd\u5e73\u6ed1\u903c\u771f\u7684\u878d\u5408\u3002RealisHuman \u6846\u67b6\u663e\u8457\u589e\u5f3a\u4e86\u4eba\u7c7b\u751f\u6210\u7684\u771f\u5b9e\u611f\uff0c\u8fd9\u53ef\u4ee5\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u6307\u6807\u7684\u663e\u8457\u6539\u8fdb\u5f97\u5230\u8bc1\u660e\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/Wangbenzhi/RealisHuman \u83b7\u53d6\u3002||\n", "2409.03636": "|**2024-09-05**|[DiffEVC: Any-to-Any Emotion Voice Conversion with Expressive Guidance](http://arxiv.org/abs/2409.03636)|null|\u60c5\u611f\u8bed\u97f3\u8f6c\u6362 (EVC) \u901a\u8fc7\u653e\u5927\u79ef\u6781\u7ebf\u7d22\u548c\u51cf\u5c11\u6d88\u6781\u7ebf\u7d22\u6765\u6539\u53d8\u8bed\u97f3\u60c5\u611f\uff0c\u4ece\u800c\u589e\u5f3a\u6c9f\u901a\u3002\u8fd9\u9879\u590d\u6742\u7684\u4efb\u52a1\u6d89\u53ca\u8bed\u97f3\u8d28\u91cf\u3001\u8bf4\u8bdd\u8005\u7279\u5f81\u548c\u5185\u5bb9\u7b49\u7ea0\u7f20\u4e0d\u6e05\u7684\u56e0\u7d20\u3002\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982 GAN \u548c\u81ea\u52a8\u7f16\u7801\u5668\uff09\u901a\u8fc7\u5b66\u4e60\u6620\u5c04\u6216\u89e3\u8026\u7279\u5f81\u5728 EVC \u4e2d\u53d6\u5f97\u4e86\u4e00\u5b9a\u7684\u6210\u529f\uff0c\u4f46\u9762\u4e34\u7740\u4e0d\u7a33\u5b9a\u6027\u548c\u8bed\u97f3\u8d28\u91cf\u4e0b\u964d\u7b49\u6311\u6218\u3002\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u7a33\u5b9a\u7684\u8bad\u7ec3\u548c\u9ad8\u8d28\u91cf\u7684\u751f\u6210\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684 EVC \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u4e92\u4fe1\u606f\u635f\u5931\u548c\u8f85\u52a9\u6a21\u578b\u6765\u89e3\u8026\u60c5\u611f\u548c\u8bf4\u8bdd\u8005\u8eab\u4efd\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u8868\u8fbe\u6027\u5f15\u5bfc\u673a\u5236\uff0c\u4ee5\u6539\u5584\u60c5\u611f\u8f6c\u6362\uff0c\u540c\u65f6\u4fdd\u6301\u8bf4\u8bdd\u8005\u7279\u5f81\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5bf9\u4e8e\u672a\u77e5\u8bf4\u8bdd\u8005\u548c\u60c5\u611f\u7684\u6709\u6548\u6027\uff0c\u5728 EVC \u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002||\n", "2409.03600": "|**2024-09-05**|[TCDiff: Triple Condition Diffusion Model with 3D Constraints for Stylizing Synthetic Faces](http://arxiv.org/abs/2409.03600)|**[link](https://github.com/bovifocr/tcdiff)**|\u4e00\u4e2a\u9c81\u68d2\u7684\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u9700\u8981\u4f7f\u7528\u5305\u542b\u5927\u91cf\u4e2a\u4f53\u4ee5\u53ca\u6bcf\u4e2a\u4e2a\u4f53\u5728\u4e0d\u540c\u6761\u4ef6\uff08\u4f8b\u5982\u59ff\u6001\u3001\u8868\u60c5\u3001\u5e74\u9f84\u3001\u566a\u58f0\u548c\u906e\u6321\uff09\u4e0b\u7684\u5927\u91cf\u6837\u672c\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002\u7531\u4e8e\u4f26\u7406\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5927\u578b\u771f\u5b9e\u4eba\u8138\u6570\u636e\u96c6\uff08\u4f8b\u5982 MS1MV3\uff09\u5df2\u88ab\u505c\u7528\uff0c\u5e76\u4e14\u5df2\u7ecf\u63d0\u51fa\u4e86\u5229\u7528 GAN \u548c\u6269\u6563\u6a21\u578b\u7684\u5408\u6210\u4eba\u8138\u751f\u6210\u5668\uff0c\u4f8b\u5982 SYNFace\u3001SFace\u3001DigiFace-1M\u3001IDiff-Face\u3001DCFace \u548c GANDiffFace\uff0c\u65e8\u5728\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u3002\u5176\u4e2d\u4e00\u4e9b\u65b9\u6cd5\u53ef\u4ee5\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u771f\u5b9e\u4eba\u8138\uff0c\u4f46\u7c7b\u5185\u5dee\u5f02\u8f83\u4f4e\uff0c\u800c\u53e6\u4e00\u4e9b\u65b9\u6cd5\u5219\u751f\u6210\u5177\u6709\u9ad8\u5dee\u5f02\u6027\u4f46\u8eab\u4efd\u4e00\u81f4\u6027\u8f83\u4f4e\u7684\u4eba\u8138\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u91cd\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff08TCDiff\uff09\uff0c\u901a\u8fc7 2D \u548c 3D \u4eba\u8138\u7ea6\u675f\u6765\u6539\u8fdb\u4ece\u771f\u5b9e\u4eba\u8138\u5230\u5408\u6210\u4eba\u8138\u7684\u4eba\u8138\u98ce\u683c\u8fc1\u79fb\uff0c\u5728\u4fdd\u6301\u5fc5\u8981\u7684\u7c7b\u5185\u9ad8\u5dee\u5f02\u6027\u7684\u540c\u65f6\u589e\u5f3a\u4eba\u8138\u8eab\u4efd\u4e00\u81f4\u6027\u3002\u4f7f\u7528\u6211\u4eec\u65b0\u7684\u6570\u636e\u96c6\u7684 1k\u30012k \u548c 5k \u7c7b\u8fdb\u884c\u8bad\u7ec3\u7684\u4eba\u8138\u8bc6\u522b\u5b9e\u9a8c\u5728 LFW\u3001CFP-FP\u3001AgeDB \u548c BUPT \u7b49\u771f\u5b9e\u4eba\u8138\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5408\u6210\u6570\u636e\u96c6\u3002\u6211\u4eec\u7684\u6e90\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u5f97\uff1ahttps://github.com/BOVIFOCR/tcdiff\u3002||\n", "2409.03550": "|**2024-09-05**|[DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any Architecture](http://arxiv.org/abs/2409.03550)|null|\u6269\u6563\u6a21\u578b (DM) \u5728\u5404\u4e2a\u9886\u57df\u90fd\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5176\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u7684\u63a8\u7406\u901f\u5ea6\u6162\u548c\u8ba1\u7b97\u9700\u6c42\u9ad8\u5374\u963b\u788d\u4e86\u5176\u53d1\u5c55\u3002\u52a0\u901fDM\u6700\u5e38\u7528\u7684\u65b9\u6cd5\u662f\u51cf\u5c11\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u53bb\u566a\u6b65\u9aa4\uff0c\u8fd9\u53ef\u4ee5\u901a\u8fc7\u66f4\u5feb\u7684\u91c7\u6837\u6c42\u89e3\u5668\u6216\u77e5\u8bc6\u84b8\u998f (KD) \u6765\u5b9e\u73b0\u3002\u4e0e\u5148\u524d\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5c06\u5927\u578b\u9884\u8bad\u7ec3DM\u7684\u529f\u80fd\u8fc1\u79fb\u5230\u66f4\u5feb\u7684\u67b6\u6784\u4e2d\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u4ee5\u72ec\u7279\u7684\u65b9\u5f0f\u4f7f\u7528KD\uff0c\u901a\u8fc7\u5c06\u751f\u6210\u80fd\u529b\u63d0\u70bc\u5230\u66f4\u5feb\u7684\u53d8\u4f53\u4e2d\u6765\u538b\u7f29DM\u3002\u6b64\u5916\uff0c\u8003\u8651\u5230\u6e90\u6570\u636e\u4e0d\u53ef\u8bbf\u95ee\u6216\u5bf9\u4e8e\u5f53\u524d\u7684\u751f\u6210\u6a21\u578b\u6765\u8bf4\u5b58\u50a8\u91cf\u592a\u5927\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u6e90\u6570\u636e\u84b8\u998f\u8303\u5f0f\uff0c\u79f0\u4e3a\u6269\u6563\u6a21\u578b\u7684\u65e0\u6570\u636e\u77e5\u8bc6\u84b8\u998f (DKDM)\u3002\u901a\u5e38\uff0c\u6211\u4eec\u5efa\u7acb\u7684DKDM\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a1) DKDM\u76ee\u6807\u51fd\u6570\uff0c\u5b83\u4f7f\u7528\u9884\u8bad\u7ec3DM\u751f\u6210\u7684\u5408\u6210\u53bb\u566a\u6570\u636e\u6765\u4f18\u5316\u66f4\u5feb\u7684DM\uff0c\u800c\u65e0\u9700\u6e90\u6570\u636e\uff1b2) \u52a8\u6001\u8fed\u4ee3\u84b8\u998f\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u7075\u6d3b\u5730\u7ec4\u7ec7\u53bb\u566a\u6570\u636e\u7684\u5408\u6210\uff0c\u9632\u6b62\u7531\u4e8e\u751f\u6210\u901f\u5ea6\u6162\u800c\u51cf\u6162\u4f18\u5316\u8fc7\u7a0b\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u6b21\u5c1d\u8bd5\u4f7f\u7528KD\u4ee5\u65e0\u6570\u636e\u7684\u65b9\u5f0f\u5c06DM\u63d0\u70bc\u5230\u4efb\u4f55\u67b6\u6784\u4e2d\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u7684DKDM\u4e0e\u5927\u591a\u6570\u73b0\u6709\u7684\u52a0\u901f\u65b9\u6cd5\uff08\u4f8b\u5982\u51cf\u5c11\u53bb\u566a\u6b65\u9aa4\u3001\u91cf\u5316\u548c\u526a\u679d\uff09\u662f\u6b63\u4ea4\u7684\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684DKDM\u80fd\u591f\u63a8\u5bfc\u51fa\u901f\u5ea6\u63d0\u9ad82\u500d\u7684DM\uff0c\u5176\u6027\u80fd\u4e0e\u57fa\u7ebf\u4fdd\u6301\u4e00\u81f4\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684DKDM\u4f7f\u9884\u8bad\u7ec3\u7684DM\u80fd\u591f\u4f5c\u4e3a\u201c\u6570\u636e\u96c6\u201d\u6765\u8bad\u7ec3\u65b0\u7684DM\u3002||\n", "2409.03514": "|**2024-09-05**|[Blended Latent Diffusion under Attention Control for Real-World Video Editing](http://arxiv.org/abs/2409.03514)|null|\u7531\u4e8e\u7f3a\u4e4f\u5b8c\u5168\u516c\u5f00\u53ef\u7528\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\uff0c\u76ee\u524d\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u503e\u5411\u4e8e\u5efa\u7acb\u5728\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e4b\u4e0a\uff0c\u7136\u800c\uff0c\u5728\u5904\u7406\u5177\u6709\u65f6\u95f4\u4fe1\u606f\u7684\u89c6\u9891\u5c40\u90e8\u7f16\u8f91\u65b9\u9762\uff0c\u5b83\u4eec\u4ecd\u7136\u9762\u4e34\u7740\u5de8\u5927\u7684\u6311\u6218\u3002\u9996\u5148\uff0c\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u8bd5\u56fe\u901a\u8fc7\u9884\u5148\u5b9a\u4e49\u7684\u63a9\u7801\u6765\u5173\u6ce8\u5c40\u90e8\u533a\u57df\u7f16\u8f91\uff0c\u4f46\u7531\u4e8e\u6bcf\u4e00\u5e27\u7684\u7a7a\u95f4\u6574\u4f53\u751f\u6210\uff0c\u5916\u90e8\u533a\u57df\u80cc\u666f\u7684\u4fdd\u7559\u5e76\u4e0d\u7406\u60f3\u3002\u6b64\u5916\uff0c\u7531\u7528\u6237\u4e13\u95e8\u63d0\u4f9b\u63a9\u7801\u662f\u4e00\u9879\u989d\u5916\u7684\u6602\u8d35\u5de5\u4f5c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u96c6\u6210\u5230\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u7684\u81ea\u4e3b\u63a9\u7801\u7b56\u7565\u3002\u6700\u540e\u4f46\u540c\u6837\u91cd\u8981\u7684\u662f\uff0c\u56fe\u50cf\u7ea7\u9884\u8bad\u7ec3\u6a21\u578b\u6ca1\u6709\u5b66\u4e60\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u800c\u8fd9\u5bf9\u4e8e\u8868\u8fbe\u8fd0\u52a8\u548c\u52a8\u6001\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u91c7\u7528\u56fe\u50cf\u7ea7\u6df7\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6765\u6267\u884c\u5c40\u90e8\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5229\u7528 DDIM \u53cd\u6f14\u6765\u83b7\u53d6\u6f5c\u5728\u5411\u91cf\u4f5c\u4e3a\u80cc\u666f\u6f5c\u5728\u5411\u91cf\uff0c\u800c\u4e0d\u662f\u968f\u673a\u566a\u58f0\u7684\u6f5c\u5728\u5411\u91cf\uff0c\u4ee5\u66f4\u597d\u5730\u4fdd\u7559\u8f93\u5165\u89c6\u9891\u7684\u80cc\u666f\u4fe1\u606f\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4ece\u6269\u6563\u6b65\u9aa4\u4e2d\u7684\u4ea4\u53c9\u6ce8\u610f\u56fe\u884d\u751f\u7684\u81ea\u4e3b\u63a9\u7801\u5236\u9020\u673a\u5236\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u5c06 U-Net \u7684\u81ea\u6ce8\u610f\u529b\u5757\u8f6c\u6362\u4e3a\u65f6\u7a7a\u5757\u6765\u589e\u5f3a\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002||\n", "2409.03455": "|**2024-09-05**|[Data-free Distillation with Degradation-prompt Diffusion for Multi-weather Image Restoration](http://arxiv.org/abs/2409.03455)|null|\u591a\u5929\u6c14\u56fe\u50cf\u590d\u539f\u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u8fdb\u5c55\uff0c\u4f46\u6a21\u578b\u5bb9\u91cf\u7684\u589e\u52a0\u548c\u6602\u8d35\u7684\u6570\u636e\u83b7\u53d6\u9650\u5236\u4e86\u5176\u5728\u5185\u5b58\u6709\u9650\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u3002\u65e0\u6570\u636e\u84b8\u998f\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u5141\u8bb8\u4ece\u9884\u8bad\u7ec3\u7684\u6559\u5e08\u6a21\u578b\u4e2d\u5b66\u4e60\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u3002\u73b0\u6709\u7684\u65e0\u6570\u636e\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5229\u7528GAN\u751f\u6210\u7684\u4f2a\u6570\u636e\u6216\u4ece\u4e92\u8054\u7f51\u6536\u96c6\u7684\u771f\u5b9e\u6570\u636e\u6765\u4f18\u5316\u6a21\u578b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u9047\u5230\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6216\u4e0e\u539f\u59cb\u6570\u636e\u5b58\u5728\u57df\u504f\u79fb\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u9000\u5316\u63d0\u793a\u6269\u6563\u7684\u65e0\u6570\u636e\u84b8\u998f\u591a\u5929\u6c14\u56fe\u50cf\u590d\u539f\u6846\u67b6\uff08D4IR\uff09\u3002\u5b83\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u4ee3\u66ffGAN\u4ee5\u907f\u514d\u6a21\u578b\u5d29\u6e83\uff0c\u5e76\u7ed3\u5408\u4e86\u9000\u5316\u611f\u77e5\u63d0\u793a\u9002\u914d\u5668\uff0c\u4ee5\u4fc3\u8fdb\u5185\u5bb9\u9a71\u52a8\u7684\u6761\u4ef6\u6269\u6563\uff0c\u4ece\u800c\u751f\u6210\u4e0e\u57df\u76f8\u5173\u7684\u56fe\u50cf\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u9996\u5148\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u7684\u9000\u5316\u63d0\u793a\u9002\u914d\u5668\uff0c\u7528\u4e8e\u4ece\u7f51\u7edc\u6536\u96c6\u7684\u9000\u5316\u56fe\u50cf\u4e2d\u6355\u83b7\u9000\u5316\u611f\u77e5\u63d0\u793a\u3002\u7136\u540e\uff0c\u5c06\u6536\u96c6\u5230\u7684\u672a\u914d\u5bf9\u7684\u5e72\u51c0\u56fe\u50cf\u6270\u52a8\u5230\u7a33\u5b9a\u6269\u6563\u7684\u6f5c\u5728\u7279\u5f81\u4e2d\uff0c\u5e76\u4ee5\u9000\u5316\u611f\u77e5\u63d0\u793a\u4e3a\u6761\u4ef6\uff0c\u5408\u6210\u65b0\u7684\u57df\u76f8\u5173\u9000\u5316\u56fe\u50cf\uff0c\u7528\u4e8e\u77e5\u8bc6\u84b8\u998f\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u4e0e\u4f7f\u7528\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u84b8\u998f\u7684\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u751a\u81f3\u4f18\u4e8e\u5176\u4ed6\u4e3b\u6d41\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u3002||\n", "2409.03417": "|**2024-09-05**|[Convergence Rates for the Maximum A Posteriori Estimator in PDE-Regression Models with Random Design](http://arxiv.org/abs/2409.03417)|null|\u6211\u4eec\u8003\u8651\u4ece\u9ad8\u65af\u56de\u5f52\u95ee\u9898$Y = \\mathscr{G}(\\theta)(Z)+\\varepsilon$\u4ea7\u751f\u7684\u6570\u636e\u4e2d\u6062\u590d\u53c2\u6570$\\theta\\in H^\\alpha$\u7684\u7edf\u8ba1\u9006\u95ee\u9898\uff0c\u5176\u4e2d$\\mathscr{G}:\\mathbb{L}^2\\to\\mathbb{L}^2$\u662f\u975e\u7ebf\u6027\u6b63\u5411\u6620\u5c04\uff0c$Z$\u662f\u968f\u673a\u8bbe\u8ba1\u70b9\uff0c$\\varepsilon$\u662f\u9ad8\u65af\u566a\u58f0\u3002\u4f30\u8ba1\u7b56\u7565\u57fa\u4e8e$\\Vert\\cdot\\Vert_{H^\\alpha}$-\u7ea6\u675f\u4e0b\u7684\u6700\u5c0f\u4e8c\u4e58\u6cd5\u3002\u6211\u4eec\u5728\u6b63\u5411\u6620\u5c04$\\mathscr{G}$\u6ee1\u8db3Lipschitz\u7c7b\u578b\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\uff0c\u5efa\u7acb\u4e86\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\u91cf$\\hat{\\theta}$\u4f5c\u4e3a\u7ed9\u5b9a\u6cdb\u51fd\u7684\u6700\u5927\u503c\u7684\u5b58\u5728\u6027\u3002\u8bc1\u660e\u4e86\u4e00\u4e2a\u4e00\u822c\u7684\u6d53\u5ea6\u7ed3\u679c\uff0c\u5e76\u7528\u5b83\u6765\u8bc1\u660e\u9884\u6d4b\u8bef\u5dee\u7684\u4e00\u81f4\u6027\u548c\u4e0a\u754c\u3002\u76f8\u5e94\u7684\u6536\u655b\u901f\u5ea6\u4e0d\u4ec5\u53cd\u6620\u4e86\u76ee\u6807\u53c2\u6570\u7684\u5e73\u6ed1\u6027\uff0c\u8fd8\u53cd\u6620\u4e86\u6f5c\u5728\u9006\u95ee\u9898\u7684\u9002\u5b9a\u6027\u3002\u6211\u4eec\u5c06\u4e00\u822c\u6a21\u578b\u5e94\u7528\u4e8e\u8fbe\u897f\u95ee\u9898\uff0c\u5176\u4e2dPDE\u7684\u672a\u77e5\u7cfb\u6570\u51fd\u6570$f$\u7684\u6062\u590d\u662f\u4ee4\u4eba\u611f\u5174\u8da3\u7684\u3002\u5bf9\u4e8e\u8fd9\u4e2a\u4f8b\u5b50\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u9884\u6d4b\u8bef\u5dee\u548c\u4f30\u8ba1\u8bef\u5dee\u7684\u76f8\u5e94\u6536\u655b\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u7b80\u8981\u8ba8\u8bba\u4e86\u8be5\u4e00\u822c\u6a21\u578b\u5bf9\u5176\u4ed6\u95ee\u9898\u7684\u9002\u7528\u6027\u3002||\n", "2409.03403": "|**2024-09-05**|[RoVi-Aug: Robot and Viewpoint Augmentation for Cross-Embodiment Robot Learning](http://arxiv.org/abs/2409.03403)|null|\u6269\u5927\u673a\u5668\u4eba\u5b66\u4e60\u89c4\u6a21\u9700\u8981\u5e9e\u5927\u800c\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\uff0c\u5982\u4f55\u6709\u6548\u5730\u91cd\u590d\u4f7f\u7528\u6536\u96c6\u5230\u7684\u6570\u636e\u5e76\u5c06\u7b56\u7565\u8fc1\u79fb\u5230\u65b0\u7684\u673a\u5668\u4eba\u5e73\u53f0\u4ecd\u7136\u662f\u4e00\u4e2a\u60ac\u800c\u672a\u51b3\u7684\u95ee\u9898\u3002\u8bf8\u5982Open-X Embodiment (OXE) \u9879\u76ee\u7b49\u65b0\u5174\u7814\u7a76\u5df2\u7ecf\u8868\u660e\uff0c\u901a\u8fc7\u7ec4\u5408\u5305\u542b\u4e0d\u540c\u673a\u5668\u4eba\u7684\u6570\u636e\u96c6\u6765\u5229\u7528\u6280\u80fd\u662f\u6709\u5e0c\u671b\u7684\u3002\u7136\u800c\uff0c\u8bb8\u591a\u6570\u636e\u96c6\u4e2d\u673a\u5668\u4eba\u7c7b\u578b\u548c\u76f8\u673a\u89d2\u5ea6\u5206\u5e03\u7684\u4e0d\u5e73\u8861\u4f7f\u5f97\u7b56\u7565\u5bb9\u6613\u8fc7\u62df\u5408\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86RoVi-Aug\uff0c\u5b83\u5229\u7528\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u5408\u6210\u5177\u6709\u4e0d\u540c\u673a\u5668\u4eba\u548c\u76f8\u673a\u89c6\u89d2\u7684\u6f14\u793a\u6765\u589e\u5f3a\u673a\u5668\u4eba\u6570\u636e\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u7269\u7406\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u901a\u8fc7\u5728\u673a\u5668\u4eba\u548c\u89c6\u70b9\u589e\u5f3a\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0cRoVi-Aug \u53ef\u4ee5\u5728\u5177\u6709\u663e\u8457\u4e0d\u540c\u76f8\u673a\u89d2\u5ea6\u7684\u672a\u77e5\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u90e8\u7f72\u3002\u4e0e Mirage \u7b49\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u7b97\u6cd5\u76f8\u6bd4\uff0cRoVi-Aug \u5728\u6d4b\u8bd5\u65f6\u4e0d\u9700\u8981\u989d\u5916\u7684\u5904\u7406\uff0c\u4e0d\u5047\u8bbe\u5df2\u77e5\u76f8\u673a\u89d2\u5ea6\uff0c\u5e76\u4e14\u5141\u8bb8\u7b56\u7565\u5fae\u8c03\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5728\u539f\u59cb\u673a\u5668\u4eba\u6570\u636e\u96c6\u548c\u589e\u5f3a\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\uff0cRoVi-Aug \u53ef\u4ee5\u5b66\u4e60\u591a\u673a\u5668\u4eba\u548c\u591a\u4efb\u52a1\u7b56\u7565\uff0c\u4ece\u800c\u5b9e\u73b0\u673a\u5668\u4eba\u548c\u6280\u80fd\u4e4b\u95f4\u66f4\u6709\u6548\u7684\u8fc1\u79fb\uff0c\u5e76\u5c06\u6210\u529f\u7387\u63d0\u9ad8\u9ad8\u8fbe 30%\u3002||\n", "2409.05798": "|**2024-09-09**|[Enhancing Preference-based Linear Bandits via Human Response Time](http://arxiv.org/abs/2409.05798)|null|Binary human choice feedback is widely used in interactive preference learning for its simplicity, but it provides limited information about preference strength. To overcome this limitation, we leverage human response times, which inversely correlate with preference strength, as complementary information. Our work integrates the EZ-diffusion model, which jointly models human choices and response times, into preference-based linear bandits. We introduce a computationally efficient utility estimator that reformulates the utility estimation problem using both choices and response times as a linear regression problem. Theoretical and empirical comparisons with traditional choice-only estimators reveal that for queries with strong preferences (\"easy\" queries), choices alone provide limited information, while response times offer valuable complementary information about preference strength. As a result, incorporating response times makes easy queries more useful. We demonstrate this advantage in the fixed-budget best-arm identification problem, with simulations based on three real-world datasets, consistently showing accelerated learning when response times are incorporated.||\n", "2409.05790": "|**2024-09-09**|[Predicting Critical Heat Flux with Uncertainty Quantification and Domain Generalization Using Conditional Variational Autoencoders and Deep Neural Networks](http://arxiv.org/abs/2409.05790)|null|Deep generative models (DGMs) have proven to be powerful in generating realistic data samples. Their capability to learn the underlying distribution of a dataset enable them to generate synthetic data samples that closely resemble the original training dataset, thus addressing the challenge of data scarcity. In this work, we investigated the capabilities of DGMs by developing a conditional variational autoencoder (CVAE) model to augment the critical heat flux (CHF) measurement data that was used to generate the 2006 Groeneveld lookup table. To determine how this approach compared to traditional methods, a fine-tuned deep neural network (DNN) regression model was created and evaluated with the same dataset. Both the CVAE and DNN models achieved small mean absolute relative errors, with the CVAE model maintaining more favorable results. To quantify the uncertainty in the model's predictions, uncertainty quantification (UQ) was performed with repeated sampling of the CVAE model and ensembling of the DNN model. Following UQ, the DNN ensemble notably improved performance when compared to the baseline DNN model, while the CVAE model achieved similar results to its non-UQ results. The CVAE model was shown to have significantly less variability and a higher confidence after assessment of the prediction-wise relative standard deviations. Evaluating domain generalization, both models achieved small mean error values when predicting both inside and outside the training domain, with predictions outside the training domain showing slightly larger errors. Overall, the CVAE model was comparable to the DNN regression model in predicting CHF values but with better uncertainty behavior.||\n", "2409.05784": "|**2024-09-09**|[Vector Quantized Diffusion Model Based Speech Bandwidth Extension](http://arxiv.org/abs/2409.05784)|null|\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668 (NAC) \u7684\u6700\u65b0\u8fdb\u5c55\u4e3a\u97f3\u9891\u4fe1\u53f7\u5904\u7406\u89e3\u9501\u4e86\u65b0\u7684\u6f5c\u529b\u3002\u8d8a\u6765\u8d8a\u591a\u7684\u7814\u7a76\u63a2\u7d22\u5229\u7528 NAC \u7684\u6f5c\u5728\u7279\u5f81\u6765\u5b8c\u6210\u5404\u79cd\u8bed\u97f3\u4fe1\u53f7\u5904\u7406\u4efb\u52a1\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u7b2c\u4e00\u79cd\u5229\u7528\u4ece NAC \u83b7\u5f97\u7684\u79bb\u6563\u7279\u5f81\u8fdb\u884c\u8bed\u97f3\u5e26\u5bbd\u6269\u5c55 (BWE) \u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u6062\u590d\u9ad8\u5ea6\u538b\u7f29\u7684\u79bb\u6563\u6807\u8bb0\u4e2d\u7684\u9ad8\u9891\u7ec6\u8282\uff0c\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u8bed\u97f3\u7684\u6e05\u6670\u5ea6\u548c\u81ea\u7136\u5ea6\u3002\u6240\u63d0\u51fa\u7684\u6846\u67b6\u57fa\u4e8e\u77e2\u91cf\u91cf\u5316\u6269\u6563\uff0c\u7ed3\u5408\u4e86\u5148\u8fdb NAC\u3001\u6269\u6563\u6a21\u578b\u548c Mamba-2 \u7684\u4f18\u52bf\uff0c\u4ee5\u91cd\u5efa\u9ad8\u9891\u8bed\u97f3\u6210\u5206\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5bf9\u6570\u8c31\u8ddd\u79bb\u548c ViSQOL \u65b9\u9762\u5747\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u663e\u7740\u63d0\u9ad8\u4e86\u8bed\u97f3\u8d28\u91cf\u3002||\n", "2409.05730": "|**2024-09-09**|[AS-Speech: Adaptive Style For Speech Synthesis](http://arxiv.org/abs/2409.05730)|null|\u8fd1\u5e74\u6765\uff0c\u6587\u672c\u5230\u8bed\u97f3\uff08TTS\uff09\u5408\u6210\u6280\u672f\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u80fd\u591f\u5728\u5e38\u89c1\u573a\u666f\u4e0b\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u3002\u5728\u672a\u77e5\u60c5\u51b5\u4e0b\uff0c\u81ea\u9002\u5e94TTS\u9700\u8981\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u6765\u9002\u5e94\u8bf4\u8bdd\u4eba\u7684\u98ce\u683c\u7279\u5f81\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u53ea\u80fd\u5206\u522b\u63d0\u53d6\u548c\u6574\u5408\u7c97\u7c92\u5ea6\u7684\u97f3\u8272\u6216\u6df7\u5408\u7684\u97f5\u5f8b\u5c5e\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86AS-Speech\uff0c\u4e00\u79cd\u5c06\u8bf4\u8bdd\u4eba\u97f3\u8272\u7279\u5f81\u548c\u97f5\u5f8b\u5c5e\u6027\u6574\u5408\u5230\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u4e2d\u7684\u81ea\u9002\u5e94\u98ce\u683c\u65b9\u6cd5\uff0c\u7528\u4e8e\u6587\u672c\u5230\u8bed\u97f3\u5408\u6210\u3002\u5177\u4f53\u6765\u8bf4\uff0cAS-Speech\u53ef\u4ee5\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u57fa\u4e8e\u6587\u672c\u7684\u97f3\u8272\u7279\u5f81\u548c\u5168\u5c40\u97f5\u5f8b\u4fe1\u606f\u51c6\u786e\u5730\u6a21\u62df\u98ce\u683c\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9ad8\u4fdd\u771f\u8bed\u97f3\u5408\u6210\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4e00\u7cfb\u5217\u81ea\u9002\u5e94TTS\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u751f\u6210\u7684\u8bed\u97f3\u5728\u97f3\u8272\u548c\u97f5\u5f8b\u65b9\u9762\u5177\u6709\u66f4\u9ad8\u7684\u81ea\u7136\u5ea6\u548c\u76f8\u4f3c\u6027\u3002||\n", "2409.05701": "|**2024-09-09**|[pFedGPA: Diffusion-based Generative Parameter Aggregation for Personalized Federated Learning](http://arxiv.org/abs/2409.05701)|null|\u8054\u90a6\u5b66\u4e60 (FL) \u662f\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6570\u636e\u4fdd\u7559\u5728\u672c\u5730\uff0c\u53ea\u6709\u6a21\u578b\u53c2\u6570\u5728\u5ba2\u6237\u7aef\u548c\u4e2d\u5fc3\u670d\u52a1\u5668\u4e4b\u95f4\u5171\u4eab\u3002\u4f20\u7edf\u7684\u8054\u90a6\u5e73\u5747 (FedAvg) \u7b49\u65b9\u6cd5\u5bf9\u8fd9\u4e9b\u901a\u5e38\u5728\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e0a\u8bad\u7ec3\u7684\u53c2\u6570\u8fdb\u884c\u7ebf\u6027\u805a\u5408\uff0c\u8fd9\u53ef\u80fd\u5ffd\u7565\u4e86\u53c2\u6570\u7a7a\u95f4\u590d\u6742\u3001\u9ad8\u7ef4\u7684\u6027\u8d28\uff0c\u5bfc\u81f4\u805a\u5408\u6a21\u578b\u7684\u6027\u80fd\u4e0b\u964d\u3002\u867d\u7136\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u7f13\u89e3\u5f02\u6784\u6570\u636e\u95ee\u9898\uff0c\u4f46\u7ebf\u6027\u805a\u5408\u7684\u5c40\u9650\u6027\u4ecd\u7136\u6ca1\u6709\u89e3\u51b3\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u751f\u6210\u53c2\u6570\u805a\u5408\u6846\u67b6\uff0c\u5373 pFedGPA\u3002\u5728\u8fd9\u4e2a\u6846\u67b6\u4e2d\uff0c\u6211\u4eec\u5728\u670d\u52a1\u5668\u4e0a\u90e8\u7f72\u4e86\u4e00\u4e2a\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u6574\u5408\u4e0d\u540c\u7684\u53c2\u6570\u5206\u5e03\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u53cd\u6f14\u65b9\u6cd5\uff0c\u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u6709\u6548\u5730\u751f\u6210\u4e00\u7ec4\u4e2a\u6027\u5316\u53c2\u6570\u3002\u8fd9\u79cd\u53cd\u6f14\u65b9\u6cd5\u5c06\u4e0a\u4f20\u7684\u53c2\u6570\u8f6c\u6362\u4e3a\u4e00\u4e2a\u6f5c\u5728\u4ee3\u7801\uff0c\u7136\u540e\u901a\u8fc7\u53bb\u566a\u91c7\u6837\u8fdb\u884c\u805a\u5408\uff0c\u751f\u6210\u6700\u7ec8\u7684\u4e2a\u6027\u5316\u53c2\u6570\u3002\u901a\u8fc7\u4f7f\u7528\u9ad8\u5bb9\u91cf\u6269\u6563\u6a21\u578b\u5bf9\u5ba2\u6237\u7aef\u6a21\u578b\u53c2\u6570\u5bf9\u5176\u7279\u5b9a\u6570\u636e\u5206\u5e03\u7684\u4f9d\u8d56\u6027\u8fdb\u884c\u7f16\u7801\uff0cpFedGPA \u53ef\u4ee5\u6709\u6548\u5730\u5c06\u6240\u6709\u5ba2\u6237\u7aef\u6a21\u578b\u53c2\u6570\u7684\u603b\u4f53\u5206\u5e03\u7684\u590d\u6742\u6027\u4e0e\u6bcf\u4e2a\u5ba2\u6237\u7aef\u53c2\u6570\u5206\u5e03\u7684\u590d\u6742\u6027\u89e3\u8026\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\u5730\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u8d85\u8fc7\u4e86\u57fa\u7ebf\u65b9\u6cd5\u3002||\n", "2409.05668": "|**2024-09-09**|[Unlearning or Concealment? A Critical Analysis and Evaluation Metrics for Unlearning in Diffusion Models](http://arxiv.org/abs/2409.05668)|null|\u8fd1\u671f\u7684\u7814\u7a76\u5df2\u7ecf\u770b\u5230\u4eba\u4eec\u5bf9\u6269\u6563\u6a21\u578b\u4e2d\u6982\u5ff5\u53bb\u9664\u548c\u76ee\u6807\u9057\u5fd8\u65b9\u6cd5\u7684\u6d53\u539a\u5174\u8da3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5bf9\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u9057\u5fd8\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7684\u767d\u76d2\u5206\u6790\uff0c\u4ee5\u63ed\u793a\u5176\u5b58\u5728\u7684\u91cd\u5927\u6f0f\u6d1e\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u73b0\u6709\u65b9\u6cd5\u4e2d\u7528\u4e8e\u9057\u5fd8\u7684\u76ee\u6807\u51fd\u6570\u5bfc\u81f4\u4e86\u8981\u9057\u5fd8\u7684\u76ee\u6807\u6982\u5ff5\u4e0e\u76f8\u5e94\u63d0\u793a\u4e4b\u95f4\u7684\u89e3\u8026\u3002\u8fd9\u662f\u4e00\u79cd\u9690\u853d\u884c\u4e3a\uff0c\u800c\u4e0d\u662f\u771f\u6b63\u7684\u9057\u5fd8\uff0c\u800c\u771f\u6b63\u7684\u9057\u5fd8\u624d\u662f\u6700\u521d\u7684\u76ee\u6807\u3002\u5f53\u524d\u65b9\u6cd5\u7684\u65e0\u6548\u6027\u4e3b\u8981\u6e90\u4e8e\u5b83\u4eec\u53ea\u5173\u6ce8\u964d\u4f4e\u7279\u5b9a\u63d0\u793a\u96c6\u7684\u751f\u6210\u6982\u7387\uff0c\u800c\u5ffd\u7565\u4e86\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u7684\u4e2d\u95f4\u5f15\u5bfc\u7684\u591a\u79cd\u5f62\u5f0f\u3002\u672c\u6587\u5bf9\u56db\u79cd\u5e38\u7528\u7684\u6269\u6563\u6a21\u578b\u9057\u5fd8\u6280\u672f\u8fdb\u884c\u4e86\u4e25\u683c\u7684\u7406\u8bba\u548c\u5b9e\u8bc1\u68c0\u9a8c\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u4e2a\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff1a\u6982\u5ff5\u68c0\u7d22\u5206\u6570\uff08CRS\uff09\u548c\u6982\u5ff5\u7f6e\u4fe1\u5ea6\u5206\u6570\uff08CCS\uff09\u3002\u8fd9\u4e9b\u6307\u6807\u57fa\u4e8e\u4e00\u4e2a\u6210\u529f\u7684\u5bf9\u6297\u653b\u51fb\u8bbe\u7f6e\uff0c\u53ef\u4ee5\u4ece\u9057\u5fd8\u7684\u6269\u6563\u6a21\u578b\u4e2d\u6062\u590d\u88ab\u9057\u5fd8\u7684\u6982\u5ff5\u3002CRS \u8861\u91cf\u7684\u662f\u9057\u5fd8\u540e\u7684\u9057\u5fd8\u6a21\u578b\u548c\u5b8c\u5168\u8bad\u7ec3\u6a21\u578b\u7684\u6f5c\u5728\u8868\u793a\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u5b83\u53cd\u6620\u4e86\u968f\u7740\u5f15\u5bfc\u91cf\u589e\u52a0\uff0c\u88ab\u9057\u5fd8\u6982\u5ff5\u7684\u68c0\u7d22\u7a0b\u5ea6\u3002CCS \u91cf\u5316\u4e86\u6a21\u578b\u5c06\u76ee\u6807\u6982\u5ff5\u5206\u914d\u7ed9\u88ab\u64cd\u7eb5\u6570\u636e\u7684\u7f6e\u4fe1\u5ea6\u3002\u5b83\u53cd\u6620\u4e86\u968f\u7740\u5f15\u5bfc\u91cf\u589e\u52a0\uff0c\u672a\u9057\u5fd8\u6a21\u578b\u7684\u751f\u6210\u7ed3\u679c\u4e0e\u539f\u59cb\u9886\u57df\u77e5\u8bc6\u4e00\u81f4\u7684\u6982\u7387\u3002\u6211\u4eec\u4f7f\u7528\u63d0\u51fa\u7684\u9488\u5bf9\u6269\u6563\u6a21\u578b\u7684\u4e25\u683c\u6307\u6807\u5bf9\u73b0\u6709\u7684\u9057\u5fd8\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u771f\u6b63\u9057\u5fd8\u6982\u5ff5\u65b9\u9762\u7684\u91cd\u5927\u7f3a\u9677\u3002\u6e90\u4ee3\u7801\uff1ahttps://respailab.github.io/unlearning-or-concealment||\n", "2409.05622": "|**2024-09-09**|[Forward KL Regularized Preference Optimization for Aligning Diffusion Policies](http://arxiv.org/abs/2409.05622)|null|\u6269\u6563\u6a21\u578b\u901a\u8fc7\u5728\u7b56\u7565\u5b66\u4e60\u4e2d\u5229\u7528\u9ad8\u5ea6\u8868\u8fbe\u7684\u6a21\u578b\u80fd\u529b\uff0c\u5728\u5e8f\u5217\u51b3\u7b56\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\u3002\u5b66\u4e60\u6269\u6563\u7b56\u7565\u7684\u4e00\u4e2a\u6838\u5fc3\u95ee\u9898\u662f\u5982\u4f55\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u4f7f\u7b56\u7565\u8f93\u51fa\u4e0e\u4eba\u7c7b\u610f\u56fe\u4fdd\u6301\u4e00\u81f4\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u5148\u524d\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u56de\u62a5\u6761\u4ef6\u7b56\u7565\u751f\u6210\u6216\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u7b56\u7565\u4f18\u5316\uff0c\u4f46\u5b83\u4eec\u90fd\u4f9d\u8d56\u4e8e\u9884\u5148\u5b9a\u4e49\u7684\u5956\u52b1\u51fd\u6570\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u5373\u7528\u4e8e\u5bf9\u9f50\u6269\u6563\u7b56\u7565\u7684\u524d\u5411 KL \u6b63\u5219\u5316\u504f\u597d\u4f18\u5316\uff0c\u4ee5\u76f4\u63a5\u5c06\u6269\u6563\u7b56\u7565\u4e0e\u504f\u597d\u5bf9\u9f50\u3002\u6211\u4eec\u9996\u5148\u4ece\u79bb\u7ebf\u6570\u636e\u96c6\u4e2d\u8bad\u7ec3\u4e00\u4e2a\u4e0d\u8003\u8651\u504f\u597d\u7684\u6269\u6563\u7b56\u7565\uff0c\u7136\u540e\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5c06\u8be5\u7b56\u7565\u4e0e\u504f\u597d\u6570\u636e\u5bf9\u9f50\u3002\u5728\u5bf9\u9f50\u9636\u6bb5\uff0c\u6211\u4eec\u5728\u6269\u6563\u7b56\u7565\u4e2d\u5236\u5b9a\u4e86\u76f4\u63a5\u504f\u597d\u5b66\u4e60\uff0c\u5176\u4e2d\u5728\u524d\u5411\u504f\u597d\u4f18\u5316\u4e2d\u91c7\u7528\u4e86 KL \u6b63\u5219\u5316\uff0c\u4ee5\u907f\u514d\u751f\u6210\u5206\u5e03\u5916\u52a8\u4f5c\u3002\u6211\u4eec\u5bf9 MetaWorld \u64cd\u4f5c\u548c D4RL \u4efb\u52a1\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u504f\u597d\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\u3002||\n", "2409.05585": "|**2024-09-09**|[Latent 3D Brain MRI Counterfactual](http://arxiv.org/abs/2409.05585)|null|\u7ed3\u6784\u6027\u8111\u90e8MRI\u7814\u7a76\u4e2d\u7684\u6837\u672c\u6570\u91cf\u901a\u5e38\u8fc7\u5c0f\uff0c\u65e0\u6cd5\u5145\u5206\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u751f\u6210\u6a21\u578b\u901a\u8fc7\u6709\u6548\u5b66\u4e60\u6570\u636e\u5206\u5e03\u548c\u751f\u6210\u9ad8\u4fdd\u771fMRI\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5e26\u6765\u4e86\u5e0c\u671b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u96be\u4ee5\u751f\u6210\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u4e4b\u5916\u7684\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u3002\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u4e00\u79cd\u65b9\u6cd5\u662f\u4f7f\u7528\u9488\u5bf93D\u4f53\u79ef\u53cd\u4e8b\u5b9e\u5f00\u53d1\u7684\u56e0\u679c\u6a21\u578b\u3002\u7136\u800c\uff0c\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u51c6\u786e\u5efa\u6a21\u56e0\u679c\u5173\u7cfb\u662f\u4e00\u9879\u6311\u6218\uff0c\u56e0\u6b64\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u751f\u6210\u8d28\u91cf\u8f83\u4f4e\u76843D\u8111\u90e8MRI\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u5185\u6784\u5efa\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\u3002\u5728\u7b2c\u4e00\u9636\u6bb5\uff0c\u6211\u4eec\u91c7\u7528VQ-VAE\u5b66\u4e60MRI\u4f53\u79ef\u7684\u7d27\u51d1\u5d4c\u5165\u3002\u968f\u540e\uff0c\u6211\u4eec\u5c06\u56e0\u679c\u6a21\u578b\u6574\u5408\u5230\u8fd9\u4e2a\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u5e76\u4f7f\u7528\u5c01\u95ed\u5f62\u5f0f\u7684\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\uff08GLM\uff09\u6267\u884c\u4e09\u6b65\u53cd\u4e8b\u5b9e\u7a0b\u5e8f\u3002\u6211\u4eec\u5bf9\u771f\u5b9e\u4e16\u754c\u7684\u9ad8\u5206\u8fa8\u7387MRI\u6570\u636e\uff081mm\uff09\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u76843D MRI\u53cd\u4e8b\u5b9e\u3002||\n", "2409.05583": "|**2024-09-09**|[Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation](http://arxiv.org/abs/2409.05583)|**[link](https://github.com/gmuraleekrishna/sas)**|\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u65e8\u5728\u5f00\u53d1\u80fd\u591f\u7406\u89e3\u548c\u6267\u884c\u4eba\u7c7b\u8bed\u8a00\u6307\u4ee4\u5e76\u4ee5\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u4ea4\u6d41\u7684\u673a\u5668\u4eba\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u751f\u6210\u9ad8\u5ea6\u8be6\u7ec6\u7684\u5bfc\u822a\u6307\u4ee4\u4ee5\u4f9b\u5177\u8eab\u673a\u5668\u4eba\u9075\u5faa\u7684\u4efb\u52a1\u3002\u5c3d\u7ba1\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u4ece\u56fe\u50cf\u5e8f\u5217\u751f\u6210\u9010\u6b65\u6307\u4ee4\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u751f\u6210\u7684\u6307\u4ee4\u5728\u6307\u79f0\u7269\u4f53\u548c\u5730\u6807\u65b9\u9762\u7f3a\u4e4f\u591a\u6837\u6027\u3002\u73b0\u6709\u7684\u8bf4\u8bdd\u8005\u6a21\u578b\u5b66\u4e60\u4e86\u4e00\u4e9b\u7b56\u7565\u6765\u89c4\u907f\u8bc4\u4f30\u6307\u6807\uff0c\u5373\u4f7f\u5bf9\u4e8e\u4f4e\u8d28\u91cf\u7684\u53e5\u5b50\u4e5f\u80fd\u83b7\u5f97\u66f4\u9ad8\u7684\u5206\u6570\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SAS\uff08\u7a7a\u95f4\u611f\u77e5\u8bf4\u8bdd\u8005\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u6307\u4ee4\u751f\u6210\u5668\u6216\u201c\u8bf4\u8bdd\u8005\u201d\u6a21\u578b\uff0c\u5b83\u5229\u7528\u73af\u5883\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u77e5\u8bc6\u6765\u751f\u6210\u66f4\u4e30\u5bcc\u7684\u6307\u4ee4\u3002\u4e3a\u4e86\u8fdb\u884c\u8bad\u7ec3\uff0c\u6211\u4eec\u5728\u5bf9\u6297\u6027\u8bbe\u7f6e\u4e2d\u91c7\u7528\u4e86\u5956\u52b1\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u907f\u514d\u8bed\u8a00\u8bc4\u4f30\u6307\u6807\u5f15\u5165\u7684\u7cfb\u7edf\u6027\u504f\u5dee\u3002\u6839\u636e\u7ecf\u9a8c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u6307\u4ee4\u751f\u6210\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u6807\u51c6\u6307\u6807\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u5f97\uff1ahttps://github.com/gmuraleekrishna/SAS\u3002||\n", "2409.05490": "|**2024-09-09**|[A Taxonomy of Miscompressions: Preparing Image Forensics for Neural Compression](http://arxiv.org/abs/2409.05490)|null|\u795e\u7ecf\u538b\u7f29\u6709\u53ef\u80fd\u5f7b\u5e95\u6539\u53d8\u6709\u635f\u56fe\u50cf\u538b\u7f29\u6280\u672f\u3002\u57fa\u4e8e\u751f\u6210\u6a21\u578b\uff0c\u6700\u8fd1\u7684\u65b9\u6848\u5728\u9ad8\u611f\u77e5\u8d28\u91cf\u4e0b\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u538b\u7f29\u7387\uff0c\u4f46\u727a\u7272\u4e86\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002\u89e3\u538b\u7f29\u56fe\u50cf\u7684\u7ec6\u8282\u53ef\u80fd\u770b\u8d77\u6765\u5728\u89c6\u89c9\u4e0a\u662f\u5b8c\u7f8e\u7684\uff0c\u4f46\u5728\u8bed\u4e49\u4e0a\u4e0e\u539f\u59cb\u56fe\u50cf\u4e0d\u540c\uff0c\u8fd9\u4f7f\u5f97\u538b\u7f29\u9519\u8bef\u96be\u4ee5\u6216\u4e0d\u53ef\u80fd\u88ab\u68c0\u6d4b\u5230\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u8fd9\u4e2a\u95ee\u9898\u7684\u7a7a\u95f4\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6682\u5b9a\u7684\u9519\u8bef\u538b\u7f29\u5206\u7c7b\u6cd5\u3002\u5b83\u5b9a\u4e49\u4e86\u4e09\u79cd\u7c7b\u578b\u7684\u201c\u53d1\u751f\u4e86\u4ec0\u4e48\u201d\uff0c\u5e76\u6709\u4e00\u4e2a\u4e8c\u8fdb\u5236\u7684\u201c\u9ad8\u5f71\u54cd\u201d\u6807\u5fd7\uff0c\u8868\u793a\u6539\u53d8\u7b26\u53f7\u7684\u9519\u8bef\u538b\u7f29\u3002\u6211\u4eec\u8ba8\u8bba\u4e86\u8be5\u5206\u7c7b\u6cd5\u5982\u4f55\u4fc3\u8fdb\u98ce\u9669\u6c9f\u901a\u548c\u7f13\u89e3\u63aa\u65bd\u7684\u7814\u7a76\u3002||\n", "2409.06633": "|**2024-09-10**|[SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse Low-Rank Adaptation](http://arxiv.org/abs/2409.06633)|null|\u8fd1\u5e74\u6765\uff0c\u6269\u6563\u6a21\u578b\u7684\u53d1\u5c55\u63a8\u52a8\u4e86\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4efb\u52a1\u7684\u663e\u8457\u8fdb\u6b65\uff0c\u5176\u4e2d\u50cfStable Diffusion\u7cfb\u5217\u8fd9\u6837\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u53d1\u6325\u4e86\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u53d7\u6a21\u578b\u526a\u679d\u6280\u672f\u7684\u542f\u53d1\uff0c\u8be5\u6280\u672f\u901a\u8fc7\u79fb\u9664\u4e0d\u91cd\u8981\u7684\u53c2\u6570\u6765\u51cf\u8f7b\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8d1f\u62c5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5145\u5206\u5229\u7528\u8fd9\u4e9b\u65e0\u6548\u53c2\u6570\uff0c\u5e76\u4f7f\u9884\u8bad\u7ec3\u6a21\u578b\u5177\u5907\u65b0\u7684\u4efb\u52a1\u7279\u5b9a\u80fd\u529b\u3002\u672c\u7814\u7a76\u9996\u5148\u8c03\u67e5\u4e86\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e2d\u53c2\u6570\u7684\u91cd\u8981\u6027\uff0c\u53d1\u73b0\u6309\u7edd\u5bf9\u503c\u8ba1\u7b97\uff0c\u6700\u5c0f\u768410%\u523020%\u7684\u53c2\u6570\u5bf9\u751f\u6210\u8fc7\u7a0b\u6ca1\u6709\u8d21\u732e\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c2\u5bdf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSaRA\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u91cd\u65b0\u5229\u7528\u8fd9\u4e9b\u6682\u65f6\u65e0\u6548\u7684\u53c2\u6570\uff0c\u76f8\u5f53\u4e8e\u4f18\u5316\u4e00\u4e2a\u7a00\u758f\u6743\u91cd\u77e9\u9635\u6765\u5b66\u4e60\u7279\u5b9a\u4efb\u52a1\u7684\u77e5\u8bc6\u3002\u4e3a\u4e86\u51cf\u8f7b\u8fc7\u62df\u5408\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6838\u8303\u6570\u7684\u4f4e\u79e9\u7a00\u758f\u8bad\u7ec3\u65b9\u6848\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u5fae\u8c03\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u6e10\u8fdb\u5f0f\u53c2\u6570\u8c03\u6574\u7b56\u7565\uff0c\u4ee5\u5145\u5206\u5229\u7528\u91cd\u65b0\u8bad\u7ec3/\u5fae\u8c03\u7684\u53c2\u6570\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u975e\u7ed3\u6784\u5316\u53cd\u5411\u4f20\u64ad\u7b56\u7565\uff0c\u53ef\u663e\u8457\u964d\u4f4e\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u5185\u5b58\u6210\u672c\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u589e\u5f3a\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e0b\u6e38\u5e94\u7528\u4e2d\u7684\u751f\u6210\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u4fdd\u6301\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4f18\u4e8eLoRA\u7b49\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u3002\u6211\u4eec\u901a\u8fc7\u5728SD\u6a21\u578b\u4e0a\u7684\u5fae\u8c03\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660eSaRA\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002SaRA\u8fd8\u5177\u6709\u4e00\u4e2a\u5b9e\u9645\u4f18\u52bf\uff0c\u5373\u53ea\u9700\u4fee\u6539\u4e00\u884c\u4ee3\u7801\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65bd\uff0c\u5e76\u4e14\u4e0e\u73b0\u6709\u65b9\u6cd5\u65e0\u7f1d\u517c\u5bb9\u3002||\n", "2409.06620": "|**2024-09-10**|[MVGaussian: High-Fidelity text-to-3D Content Generation with Multi-View Guidance and Surface Densification](http://arxiv.org/abs/2409.06620)|null|\u6587\u672c\u52303D\u5185\u5bb9\u751f\u6210\u9886\u57df\u5728\u751f\u6210\u903c\u771f\u76843D\u5bf9\u8c61\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u50cf\u5206\u6570\u84b8\u998f\u91c7\u6837\uff08SDS\uff09\u8fd9\u6837\u7684\u73b0\u6709\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u6307\u5bfc\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6307\u5bfc\u4e0d\u7cbe\u786e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u7ecf\u5e38\u9047\u5230\u201c\u4e24\u9762\u795e\u201d\u95ee\u9898\u2014\u2014\u591a\u9762\u6b67\u4e49\u3002\u6b64\u5916\uff0c\u867d\u7136\u6700\u8fd13D\u9ad8\u65af\u5206\u88c2\u7684\u8fdb\u6b65\u5df2\u7ecf\u663e\u793a\u51fa\u5176\u5728\u8868\u793a3D\u4f53\u79ef\u65b9\u9762\u7684\u529f\u6548\uff0c\u4f46\u8fd9\u79cd\u8868\u793a\u7684\u4f18\u5316\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u5f97\u5230\u63a2\u7d22\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8e\u6587\u672c\u52303D\u5185\u5bb9\u751f\u6210\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u5173\u952e\u5dee\u8ddd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u591a\u89c6\u56fe\u6307\u5bfc\u8fed\u4ee3\u5f62\u62103D\u6a21\u578b\u7684\u7ed3\u6784\uff0c\u9010\u6b65\u589e\u5f3a\u7ec6\u8282\u548c\u51c6\u786e\u6027\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5bc6\u96c6\u5316\u7b97\u6cd5\uff0c\u4f7f\u9ad8\u65af\u63a5\u8fd1\u8868\u9762\uff0c\u4f18\u5316\u751f\u6210\u6a21\u578b\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u4fdd\u771f\u5ea6\u3002\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u8868\u660e\u5b83\u80fd\u591f\u4ee5\u6700\u5c11\u7684\u65f6\u95f4\u6210\u672c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u89c6\u89c9\u8f93\u51fa\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u534a\u5c0f\u65f6\u7684\u8bad\u7ec3\u65f6\u95f4\u5185\u5c31\u80fd\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\uff0c\u4e0e\u5927\u591a\u6570\u9700\u8981\u6570\u5c0f\u65f6\u8bad\u7ec3\u65f6\u95f4\u624d\u80fd\u83b7\u5f97\u7c7b\u4f3c\u7ed3\u679c\u7684\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6548\u7387\u663e\u8457\u63d0\u9ad8\u3002||\n", "2409.06560": "|**2024-09-10**|[A Primer on Variational Inference for Physics-Informed Deep Generative Modelling](http://arxiv.org/abs/2409.06560)|null|\u53d8\u5206\u63a8\u65ad\uff08VI\uff09\u662f\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u8fd1\u4f3c\u8d1d\u53f6\u65af\u63a8\u65ad\u65b9\u6cd5\u3002\u5b83\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u51c6\u786e\u6027\u548c\u5b9e\u9645\u53ef\u5904\u7406\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002\u7531\u4e8e\u5176\u5185\u7f6e\u7684\u8d1d\u53f6\u65af\u6b63\u5219\u5316\u548c\u7075\u6d3b\u6027\uff0c\u5b83\u5728\u751f\u6210\u5efa\u6a21\u548c\u53cd\u6f14\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fd9\u5bf9\u4e8e\u7269\u7406\u76f8\u5173\u95ee\u9898\u81f3\u5173\u91cd\u8981\u3002\u63a8\u5bfc VI \u7684\u6838\u5fc3\u5b66\u4e60\u76ee\u6807\u901a\u5e38\u5fc5\u987b\u9488\u5bf9\u65b0\u7684\u5b66\u4e60\u4efb\u52a1\u8fdb\u884c\u8c03\u6574\uff0c\u5176\u4e2d\u95ee\u9898\u7684\u6027\u8d28\u51b3\u5b9a\u4e86\u611f\u5174\u8da3\u53d8\u91cf\u4e4b\u95f4\u7684\u6761\u4ef6\u4f9d\u8d56\u6027\uff0c\u4f8b\u5982\u7269\u7406\u95ee\u9898\u4e2d\u51fa\u73b0\u7684\u60c5\u51b5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4e3a\u6b63\u5411\u548c\u53cd\u5411\u95ee\u9898\u63d0\u4f9b\u4e86 VI \u7684\u6613\u4e8e\u7406\u89e3\u4e14\u5168\u9762\u7684\u6280\u672f\u4ecb\u7ecd\uff0c\u5f15\u5bfc\u8bfb\u8005\u4e86\u89e3 VI \u6846\u67b6\u7684\u6807\u51c6\u63a8\u5bfc\u53ca\u5176\u5982\u4f55\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5f97\u5230\u6700\u4f73\u5b9e\u73b0\u3002\u7136\u540e\uff0c\u6211\u4eec\u56de\u987e\u5e76\u7edf\u4e00\u4e86\u6700\u8fd1\u7684\u6587\u732e\uff0c\u8fd9\u4e9b\u6587\u732e\u4f8b\u8bc1\u4e86 VI \u6240\u5141\u8bb8\u7684\u521b\u9020\u6027\u7075\u6d3b\u6027\u3002\u672c\u6587\u9762\u5411\u5e0c\u671b\u89e3\u51b3\u57fa\u4e8e\u7269\u7406\u7684\u95ee\u9898\u5e76\u5f3a\u8c03\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u4e00\u822c\u79d1\u5b66\u53d7\u4f17\u3002||\n", "2409.06550": "|**2024-09-10**|[From LIMA to DeepLIMA: following a new path of interoperability](http://arxiv.org/abs/2409.06550)|null|\u672c\u6587\u63cf\u8ff0\u4e86 LIMA\uff08Libre Multilingual Analyzer\uff09\u6846\u67b6\u7684\u4f53\u7cfb\u7ed3\u6784\u53ca\u5176\u6700\u65b0\u53d1\u5c55\uff0c\u5176\u4e2d\u65b0\u589e\u4e86\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6587\u672c\u5206\u6790\u6a21\u5757\u3002\u6211\u4eec\u5728\u4fdd\u7559\u73b0\u6709\u53ef\u914d\u7f6e\u67b6\u6784\u4ee5\u53ca\u5148\u524d\u5f00\u53d1\u7684\u57fa\u4e8e\u89c4\u5219\u548c\u7edf\u8ba1\u7684\u5206\u6790\u7ec4\u4ef6\u7684\u53ef\u7528\u6027\u7684\u540c\u65f6\uff0c\u6269\u5c55\u4e86 LIMA \u5728\u652f\u6301\u8bed\u8a00\u6570\u91cf\u65b9\u9762\u7684\u529f\u80fd\u3002\u6211\u4eec\u5728 Universal Dependencies 2.5 \u8bed\u6599\u5e93\u3001WikiNer \u8bed\u6599\u5e93\u548c CoNLL-03 \u6570\u636e\u96c6\u4e0a\u9488\u5bf9 60 \u591a\u79cd\u8bed\u8a00\u8bad\u7ec3\u4e86\u6a21\u578b\u3002Universal Dependencies \u5141\u8bb8\u6211\u4eec\u589e\u52a0\u652f\u6301\u7684\u8bed\u8a00\u6570\u91cf\uff0c\u5e76\u751f\u6210\u53ef\u4ee5\u96c6\u6210\u5230\u5176\u4ed6\u5e73\u53f0\u7684\u6a21\u578b\u3002\u8fd9\u79cd\u666e\u904d\u5b58\u5728\u7684\u6df1\u5ea6\u5b66\u4e60\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6a21\u578b\u7684\u96c6\u6210\u4ee5\u53ca\u4f7f\u7528 Universal Dependencies \u7684\u6807\u51c6\u6ce8\u91ca\u96c6\u5408\u7684\u4f7f\u7528\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u65b0\u7684\u4e92\u64cd\u4f5c\u6027\u9014\u5f84\uff0c\u901a\u8fc7\u6a21\u578b\u548c\u6570\u636e\u7684\u89c4\u8303\u5316\uff0c\u4e0e\u66f4\u6807\u51c6\u7684\u6280\u672f\u4e92\u64cd\u4f5c\u6027\u76f8\u8f85\u76f8\u6210\uff0c\u5728 LIMA \u4e2d\u901a\u8fc7 Docker Hub \u4e0a Docker \u5bb9\u5668\u4e2d\u53ef\u7528\u7684\u670d\u52a1\u5b9e\u73b0\u3002||\n", "2409.06451": "|**2024-09-10**|[Enhancing Emotional Text-to-Speech Controllability with Natural Language Guidance through Contrastive Learning and Diffusion Models](http://arxiv.org/abs/2409.06451)|null|\u867d\u7136\u5f53\u524d\u7684\u60c5\u611f\u6587\u672c\u5230\u8bed\u97f3\uff08TTS\uff09\u7cfb\u7edf\u53ef\u4ee5\u751f\u6210\u9ad8\u5ea6\u667a\u80fd\u7684\u60c5\u611f\u8bed\u97f3\uff0c\u4f46\u5728\u8f93\u51fa\u8bed\u97f3\u7684\u60c5\u611f\u6e32\u67d3\u65b9\u9762\u5b9e\u73b0\u7cbe\u7ec6\u63a7\u5236\u4ecd\u7136\u662f\u4e00\u9879\u91cd\u5927\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 ParaEVITS\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u60c5\u611f TTS \u6846\u67b6\uff0c\u5b83\u5229\u7528\u81ea\u7136\u8bed\u8a00\u7684\u7ec4\u5408\u6027\u6765\u589e\u5f3a\u5bf9\u60c5\u611f\u6e32\u67d3\u7684\u63a7\u5236\u3002\u901a\u8fc7\u7ed3\u5408\u53d7 ParaCLAP\uff08\u4e00\u79cd\u7528\u4e8e\u8ba1\u7b97\u8bed\u7528\u5b66\u7684\u5bf9\u6bd4\u6027\u8bed\u8a00-\u97f3\u9891\u9884\u8bad\u7ec3\uff08CLAP\uff09\u6a21\u578b\uff09\u542f\u53d1\u7684\u6587\u672c-\u97f3\u9891\u7f16\u7801\u5668\uff0c\u6211\u4eec\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4ee5\u6839\u636e\u6587\u672c\u60c5\u611f\u98ce\u683c\u63cf\u8ff0\u751f\u6210\u60c5\u611f\u5d4c\u5165\u3002\u6211\u4eec\u7684\u6846\u67b6\u9996\u5148\u4f7f\u7528\u97f3\u9891\u7f16\u7801\u5668\u5728\u53c2\u8003\u97f3\u9891\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u7136\u540e\u5fae\u8c03\u6269\u6563\u6a21\u578b\u4ee5\u5904\u7406\u6765\u81ea ParaCLAP \u6587\u672c\u7f16\u7801\u5668\u7684\u6587\u672c\u8f93\u5165\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u4ec5\u4f7f\u7528\u6587\u672c\u6761\u4ef6\u5c31\u53ef\u4ee5\u64cd\u7eb5\u97f3\u8c03\u3001\u6296\u52a8\u548c\u54cd\u5ea6\u7b49\u8bed\u97f3\u5c5e\u6027\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cParaEVITS \u53ef\u4ee5\u6709\u6548\u5730\u63a7\u5236\u60c5\u611f\u6e32\u67d3\uff0c\u800c\u4e0d\u4f1a\u5f71\u54cd\u8bed\u97f3\u8d28\u91cf\u3002\u8bed\u97f3\u6f14\u793a\u516c\u5f00\u53ef\u7528\u3002||\n", "2409.06442": "|**2024-09-10**|[Prompt2Fashion: An automatically generated fashion dataset](http://arxiv.org/abs/2409.06442)|**[link](https://github.com/georgiarg/prompt2fashion)**|\u5c3d\u7ba1\u8bed\u8a00\u548c\u89c6\u89c9\u751f\u6210\u6a21\u578b\u5728\u5feb\u901f\u53d1\u5c55\u4e14\u6548\u7387\u4e0d\u65ad\u63d0\u9ad8\uff0c\u4f46\u4ecd\u7136\u7f3a\u4e4f\u5c06\u4e2a\u6027\u5316\u65f6\u5c1a\u9700\u6c42\u4e0e\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u8bbe\u8ba1\u8054\u7cfb\u8d77\u6765\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u8fd9\u9650\u5236\u4e86\u771f\u6b63\u5305\u5bb9\u548c\u5b9a\u5236\u5316\u65f6\u5c1a\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5229\u7528\u751f\u6210\u6a21\u578b\u81ea\u52a8\u6784\u5efa\u4e86\u4e00\u4e2a\u65f6\u5c1a\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6839\u636e\u7528\u6237\u7684\u6307\u793a\u9488\u5bf9\u4e0d\u540c\u7684\u573a\u5408\u3001\u98ce\u683c\u548c\u4f53\u578b\u91cf\u8eab\u5b9a\u5236\u3002\u6211\u4eec\u4f7f\u7528\u4e0d\u540c\u7684\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u6a21\u578b\uff08LLM\uff09\u548c\u63d0\u793a\u7b56\u7565\uff0c\u4e3a\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u7528\u6237\u63d0\u4f9b\u5177\u6709\u9ad8\u8d28\u91cf\u5ba1\u7f8e\u3001\u7ec6\u8282\u548c\u76f8\u5173\u6027\u7684\u4e2a\u6027\u5316\u670d\u88c5\uff0c\u5e76\u901a\u8fc7\u5b9a\u6027\u5206\u6790\u8bc1\u660e\u4e86\u8fd9\u4e00\u70b9\u3002\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u751f\u6210\u7684\u670d\u88c5\u7684\u8bc4\u4f30\u4e00\u76f4\u7531\u975e\u4e13\u5bb6\u7684\u4eba\u7c7b\u53d7\u8bd5\u8005\u8fdb\u884c\u3002\u5c3d\u7ba1\u5bf9\u751f\u6210\u7684\u8d28\u91cf\u548c\u76f8\u5173\u6027\u63d0\u4f9b\u4e86\u7ec6\u81f4\u5165\u5fae\u7684\u89c1\u89e3\uff0c\u4f46\u6211\u4eec\u5c31\u4e13\u5bb6\u77e5\u8bc6\u5bf9\u4e8e\u8bc4\u4f30\u6b64\u7c7b\u827a\u672f\u6027\u4eba\u5de5\u667a\u80fd\u751f\u6210\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027\u5c55\u5f00\u4e86\u8fdb\u4e00\u6b65\u7684\u8ba8\u8bba\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u53ef\u5728 GitHub \u4e0a\u516c\u5f00\u83b7\u53d6\uff0c\u7f51\u5740\u4e3a https://github.com/georgiarg/Prompt2Fashion\u3002||\n", "2409.06417": "|**2024-09-10**|[Fast nonparametric inference of network backbones for graph sparsification](http://arxiv.org/abs/2409.06417)|null|\u7f51\u7edc\u9aa8\u5e72\u901a\u8fc7\u4ec5\u4fdd\u7559\u6700\u91cd\u8981\u7684\u94fe\u63a5\u6765\u63d0\u4f9b\u52a0\u6743\u7f51\u7edc\u7684\u6709\u7528\u7a00\u758f\u8868\u793a\uff0c\u4ece\u800c\u5b9e\u73b0\u4e00\u7cfb\u5217\u8ba1\u7b97\u52a0\u901f\u5e76\u7b80\u5316\u590d\u6742\u7684\u7f51\u7edc\u53ef\u89c6\u5316\u3002\u5224\u65ad\u94fe\u63a5\u662f\u5426\u91cd\u8981\u7684\u6807\u51c6\u6709\u5f88\u591a\uff0c\u56e0\u6b64\u5df2\u7ecf\u5f00\u53d1\u4e86\u8bb8\u591a\u7528\u4e8e\u56fe\u7a00\u758f\u5316\u7f51\u7edc\u9aa8\u5e72\u63d0\u53d6\u7684\u65b9\u6cd5\u3002\u8fd9\u4e9b\u65b9\u6cd5\u6839\u636e\u5b83\u4eec\u662f\u5728\u6574\u4e2a\u7f51\u7edc\u8fd8\u662f\u5728\u5355\u4e2a\u8282\u70b9\u90bb\u57df\u7684\u4e0a\u4e0b\u6587\u4e2d\u8bc4\u4f30\u8fb9\u7684\u91cd\u8981\u6027\uff0c\u53ef\u4ee5\u5206\u4e3a\u5168\u5c40\u6216\u5c40\u90e8\u65b9\u6cd5\u3002\u73b0\u6709\u7f51\u7edc\u9aa8\u5e72\u63d0\u53d6\u65b9\u6cd5\u7684\u4e00\u4e2a\u5173\u952e\u9650\u5236\u662f\uff0c\u5b83\u4eec\u8981\u4e48\u4eba\u4e3a\u5730\u5c06\u9aa8\u5e72\u7684\u62d3\u6251\u7ed3\u6784\u9650\u5236\u4e3a\u7279\u5b9a\u5f62\u5f0f\uff08\u4f8b\u5982\u6811\uff09\uff0c\u8981\u4e48\u9700\u8981\u6307\u5b9a\u4e00\u4e2a\u81ea\u7531\u53c2\u6570\uff08\u4f8b\u5982\u663e\u8457\u6027\u6c34\u5e73\uff09\u6765\u786e\u5b9a\u9aa8\u5e72\u4e2d\u8981\u4fdd\u7559\u7684\u8fb9\u6570\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b8c\u5168\u975e\u53c2\u6570\u7684\u6846\u67b6\u6765\u63a8\u65ad\u52a0\u6743\u7f51\u7edc\u7684\u9aa8\u5e72\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4f7f\u7528\u4fe1\u606f\u8bba\u4e2d\u7684\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\uff08MDL\uff09\u539f\u5219\u81ea\u52a8\u9009\u62e9\u4fdd\u7559\u5728\u9aa8\u5e72\u4e2d\u7684\u6700\u4f73\u8fb9\u6570\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e24\u79cd\u7f16\u7801\u65b9\u6848\uff0c\u4f5c\u4e3a\u5168\u5c40\u548c\u5c40\u90e8\u7f51\u7edc\u9aa8\u5e72\u7684\u76ee\u6807\u51fd\u6570\uff0c\u4ee5\u53ca\u6709\u6548\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u4ee5\u6839\u636e\u8fd9\u4e9b\u76ee\u6807\u8bc6\u522b\u6700\u4f73\u9aa8\u5e72\uff0c\u5176\u8fd0\u884c\u65f6\u590d\u6742\u5ea6\u5728\u8fb9\u6570\u4e0a\u662f\u5bf9\u6570\u7ebf\u6027\u7684\u3002\u6211\u4eec\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u4ee5\u4f7f\u7528\u6700\u5927\u540e\u9a8c\uff08MAP\uff09\u4f30\u8ba1\u7a0b\u5e8f\u548c\u6e10\u8fd1\u7b49\u6548\u7684\u8d1d\u53f6\u65af\u9aa8\u5e72\u751f\u6210\u6a21\u578b\u63a8\u5e7f\u5230\u8fb9\u4e0a\u7684\u4efb\u4f55\u79bb\u6563\u6743\u91cd\u5206\u5e03\u3002\u6211\u4eec\u5728\u771f\u5b9e\u548c\u5408\u6210\u7f51\u7edc\u4e0a\u7684\u4e00\u7cfb\u5217\u4efb\u52a1\u4e2d\u5c06\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0e\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002||\n", "2409.06371": "|**2024-09-10**|[Distilling Generative-Discriminative Representations for Very Low-Resolution Face Recognition](http://arxiv.org/abs/2409.06371)|null|\u7531\u4e8e\u5206\u8fa8\u7387\u4e0b\u964d\u4f1a\u5bfc\u81f4\u4fe1\u606f\u4e30\u5bcc\u7684\u9762\u90e8\u7ec6\u8282\u4e25\u91cd\u4e22\u5931\uff0c\u56e0\u6b64\u6781\u4f4e\u5206\u8fa8\u7387\u4eba\u8138\u8bc6\u522b\u6781\u5177\u6311\u6218\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86\u751f\u6210\u8868\u793a\u548c\u8de8\u5206\u8fa8\u7387\u5bf9\u9f50\u77e5\u8bc6\u84b8\u998f\u7684\u751f\u6210-\u5224\u522b\u8868\u793a\u84b8\u998f\u65b9\u6cd5\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u4e24\u4e2a\u84b8\u998f\u6a21\u5757\u8054\u5408\u84b8\u998f\u751f\u6210\u6a21\u578b\u548c\u5224\u522b\u6a21\u578b\uff0c\u4fc3\u8fdb\u4e86\u6781\u4f4e\u5206\u8fa8\u7387\u4eba\u8138\u8bc6\u522b\u3002\u9996\u5148\uff0c\u751f\u6210\u8868\u793a\u84b8\u998f\u5c06\u9884\u5148\u8bad\u7ec3\u7528\u4e8e\u4eba\u8138\u8d85\u5206\u8fa8\u7387\u7684\u6269\u6563\u6a21\u578b\u7684\u7f16\u7801\u5668\u4f5c\u4e3a\u751f\u6210\u6559\u5e08\uff0c\u901a\u8fc7\u7279\u5f81\u56de\u5f52\u6765\u76d1\u7763\u5b66\u751f\u9aa8\u5e72\u7f51\u7edc\u7684\u5b66\u4e60\uff0c\u7136\u540e\u51bb\u7ed3\u5b66\u751f\u9aa8\u5e72\u7f51\u7edc\u3002\u4e4b\u540e\uff0c\u5224\u522b\u8868\u793a\u84b8\u998f\u8fdb\u4e00\u6b65\u8003\u8651\u5c06\u9884\u5148\u8bad\u7ec3\u597d\u7684\u4eba\u8138\u8bc6\u522b\u5668\u4f5c\u4e3a\u5224\u522b\u6559\u5e08\uff0c\u901a\u8fc7\u8de8\u5206\u8fa8\u7387\u5173\u7cfb\u5bf9\u6bd4\u84b8\u998f\u6765\u76d1\u7763\u5b66\u751f\u5934\u90e8\u7684\u5b66\u4e60\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u53ef\u4ee5\u5c06\u901a\u7528\u7684\u9aa8\u5e72\u7f51\u7edc\u8868\u793a\u8f6c\u6362\u4e3a\u5224\u522b\u5934\u90e8\u8868\u793a\uff0c\u4ece\u800c\u5f62\u6210\u4e00\u4e2a\u9c81\u68d2\u7684\u3001\u5177\u6709\u5224\u522b\u529b\u7684\u5b66\u751f\u6a21\u578b\uff0c\u7528\u4e8e\u6781\u4f4e\u5206\u8fa8\u7387\u4eba\u8138\u8bc6\u522b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6539\u8fdb\u4e86\u6781\u4f4e\u5206\u8fa8\u7387\u4eba\u8138\u4e2d\u7f3a\u5931\u7ec6\u8282\u7684\u6062\u590d\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002\u5728\u4eba\u8138\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6781\u4f4e\u5206\u8fa8\u7387\u4eba\u8138\u7684\u8bc6\u522b\u7cbe\u5ea6\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u548c\u9002\u5e94\u6027\u3002||\n", "2409.06364": "|**2024-09-10**|[What happens to diffusion model likelihood when your model is conditional?](http://arxiv.org/abs/2409.06364)|null|Diffusion Models (DMs) iteratively denoise random samples to produce high-quality data. The iterative sampling process is derived from Stochastic Differential Equations (SDEs), allowing a speed-quality trade-off chosen at inference. Another advantage of sampling with differential equations is exact likelihood computation. These likelihoods have been used to rank unconditional DMs and for out-of-domain classification. Despite the many existing and possible uses of DM likelihoods, the distinct properties captured are unknown, especially in conditional contexts such as Text-To-Image (TTI) or Text-To-Speech synthesis (TTS). Surprisingly, we find that TTS DM likelihoods are agnostic to the text input. TTI likelihood is more expressive but cannot discern confounding prompts. Our results show that applying DMs to conditional tasks reveals inconsistencies and strengthens claims that the properties of DM likelihood are unknown. This impact sheds light on the previously unknown nature of DM likelihoods. Although conditional DMs maximise likelihood, the likelihood in question is not as sensitive to the conditioning input as one expects. This investigation provides a new point-of-view on diffusion likelihoods.||\n", "2409.06355": "|**2024-09-10**|[DiffQRCoder: Diffusion-based Aesthetic QR Code Generation with Scanning Robustness Guided Iterative Refinement](http://arxiv.org/abs/2409.06355)|null|With the success of Diffusion Models for image generation, the technologies also have revolutionized the aesthetic Quick Response (QR) code generation. Despite significant improvements in visual attractiveness for the beautified codes, their scannabilities are usually sacrificed and thus hinder their practical uses in real-world scenarios. To address this issue, we propose a novel Diffusion-based QR Code generator (DiffQRCoder) to effectively craft both scannable and visually pleasing QR codes. The proposed approach introduces Scanning-Robust Perceptual Guidance (SRPG), a new diffusion guidance for Diffusion Models to guarantee the generated aesthetic codes to obey the ground-truth QR codes while maintaining their attractiveness during the denoising process. Additionally, we present another post-processing technique, Scanning Robust Manifold Projected Gradient Descent (SR-MPGD), to further enhance their scanning robustness through iterative latent space optimization. With extensive experiments, the results demonstrate that our approach not only outperforms other compared methods in Scanning Success Rate (SSR) with better or comparable CLIP aesthetic score (CLIP-aes.) but also significantly improves the SSR of the ControlNet-only approach from 60% to 99%. The subjective evaluation indicates that our approach achieves promising visual attractiveness to users as well. Finally, even with different scanning angles and the most rigorous error tolerance settings, our approach robustly achieves over 95% SSR, demonstrating its capability for real-world applications.||\n", "2409.08278": "|**2024-09-12**|[DreamHOI: Subject-Driven Generation of 3D Human-Object Interactions with Diffusion Priors](http://arxiv.org/abs/2409.08278)|null|We present DreamHOI, a novel method for zero-shot synthesis of human-object interactions (HOIs), enabling a 3D human model to realistically interact with any given object based on a textual description. This task is complicated by the varying categories and geometries of real-world objects and the scarcity of datasets encompassing diverse HOIs. To circumvent the need for extensive data, we leverage text-to-image diffusion models trained on billions of image-caption pairs. We optimize the articulation of a skinned human mesh using Score Distillation Sampling (SDS) gradients obtained from these models, which predict image-space edits. However, directly backpropagating image-space gradients into complex articulation parameters is ineffective due to the local nature of such gradients. To overcome this, we introduce a dual implicit-explicit representation of a skinned mesh, combining (implicit) neural radiance fields (NeRFs) with (explicit) skeleton-driven mesh articulation. During optimization, we transition between implicit and explicit forms, grounding the NeRF generation while refining the mesh articulation. We validate our approach through extensive experiments, demonstrating its effectiveness in generating realistic HOIs.||\n", "2409.08273": "|**2024-09-12**|[Hand-Object Interaction Pretraining from Videos](http://arxiv.org/abs/2409.08273)|null|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece 3D \u624b-\u7269\u4f53\u4ea4\u4e92\u8f68\u8ff9\u4e2d\u5b66\u4e60\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u5148\u9a8c\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5229\u7528\u91ce\u5916\u89c6\u9891\u751f\u6210\u611f\u89c9\u8fd0\u52a8\u673a\u5668\u4eba\u8f68\u8ff9\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5c06\u4eba\u624b\u548c\u88ab\u64cd\u7eb5\u7269\u4f53\u63d0\u5347\u5230\u5171\u4eab\u7684 3D \u7a7a\u95f4\u4e2d\uff0c\u5e76\u5c06\u4eba\u4f53\u52a8\u4f5c\u91cd\u5b9a\u5411\u5230\u673a\u5668\u4eba\u52a8\u4f5c\u3002\u5bf9\u8fd9\u4e9b\u6570\u636e\u8fdb\u884c\u751f\u6210\u5efa\u6a21\uff0c\u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4e2a\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u57fa\u7840\u7b56\u7565\u3002\u8be5\u7b56\u7565\u6355\u83b7\u4e86\u4e00\u4e2a\u901a\u7528\u800c\u7075\u6d3b\u7684\u64cd\u4f5c\u5148\u9a8c\u3002\u6211\u4eec\u901a\u8fc7\u7ecf\u9a8c\u8bc1\u660e\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60 (RL) \u548c\u884c\u4e3a\u514b\u9686 (BC) \u5bf9\u8be5\u7b56\u7565\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u6837\u672c\u9ad8\u6548\u9002\u5e94\uff0c\u540c\u65f6\u4e0e\u5148\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5b9a\u6027\u5b9e\u9a8c\u7ed3\u679c\u53ef\u89c1\uff1a\\url{https://hgaurav2k.github.io/hop/}\u3002||\n", "2409.08272": "|**2024-09-12**|[Click2Mask: Local Editing with Dynamic Mask Generation](http://arxiv.org/abs/2409.08272)|null|\u751f\u6210\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u5f7b\u5e95\u6539\u53d8\u4e86\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u9886\u57df\uff0c\u4f7f\u975e\u4e13\u4e1a\u4eba\u58eb\u4e5f\u80fd\u8f7b\u677e\u5b8c\u6210\u8fd9\u4e9b\u4efb\u52a1\u3002\u672c\u6587\u91cd\u70b9\u5173\u6ce8\u5c40\u90e8\u56fe\u50cf\u7f16\u8f91\uff0c\u7279\u522b\u662f\u5411\u5927\u81f4\u6307\u5b9a\u533a\u57df\u6dfb\u52a0\u65b0\u5185\u5bb9\u7684\u4efb\u52a1\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u7cbe\u786e\u7684\u63a9\u7801\u6216\u5bf9\u4f4d\u7f6e\u7684\u8be6\u7ec6\u63cf\u8ff0\uff0c\u8fd9\u53ef\u80fd\u65e2\u9ebb\u70e6\u53c8\u5bb9\u6613\u51fa\u9519\u3002\u6211\u4eec\u63d0\u51fa\u4e86 Click2Mask\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5b83\u53ea\u9700\u4e00\u4e2a\u53c2\u8003\u70b9\uff08\u4ee5\u53ca\u5185\u5bb9\u63cf\u8ff0\uff09\u5373\u53ef\u7b80\u5316\u5c40\u90e8\u7f16\u8f91\u8fc7\u7a0b\u3002\u5728\u6df7\u5408\u6f5c\u5728\u6269\u6563 (BLD) \u8fc7\u7a0b\u4e2d\uff0c\u63a9\u7801\u4f1a\u56f4\u7ed5\u8be5\u70b9\u52a8\u6001\u589e\u957f\uff0c\u5e76\u4ee5\u57fa\u4e8e CLIP \u7684\u8bed\u4e49\u635f\u5931\u4e3a\u6307\u5bfc\u3002Click2Mask \u8d85\u8d8a\u4e86\u57fa\u4e8e\u5206\u5272\u548c\u4f9d\u8d56\u5fae\u8c03\u7684\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5bf9\u7528\u6237\u66f4\u53cb\u597d\u4e14\u4e0a\u4e0b\u6587\u66f4\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6839\u636e\u4eba\u7c7b\u5224\u65ad\u548c\u81ea\u52a8\u6307\u6807\uff0c\u4e0e SoTA \u65b9\u6cd5\u76f8\u6bd4\uff0cClick2Mask \u4e0d\u4ec5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e86\u7528\u6237\u7684\u5de5\u4f5c\u91cf\uff0c\u800c\u4e14\u8fd8\u63d0\u4f9b\u4e86\u5177\u6709\u7ade\u4e89\u529b\u6216\u66f4\u4f18\u7684\u5c40\u90e8\u56fe\u50cf\u5904\u7406\u7ed3\u679c\u3002\u4e3b\u8981\u8d21\u732e\u5305\u62ec\u7b80\u5316\u7528\u6237\u8f93\u5165\u3001\u80fd\u591f\u4e0d\u53d7\u73b0\u6709\u5206\u5272\u9650\u5236\u5730\u81ea\u7531\u6dfb\u52a0\u5bf9\u8c61\uff0c\u4ee5\u53ca\u5c06\u6211\u4eec\u7684\u52a8\u6001\u63a9\u7801\u65b9\u6cd5\u96c6\u6210\u5230\u5176\u4ed6\u7f16\u8f91\u65b9\u6cd5\u4e2d\u7684\u6f5c\u529b\u3002||\n", "2409.08271": "|**2024-09-12**|[DreamBeast: Distilling 3D Fantastical Animals with Part-Aware Knowledge Transfer](http://arxiv.org/abs/2409.08271)|null|We present DreamBeast, a novel method based on score distillation sampling (SDS) for generating fantastical 3D animal assets composed of distinct parts. Existing SDS methods often struggle with this generation task due to a limited understanding of part-level semantics in text-to-image diffusion models. While recent diffusion models, such as Stable Diffusion 3, demonstrate a better part-level understanding, they are prohibitively slow and exhibit other common problems associated with single-view diffusion models. DreamBeast overcomes this limitation through a novel part-aware knowledge transfer mechanism. For each generated asset, we efficiently extract part-level knowledge from the Stable Diffusion 3 model into a 3D Part-Affinity implicit representation. This enables us to instantly generate Part-Affinity maps from arbitrary camera views, which we then use to modulate the guidance of a multi-view diffusion model during SDS to create 3D assets of fantastical animals. DreamBeast significantly enhances the quality of generated 3D creatures with user-specified part compositions while reducing computational overhead, as demonstrated by extensive quantitative and qualitative evaluations.||\n", "2409.08269": "|**2024-09-12**|[Touch2Touch: Cross-Modal Tactile Generation for Object Manipulation](http://arxiv.org/abs/2409.08269)|null|\u73b0\u4eca\u7684\u89e6\u89c9\u4f20\u611f\u5668\u5f62\u6001\u5404\u5f02\uff0c\u5c3a\u5bf8\u4e0d\u4e00\u3002\u7531\u4e8e\u6a21\u578b\u901a\u5e38\u4e0e\u7279\u5b9a\u7684\u4f20\u611f\u5668\u8bbe\u8ba1\u7ed1\u5b9a\uff0c\u56e0\u6b64\u5f00\u53d1\u901a\u7528\u7684\u89e6\u89c9\u5904\u7406\u65b9\u6cd5\u53d8\u5f97\u6781\u5177\u6311\u6218\u6027\u3002\u6211\u4eec\u901a\u8fc7\u5728\u89e6\u89c9\u4f20\u611f\u5668\u4e4b\u95f4\u8fdb\u884c\u8de8\u6a21\u6001\u9884\u6d4b\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff1a\u7ed9\u5b9a\u6765\u81ea\u4e00\u4e2a\u4f20\u611f\u5668\u7684\u89e6\u89c9\u4fe1\u53f7\uff0c\u6211\u4eec\u4f7f\u7528\u751f\u6210\u6a21\u578b\u6765\u4f30\u8ba1\u53e6\u4e00\u4e2a\u4f20\u611f\u5668\u5982\u4f55\u611f\u77e5\u76f8\u540c\u7684\u7269\u7406\u63a5\u89e6\u3002\u8fd9\u5141\u8bb8\u6211\u4eec\u5c06\u7279\u5b9a\u4e8e\u4f20\u611f\u5668\u7684\u7b97\u6cd5\u5e94\u7528\u4e8e\u751f\u6210\u7684\u4fe1\u53f7\u3002\u6211\u4eec\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u6269\u6563\u6a21\u578b\u6765\u5b9e\u73b0\u8fd9\u4e2a\u60f3\u6cd5\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u5728\u6d41\u884c\u7684 GelSlim \u548c Soft Bubble \u4f20\u611f\u5668\u4e4b\u95f4\u8fdb\u884c\u8f6c\u6362\u3002\u4f5c\u4e3a\u4e00\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff0c\u6211\u4eec\u4f7f\u7528 GelSlim \u4f20\u611f\u5668\u6267\u884c\u624b\u6301\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u540c\u65f6\u4f7f\u7528\u4ec5\u5bf9 Soft Bubble \u4fe1\u53f7\u8fdb\u884c\u64cd\u4f5c\u7684\u7b97\u6cd5\u3002\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u53ef\u4ee5\u5728 https://www.mmintlab.com/research/touch2touch/ \u4e0a\u627e\u5230\u3002||\n", "2409.08260": "|**2024-09-12**|[Improving Text-guided Object Inpainting with Semantic Pre-inpainting](http://arxiv.org/abs/2409.08260)|**[link](https://github.com/nnn-s/catdiffusion)**|Recent years have witnessed the success of large text-to-image diffusion models and their remarkable potential to generate high-quality images. The further pursuit of enhancing the editability of images has sparked significant interest in the downstream task of inpainting a novel object described by a text prompt within a designated region in the image. Nevertheless, the problem is not trivial from two aspects: 1) Solely relying on one single U-Net to align text prompt and visual object across all the denoising timesteps is insufficient to generate desired objects; 2) The controllability of object generation is not guaranteed in the intricate sampling space of diffusion model. In this paper, we propose to decompose the typical single-stage object inpainting into two cascaded processes: 1) semantic pre-inpainting that infers the semantic features of desired objects in a multi-modal feature space; 2) high-fieldity object generation in diffusion latent space that pivots on such inpainted semantic features. To achieve this, we cascade a Transformer-based semantic inpainter and an object inpainting diffusion model, leading to a novel CAscaded Transformer-Diffusion (CAT-Diffusion) framework for text-guided object inpainting. Technically, the semantic inpainter is trained to predict the semantic features of the target object conditioning on unmasked context and text prompt. The outputs of the semantic inpainter then act as the informative visual prompts to guide high-fieldity object generation through a reference adapter layer, leading to controllable object inpainting. Extensive evaluations on OpenImages-V6 and MSCOCO validate the superiority of CAT-Diffusion against the state-of-the-art methods. Code is available at \\url{https://github.com/Nnn-s/CATdiffusion}.||\n", "2409.08258": "|**2024-09-12**|[Improving Virtual Try-On with Garment-focused Diffusion Models](http://arxiv.org/abs/2409.08258)|null|Diffusion models have led to the revolutionizing of generative modeling in numerous image synthesis tasks. Nevertheless, it is not trivial to directly apply diffusion models for synthesizing an image of a target person wearing a given in-shop garment, i.e., image-based virtual try-on (VTON) task. The difficulty originates from the aspect that the diffusion process should not only produce holistically high-fidelity photorealistic image of the target person, but also locally preserve every appearance and texture detail of the given garment. To address this, we shape a new Diffusion model, namely GarDiff, which triggers the garment-focused diffusion process with amplified guidance of both basic visual appearance and detailed textures (i.e., high-frequency details) derived from the given garment. GarDiff first remoulds a pre-trained latent diffusion model with additional appearance priors derived from the CLIP and VAE encodings of the reference garment. Meanwhile, a novel garment-focused adapter is integrated into the UNet of diffusion model, pursuing local fine-grained alignment with the visual appearance of reference garment and human pose. We specifically design an appearance loss over the synthesized garment to enhance the crucial, high-frequency details. Extensive experiments on VITON-HD and DressCode datasets demonstrate the superiority of our GarDiff when compared to state-of-the-art VTON approaches. Code is publicly available at: \\href{https://github.com/siqi0905/GarDiff/tree/master}{https://github.com/siqi0905/GarDiff/tree/master}.||\n", "2409.08255": "|**2024-09-12**|[LoRID: Low-Rank Iterative Diffusion for Adversarial Purification](http://arxiv.org/abs/2409.08255)|null|This work presents an information-theoretic examination of diffusion-based purification methods, the state-of-the-art adversarial defenses that utilize diffusion models to remove malicious perturbations in adversarial examples. By theoretically characterizing the inherent purification errors associated with the Markov-based diffusion purifications, we introduce LoRID, a novel Low-Rank Iterative Diffusion purification method designed to remove adversarial perturbation with low intrinsic purification errors. LoRID centers around a multi-stage purification process that leverages multiple rounds of diffusion-denoising loops at the early time-steps of the diffusion models, and the integration of Tucker decomposition, an extension of matrix factorization, to remove adversarial noise at high-noise regimes. Consequently, LoRID increases the effective diffusion time-steps and overcomes strong adversarial attacks, achieving superior robustness performance in CIFAR-10/100, CelebA-HQ, and ImageNet datasets under both white-box and black-box settings.||\n", "2409.08251": "|**2024-09-12**|[Dynamic Prompting of Frozen Text-to-Image Diffusion Models for Panoptic Narrative Grounding](http://arxiv.org/abs/2409.08251)|null|Panoptic narrative grounding (PNG), whose core target is fine-grained image-text alignment, requires a panoptic segmentation of referred objects given a narrative caption. Previous discriminative methods achieve only weak or coarse-grained alignment by panoptic segmentation pretraining or CLIP model adaptation. Given the recent progress of text-to-image Diffusion models, several works have shown their capability to achieve fine-grained image-text alignment through cross-attention maps and improved general segmentation performance. However, the direct use of phrase features as static prompts to apply frozen Diffusion models to the PNG task still suffers from a large task gap and insufficient vision-language interaction, yielding inferior performance. Therefore, we propose an Extractive-Injective Phrase Adapter (EIPA) bypass within the Diffusion UNet to dynamically update phrase prompts with image features and inject the multimodal cues back, which leverages the fine-grained image-text alignment capability of Diffusion models more sufficiently. In addition, we also design a Multi-Level Mutual Aggregation (MLMA) module to reciprocally fuse multi-level image and phrase features for segmentation refinement. Extensive experiments on the PNG benchmark show that our method achieves new state-of-the-art performance.||\n", "2409.08240": "|**2024-09-12**|[IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation](http://arxiv.org/abs/2409.08240)|null|While Text-to-Image (T2I) diffusion models excel at generating visually appealing images of individual instances, they struggle to accurately position and control the features generation of multiple instances. The Layout-to-Image (L2I) task was introduced to address the positioning challenges by incorporating bounding boxes as spatial control signals, but it still falls short in generating precise instance features. In response, we propose the Instance Feature Generation (IFG) task, which aims to ensure both positional accuracy and feature fidelity in generated instances. To address the IFG task, we introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhances feature depiction by incorporating additional appearance tokens and utilizing an Instance Semantic Map to align instance-level features with spatial locations. The IFAdapter guides the diffusion process as a plug-and-play module, making it adaptable to various community models. For evaluation, we contribute an IFG benchmark and develop a verification pipeline to objectively compare models' abilities to generate instances with accurate positioning and features. Experimental results demonstrate that IFAdapter outperforms other models in both quantitative and qualitative evaluations.||\n", "2409.09016": "|**2024-09-13**|[Closed-Loop Visuomotor Control with Generative Expectation for Robotic Manipulation](http://arxiv.org/abs/2409.09016)|**[link](https://github.com/OpenDriveLab/CLOVER)**|Despite significant progress in robotics and embodied AI in recent years, deploying robots for long-horizon tasks remains a great challenge. Majority of prior arts adhere to an open-loop philosophy and lack real-time feedback, leading to error accumulation and undesirable robustness. A handful of approaches have endeavored to establish feedback mechanisms leveraging pixel-level differences or pre-trained visual representations, yet their efficacy and adaptability have been found to be constrained. Inspired by classic closed-loop control systems, we propose CLOVER, a closed-loop visuomotor control framework that incorporates feedback mechanisms to improve adaptive robotic control. CLOVER consists of a text-conditioned video diffusion model for generating visual plans as reference inputs, a measurable embedding space for accurate error quantification, and a feedback-driven controller that refines actions from feedback and initiates replans as needed. Our framework exhibits notable advancement in real-world robotic tasks and achieves state-of-the-art on CALVIN benchmark, improving by 8% over previous open-loop counterparts. Code and checkpoints are maintained at https://github.com/OpenDriveLab/CLOVER.|\n", "2409.08947": "|**2024-09-13**|[A Diffusion Approach to Radiance Field Relighting using Multi-Illumination Synthesis](http://arxiv.org/abs/2409.08947)|null|Relighting radiance fields is severely underconstrained for multi-view data, which is most often captured under a single illumination condition; It is especially hard for full scenes containing multiple objects. We introduce a method to create relightable radiance fields using such single-illumination data by exploiting priors extracted from 2D image diffusion models. We first fine-tune a 2D diffusion model on a multi-illumination dataset conditioned by light direction, allowing us to augment a single-illumination capture into a realistic -- but possibly inconsistent -- multi-illumination dataset from directly defined light directions. We use this augmented data to create a relightable radiance field represented by 3D Gaussian splats. To allow direct control of light direction for low-frequency lighting, we represent appearance with a multi-layer perceptron parameterized on light direction. To enforce multi-view consistency and overcome inaccuracies we optimize a per-image auxiliary feature vector. We show results on synthetic and real multi-view data under single illumination, demonstrating that our method successfully exploits 2D diffusion model priors to allow realistic 3D relighting for complete scenes. Project site https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/|\n", "2409.08917": "|**2024-09-13**|[Latent Space Score-based Diffusion Model for Probabilistic Multivariate Time Series Imputation](http://arxiv.org/abs/2409.08917)|**[link](https://github.com/gorgen2020/LSSDM_imputation)**|Accurate imputation is essential for the reliability and success of downstream tasks. Recently, diffusion models have attracted great attention in this field. However, these models neglect the latent distribution in a lower-dimensional space derived from the observed data, which limits the generative capacity of the diffusion model. Additionally, dealing with the original missing data without labels becomes particularly problematic. To address these issues, we propose the Latent Space Score-Based Diffusion Model (LSSDM) for probabilistic multivariate time series imputation. Observed values are projected onto low-dimensional latent space and coarse values of the missing data are reconstructed without knowing their ground truth values by this unsupervised learning approach. Finally, the reconstructed values are fed into a conditional diffusion model to obtain the precise imputed values of the time series. In this way, LSSDM not only possesses the power to identify the latent distribution but also seamlessly integrates the diffusion model to obtain the high-fidelity imputed values and assess the uncertainty of the dataset. Experimental results demonstrate that LSSDM achieves superior imputation performance while also providing a better explanation and uncertainty analysis of the imputation mechanism. The website of the code is \\textit{https://github.com/gorgen2020/LSSDM\\_imputation}.|\n", "2409.08906": "|**2024-09-13**|[Gaussian is All You Need: A Unified Framework for Solving Inverse Problems via Diffusion Posterior Sampling](http://arxiv.org/abs/2409.08906)|null|Diffusion models can generate a variety of high-quality images by modeling complex data distributions. Trained diffusion models can also be very effective image priors for solving inverse problems. Most of the existing diffusion-based methods integrate data consistency steps within the diffusion reverse sampling process. The data consistency steps rely on an approximate likelihood function. In this paper, we show that the existing approximations are either insufficient or computationally inefficient. To address these issues, we propose a unified likelihood approximation method that incorporates a covariance correction term to enhance the performance and avoids propagating gradients through the diffusion model. The correction term, when integrated into the reverse diffusion sampling process, achieves better convergence towards the true data posterior for selected distributions and improves performance on real-world natural image datasets. Furthermore, we present an efficient way to factorize and invert the covariance matrix of the likelihood function for several inverse problems. We present comprehensive experiments to demonstrate the effectiveness of our method over several existing approaches.|\n", "2409.08861": "|**2024-09-13**|[Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control](http://arxiv.org/abs/2409.08861)|null|Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there has not been many theoretically-sound methods for improving these models with reward fine-tuning. In this work, we cast reward fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific memoryless noise schedule must be enforced during fine-tuning, in order to account for the dependency between the noise variable and the generated samples. We also propose a new algorithm named Adjoint Matching which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We find that our approach significantly improves over existing methods for reward fine-tuning, achieving better consistency, realism, and generalization to unseen human preference reward models, while retaining sample diversity.|\n", "2409.08857": "|**2024-09-13**|[InstantDrag: Improving Interactivity in Drag-based Image Editing](http://arxiv.org/abs/2409.08857)|null|Drag-based image editing has recently gained popularity for its interactivity and precision. However, despite the ability of text-to-image models to generate samples within a second, drag editing still lags behind due to the challenge of accurately reflecting user interaction while maintaining image content. Some existing approaches rely on computationally intensive per-image optimization or intricate guidance-based methods, requiring additional inputs such as masks for movable regions and text prompts, thereby compromising the interactivity of the editing process. We introduce InstantDrag, an optimization-free pipeline that enhances interactivity and speed, requiring only an image and a drag instruction as input. InstantDrag consists of two carefully designed networks: a drag-conditioned optical flow generator (FlowGen) and an optical flow-conditioned diffusion model (FlowDiffusion). InstantDrag learns motion dynamics for drag-based image editing in real-world video datasets by decomposing the task into motion generation and motion-conditioned image generation. We demonstrate InstantDrag's capability to perform fast, photo-realistic edits without masks or text prompts through experiments on facial video datasets and general scenes. These results highlight the efficiency of our approach in handling drag-based image editing, making it a promising solution for interactive, real-time applications.|\n", "2409.08850": "|**2024-09-13**|[DX2CT: Diffusion Model for 3D CT Reconstruction from Bi or Mono-planar 2D X-ray(s)](http://arxiv.org/abs/2409.08850)|null|Computational tomography (CT) provides high-resolution medical imaging, but it can expose patients to high radiation. X-ray scanners have low radiation exposure, but their resolutions are low. This paper proposes a new conditional diffusion model, DX2CT, that reconstructs three-dimensional (3D) CT volumes from bi or mono-planar X-ray image(s). Proposed DX2CT consists of two key components: 1) modulating feature maps extracted from two-dimensional (2D) X-ray(s) with 3D positions of CT volume using a new transformer and 2) effectively using the modulated 3D position-aware feature maps as conditions of DX2CT. In particular, the proposed transformer can provide conditions with rich information of a target CT slice to the conditional diffusion model, enabling high-quality CT reconstruction. Our experiments with the bi or mono-planar X-ray(s) benchmark datasets show that proposed DX2CT outperforms several state-of-the-art methods. Our codes and model will be available at: https://www.github.com/intyeger/DX2CT.|\n", "2409.08731": "|**2024-09-13**|[DFADD: The Diffusion and Flow-Matching Based Audio Deepfake Dataset](http://arxiv.org/abs/2409.08731)|null|Mainstream zero-shot TTS production systems like Voicebox and Seed-TTS achieve human parity speech by leveraging Flow-matching and Diffusion models, respectively. Unfortunately, human-level audio synthesis leads to identity misuse and information security issues. Currently, many antispoofing models have been developed against deepfake audio. However, the efficacy of current state-of-the-art anti-spoofing models in countering audio synthesized by diffusion and flowmatching based TTS systems remains unknown. In this paper, we proposed the Diffusion and Flow-matching based Audio Deepfake (DFADD) dataset. The DFADD dataset collected the deepfake audio based on advanced diffusion and flowmatching TTS models. Additionally, we reveal that current anti-spoofing models lack sufficient robustness against highly human-like audio generated by diffusion and flow-matching TTS systems. The proposed DFADD dataset addresses this gap and provides a valuable resource for developing more resilient anti-spoofing models.|\n", "2409.08601": "|**2024-09-13**|[STA-V2A: Video-to-Audio Generation with Semantic and Temporal Alignment](http://arxiv.org/abs/2409.08601)|null|Visual and auditory perception are two crucial ways humans experience the world. Text-to-video generation has made remarkable progress over the past year, but the absence of harmonious audio in generated video limits its broader applications. In this paper, we propose Semantic and Temporal Aligned Video-to-Audio (STA-V2A), an approach that enhances audio generation from videos by extracting both local temporal and global semantic video features and combining these refined video features with text as cross-modal guidance. To address the issue of information redundancy in videos, we propose an onset prediction pretext task for local temporal feature extraction and an attentive pooling module for global semantic feature extraction. To supplement the insufficient semantic information in videos, we propose a Latent Diffusion Model with Text-to-Audio priors initialization and cross-modal guidance. We also introduce Audio-Audio Align, a new metric to assess audio-temporal alignment. Subjective and objective metrics demonstrate that our method surpasses existing Video-to-Audio models in generating audio with better quality, semantic consistency, and temporal alignment. The ablation experiment validated the effectiveness of each module. Audio samples are available at https://y-ren16.github.io/STAV2A.|\n", "2409.08583": "|**2024-09-13**|[LHQ-SVC: Lightweight and High Quality Singing Voice Conversion Modeling](http://arxiv.org/abs/2409.08583)|null|Singing Voice Conversion (SVC) has emerged as a significant subfield of Voice Conversion (VC), enabling the transformation of one singer's voice into another while preserving musical elements such as melody, rhythm, and timbre. Traditional SVC methods have limitations in terms of audio quality, data requirements, and computational complexity. In this paper, we propose LHQ-SVC, a lightweight, CPU-compatible model based on the SVC framework and diffusion model, designed to reduce model size and computational demand without sacrificing performance. We incorporate features to improve inference quality, and optimize for CPU execution by using performance tuning tools and parallel computing frameworks. Our experiments demonstrate that LHQ-SVC maintains competitive performance, with significant improvements in processing speed and efficiency across different devices. The results suggest that LHQ-SVC can meet|\n", "2409.11406": "|**2024-09-17**|[Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion](http://arxiv.org/abs/2409.11406)|null|In 3D modeling, designers often use an existing 3D model as a reference to create new ones. This practice has inspired the development of Phidias, a novel generative model that uses diffusion for reference-augmented 3D generation. Given an image, our method leverages a retrieved or user-provided 3D reference model to guide the generation process, thereby enhancing the generation quality, generalization ability, and controllability. Our model integrates three key components: 1) meta-ControlNet that dynamically modulates the conditioning strength, 2) dynamic reference routing that mitigates misalignment between the input image and 3D reference, and 3) self-reference augmentations that enable self-supervised training with a progressive curriculum. Collectively, these designs result in a clear improvement over existing methods. Phidias establishes a unified framework for 3D generation using text, image, and 3D conditions with versatile applications.|\n", "2409.11401": "|**2024-09-17**|[Teaching dark matter simulations to speak the halo language](http://arxiv.org/abs/2409.11401)|null|We develop a transformer-based conditional generative model for discrete point objects and their properties. We use it to build a model for populating cosmological simulations with gravitationally collapsed structures called dark matter halos. Specifically, we condition our model with dark matter distribution obtained from fast, approximate simulations to recover the correct three-dimensional positions and masses of individual halos. This leads to a first model that can recover the statistical properties of the halos at small scales to better than 3% level using an accelerated dark matter simulation. This trained model can then be applied to simulations with significantly larger volumes which would otherwise be computationally prohibitive with traditional simulations, and also provides a crucial missing link in making end-to-end differentiable cosmological simulations. The code, named GOTHAM (Generative cOnditional Transformer for Halo's Auto-regressive Modeling) is publicly available at \\url{https://github.com/shivampcosmo/GOTHAM}.|\n", "2409.11380": "|**2024-09-17**|[Ultrasound Image Enhancement with the Variance of Diffusion Models](http://arxiv.org/abs/2409.11380)|**[link](https://github.com/yuxin-zhang-jasmine/ius2024_diffusion)**|Ultrasound imaging, despite its widespread use in medicine, often suffers from various sources of noise and artifacts that impact the signal-to-noise ratio and overall image quality. Enhancing ultrasound images requires a delicate balance between contrast, resolution, and speckle preservation. This paper introduces a novel approach that integrates adaptive beamforming with denoising diffusion-based variance imaging to address this challenge. By applying Eigenspace-Based Minimum Variance (EBMV) beamforming and employing a denoising diffusion model fine-tuned on ultrasound data, our method computes the variance across multiple diffusion-denoised samples to produce high-quality despeckled images. This approach leverages both the inherent multiplicative noise of ultrasound and the stochastic nature of diffusion models. Experimental results on a publicly available dataset demonstrate the effectiveness of our method in achieving superior image reconstructions from single plane-wave acquisitions. The code is available at: https://github.com/Yuxin-Zhang-Jasmine/IUS2024_Diffusion.|\n", "2409.11367": "|**2024-09-17**|[OSV: One Step is Enough for High-Quality Image to Video Generation](http://arxiv.org/abs/2409.11367)|null|Video diffusion models have shown great potential in generating high-quality videos, making them an increasingly popular focus. However, their inherent iterative nature leads to substantial computational and time costs. While efforts have been made to accelerate video diffusion by reducing inference steps (through techniques like consistency distillation) and GAN training (these approaches often fall short in either performance or training stability). In this work, we introduce a two-stage training framework that effectively combines consistency distillation with GAN training to address these challenges. Additionally, we propose a novel video discriminator design, which eliminates the need for decoding the video latents and improves the final performance. Our model is capable of producing high-quality videos in merely one-step, with the flexibility to perform multi-step refinement for further performance enhancement. Our quantitative evaluation on the OpenWebVid-1M benchmark shows that our model significantly outperforms existing methods. Notably, our 1-step performance(FVD 171.15) exceeds the 8-step performance of the consistency distillation based method, AnimateLCM (FVD 184.79), and approaches the 25-step performance of advanced Stable Video Diffusion (FVD 156.94).|\n", "2409.11355": "|**2024-09-17**|[Fine-Tuning Image-Conditional Diffusion Models is Easier than You Think](http://arxiv.org/abs/2409.11355)|null|Recent work showed that large diffusion models can be reused as highly precise monocular depth estimators by casting depth estimation as an image-conditional image generation task. While the proposed model achieved state-of-the-art results, high computational demands due to multi-step inference limited its use in many scenarios. In this paper, we show that the perceived inefficiency was caused by a flaw in the inference pipeline that has so far gone unnoticed. The fixed model performs comparably to the best previously reported configuration while being more than 200$\\times$ faster. To optimize for downstream task performance, we perform end-to-end fine-tuning on top of the single-step model with task-specific losses and get a deterministic model that outperforms all other diffusion-based depth and normal estimation models on common zero-shot benchmarks. We surprisingly find that this fine-tuning protocol also works directly on Stable Diffusion and achieves comparable performance to current state-of-the-art diffusion-based depth and normal estimation models, calling into question some of the conclusions drawn from prior works.|\n", "2409.11340": "|**2024-09-17**|[OmniGen: Unified Image Generation](http://arxiv.org/abs/2409.11340)|**[link](https://github.com/vectorspacelab/omnigen)**|In this work, we introduce OmniGen, a new diffusion model for unified image generation. Unlike popular diffusion models (e.g., Stable Diffusion), OmniGen no longer requires additional modules such as ControlNet or IP-Adapter to process diverse control conditions. OmniGenis characterized by the following features: 1) Unification: OmniGen not only demonstrates text-to-image generation capabilities but also inherently supports other downstream tasks, such as image editing, subject-driven generation, and visual-conditional generation. Additionally, OmniGen can handle classical computer vision tasks by transforming them into image generation tasks, such as edge detection and human pose recognition. 2) Simplicity: The architecture of OmniGen is highly simplified, eliminating the need for additional text encoders. Moreover, it is more user-friendly compared to existing diffusion models, enabling complex tasks to be accomplished through instructions without the need for extra preprocessing steps (e.g., human pose estimation), thereby significantly simplifying the workflow of image generation. 3) Knowledge Transfer: Through learning in a unified format, OmniGen effectively transfers knowledge across different tasks, manages unseen tasks and domains, and exhibits novel capabilities. We also explore the model's reasoning capabilities and potential applications of chain-of-thought mechanism. This work represents the first attempt at a general-purpose image generation model, and there remain several unresolved issues. We will open-source the related resources at https://github.com/VectorSpaceLab/OmniGen to foster advancements in this field.|\n", "2409.11315": "|**2024-09-17**|[fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction](http://arxiv.org/abs/2409.11315)|null|Reconstructing 3D visuals from functional Magnetic Resonance Imaging (fMRI) data, introduced as Recon3DMind in our conference work, is of significant interest to both cognitive neuroscience and computer vision. To advance this task, we present the fMRI-3D dataset, which includes data from 15 participants and showcases a total of 4768 3D objects. The dataset comprises two components: fMRI-Shape, previously introduced and accessible at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape, and fMRI-Objaverse, proposed in this paper and available at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse. fMRI-Objaverse includes data from 5 subjects, 4 of whom are also part of the Core set in fMRI-Shape, with each subject viewing 3142 3D objects across 117 categories, all accompanied by text captions. This significantly enhances the diversity and potential applications of the dataset. Additionally, we propose MinD-3D, a novel framework designed to decode 3D visual information from fMRI signals. The framework first extracts and aggregates features from fMRI data using a neuro-fusion encoder, then employs a feature-bridge diffusion model to generate visual features, and finally reconstructs the 3D object using a generative transformer decoder. We establish new benchmarks by designing metrics at both semantic and structural levels to evaluate model performance. Furthermore, we assess our model's effectiveness in an Out-of-Distribution setting and analyze the attribution of the extracted features and the visual ROIs in fMRI signals. Our experiments demonstrate that MinD-3D not only reconstructs 3D objects with high semantic and spatial accuracy but also deepens our understanding of how human brain processes 3D visual information. Project page at: https://jianxgao.github.io/MinD-3D.|\n", "2409.11308": "|**2024-09-17**|[SpMis: An Investigation of Synthetic Spoken Misinformation Detection](http://arxiv.org/abs/2409.11308)|null|In recent years, speech generation technology has advanced rapidly, fueled by generative models and large-scale training techniques. While these developments have enabled the production of high-quality synthetic speech, they have also raised concerns about the misuse of this technology, particularly for generating synthetic misinformation. Current research primarily focuses on distinguishing machine-generated speech from human-produced speech, but the more urgent challenge is detecting misinformation within spoken content. This task requires a thorough analysis of factors such as speaker identity, topic, and synthesis. To address this need, we conduct an initial investigation into synthetic spoken misinformation detection by introducing an open-source dataset, SpMis. SpMis includes speech synthesized from over 1,000 speakers across five common topics, utilizing state-of-the-art text-to-speech systems. Although our results show promising detection capabilities, they also reveal substantial challenges for practical implementation, underscoring the importance of ongoing research in this critical area.|\n", "2409.11292": "|**2024-09-17**|[DroneDiffusion: Robust Quadrotor Dynamics Learning with Diffusion Models](http://arxiv.org/abs/2409.11292)|null|An inherent fragility of quadrotor systems stems from model inaccuracies and external disturbances. These factors hinder performance and compromise the stability of the system, making precise control challenging. Existing model-based approaches either make deterministic assumptions, utilize Gaussian-based representations of uncertainty, or rely on nominal models, all of which often fall short in capturing the complex, multimodal nature of real-world dynamics. This work introduces DroneDiffusion, a novel framework that leverages conditional diffusion models to learn quadrotor dynamics, formulated as a sequence generation task. DroneDiffusion achieves superior generalization to unseen, complex scenarios by capturing the temporal nature of uncertainties and mitigating error propagation. We integrate the learned dynamics with an adaptive controller for trajectory tracking with stability guarantees. Extensive experiments in both simulation and real-world flights demonstrate the robustness of the framework across a range of scenarios, including unfamiliar flight paths and varying payloads, velocities, and wind disturbances.|\n", "2409.11228": "|**2024-09-17**|[Learning Source Disentanglement in Neural Audio Codec](http://arxiv.org/abs/2409.11228)|null|Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.|\n", "2409.12189": "|**2024-09-18**|[Massively Multi-Person 3D Human Motion Forecasting with Scene Context](http://arxiv.org/abs/2409.12189)|null|\u9884\u6d4b\u957f\u671f\u76843D\u4eba\u4f53\u8fd0\u52a8\u5177\u6709\u6311\u6218\u6027\uff1a\u4eba\u7c7b\u884c\u4e3a\u7684\u968f\u673a\u6027\u4f7f\u5f97\u4ec5\u4ece\u8f93\u5165\u5e8f\u5217\u751f\u6210\u903c\u771f\u7684\u4eba\u4f53\u8fd0\u52a8\u53d8\u5f97\u56f0\u96be\u3002\u573a\u666f\u73af\u5883\u548c\u9644\u8fd1\u4eba\u5458\u8fd0\u52a8\u7684\u4fe1\u606f\u53ef\u4ee5\u6781\u5927\u5730\u5e2e\u52a9\u751f\u6210\u8fc7\u7a0b\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u573a\u666f\u611f\u77e5\u7684\u793e\u4ea4Transformer\u6a21\u578b\uff08SAST\uff09\u6765\u9884\u6d4b\u957f\u671f\uff0810\u79d2\uff09\u7684\u4eba\u4f53\u8fd0\u52a8\u3002\u4e0e\u4e4b\u524d\u7684\u6a21\u578b\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5bf9\u573a\u666f\u4e2d\u6570\u91cf\u5dee\u5f02\u5f88\u5927\u7684\u4eba\u548c\u7269\u4f53\u4e4b\u95f4\u7684\u4ea4\u4e92\u8fdb\u884c\u5efa\u6a21\u3002\u6211\u4eec\u5c06\u65f6\u95f4\u5377\u79ef\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u4e0e\u57fa\u4e8eTransformer\u7684\u74f6\u9888\u76f8\u7ed3\u5408\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u6709\u6548\u5730\u7ed3\u5408\u8fd0\u52a8\u548c\u573a\u666f\u4fe1\u606f\u3002\u6211\u4eec\u4f7f\u7528\u53bb\u566a\u6269\u6563\u6a21\u578b\u5bf9\u6761\u4ef6\u8fd0\u52a8\u5206\u5e03\u8fdb\u884c\u5efa\u6a21\u3002\u6211\u4eec\u5728Humans in Kitchens\u6570\u636e\u96c6\u4e0a\u5bf9\u6211\u4eec\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b1\u523016\u4e2a\u4eba\u548c29\u523050\u4e2a\u540c\u65f6\u53ef\u89c1\u7684\u7269\u4f53\u3002\u5728\u4e0d\u540c\u7684\u6307\u6807\u548c\u7528\u6237\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u771f\u5b9e\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u4ee3\u7801\u53ef\u5728https://github.com/felixbmuller/SAST\u83b7\u53d6\u3002|\n", "2409.12140": "|**2024-09-18**|[MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion](http://arxiv.org/abs/2409.12140)|null|\u6211\u4eec\u4ecb\u7ecdMoRAG\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u591a\u90e8\u4ef6\u878d\u5408\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7b56\u7565\uff0c\u7528\u4e8e\u57fa\u4e8e\u6587\u672c\u7684\u4eba\u4f53\u52a8\u4f5c\u751f\u6210\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u6539\u8fdb\u7684\u52a8\u4f5c\u68c0\u7d22\u8fc7\u7a0b\u83b7\u5f97\u7684\u989d\u5916\u77e5\u8bc6\u6765\u589e\u5f3a\u52a8\u4f5c\u6269\u6563\u6a21\u578b\u3002\u901a\u8fc7\u6709\u6548\u5730\u63d0\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u52a8\u4f5c\u68c0\u7d22\u4e2d\u7684\u62fc\u5199\u9519\u8bef\u548c\u6539\u5199\u95ee\u9898\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u591a\u90e8\u4ef6\u68c0\u7d22\u7b56\u7565\u6765\u63d0\u9ad8\u52a8\u4f5c\u68c0\u7d22\u5728\u8bed\u8a00\u7a7a\u95f4\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u901a\u8fc7\u68c0\u7d22\u5230\u7684\u52a8\u4f5c\u7684\u7a7a\u95f4\u7ec4\u5408\u6765\u521b\u5efa\u591a\u6837\u5316\u7684\u6837\u672c\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5229\u7528\u4f4e\u7ea7\u7684\u3001\u7279\u5b9a\u4e8e\u8eab\u4f53\u90e8\u4f4d\u7684\u52a8\u4f5c\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u6784\u5efa\u9488\u5bf9\u672a\u89c1\u8fc7\u7684\u6587\u672c\u63cf\u8ff0\u7684\u52a8\u4f5c\u6837\u672c\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u53ef\u4ee5\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u63d0\u9ad8\u52a8\u4f5c\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\u3002\u4ee3\u7801\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u793a\u4f8b\u89c6\u9891\u5c06\u5728\u4ee5\u4e0b\u7f51\u5740\u63d0\u4f9b\uff1ahttps://motion-rag.github.io/|\n", "2409.12099": "|**2024-09-18**|[Brain-Streams: fMRI-to-Image Reconstruction with Multi-modal Guidance](http://arxiv.org/abs/2409.12099)|null|\u7406\u89e3\u4eba\u7c7b\u5982\u4f55\u5904\u7406\u89c6\u89c9\u4fe1\u606f\u662f\u63ed\u793a\u5927\u8111\u6d3b\u52a8\u6f5c\u5728\u673a\u5236\u7684\u5173\u952e\u6b65\u9aa4\u4e4b\u4e00\u3002\u8fd1\u5e74\u6765\uff0c\u8fd9\u79cd\u597d\u5947\u5fc3\u63a8\u52a8\u4e86 fMRI \u56fe\u50cf\u91cd\u5efa\u4efb\u52a1\u7684\u53d1\u5c55\uff1b\u8be5\u4efb\u52a1\u65e8\u5728\u5229\u7528\u6765\u81ea\u89c6\u89c9\u523a\u6fc0\u7684 fMRI \u6570\u636e\u91cd\u5efa\u76f8\u5e94\u7684\u89c6\u89c9\u523a\u6fc0\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u5229\u7528\u5f3a\u5927\u7684\u751f\u6210\u6a21\u578b\uff08\u5982\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\uff09\u5728\u4ece\u89c6\u89c9\u6570\u636e\u96c6\u4e2d\u91cd\u5efa\u590d\u6742\u7684\u89c6\u89c9\u523a\u6fc0\uff08\u5982\u9ad8\u5206\u8fa8\u7387\u81ea\u7136\u56fe\u50cf\uff09\u65b9\u9762\u5df2\u663e\u793a\u51fa\u826f\u597d\u7684\u6548\u679c\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u91cd\u5efa\u5728\u7ed3\u6784\u4fdd\u771f\u5ea6\u65b9\u9762\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u7f3a\u4e4f\u5bf9\u5c0f\u7269\u4f53\u7684\u7ec6\u8282\u3001\u6a21\u7cca\u5f62\u72b6\u548c\u8bed\u4e49\u7ec6\u5fae\u5dee\u522b\u7684\u63cf\u8ff0\u3002\u56e0\u6b64\uff0c\u9664\u4e86\u5355\u7eaf\u7684\u89c6\u89c9\u6548\u679c\u5916\uff0c\u52a0\u5165\u989d\u5916\u7684\u8bed\u4e49\u77e5\u8bc6\u53d8\u5f97\u52bf\u5728\u5fc5\u884c\u3002\u9274\u4e8e\u6b64\uff0c\u6211\u4eec\u5229\u7528\u73b0\u4ee3 LDM \u5982\u4f55\u6709\u6548\u5730\u7ed3\u5408\u591a\u6a21\u6001\u5f15\u5bfc\uff08\u6587\u672c\u5f15\u5bfc\u3001\u89c6\u89c9\u5f15\u5bfc\u548c\u56fe\u50cf\u5e03\u5c40\uff09\u6765\u751f\u6210\u7ed3\u6784\u548c\u8bed\u4e49\u4e0a\u5408\u7406\u7684\u56fe\u50cf\u3002\u5177\u4f53\u800c\u8a00\uff0c\u53d7\u53cc\u6d41\u5047\u8bf4\u7684\u542f\u53d1\uff0c\u8be5\u5047\u8bf4\u8868\u660e\u611f\u77e5\u4fe1\u606f\u548c\u8bed\u4e49\u4fe1\u606f\u662f\u5728\u4e0d\u540c\u7684\u5927\u8111\u533a\u57df\u4e2d\u5904\u7406\u7684\uff0c\u6211\u4eec\u7684\u6846\u67b6 Brain-Streams \u5c06\u6765\u81ea\u8fd9\u4e9b\u5927\u8111\u533a\u57df\u7684 fMRI \u4fe1\u53f7\u6620\u5c04\u5230\u9002\u5f53\u7684\u5d4c\u5165\u4e2d\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u901a\u8fc7\u4ece\u8bed\u4e49\u4fe1\u606f\u533a\u57df\u63d0\u53d6\u6587\u672c\u5f15\u5bfc\uff0c\u4ece\u611f\u77e5\u4fe1\u606f\u533a\u57df\u63d0\u53d6\u89c6\u89c9\u5f15\u5bfc\uff0cBrain-Streams \u4e3a LDM \u63d0\u4f9b\u4e86\u51c6\u786e\u7684\u591a\u6a21\u6001\u5f15\u5bfc\u3002\u6211\u4eec\u5728\u4e00\u4e2a\u5305\u542b\u81ea\u7136\u56fe\u50cf\u523a\u6fc0\u548c fMRI \u6570\u636e\u7684\u771f\u5b9e fMRI \u6570\u636e\u96c6\u4e0a\uff0c\u5bf9 Brain-Streams \u7684\u91cd\u5efa\u80fd\u529b\u8fdb\u884c\u4e86\u5b9a\u91cf\u548c\u5b9a\u6027\u7684\u9a8c\u8bc1\u3002|\n", "2409.12080": "|**2024-09-18**|[Design of Ligand-Binding Proteins with Atomic Flow Matching](http://arxiv.org/abs/2409.12080)|null|\u8bbe\u8ba1\u4e0e\u5c0f\u5206\u5b50\u7ed3\u5408\u7684\u65b0\u578b\u86cb\u767d\u8d28\u662f\u8ba1\u7b97\u751f\u7269\u5b66\u4e2d\u4e00\u9879\u957f\u671f\u6311\u6218\uff0c\u5728\u5f00\u53d1\u50ac\u5316\u5242\u3001\u751f\u7269\u4f20\u611f\u5668\u7b49\u65b9\u9762\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u3002\u76ee\u524d\u7684\u8ba1\u7b97\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u76ee\u6807\u5206\u5b50\u7ed3\u5408\u59ff\u6001\u5df2\u77e5\u7684\u5047\u8bbe\uff0c\u4f46\u8fd9\u5e76\u4e0d\u603b\u662f\u53ef\u884c\u7684\uff0c\u56e0\u4e3a\u65b0\u76ee\u6807\u7684\u6784\u8c61\u901a\u5e38\u662f\u672a\u77e5\u7684\uff0c\u5e76\u4e14\u5728\u7ed3\u5408\u65f6\u5f80\u5f80\u4f1a\u53d1\u751f\u53d8\u5316\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5c06\u86cb\u767d\u8d28\u548c\u5206\u5b50\u5efa\u6a21\u4e3a\u7edf\u4e00\u7684\u751f\u7269\u6807\u8bb0\uff08biotoken\uff09\uff0c\u5e76\u63d0\u51fa\u4e86AtomFlow\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u6846\u67b6\u7684\u65b0\u578b\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u4ec5\u4ece\u4e8c\u7ef4\u76ee\u6807\u5206\u5b50\u56fe\u5373\u53ef\u8bbe\u8ba1\u914d\u4f53\u7ed3\u5408\u86cb\u767d\u3002AtomFlow\u4f5c\u7528\u4e8e\u751f\u7269\u6807\u8bb0\u7684\u4ee3\u8868\u6027\u539f\u5b50\uff0c\u6355\u6349\u914d\u4f53\u7684\u7075\u6d3b\u6027\uff0c\u5e76\u8fed\u4ee3\u751f\u6210\u914d\u4f53\u6784\u8c61\u548c\u86cb\u767d\u8d28\u9aa8\u67b6\u7ed3\u6784\u3002\u6211\u4eec\u8003\u8651\u4e86\u751f\u7269\u6807\u8bb0\u7684\u591a\u5c3a\u5ea6\u6027\u8d28\uff0c\u5e76\u901a\u8fc7\u4f7f\u7528SE(3)\u7b49\u53d8\u7ed3\u6784\u9884\u6d4b\u7f51\u7edc\u5339\u914d\u6d41\u5411\u91cf\u573a\uff0c\u8bc1\u660e\u4e86AtomFlow\u53ef\u4ee5\u5728\u86cb\u767d\u8d28\u6570\u636e\u5e93\u7684\u7ed3\u6784\u5b50\u96c6\u4e0a\u8fdb\u884c\u6709\u6548\u8bad\u7ec3\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u914d\u4f53\u7ed3\u5408\u86cb\u767d\uff0c\u5e76\u4e14\u6027\u80fd\u53ef\u4e0e\u6700\u5148\u8fdb\u7684\u6a21\u578bRFDiffusionAA\u76f8\u5ab2\u7f8e\uff0c\u540c\u65f6\u4e0d\u9700\u8981\u7ed3\u5408\u7684\u914d\u4f53\u7ed3\u6784\u3002\u4f5c\u4e3a\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0cAtomFlow\u672a\u6765\u6709\u6f5c\u529b\u5e94\u7528\u4e8e\u5404\u79cd\u751f\u7269\u5206\u5b50\u751f\u6210\u4efb\u52a1\u3002|\n", "2409.12024": "|**2024-09-18**|[LEMON: Localized Editing with Mesh Optimization and Neural Shaders](http://arxiv.org/abs/2409.12024)|null|In practical use cases, polygonal mesh editing can be faster than generating new ones, but it can still be challenging and time-consuming for users. Existing solutions for this problem tend to focus on a single task, either geometry or novel view synthesis, which often leads to disjointed results between the mesh and view. In this work, we propose LEMON, a mesh editing pipeline that combines neural deferred shading with localized mesh optimization. Our approach begins by identifying the most important vertices in the mesh for editing, utilizing a segmentation model to focus on these key regions. Given multi-view images of an object, we optimize a neural shader and a polygonal mesh while extracting the normal map and the rendered image from each view. By using these outputs as conditioning data, we edit the input images with a text-to-image diffusion model and iteratively update our dataset while deforming the mesh. This process results in a polygonal mesh that is edited according to the given text instruction, preserving the geometric characteristics of the initial mesh while focusing on the most significant areas. We evaluate our pipeline using the DTU dataset, demonstrating that it generates finely-edited meshes more rapidly than the current state-of-the-art methods. We include our code and additional results in the supplementary material.|\n", "2409.11920": "|**2024-09-18**|[Generation of Complex 3D Human Motion by Temporal and Spatial Composition of Diffusion Models](http://arxiv.org/abs/2409.11920)|null|\u672c\u6587\u65e8\u5728\u89e3\u51b3\u4e3a\u8bad\u7ec3\u9636\u6bb5\u672a\u89c1\u8fc7\u7684\u52a8\u4f5c\u7c7b\u522b\u751f\u6210\u903c\u771f\u4e09\u7ef4\u4eba\u4f53\u52a8\u4f5c\u7684\u6311\u6218\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6d89\u53ca\u5229\u7528 GPT \u6a21\u578b\u4e2d\u5305\u542b\u7684\u4eba\u4f53\u8fd0\u52a8\u77e5\u8bc6\uff0c\u5c06\u590d\u6742\u52a8\u4f5c\u5206\u89e3\u6210\u66f4\u7b80\u5355\u7684\u52a8\u4f5c\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u5728\u8bad\u7ec3\u671f\u95f4\u89c2\u5bdf\u5230\u7684\u52a8\u4f5c\u3002\u7136\u540e\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u7684\u7279\u6027\u5c06\u8fd9\u4e9b\u66f4\u7b80\u5355\u7684\u52a8\u4f5c\u7ec4\u5408\u6210\u5355\u4e2a\u903c\u771f\u7684\u52a8\u753b\u3002\u6211\u4eec\u7684\u4e3b\u5f20\u662f\uff0c\u8fd9\u79cd\u7b80\u5355\u52a8\u4f5c\u7684\u5206\u89e3\u548c\u540e\u7eed\u91cd\u7ec4\u53ef\u4ee5\u5408\u6210\u51c6\u786e\u8868\u793a\u590d\u6742\u8f93\u5165\u52a8\u4f5c\u7684\u52a8\u753b\u3002\u6b64\u65b9\u6cd5\u5728\u63a8\u7406\u9636\u6bb5\u8fd0\u884c\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u4efb\u4f55\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u96c6\u6210\uff0c\u4ece\u800c\u80fd\u591f\u5408\u6210\u8bad\u7ec3\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\u7684\u52a8\u4f5c\u7c7b\u522b\u3002\u6211\u4eec\u901a\u8fc7\u5c06\u4e24\u4e2a\u4eba\u4f53\u8fd0\u52a8\u57fa\u51c6\u6570\u636e\u96c6\u5212\u5206\u4e3a\u57fa\u672c\u52a8\u4f5c\u548c\u590d\u6742\u52a8\u4f5c\u6765\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u7136\u540e\u5c06\u5176\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002|\n", "2409.11904": "|**2024-09-18**|[Finding the Subjective Truth: Collecting 2 Million Votes for Comprehensive Gen-AI Model Evaluation](http://arxiv.org/abs/2409.11904)|null|Efficiently evaluating the performance of text-to-image models is difficult as it inherently requires subjective judgment and human preference, making it hard to compare different models and quantify the state of the art. Leveraging Rapidata's technology, we present an efficient annotation framework that sources human feedback from a diverse, global pool of annotators. Our study collected over 2 million annotations across 4,512 images, evaluating four prominent models (DALL-E 3, Flux.1, MidJourney, and Stable Diffusion) on style preference, coherence, and text-to-image alignment. We demonstrate that our approach makes it feasible to comprehensively rank image generation models based on a vast pool of annotators and show that the diverse annotator demographics reflect the world population, significantly decreasing the risk of biases.|\n", "2409.11836": "|**2024-09-18**|[NT-ViT: Neural Transcoding Vision Transformers for EEG-to-fMRI Synthesis](http://arxiv.org/abs/2409.11836)|null|This paper introduces the Neural Transcoding Vision Transformer (\\modelname), a generative model designed to estimate high-resolution functional Magnetic Resonance Imaging (fMRI) samples from simultaneous Electroencephalography (EEG) data. A key feature of \\modelname is its Domain Matching (DM) sub-module which effectively aligns the latent EEG representations with those of fMRI volumes, enhancing the model's accuracy and reliability. Unlike previous methods that tend to struggle with fidelity and reproducibility of images, \\modelname addresses these challenges by ensuring methodological integrity and higher-quality reconstructions which we showcase through extensive evaluation on two benchmark datasets; \\modelname outperforms the current state-of-the-art by a significant margin in both cases, e.g. achieving a $10\\times$ reduction in RMSE and a $3.14\\times$ increase in SSIM on the Oddball dataset. An ablation study also provides insights into the contribution of each component to the model's overall effectiveness. This development is critical in offering a new approach to lessen the time and financial constraints typically linked with high-resolution brain imaging, thereby aiding in the swift and precise diagnosis of neurological disorders. Although it is not a replacement for actual fMRI but rather a step towards making such imaging more accessible, we believe that it represents a pivotal advancement in clinical practice and neuroscience research. Code is available at \\url{https://github.com/rom42pla/ntvit}.|\n", "2409.11835": "|**2024-09-18**|[DPI-TTS: Directional Patch Interaction for Fast-Converging and Style Temporal Modeling in Text-to-Speech](http://arxiv.org/abs/2409.11835)|null|\u8fd1\u5e74\u6765\uff0c\u8bed\u97f3\u6269\u6563\u6a21\u578b\u53d1\u5c55\u8fc5\u901f\u3002\u9664\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684 U-Net \u67b6\u6784\u5916\uff0c\u57fa\u4e8e Transformer \u7684\u6a21\u578b\uff0c\u4f8b\u5982 Diffusion Transformer (DiT)\uff0c\u4e5f\u53d7\u5230\u4e86\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684 DiT \u8bed\u97f3\u6a21\u578b\u5c06\u6885\u5c14\u9891\u8c31\u56fe\u89c6\u4e3a\u4e00\u822c\u56fe\u50cf\uff0c\u5ffd\u7565\u4e86\u8bed\u97f3\u7279\u5b9a\u7684\u58f0\u5b66\u7279\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u9762\u5411\u6587\u672c\u5230\u8bed\u97f3\u7684\u5b9a\u5411\u8865\u4e01\u4ea4\u4e92\u201d\uff08DPI-TTS\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5efa\u7acb\u5728 DiT \u7684\u57fa\u7840\u4e0a\uff0c\u5728\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u5feb\u901f\u8bad\u7ec3\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cDPI-TTS \u91c7\u7528\u4e86\u4e00\u79cd\u4ece\u4f4e\u9891\u5230\u9ad8\u9891\u3001\u9010\u5e27\u6e10\u8fdb\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u66f4\u7b26\u5408\u58f0\u5b66\u7279\u6027\uff0c\u589e\u5f3a\u4e86\u751f\u6210\u8bed\u97f3\u7684\u81ea\u7136\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u98ce\u683c\u65f6\u95f4\u5efa\u6a21\u65b9\u6cd5\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u8bf4\u8bdd\u4eba\u98ce\u683c\u7684\u76f8\u4f3c\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u8bad\u7ec3\u901f\u5ea6\u63d0\u9ad8\u4e86\u8fd1 2 \u500d\uff0c\u5e76\u4e14\u660e\u663e\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002|\n", "2409.11831": "|**2024-09-18**|[RaggeDi: Diffusion-based State Estimation of Disordered Rags, Sheets, Towels and Blankets](http://arxiv.org/abs/2409.11831)|null|Cloth state estimation is an important problem in robotics. It is essential for the robot to know the accurate state to manipulate cloth and execute tasks such as robotic dressing, stitching, and covering/uncovering human beings. However, estimating cloth state accurately remains challenging due to its high flexibility and self-occlusion. This paper proposes a diffusion model-based pipeline that formulates the cloth state estimation as an image generation problem by representing the cloth state as an RGB image that describes the point-wise translation (translation map) between a pre-defined flattened mesh and the deformed mesh in a canonical space. Then we train a conditional diffusion-based image generation model to predict the translation map based on an observation. Experiments are conducted in both simulation and the real world to validate the performance of our method. Results indicate that our method outperforms two recent methods in both accuracy and speed.|\n"}, "LLM": {"2409.01909": "|**2024-09-03**|[LUK: Empowering Log Understanding with Expert Knowledge from Large Language Models](http://arxiv.org/abs/2409.01909)|**[link](https://github.com/LeaperOvO/LUK)**|Logs play a critical role in providing essential information for system monitoring and troubleshooting. Recently, with the success of pre-trained language models (PLMs) and large language models (LLMs) in natural language processing (NLP), smaller PLMs (such as BERT) and LLMs (like ChatGPT) have become the current mainstream approaches for log analysis. While LLMs possess rich knowledge, their high computational costs and unstable performance make LLMs impractical for analyzing logs directly. In contrast, smaller PLMs can be fine-tuned for specific tasks even with limited computational resources, making them more practical. However, these smaller PLMs face challenges in understanding logs comprehensively due to their limited expert knowledge. To better utilize the knowledge embedded within LLMs for log understanding, this paper introduces a novel knowledge enhancement framework, called LUK, which acquires expert knowledge from LLMs to empower log understanding on a smaller PLM. Specifically, we design a multi-expert collaboration framework based on LLMs consisting of different roles to acquire expert knowledge. In addition, we propose two novel pre-training tasks to enhance the log pre-training with expert knowledge. LUK achieves state-of-the-art results on different log analysis tasks and extensive experiments demonstrate expert knowledge from LLMs can be utilized more effectively to understand logs.|\n", "2409.00702": "|**2024-09-04**|[MARS: Matching Attribute-aware Representations for Text-based Sequential Recommendation](http://arxiv.org/abs/2409.00702)|null|Sequential recommendation aims to predict the next item a user is likely to prefer based on their sequential interaction history. Recently, text-based sequential recommendation has emerged as a promising paradigm that uses pre-trained language models to exploit textual item features to enhance performance and facilitate knowledge transfer to unseen datasets. However, existing text-based recommender models still struggle with two key challenges: (i) representing users and items with multiple attributes, and (ii) matching items with complex user interests. To address these challenges, we propose a novel model, Matching Attribute-aware Representations for Text-based Sequential Recommendation (MARS). MARS extracts detailed user and item representations through attribute-aware text encoding, capturing diverse user intents with multiple attribute-aware representations. It then computes user-item scores via attribute-wise interaction matching, effectively capturing attribute-level user preferences. Our extensive experiments demonstrate that MARS significantly outperforms existing sequential models, achieving improvements of up to 24.43% and 29.26% in Recall@10 and NDCG@10 across five benchmark datasets. Code is available at https://github.com/junieberry/MARS|\n", "2409.00323": "|**2024-08-31**|[From Prediction to Application: Language Model-based Code Knowledge Tracing with Domain Adaptive Pre-Training and Automatic Feedback System with Pedagogical Prompting for Comprehensive Programming Education](http://arxiv.org/abs/2409.00323)|null|Knowledge Tracing (KT) is a critical component in online learning, but traditional approaches face limitations in interpretability and cross-domain adaptability. This paper introduces Language Model-based Code Knowledge Tracing (CodeLKT), an innovative application of Language model-based Knowledge Tracing (LKT) to programming education. CodeLKT leverages pre-trained language models to process learning data, demonstrating superior performance over existing KT and Code KT models. We explore Domain Adaptive Pre-Training (DAPT) and Task Adaptive Pre-Training (TAPT), showing enhanced performance in the coding domain and investigating cross-domain transfer between mathematics and coding. Additionally, we present an theoretically-informed integrated system combining CodeLKT with large language models to generate personalized, in-depth feedback to support students' programming learning. This work advances the field of Code Knowledge Tracing by expanding the knowledge base with language model-based approach and offering practical implications for programming education through data-informed feedback.|\n", "2408.17354": "|**2024-08-30**|[Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage](http://arxiv.org/abs/2408.17354)|null|Fine-tuning large language models on private data for downstream applications poses significant privacy risks in potentially exposing sensitive information. Several popular community platforms now offer convenient distribution of a large variety of pre-trained models, allowing anyone to publish without rigorous verification. This scenario creates a privacy threat, as pre-trained models can be intentionally crafted to compromise the privacy of fine-tuning datasets. In this study, we introduce a novel poisoning technique that uses model-unlearning as an attack tool. This approach manipulates a pre-trained language model to increase the leakage of private data during the fine-tuning process. Our method enhances both membership inference and data extraction attacks while preserving model utility. Experimental results across different models, datasets, and fine-tuning setups demonstrate that our attacks significantly surpass baseline performance. This work serves as a cautionary note for users who download pre-trained models from unverified sources, highlighting the potential risks involved.|\n", "2408.14505": "|**2024-08-24**|[Empowering Pre-Trained Language Models for Spatio-Temporal Forecasting via Decoupling Enhanced Discrete Reprogramming](http://arxiv.org/abs/2408.14505)|null|Spatio-temporal time series forecasting plays a critical role in various real-world applications, such as transportation optimization, energy management, and climate analysis. The recent advancements in Pre-trained Language Models (PLMs) have inspired efforts to reprogram these models for time series forecasting tasks, by leveraging their superior reasoning and generalization capabilities. However, existing approaches fall short in handling complex spatial inter-series dependencies and intrinsic intra-series frequency components, limiting their spatio-temporal forecasting performance. Moreover, the linear mapping of continuous time series to a compressed subset vocabulary in reprogramming constrains the spatio-temporal semantic expressivity of PLMs and may lead to potential information bottleneck. To overcome the above limitations, we propose \\textsc{RePST}, a tailored PLM reprogramming framework for spatio-temporal forecasting. The key insight of \\textsc{RePST} is to decouple the spatio-temporal dynamics in the frequency domain, allowing better alignment with the PLM text space. Specifically, we first decouple spatio-temporal data in Fourier space and devise a structural diffusion operator to obtain temporal intrinsic and spatial diffusion signals, making the dynamics more comprehensible and predictable for PLMs. To avoid information bottleneck from a limited vocabulary, we further propose a discrete reprogramming strategy that selects relevant discrete textual information from an expanded vocabulary space in a differentiable manner. Extensive experiments on four real-world datasets show that our proposed approach significantly outperforms state-of-the-art spatio-temporal forecasting models, particularly in data-scarce scenarios.|\n", "2408.13040": "|**2024-08-23**|[SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks](http://arxiv.org/abs/2408.13040)|null|Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both storage and computation. Additionally, prompting modifies only the LM's inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. This significantly reduces the need for human labor in designing task-specific models. These advantages become even more evident as the number of tasks served by the LM scales up. Motivated by the strengths of prompting, we are the first to explore the potential of prompting speech LMs in the domain of speech processing. Recently, there has been a growing interest in converting speech into discrete units for language modeling. Our pioneer research demonstrates that these quantized speech units are highly versatile within our unified prompting framework. Not only can they serve as class labels, but they also contain rich phonetic information that can be re-synthesized back into speech signals for speech generation tasks. Specifically, we reformulate speech processing tasks into speech-to-unit generation tasks. As a result, we can seamlessly integrate tasks such as speech classification, sequence generation, and speech generation within a single, unified prompting framework. The experiment results show that the prompting method can achieve competitive performance compared to the strong fine-tuning method based on self-supervised learning models with a similar number of trainable parameters. The prompting method also shows promising results in the few-shot setting. Moreover, with the advanced speech LMs coming into the stage, the proposed prompting framework attains great potential.||\n", "2408.12779": "|**2024-08-23**|[Investigating LLM Applications in E-Commerce](http://arxiv.org/abs/2408.12779)|null|The emergence of Large Language Models (LLMs) has revolutionized natural language processing in various applications especially in e-commerce. One crucial step before the application of such LLMs in these fields is to understand and compare the performance in different use cases in such tasks. This paper explored the efficacy of LLMs in the e-commerce domain, focusing on instruction-tuning an open source LLM model with public e-commerce datasets of varying sizes and comparing the performance with the conventional models prevalent in industrial applications. We conducted a comprehensive comparison between LLMs and traditional pre-trained language models across specific tasks intrinsic to the e-commerce domain, namely classification, generation, summarization, and named entity recognition (NER). Furthermore, we examined the effectiveness of the current niche industrial application of very large LLM, using in-context learning, in e-commerce specific tasks. Our findings indicate that few-shot inference with very large LLMs often does not outperform fine-tuning smaller pre-trained models, underscoring the importance of task-specific model optimization.Additionally, we investigated different training methodologies such as single-task training, mixed-task training, and LoRA merging both within domain/tasks and between different tasks. Through rigorous experimentation and analysis, this paper offers valuable insights into the potential effectiveness of LLMs to advance natural language processing capabilities within the e-commerce industry.||\n", "2408.12125": "|**2024-08-22**|[AutoTest: Evolutionary Code Solution Selection with Test Cases](http://arxiv.org/abs/2408.12125)|null|\u968f\u7740\u4ee3\u7801\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4ece\u591a\u4e2a\u5019\u9009\u65b9\u6848\u4e2d\u9009\u62e9\u6b63\u786e\u7684\u4ee3\u7801\u65b9\u6848\u5df2\u6210\u4e3a\u4e00\u9879\u81f3\u5173\u91cd\u8981\u7684\u4efb\u52a1\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAutoTest\u7684\u65b0\u6280\u672f\uff0c\u8be5\u6280\u672f\u5c06\u81ea\u52a8\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u4e0e\u4ee3\u7801\u65b9\u6848\u6267\u884c\u76f8\u7ed3\u5408\uff0c\u5229\u7528\u8fdb\u5316\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u9009\u62e9\u8fc7\u7a0b\u3002\u9996\u5148\uff0cAutoTest\u5229\u7528\u8bf8\u5982codegen-16B\u3001code-davinci-002\u548cincoder-6B\u7b49\u5927\u578b\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u6765\u63d0\u4f9b\u4ee3\u7801\u65b9\u6848\u53ca\u5176\u76f8\u5e94\u7684\u6d4b\u8bd5\u7528\u4f8b\u3002\u7136\u540e\uff0c\u901a\u8fc7\u6267\u884c\u4ee3\u7801\u65b9\u6848\u5e76\u8bc4\u4f30\u5176\u5728\u6d4b\u8bd5\u7528\u4f8b\u4e0a\u7684\u6027\u80fd\uff0c\u5f62\u6210\u5171\u8bc6\u96c6\u3002\u57fa\u4e8e\u8fdb\u5316\u9057\u4f20\u7b97\u6cd5\u7684\u9009\u62e9\u3001\u53d8\u5f02\u548c\u4ea4\u53c9\u673a\u5236\uff0c\u901a\u8fc7\u8c03\u6574alpha\u548cbeta\u53c2\u6570\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6392\u540d\u3002\u6700\u540e\uff0c\u9009\u62e9\u6700\u4f73\u4ee3\u7801\u65b9\u6848\u3002AutoTest\u5728HumanEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002HumanEval\u6570\u636e\u96c6\u5305\u542b164\u4e2a\u7f16\u7a0b\u95ee\u9898\uff0cAutoTest\u5728pass@1\u5206\u6570\u65b9\u9762\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e86\u7ea610%\u3002||\n", "2408.11319": "|**2024-08-24**|[SarcasmBench: Towards Evaluating Large Language Models on Sarcasm Understanding](http://arxiv.org/abs/2408.11319)|null|In the era of large language models (LLMs), the task of ``System I''~-~the fast, unconscious, and intuitive tasks, e.g., sentiment analysis, text classification, etc., have been argued to be successfully solved. However, sarcasm, as a subtle linguistic phenomenon, often employs rhetorical devices like hyperbole and figuration to convey true sentiments and intentions, involving a higher level of abstraction than sentiment analysis. There is growing concern that the argument about LLMs' success may not be fully tenable when considering sarcasm understanding. To address this question, we select eleven SOTA LLMs and eight SOTA pre-trained language models (PLMs) and present comprehensive evaluations on six widely used benchmark datasets through different prompting approaches, i.e., zero-shot input/output (IO) prompting, few-shot IO prompting, chain of thought (CoT) prompting. Our results highlight three key findings: (1) current LLMs underperform supervised PLMs based sarcasm detection baselines across six sarcasm benchmarks. This suggests that significant efforts are still required to improve LLMs' understanding of human sarcasm. (2) GPT-4 consistently and significantly outperforms other LLMs across various prompting methods, with an average improvement of 14.0\\%$\\uparrow$. Claude 3 and ChatGPT demonstrate the next best performance after GPT-4. (3) Few-shot IO prompting method outperforms the other two methods: zero-shot IO and few-shot CoT. The reason is that sarcasm detection, being a holistic, intuitive, and non-rational cognitive process, is argued not to adhere to step-by-step logical reasoning, making CoT less effective in understanding sarcasm compared to its effectiveness in mathematical reasoning tasks.||\n", "2408.10548": "|**2024-08-20**|[Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution](http://arxiv.org/abs/2408.10548)|**[link](https://github.com/lanxiang1017/language-modeling-on-tabular-data-survey)**|Tabular data, a prevalent data type across various domains, presents unique challenges due to its heterogeneous nature and complex structural relationships. Achieving high predictive performance and robustness in tabular data analysis holds significant promise for numerous applications. Influenced by recent advancements in natural language processing, particularly transformer architectures, new methods for tabular data modeling have emerged. Early techniques concentrated on pre-training transformers from scratch, often encountering scalability issues. Subsequently, methods leveraging pre-trained language models like BERT have been developed, which require less data and yield enhanced performance. The recent advent of large language models, such as GPT and LLaMA, has further revolutionized the field, facilitating more advanced and diverse applications with minimal fine-tuning. Despite the growing interest, a comprehensive survey of language modeling techniques for tabular data remains absent. This paper fills this gap by providing a systematic review of the development of language modeling for tabular data, encompassing: (1) a categorization of different tabular data structures and data types; (2) a review of key datasets used in model training and tasks used for evaluation; (3) a summary of modeling techniques including widely-adopted data processing methods, popular architectures, and training objectives; (4) the evolution from adapting traditional Pre-training/Pre-trained language models to the utilization of large language models; (5) an identification of persistent challenges and potential future research directions in language modeling for tabular data analysis. GitHub page associated with this survey is available at: https://github.com/lanxiang1017/Language-Modeling-on-Tabular-Data-Survey.git.||\n", "2409.05197": "|**2024-09-08**|[Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?](http://arxiv.org/abs/2409.05197)|**[link](https://github.com/zawedcvg/are-large-language-models-attentive-readers)**|State-of-the-art Large Language Models (LLMs) are accredited with an increasing number of different capabilities, ranging from reading comprehension, over advanced mathematical and reasoning skills to possessing scientific knowledge. In this paper we focus on their multi-hop reasoning capability: the ability to identify and integrate information from multiple textual sources.   Given the concerns with the presence of simplifying cues in existing multi-hop reasoning benchmarks, which allow models to circumvent the reasoning requirement, we set out to investigate, whether LLMs are prone to exploiting such simplifying cues. We find evidence that they indeed circumvent the requirement to perform multi-hop reasoning, but they do so in more subtle ways than what was reported about their fine-tuned pre-trained language model (PLM) predecessors. Motivated by this finding, we propose a challenging multi-hop reasoning benchmark, by generating seemingly plausible multi-hop reasoning chains, which ultimately lead to incorrect answers. We evaluate multiple open and proprietary state-of-the-art LLMs, and find that their performance to perform multi-hop reasoning is affected, as indicated by up to 45% relative decrease in F1 score when presented with such seemingly plausible alternatives. We conduct a deeper analysis and find evidence that while LLMs tend to ignore misleading lexical cues, misleading reasoning paths indeed present a significant challenge.|\n", "2409.03773": "|**2024-08-21**|[CoPRA: Bridging Cross-domain Pretrained Sequence Models with Complex Structures for Protein-RNA Binding Affinity Prediction](http://arxiv.org/abs/2409.03773)|null|\u51c6\u786e\u6d4b\u91cf\u86cb\u767d\u8d28-RNA\u7ed3\u5408\u4eb2\u548c\u529b\u5728\u8bb8\u591a\u751f\u7269\u8fc7\u7a0b\u548c\u836f\u7269\u8bbe\u8ba1\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u4ee5\u524d\u7684\u86cb\u767d\u8d28-RNA\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u8ba1\u7b97\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5e8f\u5217\u6216\u7ed3\u6784\u7279\u5f81\uff0c\u65e0\u6cd5\u5168\u9762\u6355\u6349\u7ed3\u5408\u673a\u5236\u3002\u6700\u8fd1\u51fa\u73b0\u7684\u5728\u5927\u91cf\u65e0\u76d1\u7763\u86cb\u767d\u8d28\u548cRNA\u5e8f\u5217\u4e0a\u8bad\u7ec3\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u5305\u62ec\u7ed3\u5408\u4f4d\u70b9\u9884\u6d4b\u5728\u5185\u7684\u5404\u79cd\u57df\u5185\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8868\u793a\u80fd\u529b\u3002\u7136\u800c\uff0c\u534f\u540c\u5e94\u7528\u4e0d\u540c\u9886\u57df\u7684\u8bed\u8a00\u6a21\u578b\u6765\u5b8c\u6210\u590d\u6742\u7ea7\u522b\u7684\u4efb\u52a1\u4ecd\u672a\u5f97\u5230\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86CoPRA\uff0c\u901a\u8fc7\u86cb\u767d\u8d28-RNA\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u7684\u590d\u5408\u7269\u7ed3\u6784\uff0c\u5c06\u6765\u81ea\u4e0d\u540c\u751f\u7269\u9886\u57df\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8fde\u63a5\u8d77\u6765\u3002\u6211\u4eec\u9996\u6b21\u8bc1\u660e\u4e86\u8de8\u751f\u7269\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u534f\u540c\u63d0\u9ad8\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2aCo-Former\u6765\u7ed3\u5408\u8de8\u6a21\u6001\u5e8f\u5217\u548c\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8303\u56f4\u9884\u8bad\u7ec3\u7b56\u7565\u6765\u63d0\u9ad8Co-Former\u7684\u4ea4\u4e92\u7406\u89e3\u80fd\u529b\u3002\u540c\u65f6\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u6700\u5927\u7684\u86cb\u767d\u8d28-RNA\u7ed3\u5408\u4eb2\u548c\u529b\u6570\u636e\u96c6PRA310\u7528\u4e8e\u6027\u80fd\u8bc4\u4f30\u3002\u6211\u4eec\u8fd8\u5728\u4e00\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e86\u6211\u4eec\u6a21\u578b\u7684\u7a81\u53d8\u6548\u5e94\u9884\u6d4b\u80fd\u529b\u3002CoPRA\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u7684\u5206\u6790\uff0c\u5e76\u9a8c\u8bc1\u4e86CoPRA\u53ef\u4ee5\uff081\uff09\u51c6\u786e\u9884\u6d4b\u86cb\u767d\u8d28-RNA\u7ed3\u5408\u4eb2\u548c\u529b\uff1b\uff082\uff09\u7406\u89e3\u7531\u7a81\u53d8\u5f15\u8d77\u7684\u7ed3\u5408\u4eb2\u548c\u529b\u53d8\u5316\uff1b\uff083\uff09\u53d7\u76ca\u4e8e\u6570\u636e\u548c\u6a21\u578b\u89c4\u6a21\u7684\u6269\u5927\u3002||\n", "2409.06622": "|**2024-09-10**|[Exploring Italian sentence embeddings properties through multi-tasking](http://arxiv.org/abs/2409.06622)|null|We investigate to what degree existing LLMs encode abstract linguistic information in Italian in a multi-task setting. We exploit curated synthetic data on a large scale -- several Blackbird Language Matrices (BLMs) problems in Italian -- and use them to study how sentence representations built using pre-trained language models encode specific syntactic and semantic information. We use a two-level architecture to model separately a compression of the sentence embeddings into a representation that contains relevant information for a task, and a BLM task. We then investigate whether we can obtain compressed sentence representations that encode syntactic and semantic information relevant to several BLM tasks. While we expected that the sentence structure -- in terms of sequence of phrases/chunks -- and chunk properties could be shared across tasks, performance and error analysis show that the clues for the different tasks are encoded in different manners in the sentence embeddings, suggesting that abstract linguistic notions such as constituents or thematic roles does not seem to be present in the pretrained sentence embeddings.|\n", "2409.05997": "|**2024-09-09**|[TransformerRanker: A Tool for Efficiently Finding the Best-Suited Language Models for Downstream Classification Tasks](http://arxiv.org/abs/2409.05997)|**[link](https://github.com/flairnlp/transformer-ranker)**|Classification tasks in NLP are typically addressed by selecting a pre-trained language model (PLM) from a model hub, and fine-tuning it for the task at hand. However, given the very large number of PLMs that are currently available, a practical challenge is to determine which of them will perform best for a specific downstream task. With this paper, we introduce TransformerRanker, a lightweight library that efficiently ranks PLMs for classification tasks without the need for computationally costly fine-tuning. Our library implements current approaches for transferability estimation (LogME, H-Score, kNN), in combination with layer aggregation options, which we empirically showed to yield state-of-the-art rankings of PLMs (Garbas et al., 2024). We designed the interface to be lightweight and easy to use, allowing users to directly connect to the HuggingFace Transformers and Dataset libraries. Users need only select a downstream classification task and a list of PLMs to create a ranking of likely best-suited PLMs for their task. We make TransformerRanker available as a pip-installable open-source library https://github.com/flairNLP/transformer-ranker.|\n", "2409.08185": "|**2024-09-12**|[Fine-tuning Large Language Models for Entity Matching](http://arxiv.org/abs/2409.08185)|**[link](https://github.com/wbsg-uni-mannheim/tailormatch)**|Generative large language models (LLMs) are a promising alternative to pre-trained language models for entity matching due to their high zero-shot performance and their ability to generalize to unseen entities. Existing research on using LLMs for entity matching has focused on prompt engineering and in-context learning. This paper explores the potential of fine-tuning LLMs for entity matching. We analyze fine-tuning along two dimensions: 1) The representation of training examples, where we experiment with adding different types of LLM-generated explanations to the training set, and 2) the selection and generation of training examples using LLMs. In addition to the matching performance on the source dataset, we investigate how fine-tuning affects the model's ability to generalize to other in-domain datasets as well as across topical domains. Our experiments show that fine-tuning significantly improves the performance of the smaller models while the results for the larger models are mixed. Fine-tuning also improves the generalization to in-domain datasets while hurting cross-domain transfer. We show that adding structured explanations to the training set has a positive impact on the performance of three out of four LLMs, while the proposed example selection and generation methods only improve the performance of Llama 3.1 8B while decreasing the performance of GPT-4o Mini.|\n", "2409.08406": "|**2024-09-12**|[Knowledge Tagging with Large Language Model based Multi-Agent System](http://arxiv.org/abs/2409.08406)|null|Knowledge tagging for questions is vital in modern intelligent educational applications, including learning progress diagnosis, practice question recommendations, and course content organization. Traditionally, these annotations have been performed by pedagogical experts, as the task demands not only a deep semantic understanding of question stems and knowledge definitions but also a strong ability to link problem-solving logic with relevant knowledge concepts. With the advent of advanced natural language processing (NLP) algorithms, such as pre-trained language models and large language models (LLMs), pioneering studies have explored automating the knowledge tagging process using various machine learning models. In this paper, we investigate the use of a multi-agent system to address the limitations of previous algorithms, particularly in handling complex cases involving intricate knowledge definitions and strict numerical constraints. By demonstrating its superior performance on the publicly available math question knowledge tagging dataset, MathKnowCT, we highlight the significant potential of an LLM-based multi-agent system in overcoming the challenges that previous methods have encountered. Finally, through an in-depth discussion of the implications of automating knowledge tagging, we underscore the promising results of deploying LLM-based algorithms in educational contexts.|\n", "2409.10695": "|**2024-09-16**|[Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large Language Models](http://arxiv.org/abs/2409.10695)|null|We introduce Playground v3 (PGv3), our latest text-to-image model that achieves state-of-the-art (SoTA) performance across multiple testing benchmarks, excels in graphic design abilities and introduces new capabilities. Unlike traditional text-to-image generative models that rely on pre-trained language models like T5 or CLIP text encoders, our approach fully integrates Large Language Models (LLMs) with a novel structure that leverages text conditions exclusively from a decoder-only LLM. Additionally, to enhance image captioning quality-we developed an in-house captioner, capable of generating captions with varying levels of detail, enriching the diversity of text structures. We also introduce a new benchmark CapsBench to evaluate detailed image captioning performance. Experimental results demonstrate that PGv3 excels in text prompt adherence, complex reasoning, and accurate text rendering. User preference studies indicate the super-human graphic design ability of our model for common design applications, such as stickers, posters, and logo designs. Furthermore, PGv3 introduces new capabilities, including precise RGB color control and robust multilingual understanding.|\n", "2409.09501": "|**2024-09-14**|[Synthetic4Health: Generating Annotated Synthetic Clinical Letters](http://arxiv.org/abs/2409.09501)|null|Since clinical letters contain sensitive information, clinical-related datasets can not be widely applied in model training, medical research, and teaching. This work aims to generate reliable, various, and de-identified synthetic clinical letters. To achieve this goal, we explored different pre-trained language models (PLMs) for masking and generating text. After that, we worked on Bio\\_ClinicalBERT, a high-performing model, and experimented with different masking strategies. Both qualitative and quantitative methods were used for evaluation. Additionally, a downstream task, Named Entity Recognition (NER), was also implemented to assess the usability of these synthetic letters.   The results indicate that 1) encoder-only models outperform encoder-decoder models. 2) Among encoder-only models, those trained on general corpora perform comparably to those trained on clinical data when clinical information is preserved. 3) Additionally, preserving clinical entities and document structure better aligns with our objectives than simply fine-tuning the model. 4) Furthermore, different masking strategies can impact the quality of synthetic clinical letters. Masking stopwords has a positive impact, while masking nouns or verbs has a negative effect. 5) For evaluation, BERTScore should be the primary quantitative evaluation metric, with other metrics serving as supplementary references. 6) Contextual information does not significantly impact the models' understanding, so the synthetic clinical letters have the potential to replace the original ones in downstream tasks.|\n", "2409.10570": "|**2024-09-14**|[Protecting Copyright of Medical Pre-trained Language Models: Training-Free Backdoor Watermarking](http://arxiv.org/abs/2409.10570)|null|Pre-training language models followed by fine-tuning on specific tasks is standard in NLP, but traditional models often underperform when applied to the medical domain, leading to the development of specialized medical pre-trained language models (Med-PLMs). These models are valuable assets but are vulnerable to misuse and theft, requiring copyright protection. However, no existing watermarking methods are tailored for Med-PLMs, and adapting general PLMs watermarking techniques to the medical domain faces challenges such as task incompatibility, loss of fidelity, and inefficiency. To address these issues, we propose the first training-free backdoor watermarking method for Med-PLMs. Our method uses rare special symbols as trigger words, which do not impact downstream task performance, embedding watermarks by replacing their original embeddings with those of specific medical terms in the Med-PLMs' word embeddings layer. After fine-tuning the watermarked Med-PLMs on various medical downstream tasks, the final models (FMs) respond to the trigger words in the same way they would to the corresponding medical terms. This property can be utilized to extract the watermark. Experiments demonstrate that our method achieves high fidelity while effectively extracting watermarks across various medical downstream tasks. Additionally, our method demonstrates robustness against various attacks and significantly enhances the efficiency of watermark embedding, reducing the embedding time from 10 hours to 10 seconds.|\n"}, "Transformer": {"2409.02727": "|**2024-09-05**|[Pooling And Attention: What Are Effective Designs For LLM-Based Embedding Models?](http://arxiv.org/abs/2409.02727)|**[link](https://github.com/yixuantt/poolingandattn)**|The significant advancements of Large Language Models (LLMs) in generative tasks have led to a growing body of work exploring LLM-based embedding models. While these models, employing different pooling and attention strategies, have achieved state-of-the-art performance on public embedding benchmarks, questions still arise about what constitutes an effective design for LLM-based embedding models. However, these models are often trained on different datasets, using different LLM base models or training settings. Moreover, evaluations on public embedding benchmarks often fail to report statistical significance, making it difficult to determine which designs truly contribute to final performance. This complicates the process for practitioners seeking optimal training recipes for LLM-based embedding models. In this study, we conduct a large-scale experiment by training a series of LLM-based embedding models using the same training data and base model but differing in their pooling and attention strategies. The results show that there is no one-size-fits-all solution: while bidirectional attention and an additional trainable pooling layer outperform in text similarity and information retrieval tasks, they do not significantly surpass simpler designs like EOS-last token pooling and default causal attention in clustering and classification tasks. Furthermore, we propose a new pooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs of all hidden layers, rather than just the last layer, using a cross-attention network. This method proves to be statistically superior in text similarity and retrieval tasks compared to existing pooling methods. Overall, this paper sheds light on effective training strategies for LLM-based embedding models.||\n", "2409.02545": "|**2024-09-04**|[UniTT-Stereo: Unified Training of Transformer for Enhanced Stereo Matching](http://arxiv.org/abs/2409.02545)|null|Unlike other vision tasks where Transformer-based approaches are becoming increasingly common, stereo depth estimation is still dominated by convolution-based approaches. This is mainly due to the limited availability of real-world ground truth for stereo matching, which is a limiting factor in improving the performance of Transformer-based stereo approaches. In this paper, we propose UniTT-Stereo, a method to maximize the potential of Transformer-based stereo architectures by unifying self-supervised learning used for pre-training with stereo matching framework based on supervised learning. To be specific, we explore the effectiveness of reconstructing features of masked portions in an input image and at the same time predicting corresponding points in another image from the perspective of locality inductive bias, which is crucial in training models with limited training data. Moreover, to address these challenging tasks of reconstruction-and-prediction, we present a new strategy to vary a masking ratio when training the stereo model with stereo-tailored losses. State-of-the-art performance of UniTT-Stereo is validated on various benchmarks such as ETH3D, KITTI 2012, and KITTI 2015 datasets. Lastly, to investigate the advantages of the proposed approach, we provide a frequency analysis of feature maps and the analysis of locality inductive bias based on attention maps.||\n", "2409.02056": "|**2024-09-03**|[F2former: When Fractional Fourier Meets Deep Wiener Deconvolution and Selective Frequency Transformer for Image Deblurring](http://arxiv.org/abs/2409.02056)|null|Recent progress in image deblurring techniques focuses mainly on operating in both frequency and spatial domains using the Fourier transform (FT) properties. However, their performance is limited due to the dependency of FT on stationary signals and its lack of capability to extract spatial-frequency properties. In this paper, we propose a novel approach based on the Fractional Fourier Transform (FRFT), a unified spatial-frequency representation leveraging both spatial and frequency components simultaneously, making it ideal for processing non-stationary signals like images. Specifically, we introduce a Fractional Fourier Transformer (F2former), where we combine the classical fractional Fourier based Wiener deconvolution (F2WD) as well as a multi-branch encoder-decoder transformer based on a new fractional frequency aware transformer block (F2TB). We design F2TB consisting of a fractional frequency aware self-attention (F2SA) to estimate element-wise product attention based on important frequency components and a novel feed-forward network based on frequency division multiplexing (FM-FFN) to refine high and low frequency features separately for efficient latent clear image restoration. Experimental results for the cases of both motion deblurring as well as defocus deblurring show that the performance of our proposed method is superior to other state-of-the-art (SOTA) approaches.||\n", "2409.02018": "|**2024-09-03**|[TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation](http://arxiv.org/abs/2409.02018)|null|In healthcare, medical image segmentation is crucial for accurate disease diagnosis and the development of effective treatment strategies. Early detection can significantly aid in managing diseases and potentially prevent their progression. Machine learning, particularly deep convolutional neural networks, has emerged as a promising approach to addressing segmentation challenges. Traditional methods like U-Net use encoding blocks for local representation modeling and decoding blocks to uncover semantic relationships. However, these models often struggle with multi-scale objects exhibiting significant variations in texture and shape, and they frequently fail to capture long-range dependencies in the input data. Transformers designed for sequence-to-sequence predictions have been proposed as alternatives, utilizing global self-attention mechanisms. Yet, they can sometimes lack precise localization due to insufficient granular details. To overcome these limitations, we introduce TransDAE: a novel approach that reimagines the self-attention mechanism to include both spatial and channel-wise associations across the entire feature space, while maintaining computational efficiency. Additionally, TransDAE enhances the skip connection pathway with an inter-scale interaction module, promoting feature reuse and improving localization accuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on the Synaps multi-organ dataset, even without relying on pre-trained weights.||\n", "2409.01557": "|**2024-09-03**|[TASL-Net: Tri-Attention Selective Learning Network for Intelligent Diagnosis of Bimodal Ultrasound Video](http://arxiv.org/abs/2409.01557)|null|In the intelligent diagnosis of bimodal (gray-scale and contrast-enhanced) ultrasound videos, medical domain knowledge such as the way sonographers browse videos, the particular areas they emphasize, and the features they pay special attention to, plays a decisive role in facilitating precise diagnosis. Embedding medical knowledge into the deep learning network can not only enhance performance but also boost clinical confidence and reliability of the network. However, it is an intractable challenge to automatically focus on these person- and disease-specific features in videos and to enable networks to encode bimodal information comprehensively and efficiently. This paper proposes a novel Tri-Attention Selective Learning Network (TASL-Net) to tackle this challenge and automatically embed three types of diagnostic attention of sonographers into a mutual transformer framework for intelligent diagnosis of bimodal ultrasound videos. Firstly, a time-intensity-curve-based video selector is designed to mimic the temporal attention of sonographers, thus removing a large amount of redundant information while improving computational efficiency of TASL-Net. Then, to introduce the spatial attention of the sonographers for contrast-enhanced video analysis, we propose the earliest-enhanced position detector based on structural similarity variation, on which the TASL-Net is made to focus on the differences of perfusion variation inside and outside the lesion. Finally, by proposing a mutual encoding strategy that combines convolution and transformer, TASL-Net possesses bimodal attention to structure features on gray-scale videos and to perfusion variations on contrast-enhanced videos. These modules work collaboratively and contribute to superior performance. We conduct a detailed experimental validation of TASL-Net's performance on three datasets, including lung, breast, and liver.||\n", "2409.01352": "|**2024-09-02**|[Spectron: Target Speaker Extraction using Conditional Transformer with Adversarial Refinement](http://arxiv.org/abs/2409.01352)|null|Recently, attention-based transformers have become a de facto standard in many deep learning applications including natural language processing, computer vision, signal processing, etc.. In this paper, we propose a transformer-based end-to-end model to extract a target speaker's speech from a monaural multi-speaker mixed audio signal. Unlike existing speaker extraction methods, we introduce two additional objectives to impose speaker embedding consistency and waveform encoder invertibility and jointly train both speaker encoder and speech separator to better capture the speaker conditional embedding. Furthermore, we leverage a multi-scale discriminator to refine the perceptual quality of the extracted speech. Our experiments show that the use of a dual path transformer in the separator backbone along with proposed training paradigm improves the CNN baseline by $3.12$ dB points. Finally, we compare our approach with recent state-of-the-arts and show that our model outperforms existing methods by $4.1$ dB points on an average without creating additional data dependency.||\n", "2409.01193": "|**2024-09-02**|[CLIBE: Detecting Dynamic Backdoors in Transformer-based NLP Models](http://arxiv.org/abs/2409.01193)|**[link](https://github.com/raytsang123/clibe)**|Backdoors can be injected into NLP models to induce misbehavior when the input text contains a specific feature, known as a trigger, which the attacker secretly selects. Unlike fixed words, phrases, or sentences used in the static text trigger, NLP dynamic backdoor attacks design triggers associated with abstract and latent text features, making them considerably stealthier than traditional static backdoor attacks. However, existing research on NLP backdoor detection primarily focuses on defending against static backdoor attacks, while detecting dynamic backdoors in NLP models remains largely unexplored. This paper presents CLIBE, the first framework to detect dynamic backdoors in Transformer-based NLP models. CLIBE injects a \"few-shot perturbation\" into the suspect Transformer model by crafting optimized weight perturbation in the attention layers to make the perturbed model classify a limited number of reference samples as a target label. Subsequently, CLIBE leverages the generalization ability of this few-shot perturbation to determine whether the original model contains a dynamic backdoor. Extensive evaluation on three advanced NLP dynamic backdoor attacks, two widely-used Transformer frameworks, and four real-world classification tasks strongly validates the effectiveness of CLIBE. We also demonstrate the robustness of CLIBE against various adaptive attacks. Furthermore, we employ CLIBE to scrutinize 49 popular Transformer models on Hugging Face and discover one exhibiting a high probability of containing a dynamic backdoor. We have contacted Hugging Face and provided detailed evidence of this model's backdoor behavior. Moreover, we extend CLIBE to detect backdoor text generation models modified to exhibit toxic behavior. To the best of our knowledge, CLIBE is the first framework capable of detecting backdoors in text generation models without access to trigger input test samples.||\n", "2409.01068": "|**2024-09-02**|[Progressive Retinal Image Registration via Global and Local Deformable Transformations](http://arxiv.org/abs/2409.01068)|**[link](https://github.com/lyp-deeplearning/awesome-retinal-registration)**|Retinal image registration plays an important role in the ophthalmological diagnosis process. Since there exist variances in viewing angles and anatomical structures across different retinal images, keypoint-based approaches become the mainstream methods for retinal image registration thanks to their robustness and low latency. These methods typically assume the retinal surfaces are planar, and adopt feature matching to obtain the homography matrix that represents the global transformation between images. Yet, such a planar hypothesis inevitably introduces registration errors since retinal surface is approximately curved. This limitation is more prominent when registering image pairs with significant differences in viewing angles. To address this problem, we propose a hybrid registration framework called HybridRetina, which progressively registers retinal images with global and local deformable transformations. For that, we use a keypoint detector and a deformation network called GAMorph to estimate the global transformation and local deformable transformation, respectively. Specifically, we integrate multi-level pixel relation knowledge to guide the training of GAMorph. Additionally, we utilize an edge attention module that includes the geometric priors of the images, ensuring the deformation field focuses more on the vascular regions of clinical interest. Experiments on two widely-used datasets, FIRE and FLoRI21, show that our proposed HybridRetina significantly outperforms some state-of-the-art methods. The code is available at https://github.com/lyp-deeplearning/awesome-retinal-registration.||\n", "2409.00904": "|**2024-09-02**|[Multi-scale Temporal Fusion Transformer for Incomplete Vehicle Trajectory Prediction](http://arxiv.org/abs/2409.00904)|null|Motion prediction plays an essential role in autonomous driving systems, enabling autonomous vehicles to achieve more accurate local-path planning and driving decisions based on predictions of the surrounding vehicles. However, existing methods neglect the potential missing values caused by object occlusion, perception failures, etc., which inevitably degrades the trajectory prediction performance in real traffic scenarios. To address this limitation, we propose a novel end-to-end framework for incomplete vehicle trajectory prediction, named Multi-scale Temporal Fusion Transformer (MTFT), which consists of the Multi-scale Attention Head (MAH) and the Continuity Representation-guided Multi-scale Fusion (CRMF) module. Specifically, the MAH leverages the multi-head attention mechanism to parallelly capture multi-scale motion representation of trajectory from different temporal granularities, thus mitigating the adverse effect of missing values on prediction. Furthermore, the multi-scale motion representation is input into the CRMF module for multi-scale fusion to obtain the robust temporal feature of the vehicle. During the fusion process, the continuity representation of vehicle motion is first extracted across time steps to guide the fusion, ensuring that the resulting temporal feature incorporates both detailed information and the overall trend of vehicle motion, which facilitates the accurate decoding of future trajectory that is consistent with the vehicle's motion trend. We evaluate the proposed model on four datasets derived from highway and urban traffic scenarios. The experimental results demonstrate its superior performance in the incomplete vehicle trajectory prediction task compared with state-of-the-art models, e.g., a comprehensive performance improvement of more than 39% on the HighD dataset.||\n", "2409.00591": "|**2024-09-01**|[Attention-Guided Multi-scale Interaction Network for Face Super-Resolution](http://arxiv.org/abs/2409.00591)|null|Recently, CNN and Transformer hybrid networks demonstrated excellent performance in face super-resolution (FSR) tasks. Since numerous features at different scales in hybrid networks, how to fuse these multi-scale features and promote their complementarity is crucial for enhancing FSR. However, existing hybrid network-based FSR methods ignore this, only simply combining the Transformer and CNN. To address this issue, we propose an attention-guided Multi-scale interaction network (AMINet), which contains local and global feature interactions as well as encoder-decoder phases feature interactions. Specifically, we propose a Local and Global Feature Interaction Module (LGFI) to promote fusions of global features and different receptive fields' local features extracted by our Residual Depth Feature Extraction Module (RDFE). Additionally, we propose a Selective Kernel Attention Fusion Module (SKAF) to adaptively select fusions of different features within LGFI and encoder-decoder phases. Our above design allows the free flow of multi-scale features from within modules and between encoder and decoder, which can promote the complementarity of different scale features to enhance FSR. Comprehensive experiments confirm that our method consistently performs well with less computational consumption and faster inference.||\n", "2409.03621": "|**2024-09-05**|[Attend First, Consolidate Later: On the Importance of Attention in Different LLM Layers](http://arxiv.org/abs/2409.03621)|null|In decoder-based LLMs, the representation of a given layer serves two purposes: as input to the next layer during the computation of the current token; and as input to the attention mechanism of future tokens. In this work, we show that the importance of the latter role might be overestimated. To show that, we start by manipulating the representations of previous tokens; e.g. by replacing the hidden states at some layer k with random vectors. Our experimenting with four LLMs and four tasks show that this operation often leads to small to negligible drop in performance. Importantly, this happens if the manipulation occurs in the top part of the model-k is in the final 30-50% of the layers. In contrast, doing the same manipulation in earlier layers might lead to chance level performance. We continue by switching the hidden state of certain tokens with hidden states of other tokens from another prompt; e.g., replacing the word \"Italy\" with \"France\" in \"What is the capital of Italy?\". We find that when applying this switch in the top 1/3 of the model, the model ignores it (answering \"Rome\"). However if we apply it before, the model conforms to the switch (\"Paris\"). Our results hint at a two stage process in transformer-based LLMs: the first part gathers input from previous tokens, while the second mainly processes that information internally.||\n", "2409.03516": "|**2024-09-05**|[LMLT: Low-to-high Multi-Level Vision Transformer for Image Super-Resolution](http://arxiv.org/abs/2409.03516)|**[link](https://github.com/jwgdmkj/lmlt)**|Recent Vision Transformer (ViT)-based methods for Image Super-Resolution have demonstrated impressive performance. However, they suffer from significant complexity, resulting in high inference times and memory usage. Additionally, ViT models using Window Self-Attention (WSA) face challenges in processing regions outside their windows. To address these issues, we propose the Low-to-high Multi-Level Transformer (LMLT), which employs attention with varying feature sizes for each head. LMLT divides image features along the channel dimension, gradually reduces spatial size for lower heads, and applies self-attention to each head. This approach effectively captures both local and global information. By integrating the results from lower heads into higher heads, LMLT overcomes the window boundary issues in self-attention. Extensive experiments show that our model significantly reduces inference time and GPU memory usage while maintaining or even surpassing the performance of state-of-the-art ViT-based Image Super-Resolution methods. Our codes are availiable at https://github.com/jwgdmkj/LMLT.||\n", "2409.03514": "|**2024-09-05**|[Blended Latent Diffusion under Attention Control for Real-World Video Editing](http://arxiv.org/abs/2409.03514)|null|\u7531\u4e8e\u7f3a\u4e4f\u5b8c\u5168\u516c\u5f00\u53ef\u7528\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\uff0c\u5f53\u524d\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u503e\u5411\u4e8e\u5efa\u7acb\u5728\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e4b\u4e0a\uff0c\u7136\u800c\uff0c\u5b83\u4eec\u5728\u5904\u7406\u5177\u6709\u65f6\u95f4\u4fe1\u606f\u7684\u89c6\u9891\u5c40\u90e8\u7f16\u8f91\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u9996\u5148\uff0c\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u8bd5\u56fe\u901a\u8fc7\u9884\u5b9a\u4e49\u7684\u63a9\u7801\u4e13\u6ce8\u4e8e\u5c40\u90e8\u533a\u57df\u7f16\u8f91\uff0c\u4f46\u7531\u4e8e\u6bcf\u4e00\u5e27\u7684\u7a7a\u95f4\u6574\u4f53\u751f\u6210\uff0c\u533a\u57df\u5916\u80cc\u666f\u7684\u4fdd\u7559\u5e76\u4e0d\u7406\u60f3\u3002\u6b64\u5916\uff0c\u7528\u6237\u4e13\u95e8\u63d0\u4f9b\u63a9\u7801\u662f\u4e00\u9879\u989d\u5916\u7684\u6602\u8d35\u5de5\u4f5c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u96c6\u6210\u5230\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u7684\u81ea\u4e3b\u63a9\u7801\u7b56\u7565\u3002\u6700\u540e\u4f46\u540c\u6837\u91cd\u8981\u7684\u662f\uff0c\u56fe\u50cf\u7ea7\u9884\u8bad\u7ec3\u6a21\u578b\u6ca1\u6709\u5b66\u4e60\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u800c\u8fd9\u5bf9\u4e8e\u8868\u8fbe\u8fd0\u52a8\u548c\u52a8\u6001\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u91c7\u7528\u56fe\u50cf\u7ea7\u6df7\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6765\u6267\u884c\u5c40\u90e8\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5229\u7528 DDIM \u53cd\u6f14\u6765\u83b7\u53d6\u6f5c\u5728\u4ee3\u7801\u4f5c\u4e3a\u80cc\u666f\u6f5c\u5728\u4ee3\u7801\uff0c\u800c\u4e0d\u662f\u968f\u673a\u566a\u58f0\u7684\u6f5c\u5728\u4ee3\u7801\uff0c\u4ee5\u66f4\u597d\u5730\u4fdd\u7559\u8f93\u5165\u89c6\u9891\u7684\u80cc\u666f\u4fe1\u606f\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4ece\u6269\u6563\u6b65\u9aa4\u4e2d\u7684\u4ea4\u53c9\u6ce8\u610f\u56fe\u6d3e\u751f\u7684\u81ea\u4e3b\u63a9\u7801\u5236\u9020\u673a\u5236\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u5c06 U-Net \u7684\u81ea\u6ce8\u610f\u529b\u5757\u8f6c\u6362\u4e3a\u65f6\u7a7a\u5757\u6765\u589e\u5f3a\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u6709\u6548\u6027\u3002||\n", "2409.03463": "|**2024-09-05**|[Characterizing Massive Activations of Attention Mechanism in Graph Neural Networks](http://arxiv.org/abs/2409.03463)|**[link](https://github.com/msorbi/gnn-ma)**|Graph Neural Networks (GNNs) have become increasingly popular for effectively modeling data with graph structures. Recently, attention mechanisms have been integrated into GNNs to improve their ability to capture complex patterns. This paper presents the first comprehensive study revealing a critical, unexplored consequence of this integration: the emergence of Massive Activations (MAs) within attention layers. We introduce a novel method for detecting and analyzing MAs, focusing on edge features in different graph transformer architectures. Our study assesses various GNN models using benchmark datasets, including ZINC, TOX21, and PROTEINS. Key contributions include (1) establishing the direct link between attention mechanisms and MAs generation in GNNs, (2) developing a robust definition and detection method for MAs based on activation ratio distributions, (3) introducing the Explicit Bias Term (EBT) as a potential countermeasure and exploring it as an adversarial framework to assess models robustness based on the presence or absence of MAs. Our findings highlight the prevalence and impact of attention-induced MAs across different architectures, such as GraphTransformer, GraphiT, and SAN. The study reveals the complex interplay between attention mechanisms, model architecture, dataset characteristics, and MAs emergence, providing crucial insights for developing more robust and reliable graph models.||\n", "2409.03460": "|**2024-09-05**|[LowFormer: Hardware Efficient Design for Convolutional Transformer Backbones](http://arxiv.org/abs/2409.03460)|**[link](https://github.com/altair199797/lowformer)**|Research in efficient vision backbones is evolving into models that are a mixture of convolutions and transformer blocks. A smart combination of both, architecture-wise and component-wise is mandatory to excel in the speedaccuracy trade-off. Most publications focus on maximizing accuracy and utilize MACs (multiply accumulate operations) as an efficiency metric. The latter however often do not measure accurately how fast a model actually is due to factors like memory access cost and degree of parallelism. We analyzed common modules and architectural design choices for backbones not in terms of MACs, but rather in actual throughput and latency, as the combination of the latter two is a better representation of the efficiency of models in real applications. We applied the conclusions taken from that analysis to create a recipe for increasing hardware-efficiency in macro design. Additionally we introduce a simple slimmed-down version of MultiHead Self-Attention, that aligns with our analysis. We combine both macro and micro design to create a new family of hardware-efficient backbone networks called LowFormer. LowFormer achieves a remarkable speedup in terms of throughput and latency, while achieving similar or better accuracy than current state-of-the-art efficient backbones. In order to prove the generalizability of our hardware-efficient design, we evaluate our method on GPU, mobile GPU and ARM CPU. We further show that the downstream tasks object detection and semantic segmentation profit from our hardware-efficient architecture. Code and models are available at https://github.com/ altair199797/LowFormer.||\n", "2409.03332": "|**2024-09-05**|[Masked Sensory-Temporal Attention for Sensor Generalization in Quadruped Locomotion](http://arxiv.org/abs/2409.03332)|null|With the rising focus on quadrupeds, a generalized policy capable of handling different robot models and sensory inputs will be highly beneficial. Although several methods have been proposed to address different morphologies, it remains a challenge for learning-based policies to manage various combinations of proprioceptive information. This paper presents Masked Sensory-Temporal Attention (MSTA), a novel transformer-based model with masking for quadruped locomotion. It employs direct sensor-level attention to enhance sensory-temporal understanding and handle different combinations of sensor data, serving as a foundation for incorporating unseen information. This model can effectively understand its states even with a large portion of missing information, and is flexible enough to be deployed on a physical system despite the long input sequence.||\n", "2409.03223": "|**2024-09-05**|[Why mamba is effective? Exploit Linear Transformer-Mamba Network for Multi-Modality Image Fusion](http://arxiv.org/abs/2409.03223)|null|Multi-modality image fusion aims to integrate the merits of images from different sources and render high-quality fusion images. However, existing feature extraction and fusion methods are either constrained by inherent local reduction bias and static parameters during inference (CNN) or limited by quadratic computational complexity (Transformers), and cannot effectively extract and fuse features. To solve this problem, we propose a dual-branch image fusion network called Tmamba. It consists of linear Transformer and Mamba, which has global modeling capabilities while maintaining linear complexity. Due to the difference between the Transformer and Mamba structures, the features extracted by the two branches carry channel and position information respectively. T-M interaction structure is designed between the two branches, using global learnable parameters and convolutional layers to transfer position and channel information respectively. We further propose cross-modal interaction at the attention level to obtain cross-modal attention. Experiments show that our Tmamba achieves promising results in multiple fusion tasks, including infrared-visible image fusion and medical image fusion. Code with checkpoints will be available after the peer-review process.||\n", "2409.03115": "|**2024-09-04**|[Probing self-attention in self-supervised speech models for cross-linguistic differences](http://arxiv.org/abs/2409.03115)|null|Speech models have gained traction thanks to increase in accuracy from novel transformer architectures. While this impressive increase in performance across automatic speech recognition (ASR) benchmarks is noteworthy, there is still much that is unknown about the use of attention mechanisms for speech-related tasks. For example, while it is assumed that these models are learning language-independent (i.e., universal) speech representations, there has not yet been an in-depth exploration of what it would mean for the models to be language-independent. In the current paper, we explore this question within the realm of self-attention mechanisms of one small self-supervised speech transformer model (TERA). We find that even with a small model, the attention heads learned are diverse ranging from almost entirely diagonal to almost entirely global regardless of the training language. We highlight some notable differences in attention patterns between Turkish and English and demonstrate that the models do learn important phonological information during pretraining. We also present a head ablation study which shows that models across languages primarily rely on diagonal heads to classify phonemes.||\n", "2409.03103": "|**2024-09-04**|[Leveraging Interpretability in the Transformer to Automate the Proactive Scaling of Cloud Resources](http://arxiv.org/abs/2409.03103)|null|\u73b0\u4ee3Web\u670d\u52a1\u91c7\u7528\u4e91\u539f\u751f\u539f\u5219\u6765\u5229\u7528\u5fae\u670d\u52a1\u7684\u4f18\u52bf\u3002\u4e3a\u4e86\u6839\u636e\u670d\u52a1\u7b49\u7ea7\u534f\u8bae\uff08SLA\uff09\u6301\u7eed\u4fdd\u8bc1\u9ad8\u8d28\u91cf\u7684\u670d\u52a1\uff08QoS\uff09\uff0c\u786e\u4fdd\u4ee4\u4eba\u6ee1\u610f\u7684\u7528\u6237\u4f53\u9a8c\u5e76\u6700\u5927\u7a0b\u5ea6\u5730\u964d\u4f4e\u8fd0\u8425\u6210\u672c\uff0c\u5fc5\u987b\u4e3a\u6bcf\u4e2a\u5fae\u670d\u52a1\u914d\u7f6e\u9002\u91cf\u7684\u8d44\u6e90\u3002\u7136\u800c\uff0c\u51c6\u786e\u5730\u4e3a\u5fae\u670d\u52a1\u914d\u7f6e\u5145\u8db3\u7684\u8d44\u6e90\u975e\u5e38\u590d\u6742\uff0c\u5e76\u4e14\u53d6\u51b3\u4e8e\u8bb8\u591a\u56e0\u7d20\uff0c\u5305\u62ec\u5de5\u4f5c\u8d1f\u8f7d\u5f3a\u5ea6\u548c\u5fae\u670d\u52a1\u4e4b\u95f4\u590d\u6742\u7684\u4e92\u8fde\u5173\u7cfb\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u6355\u83b7\u4e86\u7aef\u5230\u7aef\u5ef6\u8fdf\u3001\u524d\u7aef\u7ea7\u522b\u7684\u8bf7\u6c42\u548c\u8d44\u6e90\u5229\u7528\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u5f00\u53d1\u7684\u6a21\u578b\u6765\u9884\u6d4b\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002\u6211\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u5229\u7528\u4e86\u65f6\u95f4\u878d\u5408Transformer\uff08TFT\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5177\u6709\u53ef\u89e3\u91ca\u6027\u7279\u5f81\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u67b6\u6784\u3002\u5f53\u9884\u6d4b\u7ed3\u679c\u8868\u660e\u4e0d\u7b26\u5408SLA\u65f6\uff0c\u6211\u4eec\u4f7f\u7528TFT\u63d0\u4f9b\u7684\u7279\u5f81\u91cd\u8981\u6027\u4f5c\u4e3a\u6838\u5cad\u56de\u5f52\uff08KRR\uff09\u4e2d\u7684\u534f\u53d8\u91cf\uff0c\u5e76\u5c06\u54cd\u5e94\u53d8\u91cf\u8bbe\u7f6e\u4e3a\u671f\u671b\u5ef6\u8fdf\uff0c\u4ee5\u5b66\u4e60\u4e0e\u7279\u5f81\u91cd\u8981\u6027\u76f8\u5173\u7684\u53c2\u6570\u3002\u8fd9\u4e9b\u5b66\u4e60\u5230\u7684\u53c2\u6570\u53cd\u6620\u4e86\u4e3a\u786e\u4fdd\u7b26\u5408SLA\u800c\u9700\u8981\u5bf9\u7279\u5f81\u8fdb\u884c\u7684\u8c03\u6574\u3002\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u57fa\u4e8e\u5fae\u670d\u52a1\u7684\u5e94\u7528\u7a0b\u5e8f\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u90e8\u7f72\u8def\u7ebf\u56fe\u3002||\n", "2409.05749": "|**2024-09-09**|[ReL-SAR: Representation Learning for Skeleton Action Recognition with Convolutional Transformers and BYOL](http://arxiv.org/abs/2409.05749)|null|\u4e3a\u4e86\u63d0\u53d6\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u7684\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u7279\u5f81\uff0c\u901a\u5e38\u9700\u8981\u5927\u91cf\u7cbe\u5fc3\u6807\u6ce8\u7684\u6570\u636e\uff0c\u800c\u6807\u6ce8\u548c\u8ba1\u7b97\u6210\u672c\u7684\u9650\u5236\u4f7f\u5f97\u8fd9\u9879\u4efb\u52a1\u6781\u5177\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u5229\u7528\u65e0\u6807\u7b7e\u9aa8\u67b6\u6570\u636e\u7684\u65e0\u76d1\u7763\u8868\u5f81\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u7528\u4e8e\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u7684\u65e0\u76d1\u7763\u8868\u5f81\u5b66\u4e60\u65b9\u6cd5\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5377\u79efTransformer\u6846\u67b6\uff0c\u540d\u4e3aReL-SAR\uff0c\u5b83\u5229\u7528\u5377\u79ef\u5c42\u548c\u6ce8\u610f\u529b\u5c42\u7684\u4e92\u8865\u6027\u6765\u8054\u5408\u5efa\u6a21\u9aa8\u67b6\u5e8f\u5217\u4e2d\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u7ebf\u7d22\u3002\u6211\u4eec\u8fd8\u5bf9\u9aa8\u67b6\u5173\u8282\u91c7\u7528\u4e86\u9009\u62e9-\u6392\u5217\u7b56\u7565\uff0c\u4ee5\u786e\u4fdd\u4ece\u9aa8\u9abc\u6570\u636e\u4e2d\u83b7\u53d6\u66f4\u591a\u4fe1\u606f\u3002\u6700\u540e\uff0c\u6211\u4eec\u5229\u7528Bootstrap Your Own Latent\uff08BYOL\uff09\u4ece\u65e0\u6807\u7b7e\u9aa8\u67b6\u5e8f\u5217\u6570\u636e\u4e2d\u5b66\u4e60\u9c81\u68d2\u7684\u8868\u5f81\u3002\u6211\u4eec\u5728\u6709\u9650\u5927\u5c0f\u7684\u6570\u636e\u96c6\uff1aMCAD\u3001IXMAS\u3001JHMDB\u548cNW-UCLA\u4e0a\u53d6\u5f97\u4e86\u975e\u5e38\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u76f8\u5bf9\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6709\u6548\u6027\u3002\u4e3a\u4e86\u786e\u4fdd\u53ef\u91cd\u590d\u6027\u548c\u53ef\u590d\u7528\u6027\uff0c\u6211\u4eec\u5728\u4ee5\u4e0b\u94fe\u63a5\u63d0\u4f9b\u4e86\u5305\u542b\u6240\u6709\u5b9e\u73b0\u53c2\u6570\u7684\u6e90\u4ee3\u7801\uff1ahttps://github.com/SafwenNaimi/Representation-Learning-for-Skeleton-Action-Recognition-with-Convolutional-Transformers-and-BYOL||\n", "2409.05587": "|**2024-09-09**|[DSDFormer: An Innovative Transformer-Mamba Framework for Robust High-Precision Driver Distraction Identification](http://arxiv.org/abs/2409.05587)|null|Driver distraction remains a leading cause of traffic accidents, posing a critical threat to road safety globally. As intelligent transportation systems evolve, accurate and real-time identification of driver distraction has become essential. However, existing methods struggle to capture both global contextual and fine-grained local features while contending with noisy labels in training datasets. To address these challenges, we propose DSDFormer, a novel framework that integrates the strengths of Transformer and Mamba architectures through a Dual State Domain Attention (DSDA) mechanism, enabling a balance between long-range dependencies and detailed feature extraction for robust driver behavior recognition. Additionally, we introduce Temporal Reasoning Confident Learning (TRCL), an unsupervised approach that refines noisy labels by leveraging spatiotemporal correlations in video sequences. Our model achieves state-of-the-art performance on the AUC-V1, AUC-V2, and 100-Driver datasets and demonstrates real-time processing efficiency on the NVIDIA Jetson AGX Orin platform. Extensive experimental results confirm that DSDFormer and TRCL significantly improve both the accuracy and robustness of driver distraction detection, offering a scalable solution to enhance road safety.||\n", "2409.05477": "|**2024-09-10**|[Retrofitting Temporal Graph Neural Networks with Transformer](http://arxiv.org/abs/2409.05477)|**[link](https://github.com/qianghuangwhu/tf-tgn)**|Temporal graph neural networks (TGNNs) outperform regular GNNs by incorporating time information into graph-based operations. However, TGNNs adopt specialized models (e.g., TGN, TGAT, and APAN ) and require tailored training frameworks (e.g., TGL and ETC). In this paper, we propose TF-TGN, which uses Transformer decoder as the backbone model for TGNN to enjoy Transformer's codebase for efficient training. In particular, Transformer achieves tremendous success for language modeling, and thus the community developed high-performance kernels (e.g., flash-attention and memory-efficient attention) and efficient distributed training schemes (e.g., PyTorch FSDP, DeepSpeed, and Megatron-LM). We observe that TGNN resembles language modeling, i.e., the message aggregation operation between chronologically occurring nodes and their temporal neighbors in TGNNs can be structured as sequence modeling. Beside this similarity, we also incorporate a series of algorithm designs including suffix infilling, temporal graph attention with self-loop, and causal masking self-attention to make TF-TGN work. During training, existing systems are slow in transforming the graph topology and conducting graph sampling. As such, we propose methods to parallelize the CSR format conversion and graph sampling. We also adapt Transformer codebase to train TF-TGN efficiently with multiple GPUs. We experiment with 9 graphs and compare with 2 state-of-the-art TGNN training frameworks. The results show that TF-TGN can accelerate training by over 2.20 while providing comparable or even superior accuracy to existing SOTA TGNNs. TF-TGN is available at https://github.com/qianghuangwhu/TF-TGN.||\n", "2409.05207": "|**2024-09-08**|[Low Latency Transformer Inference on FPGAs for Physics Applications with hls4ml](http://arxiv.org/abs/2409.05207)|null|This study presents an efficient implementation of transformer architectures in Field-Programmable Gate Arrays(FPGAs) using hls4ml. We demonstrate the strategy for implementing the multi-head attention, softmax, and normalization layer and evaluate three distinct models. Their deployment on VU13P FPGA chip achieved latency less than 2us, demonstrating the potential for real-time applications. HLS4ML compatibility with any TensorFlow-built transformer model further enhances the scalability and applicability of this work. Index Terms: FPGAs, machine learning, transformers, high energy physics, LIGO||\n", "2409.05136": "|**2024-09-08**|[MHS-STMA: Multimodal Hate Speech Detection via Scalable Transformer-Based Multilevel Attention Framework](http://arxiv.org/abs/2409.05136)|null|Social media has a significant impact on people's lives. Hate speech on social media has emerged as one of society's most serious issues recently. Text and pictures are two forms of multimodal data distributed within articles. Unimodal analysis has been the primary emphasis of earlier approaches. Additionally, when doing multimodal analysis, researchers neglect to preserve the distinctive qualities associated with each modality. The present article suggests a scalable architecture for multimodal hate content detection called transformer-based multilevel attention (STMA) to address these shortcomings. This architecture consists of three main parts: a combined attention-based deep learning mechanism, a vision attention mechanism encoder, and a caption attention-mechanism encoder. To identify hate content, each component uses various attention processes and uniquely handles multimodal data. Several studies employing multiple assessment criteria on three hate speech datasets: Hateful memes, MultiOff, and MMHS150K, validate the suggested architecture's efficacy. The outcomes demonstrate that on all three datasets, the suggested strategy performs better than the baseline approaches.||\n", "2409.04940": "|**2024-09-08**|[An Analog and Digital Hybrid Attention Accelerator for Transformers with Charge-based In-memory Computing](http://arxiv.org/abs/2409.04940)|null|The attention mechanism is a key computing kernel of Transformers, calculating pairwise correlations across the entire input sequence. The computing complexity and frequent memory access in computing self-attention put a huge burden on the system especially when the sequence length increases. This paper presents an analog and digital hybrid processor to accelerate the attention mechanism for transformers in 65nm CMOS technology. We propose an analog computing-in-memory (CIM) core, which prunes ~75% of low-score tokens on average during runtime at ultra-low power and delay. Additionally, a digital processor performs precise computations only for ~25% unpruned tokens selected by the analog CIM core, preventing accuracy degradation. Measured results show peak energy efficiency of 14.8 and 1.65 TOPS/W, and peak area efficiency of 976.6 and 79.4 GOPS/mm$^\\mathrm{2}$ in the analog core and the system-on-chip (SoC), respectively.||\n", "2409.04909": "|**2024-09-07**|[Efficient Training of Transformers for Molecule Property Prediction on Small-scale Datasets](http://arxiv.org/abs/2409.04909)|null|\u8840\u8111\u5c4f\u969c\uff08BBB\uff09\u662f\u4e00\u9053\u4fdd\u62a4\u6027\u5c4f\u969c\uff0c\u5c06\u5927\u8111\u4e0e\u5faa\u73af\u7cfb\u7edf\u9694\u5f00\uff0c\u8c03\u8282\u7269\u8d28\u8fdb\u5165\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf\u7684\u901a\u9053\u3002\u8bc4\u4f30\u6f5c\u5728\u836f\u7269\u7684BBB\u6e17\u900f\u6027\u5bf9\u4e8e\u6709\u6548\u7684\u836f\u7269\u9776\u5411\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684BBB\u6e17\u900f\u6027\u6d4b\u91cf\u5b9e\u9a8c\u65b9\u6cd5\u5177\u6709\u6311\u6218\u6027\uff0c\u5e76\u4e14\u5bf9\u4e8e\u5927\u89c4\u6a21\u7b5b\u9009\u6765\u8bf4\u4e0d\u5207\u5b9e\u9645\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u8ba1\u7b97\u65b9\u6cd5\u6765\u9884\u6d4bBBB\u6e17\u900f\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684GPS Transformer\u67b6\u6784\uff0c\u65e8\u5728\u5728\u4f4e\u6570\u636e\u60c5\u51b5\u4e0b\u8868\u73b0\u826f\u597d\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4f7f\u7528BBBP\u6570\u636e\u96c6\u7684BBB\u6e17\u900f\u6027\u9884\u6d4b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u7684ROC-AUC\u4e3a78.8%\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u6c34\u5e73\u63d0\u9ad8\u4e865.5%\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u6807\u51c6\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e0eGPS Transformer\u7ed3\u5408\u4f7f\u7528\u6bd4\u5176\u4ed6\u6ce8\u610f\u529b\u673a\u5236\u53d8\u4f53\u4e0eGPS Transformer\u7ed3\u5408\u4f7f\u7528\u8868\u73b0\u66f4\u597d\u3002||\n", "2409.04803": "|**2024-09-07**|[Cross-attention Inspired Selective State Space Models for Target Sound Extraction](http://arxiv.org/abs/2409.04803)|null|Transformer\u6a21\u578b\uff0c\u7279\u522b\u662f\u5176\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u76ee\u6807\u58f0\u97f3\u63d0\u53d6\u4e2d\u7684\u7279\u5f81\u878d\u5408\uff0c\u8be5\u4efb\u52a1\u57fa\u4e8e\u7ed9\u5b9a\u7684\u7ebf\u7d22\u63d0\u53d6\u611f\u5174\u8da3\u7684\u4fe1\u53f7\u3002\u5c3d\u7ba1\u6709\u6548\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\u8f83\u4f4e\u3002\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u6700\u8fd1\u7684Mamba\u6a21\u578b\uff0c\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e0e\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u7136\u800c\uff0c\u7531\u4e8eMamba\u65e0\u6cd5\u50cf\u4ea4\u53c9\u6ce8\u610f\u529b\u90a3\u6837\u6355\u6349\u4e0d\u540c\u5e8f\u5217\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u56e0\u6b64\u5b83\u5728\u76ee\u6807\u58f0\u97f3\u63d0\u53d6\u4e2d\u7684\u9002\u7528\u6027\u53d7\u5230\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7528\u4e8e\u76ee\u6807\u58f0\u97f3\u63d0\u53d6\u7684CrossMamba\u6a21\u578b\uff0c\u5b83\u5229\u7528Mamba\u7684\u9690\u85cf\u6ce8\u610f\u529b\u673a\u5236\u6765\u8ba1\u7b97\u7ed9\u5b9a\u7ebf\u7d22\u548c\u97f3\u9891\u6df7\u5408\u7269\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002Mamba\u7684\u8ba1\u7b97\u53ef\u4ee5\u5206\u4e3a\u67e5\u8be2\u3001\u952e\u548c\u503c\u3002\u6211\u4eec\u5229\u7528\u7ebf\u7d22\u751f\u6210\u67e5\u8be2\uff0c\u5e76\u5229\u7528\u97f3\u9891\u6df7\u5408\u7269\u5bfc\u51fa\u952e\u548c\u503c\uff0c\u9075\u5faaTransformer\u4e2d\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u7684\u539f\u7406\u3002\u6765\u81ea\u4e24\u79cd\u5177\u6709\u4ee3\u8868\u6027\u7684\u76ee\u6807\u58f0\u97f3\u63d0\u53d6\u65b9\u6cd5\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684CrossMamba\u7684\u6709\u6548\u6027\u3002||\n", "2409.04431": "|**2024-09-06**|[Theory, Analysis, and Best Practices for Sigmoid Self-Attention](http://arxiv.org/abs/2409.04431)|null|\u6ce8\u610f\u529b\u662f Transformer \u67b6\u6784\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002\u5b83\u662f\u4e00\u79cd\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u6620\u5c04\uff0c\u5c06\u6bcf\u4e2a\u5e8f\u5217\u5143\u7d20\u8f6c\u6362\u4e3a\u503c\u7684\u52a0\u6743\u548c\u3002\u6743\u91cd\u901a\u5e38\u662f\u901a\u8fc7\u952e\u548c\u67e5\u8be2\u4e4b\u95f4\u7684\u70b9\u79ef\u7684 softmax \u83b7\u5f97\u7684\u3002\u6700\u8fd1\u7684\u5de5\u4f5c\u63a2\u7d22\u4e86 Transformer \u4e2d softmax \u6ce8\u610f\u529b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f8b\u5982 ReLU \u548c sigmoid \u6fc0\u6d3b\u51fd\u6570\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6 sigmoid \u6ce8\u610f\u529b\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u6df1\u5165\u7684\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u3002\u7406\u8bba\u4e0a\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5177\u6709 sigmoid \u6ce8\u610f\u529b\u7684 Transformer \u662f\u901a\u7528\u51fd\u6570\u903c\u8fd1\u5668\uff0c\u5e76\u4e14\u4e0e softmax \u6ce8\u610f\u529b\u76f8\u6bd4\uff0c\u5177\u6709\u66f4\u597d\u7684\u6b63\u5219\u6027\u3002\u901a\u8fc7\u8be6\u7ec6\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u5728\u8bad\u7ec3\u7684\u65e9\u671f\u9636\u6bb5\u7a33\u5b9a\u8f83\u5927\u7684\u521d\u59cb\u6ce8\u610f\u529b\u8303\u6570\u662f\u6210\u529f\u8bad\u7ec3\u5177\u6709 sigmoid \u6ce8\u610f\u529b\u6a21\u578b\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u5148\u524d\u7684\u5c1d\u8bd5\u3002\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86 FLASHSIGMOID\uff0c\u8fd9\u662f\u4e00\u79cd\u786c\u4ef6\u611f\u77e5\u4e14\u5185\u5b58\u9ad8\u6548\u7684 sigmoid \u6ce8\u610f\u529b\u5b9e\u73b0\uff0c\u5728 H100 GPU \u4e0a\uff0c\u5176\u63a8\u7406\u5185\u6838\u901f\u5ea6\u6bd4 FLASHATTENTION2 \u63d0\u9ad8\u4e86 17%\u3002\u8de8\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u8bed\u97f3\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ecf\u8fc7\u9002\u5f53\u6807\u51c6\u5316\u7684 sigmoid \u6ce8\u610f\u529b\u5728\u5e7f\u6cdb\u7684\u9886\u57df\u548c\u89c4\u6a21\u4e0a\u4e0e softmax \u6ce8\u610f\u529b\u7684\u5f3a\u5927\u6027\u80fd\u76f8\u5339\u914d\uff0c\u8fd9\u662f\u5148\u524d\u5c1d\u8bd5 sigmoid \u6ce8\u610f\u529b\u6240\u65e0\u6cd5\u5b8c\u5168\u5b9e\u73b0\u7684\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u7edf\u4e00\u4e86\u73b0\u6709\u6280\u672f\uff0c\u5e76\u4e3a sigmoid \u6ce8\u610f\u529b\u4f5c\u4e3a Transformer \u4e2d softmax \u7684\u76f4\u63a5\u66ff\u4ee3\u54c1\u5efa\u7acb\u4e86\u6700\u4f73\u5b9e\u8df5\u3002||\n", "2409.04275": "|**2024-09-09**|[AttentionX: Exploiting Consensus Discrepancy In Attention from A Distributed Optimization Perspective](http://arxiv.org/abs/2409.04275)|null|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ece\u5206\u5e03\u5f0f\u4f18\u5316\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u5229\u7528\u5171\u8bc6\u5dee\u5f02\u6765\u6269\u5c55Transformer\u4e2d\u7684\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3aAttentionX\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4e58\u5b50\u4ea4\u66ff\u65b9\u5411\u6cd5\uff08PDMM\uff09\\cite{Zhang16PDMM}\u65e8\u5728\u8fed\u4ee3\u5730\u89e3\u51b3\u70b9\u5bf9\u70b9\uff08P2P\uff09\u7f51\u7edc\u4e0a\u7684\u4e00\u5927\u7c7b\u5206\u5e03\u5f0f\u4f18\u5316\u95ee\u9898\uff0c\u5176\u4e2d\u76f8\u90bb\u8282\u70b9\u6839\u636e\u4f18\u5316\u8fc7\u7a0b\u4e2d\u9884\u5b9a\u4e49\u7684\u7ebf\u6027\u8fb9\u7ea6\u675f\u9010\u6e10\u8fbe\u6210\u5171\u8bc6\u3002\u7279\u522b\u662f\u5728PDMM\u7684\u6bcf\u6b21\u8fed\u4ee3\u4e2d\uff0c\u7f51\u7edc\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u9996\u5148\u4ece\u90bb\u5c45\u8282\u70b9\u6536\u96c6\u4fe1\u606f\uff0c\u7136\u540e\u6267\u884c\u672c\u5730\u4fe1\u606f\u878d\u5408\u3002\u4ece\u9ad8\u5c42\u6b21\u6765\u770b\uff0c\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u57fa\u4e8e$KQ$-softmax\u7684$V$\u8868\u793a\u52a0\u6743\u6c42\u548c\u5bf9\u5e94\u4e8e\u4ece\u90bb\u5c45\u8282\u70b9\u6536\u96c6\u4fe1\u606f\uff0c\u800cTransformer\u4e2d\u901a\u8fc7\u524d\u9988\u7f51\u7edc\uff08FFN\uff09\u8fdb\u884c\u7684\u7279\u5f81\u5904\u7406\u5bf9\u5e94\u4e8e\u672c\u5730\u4fe1\u606f\u878d\u5408\u3002PDMM\u5229\u7528\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u4ee5\u7ebf\u6027\u8fb9\u7ea6\u675f\u7684\u6b8b\u5dee\u5f62\u5f0f\u6355\u83b7\u5386\u53f2\u5171\u8bc6\u5dee\u5f02\uff0c\u8fd9\u5bf9\u4e8e\u7b97\u6cd5\u7684\u6536\u655b\u81f3\u5173\u91cd\u8981\u3002\u53d7PDMM\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86AttentionX\uff0c\u5c06\u5171\u8bc6\u5dee\u5f02\u7eb3\u5165\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\u7684\u8f93\u51fa\u66f4\u65b0\u8868\u8fbe\u5f0f\u4e2d\u3002AttentionX\u4e2d\u7684\u5171\u8bc6\u5dee\u5f02\u662f\u6307$V$\u8868\u793a\u7684\u52a0\u6743\u6c42\u548c\u4e0e\u5176\u7f29\u653e\u540e\u7684$V$\u8868\u793a\u672c\u8eab\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u5728ViT\u548cnanoGPT\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u4e86\u5176\u826f\u597d\u7684\u6027\u80fd\u3002||\n", "2409.06603": "|**2024-09-10**|[A Practical Gated Recurrent Transformer Network Incorporating Multiple Fusions for Video Denoising](http://arxiv.org/abs/2409.06603)|null|State-of-the-art (SOTA) video denoising methods employ multi-frame simultaneous denoising mechanisms, resulting in significant delays (e.g., 16 frames), making them impractical for real-time cameras. To overcome this limitation, we propose a multi-fusion gated recurrent Transformer network (GRTN) that achieves SOTA denoising performance with only a single-frame delay. Specifically, the spatial denoising module extracts features from the current frame, while the reset gate selects relevant information from the previous frame and fuses it with current frame features via the temporal denoising module. The update gate then further blends this result with the previous frame features, and the reconstruction module integrates it with the current frame. To robustly compute attention for noisy features, we propose a residual simplified Swin Transformer with Euclidean distance (RSSTE) in the spatial and temporal denoising modules. Comparative objective and subjective results show that our GRTN achieves denoising performance comparable to SOTA multi-frame delay networks, with only a single-frame delay.||\n", "2409.06590": "|**2024-09-10**|[Lightweight Multiscale Feature Fusion Super-Resolution Network Based on Two-branch Convolution and Transformer](http://arxiv.org/abs/2409.06590)|null|The single image super-resolution(SISR) algorithms under deep learning currently have two main models, one based on convolutional neural networks and the other based on Transformer. The former uses the stacking of convolutional layers with different convolutional kernel sizes to design the model, which enables the model to better extract the local features of the image; the latter uses the self-attention mechanism to design the model, which allows the model to establish long-distance dependencies between image pixel points through the self-attention mechanism and then better extract the global features of the image. However, both of the above methods face their problems. Based on this, this paper proposes a new lightweight multi-scale feature fusion network model based on two-way complementary convolutional and Transformer, which integrates the respective features of Transformer and convolutional neural networks through a two-branch network architecture, to realize the mutual fusion of global and local information. Meanwhile, considering the partial loss of information caused by the low-pixel images trained by the deep neural network, this paper designs a modular connection method of multi-stage feature supplementation to fuse the feature maps extracted from the shallow stage of the model with those extracted from the deep stage of the model, to minimize the loss of the information in the feature images that is beneficial to the image restoration as much as possible, to facilitate the obtaining of a higher-quality restored image. The practical results finally show that the model proposed in this paper is optimal in image recovery performance when compared with other lightweight models with the same amount of parameters.||\n", "2409.06443": "|**2024-09-10**|[Knowledge Distillation via Query Selection for Detection Transformer](http://arxiv.org/abs/2409.06443)|null|Transformers have revolutionized the object detection landscape by introducing DETRs, acclaimed for their simplicity and efficacy. Despite their advantages, the substantial size of these models poses significant challenges for practical deployment, particularly in resource-constrained environments. This paper addresses the challenge of compressing DETR by leveraging knowledge distillation, a technique that holds promise for maintaining model performance while reducing size. A critical aspect of DETRs' performance is their reliance on queries to interpret object representations accurately. Traditional distillation methods often focus exclusively on positive queries, identified through bipartite matching, neglecting the rich information present in hard-negative queries. Our visual analysis indicates that hard-negative queries, focusing on foreground elements, are crucial for enhancing distillation outcomes. To this end, we introduce a novel Group Query Selection strategy, which diverges from traditional query selection in DETR distillation by segmenting queries based on their Generalized Intersection over Union (GIoU) with ground truth objects, thereby uncovering valuable hard-negative queries for distillation. Furthermore, we present the Knowledge Distillation via Query Selection for DETR (QSKD) framework, which incorporates Attention-Guided Feature Distillation (AGFD) and Local Alignment Prediction Distillation (LAPD). These components optimize the distillation process by focusing on the most informative aspects of the teacher model's intermediate features and output. Our comprehensive experimental evaluation of the MS-COCO dataset demonstrates the effectiveness of our approach, significantly improving average precision (AP) across various DETR architectures without incurring substantial computational costs. Specifically, the AP of Conditional DETR ResNet-18 increased from 35.8 to 39.9.||\n", "2409.06206": "|**2024-09-10**|[AgileIR: Memory-Efficient Group Shifted Windows Attention for Agile Image Restoration](http://arxiv.org/abs/2409.06206)|null|Image Transformers show a magnificent success in Image Restoration tasks. Nevertheless, most of transformer-based models are strictly bounded by exorbitant memory occupancy. Our goal is to reduce the memory consumption of Swin Transformer and at the same time speed up the model during training process. Thus, we introduce AgileIR, group shifted attention mechanism along with window attention, which sparsely simplifies the model in architecture. We propose Group Shifted Window Attention (GSWA) to decompose Shift Window Multi-head Self Attention (SW-MSA) and Window Multi-head Self Attention (W-MSA) into groups across their attention heads, contributing to shrinking memory usage in back propagation. In addition to that, we keep shifted window masking and its shifted learnable biases during training, in order to induce the model interacting across windows within the channel. We also re-allocate projection parameters to accelerate attention matrix calculation, which we found a negligible decrease in performance. As a result of experiment, compared with our baseline SwinIR and other efficient quantization models, AgileIR keeps the performance still at 32.20 dB on Set5 evaluation dataset, exceeding other methods with tailor-made efficient methods and saves over 50% memory while a large batch size is employed.||\n", "2409.08159": "|**2024-09-12**|[SDformer: Efficient End-to-End Transformer for Depth Completion](http://arxiv.org/abs/2409.08159)|**[link](https://github.com/jamesqian11/sdformer-for-depth-completion)**|Depth completion aims to predict dense depth maps with sparse depth measurements from a depth sensor. Currently, Convolutional Neural Network (CNN) based models are the most popular methods applied to depth completion tasks. However, despite the excellent high-end performance, they suffer from a limited representation area. To overcome the drawbacks of CNNs, a more effective and powerful method has been presented: the Transformer, which is an adaptive self-attention setting sequence-to-sequence model. While the standard Transformer quadratically increases the computational cost from the key-query dot-product of input resolution which improperly employs depth completion tasks. In this work, we propose a different window-based Transformer architecture for depth completion tasks named Sparse-to-Dense Transformer (SDformer). The network consists of an input module for the depth map and RGB image features extraction and concatenation, a U-shaped encoder-decoder Transformer for extracting deep features, and a refinement module. Specifically, we first concatenate the depth map features with the RGB image features through the input model. Then, instead of calculating self-attention with the whole feature maps, we apply different window sizes to extract the long-range depth dependencies. Finally, we refine the predicted features from the input module and the U-shaped encoder-decoder Transformer module to get the enriching depth features and employ a convolution layer to obtain the dense depth map. In practice, the SDformer obtains state-of-the-art results against the CNN-based depth completion models with lower computing loads and parameters on the NYU Depth V2 and KITTI DC datasets.|\n", "2409.07914": "|**2024-09-12**|[InterACT: Inter-dependency Aware Action Chunking with Hierarchical Attention Transformers for Bimanual Manipulation](http://arxiv.org/abs/2409.07914)|null|We present InterACT: Inter-dependency aware Action Chunking with Hierarchical Attention Transformers, a novel imitation learning framework for bimanual manipulation that integrates hierarchical attention to capture inter-dependencies between dual-arm joint states and visual inputs. InterACT consists of a Hierarchical Attention Encoder and a Multi-arm Decoder, both designed to enhance information aggregation and coordination. The encoder processes multi-modal inputs through segment-wise and cross-segment attention mechanisms, while the decoder leverages synchronization blocks to refine individual action predictions, providing the counterpart's prediction as context. Our experiments on a variety of simulated and real-world bimanual manipulation tasks demonstrate that InterACT significantly outperforms existing methods. Detailed ablation studies validate the contributions of key components of our work, including the impact of CLS tokens, cross-segment encoders, and synchronization blocks.|\n", "2409.07793": "|**2024-09-12**|[Lagrange Duality and Compound Multi-Attention Transformer for Semi-Supervised Medical Image Segmentation](http://arxiv.org/abs/2409.07793)|**[link](https://github.com/lzeeorno/lagrange-duality-and-cmaformer)**|Medical image segmentation, a critical application of semantic segmentation in healthcare, has seen significant advancements through specialized computer vision techniques. While deep learning-based medical image segmentation is essential for assisting in medical diagnosis, the lack of diverse training data causes the long-tail problem. Moreover, most previous hybrid CNN-ViT architectures have limited ability to combine various attentions in different layers of the Convolutional Neural Network. To address these issues, we propose a Lagrange Duality Consistency (LDC) Loss, integrated with Boundary-Aware Contrastive Loss, as the overall training objective for semi-supervised learning to mitigate the long-tail problem. Additionally, we introduce CMAformer, a novel network that synergizes the strengths of ResUNet and Transformer. The cross-attention block in CMAformer effectively integrates spatial attention and channel attention for multi-scale feature fusion. Overall, our results indicate that CMAformer, combined with the feature fusion framework and the new consistency loss, demonstrates strong complementarity in semi-supervised learning ensembles. We achieve state-of-the-art results on multiple public medical image datasets. Example code are available at: \\url{https://github.com/lzeeorno/Lagrange-Duality-and-CMAformer}.|\n", "2409.07541": "|**2024-09-11**|[ENACT: Entropy-based Clustering of Attention Input for Improving the Computational Performance of Object Detection Transformers](http://arxiv.org/abs/2409.07541)|**[link](https://github.com/gsavathrakis/enact)**|Transformers demonstrate competitive performance in terms of precision on the problem of vision-based object detection. However, they require considerable computational resources due to the quadratic size of the attention weights. In this work, we propose to cluster the transformer input on the basis of its entropy. The reason for this is that the self-information of each pixel (whose sum is the entropy), is likely to be similar among pixels corresponding to the same objects. Clustering reduces the size of data given as input to the transformer and therefore reduces training time and GPU memory usage, while at the same time preserves meaningful information to be passed through the remaining parts of the network. The proposed process is organized in a module called ENACT, that can be plugged-in any transformer architecture that consists of a multi-head self-attention computation in its encoder. We ran extensive experiments using the COCO object detection dataset, and three detection transformers. The obtained results demonstrate that in all tested cases, there is consistent reduction in the required computational resources, while the precision of the detection task is only slightly reduced. The code of the ENACT module will become available at https://github.com/GSavathrakis/ENACT|\n", "2409.07146": "|**2024-09-11**|[Gated Slot Attention for Efficient Linear-Time Sequence Modeling](http://arxiv.org/abs/2409.07146)|**[link](https://github.com/sustcsonglin/flash-linear-attention)**|Linear attention Transformers and their gated variants, celebrated for enabling parallel training and efficient recurrent inference, still fall short in recall-intensive tasks compared to traditional Transformers and demand significant resources for training from scratch. This paper introduces Gated Slot Attention (GSA), which enhances Attention with Bounded-memory-Control (ABC) by incorporating a gating mechanism inspired by Gated Linear Attention (GLA). Essentially, GSA comprises a two-layer GLA linked via softmax, utilizing context-aware memory reading and adaptive forgetting to improve memory capacity while maintaining compact recurrent state size. This design greatly enhances both training and inference efficiency through GLA's hardware-efficient training algorithm and reduced state size. Additionally, retaining the softmax operation is particularly beneficial in \"finetuning pretrained Transformers to RNNs\" (T2R) settings, reducing the need for extensive training from scratch. Extensive experiments confirm GSA's superior performance in scenarios requiring in-context recall and in T2R settings.|\n", "2409.06985": "|**2024-09-11**|[Enhancing Cross-domain Pre-Trained Decision Transformers with Adaptive Attention](http://arxiv.org/abs/2409.06985)|null|Recently, the pre-training of decision transformers (DT) using a different domain, such as natural language text, has generated significant attention in offline reinforcement learning (Offline RL). Although this cross-domain pre-training approach achieves superior performance compared to training from scratch in environments required short-term planning ability, the mechanisms by which pre-training benefits the fine-tuning phase remain unclear. Furthermore, we point out that the cross-domain pre-training approach hinders the extraction of distant information in environments like PointMaze that require long-term planning ability, leading to performance that is much worse than training DT from scratch. This work first analyzes these issues and found that Markov Matrix, a component that exists in pre-trained attention heads, is the key to explain the significant performance disparity of pre-trained models in different planning abilities. Inspired by our analysis, we propose a general method GPT-DTMA, which equips a pre-trained DT with Mixture of Attention (MoA), to enable adaptive learning and accommodating diverse attention requirements during fine-tuning. Extensive experiments demonstrate that the effectiveness of GPT-DTMA: it achieves superior performance in short-term environments compared to baselines, and in long-term environments, it mitigates the negative impact caused by Markov Matrix, achieving results comparable to those of DT trained from scratch.|\n", "2409.06963": "|**2024-09-11**|[Brain-Inspired Stepwise Patch Merging for Vision Transformers](http://arxiv.org/abs/2409.06963)|null|The hierarchical architecture has become a mainstream design paradigm for Vision Transformers (ViTs), with Patch Merging serving as the pivotal component that transforms a columnar architecture into a hierarchical one. Drawing inspiration from the brain's ability to integrate global and local information for comprehensive visual understanding, we propose a novel technique called Stepwise Patch Merging (SPM), which enhances the subsequent attention mechanism's ability to 'see' better. SPM comprises two critical modules: Multi-Scale Aggregation (MSA) and Guided Local Enhancement (GLE). The MSA module integrates multi-scale features to enrich feature representation, while the GLE module focuses on refining local detail extraction, thus achieving an optimal balance between long-range dependency modeling and local feature enhancement. Extensive experiments conducted on benchmark datasets, including ImageNet-1K, COCO, and ADE20K, demonstrate that SPM significantly improves the performance of various models, particularly in dense prediction tasks such as object detection and semantic segmentation. These results underscore the efficacy of SPM in enhancing model accuracy and robustness across a wide range of computer vision tasks.||\n", "2409.09007": "|**2024-09-13**|[SGFormer: Single-Layer Graph Transformers with Approximation-Free Linear Complexity](http://arxiv.org/abs/2409.09007)|**[link](https://github.com/qitianwu/sgformer)**|Learning representations on large graphs is a long-standing challenge due to the inter-dependence nature. Transformers recently have shown promising performance on small graphs thanks to its global attention for capturing all-pair interactions beyond observed structures. Existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated architectures by stacking deep attention-based propagation layers. In this paper, we attempt to evaluate the necessity of adopting multi-layer attentions in Transformers on graphs, which considerably restricts the efficiency. Specifically, we analyze a generic hybrid propagation layer, comprised of all-pair attention and graph-based propagation, and show that multi-layer propagation can be reduced to one-layer propagation, with the same capability for representation learning. It suggests a new technical path for building powerful and efficient Transformers on graphs, particularly through simplifying model architectures without sacrificing expressiveness. As exemplified by this work, we propose a Simplified Single-layer Graph Transformers (SGFormer), whose main component is a single-layer global attention that scales linearly w.r.t. graph sizes and requires none of any approximation for accommodating all-pair interactions. Empirically, SGFormer successfully scales to the web-scale graph ogbn-papers100M, yielding orders-of-magnitude inference acceleration over peer Transformers on medium-sized graphs, and demonstrates competitiveness with limited labeled data.|\n", "2409.08769": "|**2024-09-13**|[Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry](http://arxiv.org/abs/2409.08769)|**[link](https://github.com/ybkurt/vift)**|In recent years, transformer-based architectures become the de facto standard for sequence modeling in deep learning frameworks. Inspired by the successful examples, we propose a causal visual-inertial fusion transformer (VIFT) for pose estimation in deep visual-inertial odometry. This study aims to improve pose estimation accuracy by leveraging the attention mechanisms in transformers, which better utilize historical data compared to the recurrent neural network (RNN) based methods seen in recent methods. Transformers typically require large-scale data for training. To address this issue, we utilize inductive biases for deep VIO networks. Since latent visual-inertial feature vectors encompass essential information for pose estimation, we employ transformers to refine pose estimates by updating latent vectors temporally. Our study also examines the impact of data imbalance and rotation learning methods in supervised end-to-end learning of visual inertial odometry by utilizing specialized gradients in backpropagation for the elements of SE$(3)$ group. The proposed method is end-to-end trainable and requires only a monocular camera and IMU during inference. Experimental results demonstrate that VIFT increases the accuracy of monocular VIO networks, achieving state-of-the-art results when compared to previous methods on the KITTI dataset. The code will be made available at https://github.com/ybkurt/VIFT.|\n", "2409.08652": "|**2024-09-13**|[SkinFormer: Learning Statistical Texture Representation with Transformer for Skin Lesion Segmentation](http://arxiv.org/abs/2409.08652)|**[link](https://github.com/rongtao-xu/skinformer)**|Accurate skin lesion segmentation from dermoscopic images is of great importance for skin cancer diagnosis. However, automatic segmentation of melanoma remains a challenging task because it is difficult to incorporate useful texture representations into the learning process. Texture representations are not only related to the local structural information learned by CNN, but also include the global statistical texture information of the input image. In this paper, we propose a trans\\textbf{Former} network (\\textbf{SkinFormer}) that efficiently extracts and fuses statistical texture representation for \\textbf{Skin} lesion segmentation. Specifically, to quantify the statistical texture of input features, a Kurtosis-guided Statistical Counting Operator is designed. We propose Statistical Texture Fusion Transformer and Statistical Texture Enhance Transformer with the help of Kurtosis-guided Statistical Counting Operator by utilizing the transformer's global attention mechanism. The former fuses structural texture information and statistical texture information, and the latter enhances the statistical texture of multi-scale features. {Extensive experiments on three publicly available skin lesion datasets validate that our SkinFormer outperforms other SOAT methods, and our method achieves 93.2\\% Dice score on ISIC 2018. It can be easy to extend SkinFormer to segment 3D images in the future.} Our code is available at https://github.com/Rongtao-Xu/SkinFormer.|\n", "2409.08461": "|**2024-09-13**|[VistaFormer: Scalable Vision Transformers for Satellite Image Time Series Segmentation](http://arxiv.org/abs/2409.08461)|**[link](https://github.com/macdonaldezra/VistaFormer)**|We introduce VistaFormer, a lightweight Transformer-based model architecture for the semantic segmentation of remote-sensing images. This model uses a multi-scale Transformer-based encoder with a lightweight decoder that aggregates global and local attention captured in the encoder blocks. VistaFormer uses position-free self-attention layers which simplifies the model architecture and removes the need to interpolate temporal and spatial codes, which can reduce model performance when training and testing image resolutions differ. We investigate simple techniques for filtering noisy input signals like clouds and demonstrate that improved model scalability can be achieved by substituting Multi-Head Self-Attention (MHSA) with Neighbourhood Attention (NA). Experiments on the PASTIS and MTLCC crop-type segmentation benchmarks show that VistaFormer achieves better performance than comparable models and requires only 8% of the floating point operations using MHSA and 11% using NA while also using fewer trainable parameters. VistaFormer with MHSA improves on state-of-the-art mIoU scores by 0.1% on the PASTIS benchmark and 3% on the MTLCC benchmark while VistaFormer with NA improves on the MTLCC benchmark by 3.7%.|\n", "2409.11320": "|**2024-09-17**|[A short trajectory is all you need: A transformer-based model for long-time dissipative quantum dynamics](http://arxiv.org/abs/2409.11320)|null|In this communication we demonstrate that a deep artificial neural network based on a transformer architecture with self-attention layers can predict the long-time population dynamics of a quantum system coupled to a dissipative environment provided that the short-time population dynamics of the system is known. The transformer neural network model developed in this work predicts the long-time dynamics of spin-boson model efficiently and very accurately across different regimes, from weak system-bath coupling to strong coupling non-Markovian regimes. Our model is more accurate than classical forecasting models, such as recurrent neural networks and is comparable to the state-of-the-art models for simulating the dynamics of quantum dissipative systems, based on kernel ridge regression.|\n", "2409.11250": "|**2024-09-17**|[Linear Recency Bias During Training Improves Transformers' Fit to Reading Times](http://arxiv.org/abs/2409.11250)|null|Recent psycholinguistic research has compared human reading times to surprisal estimates from language models to study the factors shaping human sentence processing difficulty. Previous studies have shown a strong fit between surprisal values from Transformers and reading times. However, standard Transformers work with a lossless representation of the entire previous linguistic context, unlike models of human language processing that include memory decay. To bridge this gap, this paper evaluates a modification of the Transformer model that uses ALiBi (Press et al., 2022), a recency bias added to attention scores. Surprisal estimates with ALiBi show an improved fit to human reading times compared to a standard Transformer baseline. A subsequent analysis of attention heads suggests that ALiBi's mixture of slopes -- which determine the rate of memory decay in each attention head -- may play a role in the improvement by helping models with ALiBi to track different kinds of linguistic dependencies.|\n", "2409.10944": "|**2024-09-17**|[Contrasformer: A Brain Network Contrastive Transformer for Neurodegenerative Condition Identification](http://arxiv.org/abs/2409.10944)|**[link](https://github.com/angusmonroe/contrasformer)**|Understanding neurological disorder is a fundamental problem in neuroscience, which often requires the analysis of brain networks derived from functional magnetic resonance imaging (fMRI) data. Despite the prevalence of Graph Neural Networks (GNNs) and Graph Transformers in various domains, applying them to brain networks faces challenges. Specifically, the datasets are severely impacted by the noises caused by distribution shifts across sub-populations and the neglect of node identities, both obstruct the identification of disease-specific patterns. To tackle these challenges, we propose Contrasformer, a novel contrastive brain network Transformer. It generates a prior-knowledge-enhanced contrast graph to address the distribution shifts across sub-populations by a two-stream attention mechanism. A cross attention with identity embedding highlights the identity of nodes, and three auxiliary losses ensure group consistency. Evaluated on 4 functional brain network datasets over 4 different diseases, Contrasformer outperforms the state-of-the-art methods for brain networks by achieving up to 10.8\\% improvement in accuracy, which demonstrates its efficacy in neurological disorder identification. Case studies illustrate its interpretability, especially in the context of neuroscience. This paper provides a solution for analyzing brain networks, offering valuable insights into neurological disorders. Our code is available at \\url{https://github.com/AngusMonroe/Contrasformer}.|\n", "2409.10870": "|**2024-09-17**|[Adaptive Large Language Models By Layerwise Attention Shortcuts](http://arxiv.org/abs/2409.10870)|null|Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \\textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.|\n", "2409.10792": "|**2024-09-16**|[Recurrent Graph Transformer Network for Multiple Fault Localization in Naval Shipboard Systems](http://arxiv.org/abs/2409.10792)|null|The integration of power electronics building blocks in modern MVDC 12kV Naval ship systems enhances energy management and functionality but also introduces complex fault detection and control challenges. These challenges strain traditional fault diagnostic methods, making it difficult to detect and manage faults across multiple locations while maintaining system stability and performance. This paper proposes a temporal recurrent graph transformer network for fault diagnosis in naval MVDC 12kV shipboard systems. The deep graph neural network uses gated recurrent units to capture temporal features and a multi-head attention mechanism to extract spatial features, enhancing diagnostic accuracy. The approach effectively identifies and evaluates successive multiple faults with high precision. The method is implemented and validated on the MVDC 12kV shipboard system designed by the ESDRC team, incorporating all key components. Results show significant improvements in fault localization accuracy, with a 1-4% increase in performance metrics compared to other machine learning methods.|\n", "2409.10715": "|**2024-09-16**|[Self-Attention Limits Working Memory Capacity of Transformer-Based Models](http://arxiv.org/abs/2409.10715)|null|Recent work on Transformer-based large language models (LLMs) has revealed striking limits in their working memory capacity, similar to what has been found in human behavioral studies. Specifically, these models' performance drops significantly on N-back tasks as N increases. However, there is still a lack of mechanistic interpretability as to why this phenomenon would arise. Inspired by the executive attention theory from behavioral sciences, we hypothesize that the self-attention mechanism within Transformer-based models might be responsible for their working memory capacity limits. To test this hypothesis, we train vanilla decoder-only transformers to perform N-back tasks and find that attention scores gradually aggregate to the N-back positions over training, suggesting that the model masters the task by learning a strategy to pay attention to the relationship between the current position and the N-back position. Critically, we find that the total entropy of the attention score matrix increases as N increases, suggesting that the dispersion of attention scores might be the cause of the capacity limit observed in N-back tasks.|\n", "2409.10653": "|**2024-09-16**|[Logic Synthesis Optimization with Predictive Self-Supervision via Causal Transformers](http://arxiv.org/abs/2409.10653)|null|Contemporary hardware design benefits from the abstraction provided by high-level logic gates, streamlining the implementation of logic circuits. Logic Synthesis Optimization (LSO) operates at one level of abstraction within the Electronic Design Automation (EDA) workflow, targeting improvements in logic circuits with respect to performance metrics such as size and speed in the final layout. Recent trends in the field show a growing interest in leveraging Machine Learning (ML) for EDA, notably through ML-guided logic synthesis utilizing policy-based Reinforcement Learning (RL) methods.Despite these advancements, existing models face challenges such as overfitting and limited generalization, attributed to constrained public circuits and the expressiveness limitations of graph encoders. To address these hurdles, and tackle data scarcity issues, we introduce LSOformer, a novel approach harnessing Autoregressive transformer models and predictive SSL to predict the trajectory of Quality of Results (QoR). LSOformer integrates cross-attention modules to merge insights from circuit graphs and optimization sequences, thereby enhancing prediction accuracy for QoR metrics. Experimental studies validate the effectiveness of LSOformer, showcasing its superior performance over baseline architectures in QoR prediction tasks, where it achieves improvements of 5.74%, 4.35%, and 17.06% on the EPFL, OABCD, and proprietary circuits datasets, respectively, in inductive setup.|\n", "2409.10206": "|**2024-09-16**|[Garment Attribute Manipulation with Multi-level Attention](http://arxiv.org/abs/2409.10206)|null|In the rapidly evolving field of online fashion shopping, the need for more personalized and interactive image retrieval systems has become paramount. Existing methods often struggle with precisely manipulating specific garment attributes without inadvertently affecting others. To address this challenge, we propose GAMMA (Garment Attribute Manipulation with Multi-level Attention), a novel framework that integrates attribute-disentangled representations with a multi-stage attention-based architecture. GAMMA enables targeted manipulation of fashion image attributes, allowing users to refine their searches with high accuracy. By leveraging a dual-encoder Transformer and memory block, our model achieves state-of-the-art performance on popular datasets like Shopping100k and DeepFashion.|\n", "2409.09513": "|**2024-09-14**|[Planning Transformer: Long-Horizon Offline Reinforcement Learning with Planning Tokens](http://arxiv.org/abs/2409.09513)|null|Supervised learning approaches to offline reinforcement learning, particularly those utilizing the Decision Transformer, have shown effectiveness in continuous environments and for sparse rewards. However, they often struggle with long-horizon tasks due to the high compounding error of auto-regressive models. To overcome this limitation, we go beyond next-token prediction and introduce Planning Tokens, which contain high-level, long time-scale information about the agent's future. Predicting dual time-scale tokens at regular intervals enables our model to use these long-horizon Planning Tokens as a form of implicit planning to guide its low-level policy and reduce compounding error. This architectural modification significantly enhances performance on long-horizon tasks, establishing a new state-of-the-art in complex D4RL environments. Additionally, we demonstrate that Planning Tokens improve the interpretability of the model's policy through the interpretable plan visualisations and attention map.|\n", "2409.09266": "|**2024-09-14**|[TransformerMPC: Accelerating Model Predictive Control via Transformers](http://arxiv.org/abs/2409.09266)|null|In this paper, we address the problem of reducing the computational burden of Model Predictive Control (MPC) for real-time robotic applications. We propose TransformerMPC, a method that enhances the computational efficiency of MPC algorithms by leveraging the attention mechanism in transformers for both online constraint removal and better warm start initialization. Specifically, TransformerMPC accelerates the computation of optimal control inputs by selecting only the active constraints to be included in the MPC problem, while simultaneously providing a warm start to the optimization process. This approach ensures that the original constraints are satisfied at optimality. TransformerMPC is designed to be seamlessly integrated with any MPC solver, irrespective of its implementation. To guarantee constraint satisfaction after removing inactive constraints, we perform an offline verification to ensure that the optimal control inputs generated by the MPC solver meet all constraints. The effectiveness of TransformerMPC is demonstrated through extensive numerical simulations on complex robotic systems, achieving up to 35x improvement in runtime without any loss in performance.|\n", "2409.12026": "|**2024-09-18**|[On Vision Transformers for Classification Tasks in Side-Scan Sonar Imagery](http://arxiv.org/abs/2409.12026)|null|Side-scan sonar (SSS) imagery presents unique challenges in the classification of man-made objects on the seafloor due to the complex and varied underwater environments. Historically, experts have manually interpreted SSS images, relying on conventional machine learning techniques with hand-crafted features. While Convolutional Neural Networks (CNNs) significantly advanced automated classification in this domain, they often fall short when dealing with diverse seafloor textures, such as rocky or ripple sand bottoms, where false positive rates may increase. Recently, Vision Transformers (ViTs) have shown potential in addressing these limitations by utilizing a self-attention mechanism to capture global information in image patches, offering more flexibility in processing spatial hierarchies. This paper rigorously compares the performance of ViT models alongside commonly used CNN architectures, such as ResNet and ConvNext, for binary classification tasks in SSS imagery. The dataset encompasses diverse geographical seafloor types and is balanced between the presence and absence of man-made objects. ViT-based models exhibit superior classification performance across f1-score, precision, recall, and accuracy metrics, although at the cost of greater computational resources. CNNs, with their inductive biases, demonstrate better computational efficiency, making them suitable for deployment in resource-constrained environments like underwater vehicles. Future research directions include exploring self-supervised learning for ViTs and multi-modal fusion to further enhance performance in challenging underwater environments.|\n"}}