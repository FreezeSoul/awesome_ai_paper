{"\u591a\u6a21\u6001": {"2409.02914": "|**2024-09-04**|[Can LVLMs Obtain a Driver's License? A Benchmark Towards Reliable AGI for Autonomous Driving](http://arxiv.org/abs/2409.02914)|null|Large Vision-Language Models (LVLMs) have recently garnered significant attention, with many efforts aimed at harnessing their general knowledge to enhance the interpretability and robustness of autonomous driving models. However, LVLMs typically rely on large, general-purpose datasets and lack the specialized expertise required for professional and safe driving. Existing vision-language driving datasets focus primarily on scene understanding and decision-making, without providing explicit guidance on traffic rules and driving skills, which are critical aspects directly related to driving safety. To bridge this gap, we propose IDKB, a large-scale dataset containing over one million data items collected from various countries, including driving handbooks, theory test data, and simulated road test data. Much like the process of obtaining a driver's license, IDKB encompasses nearly all the explicit knowledge needed for driving from theory to practice. In particular, we conducted comprehensive tests on 15 LVLMs using IDKB to assess their reliability in the context of autonomous driving and provided extensive analysis. We also fine-tuned popular models, achieving notable performance improvements, which further validate the significance of our dataset. The project page can be found at: \\url{https://4dvlab.github.io/project_page/idkb.html}|\n", "2409.02882": "|**2024-09-04**|[Benchmarking Spurious Bias in Few-Shot Image Classifiers](http://arxiv.org/abs/2409.02882)|**[link](https://github.com/gtzheng/fewstab)**|Few-shot image classifiers are designed to recognize and classify new data with minimal supervision and limited data but often show reliance on spurious correlations between classes and spurious attributes, known as spurious bias. Spurious correlations commonly hold in certain samples and few-shot classifiers can suffer from spurious bias induced from them. There is an absence of an automatic benchmarking system to assess the robustness of few-shot classifiers against spurious bias. In this paper, we propose a systematic and rigorous benchmark framework, termed FewSTAB, to fairly demonstrate and quantify varied degrees of robustness of few-shot classifiers to spurious bias. FewSTAB creates few-shot evaluation tasks with biased attributes so that using them for predictions can demonstrate poor performance. To construct these tasks, we propose attribute-based sample selection strategies based on a pre-trained vision-language model, eliminating the need for manual dataset curation. This allows FewSTAB to automatically benchmark spurious bias using any existing test data. FewSTAB offers evaluation results in a new dimension along with a new design guideline for building robust classifiers. Moreover, it can benchmark spurious bias in varied degrees and enable designs for varied degrees of robustness. Its effectiveness is demonstrated through experiments on ten few-shot learning methods across three datasets. We hope our framework can inspire new designs of robust few-shot classifiers. Our code is available at https://github.com/gtzheng/FewSTAB.|\n", "2409.02834": "|**2024-09-06**|[CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of Large Multimodal Models](http://arxiv.org/abs/2409.02834)|null|Large language models (LLMs) have obtained promising results in mathematical reasoning, which is a foundational skill for human intelligence. Most previous studies focus on improving and measuring the performance of LLMs based on textual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few researchers have released English multimodal math datasets (e.g., MATHVISTA and MATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In this paper, we release a Chinese multimodal math (CMM-Math) dataset, including benchmark and training parts, to evaluate and enhance the mathematical reasoning of LMMs. CMM-Math contains over 28,000 high-quality samples, featuring a variety of problem types (e.g., multiple-choice, fill-in-the-blank, and so on) with detailed solutions across 12 grade levels from elementary to high school in China. Specifically, the visual context may be present in the questions or opinions, which makes this dataset more challenging. Through comprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math dataset face challenges, emphasizing the necessity for further improvements in LMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to handle the problems with mixed input of multiple images and text segments. We train our model using three stages, including foundational pre-training, foundational fine-tuning, and mathematical fine-tuning. The extensive experiments indicate that our model effectively improves math reasoning performance by comparing it with the SOTA LMMs over three multimodal mathematical datasets.|\n", "2409.02813": "|**2024-09-04**|[MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark](http://arxiv.org/abs/2409.02813)|null|This paper introduces MMMU-Pro, a robust version of the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark. MMMU-Pro rigorously assesses multimodal models' true understanding and reasoning capabilities through a three-step process based on MMMU: (1) filtering out questions answerable by text-only models, (2) augmenting candidate options, and (3) introducing a vision-only input setting where questions are embedded within images. This setting challenges AI to truly \"see\" and \"read\" simultaneously, testing a fundamental human cognitive skill of seamlessly integrating visual and textual information. Results show that model performance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8% to 26.9% across models. We explore the impact of OCR prompts and Chain of Thought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT generally improves performance. MMMU-Pro provides a more rigorous evaluation tool, closely mimicking real-world scenarios and offering valuable directions for future research in multimodal AI.|\n", "2409.02664": "|**2024-09-04**|[Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection](http://arxiv.org/abs/2409.02664)|null|The proliferation of deepfake faces poses huge potential negative impacts on our daily lives. Despite substantial advancements in deepfake detection over these years, the generalizability of existing methods against forgeries from unseen datasets or created by emerging generative models remains constrained. In this paper, inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach that repurposes a well-trained VLM for general deepfake detection. Motivated by the model reprogramming paradigm that manipulates the model prediction via data perturbations, our method can reprogram a pretrained VLM model (e.g., CLIP) solely based on manipulating its input without tuning the inner parameters. Furthermore, we insert a pseudo-word guided by facial identity into the text prompt. Extensive experiments on several popular benchmarks demonstrate that (1) the cross-dataset and cross-manipulation performances of deepfake detection can be significantly and consistently improved (e.g., over 88% AUC in cross-dataset setting from FF++ to WildDeepfake) using a pre-trained CLIP model with our proposed reprogramming method; (2) our superior performances are at less cost of trainable parameters, making it a promising approach for real-world applications.|\n", "2409.02530": "|**2024-09-04**|[Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models](http://arxiv.org/abs/2409.02530)|null|The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of kidney function in clinical practice. Although traditional equations and Machine Learning (ML) models using clinical and laboratory data can estimate eGFR, accurately predicting future eGFR levels remains a significant challenge for nephrologists and ML researchers. Recent advances demonstrate that Large Language Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust foundation models for diverse applications. This study investigates the potential of LMMs to predict future eGFR levels with a dataset consisting of laboratory and clinical values from 50 patients. By integrating various prompting techniques and ensembles of LMMs, our findings suggest that these models, when combined with precise prompts and visual representations of eGFR trajectories, offer predictive performance comparable to existing ML models. This research extends the application of foundation models and suggests avenues for future studies to harness these models in addressing complex medical forecasting challenges.|\n", "2409.02278": "|**2024-09-03**|[Evaluation and Comparison of Visual Language Models for Transportation Engineering Problems](http://arxiv.org/abs/2409.02278)|null|\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u6700\u65b0\u53d1\u5c55\u663e\u793a\u51fa\u5176\u5728\u56fe\u50cf\u7406\u89e3\u76f8\u5173\u5e94\u7528\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u4ea4\u901a\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4f8b\u5982\u56fe\u50cf\u5206\u7c7b\u548c\u76ee\u6807\u68c0\u6d4b\u3002\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u5305\u62ec\u62e5\u5835\u68c0\u6d4b\u548c\u88c2\u7f1d\u8bc6\u522b\uff0c\u800c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u5219\u7528\u4e8e\u8bc6\u522b\u672a\u4f69\u6234\u5934\u76d4\u7684\u884c\u4e3a\u3002\u6211\u4eec\u5e94\u7528\u4e86CLIP\u3001BLIP\u3001OWL-ViT\u3001Llava-Next\u7b49\u5f00\u6e90\u6a21\u578b\u548c\u95ed\u6e90\u6a21\u578bGPT-4o\uff0c\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4ee5\u5229\u7528\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u6765\u5b8c\u6210\u57fa\u4e8e\u89c6\u89c9\u7684\u4ea4\u901a\u4efb\u52a1\u3002\u8fd9\u4e9b\u4efb\u52a1\u662f\u901a\u8fc7\u5bf9VLM\u6a21\u578b\u5e94\u7528\u96f6\u6837\u672c\u63d0\u793a\u6765\u6267\u884c\u7684\uff0c\u56e0\u4e3a\u96f6\u6837\u672c\u63d0\u793a\u5141\u8bb8\u5728\u4e0d\u5bf9\u4efb\u52a1\u8fdb\u884c\u4efb\u4f55\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u4efb\u52a1\u3002\u5b83\u6d88\u9664\u4e86\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u6807\u6ce8\u6570\u636e\u96c6\u6216\u5fae\u8c03\u7684\u9700\u6c42\u3002\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4e0e\u57fa\u51c6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u4f46\u5728\u76ee\u6807\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4ecd\u6709\u6539\u8fdb\u7684\u7a7a\u95f4\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u5bf9\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u7a81\u51fa\u4e86\u8fd9\u4e9b\u6a21\u578b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u672a\u6765\u6539\u8fdb\u548c\u5927\u89c4\u6a21\u5b9e\u65bd\u7684\u57fa\u7ebf\u3002||\n", "2409.02253": "|**2024-09-03**|[How to Determine the Preferred Image Distribution of a Black-Box Vision-Language Model?](http://arxiv.org/abs/2409.02253)|null|\u5927\u578b\u57fa\u7840\u6a21\u578b\u5f7b\u5e95\u6539\u53d8\u4e86\u8be5\u9886\u57df\uff0c\u4f46\u9488\u5bf9\u7279\u5b9a\u89c6\u89c9\u4efb\u52a1\u4f18\u5316\u591a\u6a21\u6001\u6a21\u578b\u4ecd\u7136\u5b58\u5728\u6311\u6218\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u4e0d\u540c\u8f93\u5165\u63d0\u793a\u4e0b\u8f93\u51fa\u7684\u4e00\u81f4\u6027\uff0c\u6765\u786e\u5b9a\u9ed1\u76d2\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u9996\u9009\u56fe\u50cf\u5206\u5e03\u3002\u6211\u4eec\u5c06\u5176\u5e94\u7528\u4e8e 3D \u5bf9\u8c61\u7684\u4e0d\u540c\u6e32\u67d3\u7c7b\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9700\u8981\u7cbe\u786e\u89e3\u91ca\u590d\u6742\u7ed3\u6784\u7684\u5404\u4e2a\u9886\u57df\u7684\u6709\u6548\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u8ba1\u7b97\u673a\u8f85\u52a9\u8bbe\u8ba1 (CAD) \u4f5c\u4e3a\u793a\u4f8b\u9886\u57df\u3002\u6211\u4eec\u4f7f\u7528\u4eba\u7c7b\u53cd\u9988\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u8fdb\u4e00\u6b65\u5b8c\u5584\u4e86 VLM \u8f93\u51fa\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u91ca\u8d28\u91cf\u3002\u4e3a\u4e86\u89e3\u51b3\u4e13\u4e1a\u9886\u57df\u7f3a\u4e4f\u57fa\u51c6\u7684\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 CAD-VQA\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30 VLM \u5728 CAD \u76f8\u5173\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u65b0\u6570\u636e\u96c6\u3002\u6211\u4eec\u5bf9 CAD-VQA \u4e0a\u6700\u5148\u8fdb\u7684 VLM \u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5efa\u7acb\u4e86\u57fa\u7ebf\u6027\u80fd\u6c34\u5e73\uff0c\u4e3a\u5728\u9700\u8981\u4e13\u5bb6\u7ea7\u89c6\u89c9\u89e3\u91ca\u7684\u5404\u4e2a\u9886\u57df\u63a8\u8fdb VLM \u5728\u590d\u6742\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\u3002\u6211\u4eec\u5728 \\url{https://github.com/asgsaeid/cad_vqa} \u4e0a\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u4ee3\u7801\u3002||\n", "2409.02101": "|**2024-09-03**|[Towards Real-World Adverse Weather Image Restoration: Enhancing Clearness and Semantics with Vision-Language Models](http://arxiv.org/abs/2409.02101)|**[link](https://github.com/jiaqixuac/WResVLM)**|\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6076\u52a3\u5929\u6c14\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u5e94\u7528\u4e8e\u73b0\u5b9e\u573a\u666f\u65f6\u7684\u5c40\u9650\u6027\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6765\u589e\u5f3a\u73b0\u5b9e\u73af\u5883\u4e2d\u4e0d\u540c\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u6062\u590d\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u56fe\u50cf\u6e05\u6670\u5ea6\u8bc4\u4f30\u548c\u8bed\u4e49\u63d0\u4f9b\uff0c\u4f5c\u4e3a\u8bad\u7ec3\u6062\u590d\u6a21\u578b\u7684\u76d1\u7763\u4fe1\u53f7\u3002\u5bf9\u4e8e\u6e05\u6670\u5ea6\u589e\u5f3a\uff0c\u6211\u4eec\u4f7f\u7528\u771f\u5b9e\u6570\u636e\uff0c\u91c7\u7528\u53cc\u91cd\u7b56\u7565\uff0c\u5373\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u7684\u4f2a\u6807\u7b7e\u548c\u5929\u6c14\u63d0\u793a\u5b66\u4e60\u3002\u5bf9\u4e8e\u8bed\u4e49\u589e\u5f3a\uff0c\u6211\u4eec\u901a\u8fc7\u8c03\u6574\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63cf\u8ff0\u4e2d\u7684\u5929\u6c14\u6761\u4ef6\uff0c\u540c\u65f6\u4fdd\u7559\u8bed\u4e49\uff0c\u6765\u6574\u5408\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\u6765\u63d0\u5347\u6062\u590d\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u7684\u6076\u52a3\u5929\u6c14\u56fe\u50cf\u6062\u590d\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7ed3\u679c\uff0c\u901a\u8fc7\u4e0e\u73b0\u6709\u6700\u4f73\u5de5\u4f5c\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u6bd4\u8f83\u8bc1\u660e\u4e86\u8fd9\u4e00\u70b9\u3002||\n", "2409.02084": "|**2024-09-03**|[GraspSplats: Efficient Manipulation with 3D Feature Splatting](http://arxiv.org/abs/2409.02084)|null|The ability for robots to perform efficient and zero-shot grasping of object parts is crucial for practical applications and is becoming prevalent with recent advances in Vision-Language Models (VLMs). To bridge the 2D-to-3D gap for representations to support such a capability, existing methods rely on neural fields (NeRFs) via differentiable rendering or point-based projection methods. However, we demonstrate that NeRFs are inappropriate for scene changes due to their implicitness and point-based methods are inaccurate for part localization without rendering-based optimization. To amend these issues, we propose GraspSplats. Using depth supervision and a novel reference feature computation method, GraspSplats generates high-quality scene representations in under 60 seconds. We further validate the advantages of Gaussian-based representation by showing that the explicit and optimized geometry in GraspSplats is sufficient to natively support (1) real-time grasp sampling and (2) dynamic and articulated object manipulation with point trackers. With extensive experiments on a Franka robot, we demonstrate that GraspSplats significantly outperforms existing methods under diverse task settings. In particular, GraspSplats outperforms NeRF-based methods like F3RM and LERF-TOGO, and 2D detection methods.||\n", "2409.03521": "|**2024-09-05**|[Have Large Vision-Language Models Mastered Art History?](http://arxiv.org/abs/2409.03521)|null|The emergence of large Vision-Language Models (VLMs) has recently established new baselines in image classification across multiple domains. However, the performance of VLMs in the specific task of artwork classification, particularly art style classification of paintings - a domain traditionally mastered by art historians - has not been explored yet. Artworks pose a unique challenge compared to natural images due to their inherently complex and diverse structures, characterized by variable compositions and styles. Art historians have long studied the unique aspects of artworks, with style prediction being a crucial component of their discipline. This paper investigates whether large VLMs, which integrate visual and textual data, can effectively predict the art historical attributes of paintings. We conduct an in-depth analysis of four VLMs, namely CLIP, LLaVA, OpenFlamingo, and GPT-4o, focusing on zero-shot classification of art style, author and time period using two public benchmarks of artworks. Additionally, we present ArTest, a well-curated test set of artworks, including pivotal paintings studied by art historians.|\n", "2409.04053": "|**2024-09-06**|[COLUMBUS: Evaluating COgnitive Lateral Understanding through Multiple-choice reBUSes](http://arxiv.org/abs/2409.04053)|null|While visual question-answering (VQA) benchmarks have catalyzed the development of reasoning techniques, they have focused on vertical thinking. Effective problem-solving also necessitates lateral thinking, which remains understudied in AI and has not been used to test visual perception systems. To bridge this gap, we formulate visual lateral thinking as a multiple-choice question-answering task and describe a three-step taxonomy-driven methodology for instantiating task examples. Then, we develop COLUMBUS, a synthetic benchmark that applies the task pipeline to create QA sets with text and icon rebus puzzles based on publicly available collections of compounds and common phrases. COLUMBUS comprises over 1,000 puzzles, each with four answer candidates. While the SotA vision-language models (VLMs) achieve decent performance, our evaluation demonstrates a substantial gap between humans and models. VLMs benefit from human-curated descriptions but struggle to self-generate such representations at the right level of abstraction.|\n", "2409.03961": "|**2024-09-06**|[Generating Faithful and Salient Text from Multimodal Data](http://arxiv.org/abs/2409.03961)|null|\u867d\u7136\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u5728\u8bb8\u591a\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u5728\u751f\u6210\u6587\u672c\u65f6\u4ecd\u53ef\u80fd\u4f1a\u51fa\u73b0\u5e7b\u89c9\u3002\u5b83\u4eec\u5728\u4ece\u89c6\u89c9\u6570\u636e\u4e2d\u68c0\u6d4b\u663e\u8457\u7279\u5f81\u65b9\u9762\u7684\u6027\u80fd\u4e5f\u4e0d\u6e05\u695a\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6df7\u5408\u6a21\u6001\u6570\u636e\uff08\u5305\u62ec\u56fe\u50cf\u548c\u7ed3\u6784\u5316\u6570\u636e\uff08\u4ee5\u77e5\u8bc6\u56fe\u8c31\u6216\u8868\u683c\u8868\u793a\uff09\uff09\u751f\u6210\u5fe0\u5b9e\u4e14\u663e\u8457\u7684\u6587\u672c\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5c0f\u578b\u89c6\u89c9\u8bc4\u8bba\u5bb6\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u56fe\u50cf\u6a21\u6001\u4e2d\u8bc6\u522b\u5e7b\u89c9\u548c\u975e\u663e\u8457\u7279\u5f81\u3002\u8bc4\u8bba\u5bb6\u6a21\u578b\u8fd8\u4f1a\u751f\u6210\u663e\u8457\u56fe\u50cf\u7279\u5f81\u5217\u8868\u3002\u6b64\u4fe1\u606f\u7528\u4e8e\u540e\u671f\u7f16\u8f91\u6b65\u9aa4\uff0c\u4ee5\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\u3002\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u63d0\u9ad8\u4e86 LMM \u5728\u5fe0\u5b9e\u5ea6\u548c\u663e\u8457\u6027\u65b9\u9762\u7684\u751f\u6210\u8d28\u91cf\uff0c\u4f18\u4e8e\u6700\u8fd1\u65e8\u5728\u51cf\u5c11\u5e7b\u89c9\u7684\u6280\u672f\u3002|\n", "2409.03868": "|**2024-09-05**|[Few-shot Adaptation of Medical Vision-Language Models](http://arxiv.org/abs/2409.03868)|**[link](https://github.com/fereshteshakeri/few-shot-medvlms)**|Integrating image and text data through multi-modal learning has emerged as a new approach in medical imaging research, following its successful deployment in computer vision. While considerable efforts have been dedicated to establishing medical foundation models and their zero-shot transfer to downstream tasks, the popular few-shot setting remains relatively unexplored. Following on from the currently strong emergence of this setting in computer vision, we introduce the first structured benchmark for adapting medical vision-language models (VLMs) in a strict few-shot regime and investigate various adaptation strategies commonly used in the context of natural images. Furthermore, we evaluate a simple generalization of the linear-probe adaptation baseline, which seeks an optimal blending of the visual prototypes and text embeddings via learnable class-wise multipliers. Surprisingly, such a text-informed linear probe yields competitive performances in comparison to convoluted prompt-learning and adapter-based strategies, while running considerably faster and accommodating the black-box setting. Our extensive experiments span three different medical modalities and specialized foundation models, nine downstream tasks, and several state-of-the-art few-shot adaptation methods. We made our benchmark and code publicly available to trigger further developments in this emergent subject: \\url{https://github.com/FereshteShakeri/few-shot-MedVLMs}.|\n"}, "6DOF Object Pose": {"2409.02581": "|**2024-09-04**|[Object Gaussian for Monocular 6D Pose Estimation from Sparse Views](http://arxiv.org/abs/2409.02581)|null|Monocular object pose estimation, as a pivotal task in computer vision and robotics, heavily depends on accurate 2D-3D correspondences, which often demand costly CAD models that may not be readily available. Object 3D reconstruction methods offer an alternative, among which recent advancements in 3D Gaussian Splatting (3DGS) afford a compelling potential. Yet its performance still suffers and tends to overfit with fewer input views. Embracing this challenge, we introduce SGPose, a novel framework for sparse view object pose estimation using Gaussian-based methods. Given as few as ten views, SGPose generates a geometric-aware representation by starting with a random cuboid initialization, eschewing reliance on Structure-from-Motion (SfM) pipeline-derived geometry as required by traditional 3DGS methods. SGPose removes the dependence on CAD models by regressing dense 2D-3D correspondences between images and the reconstructed model from sparse input and random initialization, while the geometric-consistent depth supervision and online synthetic view warping are key to the success. Experiments on typical benchmarks, especially on the Occlusion LM-O dataset, demonstrate that SGPose outperforms existing methods even under sparse view constraints, under-scoring its potential in real-world applications.|\n", "2408.16547": "|**2024-08-29**|[OP-Align: Object-level and Part-level Alignment for Self-supervised Category-level Articulated Object Pose Estimation](http://arxiv.org/abs/2408.16547)|**[link](https://github.com/yc-che/op-align)**|Category-level articulated object pose estimation focuses on the pose estimation of unknown articulated objects within known categories. Despite its significance, this task remains challenging due to the varying shapes and poses of objects, expensive dataset annotation costs, and complex real-world environments. In this paper, we propose a novel self-supervised approach that leverages a single-frame point cloud to solve this task. Our model consistently generates reconstruction with a canonical pose and joint state for the entire input object, and it estimates object-level poses that reduce overall pose variance and part-level poses that align each part of the input with its corresponding part of the reconstruction. Experimental results demonstrate that our approach significantly outperforms previous self-supervised methods and is comparable to the state-of-the-art supervised methods. To assess the performance of our model in real-world scenarios, we also introduce a new real-world articulated object benchmark dataset.|\n", "2408.10450": "|**2024-08-19**|[RUMI: Rummaging Using Mutual Information](http://arxiv.org/abs/2408.10450)|null|This paper presents Rummaging Using Mutual Information (RUMI), a method for online generation of robot action sequences to gather information about the pose of a known movable object in visually-occluded environments. Focusing on contact-rich rummaging, our approach leverages mutual information between the object pose distribution and robot trajectory for action planning. From an observed partial point cloud, RUMI deduces the compatible object pose distribution and approximates the mutual information of it with workspace occupancy in real time. Based on this, we develop an information gain cost function and a reachability cost function to keep the object within the robot's reach. These are integrated into a model predictive control (MPC) framework with a stochastic dynamics model, updating the pose distribution in a closed loop. Key contributions include a new belief framework for object pose estimation, an efficient information gain computation strategy, and a robust MPC-based control scheme. RUMI demonstrates superior performance in both simulated and real tasks compared to baseline methods.|\n", "2408.08234": "|**2024-08-15**|[Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation](http://arxiv.org/abs/2408.08234)|**[link](https://github.com/varunburde/reconstruction_pose_benchmark)**|Object pose estimation is essential to many industrial applications involving robotic manipulation, navigation, and augmented reality. Current generalizable object pose estimators, i.e., approaches that do not need to be trained per object, rely on accurate 3D models. Predominantly, CAD models are used, which can be hard to obtain in practice. At the same time, it is often possible to acquire images of an object. Naturally, this leads to the question whether 3D models reconstructed from images are sufficient to facilitate accurate object pose estimation. We aim to answer this question by proposing a novel benchmark for measuring the impact of 3D reconstruction quality on pose estimation accuracy. Our benchmark provides calibrated images for object reconstruction registered with the test images of the YCB-V dataset for pose evaluation under the BOP benchmark format. Detailed experiments with multiple state-of-the-art 3D reconstruction and object pose estimation approaches show that the geometry produced by modern reconstruction methods is often sufficient for accurate pose estimation. Our experiments lead to interesting observations: (1) Standard metrics for measuring 3D reconstruction quality are not necessarily indicative of pose estimation accuracy, which shows the need for dedicated benchmarks such as ours. (2) Classical, non-learning-based approaches can perform on par with modern learning-based reconstruction techniques and can even offer a better reconstruction time-pose accuracy tradeoff. (3) There is still a sizable gap between performance with reconstructed and with CAD models. To foster research on closing this gap, our benchmark is publicly available at https://github.com/VarunBurde/reconstruction_pose_benchmark}.|\n", "2407.12207": "|**2024-07-16**|[NeuSurfEmb: A Complete Pipeline for Dense Correspondence-based 6D Object Pose Estimation without CAD Models](http://arxiv.org/abs/2407.12207)|**[link](https://github.com/ethz-asl/neusurfemb)**|State-of-the-art approaches for 6D object pose estimation assume the availability of CAD models and require the user to manually set up physically-based rendering (PBR) pipelines for synthetic training data generation. Both factors limit the application of these methods in real-world scenarios. In this work, we present a pipeline that does not require CAD models and allows training a state-of-the-art pose estimator requiring only a small set of real images as input. Our method is based on a NeuS2 object representation, that we learn through a semi-automated procedure based on Structure-from-Motion (SfM) and object-agnostic segmentation. We exploit the novel-view synthesis ability of NeuS2 and simple cut-and-paste augmentation to automatically generate photorealistic object renderings, which we use to train the correspondence-based SurfEmb pose estimator. We evaluate our method on the LINEMOD-Occlusion dataset, extensively studying the impact of its individual components and showing competitive performance with respect to approaches based on CAD models and PBR data. We additionally demonstrate the ease of use and effectiveness of our pipeline on self-collected real-world objects, showing that our method outperforms state-of-the-art CAD-model-free approaches, with better accuracy and robustness to mild occlusions. To allow the robotics community to benefit from this system, we will publicly release it at https://www.github.com/ethz-asl/neusurfemb.|\n", "2406.04316": "|**2024-06-06**|[Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking](http://arxiv.org/abs/2406.04316)|null|6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u4e00\u9879\u81f3\u5173\u91cd\u8981\u4f46\u6781\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5176\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u7f3a\u4e4f\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002\u8fd9\u79cd\u532e\u4e4f\u963b\u788d\u4e86\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u9650\u5236\u4e86\u7814\u7a76\u8fdb\u5c55\u3002\u6b64\u5916\uff0c\u53ef\u7528\u5b9e\u4f8b\u6216\u7c7b\u522b\u7684\u6570\u91cf\u6709\u9650\u4e5f\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86Omni6DPose\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ee5\u5bf9\u8c61\u7c7b\u522b\u591a\u6837\u6027\u3001\u89c4\u6a21\u5927\u548c\u5bf9\u8c61\u6750\u8d28\u591a\u6837\u6027\u4e3a\u7279\u5f81\u7684\u5927\u578b\u6570\u636e\u96c6\u3002Omni6DPose\u4e3b\u8981\u7531\u4e09\u4e2a\u90e8\u5206\u7ec4\u6210\uff1aROPE\uff08\u771f\u5b9e6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u96c6\uff09\uff0c\u5305\u542b332K\u5f20\u56fe\u50cf\uff0c\u6db5\u76d6149\u4e2a\u7c7b\u522b\u4e2d581\u4e2a\u5b9e\u4f8b\u7684\u8d85\u8fc7150\u4e07\u4e2a\u6807\u6ce8\uff1bSOPE\uff08\u6a21\u62df6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u96c6\uff09\uff0c\u5305\u542b475K\u5f20\u5728\u6df7\u5408\u73b0\u5b9e\u73af\u5883\u4e0b\u521b\u5efa\u7684\u5177\u6709\u6df1\u5ea6\u6a21\u62df\u7684\u56fe\u50cf\uff0c\u6db5\u76d6149\u4e2a\u7c7b\u522b\u4e2d4162\u4e2a\u5b9e\u4f8b\u7684\u8d85\u8fc7500\u4e07\u4e2a\u6807\u6ce8\uff1b\u4ee5\u53caROPE\u548cSOPE\u4e2d\u4f7f\u7528\u7684\u7ecf\u8fc7\u624b\u52a8\u5bf9\u9f50\u7684\u771f\u5b9e\u626b\u63cf\u7269\u4f53\u3002\u7531\u4e8e\u5de8\u5927\u7684\u53d8\u5316\u548c\u6b67\u4e49\uff0cOmni6DPose\u672c\u8eab\u5c31\u6781\u5177\u6311\u6218\u6027\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86GenPose++\uff0c\u8fd9\u662f\u5bf9SOTA\u7c7b\u522b\u7ea7\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\u7684\u589e\u5f3a\u7248\u672c\uff0c\u5b83\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6539\u8fdb\uff1a\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u63d0\u53d6\u548c\u57fa\u4e8e\u805a\u7c7b\u7684\u805a\u5408\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u57fa\u51c6\u5206\u6790\uff0c\u4ee5\u8bc4\u4f30\u5148\u524d\u65b9\u6cd5\u5728\u8fd9\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u57286D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u59ff\u6001\u8ddf\u8e2a\u65b9\u9762\u7684\u6027\u80fd\u3002|\n", "2406.02977": "|**2024-06-05**|[Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices](http://arxiv.org/abs/2406.02977)|null|As robotics and augmented reality applications increasingly rely on precise and efficient 6D object pose estimation, real-time performance on edge devices is required for more interactive and responsive systems. Our proposed Sparse Color-Code Net (SCCN) embodies a clear and concise pipeline design to effectively address this requirement. SCCN performs pixel-level predictions on the target object in the RGB image, utilizing the sparsity of essential object geometry features to speed up the Perspective-n-Point (PnP) computation process. Additionally, it introduces a novel pixel-level geometry-based object symmetry representation that seamlessly integrates with the initial pose predictions, effectively addressing symmetric object ambiguities. SCCN notably achieves an estimation rate of 19 frames per second (FPS) and 6 FPS on the benchmark LINEMOD dataset and the Occlusion LINEMOD dataset, respectively, for an NVIDIA Jetson AGX Xavier, while consistently maintaining high estimation accuracy at these rates.|\n", "2405.07801": "|**2024-05-31**|[Deep Learning-Based Object Pose Estimation: A Comprehensive Survey](http://arxiv.org/abs/2405.07801)|**[link](https://github.com/cnjianliu/awesome-object-pose-estimation)**|Object pose estimation is a fundamental computer vision problem with broad applications in augmented reality and robotics. Over the past decade, deep learning models, due to their superior accuracy and robustness, have increasingly supplanted conventional algorithms reliant on engineered point pair features. Nevertheless, several challenges persist in contemporary methods, including their dependency on labeled training data, model compactness, robustness under challenging conditions, and their ability to generalize to novel unseen objects. A recent survey discussing the progress made on different aspects of this area, outstanding challenges, and promising future directions, is missing. To fill this gap, we discuss the recent advances in deep learning-based object pose estimation, covering all three formulations of the problem, \\emph{i.e.}, instance-level, category-level, and unseen object pose estimation. Our survey also covers multiple input data modalities, degrees-of-freedom of output poses, object properties, and downstream tasks, providing the readers with a holistic understanding of this field. Additionally, it discusses training paradigms of different domains, inference modes, application areas, evaluation metrics, and benchmark datasets, as well as reports the performance of current state-of-the-art methods on these benchmarks, thereby facilitating the readers in selecting the most suitable method for their application. Finally, the survey identifies key challenges, reviews the prevailing trends along with their pros and cons, and identifies promising directions for future research. We also keep tracing the latest works at https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation.|\n", "2403.19527": "|**2024-03-28**|[Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation](http://arxiv.org/abs/2403.19527)|**[link](https://github.com/leeiieeo/ag-pose)**|Category-level 6D object pose estimation aims to estimate the rotation, translation and size of unseen instances within specific categories. In this area, dense correspondence-based methods have achieved leading performance. However, they do not explicitly consider the local and global geometric information of different instances, resulting in poor generalization ability to unseen instances with significant shape variations. To deal with this problem, we propose a novel Instance-Adaptive and Geometric-Aware Keypoint Learning method for category-level 6D object pose estimation (AG-Pose), which includes two key designs: (1) The first design is an Instance-Adaptive Keypoint Detection module, which can adaptively detect a set of sparse keypoints for various instances to represent their geometric structures. (2) The second design is a Geometric-Aware Feature Aggregation module, which can efficiently integrate the local and global geometric information into keypoint features. These two modules can work together to establish robust keypoint-level correspondences for unseen instances, thus enhancing the generalization ability of the model.Experimental results on CAMERA25 and REAL275 datasets show that the proposed AG-Pose outperforms state-of-the-art methods by a large margin without category-specific shape priors.|\n", "2403.18791": "|**2024-06-01**|[Object Pose Estimation via the Aggregation of Diffusion Features](http://arxiv.org/abs/2403.18791)|**[link](https://github.com/tianfu18/diff-feats-pose)**|Estimating the pose of objects from images is a crucial task of 3D scene understanding, and recent approaches have shown promising results on very large benchmarks. However, these methods experience a significant performance drop when dealing with unseen objects. We believe that it results from the limited generalizability of image features. To address this problem, we have an in-depth analysis on the features of diffusion models, e.g. Stable Diffusion, which hold substantial potential for modeling unseen objects. Based on this analysis, we then innovatively introduce these diffusion features for object pose estimation. To achieve this, we propose three distinct architectures that can effectively capture and aggregate diffusion features of different granularity, greatly improving the generalizability of object pose estimation. Our approach outperforms the state-of-the-art methods by a considerable margin on three popular benchmark datasets, LM, O-LM, and T-LESS. In particular, our method achieves higher accuracy than the previous best arts on unseen objects: 98.2% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the strong generalizability of our method. Our code is released at https://github.com/Tianfu18/diff-feats-pose.|\n"}, "nerf": {"2408.09130": "|**2024-08-20**|[Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark Images Using Gaussian Splatting](http://arxiv.org/abs/2408.09130)|**[link](https://github.com/yec22/Gaussian-DK)**|3D Gaussian Splatting has recently emerged as a powerful representation that can synthesize remarkable novel views using consistent multi-view images as input. However, we notice that images captured in dark environments where the scenes are not fully illuminated can exhibit considerable brightness variations and multi-view inconsistency, which poses great challenges to 3D Gaussian Splatting and severely degrades its performance. To tackle this problem, we propose Gaussian-DK. Observing that inconsistencies are mainly caused by camera imaging, we represent a consistent radiance field of the physical world using a set of anisotropic 3D Gaussians, and design a camera response module to compensate for multi-view inconsistencies. We also introduce a step-based gradient scaling strategy to constrain Gaussians near the camera, which turn out to be floaters, from splitting and cloning. Experiments on our proposed benchmark dataset demonstrate that Gaussian-DK produces high-quality renderings without ghosting and floater artifacts and significantly outperforms existing methods. Furthermore, we can also synthesize light-up images by controlling exposure levels that clearly show details in shadow areas.|\n", "2407.13520": "|**2024-09-05**|[EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian Splatting](http://arxiv.org/abs/2407.13520)|null|3D deblurring reconstruction techniques have recently seen significant advancements with the development of Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although these techniques can recover relatively clear 3D reconstructions from blurry image inputs, they still face limitations in handling severe blurring and complex camera motion. To address these issues, we propose Event-assisted 3D Deblur Reconstruction with Gaussian Splatting (EaDeblur-GS), which integrates event camera data to enhance the robustness of 3DGS against motion blur. By employing an Adaptive Deviation Estimator (ADE) network to estimate Gaussian center deviations and using novel loss functions, EaDeblur-GS achieves sharp 3D reconstructions in real-time, demonstrating performance comparable to state-of-the-art methods.|\n", "2407.07090": "|**2024-07-10**|[3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes](http://arxiv.org/abs/2407.07090)|null|Particle-based representations of radiance fields such as 3D Gaussian Splatting have found great success for reconstructing and re-rendering of complex scenes. Most existing methods render particles via rasterization, projecting them to screen space tiles for processing in a sorted order. This work instead considers ray tracing the particles, building a bounding volume hierarchy and casting a ray for each pixel using high-performance GPU ray tracing hardware. To efficiently handle large numbers of semi-transparent particles, we describe a specialized rendering algorithm which encapsulates particles with bounding meshes to leverage fast ray-triangle intersections, and shades batches of intersections in depth-order. The benefits of ray tracing are well-known in computer graphics: processing incoherent rays for secondary lighting effects such as shadows and reflections, rendering from highly-distorted cameras common in robotics, stochastically sampling rays, and more. With our renderer, this flexibility comes at little cost compared to rasterization. Experiments demonstrate the speed and accuracy of our approach, as well as several applications in computer graphics and vision. We further propose related improvements to the basic Gaussian representation, including a simple use of generalized kernel functions which significantly reduces particle hit counts.|\n", "2407.05254": "|**2024-07-07**|[GaussReg: Fast 3D Registration with Gaussian Splatting](http://arxiv.org/abs/2407.05254)|null|Point cloud registration is a fundamental problem for large-scale 3D scene scanning and reconstruction. With the help of deep learning, registration methods have evolved significantly, reaching a nearly-mature stage. As the introduction of Neural Radiance Fields (NeRF), it has become the most popular 3D scene representation as its powerful view synthesis capabilities. Regarding NeRF representation, its registration is also required for large-scale scene reconstruction. However, this topic extremly lacks exploration. This is due to the inherent challenge to model the geometric relationship among two scenes with implicit representations. The existing methods usually convert the implicit representation to explicit representation for further registration. Most recently, Gaussian Splatting (GS) is introduced, employing explicit 3D Gaussian. This method significantly enhances rendering speed while maintaining high rendering quality. Given two scenes with explicit GS representations, in this work, we explore the 3D registration task between them. To this end, we propose GaussReg, a novel coarse-to-fine framework, both fast and accurate. The coarse stage follows existing point cloud registration methods and estimates a rough alignment for point clouds from GS. We further newly present an image-guided fine registration approach, which renders images from GS to provide more detailed geometric information for precise alignment. To support comprehensive evaluation, we carefully build a scene-level dataset called ScanNet-GSReg with 1379 scenes obtained from the ScanNet dataset and collect an in-the-wild dataset called GSReg. Experimental results demonstrate our method achieves state-of-the-art performance on multiple datasets. Our GaussReg is 44 times faster than HLoc (SuperPoint as the feature extractor and SuperGlue as the matcher) with comparable accuracy.|\n", "2407.03923": "|**2024-07-04**|[CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion Blur Images](http://arxiv.org/abs/2407.03923)|null|Neural radiance fields (NeRFs) have received significant attention due to their high-quality novel view rendering ability, prompting research to address various real-world cases. One critical challenge is the camera motion blur caused by camera movement during exposure time, which prevents accurate 3D scene reconstruction. In this study, we propose continuous rigid motion-aware gaussian splatting (CRiM-GS) to reconstruct accurate 3D scene from blurry images with real-time rendering speed. Considering the actual camera motion blurring process, which consists of complex motion patterns, we predict the continuous movement of the camera based on neural ordinary differential equations (ODEs). Specifically, we leverage rigid body transformations to model the camera motion with proper regularization, preserving the shape and size of the object. Furthermore, we introduce a continuous deformable 3D transformation in the \\textit{SE(3)} field to adapt the rigid body transformation to real-world problems by ensuring a higher degree of freedom. By revisiting fundamental camera theory and employing advanced neural network training techniques, we achieve accurate modeling of continuous camera trajectories. We conduct extensive experiments, demonstrating state-of-the-art performance both quantitatively and qualitatively on benchmark datasets.|\n", "2406.18214": "|**2024-07-29**|[Trimming the Fat: Efficient Compression of 3D Gaussian Splats through Pruning](http://arxiv.org/abs/2406.18214)|**[link](https://github.com/salmanali96/trimming-the-fat)**|In recent times, the utilization of 3D models has gained traction, owing to the capacity for end-to-end training initially offered by Neural Radiance Fields and more recently by 3D Gaussian Splatting (3DGS) models. The latter holds a significant advantage by inherently easing rapid convergence during training and offering extensive editability. However, despite rapid advancements, the literature still lives in its infancy regarding the scalability of these models. In this study, we take some initial steps in addressing this gap, showing an approach that enables both the memory and computational scalability of such models. Specifically, we propose \"Trimming the fat\", a post-hoc gradient-informed iterative pruning technique to eliminate redundant information encoded in the model. Our experimental findings on widely acknowledged benchmarks attest to the effectiveness of our approach, revealing that up to 75% of the Gaussians can be removed while maintaining or even improving upon baseline performance. Our approach achieves around 50$\\times$ compression while preserving performance similar to the baseline model, and is able to speed-up computation up to 600 FPS.|\n", "2406.15149": "|**2024-06-21**|[Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks](http://arxiv.org/abs/2406.15149)|null|Simulators are powerful tools for autonomous robot learning as they offer scalable data generation, flexible design, and optimization of trajectories. However, transferring behavior learned from simulation data into the real world proves to be difficult, usually mitigated with compute-heavy domain randomization methods or further model fine-tuning. We present a method to improve generalization and robustness to distribution shifts in sim-to-real visual quadrotor navigation tasks. To this end, we first build a simulator by integrating Gaussian Splatting with quadrotor flight dynamics, and then, train robust navigation policies using Liquid neural networks. In this way, we obtain a full-stack imitation learning protocol that combines advances in 3D Gaussian splatting radiance field rendering, crafty programming of expert demonstration training data, and the task understanding capabilities of Liquid networks. Through a series of quantitative flight tests, we demonstrate the robust transfer of navigation skills learned in a single simulation scene directly to the real world. We further show the ability to maintain performance beyond the training environment under drastic distribution and physical environment changes. Our learned Liquid policies, trained on single target manoeuvres curated from a photorealistic simulated indoor flight only, generalize to multi-step hikes onboard a real hardware platform outdoors.|\n", "2406.10373": "|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|Photographs captured in unstructured tourist environments frequently exhibit variable appearances and transient occlusions, challenging accurate scene reconstruction and inducing artifacts in novel view synthesis. Although prior approaches have integrated the Neural Radiance Field (NeRF) with additional learnable modules to handle the dynamic appearances and eliminate transient objects, their extensive training demands and slow rendering speeds limit practical deployments. Recently, 3D Gaussian Splatting (3DGS) has emerged as a promising alternative to NeRF, offering superior training and inference efficiency along with better rendering quality. This paper presents Wild-GS, an innovative adaptation of 3DGS optimized for unconstrained photo collections while preserving its efficiency benefits. Wild-GS determines the appearance of each 3D Gaussian by their inherent material attributes, global illumination and camera properties per image, and point-level local variance of reflectance. Unlike previous methods that model reference features in image space, Wild-GS explicitly aligns the pixel appearance features to the corresponding local Gaussians by sampling the triplane extracted from the reference image. This novel design effectively transfers the high-frequency detailed appearance of the reference view to 3D space and significantly expedites the training process. Furthermore, 2D visibility maps and depth regularization are leveraged to mitigate the transient effects and constrain the geometry, respectively. Extensive experiments demonstrate that Wild-GS achieves state-of-the-art rendering performance and the highest efficiency in both training and inference among all the existing techniques.|\n", "2406.04253": "|**2024-06-06**|[A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation](http://arxiv.org/abs/2406.04253)|null|3D modeling has long been an important area in computer vision and computer graphics. Recently, thanks to the breakthroughs in neural representations and generative models, we witnessed a rapid development of 3D modeling. 3D human modeling, lying at the core of many real-world applications, such as gaming and animation, has attracted significant attention. Over the past few years, a large body of work on creating 3D human avatars has been introduced, forming a new and abundant knowledge base for 3D human modeling. The scale of the literature makes it difficult for individuals to keep track of all the works. This survey aims to provide a comprehensive overview of these emerging techniques for 3D human avatar modeling, from both reconstruction and generation perspectives. Firstly, we review representative methods for 3D human reconstruction, including methods based on pixel-aligned implicit function, neural radiance field, and 3D Gaussian Splatting, etc. We then summarize representative methods for 3D human generation, especially those using large language models like CLIP, diffusion models, and various 3D representations, which demonstrate state-of-the-art performance. Finally, we discuss our reflection on existing methods and open challenges for 3D human avatar modeling, shedding light on future research.|\n", "2406.02720": "|**2024-06-13**|[3D-HGS: 3D Half-Gaussian Splatting](http://arxiv.org/abs/2406.02720)|**[link](https://github.com/lihaolin88/3d-half-gaussian-splatting)**|\u7167\u7247\u7ea7\u903c\u771f\u7684\u4e09\u7ef4\u91cd\u5efa\u662f\u4e09\u7ef4\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\u3002\u7531\u4e8e\u6700\u8fd1\u795e\u7ecf\u6e32\u67d3\u6280\u672f\u7684\u51fa\u73b0\uff0c\u8be5\u9886\u57df\u53d6\u5f97\u4e86\u76f8\u5f53\u5927\u7684\u8fdb\u6b65\u3002\u8fd9\u4e9b\u6280\u672f\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5b66\u4e60\u4e09\u7ef4\u573a\u666f\u7684\u4f53\u79ef\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u6e32\u67d3\u5f97\u5230\u7684\u635f\u5931\u51fd\u6570\u6765\u7ec6\u5316\u8fd9\u4e9b\u8868\u793a\u3002\u5176\u4e2d\uff0c\u4e09\u7ef4\u9ad8\u65af\u6563\u5c04\uff083D-GS\uff09\u5df2\u6210\u4e3a\u4e00\u79cd\u91cd\u8981\u7684\u65b9\u6cd5\uff0c\u5176\u6027\u80fd\u8d85\u8fc7\u4e86\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRFs\uff09\u30023D-GS\u4f7f\u7528\u53c2\u6570\u5316\u7684\u4e09\u7ef4\u9ad8\u65af\u51fd\u6570\u6765\u5efa\u6a21\u7a7a\u95f4\u4f4d\u7f6e\u548c\u989c\u8272\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u56fe\u5757\u7684\u5feb\u901f\u6e32\u67d3\u6280\u672f\u3002\u5c3d\u7ba1\u5176\u6e32\u67d3\u6027\u80fd\u548c\u901f\u5ea6\u90fd\u5f88\u51fa\u8272\uff0c\u4f46\u4f7f\u7528\u4e09\u7ef4\u9ad8\u65af\u6838\u51fd\u6570\u5728\u51c6\u786e\u8868\u793a\u4e0d\u8fde\u7eed\u51fd\u6570\u65b9\u9762\u5b58\u5728\u56fa\u6709\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u5f62\u72b6\u4e0d\u8fde\u7eed\u7684\u8fb9\u7f18\u548c\u89d2\u843d\uff0c\u4ee5\u53ca\u5728\u989c\u8272\u4e0d\u8fde\u7eed\u7684\u4e0d\u540c\u7eb9\u7406\u4e4b\u95f4\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u8bae\u91c7\u7528\u4e09\u7ef4\u534a\u9ad8\u65af\uff083D-HGS\uff09\u6838\u51fd\u6570\uff0c\u5b83\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u6838\u51fd\u6570\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u4eec\u80fd\u591f\u63d0\u9ad8\u5f53\u524d\u4e0e3D-GS\u76f8\u5173\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u5728\u4e0d\u5f71\u54cd\u6e32\u67d3\u901f\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6e32\u67d3\u6027\u80fd\u3002||\n", "2409.03213": "|**2024-09-05**|[Optimizing 3D Gaussian Splatting for Sparse Viewpoint Scene Reconstruction](http://arxiv.org/abs/2409.03213)|null|3D Gaussian Splatting (3DGS) has emerged as a promising approach for 3D scene representation, offering a reduction in computational overhead compared to Neural Radiance Fields (NeRF). However, 3DGS is susceptible to high-frequency artifacts and demonstrates suboptimal performance under sparse viewpoint conditions, thereby limiting its applicability in robotics and computer vision. To address these limitations, we introduce SVS-GS, a novel framework for Sparse Viewpoint Scene reconstruction that integrates a 3D Gaussian smoothing filter to suppress artifacts. Furthermore, our approach incorporates a Depth Gradient Profile Prior (DGPP) loss with a dynamic depth mask to sharpen edges and 2D diffusion with Score Distillation Sampling (SDS) loss to enhance geometric consistency in novel view synthesis. Experimental evaluations on the MipNeRF-360 and SeaThru-NeRF datasets demonstrate that SVS-GS markedly improves 3D reconstruction from sparse viewpoints, offering a robust and efficient solution for scene understanding in robotics and computer vision applications.|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2409.02838": "|**2024-09-04**|[iConFormer: Dynamic Parameter-Efficient Tuning with Input-Conditioned Adaptation](http://arxiv.org/abs/2409.02838)|null|\u57fa\u4e8e\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u5b8c\u6574\u5fae\u8c03\uff08FFT\uff09\u548c\u4efb\u52a1\u7279\u5b9a\u89e3\u7801\u5668\u7684\u8fc1\u79fb\u5b66\u4e60\u968f\u7740\u6df1\u5ea6\u6a21\u578b\u7684\u6307\u6570\u7ea7\u589e\u957f\u800c\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\u3002\u4f7f\u7528\u7531\u5c0f\u578b\u53ef\u5b66\u4e60\u5c42\u7ec4\u6210\u7684\u9002\u914d\u5668\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u5df2\u6210\u4e3a FFT \u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u8bad\u7ec3\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u53ef\u6bd4\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u9002\u914d\u5668\u5bf9\u8f93\u5165\u5b9e\u4f8b\u7684\u4e0d\u7075\u6d3b\u9650\u5236\u4e86\u5176\u5728\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u4fe1\u606f\u7684\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 PEFT \u65b9\u6cd5\uff0c\u5373\u8f93\u5165\u6761\u4ef6\u5316\u7684 Transformer\uff0c\u79f0\u4e3a iConFormer\uff0c\u5b83\u5229\u7528\u4e86\u4ee5\u8f93\u5165\u5b9e\u4f8b\u4e3a\u6761\u4ef6\u7684\u52a8\u6001\u9002\u914d\u5668\u3002\u4e3a\u4e86\u786e\u4fdd\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5bf9\u8f93\u5165\u5b9e\u4f8b\u7684\u7075\u6d3b\u5b66\u4e60\u80fd\u529b\uff0c\u6211\u4eec\u5728\u52a8\u6001\u9002\u914d\u5668\u4e2d\u5f15\u5165\u4e86\u8f93\u5165\u6761\u4ef6\u5316\u7f51\u7edc\uff08iCoN\uff09\uff0c\u4ece\u800c\u5b9e\u73b0\u5b9e\u4f8b\u7ea7\u7279\u5f81\u8f6c\u6362\u3002\u5177\u4f53\u6765\u8bf4\uff0ciCoN \u4e3a\u6bcf\u4e2a\u7279\u5f81\u751f\u6210\u901a\u9053\u7ea7\u7684\u5377\u79ef\u6838\uff0c\u5e76\u4f7f\u7528\u81ea\u9002\u5e94\u5377\u79ef\u8fc7\u7a0b\u5bf9\u5176\u8fdb\u884c\u8f6c\u6362\uff0c\u4ee5\u6709\u6548\u6355\u83b7\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u4efb\u52a1\u7279\u5b9a\u548c\u7ec6\u7c92\u5ea6\u7ec6\u8282\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u4ec5\u8c03\u6574 Transformer \u4e3b\u5e72\u53c2\u6570\u7684 1.6% \u5230 2.8%\uff0ciConFormer \u5728\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u548c\u8bed\u4e49\u5206\u5272\u65b9\u9762\u5b9e\u73b0\u4e86\u4e0e FFT \u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u5b9e\u4f8b\u5206\u5272\u65b9\u9762\u4f18\u4e8e FFT\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6240\u6709\u4e0a\u8ff0\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u8fd1\u7684 PEFT \u65b9\u6cd5\u3002||\n", "2409.02546": "|**2024-09-04**|[Real-Time Dynamic Scale-Aware Fusion Detection Network: Take Road Damage Detection as an example](http://arxiv.org/abs/2409.02546)|null|\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u9053\u8def\u635f\u574f\u68c0\u6d4b (RDD) \u5bf9\u57ce\u5e02\u7684\u65e5\u5e38\u7ef4\u62a4\u548c\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u663e\u8457\u964d\u4f4e\u52b3\u52a8\u529b\u6210\u672c\u65b9\u9762\u3002\u7136\u800c\uff0c\u5f53\u524d\u57fa\u4e8e\u65e0\u4eba\u673a\u7684 RDD \u7814\u7a76\u4ecd\u9762\u4e34\u8bb8\u591a\u6311\u6218\u3002\u4f8b\u5982\uff0c\u5f62\u72b6\u548c\u65b9\u5411\u4e0d\u89c4\u5219\u7684\u635f\u574f\u3001\u80cc\u666f\u5bf9\u635f\u574f\u7684\u906e\u6321\u4ee5\u53ca\u96be\u4ee5\u533a\u5206\u635f\u574f\u548c\u80cc\u666f\uff0c\u8fd9\u4e9b\u56e0\u7d20\u90fd\u663e\u8457\u5f71\u54cd\u4e86\u65e0\u4eba\u673a\u5728\u65e5\u5e38\u5de1\u68c0\u4e2d\u68c0\u6d4b\u9053\u8def\u635f\u574f\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u63d0\u9ad8\u65e0\u4eba\u673a\u5b9e\u65f6\u9053\u8def\u635f\u574f\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u6211\u4eec\u8bbe\u8ba1\u5e76\u63d0\u51fa\u4e86\u4e09\u4e2a\u76f8\u5e94\u7684\u6a21\u5757\uff1a\u4e00\u4e2a\u80fd\u591f\u7075\u6d3b\u9002\u5e94\u5f62\u72b6\u548c\u80cc\u666f\u7684\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff1b\u4e00\u4e2a\u878d\u5408\u591a\u5c3a\u5ea6\u611f\u77e5\u5e76\u9002\u5e94\u5f62\u72b6\u548c\u80cc\u666f\u7684\u6a21\u5757\uff1b\u4e00\u4e2a\u9ad8\u6548\u7684\u4e0b\u91c7\u6837\u6a21\u5757\u3002 \u57fa\u4e8e\u8fd9\u4e9b\u6a21\u5757\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5177\u6709\u81ea\u52a8\u53bb\u9664\u80cc\u666f\u5e72\u6270\u80fd\u529b\u7684\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u9053\u8def\u635f\u574f\u68c0\u6d4b\u6a21\u578b\uff0c\u79f0\u4e3a\u52a8\u6001\u5c3a\u5ea6\u611f\u77e5\u878d\u5408\u68c0\u6d4b\u6a21\u578b (RT-DSAFDet)\u3002\u5728 UAV-PDD2023 \u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b RT-DSAFDet \u7684 mAP50 \u8fbe\u5230\u4e86 54.2%\uff0c\u6bd4\u6700\u65b0\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u6a21\u578b YOLOv10 \u7684\u9ad8\u6548\u53d8\u4f53 YOLOv10-m \u9ad8 11.1%\uff0c\u800c\u53c2\u6570\u91cf\u51cf\u5c11\u5230 1.8M\uff0cFLOPs \u51cf\u5c11\u5230 4.6G\uff0c\u5206\u522b\u964d\u4f4e\u4e86 88% \u548c 93%\u3002\u6b64\u5916\uff0c\u5728\u5927\u578b\u901a\u7528\u76ee\u6807\u68c0\u6d4b\u516c\u5f00\u6570\u636e\u96c6 MS COCO2017 \u4e0a\u4e5f\u5c55\u73b0\u4e86\u6211\u4eec\u6a21\u578b\u7684\u4f18\u8d8a\u6027\uff0c\u5176 mAP50-95 \u4e0e YOLOv9-t \u76f8\u540c\uff0c\u4f46 mAP50 \u9ad8\u51fa 0.5%\uff0c\u53c2\u6570\u91cf\u51cf\u5c11 10%\uff0cFLOPs \u51cf\u5c11 40%\u3002||\n", "2409.02486": "|**2024-09-04**|[Boosting Generalizability towards Zero-Shot Cross-Dataset Single-Image Indoor Depth by Meta-Initialization](http://arxiv.org/abs/2409.02486)|null|\u5ba4\u5185\u673a\u5668\u4eba\u7684\u5bfc\u822a\u6216\u969c\u788d\u7269\u68c0\u6d4b\u7b49\u4efb\u52a1\u4f9d\u8d56\u4e8e\u6df1\u5ea6\u4fe1\u606f\uff0c\u800c\u5355\u56fe\u50cf\u6df1\u5ea6\u4f30\u8ba1\u88ab\u5e7f\u6cdb\u7528\u4e8e\u8f85\u52a9\u611f\u77e5\u3002\u5927\u591a\u6570\u5ba4\u5185\u5355\u56fe\u50cf\u6df1\u5ea6\u9884\u6d4b\u8f83\u5c11\u5173\u6ce8\u6a21\u578b\u5bf9\u672a\u89c1\u6570\u636e\u96c6\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u66f4\u5173\u6ce8\u7cfb\u7edf\u90e8\u7f72\u7684\u91ce\u5916\u9c81\u68d2\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u5229\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u5143\u5b66\u4e60\u5728\u96f6\u6837\u672c\u8de8\u6570\u636e\u96c6\u63a8\u7406\u4e2d\u83b7\u5f97\u66f4\u9ad8\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4e0e\u7814\u7a76\u6700\u591a\u7684\u3001\u4e0e\u663e\u5f0f\u7c7b\u522b\u6807\u7b7e\u76f8\u5173\u7684\u56fe\u50cf\u5206\u7c7b\u5143\u5b66\u4e60\u4e0d\u540c\uff0c\u5bf9\u4e8e\u4e0e\u7269\u4f53\u6392\u5217\u548c\u573a\u666f\u6784\u6210\u65b9\u9762\u9ad8\u5ea6\u53d8\u5316\u7684\u5ba4\u5185\u73af\u5883\u76f8\u5173\u7684\u8fde\u7eed\u6df1\u5ea6\u503c\uff0c\u4e0d\u5b58\u5728\u660e\u786e\u7684\u4efb\u52a1\u8fb9\u754c\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u7ec6\u7c92\u5ea6\u4efb\u52a1\uff0c\u5728\u6211\u4eec\u7684\u5143\u5b66\u4e60\u516c\u5f0f\u4e2d\u5c06\u6bcf\u4e2aRGB-D\u5c0f\u6279\u91cf\u89c6\u4e3a\u4e00\u4e2a\u4efb\u52a1\u3002\u6211\u4eec\u9996\u5148\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6709\u9650\u6570\u636e\u4e0a\u8bf1\u5bfc\u51fa\u66f4\u597d\u7684\u5148\u9a8c\uff08RMSE \u6700\u9ad8\u964d\u4f4e 27.8%\uff09\u3002\u7136\u540e\uff0c\u5728\u5143\u5b66\u4e60\u521d\u59cb\u5316\u4e0a\u8fdb\u884c\u5fae\u8c03\u59cb\u7ec8\u4f18\u4e8e\u6ca1\u6709\u5143\u65b9\u6cd5\u7684\u57fa\u7ebf\u3002\u4e3a\u4e86\u5b9e\u73b0\u6cdb\u5316\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u96f6\u6837\u672c\u8de8\u6570\u636e\u96c6\u534f\u8bae\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7531\u6211\u4eec\u7684\u5143\u521d\u59cb\u5316\u8bf1\u5bfc\u7684\u66f4\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u4f5c\u4e3a\u8bb8\u591a\u73b0\u6709\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u7684\u7b80\u5355\u800c\u6709\u7528\u7684\u63d2\u4ef6\u3002\u6df1\u5ea6\u548c\u5143\u5b66\u4e60\u4ea4\u53c9\u9886\u57df\u7684\u5de5\u4f5c\u6709\u53ef\u80fd\u63a8\u52a8\u8fd9\u4e24\u9879\u7814\u7a76\u66f4\u63a5\u8fd1\u5b9e\u9645\u7684\u673a\u5668\u4eba\u548c\u673a\u5668\u611f\u77e5\u5e94\u7528\u3002||\n", "2409.02329": "|**2024-09-03**|[Site Selection for the Second Flyeye Telescope: A Simulation Study for Optimizing Near-Earth Object Discovery](http://arxiv.org/abs/2409.02329)|null|\u6b27\u6d32\u822a\u5929\u5c40 (ESA) \u6b63\u5728\u5f00\u53d1\u4e00\u4e2a\u540d\u4e3a Flyeye \u7684\u5e7f\u57df\u5de1\u5929\u671b\u8fdc\u955c\u7f51\u7edc\uff0c\u4ee5\u6539\u8fdb\u8fd1\u5730\u5929\u4f53 (NEO) \u7684\u53d1\u73b0\u3002\u8be5\u7f51\u7edc\u4e2d\u7684\u7b2c\u4e00\u4e2a\u671b\u8fdc\u955c\u5c06\u4f4d\u4e8e\u5317\u534a\u7403\u7684\u7a46\u6cd5\u62c9\u5c71\uff08\u610f\u5927\u5229\uff09\uff0c\u800c\u7b2c\u4e8c\u4e2a\u5177\u6709\u589e\u5f3a\u63a2\u6d4b\u80fd\u529b\u7684 Flyeye \u671b\u8fdc\u955c\u521a\u521a\u5f00\u59cb\u5173\u952e\u8bbe\u8ba1\u9636\u6bb5\u3002\u901a\u8fc7\u5bf9\u649e\u51fb\u8f68\u8ff9\u4e0a\u7684\u8fd1\u5730\u5929\u4f53\u8fdb\u884c\u6a21\u62df\uff0c\u7814\u7a76\u4e86\u7b2c\u4e8c\u4e2a Flyeye \u671b\u8fdc\u955c\u7684\u6f5c\u5728\u4f4d\u7f6e\u3002\u5bf9\u5927\u7ea6 3000 \u4e2a\u649e\u51fb\u5c0f\u884c\u661f\uff08\u7edd\u5bf9\u661f\u7b49\u4e3a H=25 \u548c H=28\uff09\u8fdb\u884c\u4e86\u4f20\u64ad\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e3b\u8981\u73b0\u6709\u5de1\u5929\u9879\u76ee\uff08Catalina\u3001Pan-STARRS\u3001ATLAS\uff09\u3001\u5373\u5c06\u6295\u5165\u4f7f\u7528\u7684\u8587\u62c9\u00b7\u9c81\u5bbe\u5929\u6587\u53f0 (LSST) \u4ee5\u53ca Flyeye \u53ef\u80fd\u9009\u5740\u7684\u53ef\u63a2\u6d4b\u6027\u3002 \u8003\u8651\u4e86\u667a\u5229\u3001\u5357\u975e\u548c\u5317\u534a\u7403\u7684\u7b2c\u4e8c\u4e2a\u8bbe\u65bd\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u5929\u6587\u53f0\uff0c\u5728\u6a21\u62df\u4e2d\u90fd\u8003\u8651\u4e86\u5b83\u4eec\u8fc7\u53bb\u6216\u8ba1\u5212\u7684\u6307\u5411\u7b56\u7565\u3002\u5728 LSST \u90e8\u7f72\u4e4b\u524d\uff0c\u5357\u534a\u7403\u7684\u4e00\u4e2a Flyeye \u7684\u6027\u80fd\u4e0e\u5317\u534a\u7403\u7684\u4e00\u4e2a\u671b\u8fdc\u955c\u76f8\u4f3c\u3002\u7ed3\u5408\u8d77\u6765\uff0c\u5728\u5317\u65b9\u548c\u5357\u65b9\u5404\u653e\u7f6e\u4e00\u53f0\u671b\u8fdc\u955c\u53ef\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u63a2\u6d4b\u7387\u548c\u63a2\u6d4b\u5230\u7684\u72ec\u7279\u7269\u4f53\u7684\u6570\u91cf\u3002LSST \u4e4b\u540e\uff0c\u5357\u90e8\u548c\u5317\u90e8\u7684 Flyeye \u671b\u8fdc\u955c\u4ecd\u7136\u662f\u4e92\u8865\u7684\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6a21\u62df\u8868\u660e\uff0c\u65e0\u8bba\u662f\u5728 LSST \u4e4b\u524d\u8fd8\u662f\u4e4b\u540e\uff0c\u4f4d\u4e8e\u5357\u90e8\u7684\u7b2c\u4e8c\u4e2a Flyeye \u90fd\u53ef\u4ee5\u8865\u5145\u4f4d\u4e8e\u5317\u90e8\u7684 Flyeye \u671b\u8fdc\u955c\u3002\u4f4d\u4e8e\u62c9\u897f\u62c9\u7684 Flyeye \u5c06\u5229\u7528\u5176\u4f18\u8d8a\u7684\u5927\u6c14\u6761\u4ef6\uff0c\u540c\u65f6\u5e73\u8861\u5357\u5317\u534a\u7403\u7684\u8d44\u4ea7\u3002||\n", "2409.02281": "|**2024-09-03**|[K-Origins: Better Colour Quantification for Neural Networks](http://arxiv.org/abs/2409.02281)|**[link](https://github.com/lewismmason/Thesis-Public)**|K-Origins\u662f\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u5c42\uff0c\u65e8\u5728\u5728\u5b66\u4e60\u989c\u8272\u6216\u5f3a\u5ea6\u6709\u5229\u65f6\u63d0\u9ad8\u57fa\u4e8e\u56fe\u50cf\u7684\u7f51\u7edc\u6027\u80fd\u3002 \u8d85\u8fc7 250 \u4e2a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5377\u79ef\u7f51\u7edc\u5728 16 \u4f4d\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0cK-Origins \u63d0\u9ad8\u4e86\u8bed\u4e49\u5206\u5272\u7cbe\u5ea6\uff1a\u4f4e\u4fe1\u566a\u6bd4\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u4ee5\u53ca\u5206\u5272\u5f62\u72b6\u76f8\u540c\u4f46\u989c\u8272\u4e0d\u540c\u7684\u591a\u4e2a\u76ee\u6807\u3002 \u5bf9\u4e8e\u6bcf\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570$w_k$\uff0cK-Origins \u901a\u8fc7\u516c\u5f0f $\\textbf{Y}_k = \\textbf{X}-\\textbf{J}\\cdot w_k$ \u4ece\u8f93\u5165\u7279\u5f81 $\\textbf{X}$ \u751f\u6210\u8f93\u51fa\u7279\u5f81\uff0c\u5176\u4e2d $\\textbf{J}$ \u662f\u4e00\u4e2a\u5168 1 \u77e9\u9635\u3002 \u6b64\u5916\uff0c\u8fd8\u8bad\u7ec3\u4e86\u5177\u6709\u4e0d\u540c\u611f\u53d7\u91ce\u7684\u7f51\u7edc\uff0c\u4ee5\u6839\u636e\u76ee\u6807\u7c7b\u522b\u7684\u7ef4\u5ea6\u786e\u5b9a\u6700\u4f73\u7f51\u7edc\u6df1\u5ea6\uff0c\u8fd9\u8868\u660e\u611f\u53d7\u91ce\u957f\u5ea6\u5e94\u8d85\u8fc7\u76ee\u6807\u5927\u5c0f\u3002 \u901a\u8fc7\u786e\u4fdd\u8db3\u591f\u7684\u611f\u53d7\u91ce\u957f\u5ea6\u5e76\u7ed3\u5408 K-Origins\uff0c\u6211\u4eec\u53ef\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u8bed\u4e49\u7f51\u7edc\u6027\u80fd\u3002||\n", "2409.02278": "|**2024-09-03**|[Evaluation and Comparison of Visual Language Models for Transportation Engineering Problems](http://arxiv.org/abs/2409.02278)|null|\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\u5c55\u73b0\u51fa\u5176\u5728\u56fe\u50cf\u7406\u89e3\u76f8\u5173\u5e94\u7528\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u4ea4\u901a\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4f8b\u5982\u56fe\u50cf\u5206\u7c7b\u548c\u76ee\u6807\u68c0\u6d4b\u3002\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u5305\u62ec\u62e5\u5835\u68c0\u6d4b\u548c\u88c2\u7f1d\u8bc6\u522b\uff0c\u800c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u5219\u7528\u4e8e\u8bc6\u522b\u672a\u4f69\u6234\u5934\u76d4\u7684\u884c\u4e3a\u3002\u6211\u4eec\u5e94\u7528\u4e86\u5f00\u6e90\u6a21\u578b\uff08\u5982CLIP\u3001BLIP\u3001OWL-ViT\u3001Llava-Next\uff09\u548c\u95ed\u6e90\u6a21\u578bGPT-4o\uff0c\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4ee5\u5229\u7528\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u6765\u5b8c\u6210\u57fa\u4e8e\u89c6\u89c9\u7684\u4ea4\u901a\u4efb\u52a1\u3002\u8fd9\u4e9b\u4efb\u52a1\u901a\u8fc7\u5bf9VLM\u6a21\u578b\u5e94\u7528\u96f6\u6837\u672c\u63d0\u793a\u6765\u5b8c\u6210\uff0c\u56e0\u4e3a\u96f6\u6837\u672c\u63d0\u793a\u53ef\u4ee5\u5728\u4e0d\u5bf9\u4efb\u52a1\u8fdb\u884c\u4efb\u4f55\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u4efb\u52a1\u3002\u8fd9\u6d88\u9664\u4e86\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u6807\u6ce8\u6570\u636e\u96c6\u6216\u5fae\u8c03\u7684\u9700\u6c42\u3002\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4e0e\u57fa\u51c6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u4f46\u5728\u76ee\u6807\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4ecd\u6709\u6539\u8fdb\u7684\u7a7a\u95f4\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u5bf9\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u7a81\u51fa\u4e86\u8fd9\u4e9b\u6a21\u578b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u672a\u6765\u6539\u8fdb\u548c\u5e7f\u6cdb\u5b9e\u65bd\u7684\u57fa\u51c6\u3002||\n", "2409.02035": "|**2024-09-03**|[A Modern Take on Visual Relationship Reasoning for Grasp Planning](http://arxiv.org/abs/2409.02035)|null|\u4e0e\u73b0\u5b9e\u4e16\u754c\u6742\u4e71\u573a\u666f\u4ea4\u4e92\u5bf9\u673a\u5668\u4eba\u4ee3\u7406\u63d0\u51fa\u4e86\u82e5\u5e72\u6311\u6218\uff0c\u8fd9\u4e9b\u4ee3\u7406\u9700\u8981\u7406\u89e3\u89c2\u5bdf\u5230\u7684\u7269\u4f53\u4e4b\u95f4\u590d\u6742\u7684\u7684\u7a7a\u95f4\u4f9d\u8d56\u6027\uff0c\u4ee5\u786e\u5b9a\u6700\u4f73\u62fe\u53d6\u987a\u5e8f\u6216\u6709\u6548\u7684\u7269\u4f53\u68c0\u7d22\u7b56\u7565\u3002 \u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u7ba1\u7406\u7b80\u5316\u7684\u573a\u666f\uff0c\u5e76\u4fa7\u91cd\u4e8e\u5728\u521d\u59cb\u7269\u4f53\u68c0\u6d4b\u9636\u6bb5\u4e4b\u540e\u9884\u6d4b\u6210\u5bf9\u7269\u4f53\u5173\u7cfb\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u5168\u5c40\u4e0a\u4e0b\u6587\u6216\u96be\u4ee5\u5904\u7406\u5197\u4f59\u548c\u7f3a\u5931\u7684\u7269\u4f53\u5173\u7cfb\u3002 \u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6293\u53d6\u89c4\u5212\u7684\u89c6\u89c9\u5173\u7cfb\u63a8\u7406\u7684\u73b0\u4ee3\u65b9\u6cd5\u3002 \u6211\u4eec\u4ecb\u7ecd\u4e86 D3GD\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5176\u4e2d\u5305\u62ec\u5305\u542b\u6765\u81ea 97 \u4e2a\u4e0d\u540c\u7c7b\u522b\u7684\u591a\u8fbe 35 \u4e2a\u7269\u4f53\u7684\u5206\u62e3\u573a\u666f\u3002 \u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86 D3G\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u7aef\u5230\u7aef transformer \u7684\u4f9d\u8d56\u56fe\u751f\u6210\u6a21\u578b\uff0c\u5b83\u53ef\u4ee5\u540c\u65f6\u68c0\u6d4b\u7269\u4f53\u5e76\u751f\u6210\u8868\u793a\u5176\u7a7a\u95f4\u5173\u7cfb\u7684\u90bb\u63a5\u77e9\u9635\u3002 \u8ba4\u8bc6\u5230\u6807\u51c6\u6307\u6807\u7684\u5c40\u9650\u6027\uff0c\u6211\u4eec\u9996\u6b21\u91c7\u7528\u5173\u7cfb\u5e73\u5747\u7cbe\u5ea6\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u57fa\u51c6\u6d4b\u8bd5\u3002 \u83b7\u5f97\u7684\u7ed3\u679c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u8fd9\u9879\u4efb\u52a1\u7684\u6700\u65b0\u6280\u672f\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002 \u6211\u4eec\u5728 https://paolotron.github.io/d3g.github.io \u4e0a\u516c\u5f00\u53d1\u5e03\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002||\n", "2409.01988": "|**2024-09-03**|[Compressed learning based onboard semantic compression for remote sensing platforms](http://arxiv.org/abs/2409.01988)|null|\u5730\u7403\u89c2\u6d4b (EO) \u5728\u521b\u5efa\u548c\u7ef4\u6301\u4e00\u4e2a\u5177\u6709\u5f39\u6027\u548c\u7e41\u8363\u7684\u793e\u4f1a\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u8fd9\u5bf9\u6240\u6709\u751f\u547d\u548c\u5730\u7403\u672c\u8eab\u90fd\u5177\u6709\u6df1\u8fdc\u7684\u5f71\u54cd\u3002\u536b\u661f\u3001\u822a\u7a7a\u5e73\u53f0\u4ee5\u53ca\u6700\u8fd1\u7684\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u9a7e\u9a76\u98de\u884c\u5668\u7b49\u9065\u611f\u5e73\u53f0\u90fd\u7528\u4e8e EO\u3002\u5b83\u4eec\u6536\u96c6\u5927\u91cf\u6570\u636e\uff0c\u9700\u8981\u5c06\u5176\u4e0b\u4f20\u5230\u5730\u7403\u8fdb\u884c\u8fdb\u4e00\u6b65\u5904\u7406\u548c\u5206\u6790\u3002\u8fd9\u79cd\u9ad8\u541e\u5410\u91cf\u91c7\u96c6\u7684\u74f6\u9888\u662f\u4e0b\u884c\u94fe\u8def\u5e26\u5bbd\u3002\u9700\u8981\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u56fe\u50cf\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u6765\u5e94\u5bf9\u8fd9\u79cd\u6d77\u91cf\u6570\u636e\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u901a\u8fc7\u538b\u7f29\u5b66\u4e60\u6846\u67b6\u7814\u7a76\u4e86\u8bed\u4e49\u538b\u7f29\uff0c\u8be5\u6846\u67b6\u4ec5\u5229\u7528\u5feb\u901f\u548c\u7a00\u758f\u7684\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\u6765\u7f16\u7801\u6570\u636e\u3002\u76f8\u673a\u566a\u58f0\u548c\u901a\u4fe1\u4fe1\u9053\u662f\u9020\u6210\u5931\u771f\u7684\u4e3b\u8981\u6765\u6e90\u3002\u7136\u540e\uff0c\u5b8c\u6574\u7684\u8bed\u4e49\u901a\u4fe1\u7ba1\u9053\u7531\u4e00\u4e2a\u5b66\u4e60\u5230\u7684\u4f4e\u590d\u6742\u5ea6\u538b\u7f29\u77e9\u9635\u7ec4\u6210\uff0c\u8be5\u77e9\u9635\u4f5c\u7528\u4e8e\u566a\u58f0\u76f8\u673a\u8f93\u51fa\uff0c\u4ee5\u5728\u673a\u8f7d\u751f\u6210\u4e00\u4e2a\u89c2\u6d4b\u5411\u91cf\uff0c\u8be5\u5411\u91cf\u901a\u8fc7\u901a\u4fe1\u4fe1\u9053\u4e0b\u884c\u94fe\u8def\u4f20\u8f93\uff0c\u901a\u8fc7\u5c55\u5f00\u7f51\u7edc\u5904\u7406\uff0c\u7136\u540e\u9988\u9001\u5230\u6267\u884c\u5fc5\u8981\u4e0b\u6e38\u4efb\u52a1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1b\u7814\u7a76\u4e86\u56fe\u50cf\u5206\u7c7b\u3002\u901a\u8fc7\u4f7f\u7528\u5c0f\u6ce2\u7a00\u758f\u5148\u9a8c\u5c55\u5f00 NA-ALISTA \u7684\u5c42\u6765\u8865\u507f\u5931\u771f\u3002\u56e0\u6b64\uff0c\u89e3\u7801\u662f\u4e00\u79cd\u6839\u636e\u76f8\u673a/\u73af\u5883\u4fe1\u606f\u548c\u4e0b\u6e38\u4efb\u52a1\u8bbe\u8ba1\u7684\u5373\u63d2\u5373\u7528\u65b9\u6cd5\u3002\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u901a\u8fc7\u7aef\u5230\u7aef\u65b9\u5f0f\u7684\u635f\u5931\u51fd\u6570\u4e0e\u538b\u7f29\u77e9\u9635\u548c\u5c55\u5f00\u7f51\u7edc\u8054\u5408\u5fae\u8c03\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4f4e\u538b\u7f29\u6bd4\u7684\u566a\u58f0\u73af\u5883\u4e2d\uff0c\u6dfb\u52a0\u6062\u590d\u635f\u5931\u4ee5\u53ca\u4efb\u52a1\u76f8\u5173\u635f\u5931\u53ef\u4ee5\u63d0\u9ad8\u4e0b\u6e38\u6027\u80fd\u3002||\n", "2409.01872": "|**2024-09-03**|[Latent Distillation for Continual Object Detection at the Edge](http://arxiv.org/abs/2409.01872)|**[link](https://github.com/pastifra/Continual_Nanodet)**|\u867d\u7136\u5728\u76ee\u6807\u68c0\u6d4b\u6587\u732e\u4e2d\u5b58\u5728\u8bb8\u591a\u6027\u80fd\u5353\u8d8a\u7684\u65b9\u6cd5\uff0c\u4f46\u89e3\u51b3\u6570\u636e\u5206\u5e03\u504f\u79fb\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u4e3a\u8fd9\u4e2a\u95ee\u9898\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u9002\u5e94\u65b0\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5148\u524d\u6570\u636e\u7684\u6027\u80fd\u3002\u8fd9\u5bf9\u4e8e\u8fb9\u7f18\u8bbe\u5907\u5c24\u5176\u91cd\u8981\uff0c\u8fd9\u4e9b\u8bbe\u5907\u5728\u6c7d\u8f66\u548c\u673a\u5668\u4eba\u7b49\u52a8\u6001\u73af\u5883\u4e2d\u5f88\u5e38\u89c1\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u76ee\u6807\u68c0\u6d4b\u6301\u7eed\u5b66\u4e60\uff08CLOD\uff09\u573a\u666f\u4e2d\u8fb9\u7f18\u8bbe\u5907\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u9650\u5236\u3002\u5177\u4f53\u6765\u8bf4\uff0c\uff08i\uff09\u6211\u4eec\u7814\u7a76\u4e86\u4e00\u79cd\u5f00\u6e90\u3001\u8f7b\u91cf\u7ea7\u548c\u5feb\u901f\u7684\u68c0\u6d4b\u5668 NanoDet \u5bf9\u8fb9\u7f18\u8bbe\u5907\u4e0a CLOD \u7684\u9002\u7528\u6027\uff0c\u6539\u8fdb\u4e86\u6587\u732e\u4e2d\u4f7f\u7528\u7684\u8f83\u5927\u67b6\u6784\u3002\u6b64\u5916\uff0c\uff08ii\uff09\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6f5c\u5728\u84b8\u998f\uff08LD\uff09\u7684\u65b0\u578b CL \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u663e\u7740\u5f71\u54cd\u68c0\u6d4b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u4e86\u6700\u5148\u8fdb\u7684 CL \u65b9\u6cd5\u6240\u9700\u7684\u8fd0\u7b97\u6b21\u6570\u548c\u5185\u5b58\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u8457\u540d\u7684 VOC \u548c COCO \u57fa\u51c6\u6d4b\u8bd5\u96c6\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u4e0e\u5176\u4ed6\u84b8\u998f\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6bcf\u6b21\u6a21\u578b\u66f4\u65b0\u53ef\u5c06\u84b8\u998f\u53c2\u6570\u5f00\u9500\u51cf\u5c11 74%\uff0c\u5c06\u6d6e\u70b9\u8fd0\u7b97\uff08FLOPs\uff09\u51cf\u5c11 56%\u3002||\n", "2409.01816": "|**2024-09-03**|[GeoBEV: Learning Geometric BEV Representation for Multi-view 3D Object Detection](http://arxiv.org/abs/2409.01816)|null|\u9e1f\u77b0\u56fe (BEV) \u8868\u793a\u5df2\u6210\u4e3a\u591a\u89c6\u56fe 3D \u5bf9\u8c61\u68c0\u6d4b\u7684\u4e3b\u6d41\u8303\u5f0f\uff0c\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u611f\u77e5\u80fd\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86 BEV \u8868\u793a\u7684\u51e0\u4f55\u8d28\u91cf\uff0c\u4f7f\u5176\u5904\u4e8e\u4f4e\u5206\u8fa8\u7387\u72b6\u6001\uff0c\u65e0\u6cd5\u6062\u590d\u573a\u666f\u771f\u5b9e\u7684\u51e0\u4f55\u4fe1\u606f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u5148\u524d\u65b9\u6cd5\u53d7\u9650\u4e8e\u4f4e BEV \u8868\u793a\u5206\u8fa8\u7387\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u5f84\u5411-\u7b1b\u5361\u5c14 BEV \u91c7\u6837 (RC-Sampling)\uff0c\u4ece\u800c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u5bc6\u96c6 BEV \u8868\u793a\uff0c\u800c\u65e0\u9700\u590d\u6742\u7684\u7b97\u5b50\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u76d2\u5185\u6807\u7b7e\u6765\u66ff\u4ee3\u4ece\u6fc0\u5149\u96f7\u8fbe\u70b9\u751f\u6210\u7684\u4f20\u7edf\u6df1\u5ea6\u6807\u7b7e\u3002\u6b64\u6807\u7b7e\u53cd\u6620\u4e86\u5bf9\u8c61\u7684\u5b9e\u9645\u51e0\u4f55\u7ed3\u6784\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5b83\u4eec\u7684\u8868\u9762\uff0c\u5c06\u73b0\u5b9e\u4e16\u754c\u7684\u51e0\u4f55\u4fe1\u606f\u6ce8\u5165 BEV \u8868\u793a\u4e2d\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u76d2\u5185\u6807\u7b7e\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u8d28\u5fc3\u611f\u77e5\u5185\u90e8\u635f\u5931 (CAI \u635f\u5931) \u6765\u6355\u6349\u5bf9\u8c61\u7684\u7ec6\u7c92\u5ea6\u5185\u90e8\u51e0\u4f55\u7ed3\u6784\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c06\u4e0a\u8ff0\u6a21\u5757\u96c6\u6210\u5230\u4e00\u4e2a\u540d\u4e3a GeoBEV \u7684\u65b0\u578b\u591a\u89c6\u56fe 3D \u5bf9\u8c61\u68c0\u6d4b\u6846\u67b6\u4e2d\u3002\u5728 nuScenes \u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGeoBEV \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7a81\u51fa\u4e86\u5176\u6709\u6548\u6027\u3002||\n", "2409.03530": "|**2024-09-05**|[Use of triplet loss for facial restoration in low-resolution images](http://arxiv.org/abs/2409.03530)|null|\u8fd1\u5e74\u6765\uff0c\u4eba\u8138\u8bc6\u522b (FR) \u6a21\u578b\u5df2\u6210\u4e3a\u5e94\u7528\u6700\u5e7f\u6cdb\u7684\u751f\u7269\u8bc6\u522b\u5de5\u5177\uff0c\u5728\u4f17\u591a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u6210\u679c\u3002\u7136\u800c\uff0c\u786c\u4ef6\u7684\u56fa\u6709\u6311\u6218\u6216\u62cd\u6444\u8ddd\u79bb often \u5bfc\u81f4\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u8fd9\u4f1a\u4e25\u91cd\u5f71\u54cd\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4eba\u4eec\u63d0\u51fa\u4e86\u51e0\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u751f\u6210\u9ad8\u5ea6\u903c\u771f\u7684\u4eba\u8138\u7684\u8d85\u5206\u8fa8\u7387 (SR) \u6a21\u578b\u3002\u5c3d\u7ba1\u505a\u51fa\u4e86\u8fd9\u4e9b\u52aa\u529b\uff0c\u4f46\u4eba\u8138\u8bc6\u522b\u7b97\u6cd5\u5e76\u672a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8d85\u5206\u8fa8\u7387\u6a21\u578b FTLGAN\uff0c\u5b83\u4fa7\u91cd\u4e8e\u751f\u6210\u4fdd\u7559\u4e2a\u4eba\u8eab\u4efd\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\uff0c\u4ece\u800c\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u7684\u6027\u80fd\u3002\u7ed3\u679c\u4ee4\u4eba\u4fe1\u670d\uff0c\u8868\u660e d' \u7684\u5e73\u5747\u503c\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u9ad8\u51fa 21%\uff0c\u5177\u4f53\u800c\u8a00\uff0c14x14 \u50cf\u7d20\u65f6 d' = 1.099\uff0cAUC = 0.78\uff0c28x28 \u50cf\u7d20\u65f6 d' = 2.112\uff0cAUC = 0.92\uff0c56x56 \u50cf\u7d20\u65f6 d' = 3.049\uff0cAUC = 0.98\u3002\u8fd9\u9879\u7814\u7a76\u7684\u8d21\u732e\u5728\u51e0\u4e2a\u5173\u952e\u9886\u57df\u610f\u4e49\u91cd\u5927\u3002\u9996\u5148\uff0c\u5728\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff08\u7279\u522b\u662f 14x14\u300128x28 \u548c 56x56 \u50cf\u7d20\u7684\u5206\u8fa8\u7387\uff09\u4e2d\uff0c\u4eba\u8138\u8bc6\u522b\u6027\u80fd\u53d6\u5f97\u4e86\u663e\u7740\u63d0\u9ad8\u3002\u5176\u6b21\uff0cFTLGAN \u6240\u5c55\u793a\u7684\u589e\u5f3a\u529f\u80fd\u5728\u6240\u6709\u5206\u8fa8\u7387\u4e0b\u90fd\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u54cd\u5e94\uff0c\u4e0e\u5176\u4ed6\u6bd4\u8f83\u6a21\u578b\u4e0d\u540c\uff0c\u5b83\u59cb\u7ec8\u5982\u4e00\u5730\u63d0\u4f9b\u51fa\u8272\u7684\u6027\u80fd\u3002\u7b2c\u4e09\uff0c\u4f7f\u7528\u4e09\u5143\u7ec4\u635f\u5931\u903b\u8f91\u5b9e\u65bd\u4e86\u4e00\u79cd\u521b\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u4ec5\u4f7f\u7528\u771f\u5b9e\u56fe\u50cf\u8bad\u7ec3\u8d85\u5206\u8fa8\u7387\u6a21\u578b\uff0c\u8fd9\u4e0e\u5f53\u524d\u6a21\u578b\u5f62\u6210\u5bf9\u6bd4\uff0c\u5e76\u6269\u5c55\u4e86\u6f5c\u5728\u7684\u73b0\u5b9e\u5e94\u7528\u3002\u6700\u540e\uff0c\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5728\u6a21\u578b\u8bad\u7ec3\u671f\u95f4\u5c06\u4eba\u8138\u8bc6\u522b\u8d28\u91cf\u4f5c\u4e3a\u635f\u5931\u7eb3\u5165\u5176\u4e2d\uff0c\u4e13\u95e8\u89e3\u51b3\u4e86\u63d0\u9ad8\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5206\u7c7b\u6027\u80fd\u7684\u6311\u6218\u3002||\n", "2409.03521": "|**2024-09-05**|[Have Large Vision-Language Models Mastered Art History?](http://arxiv.org/abs/2409.03521)|null|\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u51fa\u73b0\u6700\u8fd1\u5728\u8de8\u591a\u4e2a\u9886\u57df\u7684\u56fe\u50cf\u5206\u7c7b\u65b9\u9762\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002\u7136\u800c\uff0cVLM \u5728\u827a\u672f\u54c1\u5206\u7c7b\u8fd9\u4e00\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u7ed8\u753b\u827a\u672f\u98ce\u683c\u5206\u7c7b\u2014\u2014\u4f20\u7edf\u4e0a\u7531\u827a\u672f\u53f2\u5b66\u5bb6\u638c\u63e1\u7684\u9886\u57df\u2014\u2014\u5c1a\u672a\u5f97\u5230\u63a2\u7d22\u3002\u4e0e\u81ea\u7136\u56fe\u50cf\u76f8\u6bd4\uff0c\u827a\u672f\u54c1\u7531\u4e8e\u5176\u56fa\u6709\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u7ed3\u6784\uff08\u4ee5\u591a\u53d8\u7684\u6784\u56fe\u548c\u98ce\u683c\u4e3a\u7279\u5f81\uff09\u800c\u6784\u6210\u4e86\u72ec\u7279\u7684\u6311\u6218\u3002\u827a\u672f\u53f2\u5b66\u5bb6\u957f\u671f\u4ee5\u6765\u4e00\u76f4\u5728\u7814\u7a76\u827a\u672f\u54c1\u7684\u72ec\u7279\u65b9\u9762\uff0c\u800c\u98ce\u683c\u9884\u6d4b\u662f\u5176\u5b66\u79d1\u7684\u4e00\u4e2a\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u3002\u672c\u6587\u7814\u7a76\u4e86\u96c6\u6210\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u7684\u5927\u578b VLM \u662f\u5426\u53ef\u4ee5\u6709\u6548\u5730\u9884\u6d4b\u7ed8\u753b\u7684\u827a\u672f\u53f2\u5c5e\u6027\u3002\u6211\u4eec\u5bf9\u56db\u79cd VLM\uff08\u5373 CLIP\u3001LLaVA\u3001OpenFlamingo \u548c GPT-4o\uff09\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u91cd\u70b9\u5173\u6ce8\u4f7f\u7528\u4e24\u4e2a\u516c\u5171\u827a\u672f\u54c1\u57fa\u51c6\u5bf9\u827a\u672f\u98ce\u683c\u3001\u4f5c\u8005\u548c\u65f6\u95f4\u6bb5\u8fdb\u884c\u96f6\u6837\u672c\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86 ArTest\uff0c\u8fd9\u662f\u4e00\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u827a\u672f\u54c1\u6d4b\u8bd5\u96c6\uff0c\u5176\u4e2d\u5305\u62ec\u827a\u672f\u53f2\u5b66\u5bb6\u7814\u7a76\u7684\u5173\u952e\u7ed8\u753b\u4f5c\u54c1\u3002||\n", "2409.03516": "|**2024-09-05**|[LMLT: Low-to-high Multi-Level Vision Transformer for Image Super-Resolution](http://arxiv.org/abs/2409.03516)|**[link](https://github.com/jwgdmkj/lmlt)**|\u8fd1\u5e74\u6765\uff0c\u57fa\u4e8e\u89c6\u89c9Transformer (ViT) \u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5b58\u5728\u590d\u6742\u6027\u9ad8\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u63a8\u7406\u65f6\u95f4\u548c\u5185\u5b58\u4f7f\u7528\u91cf\u5927\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u673a\u5236(WSA) \u7684ViT\u6a21\u578b\u5728\u5904\u7406\u7a97\u53e3\u533a\u57df\u5916\u7684\u4fe1\u606f\u65f6\u9762\u4e34\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4f4e\u5230\u9ad8\u591a\u7ea7Transformer (LMLT)\uff0c\u5b83\u5bf9\u6bcf\u4e2a\u5934\u91c7\u7528\u4e0d\u540c\u7279\u5f81\u5927\u5c0f\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002LMLT \u6cbf\u901a\u9053\u7ef4\u5ea6\u5212\u5206\u56fe\u50cf\u7279\u5f81\uff0c\u9010\u6e10\u51cf\u5c0f\u4f4e\u5c42\u5934\u7684\u7a7a\u95f4\u5927\u5c0f\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u5934\u5e94\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3002\u8fd9\u79cd\u65b9\u6cd5\u6709\u6548\u5730\u6355\u83b7\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u3002\u901a\u8fc7\u5c06\u4f4e\u5c42\u5934\u7684\u7ed3\u679c\u6574\u5408\u5230\u9ad8\u5c42\u5934\u4e2d\uff0cLMLT \u514b\u670d\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u7a97\u53e3\u8fb9\u754c\u95ee\u9898\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u4fdd\u6301\u751a\u81f3\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u57fa\u4e8e ViT \u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u7684\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\u548c GPU \u5185\u5b58\u4f7f\u7528\u91cf\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/jwgdmkj/LMLT \u83b7\u53d6\u3002||\n", "2409.03458": "|**2024-09-05**|[Non-Uniform Illumination Attack for Fooling Convolutional Neural Networks](http://arxiv.org/abs/2409.03458)|**[link](https://github.com/Akshayjain97/Non-Uniform_Illumination)**|\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u867d\u7136\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u653b\u51fb\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u4eba\u7c7b\u5bb9\u6613\u8bc6\u522b\u7684\u5fae\u5c0f\u56fe\u50cf\u6270\u52a8\u65f6\u3002\u8fd9\u79cd\u5f31\u70b9\u901a\u5e38\u88ab\u79f0\u4e3a\u201c\u653b\u51fb\u201d\uff0c\u7a81\u663e\u4e86CNN\u7684\u9c81\u68d2\u6027\u6709\u9650\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u589e\u5f3a\u5176\u62b5\u6297\u6b64\u7c7b\u64cd\u7eb5\u7684\u80fd\u529b\u3002\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u975e\u5747\u5300\u7167\u660e\uff08NUI\uff09\u653b\u51fb\u6280\u672f\uff0c\u8be5\u6280\u672f\u4f7f\u7528\u4e0d\u540c\u7684NUI\u63a9\u7801\u5bf9\u56fe\u50cf\u8fdb\u884c\u7ec6\u5fae alteration\u3002\u6211\u4eec\u5728\u5e7f\u6cdb\u63a5\u53d7\u7684\u6570\u636e\u96c6\uff08\u5305\u62ecCIFAR10\u3001TinyImageNet\u548cCalTech256\uff09\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u91cd\u70b9\u5173\u6ce812\u79cd\u4e0d\u540cNUI\u653b\u51fb\u6a21\u578b\u7684\u56fe\u50cf\u5206\u7c7b\u3002\u8bc4\u4f30\u4e86VGG\u3001ResNet\u3001MobilenetV3-small\u548cInceptionV3\u6a21\u578b\u5bf9NUI\u653b\u51fb\u7684\u62b5\u6297\u529b\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cCNN\u6a21\u578b\u5728\u906d\u53d7NUI\u653b\u51fb\u65f6\uff0c\u5206\u7c7b\u7cbe\u5ea6\u5927\u5e45\u4e0b\u964d\uff0c\u8868\u660e\u5b83\u4eec\u5728\u975e\u5747\u5300\u7167\u660e\u4e0b\u7684\u8106\u5f31\u6027\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u79cd\u60c5\u51b5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u5fa1\u7b56\u7565\uff0c\u5c06\u901a\u8fc7\u65b0\u7684NUI\u53d8\u6362\u751f\u6210\u7684NUI\u653b\u51fb\u56fe\u50cf\u5305\u542b\u5230\u8bad\u7ec3\u96c6\u4e2d\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5f53CNN\u6a21\u578b\u9762\u5bf9\u53d7NUI\u653b\u51fb\u5f71\u54cd\u7684\u6270\u52a8\u56fe\u50cf\u65f6\uff0c\u5176\u6027\u80fd\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002\u8be5\u7b56\u7565\u65e8\u5728\u589e\u5f3aCNN\u6a21\u578b\u5bf9NUI\u653b\u51fb\u7684\u62b5\u6297\u529b\u3002||\n", "2409.03377": "|**2024-09-05**|[Raw Speech Enhancement with Deep State Space Modeling](http://arxiv.org/abs/2409.03377)|**[link](https://github.com/Brainchip-Inc/aTENNuate)**|\u6211\u4eec\u63d0\u51fa\u4e86 aTENNuate\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u7684\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u81ea\u7f16\u7801\u5668\uff0c\u4e13\u4e3a\u9ad8\u6548\u7684\u5728\u7ebf\u539f\u59cb\u8bed\u97f3\u589e\u5f3a\u800c\u914d\u7f6e\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u3002\u8be5\u7f51\u7edc\u7684\u6027\u80fd\u4e3b\u8981\u5728\u539f\u59cb\u8bed\u97f3\u53bb\u566a\u65b9\u9762\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5728\u8d85\u5206\u8fa8\u7387\u548c\u53bb\u91cf\u5316\u7b49\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u989d\u5916\u8bc4\u4f30\u3002\u6211\u4eec\u5728 VoiceBank + DEMAND \u548c Microsoft DNS1 \u5408\u6210\u6d4b\u8bd5\u96c6\u4e0a\u5bf9 aTENNuate \u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u8be5\u7f51\u7edc\u5728 PESQ \u5206\u6570\u3001\u53c2\u6570\u6570\u91cf\u3001MAC \u548c\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8e\u4ee5\u524d\u7684\u5b9e\u65f6\u53bb\u566a\u6a21\u578b\u3002\u5373\u4f7f\u4f5c\u4e3a\u539f\u59cb\u6ce2\u5f62\u5904\u7406\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4e5f\u80fd\u4fdd\u6301\u5bf9\u5e72\u51c0\u4fe1\u53f7\u7684\u9ad8\u4fdd\u771f\u5ea6\uff0c\u5e76\u4e14\u53ef\u542c\u89c1\u7684\u4f2a\u5f71\u6781\u5c11\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u5c06\u566a\u58f0\u8f93\u5165\u538b\u7f29\u81f3 4000Hz \u548c 4 \u4f4d\uff0c\u8be5\u6a21\u578b\u4ecd\u80fd\u4fdd\u6301\u826f\u597d\u7684\u6027\u80fd\uff0c\u8fd9\u8868\u660e\u5b83\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u5177\u6709\u4e00\u822c\u7684\u8bed\u97f3\u589e\u5f3a\u80fd\u529b\u3002||\n", "2409.03368": "|**2024-09-05**|[Training-free Conversion of Pretrained ANNs to SNNs for Low-Power and High-Performance Applications](http://arxiv.org/abs/2409.03368)|null|\u8109\u51b2\u795e\u7ecf\u7f51\u7edc (SNN) \u7531\u4e8e\u5176\u63a8\u7406\u901f\u5ea6\u5feb\u3001\u529f\u8017\u4f4e\u7b49\u4f18\u52bf\uff0c\u5df2\u6210\u4e3a\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc (ANN) \u7684\u4e00\u79cd\u5f88\u6709\u524d\u9014\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u8bad\u7ec3\u7b97\u6cd5\u963b\u788d\u4e86\u5b83\u4eec\u7684\u5e7f\u6cdb\u5e94\u7528\u3002\u73b0\u6709\u7684 SNN \u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\u6bd4 ANN \u9700\u8981\u66f4\u591a\u7684\u5185\u5b58\u548c\u65f6\u95f4\u3002\u5373\u4f7f\u662f\u5e38\u7528\u7684 ANN-SNN \u8f6c\u6362\u65b9\u6cd5\u4e5f\u9700\u8981\u91cd\u65b0\u8bad\u7ec3 ANN \u4ee5\u63d0\u9ad8\u8f6c\u6362\u6548\u7387\uff0c\u4ece\u800c\u4ea7\u751f\u989d\u5916\u7684\u8ba1\u7b97\u6210\u672c\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u514d\u8bad\u7ec3 ANN-SNN \u8f6c\u6362\u6d41\u7a0b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u9884\u5148\u8bad\u7ec3\u597d\u7684 ANN \u6a21\u578b\u76f4\u63a5\u8f6c\u6362\u4e3a\u9ad8\u6027\u80fd SNN\uff0c\u65e0\u9700\u989d\u5916\u7684\u8bad\u7ec3\u3002\u8be5\u8f6c\u6362\u6d41\u7a0b\u5305\u62ec\u4e00\u4e2a\u57fa\u4e8e\u5c40\u90e8\u5b66\u4e60\u7684\u9608\u503c\u5e73\u8861\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u8ba1\u7b97\u6700\u4f73\u9608\u503c\u5e76\u901a\u8fc7\u901a\u9053\u7f29\u653e\u5bf9\u9608\u503c\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8c03\u6574\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u6846\u67b6\u5728\u4e09\u4e2a\u5178\u578b\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u53ef\u6269\u5c55\u6027\uff1a\u56fe\u50cf\u5206\u7c7b\u3001\u8bed\u4e49\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\u3002\u8fd9\u5c55\u793a\u4e86\u5176\u5bf9\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u7684\u9002\u7528\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u8f6c\u6362\u540e\u7684 SNN \u7684\u80fd\u8017\uff0c\u8bc1\u660e\u4e86\u5b83\u4eec\u4e0e\u4f20\u7edf ANN \u76f8\u6bd4\u5177\u6709\u4f18\u8d8a\u7684\u4f4e\u529f\u8017\u4f18\u52bf\u3002\u6211\u4eec\u7684\u514d\u8bad\u7ec3\u7b97\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7a81\u51fa\u4e86\u5176\u5b9e\u7528\u6027\u548c\u6548\u7387\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u5f00\u6e90\u9884\u8bad\u7ec3 ANN \u6a21\u578b\u548c\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u7b80\u5316\u4e86 SNN \u7684\u90e8\u7f72\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u4f4e\u529f\u8017\u7684\u63a8\u7406\uff0c\u5e76\u4e14\u6027\u80fd\u635f\u5931\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u3002||\n", "2409.03320": "|**2024-09-05**|[YOLO-PPA based Efficient Traffic Sign Detection for Cruise Control in Autonomous Driving](http://arxiv.org/abs/2409.03320)|null|\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u9ad8\u6548\u3001\u51c6\u786e\u5730\u68c0\u6d4b\u4ea4\u901a\u6807\u5fd7\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8ddd\u79bb\u8d8a\u8fdc\uff0c\u4ea4\u901a\u6807\u5fd7\u8d8a\u5c0f\u3002\u73b0\u6709\u7684\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u5f88\u96be\u68c0\u6d4b\u5230\u8fd9\u4e9b\u5c0f\u5c3a\u5bf8\u7684\u6807\u5fd7\u3002\u6b64\u5916\uff0c\u8f66\u8f7d\u5d4c\u5165\u5f0f\u8bbe\u5907\u7684\u6027\u80fd\u9650\u5236\u4e86\u68c0\u6d4b\u6a21\u578b\u7684\u89c4\u6a21\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e YOLO PPA \u7684\u4ea4\u901a\u6807\u5fd7\u68c0\u6d4b\u7b97\u6cd5\u3002\u5728 GTSDB \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u539f\u59cb YOLO \u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5c06\u63a8\u7406\u6548\u7387\u63d0\u9ad8\u4e86 11.2%\uff0cmAP 50 \u4e5f\u63d0\u9ad8\u4e86 93.2%\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684 YOLO PPA \u7684\u6709\u6548\u6027\u3002||\n", "2409.03192": "|**2024-09-05**|[PEPL: Precision-Enhanced Pseudo-Labeling for Fine-Grained Image Classification in Semi-Supervised Learning](http://arxiv.org/abs/2409.03192)|null|\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u7684\u51fa\u73b0\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u6b65\u3002\u7136\u800c\uff0c\u8be6\u7ec6\u6807\u6ce8\u7684\u7f3a\u4e4f\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u83b7\u53d6\u9ad8\u8d28\u91cf\u6807\u8bb0\u6570\u636e\u7684\u6210\u672c\u9ad8\u6602\u6216\u8017\u65f6\u7684\u60c5\u51b5\u4e0b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e13\u4e3a\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u5185\u7684\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u8bbe\u8ba1\u7684\u7cbe\u5ea6\u589e\u5f3a\u578b\u4f2a\u6807\u7b7e\uff08PEPL\uff09\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4f2a\u6807\u7b7e\u6765\u5229\u7528\u4e30\u5bcc\u7684\u672a\u6807\u8bb0\u6570\u636e\uff0c\u8fd9\u4e9b\u4f2a\u6807\u7b7e\u901a\u8fc7\u4e24\u4e2a\u5173\u952e\u9636\u6bb5\u9010\u6b65\u7ec6\u5316\uff1a\u521d\u59cb\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u8bed\u4e49\u6df7\u5408\u4f2a\u6807\u7b7e\u751f\u6210\u3002\u8fd9\u4e9b\u9636\u6bb5\u5229\u7528\u7c7b\u6fc0\u6d3b\u56fe\uff08CAM\uff09\u6765\u51c6\u786e\u4f30\u8ba1\u8bed\u4e49\u5185\u5bb9\u5e76\u751f\u6210\u7ec6\u5316\u6807\u7b7e\uff0c\u8fd9\u4e9b\u6807\u7b7e\u6355\u83b7\u4e86\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6240\u9700\u7684\u57fa\u672c\u7ec6\u8282\u3002\u901a\u8fc7\u5173\u6ce8\u8bed\u4e49\u7ea7\u4fe1\u606f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u6807\u51c6\u6570\u636e\u589e\u5f3a\u548c\u56fe\u50cf\u6df7\u5408\u6280\u672f\u5728\u4fdd\u7559\u5173\u952e\u7ec6\u7c92\u5ea6\u7279\u5f81\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002\u6211\u4eec\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u76f8\u5bf9\u4e8e\u73b0\u6709\u534a\u76d1\u7763\u7b56\u7565\u7684\u663e\u8457\u6539\u8fdb\uff0c\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u5728https://github.com/TianSuya/SemiFG\u5f00\u6e90\u3002||\n", "2409.03137": "|**2024-09-05**|[The AdEMAMix Optimizer: Better, Faster, Older](http://arxiv.org/abs/2409.03137)|null|\u57fa\u4e8e\u52a8\u91cf\u7684\u4f18\u5316\u5668\u662f\u4f17\u591a\u673a\u5668\u5b66\u4e60\u5e94\u7528\u7684\u6838\u5fc3\u3002\u8fd9\u4e9b\u4f18\u5316\u5668\u901a\u5e38\u4f9d\u8d56\u4e8e\u68af\u5ea6\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747 (EMA)\uff0c\u5b83\u4f1a\u4ee5\u6307\u6570\u65b9\u5f0f\u8870\u51cf\u65e7\u68af\u5ea6\u5bf9\u5f53\u524d\u68af\u5ea6\u7684\u8d21\u732e\u3002\u8fd9\u662f\u56e0\u4e3a\u68af\u5ea6\u662f\u5c40\u90e8\u7684\u7ebf\u6027\u8fd1\u4f3c\uff0c\u5f53\u8fed\u4ee3\u70b9\u5728\u635f\u5931\u51fd\u6570\u66f2\u9762\u4e0a\u79fb\u52a8\u65f6\uff0c\u65e7\u68af\u5ea6\u7684\u76f8\u5173\u6027\u4f1a\u964d\u4f4e\u3002\u8fd9\u9879\u5de5\u4f5c\u5bf9\u4f7f\u7528\u5355\u4e2a EMA \u6765\u7d2f\u79ef\u8fc7\u53bb\u68af\u5ea6\u7684\u505a\u6cd5\u63d0\u51fa\u4e86\u8d28\u7591\uff0c\u5e76\u901a\u8fc7\u7ecf\u9a8c\u8bc1\u660e\u4e86\u8fd9\u79cd\u9009\u62e9\u53ef\u80fd\u662f\u6b21\u4f18\u7684\uff1a\u5355\u4e2a EMA \u65e0\u6cd5\u540c\u65f6\u5bf9\u6700\u8fd1\u7684\u68af\u5ea6\u8d4b\u4e88\u9ad8\u6743\u91cd\uff0c\u5e76\u5bf9\u8f83\u65e7\u7684\u68af\u5ea6\u8d4b\u4e88\u4e0d\u53ef\u5ffd\u7565\u7684\u6743\u91cd\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c2\u5bdf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 AdEMAMix\uff0c\u5b83\u662f\u5bf9 Adam \u4f18\u5316\u5668\u7684\u4e00\u79cd\u7b80\u5355\u4fee\u6539\uff0c\u5b83\u6df7\u5408\u4e86\u4e24\u4e2a EMA\uff0c\u4ee5\u66f4\u597d\u5730\u5229\u7528\u8fc7\u53bb\u7684\u68af\u5ea6\u3002\u6211\u4eec\u5728\u8bed\u8a00\u5efa\u6a21\u548c\u56fe\u50cf\u5206\u7c7b\u65b9\u9762\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u68af\u5ea6\u5728\u6570\u4e07\u6b65\u5185\u4ecd\u7136\u5177\u6709\u76f8\u5173\u6027\u3002\u5b83\u4eec\u6709\u52a9\u4e8e\u66f4\u5feb\u5730\u6536\u655b\uff0c\u5e76\u4e14\u901a\u5e38\u6536\u655b\u5230\u66f4\u4f4e\u7684\u6700\u5c0f\u503c\uff1a\u4f8b\u5982\uff0c\u4e00\u4e2a\u5728 1010 \u4ebf\u4e2a\u8bcd\u7b26\u4e0a\u8bad\u7ec3\u7684\u5177\u6709 13 \u4ebf\u4e2a\u53c2\u6570\u7684 AdEMAMix LLM \u7684\u6027\u80fd\u4e0e\u5728\u4e00\u4e2a 1970 \u4ebf\u4e2a\u8bcd\u7b26\u4e0a\u8bad\u7ec3\u7684 AdamW \u6a21\u578b\u76f8\u5f53\uff08+95%\uff09\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u51cf\u7f13\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6a21\u578b\u9057\u5fd8\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u9f13\u52b1\u8fdb\u4e00\u6b65\u63a2\u7d22\u5229\u7528\u8fc7\u53bb\u68af\u5ea6\u7684\u4e0d\u540c\u7c7b\u578b\u7684\u51fd\u6570\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f EMA\u3002||\n", "2409.03022": "|**2024-09-04**|[Boundless: Generating Photorealistic Synthetic Data for Object Detection in Urban Streetscapes](http://arxiv.org/abs/2409.03022)|**[link](https://github.com/zk2172-columbia/boundless)**|\u6211\u4eec\u4ecb\u7ecdBoundless\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u5bc6\u96c6\u7684\u57ce\u5e02\u8857\u666f\u4e2d\u5b9e\u73b0\u9ad8\u5ea6\u51c6\u786e\u7684\u76ee\u6807\u68c0\u6d4b\u7684\u903c\u771f\u5408\u6210\u6570\u636e\u751f\u6210\u7cfb\u7edf\u3002Boundless\u53ef\u4ee5\u7528\u81ea\u52a8\u5316\u548c\u53ef\u914d\u7f6e\u7684\u8fc7\u7a0b\u53d6\u4ee3\u5927\u89c4\u6a21\u7684\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u6536\u96c6\u548c\u624b\u52a8\u5730\u9762\u5b9e\u51b5\u76ee\u6807\u6ce8\u91ca\uff08\u6807\u8bb0\uff09\u3002Boundless\u57fa\u4e8e\u865a\u5e7b\u5f15\u64ce5 (UE5) \u57ce\u5e02\u793a\u4f8b\u9879\u76ee\uff0c\u5e76\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u7684\u7167\u660e\u548c\u573a\u666f\u53d8\u5316\u6761\u4ef6\u4e0b\u51c6\u786e\u6536\u96c63D\u8fb9\u754c\u6846\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u5728Boundless\u751f\u6210\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u4ece\u4e2d\u7a7a\u76f8\u673a\u83b7\u53d6\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u63a8\u7406\u65f6\u7684\u6027\u80fd\u3002\u6211\u4eec\u5c06Boundless\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u4e0eCARLA\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u89c2\u5bdf\u52307.8 mAP\u7684\u6539\u8fdb\u3002\u6211\u4eec\u53d6\u5f97\u7684\u7ed3\u679c\u652f\u6301\u4e86\u5408\u6210\u6570\u636e\u751f\u6210\u662f\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u7528\u4e8e\u8bad\u7ec3/\u5fae\u8c03\u7528\u4e8e\u57ce\u5e02\u573a\u666f\u7684\u53ef\u6269\u5c55\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u3002||\n", "2409.05650": "|**2024-09-09**|[Replay Consolidation with Label Propagation for Continual Object Detection](http://arxiv.org/abs/2409.05650)|null|Object Detection is a highly relevant computer vision problem with many applications such as robotics and autonomous driving. Continual Learning~(CL) considers a setting where a model incrementally learns new information while retaining previously acquired knowledge. This is particularly challenging since Deep Learning models tend to catastrophically forget old knowledge while training on new data. In particular, Continual Learning for Object Detection~(CLOD) poses additional difficulties compared to CL for Classification. In CLOD, images from previous tasks may contain unknown classes that could reappear labeled in future tasks. These missing annotations cause task interference issues for replay-based approaches. As a result, most works in the literature have focused on distillation-based approaches. However, these approaches are effective only when there is a strong overlap of classes across tasks. To address the issues of current methodologies, we propose a novel technique to solve CLOD called Replay Consolidation with Label Propagation for Object Detection (RCLPOD). Based on the replay method, our solution avoids task interference issues by enhancing the buffer memory samples. Our method is evaluated against existing techniques in CLOD literature, demonstrating its superior performance on established benchmarks like VOC and COCO.|\n", "2409.05564": "|**2024-09-09**|[LEROjD: Lidar Extended Radar-Only Object Detection](http://arxiv.org/abs/2409.05564)|**[link](https://github.com/rst-tu-dortmund/lerojd)**|Accurate 3D object detection is vital for automated driving. While lidar sensors are well suited for this task, they are expensive and have limitations in adverse weather conditions. 3+1D imaging radar sensors offer a cost-effective, robust alternative but face challenges due to their low resolution and high measurement noise. Existing 3+1D imaging radar datasets include radar and lidar data, enabling cross-modal model improvements. Although lidar should not be used during inference, it can aid the training of radar-only object detectors. We explore two strategies to transfer knowledge from the lidar to the radar domain and radar-only object detectors: 1. multi-stage training with sequential lidar point cloud thin-out, and 2. cross-modal knowledge distillation. In the multi-stage process, three thin-out methods are examined. Our results show significant performance gains of up to 4.2 percentage points in mean Average Precision with multi-stage training and up to 3.9 percentage points with knowledge distillation by initializing the student with the teacher's weights. The main benefit of these approaches is their applicability to other 3D object detection networks without altering their architecture, as we show by analyzing it on two different object detectors. Our code is available at https://github.com/rst-tu-dortmund/lerojd|\n", "2409.05162": "|**2024-09-08**|[Can OOD Object Detectors Learn from Foundation Models?](http://arxiv.org/abs/2409.05162)|**[link](https://github.com/cvmi-lab/syncood)**|Out-of-distribution (OOD) object detection is a challenging task due to the absence of open-set OOD data. Inspired by recent advancements in text-to-image generative models, such as Stable Diffusion, we study the potential of generative models trained on large-scale open-set data to synthesize OOD samples, thereby enhancing OOD object detection. We introduce SyncOOD, a simple data curation method that capitalizes on the capabilities of large foundation models to automatically extract meaningful OOD data from text-to-image generative models. This offers the model access to open-world knowledge encapsulated within off-the-shelf foundation models. The synthetic OOD samples are then employed to augment the training of a lightweight, plug-and-play OOD detector, thus effectively optimizing the in-distribution (ID)/OOD decision boundaries. Extensive experiments across multiple benchmarks demonstrate that SyncOOD significantly outperforms existing methods, establishing new state-of-the-art performance with minimal synthetic data usage.|\n", "2409.04999": "|**2024-09-08**|[Visual Grounding with Multi-modal Conditional Adaptation](http://arxiv.org/abs/2409.04999)|**[link](https://github.com/mr-bigworth/mmca)**|Visual grounding is the task of locating objects specified by natural language expressions. Existing methods extend generic object detection frameworks to tackle this task. They typically extract visual and textual features separately using independent visual and textual encoders, then fuse these features in a multi-modal decoder for final prediction. However, visual grounding presents unique challenges. It often involves locating objects with different text descriptions within the same image. Existing methods struggle with this task because the independent visual encoder produces identical visual features for the same image, limiting detection performance. Some recently approaches propose various language-guided visual encoders to address this issue, but they mostly rely solely on textual information and require sophisticated designs. In this paper, we introduce Multi-modal Conditional Adaptation (MMCA), which enables the visual encoder to adaptively update weights, directing its focus towards text-relevant regions. Specifically, we first integrate information from different modalities to obtain multi-modal embeddings. Then we utilize a set of weighting coefficients, which generated from the multimodal embeddings, to reorganize the weight update matrices and apply them to the visual encoder of the visual grounding model. Extensive experiments on four widely used datasets demonstrate that MMCA achieves significant improvements and state-of-the-art results. Ablation experiments further demonstrate the lightweight and efficiency of our method. Our source code is available at: https://github.com/Mr-Bigworth/MMCA.|\n", "2409.04979": "|**2024-09-08**|[RCBEVDet++: Toward High-accuracy Radar-Camera Fusion 3D Perception Network](http://arxiv.org/abs/2409.04979)|null|Perceiving the surrounding environment is a fundamental task in autonomous driving. To obtain highly accurate perception results, modern autonomous driving systems typically employ multi-modal sensors to collect comprehensive environmental data. Among these, the radar-camera multi-modal perception system is especially favored for its excellent sensing capabilities and cost-effectiveness. However, the substantial modality differences between radar and camera sensors pose challenges in fusing information. To address this problem, this paper presents RCBEVDet, a radar-camera fusion 3D object detection framework. Specifically, RCBEVDet is developed from an existing camera-based 3D object detector, supplemented by a specially designed radar feature extractor, RadarBEVNet, and a Cross-Attention Multi-layer Fusion (CAMF) module. Firstly, RadarBEVNet encodes sparse radar points into a dense bird's-eye-view (BEV) feature using a dual-stream radar backbone and a Radar Cross Section aware BEV encoder. Secondly, the CAMF module utilizes a deformable attention mechanism to align radar and camera BEV features and adopts channel and spatial fusion layers to fuse them. To further enhance RCBEVDet's capabilities, we introduce RCBEVDet++, which advances the CAMF through sparse fusion, supports query-based multi-view camera perception models, and adapts to a broader range of perception tasks. Extensive experiments on the nuScenes show that our method integrates seamlessly with existing camera-based 3D perception models and improves their performance across various perception tasks. Furthermore, our method achieves state-of-the-art radar-camera fusion results in 3D object detection, BEV semantic segmentation, and 3D multi-object tracking tasks. Notably, with ViT-L as the image backbone, RCBEVDet++ achieves 72.73 NDS and 67.34 mAP in 3D object detection without test-time augmentation or model ensembling.|\n", "2409.04975": "|**2024-09-08**|[PatchAlign:Fair and Accurate Skin Disease Image Classification by Alignment with Clinical Labels](http://arxiv.org/abs/2409.04975)|**[link](https://github.com/aayushmanace/patchalign24)**|\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u76ae\u80a4\u75c5\u53d8\u8bca\u65ad\u81ea\u52a8\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\u3002\u7136\u800c\uff0c\u5728\u90e8\u7f72\u8fd9\u4e9b\u6a21\u578b\u4e4b\u524d\uff0c\u9700\u8981\u89e3\u51b3\u5176\u9884\u6d4b\u4e2d\u5b58\u5728\u7684\u79cd\u65cf\u5dee\u5f02\u95ee\u9898\u3002\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a PatchAlign \u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0e\u76ae\u80a4\u75c5\u4e34\u5e8a\u6587\u672c\u8868\u5f81\u5bf9\u9f50\u6765\u63d0\u9ad8\u76ae\u80a4\u75c5\u56fe\u50cf\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u3002PatchAlign \u4f7f\u7528\u56fe\u6700\u4f18\u4f20\u8f93 (GOT) \u635f\u5931\u4f5c\u4e3a\u6b63\u5219\u5316\u5668\u6765\u6267\u884c\u8de8\u57df\u5bf9\u9f50\u3002\u5373\u4f7f\u5728\u8bad\u7ec3\u6837\u672c\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u83b7\u5f97\u7684\u8868\u5f81\u4e5f\u662f\u7a33\u5065\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u5f88\u597d\u5730\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u80a4\u8272\u3002\u4e3a\u4e86\u51cf\u5c11\u4e34\u5e8a\u76ae\u80a4\u75c5\u56fe\u50cf\u4e2d\u566a\u58f0\u548c\u4f2a\u5f71\u7684\u5f71\u54cd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u63a9\u7801\u56fe\u6700\u4f18\u4f20\u8f93\uff0c\u7528\u4e8e\u8de8\u57df\u5bf9\u9f50\uff0c\u8fdb\u4e00\u6b65\u6539\u5584\u4e86\u516c\u5e73\u6027\u6307\u6807\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u5177\u6709\u4e0d\u540c\u76ae\u80a4\u7c7b\u578b\u7684\u76ae\u80a4\u75c5\u53d8\u6570\u636e\u96c6\u4e0a\u5c06\u6211\u4eec\u7684\u6a21\u578b\u4e0e\u6700\u5148\u8fdb\u7684 FairDisCo \u8fdb\u884c\u4e86\u6bd4\u8f83\uff1aFitzpatrick17k \u548c Diverse Dermatology Images (DDI)\u3002\u4e0e FairDisCo \u76f8\u6bd4\uff0cPatchAlign \u5728 Fitzpatrick17k \u4e0a\u5c06\u76ae\u80a4\u75c5\u56fe\u50cf\u5206\u7c7b\u7684\u51c6\u786e\u6027\u63d0\u9ad8\u4e86 2.8%\uff08\u57df\u5185\uff09\u548c 6.2%\uff08\u8de8\u57df\uff09\uff0c\u5728 DDI \u4e0a\u63d0\u9ad8\u4e86 4.2%\uff08\u57df\u5185\uff09\u3002\u6b64\u5916\uff0c\u5b83\u6301\u7eed\u6539\u5584\u4e86\u4e0d\u540c\u80a4\u8272\u771f\u5b9e\u9633\u6027\u7387\u7684\u516c\u5e73\u6027\u3002\u7528\u4e8e\u5b9e\u73b0\u7684\u6e90\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b GitHub \u5b58\u50a8\u5e93\u4e2d\u83b7\u53d6\uff1ahttps://github.com/aayushmanace/PatchAlign24\uff0c\u53ef\u4ee5\u8f7b\u677e\u590d\u73b0\u548c\u8fdb\u4e00\u6b65\u8bd5\u9a8c\u3002|\n", "2409.04915": "|**2024-09-07**|[Activation Function Optimization Scheme for Image Classification](http://arxiv.org/abs/2409.04915)|null|Activation function has a significant impact on the dynamics, convergence, and performance of deep neural networks. The search for a consistent and high-performing activation function has always been a pursuit during deep learning model development. Existing state-of-the-art activation functions are manually designed with human expertise except for Swish. Swish was developed using a reinforcement learning-based search strategy. In this study, we propose an evolutionary approach for optimizing activation functions specifically for image classification tasks, aiming to discover functions that outperform current state-of-the-art options. Through this optimization framework, we obtain a series of high-performing activation functions denoted as Exponential Error Linear Unit (EELU). The developed activation functions are evaluated for image classification tasks from two perspectives: (1) five state-of-the-art neural network architectures, such as ResNet50, AlexNet, VGG16, MobileNet, and Compact Convolutional Transformer which cover computationally heavy to light neural networks, and (2) eight standard datasets, including CIFAR10, Imagenette, MNIST, Fashion MNIST, Beans, Colorectal Histology, CottonWeedID15, and TinyImageNet which cover from typical machine vision benchmark, agricultural image applications to medical image applications. Finally, we statistically investigate the generalization of the resultant activation functions developed through the optimization scheme. With a Friedman test, we conclude that the optimization scheme is able to generate activation functions that outperform the existing standard ones in 92.8% cases among 28 different cases studied, and $-x\\cdot erf(e^{-x})$ is found to be the best activation function for image classification generated by the optimization scheme.|\n", "2409.04817": "|**2024-09-07**|[SSFam: Scribble Supervised Salient Object Detection Family](http://arxiv.org/abs/2409.04817)|**[link](https://github.com/liuzywen/ssfam)**|Scribble supervised salient object detection (SSSOD) constructs segmentation ability of attractive objects from surroundings under the supervision of sparse scribble labels. For the better segmentation, depth and thermal infrared modalities serve as the supplement to RGB images in the complex scenes. Existing methods specifically design various feature extraction and multi-modal fusion strategies for RGB, RGB-Depth, RGB-Thermal, and Visual-Depth-Thermal image input respectively, leading to similar model flood. As the recently proposed Segment Anything Model (SAM) possesses extraordinary segmentation and prompt interactive capability, we propose an SSSOD family based on SAM, named SSFam, for the combination input with different modalities. Firstly, different modal-aware modulators are designed to attain modal-specific knowledge which cooperates with modal-agnostic information extracted from the frozen SAM encoder for the better feature ensemble. Secondly, a siamese decoder is tailored to bridge the gap between the training with scribble prompt and the testing with no prompt for the stronger decoding ability. Our model demonstrates the remarkable performance among combinations of different modalities and refreshes the highest level of scribble supervised methods and comes close to the ones of fully supervised methods. https://github.com/liuzywen/SSFam|\n", "2409.04801": "|**2024-09-07**|[SpotActor: Training-Free Layout-Controlled Consistent Image Generation](http://arxiv.org/abs/2409.04801)|null|Text-to-image diffusion models significantly enhance the efficiency of artistic creation with high-fidelity image generation. However, in typical application scenarios like comic book production, they can neither place each subject into its expected spot nor maintain the consistent appearance of each subject across images. For these issues, we pioneer a novel task, Layout-to-Consistent-Image (L2CI) generation, which produces consistent and compositional images in accordance with the given layout conditions and text prompts. To accomplish this challenging task, we present a new formalization of dual energy guidance with optimization in a dual semantic-latent space and thus propose a training-free pipeline, SpotActor, which features a layout-conditioned backward update stage and a consistent forward sampling stage. In the backward stage, we innovate a nuanced layout energy function to mimic the attention activations with a sigmoid-like objective. While in the forward stage, we design Regional Interconnection Self-Attention (RISA) and Semantic Fusion Cross-Attention (SFCA) mechanisms that allow mutual interactions across images. To evaluate the performance, we present ActorBench, a specified benchmark with hundreds of reasonable prompt-box pairs stemming from object detection datasets. Comprehensive experiments are conducted to demonstrate the effectiveness of our method. The results prove that SpotActor fulfills the expectations of this task and showcases the potential for practical applications with superior layout alignment, subject consistency, prompt conformity and background diversity.|\n", "2409.04778": "|**2024-09-07**|[LoCa: Logit Calibration for Knowledge Distillation](http://arxiv.org/abs/2409.04778)|null|Knowledge Distillation (KD), aiming to train a better student model by mimicking the teacher model, plays an important role in model compression. One typical way is to align the output logits. However, we find a common issue named mis-instruction, that the student would be misled when the predictions based on teacher logits do not follow the labels. Meanwhile, there is other useful dark knowledge in the logits such as the class discriminability, which is vital for distillation. In this paper, we propose a simple yet effective Logit Calibration (LoCa) method, which calibrates the logits from the teacher model based on the ground-truth labels. The key insight is to correct the prediction (to address the mis-instruction issue) and maintain useful dark knowledge simultaneously. Our proposed LoCa does not require any additional parameters. Empirical results on image classification and text generation tasks demonstrate that LoCa can effectively improve the performance of baselines.|\n"}, "\u751f\u6210\u6a21\u578b": {"2409.02919": "|**2024-09-04**|[HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts](http://arxiv.org/abs/2409.02919)|**[link](https://github.com/Liuxinyv/HiPrompt)**|\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u751f\u6210\u66f4\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5728\u5904\u7406\u7269\u4f53\u91cd\u590d\u548c\u7ed3\u6784\u4f2a\u5f71\u65b9\u9762\u5e38\u5e38\u9047\u5230\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u6269\u5c55\u5230 4K \u53ca\u66f4\u9ad8\u5206\u8fa8\u7387\u65f6\u3002\u6211\u4eec\u53d1\u73b0\u95ee\u9898\u5728\u4e8e\uff0c\u5355\u4e2a\u63d0\u793a\u751f\u6210\u591a\u4e2a\u5c3a\u5ea6\u7684\u65b9\u5f0f\u6548\u7387\u4f4e\u4e0b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 HiPrompt\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u987b\u5fae\u8c03\u7684\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u901a\u8fc7\u5f15\u5165\u5206\u5c42\u63d0\u793a\u6765\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002\u5206\u5c42\u63d0\u793a\u63d0\u4f9b\u5168\u5c40\u548c\u5c40\u90e8\u6307\u5bfc\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5168\u5c40\u6307\u5bfc\u6765\u81ea\u63cf\u8ff0\u6574\u4f53\u5185\u5bb9\u7684\u7528\u6237\u8f93\u5165\uff0c\u800c\u5c40\u90e8\u6307\u5bfc\u5219\u5229\u7528\u6765\u81ea MLLM \u7684\u9010\u5757\u63cf\u8ff0\u6765\u7cbe\u5fc3\u6307\u5bfc\u5c40\u90e8\u7ed3\u6784\u548c\u7eb9\u7406\u7684\u751f\u6210\u3002\u6b64\u5916\uff0c\u5728\u9006\u5411\u53bb\u566a\u8fc7\u7a0b\u4e2d\uff0c\u751f\u6210\u7684\u566a\u58f0\u88ab\u5206\u89e3\u4e3a\u4f4e\u9891\u548c\u9ad8\u9891\u7a7a\u95f4\u5206\u91cf\u3002\u8fd9\u4e9b\u5206\u91cf\u4ee5\u591a\u4e2a\u63d0\u793a\u7ea7\u522b\u4e3a\u6761\u4ef6\uff0c\u5305\u62ec\u8be6\u7ec6\u7684\u9010\u5757\u63cf\u8ff0\u548c\u66f4\u5e7f\u6cdb\u7684\u56fe\u50cf\u7ea7\u63d0\u793a\uff0c\u4ece\u800c\u4fc3\u8fdb\u5728\u5206\u5c42\u8bed\u4e49\u6307\u5bfc\u4e0b\u7684\u63d0\u793a\u5f15\u5bfc\u53bb\u566a\u3002\u5b83\u8fdb\u4e00\u6b65\u5141\u8bb8\u751f\u6210\u8fc7\u7a0b\u66f4\u591a\u5730\u5173\u6ce8\u5c40\u90e8\u7a7a\u95f4\u533a\u57df\uff0c\u5e76\u786e\u4fdd\u751f\u6210\u7684\u56fe\u50cf\u5728\u9ad8\u6e05\u6670\u5ea6\u4e0b\u4fdd\u6301\u4e00\u81f4\u7684\u5c40\u90e8\u548c\u5168\u5c40\u8bed\u4e49\u3001\u7ed3\u6784\u548c\u7eb9\u7406\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cHiPrompt \u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u7269\u4f53\u91cd\u590d\u5e76\u63d0\u9ad8\u4e86\u7ed3\u6784\u8d28\u91cf\u3002||\n", "2409.02915": "|**2024-09-04**|[Latent Watermarking of Audio Generative Models](http://arxiv.org/abs/2409.02915)|null|\u97f3\u9891\u751f\u6210\u6a21\u578b\u7684\u8fdb\u6b65\u7ed9\u5176\u8d1f\u8d23\u4efb\u7684\u62ab\u9732\u548c\u6ee5\u7528\u68c0\u6d4b\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u901a\u8fc7\u5bf9\u5176\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u7279\u5b9a\u6c34\u5370\u6765\u6807\u8bb0\u6f5c\u5728\u751f\u6210\u6a21\u578b\u7684\u65b9\u6cd5\u3002\u7531\u6b64\u4ea7\u751f\u7684\u6c34\u5370\u6a21\u578b\u751f\u6210\u7684\u6f5c\u5728\u8868\u793a\uff0c\u5176\u89e3\u7801\u8f93\u51fa\u53ef\u4ee5\u88ab\u9ad8\u7f6e\u4fe1\u5ea6\u5730\u68c0\u6d4b\u5230\uff0c\u800c\u65e0\u8bba\u4f7f\u7528\u4f55\u79cd\u89e3\u7801\u65b9\u6cd5\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e0\u9700\u8fdb\u884c\u4e8b\u540e\u6c34\u5370\u6b65\u9aa4\u5373\u53ef\u68c0\u6d4b\u751f\u6210\u7684\u5185\u5bb9\u3002\u5b83\u4e3a\u5f00\u6e90\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6709\u52a9\u4e8e\u8bc6\u522b\u90a3\u4e9b\u5728\u672a\u9075\u5b88\u8bb8\u53ef\u6761\u6b3e\u7684\u60c5\u51b5\u4e0b\u5bf9\u8fd9\u4e9b\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u6216\u4f7f\u7528\u7684\u884d\u751f\u4f5c\u54c1\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u5bf9\u6f5c\u5728\u751f\u6210\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u540e\uff0c\u751f\u6210\u8f93\u51fa\u7684\u68c0\u6d4b\u7cbe\u5ea6\u4e5f\u80fd\u5728\u5047\u9633\u6027\u7387\u4e3a$10^{-3}$\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230 75% \u4ee5\u4e0a\u3002||\n", "2409.02908": "|**2024-09-04**|[Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling](http://arxiv.org/abs/2409.02908)|null|\u63a9\u7801\u6269\u6563\u6a21\u578b (MDM) \u7531\u4e8e\u5176\u76f8\u8f83\u4e8e\u5176\u4ed6\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5df2\u6210\u4e3a\u79bb\u6563\u6570\u636e\u751f\u6210\u5efa\u6a21\u7684\u70ed\u95e8\u7814\u7a76\u8bfe\u9898\uff0c\u5e76\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u4e0e\u81ea\u56de\u5f52\u6a21\u578b (ARM) \u5c55\u5f00\u7ade\u4e89\u3002\u6700\u8fd1\u7b80\u5316\u63a9\u7801\u6269\u6563\u6846\u67b6\u7684\u52aa\u529b\u8fdb\u4e00\u6b65\u4f7f\u5176\u4e0e\u8fde\u7eed\u7a7a\u95f4\u6269\u6563\u6a21\u578b\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u83b7\u5f97\u4e86\u66f4\u6709\u539f\u5219\u7684\u8bad\u7ec3\u548c\u91c7\u6837\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63ed\u793a\u4e86 MDM \u7684\u8bad\u7ec3\u548c\u91c7\u6837\u5728\u7406\u8bba\u4e0a\u90fd\u53ef\u4ee5\u6446\u8131\u65f6\u95f4\u53d8\u91cf\uff08\u53ef\u4ee5\u8bf4\u662f\u6269\u6563\u6a21\u578b\u7684\u5173\u952e\u7279\u5f81\uff09\uff0c\u5e76\u4e14\u7b49\u6548\u4e8e\u63a9\u7801\u6a21\u578b\u3002\u6211\u4eec\u5728\u91c7\u6837\u65b9\u9762\u7684\u8054\u7cfb\u662f\u901a\u8fc7\u6211\u4eec\u63d0\u51fa\u7684\u9996\u6b21\u547d\u4e2d\u91c7\u6837\u5668 (FHS) \u5efa\u7acb\u7684\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 FHS \u5728\u7406\u8bba\u4e0a\u7b49\u6548\u4e8e MDM \u7684\u539f\u59cb\u751f\u6210\u8fc7\u7a0b\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8017\u65f6\u7684\u5206\u7c7b\u91c7\u6837\uff0c\u5e76\u5b9e\u73b0\u4e86 20 \u500d\u7684\u52a0\u901f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u7814\u7a76\u5bf9\u5148\u524d\u5173\u4e8e MDM \u5728\u751f\u6210\u56f0\u60d1\u5ea6\u65b9\u9762\u53ef\u4ee5\u8d85\u8d8a ARM \u7684\u8bf4\u6cd5\u63d0\u51fa\u4e86\u8d28\u7591\u3002\u6211\u4eec\u9996\u6b21\u53d1\u73b0\u4e86\u4e00\u4e2a\u6f5c\u5728\u7684\u6570\u503c\u95ee\u9898\uff0c\u5373\u4f7f\u4f7f\u7528 32 \u4f4d\u6d6e\u70b9\u7cbe\u5ea6\uff0c\u4e5f\u4f1a\u5bfc\u81f4\u4e0d\u51c6\u786e\u7684\u5206\u7c7b\u91c7\u6837\u3002\u6211\u4eec\u8868\u660e\uff0c\u8be5\u6570\u503c\u95ee\u9898\u5728\u7406\u8bba\u4e0a\u548c\u7ecf\u9a8c\u4e0a\u90fd\u964d\u4f4e\u4e86\u6709\u6548\u6e29\u5ea6\uff0c\u5bfc\u81f4\u5148\u524d\u6587\u732e\u4e2d\u5bf9 MDM \u751f\u6210\u7ed3\u679c\u7684\u8bc4\u4f30\u4e0d\u516c\u5e73\u3002||\n", "2409.02851": "|**2024-09-04**|[Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video Diffusion Models](http://arxiv.org/abs/2409.02851)|**[link](https://github.com/Human-VDM/Human-VDM)**|\u4ece\u5355\u5f20RGB\u56fe\u50cf\u751f\u6210\u903c\u771f3D\u4eba\u4f53\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u7cbe\u786e\u7684\u51e0\u4f55\u5efa\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u7eb9\u7406\u548c\u5408\u7406\u7684\u4e0d\u53ef\u89c1\u90e8\u5206\u751f\u6210\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u8fdb\u884c3D\u4eba\u4f53\u751f\u6210\uff0c\u4f46\u5b83\u4eec\u7ecf\u5e38\u9762\u4e34\u89c6\u89d2\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u8fd9\u963b\u788d\u4e86\u9ad8\u8d28\u91cf3D\u4eba\u4f53\u7684\u751f\u6210\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Human-VDM\uff0c\u4e00\u79cd\u4f7f\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u4ece\u5355\u5f20RGB\u56fe\u50cf\u751f\u62103D\u4eba\u4f53\u7684\u65b0\u65b9\u6cd5\u3002Human-VDM\u4f7f\u7528\u9ad8\u65af\u6e32\u67d3\u4e3a3D\u4eba\u4f53\u751f\u6210\u63d0\u4f9b\u4e86\u65f6\u95f4\u4e0a\u4e00\u81f4\u7684\u89c6\u56fe\u3002\u5b83\u7531\u4e09\u4e2a\u6a21\u5757\u7ec4\u6210\uff1a\u89c6\u56fe\u4e00\u81f4\u7684\u4eba\u4f53\u89c6\u9891\u6269\u6563\u6a21\u5757\u3001\u89c6\u9891\u589e\u5f3a\u6a21\u5757\u548c\u9ad8\u65af\u6e32\u67d3\u6a21\u5757\u3002\u9996\u5148\uff0c\u5c06\u5355\u5f20\u56fe\u50cf\u8f93\u5165\u4eba\u4f53\u89c6\u9891\u6269\u6563\u6a21\u5757\u4ee5\u751f\u6210\u8fde\u8d2f\u7684\u4eba\u4f53\u89c6\u9891\u3002\u63a5\u4e0b\u6765\uff0c\u89c6\u9891\u589e\u5f3a\u6a21\u5757\u5e94\u7528\u8d85\u5206\u8fa8\u7387\u548c\u89c6\u9891\u63d2\u503c\u6765\u589e\u5f3a\u751f\u6210\u89c6\u9891\u7684\u7eb9\u7406\u548c\u51e0\u4f55\u5e73\u6ed1\u5ea6\u3002\u6700\u540e\uff0c3D\u4eba\u4f53\u9ad8\u65af\u6e32\u67d3\u6a21\u5757\u5728\u8fd9\u4e9b\u9ad8\u5206\u8fa8\u7387\u548c\u89c6\u89d2\u4e00\u81f4\u7684\u56fe\u50cf\u7684\u6307\u5bfc\u4e0b\u5b66\u4e60\u903c\u771f\u7684\u4eba\u4f53\u3002\u5b9e\u9a8c\u8868\u660e\uff0cHuman-VDM\u53ef\u4ee5\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u4eba\u4f53\uff0c\u5728\u751f\u6210\u8d28\u91cf\u548c\u6570\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://human-vdm.github.io/Human-VDM/||\n", "2409.02845": "|**2024-09-04**|[Multi-Track MusicLDM: Towards Versatile Music Generation with Latent Diffusion Model](http://arxiv.org/abs/2409.02845)|null|\u6269\u6563\u6a21\u578b\u5728\u6d89\u53ca\u97f3\u9891\u548c\u97f3\u4e50\u7684\u8de8\u6a21\u6001\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f8b\u5982\u6587\u672c\u5230\u58f0\u97f3\u548c\u6587\u672c\u5230\u97f3\u4e50\u7684\u751f\u6210\u3002\u8fd9\u4e9b\u6587\u672c\u63a7\u5236\u7684\u97f3\u4e50\u751f\u6210\u6a21\u578b\u901a\u5e38\u4fa7\u91cd\u4e8e\u901a\u8fc7\u6355\u6349\u5168\u5c40\u97f3\u4e50\u5c5e\u6027\uff08\u5982\u6d41\u6d3e\u548c\u60c5\u7eea\uff09\u6765\u751f\u6210\u97f3\u4e50\u3002\u7136\u800c\uff0c\u97f3\u4e50\u521b\u4f5c\u662f\u4e00\u9879\u590d\u6742\u7684\u591a\u5c42\u6b21\u4efb\u52a1\uff0c\u901a\u5e38\u5c06\u97f3\u4e50\u7f16\u6392\u4f5c\u4e3a\u521b\u4f5c\u8fc7\u7a0b\u7684\u4e00\u4e2a\u7ec4\u6210\u90e8\u5206\u3002\u6b64\u8fc7\u7a0b\u6d89\u53ca\u521b\u4f5c\u6bcf\u4e2a\u4e50\u5668\u90e8\u5206\uff0c\u4f7f\u5176\u5728\u8282\u594f\u3001\u529b\u5ea6\u3001\u548c\u58f0\u548c\u65cb\u5f8b\u65b9\u9762\u4e0e\u73b0\u6709\u90e8\u5206\u4fdd\u6301\u4e00\u81f4\uff0c\u8fd9\u9700\u8981\u6bd4\u6587\u672c\u63d0\u793a\u901a\u5e38\u63d0\u4f9b\u7684\u66f4\u7cbe\u786e\u7684\u97f3\u8f68\u63a7\u5236\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5c06 MusicLDM\uff08\u4e00\u79cd\u7528\u4e8e\u97f3\u4e50\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff09\u6269\u5c55\u4e3a\u591a\u8f68\u751f\u6210\u6a21\u578b\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002\u901a\u8fc7\u5b66\u4e60\u5171\u4eab\u4e0a\u4e0b\u6587\u7684\u97f3\u8f68\u7684\u8054\u5408\u6982\u7387\uff0c\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u8de8\u591a\u4e2a\u97f3\u8f68\u751f\u6210\u5f7c\u6b64\u826f\u597d\u5bf9\u5e94\u7684\u97f3\u4e50\uff0c\u65e0\u8bba\u662f\u6709\u6761\u4ef6\u5730\u8fd8\u662f\u65e0\u6761\u4ef6\u5730\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6a21\u578b\u8fd8\u80fd\u591f\u8fdb\u884c\u7f16\u66f2\u751f\u6210\uff0c\u5176\u4e2d\u6a21\u578b\u53ef\u4ee5\u5728\u7ed9\u5b9a\u5176\u4ed6\u97f3\u8f68\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u4efb\u4f55\u97f3\u8f68\u5b50\u96c6\uff08\u4f8b\u5982\uff0c\u751f\u6210\u4e0e\u7ed9\u5b9a\u8d1d\u65af\u548c\u9f13\u97f3\u8f68\u4e92\u8865\u7684\u94a2\u7434\u97f3\u8f68\uff09\u3002\u6211\u4eec\u5c06\u6211\u4eec\u7684\u6a21\u578b\u4e0e\u73b0\u6709\u7684\u591a\u8f68\u751f\u6210\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u603b\u751f\u6210\u4efb\u52a1\u548c\u7f16\u66f2\u751f\u6210\u4efb\u52a1\u7684\u5ba2\u89c2\u6307\u6807\u4e0a\u90fd\u53d6\u5f97\u4e86\u76f8\u5f53\u5927\u7684\u6539\u8fdb\u3002||\n", "2409.02683": "|**2024-09-04**|[Rethinking HTG Evaluation: Bridging Generation and Recognition](http://arxiv.org/abs/2409.02683)|null|\u751f\u6210\u6a21\u578b\u5728\u81ea\u7136\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u8bc4\u4f30\u5df2\u5f97\u5230\u5e7f\u6cdb\u7814\u7a76\u3002\u5373\u4f7f\u5728\u8bf8\u5982\u624b\u5199\u751f\u6210\uff08HTG\uff09\u7b49\u5177\u6709\u72ec\u7279\u7279\u6b8a\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u4f7f\u7528\u4e86\u7c7b\u4f3c\u7684\u534f\u8bae\u548c\u6307\u6807\uff0c\u5373\u4f7f\u5b83\u4eec\u53ef\u80fd\u5e76\u975e\u5b8c\u5168\u5408\u9002\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e09\u79cd\u4e13\u4e3a HTG \u8bc4\u4f30\u91cf\u8eab\u5b9a\u5236\u7684\u5ea6\u91cf\u6307\u6807\uff1a$ \\text{HTG}_{\\text{HTR}} $\u3001$ \\text{HTG}_{\\text{style}} $ \u548c $ \\text{HTG}_{\\text{OOV}} $\uff0c\u5e76\u8ba4\u4e3a\u5b83\u4eec\u66f4\u4fbf\u4e8e\u8bc4\u4f30\u751f\u6210\u624b\u5199\u56fe\u50cf\u7684\u8d28\u91cf\u3002\u8fd9\u4e9b\u6307\u6807\u4f9d\u8d56\u4e8e\u624b\u5199\u6587\u672c\u8bc6\u522b\u548c\u4e66\u5199\u8005\u8bc6\u522b\u6a21\u578b\u7684\u8bc6\u522b\u9519\u8bef/\u51c6\u786e\u7387\uff0c\u5e76\u5f3a\u8c03\u4e66\u5199\u98ce\u683c\u3001\u6587\u672c\u5185\u5bb9\u548c\u591a\u6837\u6027\u662f\u7b26\u5408\u624b\u5199\u56fe\u50cf\u5185\u5bb9\u7684\u4e3b\u8981\u65b9\u9762\u3002\u6211\u4eec\u5728 IAM \u624b\u5199\u6570\u636e\u5e93\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u8bf8\u5982 FID \u4e4b\u7c7b\u7684\u5e7f\u6cdb\u4f7f\u7528\u7684\u6307\u6807\u65e0\u6cd5\u6b63\u786e\u91cf\u5316\u751f\u6210\u624b\u5199\u6837\u672c\u7684\u591a\u6837\u6027\u548c\u5b9e\u7528\u6027\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6307\u6807\u4fe1\u606f\u66f4\u4e30\u5bcc\uff0c\u5e76\u5f3a\u8c03\u4e86 HTG \u4e2d\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\u7684\u5fc5\u8981\u6027\u3002\u6240\u63d0\u51fa\u7684\u6307\u6807\u4e3a\u8bc4\u4f30 HTG \u8d28\u91cf\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u3001\u4fe1\u606f\u66f4\u4e30\u5bcc\u7684\u534f\u8bae\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8 HTR \u7684\u6027\u80fd\u3002\u8bc4\u4f30\u534f\u8bae\u7684\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u5f97\uff1ahttps://github.com/koninik/HTG_evaluation\u3002||\n", "2409.02668": "|**2024-09-04**|[Introduction to Machine Learning](http://arxiv.org/abs/2409.02668)|null|\u672c\u4e66\u4ecb\u7ecd\u4e86\u673a\u5668\u5b66\u4e60\u4e2d\u8bb8\u591a\u7b97\u6cd5\u7684\u5f00\u53d1\u548c\u5206\u6790\u6240\u4f9d\u8d56\u7684\u6570\u5b66\u57fa\u7840\u548c\u6280\u672f\u3002\u672c\u4e66\u9996\u5148\u4ecb\u7ecd\u4e86\u8d2f\u7a7f\u5168\u4e66\u7684\u7b26\u53f7\u8868\u793a\uff0c\u5e76\u56de\u987e\u4e86\u5fae\u79ef\u5206\u3001\u7ebf\u6027\u4ee3\u6570\u548c\u6982\u7387\u8bba\u7684\u57fa\u672c\u6982\u5ff5\uff0c\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6d4b\u5ea6\u8bba\u672f\u8bed\uff0c\u53ef\u4f5c\u4e3a\u4f7f\u7528\u8fd9\u4e9b\u5de5\u5177\u7684\u90e8\u5206\u7684\u9605\u8bfb\u6307\u5357\u3002\u5bfc\u8bba\u7ae0\u8282\u8fd8\u63d0\u4f9b\u4e86\u77e9\u9635\u5206\u6790\u548c\u4f18\u5316\u7684\u80cc\u666f\u77e5\u8bc6\u3002\u540e\u9762\u7684\u7ae0\u8282\u4e3a\u672c\u4e66\u4e2d\u4f7f\u7528\u7684\u8bb8\u591a\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5305\u62ec\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u3001\u8fd1\u4f3c\u65b9\u6cd5\u7b49\u3002\u5728\u8ba8\u8bba\u4e86\u7edf\u8ba1\u9884\u6d4b\u7684\u57fa\u672c\u6982\u5ff5\u4e4b\u540e\uff0c\u672c\u4e66\u4ecb\u7ecd\u4e86\u518d\u751f\u6838\u7406\u8bba\u548c\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u6280\u672f\uff0c\u8fd9\u4e9b\u6280\u672f\u5728\u8bb8\u591a\u5730\u65b9\u90fd\u6709\u5e94\u7528\uff0c\u7136\u540e\u4ecb\u7ecd\u4e86\u5404\u79cd\u76d1\u7763\u7edf\u8ba1\u5b66\u4e60\u7b97\u6cd5\uff0c\u5305\u62ec\u7ebf\u6027\u65b9\u6cd5\u3001\u652f\u6301\u5411\u91cf\u673a\u3001\u51b3\u7b56\u6811\u3001boosting\u548c\u795e\u7ecf\u7f51\u7edc\u3002\u63a5\u4e0b\u6765\u8f6c\u5411\u751f\u6210\u65b9\u6cd5\uff0c\u9996\u5148\u4ecb\u7ecd\u4e86\u91c7\u6837\u65b9\u6cd5\u548c\u9a6c\u5c14\u53ef\u592b\u94fe\u7406\u8bba\u3002\u63a5\u4e0b\u6765\u7684\u7ae0\u8282\u63cf\u8ff0\u4e86\u56fe\u6a21\u578b\u7406\u8bba\uff0c\u4ecb\u7ecd\u4e86\u6f5c\u53d8\u91cf\u6a21\u578b\u7684\u53d8\u5206\u65b9\u6cd5\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u751f\u6210\u6a21\u578b\u3002\u63a5\u4e0b\u6765\u7684\u7ae0\u8282\u91cd\u70b9\u4ecb\u7ecd\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ec\u805a\u7c7b\u3001\u56e0\u5b50\u5206\u6790\u548c\u6d41\u5f62\u5b66\u4e60\u3002\u672c\u4e66\u7684\u6700\u540e\u4e00\u7ae0\u504f\u5411\u7406\u8bba\uff0c\u8ba8\u8bba\u4e86\u96c6\u4e2d\u4e0d\u7b49\u5f0f\u548c\u6cdb\u5316\u754c\u3002||\n", "2409.02664": "|**2024-09-04**|[Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection](http://arxiv.org/abs/2409.02664)|null|The proliferation of deepfake faces poses huge potential negative impacts on our daily lives. Despite substantial advancements in deepfake detection over these years, the generalizability of existing methods against forgeries from unseen datasets or created by emerging generative models remains constrained. In this paper, inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach that repurposes a well-trained VLM for general deepfake detection. Motivated by the model reprogramming paradigm that manipulates the model prediction via data perturbations, our method can reprogram a pretrained VLM model (e.g., CLIP) solely based on manipulating its input without tuning the inner parameters. Furthermore, we insert a pseudo-word guided by facial identity into the text prompt. Extensive experiments on several popular benchmarks demonstrate that (1) the cross-dataset and cross-manipulation performances of deepfake detection can be significantly and consistently improved (e.g., over 88% AUC in cross-dataset setting from FF++ to WildDeepfake) using a pre-trained CLIP model with our proposed reprogramming method; (2) our superior performances are at less cost of trainable parameters, making it a promising approach for real-world applications.||\n", "2409.02657": "|**2024-09-04**|[PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for One-Shot Talking Head Generation](http://arxiv.org/abs/2409.02657)|null|While previous audio-driven talking head generation (THG) methods generate head poses from driving audio, the generated poses or lips cannot match the audio well or are not editable. In this study, we propose \\textbf{PoseTalk}, a THG system that can freely generate lip-synchronized talking head videos with free head poses conditioned on text prompts and audio. The core insight of our method is using head pose to connect visual, linguistic, and audio signals. First, we propose to generate poses from both audio and text prompts, where the audio offers short-term variations and rhythm correspondence of the head movements and the text prompts describe the long-term semantics of head motions. To achieve this goal, we devise a Pose Latent Diffusion (PLD) model to generate motion latent from text prompts and audio cues in a pose latent space. Second, we observe a loss-imbalance problem: the loss for the lip region contributes less than 4\\% of the total reconstruction loss caused by both pose and lip, making optimization lean towards head movements rather than lip shapes. To address this issue, we propose a refinement-based learning strategy to synthesize natural talking videos using two cascaded networks, i.e., CoarseNet, and RefineNet. The CoarseNet estimates coarse motions to produce animated images in novel poses and the RefineNet focuses on learning finer lip motions by progressively estimating lip motions from low-to-high resolutions, yielding improved lip-synchronization performance. Experiments demonstrate our pose prediction strategy achieves better pose diversity and realness compared to text-only or audio-only, and our video generator model outperforms state-of-the-art methods in synthesizing talking videos with natural head motions. Project: https://junleen.github.io/projects/posetalk.||\n", "2409.02653": "|**2024-09-04**|[Skip-and-Play: Depth-Driven Pose-Preserved Image Generation for Any Objects](http://arxiv.org/abs/2409.02653)|null|The emergence of diffusion models has enabled the generation of diverse high-quality images solely from text, prompting subsequent efforts to enhance the controllability of these models. Despite the improvement in controllability, pose control remains limited to specific objects (e.g., humans) or poses (e.g., frontal view) due to the fact that pose is generally controlled via camera parameters (e.g., rotation angle) or keypoints (e.g., eyes, nose). Specifically, camera parameters-conditional pose control models generate unrealistic images depending on the object, owing to the small size of 3D datasets for training. Also, keypoint-based approaches encounter challenges in acquiring reliable keypoints for various objects (e.g., church) or poses (e.g., back view). To address these limitations, we propose depth-based pose control, as depth maps are easily obtainable from a single depth estimation model regardless of objects and poses, unlike camera parameters and keypoints. However, depth-based pose control confronts issues of shape dependency, as depth maps influence not only the pose but also the shape of the generated images. To tackle this issue, we propose Skip-and-Play (SnP), designed via analysis of the impact of three components of depth-conditional ControlNet on the pose and the shape of the generated images. To be specific, based on the analysis, we selectively skip parts of the components to mitigate shape dependency on the depth map while preserving the pose. Through various experiments, we demonstrate the superiority of SnP over baselines and showcase the ability of SnP to generate images of diverse objects and poses. Remarkably, SnP exhibits the ability to generate images even when the objects in the condition (e.g., a horse) and the prompt (e.g., a hedgehog) differ from each other.||\n", "2409.03757": "|**2024-09-05**|[Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding](http://arxiv.org/abs/2409.03757)|**[link](https://github.com/yunzeman/lexicon3d)**|\u590d\u6742\u4e09\u7ef4\u573a\u666f\u7406\u89e3\u8fd1\u5e74\u6765\u5907\u53d7\u5173\u6ce8\uff0c\u573a\u666f\u7f16\u7801\u7b56\u7565\u5728\u5176\u4e2d\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u9488\u5bf9\u4e0d\u540c\u573a\u666f\u7684\u6700\u4f73\u573a\u666f\u7f16\u7801\u7b56\u7565\u4ecd\u4e0d\u660e\u786e\uff0c\u7279\u522b\u662f\u4e0e\u57fa\u4e8e\u56fe\u50cf\u7684\u7f16\u7801\u7b56\u7565\u76f8\u6bd4\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5bf9\u7528\u4e8e\u4e09\u7ef4\u573a\u666f\u7406\u89e3\u7684\u5404\u79cd\u89c6\u89c9\u7f16\u7801\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u7814\u7a76\uff0c\u786e\u5b9a\u4e86\u6bcf\u4e2a\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u6db5\u76d6\u4e86\u4e03\u79cd\u89c6\u89c9\u57fa\u7840\u7f16\u7801\u5668\uff0c\u5305\u62ec\u57fa\u4e8e\u56fe\u50cf\u3001\u57fa\u4e8e\u89c6\u9891\u548c\u4e09\u7ef4\u57fa\u7840\u6a21\u578b\u3002\u6211\u4eec\u5728\u56db\u4e2a\u4efb\u52a1\u4e2d\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\uff1a\u89c6\u89c9\u8bed\u8a00\u573a\u666f\u63a8\u7406\u3001\u89c6\u89c9\u5b9a\u4f4d\u3001\u5206\u5272\u548c\u914d\u51c6\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u4fa7\u91cd\u4e8e\u573a\u666f\u7406\u89e3\u7684\u4e0d\u540c\u65b9\u9762\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u5f97\u51fa\u4e86\u4ee5\u4e0b\u4e3b\u8981\u53d1\u73b0\uff1aDINOv2 \u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u89c6\u9891\u6a21\u578b\u5728\u5bf9\u8c61\u7ea7\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6269\u6563\u6a21\u578b\u6709\u5229\u4e8e\u51e0\u4f55\u4efb\u52a1\uff0c\u800c\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u8bed\u8a00\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u610f\u60f3\u4e0d\u5230\u7684\u5c40\u9650\u6027\u3002\u8fd9\u4e9b\u89c1\u89e3\u6311\u6218\u4e86\u4e00\u4e9b\u4f20\u7edf\u8ba4\u77e5\uff0c\u4e3a\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u672a\u6765\u7684\u89c6\u89c9\u8bed\u8a00\u548c\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e2d\u9700\u8981\u66f4\u7075\u6d3b\u7684\u7f16\u7801\u5668\u9009\u62e9\u3002||\n", "2409.03745": "|**2024-09-05**|[ArtiFade: Learning to Generate High-quality Subject from Blemished Images](http://arxiv.org/abs/2409.03745)|null|\u4ee5\u4e3b\u9898\u4e3a\u4e3b\u5bfc\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6280\u672f\u5728\u5b66\u4e60\u548c\u6355\u6349\u4e3b\u9898\u7279\u5f81\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u5373\u4f7f\u53ea\u4f7f\u7528\u6709\u9650\u6570\u91cf\u7684\u56fe\u50cf\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u5f53\u8f93\u5165\u56fe\u50cf\u5b58\u5728\u7455\u75b5\u65f6\uff0c\u53ef\u80fd\u96be\u4ee5\u751f\u6210\u5408\u7406\u7684\u56fe\u50cf\u3002\u8fd9\u4e3b\u8981\u5f52\u56e0\u4e8e\u5f53\u524d\u6280\u672f\u5728\u533a\u5206\u4e3b\u9898\u76f8\u5173\u7279\u5f81\u548c\u5e72\u6270\u6027\u7455\u75b5\u65b9\u9762\u7684\u80fd\u529b\u4e0d\u8db3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86ArtiFade\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u6210\u529f\u5730\u4ece\u6709\u7455\u75b5\u7684\u6570\u636e\u96c6\u4e2d\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u7684\u65e0\u7455\u75b5\u56fe\u50cf\u3002\u5177\u4f53\u6765\u8bf4\uff0cArtiFade\u5229\u7528\u9884\u5148\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5fae\u8c03\u6765\u6d88\u9664\u7455\u75b5\u3002\u901a\u8fc7\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u5305\u542b\u65e0\u7455\u75b5\u56fe\u50cf\u53ca\u5176\u5bf9\u5e94\u7684\u6709\u7455\u75b5\u56fe\u50cf\u7684\u4e13\u95e8\u6570\u636e\u96c6\u6765\u5b9e\u73b0\u7455\u75b5\u7684\u6d88\u9664\u3002ArtiFade\u8fd8\u786e\u4fdd\u4e86\u4fdd\u7559\u6269\u6563\u6a21\u578b\u4e2d\u56fa\u6709\u7684\u539f\u59cb\u751f\u6210\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u4e3b\u9898\u9a71\u52a8\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u548c\u65e0\u7455\u75b5\u56fe\u50cf\u65b9\u9762\u7684\u6574\u4f53\u6027\u80fd\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u4e3a\u8fd9\u9879\u4efb\u52a1\u8bbe\u8ba1\u4e86\u8bc4\u4f30\u57fa\u51c6\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86ArtiFade\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u60c5\u51b5\u4e0b\u90fd\u80fd\u6709\u6548\u53bb\u9664\u7455\u75b5\u7684\u6cdb\u5316\u80fd\u529b\u3002||\n", "2409.03644": "|**2024-09-05**|[RealisHuman: A Two-Stage Approach for Refining Malformed Human Parts in Generated Images](http://arxiv.org/abs/2409.03644)|null|\u8fd1\u5e74\u6765\uff0c\u6269\u6563\u6a21\u578b\u5f7b\u5e95\u6539\u53d8\u4e86\u89c6\u89c9\u751f\u6210\u9886\u57df\uff0c\u5176\u6027\u80fd\u8d85\u8d8a\u4e86\u751f\u6210\u5bf9\u6297\u7f51\u7edc (GANs) \u7b49\u4f20\u7edf\u6846\u67b6\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4eba\u7c7b\u53ca\u5176\u8bed\u4e49\u90e8\u5206\uff08\u5982\u624b\u548c\u8138\uff09\u590d\u6742\u7684\u7ed3\u6784\uff0c\u751f\u6210\u5177\u6709\u771f\u5b9e\u611f\u7684\u4eba\u7c7b\u56fe\u50cf\u4ecd\u7136\u662f\u4e00\u9879\u91cd\u5927\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a RealisHuman \u7684\u65b0\u578b\u540e\u5904\u7406\u89e3\u51b3\u65b9\u6848\u3002RealisHuman \u6846\u67b6\u5206\u4e24\u4e2a\u9636\u6bb5\u8fd0\u884c\u3002\u9996\u5148\uff0c\u5b83\u4f7f\u7528\u539f\u59cb\u7684\u7578\u5f62\u90e8\u5206\u4f5c\u4e3a\u53c2\u8003\uff0c\u751f\u6210\u903c\u771f\u7684\u4eba\u4f53\u90e8\u4f4d\uff08\u5982\u624b\u6216\u8138\uff09\uff0c\u786e\u4fdd\u7ec6\u8282\u4e0e\u539f\u59cb\u56fe\u50cf\u4e00\u81f4\u3002\u5176\u6b21\uff0c\u5b83\u901a\u8fc7\u91cd\u65b0\u7ed8\u5236\u5468\u56f4\u533a\u57df\u5c06\u6821\u6b63\u540e\u7684\u4eba\u4f53\u90e8\u4f4d\u65e0\u7f1d\u5730\u878d\u5165\u5230\u5176\u5bf9\u5e94\u7684\u4f4d\u7f6e\uff0c\u4ee5\u786e\u4fdd\u5e73\u6ed1\u903c\u771f\u7684\u878d\u5408\u3002RealisHuman \u6846\u67b6\u663e\u8457\u589e\u5f3a\u4e86\u4eba\u7c7b\u751f\u6210\u7684\u771f\u5b9e\u611f\uff0c\u8fd9\u53ef\u4ee5\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u6307\u6807\u7684\u663e\u8457\u6539\u8fdb\u5f97\u5230\u8bc1\u660e\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/Wangbenzhi/RealisHuman \u83b7\u53d6\u3002||\n", "2409.03636": "|**2024-09-05**|[DiffEVC: Any-to-Any Emotion Voice Conversion with Expressive Guidance](http://arxiv.org/abs/2409.03636)|null|\u60c5\u611f\u8bed\u97f3\u8f6c\u6362 (EVC) \u901a\u8fc7\u653e\u5927\u79ef\u6781\u7ebf\u7d22\u548c\u51cf\u5c11\u6d88\u6781\u7ebf\u7d22\u6765\u6539\u53d8\u8bed\u97f3\u60c5\u611f\uff0c\u4ece\u800c\u589e\u5f3a\u6c9f\u901a\u3002\u8fd9\u9879\u590d\u6742\u7684\u4efb\u52a1\u6d89\u53ca\u8bed\u97f3\u8d28\u91cf\u3001\u8bf4\u8bdd\u8005\u7279\u5f81\u548c\u5185\u5bb9\u7b49\u7ea0\u7f20\u4e0d\u6e05\u7684\u56e0\u7d20\u3002\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982 GAN \u548c\u81ea\u52a8\u7f16\u7801\u5668\uff09\u901a\u8fc7\u5b66\u4e60\u6620\u5c04\u6216\u89e3\u8026\u7279\u5f81\u5728 EVC \u4e2d\u53d6\u5f97\u4e86\u4e00\u5b9a\u7684\u6210\u529f\uff0c\u4f46\u9762\u4e34\u7740\u4e0d\u7a33\u5b9a\u6027\u548c\u8bed\u97f3\u8d28\u91cf\u4e0b\u964d\u7b49\u6311\u6218\u3002\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u7a33\u5b9a\u7684\u8bad\u7ec3\u548c\u9ad8\u8d28\u91cf\u7684\u751f\u6210\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684 EVC \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u4e92\u4fe1\u606f\u635f\u5931\u548c\u8f85\u52a9\u6a21\u578b\u6765\u89e3\u8026\u60c5\u611f\u548c\u8bf4\u8bdd\u8005\u8eab\u4efd\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u8868\u8fbe\u6027\u5f15\u5bfc\u673a\u5236\uff0c\u4ee5\u6539\u5584\u60c5\u611f\u8f6c\u6362\uff0c\u540c\u65f6\u4fdd\u6301\u8bf4\u8bdd\u8005\u7279\u5f81\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5bf9\u4e8e\u672a\u77e5\u8bf4\u8bdd\u8005\u548c\u60c5\u611f\u7684\u6709\u6548\u6027\uff0c\u5728 EVC \u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002||\n", "2409.03600": "|**2024-09-05**|[TCDiff: Triple Condition Diffusion Model with 3D Constraints for Stylizing Synthetic Faces](http://arxiv.org/abs/2409.03600)|**[link](https://github.com/bovifocr/tcdiff)**|\u4e00\u4e2a\u9c81\u68d2\u7684\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u9700\u8981\u4f7f\u7528\u5305\u542b\u5927\u91cf\u4e2a\u4f53\u4ee5\u53ca\u6bcf\u4e2a\u4e2a\u4f53\u5728\u4e0d\u540c\u6761\u4ef6\uff08\u4f8b\u5982\u59ff\u6001\u3001\u8868\u60c5\u3001\u5e74\u9f84\u3001\u566a\u58f0\u548c\u906e\u6321\uff09\u4e0b\u7684\u5927\u91cf\u6837\u672c\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002\u7531\u4e8e\u4f26\u7406\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5927\u578b\u771f\u5b9e\u4eba\u8138\u6570\u636e\u96c6\uff08\u4f8b\u5982 MS1MV3\uff09\u5df2\u88ab\u505c\u7528\uff0c\u5e76\u4e14\u5df2\u7ecf\u63d0\u51fa\u4e86\u5229\u7528 GAN \u548c\u6269\u6563\u6a21\u578b\u7684\u5408\u6210\u4eba\u8138\u751f\u6210\u5668\uff0c\u4f8b\u5982 SYNFace\u3001SFace\u3001DigiFace-1M\u3001IDiff-Face\u3001DCFace \u548c GANDiffFace\uff0c\u65e8\u5728\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u3002\u5176\u4e2d\u4e00\u4e9b\u65b9\u6cd5\u53ef\u4ee5\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u771f\u5b9e\u4eba\u8138\uff0c\u4f46\u7c7b\u5185\u5dee\u5f02\u8f83\u4f4e\uff0c\u800c\u53e6\u4e00\u4e9b\u65b9\u6cd5\u5219\u751f\u6210\u5177\u6709\u9ad8\u5dee\u5f02\u6027\u4f46\u8eab\u4efd\u4e00\u81f4\u6027\u8f83\u4f4e\u7684\u4eba\u8138\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u91cd\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff08TCDiff\uff09\uff0c\u901a\u8fc7 2D \u548c 3D \u4eba\u8138\u7ea6\u675f\u6765\u6539\u8fdb\u4ece\u771f\u5b9e\u4eba\u8138\u5230\u5408\u6210\u4eba\u8138\u7684\u4eba\u8138\u98ce\u683c\u8fc1\u79fb\uff0c\u5728\u4fdd\u6301\u5fc5\u8981\u7684\u7c7b\u5185\u9ad8\u5dee\u5f02\u6027\u7684\u540c\u65f6\u589e\u5f3a\u4eba\u8138\u8eab\u4efd\u4e00\u81f4\u6027\u3002\u4f7f\u7528\u6211\u4eec\u65b0\u7684\u6570\u636e\u96c6\u7684 1k\u30012k \u548c 5k \u7c7b\u8fdb\u884c\u8bad\u7ec3\u7684\u4eba\u8138\u8bc6\u522b\u5b9e\u9a8c\u5728 LFW\u3001CFP-FP\u3001AgeDB \u548c BUPT \u7b49\u771f\u5b9e\u4eba\u8138\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5408\u6210\u6570\u636e\u96c6\u3002\u6211\u4eec\u7684\u6e90\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u5f97\uff1ahttps://github.com/BOVIFOCR/tcdiff\u3002||\n", "2409.03550": "|**2024-09-05**|[DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any Architecture](http://arxiv.org/abs/2409.03550)|null|\u6269\u6563\u6a21\u578b (DM) \u5728\u5404\u4e2a\u9886\u57df\u90fd\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5176\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u7684\u63a8\u7406\u901f\u5ea6\u6162\u548c\u8ba1\u7b97\u9700\u6c42\u9ad8\u5374\u963b\u788d\u4e86\u5176\u53d1\u5c55\u3002\u52a0\u901fDM\u6700\u5e38\u7528\u7684\u65b9\u6cd5\u662f\u51cf\u5c11\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u53bb\u566a\u6b65\u9aa4\uff0c\u8fd9\u53ef\u4ee5\u901a\u8fc7\u66f4\u5feb\u7684\u91c7\u6837\u6c42\u89e3\u5668\u6216\u77e5\u8bc6\u84b8\u998f (KD) \u6765\u5b9e\u73b0\u3002\u4e0e\u5148\u524d\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5c06\u5927\u578b\u9884\u8bad\u7ec3DM\u7684\u529f\u80fd\u8fc1\u79fb\u5230\u66f4\u5feb\u7684\u67b6\u6784\u4e2d\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u4ee5\u72ec\u7279\u7684\u65b9\u5f0f\u4f7f\u7528KD\uff0c\u901a\u8fc7\u5c06\u751f\u6210\u80fd\u529b\u63d0\u70bc\u5230\u66f4\u5feb\u7684\u53d8\u4f53\u4e2d\u6765\u538b\u7f29DM\u3002\u6b64\u5916\uff0c\u8003\u8651\u5230\u6e90\u6570\u636e\u4e0d\u53ef\u8bbf\u95ee\u6216\u5bf9\u4e8e\u5f53\u524d\u7684\u751f\u6210\u6a21\u578b\u6765\u8bf4\u5b58\u50a8\u91cf\u592a\u5927\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u6e90\u6570\u636e\u84b8\u998f\u8303\u5f0f\uff0c\u79f0\u4e3a\u6269\u6563\u6a21\u578b\u7684\u65e0\u6570\u636e\u77e5\u8bc6\u84b8\u998f (DKDM)\u3002\u901a\u5e38\uff0c\u6211\u4eec\u5efa\u7acb\u7684DKDM\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a1) DKDM\u76ee\u6807\u51fd\u6570\uff0c\u5b83\u4f7f\u7528\u9884\u8bad\u7ec3DM\u751f\u6210\u7684\u5408\u6210\u53bb\u566a\u6570\u636e\u6765\u4f18\u5316\u66f4\u5feb\u7684DM\uff0c\u800c\u65e0\u9700\u6e90\u6570\u636e\uff1b2) \u52a8\u6001\u8fed\u4ee3\u84b8\u998f\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u7075\u6d3b\u5730\u7ec4\u7ec7\u53bb\u566a\u6570\u636e\u7684\u5408\u6210\uff0c\u9632\u6b62\u7531\u4e8e\u751f\u6210\u901f\u5ea6\u6162\u800c\u51cf\u6162\u4f18\u5316\u8fc7\u7a0b\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u6b21\u5c1d\u8bd5\u4f7f\u7528KD\u4ee5\u65e0\u6570\u636e\u7684\u65b9\u5f0f\u5c06DM\u63d0\u70bc\u5230\u4efb\u4f55\u67b6\u6784\u4e2d\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u7684DKDM\u4e0e\u5927\u591a\u6570\u73b0\u6709\u7684\u52a0\u901f\u65b9\u6cd5\uff08\u4f8b\u5982\u51cf\u5c11\u53bb\u566a\u6b65\u9aa4\u3001\u91cf\u5316\u548c\u526a\u679d\uff09\u662f\u6b63\u4ea4\u7684\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684DKDM\u80fd\u591f\u63a8\u5bfc\u51fa\u901f\u5ea6\u63d0\u9ad82\u500d\u7684DM\uff0c\u5176\u6027\u80fd\u4e0e\u57fa\u7ebf\u4fdd\u6301\u4e00\u81f4\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684DKDM\u4f7f\u9884\u8bad\u7ec3\u7684DM\u80fd\u591f\u4f5c\u4e3a\u201c\u6570\u636e\u96c6\u201d\u6765\u8bad\u7ec3\u65b0\u7684DM\u3002||\n", "2409.03514": "|**2024-09-05**|[Blended Latent Diffusion under Attention Control for Real-World Video Editing](http://arxiv.org/abs/2409.03514)|null|\u7531\u4e8e\u7f3a\u4e4f\u5b8c\u5168\u516c\u5f00\u53ef\u7528\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\uff0c\u76ee\u524d\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u503e\u5411\u4e8e\u5efa\u7acb\u5728\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e4b\u4e0a\uff0c\u7136\u800c\uff0c\u5728\u5904\u7406\u5177\u6709\u65f6\u95f4\u4fe1\u606f\u7684\u89c6\u9891\u5c40\u90e8\u7f16\u8f91\u65b9\u9762\uff0c\u5b83\u4eec\u4ecd\u7136\u9762\u4e34\u7740\u5de8\u5927\u7684\u6311\u6218\u3002\u9996\u5148\uff0c\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u8bd5\u56fe\u901a\u8fc7\u9884\u5148\u5b9a\u4e49\u7684\u63a9\u7801\u6765\u5173\u6ce8\u5c40\u90e8\u533a\u57df\u7f16\u8f91\uff0c\u4f46\u7531\u4e8e\u6bcf\u4e00\u5e27\u7684\u7a7a\u95f4\u6574\u4f53\u751f\u6210\uff0c\u5916\u90e8\u533a\u57df\u80cc\u666f\u7684\u4fdd\u7559\u5e76\u4e0d\u7406\u60f3\u3002\u6b64\u5916\uff0c\u7531\u7528\u6237\u4e13\u95e8\u63d0\u4f9b\u63a9\u7801\u662f\u4e00\u9879\u989d\u5916\u7684\u6602\u8d35\u5de5\u4f5c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u96c6\u6210\u5230\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u7684\u81ea\u4e3b\u63a9\u7801\u7b56\u7565\u3002\u6700\u540e\u4f46\u540c\u6837\u91cd\u8981\u7684\u662f\uff0c\u56fe\u50cf\u7ea7\u9884\u8bad\u7ec3\u6a21\u578b\u6ca1\u6709\u5b66\u4e60\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u800c\u8fd9\u5bf9\u4e8e\u8868\u8fbe\u8fd0\u52a8\u548c\u52a8\u6001\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u91c7\u7528\u56fe\u50cf\u7ea7\u6df7\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6765\u6267\u884c\u5c40\u90e8\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5229\u7528 DDIM \u53cd\u6f14\u6765\u83b7\u53d6\u6f5c\u5728\u5411\u91cf\u4f5c\u4e3a\u80cc\u666f\u6f5c\u5728\u5411\u91cf\uff0c\u800c\u4e0d\u662f\u968f\u673a\u566a\u58f0\u7684\u6f5c\u5728\u5411\u91cf\uff0c\u4ee5\u66f4\u597d\u5730\u4fdd\u7559\u8f93\u5165\u89c6\u9891\u7684\u80cc\u666f\u4fe1\u606f\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4ece\u6269\u6563\u6b65\u9aa4\u4e2d\u7684\u4ea4\u53c9\u6ce8\u610f\u56fe\u884d\u751f\u7684\u81ea\u4e3b\u63a9\u7801\u5236\u9020\u673a\u5236\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u5c06 U-Net \u7684\u81ea\u6ce8\u610f\u529b\u5757\u8f6c\u6362\u4e3a\u65f6\u7a7a\u5757\u6765\u589e\u5f3a\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002||\n", "2409.03455": "|**2024-09-05**|[Data-free Distillation with Degradation-prompt Diffusion for Multi-weather Image Restoration](http://arxiv.org/abs/2409.03455)|null|\u591a\u5929\u6c14\u56fe\u50cf\u590d\u539f\u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u8fdb\u5c55\uff0c\u4f46\u6a21\u578b\u5bb9\u91cf\u7684\u589e\u52a0\u548c\u6602\u8d35\u7684\u6570\u636e\u83b7\u53d6\u9650\u5236\u4e86\u5176\u5728\u5185\u5b58\u6709\u9650\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u3002\u65e0\u6570\u636e\u84b8\u998f\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u5141\u8bb8\u4ece\u9884\u8bad\u7ec3\u7684\u6559\u5e08\u6a21\u578b\u4e2d\u5b66\u4e60\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u3002\u73b0\u6709\u7684\u65e0\u6570\u636e\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5229\u7528GAN\u751f\u6210\u7684\u4f2a\u6570\u636e\u6216\u4ece\u4e92\u8054\u7f51\u6536\u96c6\u7684\u771f\u5b9e\u6570\u636e\u6765\u4f18\u5316\u6a21\u578b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u9047\u5230\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6216\u4e0e\u539f\u59cb\u6570\u636e\u5b58\u5728\u57df\u504f\u79fb\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u9000\u5316\u63d0\u793a\u6269\u6563\u7684\u65e0\u6570\u636e\u84b8\u998f\u591a\u5929\u6c14\u56fe\u50cf\u590d\u539f\u6846\u67b6\uff08D4IR\uff09\u3002\u5b83\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u4ee3\u66ffGAN\u4ee5\u907f\u514d\u6a21\u578b\u5d29\u6e83\uff0c\u5e76\u7ed3\u5408\u4e86\u9000\u5316\u611f\u77e5\u63d0\u793a\u9002\u914d\u5668\uff0c\u4ee5\u4fc3\u8fdb\u5185\u5bb9\u9a71\u52a8\u7684\u6761\u4ef6\u6269\u6563\uff0c\u4ece\u800c\u751f\u6210\u4e0e\u57df\u76f8\u5173\u7684\u56fe\u50cf\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u9996\u5148\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u7684\u9000\u5316\u63d0\u793a\u9002\u914d\u5668\uff0c\u7528\u4e8e\u4ece\u7f51\u7edc\u6536\u96c6\u7684\u9000\u5316\u56fe\u50cf\u4e2d\u6355\u83b7\u9000\u5316\u611f\u77e5\u63d0\u793a\u3002\u7136\u540e\uff0c\u5c06\u6536\u96c6\u5230\u7684\u672a\u914d\u5bf9\u7684\u5e72\u51c0\u56fe\u50cf\u6270\u52a8\u5230\u7a33\u5b9a\u6269\u6563\u7684\u6f5c\u5728\u7279\u5f81\u4e2d\uff0c\u5e76\u4ee5\u9000\u5316\u611f\u77e5\u63d0\u793a\u4e3a\u6761\u4ef6\uff0c\u5408\u6210\u65b0\u7684\u57df\u76f8\u5173\u9000\u5316\u56fe\u50cf\uff0c\u7528\u4e8e\u77e5\u8bc6\u84b8\u998f\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u4e0e\u4f7f\u7528\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u84b8\u998f\u7684\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u751a\u81f3\u4f18\u4e8e\u5176\u4ed6\u4e3b\u6d41\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u3002||\n", "2409.03417": "|**2024-09-05**|[Convergence Rates for the Maximum A Posteriori Estimator in PDE-Regression Models with Random Design](http://arxiv.org/abs/2409.03417)|null|\u6211\u4eec\u8003\u8651\u4ece\u9ad8\u65af\u56de\u5f52\u95ee\u9898$Y = \\mathscr{G}(\\theta)(Z)+\\varepsilon$\u4ea7\u751f\u7684\u6570\u636e\u4e2d\u6062\u590d\u53c2\u6570$\\theta\\in H^\\alpha$\u7684\u7edf\u8ba1\u9006\u95ee\u9898\uff0c\u5176\u4e2d$\\mathscr{G}:\\mathbb{L}^2\\to\\mathbb{L}^2$\u662f\u975e\u7ebf\u6027\u6b63\u5411\u6620\u5c04\uff0c$Z$\u662f\u968f\u673a\u8bbe\u8ba1\u70b9\uff0c$\\varepsilon$\u662f\u9ad8\u65af\u566a\u58f0\u3002\u4f30\u8ba1\u7b56\u7565\u57fa\u4e8e$\\Vert\\cdot\\Vert_{H^\\alpha}$-\u7ea6\u675f\u4e0b\u7684\u6700\u5c0f\u4e8c\u4e58\u6cd5\u3002\u6211\u4eec\u5728\u6b63\u5411\u6620\u5c04$\\mathscr{G}$\u6ee1\u8db3Lipschitz\u7c7b\u578b\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\uff0c\u5efa\u7acb\u4e86\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\u91cf$\\hat{\\theta}$\u4f5c\u4e3a\u7ed9\u5b9a\u6cdb\u51fd\u7684\u6700\u5927\u503c\u7684\u5b58\u5728\u6027\u3002\u8bc1\u660e\u4e86\u4e00\u4e2a\u4e00\u822c\u7684\u6d53\u5ea6\u7ed3\u679c\uff0c\u5e76\u7528\u5b83\u6765\u8bc1\u660e\u9884\u6d4b\u8bef\u5dee\u7684\u4e00\u81f4\u6027\u548c\u4e0a\u754c\u3002\u76f8\u5e94\u7684\u6536\u655b\u901f\u5ea6\u4e0d\u4ec5\u53cd\u6620\u4e86\u76ee\u6807\u53c2\u6570\u7684\u5e73\u6ed1\u6027\uff0c\u8fd8\u53cd\u6620\u4e86\u6f5c\u5728\u9006\u95ee\u9898\u7684\u9002\u5b9a\u6027\u3002\u6211\u4eec\u5c06\u4e00\u822c\u6a21\u578b\u5e94\u7528\u4e8e\u8fbe\u897f\u95ee\u9898\uff0c\u5176\u4e2dPDE\u7684\u672a\u77e5\u7cfb\u6570\u51fd\u6570$f$\u7684\u6062\u590d\u662f\u4ee4\u4eba\u611f\u5174\u8da3\u7684\u3002\u5bf9\u4e8e\u8fd9\u4e2a\u4f8b\u5b50\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u9884\u6d4b\u8bef\u5dee\u548c\u4f30\u8ba1\u8bef\u5dee\u7684\u76f8\u5e94\u6536\u655b\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u7b80\u8981\u8ba8\u8bba\u4e86\u8be5\u4e00\u822c\u6a21\u578b\u5bf9\u5176\u4ed6\u95ee\u9898\u7684\u9002\u7528\u6027\u3002||\n", "2409.03403": "|**2024-09-05**|[RoVi-Aug: Robot and Viewpoint Augmentation for Cross-Embodiment Robot Learning](http://arxiv.org/abs/2409.03403)|null|\u6269\u5927\u673a\u5668\u4eba\u5b66\u4e60\u89c4\u6a21\u9700\u8981\u5e9e\u5927\u800c\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\uff0c\u5982\u4f55\u6709\u6548\u5730\u91cd\u590d\u4f7f\u7528\u6536\u96c6\u5230\u7684\u6570\u636e\u5e76\u5c06\u7b56\u7565\u8fc1\u79fb\u5230\u65b0\u7684\u673a\u5668\u4eba\u5e73\u53f0\u4ecd\u7136\u662f\u4e00\u4e2a\u60ac\u800c\u672a\u51b3\u7684\u95ee\u9898\u3002\u8bf8\u5982Open-X Embodiment (OXE) \u9879\u76ee\u7b49\u65b0\u5174\u7814\u7a76\u5df2\u7ecf\u8868\u660e\uff0c\u901a\u8fc7\u7ec4\u5408\u5305\u542b\u4e0d\u540c\u673a\u5668\u4eba\u7684\u6570\u636e\u96c6\u6765\u5229\u7528\u6280\u80fd\u662f\u6709\u5e0c\u671b\u7684\u3002\u7136\u800c\uff0c\u8bb8\u591a\u6570\u636e\u96c6\u4e2d\u673a\u5668\u4eba\u7c7b\u578b\u548c\u76f8\u673a\u89d2\u5ea6\u5206\u5e03\u7684\u4e0d\u5e73\u8861\u4f7f\u5f97\u7b56\u7565\u5bb9\u6613\u8fc7\u62df\u5408\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86RoVi-Aug\uff0c\u5b83\u5229\u7528\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u5408\u6210\u5177\u6709\u4e0d\u540c\u673a\u5668\u4eba\u548c\u76f8\u673a\u89c6\u89d2\u7684\u6f14\u793a\u6765\u589e\u5f3a\u673a\u5668\u4eba\u6570\u636e\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u7269\u7406\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u901a\u8fc7\u5728\u673a\u5668\u4eba\u548c\u89c6\u70b9\u589e\u5f3a\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0cRoVi-Aug \u53ef\u4ee5\u5728\u5177\u6709\u663e\u8457\u4e0d\u540c\u76f8\u673a\u89d2\u5ea6\u7684\u672a\u77e5\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u90e8\u7f72\u3002\u4e0e Mirage \u7b49\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u7b97\u6cd5\u76f8\u6bd4\uff0cRoVi-Aug \u5728\u6d4b\u8bd5\u65f6\u4e0d\u9700\u8981\u989d\u5916\u7684\u5904\u7406\uff0c\u4e0d\u5047\u8bbe\u5df2\u77e5\u76f8\u673a\u89d2\u5ea6\uff0c\u5e76\u4e14\u5141\u8bb8\u7b56\u7565\u5fae\u8c03\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5728\u539f\u59cb\u673a\u5668\u4eba\u6570\u636e\u96c6\u548c\u589e\u5f3a\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\uff0cRoVi-Aug \u53ef\u4ee5\u5b66\u4e60\u591a\u673a\u5668\u4eba\u548c\u591a\u4efb\u52a1\u7b56\u7565\uff0c\u4ece\u800c\u5b9e\u73b0\u673a\u5668\u4eba\u548c\u6280\u80fd\u4e4b\u95f4\u66f4\u6709\u6548\u7684\u8fc1\u79fb\uff0c\u5e76\u5c06\u6210\u529f\u7387\u63d0\u9ad8\u9ad8\u8fbe 30%\u3002||\n", "2409.05798": "|**2024-09-09**|[Enhancing Preference-based Linear Bandits via Human Response Time](http://arxiv.org/abs/2409.05798)|null|Binary human choice feedback is widely used in interactive preference learning for its simplicity, but it provides limited information about preference strength. To overcome this limitation, we leverage human response times, which inversely correlate with preference strength, as complementary information. Our work integrates the EZ-diffusion model, which jointly models human choices and response times, into preference-based linear bandits. We introduce a computationally efficient utility estimator that reformulates the utility estimation problem using both choices and response times as a linear regression problem. Theoretical and empirical comparisons with traditional choice-only estimators reveal that for queries with strong preferences (\"easy\" queries), choices alone provide limited information, while response times offer valuable complementary information about preference strength. As a result, incorporating response times makes easy queries more useful. We demonstrate this advantage in the fixed-budget best-arm identification problem, with simulations based on three real-world datasets, consistently showing accelerated learning when response times are incorporated.|\n", "2409.05790": "|**2024-09-09**|[Predicting Critical Heat Flux with Uncertainty Quantification and Domain Generalization Using Conditional Variational Autoencoders and Deep Neural Networks](http://arxiv.org/abs/2409.05790)|null|Deep generative models (DGMs) have proven to be powerful in generating realistic data samples. Their capability to learn the underlying distribution of a dataset enable them to generate synthetic data samples that closely resemble the original training dataset, thus addressing the challenge of data scarcity. In this work, we investigated the capabilities of DGMs by developing a conditional variational autoencoder (CVAE) model to augment the critical heat flux (CHF) measurement data that was used to generate the 2006 Groeneveld lookup table. To determine how this approach compared to traditional methods, a fine-tuned deep neural network (DNN) regression model was created and evaluated with the same dataset. Both the CVAE and DNN models achieved small mean absolute relative errors, with the CVAE model maintaining more favorable results. To quantify the uncertainty in the model's predictions, uncertainty quantification (UQ) was performed with repeated sampling of the CVAE model and ensembling of the DNN model. Following UQ, the DNN ensemble notably improved performance when compared to the baseline DNN model, while the CVAE model achieved similar results to its non-UQ results. The CVAE model was shown to have significantly less variability and a higher confidence after assessment of the prediction-wise relative standard deviations. Evaluating domain generalization, both models achieved small mean error values when predicting both inside and outside the training domain, with predictions outside the training domain showing slightly larger errors. Overall, the CVAE model was comparable to the DNN regression model in predicting CHF values but with better uncertainty behavior.|\n", "2409.05784": "|**2024-09-09**|[Vector Quantized Diffusion Model Based Speech Bandwidth Extension](http://arxiv.org/abs/2409.05784)|null|\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668 (NAC) \u7684\u6700\u65b0\u8fdb\u5c55\u4e3a\u97f3\u9891\u4fe1\u53f7\u5904\u7406\u89e3\u9501\u4e86\u65b0\u7684\u6f5c\u529b\u3002\u8d8a\u6765\u8d8a\u591a\u7684\u7814\u7a76\u63a2\u7d22\u5229\u7528 NAC \u7684\u6f5c\u5728\u7279\u5f81\u6765\u5b8c\u6210\u5404\u79cd\u8bed\u97f3\u4fe1\u53f7\u5904\u7406\u4efb\u52a1\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u7b2c\u4e00\u79cd\u5229\u7528\u4ece NAC \u83b7\u5f97\u7684\u79bb\u6563\u7279\u5f81\u8fdb\u884c\u8bed\u97f3\u5e26\u5bbd\u6269\u5c55 (BWE) \u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u6062\u590d\u9ad8\u5ea6\u538b\u7f29\u7684\u79bb\u6563\u6807\u8bb0\u4e2d\u7684\u9ad8\u9891\u7ec6\u8282\uff0c\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u8bed\u97f3\u7684\u6e05\u6670\u5ea6\u548c\u81ea\u7136\u5ea6\u3002\u6240\u63d0\u51fa\u7684\u6846\u67b6\u57fa\u4e8e\u77e2\u91cf\u91cf\u5316\u6269\u6563\uff0c\u7ed3\u5408\u4e86\u5148\u8fdb NAC\u3001\u6269\u6563\u6a21\u578b\u548c Mamba-2 \u7684\u4f18\u52bf\uff0c\u4ee5\u91cd\u5efa\u9ad8\u9891\u8bed\u97f3\u6210\u5206\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5bf9\u6570\u8c31\u8ddd\u79bb\u548c ViSQOL \u65b9\u9762\u5747\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u663e\u7740\u63d0\u9ad8\u4e86\u8bed\u97f3\u8d28\u91cf\u3002|\n", "2409.05730": "|**2024-09-09**|[AS-Speech: Adaptive Style For Speech Synthesis](http://arxiv.org/abs/2409.05730)|null|\u8fd1\u5e74\u6765\uff0c\u6587\u672c\u5230\u8bed\u97f3\uff08TTS\uff09\u5408\u6210\u6280\u672f\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u80fd\u591f\u5728\u5e38\u89c1\u573a\u666f\u4e0b\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u3002\u5728\u672a\u77e5\u60c5\u51b5\u4e0b\uff0c\u81ea\u9002\u5e94TTS\u9700\u8981\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u6765\u9002\u5e94\u8bf4\u8bdd\u4eba\u7684\u98ce\u683c\u7279\u5f81\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u53ea\u80fd\u5206\u522b\u63d0\u53d6\u548c\u6574\u5408\u7c97\u7c92\u5ea6\u7684\u97f3\u8272\u6216\u6df7\u5408\u7684\u97f5\u5f8b\u5c5e\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86AS-Speech\uff0c\u4e00\u79cd\u5c06\u8bf4\u8bdd\u4eba\u97f3\u8272\u7279\u5f81\u548c\u97f5\u5f8b\u5c5e\u6027\u6574\u5408\u5230\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u4e2d\u7684\u81ea\u9002\u5e94\u98ce\u683c\u65b9\u6cd5\uff0c\u7528\u4e8e\u6587\u672c\u5230\u8bed\u97f3\u5408\u6210\u3002\u5177\u4f53\u6765\u8bf4\uff0cAS-Speech\u53ef\u4ee5\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u57fa\u4e8e\u6587\u672c\u7684\u97f3\u8272\u7279\u5f81\u548c\u5168\u5c40\u97f5\u5f8b\u4fe1\u606f\u51c6\u786e\u5730\u6a21\u62df\u98ce\u683c\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9ad8\u4fdd\u771f\u8bed\u97f3\u5408\u6210\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4e00\u7cfb\u5217\u81ea\u9002\u5e94TTS\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u751f\u6210\u7684\u8bed\u97f3\u5728\u97f3\u8272\u548c\u97f5\u5f8b\u65b9\u9762\u5177\u6709\u66f4\u9ad8\u7684\u81ea\u7136\u5ea6\u548c\u76f8\u4f3c\u6027\u3002|\n", "2409.05701": "|**2024-09-09**|[pFedGPA: Diffusion-based Generative Parameter Aggregation for Personalized Federated Learning](http://arxiv.org/abs/2409.05701)|null|\u8054\u90a6\u5b66\u4e60 (FL) \u662f\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6570\u636e\u4fdd\u7559\u5728\u672c\u5730\uff0c\u53ea\u6709\u6a21\u578b\u53c2\u6570\u5728\u5ba2\u6237\u7aef\u548c\u4e2d\u5fc3\u670d\u52a1\u5668\u4e4b\u95f4\u5171\u4eab\u3002\u4f20\u7edf\u7684\u8054\u90a6\u5e73\u5747 (FedAvg) \u7b49\u65b9\u6cd5\u5bf9\u8fd9\u4e9b\u901a\u5e38\u5728\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e0a\u8bad\u7ec3\u7684\u53c2\u6570\u8fdb\u884c\u7ebf\u6027\u805a\u5408\uff0c\u8fd9\u53ef\u80fd\u5ffd\u7565\u4e86\u53c2\u6570\u7a7a\u95f4\u590d\u6742\u3001\u9ad8\u7ef4\u7684\u6027\u8d28\uff0c\u5bfc\u81f4\u805a\u5408\u6a21\u578b\u7684\u6027\u80fd\u4e0b\u964d\u3002\u867d\u7136\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u7f13\u89e3\u5f02\u6784\u6570\u636e\u95ee\u9898\uff0c\u4f46\u7ebf\u6027\u805a\u5408\u7684\u5c40\u9650\u6027\u4ecd\u7136\u6ca1\u6709\u89e3\u51b3\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u751f\u6210\u53c2\u6570\u805a\u5408\u6846\u67b6\uff0c\u5373 pFedGPA\u3002\u5728\u8fd9\u4e2a\u6846\u67b6\u4e2d\uff0c\u6211\u4eec\u5728\u670d\u52a1\u5668\u4e0a\u90e8\u7f72\u4e86\u4e00\u4e2a\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u6574\u5408\u4e0d\u540c\u7684\u53c2\u6570\u5206\u5e03\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u53cd\u6f14\u65b9\u6cd5\uff0c\u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u6709\u6548\u5730\u751f\u6210\u4e00\u7ec4\u4e2a\u6027\u5316\u53c2\u6570\u3002\u8fd9\u79cd\u53cd\u6f14\u65b9\u6cd5\u5c06\u4e0a\u4f20\u7684\u53c2\u6570\u8f6c\u6362\u4e3a\u4e00\u4e2a\u6f5c\u5728\u4ee3\u7801\uff0c\u7136\u540e\u901a\u8fc7\u53bb\u566a\u91c7\u6837\u8fdb\u884c\u805a\u5408\uff0c\u751f\u6210\u6700\u7ec8\u7684\u4e2a\u6027\u5316\u53c2\u6570\u3002\u901a\u8fc7\u4f7f\u7528\u9ad8\u5bb9\u91cf\u6269\u6563\u6a21\u578b\u5bf9\u5ba2\u6237\u7aef\u6a21\u578b\u53c2\u6570\u5bf9\u5176\u7279\u5b9a\u6570\u636e\u5206\u5e03\u7684\u4f9d\u8d56\u6027\u8fdb\u884c\u7f16\u7801\uff0cpFedGPA \u53ef\u4ee5\u6709\u6548\u5730\u5c06\u6240\u6709\u5ba2\u6237\u7aef\u6a21\u578b\u53c2\u6570\u7684\u603b\u4f53\u5206\u5e03\u7684\u590d\u6742\u6027\u4e0e\u6bcf\u4e2a\u5ba2\u6237\u7aef\u53c2\u6570\u5206\u5e03\u7684\u590d\u6742\u6027\u89e3\u8026\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\u5730\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u8d85\u8fc7\u4e86\u57fa\u7ebf\u65b9\u6cd5\u3002|\n", "2409.05668": "|**2024-09-09**|[Unlearning or Concealment? A Critical Analysis and Evaluation Metrics for Unlearning in Diffusion Models](http://arxiv.org/abs/2409.05668)|null|\u8fd1\u671f\u7684\u7814\u7a76\u5df2\u7ecf\u770b\u5230\u4eba\u4eec\u5bf9\u6269\u6563\u6a21\u578b\u4e2d\u6982\u5ff5\u53bb\u9664\u548c\u76ee\u6807\u9057\u5fd8\u65b9\u6cd5\u7684\u6d53\u539a\u5174\u8da3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5bf9\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u9057\u5fd8\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7684\u767d\u76d2\u5206\u6790\uff0c\u4ee5\u63ed\u793a\u5176\u5b58\u5728\u7684\u91cd\u5927\u6f0f\u6d1e\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u73b0\u6709\u65b9\u6cd5\u4e2d\u7528\u4e8e\u9057\u5fd8\u7684\u76ee\u6807\u51fd\u6570\u5bfc\u81f4\u4e86\u8981\u9057\u5fd8\u7684\u76ee\u6807\u6982\u5ff5\u4e0e\u76f8\u5e94\u63d0\u793a\u4e4b\u95f4\u7684\u89e3\u8026\u3002\u8fd9\u662f\u4e00\u79cd\u9690\u853d\u884c\u4e3a\uff0c\u800c\u4e0d\u662f\u771f\u6b63\u7684\u9057\u5fd8\uff0c\u800c\u771f\u6b63\u7684\u9057\u5fd8\u624d\u662f\u6700\u521d\u7684\u76ee\u6807\u3002\u5f53\u524d\u65b9\u6cd5\u7684\u65e0\u6548\u6027\u4e3b\u8981\u6e90\u4e8e\u5b83\u4eec\u53ea\u5173\u6ce8\u964d\u4f4e\u7279\u5b9a\u63d0\u793a\u96c6\u7684\u751f\u6210\u6982\u7387\uff0c\u800c\u5ffd\u7565\u4e86\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u7684\u4e2d\u95f4\u5f15\u5bfc\u7684\u591a\u79cd\u5f62\u5f0f\u3002\u672c\u6587\u5bf9\u56db\u79cd\u5e38\u7528\u7684\u6269\u6563\u6a21\u578b\u9057\u5fd8\u6280\u672f\u8fdb\u884c\u4e86\u4e25\u683c\u7684\u7406\u8bba\u548c\u5b9e\u8bc1\u68c0\u9a8c\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u4e2a\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff1a\u6982\u5ff5\u68c0\u7d22\u5206\u6570\uff08CRS\uff09\u548c\u6982\u5ff5\u7f6e\u4fe1\u5ea6\u5206\u6570\uff08CCS\uff09\u3002\u8fd9\u4e9b\u6307\u6807\u57fa\u4e8e\u4e00\u4e2a\u6210\u529f\u7684\u5bf9\u6297\u653b\u51fb\u8bbe\u7f6e\uff0c\u53ef\u4ee5\u4ece\u9057\u5fd8\u7684\u6269\u6563\u6a21\u578b\u4e2d\u6062\u590d\u88ab\u9057\u5fd8\u7684\u6982\u5ff5\u3002CRS \u8861\u91cf\u7684\u662f\u9057\u5fd8\u540e\u7684\u9057\u5fd8\u6a21\u578b\u548c\u5b8c\u5168\u8bad\u7ec3\u6a21\u578b\u7684\u6f5c\u5728\u8868\u793a\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u5b83\u53cd\u6620\u4e86\u968f\u7740\u5f15\u5bfc\u91cf\u589e\u52a0\uff0c\u88ab\u9057\u5fd8\u6982\u5ff5\u7684\u68c0\u7d22\u7a0b\u5ea6\u3002CCS \u91cf\u5316\u4e86\u6a21\u578b\u5c06\u76ee\u6807\u6982\u5ff5\u5206\u914d\u7ed9\u88ab\u64cd\u7eb5\u6570\u636e\u7684\u7f6e\u4fe1\u5ea6\u3002\u5b83\u53cd\u6620\u4e86\u968f\u7740\u5f15\u5bfc\u91cf\u589e\u52a0\uff0c\u672a\u9057\u5fd8\u6a21\u578b\u7684\u751f\u6210\u7ed3\u679c\u4e0e\u539f\u59cb\u9886\u57df\u77e5\u8bc6\u4e00\u81f4\u7684\u6982\u7387\u3002\u6211\u4eec\u4f7f\u7528\u63d0\u51fa\u7684\u9488\u5bf9\u6269\u6563\u6a21\u578b\u7684\u4e25\u683c\u6307\u6807\u5bf9\u73b0\u6709\u7684\u9057\u5fd8\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u771f\u6b63\u9057\u5fd8\u6982\u5ff5\u65b9\u9762\u7684\u91cd\u5927\u7f3a\u9677\u3002\u6e90\u4ee3\u7801\uff1ahttps://respailab.github.io/unlearning-or-concealment|\n", "2409.05622": "|**2024-09-09**|[Forward KL Regularized Preference Optimization for Aligning Diffusion Policies](http://arxiv.org/abs/2409.05622)|null|\u6269\u6563\u6a21\u578b\u901a\u8fc7\u5728\u7b56\u7565\u5b66\u4e60\u4e2d\u5229\u7528\u9ad8\u5ea6\u8868\u8fbe\u7684\u6a21\u578b\u80fd\u529b\uff0c\u5728\u5e8f\u5217\u51b3\u7b56\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\u3002\u5b66\u4e60\u6269\u6563\u7b56\u7565\u7684\u4e00\u4e2a\u6838\u5fc3\u95ee\u9898\u662f\u5982\u4f55\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u4f7f\u7b56\u7565\u8f93\u51fa\u4e0e\u4eba\u7c7b\u610f\u56fe\u4fdd\u6301\u4e00\u81f4\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u5148\u524d\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u56de\u62a5\u6761\u4ef6\u7b56\u7565\u751f\u6210\u6216\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u7b56\u7565\u4f18\u5316\uff0c\u4f46\u5b83\u4eec\u90fd\u4f9d\u8d56\u4e8e\u9884\u5148\u5b9a\u4e49\u7684\u5956\u52b1\u51fd\u6570\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u5373\u7528\u4e8e\u5bf9\u9f50\u6269\u6563\u7b56\u7565\u7684\u524d\u5411 KL \u6b63\u5219\u5316\u504f\u597d\u4f18\u5316\uff0c\u4ee5\u76f4\u63a5\u5c06\u6269\u6563\u7b56\u7565\u4e0e\u504f\u597d\u5bf9\u9f50\u3002\u6211\u4eec\u9996\u5148\u4ece\u79bb\u7ebf\u6570\u636e\u96c6\u4e2d\u8bad\u7ec3\u4e00\u4e2a\u4e0d\u8003\u8651\u504f\u597d\u7684\u6269\u6563\u7b56\u7565\uff0c\u7136\u540e\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5c06\u8be5\u7b56\u7565\u4e0e\u504f\u597d\u6570\u636e\u5bf9\u9f50\u3002\u5728\u5bf9\u9f50\u9636\u6bb5\uff0c\u6211\u4eec\u5728\u6269\u6563\u7b56\u7565\u4e2d\u5236\u5b9a\u4e86\u76f4\u63a5\u504f\u597d\u5b66\u4e60\uff0c\u5176\u4e2d\u5728\u524d\u5411\u504f\u597d\u4f18\u5316\u4e2d\u91c7\u7528\u4e86 KL \u6b63\u5219\u5316\uff0c\u4ee5\u907f\u514d\u751f\u6210\u5206\u5e03\u5916\u52a8\u4f5c\u3002\u6211\u4eec\u5bf9 MetaWorld \u64cd\u4f5c\u548c D4RL \u4efb\u52a1\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u504f\u597d\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\u3002|\n", "2409.05585": "|**2024-09-09**|[Latent 3D Brain MRI Counterfactual](http://arxiv.org/abs/2409.05585)|null|\u7ed3\u6784\u6027\u8111\u90e8MRI\u7814\u7a76\u4e2d\u7684\u6837\u672c\u6570\u91cf\u901a\u5e38\u8fc7\u5c0f\uff0c\u65e0\u6cd5\u5145\u5206\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u751f\u6210\u6a21\u578b\u901a\u8fc7\u6709\u6548\u5b66\u4e60\u6570\u636e\u5206\u5e03\u548c\u751f\u6210\u9ad8\u4fdd\u771fMRI\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5e26\u6765\u4e86\u5e0c\u671b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u96be\u4ee5\u751f\u6210\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u4e4b\u5916\u7684\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u3002\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u4e00\u79cd\u65b9\u6cd5\u662f\u4f7f\u7528\u9488\u5bf93D\u4f53\u79ef\u53cd\u4e8b\u5b9e\u5f00\u53d1\u7684\u56e0\u679c\u6a21\u578b\u3002\u7136\u800c\uff0c\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u51c6\u786e\u5efa\u6a21\u56e0\u679c\u5173\u7cfb\u662f\u4e00\u9879\u6311\u6218\uff0c\u56e0\u6b64\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u751f\u6210\u8d28\u91cf\u8f83\u4f4e\u76843D\u8111\u90e8MRI\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u5185\u6784\u5efa\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\u3002\u5728\u7b2c\u4e00\u9636\u6bb5\uff0c\u6211\u4eec\u91c7\u7528VQ-VAE\u5b66\u4e60MRI\u4f53\u79ef\u7684\u7d27\u51d1\u5d4c\u5165\u3002\u968f\u540e\uff0c\u6211\u4eec\u5c06\u56e0\u679c\u6a21\u578b\u6574\u5408\u5230\u8fd9\u4e2a\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u5e76\u4f7f\u7528\u5c01\u95ed\u5f62\u5f0f\u7684\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\uff08GLM\uff09\u6267\u884c\u4e09\u6b65\u53cd\u4e8b\u5b9e\u7a0b\u5e8f\u3002\u6211\u4eec\u5bf9\u771f\u5b9e\u4e16\u754c\u7684\u9ad8\u5206\u8fa8\u7387MRI\u6570\u636e\uff081mm\uff09\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u76843D MRI\u53cd\u4e8b\u5b9e\u3002|\n", "2409.05583": "|**2024-09-09**|[Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation](http://arxiv.org/abs/2409.05583)|**[link](https://github.com/gmuraleekrishna/sas)**|\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u65e8\u5728\u5f00\u53d1\u80fd\u591f\u7406\u89e3\u548c\u6267\u884c\u4eba\u7c7b\u8bed\u8a00\u6307\u4ee4\u5e76\u4ee5\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u4ea4\u6d41\u7684\u673a\u5668\u4eba\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u751f\u6210\u9ad8\u5ea6\u8be6\u7ec6\u7684\u5bfc\u822a\u6307\u4ee4\u4ee5\u4f9b\u5177\u8eab\u673a\u5668\u4eba\u9075\u5faa\u7684\u4efb\u52a1\u3002\u5c3d\u7ba1\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u4ece\u56fe\u50cf\u5e8f\u5217\u751f\u6210\u9010\u6b65\u6307\u4ee4\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u751f\u6210\u7684\u6307\u4ee4\u5728\u6307\u79f0\u7269\u4f53\u548c\u5730\u6807\u65b9\u9762\u7f3a\u4e4f\u591a\u6837\u6027\u3002\u73b0\u6709\u7684\u8bf4\u8bdd\u8005\u6a21\u578b\u5b66\u4e60\u4e86\u4e00\u4e9b\u7b56\u7565\u6765\u89c4\u907f\u8bc4\u4f30\u6307\u6807\uff0c\u5373\u4f7f\u5bf9\u4e8e\u4f4e\u8d28\u91cf\u7684\u53e5\u5b50\u4e5f\u80fd\u83b7\u5f97\u66f4\u9ad8\u7684\u5206\u6570\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SAS\uff08\u7a7a\u95f4\u611f\u77e5\u8bf4\u8bdd\u8005\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u6307\u4ee4\u751f\u6210\u5668\u6216\u201c\u8bf4\u8bdd\u8005\u201d\u6a21\u578b\uff0c\u5b83\u5229\u7528\u73af\u5883\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u77e5\u8bc6\u6765\u751f\u6210\u66f4\u4e30\u5bcc\u7684\u6307\u4ee4\u3002\u4e3a\u4e86\u8fdb\u884c\u8bad\u7ec3\uff0c\u6211\u4eec\u5728\u5bf9\u6297\u6027\u8bbe\u7f6e\u4e2d\u91c7\u7528\u4e86\u5956\u52b1\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u907f\u514d\u8bed\u8a00\u8bc4\u4f30\u6307\u6807\u5f15\u5165\u7684\u7cfb\u7edf\u6027\u504f\u5dee\u3002\u6839\u636e\u7ecf\u9a8c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u6307\u4ee4\u751f\u6210\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u6807\u51c6\u6307\u6807\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u5f97\uff1ahttps://github.com/gmuraleekrishna/SAS\u3002|\n", "2409.05490": "|**2024-09-09**|[A Taxonomy of Miscompressions: Preparing Image Forensics for Neural Compression](http://arxiv.org/abs/2409.05490)|null|\u795e\u7ecf\u538b\u7f29\u6709\u53ef\u80fd\u5f7b\u5e95\u6539\u53d8\u6709\u635f\u56fe\u50cf\u538b\u7f29\u6280\u672f\u3002\u57fa\u4e8e\u751f\u6210\u6a21\u578b\uff0c\u6700\u8fd1\u7684\u65b9\u6848\u5728\u9ad8\u611f\u77e5\u8d28\u91cf\u4e0b\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u538b\u7f29\u7387\uff0c\u4f46\u727a\u7272\u4e86\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002\u89e3\u538b\u7f29\u56fe\u50cf\u7684\u7ec6\u8282\u53ef\u80fd\u770b\u8d77\u6765\u5728\u89c6\u89c9\u4e0a\u662f\u5b8c\u7f8e\u7684\uff0c\u4f46\u5728\u8bed\u4e49\u4e0a\u4e0e\u539f\u59cb\u56fe\u50cf\u4e0d\u540c\uff0c\u8fd9\u4f7f\u5f97\u538b\u7f29\u9519\u8bef\u96be\u4ee5\u6216\u4e0d\u53ef\u80fd\u88ab\u68c0\u6d4b\u5230\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u8fd9\u4e2a\u95ee\u9898\u7684\u7a7a\u95f4\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6682\u5b9a\u7684\u9519\u8bef\u538b\u7f29\u5206\u7c7b\u6cd5\u3002\u5b83\u5b9a\u4e49\u4e86\u4e09\u79cd\u7c7b\u578b\u7684\u201c\u53d1\u751f\u4e86\u4ec0\u4e48\u201d\uff0c\u5e76\u6709\u4e00\u4e2a\u4e8c\u8fdb\u5236\u7684\u201c\u9ad8\u5f71\u54cd\u201d\u6807\u5fd7\uff0c\u8868\u793a\u6539\u53d8\u7b26\u53f7\u7684\u9519\u8bef\u538b\u7f29\u3002\u6211\u4eec\u8ba8\u8bba\u4e86\u8be5\u5206\u7c7b\u6cd5\u5982\u4f55\u4fc3\u8fdb\u98ce\u9669\u6c9f\u901a\u548c\u7f13\u89e3\u63aa\u65bd\u7684\u7814\u7a76\u3002|\n"}, "LLM": {"2409.01909": "|**2024-09-03**|[LUK: Empowering Log Understanding with Expert Knowledge from Large Language Models](http://arxiv.org/abs/2409.01909)|**[link](https://github.com/LeaperOvO/LUK)**|\u65e5\u5fd7\u5728\u4e3a\u7cfb\u7edf\u76d1\u63a7\u548c\u6545\u969c\u6392\u9664\u63d0\u4f9b\u5fc5\u8981\u4fe1\u606f\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u8fd1\u5e74\u6765\uff0c\u968f\u7740\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u9886\u57df\u7684\u6210\u529f\uff0c\u8f83\u5c0f\u7684PLM\uff08\u5982BERT\uff09\u548cLLM\uff08\u5982ChatGPT\uff09\u5df2\u6210\u4e3a\u5f53\u524d\u65e5\u5fd7\u5206\u6790\u7684\u4e3b\u6d41\u65b9\u6cd5\u3002\u867d\u7136LLM\u62e5\u6709\u4e30\u5bcc\u7684\u77e5\u8bc6\uff0c\u4f46\u5176\u9ad8\u6602\u7684\u8ba1\u7b97\u6210\u672c\u548c\u4e0d\u7a33\u5b9a\u7684\u6027\u80fd\u4f7f\u5f97LLM\u65e0\u6cd5\u76f4\u63a5\u7528\u4e8e\u5206\u6790\u65e5\u5fd7\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u8f83\u5c0f\u7684PLM\u5373\u4f7f\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u53ef\u4ee5\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u5fae\u8c03\uff0c\u56e0\u6b64\u66f4\u5177\u5b9e\u7528\u6027\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7f3a\u4e4f\u4e13\u4e1a\u77e5\u8bc6\uff0c\u8fd9\u4e9b\u8f83\u5c0f\u7684PLM\u5728\u5168\u9762\u7406\u89e3\u65e5\u5fd7\u65b9\u9762\u9762\u4e34\u7740\u6311\u6218\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u5229\u7528LLM\u4e2d\u5d4c\u5165\u7684\u77e5\u8bc6\u6765\u7406\u89e3\u65e5\u5fd7\uff0c\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u77e5\u8bc6\u589e\u5f3a\u6846\u67b6\uff0c\u79f0\u4e3aLUK\uff0c\u5b83\u4eceLLM\u4e2d\u83b7\u53d6\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4ee5\u589e\u5f3a\u5c0f\u578bPLM\u4e0a\u7684\u65e5\u5fd7\u7406\u89e3\u80fd\u529b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u4e13\u5bb6\u534f\u4f5c\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7531\u4e0d\u540c\u7684\u89d2\u8272\u7ec4\u6210\uff0c\u7528\u4e8e\u83b7\u53d6\u4e13\u5bb6\u77e5\u8bc6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u7684\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u4ee5\u5229\u7528\u4e13\u5bb6\u77e5\u8bc6\u589e\u5f3a\u65e5\u5fd7\u9884\u8bad\u7ec3\u3002LUK\u5728\u4e0d\u540c\u7684\u65e5\u5fd7\u5206\u6790\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u5229\u7528LLM\u4e2d\u7684\u4e13\u5bb6\u77e5\u8bc6\u6765\u7406\u89e3\u65e5\u5fd7\u3002|\n", "2409.00702": "|**2024-09-04**|[MARS: Matching Attribute-aware Representations for Text-based Sequential Recommendation](http://arxiv.org/abs/2409.00702)|null|\u5e8f\u5217\u63a8\u8350\u65e8\u5728\u6839\u636e\u7528\u6237\u7684\u987a\u5e8f\u4ea4\u4e92\u5386\u53f2\u9884\u6d4b\u7528\u6237\u53ef\u80fd\u559c\u6b22\u7684\u4e0b\u4e00\u4e2a\u9879\u76ee\u3002\u6700\u8fd1\uff0c\u57fa\u4e8e\u6587\u672c\u7684\u5e8f\u5217\u63a8\u8350\u5df2\u6210\u4e3a\u4e00\u79cd\u5f88\u6709\u524d\u666f\u7684\u8303\u4f8b\uff0c\u5b83\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u6765\u5229\u7528\u6587\u672c\u9879\u76ee\u7279\u5f81\u6765\u63d0\u9ad8\u6027\u80fd\u5e76\u5c06\u77e5\u8bc6\u8fc1\u79fb\u5230\u672a\u89c1\u6570\u636e\u96c6\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u6587\u672c\u7684\u63a8\u8350\u6a21\u578b\u4ecd\u7136\u9762\u4e34\u7740\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\uff08i\uff09\u7528\u591a\u4e2a\u5c5e\u6027\u8868\u793a\u7528\u6237\u548c\u9879\u76ee\uff0c\u4ee5\u53ca\uff08ii\uff09\u5c06\u9879\u76ee\u4e0e\u590d\u6742\u7684 \u7528\u6237\u5174\u8da3\u76f8\u5339\u914d\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u578b\uff0c\u5373\u7528\u4e8e\u57fa\u4e8e\u6587\u672c\u7684\u5e8f\u5217\u63a8\u8350\u7684\u5339\u914d\u5c5e\u6027\u611f\u77e5\u8868\u793a (MARS)\u3002MARS \u901a\u8fc7\u5c5e\u6027\u611f\u77e5\u6587\u672c\u7f16\u7801\u63d0\u53d6\u8be6\u7ec6\u7684\u7528\u6237\u548c\u9879\u76ee\u8868\u793a\uff0c\u5e76\u4f7f\u7528\u591a\u4e2a\u5c5e\u6027\u611f\u77e5\u8868\u793a\u6355\u83b7\u4e0d\u540c\u7684\u7528\u6237\u610f\u56fe\u3002\u7136\u540e\uff0c\u5b83\u901a\u8fc7\u5c5e\u6027\u65b9\u9762\u7684\u4ea4\u4e92\u5339\u914d\u8ba1\u7b97\u7528\u6237-\u9879\u76ee\u5206\u6570\uff0c\u6709\u6548\u5730\u6355\u83b7\u5c5e\u6027\u7ea7\u522b\u7684\u7528\u6237\u504f\u597d\u3002\u6211\u4eec\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMARS \u660e\u663e\u4f18\u4e8e\u73b0\u6709\u7684\u5e8f\u5217\u6a21\u578b\uff0c\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a Recall@10 \u548c NDCG@10 \u5206\u522b\u63d0\u9ad8\u4e86 24.43% \u548c 29.26%\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/junieberry/MARS \u83b7\u53d6\u3002|\n", "2409.00323": "|**2024-08-31**|[From Prediction to Application: Language Model-based Code Knowledge Tracing with Domain Adaptive Pre-Training and Automatic Feedback System with Pedagogical Prompting for Comprehensive Programming Education](http://arxiv.org/abs/2409.00323)|null|\u77e5\u8bc6\u8ffd\u8e2a (KT)\u662f\u5728\u7ebf\u5b66\u4e60\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u548c\u8de8\u9886\u57df\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u77e5\u8bc6\u8ffd\u8e2a (CodeLKT)\uff0c\u8fd9\u662f\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u8ffd\u8e2a (LKT) \u5728\u7f16\u7a0b\u6559\u80b2\u4e2d\u7684\u4e00\u79cd\u521b\u65b0\u5e94\u7528\u3002CodeLKT \u5229\u7528\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u6765\u5904\u7406\u5b66\u4e60\u6570\u636e\uff0c\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709 KT \u548c\u4ee3\u7801 KT \u6a21\u578b\u7684\u6027\u80fd\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3 (DAPT) \u548c\u4efb\u52a1\u81ea\u9002\u5e94\u9884\u8bad\u7ec3 (TAPT)\uff0c\u5c55\u793a\u4e86\u5728\u7f16\u7801\u9886\u57df\u7684\u589e\u5f3a\u6027\u80fd\uff0c\u5e76\u7814\u7a76\u4e86\u6570\u5b66\u548c\u7f16\u7801\u4e4b\u95f4\u7684\u8de8\u9886\u57df\u8fc1\u79fb\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u4e0a\u53ef\u884c\u7684\u96c6\u6210\u7cfb\u7edf\uff0c\u5c06 CodeLKT \u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u4ee5\u751f\u6210\u4e2a\u6027\u5316\u7684\u3001\u6df1\u5165\u7684\u53cd\u9988\uff0c\u4ee5\u652f\u6301\u5b66\u751f\u7684\u7f16\u7a0b\u5b66\u4e60\u3002\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u6269\u5c55\u77e5\u8bc6\u5e93\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u53cd\u9988\u4e3a\u7f16\u7a0b\u6559\u80b2\u63d0\u4f9b\u5b9e\u9645\u610f\u4e49\uff0c\u4ece\u800c\u63a8\u8fdb\u4e86\u4ee3\u7801\u77e5\u8bc6\u8ffd\u8e2a\u9886\u57df\u7684\u53d1\u5c55\u3002|\n", "2408.17354": "|**2024-08-30**|[Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage](http://arxiv.org/abs/2408.17354)|null|\u5728\u79c1\u6709\u6570\u636e\u4e0a\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0b\u6e38\u5e94\u7528\u4e2d\u5b58\u5728\u6cc4\u9732\u654f\u611f\u4fe1\u606f\u7684\u91cd\u5927\u9690\u79c1\u98ce\u9669\u3002\u4e00\u4e9b\u6d41\u884c\u7684\u793e\u533a\u5e73\u53f0\u73b0\u5728\u63d0\u4f9b\u5404\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4fbf\u6377\u5206\u53d1\uff0c\u5141\u8bb8\u4efb\u4f55\u4eba\u53d1\u5e03\u800c\u65e0\u9700\u4e25\u683c\u9a8c\u8bc1\u3002\u8fd9\u79cd\u60c5\u51b5\u9020\u6210\u4e86\u4e00\u79cd\u9690\u79c1\u5a01\u80c1\uff0c\u56e0\u4e3a\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u88ab\u6545\u610f\u8bbe\u8ba1\u6765\u635f\u5bb3\u5fae\u8c03\u6570\u636e\u96c6\u7684\u9690\u79c1\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u6295\u6bd2\u6280\u672f\uff0c\u8be5\u6280\u672f\u4f7f\u7528\u6a21\u578b\u9057\u5fd8\u4f5c\u4e3a\u653b\u51fb\u5de5\u5177\u3002\u8fd9\u79cd\u65b9\u6cd5\u64cd\u7eb5\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u589e\u52a0\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u79c1\u6709\u6570\u636e\u7684\u6cc4\u6f0f\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u589e\u5f3a\u4e86\u6210\u5458\u63a8\u7406\u548c\u6570\u636e\u63d0\u53d6\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u6a21\u578b\u6548\u7528\u3002\u8de8\u4e0d\u540c\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u5fae\u8c03\u8bbe\u7f6e\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u653b\u51fb\u660e\u663e\u4f18\u4e8e\u57fa\u7ebf\u6027\u80fd\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4ece\u672a\u7ecf\u9a8c\u8bc1\u7684\u6765\u6e90\u4e0b\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u7528\u6237\u6572\u54cd\u4e86\u8b66\u949f\uff0c\u7a81\u51fa\u4e86\u5176\u4e2d\u6d89\u53ca\u7684\u6f5c\u5728\u98ce\u9669\u3002|\n", "2408.14505": "|**2024-08-24**|[Empowering Pre-Trained Language Models for Spatio-Temporal Forecasting via Decoupling Enhanced Discrete Reprogramming](http://arxiv.org/abs/2408.14505)|null|\u65f6\u7a7a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5728\u4ea4\u901a\u4f18\u5316\u3001\u80fd\u6e90\u7ba1\u7406\u548c\u6c14\u5019\u5206\u6790\u7b49\u5404\u79cd\u73b0\u5b9e\u5e94\u7528\u4e2d\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b (PLM) \u7684\u6700\u65b0\u8fdb\u5c55\u542f\u53d1\u4e86\u4eba\u4eec\u5229\u7528\u5176\u5353\u8d8a\u7684\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5bf9\u8fd9\u4e9b\u6a21\u578b\u8fdb\u884c\u91cd\u65b0\u7f16\u7a0b\u4ee5\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7684\u8de8\u5e8f\u5217\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\u548c\u5e8f\u5217\u5185\u56fa\u6709\u9891\u7387\u5206\u91cf\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u65f6\u7a7a\u9884\u6d4b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5728\u91cd\u65b0\u7f16\u7a0b\u4e2d\u5c06\u8fde\u7eed\u65f6\u95f4\u5e8f\u5217\u7ebf\u6027\u6620\u5c04\u5230\u538b\u7f29\u7684\u5b50\u96c6\u8bcd\u6c47\u8868\u9650\u5236\u4e86 PLM \u7684\u65f6\u7a7a\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u53ef\u80fd\u5bfc\u81f4\u6f5c\u5728\u7684\u4fe1\u606f\u74f6\u9888\u3002\u4e3a\u4e86\u514b\u670d\u4e0a\u8ff0\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 \\textsc{RePST}\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e3a\u65f6\u7a7a\u9884\u6d4b\u91cf\u8eab\u5b9a\u5236\u7684 PLM \u91cd\u65b0\u7f16\u7a0b\u6846\u67b6\u3002\\textsc{RePST} \u7684\u5173\u952e\u89c1\u89e3\u662f\u5728\u9891\u57df\u4e2d\u89e3\u8026\u65f6\u7a7a\u52a8\u6001\uff0c\u4ece\u800c\u66f4\u597d\u5730\u4e0e PLM \u6587\u672c\u7a7a\u95f4\u4fdd\u6301\u4e00\u81f4\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u5728\u5085\u7acb\u53f6\u7a7a\u95f4\u4e2d\u89e3\u8026\u65f6\u7a7a\u6570\u636e\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ed3\u6784\u6269\u6563\u7b97\u5b50\u6765\u83b7\u5f97\u65f6\u95f4\u5185\u5728\u4fe1\u53f7\u548c\u7a7a\u95f4\u6269\u6563\u4fe1\u53f7\uff0c\u4f7f PLM \u66f4\u5bb9\u6613\u7406\u89e3\u548c\u9884\u6d4b\u52a8\u6001\u53d8\u5316\u3002\u4e3a\u4e86\u907f\u514d\u8bcd\u6c47\u91cf\u6709\u9650\u5e26\u6765\u7684\u4fe1\u606f\u74f6\u9888\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u79bb\u6563\u91cd\u65b0\u7f16\u7a0b\u7b56\u7565\uff0c\u4ee5\u53ef\u5fae\u5206\u7684\u65b9\u5f0f\u4ece\u6269\u5c55\u7684\u8bcd\u6c47\u7a7a\u95f4\u4e2d\u9009\u62e9\u76f8\u5173\u7684\u79bb\u6563\u6587\u672c\u4fe1\u606f\u3002\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65f6\u7a7a\u9884\u6d4b\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u3002|\n", "2408.13040": "|**2024-08-23**|[SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks](http://arxiv.org/abs/2408.13040)|null|\u63d0\u793a\u5df2\u6210\u4e3a\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b (LM) \u7684\u4e00\u79cd\u5b9e\u7528\u65b9\u6cd5\u3002\u8fd9\u79cd\u65b9\u6cd5\u5177\u6709\u51e0\u4e2a\u4f18\u70b9\u3002\u5b83\u5141\u8bb8 LM \u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u53ea\u9700\u6700\u5c11\u7684\u8bad\u7ec3\u548c\u53c2\u6570\u66f4\u65b0\uff0c\u4ece\u800c\u5728\u5b58\u50a8\u548c\u8ba1\u7b97\u65b9\u9762\u90fd\u5b9e\u73b0\u4e86\u6548\u7387\u3002\u6b64\u5916\uff0c\u63d0\u793a\u4ec5\u4fee\u6539 LM \u7684\u8f93\u5165\u5e76\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u4ee5\u7edf\u4e00\u7684\u65b9\u5f0f\u5904\u7406\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002\u8fd9\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u8bbe\u8ba1\u7279\u5b9a\u4efb\u52a1\u6a21\u578b\u7684\u9700\u6c42\u3002\u968f\u7740 LM \u670d\u52a1\u7684\u4efb\u52a1\u6570\u91cf\u7684\u589e\u52a0\uff0c\u8fd9\u4e9b\u4f18\u52bf\u53d8\u5f97\u66f4\u52a0\u660e\u663e\u3002\u5728\u63d0\u793a\u4f18\u52bf\u7684\u63a8\u52a8\u4e0b\uff0c\u6211\u4eec\u7387\u5148\u63a2\u7d22\u4e86\u5728\u8bed\u97f3\u5904\u7406\u9886\u57df\u4e2d\u63d0\u793a\u8bed\u97f3 LM \u7684\u6f5c\u529b\u3002\u6700\u8fd1\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u5173\u6ce8\u5c06\u8bed\u97f3\u8f6c\u6362\u4e3a\u79bb\u6563\u5355\u5143\u4ee5\u8fdb\u884c\u8bed\u8a00\u5efa\u6a21\u3002\u6211\u4eec\u7684\u5148\u9a71\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u4e9b\u91cf\u5316\u7684\u8bed\u97f3\u5355\u5143\u5728\u6211\u4eec\u7edf\u4e00\u7684\u63d0\u793a\u6846\u67b6\u5185\u5177\u6709\u9ad8\u5ea6\u7684\u591a\u529f\u80fd\u6027\u3002\u5b83\u4eec\u4e0d\u4ec5\u53ef\u4ee5\u4f5c\u4e3a\u7c7b\u522b\u6807\u7b7e\uff0c\u8fd8\u53ef\u4ee5\u5305\u542b\u4e30\u5bcc\u7684\u8bed\u97f3\u4fe1\u606f\uff0c\u53ef\u4ee5\u91cd\u65b0\u5408\u6210\u56de\u8bed\u97f3\u4fe1\u53f7\u4ee5\u7528\u4e8e\u8bed\u97f3\u751f\u6210\u4efb\u52a1\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5c06\u8bed\u97f3\u5904\u7406\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8bed\u97f3\u5230\u5355\u5143\u7684\u751f\u6210\u4efb\u52a1\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u8bed\u97f3\u5206\u7c7b\u3001\u5e8f\u5217\u751f\u6210\u548c\u8bed\u97f3\u751f\u6210\u7b49\u4efb\u52a1\u65e0\u7f1d\u96c6\u6210\u5230\u4e00\u4e2a\u7edf\u4e00\u7684\u63d0\u793a\u6846\u67b6\u4e2d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5177\u6709\u76f8\u4f3c\u6570\u91cf\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u7684\u5f3a\u5fae\u8c03\u65b9\u6cd5\u76f8\u6bd4\uff0c\u63d0\u793a\u65b9\u6cd5\u53ef\u4ee5\u53d6\u5f97\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u63d0\u793a\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u4e5f\u663e\u793a\u51fa\u53ef\u559c\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u968f\u7740\u5148\u8fdb\u8bed\u97f3 LM \u7684\u51fa\u73b0\uff0c\u6240\u63d0\u51fa\u7684\u63d0\u793a\u6846\u67b6\u5177\u6709\u5de8\u5927\u7684\u6f5c\u529b\u3002|\n", "2408.12779": "|**2024-08-23**|[Investigating LLM Applications in E-Commerce](http://arxiv.org/abs/2408.12779)|null|\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u51fa\u73b0\u5f7b\u5e95\u6539\u53d8\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u7535\u5b50\u5546\u52a1\u9886\u57df\u3002\u5728\u8fd9\u4e9b\u9886\u57df\u5e94\u7528\u6b64\u7c7b LLM \u4e4b\u524d\uff0c\u4e00\u4e2a\u5173\u952e\u6b65\u9aa4\u662f\u4e86\u89e3\u548c\u6bd4\u8f83\u6b64\u7c7b\u4efb\u52a1\u5728\u4e0d\u540c\u7528\u4f8b\u4e2d\u7684\u6027\u80fd\u3002\u672c\u6587\u63a2\u8ba8\u4e86 LLM \u5728\u7535\u5b50\u5546\u52a1\u9886\u57df\u4e2d\u7684\u6709\u6548\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u4f7f\u7528\u4e0d\u540c\u89c4\u6a21\u7684\u516c\u5171\u7535\u5b50\u5546\u52a1\u6570\u636e\u96c6\u5bf9\u5f00\u6e90 LLM \u6a21\u578b\u8fdb\u884c\u6307\u4ee4\u5fae\u8c03\uff0c\u5e76\u5c06\u6027\u80fd\u4e0e\u5de5\u4e1a\u5e94\u7528\u4e2d\u6d41\u884c\u7684\u4f20\u7edf\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002\u6211\u4eec\u5bf9 LLM \u548c\u4f20\u7edf\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u7535\u5b50\u5546\u52a1\u9886\u57df\u56fa\u6709\u7684\u7279\u5b9a\u4efb\u52a1\uff08\u5373\u5206\u7c7b\u3001\u751f\u6210\u3001\u6458\u8981\u548c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b (NER)\uff09\u8fdb\u884c\u4e86\u5168\u9762\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u68c0\u67e5\u4e86\u5f53\u524d\u5229\u57fa\u5de5\u4e1a\u5e94\u7528\u4e2d\u8d85\u5927\u578b LLM\uff08\u4f7f\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\uff09\u5728\u7279\u5b9a\u7535\u5b50\u5546\u52a1\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8d85\u5927\u578b LLM \u7684\u5c11\u6837\u672c\u63a8\u7406\u901a\u5e38\u4f18\u4e8e\u5fae\u8c03\u8f83\u5c0f\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u8fd9\u7a81\u51fa\u4e86\u7279\u5b9a\u4efb\u52a1\u6a21\u578b\u4f18\u5316\u7684\u91cd\u8981\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u7814\u7a76\u4e86\u4e0d\u540c\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f8b\u5982\u5355\u4efb\u52a1\u8bad\u7ec3\u3001\u6df7\u5408\u4efb\u52a1\u8bad\u7ec3\u548c LoRA \u5408\u5e76\uff08\u5728\u57df/\u4efb\u52a1\u5185\u4ee5\u53ca\u4e0d\u540c\u4efb\u52a1\u4e4b\u95f4\uff09\u3002\u901a\u8fc7\u4e25\u683c\u7684\u5b9e\u9a8c\u548c\u5206\u6790\uff0c\u672c\u6587\u4e3a\u4e86\u89e3 LLM \u5728\u63d0\u9ad8\u7535\u5b50\u5546\u52a1\u884c\u4e1a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u5728\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002|\n", "2408.12125": "|**2024-08-22**|[AutoTest: Evolutionary Code Solution Selection with Test Cases](http://arxiv.org/abs/2408.12125)|null|\u968f\u7740\u4ee3\u7801\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4ece\u591a\u4e2a\u5019\u9009\u65b9\u6848\u4e2d\u9009\u62e9\u6b63\u786e\u7684\u4ee3\u7801\u65b9\u6848\u5df2\u6210\u4e3a\u4e00\u9879\u81f3\u5173\u91cd\u8981\u7684\u4efb\u52a1\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAutoTest\u7684\u65b0\u6280\u672f\uff0c\u8be5\u6280\u672f\u5c06\u81ea\u52a8\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u4e0e\u4ee3\u7801\u65b9\u6848\u6267\u884c\u76f8\u7ed3\u5408\uff0c\u5229\u7528\u8fdb\u5316\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u9009\u62e9\u8fc7\u7a0b\u3002\u9996\u5148\uff0cAutoTest\u5229\u7528\u8bf8\u5982codegen-16B\u3001code-davinci-002\u548cincoder-6B\u7b49\u5927\u578b\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u6765\u63d0\u4f9b\u4ee3\u7801\u65b9\u6848\u53ca\u5176\u76f8\u5e94\u7684\u6d4b\u8bd5\u7528\u4f8b\u3002\u7136\u540e\uff0c\u901a\u8fc7\u6267\u884c\u4ee3\u7801\u65b9\u6848\u5e76\u8bc4\u4f30\u5176\u5728\u6d4b\u8bd5\u7528\u4f8b\u4e0a\u7684\u6027\u80fd\uff0c\u5f62\u6210\u5171\u8bc6\u96c6\u3002\u57fa\u4e8e\u8fdb\u5316\u9057\u4f20\u7b97\u6cd5\u7684\u9009\u62e9\u3001\u53d8\u5f02\u548c\u4ea4\u53c9\u673a\u5236\uff0c\u901a\u8fc7\u8c03\u6574alpha\u548cbeta\u53c2\u6570\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6392\u540d\u3002\u6700\u540e\uff0c\u9009\u62e9\u6700\u4f73\u4ee3\u7801\u65b9\u6848\u3002AutoTest\u5728HumanEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002HumanEval\u6570\u636e\u96c6\u5305\u542b164\u4e2a\u7f16\u7a0b\u95ee\u9898\uff0cAutoTest\u5728pass@1\u5206\u6570\u65b9\u9762\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e86\u7ea610%\u3002|\n", "2408.11319": "|**2024-08-24**|[SarcasmBench: Towards Evaluating Large Language Models on Sarcasm Understanding](http://arxiv.org/abs/2408.11319)|null|In the era of large language models (LLMs), the task of ``System I''~-~the fast, unconscious, and intuitive tasks, e.g., sentiment analysis, text classification, etc., have been argued to be successfully solved. However, sarcasm, as a subtle linguistic phenomenon, often employs rhetorical devices like hyperbole and figuration to convey true sentiments and intentions, involving a higher level of abstraction than sentiment analysis. There is growing concern that the argument about LLMs' success may not be fully tenable when considering sarcasm understanding. To address this question, we select eleven SOTA LLMs and eight SOTA pre-trained language models (PLMs) and present comprehensive evaluations on six widely used benchmark datasets through different prompting approaches, i.e., zero-shot input/output (IO) prompting, few-shot IO prompting, chain of thought (CoT) prompting. Our results highlight three key findings: (1) current LLMs underperform supervised PLMs based sarcasm detection baselines across six sarcasm benchmarks. This suggests that significant efforts are still required to improve LLMs' understanding of human sarcasm. (2) GPT-4 consistently and significantly outperforms other LLMs across various prompting methods, with an average improvement of 14.0\\%$\\uparrow$. Claude 3 and ChatGPT demonstrate the next best performance after GPT-4. (3) Few-shot IO prompting method outperforms the other two methods: zero-shot IO and few-shot CoT. The reason is that sarcasm detection, being a holistic, intuitive, and non-rational cognitive process, is argued not to adhere to step-by-step logical reasoning, making CoT less effective in understanding sarcasm compared to its effectiveness in mathematical reasoning tasks.||\n", "2408.10548": "|**2024-08-20**|[Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution](http://arxiv.org/abs/2408.10548)|**[link](https://github.com/lanxiang1017/language-modeling-on-tabular-data-survey)**|Tabular data, a prevalent data type across various domains, presents unique challenges due to its heterogeneous nature and complex structural relationships. Achieving high predictive performance and robustness in tabular data analysis holds significant promise for numerous applications. Influenced by recent advancements in natural language processing, particularly transformer architectures, new methods for tabular data modeling have emerged. Early techniques concentrated on pre-training transformers from scratch, often encountering scalability issues. Subsequently, methods leveraging pre-trained language models like BERT have been developed, which require less data and yield enhanced performance. The recent advent of large language models, such as GPT and LLaMA, has further revolutionized the field, facilitating more advanced and diverse applications with minimal fine-tuning. Despite the growing interest, a comprehensive survey of language modeling techniques for tabular data remains absent. This paper fills this gap by providing a systematic review of the development of language modeling for tabular data, encompassing: (1) a categorization of different tabular data structures and data types; (2) a review of key datasets used in model training and tasks used for evaluation; (3) a summary of modeling techniques including widely-adopted data processing methods, popular architectures, and training objectives; (4) the evolution from adapting traditional Pre-training/Pre-trained language models to the utilization of large language models; (5) an identification of persistent challenges and potential future research directions in language modeling for tabular data analysis. GitHub page associated with this survey is available at: https://github.com/lanxiang1017/Language-Modeling-on-Tabular-Data-Survey.git.||\n", "2409.05197": "|**2024-09-08**|[Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?](http://arxiv.org/abs/2409.05197)|null|\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u88ab\u8ba4\u4e3a\u5177\u5907\u8d8a\u6765\u8d8a\u591a\u7684\u4e0d\u540c\u80fd\u529b\uff0c\u4ece\u9605\u8bfb\u7406\u89e3\u5230\u9ad8\u7ea7\u6570\u5b66\u548c\u63a8\u7406\u6280\u80fd\uff0c\u518d\u5230\u62e5\u6709\u79d1\u5b66\u77e5\u8bc6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5173\u6ce8\u5b83\u4eec\u7684\u591a\u8df3\u63a8\u7406\u80fd\u529b\uff1a\u4ece\u591a\u4e2a\u6587\u672c\u6e90\u8bc6\u522b\u548c\u6574\u5408\u4fe1\u606f\u7684\u80fd\u529b\u3002\u9274\u4e8e\u5bf9\u73b0\u6709\u591a\u8df3\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b58\u5728\u7b80\u5316\u7ebf\u7d22\u7684\u62c5\u5fe7\uff0c\u8fd9\u4e9b\u7ebf\u7d22\u5141\u8bb8\u6a21\u578b\u7ed5\u8fc7\u63a8\u7406\u8981\u6c42\uff0c\u6211\u4eec\u7740\u624b\u8c03\u67e5 LLM \u662f\u5426\u5bb9\u6613\u5229\u7528\u6b64\u7c7b\u7b80\u5316\u7ebf\u7d22\u3002\u6211\u4eec\u53d1\u73b0\u6709\u8bc1\u636e\u8868\u660e\u5b83\u4eec\u786e\u5b9e\u7ed5\u8fc7\u4e86\u6267\u884c\u591a\u8df3\u63a8\u7406\u7684\u8981\u6c42\uff0c\u4f46\u5b83\u4eec\u8fd9\u6837\u505a\u7684\u65b9\u5f0f\u6bd4\u5bf9\u5176\u7ecf\u8fc7\u5fae\u8c03\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b (PLM) \u524d\u8f88\u7684\u62a5\u9053\u66f4\u4e3a\u5fae\u5999\u3002\u57fa\u4e8e\u8fd9\u4e00\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u8df3\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u751f\u6210\u770b\u4f3c\u5408\u7406\u7684\u591a\u8df3\u63a8\u7406\u94fe\uff0c\u6700\u7ec8\u5f97\u51fa\u9519\u8bef\u7b54\u6848\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u4e2a\u5f00\u653e\u548c\u4e13\u6709\u7684\u6700\u5148\u8fdb LLM\uff0c\u53d1\u73b0\u5b83\u4eec\u6267\u884c\u591a\u8df3\u63a8\u7406\u7684\u6027\u80fd\u53d7\u5230\u5f71\u54cd\uff0c\u5982\u5728\u51fa\u73b0\u6b64\u7c7b\u770b\u4f3c\u5408\u7406\u7684\u66ff\u4ee3\u65b9\u6848\u65f6 F1 \u5206\u6570\u76f8\u5bf9\u964d\u4f4e\u9ad8\u8fbe 45% \u6240\u793a\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u66f4\u6df1\u5165\u7684\u5206\u6790\uff0c\u53d1\u73b0\u867d\u7136 LLM \u503e\u5411\u4e8e\u5ffd\u7565\u8bef\u5bfc\u6027\u7684\u8bcd\u6c47\u7ebf\u7d22\uff0c\u4f46\u8bef\u5bfc\u6027\u7684\u63a8\u7406\u8def\u5f84\u786e\u5b9e\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\u3002|\n", "2409.03773": "|**2024-08-21**|[CoPRA: Bridging Cross-domain Pretrained Sequence Models with Complex Structures for Protein-RNA Binding Affinity Prediction](http://arxiv.org/abs/2409.03773)|null|\u51c6\u786e\u6d4b\u91cf\u86cb\u767d\u8d28-RNA\u7ed3\u5408\u4eb2\u548c\u529b\u5728\u8bb8\u591a\u751f\u7269\u8fc7\u7a0b\u548c\u836f\u7269\u8bbe\u8ba1\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u4ee5\u524d\u7684\u86cb\u767d\u8d28-RNA\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u8ba1\u7b97\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5e8f\u5217\u6216\u7ed3\u6784\u7279\u5f81\uff0c\u65e0\u6cd5\u5168\u9762\u6355\u6349\u7ed3\u5408\u673a\u5236\u3002\u6700\u8fd1\u51fa\u73b0\u7684\u5728\u5927\u91cf\u65e0\u76d1\u7763\u86cb\u767d\u8d28\u548cRNA\u5e8f\u5217\u4e0a\u8bad\u7ec3\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u5305\u62ec\u7ed3\u5408\u4f4d\u70b9\u9884\u6d4b\u5728\u5185\u7684\u5404\u79cd\u57df\u5185\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8868\u793a\u80fd\u529b\u3002\u7136\u800c\uff0c\u534f\u540c\u5e94\u7528\u4e0d\u540c\u9886\u57df\u7684\u8bed\u8a00\u6a21\u578b\u6765\u5b8c\u6210\u590d\u6742\u7ea7\u522b\u7684\u4efb\u52a1\u4ecd\u672a\u5f97\u5230\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86CoPRA\uff0c\u901a\u8fc7\u86cb\u767d\u8d28-RNA\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u7684\u590d\u5408\u7269\u7ed3\u6784\uff0c\u5c06\u6765\u81ea\u4e0d\u540c\u751f\u7269\u9886\u57df\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8fde\u63a5\u8d77\u6765\u3002\u6211\u4eec\u9996\u6b21\u8bc1\u660e\u4e86\u8de8\u751f\u7269\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u534f\u540c\u63d0\u9ad8\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2aCo-Former\u6765\u7ed3\u5408\u8de8\u6a21\u6001\u5e8f\u5217\u548c\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8303\u56f4\u9884\u8bad\u7ec3\u7b56\u7565\u6765\u63d0\u9ad8Co-Former\u7684\u4ea4\u4e92\u7406\u89e3\u80fd\u529b\u3002\u540c\u65f6\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u6700\u5927\u7684\u86cb\u767d\u8d28-RNA\u7ed3\u5408\u4eb2\u548c\u529b\u6570\u636e\u96c6PRA310\u7528\u4e8e\u6027\u80fd\u8bc4\u4f30\u3002\u6211\u4eec\u8fd8\u5728\u4e00\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e86\u6211\u4eec\u6a21\u578b\u7684\u7a81\u53d8\u6548\u5e94\u9884\u6d4b\u80fd\u529b\u3002CoPRA\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u7684\u5206\u6790\uff0c\u5e76\u9a8c\u8bc1\u4e86CoPRA\u53ef\u4ee5\uff081\uff09\u51c6\u786e\u9884\u6d4b\u86cb\u767d\u8d28-RNA\u7ed3\u5408\u4eb2\u548c\u529b\uff1b\uff082\uff09\u7406\u89e3\u7531\u7a81\u53d8\u5f15\u8d77\u7684\u7ed3\u5408\u4eb2\u548c\u529b\u53d8\u5316\uff1b\uff083\uff09\u53d7\u76ca\u4e8e\u6570\u636e\u548c\u6a21\u578b\u89c4\u6a21\u7684\u6269\u5927\u3002|\n"}, "Transformer": {"2409.02727": "|**2024-09-05**|[Pooling And Attention: What Are Effective Designs For LLM-Based Embedding Models?](http://arxiv.org/abs/2409.02727)|**[link](https://github.com/yixuantt/poolingandattn)**|The significant advancements of Large Language Models (LLMs) in generative tasks have led to a growing body of work exploring LLM-based embedding models. While these models, employing different pooling and attention strategies, have achieved state-of-the-art performance on public embedding benchmarks, questions still arise about what constitutes an effective design for LLM-based embedding models. However, these models are often trained on different datasets, using different LLM base models or training settings. Moreover, evaluations on public embedding benchmarks often fail to report statistical significance, making it difficult to determine which designs truly contribute to final performance. This complicates the process for practitioners seeking optimal training recipes for LLM-based embedding models. In this study, we conduct a large-scale experiment by training a series of LLM-based embedding models using the same training data and base model but differing in their pooling and attention strategies. The results show that there is no one-size-fits-all solution: while bidirectional attention and an additional trainable pooling layer outperform in text similarity and information retrieval tasks, they do not significantly surpass simpler designs like EOS-last token pooling and default causal attention in clustering and classification tasks. Furthermore, we propose a new pooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs of all hidden layers, rather than just the last layer, using a cross-attention network. This method proves to be statistically superior in text similarity and retrieval tasks compared to existing pooling methods. Overall, this paper sheds light on effective training strategies for LLM-based embedding models.||\n", "2409.02545": "|**2024-09-04**|[UniTT-Stereo: Unified Training of Transformer for Enhanced Stereo Matching](http://arxiv.org/abs/2409.02545)|null|Unlike other vision tasks where Transformer-based approaches are becoming increasingly common, stereo depth estimation is still dominated by convolution-based approaches. This is mainly due to the limited availability of real-world ground truth for stereo matching, which is a limiting factor in improving the performance of Transformer-based stereo approaches. In this paper, we propose UniTT-Stereo, a method to maximize the potential of Transformer-based stereo architectures by unifying self-supervised learning used for pre-training with stereo matching framework based on supervised learning. To be specific, we explore the effectiveness of reconstructing features of masked portions in an input image and at the same time predicting corresponding points in another image from the perspective of locality inductive bias, which is crucial in training models with limited training data. Moreover, to address these challenging tasks of reconstruction-and-prediction, we present a new strategy to vary a masking ratio when training the stereo model with stereo-tailored losses. State-of-the-art performance of UniTT-Stereo is validated on various benchmarks such as ETH3D, KITTI 2012, and KITTI 2015 datasets. Lastly, to investigate the advantages of the proposed approach, we provide a frequency analysis of feature maps and the analysis of locality inductive bias based on attention maps.||\n", "2409.02056": "|**2024-09-03**|[F2former: When Fractional Fourier Meets Deep Wiener Deconvolution and Selective Frequency Transformer for Image Deblurring](http://arxiv.org/abs/2409.02056)|null|Recent progress in image deblurring techniques focuses mainly on operating in both frequency and spatial domains using the Fourier transform (FT) properties. However, their performance is limited due to the dependency of FT on stationary signals and its lack of capability to extract spatial-frequency properties. In this paper, we propose a novel approach based on the Fractional Fourier Transform (FRFT), a unified spatial-frequency representation leveraging both spatial and frequency components simultaneously, making it ideal for processing non-stationary signals like images. Specifically, we introduce a Fractional Fourier Transformer (F2former), where we combine the classical fractional Fourier based Wiener deconvolution (F2WD) as well as a multi-branch encoder-decoder transformer based on a new fractional frequency aware transformer block (F2TB). We design F2TB consisting of a fractional frequency aware self-attention (F2SA) to estimate element-wise product attention based on important frequency components and a novel feed-forward network based on frequency division multiplexing (FM-FFN) to refine high and low frequency features separately for efficient latent clear image restoration. Experimental results for the cases of both motion deblurring as well as defocus deblurring show that the performance of our proposed method is superior to other state-of-the-art (SOTA) approaches.||\n", "2409.02018": "|**2024-09-03**|[TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation](http://arxiv.org/abs/2409.02018)|null|In healthcare, medical image segmentation is crucial for accurate disease diagnosis and the development of effective treatment strategies. Early detection can significantly aid in managing diseases and potentially prevent their progression. Machine learning, particularly deep convolutional neural networks, has emerged as a promising approach to addressing segmentation challenges. Traditional methods like U-Net use encoding blocks for local representation modeling and decoding blocks to uncover semantic relationships. However, these models often struggle with multi-scale objects exhibiting significant variations in texture and shape, and they frequently fail to capture long-range dependencies in the input data. Transformers designed for sequence-to-sequence predictions have been proposed as alternatives, utilizing global self-attention mechanisms. Yet, they can sometimes lack precise localization due to insufficient granular details. To overcome these limitations, we introduce TransDAE: a novel approach that reimagines the self-attention mechanism to include both spatial and channel-wise associations across the entire feature space, while maintaining computational efficiency. Additionally, TransDAE enhances the skip connection pathway with an inter-scale interaction module, promoting feature reuse and improving localization accuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on the Synaps multi-organ dataset, even without relying on pre-trained weights.||\n", "2409.01557": "|**2024-09-03**|[TASL-Net: Tri-Attention Selective Learning Network for Intelligent Diagnosis of Bimodal Ultrasound Video](http://arxiv.org/abs/2409.01557)|null|In the intelligent diagnosis of bimodal (gray-scale and contrast-enhanced) ultrasound videos, medical domain knowledge such as the way sonographers browse videos, the particular areas they emphasize, and the features they pay special attention to, plays a decisive role in facilitating precise diagnosis. Embedding medical knowledge into the deep learning network can not only enhance performance but also boost clinical confidence and reliability of the network. However, it is an intractable challenge to automatically focus on these person- and disease-specific features in videos and to enable networks to encode bimodal information comprehensively and efficiently. This paper proposes a novel Tri-Attention Selective Learning Network (TASL-Net) to tackle this challenge and automatically embed three types of diagnostic attention of sonographers into a mutual transformer framework for intelligent diagnosis of bimodal ultrasound videos. Firstly, a time-intensity-curve-based video selector is designed to mimic the temporal attention of sonographers, thus removing a large amount of redundant information while improving computational efficiency of TASL-Net. Then, to introduce the spatial attention of the sonographers for contrast-enhanced video analysis, we propose the earliest-enhanced position detector based on structural similarity variation, on which the TASL-Net is made to focus on the differences of perfusion variation inside and outside the lesion. Finally, by proposing a mutual encoding strategy that combines convolution and transformer, TASL-Net possesses bimodal attention to structure features on gray-scale videos and to perfusion variations on contrast-enhanced videos. These modules work collaboratively and contribute to superior performance. We conduct a detailed experimental validation of TASL-Net's performance on three datasets, including lung, breast, and liver.||\n", "2409.01352": "|**2024-09-02**|[Spectron: Target Speaker Extraction using Conditional Transformer with Adversarial Refinement](http://arxiv.org/abs/2409.01352)|null|Recently, attention-based transformers have become a de facto standard in many deep learning applications including natural language processing, computer vision, signal processing, etc.. In this paper, we propose a transformer-based end-to-end model to extract a target speaker's speech from a monaural multi-speaker mixed audio signal. Unlike existing speaker extraction methods, we introduce two additional objectives to impose speaker embedding consistency and waveform encoder invertibility and jointly train both speaker encoder and speech separator to better capture the speaker conditional embedding. Furthermore, we leverage a multi-scale discriminator to refine the perceptual quality of the extracted speech. Our experiments show that the use of a dual path transformer in the separator backbone along with proposed training paradigm improves the CNN baseline by $3.12$ dB points. Finally, we compare our approach with recent state-of-the-arts and show that our model outperforms existing methods by $4.1$ dB points on an average without creating additional data dependency.||\n", "2409.01193": "|**2024-09-02**|[CLIBE: Detecting Dynamic Backdoors in Transformer-based NLP Models](http://arxiv.org/abs/2409.01193)|null|Backdoors can be injected into NLP models to induce misbehavior when the input text contains a specific feature, known as a trigger, which the attacker secretly selects. Unlike fixed words, phrases, or sentences used in the static text trigger, NLP dynamic backdoor attacks design triggers associated with abstract and latent text features, making them considerably stealthier than traditional static backdoor attacks. However, existing research on NLP backdoor detection primarily focuses on defending against static backdoor attacks, while detecting dynamic backdoors in NLP models remains largely unexplored. This paper presents CLIBE, the first framework to detect dynamic backdoors in Transformer-based NLP models. CLIBE injects a \"few-shot perturbation\" into the suspect Transformer model by crafting optimized weight perturbation in the attention layers to make the perturbed model classify a limited number of reference samples as a target label. Subsequently, CLIBE leverages the generalization ability of this few-shot perturbation to determine whether the original model contains a dynamic backdoor. Extensive evaluation on three advanced NLP dynamic backdoor attacks, two widely-used Transformer frameworks, and four real-world classification tasks strongly validates the effectiveness of CLIBE. We also demonstrate the robustness of CLIBE against various adaptive attacks. Furthermore, we employ CLIBE to scrutinize 49 popular Transformer models on Hugging Face and discover one exhibiting a high probability of containing a dynamic backdoor. We have contacted Hugging Face and provided detailed evidence of this model's backdoor behavior. Moreover, we extend CLIBE to detect backdoor text generation models modified to exhibit toxic behavior. To the best of our knowledge, CLIBE is the first framework capable of detecting backdoors in text generation models without access to trigger input test samples.||\n", "2409.01068": "|**2024-09-02**|[Progressive Retinal Image Registration via Global and Local Deformable Transformations](http://arxiv.org/abs/2409.01068)|**[link](https://github.com/lyp-deeplearning/awesome-retinal-registration)**|Retinal image registration plays an important role in the ophthalmological diagnosis process. Since there exist variances in viewing angles and anatomical structures across different retinal images, keypoint-based approaches become the mainstream methods for retinal image registration thanks to their robustness and low latency. These methods typically assume the retinal surfaces are planar, and adopt feature matching to obtain the homography matrix that represents the global transformation between images. Yet, such a planar hypothesis inevitably introduces registration errors since retinal surface is approximately curved. This limitation is more prominent when registering image pairs with significant differences in viewing angles. To address this problem, we propose a hybrid registration framework called HybridRetina, which progressively registers retinal images with global and local deformable transformations. For that, we use a keypoint detector and a deformation network called GAMorph to estimate the global transformation and local deformable transformation, respectively. Specifically, we integrate multi-level pixel relation knowledge to guide the training of GAMorph. Additionally, we utilize an edge attention module that includes the geometric priors of the images, ensuring the deformation field focuses more on the vascular regions of clinical interest. Experiments on two widely-used datasets, FIRE and FLoRI21, show that our proposed HybridRetina significantly outperforms some state-of-the-art methods. The code is available at https://github.com/lyp-deeplearning/awesome-retinal-registration.||\n", "2409.00904": "|**2024-09-02**|[Multi-scale Temporal Fusion Transformer for Incomplete Vehicle Trajectory Prediction](http://arxiv.org/abs/2409.00904)|null|Motion prediction plays an essential role in autonomous driving systems, enabling autonomous vehicles to achieve more accurate local-path planning and driving decisions based on predictions of the surrounding vehicles. However, existing methods neglect the potential missing values caused by object occlusion, perception failures, etc., which inevitably degrades the trajectory prediction performance in real traffic scenarios. To address this limitation, we propose a novel end-to-end framework for incomplete vehicle trajectory prediction, named Multi-scale Temporal Fusion Transformer (MTFT), which consists of the Multi-scale Attention Head (MAH) and the Continuity Representation-guided Multi-scale Fusion (CRMF) module. Specifically, the MAH leverages the multi-head attention mechanism to parallelly capture multi-scale motion representation of trajectory from different temporal granularities, thus mitigating the adverse effect of missing values on prediction. Furthermore, the multi-scale motion representation is input into the CRMF module for multi-scale fusion to obtain the robust temporal feature of the vehicle. During the fusion process, the continuity representation of vehicle motion is first extracted across time steps to guide the fusion, ensuring that the resulting temporal feature incorporates both detailed information and the overall trend of vehicle motion, which facilitates the accurate decoding of future trajectory that is consistent with the vehicle's motion trend. We evaluate the proposed model on four datasets derived from highway and urban traffic scenarios. The experimental results demonstrate its superior performance in the incomplete vehicle trajectory prediction task compared with state-of-the-art models, e.g., a comprehensive performance improvement of more than 39% on the HighD dataset.||\n", "2409.00591": "|**2024-09-01**|[Attention-Guided Multi-scale Interaction Network for Face Super-Resolution](http://arxiv.org/abs/2409.00591)|null|Recently, CNN and Transformer hybrid networks demonstrated excellent performance in face super-resolution (FSR) tasks. Since numerous features at different scales in hybrid networks, how to fuse these multi-scale features and promote their complementarity is crucial for enhancing FSR. However, existing hybrid network-based FSR methods ignore this, only simply combining the Transformer and CNN. To address this issue, we propose an attention-guided Multi-scale interaction network (AMINet), which contains local and global feature interactions as well as encoder-decoder phases feature interactions. Specifically, we propose a Local and Global Feature Interaction Module (LGFI) to promote fusions of global features and different receptive fields' local features extracted by our Residual Depth Feature Extraction Module (RDFE). Additionally, we propose a Selective Kernel Attention Fusion Module (SKAF) to adaptively select fusions of different features within LGFI and encoder-decoder phases. Our above design allows the free flow of multi-scale features from within modules and between encoder and decoder, which can promote the complementarity of different scale features to enhance FSR. Comprehensive experiments confirm that our method consistently performs well with less computational consumption and faster inference.||\n", "2409.03621": "|**2024-09-05**|[Attend First, Consolidate Later: On the Importance of Attention in Different LLM Layers](http://arxiv.org/abs/2409.03621)|null|In decoder-based LLMs, the representation of a given layer serves two purposes: as input to the next layer during the computation of the current token; and as input to the attention mechanism of future tokens. In this work, we show that the importance of the latter role might be overestimated. To show that, we start by manipulating the representations of previous tokens; e.g. by replacing the hidden states at some layer k with random vectors. Our experimenting with four LLMs and four tasks show that this operation often leads to small to negligible drop in performance. Importantly, this happens if the manipulation occurs in the top part of the model-k is in the final 30-50% of the layers. In contrast, doing the same manipulation in earlier layers might lead to chance level performance. We continue by switching the hidden state of certain tokens with hidden states of other tokens from another prompt; e.g., replacing the word \"Italy\" with \"France\" in \"What is the capital of Italy?\". We find that when applying this switch in the top 1/3 of the model, the model ignores it (answering \"Rome\"). However if we apply it before, the model conforms to the switch (\"Paris\"). Our results hint at a two stage process in transformer-based LLMs: the first part gathers input from previous tokens, while the second mainly processes that information internally.||\n", "2409.03516": "|**2024-09-05**|[LMLT: Low-to-high Multi-Level Vision Transformer for Image Super-Resolution](http://arxiv.org/abs/2409.03516)|**[link](https://github.com/jwgdmkj/lmlt)**|Recent Vision Transformer (ViT)-based methods for Image Super-Resolution have demonstrated impressive performance. However, they suffer from significant complexity, resulting in high inference times and memory usage. Additionally, ViT models using Window Self-Attention (WSA) face challenges in processing regions outside their windows. To address these issues, we propose the Low-to-high Multi-Level Transformer (LMLT), which employs attention with varying feature sizes for each head. LMLT divides image features along the channel dimension, gradually reduces spatial size for lower heads, and applies self-attention to each head. This approach effectively captures both local and global information. By integrating the results from lower heads into higher heads, LMLT overcomes the window boundary issues in self-attention. Extensive experiments show that our model significantly reduces inference time and GPU memory usage while maintaining or even surpassing the performance of state-of-the-art ViT-based Image Super-Resolution methods. Our codes are availiable at https://github.com/jwgdmkj/LMLT.||\n", "2409.03514": "|**2024-09-05**|[Blended Latent Diffusion under Attention Control for Real-World Video Editing](http://arxiv.org/abs/2409.03514)|null|\u7531\u4e8e\u7f3a\u4e4f\u5b8c\u5168\u516c\u5f00\u53ef\u7528\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\uff0c\u5f53\u524d\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u503e\u5411\u4e8e\u5efa\u7acb\u5728\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e4b\u4e0a\uff0c\u7136\u800c\uff0c\u5b83\u4eec\u5728\u5904\u7406\u5177\u6709\u65f6\u95f4\u4fe1\u606f\u7684\u89c6\u9891\u5c40\u90e8\u7f16\u8f91\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u9996\u5148\uff0c\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u8bd5\u56fe\u901a\u8fc7\u9884\u5b9a\u4e49\u7684\u63a9\u7801\u4e13\u6ce8\u4e8e\u5c40\u90e8\u533a\u57df\u7f16\u8f91\uff0c\u4f46\u7531\u4e8e\u6bcf\u4e00\u5e27\u7684\u7a7a\u95f4\u6574\u4f53\u751f\u6210\uff0c\u533a\u57df\u5916\u80cc\u666f\u7684\u4fdd\u7559\u5e76\u4e0d\u7406\u60f3\u3002\u6b64\u5916\uff0c\u7528\u6237\u4e13\u95e8\u63d0\u4f9b\u63a9\u7801\u662f\u4e00\u9879\u989d\u5916\u7684\u6602\u8d35\u5de5\u4f5c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u96c6\u6210\u5230\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u7684\u81ea\u4e3b\u63a9\u7801\u7b56\u7565\u3002\u6700\u540e\u4f46\u540c\u6837\u91cd\u8981\u7684\u662f\uff0c\u56fe\u50cf\u7ea7\u9884\u8bad\u7ec3\u6a21\u578b\u6ca1\u6709\u5b66\u4e60\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u800c\u8fd9\u5bf9\u4e8e\u8868\u8fbe\u8fd0\u52a8\u548c\u52a8\u6001\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u91c7\u7528\u56fe\u50cf\u7ea7\u6df7\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6765\u6267\u884c\u5c40\u90e8\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5229\u7528 DDIM \u53cd\u6f14\u6765\u83b7\u53d6\u6f5c\u5728\u4ee3\u7801\u4f5c\u4e3a\u80cc\u666f\u6f5c\u5728\u4ee3\u7801\uff0c\u800c\u4e0d\u662f\u968f\u673a\u566a\u58f0\u7684\u6f5c\u5728\u4ee3\u7801\uff0c\u4ee5\u66f4\u597d\u5730\u4fdd\u7559\u8f93\u5165\u89c6\u9891\u7684\u80cc\u666f\u4fe1\u606f\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4ece\u6269\u6563\u6b65\u9aa4\u4e2d\u7684\u4ea4\u53c9\u6ce8\u610f\u56fe\u6d3e\u751f\u7684\u81ea\u4e3b\u63a9\u7801\u5236\u9020\u673a\u5236\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u5c06 U-Net \u7684\u81ea\u6ce8\u610f\u529b\u5757\u8f6c\u6362\u4e3a\u65f6\u7a7a\u5757\u6765\u589e\u5f3a\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u6709\u6548\u6027\u3002||\n", "2409.03463": "|**2024-09-05**|[Characterizing Massive Activations of Attention Mechanism in Graph Neural Networks](http://arxiv.org/abs/2409.03463)|**[link](https://github.com/msorbi/gnn-ma)**|Graph Neural Networks (GNNs) have become increasingly popular for effectively modeling data with graph structures. Recently, attention mechanisms have been integrated into GNNs to improve their ability to capture complex patterns. This paper presents the first comprehensive study revealing a critical, unexplored consequence of this integration: the emergence of Massive Activations (MAs) within attention layers. We introduce a novel method for detecting and analyzing MAs, focusing on edge features in different graph transformer architectures. Our study assesses various GNN models using benchmark datasets, including ZINC, TOX21, and PROTEINS. Key contributions include (1) establishing the direct link between attention mechanisms and MAs generation in GNNs, (2) developing a robust definition and detection method for MAs based on activation ratio distributions, (3) introducing the Explicit Bias Term (EBT) as a potential countermeasure and exploring it as an adversarial framework to assess models robustness based on the presence or absence of MAs. Our findings highlight the prevalence and impact of attention-induced MAs across different architectures, such as GraphTransformer, GraphiT, and SAN. The study reveals the complex interplay between attention mechanisms, model architecture, dataset characteristics, and MAs emergence, providing crucial insights for developing more robust and reliable graph models.||\n", "2409.03460": "|**2024-09-05**|[LowFormer: Hardware Efficient Design for Convolutional Transformer Backbones](http://arxiv.org/abs/2409.03460)|**[link](https://github.com/altair199797/lowformer)**|Research in efficient vision backbones is evolving into models that are a mixture of convolutions and transformer blocks. A smart combination of both, architecture-wise and component-wise is mandatory to excel in the speedaccuracy trade-off. Most publications focus on maximizing accuracy and utilize MACs (multiply accumulate operations) as an efficiency metric. The latter however often do not measure accurately how fast a model actually is due to factors like memory access cost and degree of parallelism. We analyzed common modules and architectural design choices for backbones not in terms of MACs, but rather in actual throughput and latency, as the combination of the latter two is a better representation of the efficiency of models in real applications. We applied the conclusions taken from that analysis to create a recipe for increasing hardware-efficiency in macro design. Additionally we introduce a simple slimmed-down version of MultiHead Self-Attention, that aligns with our analysis. We combine both macro and micro design to create a new family of hardware-efficient backbone networks called LowFormer. LowFormer achieves a remarkable speedup in terms of throughput and latency, while achieving similar or better accuracy than current state-of-the-art efficient backbones. In order to prove the generalizability of our hardware-efficient design, we evaluate our method on GPU, mobile GPU and ARM CPU. We further show that the downstream tasks object detection and semantic segmentation profit from our hardware-efficient architecture. Code and models are available at https://github.com/ altair199797/LowFormer.||\n", "2409.03332": "|**2024-09-05**|[Masked Sensory-Temporal Attention for Sensor Generalization in Quadruped Locomotion](http://arxiv.org/abs/2409.03332)|null|With the rising focus on quadrupeds, a generalized policy capable of handling different robot models and sensory inputs will be highly beneficial. Although several methods have been proposed to address different morphologies, it remains a challenge for learning-based policies to manage various combinations of proprioceptive information. This paper presents Masked Sensory-Temporal Attention (MSTA), a novel transformer-based model with masking for quadruped locomotion. It employs direct sensor-level attention to enhance sensory-temporal understanding and handle different combinations of sensor data, serving as a foundation for incorporating unseen information. This model can effectively understand its states even with a large portion of missing information, and is flexible enough to be deployed on a physical system despite the long input sequence.||\n", "2409.03223": "|**2024-09-05**|[Why mamba is effective? Exploit Linear Transformer-Mamba Network for Multi-Modality Image Fusion](http://arxiv.org/abs/2409.03223)|null|Multi-modality image fusion aims to integrate the merits of images from different sources and render high-quality fusion images. However, existing feature extraction and fusion methods are either constrained by inherent local reduction bias and static parameters during inference (CNN) or limited by quadratic computational complexity (Transformers), and cannot effectively extract and fuse features. To solve this problem, we propose a dual-branch image fusion network called Tmamba. It consists of linear Transformer and Mamba, which has global modeling capabilities while maintaining linear complexity. Due to the difference between the Transformer and Mamba structures, the features extracted by the two branches carry channel and position information respectively. T-M interaction structure is designed between the two branches, using global learnable parameters and convolutional layers to transfer position and channel information respectively. We further propose cross-modal interaction at the attention level to obtain cross-modal attention. Experiments show that our Tmamba achieves promising results in multiple fusion tasks, including infrared-visible image fusion and medical image fusion. Code with checkpoints will be available after the peer-review process.||\n", "2409.03115": "|**2024-09-04**|[Probing self-attention in self-supervised speech models for cross-linguistic differences](http://arxiv.org/abs/2409.03115)|null|Speech models have gained traction thanks to increase in accuracy from novel transformer architectures. While this impressive increase in performance across automatic speech recognition (ASR) benchmarks is noteworthy, there is still much that is unknown about the use of attention mechanisms for speech-related tasks. For example, while it is assumed that these models are learning language-independent (i.e., universal) speech representations, there has not yet been an in-depth exploration of what it would mean for the models to be language-independent. In the current paper, we explore this question within the realm of self-attention mechanisms of one small self-supervised speech transformer model (TERA). We find that even with a small model, the attention heads learned are diverse ranging from almost entirely diagonal to almost entirely global regardless of the training language. We highlight some notable differences in attention patterns between Turkish and English and demonstrate that the models do learn important phonological information during pretraining. We also present a head ablation study which shows that models across languages primarily rely on diagonal heads to classify phonemes.||\n", "2409.03103": "|**2024-09-04**|[Leveraging Interpretability in the Transformer to Automate the Proactive Scaling of Cloud Resources](http://arxiv.org/abs/2409.03103)|null|\u73b0\u4ee3Web\u670d\u52a1\u91c7\u7528\u4e91\u539f\u751f\u539f\u5219\u6765\u5229\u7528\u5fae\u670d\u52a1\u7684\u4f18\u52bf\u3002\u4e3a\u4e86\u6839\u636e\u670d\u52a1\u7b49\u7ea7\u534f\u8bae\uff08SLA\uff09\u6301\u7eed\u4fdd\u8bc1\u9ad8\u8d28\u91cf\u7684\u670d\u52a1\uff08QoS\uff09\uff0c\u786e\u4fdd\u4ee4\u4eba\u6ee1\u610f\u7684\u7528\u6237\u4f53\u9a8c\u5e76\u6700\u5927\u7a0b\u5ea6\u5730\u964d\u4f4e\u8fd0\u8425\u6210\u672c\uff0c\u5fc5\u987b\u4e3a\u6bcf\u4e2a\u5fae\u670d\u52a1\u914d\u7f6e\u9002\u91cf\u7684\u8d44\u6e90\u3002\u7136\u800c\uff0c\u51c6\u786e\u5730\u4e3a\u5fae\u670d\u52a1\u914d\u7f6e\u5145\u8db3\u7684\u8d44\u6e90\u975e\u5e38\u590d\u6742\uff0c\u5e76\u4e14\u53d6\u51b3\u4e8e\u8bb8\u591a\u56e0\u7d20\uff0c\u5305\u62ec\u5de5\u4f5c\u8d1f\u8f7d\u5f3a\u5ea6\u548c\u5fae\u670d\u52a1\u4e4b\u95f4\u590d\u6742\u7684\u4e92\u8fde\u5173\u7cfb\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u6355\u83b7\u4e86\u7aef\u5230\u7aef\u5ef6\u8fdf\u3001\u524d\u7aef\u7ea7\u522b\u7684\u8bf7\u6c42\u548c\u8d44\u6e90\u5229\u7528\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u5f00\u53d1\u7684\u6a21\u578b\u6765\u9884\u6d4b\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002\u6211\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u5229\u7528\u4e86\u65f6\u95f4\u878d\u5408Transformer\uff08TFT\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5177\u6709\u53ef\u89e3\u91ca\u6027\u7279\u5f81\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u67b6\u6784\u3002\u5f53\u9884\u6d4b\u7ed3\u679c\u8868\u660e\u4e0d\u7b26\u5408SLA\u65f6\uff0c\u6211\u4eec\u4f7f\u7528TFT\u63d0\u4f9b\u7684\u7279\u5f81\u91cd\u8981\u6027\u4f5c\u4e3a\u6838\u5cad\u56de\u5f52\uff08KRR\uff09\u4e2d\u7684\u534f\u53d8\u91cf\uff0c\u5e76\u5c06\u54cd\u5e94\u53d8\u91cf\u8bbe\u7f6e\u4e3a\u671f\u671b\u5ef6\u8fdf\uff0c\u4ee5\u5b66\u4e60\u4e0e\u7279\u5f81\u91cd\u8981\u6027\u76f8\u5173\u7684\u53c2\u6570\u3002\u8fd9\u4e9b\u5b66\u4e60\u5230\u7684\u53c2\u6570\u53cd\u6620\u4e86\u4e3a\u786e\u4fdd\u7b26\u5408SLA\u800c\u9700\u8981\u5bf9\u7279\u5f81\u8fdb\u884c\u7684\u8c03\u6574\u3002\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u57fa\u4e8e\u5fae\u670d\u52a1\u7684\u5e94\u7528\u7a0b\u5e8f\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u90e8\u7f72\u8def\u7ebf\u56fe\u3002||\n", "2409.05749": "|**2024-09-09**|[ReL-SAR: Representation Learning for Skeleton Action Recognition with Convolutional Transformers and BYOL](http://arxiv.org/abs/2409.05749)|null|\u4e3a\u4e86\u63d0\u53d6\u7a33\u5065\u4e14\u53ef\u6cdb\u5316\u7684\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u7279\u5f81\uff0c\u901a\u5e38\u9700\u8981\u5927\u91cf\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\uff0c\u8fd9\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u53d7\u5230\u6807\u6ce8\u548c\u8ba1\u7b97\u6210\u672c\u7684\u963b\u788d\u3002\u56e0\u6b64\uff0c\u65e0\u76d1\u7763\u8868\u5f81\u5b66\u4e60\u5bf9\u4e8e\u5229\u7528\u672a\u6807\u8bb0\u7684\u9aa8\u67b6\u6570\u636e\u81f3\u5173\u91cd\u8981\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u7528\u4e8e\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u7684\u65e0\u76d1\u7763\u8868\u5f81\u5b66\u4e60\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5377\u79efTransformer\u6846\u67b6\uff0c\u540d\u4e3aReL-SAR\uff0c\u5b83\u5229\u7528\u5377\u79ef\u5c42\u548c\u6ce8\u610f\u529b\u5c42\u7684\u4e92\u8865\u6027\u6765\u8054\u5408\u5efa\u6a21\u9aa8\u67b6\u5e8f\u5217\u4e2d\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u7ebf\u7d22\u3002\u6211\u4eec\u8fd8\u5bf9\u9aa8\u67b6\u5173\u8282\u4f7f\u7528\u9009\u62e9\u6392\u5217\u7b56\u7565\uff0c\u4ee5\u786e\u4fdd\u4ece\u9aa8\u9abc\u6570\u636e\u4e2d\u83b7\u5f97\u66f4\u591a\u4fe1\u606f\u63cf\u8ff0\u3002\u6700\u540e\uff0c\u6211\u4eec\u5229\u7528Bootstrap Your Own Latent (BYOL) \u4ece\u672a\u6807\u8bb0\u7684\u9aa8\u67b6\u5e8f\u5217\u6570\u636e\u4e2d\u5b66\u4e60\u9c81\u68d2\u7684\u8868\u5f81\u3002\u6211\u4eec\u5728\u6709\u9650\u5927\u5c0f\u7684\u6570\u636e\u96c6\uff08MCAD\u3001IXMAS\u3001JHMDB \u548c NW-UCLA\uff09\u4e0a\u53d6\u5f97\u4e86\u975e\u5e38\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u4e3a\u4e86\u786e\u4fdd\u53ef\u91cd\u590d\u6027\u548c\u53ef\u91cd\u7528\u6027\uff0c\u5305\u542b\u6240\u6709\u5b9e\u73b0\u53c2\u6570\u7684\u6e90\u4ee3\u7801\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u63d0\u4f9b\uff1ahttps://github.com/SafwenNaimi/Representation-Learning-for-Skeleton-Action-Recognition-with-Convolutional-Transformers-and-BYOL|\n", "2409.05587": "|**2024-09-09**|[DSDFormer: An Innovative Transformer-Mamba Framework for Robust High-Precision Driver Distraction Identification](http://arxiv.org/abs/2409.05587)|null|\u9a7e\u9a76\u5458\u5206\u5fc3\u4ecd\u7136\u662f\u4ea4\u901a\u4e8b\u6545\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u5bf9\u5168\u7403\u9053\u8def\u5b89\u5168\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002\u968f\u7740\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u4e0d\u65ad\u53d1\u5c55\uff0c\u51c6\u786e\u3001\u5b9e\u65f6\u5730\u8bc6\u522b\u9a7e\u9a76\u5458\u5206\u5fc3\u884c\u4e3a\u5df2\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u5904\u7406\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u566a\u58f0\u6807\u7b7e\u7684\u540c\u65f6\uff0c\u517c\u987e\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u7ec6\u7c92\u5ea6\u7684\u5c40\u90e8\u7279\u5f81\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86DSDFormer\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u8fc7\u53cc\u72b6\u6001\u57df\u6ce8\u610f\u529b\uff08DSDA\uff09\u673a\u5236\u6574\u5408Transformer\u548cMamba\u67b6\u6784\u4f18\u52bf\u7684\u65b0\u6846\u67b6\uff0c\u80fd\u591f\u5728\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u548c\u8be6\u7ec6\u7279\u5f81\u63d0\u53d6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4ece\u800c\u5b9e\u73b0\u7a33\u5065\u7684\u9a7e\u9a76\u5458\u884c\u4e3a\u8bc6\u522b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u65f6\u95f4\u63a8\u7406\u7f6e\u4fe1\u5b66\u4e60\uff08TRCL\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u89c6\u9891\u5e8f\u5217\u4e2d\u7684\u65f6\u7a7a\u76f8\u5173\u6027\u6765\u7ec6\u5316\u566a\u58f0\u6807\u7b7e\u3002\u6211\u4eec\u7684\u6a21\u578b\u5728AUC-V1\u3001AUC-V2\u548c100-Driver\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728NVIDIA Jetson AGX Orin\u5e73\u53f0\u4e0a\u5c55\u793a\u4e86\u5b9e\u65f6\u5904\u7406\u6548\u7387\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDSDFormer\u548cTRCL\u663e\u8457\u63d0\u9ad8\u4e86\u9a7e\u9a76\u5458\u5206\u5fc3\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u589e\u5f3a\u9053\u8def\u5b89\u5168\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002|\n", "2409.05477": "|**2024-09-09**|[Retrofitting Temporal Graph Neural Networks with Transformer](http://arxiv.org/abs/2409.05477)|**[link](https://github.com/qianghuangwhu/tf-tgn)**|\u65f6\u5e8f\u56fe\u795e\u7ecf\u7f51\u7edc (TGNN) \u901a\u8fc7\u5c06\u65f6\u95f4\u4fe1\u606f\u7eb3\u5165\u57fa\u4e8e\u56fe\u7684\u64cd\u4f5c\uff0c\u4f18\u4e8e\u5e38\u89c4 GNN\u3002\u7136\u800c\uff0cTGNN \u91c7\u7528\u4e13\u95e8\u7684\u6a21\u578b\uff08\u4f8b\u5982 TGN\u3001TGAT \u548c APAN\uff09\u5e76\u4e14\u9700\u8981\u5b9a\u5236\u7684\u8bad\u7ec3\u6846\u67b6\uff08\u4f8b\u5982 TGL \u548c ETC\uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 TF-TGN\uff0c\u5b83\u4f7f\u7528 Transformer \u89e3\u7801\u5668\u4f5c\u4e3a TGNN \u7684\u9aa8\u5e72\u6a21\u578b\uff0c\u4ee5\u4eab\u53d7 Transformer \u7684\u4ee3\u7801\u5e93\u4ee5\u8fdb\u884c\u9ad8\u6548\u8bad\u7ec3\u3002\u7279\u522b\u662f\uff0cTransformer \u5728\u8bed\u8a00\u5efa\u6a21\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\uff0c\u56e0\u6b64\u793e\u533a\u5f00\u53d1\u4e86\u9ad8\u6027\u80fd\u5185\u6838\uff08\u4f8b\u5982\uff0c\u5feb\u901f\u6ce8\u610f\u529b\u673a\u5236\u548c\u5185\u5b58\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\uff09\u548c\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u65b9\u6848\uff08\u4f8b\u5982\uff0cPyTorch FSDP\u3001DeepSpeed \u548c Megatron-LM\uff09\u3002\u6211\u4eec\u89c2\u5bdf\u5230 TGNN \u7c7b\u4f3c\u4e8e\u8bed\u8a00\u5efa\u6a21\uff0c\u5373 TGNN \u4e2d\u6309\u65f6\u95f4\u987a\u5e8f\u51fa\u73b0\u7684\u8282\u70b9\u4e0e\u5176\u65f6\u95f4\u90bb\u5c45\u4e4b\u95f4\u7684\u6d88\u606f\u805a\u5408\u64cd\u4f5c\u53ef\u4ee5\u6784\u5efa\u4e3a\u5e8f\u5217\u5efa\u6a21\u3002\u9664\u4e86\u8fd9\u79cd\u76f8\u4f3c\u6027\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u7ed3\u5408\u4e86\u4e00\u7cfb\u5217\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u5305\u62ec\u540e\u7f00\u586b\u5145\u3001\u5e26\u6709\u81ea\u5faa\u73af\u7684\u65f6\u95f4\u56fe\u6ce8\u610f\u529b\u548c\u56e0\u679c\u63a9\u853d\u81ea\u6ce8\u610f\u529b\uff0c\u4ee5\u4f7f TF-TGN \u53d1\u6325\u4f5c\u7528\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u73b0\u6709\u7cfb\u7edf\u5728\u8f6c\u6362\u56fe\u62d3\u6251\u548c\u8fdb\u884c\u56fe\u91c7\u6837\u65b9\u9762\u901f\u5ea6\u8f83\u6162\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5e76\u884c\u5316 CSR \u683c\u5f0f\u8f6c\u6362\u548c\u56fe\u91c7\u6837\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u8fd8\u91c7\u7528 Transformer \u4ee3\u7801\u5e93\u6765\u4f7f\u7528\u591a\u4e2a GPU \u9ad8\u6548\u5730\u8bad\u7ec3 TF-TGN\u3002\u6211\u4eec\u7528 9 \u4e2a\u56fe\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5e76\u4e0e 2 \u4e2a\u6700\u5148\u8fdb\u7684 TGNN \u8bad\u7ec3\u6846\u67b6\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7ed3\u679c\u8868\u660e\uff0cTF-TGN \u53ef\u4ee5\u5c06\u8bad\u7ec3\u901f\u5ea6\u63d0\u9ad8 2.20 \u500d\u4ee5\u4e0a\uff0c\u540c\u65f6\u63d0\u4f9b\u4e0e\u73b0\u6709 SOTA TGNN \u76f8\u5f53\u751a\u81f3\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002TF-TGN \u4ee3\u7801\u53ef\u5728 https://github.com/qianghuangwhu/TF-TGN \u83b7\u53d6\u3002|\n", "2409.05207": "|**2024-09-08**|[Low Latency Transformer Inference on FPGAs for Physics Applications with hls4ml](http://arxiv.org/abs/2409.05207)|null|\u672c\u7814\u7a76\u5229\u7528hls4ml\uff0c\u5c55\u793a\u4e86\u5728\u73b0\u573a\u53ef\u7f16\u7a0b\u95e8\u9635\u5217\uff08FPGA\uff09\u4e2d\u9ad8\u6548\u5b9e\u73b0Transformer\u67b6\u6784\u7684\u65b9\u6848\u3002\u6211\u4eec\u9610\u8ff0\u4e86\u5b9e\u73b0\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u3001softmax\u548c\u5f52\u4e00\u5316\u5c42\u7684\u7b56\u7565\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u6a21\u578b\u3002\u5b83\u4eec\u5728VU13P FPGA\u82af\u7247\u4e0a\u7684\u90e8\u7f72\u5b9e\u73b0\u4e86\u4f4e\u4e8e2us\u7684\u5ef6\u8fdf\uff0c\u5c55\u73b0\u4e86\u5b9e\u65f6\u5e94\u7528\u7684\u6f5c\u529b\u3002HLS4ML\u4e0e\u4efb\u4f55\u57fa\u4e8eTensorFlow\u6784\u5efa\u7684Transformer\u6a21\u578b\u7684\u517c\u5bb9\u6027\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u8fd9\u9879\u5de5\u4f5c\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u7528\u6027\u3002\u5173\u952e\u8bcd\uff1aFPGA\uff0c\u673a\u5668\u5b66\u4e60\uff0cTransformer\uff0c\u9ad8\u80fd\u7269\u7406\uff0cLIGO|\n", "2409.05136": "|**2024-09-08**|[MHS-STMA: Multimodal Hate Speech Detection via Scalable Transformer-Based Multilevel Attention Framework](http://arxiv.org/abs/2409.05136)|null|\u793e\u4ea4\u5a92\u4f53\u5bf9\u4eba\u4eec\u7684\u751f\u6d3b\u4ea7\u751f\u4e86\u91cd\u5927\u5f71\u54cd\u3002\u4ec7\u6068\u8a00\u8bba\u5728\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u51fa\u73b0\u662f\u8fd1\u671f\u793e\u4f1a\u9762\u4e34\u7684\u6700\u4e25\u91cd\u95ee\u9898\u4e4b\u4e00\u3002\u6587\u672c\u548c\u56fe\u7247\u662f\u5728\u6587\u7ae0\u4e2d\u53d1\u5e03\u7684\u4e24\u79cd\u591a\u6a21\u6001\u6570\u636e\u5f62\u5f0f\u3002\u5355\u6a21\u6001\u5206\u6790\u4e00\u76f4\u662f\u65e9\u671f\u65b9\u6cd5\u7684\u4e3b\u8981\u91cd\u70b9\u3002\u6b64\u5916\uff0c\u5728\u8fdb\u884c\u591a\u6a21\u6001\u5206\u6790\u65f6\uff0c\u7814\u7a76\u4eba\u5458\u5ffd\u7565\u4e86\u4fdd\u7559\u4e0e\u6bcf\u79cd\u6a21\u6001\u76f8\u5173\u7684\u72ec\u7279\u54c1\u8d28\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u4ec7\u6068\u5185\u5bb9\u68c0\u6d4b\u7684\u53ef\u6269\u5c55\u67b6\u6784\uff0c\u79f0\u4e3a\u57fa\u4e8eTransformer\u7684\u591a\u7ea7\u6ce8\u610f\uff08STMA\uff09\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u7f3a\u70b9\u3002\u8be5\u67b6\u6784\u7531\u4e09\u4e2a\u4e3b\u8981\u90e8\u5206\u7ec4\u6210\uff1a\u57fa\u4e8e\u7ec4\u5408\u6ce8\u610f\u529b\u7684\u6df1\u5ea6\u5b66\u4e60\u673a\u5236\u3001\u89c6\u89c9\u6ce8\u610f\u529b\u673a\u5236\u7f16\u7801\u5668\u548c\u5b57\u5e55\u6ce8\u610f\u529b\u673a\u5236\u7f16\u7801\u5668\u3002\u4e3a\u4e86\u8bc6\u522b\u4ec7\u6068\u5185\u5bb9\uff0c\u6bcf\u4e2a\u7ec4\u4ef6\u4f7f\u7528\u5404\u79cd\u6ce8\u610f\u529b\u8fc7\u7a0b\u5e76\u72ec\u7279\u5730\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u3002\u51e0\u9879\u7814\u7a76\u91c7\u7528\u591a\u4e2a\u8bc4\u4f30\u6807\u51c6\u5bf9\u4e09\u4e2a\u4ec7\u6068\u8a00\u8bba\u6570\u636e\u96c6\uff08\u4ec7\u6068\u6a21\u56e0\u3001MultiOff \u548c MMHS150K\uff09\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u5b9e\u4e86\u6240\u63d0\u51fa\u7684\u4f53\u7cfb\u7ed3\u6784\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6240\u6709\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u51fa\u7684\u7b56\u7565\u7684\u6027\u80fd\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002|\n", "2409.04940": "|**2024-09-08**|[An Analog and Digital Hybrid Attention Accelerator for Transformers with Charge-based In-memory Computing](http://arxiv.org/abs/2409.04940)|null|\u6ce8\u610f\u529b\u673a\u5236\u662f Transformers \u7684\u5173\u952e\u8ba1\u7b97\u5185\u6838\uff0c\u7528\u4e8e\u8ba1\u7b97\u6574\u4e2a\u8f93\u5165\u5e8f\u5217\u7684\u6210\u5bf9\u76f8\u5173\u6027\u3002\u8ba1\u7b97\u81ea\u6ce8\u610f\u529b\u673a\u5236\u65f6\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u9891\u7e41\u7684\u5185\u5b58\u8bbf\u95ee\u7ed9\u7cfb\u7edf\u5e26\u6765\u4e86\u5de8\u5927\u7684\u8d1f\u62c5\uff0c\u5c24\u5176\u662f\u5728\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\u65f6\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u62df\u548c\u6570\u5b57\u6df7\u5408\u5904\u7406\u5668\uff0c\u4ee5\u52a0\u901f\u91c7\u7528 65 \u7eb3\u7c73 CMOS \u6280\u672f\u5b9e\u73b0\u7684 Transformers \u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u62df\u5185\u5b58\u8ba1\u7b97 (CIM) \u5185\u6838\uff0c\u5b83\u53ef\u4ee5\u5728\u8fd0\u884c\u65f6\u4ee5\u8d85\u4f4e\u529f\u8017\u548c\u5ef6\u8fdf\u5e73\u5747\u526a\u679d\u7ea6 75% \u7684\u4f4e\u5206\u503c\u6807\u8bb0\u3002\u6b64\u5916\uff0c\u6570\u5b57\u5904\u7406\u5668\u4ec5\u5bf9\u6a21\u62df CIM \u5185\u6838\u9009\u62e9\u7684\u7ea6 25% \u672a\u526a\u679d\u6807\u8bb0\u6267\u884c\u7cbe\u786e\u8ba1\u7b97\uff0c\u9632\u6b62\u7cbe\u5ea6\u4e0b\u964d\u3002\u6d4b\u91cf\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u62df\u5185\u6838\u548c\u7247\u4e0a\u7cfb\u7edf (SoC) \u7684\u5cf0\u503c\u80fd\u6548\u5206\u522b\u4e3a 14.8 \u548c 1.65 TOPS/W\uff0c\u5cf0\u503c\u9762\u79ef\u6548\u7387\u5206\u522b\u4e3a 976.6 \u548c 79.4 GOPS/mm$^\\mathrm{2}$\u3002|\n", "2409.04909": "|**2024-09-07**|[Efficient Training of Transformers for Molecule Property Prediction on Small-scale Datasets](http://arxiv.org/abs/2409.04909)|null|\u8840\u8111\u5c4f\u969c\uff08BBB\uff09\u662f\u4e00\u9053\u4fdd\u62a4\u6027\u5c4f\u969c\uff0c\u5c06\u5927\u8111\u4e0e\u5faa\u73af\u7cfb\u7edf\u9694\u5f00\uff0c\u8c03\u8282\u7269\u8d28\u8fdb\u5165\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf\u7684\u901a\u9053\u3002\u8bc4\u4f30\u6f5c\u5728\u836f\u7269\u7684BBB\u6e17\u900f\u6027\u5bf9\u4e8e\u6709\u6548\u7684\u836f\u7269\u9776\u5411\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684BBB\u6e17\u900f\u6027\u6d4b\u91cf\u5b9e\u9a8c\u65b9\u6cd5\u5177\u6709\u6311\u6218\u6027\uff0c\u5e76\u4e14\u5bf9\u4e8e\u5927\u89c4\u6a21\u7b5b\u9009\u6765\u8bf4\u4e0d\u5207\u5b9e\u9645\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u8ba1\u7b97\u65b9\u6cd5\u6765\u9884\u6d4bBBB\u6e17\u900f\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684GPS Transformer\u67b6\u6784\uff0c\u65e8\u5728\u5728\u4f4e\u6570\u636e\u60c5\u51b5\u4e0b\u8868\u73b0\u826f\u597d\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4f7f\u7528BBBP\u6570\u636e\u96c6\u7684BBB\u6e17\u900f\u6027\u9884\u6d4b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u7684ROC-AUC\u4e3a78.8%\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u6c34\u5e73\u63d0\u9ad8\u4e865.5%\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u6807\u51c6\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e0eGPS Transformer\u7ed3\u5408\u4f7f\u7528\u6bd4\u5176\u4ed6\u6ce8\u610f\u529b\u673a\u5236\u53d8\u4f53\u4e0eGPS Transformer\u7ed3\u5408\u4f7f\u7528\u8868\u73b0\u66f4\u597d\u3002|\n", "2409.04803": "|**2024-09-07**|[Cross-attention Inspired Selective State Space Models for Target Sound Extraction](http://arxiv.org/abs/2409.04803)|null|Transformer\u6a21\u578b\uff0c\u7279\u522b\u662f\u5176\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u76ee\u6807\u58f0\u97f3\u63d0\u53d6\u4e2d\u7684\u7279\u5f81\u878d\u5408\uff0c\u8be5\u4efb\u52a1\u57fa\u4e8e\u7ed9\u5b9a\u7684\u7ebf\u7d22\u63d0\u53d6\u611f\u5174\u8da3\u7684\u4fe1\u53f7\u3002\u5c3d\u7ba1\u6709\u6548\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\u8f83\u4f4e\u3002\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u6700\u8fd1\u7684Mamba\u6a21\u578b\uff0c\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e0e\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u7136\u800c\uff0c\u7531\u4e8eMamba\u65e0\u6cd5\u50cf\u4ea4\u53c9\u6ce8\u610f\u529b\u90a3\u6837\u6355\u6349\u4e0d\u540c\u5e8f\u5217\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u56e0\u6b64\u5b83\u5728\u76ee\u6807\u58f0\u97f3\u63d0\u53d6\u4e2d\u7684\u9002\u7528\u6027\u53d7\u5230\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7528\u4e8e\u76ee\u6807\u58f0\u97f3\u63d0\u53d6\u7684CrossMamba\u6a21\u578b\uff0c\u5b83\u5229\u7528Mamba\u7684\u9690\u85cf\u6ce8\u610f\u529b\u673a\u5236\u6765\u8ba1\u7b97\u7ed9\u5b9a\u7ebf\u7d22\u548c\u97f3\u9891\u6df7\u5408\u7269\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002Mamba\u7684\u8ba1\u7b97\u53ef\u4ee5\u5206\u4e3a\u67e5\u8be2\u3001\u952e\u548c\u503c\u3002\u6211\u4eec\u5229\u7528\u7ebf\u7d22\u751f\u6210\u67e5\u8be2\uff0c\u5e76\u5229\u7528\u97f3\u9891\u6df7\u5408\u7269\u5bfc\u51fa\u952e\u548c\u503c\uff0c\u9075\u5faaTransformer\u4e2d\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u7684\u539f\u7406\u3002\u6765\u81ea\u4e24\u79cd\u5177\u6709\u4ee3\u8868\u6027\u7684\u76ee\u6807\u58f0\u97f3\u63d0\u53d6\u65b9\u6cd5\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684CrossMamba\u7684\u6709\u6548\u6027\u3002|\n", "2409.04431": "|**2024-09-06**|[Theory, Analysis, and Best Practices for Sigmoid Self-Attention](http://arxiv.org/abs/2409.04431)|null|\u6ce8\u610f\u529b\u662f Transformer \u67b6\u6784\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002\u5b83\u662f\u4e00\u79cd\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u6620\u5c04\uff0c\u5c06\u6bcf\u4e2a\u5e8f\u5217\u5143\u7d20\u8f6c\u6362\u4e3a\u503c\u7684\u52a0\u6743\u548c\u3002\u6743\u91cd\u901a\u5e38\u662f\u901a\u8fc7\u952e\u548c\u67e5\u8be2\u4e4b\u95f4\u7684\u70b9\u79ef\u7684 softmax \u83b7\u5f97\u7684\u3002\u6700\u8fd1\u7684\u5de5\u4f5c\u63a2\u7d22\u4e86 Transformer \u4e2d softmax \u6ce8\u610f\u529b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f8b\u5982 ReLU \u548c sigmoid \u6fc0\u6d3b\u51fd\u6570\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6 sigmoid \u6ce8\u610f\u529b\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u6df1\u5165\u7684\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u3002\u7406\u8bba\u4e0a\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5177\u6709 sigmoid \u6ce8\u610f\u529b\u7684 Transformer \u662f\u901a\u7528\u51fd\u6570\u903c\u8fd1\u5668\uff0c\u5e76\u4e14\u4e0e softmax \u6ce8\u610f\u529b\u76f8\u6bd4\uff0c\u5177\u6709\u66f4\u597d\u7684\u6b63\u5219\u6027\u3002\u901a\u8fc7\u8be6\u7ec6\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u5728\u8bad\u7ec3\u7684\u65e9\u671f\u9636\u6bb5\u7a33\u5b9a\u8f83\u5927\u7684\u521d\u59cb\u6ce8\u610f\u529b\u8303\u6570\u662f\u6210\u529f\u8bad\u7ec3\u5177\u6709 sigmoid \u6ce8\u610f\u529b\u6a21\u578b\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u5148\u524d\u7684\u5c1d\u8bd5\u3002\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86 FLASHSIGMOID\uff0c\u8fd9\u662f\u4e00\u79cd\u786c\u4ef6\u611f\u77e5\u4e14\u5185\u5b58\u9ad8\u6548\u7684 sigmoid \u6ce8\u610f\u529b\u5b9e\u73b0\uff0c\u5728 H100 GPU \u4e0a\uff0c\u5176\u63a8\u7406\u5185\u6838\u901f\u5ea6\u6bd4 FLASHATTENTION2 \u63d0\u9ad8\u4e86 17%\u3002\u8de8\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u8bed\u97f3\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ecf\u8fc7\u9002\u5f53\u6807\u51c6\u5316\u7684 sigmoid \u6ce8\u610f\u529b\u5728\u5e7f\u6cdb\u7684\u9886\u57df\u548c\u89c4\u6a21\u4e0a\u4e0e softmax \u6ce8\u610f\u529b\u7684\u5f3a\u5927\u6027\u80fd\u76f8\u5339\u914d\uff0c\u8fd9\u662f\u5148\u524d\u5c1d\u8bd5 sigmoid \u6ce8\u610f\u529b\u6240\u65e0\u6cd5\u5b8c\u5168\u5b9e\u73b0\u7684\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u7edf\u4e00\u4e86\u73b0\u6709\u6280\u672f\uff0c\u5e76\u4e3a sigmoid \u6ce8\u610f\u529b\u4f5c\u4e3a Transformer \u4e2d softmax \u7684\u76f4\u63a5\u66ff\u4ee3\u54c1\u5efa\u7acb\u4e86\u6700\u4f73\u5b9e\u8df5\u3002|\n", "2409.04275": "|**2024-09-09**|[AttentionX: Exploiting Consensus Discrepancy In Attention from A Distributed Optimization Perspective](http://arxiv.org/abs/2409.04275)|null|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ece\u5206\u5e03\u5f0f\u4f18\u5316\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u5229\u7528\u5171\u8bc6\u5dee\u5f02\u6765\u6269\u5c55Transformer\u4e2d\u7684\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3aAttentionX\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4e58\u5b50\u4ea4\u66ff\u65b9\u5411\u6cd5\uff08PDMM\uff09\\cite{Zhang16PDMM}\u65e8\u5728\u8fed\u4ee3\u5730\u89e3\u51b3\u70b9\u5bf9\u70b9\uff08P2P\uff09\u7f51\u7edc\u4e0a\u7684\u4e00\u5927\u7c7b\u5206\u5e03\u5f0f\u4f18\u5316\u95ee\u9898\uff0c\u5176\u4e2d\u76f8\u90bb\u8282\u70b9\u6839\u636e\u4f18\u5316\u8fc7\u7a0b\u4e2d\u9884\u5b9a\u4e49\u7684\u7ebf\u6027\u8fb9\u7ea6\u675f\u9010\u6e10\u8fbe\u6210\u5171\u8bc6\u3002\u7279\u522b\u662f\u5728PDMM\u7684\u6bcf\u6b21\u8fed\u4ee3\u4e2d\uff0c\u7f51\u7edc\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u9996\u5148\u4ece\u90bb\u5c45\u8282\u70b9\u6536\u96c6\u4fe1\u606f\uff0c\u7136\u540e\u6267\u884c\u672c\u5730\u4fe1\u606f\u878d\u5408\u3002\u4ece\u9ad8\u5c42\u6b21\u6765\u770b\uff0c\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u57fa\u4e8e$KQ$-softmax\u7684$V$\u8868\u793a\u52a0\u6743\u6c42\u548c\u5bf9\u5e94\u4e8e\u4ece\u90bb\u5c45\u8282\u70b9\u6536\u96c6\u4fe1\u606f\uff0c\u800cTransformer\u4e2d\u901a\u8fc7\u524d\u9988\u7f51\u7edc\uff08FFN\uff09\u8fdb\u884c\u7684\u7279\u5f81\u5904\u7406\u5bf9\u5e94\u4e8e\u672c\u5730\u4fe1\u606f\u878d\u5408\u3002PDMM\u5229\u7528\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u4ee5\u7ebf\u6027\u8fb9\u7ea6\u675f\u7684\u6b8b\u5dee\u5f62\u5f0f\u6355\u83b7\u5386\u53f2\u5171\u8bc6\u5dee\u5f02\uff0c\u8fd9\u5bf9\u4e8e\u7b97\u6cd5\u7684\u6536\u655b\u81f3\u5173\u91cd\u8981\u3002\u53d7PDMM\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86AttentionX\uff0c\u5c06\u5171\u8bc6\u5dee\u5f02\u7eb3\u5165\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\u7684\u8f93\u51fa\u66f4\u65b0\u8868\u8fbe\u5f0f\u4e2d\u3002AttentionX\u4e2d\u7684\u5171\u8bc6\u5dee\u5f02\u662f\u6307$V$\u8868\u793a\u7684\u52a0\u6743\u6c42\u548c\u4e0e\u5176\u7f29\u653e\u540e\u7684$V$\u8868\u793a\u672c\u8eab\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u5728ViT\u548cnanoGPT\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u4e86\u5176\u826f\u597d\u7684\u6027\u80fd\u3002|\n"}}