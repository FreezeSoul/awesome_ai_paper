{"\u591a\u6a21\u6001": {"2406.10228": "|**2024-06-14**|**VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models**|[2406.10228](http://arxiv.org/abs/2406.10228)|null|**\u591a\u6a21\u6001\u5927\u578b\u6a21\u578b (MLLM) \u7684\u5feb\u901f\u53d1\u5c55\u5c55\u73b0\u4e86\u5176\u5728\u5904\u7406\u89c6\u89c9\u548c\u8bed\u8a00\u878d\u5408\u4efb\u52a1\u65b9\u9762\u7684\u60ca\u4eba\u80fd\u529b\u3002\u7136\u800c\uff0c\u76ee\u524d\u5927\u591a\u6570\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u90fd\u5c40\u9650\u4e8e\u89c6\u89c9\u548c\u6587\u672c\u4e0a\u4e0b\u6587\u8303\u56f4\u8f83\u7a84\u7684\u573a\u666f\u3002\u5f53\u9762\u5bf9\u590d\u6742\u7684\u7406\u89e3\u4efb\u52a1\u65f6\uff0c\u8fd9\u4e9b\u6a21\u578b\u5f80\u5f80\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u4efb\u52a1\u9700\u8981\u5728\u5927\u91cf\u4e0d\u76f8\u5173\u7684\u3001\u53ef\u80fd\u5177\u6709\u8bef\u5bfc\u6027\u7684\u6587\u672c\u548c\u56fe\u50cf\u4fe1\u606f\u4e2d\u8fdb\u884c\u5bfc\u822a\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u9879\u65b0\u7684\u3001\u8981\u6c42\u66f4\u9ad8\u7684\u4efb\u52a1\uff0c\u79f0\u4e3a\u4ea4\u9519\u56fe\u6587\u7406\u89e3 (IITC)\u3002\u8fd9\u9879\u4efb\u52a1\u8981\u6c42\u6a21\u578b\u8fa8\u522b\u5e76\u5ffd\u7565\u56fe\u50cf\u548c\u6587\u672c\u4e2d\u7684\u591a\u4f59\u5143\u7d20\uff0c\u4ee5\u51c6\u786e\u56de\u7b54\u95ee\u9898\uff0c\u5e76\u9075\u5faa\u590d\u6742\u7684\u6307\u4ee4\u6765\u7cbe\u786e\u5b9a\u4f4d\u76f8\u5173\u56fe\u50cf\u3002\u4e3a\u4e86\u652f\u6301\u8fd9\u9879\u4efb\u52a1\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684 VEGA \u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u4e13\u4e3a\u79d1\u5b66\u5185\u5bb9\u7684 IITC \u4efb\u52a1\u800c\u5b9a\u5236\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5b50\u4efb\u52a1\uff0c\u5373\u56fe\u6587\u5173\u8054 (ITA)\uff0c\u4ee5\u5b8c\u5584\u56fe\u50cf-\u6587\u672c\u5173\u8054\u6280\u80fd\u3002\u6211\u4eec\u5bf9\u56db\u4e2a\u9886\u5148\u7684\u95ed\u6e90\u6a21\u578b\u4ee5\u53ca\u4f7f\u7528 VEGA \u7684\u5404\u79cd\u5f00\u6e90\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u7a81\u51fa\u4e86 IITC \u4efb\u52a1\u7684\u4e25\u82db\u6027\u3002\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5982 Gemini-1.5-pro \u548c GPT4V\uff0c\u4e5f\u53ea\u53d6\u5f97\u4e86\u6709\u9650\u7684\u6210\u529f\u3002\u901a\u8fc7\u91c7\u7528\u591a\u4efb\u52a1\u3001\u591a\u5c3a\u5ea6\u7684\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u6211\u4eec\u5728 IITC \u4efb\u52a1\u4e0a\u4e3a MLLM  \u5efa\u7acb\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u57fa\u7ebf\uff0c\u5728\u56fe\u50cf\u5173\u8054\u65b9\u9762\u8fbe\u5230\u4e86 85.8% \u7684\u51c6\u786e\u7387\uff0cRouge \u5f97\u5206\u8fbe\u5230\u4e86 0.508\u3002\u8fd9\u4e9b\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u6570\u636e\u96c6\u5728\u63d0\u9ad8 MLLM  \u5bf9\u7ec6\u5fae\u56fe\u6587\u7406\u89e3\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002 \n**|\n", "2406.10227": "|**2024-06-14**|**VideoGUI: A Benchmark for GUI Automation from Instructional Videos**|[2406.10227](http://arxiv.org/abs/2406.10227)|null|**\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u81ea\u52a8\u5316\u901a\u8fc7\u534f\u52a9\u5b8c\u6210\u8ba1\u7b97\u673a\u4efb\u52a1\uff0c\u4e3a\u63d0\u9ad8\u4eba\u7c7b\u751f\u4ea7\u529b\u5e26\u6765\u4e86\u5de8\u5927\u7684\u5e0c\u671b\u3002\u73b0\u6709\u7684\u4efb\u52a1\u5236\u5b9a\u4e3b\u8981\u96c6\u4e2d\u4e8e\u53ef\u4ee5\u901a\u8fc7\u5355\u4e00\u8bed\u8a00\u6307\u4ee4\u6307\u5b9a\u7684\u7b80\u5355\u4efb\u52a1\uff0c\u4f8b\u5982\u201c\u63d2\u5165\u65b0\u5e7b\u706f\u7247\u201d\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 VideoGUI\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30 GUI \u52a9\u624b\u5728\u4ee5\u89c6\u89c9\u4e3a\u4e2d\u5fc3\u7684 GUI \u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u6e90\u81ea\u9ad8\u8d28\u91cf\u7684\u7f51\u7edc\u6559\u5b66\u89c6\u9891\uff0c\u4e13\u6ce8\u4e8e\u6d89\u53ca\u4e13\u4e1a\u548c\u65b0\u9896\u8f6f\u4ef6\uff08\u4f8b\u5982\uff0cAdobe Photoshop \u6216 Stable Diffusion WebUI\uff09\u4ee5\u53ca\u590d\u6742\u6d3b\u52a8\uff08\u4f8b\u5982\uff0c\u89c6\u9891\u7f16\u8f91\uff09\u7684\u4efb\u52a1\u3002VideoGUI \u901a\u8fc7\u5206\u5c42\u6d41\u7a0b\u8bc4\u4f30 GUI \u52a9\u624b\uff0c\u5141\u8bb8\u8bc6\u522b\u5b83\u4eec\u53ef\u80fd\u5931\u8d25\u7684\u7279\u5b9a\u7ea7\u522b\uff1a\uff08i\uff09\u9ad8\u7ea7\u89c4\u5212\uff1a\u5728\u6ca1\u6709\u8bed\u8a00\u63cf\u8ff0\u7684\u60c5\u51b5\u4e0b\uff0c\u6839\u636e\u89c6\u89c9\u6761\u4ef6\u91cd\u5efa\u7a0b\u5e8f\u6027\u5b50\u4efb\u52a1\uff1b\uff08ii\uff09\u4e2d\u7ea7\u89c4\u5212\uff1a\u6839\u636e\u89c6\u89c9\u72b6\u6001\uff08\u5373\u5c4f\u5e55\u622a\u56fe\uff09\u548c\u76ee\u6807\u751f\u6210\u7cbe\u786e\u52a8\u4f5c\u53d9\u8ff0\u7684\u5e8f\u5217\uff1b\uff08iii\uff09\u539f\u5b50\u52a8\u4f5c\u6267\u884c\uff1a\u6267\u884c\u7279\u5b9a\u52a8\u4f5c\uff0c\u4f8b\u5982\u51c6\u786e\u5355\u51fb\u6307\u5b9a\u5143\u7d20\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u7ea7\u522b\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u8de8\u5404\u4e2a\u7ef4\u5ea6\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u63d0\u4f9b\u6e05\u6670\u7684\u4fe1\u53f7\uff0c\u4f8b\u5982\u539f\u5b50\u52a8\u4f5c\u6267\u884c\u4e2d\u5355\u51fb\u3001\u62d6\u52a8\u3001\u952e\u5165\u548c\u6eda\u52a8\u7684\u4e2a\u4f53\u6027\u80fd\u3002\u6211\u4eec\u5bf9 VideoGUI \u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b GPT4o \u5728\u4ee5\u89c6\u89c9\u4e3a\u4e2d\u5fc3\u7684 GUI \u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7ea7\u89c4\u5212\u65b9\u9762\u3002**|\n", "2406.10221": "|**2024-06-14**|**Short Film Dataset (SFD): A Benchmark for Story-Level Video Understanding**|[2406.10221](http://arxiv.org/abs/2406.10221)|null|**\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\u6781\u5927\u5730\u63a8\u52a8\u4e86\u89c6\u9891\u7406\u89e3\u7684\u53d1\u5c55\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u6570\u636e\u96c6\u548c\u4efb\u52a1\u5b58\u5728\u663e\u8457\u7684\u5c40\u9650\u6027\u3002\u5927\u591a\u6570\u6570\u636e\u96c6\u4ec5\u9650\u4e8e\u4e8b\u4ef6\u6709\u9650\u3001\u53d9\u8ff0\u8303\u56f4\u72ed\u7a84\u7684\u77ed\u89c6\u9891\u3002\u4f8b\u5982\uff0c\u5305\u542b\u6559\u5b66\u548c\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u7684\u6570\u636e\u96c6\u901a\u5e38\u53ea\u8bb0\u5f55\u4e00\u4e2a\u4eba\u5728\u5355\u4e2a\u573a\u666f\u4e2d\u7684\u6d3b\u52a8\u3002\u5c3d\u7ba1\u4e00\u4e9b\u7535\u5f71\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u5185\u5bb9\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u4ec5\u9650\u4e8e\u77ed\u671f\u4efb\u52a1\uff0c\u7f3a\u4e4f\u516c\u5f00\u53ef\u7528\u7684\u89c6\u9891\uff0c\u5e76\u4e14\u7531\u4e8e\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u4f7f\u7528\u4e86\u7535\u5f71\u8bba\u575b\u548c\u5176\u4ed6\u8d44\u6e90\uff0c\u56e0\u6b64\u7ecf\u5e38\u9047\u5230\u6570\u636e\u6cc4\u9732\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5305\u542b 1,078 \u90e8\u516c\u5f00\u53ef\u7528\u7684\u4e1a\u4f59\u7535\u5f71\u7684\u77ed\u7247\u6570\u636e\u96c6 (SFD)\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u5404\u79cd\u7c7b\u578b\uff0c\u5e76\u4e14\u6570\u636e\u6cc4\u9732\u95ee\u9898\u6781\u5c11\u3002SFD \u4ee5\u591a\u9879\u9009\u62e9\u548c\u5f00\u653e\u5f0f\u95ee\u7b54\u7684\u5f62\u5f0f\u63d0\u4f9b\u9762\u5411\u957f\u671f\u6545\u4e8b\u7684\u89c6\u9891\u4efb\u52a1\u3002\u6211\u4eec\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u5f3a\u8c03\u4e86\u89e3\u51b3 SFD \u4efb\u52a1\u9700\u8981\u8fdb\u884c\u957f\u671f\u63a8\u7406\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u53d1\u73b0\u7535\u5f71\u811a\u672c\u4e2d\u5b58\u5728\u5f3a\u70c8\u7684\u4fe1\u53f7\uff0c\u5bfc\u81f4\u4eba\u7c7b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u76f8\u5f53\u3002\u6211\u4eec\u8fd8\u53d1\u73b0\uff0c\u4e0e\u4ec5\u4f7f\u7528\u89c6\u89c9\u6570\u636e\u76f8\u6bd4\uff0c\u5f53\u524d\u6a21\u578b\u7684\u6027\u80fd\u660e\u663e\u4f4e\u4e8e\u4eba\u7c7b\u3002 \n**|\n", "2406.10215": "|**2024-06-14**|**DevBench: A multimodal developmental benchmark for language learning**|[2406.10215](http://arxiv.org/abs/2406.10215)|null|**\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u513f\u7ae5\u7684\u5b66\u4e60\u8f68\u8ff9\u6709\u591a\u4e48\u76f8\u4f3c\uff08\u6216\u4e0d\u540c\uff09\uff1f\u6700\u8fd1\u7684\u5efa\u6a21\u5de5\u4f5c\u8bd5\u56fe\u901a\u8fc7\u6784\u5efa\u4f7f\u7528\u66f4\u5c11\u6570\u636e\uff08\u5c24\u5176\u662f\u591a\u6a21\u6001\u81ea\u7136\u6570\u636e\uff09\u8bad\u7ec3\u7684\u6a21\u578b\u6765\u7406\u89e3\u6a21\u578b\u548c\u4eba\u7c7b\u6570\u636e\u6548\u7387\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u7136\u800c\uff0c\u6b64\u7c7b\u6a21\u578b\u901a\u5e38\u5728\u6210\u4eba\u6c34\u5e73\u7684\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u6d4b\u8bd5\u7684\u8bed\u8a00\u80fd\u529b\u5e7f\u5ea6\u6709\u9650\uff0c\u5e76\u4e14\u6ca1\u6709\u4e0e\u884c\u4e3a\u6570\u636e\u8fdb\u884c\u76f4\u63a5\u6bd4\u8f83\u3002\u6211\u4eec\u5f15\u5165\u4e86 DevBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e03\u9879\u8bed\u8a00\u8bc4\u4f30\u4efb\u52a1\uff0c\u6db5\u76d6\u8bcd\u6c47\u3001\u53e5\u6cd5\u548c\u8bed\u4e49\u80fd\u529b\u9886\u57df\uff0c\u4ee5\u53ca\u513f\u7ae5\u548c\u6210\u4eba\u7684\u884c\u4e3a\u6570\u636e\u3002\u6211\u4eec\u9488\u5bf9\u8fd9\u4e9b\u4efb\u52a1\u8bc4\u4f30\u4e86\u4e00\u7ec4\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4e0d\u4ec5\u6bd4\u8f83\u4e86\u6a21\u578b\u548c\u4eba\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u6bd4\u8f83\u4e86\u4ed6\u4eec\u7684\u54cd\u5e94\u6a21\u5f0f\u3002\u5728\u5404\u9879\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u5728\u63a5\u8fd1\u4eba\u7c7b\u54cd\u5e94\u6a21\u5f0f\u65b9\u9762\u8868\u73b0\u51fa\u5dee\u5f02\uff0c\u5e76\u4e14\u5728\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\u7684\u6a21\u578b\u4e5f\u66f4\u63a5\u8fd1\u4eba\u7c7b\u884c\u4e3a\u54cd\u5e94\u3002\u6211\u4eec\u8fd8\u68c0\u67e5\u4e86 OpenCLIP \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u53d1\u5c55\u8f68\u8ff9\uff0c\u53d1\u73b0\u66f4\u591a\u7684\u8bad\u7ec3\u4f1a\u4f7f\u5176\u66f4\u63a5\u8fd1\u6210\u4eba\u54cd\u5e94\u6a21\u5f0f\u3002\u56e0\u6b64\uff0cDevBench \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7528\u4e8e\u6bd4\u8f83\u6a21\u578b\u4e0e\u4eba\u7c7b\u8bed\u8a00\u53d1\u5c55\u7684\u57fa\u51c6\u3002\u8fd9\u4e9b\u6bd4\u8f83\u7a81\u51fa\u4e86\u6a21\u578b\u548c\u4eba\u7c7b\u8bed\u8a00\u5b66\u4e60\u8fc7\u7a0b\u7684\u4e0d\u540c\u4e4b\u5904\uff0c\u4e3a\u4e86\u89e3\u6539\u8fdb\u8bed\u8a00\u6a21\u578b\u7684\u5207\u5165\u70b9\u63d0\u4f9b\u4e86\u601d\u8def\u3002 \n**|\n", "2406.10185": "|**2024-06-14**|**Detecting and Evaluating Medical Hallucinations in Large Vision Language Models**|[2406.10185](http://arxiv.org/abs/2406.10185)|null|**\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u5728\u533b\u7597\u5e94\u7528\u4e2d\u8d8a\u6765\u8d8a\u4e0d\u53ef\u6216\u7f3a\uff0c\u5305\u62ec\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u548c\u5f71\u50cf\u62a5\u544a\u751f\u6210\u3002\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u7ee7\u627f\u4e86\u57fa\u7840\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u5f3a\u5927\u529f\u80fd\uff0c\u4f46\u5b83\u4eec\u4e5f\u7ee7\u627f\u4e86\u6613\u4ea7\u751f\u5e7b\u89c9\u7684\u503e\u5411\uff0c\u8fd9\u5728\u5bb9\u9519\u7387\u6781\u4f4e\u7684\u533b\u7597\u73af\u5883\u4e2d\u662f\u4e00\u4e2a\u91cd\u5927\u95ee\u9898\u3002\u7136\u800c\uff0c\u76ee\u524d\u8fd8\u6ca1\u6709\u4e13\u95e8\u9488\u5bf9\u533b\u5b66\u9886\u57df\u5e7b\u89c9\u68c0\u6d4b\u548c\u8bc4\u4f30\u7684\u65b9\u6cd5\u6216\u57fa\u51c6\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86 Med-HallMark\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4e13\u95e8\u4e3a\u533b\u5b66\u591a\u6a21\u6001\u9886\u57df\u5185\u7684\u5e7b\u89c9\u68c0\u6d4b\u548c\u8bc4\u4f30\u800c\u8bbe\u8ba1\u7684\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u63d0\u4f9b\u4e86\u591a\u4efb\u52a1\u5e7b\u89c9\u652f\u6301\u3001\u591a\u65b9\u9762\u7684\u5e7b\u89c9\u6570\u636e\u548c\u5206\u5c42\u7684\u5e7b\u89c9\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 MediHall Score\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u533b\u5b66\u8bc4\u4f30\u6307\u6807\uff0c\u65e8\u5728\u901a\u8fc7\u5206\u5c42\u8bc4\u5206\u7cfb\u7edf\u8bc4\u4f30 LVLM \u7684\u5e7b\u89c9\uff0c\u8be5\u7cfb\u7edf\u8003\u8651\u4e86\u5e7b\u89c9\u7684\u4e25\u91cd\u7a0b\u5ea6\u548c\u7c7b\u578b\uff0c\u4ece\u800c\u80fd\u591f\u5bf9\u6f5c\u5728\u7684\u4e34\u5e8a\u5f71\u54cd\u8fdb\u884c\u7ec6\u81f4\u7684\u8bc4\u4f30\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86 MediHallDetector\uff0c\u8fd9\u662f\u4e00\u79cd\u4e3a\u7cbe\u786e\u68c0\u6d4b\u5e7b\u89c9\u800c\u8bbe\u8ba1\u7684\u65b0\u578b\u533b\u5b66 LVLM\uff0c\u5b83\u91c7\u7528\u591a\u4efb\u52a1\u8bad\u7ec3\u8fdb\u884c\u5e7b\u89c9\u68c0\u6d4b\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u6211\u4eec\u4f7f\u7528\u6211\u4eec\u7684\u57fa\u51c6\u4e3a\u6d41\u884c\u7684 LVLM \u5efa\u7acb\u4e86\u57fa\u7ebf\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u6307\u6807\u76f8\u6bd4\uff0cMediHall Score \u5bf9\u5e7b\u89c9\u5f71\u54cd\u63d0\u4f9b\u4e86\u66f4\u7ec6\u81f4\u7684\u7406\u89e3\uff0c\u5e76\u8bc1\u660e\u4e86 MediHallDetector \u7684\u589e\u5f3a\u6027\u80fd\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u9879\u5de5\u4f5c\u80fd\u591f\u663e\u7740\u63d0\u9ad8 LVLM \u5728\u533b\u7597\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u7684\u6240\u6709\u8d44\u6e90\u5c06\u5f88\u5feb\u53d1\u5e03\u3002 \n**|\n", "2406.10165": "|**2024-06-14**|**CarLLaVA: Vision language models for camera-only closed-loop driving**|[2406.10165](http://arxiv.org/abs/2406.10165)|null|**In this technical report, we present CarLLaVA, a Vision Language Model (VLM) for autonomous driving, developed for the CARLA Autonomous Driving Challenge 2.0. CarLLaVA uses the vision encoder of the LLaVA VLM and the LLaMA architecture as backbone, achieving state-of-the-art closed-loop driving performance with only camera input and without the need for complex or expensive labels. Additionally, we show preliminary results on predicting language commentary alongside the driving output. CarLLaVA uses a semi-disentangled output representation of both path predictions and waypoints, getting the advantages of the path for better lateral control and the waypoints for better longitudinal control. We propose an efficient training recipe to train on large driving datasets without wasting compute on easy, trivial data. CarLLaVA ranks 1st place in the sensor track of the CARLA Autonomous Driving Challenge 2.0 outperforming the previous state of the art by 458% and the best concurrent submission by 32.6%.**|\n", "2406.10157": "|**2024-06-14**|**RoboGolf: Mastering Real-World Minigolf with a Reflective Multi-Modality Vision-Language Model**|[2406.10157](http://arxiv.org/abs/2406.10157)|null|**Minigolf, a game with countless court layouts, and complex ball motion, constitutes a compelling real-world testbed for the study of embodied intelligence. As it not only challenges spatial and kinodynamic reasoning but also requires reflective and corrective capacities to address erroneously designed courses. We introduce RoboGolf, a framework that perceives dual-camera visual inputs with nested VLM-empowered closed-loop control and reflective equilibrium loop. Extensive experiments demonstrate the effectiveness of RoboGolf on challenging minigolf courts including those that are impossible to finish.**|\n", "2406.10100": "|**2024-06-14**|**SkySenseGPT: A Fine-Grained Instruction Tuning Dataset and Model for Remote Sensing Vision-Language Understanding**|[2406.10100](http://arxiv.org/abs/2406.10100)|**[link](https://github.com/luo-z13/skysensegpt)**|**Remote Sensing Large Multi-Modal Models (RSLMMs) are developing rapidly and showcase significant capabilities in remote sensing imagery (RSI) comprehension. However, due to the limitations of existing datasets, RSLMMs have shortcomings in understanding the rich semantic relations among objects in complex remote sensing scenes. To unlock RSLMMs' complex comprehension ability, we propose a large-scale instruction tuning dataset FIT-RS, containing 1,800,851 instruction samples. FIT-RS covers common interpretation tasks and innovatively introduces several complex comprehension tasks of escalating difficulty, ranging from relation reasoning to image-level scene graph generation. Based on FIT-RS, we build the FIT-RSFG benchmark. Furthermore, we establish a new benchmark to evaluate the fine-grained relation comprehension capabilities of LMMs, named FIT-RSRC. Based on combined instruction data, we propose SkySenseGPT, which achieves outstanding performance on both public datasets and FIT-RSFG, surpassing existing RSLMMs. We hope the FIT-RS dataset can enhance the relation comprehension capability of RSLMMs and provide a large-scale fine-grained data source for the remote sensing community. The dataset will be available at https://github.com/Luo-Z13/SkySenseGPT**|\n", "2406.10057": "|**2024-06-14**|**First Multi-Dimensional Evaluation of Flowchart Comprehension for Multimodal Large Language Models**|[2406.10057](http://arxiv.org/abs/2406.10057)|null|**\u968f\u7740\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u7efc\u5408\u80fd\u529b\u65e5\u76ca\u5f3a\u5927\u3002\u4e3a\u4e86\u8bc4\u4f30MLLM\u7684\u5404\u79cd\u80fd\u529b\uff0c\u6d8c\u73b0\u51fa numerous \u8bc4\u4f30\u7cfb\u7edf\u3002\u4f46\u76ee\u524d\u4ecd\u7f3a\u4e4f\u4e00\u79cd\u5168\u9762\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30MLLM\u5728\u4e0e\u6d41\u7a0b\u56fe\u76f8\u5173\u7684\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u800c\u6d41\u7a0b\u56fe\u5728\u65e5\u5e38\u751f\u6d3b\u548c\u5de5\u4f5c\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u7efc\u5408\u6027\u65b9\u6cd5FlowCE\uff0c\u7528\u4e8e\u4ece\u591a\u4e2a\u7ef4\u5ea6\u8bc4\u4f30MLLM\u5728\u4e0e\u6d41\u7a0b\u56fe\u76f8\u5173\u7684\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002\u5b83\u6db5\u76d6\u4e86\u5bf9MLLM\u5728\u6d41\u7a0b\u56fe\u4e0a\u7684\u63a8\u7406\u3001\u5b9a\u4f4d\u8bc6\u522b\u3001\u4fe1\u606f\u63d0\u53d6\u3001\u903b\u8f91\u9a8c\u8bc1\u548c\u603b\u7ed3\u7b49\u65b9\u9762\u7684\u80fd\u529b\u8bc4\u4f30\u3002\u7136\u800c\uff0c\u6211\u4eec\u53d1\u73b0\u5373\u4f7f\u662fGPT4o\u6a21\u578b\u4e5f\u53ea\u83b7\u5f97\u4e8656.63\u5206\u3002\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\uff0cPhi-3-Vision\u83b7\u5f97\u4e86\u6700\u9ad8\u768449.97\u5206\u3002\u6211\u4eec\u5e0c\u671bFlowCE\u80fd\u591f\u4e3a\u672a\u6765\u57fa\u4e8e\u6d41\u7a0b\u56fe\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7814\u7a76\u505a\u51fa\u8d21\u732e\u3002\u6211\u4eec\u5c06\u5f00\u6e90\u8be5\u9879\u76ee\uff1a\\url{https://github.com/360AILAB-NLP/FlowCE} \n**|\n", "2406.09988": "|**2024-06-14**|**Details Make a Difference: Object State-Sensitive Neurorobotic Task Planning**|[2406.09988](http://arxiv.org/abs/2406.09988)|**[link](https://github.com/xiao-wen-sun/ossa)**|**\u4e00\u4e2a\u7269\u4f53\u7684\u72b6\u6001\u53cd\u6620\u4e86\u5b83\u5f53\u524d\u7684\u72b6\u6001\u6216\u72b6\u51b5\uff0c\u8fd9\u5bf9\u673a\u5668\u4eba\u7684\u4efb\u52a1\u89c4\u5212\u548c\u64cd\u4f5c\u975e\u5e38\u91cd\u8981\u3002\u7136\u800c\uff0c\u68c0\u6d4b\u7269\u4f53\u7684\u72b6\u6001\u5e76\u4e3a\u673a\u5668\u4eba\u751f\u6210\u72b6\u6001\u654f\u611f\u7684\u8ba1\u5212\u5177\u6709\u6311\u6218\u6027\u3002\u8fd1\u5e74\u6765\uff0c\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u751f\u6210\u8ba1\u5212\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u636e\u6211\u4eec\u6240\u77e5\uff0c\u51e0\u4e4e\u6ca1\u6709\u4efb\u4f55\u5173\u4e8eLLM\u6216VLM\u662f\u5426\u4e5f\u80fd\u751f\u6210\u7269\u4f53\u72b6\u6001\u654f\u611f\u8ba1\u5212\u7684\u7814\u7a76\u3002\u4e3a\u4e86\u7814\u7a76\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u7269\u4f53\u72b6\u6001\u654f\u611f\u4ee3\u7406\uff08OSSA\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7531\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u8d4b\u80fd\u7684\u4efb\u52a1\u89c4\u5212\u4ee3\u7406\u3002\u6211\u4eec\u4e3aOSSA\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\uff08i\uff09\u7531\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u5904\u7406\u6a21\u5757\uff08\u5bc6\u96c6\u5b57\u5e55\u6a21\u578b\uff0cDCM\uff09\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6a21\u578b\uff08LLM\uff09\u7ec4\u6210\u7684\u6a21\u5757\u5316\u6a21\u578b\uff0c\u4ee5\u53ca\uff08ii\uff09\u4ec5\u7531VLM\u7ec4\u6210\u7684\u5355\u7247\u6a21\u578b\u3002\u4e3a\u4e86\u5b9a\u91cf\u8bc4\u4f30\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u6211\u4eec\u4f7f\u7528\u4e86\u684c\u9762\u573a\u666f\uff0c\u5176\u4e2d\u4efb\u52a1\u662f\u6e05\u7406\u684c\u9762\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8003\u8651\u7269\u4f53\u72b6\u6001\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6570\u636e\u96c6\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u53ef\u4ee5\u7528\u4e8e\u7269\u4f53\u72b6\u6001\u654f\u611f\u7684\u4efb\u52a1\uff0c\u4f46\u5355\u7247\u65b9\u6cd5\u4f18\u4e8e\u6a21\u5757\u5316\u65b9\u6cd5\u3002OSSA\u7684\u4ee3\u7801\u53ef\u5728\\url{https://github.com/Xiao-wen-Sun/OSSA}\u83b7\u53d6\u3002 \n**|\n"}, "6DOF Object Pose": {"2406.04316": "|**2024-06-06**|**Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking**|[2406.04316](http://arxiv.org/abs/2406.04316)|null|**6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u4e00\u9879\u81f3\u5173\u91cd\u8981\u4f46\u6781\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5176\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u7f3a\u4e4f\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002\u8fd9\u79cd\u7a00\u7f3a\u6027\u963b\u788d\u4e86\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u9650\u5236\u4e86\u7814\u7a76\u8fdb\u5c55\u3002\u6b64\u5916\uff0c\u53ef\u7528\u5b9e\u4f8b\u6216\u7c7b\u522b\u7684\u6570\u91cf\u6709\u9650\u4e5f\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86 Omni6DPose\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ee5\u5bf9\u8c61\u7c7b\u522b\u591a\u6837\u6027\u3001\u89c4\u6a21\u5927\u548c\u5bf9\u8c61\u6750\u8d28\u591a\u6837\u6027\u4e3a\u7279\u5f81\u7684\u5927\u578b\u6570\u636e\u96c6\u3002Omni6DPose \u4e3b\u8981\u5206\u4e3a\u4e09\u4e2a\u90e8\u5206\uff1aROPE\uff08\u771f\u5b9e 6D \u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u96c6\uff09\uff0c\u5305\u542b 332K \u5f20\u56fe\u50cf\uff0c\u5bf9 149 \u4e2a\u7c7b\u522b\u4e2d 581 \u4e2a\u5b9e\u4f8b\u8fdb\u884c\u4e86\u8d85\u8fc7 150 \u4e07\u6b21\u6807\u6ce8\uff1bSOPE\uff08\u6a21\u62df 6D \u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u96c6\uff09\uff0c\u5305\u542b\u5728\u6df7\u5408\u73b0\u5b9e\u73af\u5883\u4e2d\u521b\u5efa\u7684 475K \u5f20\u56fe\u50cf\uff0c\u5e76\u8fdb\u884c\u6df1\u5ea6\u6a21\u62df\uff0c\u5bf9\u76f8\u540c 149 \u4e2a\u7c7b\u522b\u4e2d 4162 \u4e2a\u5b9e\u4f8b\u8fdb\u884c\u4e86\u8d85\u8fc7 500 \u4e07\u6b21\u6807\u6ce8\uff1b\u4ee5\u53ca\u5728 ROPE \u548c SOPE \u4e2d\u5747\u4f7f\u7528\u7684\u624b\u52a8\u5bf9\u9f50\u7684\u771f\u5b9e\u626b\u63cf\u7269\u4f53\u3002\u7531\u4e8e\u5b58\u5728\u5927\u91cf\u53d8\u5316\u548c\u6b67\u4e49\uff0cOmni6DPose \u672c\u8eab\u5c31\u6781\u5177\u6311\u6218\u6027\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86 GenPose++\uff0c\u5b83\u662f SOTA \u7c7b\u522b\u7ea7\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\u7684\u589e\u5f3a\u7248\u672c\uff0c\u5b83\u5305\u542b\u4e24\u9879\u5173\u952e\u6539\u8fdb\uff1a\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u63d0\u53d6\u548c\u57fa\u4e8e\u805a\u7c7b\u7684\u805a\u5408\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u57fa\u51c6\u5206\u6790\uff0c\u4ee5\u8bc4\u4f30\u5148\u524d\u65b9\u6cd5\u5728\u8fd9\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5728 6D \u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u59ff\u6001\u8ddf\u8e2a\u65b9\u9762\u7684\u6027\u80fd\u3002 \n**|\n", "2406.02977": "|**2024-06-05**|**Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices**|[2406.02977](http://arxiv.org/abs/2406.02977)|null|**\u968f\u7740\u673a\u5668\u4eba\u548c\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u8d8a\u6765\u8d8a\u4f9d\u8d56\u4e8e\u7cbe\u786e\u9ad8\u6548\u76846D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u6027\u80fd\u5bf9\u4e8e\u66f4\u5177\u4ea4\u4e92\u6027\u548c\u54cd\u5e94\u6027\u7684\u7cfb\u7edf\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u63d0\u51fa\u7684\u7a00\u758f\u989c\u8272\u4ee3\u7801\u7f51\u7edc\uff08SCCN\uff09\u4f53\u73b0\u4e86\u4e00\u79cd\u6e05\u6670\u7b80\u6d01\u7684\u7ba1\u9053\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u3002SCCN\u5bf9RGB\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u7269\u4f53\u8fdb\u884c\u50cf\u7d20\u7ea7\u9884\u6d4b\uff0c\u5229\u7528\u57fa\u672c\u7269\u4f53\u51e0\u4f55\u7279\u5f81\u7684\u7a00\u758f\u6027\u6765\u52a0\u901f\u900f\u89c6n\u70b9\uff08PnP\uff09\u8ba1\u7b97\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u5b83\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u50cf\u7d20\u7ea7\u51e0\u4f55\u7684\u7269\u4f53\u5bf9\u79f0\u8868\u793a\uff0c\u8be5\u8868\u793a\u4e0e\u521d\u59cb\u59ff\u6001\u9884\u6d4b\u65e0\u7f1d\u96c6\u6210\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5bf9\u79f0\u7269\u4f53\u7684\u6b67\u4e49\u6027\u3002 \u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSCCN\u5728NVIDIA Jetson AGX Xavier\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86\u5728\u57fa\u51c6LINEMOD\u6570\u636e\u96c6\u548c\u906e\u6321LINEMOD\u6570\u636e\u96c6\u4e0a\u6bcf\u79d219\u5e27\uff08FPS\uff09\u548c6 FPS\u7684\u4f30\u8ba1\u901f\u7387\uff0c\u540c\u65f6\u5728\u8fd9\u4e9b\u901f\u7387\u4e0b\u59cb\u7ec8\u4fdd\u6301\u8f83\u9ad8\u7684\u4f30\u8ba1\u7cbe\u5ea6\u3002 \n**|\n", "2405.11677": "|**2024-05-19**|**Advancing 6-DoF Instrument Pose Estimation in Variable X-Ray Imaging Geometries**|[2405.11677](http://arxiv.org/abs/2405.11677)|**[link](https://github.com/cviviers/YOLOv5-6D-Pose)**|**\u5728\u5fae\u521b\u624b\u672f\u4e2d\uff0c\u5bf9\u624b\u672f\u5668\u68b0\u8fdb\u884c\u7cbe\u786e\u7684 6 \u81ea\u7531\u5ea6\u59ff\u6001\u4f30\u8ba1\u53ef\u4ee5\u663e\u8457\u6539\u5584\u6cbb\u7597\u7b56\u7565\u548c\u6700\u7ec8\u624b\u672f\u7ed3\u679c\u3002\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5df2\u7ecf\u53d6\u5f97\u4e86\u51c6\u786e\u7684\u7ed3\u679c\uff0c\u4f46\u5b83\u4eec\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u5bf9\u8c61\u5b9a\u5236\u65b9\u6cd5\uff0c\u5e76\u4e14\u9700\u8981\u8d39\u529b\u7684\u8bbe\u7f6e\u548c\u8bad\u7ec3\u73af\u5883\uff08\u901a\u5e38\u5ef6\u4f38\u5230\u5e7f\u6cdb\u7684\u6a21\u62df\uff09\uff0c\u540c\u65f6\u7f3a\u4e4f\u5b9e\u65f6\u8ba1\u7b97\u80fd\u529b\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e X \u5c04\u7ebf\u7cfb\u7edf\u4e2d 6 \u81ea\u7531\u5ea6\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u7684\u901a\u7528\u6570\u636e\u91c7\u96c6\u65b9\u6cd5\u3001\u4e00\u79cd\u65b0\u9896\u4e14\u901a\u7528\u7684 YOLOv5-6D \u59ff\u6001\u67b6\u6784\uff08\u7528\u4e8e\u51c6\u786e\u5feb\u901f\u7684\u5bf9\u8c61\u59ff\u6001\u4f30\u8ba1\uff09\u4ee5\u53ca\u4e00\u79cd\u5728\u5355\u76ee\u9525\u675f X \u5c04\u7ebf\u56fe\u50cf\u91c7\u96c6\u51e0\u4f55\u5f62\u72b6\u8003\u8651\u4e0b\u8fdb\u884c\u624b\u672f\u87ba\u9489\u59ff\u6001\u4f30\u8ba1\u7684\u5b8c\u6574\u65b9\u6cd5\u3002\u6240\u63d0\u51fa\u7684 YOLOv5-6D \u59ff\u6001\u6a21\u578b\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u5728 GPU \u4e0a\u7684\u901f\u5ea6\u76f8\u5f53\u5feb\uff0c\u4e3a 42 FPS\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u7684 X \u5c04\u7ebf\u91c7\u96c6\u51e0\u4f55\u5f62\u72b6\u548c\u8bed\u4e49\u56fe\u50cf\u590d\u6742\u6027\uff0c\u4ece\u800c\u80fd\u591f\u5728\u4e0d\u540c\u9886\u57df\u5b9e\u73b0\u51c6\u786e\u7684\u59ff\u6001\u4f30\u8ba1\u3002\u6700\u540e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u810a\u67f1\u624b\u672f\u671f\u95f4\u5bf9\u9aa8\u87ba\u9489\u59ff\u6001\u4f30\u8ba1\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u673a\u8f85\u52a9\u5f15\u5bfc\u3002\u8be5\u6a21\u578b\u901a\u8fc7 0.1 ADD-S \u6307\u6807\u5b9e\u73b0\u4e86 92.41% \u7684\u7cbe\u5ea6\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u9ad8\u624b\u672f\u7cbe\u5ea6\u548c\u60a3\u8005\u9884\u540e\u65b9\u9762\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002YOLOv5-6D \u7684\u4ee3\u7801\u53ef\u5728 https://github.com/cviviers/YOLOv5-6D-Pose \u516c\u5f00\u83b7\u53d6\u3002 \n**|\n", "2405.11257": "|**2024-05-18**|**PS6D: Point Cloud Based Symmetry-Aware 6D Object Pose Estimation in Robot Bin-Picking**|[2405.11257](http://arxiv.org/abs/2405.11257)|null|**6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u5728\u8bb8\u591a\u9886\u57df\u90fd\u626e\u6f14\u7740\u81f3\u5173\u91cd\u8981\u7684\u89d2\u8272\uff0c\u7279\u522b\u662f\u5728\u5de5\u4e1a\u5de5\u4ef6\u6293\u53d6\u65b9\u9762\u3002\u9488\u5bf9\u9508\u8ff9\u3001\u9ad8\u53cd\u5c04\u7387\u548c\u7f3a\u4e4f\u7eb9\u7406\u7b49\u6311\u6218\uff0c\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u70b9\u4e91\u7684\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\uff08PS6D\uff09\u3002PS6D\u4e13\u6ce8\u4e8e\u7ec6\u957f\u548c\u591a\u5bf9\u79f0\u7269\u4f53\u3002\u5b83\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u8bbe\u8ba1\u4e86\u5bf9\u79f0\u611f\u77e5\u7684\u65cb\u8f6c\u635f\u5931\u548c\u4e2d\u5fc3\u8ddd\u79bb\u654f\u611f\u7684\u5e73\u79fb\u635f\u5931\uff0c\u5c06\u6bcf\u4e2a\u70b9\u7684\u59ff\u6001\u56de\u5f52\u5230\u5b9e\u4f8b\u7684\u8d28\u5fc3\uff0c\u7136\u540e\u4f7f\u7528\u4e24\u9636\u6bb5\u805a\u7c7b\u65b9\u6cd5\u5b8c\u6210\u5b9e\u4f8b\u5206\u5272\u548c\u59ff\u6001\u4f30\u8ba1\u3002\u6765\u81eaSil'eane\u548cIPA\u6570\u636e\u96c6\u7684\u5bf9\u8c61\u4ee5\u53ca\u6765\u81ea\u5de5\u4e1a\u5b9e\u8df5\u7684\u5178\u578b\u5de5\u4ef6\u88ab\u7528\u4e8e\u751f\u6210\u6570\u636e\u548c\u8bc4\u4f30\u7b97\u6cd5\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cPS6D\u5728F$_{1_{inst}}$\u65b9\u9762\u63d0\u9ad8\u4e8611.5%\uff0c\u5728\u53ec\u56de\u7387\u65b9\u9762\u63d0\u9ad8\u4e8614.8%\u3002PS6D\u7684\u4e3b\u8981\u90e8\u5206\u5df2\u90e8\u7f72\u5230Mech-Mind\u7684\u8f6f\u4ef6\u4e2d\uff0c\u5e76\u5728\u5206\u62e3\u5b9e\u9a8c\u4e2d\u8fbe\u5230\u4e8691.7%\u7684\u6210\u529f\u7387\uff0c\u6807\u5fd7\u7740\u5176\u5728\u5de5\u4e1a\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002 \n**|\n", "2405.07801": "|**2024-05-31**|**Deep Learning-Based Object Pose Estimation: A Comprehensive Survey**|[2405.07801](http://arxiv.org/abs/2405.07801)|**[link](https://github.com/cnjianliu/awesome-object-pose-estimation)**|**Object pose estimation is a fundamental computer vision problem with broad applications in augmented reality and robotics. Over the past decade, deep learning models, due to their superior accuracy and robustness, have increasingly supplanted conventional algorithms reliant on engineered point pair features. Nevertheless, several challenges persist in contemporary methods, including their dependency on labeled training data, model compactness, robustness under challenging conditions, and their ability to generalize to novel unseen objects. A recent survey discussing the progress made on different aspects of this area, outstanding challenges, and promising future directions, is missing. To fill this gap, we discuss the recent advances in deep learning-based object pose estimation, covering all three formulations of the problem, \\emph{i.e.}, instance-level, category-level, and unseen object pose estimation. Our survey also covers multiple input data modalities, degrees-of-freedom of output poses, object properties, and downstream tasks, providing the readers with a holistic understanding of this field. Additionally, it discusses training paradigms of different domains, inference modes, application areas, evaluation metrics, and benchmark datasets, as well as reports the performance of current state-of-the-art methods on these benchmarks, thereby facilitating the readers in selecting the most suitable method for their application. Finally, the survey identifies key challenges, reviews the prevailing trends along with their pros and cons, and identifies promising directions for future research. We also keep tracing the latest works at https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation.**|\n", "2405.01472": "|**2024-05-02**|**IntervenGen: Interventional Data Generation for Robust and Data-Efficient Robot Imitation Learning**|[2405.01472](http://arxiv.org/abs/2405.01472)|null|**Imitation learning is a promising paradigm for training robot control policies, but these policies can suffer from distribution shift, where the conditions at evaluation time differ from those in the training data. A popular approach for increasing policy robustness to distribution shift is interactive imitation learning (i.e., DAgger and variants), where a human operator provides corrective interventions during policy rollouts. However, collecting a sufficient amount of interventions to cover the distribution of policy mistakes can be burdensome for human operators. We propose IntervenGen (I-Gen), a novel data generation system that can autonomously produce a large set of corrective interventions with rich coverage of the state space from a small number of human interventions. We apply I-Gen to 4 simulated environments and 1 physical environment with object pose estimation error and show that it can increase policy robustness by up to 39x with only 10 human interventions. Videos and more results are available at https://sites.google.com/view/intervengen2024.**|\n", "2404.11139": "|**2024-04-17**|**GeoReF: Geometric Alignment Across Shape Variation for Category-level Object Pose Refinement**|[2404.11139](http://arxiv.org/abs/2404.11139)|null|**\u7269\u4f53\u59ff\u6001\u4f18\u5316\u5bf9\u4e8e\u9c81\u68d2\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u81f3\u5173\u91cd\u8981\u3002\u5148\u524d\u7684\u5de5\u4f5c\u5728\u5b9e\u4f8b\u7ea7\u7269\u4f53\u59ff\u6001\u4f18\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7c7b\u522b\u5185\u8f83\u5927\u7684\u5f62\u72b6\u53d8\u5316\u4ee5\u53ca\u76ee\u6807\u7269\u4f53\u4e0e\u5f62\u72b6\u5148\u9a8c\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u7c7b\u522b\u7ea7\u59ff\u6001\u4f18\u5316\u662f\u4e00\u4e2a\u66f4\u5177\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7c7b\u522b\u7ea7\u7269\u4f53\u59ff\u6001\u4f18\u5316\u67b6\u6784\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u96c6\u6210\u4e86 HS \u5c42\u548c\u53ef\u5b66\u4e60\u7684\u4eff\u5c04\u53d8\u6362\uff0c\u65e8\u5728\u589e\u5f3a\u51e0\u4f55\u4fe1\u606f\u7684\u63d0\u53d6\u548c\u5bf9\u9f50\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u8de8\u70b9\u4e91\u53d8\u6362\u673a\u5236\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u878d\u5408\u4e0d\u540c\u7684\u6570\u636e\u6e90\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u7ed3\u5408\u5f62\u72b6\u5148\u9a8c\u4fe1\u606f\u8fdb\u884c\u5e73\u79fb\u548c\u5c3a\u5bf8\u8bef\u5dee\u9884\u6d4b\uff0c\u7a81\u7834\u4e86\u6a21\u578b\u7684\u6781\u9650\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u6765\u8bc1\u660e\u6240\u63d0\u51fa\u6846\u67b6\u7684\u6709\u6548\u6027\u3002\u901a\u8fc7\u5927\u91cf\u7684\u5b9a\u91cf\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5728\u6240\u6709\u6307\u6807\u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u90fd\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u7684\u6539\u8fdb\u3002 \n**|\n", "2404.05626": "|**2024-04-08**|**Learning a Category-level Object Pose Estimator without Pose Annotations**|[2404.05626](http://arxiv.org/abs/2404.05626)|null|**\u4e09\u7ef4\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u4ee5\u5f80\u7684\u5de5\u4f5c\u603b\u662f\u9700\u8981\u6570\u5343\u5f20\u5e26\u6709\u6807\u6ce8\u59ff\u6001\u7684\u7269\u4f53\u56fe\u50cf\u6765\u5b66\u4e60\u4e09\u7ef4\u59ff\u6001\u5bf9\u5e94\u5173\u7cfb\uff0c\u8fd9\u5bf9\u4e8e\u6807\u6ce8\u6765\u8bf4\u65e2\u8d39\u529b\u53c8\u8017\u65f6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u5728\u6ca1\u6709\u59ff\u6001\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u7c7b\u522b\u7ea7\u4e09\u7ef4\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u5668\u3002\u6211\u4eec\u6ca1\u6709\u4f7f\u7528\u624b\u52a8\u6807\u6ce8\u7684\u56fe\u50cf\uff0c\u800c\u662f\u5229\u7528\u6269\u6563\u6a21\u578b\uff08\u4f8b\u5982\uff0cZero-1-to-3\uff09\u751f\u6210\u4e00\u7ec4\u59ff\u6001\u5dee\u5f02\u53ef\u63a7\u7684\u56fe\u50cf\uff0c\u5e76\u5efa\u8bae\u4f7f\u7528\u8fd9\u4e9b\u56fe\u50cf\u6765\u5b66\u4e60\u6211\u4eec\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u5668\u3002\u76f4\u63a5\u4f7f\u7528\u539f\u59cb\u6269\u6563\u6a21\u578b\u4f1a\u5bfc\u81f4\u56fe\u50cf\u51fa\u73b0\u59ff\u6001\u566a\u58f0\u548c\u4f2a\u5f71\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9996\u5148\uff0c\u6211\u4eec\u5229\u7528\u4ece\u4e13\u95e8\u8bbe\u8ba1\u7684\u5bf9\u6bd4\u59ff\u6001\u5b66\u4e60\u4e2d\u5b66\u4e60\u5230\u7684\u56fe\u50cf\u7f16\u7801\u5668\u6765\u8fc7\u6ee4\u4e0d\u5408\u7406\u7684\u7ec6\u8282\u5e76\u63d0\u53d6\u56fe\u50cf\u7279\u5f81\u56fe\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u7b56\u7565\uff0c\u5141\u8bb8\u6a21\u578b\u4ece\u90a3\u4e9b\u751f\u6210\u7684\u56fe\u50cf\u96c6\u4e2d\u5b66\u4e60\u7269\u4f53\u59ff\u6001\uff0c\u800c\u65e0\u9700\u77e5\u9053\u5176\u89c4\u8303\u59ff\u6001\u7684\u5bf9\u9f50\u65b9\u5f0f\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u4ece\u5355\u6b21\u62cd\u6444\u8bbe\u7f6e\uff08\u4f5c\u4e3a\u59ff\u6001\u5b9a\u4e49\uff09\u4e2d\u8fdb\u884c\u7c7b\u522b\u7ea7\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u540c\u65f6\u5728\u5c11\u6837\u672c\u7c7b\u522b\u7ea7\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002 \n**|\n", "2403.19527": "|**2024-03-28**|**Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation**|[2403.19527](http://arxiv.org/abs/2403.19527)|**[link](https://github.com/leeiieeo/ag-pose)**|**\u7c7b\u522b\u7ea7 6D \u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65e8\u5728\u4f30\u8ba1\u7279\u5b9a\u7c7b\u522b\u4e2d\u672a\u89c1\u5b9e\u4f8b\u7684\u65cb\u8f6c\u3001\u5e73\u79fb\u548c\u5c3a\u5bf8\u3002\u5728\u8fd9\u4e00\u9886\u57df\uff0c\u57fa\u4e8e\u5bc6\u96c6\u5bf9\u5e94\u7684\u7b97\u6cd5\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5b83\u4eec\u6ca1\u6709\u660e\u786e\u5730\u8003\u8651\u4e0d\u540c\u5b9e\u4f8b\u7684\u5c40\u90e8\u548c\u5168\u5c40\u51e0\u4f55\u4fe1\u606f\uff0c\u5bfc\u81f4\u5bf9\u5f62\u72b6\u53d8\u5316\u663e\u8457\u7684\u672a\u89c1\u5b9e\u4f8b\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7528\u4e8e\u7c7b\u522b\u7ea7 6D \u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u5b9e\u4f8b\u81ea\u9002\u5e94\u51e0\u4f55\u611f\u77e5\u5173\u952e\u70b9\u5b66\u4e60\u7b97\u6cd5\uff08AG-Pose\uff09\uff0c\u5b83\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a\uff081\uff09\u7b2c\u4e00\u4e2a\u8bbe\u8ba1\u662f\u5b9e\u4f8b\u81ea\u9002\u5e94\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u5757\uff0c\u5b83\u53ef\u4ee5\u81ea\u9002\u5e94\u5730\u68c0\u6d4b\u4e00\u7ec4\u7a00\u758f\u5173\u952e\u70b9\u6765\u8868\u793a\u5404\u79cd\u5b9e\u4f8b\u7684\u51e0\u4f55\u7ed3\u6784\u3002\uff082\uff09\u7b2c\u4e8c\u4e2a\u8bbe\u8ba1\u662f\u51e0\u4f55\u611f\u77e5\u7279\u5f81\u805a\u5408\u6a21\u5757\uff0c\u5b83\u53ef\u4ee5\u6709\u6548\u5730\u5c06\u5c40\u90e8\u548c\u5168\u5c40\u51e0\u4f55\u4fe1\u606f\u6574\u5408\u5230\u5173\u952e\u70b9\u7279\u5f81\u4e2d\u3002\u8fd9\u4e24\u4e2a\u6a21\u5757\u53ef\u4ee5\u534f\u540c\u5de5\u4f5c\uff0c\u4e3a\u672a\u89c1\u8fc7\u7684\u5b9e\u4f8b\u5efa\u7acb\u9c81\u68d2\u7684\u5173\u952e\u70b9\u7ea7\u5bf9\u5e94\u5173\u7cfb\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728 CAMERA25 \u548c REAL275 \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 AG-Pose \u5728\u6ca1\u6709\u7c7b\u522b\u7279\u5b9a\u5f62\u72b6\u5148\u9a8c\u7684\u60c5\u51b5\u4e0b\uff0c\u5927\u5e45\u5ea6\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002 \n**|\n", "2403.18791": "|**2024-06-01**|**Object Pose Estimation via the Aggregation of Diffusion Features**|[2403.18791](http://arxiv.org/abs/2403.18791)|**[link](https://github.com/tianfu18/diff-feats-pose)**|**\u4ece\u56fe\u50cf\u4e2d\u4f30\u8ba1\u7269\u4f53\u59ff\u6001\u662f 3D \u573a\u666f\u7406\u89e3\u7684\u4e00\u9879\u5173\u952e\u4efb\u52a1\uff0c\u6700\u8fd1\u7684\u65b9\u6cd5\u5728\u975e\u5e38\u5927\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u53ef\u559c\u7684\u7ed3\u679c\u3002 \u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5904\u7406\u672a\u89c1\u8fc7\u7269\u4f53\u65f6\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\u3002\u6211\u4eec\u8ba4\u4e3a\u8fd9\u662f\u7531\u4e8e\u56fe\u50cf\u7279\u5f81\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u9020\u6210\u7684\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u6df1\u5165\u5206\u6790\u4e86\u6269\u6563\u6a21\u578b\uff08\u4f8b\u5982 Stable Diffusion\uff09\u7684\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u5728\u5bf9\u672a\u89c1\u8fc7\u7269\u4f53\u8fdb\u884c\u5efa\u6a21\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002 \u5728\u6b64\u5206\u6790\u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u521b\u65b0\u6027\u5730\u5c06\u8fd9\u4e9b\u6269\u6563\u7279\u5f81\u5f15\u5165\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u3002 \u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u67b6\u6784\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u6355\u83b7\u548c\u805a\u5408\u4e0d\u540c\u7c92\u5ea6\u7684\u6269\u6563\u7279\u5f81\uff0c\u5927\u5927\u63d0\u9ad8\u4e86\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u6cdb\u5316\u80fd\u529b\u3002 \u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e09\u4e2a\u6d41\u884c\u7684\u57fa\u51c6\u6570\u636e\u96c6 LM\u3001O-LM \u548c T-LESS \u4e0a\u7684\u6027\u80fd\u5927\u5927\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002 \u5c24\u5176\u503c\u5f97\u4e00\u63d0\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u672a\u89c1\u8fc7\u7269\u4f53\u4e0a\u53d6\u5f97\u4e86\u6bd4\u4e4b\u524d\u6700\u4f73\u7ed3\u679c\u66f4\u9ad8\u7684\u51c6\u786e\u7387\uff1a\u5728 Unseen LM \u4e0a\u4e3a 98.2% vs. 93.5%\uff0c\u5728 Unseen O-LM \u4e0a\u4e3a 85.9% vs. 76.3%\uff0c\u663e\u793a\u51fa\u6211\u4eec\u65b9\u6cd5\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002 \u6211\u4eec\u7684\u4ee3\u7801\u5df2\u53d1\u5e03\u5728 https://github.com/Tianfu18/diff-feats-pose\u3002 \n**|\n"}, "nerf": {"2406.10111": "|**2024-06-14**|**GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors**|[2406.10111](http://arxiv.org/abs/2406.10111)|null|**\u4ece\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u89c6\u56fe\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u65b0\u89c6\u89d2\u5408\u6210\uff08HRNVS\uff09\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u3002\u4ee5\u524d\u7684\u65b9\u6cd5\u4ece\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u89c6\u56fe\u4f18\u5316\u9ad8\u5206\u8fa8\u7387\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\uff0c\u4f46\u6e32\u67d3\u901f\u5ea6\u7f13\u6162\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u57fa\u4e8e\u4e09\u7ef4\u9ad8\u65af\u6e32\u67d3\uff083DGS\uff09\u6765\u5f00\u53d1\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u56e0\u4e3a\u5b83\u80fd\u591f\u4ee5\u66f4\u5feb\u7684\u6e32\u67d3\u901f\u5ea6\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u3002\u4e3a\u4e86\u7f13\u89e3\u9ad8\u5206\u8fa8\u7387\u5408\u6210\u7684\u6570\u636e\u77ed\u7f3a\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u8bae\u5229\u7528\u73b0\u6210\u7684\u4e8c\u7ef4\u6269\u6563\u5148\u9a8c\uff0c\u901a\u8fc7\u5206\u6570\u84b8\u998f\u91c7\u6837\uff08SDS\uff09\u5c06\u4e8c\u7ef4\u77e5\u8bc6\u63d0\u53d6\u5230\u4e09\u7ef4\u7a7a\u95f4\u3002\u7136\u800c\uff0c\u7531\u4e8e\u751f\u6210\u5148\u9a8c\u5e26\u6765\u7684\u968f\u673a\u6027\uff0c\u5c06SDS\u76f4\u63a5\u5e94\u7528\u4e8e\u57fa\u4e8e\u9ad8\u65af\u76843D\u8d85\u5206\u8fa8\u7387\u4f1a\u5bfc\u81f4\u4e0d\u5e0c\u671b\u51fa\u73b0\u4e14\u5197\u4f59\u76843D\u9ad8\u65af\u57fa\u5143\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u6280\u672f\u6765\u51cf\u5c11SDS\u5f15\u5165\u7684\u968f\u673a\u6270\u52a8\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec 1\uff09\u4f7f\u7528\u9000\u706b\u7b56\u7565\u7f29\u5c0fSDS\u4e2d\u6269\u6563\u65f6\u95f4\u6b65\u957f\u7684\u8303\u56f4\uff1b2\uff09\u5728\u5bc6\u96c6\u5316\u8fc7\u7a0b\u4e2d\u968f\u673a\u4e22\u5f03\u5197\u4f59\u7684\u9ad8\u65af\u57fa\u5143\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684GaussainSR\u65b9\u6cd5\u53ef\u4ee5\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0c\u4ec5\u4f7f\u7528\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u5c31\u80fd\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684HRNVS\u7ed3\u679c\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://chchnii.github.io/GaussianSR/ \n**|\n", "2406.09801": "|**2024-06-14**|**RaNeuS: Ray-adaptive Neural Surface Reconstruction**|[2406.09801](http://arxiv.org/abs/2406.09801)|**[link](https://github.com/wangyida/ra-neus)**|**\u6211\u4eec\u7684\u76ee\u6807\u662f\u5229\u7528\u53ef\u5fae\u5206\u8f90\u5c04\u573a\uff08\u4f8b\u5982 NeRF\uff09\u6765\u91cd\u5efa\u8be6\u7ec6\u7684 3D \u8868\u9762\uff0c\u4ee5\u53ca\u751f\u6210\u6807\u51c6\u7684\u65b0\u9896\u89c6\u56fe\u6e32\u67d3\u3002\u76ee\u524d\u5df2\u6709\u4e00\u4e9b\u76f8\u5173\u65b9\u6cd5\u53ef\u4ee5\u6267\u884c\u6b64\u7c7b\u4efb\u52a1\uff0c\u901a\u5e38\u662f\u5229\u7528\u7b26\u53f7\u8ddd\u79bb\u573a (SDF)\u3002\u7136\u800c\uff0c\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u4ecd\u7136\u65e0\u6cd5\u6b63\u786e\u91cd\u5efa\u5c0f\u89c4\u6a21\u7ec6\u8282\uff0c\u4f8b\u5982\u6811\u53f6\u3001\u7ef3\u7d22\u548c\u7eba\u7ec7\u54c1\u8868\u9762\u3002\u8003\u8651\u5230\u4e0d\u540c\u7684\u65b9\u6cd5\u4f7f\u7528\u5168\u5c40\u5e38\u6570 Eikonal \u6b63\u5219\u5316\u6765\u5236\u5b9a\u548c\u4f18\u5316\u4ece SDF \u5230\u8f90\u5c04\u573a\u7684\u6295\u5f71\uff0c\u6211\u4eec\u901a\u8fc7\u5c04\u7ebf\u52a0\u6743\u56e0\u5b50\u8fdb\u884c\u6539\u8fdb\uff0c\u4ee5\u4f18\u5148\u8003\u8651\u6e32\u67d3\u548c\u96f6\u4ea4\u53c9\u8868\u9762\u62df\u5408\uff0c\u5e76\u5728\u5efa\u7acb\u5b8c\u7f8e SDF \u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u3002\u6211\u4eec\u5efa\u8bae\u81ea\u9002\u5e94\u5730\u8c03\u6574\u7b26\u53f7\u8ddd\u79bb\u573a\u4e0a\u7684\u6b63\u5219\u5316\uff0c\u4ee5\u4fbf\u4e0d\u4ee4\u4eba\u6ee1\u610f\u7684\u6e32\u67d3\u5c04\u7ebf\u4e0d\u4f1a\u5f3a\u5236\u6267\u884c\u65e0\u6548\u7684\u5f3a Eikonal \u6b63\u5219\u5316\uff0c\u5e76\u5141\u8bb8\u6765\u81ea\u5b66\u4e60\u826f\u597d\u7684\u8f90\u5c04\u533a\u57df\u7684\u68af\u5ea6\u6709\u6548\u5730\u53cd\u5411\u4f20\u64ad\u5230 SDF\u3002\u56e0\u6b64\uff0c\u5e73\u8861\u8fd9\u4e24\u4e2a\u76ee\u6807\u4ee5\u751f\u6210\u51c6\u786e\u548c\u8be6\u7ec6\u7684\u8868\u9762\u3002\u6b64\u5916\uff0c\u8003\u8651\u5230 SDF \u4e2d\u7684\u96f6\u4ea4\u53c9\u8868\u9762\u548c\u8f90\u5c04\u573a\u4e2d\u7684\u6e32\u67d3\u70b9\u4e4b\u95f4\u662f\u5426\u5b58\u5728\u51e0\u4f55\u504f\u5dee\uff0c\u6295\u5f71\u4e5f\u53ef\u4ee5\u6839\u636e\u4f18\u5316\u671f\u95f4\u4e0d\u540c\u7684 3D \u4f4d\u7f6e\u8fdb\u884c\u8c03\u6574\u3002\u6211\u4eec\u63d0\u51fa\u7684 RaNeuS \u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u90fd\u5f97\u5230\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u5728\u65b0\u9896\u89c6\u56fe\u5408\u6210\u548c\u51e0\u4f55\u91cd\u5efa\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002 \n**|\n", "2406.08943": "|**2024-06-13**|**Neural NeRF Compression**|[2406.08943](http://arxiv.org/abs/2406.08943)|null|**Neural Radiance Fields (NeRFs) have emerged as powerful tools for capturing detailed 3D scenes through continuous volumetric representations. Recent NeRFs utilize feature grids to improve rendering quality and speed; however, these representations introduce significant storage overhead. This paper presents a novel method for efficiently compressing a grid-based NeRF model, addressing the storage overhead concern. Our approach is based on the non-linear transform coding paradigm, employing neural compression for compressing the model's feature grids. Due to the lack of training data involving many i.i.d scenes, we design an encoder-free, end-to-end optimized approach for individual scenes, using lightweight decoders. To leverage the spatial inhomogeneity of the latent feature grids, we introduce an importance-weighted rate-distortion objective and a sparse entropy model employing a masking mechanism. Our experimental results validate that our proposed method surpasses existing works in terms of grid-based NeRF compression efficacy and reconstruction quality.**|\n", "2406.08894": "|**2024-06-13**|**OpenMaterial: A Comprehensive Dataset of Complex Materials for 3D Reconstruction**|[2406.08894](http://arxiv.org/abs/2406.08894)|null|**Recent advances in deep learning such as neural radiance fields and implicit neural representations have significantly propelled the field of 3D reconstruction. However, accurately reconstructing objects with complex optical properties, such as metals and glass, remains a formidable challenge due to their unique specular and light-transmission characteristics. To facilitate the development of solutions to these challenges, we introduce the OpenMaterial dataset, comprising 1001 objects made of 295 distinct materials-including conductors, dielectrics, plastics, and their roughened variants- and captured under 723 diverse lighting conditions. To this end, we utilized physics-based rendering with laboratory-measured Indices of Refraction (IOR) and generated high-fidelity multiview images that closely replicate real-world objects. OpenMaterial provides comprehensive annotations, including 3D shape, material type, camera pose, depth, and object mask. It stands as the first large-scale dataset enabling quantitative evaluations of existing algorithms on objects with diverse and challenging materials, thereby paving the way for the development of 3D reconstruction algorithms capable of handling complex material properties.**|\n", "2406.08759": "|**2024-06-13**|**Gaussian-Forest: Hierarchical-Hybrid 3D Gaussian Splatting for Compressed Scene Modeling**|[2406.08759](http://arxiv.org/abs/2406.08759)|null|**\u8fd1\u5e74\u6765\uff0c\u65b0\u89c6\u89d2\u5408\u6210\u9886\u57df\u89c1\u8bc1\u4e86\u4e09\u7ef4\u9ad8\u65af splatting \u6280\u672f\u7684\u5174\u8d77\uff0c\u8be5\u6280\u672f\u4ee5\u57fa\u4e8e\u70b9\u7684\u65b9\u5f0f\u8868\u793a\u573a\u666f\u5e76\u901a\u8fc7\u5149\u6805\u5316\u8fdb\u884c\u6e32\u67d3\u3002\u4e0e\u4f9d\u8d56\u4e8e\u5149\u7ebf\u8ffd\u8e2a\u7684\u8f90\u5c04\u573a\u4e0d\u540c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6e32\u67d3\u8d28\u91cf\u548c\u901f\u5ea6\u3002\u7136\u800c\uff0c\u4e09\u7ef4\u9ad8\u65af\u7684\u663e\u5f0f\u548c\u975e\u7ed3\u6784\u5316\u7279\u6027\u5e26\u6765\u4e86\u5de8\u5927\u7684\u5b58\u50a8\u6311\u6218\uff0c\u963b\u788d\u4e86\u5176\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u9ad8\u65af\u68ee\u6797\u5efa\u6a21\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u573a\u666f\u5206\u5c42\u8868\u793a\u4e3a\u6df7\u5408\u4e09\u7ef4\u9ad8\u65af\u68ee\u6797\u3002\u6bcf\u4e2a\u6df7\u5408\u9ad8\u65af\u4fdd\u7559\u5176\u72ec\u7279\u7684\u663e\u5f0f\u5c5e\u6027\uff0c\u540c\u65f6\u4e0e\u5176\u5144\u5f1f\u9ad8\u65af\u5171\u4eab\u9690\u5f0f\u5c5e\u6027\uff0c\u4ece\u800c\u4f7f\u7528\u66f4\u5c11\u7684\u53d8\u91cf\u4f18\u5316\u53c2\u6570\u5316\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8bbe\u8ba1\u4e86\u81ea\u9002\u5e94\u751f\u957f\u548c\u4fee\u526a\u7b56\u7565\uff0c\u786e\u4fdd\u5728\u590d\u6742\u533a\u57df\u7684\u8be6\u7ec6\u8868\u793a\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u6240\u9700\u9ad8\u65af\u7684\u6570\u91cf\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u9ad8\u65af\u68ee\u6797\u4e0d\u4ec5\u4fdd\u6301\u4e86\u76f8\u5f53\u7684\u901f\u5ea6\u548c\u8d28\u91cf\uff0c\u800c\u4e14\u5b9e\u73b0\u4e86\u8d85\u8fc7 10 \u500d\u7684\u538b\u7f29\u7387\uff0c\u6807\u5fd7\u7740\u9ad8\u6548\u573a\u666f\u5efa\u6a21\u7684\u91cd\u5927\u8fdb\u6b65\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/Xian-Bei/GaussianForest \u83b7\u53d6\u3002 \n**|\n", "2406.08009": "|**2024-06-12**|**OpenObj: Open-Vocabulary Object-Level Neural Radiance Fields with Fine-Grained Understanding**|[2406.08009](http://arxiv.org/abs/2406.08009)|**[link](https://github.com/BIT-DYN/OpenObj)**|**\u8fd1\u5e74\u6765\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u8fdb\u884c\u5f00\u653e\u8bcd\u6c47 3D \u573a\u666f\u91cd\u5efa\u5f15\u8d77\u4e86\u4eba\u4eec\u7684\u6781\u5927\u5174\u8da3\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u5f00\u653e\u96c6\u68c0\u7d22\u65b9\u9762\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\u6027\uff1a\u5b83\u4eec\u8981\u4e48\u4fa7\u91cd\u4e8e\u5b66\u4e60\u9010\u70b9\u7279\u5f81\uff0c\u5bfc\u81f4\u8bed\u4e49\u7406\u89e3\u6a21\u7cca\uff0c\u8981\u4e48\u4ec5\u4ec5\u5904\u7406\u5bf9\u8c61\u7ea7\u91cd\u5efa\uff0c\u4ece\u800c\u5ffd\u7565\u4e86\u5bf9\u8c61\u5185\u90e8\u7684\u590d\u6742\u7ec6\u8282\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86 OpenObj\uff0c\u8fd9\u662f\u4e00\u79cd\u6784\u5efa\u5177\u6709\u7ec6\u7c92\u5ea6\u7406\u89e3\u7684\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u7ea7\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u7684\u521b\u65b0\u65b9\u6cd5\u3002\u672c\u8d28\u4e0a\uff0cOpenObj \u5efa\u7acb\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5bf9\u8c61\u7ea7\u522b\u8fdb\u884c\u9ad8\u6548\u4e14\u4e25\u5bc6\u7684\u573a\u666f\u5efa\u6a21\u548c\u7406\u89e3\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06\u96f6\u4ef6\u7ea7\u7279\u5f81\u7eb3\u5165\u795e\u7ecf\u573a\uff0c\u4ece\u800c\u80fd\u591f\u5bf9\u5bf9\u8c61\u5185\u90e8\u8fdb\u884c\u7ec6\u81f4\u7684\u8868\u793a\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u6355\u83b7\u5bf9\u8c61\u7ea7\u5b9e\u4f8b\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u7ec6\u7c92\u5ea6\u7684\u7406\u89e3\u3002\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u7ed3\u679c\u8868\u660e\uff0cOpenObj \u5728\u96f6\u6837\u672c\u8bed\u4e49\u5206\u5272\u548c\u68c0\u7d22\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0cOpenObj \u652f\u6301\u591a\u79cd\u89c4\u6a21\u7684\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u4efb\u52a1\uff0c\u5305\u62ec\u5168\u5c40\u79fb\u52a8\u548c\u5c40\u90e8\u64cd\u4f5c\u3002 \n**|\n", "2406.07828": "|**2024-06-12**|**Spatial Annealing Smoothing for Efficient Few-shot Neural Rendering**|[2406.07828](http://arxiv.org/abs/2406.07828)|**[link](https://github.com/pulangk97/SANeRF)**|**\u57fa\u4e8e\u6df7\u5408\u8868\u5f81\u7684\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u5728\u91cd\u5efa\u573a\u666f\u4ee5\u8fdb\u884c\u89c6\u56fe\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\uff0c\u5e76\u5177\u6709\u5f88\u9ad8\u7684\u6548\u7387\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5f53\u89c6\u56fe\u8f93\u5165\u7a00\u758f\u65f6\uff0c\u5b83\u4eec\u7684\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\u3002\u867d\u7136\u5df2\u7ecf\u8bbe\u8ba1\u4e86\u5404\u79cd\u6b63\u5219\u5316\u7b56\u7565\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u4f9d\u8d56\u4e8e\u4f4e\u6548\u7684\u5047\u8bbe\u6216\u4e0e\u6df7\u5408\u6a21\u578b\u4e0d\u517c\u5bb9\u3002\u663e\u7136\uff0c\u9700\u8981\u4e00\u79cd\u5728\u6df7\u5408\u6846\u67b6\u5185\u4fdd\u6301\u6548\u7387\u5e76\u63d0\u9ad8\u5bf9\u7a00\u758f\u89c6\u56fe\u7684\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u51c6\u786e\u9ad8\u6548\u7684\u5c11\u6837\u672c\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff0c\u79f0\u4e3a\u7a7a\u95f4\u9000\u706b\u5e73\u6ed1\u6b63\u5219\u5316 NeRF (SANeRF)\uff0c\u5b83\u4e13\u4e3a\u9884\u8fc7\u6ee4\u9a71\u52a8\u7684\u6df7\u5408\u8868\u5f81\u67b6\u6784\u800c\u8bbe\u8ba1\u3002\u6211\u4eec\u5b9e\u73b0\u4e86\u4ece\u521d\u59cb\u5927\u503c\u5f00\u59cb\u7684\u6837\u672c\u7a7a\u95f4\u5927\u5c0f\u7684\u6307\u6570\u7ea7\u51cf\u5c11\u3002\u8fd9\u79cd\u65b9\u6cd5\u5bf9\u4e8e\u7a33\u5b9a\u8bad\u7ec3\u9636\u6bb5\u7684\u65e9\u671f\u9636\u6bb5\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u663e\u7740\u6709\u52a9\u4e8e\u589e\u5f3a\u540e\u7eed\u7684\u7ec6\u8282\u7ec6\u5316\u8fc7\u7a0b\u3002\u6211\u4eec\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u53ea\u9700\u6dfb\u52a0\u4e00\u884c\u4ee3\u7801\uff0c\u4e0e\u5f53\u524d\u7684\u5c11\u6837\u672c NeRF \u65b9\u6cd5\u76f8\u6bd4\uff0cSANeRF \u5c31\u80fd\u63d0\u4f9b\u5353\u8d8a\u7684\u6e32\u67d3\u8d28\u91cf\u548c\u66f4\u5feb\u7684\u91cd\u5efa\u901f\u5ea6\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728 Blender \u6570\u636e\u96c6\u4e0a\uff0cSANeRF \u7684 PSNR \u6bd4 FreeNeRF \u9ad8 0.3 dB\uff0c\u540c\u65f6\u91cd\u5efa\u901f\u5ea6\u63d0\u9ad8\u4e86 700 \u500d\u3002 \n**|\n", "2406.07520": "|**2024-06-11**|**Neural Gaffer: Relighting Any Object via Diffusion**|[2406.07520](http://arxiv.org/abs/2406.07520)|null|**\u5355\u56fe\u50cf\u91cd\u6253\u5149\u662f\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5b83\u6d89\u53ca\u5bf9\u51e0\u4f55\u3001\u6750\u8d28\u548c\u5149\u7167\u4e4b\u95f4\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u7684\u63a8\u7406\u3002\u8bb8\u591a\u5148\u524d\u7684\u65b9\u6cd5\u8981\u4e48\u53ea\u652f\u6301\u7279\u5b9a\u7c7b\u522b\u7684\u56fe\u50cf\uff08\u4f8b\u5982\u8096\u50cf\uff09\uff0c\u8981\u4e48\u9700\u8981\u7279\u6b8a\u7684\u62cd\u6444\u6761\u4ef6\uff08\u4f8b\u5982\u4f7f\u7528\u624b\u7535\u7b52\uff09\u3002\u6216\u8005\uff0c\u4e00\u4e9b\u65b9\u6cd5\u5c06\u573a\u666f\u660e\u786e\u5206\u89e3\u4e3a\u5185\u5728\u7ec4\u4ef6\uff0c\u4f8b\u5982\u6cd5\u7ebf\u548cBRDF\uff0c\u4f46\u8fd9\u53ef\u80fd\u4e0d\u51c6\u786e\u6216\u8868\u8fbe\u4e0d\u8db3\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7aef\u5230\u7aef\u4e8c\u7ef4\u91cd\u6253\u5149\u6269\u6563\u6a21\u578b\uff0c\u79f0\u4e3aNeural Gaffer\uff0c\u5b83\u53ef\u4ee5\u62cd\u6444\u4efb\u4f55\u7269\u4f53\u7684\u5355\u4e2a\u56fe\u50cf\uff0c\u5e76\u53ef\u4ee5\u5728\u4efb\u4f55\u65b0\u7684\u73af\u5883\u5149\u7167\u6761\u4ef6\u4e0b\u5408\u6210\u51c6\u786e\u3001\u9ad8\u8d28\u91cf\u7684\u91cd\u6253\u5149\u56fe\u50cf\uff0c\u53ea\u9700\u7b80\u5355\u5730\u5c06\u56fe\u50cf\u751f\u6210\u5668\u8bbe\u7f6e\u4e3a\u4ee5\u76ee\u6807\u73af\u5883\u56fe\u4e3a\u6761\u4ef6\uff0c\u800c\u65e0\u9700\u8fdb\u884c\u663e\u5f0f\u573a\u666f\u5206\u89e3\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5efa\u7acb\u5728\u9884\u5148\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u7684\u57fa\u7840\u4e0a\uff0c\u5e76\u5728\u5408\u6210\u7684\u91cd\u6253\u5149\u6570\u636e\u96c6\u4e0a\u5bf9\u5176\u8fdb\u884c\u5fae\u8c03\uff0c\u63ed\u793a\u5e76\u5229\u7528\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u5bf9\u5149\u7167\u7684\u5185\u5728\u7406\u89e3\u3002\u6211\u4eec\u5728\u5408\u6210\u56fe\u50cf\u548c\u6765\u81ea\u4e92\u8054\u7f51\u7684\u771f\u5b9e\u56fe\u50cf\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u6a21\u578b\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u5728\u6cdb\u5316\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u4e0e\u5176\u4ed6\u751f\u6210\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u8bb8\u591a\u4e0b\u6e38\u4e8c\u7ef4\u4efb\u52a1\uff0c\u4f8b\u5982\u57fa\u4e8e\u6587\u672c\u7684\u91cd\u6253\u5149\u548c\u5bf9\u8c61\u63d2\u5165\u3002\u6211\u4eec\u7684\u6a21\u578b\u8fd8\u53ef\u4ee5\u4f5c\u4e3a\u4e09\u7ef4\u4efb\u52a1\uff08\u4f8b\u5982\u5bf9\u8f90\u5c04\u573a\u8fdb\u884c\u91cd\u6253\u5149\uff09\u7684\u5f3a\u5927\u91cd\u6253\u5149\u5148\u9a8c\u3002**|\n", "2406.07431": "|**2024-06-11**|**Active Scout: Multi-Target Tracking Using Neural Radiance Fields in Dense Urban Environments**|[2406.07431](http://arxiv.org/abs/2406.07431)|null|**\u6211\u4eec\u7814\u7a76\u4e86\u9ad8\u5ea6\u906e\u6321\u7684\u57ce\u5e02\u73af\u5883\uff08\u4f8b\u5982\u57ce\u5e02\u4e2d\u7684\u9ad8\u5c42\u5efa\u7b51\uff09\u4e2d\u7684\u8ffd\u8e2a-\u9003\u907f\u6e38\u620f\uff0c\u5176\u4e2d\u4fa6\u5bdf\u5458\uff08\u56db\u65cb\u7ffc\u98de\u884c\u5668\uff09\u8ddf\u8e2a\u5730\u9762\u4e0a\u7684\u591a\u4e2a\u52a8\u6001\u76ee\u6807\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u53ef\u4ee5\u4f7f\u7528\u6765\u81ea\u4e0d\u540c\u6709\u5229\u4f4d\u7f6e\u7684 RGB \u548c\u6df1\u5ea6\u56fe\u50cf\u2014\u2014\u5728\u7ebf\u2014\u2014\u6784\u5efa\u57ce\u5e02\u7684\u57fa\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u7684\u8868\u793a\u3002\u8fd9\u79cd\u8868\u793a\u7528\u4e8e\u8ba1\u7b97\u4fe1\u606f\u589e\u76ca\uff0c\u4ee5\u63a2\u7d22\u57ce\u5e02\u672a\u77e5\u90e8\u5206\u5e76\u8ddf\u8e2a\u76ee\u6807\u2014\u2014\u4ece\u800c\u63d0\u4f9b\u4e00\u79cd\u5b8c\u5168\u57fa\u4e8e\u7b2c\u4e00\u6027\u539f\u7406\u7684\u65b9\u6cd5\u6765\u4e3b\u52a8\u8ddf\u8e2a\u52a8\u6001\u76ee\u6807\u3002\u6211\u4eec\u4f7f\u7528\u57fa\u4e8e\u8d39\u57ce\u548c\u7ebd\u7ea6\u5e02\u5f00\u653e\u8857\u9053\u5730\u56fe\u6570\u636e\u7684\u5b9a\u5236\u6a21\u62df\u5668\u8bc1\u660e\uff0c\u6211\u4eec\u53ef\u4ee5\u5728 300 \u6b65\u5185\u63a2\u7d22\u548c\u5b9a\u4f4d 20 \u4e2a\u9759\u6b62\u76ee\u6807\u3002\u8fd9\u6bd4\u4e0d\u4f7f\u7528\u4e3b\u52a8\u611f\u77e5\u7684\u8d2a\u5a6a\u57fa\u7ebf\u6162\u3002\u4f46\u5bf9\u4e8e\u4e3b\u52a8\u8eb2\u907f\u906e\u6321\u7269\u7684\u52a8\u6001\u76ee\u6807\uff0c\u6211\u4eec\u8bc1\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u6700\u591a\u53ef\u4ee5\u5c06\u8ddf\u8e2a\u8bef\u5dee\u4fdd\u6301\u5728 200 \u7c73\u4ee5\u5185\uff1b\u8d2a\u5a6a\u57fa\u7ebf\u7684\u8ddf\u8e2a\u8bef\u5dee\u53ef\u9ad8\u8fbe 600 \u7c73\u3002\u6211\u4eec\u89c2\u5bdf\u5230\u4fa6\u5bdf\u7b56\u7565\u4e2d\u7684\u4e00\u4e9b\u6709\u8da3\u7279\u6027\uff0c\u4f8b\u5982\uff0c\u5b83\u4f1a\u5b9a\u671f\u5207\u6362\u6ce8\u610f\u529b\u4ee5\u8ddf\u8e2a\u4e0d\u540c\u7684\u76ee\u6807\uff0c\u968f\u7740 NeRF \u8868\u793a\u8d28\u91cf\u968f\u65f6\u95f4\u63a8\u79fb\u800c\u63d0\u9ad8\uff0c\u4fa6\u5bdf\u5728\u76ee\u6807\u8ddf\u8e2a\u65b9\u9762\u4e5f\u53d8\u5f97\u66f4\u597d\u3002 \n**|\n", "2406.07329": "|**2024-06-11**|**Cinematic Gaussians: Real-Time HDR Radiance Fields with Depth of Field**|[2406.07329](http://arxiv.org/abs/2406.07329)|null|**\u8f90\u5c04\u573a\u65b9\u6cd5\u4ee3\u8868\u4e86\u4ece\u591a\u89c6\u56fe\u7167\u7247\u91cd\u5efa\u590d\u6742\u573a\u666f\u7684\u6700\u65b0\u6280\u672f\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u91cd\u5efa\u901a\u5e38\u5b58\u5728\u4ee5\u4e0b\u4e00\u4e2a\u6216\u4e24\u4e2a\u9650\u5236\uff1a\u9996\u5148\uff0c\u5b83\u4eec\u901a\u5e38\u4ee5\u4f4e\u52a8\u6001\u8303\u56f4 (LDR) \u8868\u793a\u573a\u666f\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5149\u7167\u5747\u5300\u7684\u73af\u5883\u4e2d\u7684\u4f7f\u7528\uff0c\u5e76\u963b\u788d\u4e86\u6c89\u6d78\u5f0f\u89c2\u770b\u4f53\u9a8c\u3002\u5176\u6b21\uff0c\u5b83\u4eec\u4f9d\u8d56\u4e8e\u9488\u5b54\u76f8\u673a\u6a21\u578b\uff0c\u5047\u8bbe\u6240\u6709\u573a\u666f\u5143\u7d20\u5728\u8f93\u5165\u56fe\u50cf\u4e2d\u90fd\u662f\u5bf9\u7126\u7684\uff0c\u8fd9\u5e26\u6765\u4e86\u5b9e\u9645\u6311\u6218\uff0c\u5e76\u4f7f\u65b0\u89c6\u56fe\u5408\u6210\u8fc7\u7a0b\u4e2d\u7684\u91cd\u65b0\u5bf9\u7126\u53d8\u5f97\u590d\u6742\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e 3D \u9ad8\u65af splatting \u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5177\u6709\u4e0d\u540c\u66dd\u5149\u65f6\u95f4\u3001\u5149\u5708\u548c\u7126\u8ddd\u7684\u573a\u666f\u591a\u89c6\u56fe LDR \u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u6765\u91cd\u5efa\u9ad8\u52a8\u6001\u8303\u56f4 (HDR) \u8f90\u5c04\u573a\u3002\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8e\u8584\u900f\u955c\u76f8\u673a\u6a21\u578b\u7684\u9ad8\u65af\u89e3\u6790\u5377\u79ef\u4ee5\u53ca\u8272\u8c03\u6620\u5c04\u6a21\u5757\uff0c\u6211\u4eec\u7684\u91cd\u5efa\u80fd\u591f\u6e32\u67d3\u5177\u6709\u7075\u6d3b\u91cd\u5bf9\u7126\u529f\u80fd\u7684 HDR \u5185\u5bb9\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u6211\u4eec\u5bf9 HDR \u548c\u666f\u6df1\u7684\u7ec4\u5408\u5904\u7406\u4fc3\u8fdb\u4e86\u5b9e\u65f6\u7535\u5f71\u6e32\u67d3\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u6c34\u5e73\u3002 \n**|\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2406.10225": "|**2024-06-14**|**SatDiffMoE: A Mixture of Estimation Method for Satellite Image Super-resolution with Latent Diffusion Models**|[2406.10225](http://arxiv.org/abs/2406.10225)|null|**During the acquisition of satellite images, there is generally a trade-off between spatial resolution and temporal resolution (acquisition frequency) due to the onboard sensors of satellite imaging systems. High-resolution satellite images are very important for land crop monitoring, urban planning, wildfire management and a variety of applications. It is a significant yet challenging task to achieve high spatial-temporal resolution in satellite imaging. With the advent of diffusion models, we can now learn strong generative priors to generate realistic satellite images with high resolution, which can be utilized to promote the super-resolution task as well. In this work, we propose a novel diffusion-based fusion algorithm called \\textbf{SatDiffMoE} that can take an arbitrary number of sequential low-resolution satellite images at the same location as inputs, and fuse them into one high-resolution reconstructed image with more fine details, by leveraging and fusing the complementary information from different time points. Our algorithm is highly flexible and allows training and inference on arbitrary number of low-resolution images. Experimental results show that our proposed SatDiffMoE method not only achieves superior performance for the satellite image super-resolution tasks on a variety of datasets, but also gets an improved computational efficiency with reduced model parameters, compared with previous methods.**|\n", "2406.10224": "|**2024-06-14**|**EFM3D: A Benchmark for Measuring Progress Towards 3D Egocentric Foundation Models**|[2406.10224](http://arxiv.org/abs/2406.10224)|null|**The advent of wearable computers enables a new source of context for AI that is embedded in egocentric sensor data. This new egocentric data comes equipped with fine-grained 3D location information and thus presents the opportunity for a novel class of spatial foundation models that are rooted in 3D space. To measure progress on what we term Egocentric Foundation Models (EFMs) we establish EFM3D, a benchmark with two core 3D egocentric perception tasks. EFM3D is the first benchmark for 3D object detection and surface regression on high quality annotated egocentric data of Project Aria. We propose Egocentric Voxel Lifting (EVL), a baseline for 3D EFMs. EVL leverages all available egocentric modalities and inherits foundational capabilities from 2D foundation models. This model, trained on a large simulated dataset, outperforms existing methods on the EFM3D benchmark.**|\n", "2406.10139": "|**2024-06-14**|**YOLOv1 to YOLOv10: A comprehensive review of YOLO variants and their application in the agricultural domain**|[2406.10139](http://arxiv.org/abs/2406.10139)|null|**\u8fd9\u7bc7\u7efc\u8ff0\u8c03\u67e5\u4e86\u4ece YOLOv1 \u5230\u6700\u5148\u8fdb\u7684 YOLOv10 \u7684\u5404\u79cd YOLO \u53d8\u4f53\u5728\u519c\u4e1a\u53d1\u5c55\u4e2d\u7684\u53d8\u9769\u6f5c\u529b\u3002\u4e3b\u8981\u76ee\u6807\u662f\u9610\u660e\u8fd9\u4e9b\u5c16\u7aef\u7684\u7269\u4f53\u68c0\u6d4b\u6a21\u578b\u5982\u4f55\u80fd\u591f\u91cd\u632f\u548c\u4f18\u5316\u519c\u4e1a\u7684\u5404\u4e2a\u65b9\u9762\uff0c\u4ece\u4f5c\u7269\u76d1\u6d4b\u5230\u7272\u755c\u7ba1\u7406\u3002\u5b83\u65e8\u5728\u5b9e\u73b0\u5173\u952e\u76ee\u6807\uff0c\u5305\u62ec\u786e\u5b9a\u519c\u4e1a\u4e2d\u7684\u5f53\u4ee3\u6311\u6218\u3001\u8be6\u7ec6\u8bc4\u4f30 YOLO \u7684\u589e\u91cf\u8fdb\u6b65\uff0c\u4ee5\u53ca\u63a2\u7d22\u5176\u5728\u519c\u4e1a\u4e2d\u7684\u5177\u4f53\u5e94\u7528\u3002\u8fd9\u662f\u9996\u6279\u5c06\u6700\u65b0\u7684 YOLOv10 \u5305\u542b\u8fdb\u6765\u7684\u7efc\u8ff0\u4e4b\u4e00\uff0c\u4e3a\u5176\u5bf9\u4eba\u5de5\u667a\u80fd\u548c\u81ea\u52a8\u5316\u65f6\u4ee3\u7cbe\u51c6\u519c\u4e1a\u548c\u53ef\u6301\u7eed\u519c\u4e1a\u5b9e\u8df5\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002\u6b64\u5916\uff0c\u8be5\u7efc\u8ff0\u5bf9 YOLO \u7684\u6027\u80fd\u8fdb\u884c\u4e86\u6279\u5224\u6027\u5206\u6790\uff0c\u7efc\u5408\u4e86\u73b0\u6709\u7814\u7a76\uff0c\u5e76\u9884\u6d4b\u4e86\u672a\u6765\u8d8b\u52bf\u3002\u901a\u8fc7\u4ed4\u7ec6\u7814\u7a76 YOLO \u53d8\u4f53\u4e2d\u5305\u542b\u7684\u72ec\u7279\u529f\u80fd\u53ca\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\uff0c\u672c\u7efc\u8ff0\u4e3a YOLO \u53d8\u4f53\u4e0e\u519c\u4e1a\u4e4b\u95f4\u4e0d\u65ad\u53d1\u5c55\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002\u8fd9\u4e9b\u53d1\u73b0\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u7cbe\u51c6\u519c\u4e1a\u548c\u53ef\u6301\u7eed\u519c\u4e1a\u5b9e\u8df5\u7684\u6f5c\u529b\uff0c\u6807\u5fd7\u7740\u5728\u519c\u4e1a\u9886\u57df\u6574\u5408\u5148\u8fdb\u7684\u7269\u4f53\u68c0\u6d4b\u6280\u672f\u65b9\u9762\u8fc8\u51fa\u4e86\u91cd\u8981\u7684\u4e00\u6b65\u3002 \n**|\n", "2406.10111": "|**2024-06-14**|**GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors**|[2406.10111](http://arxiv.org/abs/2406.10111)|null|**\u4ece\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u89c6\u56fe\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u65b0\u89c6\u56fe\u5408\u6210 (HRNVS) \u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u3002\u5148\u524d\u7684\u65b9\u6cd5\u4ece\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u89c6\u56fe\u4f18\u5316\u9ad8\u5206\u8fa8\u7387\u795e\u7ecf\u8f90\u5c04\u573a (NeRF)\uff0c\u4f46\u6e32\u67d3\u901f\u5ea6\u7f13\u6162\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u57fa\u4e8e 3D \u9ad8\u65af\u6837\u6761 (3DGS) \u5f00\u53d1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u56e0\u4e3a\u5b83\u80fd\u591f\u4ee5\u66f4\u5feb\u7684\u6e32\u67d3\u901f\u5ea6\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002\u4e3a\u4e86\u7f13\u89e3\u66f4\u9ad8\u5206\u8fa8\u7387\u5408\u6210\u7684\u6570\u636e\u77ed\u7f3a\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u8bae\u5229\u7528\u73b0\u6210\u7684 2D \u6269\u6563\u5148\u9a8c\uff0c\u901a\u8fc7\u5206\u6570\u84b8\u998f\u91c7\u6837 (SDS) \u5c06 2D \u77e5\u8bc6\u63d0\u53d6\u5230 3D \u4e2d\u3002\u7136\u800c\uff0c\u7531\u4e8e\u751f\u6210\u5148\u9a8c\u5e26\u6765\u7684\u968f\u673a\u6027\uff0c\u5c06 SDS \u76f4\u63a5\u5e94\u7528\u4e8e\u57fa\u4e8e\u9ad8\u65af\u7684 3D \u8d85\u5206\u8fa8\u7387\u4f1a\u5bfc\u81f4\u4e0d\u5e0c\u671b\u7684\u548c\u5197\u4f59\u7684 3D \u9ad8\u65af\u57fa\u5143\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u6280\u672f\u6765\u51cf\u5c11 SDS \u5f15\u5165\u7684\u968f\u673a\u6270\u52a8\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec 1) \u4f7f\u7528\u9000\u706b\u7b56\u7565\u7f29\u5c0f SDS \u4e2d\u6269\u6563\u65f6\u95f4\u6b65\u957f\u7684\u8303\u56f4\uff1b2) \u5728\u5bc6\u96c6\u5316\u8fc7\u7a0b\u4e2d\u968f\u673a\u4e22\u5f03\u5197\u4f59\u7684\u9ad8\u65af\u57fa\u5143\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684 GaussainSR \u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\uff0c\u5c31\u53ef\u4ee5\u5728 HRNVS \u4e0a\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://chchnii.github.io/GaussianSR/ \n**|\n", "2406.09935": "|**2024-06-14**|**Forgetting Order of Continual Learning: Examples That are Learned First are Forgotten Last**|[2406.09935](http://arxiv.org/abs/2406.09935)|null|**\u707e\u96be\u6027\u9057\u5fd8\u662f\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u6a21\u578b\u5728\u5b66\u4e60\u65b0\u6570\u636e\u65f6\u7ecf\u5e38\u4f1a\u5fd8\u8bb0\u4e4b\u524d\u7684\u4efb\u52a1\u3002\u6211\u4eec\u7684\u5b9e\u8bc1\u5206\u6790\u8868\u660e\uff0c\u707e\u96be\u6027\u9057\u5fd8\u4e0e\u6837\u672c\u5b66\u4e60\u901f\u5ea6\u4e4b\u95f4\u5b58\u5728\u5f88\u5f3a\u7684\u76f8\u5173\u6027\uff1a\u65e9\u671f\u5b66\u4e60\u7684\u6837\u672c\u5f88\u5c11\u88ab\u9057\u5fd8\uff0c\u800c\u540e\u671f\u5b66\u4e60\u7684\u6837\u672c\u66f4\u5bb9\u6613\u88ab\u9057\u5fd8\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u57fa\u4e8e\u56de\u653e\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u5229\u7528\u8fd9\u79cd\u73b0\u8c61\uff0c\u901a\u8fc7\u5173\u6ce8\u4e2d\u7b49\u5b66\u4e60\u901f\u5ea6\u7684\u6837\u672c\u8fdb\u884c\u590d\u4e60\u3002\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u91d1\u53d1\u59d1\u5a18\u201d\uff08Goldilocks\uff09\u7684\u65b0\u578b\u56de\u653e\u7f13\u51b2\u533a\u91c7\u6837\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8fc7\u6ee4\u6389\u5b66\u4e60\u901f\u5ea6\u8fc7\u5feb\u6216\u8fc7\u6162\u7684\u6837\u672c\uff0c\u4fdd\u7559\u5b66\u4e60\u901f\u5ea6\u9002\u4e2d\u7684\u6837\u672c\u3002\u201c\u91d1\u53d1\u59d1\u5a18\u201d\u6539\u8fdb\u4e86\u73b0\u6709\u7684\u6301\u7eed\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u591a\u4e2a\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002 \n**|\n", "2406.09922": "|**2024-06-14**|**Exact Sparse Representation Recovery in Signal Demixing and Group BLASSO**|[2406.09922](http://arxiv.org/abs/2406.09922)|null|**\u672c\u6587\u7b80\u8981\u4ecb\u7ecd\u4e86(Carioni \u548c Del Grande, arXiv:2311.08072, 2023) \u4e2d\u63d0\u51fa\u7684\u51f8\u6b63\u5219\u5316\u4f18\u5316\u95ee\u9898\u4e2d\u7a00\u758f\u8868\u793a\u6062\u590d\u7684\u7406\u8bba\u3002\u6211\u4eec\u5173\u6ce8\u4e8e\u672a\u77e5\u91cf\u5c5e\u4e8e\u5df4\u62ff\u8d6b\u7a7a\u95f4\u4e14\u6d4b\u91cf\u503c\u53d6\u81ea\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7684\u60c5\u51b5\uff0c\u63a2\u7d22\u4e86\u6b64\u7c7b\u8bbe\u7f6e\u4e0b\u4f18\u5316\u95ee\u9898\u6700\u5c0f\u503c\u70b9\u7684\u6027\u8d28\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5206\u6790\u4e86\u4e00\u4e2aTikhonov\u6b63\u5219\u5316\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5176\u4e2d$y_0$\u662f\u6d4b\u91cf\u6570\u636e\uff0c$w$\u8868\u793a\u566a\u58f0\uff0c$\\lambda$\u662f\u6b63\u5219\u5316\u53c2\u6570\u3002\u901a\u8fc7\u5f15\u5165\u5ea6\u91cf\u975e\u9000\u5316\u6e90\u6761\u4ef6 (MNDSC) \u5e76\u8003\u8651\u8db3\u591f\u5c0f\u7684$\\lambda$\u548c$w$\uff0c\u6211\u4eec\u4e3a\u95ee\u9898\u5efa\u7acb\u4e86\u7cbe\u786e\u7a00\u758f\u8868\u793a\u6062\u590d (ESRR)\uff0c\u8fd9\u610f\u5473\u7740\u6700\u5c0f\u503c\u70b9\u662f\u552f\u4e00\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u7cbe\u786e\u5730\u6062\u590d\u539f\u59cb\u6570\u636e\u7684\u7a00\u758f\u8868\u793a\u3002\u7136\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u4e24\u4e2a\u65b0\u7684\u5e94\u7528\u5f3a\u8c03\u4e86\u8fd9\u4e00\u7406\u8bba\u7ed3\u679c\u7684\u5b9e\u9645\u610f\u4e49\uff1a\u4fe1\u53f7\u5206\u79bb\u548c\u4f7f\u7528 Group BLASSO \u7684\u8d85\u5206\u8fa8\u7387\u3002\u8fd9\u4e9b\u5e94\u7528\u5f3a\u8c03\u4e86\u6211\u4eec\u7ed3\u679c\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u91cd\u8981\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u9886\u57df\u7684\u6f5c\u529b\u3002 \n**|\n", "2406.09914": "|**2024-06-14**|**Robust compressive tracking via online weighted multiple instance learning**|[2406.09914](http://arxiv.org/abs/2406.09914)|null|**Developing a robust object tracker is a challenging task due to factors such as occlusion, motion blur, fast motion, illumination variations, rotation, background clutter, low resolution and deformation across the frames. In the literature, lots of good approaches based on sparse representation have already been presented to tackle the above problems. However, most of the algorithms do not focus on the learning of sparse representation. They only consider the modeling of target appearance and therefore drift away from the target with the imprecise training samples. By considering all the above factors in mind, we have proposed a visual object tracking algorithm by integrating a coarse-to-fine search strategy based on sparse representation and the weighted multiple instance learning (WMIL) algorithm. Compared with the other trackers, our approach has more information of the original signal with less complexity due to the coarse-to-fine search method, and also has weights for important samples. Thus, it can easily discriminate the background features from the foreground. Furthermore, we have also selected the samples from the un-occluded sub-regions to efficiently develop the strong classifier. As a consequence, a stable and robust object tracker is achieved to tackle all the aforementioned problems. Experimental results with quantitative as well as qualitative analysis on challenging benchmark datasets show the accuracy and efficiency of our method.**|\n", "2406.09768": "|**2024-06-14**|**Bayesian Conditioned Diffusion Models for Inverse Problems**|[2406.09768](http://arxiv.org/abs/2406.09768)|null|**\u6269\u6563\u6a21\u578b\u6700\u8fd1\u5728\u8bb8\u591a\u56fe\u50cf\u91cd\u5efa\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fd9\u4e9b\u4efb\u52a1\u6d89\u53ca\u57fa\u4e8e\u524d\u5411\u6d4b\u91cf\u7b97\u5b50\u7684\u9006\u95ee\u9898\u3002\u4e00\u4e2a\u5e38\u89c1\u7684\u6846\u67b6\u662f\u4f7f\u7528\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u65e0\u6761\u4ef6\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u968f\u540e\u7ecf\u8fc7\u540e\u9a8c\u6761\u4ef6\u5316\u4ee5\u8fdb\u884c\u91cd\u5efa\uff0c\u8fd9\u79cd\u65b9\u6cd5\u901a\u5e38\u5b58\u5728\u4efb\u52a1\u6027\u80fd\u6b20\u4f73\u7684\u95ee\u9898\u3002\u867d\u7136\u4e5f\u6709\u4eba\u63d0\u51fa\u4e86\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u6761\u4ef6\u6a21\u578b\uff0c\u4f46\u76ee\u524d\u7684\u65b9\u6cd5\u542f\u53d1\u5f0f\u5730\u5c06\u6d4b\u91cf\u6570\u636e\u4f5c\u4e3a\u6734\u7d20\u7684\u8f93\u5165\u901a\u9053\u6ce8\u5165\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u91c7\u6837\u4e0d\u51c6\u786e\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u7684\u6700\u4f73\u6761\u4ef6\u5316\u95ee\u9898\uff0c\u4ee5\u89e3\u51b3\u56fe\u50cf\u91cd\u5efa\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u9006\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5206\u6570\u51fd\u6570\u7684\u6269\u6563\u6a21\u578b\u8d1d\u53f6\u65af\u6761\u4ef6\u5316\u6280\u672fBCDM\uff0c\u8be5\u6280\u672f\u57fa\u4e8e\u7ed9\u5b9a\u6d4b\u91cf\u6570\u636e\u7684\u671f\u671b\u56fe\u50cf\u7684\u6761\u4ef6\u5206\u5e03\u3002\u6211\u4eec\u4e25\u683c\u63a8\u5bfc\u51fa\u8868\u8fbe\u548c\u8bad\u7ec3\u6761\u4ef6\u5206\u6570\u51fd\u6570\u7684\u7406\u8bba\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684\u6280\u672f\u5728\u56fe\u50cf\u53bb\u6df7\u53e0\u3001\u53bb\u6a21\u7cca\u3001\u8d85\u5206\u8fa8\u7387\u548c\u4fee\u590d\u65b9\u9762\u7684\u6700\u65b0\u6027\u80fd\u3002 \n**|\n", "2406.09731": "|**2024-06-14**|**Automated GIS-Based Framework for Detecting Crosswalk Changes from Bi-Temporal High-Resolution Aerial Images**|[2406.09731](http://arxiv.org/abs/2406.09731)|null|**Identification of changes in pavement markings has become crucial for infrastructure monitoring, maintenance, development, traffic management, and safety. Automated extraction of roadway geometry is critical in helping with this, given the increasing availability of high-resolution images and advancements in computer vision and object detection. Specifically, due to the substantial volume of satellite and high-resolution aerial images captured at different time instances, change detection has become a viable solution. In this study, an automated framework is developed to detect changes in crosswalks of Orange, Osceola, and Seminole counties in Florida, utilizing data extracted from high-resolution images obtained at various time intervals. Specifically, for Orange County, crosswalk changes between 2019 and 2021 were manually extracted, verified, and categorized as either new or modified crosswalks. For Seminole County, the developed model was used to automatically extract crosswalk changes between 2018 and 2021, while for Osceola County, changes between 2019 and 2020 were extracted. Findings indicate that Orange County witnessed approximately 2,094 crosswalk changes, with 312 occurring on state roads. In Seminole and Osceola counties, on the other hand, 1,040 and 1,402 crosswalk changes were observed on both local and state roads, respectively. Among these, 340 and 344 were identified on state roads in Seminole and Osceola, respectively. Spatiotemporal changes observed in crosswalks can be utilized to regularly update the existing crosswalk inventories, which is essential for agencies engaged in traffic and safety studies. Data extracted from these crosswalk changes can be combined with traffic and crash data to provide valuable insights to policymakers.**|\n", "2406.09653": "|**2024-06-14**|**An alternate approach for estimating grain-growth kinetics**|[2406.09653](http://arxiv.org/abs/2406.09653)|null|**Rate of grain growth, which aides in achieving desired properties in polycrystalline materials, is conventionally estimated by measuring the size of grains and tracking its change in micrographs reflecting the temporal evolution. Techniques adopting this conventional approach demand an absolute distinction between the grains and the interface separating them to yield an accurate result. Edge-detection, segmentation and other deep-learning algorithms are increasingly adopted to expose the network of boundaries and the associated grains precisely. An alternate approach for measuring grain-growth kinetics, that curtails the need for advanced image-processing treatment, is presented in this work. Grain-growth rate in the current technique is ascertained by \\textit{counting} the number of triple-( and quadruple-) junctions, and monitoring its change during the microstructural evolution. The shifted focus of this junction-based treatment minimises the significance of a well-defined grain-boundary network, and consequently, the involvement of the sophisticated techniques that expose them. A regression-based object-detection algorithm is extended to realise, and count, the number of junctions in polycrystalline microstructures. By examining the change in the number of junctions with time, the growth rate is subsequently determined.Growth kinetics estimated by the present junction-based approach, across a wide-range of multiphase polycrystalline microstructures, agree convincingly with the outcomes of the conventional treatment.Besides offering a novel technique for grain-growth measurement, the analysis accompanying the current work unravels a trend, compatible with the topological events, in the progressive evolution of the triple-junctions count. The present approach, through its underlying algorithm, provides a promising option for monitoring grain-growth during in-situ investigations.**|\n"}, "\u6a21\u578b\u538b\u7f29/\u4f18\u5316": {"2406.09719": "|**2024-06-14**|**Self-Knowledge Distillation for Learning Ambiguity**|[2406.09719](http://arxiv.org/abs/2406.09719)|null|**Recent language models have shown remarkable performance on natural language understanding (NLU) tasks. However, they are often sub-optimal when faced with ambiguous samples that can be interpreted in multiple ways, over-confidently predicting a single label without consideration for its correctness. To address this issue, we propose a novel self-knowledge distillation method that enables models to learn label distributions more accurately by leveraging knowledge distilled from their lower layers. This approach also includes a learning phase that re-calibrates the unnecessarily strengthened confidence for training samples judged as extremely ambiguous based on the distilled distribution knowledge. We validate our method on diverse NLU benchmark datasets and the experimental results demonstrate its effectiveness in producing better label distributions. Particularly, through the process of re-calibrating the confidence for highly ambiguous samples, the issue of over-confidence when predictions for unseen samples do not match with their ground-truth labels has been significantly alleviated. This has been shown to contribute to generating better distributions than the existing state-of-the-art method. Moreover, our method is more efficient in training the models compared to the existing method, as it does not involve additional training processes to refine label distributions.**|\n", "2406.09664": "|**2024-06-14**|**Frequency-mix Knowledge Distillation for Fake Speech Detection**|[2406.09664](http://arxiv.org/abs/2406.09664)|null|**\u5728\u7535\u8bdd\u573a\u666f\u4e2d\uff0c\u6253\u51fb\u8bed\u97f3\u6b3a\u9a97\u653b\u51fb\u7684\u865a\u5047\u8bed\u97f3\u68c0\u6d4b (FSD) \u4efb\u52a1\u6781\u5177\u6311\u6218\u6027\u3002\u6570\u636e\u589e\u5f3a (DA) \u65b9\u6cd5\u88ab\u8ba4\u4e3a\u662f\u89e3\u51b3\u7535\u8bdd\u573a\u666f\u4e2d FSD \u4efb\u52a1\u7684\u6709\u6548\u624b\u6bb5\uff0c\u901a\u5e38\u5206\u4e3a\u65f6\u57df\u548c\u9891\u57df\u4e24\u4e2a\u9636\u6bb5\u3002\u867d\u7136\u6bcf\u79cd\u65b9\u6cd5\u90fd\u6709\u5176\u4f18\u70b9\uff0c\u4f46\u90fd\u53ef\u80fd\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 DA \u65b9\u6cd5\uff0c\u5373\u9891\u7387\u6df7\u5408 (Freqmix)\uff0c\u5e76\u5f15\u5165\u4e86 Freqmix \u77e5\u8bc6\u84b8\u998f (FKD) \u6765\u589e\u5f3a\u6a21\u578b\u4fe1\u606f\u63d0\u53d6\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u4f7f\u7528 Freqmix \u589e\u5f3a\u7684\u6570\u636e\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u7684\u8f93\u5165\uff0c\u800c\u5b66\u751f\u6a21\u578b\u7684\u8f93\u5165\u5219\u7ecf\u8fc7\u65f6\u57df DA \u65b9\u6cd5\u5904\u7406\u3002\u6211\u4eec\u4f7f\u7528\u591a\u7ea7\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\u6765\u6062\u590d\u4fe1\u606f\u5e76\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728 ASVspoof 2021 LA \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e86 31%\uff0c\u5e76\u4e14\u5728 ASVspoof 2021 DF \u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002 \n**|\n", "2406.09627": "|**2024-06-13**|**RobustSAM: Segment Anything Robustly on Degraded Images**|[2406.09627](http://arxiv.org/abs/2406.09627)|null|**\u56fe\u50cf\u5206\u5272\u9886\u57df\u4e2d\uff0cSegment Anything Model (SAM) \u6a21\u578b\u51ed\u501f\u5176\u5f3a\u5927\u7684\u96f6\u6837\u672c\u5206\u5272\u80fd\u529b\u548c\u7075\u6d3b\u7684\u63d0\u793a\u7cfb\u7edf\uff0c\u5df2\u6210\u4e3a\u4e00\u79cd\u53d8\u9769\u6027\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u4f1a\u5f71\u54cd\u5176\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Robust Segment Anything Model (RobustSAM)\uff0c\u5b83\u5728\u4fdd\u6301 SAM \u7684\u63d0\u793a\u6027\u548c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\uff0c\u589e\u5f3a\u4e86\u5176\u5728\u4f4e\u8d28\u91cf\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u9884\u8bad\u7ec3\u7684 SAM \u6a21\u578b\uff0c\u4ec5\u589e\u52a0\u4e86\u5c11\u91cf\u53c2\u6570\u548c\u8ba1\u7b97\u9700\u6c42\u3002RobustSAM \u7684\u989d\u5916\u53c2\u6570\u53ef\u4ee5\u5728 8 \u4e2a GPU \u4e0a\u7528 30 \u5c0f\u65f6\u5185\u5b8c\u6210\u4f18\u5316\uff0c\u8fd9\u8bc1\u660e\u4e86\u5176\u5bf9\u4e8e\u5178\u578b\u7814\u7a76\u5b9e\u9a8c\u5ba4\u7684\u53ef\u884c\u6027\u548c\u5b9e\u7528\u6027\u3002\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86 Robust-Seg \u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b 688K \u4e2a\u56fe\u50cf-\u63a9\u7801\u5bf9\uff0c\u5177\u6709\u4e0d\u540c\u7684\u9000\u5316\u7a0b\u5ea6\uff0c\u65e8\u5728\u4f18\u5316\u6211\u4eec\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002\u8de8\u591a\u4e2a\u5206\u5272\u4efb\u52a1\u548c\u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRobustSAM \u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\uff0c\u7a81\u51fa\u4e86\u5176\u5728\u5e7f\u6cdb\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u57fa\u4e8e SAM \u7684\u4e0b\u6e38\u4efb\u52a1\uff08\u4f8b\u5982\u5355\u56fe\u50cf\u53bb\u96fe\u548c\u53bb\u6a21\u7cca\uff09\u7684\u6027\u80fd\u3002 \n**|\n", "2406.09021": "|**2024-06-13**|**Contextual Distillation Model for Diversified Recommendation**|[2406.09021](http://arxiv.org/abs/2406.09021)|null|**\u63a8\u8350\u7684\u591a\u6837\u6027\u4e0e\u51c6\u786e\u6027\u5728\u6539\u5584\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u540c\u6837\u91cd\u8981\u3002\u73b0\u6709\u7684\u7814\u7a76\uff0c\u4f8b\u5982\u884c\u5217\u5f0f\u70b9\u8fc7\u7a0b\uff08DPP\uff09\u548c\u6700\u5927\u8fb9\u7f18\u76f8\u5173\u6027\uff08MMR\uff09\uff0c\u91c7\u7528\u8d2a\u5a6a\u8303\u5f0f\u6765\u8fed\u4ee3\u5730\u9009\u62e9\u540c\u65f6\u4f18\u5316\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u7684\u9879\u76ee\u3002\u7136\u800c\uff0c\u5148\u524d\u7684\u65b9\u6cd5\u901a\u5e38\u8868\u73b0\u51fa\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u5c06\u5176\u5e94\u7528\u9650\u5236\u5728\u91cd\u6392\u5e8f\u9636\u6bb5\uff0c\u5e76\u4e14\u4e0d\u9002\u7528\u4e8e\u5019\u9009\u9879\u76ee\u6c60\u66f4\u5927\u7684\u5176\u4ed6\u63a8\u8350\u9636\u6bb5\uff0c\u4f8b\u5982\u9884\u6392\u5e8f\u548c\u6392\u5e8f\u9636\u6bb5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u84b8\u998f\u6a21\u578b\uff08CDM\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u89e3\u51b3\u591a\u6837\u5316\u7684\u6709\u6548\u63a8\u8350\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u63a8\u8350\u6d41\u7a0b\u7684\u6240\u6709\u9636\u6bb5\u7684\u90e8\u7f72\u3002\u5177\u4f53\u6765\u8bf4\uff0cCDM \u5229\u7528\u540c\u4e00\u7528\u6237\u8bf7\u6c42\u4e2d\u7684\u5019\u9009\u9879\u76ee\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u6765\u589e\u5f3a\u7ed3\u679c\u7684\u591a\u6837\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6bd4\u4e0a\u4e0b\u6587\u7f16\u7801\u5668\uff0c\u5b83\u91c7\u7528\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u6b63\u9762\u548c\u8d1f\u9762\u4e0a\u4e0b\u6587\u8fdb\u884c\u5efa\u6a21\u3002\u5bf9\u4e8e CDM \u7684\u8bad\u7ec3\uff0c\u6211\u4eec\u5c06\u6bcf\u4e2a\u76ee\u6807\u9879\u76ee\u4e0e\u5176\u4e0a\u4e0b\u6587\u5d4c\u5165\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u5229\u7528\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u6765\u5b66\u4e60 MMR \u7b97\u6cd5\u4e0b\u6bcf\u4e2a\u76ee\u6807\u9879\u76ee\u7684\u83b7\u80dc\u6982\u7387\uff0c\u5176\u4e2d\u6559\u5e08\u6765\u81ea MMR \u8f93\u51fa\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u63a8\u8350\u548c\u5b66\u751f\u6a21\u578b\u5206\u6570\u7684\u7ebf\u6027\u7ec4\u5408\u6765\u6267\u884c\u6392\u5e8f\uff0c\u786e\u4fdd\u591a\u6837\u6027\u548c\u6548\u7387\u3002\u6211\u4eec\u5bf9\u4e24\u4e2a\u5de5\u4e1a\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u79bb\u7ebf\u8bc4\u4f30\uff0c\u5e76\u5728\u77ed\u89c6\u9891\u5e73\u53f0\u5feb\u624b\u4e0a\u5bf9 CDM \u8fdb\u884c\u4e86\u5728\u7ebf A/B \u6d4b\u8bd5\u3002\u6307\u6807\u663e\u793a\uff0c\u63a8\u8350\u8d28\u91cf\u548c\u591a\u6837\u6027\u65b9\u9762\u5747\u6709\u663e\u7740\u63d0\u9ad8\uff0c\u8fd9\u6709\u529b\u5730\u8bc1\u660e\u4e86 CDM \u7684\u6709\u6548\u6027\u3002 \n**|\n", "2406.08634": "|**2024-06-12**|**Unveiling Incomplete Modality Brain Tumor Segmentation: Leveraging Masked Predicted Auto-Encoder and Divergence Learning**|[2406.08634](http://arxiv.org/abs/2406.08634)|null|**\u8111\u80bf\u7624\u5206\u5272\u4ecd\u7136\u662f\u4e00\u9879\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u591a\u6a21\u6001\u78c1\u5171\u632f\u6210\u50cf (MRI) \u7684\u60c5\u51b5\u4e0b\uff0c\u4e34\u5e8a\u73af\u5883\u4e2d\u7ecf\u5e38\u51fa\u73b0\u6a21\u6001\u56fe\u50cf\u7f3a\u5931\u7684\u60c5\u51b5\uff0c\u5bfc\u81f4\u5206\u5272\u7cbe\u5ea6\u964d\u4f4e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u63a9\u7801\u9884\u6d4b\u9884\u8bad\u7ec3\u7684\u65b0\u7b56\u7565\uff0c\u80fd\u591f\u4ece\u4e0d\u5b8c\u6574\u7684\u6a21\u6001\u6570\u636e\u4e2d\u5b66\u4e60\u9c81\u68d2\u7684\u7279\u5f81\u3002\u6b64\u5916\uff0c\u5728\u5fae\u8c03\u9636\u6bb5\uff0c\u6211\u4eec\u5229\u7528\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u6765\u5bf9\u9f50\u5b8c\u6574\u6a21\u6001\u6570\u636e\u548c\u7f3a\u5931\u6a21\u6001\u6570\u636e\u4e4b\u95f4\u7684\u7279\u5f81\uff0c\u540c\u65f6\u589e\u5f3a\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u5229\u7528 Holder \u4f2a\u6563\u5ea6\u800c\u4e0d\u662f KLD \u6765\u8ba1\u7b97\u84b8\u998f\u635f\u5931\uff0c\u4ece\u800c\u63d0\u4f9b\u66f4\u597d\u7684\u6570\u5b66\u53ef\u89e3\u91ca\u6027\u548c\u6027\u8d28\u3002\u5728 BRATS2018 \u548c BRATS2020 \u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u3002 \n**|\n", "2406.08226": "|**2024-06-12**|**DistilDoc: Knowledge Distillation for Visually-Rich Document Applications**|[2406.08226](http://arxiv.org/abs/2406.08226)|null|**This work explores knowledge distillation (KD) for visually-rich document (VRD) applications such as document layout analysis (DLA) and document image classification (DIC). While VRD research is dependent on increasingly sophisticated and cumbersome models, the field has neglected to study efficiency via model compression. Here, we design a KD experimentation methodology for more lean, performant models on document understanding (DU) tasks that are integral within larger task pipelines. We carefully selected KD strategies (response-based, feature-based) for distilling knowledge to and from backbones with different architectures (ResNet, ViT, DiT) and capacities (base, small, tiny). We study what affects the teacher-student knowledge gap and find that some methods (tuned vanilla KD, MSE, SimKD with an apt projector) can consistently outperform supervised student training. Furthermore, we design downstream task setups to evaluate covariate shift and the robustness of distilled DLA models on zero-shot layout-aware document visual question answering (DocVQA). DLA-KD experiments result in a large mAP knowledge gap, which unpredictably translates to downstream robustness, accentuating the need to further explore how to efficiently obtain more semantic document layout awareness.**|\n", "2406.08119": "|**2024-06-12**|**Low-Complexity Acoustic Scene Classification Using Parallel Attention-Convolution Network**|[2406.08119](http://arxiv.org/abs/2406.08119)|null|**\u8fd9\u9879\u5de5\u4f5c\u662f\u6211\u4eec\u63d0\u4ea4\u7ed9DCASE2023\u6311\u6218\u8d5b\u4efb\u52a11\u7684\u4e00\u4e2a\u6539\u8fdb\u7cfb\u7edf\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7684\u58f0\u573a\u666f\u5206\u7c7b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528\u5e76\u884c\u6ce8\u610f\u529b-\u5377\u79ef\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u7531\u56db\u4e2a\u6a21\u5757\u7ec4\u6210\uff0c\u5305\u62ec\u9884\u5904\u7406\u3001\u878d\u5408\u3001\u5168\u5c40\u548c\u5c40\u90e8\u4e0a\u4e0b\u6587\u4fe1\u606f\u63d0\u53d6\u3002\u6240\u63d0\u51fa\u7684\u7f51\u7edc\u5728\u4ece\u6bcf\u4e2a\u97f3\u9891\u7247\u6bb5\u4e2d\u6355\u83b7\u5168\u5c40\u548c\u5c40\u90e8\u4e0a\u4e0b\u6587\u4fe1\u606f\u65b9\u9762\u8ba1\u7b97\u6548\u7387\u5f88\u9ad8\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06\u5176\u4ed6\u6280\u672f\u96c6\u6210\u5230\u6211\u4eec\u7684\u65b9\u6cd5\u4e2d\uff0c\u4f8b\u5982\u77e5\u8bc6\u84b8\u998f\u3001\u6570\u636e\u589e\u5f3a\u548c\u81ea\u9002\u5e94\u6b8b\u5dee\u5f52\u4e00\u5316\u3002\u5728DCASE2023\u6311\u6218\u8d5b\u7684\u5b98\u65b9\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u65f6\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u83b7\u5f97\u4e8656.10%\u7684\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u53c2\u6570\u91cf\u4e3a5.21\u5343\uff0c\u4e58\u52a0\u8fd0\u7b97\u6b21\u6570\u4e3a144\u4e07\u6b21\u3002\u5b83\u5728\u51c6\u786e\u6027\u548c\u590d\u6742\u5ea6\u4e0a\u90fd\u8d85\u8fc7\u4e86DCASE2023\u6311\u6218\u8d5b\u7684\u524d\u4e24\u540d\u7cfb\u7edf\uff0c\u83b7\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u4ee3\u7801\u4f4d\u4e8e\uff1ahttps://github.com/Jessytan/Low-complexity-ASC\u3002 \n**|\n", "2406.08528": "|**2024-06-14**|**Adaptive Teaching with Shared Classifier for Knowledge Distillation**|[2406.08528](http://arxiv.org/abs/2406.08528)|**[link](https://github.com/random2314235/atsc)**|**Knowledge distillation (KD) is a technique used to transfer knowledge from an overparameterized teacher network to a less-parameterized student network, thereby minimizing the incurred performance loss. KD methods can be categorized into offline and online approaches. Offline KD leverages a powerful pretrained teacher network, while online KD allows the teacher network to be adjusted dynamically to enhance the learning effectiveness of the student network. Recently, it has been discovered that sharing the classifier of the teacher network can significantly boost the performance of the student network with only a minimal increase in the number of network parameters. Building on these insights, we propose adaptive teaching with a shared classifier (ATSC). In ATSC, the pretrained teacher network self-adjusts to better align with the learning needs of the student network based on its capabilities, and the student network benefits from the shared classifier, enhancing its performance. Additionally, we extend ATSC to environments with multiple teachers. We conduct extensive experiments, demonstrating the effectiveness of the proposed KD method. Our approach achieves state-of-the-art results on the CIFAR-100 and ImageNet datasets in both single-teacher and multiteacher scenarios, with only a modest increase in the number of required model parameters. The source code is publicly available at https://github.com/random2314235/ATSC.**|\n", "2406.07909": "|**2024-06-12**|**Guiding Frame-Level CTC Alignments Using Self-knowledge Distillation**|[2406.07909](http://arxiv.org/abs/2406.07909)|null|**Transformer encoder with connectionist temporal classification (CTC) framework is widely used for automatic speech recognition (ASR). However, knowledge distillation (KD) for ASR displays a problem of disagreement between teacher-student models in frame-level alignment which ultimately hinders it from improving the student model's performance. In order to resolve this problem, this paper introduces a self-knowledge distillation (SKD) method that guides the frame-level alignment during the training time. In contrast to the conventional method using separate teacher and student models, this study introduces a simple and effective method sharing encoder layers and applying the sub-model as the student model. Overall, our approach is effective in improving both the resource efficiency as well as performance. We also conducted an experimental analysis of the spike timings to illustrate that the proposed method improves performance by reducing the alignment disagreement.**|\n", "2406.07876": "|**2024-06-12**|**Small Scale Data-Free Knowledge Distillation**|[2406.07876](http://arxiv.org/abs/2406.07876)|**[link](https://github.com/osvai/ssd-kd)**|**\u65e0\u6570\u636e\u77e5\u8bc6\u84b8\u998f\u80fd\u591f\u5229\u7528\u5927\u578b\u6559\u5e08\u7f51\u7edc\u5b66\u4e60\u5230\u7684\u77e5\u8bc6\u6765\u589e\u5f3a\u5c0f\u578b\u5b66\u751f\u7f51\u7edc\u7684\u8bad\u7ec3\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u539f\u59cb\u8bad\u7ec3\u6570\u636e\uff0c\u4ece\u800c\u907f\u514d\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9690\u79c1\u3001\u5b89\u5168\u548c\u4e13\u6709\u98ce\u9669\u3002\u5728\u8fd9\u4e00\u7814\u7a76\u65b9\u5411\u4e0a\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9075\u5faa\u4e00\u79cd\u53cd\u6f14-\u84b8\u998f\u8303\u5f0f\uff0c\u5176\u4e2d\u4f7f\u7528\u5728\u9884\u8bad\u7ec3\u6559\u5e08\u7f51\u7edc\u6307\u5bfc\u4e0b\u52a8\u6001\u8bad\u7ec3\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u6765\u5408\u6210\u7528\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u5927\u89c4\u6a21\u6837\u672c\u96c6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u4e86\u8fd9\u79cd\u5e38\u89c1\u7684\u65e0\u6570\u636e\u77e5\u8bc6\u84b8\u998f\u8303\u5f0f\uff0c\u5e76\u901a\u8fc7\u201c\u7528\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u5c0f\u89c4\u6a21\u53cd\u6f14\u6570\u636e\u201d\u7684\u89c6\u89d2\u5c55\u793a\u4e86\u5728\u6574\u4f53\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u4ecd\u6709\u76f8\u5f53\u5927\u7684\u63d0\u5347\u7a7a\u95f4\u3002\u57fa\u4e8e\u4e09\u4e2a\u7ecf\u9a8c\u8bc1\u7684\u89c2\u5bdf\u7ed3\u679c\uff0c\u8fd9\u4e9b\u89c2\u5bdf\u7ed3\u679c\u8868\u660e\u4e86\u5728\u6570\u636e\u53cd\u6f14\u548c\u84b8\u998f\u8fc7\u7a0b\u4e2d\u5982\u4f55\u5e73\u8861\u7c7b\u522b\u5206\u5e03\u5728\u5408\u6210\u6837\u672c\u591a\u6837\u6027\u548c\u96be\u5ea6\u65b9\u9762\u7684\u6743\u8861\u7684\u91cd\u8981\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5c0f\u89c4\u6a21\u65e0\u6570\u636e\u77e5\u8bc6\u84b8\u998fSSD-KD\u3002\u5177\u4f53\u800c\u8a00\uff0cSSD-KD\u5f15\u5165\u4e86\u4e00\u4e2a\u8c03\u5236\u51fd\u6570\u6765\u5e73\u8861\u5408\u6210\u6837\u672c\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u4f18\u5148\u91c7\u6837\u51fd\u6570\u6765\u9009\u62e9\u5408\u9002\u7684\u6837\u672c\uff0c\u8fd9\u4e9b\u51fd\u6570\u7531\u52a8\u6001\u56de\u653e\u7f13\u51b2\u533a\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8f85\u52a9\u3002\u56e0\u6b64\uff0cSSD-KD\u53ef\u4ee5\u5728\u6781\u5c0f\u89c4\u6a21\u7684\u5408\u6210\u6837\u672c\uff08\u4f8b\u5982\uff0c\u6bd4\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u5c1110\u500d\uff09\u4e0a\u8fdb\u884c\u84b8\u998f\u8bad\u7ec3\uff0c\u8fd9\u4f7f\u5f97\u6574\u4f53\u8bad\u7ec3\u6548\u7387\u6bd4\u8bb8\u591a\u4e3b\u6d41\u65b9\u6cd5\u5feb\u4e00\u5230\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u4fdd\u6301\u4f18\u4e8e\u6216\u53ef\u5ab2\u7f8e\u7684\u6a21\u578b\u6027\u80fd\uff0c\u8fd9\u4e00\u70b9\u5728\u6d41\u884c\u7684\u56fe\u50cf\u5206\u7c7b\u548c\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u5230\u4e86\u8bc1\u660e\u3002\u4ee3\u7801\u53ef\u5728https://github.com/OSVAI/SSD-KD\u83b7\u53d6\u3002 \n**|\n"}, "OCR": {"2406.10085": "|**2024-06-14**|**Enhancing Question Answering on Charts Through Effective Pre-training Tasks**|[2406.10085](http://arxiv.org/abs/2406.10085)|null|**\u8981\u5b8c\u5168\u7406\u89e3\u4e00\u4e2a\u6587\u6863\uff0c\u4ec5\u4f7f\u7528\u6587\u672c\u4fe1\u606f\u662f\u4e0d\u591f\u7684\u3002\u7406\u89e3\u89c6\u89c9\u7ebf\u7d22\uff0c\u5982\u5e03\u5c40\u548c\u56fe\u8868\uff0c\u4e5f\u662f\u5fc5\u8981\u7684\u3002\u867d\u7136\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6587\u6863\u7406\u89e3\u65b9\u6cd5\uff08\u57fa\u4e8e OCR \u548c\u975e\u57fa\u4e8e OCR \u7684\uff09\u8fd0\u4f5c\u826f\u597d\uff0c\u4f46\u5c1a\u672a\u5bf9\u5176\u529f\u80fd\u548c\u5c40\u9650\u6027\u8fdb\u884c\u5168\u9762\u5206\u6790\u3002\u56e0\u6b64\uff0c\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u5f53\u524d VisualQA \u6a21\u578b\u5728\u5e94\u7528\u4e8e\u56fe\u8868\u65f6\u5b58\u5728\u7684\u5c40\u9650\u6027\u3002\u4e3a\u4e86\u8c03\u67e5\u6700\u5148\u8fdb\u6a21\u578b\u7684\u7f3a\u70b9\uff0c\u6211\u4eec\u4ee5 ChartQA \u4e3a\u4f8b\u8fdb\u884c\u4e86\u5168\u9762\u7684\u884c\u4e3a\u5206\u6790\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u6a21\u578b\u5728\u56de\u7b54\u4e0e\u56fe\u8868\u7ed3\u6784\u548c\u89c6\u89c9\u4e0a\u4e0b\u6587\u4ee5\u53ca\u6570\u503c\u4fe1\u606f\u76f8\u5173\u7684\u95ee\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e09\u4e2a\u7b80\u5355\u7684\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u4ece\u7ed3\u6784\u89c6\u89c9\u77e5\u8bc6\u53ca\u5176\u5bf9\u6570\u503c\u95ee\u9898\u7684\u7406\u89e3\u65b9\u9762\u52a0\u5f3a\u4e86\u73b0\u6709\u6a21\u578b\u3002\u6211\u4eec\u5728\u4e09\u4e2a\u56fe\u8868\u6570\u636e\u96c6\uff08\u5305\u62ec\u63d0\u53d6\u6027\u548c\u62bd\u8c61\u6027\u95ee\u9898\u6570\u636e\u96c6\uff09\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff08\u79f0\u4e3a MatCha-v2\uff09\uff0c\u5e76\u89c2\u5bdf\u5230\u5b83\u6bd4\u57fa\u7ebf\u6a21\u578b\u5e73\u5747\u63d0\u9ad8\u4e86 1.7%\u3002 \n**|\n", "2406.09779": "|**2024-06-14**|**OSPC: Detecting Harmful Memes with Large Language Model as a Catalyst**|[2406.09779](http://arxiv.org/abs/2406.09779)|null|**\u4f5c\u4e3a\u7f51\u7edc\u4e0a\u5feb\u901f\u4f20\u64ad\u4e2a\u4eba\u89c2\u70b9\u548c\u7acb\u573a\u7684\u5a92\u4ecb\uff0c\u8ff7\u56e0\u4e5f\u7ed9\u793e\u4f1a\u504f\u89c1\u548c\u6b67\u89c6\u7684\u4f20\u64ad\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u65b0\u52a0\u5761\u591a\u5143\u6587\u5316\u548c\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u68c0\u6d4b\u6709\u5bb3\u8ff7\u56e0\u7684\u65b0\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6574\u5408\u4e86\u56fe\u50cf\u63cf\u8ff0\u3001\u5149\u5b66\u5b57\u7b26\u8bc6\u522b (OCR) \u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5206\u6790\uff0c\u4ee5\u5168\u9762\u7406\u89e3\u548c\u5206\u7c7b\u6709\u5bb3\u8ff7\u56e0\u3002\u8be5\u7cfb\u7edf\u5229\u7528 BLIP \u6a21\u578b\u8fdb\u884c\u56fe\u50cf\u63cf\u8ff0\uff0c\u4f7f\u7528 PP-OCR \u548c TrOCR \u8fdb\u884c\u591a\u79cd\u8bed\u8a00\u7684\u6587\u672c\u8bc6\u522b\uff0c\u5e76\u4f7f\u7528 Qwen LLM \u8fdb\u884c\u7ec6\u81f4\u5165\u5fae\u7684\u8bed\u8a00\u7406\u89e3\uff0c\u80fd\u591f\u8bc6\u522b\u4ee5\u82f1\u8bed\u3001\u4e2d\u6587\u3001\u9a6c\u6765\u8bed\u548c\u6cf0\u7c73\u5c14\u8bed\u521b\u5efa\u7684\u8ff7\u56e0\u4e2d\u7684\u6709\u5bb3\u5185\u5bb9\u3002\u4e3a\u4e86\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\uff0c\u6211\u4eec\u5229\u7528 GPT-4V \u6807\u6ce8\u7684\u989d\u5916\u6570\u636e\u5bf9\u65b9\u6cd5\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u65e8\u5728\u5c06 GPT-4V \u5bf9\u6709\u5bb3\u8ff7\u56e0\u7684\u7406\u89e3\u80fd\u529b\u63d0\u70bc\u5230\u6211\u4eec\u7684\u7cfb\u7edf\u4e2d\u3002\u6211\u4eec\u7684\u6846\u67b6\u5728\u7531\u65b0\u52a0\u5761\u4eba\u5de5\u667a\u80fd\u4e3e\u529e\u7684\u7f51\u7edc\u5b89\u5168\u5956\u6311\u6218\u8d5b\u7684\u516c\u5f00\u6392\u884c\u699c\u4e0a\u540d\u5217\u524d\u8305\uff0cAUROC \u4e3a 0.7749\uff0c\u51c6\u786e\u7387\u4e3a 0.7087\uff0c\u8fdc\u8fdc\u9886\u5148\u4e8e\u5176\u4ed6\u56e2\u961f\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u4e4b\u524d\u7684\u57fa\u51c6\uff0cFLAVA \u7684 AUROC \u4e3a 0.5695\uff0cVisualBERT \u7684 AUROC \u4e3a 0.5561\u3002**|\n", "2406.08255": "|**2024-06-12**|**M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation**|[2406.08255](http://arxiv.org/abs/2406.08255)|**[link](https://github.com/amazon-science/m3t-multi-modal-translation-bench)**|**\u6587\u6863\u7ffb\u8bd1\u5bf9\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1 (NMT) \u7cfb\u7edf\u63d0\u51fa\u4e86\u6311\u6218\u3002\u5927\u591a\u6570\u6587\u6863\u7ea7 NMT \u7cfb\u7edf\u4f9d\u8d56\u4e8e\u7cbe\u5fc3\u6574\u7406\u7684\u53e5\u5b50\u7ea7\u5e73\u884c\u6570\u636e\uff0c\u5047\u8bbe\u53ef\u4ee5\u4ece\u6587\u6863\u4e2d\u5b8c\u7f8e\u5730\u63d0\u53d6\u6587\u672c\u53ca\u5176\u7cbe\u786e\u7684\u9605\u8bfb\u987a\u5e8f\u3002\u8fd9\u4e9b\u7cfb\u7edf\u4e5f\u503e\u5411\u4e8e\u5ffd\u7565\u989d\u5916\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u4f8b\u5982\u6587\u6863\u5e03\u5c40\uff0c\u8ba4\u4e3a\u5b83\u4eec\u65e0\u5173\u7d27\u8981\u3002\u7136\u800c\uff0c\u73b0\u5b9e\u4e16\u754c\u7684\u6587\u6863\u901a\u5e38\u5177\u6709\u590d\u6742\u7684\u6587\u672c\u5e03\u5c40\uff0c\u8fd9\u4e0e\u8fd9\u4e9b\u5047\u8bbe\u76f8\u77db\u76fe\u3002\u4ece\u5149\u5b66\u5b57\u7b26\u8bc6\u522b (OCR) \u6216\u542f\u53d1\u5f0f\u89c4\u5219\u4e2d\u63d0\u53d6\u4fe1\u606f\u4f1a\u5bfc\u81f4\u9519\u8bef\uff0c\u5e76\u4e14\u5e03\u5c40\uff08\u4f8b\u5982\uff0c\u6bb5\u843d\u3001\u6807\u9898\uff09\u53ef\u80fd\u4f1a\u4f20\u8fbe\u6587\u672c\u4e0d\u540c\u90e8\u5206\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u8fd9\u79cd\u590d\u6742\u6027\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684 PDF \u6587\u6863\u4e2d\u5c24\u4e3a\u660e\u663e\uff0c\u8fd9\u4e9b\u6587\u6863\u4ee5\u89c6\u89c9\u65b9\u5f0f\u5448\u73b0\u4fe1\u606f\u3002\u672c\u6587\u901a\u8fc7\u4ecb\u7ecd M3T \u6765\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0cM3T \u662f\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4e13\u4e3a\u8bc4\u4f30 NMT \u7cfb\u7edf\u5728\u7ffb\u8bd1\u534a\u7ed3\u6784\u5316\u6587\u6863\u7684\u7efc\u5408\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u800c\u5b9a\u5236\u3002\u8be5\u6570\u636e\u96c6\u65e8\u5728\u5f25\u5408\u6587\u6863\u7ea7 NMT \u7cfb\u7edf\u4e2d\u7684\u8bc4\u4f30\u5dee\u8ddd\uff0c\u627f\u8ba4\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u7a0b\u5e8f\u4e2d\u4e30\u5bcc\u7684\u6587\u672c\u5e03\u5c40\u5e26\u6765\u7684\u6311\u6218\u3002 \n**|\n", "2406.06462": "|**2024-06-10**|**VCR: Visual Caption Restoration**|[2406.06462](http://arxiv.org/abs/2406.06462)|**[link](https://github.com/tianyu-z/vcr)**|**\u6211\u4eec\u63d0\u51fa\u4e86\u89c6\u89c9\u5b57\u5e55\u4fee\u590d (VCR)\uff0c\u8fd9\u662f\u4e00\u9879\u65b0\u9896\u7684\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\uff0c\u5b83\u6311\u6218\u6a21\u578b\u5229\u7528\u56fe\u50cf\u4e2d\u7684\u50cf\u7d20\u7ea7\u63d0\u793a\u6765\u51c6\u786e\u5730\u4fee\u590d\u88ab\u90e8\u5206\u906e\u6321\u7684\u6587\u672c\u3002\u8fd9\u9879\u4efb\u52a1\u6e90\u4e8e\u4ee5\u4e0b\u89c2\u5bdf\u7ed3\u679c\uff1a\u5d4c\u5165\u5728\u56fe\u50cf\u4e2d\u7684\u6587\u672c\u4e0e\u5e38\u89c1\u7684\u89c6\u89c9\u5143\u7d20\u548c\u81ea\u7136\u8bed\u8a00\u6709\u7740\u672c\u8d28\u7684\u4e0d\u540c\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u5bf9\u89c6\u89c9\u3001\u6587\u672c\u548c\u5d4c\u5165\u5728\u56fe\u50cf\u4e2d\u7684\u6587\u672c\u8fdb\u884c\u6a21\u6001\u5bf9\u9f50\u3002\u867d\u7136\u8bb8\u591a\u5de5\u4f5c\u5df2\u7ecf\u5c06\u5d4c\u5165\u5728\u56fe\u50cf\u4e2d\u7684\u6587\u672c\u6574\u5408\u5230\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u4f46\u8fd9\u4e9b\u4efb\u52a1\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\u6216\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\uff0c\u4ece\u800c\u5c06\u4efb\u52a1\u7b80\u5316\u4e3a\u4e3b\u8981\u57fa\u4e8e\u6587\u672c\u7684\u5904\u7406\u3002\u7136\u800c\uff0c\u5728 VCR \u4e2d\uff0c\u57fa\u4e8e\u6587\u672c\u7684\u5904\u7406\u53d8\u5f97\u65e0\u6548\uff0c\u56e0\u4e3a\u51c6\u786e\u7684\u6587\u672c\u4fee\u590d\u53d6\u51b3\u4e8e\u6765\u81ea\u63d0\u4f9b\u7684\u56fe\u50cf\u3001\u4e0a\u4e0b\u6587\u548c\u6765\u81ea\u88ab\u906e\u6321\u6587\u672c\u7684\u5fae\u5c0f\u66b4\u9732\u533a\u57df\u7684\u7ec6\u5fae\u7ebf\u7d22\u7684\u7ec4\u5408\u4fe1\u606f\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u6d41\u7a0b\uff0c\u4f7f\u7528\u56fe\u50cf-\u5b57\u5e55\u5bf9\u751f\u6210 VCR \u4efb\u52a1\u7684\u5408\u6210\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u53ef\u8c03\u8282\u7684\u5b57\u5e55\u53ef\u89c1\u6027\u6765\u63a7\u5236\u4efb\u52a1\u96be\u5ea6\u3002\u5229\u7528\u6b64\u6d41\u7a0b\uff0c\u6211\u4eec\u4f7f\u7528\u6765\u81ea\u7ef4\u57fa\u767e\u79d1\u7684\u5e26\u6709\u5b57\u5e55\u7684\u56fe\u50cf\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a VCR-Wiki \u7684 VCR \u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b 211 \u4e07\u4e2a\u82f1\u6587\u5b9e\u4f53\u548c 34.6 \u4e07\u4e2a\u4e2d\u6587\u5b9e\u4f53\uff0c\u5206\u4e3a\u7b80\u5355\u548c\u56f0\u96be\u4e24\u79cd\u53d8\u4f53\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728 VCR \u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u660e\u663e\u843d\u540e\u4e8e\u4eba\u7c7b\uff0c\u4ec5\u4ec5\u5728\u6211\u4eec\u7684\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u6a21\u578b\u5e76\u4e0d\u4f1a\u5e26\u6765\u663e\u8457\u7684\u6539\u8fdb\u3002\u6211\u4eec\u53d1\u5e03\u4e86 VCR-Wiki \u548c\u6570\u636e\u6784\u5efa\u4ee3\u7801\uff0c\u4ee5\u4fc3\u8fdb\u672a\u6765\u7684\u7814\u7a76\u3002**|\n", "2406.04635": "|**2024-06-07**|**Scaling Automatic Extraction of Pseudocode**|[2406.04635](http://arxiv.org/abs/2406.04635)|null|**\u5b66\u672f\u8bba\u6587\u4e2d\u7684\u4f2a\u4ee3\u7801\u63d0\u4f9b\u4e86\u4e00\u79cd\u8868\u8fbe\u5176\u4e2d\u7b97\u6cd5\u7684\u7b80\u6d01\u65b9\u6cd5\u3002\u4f2a\u4ee3\u7801\u4e5f\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u4e2d\u95f4\u8868\u793a\uff0c\u6709\u52a9\u4e8e\u5f25\u5408\u7f16\u7a0b\u8bed\u8a00\u548c\u81ea\u7136\u8bed\u8a00\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u8bbf\u95ee\u5927\u91cf\u4f2a\u4ee3\u7801\u96c6\u5408\u53ef\u4ee5\u5e26\u6765\u5404\u79cd\u597d\u5904\uff0c\u4ece\u589e\u5f3a\u7b97\u6cd5\u7406\u89e3\u3001\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7684\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u5230\u652f\u6301\u57fa\u4e8e NLP \u6216\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u6a21\u578b\u5b8c\u6210\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u548c\u5149\u5b66\u5b57\u7b26\u8bc6\u522b (OCR) \u7b49\u4efb\u52a1\u3002\u6211\u4eec\u901a\u8fc7\u4ece arXiv \u8bba\u6587\u4e2d\u63d0\u53d6\u8fd1 320,000 \u4e2a\u4f2a\u4ee3\u7801\u793a\u4f8b\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u5927\u578b\u4f2a\u4ee3\u7801\u96c6\u5408\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u6d89\u53ca\u626b\u63cf\u8d85\u8fc7 220 \u4e07\u7bc7\u5b66\u672f\u8bba\u6587\uff0c\u5176\u4e2d 1,000 \u7bc7\u7ecf\u8fc7\u4eba\u5de5\u68c0\u67e5\u548c\u6807\u8bb0\u3002\u9274\u4e8e\u96c6\u5408\u56fa\u6709\u7684\u5f02\u8d28\u6027\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec\u4e00\u4e2a\u4e3a\u4f18\u5316\u8986\u76d6\u8303\u56f4\u800c\u5b9a\u5236\u7684\u63d0\u53d6\u673a\u5236\uff0c\u4ee5\u53ca\u4e00\u4e2a\u57fa\u4e8e\u968f\u673a\u62bd\u6837\u7684\u9a8c\u8bc1\u673a\u5236\uff0c\u4ee5\u68c0\u67e5\u5176\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u5bf9\u5e38\u89c1\u4f2a\u4ee3\u7801\u7ed3\u6784\u7684\u89c1\u89e3\uff0c\u5e76\u8f85\u4ee5\u805a\u7c7b\u548c\u7edf\u8ba1\u5206\u6790\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u4e9b\u5206\u6790\u8868\u660e\u4f2a\u4ee3\u7801\u7684\u4f7f\u7528\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u7a81\u51fa\u4e86\u5b83\u4eec\u65e5\u76ca\u589e\u957f\u7684\u91cd\u8981\u6027\u3002 \n**|\n", "2406.04493": "|**2024-06-06**|**CORU: Comprehensive Post-OCR Parsing and Receipt Understanding Dataset**|[2406.04493](http://arxiv.org/abs/2406.04493)|**[link](https://github.com/update-for-integrated-business-ai/coru)**|**In the fields of Optical Character Recognition (OCR) and Natural Language Processing (NLP), integrating multilingual capabilities remains a critical challenge, especially when considering languages with complex scripts such as Arabic. This paper introduces the Comprehensive Post-OCR Parsing and Receipt Understanding Dataset (CORU), a novel dataset specifically designed to enhance OCR and information extraction from receipts in multilingual contexts involving Arabic and English. CORU consists of over 20,000 annotated receipts from diverse retail settings, including supermarkets and clothing stores, alongside 30,000 annotated images for OCR that were utilized to recognize each detected line, and 10,000 items annotated for detailed information extraction. These annotations capture essential details such as merchant names, item descriptions, total prices, receipt numbers, and dates. They are structured to support three primary computational tasks: object detection, OCR, and information extraction. We establish the baseline performance for a range of models on CORU to evaluate the effectiveness of traditional methods, like Tesseract OCR, and more advanced neural network-based approaches. These baselines are crucial for processing the complex and noisy document layouts typical of real-world receipts and for advancing the state of automated multilingual document processing. Our datasets are publicly accessible (https://github.com/Update-For-Integrated-Business-AI/CORU).**|\n", "2406.01033": "|**2024-06-03**|**Generalized Jersey Number Recognition Using Multi-task Learning With Orientation-guided Weight Refinement**|[2406.01033](http://arxiv.org/abs/2406.01033)|null|**Jersey number recognition (JNR) has always been an important task in sports analytics. Improving recognition accuracy remains an ongoing challenge because images are subject to blurring, occlusion, deformity, and low resolution. Recent research has addressed these problems using number localization and optical character recognition. Some approaches apply player identification schemes to image sequences, ignoring the impact of human body rotation angles on jersey digit identification. Accurately predicting the number of jersey digits by using a multi-task scheme to recognize each individual digit enables more robust results. Based on the above considerations, this paper proposes a multi-task learning method called the angle-digit refine scheme (ADRS), which combines human body orientation angles and digit number clues to recognize athletic jersey numbers. Based on our experimental results, our approach increases inference information, significantly improving prediction accuracy. Compared to state-of-the-art methods, which can only handle a single type of sport, the proposed method produces a more diverse and practical JNR application. The incorporation of diverse types of team sports such as soccer, football, basketball, volleyball, and baseball into our dataset contributes greatly to generalized JNR in sports analytics. Our accuracy achieves 64.07% on Top-1 and 89.97% on Top-2, with corresponding F1 scores of 67.46% and 90.64%, respectively.**|\n", "2405.20156": "|**2024-05-30**|**Scaling up archival text analysis with the blockmodeling of n-gram networks -- A case study of Bulgaria's representation in the Osservatore Romano (January-May 1877)**|[2405.20156](http://arxiv.org/abs/2405.20156)|null|**\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5e94\u7528\u7f51\u7edc\u805a\u7c7b\u65b9\u6cd5\u5206\u6790 1877 \u5e74 1 \u6708\u81f3 5 \u6708\u671f\u95f4\u51fa\u7248\u7684 123 \u671f\u300a\u7f57\u9a6c\u89c2\u5bdf\u62a5\u300b\u5bf9\u4fdd\u52a0\u5229\u4e9a\u7684\u62a5\u9053\uff0c\u4ece\u800c\u5f25\u5408\u6863\u6848\u6587\u672c\u5206\u6790\u4e0e\u7f51\u7edc\u5206\u6790\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u672c\u7814\u7a76\u5229\u7528\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\u548c\u5e7f\u4e49\u540c\u8d28\u6027\u5757\u6a21\u578b\u6784\u5efa\u4e86\u76f8\u5173\u5173\u952e\u8bcd\u7f51\u7edc\u3002\u5305\u62ec\u201c\u4fdd\u52a0\u5229\u4e9a\u201d\u548c\u201c\u4fc4\u7f57\u65af\u201d\u7684\u96c6\u5408\u7f51\u7edc\u540c\u6784\u6027\u8f83\u9ad8\uff0c\u5e76\u4e14\u4e0e\u201c\u5fb7\u56fd\u201d\u3001\u201c\u82f1\u56fd\u201d\u548c\u201c\u6218\u4e89\u201d\u7684\u96c6\u5408\u7f51\u7edc\u6709\u5f88\u5927\u7a0b\u5ea6\u7684\u91cd\u53e0\u3002\u5728\u7ed3\u6784\u65b9\u9762\uff0c\u8fd9\u4e24\u4e2a\u7f51\u7edc\u7684\u5757\u6a21\u578b\u5448\u73b0\u51fa\u6e05\u6670\u7684\u6838\u5fc3-\u534a\u8fb9\u7f18-\u8fb9\u7f18\u7ed3\u6784\uff0c\u53cd\u6620\u4e86\u62a5\u7eb8\u62a5\u9053\u4e2d\u6982\u5ff5\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u8be5\u62a5\u7684\u8bcd\u6c47\u9009\u62e9\u6709\u6548\u5730\u6d88\u89e3\u4e86\u4fdd\u52a0\u5229\u4e9a\u6c11\u65cf\u590d\u5174\u8fd0\u52a8\u7684\u5408\u6cd5\u6027\uff0c\u7a81\u51fa\u4e86\u7f57\u9a6c\u6559\u5ef7\u5bf9\u8be5\u62a5\u7f16\u8f91\u8def\u7ebf\u7684\u5f71\u54cd\u3002 \n**|\n", "2405.19765": "|**2024-05-30**|**Towards Unified Multi-granularity Text Detection with Interactive Attention**|[2405.19765](http://arxiv.org/abs/2405.19765)|null|**\u73b0\u6709\u7684OCR\u5f15\u64ce\u6216\u6587\u6863\u56fe\u50cf\u5206\u6790\u7cfb\u7edf\u901a\u5e38\u4f9d\u8d56\u4e8e\u9488\u5bf9\u4e0d\u540c\u573a\u666f\u548c\u7c92\u5ea6\u7684\u6587\u672c\u68c0\u6d4b\u8bad\u7ec3\u5355\u72ec\u7684\u6a21\u578b\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u5de8\u5927\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u8d44\u6e90\u9700\u6c42\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u201c\u68c0\u6d4b\u4efb\u4f55\u6587\u672c\u201d\uff08DAT\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5148\u8fdb\u7684\u8303\u4f8b\uff0c\u5b83\u5c06\u573a\u666f\u6587\u672c\u68c0\u6d4b\u3001\u5e03\u5c40\u5206\u6790\u548c\u6587\u6863\u9875\u9762\u68c0\u6d4b\u65e0\u7f1d\u5730\u7edf\u4e00\u5230\u4e00\u4e2a cohesive \u7684\u7aef\u5230\u7aef\u6a21\u578b\u4e2d\u3002\u8fd9\u79cd\u8bbe\u8ba1\u4f7f DAT \u80fd\u591f\u6709\u6548\u5730\u7ba1\u7406\u4e0d\u540c\u7c92\u5ea6\u7684\u6587\u672c\u5b9e\u4f8b\uff0c\u5305\u62ec*\u5355\u8bcd*\u3001*\u884c*\u3001*\u6bb5\u843d*\u548c*\u9875\u9762*\u3002DAT \u7684\u4e00\u9879\u5173\u952e\u521b\u65b0\u662f\u8de8\u7c92\u5ea6\u4ea4\u4e92\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5b83\u901a\u8fc7\u5173\u8054\u4e0d\u540c\u6587\u672c\u67e5\u8be2\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u4e0d\u540c\u7c92\u5ea6\u6587\u672c\u5b9e\u4f8b\u7684\u8868\u793a\u5b66\u4e60\u3002\u56e0\u6b64\uff0c\u5b83\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u591a\u4e2a\u6587\u672c\u7c92\u5ea6\u4e0a\u5b9e\u73b0\u4e92\u60e0\u4e92\u5229\u7684\u68c0\u6d4b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u63d0\u793a\u7684\u5206\u5272\u6a21\u5757\u4f18\u5316\u4e86\u4efb\u610f\u66f2\u7387\u548c\u590d\u6742\u5e03\u5c40\u6587\u672c\u7684\u68c0\u6d4b\u7ed3\u679c\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86 DAT \u7684\u51c6\u786e\u6027\u5e76\u6269\u5c55\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u9002\u7528\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDAT \u5728\u5404\u79cd\u6587\u672c\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5305\u62ec\u591a\u65b9\u5411/\u4efb\u610f\u5f62\u72b6\u573a\u666f\u6587\u672c\u68c0\u6d4b\u3001\u6587\u6863\u5e03\u5c40\u5206\u6790\u548c\u9875\u9762\u68c0\u6d4b\u4efb\u52a1\u3002 \n**|\n", "2405.18620": "|**2024-05-28**|**RealitySummary: On-Demand Mixed Reality Document Enhancement using Large Language Models**|[2405.18620](http://arxiv.org/abs/2405.18620)|null|**\u6211\u4eec\u63a8\u51fa\u4e86 RealitySummary\uff0c\u8fd9\u662f\u4e00\u79cd\u6df7\u5408\u73b0\u5b9e\u9605\u8bfb\u52a9\u624b\uff0c\u53ef\u4ee5\u4f7f\u7528\u6309\u9700\u6587\u672c\u63d0\u53d6\u3001\u6458\u8981\u548c\u589e\u5f3a\u529f\u80fd\u6765\u589e\u5f3a\u4efb\u4f55\u5370\u5237\u6216\u6570\u5b57\u6587\u6863\u3002\u867d\u7136\u589e\u5f3a\u9605\u8bfb\u5de5\u5177\u627f\u8bfa\u901a\u8fc7\u8986\u76d6\u6570\u5b57\u5185\u5bb9\u6765\u589e\u5f3a\u7269\u7406\u9605\u8bfb\u4f53\u9a8c\uff0c\u4f46\u4ee5\u524d\u7684\u7cfb\u7edf\u901a\u5e38\u9700\u8981\u9884\u5904\u7406\u6587\u6863\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u901a\u7528\u6027\u548c\u73b0\u5b9e\u7528\u4f8b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a2\u7d22\u6309\u9700\u6587\u6863\u589e\u5f3a\u3002\u4e3a\u4e86\u89e3\u4e0d\u540c\u6587\u6863\u7684\u901a\u7528\u6280\u672f\uff0c\u6211\u4eec\u9996\u5148\u8fdb\u884c\u4e86\u4e00\u9879\u63a2\u7d22\u6027\u8bbe\u8ba1\u7814\u7a76\uff0c\u786e\u5b9a\u4e86\u4e94\u7c7b\u6587\u6863\u589e\u5f3a\u529f\u80fd\uff08\u6458\u8981\u3001\u589e\u5f3a\u3001\u5bfc\u822a\u3001\u6bd4\u8f83\u548c\u63d0\u53d6\uff09\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u53ef\u4ee5\u4f7f\u7528 Google Cloud OCR \u548c GPT-4 \u81ea\u52a8\u63d0\u53d6\u548c\u6c47\u603b\u6587\u672c\uff0c\u7136\u540e\u4f7f\u7528 Microsoft Hololens 2 \u548c Apple Vision Pro \u5728\u6587\u6863\u5468\u56f4\u5d4c\u5165\u4fe1\u606f\u3002\u6211\u4eec\u5c55\u793a\u4e86\u516d\u79cd\u7279\u5b9a\u6587\u6863\u589e\u5f3a\u7684\u5b9e\u65f6\u793a\u4f8b\uff1a1) \u6458\u8981\uff0c2) \u6bd4\u8f83\u8868\uff0c3) \u65f6\u95f4\u7ebf\uff0c4) \u5173\u952e\u8bcd\u5217\u8868\uff0c5) \u6458\u8981\u7a81\u51fa\u663e\u793a\u548c 6) \u4fe1\u606f\u5361\u3002\u53ef\u7528\u6027\u7814\u7a76 (N=12) \u548c\u91ce\u5916\u7814\u7a76 (N=11) \u7684\u7ed3\u679c\u7a81\u51fa\u4e86\u6309\u9700 MR \u6587\u6863\u589e\u5f3a\u7684\u6f5c\u5728\u597d\u5904\u4ee5\u53ca\u672a\u6765\u7814\u7a76\u7684\u673a\u4f1a\u3002 \n**|\n"}, "\u751f\u6210\u6a21\u578b": {"2406.10225": "|**2024-06-14**|**SatDiffMoE: A Mixture of Estimation Method for Satellite Image Super-resolution with Latent Diffusion Models**|[2406.10225](http://arxiv.org/abs/2406.10225)|null|**\u5728\u83b7\u53d6\u536b\u661f\u56fe\u50cf\u65f6\uff0c\u7531\u4e8e\u536b\u661f\u6210\u50cf\u7cfb\u7edf\u673a\u8f7d\u4f20\u611f\u5668\u7684\u9650\u5236\uff0c\u901a\u5e38\u9700\u8981\u5728\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u65f6\u95f4\u5206\u8fa8\u7387\uff08\u83b7\u53d6\u9891\u7387\uff09\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u5bf9\u4e8e\u571f\u5730\u4f5c\u7269\u76d1\u6d4b\u3001\u57ce\u5e02\u89c4\u5212\u3001\u91ce\u706b\u7ba1\u7406\u548c\u5404\u79cd\u5e94\u7528\u975e\u5e38\u91cd\u8981\u3002\u5b9e\u73b0\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u536b\u661f\u6210\u50cf\u662f\u4e00\u9879\u91cd\u8981\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u968f\u7740\u6269\u6563\u6a21\u578b\u7684\u51fa\u73b0\uff0c\u6211\u4eec\u73b0\u5728\u53ef\u4ee5\u5b66\u4e60\u5f3a\u5927\u7684\u751f\u6210\u5148\u9a8c\uff0c\u4ee5\u751f\u6210\u5177\u6709\u9ad8\u5206\u8fa8\u7387\u7684\u771f\u5b9e\u536b\u661f\u56fe\u50cf\uff0c\u8fd9\u4e5f\u53ef\u4ee5\u7528\u4e8e\u4fc3\u8fdb\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6269\u6563\u7684\u878d\u5408\u7b97\u6cd5\uff0c\u79f0\u4e3a SatDiffMoE\uff0c\u5b83\u53ef\u4ee5\u5c06\u4efb\u610f\u6570\u91cf\u7684\u540c\u4e00\u4f4d\u7f6e\u7684\u8fde\u7eed\u4f4e\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u901a\u8fc7\u5229\u7528\u548c\u878d\u5408\u6765\u81ea\u4e0d\u540c\u65f6\u95f4\u70b9\u7684\u8865\u5145\u4fe1\u606f\uff0c\u5c06\u5b83\u4eec\u878d\u5408\u6210\u4e00\u4e2a\u5177\u6709\u66f4\u591a\u7cbe\u7ec6\u7ec6\u8282\u7684\u9ad8\u5206\u8fa8\u7387\u91cd\u5efa\u56fe\u50cf\u3002\u6211\u4eec\u7684\u7b97\u6cd5\u975e\u5e38\u7075\u6d3b\uff0c\u5141\u8bb8\u5bf9\u4efb\u610f\u6570\u91cf\u7684\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\u548c\u63a8\u7406\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684 SatDiffMoE \u65b9\u6cd5\u4e0d\u4ec5\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u7684\u536b\u661f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u800c\u4e14\u4e0e\u4ee5\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8fd8\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u5e76\u51cf\u5c11\u4e86\u6a21\u578b\u53c2\u6570\u3002 \n**|\n", "2406.10214": "|**2024-06-14**|**Universal randomised signatures for generative time series modelling**|[2406.10214](http://arxiv.org/abs/2406.10214)|**[link](https://github.com/niklaswalter/randomised-signature-timeseries-generation)**|**\u968f\u673a\u7b7e\u540d\u5df2\u88ab\u63d0\u51fa\u4f5c\u4e3a\u4e00\u79cd\u7075\u6d3b\u4e14\u6613\u4e8e\u5b9e\u73b0\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7528\u4e8e\u66ff\u4ee3\u5df2\u5efa\u7acb\u7684\u8def\u5f84\u7b7e\u540d\u65b9\u6cd5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u91c7\u7528\u968f\u673a\u7b7e\u540d\uff0c\u672c\u7740\u6c34\u5e93\u8ba1\u7b97\u7684\u7cbe\u795e\uff0c\u4e3a\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5f15\u5165\u4e86\u4e00\u4e2a\u751f\u6210\u6a21\u578b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u79bb\u6563\u65f6\u95f4\u968f\u673a\u7b7e\u540d\u7684\u5168\u65b0Wasserstein\u7c7b\u578b\u8ddd\u79bb\u3002\u8fd9\u79cd\u6982\u7387\u6d4b\u5ea6\u7a7a\u95f4\u4e0a\u7684\u5ea6\u91cf\u6355\u6349\u4e86\uff08\u6761\u4ef6\uff09\u5206\u5e03\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002\u6211\u4eec\u5173\u4e8e\u968f\u673a\u7b7e\u540d\u5728\u4ee5\u57fa\u7840\u8def\u5f84\u4e3a\u8f93\u5165\u7684\u8fde\u7eed\u51fd\u6570\u7a7a\u95f4\u4e0a\u7684\u901a\u7528\u903c\u8fd1\u6027\u7684\u65b0\u7ed3\u679c\u8bc1\u660e\u4e86\u5176\u4f7f\u7528\u7684\u5408\u7406\u6027\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06\u5ea6\u91cf\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u57fa\u4e8e\u6c34\u5e93\u795e\u7ecf\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u7684\u5408\u6210\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u975e\u5bf9\u6297\u6027\u751f\u6210\u5668\u6a21\u578b\u4e2d\u3002\u6211\u4eec\u5c06\u6a21\u578b\u7684\u7ed3\u679c\u4e0e\u73b0\u6709\u6587\u732e\u4e2d\u7684\u57fa\u51c6\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002 \n**|\n", "2406.10211": "|**2024-06-14**|**DiffusionBlend: Learning 3D Image Prior through Position-aware Diffusion Score Blending for 3D Computed Tomography Reconstruction**|[2406.10211](http://arxiv.org/abs/2406.10211)|null|**Diffusion models face significant challenges when employed for large-scale medical image reconstruction in real practice such as 3D Computed Tomography (CT). Due to the demanding memory, time, and data requirements, it is difficult to train a diffusion model directly on the entire volume of high-dimensional data to obtain an efficient 3D diffusion prior. Existing works utilizing diffusion priors on single 2D image slice with hand-crafted cross-slice regularization would sacrifice the z-axis consistency, which results in severe artifacts along the z-axis. In this work, we propose a novel framework that enables learning the 3D image prior through position-aware 3D-patch diffusion score blending for reconstructing large-scale 3D medical images. To the best of our knowledge, we are the first to utilize a 3D-patch diffusion prior for 3D medical image reconstruction. Extensive experiments on sparse view and limited angle CT reconstruction show that our DiffusionBlend method significantly outperforms previous methods and achieves state-of-the-art performance on real-world CT reconstruction problems with high-dimensional 3D image (i.e., $256 \\times 256 \\times 500$). Our algorithm also comes with better or comparable computational efficiency than previous state-of-the-art methods.**|\n", "2406.10210": "|**2024-06-14**|**Make It Count: Text-to-Image Generation with an Accurate Number of Objects**|[2406.10210](http://arxiv.org/abs/2406.10210)|null|**Despite the unprecedented success of text-to-image diffusion models, controlling the number of depicted objects using text is surprisingly hard. This is important for various applications from technical documents, to children's books to illustrating cooking recipes. Generating object-correct counts is fundamentally challenging because the generative model needs to keep a sense of separate identity for every instance of the object, even if several objects look identical or overlap, and then carry out a global computation implicitly during generation. It is still unknown if such representations exist. To address count-correct generation, we first identify features within the diffusion model that can carry the object identity information. We then use them to separate and count instances of objects during the denoising process and detect over-generation and under-generation. We fix the latter by training a model that predicts both the shape and location of a missing object, based on the layout of existing ones, and show how it can be used to guide denoising with correct object count. Our approach, CountGen, does not depend on external source to determine object layout, but rather uses the prior from the diffusion model itself, creating prompt-dependent and seed-dependent layouts. Evaluated on two benchmark datasets, we find that CountGen strongly outperforms the count-accuracy of existing baselines.**|\n", "2406.10208": "|**2024-06-14**|**Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual Visual Text Rendering**|[2406.10208](http://arxiv.org/abs/2406.10208)|null|**Recently, Glyph-ByT5 has achieved highly accurate visual text rendering performance in graphic design images. However, it still focuses solely on English and performs relatively poorly in terms of visual appeal. In this work, we address these two fundamental limitations by presenting Glyph-ByT5-v2 and Glyph-SDXL-v2, which not only support accurate visual text rendering for 10 different languages but also achieve much better aesthetic quality. To achieve this, we make the following contributions: (i) creating a high-quality multilingual glyph-text and graphic design dataset consisting of more than 1 million glyph-text pairs and 10 million graphic design image-text pairs covering nine other languages, (ii) building a multilingual visual paragraph benchmark consisting of 1,000 prompts, with 100 for each language, to assess multilingual visual spelling accuracy, and (iii) leveraging the latest step-aware preference learning approach to enhance the visual aesthetic quality. With the combination of these techniques, we deliver a powerful customized multilingual text encoder, Glyph-ByT5-v2, and a strong aesthetic graphic generation model, Glyph-SDXL-v2, that can support accurate spelling in 10 different languages. We perceive our work as a significant advancement, considering that the latest DALL-E3 and Ideogram 1.0 still struggle with the multilingual visual text rendering task.**|\n", "2406.10197": "|**2024-06-14**|**Crafting Parts for Expressive Object Composition**|[2406.10197](http://arxiv.org/abs/2406.10197)|null|**Text-to-image generation from large generative models like Stable Diffusion, DALLE-2, etc., have become a common base for various tasks due to their superior quality and extensive knowledge bases. As image composition and generation are creative processes the artists need control over various parts of the images being generated. We find that just adding details about parts in the base text prompt either leads to an entirely different image (e.g., missing/incorrect identity) or the extra part details simply being ignored. To mitigate these issues, we introduce PartCraft, which enables image generation based on fine-grained part-level details specified for objects in the base text prompt. This allows more control for artists and enables novel object compositions by combining distinctive object parts. PartCraft first localizes object parts by denoising the object region from a specific diffusion process. This enables each part token to be localized to the right object region. After obtaining part masks, we run a localized diffusion process in each of the part regions based on fine-grained part descriptions and combine them to produce the final image. All the stages of PartCraft are based on repurposing a pre-trained diffusion model, which enables it to generalize across various domains without training. We demonstrate the effectiveness of part-level control provided by PartCraft qualitatively through visual examples and quantitatively in comparison to the contemporary baselines.**|\n", "2406.10175": "|**2024-06-14**|**Enhancing Incomplete Multi-modal Brain Tumor Segmentation with Intra-modal Asymmetry and Inter-modal Dependency**|[2406.10175](http://arxiv.org/abs/2406.10175)|null|**\u8fd1\u5e74\u6765\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u591a\u6a21\u6001MRI\u56fe\u50cf\u8111\u80bf\u7624\u5206\u5272(BTS)\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u5b9e\u8df5\u4e2d\u4e00\u4e2a\u5e38\u89c1\u95ee\u9898\u662f\uff0c\u7531\u4e8e\u626b\u63cf\u65b9\u6848\u548c\u60a3\u8005\u60c5\u51b5\u4e0d\u540c\uff0c\u67d0\u4e9b\u6a21\u6001\u7684\u56fe\u50cf\u53ef\u80fd\u65e0\u6cd5\u83b7\u5f97\uff0c\u8fd9\u4f7f\u5f97\u4ece\u4e0d\u5b8c\u6574MRI\u6a21\u6001\u8fdb\u884c\u5206\u5272\u6210\u4e3a\u4e00\u4e2a\u96be\u9898\u3002\u4ee5\u5f80\u7684\u65b9\u6cd5\u8bd5\u56fe\u901a\u8fc7\u878d\u5408\u53ef\u83b7\u5f97\u7684\u591a\u6a21\u6001\u7279\u5f81\u3001\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u548c\u4f7f\u7528\u751f\u6210\u6a21\u578b\u5408\u6210\u7f3a\u5931\u6a21\u6001\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5ffd\u7565\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u5185\u5728\u95ee\u9898\uff0c\u4f8b\u5982\u8bad\u7ec3\u6837\u672c\u6709\u9650\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5b58\u5728\u80bf\u7624\u7684\u60c5\u51b5\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u7f3a\u5931\u6a21\u6001\u5b50\u96c6\u8bad\u7ec3\u548c\u90e8\u7f72\u7279\u5b9a\u7684\u6a21\u578b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u4ece\u4e24\u4e2a\u65b9\u9762\u589e\u5f3aBTS\u6a21\u578b\u3002\u9996\u5148\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u751f\u6210\u4e00\u4e2a\u6db5\u76d6\u5404\u79cd\u80bf\u7624\u5f62\u72b6\u548c\u8111\u90e8\u89e3\u5256\u7ed3\u6784\u7ec4\u5408\u7684\u591a\u6837\u5316\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u5176\u6b21\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540e\u8bad\u7ec3\u9636\u6bb5\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u53ea\u6709\u90e8\u5206\u6a21\u6001\u53ef\u7528\u7684\u60c5\u51b5\u4e0b\uff0c\u91cd\u5efa\u9884\u6d4b\u7ed3\u679c\u4e2d\u7f3a\u5931\u7684\u6a21\u6001\u3002\u4e3a\u4e86\u5b9e\u73b0\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u6211\u4eec\u4ece\u6982\u5ff5\u4e0a\u5c06MRI\u56fe\u50cf\u89e3\u8026\u4e3a\u4e24\u90e8\u5206\uff1a\u201c\u89e3\u5256\u7ed3\u6784\u201d\u548c\u201c\u80bf\u7624\u201d\u3002\u6211\u4eec\u4f7f\u7528\u4ece\u4e0d\u540c\u8bad\u7ec3\u6837\u672c\u7684\u89e3\u5256\u7ed3\u6784\u548c\u80bf\u7624\u90e8\u5206\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5bf9BTS\u6a21\u578b\u8fdb\u884c\u9884\u8bad\u7ec3\u3002\u2026\u2026\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5e76\u5728\u4e09\u4e2a\u8111\u80bf\u7624\u5206\u5272\u6570\u636e\u96c6BRATS2020\u3001BRATS2018\u548cBRATS2015\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002 \n**|\n", "2406.10126": "|**2024-06-14**|**Training-free Camera Control for Video Generation**|[2406.10126](http://arxiv.org/abs/2406.10126)|null|**\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u4e3a\u73b0\u6210\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u76f8\u673a\u8fd0\u52a8\u63a7\u5236\u3002\u4e0e\u4ee5\u5f80\u7684\u5de5\u4f5c\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u9700\u8981\u5728\u76f8\u673a\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4efb\u4f55\u76d1\u7763\u5fae\u8c03\uff0c\u4e5f\u4e0d\u9700\u8981\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u8fdb\u884c\u81ea\u76d1\u7763\u8bad\u7ec3\u3002\u76f8\u53cd\uff0c\u5b83\u53ef\u4ee5\u4e0e\u5927\u591a\u6570\u9884\u8bad\u7ec3\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u5373\u63d2\u5373\u7528\uff0c\u5e76\u4ee5\u5355\u4e2a\u56fe\u50cf\u6216\u6587\u672c\u63d0\u793a\u4f5c\u4e3a\u8f93\u5165\u751f\u6210\u76f8\u673a\u53ef\u63a7\u89c6\u9891\u3002\u6211\u4eec\u5de5\u4f5c\u7684\u7075\u611f\u6765\u81ea\u4e8e\u4e2d\u95f4\u5c42\u6f5c\u5728\u8868\u793a\u5bf9\u751f\u6210\u7ed3\u679c\u5177\u6709\u7684\u5e03\u5c40\u5148\u9a8c\uff0c\u56e0\u6b64\u91cd\u65b0\u6392\u5217\u5176\u4e2d\u7684\u566a\u58f0\u50cf\u7d20\u4e5f\u5c06\u4f7f\u8f93\u51fa\u5185\u5bb9\u91cd\u65b0\u5206\u914d\u3002\u7531\u4e8e\u76f8\u673a\u8fd0\u52a8\u4e5f\u53ef\u4ee5\u770b\u4f5c\u662f\u7531\u89c6\u89d2\u53d8\u5316\u5f15\u8d77\u7684\u50cf\u7d20\u91cd\u6392\uff0c\u56e0\u6b64\u5982\u679c\u89c6\u9891\u7684\u566a\u58f0\u6f5c\u5728\u8868\u793a\u53d1\u751f\u76f8\u5e94\u7684\u53d8\u5316\uff0c\u5219\u53ef\u4ee5\u6309\u7167\u7279\u5b9a\u7684\u76f8\u673a\u8fd0\u52a8\u5bf9\u89c6\u9891\u8fdb\u884c\u91cd\u65b0\u7ec4\u7ec7\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86CamTrol\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e3a\u89c6\u9891\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u76f8\u673a\u63a7\u5236\u80fd\u529b\u3002\u5b83\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u7a0b\u5b9e\u73b0\u3002\u9996\u5148\uff0c\u6211\u4eec\u57283D\u70b9\u4e91\u7a7a\u95f4\u4e2d\u901a\u8fc7\u663e\u5f0f\u76f8\u673a\u8fd0\u52a8\u5bf9\u56fe\u50cf\u5e03\u5c40\u91cd\u6392\u8fdb\u884c\u5efa\u6a21\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5229\u7528\u4e00\u7cfb\u5217\u91cd\u65b0\u6392\u5217\u7684\u56fe\u50cf\u5f62\u6210\u7684\u566a\u58f0\u6f5c\u5728\u8868\u793a\u7684\u5e03\u5c40\u5148\u9a8c\uff0c\u751f\u6210\u5177\u6709\u76f8\u673a\u8fd0\u52a8\u7684\u89c6\u9891\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u63a7\u5236\u751f\u6210\u89c6\u9891\u7684\u76f8\u673a\u8fd0\u52a8\u65b9\u9762\u5177\u6709\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u751f\u6210\u5177\u6709\u52a8\u6001\u5185\u5bb9\u76843D\u65cb\u8f6c\u89c6\u9891\u65b9\u9762\u53ef\u4ee5\u4ea7\u751f\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://lifedecoder.github.io/CamTrol/\u3002 \n**|\n", "2406.10108": "|**2024-06-14**|**Precipitation Nowcasting Using Physics Informed Discriminator Generative Models**|[2406.10108](http://arxiv.org/abs/2406.10108)|null|**\u77ed\u4e34\u9884\u62a5\u5229\u7528\u5b9e\u65f6\u5927\u6c14\u6761\u4ef6\u6765\u9884\u6d4b\u77ed\u671f\u5929\u6c14\u3002\u5305\u62ec PySTEPS \u5728\u5185\u7684\u6700\u5148\u8fdb\u6a21\u578b\u5728\u51c6\u786e\u9884\u6d4b\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u65b9\u9762\u9047\u5230\u56f0\u96be\uff0c\u56e0\u4e3a\u5b83\u4eec\u7684\u5206\u5e03\u6a21\u5f0f\u4e0d\u53ef\u9884\u6d4b\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u5229\u7528\u8377\u5170\u7687\u5bb6\u6c14\u8c61\u7814\u7a76\u6240 (KNMI) \u7684\u964d\u6c34\u548c\u6c14\u8c61\u6570\u636e\u8fdb\u884c\u964d\u6c34\u77ed\u4e34\u9884\u62a5\u3002\u8be5\u6a21\u578b\u7684\u7075\u611f\u6765\u81ea\u4e8e\u65b0\u9896\u7684\u7269\u7406\u4fe1\u606f\u9274\u522b\u5668 GAN (PID-GAN) \u516c\u5f0f\uff0c\u5c06\u57fa\u4e8e\u7269\u7406\u7684\u76d1\u7763\u76f4\u63a5\u96c6\u6210\u5230\u5bf9\u6297\u5b66\u4e60\u6846\u67b6\u4e2d\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u91c7\u7528 GAN \u7ed3\u6784\uff0c\u4ee5\u77e2\u91cf\u91cf\u5316\u751f\u6210\u5bf9\u6297\u7f51\u7edc (VQ-GAN) \u548c Transformer \u4f5c\u4e3a\u751f\u6210\u5668\uff0c\u5e76\u4ee5\u65f6\u95f4\u9274\u522b\u5668\u4f5c\u4e3a\u9274\u522b\u5668\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cPID-GAN \u6a21\u578b\u5728\u4e0b\u6e38\u964d\u6c34\u77ed\u4e34\u9884\u62a5\u6307\u6807\u65b9\u9762\u4f18\u4e8e\u6570\u503c\u6a21\u578b\u548c\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u3002 \n**|\n", "2406.10019": "|**2024-06-14**|**Group and Shuffle: Efficient Structured Orthogonal Parametrization**|[2406.10019](http://arxiv.org/abs/2406.10019)|null|**\u795e\u7ecf\u7f51\u7edc\u89c4\u6a21\u7684\u4e0d\u65ad\u589e\u5927\u5bfc\u81f4\u5bf9\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u6700\u8fd1\uff0c\u4e00\u79cd\u6b63\u4ea4\u5fae\u8c03\u8303\u5f0f\u88ab\u63d0\u51fa\uff0c\u5b83\u4f7f\u7528\u6b63\u4ea4\u77e9\u9635\u6765\u8c03\u6574\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6743\u91cd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u7ed3\u6784\u5316\u77e9\u9635\u7c7b\u522b\uff0c\u5b83\u7edf\u4e00\u5e76\u6982\u62ec\u4e86\u5148\u524d\u5de5\u4f5c\u4e2d\u7684\u7ed3\u6784\u5316\u7c7b\u522b\u3002\u6211\u4eec\u7814\u7a76\u4e86\u6b64\u7c7b\u7684\u5c5e\u6027\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efa\u4e86\u7ed3\u6784\u5316\u6b63\u4ea4\u53c2\u6570\u5316\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u8fd9\u79cd\u53c2\u6570\u5316\u6765\u4fee\u6539\u6b63\u4ea4\u5fae\u8c03\u6846\u67b6\uff0c\u4ece\u800c\u63d0\u9ad8\u53c2\u6570\u548c\u8ba1\u7b97\u6548\u7387\u3002\u6211\u4eec\u5728\u4e0d\u540c\u7684\u9886\u57df\u5bf9\u6211\u4eec\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u5305\u62ec\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u9002\u5e94\u548c\u8bed\u8a00\u5efa\u6a21\u4e2d\u7684\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u9488\u5bf9\u6b63\u4ea4\u5377\u79ef\u8c03\u6574\u4e86\u6211\u4eec\u7684\u7ed3\u6784\uff0c\u5e76\u4f7f\u7528 1-Lipschitz \u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002 \n**|\n"}, "LLM": {"2406.10130": "|**2024-06-14**|**The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Pre-trained Language Models**|[2406.10130](http://arxiv.org/abs/2406.10130)|null|**\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b (PLM) \u5df2\u88ab\u8bc1\u5b9e\u5305\u542b\u6709\u5bb3\u4fe1\u606f\uff0c\u4f8b\u5982\u793e\u4f1a\u504f\u89c1\uff0c\u8fd9\u53ef\u80fd\u4f1a\u9020\u6210\u8d1f\u9762\u793e\u4f1a\u5f71\u54cd\uff0c\u751a\u81f3\u5728\u5e94\u7528\u4e2d\u5e26\u6765\u707e\u96be\u6027\u540e\u679c\u3002\u4ee5\u524d\u9488\u5bf9\u8fd9\u4e2a\u95ee\u9898\u7684\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u4f7f\u7528\u9ed1\u76d2\u65b9\u6cd5\uff08\u4f8b\u5982\u63a2\u6d4b\uff09\u901a\u8fc7\u89c2\u5bdf\u6a21\u578b\u8f93\u51fa\u6765\u68c0\u6d4b\u548c\u91cf\u5316 PLM \u4e2d\u7684\u793e\u4f1a\u504f\u89c1\u3002\u56e0\u6b64\uff0c\u4ee5\u524d\u7684\u53bb\u504f\u65b9\u6cd5\u4e3b\u8981\u662f\u5728\u65b0\u6784\u5efa\u7684\u53cd\u523b\u677f\u5370\u8c61\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u751a\u81f3\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u800c\u8fd9\u4e9b\u6570\u636e\u96c6\u7684\u6210\u672c\u5f88\u9ad8\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u8bd5\u56fe\u901a\u8fc7\u5f15\u5165\u201c\u793e\u4f1a\u504f\u89c1\u795e\u7ecf\u5143\u201d\u7684\u6982\u5ff5\u6765\u63ed\u793a\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u793e\u4f1a\u504f\u89c1\u7684\u5965\u79d8\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u96c6\u6210\u68af\u5ea6\u5dee\u8ddd (IG$^2$)\u201d\u6765\u51c6\u786e\u67e5\u660e\u8bed\u8a00\u6a21\u578b\u4e2d\u53ef\u5f52\u56e0\u4e8e\u4e0d\u826f\u884c\u4e3a\uff08\u4f8b\u5982\u793e\u4f1a\u504f\u89c1\uff09\u7684\u5355\u5143\uff08\u5373\u795e\u7ecf\u5143\uff09\u3002\u901a\u8fc7\u5c06\u4e0d\u826f\u884c\u4e3a\u5f62\u5f0f\u5316\u4e3a\u8bed\u8a00\u7684\u5206\u5e03\u5c5e\u6027\uff0c\u6211\u4eec\u4f7f\u7528\u5e26\u6709\u60c5\u611f\u7684\u63d0\u793a\u6765\u5f15\u51fa\u4e0e\u8fd9\u4e9b\u60c5\u611f\u76f8\u5173\u7684\u654f\u611f\u8bcd\u7c7b\u522b\uff08\u4eba\u53e3\u7edf\u8ba1\uff09\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684 IG$^2$ \u5c06\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\u7684\u5206\u5e03\u4e0d\u5747\u5f52\u56e0\u4e8e\u7279\u5b9a\u7684\u793e\u4f1a\u504f\u89c1\u795e\u7ecf\u5143\uff0c\u8fd9\u4e9b\u795e\u7ecf\u5143\u8ddf\u8e2a PLM \u5355\u5143\u5185\u90e8\u4e0d\u9700\u8981\u7684\u884c\u4e3a\u8f68\u8ff9\u4ee5\u5b9e\u73b0\u4e92\u64cd\u4f5c\u6027\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u6211\u4eec\u53ef\u89e3\u91ca\u7684\u6280\u672f\uff0c\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u201c\u504f\u89c1\u795e\u7ecf\u5143\u6291\u5236 (BNS)\u201d\u6765\u51cf\u8f7b\u793e\u4f1a\u504f\u89c1\u3002\u901a\u8fc7\u7814\u7a76 BERT\u3001RoBERTa \u53ca\u5176\u4e0e\u53bb\u504f FairBERTa \u7684\u5f52\u56e0\u5dee\u5f02\uff0cIG$^2$ \u4f7f\u6211\u4eec\u80fd\u591f\u5b9a\u4f4d\u548c\u6291\u5236\u5df2\u8bc6\u522b\u7684\u795e\u7ecf\u5143\uff0c\u5e76\u8fdb\u4e00\u6b65\u51cf\u8f7b\u4e0d\u826f\u884c\u4e3a\u3002\u6839\u636e StereoSet \u4e2d\u7684\u5148\u524d\u6307\u6807\u8861\u91cf\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4ee5\u4f4e\u6210\u672c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7a0b\u5ea6\u7684\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u3002 \n**|\n", "2406.09938": "|**2024-06-14**|**Experiments in News Bias Detection with Pre-Trained Neural Transformers**|[2406.09938](http://arxiv.org/abs/2406.09938)|null|**The World Wide Web provides unrivalled access to information globally, including factual news reporting and commentary. However, state actors and commercial players increasingly spread biased (distorted) or fake (non-factual) information to promote their agendas. We compare several large, pre-trained language models on the task of sentence-level news bias detection and sub-type classification, providing quantitative and qualitative results.**|\n", "2406.09790": "|**2024-06-14**|**Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity**|[2406.09790](http://arxiv.org/abs/2406.09790)|null|**Semantic Textual Similarity (STS) constitutes a critical research direction in computational linguistics and serves as a key indicator of the encoding capabilities of embedding models. Driven by advances in pre-trained language models and contrastive learning techniques, leading sentence representation methods can already achieved average Spearman's correlation scores of approximately 86 across seven STS benchmarks in SentEval. However, further improvements have become increasingly marginal, with no existing method attaining an average score higher than 87 on these tasks. This paper conducts an in-depth analysis of this phenomenon and concludes that the upper limit for Spearman's correlation scores using contrastive learning is 87.5. To transcend this ceiling, we propose an innovative approach termed Pcc-tuning, which employs Pearson's correlation coefficient as a loss function to refine model performance beyond contrastive learning. Experimental results demonstrate that Pcc-tuning markedly surpasses previous state-of-the-art strategies, raising the Spearman's correlation score to above 90.**|\n", "2406.09206": "|**2024-06-13**|**Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models**|[2406.09206](http://arxiv.org/abs/2406.09206)|null|**Active learning is an iterative labeling process that is used to obtain a small labeled subset, despite the absence of labeled data, thereby enabling to train a model for supervised tasks such as text classification. While active learning has made considerable progress in recent years due to improvements provided by pre-trained language models, there is untapped potential in the often neglected unlabeled portion of the data, although it is available in considerably larger quantities than the usually small set of labeled data. Here we investigate how self-training, a semi-supervised approach where a model is used to obtain pseudo-labels from the unlabeled data, can be used to improve the efficiency of active learning for text classification. Starting with an extensive reproduction of four previous self-training approaches, some of which are evaluated for the first time in the context of active learning or natural language processing, we devise HAST, a new and effective self-training strategy, which is evaluated on four text classification benchmarks, on which it outperforms the reproduced self-training approaches and reaches classification results comparable to previous experiments for three out of four datasets, using only 25% of the data.**|\n", "2406.08812": "|**2024-06-13**|**Generating Speakers by Prompting Listener Impressions for Pre-trained Multi-Speaker Text-to-Speech Systems**|[2406.08812](http://arxiv.org/abs/2406.08812)|null|**\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u63cf\u8ff0\u8bf4\u8bdd\u8005\u7279\u5f81\u7684\u63d0\u793a\u6765\u6307\u5b9a\u548c\u63a7\u5236\u5408\u6210\u8bed\u97f3\u7684\u58f0\u5b66\u7279\u5f81\u3002\u4e0e\u4ee5\u5f80\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u542c\u8005\u7684\u5370\u8c61\u6765\u6784\u5efa\u63d0\u793a\uff0c\u8fd9\u4e9b\u63d0\u793a\u66f4\u5bb9\u6613\u6536\u96c6\uff0c\u5e76\u4e14\u66f4\u81ea\u7136\u5730\u4e0e\u8bf4\u8bdd\u8005\u7279\u5f81\u7684\u65e5\u5e38\u63cf\u8ff0\u76f8\u4e00\u81f4\u3002\u6211\u4eec\u91c7\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u6280\u672f\uff0c\u5feb\u901f\u5730\u5c06\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u8c03\u6574\u5230\u6211\u4eec\u7684\u9700\u6c42\uff0c\u4ece\u800c\u4fbf\u4e8e\u4ece\u63d0\u793a\u6587\u672c\u4e2d\u63d0\u53d6\u4e0e\u8bf4\u8bdd\u8005\u76f8\u5173\u7684\u7279\u5f81\u3002\u6b64\u5916\uff0c\u4e0e\u5176\u4ed6\u63d0\u793a\u9a71\u52a8\u7684\u6587\u672c\u5230\u8bed\u97f3\uff08TTS\uff09\u7cfb\u7edf\u4e0d\u540c\uff0c\u6211\u4eec\u5c06\u63d0\u793a\u5230\u8bf4\u8bdd\u8005\u6a21\u5757\u4e0e\u591a\u8bf4\u8bdd\u8005TTS\u7cfb\u7edf\u5206\u79bb\uff0c\u589e\u5f3a\u4e86\u7cfb\u7edf\u7075\u6d3b\u6027\uff0c\u5e76\u63d0\u9ad8\u4e86\u4e0e\u5404\u79cd\u9884\u8bad\u7ec3\u7684\u591a\u8bf4\u8bdd\u8005TTS\u7cfb\u7edf\u7684\u517c\u5bb9\u6027\u3002\u6b64\u5916\uff0c\u5bf9\u4e8e\u63d0\u793a\u5230\u8bf4\u8bdd\u8005\u7279\u5f81\u6a21\u5757\uff0c\u6211\u4eec\u8fd8\u6bd4\u8f83\u4e86\u5224\u522b\u65b9\u6cd5\u548c\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u6211\u4eec\u53d1\u73b0\u7ed3\u5408\u8fd9\u4e24\u79cd\u65b9\u6cd5\u53ef\u4ee5\u5e2e\u52a9\u7cfb\u7edf\u66f4\u597d\u5730\u4ece\u63d0\u793a\u4e2d\u540c\u65f6\u6355\u6349\u4e0e\u8bf4\u8bdd\u8005\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u5e76\u751f\u6210\u66f4\u9ad8\u4fdd\u771f\u5ea6\u7684\u8bed\u97f3\u3002 \n**|\n", "2406.08633": "|**2024-06-12**|**Unraveling Code-Mixing Patterns in Migration Discourse: Automated Detection and Analysis of Online Conversations on Reddit**|[2406.08633](http://arxiv.org/abs/2406.08633)|null|**\u5168\u7403\u79fb\u6c11\u6a21\u5f0f\u7684\u6fc0\u589e\u51f8\u663e\u4e86\u5c06\u79fb\u6c11\u65e0\u7f1d\u878d\u5165\u4e1c\u9053\u793e\u533a\u7684\u5fc5\u8981\u6027\uff0c\u8fd9\u9700\u8981\u5305\u5bb9\u548c\u503c\u5f97\u4fe1\u8d56\u7684\u516c\u5171\u670d\u52a1\u3002\u5c3d\u7ba1\u5317\u6b27\u56fd\u5bb6\u62e5\u6709\u5f3a\u5927\u7684\u516c\u5171\u90e8\u95e8\u57fa\u7840\u8bbe\u65bd\uff0c\u4f46\u8fd1\u671f\u79fb\u6c11\u5728\u83b7\u53d6\u8fd9\u4e9b\u670d\u52a1\u65b9\u9762\u7ecf\u5e38\u9047\u5230\u969c\u788d\uff0c\u52a0\u5267\u4e86\u793e\u4f1a\u5dee\u8ddd\u5e76\u524a\u5f31\u4e86\u4fe1\u4efb\u3002 \u5728\u8fd9\u9879\u52aa\u529b\u4e2d\uff0c\u89e3\u51b3\u6570\u5b57\u4e0d\u5e73\u7b49\u548c\u8bed\u8a00\u591a\u6837\u6027\u81f3\u5173\u91cd\u8981\u3002 \u672c\u6587\u63a2\u8ba8\u4e86\u4ee3\u7801\u6df7\u5408\u7684\u4f7f\u7528\uff0c\u8fd9\u662f\u4e00\u79cd\u5728\u591a\u8bed\u8a00\u4f7f\u7528\u8005\u4e2d\u666e\u904d\u5b58\u5728\u7684\u4ea4\u6d41\u7b56\u7565\uff0c\u7528\u4e8e Reddit \u7b49\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4e0a\u4e0e\u79fb\u6c11\u76f8\u5173\u7684\u8ba8\u8bba\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u591a\u8bed\u8a00\u4ee3\u7801\u6df7\u5408\u6587\u672c\u8bc6\u522b\u96c6\u6210\u5b66\u4e60 (ELMICT)\uff0c\u8fd9\u662f\u4e00\u79cd\u65e8\u5728\u81ea\u52a8\u68c0\u6d4b\u4e0e\u79fb\u6c11\u76f8\u5173\u7684\u8ba8\u8bba\u4e2d\u4ee3\u7801\u6df7\u5408\u6d88\u606f\u7684\u65b0\u65b9\u6cd5\u3002ELMICT \u5229\u7528\u96c6\u6210\u5b66\u4e60\u6280\u672f\u6765\u7ec4\u5408\u591a\u4e2a\u5206\u8bcd\u5668\u7684\u8f93\u51fa\u548c\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u8bc6\u522b\u8de8\u5404\u79cd\u8bed\u8a00\u548c\u4e0a\u4e0b\u6587\u7684\u4ee3\u7801\u6df7\u5408\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u6027\u80fd\uff08F1 \u503c\u5927\u4e8e 0.95\uff09\uff0c\u7279\u522b\u662f\u5728\u8de8\u8bed\u8a00\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\uff08\u5e73\u5747 F1 \u503c\u5927\u4e8e 0.70\uff09\u3002\u6b64\u5916\uff0cELMICT \u7684\u4f7f\u7528\u6709\u52a9\u4e8e\u5206\u6790\u4e0e\u79fb\u6c11\u76f8\u5173\u7684\u4e3b\u9898\u4e2d\u4ee3\u7801\u6df7\u5408\u7684\u6d41\u884c\u7a0b\u5ea6\uff0c\u4e0e Reddit \u4e0a\u7684\u5176\u4ed6\u4e3b\u9898\u7c7b\u522b\u76f8\u6bd4\uff0c\u63ed\u793a\u4e86\u79fb\u6c11\u793e\u533a\u5173\u6ce8\u7684\u8bdd\u9898\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u79fb\u6c11\u5728\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4e0a\u91c7\u7528\u7684\u4ea4\u6d41\u7b56\u7565\uff0c\u4e3a\u5f00\u53d1\u5305\u5bb9\u6027\u6570\u5b57\u516c\u5171\u670d\u52a1\u548c\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u542f\u793a\u3002\u901a\u8fc7\u89e3\u51b3\u672c\u7814\u7a76\u4e2d\u63d0\u51fa\u7684\u7814\u7a76\u95ee\u9898\uff0c\u6211\u4eec\u6709\u52a9\u4e8e\u7406\u89e3\u79fb\u6c11\u8bdd\u8bed\u4e2d\u7684\u8bed\u8a00\u591a\u6837\u6027\uff0c\u5e76\u4e3a\u6784\u5efa\u591a\u5143\u6587\u5316\u793e\u4f1a\u4fe1\u4efb\u7684\u66f4\u6709\u6548\u5de5\u5177\u94fa\u5e73\u9053\u8def\u3002 \n**|\n", "2406.08426": "|**2024-06-12**|**Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL**|[2406.08426](http://arxiv.org/abs/2406.08426)|null|**\u6839\u636e\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u751f\u6210\u51c6\u786e\u7684SQL\uff08\u6587\u672c\u5230SQL\uff09\u662f\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u7528\u6237\u95ee\u9898\u7406\u89e3\u3001\u6570\u636e\u5e93\u6a21\u5f0f\u7406\u89e3\u548cSQL\u751f\u6210\u90fd\u5177\u6709\u6311\u6218\u6027\u3002\u4f20\u7edf\u7684\u6587\u672c\u5230SQL\u7cfb\u7edf\u5305\u62ec\u4eba\u5de5\u5de5\u7a0b\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u3002\u968f\u540e\uff0c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\u88ab\u5f00\u53d1\u51fa\u6765\u5e76\u7528\u4e8e\u6587\u672c\u5230SQL\u4efb\u52a1\uff0c\u53d6\u5f97\u4e86\u53ef\u559c\u7684\u6027\u80fd\u3002\u968f\u7740\u73b0\u4ee3\u6570\u636e\u5e93\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u76f8\u5e94\u7684\u7528\u6237\u95ee\u9898\u4e5f\u66f4\u5177\u6311\u6218\u6027\uff0c\u7406\u89e3\u80fd\u529b\u6709\u9650\u7684PLM\u53ef\u80fd\u4f1a\u5bfc\u81f4\u9519\u8bef\u7684SQL\u751f\u6210\u3002\u8fd9\u5c31\u9700\u8981\u66f4\u590d\u6742\u548c\u66f4\u6709\u9488\u5bf9\u6027\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u800c\u8fd9\u53cd\u8fc7\u6765\u53c8\u9650\u5236\u4e86\u57fa\u4e8ePLM\u7684\u7cfb\u7edf\u7684\u5e94\u7528\u3002\u6700\u8fd1\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u7684\u80fd\u529b\uff0c\u56e0\u4e3a\u6a21\u578b\u89c4\u6a21\u4e0d\u65ad\u6269\u5927\u3002\u56e0\u6b64\uff0c\u6574\u5408\u57fa\u4e8eLLM\u7684\u5b9e\u73b0\u53ef\u4ee5\u4e3a\u6587\u672c\u5230SQL\u7814\u7a76\u5e26\u6765\u72ec\u7279\u7684\u673a\u4f1a\u3001\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\u3002\u5728\u8fd9\u7bc7\u7efc\u8ff0\u4e2d\uff0c\u6211\u4eec\u5168\u9762\u56de\u987e\u4e86\u57fa\u4e8eLLM\u7684\u6587\u672c\u5230SQL\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7b80\u8981\u6982\u8ff0\u4e86\u5f53\u524d\u7684\u6311\u6218\u548c\u6587\u672c\u5230SQL\u7684\u6f14\u53d8\u8fc7\u7a0b\u3002\u7136\u540e\uff0c\u6211\u4eec\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230SQL\u7cfb\u7edf\u7684\u6570\u636e\u96c6\u548c\u6307\u6807\u3002\u4e4b\u540e\uff0c\u6211\u4eec\u7cfb\u7edf\u5730\u5206\u6790\u4e86\u57fa\u4e8eLLM\u7684\u6587\u672c\u5230SQL\u7684\u6700\u65b0\u8fdb\u5c55\u3002\u6700\u540e\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u8be5\u9886\u57df remaining challenges \u5e76\u5bf9\u672a\u6765\u65b9\u5411\u63d0\u51fa\u4e86\u671f\u671b\u3002 \n**|\n", "2406.08246": "|**2024-06-12**|**Leveraging Large Language Models for Web Scraping**|[2406.08246](http://arxiv.org/abs/2406.08246)|null|**\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u590d\u5236\u4eba\u7c7b\u4efb\u52a1\u548c\u63d0\u9ad8\u751f\u4ea7\u529b\u65b9\u9762\u8868\u73b0\u51fa\u975e\u51e1\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4f18\u5148\u8003\u8651\u6d41\u7545\u6027\u800c\u975e\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u4ee5\u53ca\u5904\u7406\u7279\u5b9a\u4fe1\u606f\u7684\u5c40\u9650\u6027\uff0cLLM \u76f4\u63a5\u5e94\u7528\u4e8e\u6570\u636e\u63d0\u53d6\u5b58\u5728\u5c40\u9650\u6027\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u672c\u7814\u7a76\u5229\u7528\u9884\u8bad\u7ec3 LLM \u7684\u77e5\u8bc6\u8868\u793a\u80fd\u529b\u548c RAG \u6a21\u578b\u7684\u76ee\u6807\u4fe1\u606f\u8bbf\u95ee\u80fd\u529b\uff0c\u7814\u7a76\u4e86\u4e00\u79cd\u901a\u7528\u7684\u3001\u51c6\u786e\u7684 RAG \u6a21\u578b\u6570\u636e\u6293\u53d6\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u4e13\u4e3a\u8bed\u8a00\u751f\u6210\u800c\u8bbe\u8ba1\u3002\u4e3a\u4e86\u4ee5\u66f4\u6a21\u5757\u5316\u548c\u53ef\u89e3\u91ca\u7684\u65b9\u5f0f\u83b7\u53d6\u77e5\u8bc6\uff0c\u6211\u4eec\u4f7f\u7528\u5177\u6709\u6f5c\u5728\u77e5\u8bc6\u68c0\u7d22\u5668\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u5141\u8bb8\u6a21\u578b\u4ece\u5927\u578b\u8bed\u6599\u5e93\u4e2d\u68c0\u7d22\u6587\u6863\u5e76\u5bf9\u5176\u8fdb\u884c\u5173\u6ce8\u3002\u6211\u4eec\u5229\u7528 RAG \u6a21\u578b\u67b6\u6784\uff0c\u6df1\u5165\u5206\u6790\u4e86\u5b83\u4eec\u5728\u4ee5\u4e0b\u4e09\u9879\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff1a(i) HTML \u5143\u7d20\u7684\u8bed\u4e49\u5206\u7c7b\uff0c(ii) \u5bf9 HTML \u6587\u672c\u8fdb\u884c\u5206\u5757\u4ee5\u5b9e\u73b0\u6709\u6548\u7406\u89e3\uff0c\u4ee5\u53ca (iii) \u6bd4\u8f83\u4e0d\u540c LLM \u548c\u6392\u5e8f\u7b97\u6cd5\u7684\u7ed3\u679c\u3002\u867d\u7136\u4e4b\u524d\u7684\u5de5\u4f5c\u5df2\u7ecf\u5f00\u53d1\u4e86\u7528\u4e8e HTML \u7406\u89e3\u548c\u63d0\u53d6\u7684\u4e13\u7528\u67b6\u6784\u548c\u8bad\u7ec3\u7a0b\u5e8f\uff0c\u4f46\u6211\u4eec\u8868\u660e\uff0c\u5728\u6807\u51c6\u81ea\u7136\u8bed\u8a00\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u5e76\u6dfb\u52a0\u6709\u6548\u7684\u5206\u5757\u3001\u641c\u7d22\u548c\u6392\u5e8f\u7b97\u6cd5\u7684 LLM \u53ef\u4ee5\u8bc1\u660e\u662f\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u6293\u53d6\u5de5\u5177\uff0c\u53ef\u4ee5\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u63d0\u53d6\u590d\u6742\u6570\u636e\u3002\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u5305\u62ec\u89e3\u51b3\u6240\u63d0\u51fa\u7684\u57fa\u4e8e RAG \u7684\u6570\u636e\u63d0\u53d6\u6846\u67b6\u4e2d\u7684\u6765\u6e90\u8ddf\u8e2a\u548c\u52a8\u6001\u77e5\u8bc6\u66f4\u65b0\u6311\u6218\u3002\u901a\u8fc7\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u8fd9\u79cd\u65b9\u6cd5\u6709\u53ef\u80fd\u5f7b\u5e95\u6539\u53d8\u4ece\u5927\u91cf\u6587\u672c\u4fe1\u606f\u5e93\u4e2d\u63d0\u53d6\u6570\u636e\u7684\u65b9\u5f0f\u3002 \n**|\n", "2406.07970": "|**2024-06-12**|**Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation**|[2406.07970](http://arxiv.org/abs/2406.07970)|**[link](https://github.com/JoyeBright/ICLviaQE)**|**\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u8f93\u51fa\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u5728\u673a\u5668\u7ffb\u8bd1 (MT) \u4e2d\uff0c\u4e0e\u67e5\u8be2\uff08\u5373\u5f85\u7ffb\u8bd1\u6587\u672c\uff09\u4e00\u8d77\u63d0\u4f9b\u7684\u4e0a\u4e0b\u6587\u793a\u4f8b (ICE) \u7684\u8d28\u91cf\u5bc6\u5207\u76f8\u5173\u3002\u8fd9\u4e9b ICE \u7684\u6709\u6548\u6027\u53d7\u591a\u79cd\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u4f8b\u5982\u6e90\u6587\u672c\u7684\u9886\u57df\u3001ICE \u7684\u5448\u73b0\u987a\u5e8f\u3001\u793a\u4f8b\u6570\u91cf\u4ee5\u53ca\u4f7f\u7528\u7684\u63d0\u793a\u6a21\u677f\u3002\u81ea\u7136\u5730\uff0c\u9009\u62e9\u6700\u5177\u5f71\u54cd\u529b\u7684 ICE \u53d6\u51b3\u4e8e\u7406\u89e3\u8fd9\u4e9b\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u6700\u7ec8\u7684\u7ffb\u8bd1\u8d28\u91cf\uff0c\u800c\u8fd9\u6700\u7ec8\u4f9d\u8d56\u4e8e\u7ffb\u8bd1\u53c2\u8003\u6216\u4eba\u5de5\u5224\u65ad\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60 (ICL) \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7279\u5b9a\u9886\u57df\u8d28\u91cf\u4f30\u8ba1 (QE) \u6307\u5bfc\u7684\u641c\u7d22\u7b97\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528 XGLM \u6a21\u578b\uff0c\u65e0\u9700\u7ffb\u8bd1\u53c2\u8003\u5373\u53ef\u4f30\u8ba1\u6700\u7ec8\u7684\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4ece\u800c\u4e3a MT \u9009\u62e9\u6709\u6548\u7684 ICE \u4ee5\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u7ffb\u8bd1\u8d28\u91cf\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u7684 ICL \u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\uff0c\u5e76\u4e14\u4e0e\u5fae\u8c03\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b (PLM)\uff08\u7279\u522b\u662f mBART-50\uff09\u76f8\u6bd4\uff0c\u7ffb\u8bd1\u6027\u80fd\u66f4\u9ad8\u3002 \n**|\n", "2406.07886": "|**2024-06-12**|**Label-aware Hard Negative Sampling Strategies with Momentum Contrastive Learning for Implicit Hate Speech Detection**|[2406.07886](http://arxiv.org/abs/2406.07886)|**[link](https://github.com/hanyang-hcc-lab/lahn)**|**\u68c0\u6d4b\u672a\u76f4\u63a5\u8868\u8fbe\u4ec7\u6068\u7684\u9690\u6027\u4ec7\u6068\u8a00\u8bba\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5c1d\u8bd5\u901a\u8fc7\u5c06\u5bf9\u6bd4\u5b66\u4e60\u5e94\u7528\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08\u5982BERT\u548cRoBERTa\uff09\u6765\u68c0\u6d4b\u9690\u6027\u4ec7\u6068\u8a00\u8bba\uff0c\u4f46\u4e0e\u57fa\u4e8e\u4ea4\u53c9\u71b5\u635f\u5931\u7684\u5b66\u4e60\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u4ecd\u7136\u6ca1\u6709\u663e\u8457\u4f18\u52bf\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u57fa\u4e8e\u968f\u673a\u62bd\u6837\u6279\u6b21\u6570\u636e\u7684\u5bf9\u6bd4\u5b66\u4e60\u5e76\u4e0d\u80fd\u9f13\u52b1\u6a21\u578b\u5b66\u4e60\u56f0\u96be\u8d1f\u6837\u672c\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6807\u7b7e\u611f\u77e5\u56f0\u96be\u8d1f\u6837\u672c\u91c7\u6837\u7b56\u7565\uff08LAHN\uff09\uff0c\u8be5\u7b56\u7565\u9f13\u52b1\u6a21\u578b\u4f7f\u7528\u52a8\u91cf\u96c6\u6210\u7684\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4ece\u56f0\u96be\u8d1f\u6837\u672c\u4e2d\u5b66\u4e60\u8be6\u7ec6\u7279\u5f81\uff0c\u800c\u4e0d\u662f\u968f\u673a\u6279\u6b21\u4e2d\u7684\u7b80\u5355\u8d1f\u6837\u672c\u3002\u5728\u6570\u636e\u96c6\u5185\u548c\u8de8\u6570\u636e\u96c6\u7684\u9690\u6027\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0cLAHN \u7684\u6027\u80fd\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/Hanyang-HCC-Lab/LAHN \u83b7\u53d6\u3002 \n**|\n"}, "Transformer": {"2406.10082": "|**2024-06-14**|**Whisper-Flamingo: Integrating Visual Features into Whisper for Audio-Visual Speech Recognition and Translation**|[2406.10082](http://arxiv.org/abs/2406.10082)|**[link](https://github.com/roudimit/whisper-flamingo)**|**Audio-Visual Speech Recognition (AVSR) uses lip-based video to improve performance in noise. Since videos are harder to obtain than audio, the video training data of AVSR models is usually limited to a few thousand hours. In contrast, speech models such as Whisper are trained with hundreds of thousands of hours of data, and thus learn a better speech-to-text decoder. The huge training data difference motivates us to adapt Whisper to handle video inputs. Inspired by Flamingo which injects visual features into language models, we propose Whisper-Flamingo which integrates visual features into the Whisper speech recognition and translation model with gated cross attention. Our audio-visual Whisper-Flamingo outperforms audio-only Whisper on English speech recognition and En-X translation for 6 languages in noisy conditions. Moreover, Whisper-Flamingo is a versatile model and conducts all of these tasks using one set of parameters, while prior methods are trained separately on each language.**|\n", "2406.10052": "|**2024-06-14**|**Simul-Whisper: Attention-Guided Streaming Whisper with Truncation Detection**|[2406.10052](http://arxiv.org/abs/2406.10052)|null|**As a robust and large-scale multilingual speech recognition model, Whisper has demonstrated impressive results in many low-resource and out-of-distribution scenarios. However, its encoder-decoder structure hinders its application to streaming speech recognition. In this paper, we introduce Simul-Whisper, which uses the time alignment embedded in Whisper's cross-attention to guide auto-regressive decoding and achieve chunk-based streaming ASR without any fine-tuning of the pre-trained model. Furthermore, we observe the negative effect of the truncated words at the chunk boundaries on the decoding results and propose an integrate-and-fire-based truncation detection model to address this issue. Experiments on multiple languages and Whisper architectures show that Simul-Whisper achieves an average absolute word error rate degradation of only 1.46% at a chunk size of 1 second, which significantly outperforms the current state-of-the-art baseline.**|\n", "2406.09742": "|**2024-06-14**|**IFA: Interaction Fidelity Attention for Entire Lifelong Behaviour Sequence Modeling**|[2406.09742](http://arxiv.org/abs/2406.09742)|null|**\u7528\u6237\u7684\u7ec8\u8eab\u884c\u4e3a\u5e8f\u5217\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u7528\u6237\u504f\u597d\u4fe1\u606f\uff0c\u5e76\u5728\u63a8\u8350\u4efb\u52a1\u4e2d\u83b7\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u4f46\u4e5f\u663e\u8457\u589e\u52a0\u4e86\u8ba1\u7b97\u91cf\u3002\u4e3a\u4e86\u6ee1\u8db3\u5728\u7ebf\u670d\u52a1\u4e2d\u4e25\u683c\u7684\u5ef6\u8fdf\u8981\u6c42\uff0c\u901a\u5e38\u4f1a\u6839\u636e\u4e0e\u76ee\u6807\u5546\u54c1\u7684\u76f8\u4f3c\u5ea6\u5bf9\u4e00\u4e2a\u8f83\u77ed\u7684\u5b50\u5e8f\u5217\u8fdb\u884c\u91c7\u6837\u3002\u7136\u800c\uff0c\u4e0d\u5728\u5b50\u5e8f\u5217\u4e2d\u7684\u5546\u54c1\u4f1a\u88ab\u4e22\u5f03\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u4fe1\u606f\u4e22\u5931\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u6548\u8303\u5f0f\u6765\u5bf9\u5b8c\u6574\u7684\u7ec8\u8eab\u5e8f\u5217\u8fdb\u884c\u5efa\u6a21\uff0c\u79f0\u4e3a\u4ea4\u4e92\u4fdd\u771f\u5ea6\u6ce8\u610f\u529b\uff08IFA\uff09\u3002\u5728IFA\u4e2d\uff0c\u6211\u4eec\u5c06\u5019\u9009\u96c6\u4e2d\u7684\u6240\u6709\u76ee\u6807\u5546\u54c1\u4e00\u6b21\u6027\u8f93\u5165\u6a21\u578b\uff0c\u5e76\u5229\u7528\u7ebf\u6027Transformer\u6765\u964d\u4f4e\u5019\u9009\u96c6\u548c\u5e8f\u5217\u4e4b\u95f4\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u7684\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u800c\u4e0d\u4f1a\u4e22\u5931\u4efb\u4f55\u4ea4\u4e92\u4fe1\u606f\u3002\u6211\u4eec\u8fd8\u989d\u5916\u5730\u5bf9\u6240\u6709\u76ee\u6807\u5546\u54c1\u4e4b\u95f4\u7684\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\uff0c\u4ee5\u5b9e\u73b0\u6700\u4f73\u96c6\u5408\u751f\u6210\uff0c\u5e76\u8bbe\u8ba1\u4e86\u635f\u5931\u51fd\u6570\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u4e00\u81f4\u6027\u3002\u6211\u4eec\u5728\u5feb\u624b\u63a8\u8350\u7cfb\u7edf\u7684\u79bb\u7ebf\u548c\u5728\u7ebf\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u6211\u4eec\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002 \n**|\n", "2406.09618": "|**2024-06-13**|**Multi-Modal Retrieval For Large Language Model Based Speech Recognition**|[2406.09618](http://arxiv.org/abs/2406.09618)|null|**\u68c0\u7d22\u662f\u4e00\u79cd\u5e7f\u6cdb\u91c7\u7528\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5229\u7528\u5916\u90e8\u4fe1\u606f\u6539\u8fdb\u8bed\u8a00\u6a21\u578b\u3002\u968f\u7740\u8be5\u9886\u57df\u5411\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\uff0c\u91cd\u8981\u7684\u662f\u6269\u5c55\u7eaf\u6587\u672c\u65b9\u6cd5\uff0c\u5c06\u5176\u4ed6\u6a21\u6001\u4e5f\u7eb3\u5165\u68c0\u7d22\u4e2d\uff0c\u4ee5\u4fbf\u5e94\u7528\u4e8e\u5404\u79cd\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u548c\u6570\u636e\u7c7b\u578b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u591a\u6a21\u6001\u68c0\u7d22\u65b9\u6cd5\uff1akNN-LM \u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u6280\u672f\u3002\u6211\u4eec\u901a\u8fc7\u5c06\u68c0\u7d22\u65b9\u6cd5\u5e94\u7528\u4e8e\u53ef\u8bbf\u95ee\u5916\u90e8\u4fe1\u606f\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\uff0c\u51ed\u7ecf\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002\u5728\u6b64\u8bbe\u7f6e\u4e0b\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u57fa\u4e8e\u8bed\u97f3\u7684\u591a\u6a21\u6001\u68c0\u7d22\u4f18\u4e8e\u57fa\u4e8e\u6587\u672c\u7684\u68c0\u7d22\uff0c\u5e76\u4e14\u4e0e\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8bcd\u9519\u8bef\u7387\u964d\u4f4e\u4e86\u9ad8\u8fbe 50%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728 Spoken-Squad \u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u8bc6\u522b\u7ed3\u679c\u3002 \n**|\n", "2406.09016": "|**2024-06-13**|**Cross-Modal Learning for Anomaly Detection in Fused Magnesium Smelting Process: Methodology and Benchmark**|[2406.09016](http://arxiv.org/abs/2406.09016)|null|**\u7194\u9541\u7089 (FMF) \u662f\u6c27\u5316\u9541\u751f\u4ea7\u4e2d\u7684\u5173\u952e\u5de5\u4e1a\u8bbe\u5907\uff0c\u5f02\u5e38\u68c0\u6d4b\u5bf9\u5176\u9ad8\u6548\u3001\u7a33\u5b9a\u548c\u5b89\u5168\u8fd0\u884c\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u73b0\u6709\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u4e8e\u4f7f\u7528\u8fc7\u7a0b\u53d8\u91cf\uff08\u5982\u7535\u5f27\u7535\u6d41\uff09\u5206\u6790\u4e3b\u8981\u5f02\u5e38\u6216\u57fa\u4e8e\u5f02\u5e38\u89c6\u89c9\u7279\u5f81\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\uff0c\u800c\u5ffd\u7565\u4e86\u8de8\u6a21\u6001\u4fe1\u606f\u7684\u5185\u5728\u5173\u8054\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001 Transformer\uff08\u79f0\u4e3a FmFormer\uff09\uff0c\u65e8\u5728\u901a\u8fc7\u63a2\u7d22\u89c6\u89c9\u7279\u5f81\uff08\u89c6\u9891\uff09\u548c\u8fc7\u7a0b\u53d8\u91cf\uff08\u7535\u6d41\uff09\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u6765\u4fc3\u8fdb\u7194\u878d\u9541\u51b6\u70bc\u8fc7\u7a0b\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6807\u8bb0\u5316\u8303\u5f0f\uff0c\u4ee5\u591a\u5c3a\u5ea6\u65b9\u5f0f\u6709\u6548\u5730\u5f25\u5408 3D \u89c6\u9891\u6a21\u6001\u548c 1D \u7535\u6d41\u6a21\u6001\u4e4b\u95f4\u7684\u5de8\u5927\u7ef4\u5ea6\u5dee\u8ddd\uff0c\u4ece\u800c\u5b9e\u73b0\u50cf\u7d20\u7ea7\u5f02\u5e38\u68c0\u6d4b\u7684\u5206\u5c42\u91cd\u5efa\u3002\u968f\u540e\uff0cFmFormer \u5229\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u6bcf\u4e2a\u6a21\u6001\u5185\u7684\u5185\u90e8\u7279\u5f81\uff0c\u5e76\u5229\u7528\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6355\u83b7\u6a21\u6001\u95f4\u7684\u76f8\u5173\u6027\u3002\u4e3a\u4e86\u9a8c\u8bc1\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f00\u521b\u6027\u7684\u7194\u878d\u9541\u51b6\u70bc\u8fc7\u7a0b\u8de8\u6a21\u6001\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u5305\u542b\u8d85\u8fc7 220 \u4e07\u4e2a\u6837\u672c\u7684\u540c\u6b65\u91c7\u96c6\u7684\u89c6\u9891\u548c\u7535\u6d41\u6570\u636e\u3002\u5229\u7528\u8de8\u6a21\u6001\u5b66\u4e60\uff0c\u6240\u63d0\u51fa\u7684 FmFormer \u5728\u68c0\u6d4b\u5f02\u5e38\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u7535\u6d41\u6ce2\u52a8\u548c\u6d53\u5bc6\u6c34\u96fe\u9020\u6210\u7684\u89c6\u89c9\u906e\u6321\u7b49\u6781\u7aef\u5e72\u6270\u4e0b\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u548c\u57fa\u51c6\u7ecf\u8fc7\u4e00\u4e9b\u4fee\u6539\u540e\u53ef\u80fd\u9002\u7528\u4e8e\u5176\u4ed6\u5de5\u4e1a\u5e94\u7528\u3002\u8be5\u57fa\u51c6\u5c06\u5728 https://github.com/GaochangWu/FMF-Benchmark \u4e0a\u53d1\u5e03\u3002 \n**|\n", "2406.08907": "|**2024-06-13**|**Dual Attribute-Spatial Relation Alignment for 3D Visual Grounding**|[2406.08907](http://arxiv.org/abs/2406.08907)|null|**\u4e09\u7ef4\u89c6\u89c9\u5b9a\u4f4d\u662f\u4e00\u4e2a\u65b0\u5174\u7684\u7814\u7a76\u9886\u57df\uff0c\u81f4\u529b\u4e8e\u5728\u4e09\u7ef4\u7269\u7406\u4e16\u754c\u548c\u81ea\u7136\u8bed\u8a00\u4e4b\u95f4\u5efa\u7acb\u8054\u7cfb\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u73b0\u5177\u8eab\u667a\u80fd\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DASANet\uff0c\u4e00\u79cd\u53cc\u91cd\u5c5e\u6027-\u7a7a\u95f4\u5173\u7cfb\u5bf9\u9f50\u7f51\u7edc\uff0c\u5b83\u5206\u522b\u5bf9\u8bed\u8a00\u548c 3D \u89c6\u89c9\u6a21\u6001\u4e4b\u95f4\u7684\u5bf9\u8c61\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u7279\u5f81\u8fdb\u884c\u5efa\u6a21\u548c\u5bf9\u9f50\u3002\u6211\u4eec\u5c06\u8bed\u8a00\u548c 3D \u70b9\u4e91\u8f93\u5165\u5206\u89e3\u4e3a\u4e24\u4e2a\u72ec\u7acb\u7684\u90e8\u5206\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53cc\u5206\u652f\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u4ee5\u5206\u522b\u5bf9\u5206\u89e3\u540e\u7684\u8f93\u5165\u8fdb\u884c\u5efa\u6a21\uff0c\u540c\u65f6\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5728\u5c5e\u6027-\u7a7a\u95f4\u7279\u5f81\u878d\u5408\u4e2d\u4fdd\u7559\u5168\u5c40\u4e0a\u4e0b\u6587\u3002\u6211\u4eec\u7684 DASANet \u5728 Nr3D \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u5b9a\u4f4d\u7cbe\u5ea6 65.1%\uff0c\u6bd4\u6700\u4f73\u7ade\u4e89\u5bf9\u624b\u9ad8 1.3%\u3002\u6b64\u5916\uff0c\u4e24\u4e2a\u5206\u652f\u7684\u53ef\u89c6\u5316\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u9ad8\u6548\u4e14\u9ad8\u5ea6\u53ef\u89e3\u91ca\u7684\u3002 \n**|\n", "2406.08204": "|**2024-06-12**|**Diffusion-Promoted HDR Video Reconstruction**|[2406.08204](http://arxiv.org/abs/2406.08204)|null|**\u9ad8\u52a8\u6001\u8303\u56f4 (HDR) \u89c6\u9891\u91cd\u5efa\u65e8\u5728\u4ece\u4ee5\u4ea4\u66ff\u66dd\u5149\u62cd\u6444\u7684\u4f4e\u52a8\u6001\u8303\u56f4 (LDR) \u5e27\u751f\u6210 HDR \u89c6\u9891\u3002\u5927\u591a\u6570\u73b0\u6709\u5de5\u4f5c\u4ec5\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u56de\u5f52\u7684\u8303\u5f0f\uff0c\u5bfc\u81f4\u4e86\u8bf8\u5982\u91cd\u5f71\u4f2a\u5f71\u548c\u9971\u548c\u533a\u57df\u7ec6\u8282\u4e22\u5931\u7b49\u4e0d\u5229\u5f71\u54cd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e HDR \u89c6\u9891\u91cd\u5efa\u7684\u6269\u6563\u4fc3\u8fdb\u65b9\u6cd5\uff0c\u79f0\u4e3a HDR-V-Diff\uff0c\u5b83\u7ed3\u5408\u4e86\u6269\u6563\u6a21\u578b\u6765\u6355\u6349 HDR \u5206\u5e03\u3002\u56e0\u6b64\uff0cHDR-V-Diff \u53ef\u4ee5\u91cd\u5efa\u5177\u6709\u903c\u771f\u7ec6\u8282\u7684 HDR \u89c6\u9891\uff0c\u540c\u65f6\u51cf\u5c11\u91cd\u5f71\u4f2a\u5f71\u3002\u7136\u800c\uff0c\u76f4\u63a5\u5f15\u5165\u89c6\u9891\u6269\u6563\u6a21\u578b\u4f1a\u5e26\u6765\u5de8\u5927\u7684\u8ba1\u7b97\u8d1f\u62c5\u3002\u4e3a\u4e86\u51cf\u8f7b\u8fd9\u79cd\u8d1f\u62c5\uff0c\u6211\u4eec\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u79cd HDR \u9690\u5f0f\u6269\u6563\u6a21\u578b (HDR-LDM) \u6765\u5b66\u4e60\u5355\u4e2a HDR \u5e27\u7684\u5206\u5e03\u5148\u9a8c\u3002\u5177\u4f53\u6765\u8bf4\uff0cHDR-LDM \u7ed3\u5408\u4e86\u4e00\u79cd\u8272\u8c03\u6620\u5c04\u7b56\u7565\u5c06 HDR \u5e27\u538b\u7f29\u5230\u9690\u7a7a\u95f4\u4e2d\uff0c\u5e76\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u66dd\u5149\u5d4c\u5165\u65b9\u6cd5\u5c06\u66dd\u5149\u4fe1\u606f\u805a\u5408\u5230\u6269\u6563\u8fc7\u7a0b\u4e2d\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65f6\u95f4\u4e00\u81f4\u6027\u5bf9\u9f50\u6a21\u5757 (TCAM) \u6765\u5b66\u4e60\u65f6\u95f4\u4fe1\u606f\u4f5c\u4e3a HDR-LDM \u7684\u8865\u5145\uff0c\u8be5\u6a21\u5757\u5728\u89c6\u9891\u5e27\u7684\u4e0d\u540c\u5c3a\u5ea6\u4e0a\u8fdb\u884c\u4ece\u7c97\u5230\u7cbe\u7684\u7279\u5f81\u5bf9\u9f50\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u96f6\u521d\u59cb\u5316\u4ea4\u53c9\u6ce8\u610f\u529b (ZiCA) \u673a\u5236\uff0c\u4ee5\u6709\u6548\u5730\u6574\u5408\u5b66\u4e60\u5230\u7684\u5206\u5e03\u5148\u9a8c\u548c\u65f6\u95f4\u4fe1\u606f\u6765\u751f\u6210 HDR \u5e27\u3002\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86 HDR-V-Diff \u5728\u591a\u4e2a\u4ee3\u8868\u6027\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002 \n**|\n", "2406.07841": "|**2024-06-12**|**Labeling Comic Mischief Content in Online Videos with a Multimodal Hierarchical-Cross-Attention Model**|[2406.07841](http://arxiv.org/abs/2406.07841)|**[link](https://github.com/RiTUAL-UH/Comic-Mischief-Prediction)**|**\u6211\u4eec\u89e3\u51b3\u4e86\u5728\u7ebf\u5a92\u4f53\u4e2d\u68c0\u6d4b\u53ef\u7591\u5185\u5bb9\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u6709\u8da3\u6076\u4f5c\u5267\u8fd9\u4e00\u5b50\u7c7b\u522b\u3002\u8fd9\u7c7b\u5185\u5bb9\u5c06\u66b4\u529b\u3001\u6210\u4eba\u5185\u5bb9\u6216\u8bbd\u523a\u4e0e\u5e7d\u9ed8\u7b49\u5143\u7d20\u7ed3\u5408\u5728\u4e00\u8d77\uff0c\u4f7f\u5176\u96be\u4ee5\u88ab\u68c0\u6d4b\u5230\u3002\u91c7\u7528\u591a\u6a21\u6001\u65b9\u6cd5\u5bf9\u4e8e\u6355\u6349\u6709\u8da3\u6076\u4f5c\u5267\u5185\u5bb9\u4e2d\u56fa\u6709\u7684\u5fae\u5999\u7ec6\u8282\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u9488\u5bf9\u6709\u8da3\u6076\u4f5c\u5267\u68c0\u6d4b\u4efb\u52a1\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7aef\u5230\u7aef\u591a\u6a21\u6001\u7cfb\u7edf\u3002\u4f5c\u4e3a\u8fd9\u9879\u8d21\u732e\u7684\u4e00\u90e8\u5206\uff0c\u6211\u4eec\u53d1\u5e03\u4e86\u4e00\u4e2a\u9488\u5bf9\u76ee\u6807\u4efb\u52a1\u7684\u65b0\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u4e09\u79cd\u6a21\u6001\uff1a\u89c6\u9891\u3001\u6587\u672c\uff08\u89c6\u9891\u5b57\u5e55\u548c\u9690\u85cf\u5f0f\u5b57\u5e55\uff09\u548c\u97f3\u9891\u3002\u6211\u4eec\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b\u5b57\u5e55\u7684\u5206\u5c42\u4ea4\u53c9\u6ce8\u610f\u6a21\u578b\uff08HICCAP\uff09\uff0c\u4ee5\u6355\u6349\u8fd9\u4e9b\u6a21\u6001\u4e4b\u95f4\u9519\u7efc\u590d\u6742\u7684\u5173\u7cfb\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\u548c\u7528\u4e8e\u6709\u8da3\u6076\u4f5c\u5267\u68c0\u6d4b\u53ca\u5176\u7c7b\u578b\u5206\u7c7b\u7684\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002\u8fd9\u5f3a\u8c03\u4e86\u6211\u4eec\u7684\u7cfb\u7edf\u8d4b\u80fd\u7528\u6237\u3001\u4f7f\u5176\u80fd\u591f\u5bf9\u81ea\u5df1\u9009\u62e9\u89c2\u770b\u7684\u5728\u7ebf\u5185\u5bb9\u505a\u51fa\u660e\u667a\u51b3\u5b9a\u7684\u6f5c\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5728 UCF101\u3001HMDB51 \u548c XD-Violence \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5c06\u6211\u4eec\u7684\u6a21\u578b\u4e0e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u5c55\u793a\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u6a21\u578b\u5728\u5404\u79cd\u573a\u666f\u4e0b\u7684\u51fa\u8272\u6027\u80fd\u3002 \n**|\n", "2406.07648": "|**2024-06-11**|**M-LRM: Multi-view Large Reconstruction Model**|[2406.07648](http://arxiv.org/abs/2406.07648)|null|**Despite recent advancements in the Large Reconstruction Model (LRM) demonstrating impressive results, when extending its input from single image to multiple images, it exhibits inefficiencies, subpar geometric and texture quality, as well as slower convergence speed than expected.   It is attributed to that, LRM formulates 3D reconstruction as a naive images-to-3D translation problem, ignoring the strong 3D coherence among the input images. In this paper, we propose a Multi-view Large Reconstruction Model (M-LRM) designed to efficiently reconstruct high-quality 3D shapes from multi-views in a 3D-aware manner. Specifically, we introduce a multi-view consistent cross-attention scheme to enable M-LRM to accurately query information from the input images. Moreover, we employ the 3D priors of the input multi-view images to initialize the tri-plane tokens. Compared to LRM, the proposed M-LRM can produce a tri-plane NeRF with $128 \\times 128$ resolution and generate 3D shapes of high fidelity. Experimental studies demonstrate that our model achieves a significant performance gain and faster training convergence than LRM. Project page: https://murphylmf.github.io/M-LRM/**|\n", "2406.07209": "|**2024-06-11**|**MS-Diffusion: Multi-subject Zero-shot Image Personalization with Layout Guidance**|[2406.07209](http://arxiv.org/abs/2406.07209)|**[link](https://github.com/MS-Diffusion/MS-Diffusion)**|**Recent advancements in text-to-image generation models have dramatically enhanced the generation of photorealistic images from textual prompts, leading to an increased interest in personalized text-to-image applications, particularly in multi-subject scenarios. However, these advances are hindered by two main challenges: firstly, the need to accurately maintain the details of each referenced subject in accordance with the textual descriptions; and secondly, the difficulty in achieving a cohesive representation of multiple subjects in a single image without introducing inconsistencies. To address these concerns, our research introduces the MS-Diffusion framework for layout-guided zero-shot image personalization with multi-subjects. This innovative approach integrates grounding tokens with the feature resampler to maintain detail fidelity among subjects. With the layout guidance, MS-Diffusion further improves the cross-attention to adapt to the multi-subject inputs, ensuring that each subject condition acts on specific areas. The proposed multi-subject cross-attention orchestrates harmonious inter-subject compositions while preserving the control of texts. Comprehensive quantitative and qualitative experiments affirm that this method surpasses existing models in both image and text fidelity, promoting the development of personalized text-to-image generation.**|\n"}, "3DGS": {"2406.10219": "|**2024-06-14**|**PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting**|[2406.10219](http://arxiv.org/abs/2406.10219)|null|**\u8fd1\u5e74\u6765\uff0c\u65b0\u89c6\u89d2\u5408\u6210\u6280\u672f\u7684\u8fdb\u6b65\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6e32\u67d3\u901f\u5ea6\u548c\u9ad8\u91cd\u5efa\u7cbe\u5ea6\u3002\u4e09\u7ef4\u9ad8\u65af splatting (3D-GS) \u662f\u4e00\u79cd\u57fa\u7840\u7684\u57fa\u4e8e\u70b9\u7684\u4e09\u7ef4\u573a\u666f\u53c2\u6570\u5316\u8868\u793a\u65b9\u6cd5\uff0c\u5b83\u5c06\u573a\u666f\u5efa\u6a21\u4e3a\u5927\u91cf\u4e09\u7ef4\u9ad8\u65af\u51fd\u6570\u7684\u96c6\u5408\u3002\u590d\u6742\u7684\u573a\u666f\u53ef\u80fd\u5305\u542b\u6570\u767e\u4e07\u4e2a\u9ad8\u65af\u51fd\u6570\uff0c\u5bfc\u81f4\u5b58\u50a8\u548c\u5185\u5b58\u9700\u6c42\u5de8\u5927\uff0c\u9650\u5236\u4e86 3D-GS \u5728\u8d44\u6e90\u6709\u9650\u7684\u8bbe\u5907\u4e0a\u7684\u53ef\u884c\u6027\u3002\u76ee\u524d\uff0c\u901a\u8fc7\u526a\u679d\u9ad8\u65af\u51fd\u6570\u6765\u538b\u7f29\u8fd9\u4e9b\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6280\u672f\u4f9d\u8d56\u4e8e\u7ed3\u5408\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u786e\u5b9a\u8981\u79fb\u9664\u54ea\u4e9b\u9ad8\u65af\u51fd\u6570\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u539f\u7406\u7684\u7a7a\u95f4\u654f\u611f\u6027\u526a\u679d\u8bc4\u5206\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u8fd9\u4e9b\u65b9\u6cd5\u3002\u5b83\u88ab\u8ba1\u7b97\u4e3a\u8bad\u7ec3\u89c6\u56fe\u4e0a\u7684\u91cd\u5efa\u8bef\u5dee\u76f8\u5bf9\u4e8e\u6bcf\u4e2a\u9ad8\u65af\u51fd\u6570\u7684\u7a7a\u95f4\u53c2\u6570\u7684\u4e8c\u9636\u8fd1\u4f3c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u8f6e\u526a\u679d-\u7cbe\u70bc\u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u53ef\u4ee5\u5e94\u7528\u4e8e\u4efb\u4f55\u9884\u8bad\u7ec3\u7684 3D-GS \u6a21\u578b\uff0c\u800c\u65e0\u9700\u66f4\u6539\u8bad\u7ec3\u6d41\u7a0b\u3002\u5728\u526a\u679d\u6389 88.44% \u7684\u9ad8\u65af\u51fd\u6570\u540e\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u6211\u4eec\u7684 PUP 3D-GS \u6d41\u7a0b\u5c06 3D-GS \u7684\u5e73\u5747\u6e32\u67d3\u901f\u5ea6\u63d0\u9ad8\u4e86 2.65 \u500d\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u66f4\u591a\u663e\u8457\u7684\u524d\u666f\u4fe1\u606f\uff0c\u5e76\u5728\u6765\u81ea Mip-NeRF 360\u3001Tanks & Temples \u548c Deep Blending \u6570\u636e\u96c6\u7684\u573a\u666f\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u4ee5\u524d\u7684\u526a\u679d\u6280\u672f\u66f4\u9ad8\u7684\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u3002 \n**|\n", "2406.10111": "|**2024-06-14**|**GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors**|[2406.10111](http://arxiv.org/abs/2406.10111)|null|**\u4ece\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u89c6\u56fe\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u65b0\u89c6\u89d2\u5408\u6210 (HRNVS) \u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u3002\u4ee5\u524d\u7684\u65b9\u6cd5\u4ece\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u89c6\u56fe\u4f18\u5316\u9ad8\u5206\u8fa8\u7387\u795e\u7ecf\u8f90\u5c04\u573a (NeRF)\uff0c\u4f46\u6e32\u67d3\u901f\u5ea6\u7f13\u6162\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u57fa\u4e8e 3D \u9ad8\u65af\u6837\u6761 (3DGS) \u5f00\u53d1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u56e0\u4e3a\u5b83\u80fd\u591f\u4ee5\u66f4\u5feb\u7684\u6e32\u67d3\u901f\u5ea6\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002\u4e3a\u4e86\u7f13\u89e3\u66f4\u9ad8\u5206\u8fa8\u7387\u5408\u6210\u7684\u6570\u636e\u77ed\u7f3a\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u4f7f\u7528\u5206\u6570\u84b8\u998f\u91c7\u6837 (SDS) \u5c06 2D \u77e5\u8bc6\u63d0\u53d6\u5230 3D \u4e2d\u6765\u5229\u7528\u73b0\u6210\u7684 2D \u6269\u6563\u5148\u9a8c\u3002\u7136\u800c\uff0c\u7531\u4e8e\u751f\u6210\u5148\u9a8c\u5e26\u6765\u7684\u968f\u673a\u6027\uff0c\u5c06 SDS \u76f4\u63a5\u5e94\u7528\u4e8e\u57fa\u4e8e\u9ad8\u65af\u7684 3D \u8d85\u5206\u8fa8\u7387\u4f1a\u5bfc\u81f4\u4e0d\u5e0c\u671b\u51fa\u73b0\u4e14\u5197\u4f59\u7684 3D \u9ad8\u65af\u56fe\u5143\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u6280\u672f\u6765\u51cf\u5c11 SDS \u5f15\u5165\u7684\u968f\u673a\u6270\u52a8\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec 1) \u4f7f\u7528\u9000\u706b\u7b56\u7565\u7f29\u5c0f SDS \u4e2d\u6269\u6563\u65f6\u95f4\u6b65\u957f\u7684\u8303\u56f4\uff1b2) \u5728\u5bc6\u96c6\u5316\u8fc7\u7a0b\u4e2d\u968f\u673a\u4e22\u5f03\u5197\u4f59\u7684\u9ad8\u65af\u56fe\u5143\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684 GaussainSR \u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\uff0c\u5373\u53ef\u83b7\u5f97 HRNVS \u7684\u9ad8\u8d28\u91cf\u7ed3\u679c\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://chchnii.github.io/GaussianSR/ \n**|\n", "2406.09850": "|**2024-06-14**|**GradeADreamer: Enhanced Text-to-3D Generation Using Gaussian Splatting and Multi-View Diffusion**|[2406.09850](http://arxiv.org/abs/2406.09850)|**[link](https://github.com/trapoom555/gradeadreamer)**|**\u6587\u672c\u52303D\u751f\u6210\u5df2\u7ecf\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6548\u679c\uff0c\u4f46\u4ecd\u7136\u9762\u4e34\u7740\u4e00\u4e9b\u5e38\u89c1\u7684\u6311\u6218\uff0c\u4f8b\u5982\u591a\u9762 Janus \u95ee\u9898\u4ee5\u53ca\u9ad8\u8d28\u91cf\u8d44\u4ea7\u751f\u6210\u65f6\u95f4\u8fc7\u957f\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u540d\u4e3a GradeADreamer \u7684\u65b0\u578b\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u6c34\u7ebf\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\u8be5\u6d41\u6c34\u7ebf\u80fd\u591f\u4ec5\u4f7f\u7528\u5355\u4e2a RTX 3090 GPU \u5728 30 \u5206\u949f\u5185\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8d44\u4ea7\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u91c7\u7528\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b MVDream \u751f\u6210\u9ad8\u65af\u7403\u4f53\u4f5c\u4e3a\u5148\u9a8c\uff0c\u7136\u540e\u4f7f\u7528 StableDiffusion \u4f18\u5316\u51e0\u4f55\u5f62\u72b6\u548c\u7eb9\u7406\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u51cf\u8f7b\u4e86\u591a\u9762 Janus \u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u5e73\u5747\u7528\u6237\u504f\u597d\u6392\u540d\u3002\u9879\u76ee\u4ee3\u7801\u53ef\u5728 https://github.com/trapoom555/GradeADreamer \u83b7\u53d6\u3002 \n**|\n", "2406.09733": "|**2024-06-14**|**Unified Gaussian Primitives for Scene Representation and Rendering**|[2406.09733](http://arxiv.org/abs/2406.09733)|null|**\u5728\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\uff0c\u5bfb\u627e\u4e00\u79cd\u7edf\u4e00\u7684\u573a\u666f\u8868\u793a\u65b9\u6cd5\u4ecd\u7136\u662f\u4e00\u4e2a\u7814\u7a76\u6311\u6218\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u7f51\u683c\u7684\u8868\u793a\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u5bc6\u96c6\u7684\u3001\u6a21\u7cca\u7684\u5143\u7d20\uff0c\u5e76\u4e14\u4e3a\u8fc7\u6ee4\u548c\u53ef\u5fae\u6e32\u67d3\u5f15\u5165\u4e86\u989d\u5916\u7684\u590d\u6742\u6027\u3002\u76f8\u53cd\uff0c\u57fa\u4e8e\u4f53\u7d20\u7684\u8868\u793a\u65b9\u6cd5\u96be\u4ee5\u6a21\u62df\u575a\u786c\u7684\u8868\u9762\uff0c\u5e76\u4e14\u5b58\u5728\u5185\u5b58\u9700\u6c42\u5927\u7684\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e09\u7ef4\u9ad8\u65af\u5206\u5e03\u7684\u901a\u7528\u6e32\u67d3\u57fa\u5143\uff0c\u7528\u4e8e\u7edf\u4e00\u573a\u666f\u8868\u793a\uff0c\u5176\u7279\u5f81\u662f\u5177\u6709\u4ece\u5149\u6ed1\u8868\u9762\u5230\u6a21\u7cca\u5143\u7d20\u7684\u591a\u79cd\u5916\u89c2\uff0c\u4ee5\u53ca\u57fa\u4e8e\u7269\u7406\u7684\u6563\u5c04\uff0c\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u5168\u5c40\u7167\u660e\u3002\u6211\u4eec\u57fa\u4e8e\u975e\u6307\u6570\u4f20\u8f93\u5236\u5b9a\u4e86\u57fa\u5143\u7684\u6e32\u67d3\u7406\u8bba\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e0e\u8499\u7279\u5361\u7f57\u8def\u5f84\u8ffd\u8e2a\u517c\u5bb9\u7684\u9ad8\u6548\u6e32\u67d3\u64cd\u4f5c\u3002\u65b0\u7684\u8868\u793a\u65b9\u6cd5\u53ef\u4ee5\u4ece\u4e0d\u540c\u7684\u6765\u6e90\u8f6c\u6362\u800c\u6765\uff0c\u5305\u62ec\u7f51\u683c\u548c\u4e09\u7ef4\u9ad8\u65af splatting\uff0c\u5e76\u4e14\u7531\u4e8e\u5176\u53ef\u5fae\u6027\uff0c\u53ef\u4ee5\u901a\u8fc7\u900f\u5c04\u7387\u4f18\u5316\u8fdb\u4e00\u6b65\u7ec6\u5316\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u8868\u793a\u65b9\u6cd5\u5728\u5404\u79cd\u6e32\u67d3\u5e94\u7528\u4e2d\u7684\u591a\u529f\u80fd\u6027\uff0c\u4f8b\u5982\u5168\u5c40\u7167\u660e\u548c\u5916\u89c2\u7f16\u8f91\uff0c\u540c\u65f6\u81ea\u7136\u5730\u652f\u6301\u4efb\u610f\u7167\u660e\u6761\u4ef6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06\u6211\u4eec\u7684\u8868\u793a\u65b9\u6cd5\u4e0e\u73b0\u6709\u7684\u4f53\u79ef\u8868\u793a\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u7a81\u51fa\u4e86\u5176\u5728\u7ec6\u8282\u518d\u73b0\u65b9\u9762\u7684\u6548\u7387\u3002 \n**|\n", "2406.09395": "|**2024-06-13**|**Modeling Ambient Scene Dynamics for Free-view Synthesis**|[2406.09395](http://arxiv.org/abs/2406.09395)|null|**\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5355\u76ee\u6355\u6349\u4e2d\u52a8\u6001\u5408\u6210\u81ea\u7531\u89c6\u89d2\u7684\u5468\u56f4\u573a\u666f\uff0c\u4e3a\u89c2\u770b\u4f53\u9a8c\u5e26\u6765\u6c89\u6d78\u5f0f\u7684\u8d28\u91cf\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5efa\u7acb\u57283D\u9ad8\u65af\u6e32\u67d3\uff083DGS\uff09\u7684\u6700\u65b0\u8fdb\u5c55\u4e4b\u4e0a\uff0c\u8be5\u6280\u672f\u53ef\u4ee5\u5fe0\u5b9e\u5730\u91cd\u5efa\u590d\u6742\u7684\u9759\u6001\u573a\u666f\u3002\u5148\u524d\u5c063DGS\u6269\u5c55\u5230\u8868\u793a\u52a8\u6001\u7684\u5c1d\u8bd5\u4ec5\u9650\u4e8e\u6709\u754c\u573a\u666f\u6216\u9700\u8981\u591a\u6444\u50cf\u673a\u6355\u6349\uff0c\u5e76\u4e14\u901a\u5e38\u65e0\u6cd5\u63a8\u5e7f\u5230\u672a\u89c1\u8fc7\u7684\u8fd0\u52a8\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5b9e\u9645\u5e94\u7528\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u73af\u5883\u8fd0\u52a8\u7684\u5468\u671f\u6027\u6765\u5b66\u4e60\u8fd0\u52a8\u8f68\u8ff9\u6a21\u578b\uff0c\u4ee5\u53ca\u4ed4\u7ec6\u7684\u6b63\u5219\u5316\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e9b\u91cd\u8981\u7684\u5b9e\u7528\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u57fa\u7ebf3DGS\u9759\u6001\u91cd\u5efa\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u5e76\u63d0\u9ad8\u5bf9GPU\u5185\u5b58\u5bc6\u96c6\u578b\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u7684\u5185\u5b58\u6548\u7387\u3002\u6211\u4eec\u5c55\u793a\u4e86\u5177\u6709\u590d\u6742\u7eb9\u7406\u548c\u7cbe\u7ec6\u7ed3\u6784\u5143\u7d20\u7684\u591a\u4e2a\u5468\u56f4\u81ea\u7136\u573a\u666f\u7684\u9ad8\u8d28\u91cf\u903c\u771f\u65b0\u9896\u89c6\u56fe\u5408\u6210\u3002 \n**|\n", "2406.09377": "|**2024-06-13**|**GGHead: Fast and Generalizable 3D Gaussian Heads**|[2406.09377](http://arxiv.org/abs/2406.09377)|null|**Learning 3D head priors from large 2D image collections is an important step towards high-quality 3D-aware human modeling. A core requirement is an efficient architecture that scales well to large-scale datasets and large image resolutions. Unfortunately, existing 3D GANs struggle to scale to generate samples at high resolutions due to their relatively slow train and render speeds, and typically have to rely on 2D superresolution networks at the expense of global 3D consistency. To address these challenges, we propose Generative Gaussian Heads (GGHead), which adopts the recent 3D Gaussian Splatting representation within a 3D GAN framework. To generate a 3D representation, we employ a powerful 2D CNN generator to predict Gaussian attributes in the UV space of a template head mesh. This way, GGHead exploits the regularity of the template's UV layout, substantially facilitating the challenging task of predicting an unstructured set of 3D Gaussians. We further improve the geometric fidelity of the generated 3D representations with a novel total variation loss on rendered UV coordinates. Intuitively, this regularization encourages that neighboring rendered pixels should stem from neighboring Gaussians in the template's UV space. Taken together, our pipeline can efficiently generate 3D heads trained only from single-view 2D image observations. Our proposed framework matches the quality of existing 3D head GANs on FFHQ while being both substantially faster and fully 3D consistent. As a result, we demonstrate real-time generation and rendering of high-quality 3D-consistent heads at $1024^2$ resolution for the first time.**|\n", "2406.08920": "|**2024-06-14**|**AV-GS: Learning Material and Geometry Aware Priors for Novel View Acoustic Synthesis**|[2406.08920](http://arxiv.org/abs/2406.08920)|null|**Novel view acoustic synthesis (NVAS) aims to render binaural audio at any target viewpoint, given a mono audio emitted by a sound source at a 3D scene. Existing methods have proposed NeRF-based implicit models to exploit visual cues as a condition for synthesizing binaural audio. However, in addition to low efficiency originating from heavy NeRF rendering, these methods all have a limited ability of characterizing the entire scene environment such as room geometry, material properties, and the spatial relation between the listener and sound source. To address these issues, we propose a novel Audio-Visual Gaussian Splatting (AV-GS) model. To obtain a material-aware and geometry-aware condition for audio synthesis, we learn an explicit point-based scene representation with an audio-guidance parameter on locally initialized Gaussian points, taking into account the space relation from the listener and sound source. To make the visual scene model audio adaptive, we propose a point densification and pruning strategy to optimally distribute the Gaussian points, with the per-point contribution in sound propagation (e.g., more points needed for texture-less wall surfaces as they affect sound path diversion). Extensive experiments validate the superiority of our AV-GS over existing alternatives on the real-world RWAS and simulation-based SoundSpaces datasets.**|\n", "2406.08759": "|**2024-06-13**|**Gaussian-Forest: Hierarchical-Hybrid 3D Gaussian Splatting for Compressed Scene Modeling**|[2406.08759](http://arxiv.org/abs/2406.08759)|null|**The field of novel-view synthesis has recently witnessed the emergence of 3D Gaussian Splatting, which represents scenes in a point-based manner and renders through rasterization. This methodology, in contrast to Radiance Fields that rely on ray tracing, demonstrates superior rendering quality and speed. However, the explicit and unstructured nature of 3D Gaussians poses a significant storage challenge, impeding its broader application. To address this challenge, we introduce the Gaussian-Forest modeling framework, which hierarchically represents a scene as a forest of hybrid 3D Gaussians. Each hybrid Gaussian retains its unique explicit attributes while sharing implicit ones with its sibling Gaussians, thus optimizing parameterization with significantly fewer variables. Moreover, adaptive growth and pruning strategies are designed, ensuring detailed representation in complex regions and a notable reduction in the number of required Gaussians. Extensive experiments demonstrate that Gaussian-Forest not only maintains comparable speed and quality but also achieves a compression rate surpassing 10 times, marking a significant advancement in efficient scene modeling. Codes are available at https://github.com/Xian-Bei/GaussianForest.**|\n", "2406.08488": "|**2024-06-12**|**ICE-G: Image Conditional Editing of 3D Gaussian Splats**|[2406.08488](http://arxiv.org/abs/2406.08488)|null|**\u8fd1\u5e74\u6765\uff0c\u51fa\u73b0\u4e86\u8bb8\u591a\u521b\u5efa\u9ad8\u8d28\u91cf3D\u8d44\u4ea7\u548c\u573a\u666f\u7684\u6280\u672f\u3002\u7136\u800c\uff0c\u5728\u7f16\u8f91\u8fd9\u4e9b\u5bf9\u8c61\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u901f\u5ea6\u6162\uff0c\u8981\u4e48\u5728\u8d28\u91cf\u4e0a\u59a5\u534f\uff0c\u8981\u4e48\u65e0\u6cd5\u63d0\u4f9b\u8db3\u591f\u7684\u5b9a\u5236\u5316\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4ece\u5355\u4e2a\u53c2\u8003\u89c6\u56fe\u5feb\u901f\u7f16\u8f913D\u6a21\u578b\u3002\u6211\u4eec\u7684\u6280\u672f\u9996\u5148\u5bf9\u7f16\u8f91\u56fe\u50cf\u8fdb\u884c\u5206\u5272\uff0c\u7136\u540e\u4f7f\u7528DINO\u7279\u5f81\u5728\u9009\u5b9a\u7684\u5206\u5272\u6570\u636e\u96c6\u89c6\u56fe\u4e2d\u5339\u914d\u8bed\u4e49\u4e0a\u5bf9\u5e94\u7684\u533a\u57df\u3002\u7136\u540e\uff0c\u53ef\u4ee5\u4ee5\u8bed\u4e49\u5408\u7406\u7684\u65b9\u5f0f\u5c06\u7f16\u8f91\u56fe\u50cf\u7279\u5b9a\u533a\u57df\u7684\u989c\u8272\u6216\u7eb9\u7406\u66f4\u6539\u81ea\u52a8\u5e94\u7528\u4e8e\u5176\u4ed6\u89c6\u56fe\u3002\u8fd9\u4e9b\u7f16\u8f91\u540e\u7684\u89c6\u56fe\u5145\u5f53\u66f4\u65b0\u540e\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u8fdb\u4e00\u6b65\u8bad\u7ec3\u548c\u91cd\u65b0\u6837\u5f0f\u53163D\u573a\u666f\u3002\u6700\u7ec8\u7ed3\u679c\u662f\u4e00\u4e2a\u7ecf\u8fc7\u7f16\u8f91\u76843D\u6a21\u578b\u3002\u6211\u4eec\u7684\u6846\u67b6\u652f\u6301\u5404\u79cd\u7f16\u8f91\u4efb\u52a1\uff0c\u4f8b\u5982\u624b\u52a8\u5c40\u90e8\u7f16\u8f91\u3001\u57fa\u4e8e\u5bf9\u5e94\u7684\u4efb\u4f55\u793a\u4f8b\u56fe\u50cf\u7684\u98ce\u683c\u8fc1\u79fb\uff0c\u4ee5\u53ca\u6765\u81ea\u591a\u4e2a\u793a\u4f8b\u56fe\u50cf\u7684\u4e0d\u540c\u98ce\u683c\u7684\u7ec4\u5408\u3002\u6211\u4eec\u4f7f\u7528\u9ad8\u65af\u6837\u6761\u4f5c\u4e3a\u6211\u4eec\u7684\u4e3b\u89813D\u8868\u793a\uff0c\u56e0\u4e3a\u5b83\u901f\u5ea6\u5feb\u4e14\u6613\u4e8e\u5c40\u90e8\u7f16\u8f91\uff0c\u4f46\u6211\u4eec\u7684\u6280\u672f\u4e5f\u9002\u7528\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5982NeRFs\u3002\u6211\u4eec\u901a\u8fc7\u591a\u4e2a\u4f8b\u5b50\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u4ea7\u751f\u66f4\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u63d0\u4f9b\u5bf9\u7f16\u8f91\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002\u9879\u76ee\u9875\u9762\uff1aice-gaussian.github.io \n**|\n", "2406.08475": "|**2024-06-12**|**Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models**|[2406.08475](http://arxiv.org/abs/2406.08475)|null|**\u4ece\u5355\u5f20RGB\u56fe\u50cf\u521b\u5efa\u903c\u771f\u7684\u4eba\u7269\u5316\u8eab\u662f\u4e00\u4e2a\u9887\u5177\u5438\u5f15\u529b\u4f46\u6781\u5177\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u7531\u4e8e\u5176\u4e0d\u9002\u5b9a\u6027\uff0c\u8fd1\u671f\u5de5\u4f5c\u5229\u7528\u5728\u5927\u578b\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u76842D\u6269\u6563\u6a21\u578b\u7684\u5f3a\u5927\u5148\u9a8c\u3002\u5c3d\u7ba12D\u6269\u6563\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u65e0\u6cd5\u63d0\u4f9b\u5177\u6709\u4fdd\u8bc13D\u4e00\u81f4\u6027\u7684\u591a\u89c6\u56fe\u5f62\u72b6\u5148\u9a8c\u3002\u6211\u4eec\u63d0\u51fa\u4e86Human 3Diffusion\uff1a\u901a\u8fc7\u663e\u5f0f3D\u4e00\u81f4\u6027\u6269\u6563\u521b\u5efa\u903c\u771f\u7684\u4eba\u7269\u5316\u8eab\u3002\u6211\u4eec\u7684\u4e3b\u8981\u89c1\u89e3\u662f\uff0c2D\u591a\u89c6\u56fe\u6269\u6563\u548c3D\u91cd\u5efa\u6a21\u578b\u53ef\u4ee5\u76f8\u4e92\u63d0\u4f9b\u8865\u5145\u4fe1\u606f\uff0c\u901a\u8fc7\u5c06\u5b83\u4eec\u7d27\u5bc6\u8026\u5408\uff0c\u6211\u4eec\u53ef\u4ee5\u5145\u5206\u5229\u7528\u4e24\u79cd\u6a21\u578b\u7684\u6f5c\u529b\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u50cf\u6761\u4ef6\u751f\u6210\u5f0f3D\u9ad8\u65af splat \u91cd\u5efa\u6a21\u578b\uff0c\u5b83\u5229\u7528\u4e86\u6765\u81ea2D\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b\u7684\u5148\u9a8c\uff0c\u5e76\u63d0\u4f9b\u4e86\u663e\u5f0f\u76843D\u8868\u793a\uff0c\u8fd9\u8fdb\u4e00\u6b65\u6307\u5bfc\u4e862D\u53cd\u5411\u91c7\u6837\u8fc7\u7a0b\u4ee5\u83b7\u5f97\u66f4\u597d\u76843D\u4e00\u81f4\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u80fd\u591f\u4ece\u5355\u5f20RGB\u56fe\u50cf\u521b\u5efa\u903c\u771f\u7684\u4eba\u7269\u5316\u8eab\uff0c\u5728\u51e0\u4f55\u5f62\u72b6\u548c\u5916\u89c2\u65b9\u9762\u5747\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u3002\u5927\u91cf\u7684\u6d88\u878d\u5b9e\u9a8c\u4e5f\u9a8c\u8bc1\u4e86\u6211\u4eec\u8bbe\u8ba1\u7684\u6709\u6548\u6027\uff0c(1) \u751f\u6210\u5f0f3D\u91cd\u5efa\u4e2d\u7684\u591a\u89c6\u56fe2D\u5148\u9a8c\u6761\u4ef6\u548c (2) \u901a\u8fc7\u663e\u5f0f3D\u8868\u793a\u5bf9\u91c7\u6837\u8f68\u8ff9\u8fdb\u884c\u4e00\u81f4\u6027\u7ec6\u5316\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5728https://yuxuan-xue.com/human-3diffusion\u4e0a\u53d1\u5e03\u3002 \n**|\n"}, "3D/CG": {"2406.09792": "|**2024-06-14**|**A Two-Stage Masked Autoencoder Based Network for Indoor Depth Completion**|[2406.09792](http://arxiv.org/abs/2406.09792)|**[link](https://github.com/kailaisun/indoor-depth-completion)**|**\u6df1\u5ea6\u56fe\u50cf\u5728\u4e09\u7ef4\u91cd\u5efa\u3001\u81ea\u52a8\u9a7e\u9a76\u3001\u589e\u5f3a\u73b0\u5b9e\u3001\u673a\u5668\u4eba\u5bfc\u822a\u548c\u573a\u666f\u7406\u89e3\u7b49\u9886\u57df\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u5546\u7528\u7ea7\u6df1\u5ea6\u76f8\u673a\u96be\u4ee5\u611f\u77e5\u660e\u4eae\u3001\u5149\u6ed1\u3001\u900f\u660e\u548c\u8fdc\u5904\u8868\u9762\u7684\u6df1\u5ea6\u3002\u5c3d\u7ba1\u73b0\u6709\u7684\u6df1\u5ea6\u8865\u5168\u65b9\u6cd5\u5df2\u7ecf\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5e94\u7528\u4e8e\u590d\u6742\u7684\u5ba4\u5185\u573a\u666f\u65f6\uff0c\u5b83\u4eec\u7684\u6027\u80fd\u4ecd\u7136\u6709\u9650\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e24\u6b65 Transformer \u7684\u5ba4\u5185\u6df1\u5ea6\u8865\u5168\u7f51\u7edc\u3002\u4e0e\u73b0\u6709\u7684\u6df1\u5ea6\u8865\u5168\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u91c7\u7528\u57fa\u4e8e\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u6765\u5b66\u4e60\u7f3a\u5931\u6df1\u5ea6\u503c\u7684\u6709\u6548\u6f5c\u5728\u8868\u793a\uff1b\u7136\u540e\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6807\u8bb0\u878d\u5408\u673a\u5236\u7684\u89e3\u7801\u5668\uff0c\u4ee5\u4ece\u8054\u5408 RGB \u548c\u4e0d\u5b8c\u6574\u6df1\u5ea6\u56fe\u50cf\u4e2d\u5b8c\u6210\uff08\u5373\u91cd\u5efa\uff09\u5b8c\u6574\u6df1\u5ea6\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u7f51\u7edc\u5728 Matterport3D \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002 \u6b64\u5916\uff0c\u4e3a\u4e86\u9a8c\u8bc1\u6df1\u5ea6\u8865\u5168\u4efb\u52a1\u7684\u91cd\u8981\u6027\uff0c\u6211\u4eec\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u5e94\u7528\u4e8e\u5ba4\u5185\u4e09\u7ef4\u91cd\u5efa\u3002\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u6f14\u793a\u53ef\u5728 https://github.com/kailaisun/Indoor-Depth-Completion \u83b7\u53d6\u3002 \n**|\n", "2406.09756": "|**2024-06-14**|**Grounding Image Matching in 3D with MASt3R**|[2406.09756](http://arxiv.org/abs/2406.09756)|null|**\u56fe\u50cf\u5339\u914d\u662f\u6240\u6709\u9ad8\u6027\u80fd\u4e09\u7ef4\u89c6\u89c9\u7b97\u6cd5\u548c\u6d41\u7a0b\u4e2d\u7684\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u5339\u914d\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u4e0e\u76f8\u673a\u59ff\u6001\u548c\u573a\u666f\u51e0\u4f55\u5bc6\u5207\u76f8\u5173\u76843D\u95ee\u9898\uff0c\u4f46\u5b83\u901a\u5e38\u88ab\u89c6\u4e3a\u4e00\u4e2a2D\u95ee\u9898\u3002\u8fd9\u770b\u4f3c\u5408\u7406\uff0c\u56e0\u4e3a\u5339\u914d\u7684\u76ee\u6807\u662f\u57282D\u50cf\u7d20\u573a\u4e4b\u95f4\u5efa\u7acb\u5bf9\u5e94\u5173\u7cfb\uff0c\u4f46\u4f3c\u4e4e\u4e5f\u662f\u4e00\u4e2a\u6f5c\u5728\u7684\u5371\u9669\u9009\u62e9\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u91c7\u53d6\u4e86\u4e0d\u540c\u7684\u7acb\u573a\uff0c\u5e76\u5efa\u8bae\u5c06\u5339\u914d\u89c6\u4e3a\u4e00\u98793D\u4efb\u52a1\uff0c\u5e76\u4f7f\u7528DUSt3R\uff0c\u8fd9\u662f\u4e00\u4e2a\u6700\u8fd1\u51fa\u73b0\u7684\u3001\u57fa\u4e8eTransformer\u7684\u5f3a\u59273D\u91cd\u5efa\u6846\u67b6\u3002\u57fa\u4e8e\u70b9\u56fe\u56de\u5f52\uff0c\u8be5\u65b9\u6cd5\u5728\u5339\u914d\u5177\u6709\u6781\u7aef\u89c6\u70b9\u53d8\u5316\u7684\u89c6\u56fe\u65f6\u8868\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u7cbe\u5ea6\u6709\u9650\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u5728\u4fdd\u6301\u5176\u9c81\u68d2\u6027\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u8fd9\u79cd\u65b9\u6cd5\u7684\u5339\u914d\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5efa\u8bae\u5728DUSt3R\u7f51\u7edc\u4e2d\u589e\u52a0\u4e00\u4e2a\u65b0\u7684\u5934\u90e8\uff0c\u8f93\u51fa\u5bc6\u96c6\u7684\u5c40\u90e8\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u989d\u5916\u7684\u5339\u914d\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u89e3\u51b3\u4e86\u5bc6\u96c6\u5339\u914d\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5982\u679c\u4e0d\u4ed4\u7ec6\u5904\u7406\uff0c\u8fd9\u5bf9\u4e8e\u4e0b\u6e38\u5e94\u7528\u6765\u8bf4\u901f\u5ea6\u4f1a\u975e\u5e38\u6162\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u5feb\u901f\u4e92\u60e0\u5339\u914d\u65b9\u6848\uff0c\u5b83\u4e0d\u4ec5\u5c06\u5339\u914d\u901f\u5ea6\u63d0\u9ad8\u4e86\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u800c\u4e14\u8fd8\u5177\u6709\u7406\u8bba\u4e0a\u7684\u4fdd\u8bc1\uff0c\u5e76\u4e14\u6700\u7ec8\u4ea7\u751f\u4e86\u66f4\u597d\u7684\u7ed3\u679c\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684MASt3R\u65b9\u6cd5\u5728\u591a\u4e2a\u5339\u914d\u4efb\u52a1\u4e0a\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002\u7279\u522b\u662f\uff0c\u5728\u6781\u5177\u6311\u6218\u6027\u7684\u65e0\u5730\u56fe\u5b9a\u4f4d\u6570\u636e\u96c6\u4e0a\uff0c\u5b83\u7684VCRE AUC\u6bd4\u5df2\u53d1\u8868\u7684\u6700\u4f73\u65b9\u6cd5\u63d0\u9ad8\u4e8630%\uff08\u7edd\u5bf9\u6539\u8fdb\uff09\u3002 \n**|\n", "2406.09613": "|**2024-06-13**|**ImageNet3D: Towards General-Purpose Object-Level 3D Understanding**|[2406.09613](http://arxiv.org/abs/2406.09613)|**[link](https://github.com/wufeim/imagenet3d)**|**A vision model with general-purpose object-level 3D understanding should be capable of inferring both 2D (e.g., class name and bounding box) and 3D information (e.g., 3D location and 3D viewpoint) for arbitrary rigid objects in natural images. This is a challenging task, as it involves inferring 3D information from 2D signals and most importantly, generalizing to rigid objects from unseen categories. However, existing datasets with object-level 3D annotations are often limited by the number of categories or the quality of annotations. Models developed on these datasets become specialists for certain categories or domains, and fail to generalize. In this work, we present ImageNet3D, a large dataset for general-purpose object-level 3D understanding. ImageNet3D augments 200 categories from the ImageNet dataset with 2D bounding box, 3D pose, 3D location annotations, and image captions interleaved with 3D information. With the new annotations available in ImageNet3D, we could (i) analyze the object-level 3D awareness of visual foundation models, and (ii) study and develop general-purpose models that infer both 2D and 3D information for arbitrary rigid objects in natural images, and (iii) integrate unified 3D models with large language models for 3D-related reasoning.. We consider two new tasks, probing of object-level 3D awareness and open vocabulary pose estimation, besides standard classification and pose estimation. Experimental results on ImageNet3D demonstrate the potential of our dataset in building vision models with stronger general-purpose object-level 3D understanding.**|\n", "2406.09383": "|**2024-06-13**|**Multiagent Multitraversal Multimodal Self-Driving: Open MARS Dataset**|[2406.09383](http://arxiv.org/abs/2406.09383)|null|**\u5927\u89c4\u6a21\u6570\u636e\u96c6\u63a8\u52a8\u4e86\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7814\u7a76\u7684\u6700\u65b0\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u901a\u5e38\u662f\u4ece\u4e00\u8f86\u6c7d\u8f66\u5bf9\u67d0\u4e2a\u5730\u70b9\u7684\u4e00\u6b21\u6027\u901a\u884c\u4e2d\u6536\u96c6\u7684\uff0c\u7f3a\u4e4f\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u6216\u5bf9\u540c\u4e00\u5730\u70b9\u7684\u91cd\u590d\u904d\u5386\u3002\u8fd9\u4e9b\u4fe1\u606f\u53ef\u80fd\u4f1a\u6539\u53d8\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u80fd\u529b\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u4e0e\u81ea\u52a8\u9a7e\u9a76\u516c\u53f8May Mobility\u5408\u4f5c\uff0c\u63a8\u51fa\u4e86MARS\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u7edf\u4e00\u4e86\u652f\u6301\u591a\u667a\u80fd\u4f53\u3001\u591a\u8def\u5f84\u548c\u591a\u6a21\u6001\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7814\u7a76\u7684\u573a\u666f\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0cMARS\u662f\u901a\u8fc7\u5728\u67d0\u4e2a\u5730\u7406\u533a\u57df\u5185\u884c\u9a76\u7684\u8f66\u961f\u6536\u96c6\u7684\u3002\u6bcf\u8f86\u8f66\u90fd\u6709\u81ea\u5df1\u7684\u8def\u7ebf\uff0c\u4e0d\u540c\u7684\u8f66\u8f86\u53ef\u80fd\u4f1a\u51fa\u73b0\u5728\u9644\u8fd1\u7684\u4f4d\u7f6e\u3002\u6bcf\u8f86\u8f66\u90fd\u914d\u5907\u4e86\u6fc0\u5149\u96f7\u8fbe\u548c\u73af\u89c6RGB\u6444\u50cf\u5934\u3002\u6211\u4eec\u5728MARS\u4e2d\u7b56\u5212\u4e86\u4e24\u4e2a\u5b50\u96c6\uff1a\u4e00\u4e2a\u5b50\u96c6\u4fc3\u8fdb\u4e86\u591a\u8f86\u8f66\u540c\u65f6\u51fa\u73b0\u5728\u540c\u4e00\u5730\u70b9\u7684\u534f\u540c\u9a7e\u9a76\uff0c\u53e6\u4e00\u4e2a\u5b50\u96c6\u901a\u8fc7\u591a\u8f86\u8f66\u5bf9\u540c\u4e00\u5730\u70b9\u7684\u5f02\u6b65\u904d\u5386\u5b9e\u73b0\u4e86\u8bb0\u5fc6\u56de\u6eaf\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u4f4d\u7f6e\u8bc6\u522b\u548c\u795e\u7ecf\u91cd\u5efa\u5b9e\u9a8c\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0cMARS\u5e26\u6765\u4e86\u65b0\u7684\u7814\u7a76\u673a\u4f1a\u548c\u6311\u6218\uff0c\u4f8b\u5982\u591a\u8def\u5f84\u4e09\u7ef4\u91cd\u5efa\u3001\u591a\u667a\u80fd\u4f53\u611f\u77e5\u548c\u65e0\u76d1\u7763\u76ee\u6807\u53d1\u73b0\u3002\u6211\u4eec\u7684\u6570\u636e\u548c\u4ee3\u7801\u53ef\u4ee5\u5728https://ai4ce.github.io/MARS/\u627e\u5230\u3002**|\n", "2406.09371": "|**2024-06-13**|**LRM-Zero: Training Large Reconstruction Models with Synthesized Data**|[2406.09371](http://arxiv.org/abs/2406.09371)|null|**We present LRM-Zero, a Large Reconstruction Model (LRM) trained entirely on synthesized 3D data, achieving high-quality sparse-view 3D reconstruction. The core of LRM-Zero is our procedural 3D dataset, Zeroverse, which is automatically synthesized from simple primitive shapes with random texturing and augmentations (e.g., height fields, boolean differences, and wireframes). Unlike previous 3D datasets (e.g., Objaverse) which are often captured or crafted by humans to approximate real 3D data, Zeroverse completely ignores realistic global semantics but is rich in complex geometric and texture details that are locally similar to or even more intricate than real objects. We demonstrate that our LRM-Zero, trained with our fully synthesized Zeroverse, can achieve high visual quality in the reconstruction of real-world objects, competitive with models trained on Objaverse. We also analyze several critical design choices of Zeroverse that contribute to LRM-Zero's capability and training stability. Our work demonstrates that 3D reconstruction, one of the core tasks in 3D vision, can potentially be addressed without the semantics of real-world objects. The Zeroverse's procedural synthesis code and interactive visualization are available at: https://desaixie.github.io/lrm-zero/.**|\n", "2406.08894": "|**2024-06-13**|**OpenMaterial: A Comprehensive Dataset of Complex Materials for 3D Reconstruction**|[2406.08894](http://arxiv.org/abs/2406.08894)|null|**\u8fd1\u5e74\u6765\uff0c\u8bf8\u5982\u795e\u7ecf\u8f90\u5c04\u573a\u548c\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7b49\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u7684\u8fdb\u6b65\u663e\u8457\u63a8\u52a8\u4e86\u4e09\u7ef4\u91cd\u5efa\u9886\u57df\u7684\u53d1\u5c55\u3002\u7136\u800c\uff0c\u7531\u4e8e\u91d1\u5c5e\u548c\u73bb\u7483\u7b49\u5177\u6709\u590d\u6742\u5149\u5b66\u7279\u6027\u7684\u7269\u4f53\u5177\u6709\u72ec\u7279\u7684\u955c\u9762\u53cd\u5c04\u548c\u900f\u5149\u7279\u6027\uff0c\u56e0\u6b64\u5bf9\u5176\u8fdb\u884c\u7cbe\u786e\u91cd\u5efa\u4ecd\u7136\u662f\u4e00\u9879\u8270\u5de8\u7684\u6311\u6218\u3002\u4e3a\u4e86\u4fc3\u8fdb\u9488\u5bf9\u8fd9\u4e9b\u6311\u6218\u7684\u89e3\u51b3\u65b9\u6848\u7684\u5f00\u53d1\uff0c\u6211\u4eec\u5f15\u5165\u4e86 OpenMaterial \u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b 1001 \u4e2a\u7531 295 \u79cd\u4e0d\u540c\u6750\u6599\uff08\u5305\u62ec\u5bfc\u4f53\u3001\u7535\u4ecb\u8d28\u3001\u5851\u6599\u53ca\u5176\u7c97\u7cd9\u53d8\u4f53\uff09\u5236\u6210\u7684\u7269\u4f53\uff0c\u5e76\u5728 723 \u79cd\u4e0d\u540c\u7684\u7167\u660e\u6761\u4ef6\u4e0b\u8fdb\u884c\u4e86\u6355\u6349\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5229\u7528\u57fa\u4e8e\u7269\u7406\u7684\u6e32\u67d3\u6280\u672f\uff0c\u7ed3\u5408\u5b9e\u9a8c\u5ba4\u6d4b\u91cf\u7684\u6298\u5c04\u7387 (IOR)\uff0c\u751f\u6210\u4e86\u9ad8\u5ea6\u903c\u771f\u7684\u591a\u89c6\u56fe\u56fe\u50cf\uff0c\u903c\u771f\u5730\u518d\u73b0\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u7269\u4f53\u3002OpenMaterial \u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6ce8\u91ca\uff0c\u5305\u62ec\u4e09\u7ef4\u5f62\u72b6\u3001\u6750\u6599\u7c7b\u578b\u3001\u76f8\u673a\u59ff\u6001\u3001\u6df1\u5ea6\u548c\u7269\u4f53\u63a9\u7801\u3002\u5b83\u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u5bf9\u5177\u6709\u591a\u6837\u6027\u548c\u6311\u6218\u6027\u6750\u6599\u7684\u7269\u4f53\u8fdb\u884c\u73b0\u6709\u7b97\u6cd5\u5b9a\u91cf\u8bc4\u4f30\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u4ece\u800c\u4e3a\u5f00\u53d1\u80fd\u591f\u5904\u7406\u590d\u6742\u6750\u6599\u7279\u6027\u7684\u4e09\u7ef4\u91cd\u5efa\u7b97\u6cd5\u94fa\u5e73\u4e86\u9053\u8def\u3002**|\n", "2406.08475": "|**2024-06-12**|**Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models**|[2406.08475](http://arxiv.org/abs/2406.08475)|null|**\u4ece\u5355\u5f20RGB\u56fe\u50cf\u521b\u5efa\u903c\u771f\u7684\u4eba\u7269\u5316\u8eab\u662f\u4e00\u4e2a\u6781\u5177\u5438\u5f15\u529b\u4f46\u5145\u6ee1\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u7531\u4e8e\u5176\u672c\u8eab\u7684\u4e0d\u9002\u5b9a\u6027\uff0c\u8fd1\u671f\u5de5\u4f5c\u5229\u7528\u4e86\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u76842D\u6269\u6563\u6a21\u578b\u7684\u5f3a\u5927\u5148\u9a8c\u3002\u5c3d\u7ba12D\u6269\u6563\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u65e0\u6cd5\u63d0\u4f9b\u5177\u6709\u4fdd\u8bc13D\u4e00\u81f4\u6027\u7684\u591a\u89c6\u56fe\u5f62\u72b6\u5148\u9a8c\u3002\u6211\u4eec\u63d0\u51fa\u4e86Human 3Diffusion\uff1a\u901a\u8fc7\u663e\u5f0f3D\u4e00\u81f4\u6027\u6269\u6563\u521b\u5efa\u903c\u771f\u7684\u4eba\u7269\u5316\u8eab\u3002\u6211\u4eec\u7684\u4e3b\u8981\u89c1\u89e3\u662f\uff0c2D\u591a\u89c6\u56fe\u6269\u6563\u548c3D\u91cd\u5efa\u6a21\u578b\u53ef\u4ee5\u4e3a\u5f7c\u6b64\u63d0\u4f9b\u8865\u5145\u4fe1\u606f\uff0c\u5e76\u4e14\u901a\u8fc7\u5c06\u5b83\u4eec\u7d27\u5bc6\u8026\u5408\uff0c\u6211\u4eec\u53ef\u4ee5\u5145\u5206\u5229\u7528\u4e24\u79cd\u6a21\u578b\u7684\u6f5c\u529b\u3002\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u50cf\u6761\u4ef6\u751f\u6210\u5f0f3D Gaussian Splats\u91cd\u5efa\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u4e86\u6765\u81ea2D\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b\u7684\u5148\u9a8c\uff0c\u5e76\u63d0\u4f9b\u4e86\u663e\u5f0f\u76843D\u8868\u793a\uff0c\u4ece\u800c\u8fdb\u4e00\u6b65\u6307\u5bfc2D\u9006\u5411\u91c7\u6837\u8fc7\u7a0b\u4ee5\u83b7\u5f97\u66f4\u597d\u76843D\u4e00\u81f4\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u80fd\u591f\u4ece\u5355\u5f20RGB\u56fe\u50cf\u521b\u5efa\u903c\u771f\u7684\u4eba\u7269\u5316\u8eab\uff0c\u5728\u51e0\u4f55\u5f62\u72b6\u548c\u5916\u89c2\u65b9\u9762\u90fd\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u3002\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\u4e5f\u9a8c\u8bc1\u4e86\u6211\u4eec\u8bbe\u8ba1\u7684\u6709\u6548\u6027\uff0c(1) \u751f\u6210\u5f0f3D\u91cd\u5efa\u4e2d\u7684\u591a\u89c6\u56fe2D\u5148\u9a8c\u6761\u4ef6\u548c(2) \u901a\u8fc7\u663e\u5f0f3D\u8868\u793a\u5bf9\u91c7\u6837\u8f68\u8ff9\u8fdb\u884c\u4e00\u81f4\u6027\u7ec6\u5316\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5728https://yuxuan-xue.com/human-3diffusion\u4e0a\u53d1\u5e03\u3002 \n**|\n", "2406.08176": "|**2024-06-12**|**Category-level Neural Field for Reconstruction of Partially Observed Objects in Indoor Environment**|[2406.08176](http://arxiv.org/abs/2406.08176)|null|**\u795e\u7ecf\u9690\u5f0f\u8868\u793a\u6cd5\u5728 3D \u91cd\u5efa\u9886\u57df\u53d6\u5f97\u4e86\u8bf8\u591a\u6210\u529f\u6848\u4f8b\uff0c\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u5e94\u7528\u4e8e\u573a\u666f\u7406\u89e3\u6216\u7f16\u8f91\u7b49\u9886\u57df\uff0c\u4e00\u4e9b\u7814\u7a76\u5df2\u7ecf\u5c55\u793a\u4e86\u5728\u5bf9\u8c61\u7ec4\u5408\u91cd\u5efa\u65b9\u9762\u7684\u8fdb\u5c55\u3002\u5c3d\u7ba1\u5b83\u4eec\u5728\u5df2\u89c2\u5bdf\u533a\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u91cd\u5efa\u90e8\u5206\u89c2\u5bdf\u5230\u7684\u5bf9\u8c61\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u5c40\u9650\u6027\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u7c7b\u522b\u7ea7\u795e\u7ecf\u573a\uff0c\u5b83\u53ef\u4ee5\u5b66\u4e60\u573a\u666f\u4e2d\u5c5e\u4e8e\u540c\u4e00\u7c7b\u522b\u7684\u5bf9\u8c61\u4e4b\u95f4\u6709\u610f\u4e49\u7684\u5171\u540c 3D \u4fe1\u606f\u3002\u6211\u4eec\u7684\u5173\u952e\u601d\u60f3\u662f\u6839\u636e\u89c2\u5bdf\u5230\u7684\u5f62\u72b6\u5bf9\u5bf9\u8c61\u8fdb\u884c\u5b50\u5206\u7c7b\uff0c\u4ee5\u4fbf\u66f4\u597d\u5730\u8bad\u7ec3\u7c7b\u522b\u7ea7\u6a21\u578b\u3002\u7136\u540e\uff0c\u6211\u4eec\u5229\u7528\u795e\u7ecf\u573a\u6765\u6267\u884c\u5177\u6709\u6311\u6218\u6027\u7684\u90e8\u5206\u89c2\u5bdf\u5bf9\u8c61\u914d\u51c6\u4efb\u52a1\uff0c\u65b9\u6cd5\u662f\u9009\u62e9\u57fa\u4e8e\u5c04\u7ebf\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u9009\u62e9\u548c\u5bf9\u9f50\u4ee3\u8868\u6027\u5bf9\u8c61\u3002\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6539\u8fdb\u4e86\u591a\u4e2a\u7c7b\u522b\u4e2d\u672a\u89c2\u5bdf\u90e8\u5206\u7684\u91cd\u5efa\u6548\u679c\u3002 \n**|\n", "2406.07648": "|**2024-06-11**|**M-LRM: Multi-view Large Reconstruction Model**|[2406.07648](http://arxiv.org/abs/2406.07648)|null|**\u5c3d\u7ba1\u5927\u578b\u91cd\u5efa\u6a21\u578b\uff08LRM\uff09\u8fd1\u5e74\u6765\u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u8fdb\u5c55\uff0c\u4f46\u5f53\u5c06\u5176\u8f93\u5165\u4ece\u5355\u5f20\u56fe\u50cf\u6269\u5c55\u5230\u591a\u5f20\u56fe\u50cf\u65f6\uff0c\u5b83\u8868\u73b0\u51fa\u6548\u7387\u4f4e\u4e0b\u3001\u51e0\u4f55\u548c\u7eb9\u7406\u8d28\u91cf\u6b20\u4f73\u4ee5\u53ca\u6536\u655b\u901f\u5ea6\u4f4e\u4e8e\u9884\u671f\u7b49\u95ee\u9898\u3002\u8fd9\u5f52\u56e0\u4e8eLRM\u5c063D\u91cd\u5efa\u89c6\u4e3a\u4e00\u4e2a\u7b80\u5355\u7684\u56fe\u50cf\u52303D\u7684\u8f6c\u6362\u95ee\u9898\uff0c\u800c\u5ffd\u7565\u4e86\u8f93\u5165\u56fe\u50cf\u4e4b\u95f4\u5f3a\u5927\u76843D\u4e00\u81f4\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u56fe\u5927\u578b\u91cd\u5efa\u6a21\u578b\uff08M-LRM\uff09\uff0c\u65e8\u5728\u4ee53D\u611f\u77e5\u7684\u65b9\u5f0f\u4ece\u591a\u89c6\u56fe\u6709\u6548\u5730\u91cd\u5efa\u9ad8\u8d28\u91cf\u76843D\u5f62\u72b6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u4ea4\u53c9\u6ce8\u610f\u673a\u5236\uff0c\u4f7fM-LRM\u80fd\u591f\u51c6\u786e\u5730\u4ece\u8f93\u5165\u56fe\u50cf\u4e2d\u67e5\u8be2\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528\u8f93\u5165\u591a\u89c6\u56fe\u56fe\u50cf\u76843D\u5148\u9a8c\u4fe1\u606f\u6765\u521d\u59cb\u5316\u4e09\u5e73\u9762token\u3002\u4e0eLRM\u76f8\u6bd4\uff0c\u6211\u4eec\u63d0\u51fa\u7684M-LRM\u53ef\u4ee5\u751f\u6210\u5206\u8fa8\u7387\u4e3a128\u00d7128\u7684\u4e09\u5e73\u9762NeRF\uff0c\u5e76\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u76843D\u5f62\u72b6\u3002\u5b9e\u9a8c\u7814\u7a76\u8868\u660e\uff0c\u4e0eLRM\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u66f4\u5feb\u7684\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://murphylmf.github.io/M-LRM/ \n**|\n", "2406.07111": "|**2024-06-11**|**NeRSP: Neural 3D Reconstruction for Reflective Objects with Sparse Polarized Images**|[2406.07111](http://arxiv.org/abs/2406.07111)|null|**\u6211\u4eec\u63d0\u51fa\u4e86NeRSP\uff0c\u4e00\u79cd\u5229\u7528\u7a00\u758f\u504f\u632f\u56fe\u50cf\u91cd\u5efa\u53cd\u5c04\u8868\u9762\u7684\u795e\u7ecf\u4e09\u7ef4\u91cd\u5efa\u6280\u672f\u3002\u53cd\u5c04\u8868\u9762\u7684\u91cd\u5efa\u6781\u5177\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u955c\u9762\u53cd\u5c04\u4e0e\u89c6\u70b9\u76f8\u5173\uff0c\u56e0\u6b64\u8fdd\u53cd\u4e86\u591a\u89c6\u56fe\u7acb\u4f53\u7684\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u7a00\u758f\u56fe\u50cf\u8f93\u5165\u4f5c\u4e3a\u4e00\u79cd\u5b9e\u9645\u7684\u91c7\u96c6\u8bbe\u7f6e\uff0c\u7531\u4e8e\u7f3a\u4e4f\u5bf9\u5e94\u5339\u914d\uff0c\u901a\u5e38\u4f1a\u5bfc\u81f4\u7ed3\u679c\u4e0d\u5b8c\u6574\u6216\u5931\u771f\u3002\u672c\u6587\u5229\u7528\u504f\u632f\u56fe\u50cf\uff0c\u5171\u540c\u89e3\u51b3\u4e86\u6765\u81ea\u7a00\u758f\u8f93\u5165\u548c\u53cd\u5c04\u8868\u9762\u7684\u6311\u6218\u3002\u6211\u4eec\u4ece\u504f\u632f\u56fe\u50cf\u5f62\u6210\u6a21\u578b\u548c\u591a\u89c6\u56fe\u65b9\u4f4d\u89d2\u4e00\u81f4\u6027\u4e2d\u63a8\u5bfc\u51fa\u5149\u5ea6\u548c\u51e0\u4f55\u7ebf\u7d22\uff0c\u5b83\u4eec\u5171\u540c\u4f18\u5316\u4e86\u901a\u8fc7\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u5efa\u6a21\u7684\u8868\u9762\u51e0\u4f55\u5f62\u72b6\u3002\u6839\u636e\u6211\u4eec\u5bf9\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u4ec5\u7528 6 \u4e2a\u89c6\u56fe\u4f5c\u4e3a\u8f93\u5165\u5c31\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8868\u9762\u91cd\u5efa\u7ed3\u679c\u3002 \n**|\n"}, "\u5404\u7c7b\u5b66\u4e60\u65b9\u5f0f": {"2406.10115": "|**2024-06-14**|**Shelf-Supervised Multi-Modal Pre-Training for 3D Object Detection**|[2406.10115](http://arxiv.org/abs/2406.10115)|null|**\u6700\u5148\u8fdb\u7684\u4e09\u7ef4\u7269\u4f53\u63a2\u6d4b\u5668\u901a\u5e38\u5728\u6d77\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u6807\u6ce8\u4e09\u7ef4\u8fb9\u754c\u6846\u4ecd\u7136\u975e\u5e38\u6602\u8d35\u4e14\u8017\u65f6\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u6fc0\u5149\u96f7\u8fbe\u800c\u8a00\u3002\u56e0\u6b64\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528\u672a\u6807\u8bb0\u6570\u636e\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u53ef\u4ee5\u63d0\u9ad8\u6709\u9650\u6807\u7b7e\u60c5\u51b5\u4e0b\u7684\u68c0\u6d4b\u7cbe\u5ea6\u3002\u76ee\u524d\u7684\u65b9\u6cd5\u5c06\u56fe\u50cf\u9886\u57df\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u6700\u4f73\u5b9e\u8df5\u5e94\u7528\u4e8e\u70b9\u4e91\uff08\u4f8b\u5982\u5bf9\u6bd4\u5b66\u4e60\uff09\u3002\u7136\u800c\uff0c\u516c\u5f00\u53ef\u7528\u7684\u4e09\u7ef4\u6570\u636e\u96c6\u6bd4\u7528\u4e8e\u57fa\u4e8e\u56fe\u50cf\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u6570\u636e\u96c6\u8981\u5c0f\u5f97\u591a\uff0c\u591a\u6837\u6027\u4e5f\u5dee\u5f97\u591a\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u6709\u6548\u6027\u3002\u7136\u800c\uff0c\u6211\u4eec\u6ce8\u610f\u5230\uff0c\u6b64\u7c7b\u6570\u636e\u901a\u5e38\u4ee5\u591a\u6a21\u6001\u65b9\u5f0f\u6536\u96c6\uff0c\u901a\u5e38\u4e0e\u56fe\u50cf\u914d\u5bf9\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u4e0e\u5176\u4ec5\u4f7f\u7528\u81ea\u76d1\u7763\u76ee\u6807\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4e0d\u5982\u4f7f\u7528\u5728\u4e92\u8054\u7f51\u89c4\u6a21\u56fe\u50cf\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u57fa\u4e8e\u56fe\u50cf\u7684\u57fa\u7840\u6a21\u578b\u6765\u5f15\u5bfc\u70b9\u4e91\u8868\u793a\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u73b0\u6210\u76d1\u7763\u201d\u65b9\u6cd5\uff08\u4f8b\u5982\uff0c\u4f7f\u7528\u73b0\u6210\u7684\u56fe\u50cf\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u76d1\u7763\uff09\uff0c\u7528\u4e8e\u4ece\u914d\u5bf9\u7684 RGB \u548c\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u4e2d\u751f\u6210\u96f6\u6837\u672c\u4e09\u7ef4\u8fb9\u754c\u6846\u3002\u4f7f\u7528\u6b64\u7c7b\u4f2a\u6807\u7b7e\u9884\u8bad\u7ec3\u4e09\u7ef4\u63a2\u6d4b\u5668\u6bd4\u5148\u524d\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u4efb\u52a1\u80fd\u663e\u8457\u63d0\u9ad8\u534a\u76d1\u7763\u68c0\u6d4b\u7cbe\u5ea6\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u57fa\u4e8e\u56fe\u50cf\u7684\u201c\u73b0\u6210\u76d1\u7763\u201d\u6709\u52a9\u4e8e\u8bad\u7ec3\u4ec5\u4f7f\u7528\u6fc0\u5149\u96f7\u8fbe\u548c\u591a\u6a21\u6001\uff08RGB + \u6fc0\u5149\u96f7\u8fbe\uff09\u7684\u63a2\u6d4b\u5668\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728 nuScenes \u548c WOD \u4e0a\u7684\u6709\u6548\u6027\uff0c\u5728\u6709\u9650\u6570\u636e\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7684\u5de5\u4f5c\u3002 \n**|\n", "2406.09973": "|**2024-06-14**|**InstructRL4Pix: Training Diffusion for Image Editing by Reinforcement Learning**|[2406.09973](http://arxiv.org/abs/2406.09973)|null|**\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u5728\u4f7f\u7528\u81ea\u7136\u4eba\u7c7b\u8bed\u8a00\u64cd\u7eb5\u56fe\u50cf\u7684\u89c6\u89c9\u5185\u5bb9\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u73b0\u6709\u6a21\u578b\u53d7\u5230\u6570\u636e\u96c6\u8d28\u91cf\u7684\u9650\u5236\uff0c\u65e0\u6cd5\u51c6\u786e\u5b9a\u4f4d\u5177\u6709\u590d\u6742\u5bf9\u8c61\u5173\u7cfb\u7684\u56fe\u50cf\u4e2d\u7684\u7f16\u8f91\u533a\u57df\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5 (InstructRL4Pix)\uff0c\u4ee5\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u751f\u6210\u7531\u76ee\u6807\u5bf9\u8c61\u7684\u6ce8\u610f\u529b\u56fe\u5f15\u5bfc\u7684\u56fe\u50cf\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u8ba1\u7b97\u6ce8\u610f\u529b\u56fe\u4e4b\u95f4\u7684\u8ddd\u79bb\u4f5c\u4e3a\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316 (PPO) \u5bf9\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ece\u800c\u6700\u5927\u5316\u5956\u52b1\u6a21\u578b\u7684\u8f93\u51fa\u3002\u6211\u4eec\u5728\u5bf9\u8c61\u63d2\u5165\u3001\u79fb\u9664\u3001\u66ff\u6362\u548c\u53d8\u6362\u65b9\u9762\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u6a21\u578b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cInstructRL4Pix\u7a81\u7834\u4e86\u4f20\u7edf\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u5229\u7528\u65e0\u76d1\u7763\u5b66\u4e60\u4f18\u5316\u7f16\u8f91\u76ee\u6807\uff0c\u5e76\u6839\u636e\u81ea\u7136\u7684\u4eba\u7c7b\u6307\u4ee4\u5b9e\u73b0\u7cbe\u51c6\u7684\u56fe\u50cf\u7f16\u8f91\u3002 \n**|\n", "2406.09949": "|**2024-06-14**|**Neural Concept Binder**|[2406.09949](http://arxiv.org/abs/2406.09949)|**[link](https://github.com/ml-research/neuralconceptbinder)**|**The challenge in object-based visual reasoning lies in generating descriptive yet distinct concept representations. Moreover, doing this in an unsupervised fashion requires human users to understand a model's learned concepts and potentially revise false concepts. In addressing this challenge, we introduce the Neural Concept Binder, a new framework for deriving discrete concept representations resulting in what we term \"concept-slot encodings\". These encodings leverage both \"soft binding\" via object-centric block-slot encodings and \"hard binding\" via retrieval-based inference. The Neural Concept Binder facilitates straightforward concept inspection and direct integration of external knowledge, such as human input or insights from other AI models like GPT-4. Additionally, we demonstrate that incorporating the hard binding mechanism does not compromise performance; instead, it enables seamless integration into both neural and symbolic modules for intricate reasoning tasks, as evidenced by evaluations on our newly introduced CLEVR-Sudoku dataset.**|\n", "2406.09935": "|**2024-06-14**|**Forgetting Order of Continual Learning: Examples That are Learned First are Forgotten Last**|[2406.09935](http://arxiv.org/abs/2406.09935)|null|**Catastrophic forgetting poses a significant challenge in continual learning, where models often forget previous tasks when trained on new data. Our empirical analysis reveals a strong correlation between catastrophic forgetting and the learning speed of examples: examples learned early are rarely forgotten, while those learned later are more susceptible to forgetting. We demonstrate that replay-based continual learning methods can leverage this phenomenon by focusing on mid-learned examples for rehearsal. We introduce Goldilocks, a novel replay buffer sampling method that filters out examples learned too quickly or too slowly, keeping those learned at an intermediate speed. Goldilocks improves existing continual learning algorithms, leading to state-of-the-art performance across several image classification tasks.**|\n", "2406.09896": "|**2024-06-14**|**Exploring the Benefits of Vision Foundation Models for Unsupervised Domain Adaptation**|[2406.09896](http://arxiv.org/abs/2406.09896)|**[link](https://github.com/tue-mps/vfm-uda)**|**\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\uff0c\u5b9e\u73b0\u8de8\u4e0d\u540c\u6570\u636e\u57df\u7684\u7a33\u5065\u6cdb\u5316\u4ecd\u7136\u662f\u4e00\u9879\u91cd\u5927\u6311\u6218\u3002\u8fd9\u4e00\u6311\u6218\u5728\u5b89\u5168\u5173\u952e\u578b\u5e94\u7528\u4e2d\u5c24\u4e3a\u91cd\u8981\uff0c\u56e0\u4e3a\u5728\u8fd9\u4e9b\u5e94\u7528\u4e2d\uff0c\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u7cfb\u7edf\u5fc5\u987b\u5728\u8bad\u7ec3\u671f\u95f4\u672a\u89c1\u7684\u5404\u79cd\u73af\u5883\u6761\u4ef6\u4e0b\u53ef\u9760\u5730\u6267\u884c\u4efb\u52a1\u3002\u6211\u4eec\u7684\u7814\u7a76\u8c03\u67e5\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFM\uff09\u548c\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08UDA\uff09\u65b9\u6cd5\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u662f\u5426\u4e92\u8865\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5c06VFM\u4e0eUDA\u76f8\u7ed3\u5408\u6709\u4e24\u4e2a\u4e3b\u8981\u597d\u5904\uff1a\uff08a\uff09\u5b83\u53ef\u4ee5\u5728\u4fdd\u6301VFM\u7684\u5206\u5e03\u5916\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u66f4\u597d\u7684UDA\u6027\u80fd\uff0c\u4ee5\u53ca\uff08b\uff09\u5b83\u4f7f\u67d0\u4e9b\u8017\u65f6\u7684UDA\u7ec4\u4ef6\u53d8\u5f97\u5197\u4f59\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5728\u6a21\u578b\u5927\u5c0f\u76f8\u540c\u7684\u60c5\u51b5\u4e0b\uff0c\u7531\u6b64\u4ea7\u751f\u7684VFM-UDA\u65b9\u6cd5\u6bd4\u4e4b\u524d\u7684\u975eVFM\u6700\u4f73\u65b9\u6cd5\u5b9e\u73b0\u4e868.4\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u5728UDA\u8bbe\u7f6e\u4e2d\u5c06\u6027\u80fd\u63d0\u9ad8\u4e86+1.2 mIoU\uff0c\u5728\u5206\u5e03\u5916\u6cdb\u5316\u65b9\u9762\u63d0\u9ad8\u4e86+6.1 mIoU\u3002\u6b64\u5916\uff0c\u5f53\u6211\u4eec\u4f7f\u7528\u53c2\u6570\u591a3.6\u500d\u7684VFM\u65f6\uff0cVFM-UDA\u65b9\u6cd5\u4ecd\u7136\u4fdd\u6301\u4e863.3\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u5c06UDA\u6027\u80fd\u63d0\u9ad8\u4e86+3.1 mIoU\uff0c\u5c06\u5206\u5e03\u5916\u6027\u80fd\u63d0\u9ad8\u4e86+10.3 mIoU\u3002\u8fd9\u4e9b\u7ed3\u679c\u5f3a\u8c03\u4e86\u5c06VFM\u4e0eUDA\u76f8\u7ed3\u5408\u7684\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\u8bbe\u5b9a\u4e86\u65b0\u7684\u6807\u51c6\u548c\u57fa\u7ebf\u3002 \n**|\n", "2406.09831": "|**2024-06-14**|**Federated Learning driven Large Language Models for Swarm Intelligence: A Survey**|[2406.09831](http://arxiv.org/abs/2406.09831)|null|**\u8054\u90a6\u5b66\u4e60 (FL) \u4e3a\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f15\u4eba\u6ce8\u76ee\u7684\u6846\u67b6\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u6570\u636e\u9690\u79c1\u548c\u53bb\u4e2d\u5fc3\u5316\u6311\u6218\u3002\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8054\u90a6\u5b66\u4e60\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u7279\u522b\u5173\u6ce8\u673a\u5668\u9057\u5fd8\uff0c\u8fd9\u662f\u9075\u5b88\u201c\u88ab\u9057\u5fd8\u6743\u201d\u7b49\u9690\u79c1\u6cd5\u89c4\u7684\u5173\u952e\u65b9\u9762\u3002\u5728\u8054\u90a6 LLM \u7684\u80cc\u666f\u4e0b\uff0c\u673a\u5668\u9057\u5fd8\u6d89\u53ca\u7cfb\u7edf\u5730\u3001\u5b89\u5168\u5730\u4ece\u5b66\u4e60\u7684\u6a21\u578b\u4e2d\u5220\u9664\u4e2a\u4eba\u6570\u636e\u8d21\u732e\uff0c\u800c\u65e0\u9700\u4ece\u5934\u5f00\u59cb\u91cd\u65b0\u8bad\u7ec3\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u5404\u79cd\u80fd\u591f\u5b9e\u73b0\u6709\u6548\u9057\u5fd8\u7684\u7b56\u7565\uff0c\u4f8b\u5982\u6270\u52a8\u6280\u672f\u3001\u6a21\u578b\u5206\u89e3\u548c\u589e\u91cf\u5b66\u4e60\uff0c\u5e76\u5f3a\u8c03\u4e86\u5b83\u4eec\u5bf9\u7ef4\u62a4\u6a21\u578b\u6027\u80fd\u548c\u6570\u636e\u9690\u79c1\u7684\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8 \u0628\u0631\u0631\u0633\u06cc\u4e86\u6587\u732e\u4e2d\u7684\u6848\u4f8b\u7814\u7a76\u548c\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4ee5\u8bc4\u4f30\u8fd9\u4e9b\u65b9\u6cd5\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002\u6211\u4eec\u7684\u8c03\u67e5\u8868\u660e\uff0c\u4eba\u4eec\u5bf9\u5f00\u53d1\u66f4\u5065\u58ee\u548c\u53ef\u6269\u5c55\u7684\u8054\u90a6\u9057\u5fd8\u65b9\u6cd5\u8d8a\u6765\u8d8a\u611f\u5174\u8da3\uff0c\u8fd9\u8868\u660e\u4eba\u5de5\u667a\u80fd\u4f26\u7406\u4e0e\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u6280\u672f\u4ea4\u53c9\u9886\u57df\u662f\u672a\u6765\u7814\u7a76\u7684\u91cd\u8981\u65b9\u5411\u3002 \n**|\n", "2406.09799": "|**2024-06-14**|**GeoSEE: Regional Socio-Economic Estimation With a Large Language Model**|[2406.09799](http://arxiv.org/abs/2406.09799)|null|**\u8d85\u8d8a\u4f20\u7edf\u7684\u8c03\u67e5\u65b9\u5f0f\uff0c\u5c06\u5f02\u6784\u6570\u636e\u6e90\u4e0e\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u63a8\u7406\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u4e3a\u8861\u91cf\u5927\u8303\u56f4\u5730\u7406\u533a\u57df\u7684\u793e\u4f1a\u7ecf\u6d4e\u72b6\u51b5\uff08\u5982\u8d2b\u56f0\u548c\u4eba\u53e3\uff09\u5e26\u6765\u4e86\u65b0\u7684\u673a\u9047\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a GeoSEE \u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5229\u7528\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u9a71\u52a8\u7684\u7edf\u4e00\u7ba1\u9053\u6765\u4f30\u7b97\u5404\u79cd\u793e\u4f1a\u7ecf\u6d4e\u6307\u6807\u3002GeoSEE \u63a5\u6536\u4e00\u7ec4\u4e0d\u540c\u7684\u4fe1\u606f\u6a21\u5757\uff08\u5305\u62ec\u4ece\u536b\u661f\u56fe\u50cf\u9884\u5148\u6784\u5efa\u7684\u6a21\u5757\uff09\uff0c\u5e76\u9488\u5bf9\u6bcf\u4e2a\u6307\u6807\u548c\u56fd\u5bb6/\u5730\u533a\u9009\u62e9\u8981\u4f7f\u7528\u7684\u6a21\u5757\u8fdb\u884c\u4f30\u7b97\u3002\u8fd9\u79cd\u9009\u62e9\u7531 LLM \u7684\u5148\u9a8c\u793e\u4f1a\u5730\u7406\u77e5\u8bc6\u5f15\u5bfc\uff0c\u5176\u529f\u80fd\u7c7b\u4f3c\u4e8e\u9886\u57df\u4e13\u5bb6\u7684\u89c1\u89e3\u3002\u7136\u540e\uff0c\u7cfb\u7edf\u5728\u4ee5\u81ea\u7136\u8bed\u8a00\u6587\u672c\u683c\u5f0f\u805a\u5408\u6765\u81ea\u6240\u9009\u6a21\u5757\u7684\u7ed3\u679c\u540e\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u8ba1\u7b97\u76ee\u6807\u6307\u6807\u3002\u5bf9\u5904\u4e8e\u4e0d\u540c\u53d1\u5c55\u9636\u6bb5\u7684\u56fd\u5bb6/\u5730\u533a\u8fdb\u884c\u7684\u5168\u9762\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u548c\u5c11\u6837\u672c\u60c5\u51b5\u4e0b\u5747\u4f18\u4e8e\u5176\u4ed6\u9884\u6d4b\u6a21\u578b\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u7684\u53d1\u5c55\u4e2d\u56fd\u5bb6\u6216\u6b20\u53d1\u8fbe\u56fd\u5bb6\u7684\u53ef\u9760\u6027\u80fd\uff0c\u4ee5\u53ca\u5176\u6210\u672c\u6548\u76ca\uff0c\u51f8\u663e\u4e86\u5176\u5728\u5168\u7403\u8303\u56f4\u5185\u6301\u7eed\u652f\u6301\u548c\u76d1\u6d4b\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\uff08\u5982\u51cf\u8d2b\u548c\u516c\u5e73\u589e\u957f\uff09\u8fdb\u5c55\u7684\u6f5c\u529b\u3002 \n**|\n", "2406.09790": "|**2024-06-14**|**Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity**|[2406.09790](http://arxiv.org/abs/2406.09790)|null|**\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u5ea6\uff08STS\uff09\u662f\u8ba1\u7b97\u8bed\u8a00\u5b66\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u7814\u7a76\u65b9\u5411\uff0c\u662f\u8861\u91cf\u5d4c\u5165\u6a21\u578b\u7f16\u7801\u80fd\u529b\u7684\u5173\u952e\u6307\u6807\u3002\u5728\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\u7684\u63a8\u52a8\u4e0b\uff0c\u9886\u5148\u7684\u53e5\u5b50\u8868\u793a\u65b9\u6cd5\u5728 SentEval \u7684\u4e03\u4e2a STS \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5df2\u7ecf\u53ef\u4ee5\u8fbe\u5230\u7ea6 86 \u7684\u5e73\u5747 Spearman \u76f8\u5173\u7cfb\u6570\u5f97\u5206\u3002\u7136\u800c\uff0c\u8fdb\u4e00\u6b65\u7684\u6539\u8fdb\u53d8\u5f97\u8d8a\u6765\u8d8a\u5fae\u4e0d\u8db3\u9053\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u7684\u5e73\u5747\u5f97\u5206\u90fd\u6ca1\u6709\u8d85\u8fc7 87\u3002\u672c\u6587\u6df1\u5165\u5206\u6790\u4e86\u8fd9\u4e00\u73b0\u8c61\uff0c\u5f97\u51fa\u7ed3\u8bba\uff1a\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u7684 Spearman \u76f8\u5173\u7cfb\u6570\u5f97\u5206\u7684\u4e0a\u9650\u4e3a 87.5\u3002\u4e3a\u4e86\u7a81\u7834\u8fd9\u4e00\u4e0a\u9650\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Pcc-tuning \u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528 Pearson \u76f8\u5173\u7cfb\u6570\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u6539\u8fdb\u5bf9\u6bd4\u5b66\u4e60\u4e4b\u5916\u7684\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPcc-tuning \u660e\u663e\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u7684\u7b56\u7565\uff0c\u5c06 Spearman \u76f8\u5173\u7cfb\u6570\u5f97\u5206\u63d0\u9ad8\u5230 90 \u4ee5\u4e0a\u3002 \n**|\n", "2406.09782": "|**2024-06-14**|**Unsupervised Monocular Depth Estimation Based on Hierarchical Feature-Guided Diffusion**|[2406.09782](http://arxiv.org/abs/2406.09782)|null|**Unsupervised monocular depth estimation has received widespread attention because of its capability to train without ground truth. In real-world scenarios, the images may be blurry or noisy due to the influence of weather conditions and inherent limitations of the camera. Therefore, it is particularly important to develop a robust depth estimation model. Benefiting from the training strategies of generative networks, generative-based methods often exhibit enhanced robustness. In light of this, we employ a well-converging diffusion model among generative networks for unsupervised monocular depth estimation. Additionally, we propose a hierarchical feature-guided denoising module. This model significantly enriches the model's capacity for learning and interpreting depth distribution by fully leveraging image features to guide the denoising process. Furthermore, we explore the implicit depth within reprojection and design an implicit depth consistency loss. This loss function serves to enhance the performance of the model and ensure the scale consistency of depth within a video sequence. We conduct experiments on the KITTI, Make3D, and our self-collected SIMIT datasets. The results indicate that our approach stands out among generative-based models, while also showcasing remarkable robustness.**|\n", "2406.09710": "|**2024-06-14**|**Fine-Grained Urban Flow Inference with Multi-scale Representation Learning**|[2406.09710](http://arxiv.org/abs/2406.09710)|null|**Fine-grained urban flow inference (FUFI) is a crucial transportation service aimed at improving traffic efficiency and safety. FUFI can infer fine-grained urban traffic flows based solely on observed coarse-grained data. However, most of existing methods focus on the influence of single-scale static geographic information on FUFI, neglecting the interactions and dynamic information between different-scale regions within the city. Different-scale geographical features can capture redundant information from the same spatial areas. In order to effectively learn multi-scale information across time and space, we propose an effective fine-grained urban flow inference model called UrbanMSR, which uses self-supervised contrastive learning to obtain dynamic multi-scale representations of neighborhood-level and city-level geographic information, and fuses multi-scale representations to improve fine-grained accuracy. The fusion of multi-scale representations enhances fine-grained. We validate the performance through extensive experiments on three real-world datasets. The resutls compared with state-of-the-art methods demonstrate the superiority of the proposed model.**|\n"}}