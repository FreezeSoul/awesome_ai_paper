{"\u591a\u6a21\u6001": {"2409.02914": "|**2024-09-04**|[Can LVLMs Obtain a Driver's License? A Benchmark Towards Reliable AGI for Autonomous Driving](http://arxiv.org/abs/2409.02914)|null|Large Vision-Language Models (LVLMs) have recently garnered significant attention, with many efforts aimed at harnessing their general knowledge to enhance the interpretability and robustness of autonomous driving models. However, LVLMs typically rely on large, general-purpose datasets and lack the specialized expertise required for professional and safe driving. Existing vision-language driving datasets focus primarily on scene understanding and decision-making, without providing explicit guidance on traffic rules and driving skills, which are critical aspects directly related to driving safety. To bridge this gap, we propose IDKB, a large-scale dataset containing over one million data items collected from various countries, including driving handbooks, theory test data, and simulated road test data. Much like the process of obtaining a driver's license, IDKB encompasses nearly all the explicit knowledge needed for driving from theory to practice. In particular, we conducted comprehensive tests on 15 LVLMs using IDKB to assess their reliability in the context of autonomous driving and provided extensive analysis. We also fine-tuned popular models, achieving notable performance improvements, which further validate the significance of our dataset. The project page can be found at: \\url{https://4dvlab.github.io/project_page/idkb.html}||\n", "2409.02882": "|**2024-09-04**|[Benchmarking Spurious Bias in Few-Shot Image Classifiers](http://arxiv.org/abs/2409.02882)|**[link](https://github.com/gtzheng/fewstab)**|Few-shot image classifiers are designed to recognize and classify new data with minimal supervision and limited data but often show reliance on spurious correlations between classes and spurious attributes, known as spurious bias. Spurious correlations commonly hold in certain samples and few-shot classifiers can suffer from spurious bias induced from them. There is an absence of an automatic benchmarking system to assess the robustness of few-shot classifiers against spurious bias. In this paper, we propose a systematic and rigorous benchmark framework, termed FewSTAB, to fairly demonstrate and quantify varied degrees of robustness of few-shot classifiers to spurious bias. FewSTAB creates few-shot evaluation tasks with biased attributes so that using them for predictions can demonstrate poor performance. To construct these tasks, we propose attribute-based sample selection strategies based on a pre-trained vision-language model, eliminating the need for manual dataset curation. This allows FewSTAB to automatically benchmark spurious bias using any existing test data. FewSTAB offers evaluation results in a new dimension along with a new design guideline for building robust classifiers. Moreover, it can benchmark spurious bias in varied degrees and enable designs for varied degrees of robustness. Its effectiveness is demonstrated through experiments on ten few-shot learning methods across three datasets. We hope our framework can inspire new designs of robust few-shot classifiers. Our code is available at https://github.com/gtzheng/FewSTAB.||\n", "2409.02834": "|**2024-09-06**|[CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of Large Multimodal Models](http://arxiv.org/abs/2409.02834)|null|Large language models (LLMs) have obtained promising results in mathematical reasoning, which is a foundational skill for human intelligence. Most previous studies focus on improving and measuring the performance of LLMs based on textual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few researchers have released English multimodal math datasets (e.g., MATHVISTA and MATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In this paper, we release a Chinese multimodal math (CMM-Math) dataset, including benchmark and training parts, to evaluate and enhance the mathematical reasoning of LMMs. CMM-Math contains over 28,000 high-quality samples, featuring a variety of problem types (e.g., multiple-choice, fill-in-the-blank, and so on) with detailed solutions across 12 grade levels from elementary to high school in China. Specifically, the visual context may be present in the questions or opinions, which makes this dataset more challenging. Through comprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math dataset face challenges, emphasizing the necessity for further improvements in LMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to handle the problems with mixed input of multiple images and text segments. We train our model using three stages, including foundational pre-training, foundational fine-tuning, and mathematical fine-tuning. The extensive experiments indicate that our model effectively improves math reasoning performance by comparing it with the SOTA LMMs over three multimodal mathematical datasets.||\n", "2409.02813": "|**2024-09-04**|[MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark](http://arxiv.org/abs/2409.02813)|null|This paper introduces MMMU-Pro, a robust version of the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark. MMMU-Pro rigorously assesses multimodal models' true understanding and reasoning capabilities through a three-step process based on MMMU: (1) filtering out questions answerable by text-only models, (2) augmenting candidate options, and (3) introducing a vision-only input setting where questions are embedded within images. This setting challenges AI to truly \"see\" and \"read\" simultaneously, testing a fundamental human cognitive skill of seamlessly integrating visual and textual information. Results show that model performance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8% to 26.9% across models. We explore the impact of OCR prompts and Chain of Thought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT generally improves performance. MMMU-Pro provides a more rigorous evaluation tool, closely mimicking real-world scenarios and offering valuable directions for future research in multimodal AI.||\n", "2409.02664": "|**2024-09-04**|[Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection](http://arxiv.org/abs/2409.02664)|null|The proliferation of deepfake faces poses huge potential negative impacts on our daily lives. Despite substantial advancements in deepfake detection over these years, the generalizability of existing methods against forgeries from unseen datasets or created by emerging generative models remains constrained. In this paper, inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach that repurposes a well-trained VLM for general deepfake detection. Motivated by the model reprogramming paradigm that manipulates the model prediction via data perturbations, our method can reprogram a pretrained VLM model (e.g., CLIP) solely based on manipulating its input without tuning the inner parameters. Furthermore, we insert a pseudo-word guided by facial identity into the text prompt. Extensive experiments on several popular benchmarks demonstrate that (1) the cross-dataset and cross-manipulation performances of deepfake detection can be significantly and consistently improved (e.g., over 88% AUC in cross-dataset setting from FF++ to WildDeepfake) using a pre-trained CLIP model with our proposed reprogramming method; (2) our superior performances are at less cost of trainable parameters, making it a promising approach for real-world applications.||\n", "2409.02530": "|**2024-09-04**|[Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models](http://arxiv.org/abs/2409.02530)|null|The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of kidney function in clinical practice. Although traditional equations and Machine Learning (ML) models using clinical and laboratory data can estimate eGFR, accurately predicting future eGFR levels remains a significant challenge for nephrologists and ML researchers. Recent advances demonstrate that Large Language Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust foundation models for diverse applications. This study investigates the potential of LMMs to predict future eGFR levels with a dataset consisting of laboratory and clinical values from 50 patients. By integrating various prompting techniques and ensembles of LMMs, our findings suggest that these models, when combined with precise prompts and visual representations of eGFR trajectories, offer predictive performance comparable to existing ML models. This research extends the application of foundation models and suggests avenues for future studies to harness these models in addressing complex medical forecasting challenges.||\n", "2409.02278": "|**2024-09-03**|[Evaluation and Comparison of Visual Language Models for Transportation Engineering Problems](http://arxiv.org/abs/2409.02278)|null|\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u6700\u65b0\u53d1\u5c55\u663e\u793a\u51fa\u5176\u5728\u56fe\u50cf\u7406\u89e3\u76f8\u5173\u5e94\u7528\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u4ea4\u901a\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4f8b\u5982\u56fe\u50cf\u5206\u7c7b\u548c\u76ee\u6807\u68c0\u6d4b\u3002\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u5305\u62ec\u62e5\u5835\u68c0\u6d4b\u548c\u88c2\u7f1d\u8bc6\u522b\uff0c\u800c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u5219\u7528\u4e8e\u8bc6\u522b\u672a\u4f69\u6234\u5934\u76d4\u7684\u884c\u4e3a\u3002\u6211\u4eec\u5e94\u7528\u4e86CLIP\u3001BLIP\u3001OWL-ViT\u3001Llava-Next\u7b49\u5f00\u6e90\u6a21\u578b\u548c\u95ed\u6e90\u6a21\u578bGPT-4o\uff0c\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4ee5\u5229\u7528\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u6765\u5b8c\u6210\u57fa\u4e8e\u89c6\u89c9\u7684\u4ea4\u901a\u4efb\u52a1\u3002\u8fd9\u4e9b\u4efb\u52a1\u662f\u901a\u8fc7\u5bf9VLM\u6a21\u578b\u5e94\u7528\u96f6\u6837\u672c\u63d0\u793a\u6765\u6267\u884c\u7684\uff0c\u56e0\u4e3a\u96f6\u6837\u672c\u63d0\u793a\u5141\u8bb8\u5728\u4e0d\u5bf9\u4efb\u52a1\u8fdb\u884c\u4efb\u4f55\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u4efb\u52a1\u3002\u5b83\u6d88\u9664\u4e86\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u6807\u6ce8\u6570\u636e\u96c6\u6216\u5fae\u8c03\u7684\u9700\u6c42\u3002\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4e0e\u57fa\u51c6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u4f46\u5728\u76ee\u6807\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4ecd\u6709\u6539\u8fdb\u7684\u7a7a\u95f4\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u5bf9\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u7a81\u51fa\u4e86\u8fd9\u4e9b\u6a21\u578b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u672a\u6765\u6539\u8fdb\u548c\u5927\u89c4\u6a21\u5b9e\u65bd\u7684\u57fa\u7ebf\u3002||\n", "2409.02253": "|**2024-09-03**|[How to Determine the Preferred Image Distribution of a Black-Box Vision-Language Model?](http://arxiv.org/abs/2409.02253)|**[link](https://github.com/asgsaeid/cad_vqa)**|\u5927\u578b\u57fa\u7840\u6a21\u578b\u5f7b\u5e95\u6539\u53d8\u4e86\u8be5\u9886\u57df\uff0c\u4f46\u9488\u5bf9\u7279\u5b9a\u89c6\u89c9\u4efb\u52a1\u4f18\u5316\u591a\u6a21\u6001\u6a21\u578b\u4ecd\u7136\u5b58\u5728\u6311\u6218\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u4e0d\u540c\u8f93\u5165\u63d0\u793a\u4e0b\u8f93\u51fa\u7684\u4e00\u81f4\u6027\uff0c\u6765\u786e\u5b9a\u9ed1\u76d2\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u9996\u9009\u56fe\u50cf\u5206\u5e03\u3002\u6211\u4eec\u5c06\u5176\u5e94\u7528\u4e8e 3D \u5bf9\u8c61\u7684\u4e0d\u540c\u6e32\u67d3\u7c7b\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9700\u8981\u7cbe\u786e\u89e3\u91ca\u590d\u6742\u7ed3\u6784\u7684\u5404\u4e2a\u9886\u57df\u7684\u6709\u6548\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u8ba1\u7b97\u673a\u8f85\u52a9\u8bbe\u8ba1 (CAD) \u4f5c\u4e3a\u793a\u4f8b\u9886\u57df\u3002\u6211\u4eec\u4f7f\u7528\u4eba\u7c7b\u53cd\u9988\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u8fdb\u4e00\u6b65\u5b8c\u5584\u4e86 VLM \u8f93\u51fa\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u91ca\u8d28\u91cf\u3002\u4e3a\u4e86\u89e3\u51b3\u4e13\u4e1a\u9886\u57df\u7f3a\u4e4f\u57fa\u51c6\u7684\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 CAD-VQA\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30 VLM \u5728 CAD \u76f8\u5173\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u65b0\u6570\u636e\u96c6\u3002\u6211\u4eec\u5bf9 CAD-VQA \u4e0a\u6700\u5148\u8fdb\u7684 VLM \u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5efa\u7acb\u4e86\u57fa\u7ebf\u6027\u80fd\u6c34\u5e73\uff0c\u4e3a\u5728\u9700\u8981\u4e13\u5bb6\u7ea7\u89c6\u89c9\u89e3\u91ca\u7684\u5404\u4e2a\u9886\u57df\u63a8\u8fdb VLM \u5728\u590d\u6742\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\u3002\u6211\u4eec\u5728 \\url{https://github.com/asgsaeid/cad_vqa} \u4e0a\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u4ee3\u7801\u3002||\n", "2409.02101": "|**2024-09-03**|[Towards Real-World Adverse Weather Image Restoration: Enhancing Clearness and Semantics with Vision-Language Models](http://arxiv.org/abs/2409.02101)|**[link](https://github.com/jiaqixuac/WResVLM)**|\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6076\u52a3\u5929\u6c14\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u5e94\u7528\u4e8e\u73b0\u5b9e\u573a\u666f\u65f6\u7684\u5c40\u9650\u6027\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6765\u589e\u5f3a\u73b0\u5b9e\u73af\u5883\u4e2d\u4e0d\u540c\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u6062\u590d\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u56fe\u50cf\u6e05\u6670\u5ea6\u8bc4\u4f30\u548c\u8bed\u4e49\u63d0\u4f9b\uff0c\u4f5c\u4e3a\u8bad\u7ec3\u6062\u590d\u6a21\u578b\u7684\u76d1\u7763\u4fe1\u53f7\u3002\u5bf9\u4e8e\u6e05\u6670\u5ea6\u589e\u5f3a\uff0c\u6211\u4eec\u4f7f\u7528\u771f\u5b9e\u6570\u636e\uff0c\u91c7\u7528\u53cc\u91cd\u7b56\u7565\uff0c\u5373\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u7684\u4f2a\u6807\u7b7e\u548c\u5929\u6c14\u63d0\u793a\u5b66\u4e60\u3002\u5bf9\u4e8e\u8bed\u4e49\u589e\u5f3a\uff0c\u6211\u4eec\u901a\u8fc7\u8c03\u6574\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63cf\u8ff0\u4e2d\u7684\u5929\u6c14\u6761\u4ef6\uff0c\u540c\u65f6\u4fdd\u7559\u8bed\u4e49\uff0c\u6765\u6574\u5408\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\u6765\u63d0\u5347\u6062\u590d\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u7684\u6076\u52a3\u5929\u6c14\u56fe\u50cf\u6062\u590d\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7ed3\u679c\uff0c\u901a\u8fc7\u4e0e\u73b0\u6709\u6700\u4f73\u5de5\u4f5c\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u6bd4\u8f83\u8bc1\u660e\u4e86\u8fd9\u4e00\u70b9\u3002||\n", "2409.02084": "|**2024-09-03**|[GraspSplats: Efficient Manipulation with 3D Feature Splatting](http://arxiv.org/abs/2409.02084)|null|The ability for robots to perform efficient and zero-shot grasping of object parts is crucial for practical applications and is becoming prevalent with recent advances in Vision-Language Models (VLMs). To bridge the 2D-to-3D gap for representations to support such a capability, existing methods rely on neural fields (NeRFs) via differentiable rendering or point-based projection methods. However, we demonstrate that NeRFs are inappropriate for scene changes due to their implicitness and point-based methods are inaccurate for part localization without rendering-based optimization. To amend these issues, we propose GraspSplats. Using depth supervision and a novel reference feature computation method, GraspSplats generates high-quality scene representations in under 60 seconds. We further validate the advantages of Gaussian-based representation by showing that the explicit and optimized geometry in GraspSplats is sufficient to natively support (1) real-time grasp sampling and (2) dynamic and articulated object manipulation with point trackers. With extensive experiments on a Franka robot, we demonstrate that GraspSplats significantly outperforms existing methods under diverse task settings. In particular, GraspSplats outperforms NeRF-based methods like F3RM and LERF-TOGO, and 2D detection methods.||\n", "2409.03521": "|**2024-09-05**|[Have Large Vision-Language Models Mastered Art History?](http://arxiv.org/abs/2409.03521)|null|The emergence of large Vision-Language Models (VLMs) has recently established new baselines in image classification across multiple domains. However, the performance of VLMs in the specific task of artwork classification, particularly art style classification of paintings - a domain traditionally mastered by art historians - has not been explored yet. Artworks pose a unique challenge compared to natural images due to their inherently complex and diverse structures, characterized by variable compositions and styles. Art historians have long studied the unique aspects of artworks, with style prediction being a crucial component of their discipline. This paper investigates whether large VLMs, which integrate visual and textual data, can effectively predict the art historical attributes of paintings. We conduct an in-depth analysis of four VLMs, namely CLIP, LLaVA, OpenFlamingo, and GPT-4o, focusing on zero-shot classification of art style, author and time period using two public benchmarks of artworks. Additionally, we present ArTest, a well-curated test set of artworks, including pivotal paintings studied by art historians.||\n", "2409.04053": "|**2024-09-06**|[COLUMBUS: Evaluating COgnitive Lateral Understanding through Multiple-choice reBUSes](http://arxiv.org/abs/2409.04053)|null|\u867d\u7136\u89c6\u89c9\u95ee\u7b54 (VQA) \u57fa\u51c6\u6d4b\u8bd5\u63a8\u52a8\u4e86\u63a8\u7406\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4f46\u5b83\u4eec\u4e00\u76f4\u4e13\u6ce8\u4e8e\u5782\u76f4\u601d\u7ef4\u3002\u6709\u6548\u7684\u89e3\u51b3\u95ee\u9898\u8fd8\u9700\u8981\u6a2a\u5411\u601d\u7ef4\uff0c\u800c\u6a2a\u5411\u601d\u7ef4\u5728\u4eba\u5de5\u667a\u80fd\u9886\u57df\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u4e5f\u6ca1\u6709\u7528\u4e8e\u6d4b\u8bd5\u89c6\u89c9\u611f\u77e5\u7cfb\u7edf\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5c06\u89c6\u89c9\u6a2a\u5411\u601d\u7ef4\u5f62\u5f0f\u5316\u4e3a\u4e00\u4e2a\u591a\u9879\u9009\u62e9\u9898\u95ee\u7b54\u4efb\u52a1\uff0c\u5e76\u63cf\u8ff0\u4e86\u4e00\u4e2a\u7531\u5206\u7c7b\u6cd5\u9a71\u52a8\u7684\u4e09\u6b65\u6cd5\u6765\u5b9e\u4f8b\u5316\u4efb\u52a1\u793a\u4f8b\u3002\u7136\u540e\uff0c\u6211\u4eec\u5f00\u53d1\u4e86 COLUMBUS\uff0c\u8fd9\u662f\u4e00\u4e2a\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b83\u5e94\u7528\u4efb\u52a1\u7ba1\u9053\uff0c\u6839\u636e\u516c\u5f00\u53ef\u7528\u7684\u5316\u5408\u7269\u548c\u5e38\u7528\u77ed\u8bed\u96c6\u5408\uff0c\u521b\u5efa\u5e26\u6709\u6587\u672c\u548c\u56fe\u6807\u5b57\u8c1c\u7684 QA \u96c6\u3002COLUMBUS \u5305\u542b\u8d85\u8fc7 1,000 \u4e2a\u8c1c\u9898\uff0c\u6bcf\u4e2a\u8c1c\u9898\u6709\u56db\u4e2a\u5019\u9009\u7b54\u6848\u3002\u867d\u7136\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u53d6\u5f97\u4e86\u4e0d\u9519\u7684\u6027\u80fd\uff0c\u4f46\u6211\u4eec\u7684\u8bc4\u4f30\u8868\u660e\u4eba\u7c7b\u548c\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u5de8\u5927\u5dee\u8ddd\u3002VLM \u53d7\u76ca\u4e8e\u4eba\u5de5\u7b56\u5212\u7684\u63cf\u8ff0\uff0c\u4f46\u5728\u6b63\u786e\u7684\u62bd\u8c61\u7ea7\u522b\u4e0a\u96be\u4ee5\u81ea\u884c\u751f\u6210\u6b64\u7c7b\u8868\u793a\u3002||\n", "2409.03961": "|**2024-09-06**|[Generating Faithful and Salient Text from Multimodal Data](http://arxiv.org/abs/2409.03961)|**[link](https://github.com/TahsinaHashem/FaithD2T)**|\u867d\u7136\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u5728\u8bb8\u591a\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u5728\u751f\u6210\u6587\u672c\u65f6\u4ecd\u53ef\u80fd\u4f1a\u51fa\u73b0\u5e7b\u89c9\u3002\u5b83\u4eec\u5728\u4ece\u89c6\u89c9\u6570\u636e\u4e2d\u68c0\u6d4b\u663e\u8457\u7279\u5f81\u65b9\u9762\u7684\u6027\u80fd\u4e5f\u4e0d\u6e05\u695a\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6df7\u5408\u6a21\u6001\u6570\u636e\uff08\u5305\u62ec\u56fe\u50cf\u548c\u7ed3\u6784\u5316\u6570\u636e\uff08\u4ee5\u77e5\u8bc6\u56fe\u8c31\u6216\u8868\u683c\u8868\u793a\uff09\uff09\u751f\u6210\u5fe0\u5b9e\u4e14\u663e\u8457\u7684\u6587\u672c\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5c0f\u578b\u89c6\u89c9\u8bc4\u8bba\u5bb6\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u56fe\u50cf\u6a21\u6001\u4e2d\u8bc6\u522b\u5e7b\u89c9\u548c\u975e\u663e\u8457\u7279\u5f81\u3002\u8bc4\u8bba\u5bb6\u6a21\u578b\u8fd8\u4f1a\u751f\u6210\u663e\u8457\u56fe\u50cf\u7279\u5f81\u5217\u8868\u3002\u6b64\u4fe1\u606f\u7528\u4e8e\u540e\u671f\u7f16\u8f91\u6b65\u9aa4\uff0c\u4ee5\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\u3002\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u63d0\u9ad8\u4e86 LMM \u5728\u5fe0\u5b9e\u5ea6\u548c\u663e\u8457\u6027\u65b9\u9762\u7684\u751f\u6210\u8d28\u91cf\uff0c\u4f18\u4e8e\u6700\u8fd1\u65e8\u5728\u51cf\u5c11\u5e7b\u89c9\u7684\u6280\u672f\u3002||\n", "2409.03868": "|**2024-09-05**|[Few-shot Adaptation of Medical Vision-Language Models](http://arxiv.org/abs/2409.03868)|**[link](https://github.com/fereshteshakeri/few-shot-medvlms)**|Integrating image and text data through multi-modal learning has emerged as a new approach in medical imaging research, following its successful deployment in computer vision. While considerable efforts have been dedicated to establishing medical foundation models and their zero-shot transfer to downstream tasks, the popular few-shot setting remains relatively unexplored. Following on from the currently strong emergence of this setting in computer vision, we introduce the first structured benchmark for adapting medical vision-language models (VLMs) in a strict few-shot regime and investigate various adaptation strategies commonly used in the context of natural images. Furthermore, we evaluate a simple generalization of the linear-probe adaptation baseline, which seeks an optimal blending of the visual prototypes and text embeddings via learnable class-wise multipliers. Surprisingly, such a text-informed linear probe yields competitive performances in comparison to convoluted prompt-learning and adapter-based strategies, while running considerably faster and accommodating the black-box setting. Our extensive experiments span three different medical modalities and specialized foundation models, nine downstream tasks, and several state-of-the-art few-shot adaptation methods. We made our benchmark and code publicly available to trigger further developments in this emergent subject: \\url{https://github.com/FereshteShakeri/few-shot-MedVLMs}.||\n", "2409.06351": "|**2024-09-10**|[MAGDA: Multi-agent guideline-driven diagnostic assistance](http://arxiv.org/abs/2409.06351)|null|\u5728\u6025\u8bca\u79d1\u3001\u4e61\u6751\u533b\u9662\u6216\u6b20\u53d1\u8fbe\u5730\u533a\u7684\u8bca\u6240\uff0c\u4e34\u5e8a\u533b\u751f\u5f80\u5f80\u7f3a\u4e4f\u8bad\u7ec3\u6709\u7d20\u7684\u653e\u5c04\u79d1\u533b\u751f\u8fdb\u884c\u5feb\u901f\u56fe\u50cf\u5206\u6790\uff0c\u8fd9\u53ef\u80fd\u5bf9\u60a3\u8005\u7684\u533b\u7597\u4fdd\u5065\u4ea7\u751f\u4e0d\u5229\u5f71\u54cd\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6709\u53ef\u80fd\u901a\u8fc7\u63d0\u4f9b\u6709\u52a9\u4e8e\u4e34\u5e8a\u533b\u751f\u505a\u51fa\u51b3\u7b56\u7684\u89c1\u89e3\uff0c\u4ece\u800c\u51cf\u8f7b\u4ed6\u4eec\u7684\u4e00\u4e9b\u538b\u529b\u3002\u867d\u7136\u8fd9\u4e9b LLM \u5728\u533b\u5b66\u8003\u8bd5\u4e2d\u53d6\u5f97\u4e86\u5f88\u9ad8\u7684\u6d4b\u8bd5\u6210\u7ee9\uff0c\u5c55\u793a\u4e86\u5176\u4e30\u5bcc\u7684\u7406\u8bba\u533b\u5b66\u77e5\u8bc6\uff0c\u4f46\u5b83\u4eec\u5f80\u5f80\u4e0d\u9075\u5faa\u533b\u5b66\u6307\u5357\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u96f6\u6837\u672c\u6307\u5357\u9a71\u52a8\u51b3\u7b56\u652f\u6301\u65b9\u6cd5\u3002\u6211\u4eec\u6a21\u62df\u4e86\u4e00\u4e2a\u7531\u591a\u4e2a LLM \u4ee3\u7406\u7ec4\u6210\u7684\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u589e\u5f3a\u4e86\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u8fd9\u4e9b\u4ee3\u7406\u534f\u4f5c\u4ee5\u8fbe\u6210\u60a3\u8005\u8bca\u65ad\u3002\u5728\u5411\u4ee3\u7406\u63d0\u4f9b\u7b80\u5355\u7684\u8bca\u65ad\u6307\u5357\u540e\uff0c\u4ed6\u4eec\u5c06\u6839\u636e\u8fd9\u4e9b\u6307\u5357\u5408\u6210\u63d0\u793a\u5e76\u7b5b\u9009\u56fe\u50cf\u4ee5\u67e5\u627e\u7ed3\u679c\u3002\u6700\u540e\uff0c\u4ed6\u4eec\u4e3a\u81ea\u5df1\u7684\u8bca\u65ad\u63d0\u4f9b\u6613\u4e8e\u7406\u89e3\u7684\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u7136\u540e\u5bf9\u5176\u8fdb\u884c\u81ea\u6211\u5b8c\u5584\uff0c\u4ee5\u8003\u8651\u75be\u75c5\u4e4b\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\u3002\u7531\u4e8e\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u96f6\u6837\u672c\u7684\uff0c\u56e0\u6b64\u5b83\u9002\u7528\u4e8e\u7f55\u89c1\u75be\u75c5\u7684\u8bbe\u7f6e\uff0c\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0c\u8bad\u7ec3\u6570\u636e\u6709\u9650\uff0c\u4f46\u53ef\u4ee5\u4f7f\u7528\u4e13\u5bb6\u5236\u5b9a\u7684\u75be\u75c5\u63cf\u8ff0\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u80f8\u90e8 X \u5149\u6570\u636e\u96c6 CheXpert \u548c ChestX-ray 14 Longtail \u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u76f8\u5bf9\u4e8e\u73b0\u6709\u96f6\u6837\u672c\u65b9\u6cd5\u7684\u6027\u80fd\u6539\u8fdb\u4ee5\u53ca\u5bf9\u7f55\u89c1\u75be\u75c5\u7684\u6cdb\u5316\u80fd\u529b\u3002||\n", "2409.06210": "|**2024-09-10**|[INTRA: Interaction Relationship-aware Weakly Supervised Affordance Grounding](http://arxiv.org/abs/2409.06210)|null|\u53ef\u4f9b\u6027\u662f\u6307\u7269\u4f53\u56fa\u6709\u7684\u6f5c\u5728\u4ea4\u4e92\u65b9\u5f0f\u3002\u5bf9\u53ef\u4f9b\u6027\u7684\u611f\u77e5\u53ef\u4ee5\u8ba9\u667a\u80fd\u4f53\u9ad8\u6548\u5730\u5728\u65b0\u73af\u5883\u4e2d\u5bfc\u822a\u548c\u4ea4\u4e92\u3002\u5f31\u76d1\u7763\u53ef\u4f9b\u6027\u57fa\u7840\u53ef\u4ee5\u8ba9\u667a\u80fd\u4f53\u5728\u6ca1\u6709\u6602\u8d35\u7684\u50cf\u7d20\u7ea7\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u53ef\u4f9b\u6027\u7684\u6982\u5ff5\uff0c\u4f46\u9700\u8981\u4f7f\u7528\u4ee5\u73af\u5883\u4e3a\u4e2d\u5fc3\u7684\u56fe\u50cf\u3002\u5c3d\u7ba1\u5f31\u76d1\u7763\u53ef\u4f9b\u6027\u57fa\u7840\u7684\u6700\u65b0\u8fdb\u5c55\u5df2\u7ecf\u53d6\u5f97\u4e86\u53ef\u559c\u7684\u6210\u679c\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u4e00\u4e9b\u6311\u6218\uff0c\u4f8b\u5982\u9700\u8981\u914d\u5bf9\u7684\u4ee5\u73af\u5883\u4e3a\u4e2d\u5fc3\u548c\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u4e3a\u5355\u4e2a\u7269\u4f53\u57fa\u7840\u591a\u79cd\u53ef\u4f9b\u6027\u7684\u590d\u6742\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4ea4\u4e92\u5173\u7cfb\u611f\u77e5\u7684\u5f31\u76d1\u7763\u53ef\u4f9b\u6027\u57fa\u7840 (INTRA)\u3002\u4e0e\u73b0\u6709\u6280\u672f\u4e0d\u540c\uff0cINTRA \u5c06\u8fd9\u4e2a\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8868\u5f81\u5b66\u4e60\uff0c\u901a\u8fc7\u4ec5\u4f7f\u7528\u4ee5\u73af\u5883\u4e3a\u4e2d\u5fc3\u7684\u56fe\u50cf\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\u6765\u8bc6\u522b\u4ea4\u4e92\u7684\u72ec\u7279\u7279\u5f81\uff0c\u4ece\u800c\u6d88\u9664\u4e86\u5bf9\u914d\u5bf9\u6570\u636e\u96c6\u7684\u9700\u6c42\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u6765\u7075\u6d3b\u5730\u4f7f\u7528\u4efb\u4f55\u6587\u672c\u8fdb\u884c\u53ef\u4f9b\u6027\u57fa\u7840\uff0c\u8bbe\u8ba1\u4e86\u4ee5\u6587\u672c\u4e3a\u6761\u4ef6\u7684\u53ef\u4f9b\u6027\u6620\u5c04\u751f\u6210\uff0c\u4ee5\u53cd\u6620\u4ea4\u4e92\u5173\u7cfb\u4ee5\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u6211\u4eec\u7684\u6587\u672c\u540c\u4e49\u8bcd\u589e\u5f3a\u6765\u589e\u5f3a\u9c81\u68d2\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728 AGD20K\u3001IIT-AFF\u3001CAD \u548c UMD \u7b49\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002\u6b64\u5916\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5bf9\u5408\u6210\u56fe\u50cf/\u63d2\u56fe\u5177\u6709\u663e\u8457\u7684\u9886\u57df\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4e14\u80fd\u591f\u5bf9\u65b0\u7684\u4ea4\u4e92\u548c\u7269\u4f53\u8fdb\u884c\u53ef\u4f9b\u6027\u57fa\u7840\u3002||\n", "2409.06166": "|**2024-09-10**|[Revisiting Prompt Pretraining of Vision-Language Models](http://arxiv.org/abs/2409.06166)|null|\u63d0\u793a\u5b66\u4e60\u662f\u4e00\u79cd\u6709\u6548\u7684\u5b9a\u5236\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u4ee5\u9002\u5e94\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u5b83\u4ec5\u9700\u5fae\u8c03\u8f93\u5165\u63d0\u793a\u8bcd\u7b26\u7684\u5c11\u91cf\u53c2\u6570\u3002\u8fd1\u5e74\u6765\uff0c\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08\u4f8b\u5982 ImageNet-21K\uff09\u4e0a\u8fdb\u884c\u63d0\u793a\u9884\u8bad\u7ec3\u5df2\u6210\u4e3a\u901a\u7528\u89c6\u89c9\u8bc6\u522b\u63d0\u793a\u5b66\u4e60\u7684\u5173\u952e\u3002\u7136\u800c\uff0c\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u5e76\u89c2\u5bdf\u5230\uff0c\u5728\u63d0\u793a\u9884\u8bad\u7ec3\u671f\u95f4\uff0c\u9274\u4e8e\u56fe\u50cf\u6570\u91cf\u5e9e\u5927\uff0c\u6709\u9650\u7684\u53ef\u5b66\u4e60\u63d0\u793a\u53ef\u80fd\u4f1a\u9762\u4e34\u6b20\u62df\u5408\u7684\u98ce\u9669\uff0c\u540c\u65f6\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u91cd\u65b0\u5ba1\u89c6\u63d0\u793a\u9884\u8bad\u7ec3\u201d\uff08RPP\uff09\u7684\u901a\u7528\u6846\u67b6\uff0c\u65e8\u5728\u4ece\u63d0\u793a\u7ed3\u6784\u548c\u63d0\u793a\u76d1\u7763\u4e24\u4e2a\u65b9\u9762\u63d0\u9ad8\u62df\u5408\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5bf9\u4e8e\u63d0\u793a\u7ed3\u6784\uff0c\u6211\u4eec\u6253\u7834\u4e86\u67e5\u8be2\u3001\u952e\u548c\u503c\u5411\u91cf\u5747\u6765\u81ea\u5171\u4eab\u7684\u53ef\u5b66\u4e60\u63d0\u793a\u8bcd\u7b26\u7684\u5e38\u89c1\u505a\u6cd5\u7684\u9650\u5236\u3002\u76f8\u53cd\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u975e\u5171\u4eab\u7684\u72ec\u7acb\u67e5\u8be2\u3001\u952e\u548c\u503c\u53ef\u5b66\u4e60\u63d0\u793a\uff0c\u4ece\u800c\u901a\u8fc7\u589e\u52a0\u53c2\u6570\u591a\u6837\u6027\u6765\u589e\u5f3a\u6a21\u578b\u7684\u62df\u5408\u80fd\u529b\u3002\u5bf9\u4e8e\u63d0\u793a\u76d1\u7763\uff0c\u6211\u4eec\u8fd8\u5229\u7528\u4e86\u7531\u9884\u8bad\u7ec3\u7684\u5bf9\u6bd4\u8bed\u8a00\u56fe\u50cf\u9884\u8bad\u7ec3 (CLIP) \u6559\u5e08\u6a21\u578b\u63d0\u4f9b\u7684\u96f6\u6837\u672c\u6982\u7387\u9884\u6d4b\u5f97\u5230\u7684\u8f6f\u6807\u7b7e\u3002\u8fd9\u4e9b\u8f6f\u6807\u7b7e\u53ef\u4ee5\u66f4\u7ec6\u81f4\u3001\u66f4\u5168\u9762\u5730\u6d1e\u5bdf\u7c7b\u95f4\u5173\u7cfb\uff0c\u4ece\u800c\u8d4b\u4e88\u9884\u8bad\u7ec3\u8fc7\u7a0b\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002RPP \u4ea7\u751f\u66f4\u7a33\u5065\u7684\u63d0\u793a\u521d\u59cb\u5316\uff0c\u589e\u5f3a\u5176\u5728\u5404\u79cd\u89c6\u89c9\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u8fc1\u79fb\u80fd\u529b\u3002\u8de8\u591a\u4e2a\u57fa\u51c6\u7684\u5b9e\u9a8c\u4e00\u81f4\u8bc1\u5b9e\u4e86\u6211\u4eec\u9884\u8bad\u7ec3\u63d0\u793a\u7684\u6700\u65b0\u6027\u80fd\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5f88\u5feb\u53d1\u5e03\u3002||\n", "2409.06078": "|**2024-09-09**|[PEERNet: An End-to-End Profiling Tool for Real-Time Networked Robotic Systems](http://arxiv.org/abs/2409.06078)|**[link](https://github.com/utaustin-swarmlab/peernet)**|\u7f51\u7edc\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u3001\u65e0\u4eba\u673a\u7fa4\u548c\u8fdc\u7a0b\u624b\u672f\u7b49\u5e94\u7528\u4e2d\u9700\u8981\u5e73\u8861\u8ba1\u7b97\u3001\u529f\u8017\u548c\u5ef6\u8fdf\u7ea6\u675f\u3002\u8be5\u9886\u57df\u7684\u6838\u5fc3\u95ee\u9898\u662f\u4f55\u65f6\u5c06\u8ba1\u7b97\u91cf\u5927\u7684\u4efb\u52a1\u5378\u8f7d\u5230\u4e91\u7aef\uff08\u8fdc\u7a0b\u670d\u52a1\u5668\uff09\u4ee5\u6362\u53d6\u901a\u4fe1\u5ef6\u8fdf\u3002\u4efb\u52a1\u5378\u8f7d\u7b97\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5bf9\u7cfb\u7edf\u7279\u5b9a\u6027\u80fd\u6307\u6807\u7684\u7cbe\u786e\u4e86\u89e3\uff0c\u4f8b\u5982\u4f20\u611f\u5668\u6570\u636e\u901f\u7387\u3001\u7f51\u7edc\u5e26\u5bbd\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5ef6\u8fdf\u3002\u867d\u7136\u8fd9\u4e9b\u6307\u6807\u53ef\u4ee5\u5728\u7cfb\u7edf\u8bbe\u8ba1\u671f\u95f4\u8fdb\u884c\u5efa\u6a21\uff0c\u4f46\u8fde\u63a5\u8d28\u91cf\u3001\u670d\u52a1\u5668\u8d1f\u8f7d\u548c\u786c\u4ef6\u6761\u4ef6\u7684\u4e0d\u786e\u5b9a\u6027\u4f1a\u5bfc\u81f4\u5b9e\u65f6\u6027\u80fd\u53d8\u5316\uff0c\u4ece\u800c\u5f71\u54cd\u6574\u4f53\u6027\u80fd\u3002\u6211\u4eec\u63a8\u51fa\u4e86 PEERNet\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u4e91\u673a\u5668\u4eba\u7684\u7aef\u5230\u7aef\u5b9e\u65f6\u5206\u6790\u5de5\u5177\u3002PEERNet \u901a\u8fc7\u5bf9\u4f20\u611f\u5668\u3001\u7f51\u7edc\u3001\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\u548c\u8bbe\u5907\u7b49\u7cfb\u7edf\u7ec4\u4ef6\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u4f46\u81ea\u9002\u5e94\u7684\u5206\u6790\uff0c\u4ece\u800c\u80fd\u591f\u5728\u5f02\u6784\u786c\u4ef6\u4e0a\u8fdb\u884c\u6027\u80fd\u76d1\u63a7\u3002\u6211\u4eec\u901a\u8fc7\u7f51\u7edc\u673a\u5668\u4eba\u4efb\u52a1\u5c55\u793a\u4e86 PEERNet \u7684\u529f\u80fd\uff0c\u4f8b\u5982\u57fa\u4e8e\u56fe\u50cf\u7684 Franka Emika Panda \u673a\u68b0\u81c2\u8fdc\u7a0b\u64cd\u4f5c\u548c\u4f7f\u7528 Nvidia Jetson Orin \u67e5\u8be2\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002PEERNet \u63ed\u793a\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u975e\u76f4\u89c2\u7684\u7684\u884c\u4e3a\uff0c\u4f8b\u5982\u975e\u5bf9\u79f0\u7f51\u7edc\u4f20\u8f93\u548c\u53cc\u5cf0\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u5f3a\u8c03\u4e86\u7f51\u7edc\u673a\u5668\u4eba\u4e2d\u57fa\u51c6\u6d4b\u8bd5\u7684\u6709\u6548\u6027\u548c\u91cd\u8981\u6027\uff0c\u8bc1\u660e\u4e86 PEERNet \u7684\u9002\u5e94\u6027\u3002\u6211\u4eec\u7684\u4ee3\u7801\u662f\u5f00\u6e90\u7684\uff0c\u53ef\u5728 github.com/UTAustin-SwarmLab/PEERNet \u83b7\u53d6\u3002||\n", "2409.05493": "|**2024-09-09**|[DexDiff: Towards Extrinsic Dexterity Manipulation of Ungraspable Objects in Unrestricted Environments](http://arxiv.org/abs/2409.05493)|null|\u6293\u53d6\u53c8\u5927\u53c8\u5e73\u7684\u7269\u4f53\uff08\u4f8b\u5982\u4e66\u6216\u5e73\u5e95\u9505\uff09\u901a\u5e38\u88ab\u8ba4\u4e3a\u662f\u4e00\u9879\u65e0\u6cd5\u5b8c\u6210\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u6293\u53d6\u59ff\u52bf\u65e0\u6cd5\u4f01\u53ca\uff0c\u8fd9\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u4ee5\u524d\u7684\u5de5\u4f5c\u5229\u7528\u5899\u58c1\u6216\u684c\u5b50\u8fb9\u7f18\u7b49\u5916\u90e8\u7075\u6d3b\u6027\u6765\u6293\u53d6\u6b64\u7c7b\u7269\u4f53\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4ec5\u9650\u4e8e\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u7b56\u7565\uff0c\u5e76\u4e14\u7f3a\u4e4f\u5bfb\u627e\u9884\u6293\u53d6\u6761\u4ef6\u7684\u4efb\u52a1\u89c4\u5212\u3002\u8fd9\u4f7f\u5f97\u9002\u5e94\u5404\u79cd\u73af\u5883\u548c\u5916\u90e8\u7075\u6d3b\u6027\u7ea6\u675f\u53d8\u5f97\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DexDiff\uff0c\u4e00\u79cd\u7528\u4e8e\u5177\u6709\u5916\u90e8\u7075\u6d3b\u6027\u7684\u957f\u89c6\u91ce\u89c4\u5212\u7684\u7a33\u5065\u673a\u5668\u4eba\u64cd\u4f5c\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u6765\u611f\u77e5\u73af\u5883\u72b6\u6001\u5e76\u751f\u6210\u9ad8\u7ea7\u4efb\u52a1\u8ba1\u5212\uff0c\u7136\u540e\u4f7f\u7528\u76ee\u6807\u6761\u4ef6\u52a8\u4f5c\u6269\u6563 (GCAD) \u6a21\u578b\u6765\u9884\u6d4b\u4f4e\u7ea7\u52a8\u4f5c\u5e8f\u5217\u3002\u8be5\u6a21\u578b\u4ece\u79bb\u7ebf\u6570\u636e\u4e2d\u5b66\u4e60\u4f4e\u7ea7\u7b56\u7565\uff0c\u5e76\u5c06\u9ad8\u7ea7\u89c4\u5212\u5f15\u5bfc\u7684\u7d2f\u79ef\u5956\u52b1\u4f5c\u4e3a\u76ee\u6807\u6761\u4ef6\uff0c\u4ece\u800c\u53ef\u4ee5\u6539\u8fdb\u5bf9\u673a\u5668\u4eba\u52a8\u4f5c\u7684\u9884\u6d4b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u53ef\u4ee5\u6709\u6548\u5730\u6267\u884c\u65e0\u6cd5\u5b8c\u6210\u7684\u4efb\u52a1\uff0c\u800c\u4e14\u53ef\u4ee5\u6cdb\u5316\u5230\u4ee5\u524d\u4ece\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u3002\u5b83\u5728\u6a21\u62df\u4e2d\u7684\u6210\u529f\u7387\u6bd4\u57fa\u7ebf\u9ad8 47%\uff0c\u5e76\u6709\u52a9\u4e8e\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u9ad8\u6548\u90e8\u7f72\u548c\u64cd\u4f5c\u3002||\n", "2409.05076": "|**2024-09-08**|[PIP: Detecting Adversarial Examples in Large Vision-Language Models via Attention Patterns of Irrelevant Probe Questions](http://arxiv.org/abs/2409.05076)|**[link](https://github.com/btzyd/pip)**|\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u5df2\u7ecf\u5c55\u793a\u51fa\u5f3a\u5927\u7684\u591a\u6a21\u6001\u80fd\u529b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4e5f\u9762\u4e34\u7740\u4e25\u91cd\u7684\u5b89\u5168\u95ee\u9898\uff0c\u56e0\u4e3a\u653b\u51fb\u8005\u53ef\u4ee5\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5bf9\u6297\u6837\u672c\u5728 LVLM \u4e2d\u5f15\u53d1\u9c81\u68d2\u6027\u95ee\u9898\u3002\u56e0\u6b64\uff0cLVLM \u8feb\u5207\u9700\u8981\u9488\u5bf9\u5bf9\u6297\u6837\u672c\u7684\u68c0\u6d4b\u5de5\u5177\uff0c\u4ee5\u9632\u6b62\u51fa\u73b0\u9519\u8bef\u54cd\u5e94\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u53d1\u73b0\uff0c\u5f53\u4f7f\u7528\u63a2\u6d4b\u95ee\u9898\u65f6\uff0cLVLM \u5bf9\u5e72\u51c0\u56fe\u50cf\u8868\u73b0\u51fa\u89c4\u5f8b\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a PIP \u7684\u975e\u5e38\u89c4\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u4e00\u4e2a\u968f\u673a\u9009\u62e9\u7684\u65e0\u5173\u63a2\u6d4b\u95ee\u9898\uff08\u4f8b\u5982\uff0c\u201c\u6709\u949f\u8868\u5417\uff1f\u201d\uff09\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u6765\u533a\u5206\u5bf9\u6297\u6837\u672c\u548c\u5e72\u51c0\u6837\u672c\u3002\u65e0\u8bba\u5f85\u6d4b\u56fe\u50cf\u53ca\u5176\u5bf9\u5e94\u7684\u95ee\u9898\u662f\u4ec0\u4e48\uff0cPIP \u53ea\u9700\u8981\u5bf9\u5f85\u6d4b\u56fe\u50cf\u548c\u63a2\u6d4b\u95ee\u9898\u8fdb\u884c\u4e00\u6b21\u989d\u5916\u7684\u63a8\u7406\uff0c\u5373\u53ef\u6210\u529f\u68c0\u6d4b\u5bf9\u6297\u6837\u672c\u3002\u5373\u4f7f\u5728\u9ed1\u76d2\u653b\u51fb\u548c\u5f00\u653e\u6570\u636e\u96c6\u573a\u666f\u4e0b\uff0c\u6211\u4eec\u7684 PIP \u4e0e\u7b80\u5355\u7684 SVM \u76f8\u7ed3\u5408\uff0c\u4ecd\u7136\u53ef\u4ee5\u5b9e\u73b0\u8d85\u8fc7 98% \u7684\u53ec\u56de\u7387\u548c\u8d85\u8fc7 90% \u7684\u7cbe\u786e\u7387\u3002\u6211\u4eec\u7684 PIP \u662f\u9996\u6b21\u5c1d\u8bd5\u901a\u8fc7\u7b80\u5355\u7684\u65e0\u5173\u63a2\u6d4b\u95ee\u9898\u6765\u68c0\u6d4b\u9488\u5bf9 LVLM \u7684\u5bf9\u6297\u653b\u51fb\uff0c\u4e3a\u66f4\u6df1\u5165\u5730\u7406\u89e3\u548c\u53cd\u601d LVLM \u63d0\u4f9b\u4e86\u601d\u8def\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/btzyd/pip \u83b7\u53d6\u3002||\n", "2409.05916": "|**2024-09-07**|[Unlocking Potential Binders: Multimodal Pretraining DEL-Fusion for Denoising DNA-Encoded Libraries](http://arxiv.org/abs/2409.05916)|null|\u5728\u836f\u7269\u53d1\u73b0\u9886\u57df\uff0cDNA \u7f16\u7801\u5316\u5408\u7269\u5e93 (DEL) \u7b5b\u9009\u6280\u672f\u5df2\u6210\u4e3a\u8bc6\u522b\u9ad8\u4eb2\u548c\u529b\u5316\u5408\u7269\u7684\u6709\u6548\u65b9\u6cd5\u3002\u7136\u800c\uff0cDEL \u7b5b\u9009\u9762\u4e34\u7740\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff1a\u590d\u6742\u751f\u7269\u7cfb\u7edf\u4e2d\u975e\u7279\u5f02\u6027\u76f8\u4e92\u4f5c\u7528\u4ea7\u751f\u7684\u566a\u58f0\u3002\u5728 DEL \u5e93\u4e0a\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u5df2\u88ab\u7528\u4e8e\u63d0\u53d6\u5316\u5408\u7269\u7279\u5f81\uff0c\u65e8\u5728\u5bf9\u6570\u636e\u8fdb\u884c\u53bb\u566a\u5e76\u53d1\u73b0\u6f5c\u5728\u7684\u6cbb\u7597\u9776\u70b9\u7ed3\u5408\u5242\u3002\u7136\u800c\uff0cDEL \u7684\u56fa\u6709\u7ed3\u6784\u53d7\u9650\u4e8e\u7ed3\u6784\u5355\u5143\u7684\u6709\u9650\u591a\u6837\u6027\uff0c\u8fd9\u5f71\u54cd\u4e86\u5316\u5408\u7269\u7f16\u7801\u5668\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u5728\u5355\u4e00\u7ea7\u522b\u6355\u83b7\u5316\u5408\u7269\u7279\u5f81\uff0c\u8fdb\u4e00\u6b65\u9650\u5236\u4e86\u53bb\u566a\u7b56\u7565\u7684\u6709\u6548\u6027\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u9884\u8bad\u7ec3 DEL-Fusion \u6a21\u578b (MPDF)\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u9884\u8bad\u7ec3\u589e\u5f3a\u7f16\u7801\u5668\u80fd\u529b\uff0c\u5e76\u5728\u4e0d\u540c\u5c3a\u5ea6\u4e0a\u6574\u5408\u5316\u5408\u7269\u7279\u5f81\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u5728\u4e0d\u540c\u5316\u5408\u7269\u8868\u793a\u53ca\u5176\u6587\u672c\u63cf\u8ff0\u4e4b\u95f4\u5e94\u7528\u5bf9\u6bd4\u76ee\u6807\u7684\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u589e\u5f3a\u4e86\u5316\u5408\u7269\u7f16\u7801\u5668\u83b7\u53d6\u901a\u7528\u7279\u5f81\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 DEL-fusion \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u878d\u5408\u4e86\u539f\u5b50\u3001\u4e9a\u5206\u5b50\u548c\u5206\u5b50\u6c34\u5e73\u7684\u5316\u5408\u7269\u4fe1\u606f\uff0c\u8fd9\u4e9b\u4fe1\u606f\u7531\u5404\u79cd\u5316\u5408\u7269\u7f16\u7801\u5668\u6355\u83b7\u3002\u8fd9\u4e9b\u521b\u65b0\u7684\u534f\u540c\u4f5c\u7528\u4f7f MPDF \u5177\u5907\u4e30\u5bcc\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u4ece\u800c\u5b9e\u73b0\u5168\u9762\u7684\u4e0b\u6e38\u53bb\u566a\u3002\u5728\u4e09\u4e2a DEL \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u8bc4\u4f30\u8868\u660e\uff0cMPDF \u5728\u9a8c\u8bc1\u4efb\u52a1\u7684\u6570\u636e\u5904\u7406\u548c\u5206\u6790\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cMPDF \u4e3a\u8bc6\u522b\u9ad8\u4eb2\u548c\u529b\u5206\u5b50\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u4e3a\u6539\u8fdb DEL \u5728\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002||\n", "2409.04828": "|**2024-09-07**|[POINTS: Improving Your Vision-language Model with Affordable Strategies](http://arxiv.org/abs/2409.04828)|null|\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u5728\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\u548c\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0c\u4ecd\u7136\u5b58\u5728\u51e0\u4e2a\u5173\u952e\u95ee\u9898\uff1a1\uff09\u4e13\u6709\u6a21\u578b\u7684\u67b6\u6784\u5f80\u5f80\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u800c\u5f00\u6e90\u6a21\u578b\u9700\u8981\u5bf9\u5176\u8bad\u7ec3\u7b56\u7565\u8fdb\u884c\u66f4\u8be6\u7ec6\u7684\u6d88\u878d\u7814\u7a76\u30022\uff09\u5f00\u6e90\u5de5\u4f5c\u4e2d\u7684\u9884\u8bad\u7ec3\u6570\u636e\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u6570\u636e\u96c6\u662f\u6839\u636e\u7ecf\u9a8c\u6dfb\u52a0\u7684\uff0c\u8fd9\u4f7f\u5f97\u8fc7\u7a0b\u53d8\u5f97\u7e41\u7410\u30023\uff09\u5fae\u8c03\u901a\u5e38\u4fa7\u91cd\u4e8e\u6dfb\u52a0\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u6536\u76ca\u9012\u51cf\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4ee5\u4e0b\u8d21\u732e\uff1a1\uff09\u6211\u4eec\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5f15\u5165\u4e86\u6709\u6548\u7684\u6539\u8fdb\uff0c\u5e76\u5bf9\u6bcf\u79cd\u6280\u672f\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6d88\u878d\u548c\u9a8c\u8bc1\u30022\uff09\u53d7\u8fd1\u671f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5de5\u4f5c\u7684\u542f\u53d1\uff0c\u6211\u4eec\u4f7f\u7528\u56f0\u60d1\u5ea6\u5bf9\u9884\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u8fc7\u6ee4\uff0c\u9009\u62e9\u56f0\u60d1\u5ea6\u6700\u4f4e\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u6211\u4eec\u80fd\u591f\u5728\u7cbe\u9009\u7684 1M \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u30023\uff09\u5728\u89c6\u89c9\u6307\u4ee4\u5fae\u8c03\u671f\u95f4\uff0c\u5f53\u6dfb\u52a0\u66f4\u591a\u6570\u636e\u96c6\u7684\u6536\u76ca\u5fae\u4e4e\u5176\u5fae\u65f6\uff0c\u6211\u4eec\u5bf9\u4e0d\u540c\u6570\u636e\u96c6\u4f7f\u7528\u4e86\u6a21\u578b\u878d\u5408\u3002\u8fd9\u4e9b\u521b\u65b0\u4ea7\u751f\u4e86\u4e00\u4e2a 9B \u53c2\u6570\u7684\u6a21\u578b\uff0c\u5176\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u6a21\u578b\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002\u6211\u4eec\u7684\u7b56\u7565\u9ad8\u6548\u4e14\u8f7b\u91cf\u7ea7\uff0c\u56e0\u6b64\u793e\u533a\u5f88\u5bb9\u6613\u91c7\u7528\u3002||\n", "2409.04796": "|**2024-09-07**|[Enhancing Outlier Knowledge for Few-Shot Out-of-Distribution Detection with Extensible Local Prompts](http://arxiv.org/abs/2409.04796)|null|\u5206\u5e03\u5916 (OOD) \u68c0\u6d4b\u65e8\u5728\u533a\u5206\u5df2\u77e5\u7c7b\u522b\u4e4b\u5916\u7684\u5f02\u5e38\u503c\uff0c\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5df2\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u51fa\u73b0\u6fc0\u53d1\u4e86\u4eba\u4eec\u5bf9\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u5fae\u8c03\u6765\u589e\u5f3a VLM \u7684 OOD \u68c0\u6d4b\u7684\u5174\u8da3\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4fa7\u91cd\u4e8e\u4f18\u5316\u5168\u5c40\u63d0\u793a\uff0c\u800c\u5ffd\u7565\u4e86\u5bf9\u5f02\u5e38\u503c\u7684\u5c40\u90e8\u4fe1\u606f\u7684\u7cbe\u7ec6\u5229\u7528\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u51bb\u7ed3\u5168\u5c40\u63d0\u793a\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ece\u7c97\u5230\u7cbe\u7684\u5fae\u8c03\u8303\u5f0f\uff0c\u4ee5\u5f3a\u8c03\u4f7f\u7528\u5c40\u90e8\u63d0\u793a\u8fdb\u884c\u533a\u57df\u589e\u5f3a\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u7ec4\u6210\u90e8\u5206\uff1a\u5168\u5c40\u63d0\u793a\u5f15\u5bfc\u7684\u8d1f\u589e\u5f3a\u548c\u5c40\u90e8\u63d0\u793a\u589e\u5f3a\u7684\u533a\u57df\u6b63\u5219\u5316\u3002\u524d\u8005\u5229\u7528\u51bb\u7ed3\u7684\u3001\u7c97\u7565\u7684\u5168\u5c40\u63d0\u793a\u4f5c\u4e3a\u6307\u5bfc\u7ebf\u7d22\u6765\u5408\u5e76\u8d1f\u589e\u5f3a\uff0c\u4ece\u800c\u5229\u7528\u5c40\u90e8\u5f02\u5e38\u503c\u77e5\u8bc6\u3002\u540e\u8005\u91c7\u7528\u53ef\u8bad\u7ec3\u7684\u5c40\u90e8\u63d0\u793a\u548c\u533a\u57df\u6b63\u5219\u5316\u6765\u6709\u6548\u5730\u6355\u83b7\u5c40\u90e8\u4fe1\u606f\uff0c\u4ece\u800c\u5e2e\u52a9\u8bc6\u522b\u5f02\u5e38\u503c\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u533a\u57df\u76f8\u5173\u6307\u6807\uff0c\u4ee5\u589e\u5f3a OOD \u68c0\u6d4b\u7684\u4e30\u5bcc\u6027\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u6211\u4eec\u7684\u65b9\u6cd5\u4ec5\u63a2\u7d22\u589e\u5f3a\u5c40\u90e8\u63d0\u793a\uff0c\u56e0\u6b64\u53ef\u4ee5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4e0e\u8bad\u7ec3\u597d\u7684\u5168\u5c40\u63d0\u793a\u65e0\u7f1d\u96c6\u6210\uff0c\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002\u7efc\u5408\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728 ImageNet-1k \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684 4 \u6b21\u6837\u672c\u5fae\u8c03\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5c06\u5e73\u5747 FPR95 \u964d\u4f4e\u4e86 5.17%\uff0c\u751a\u81f3\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u7684 16 \u6b21\u6837\u672c\u5fae\u8c03\u7ed3\u679c\u3002||\n", "2409.08202": "|**2024-09-12**|[What Makes a Maze Look Like a Maze?](http://arxiv.org/abs/2409.08202)|null|\u4eba\u7c7b\u89c6\u89c9\u7406\u89e3\u7684\u4e00\u4e2a\u72ec\u7279\u4e4b\u5904\u5728\u4e8e\u80fd\u591f\u7075\u6d3b\u5730\u89e3\u91ca\u62bd\u8c61\u6982\u5ff5\uff1a\u83b7\u53d6\u89e3\u91ca\u5176\u8c61\u5f81\u610f\u4e49\u7684\u63d0\u5347\u89c4\u5219\uff0c\u5c06\u5b83\u4eec\u5e94\u7528\u4e8e\u719f\u6089\u548c\u4e0d\u719f\u6089\u7684\u8bed\u5883\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u9884\u6d4b\u6216\u63a8\u7406\u3002\u867d\u7136\u73b0\u6210\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u64c5\u957f\u5bf9\u56fe\u50cf\u8fdb\u884c\u5b57\u9762\u89e3\u91ca\uff08\u4f8b\u5982\uff0c\u8bc6\u522b\u6811\u679d\u7b49\u7269\u4f53\u7c7b\u522b\uff09\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u96be\u4ee5\u7406\u89e3\u6b64\u7c7b\u89c6\u89c9\u62bd\u8c61\u6982\u5ff5\uff08\u4f8b\u5982\uff0c\u6811\u679d\u7684\u6392\u5217\u65b9\u5f0f\u5982\u4f55\u5f62\u6210\u8ff7\u5bab\u7684\u5899\u58c1\uff09\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6df1\u5ea6\u6a21\u5f0f\u57fa\u7840\uff08DSG\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u5229\u7528\u89c6\u89c9\u62bd\u8c61\u7684\u663e\u5f0f\u7ed3\u6784\u5316\u8868\u793a\u8fdb\u884c\u57fa\u7840\u5316\u548c\u63a8\u7406\u7684\u6846\u67b6\u3002DSG \u7684\u6838\u5fc3\u662f\u6a21\u5f0f\u2014\u2014\u62bd\u8c61\u6982\u5ff5\u7684\u4f9d\u8d56\u56fe\u63cf\u8ff0\uff0c\u5c06\u5b83\u4eec\u5206\u89e3\u6210\u66f4\u539f\u59cb\u7ea7\u522b\u7684\u7b26\u53f7\u3002DSG \u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u63d0\u53d6\u6a21\u5f0f\uff0c\u7136\u540e\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c06\u6a21\u5f0f\u7684\u5177\u4f53\u7ec4\u4ef6\u5230\u62bd\u8c61\u7ec4\u4ef6\u5206\u5c42\u5730\u57fa\u7840\u5316\u5230\u56fe\u50cf\u4e0a\u3002\u57fa\u7840\u5316\u7684\u6a21\u5f0f\u7528\u4e8e\u589e\u5f3a\u89c6\u89c9\u62bd\u8c61\u7406\u89e3\u3002\u6211\u4eec\u5728\u65b0\u7684\u89c6\u89c9\u62bd\u8c61\u6570\u636e\u96c6\u4e0a\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86 DSG \u548c\u4e0d\u540c\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u5404\u79cd\u73b0\u5b9e\u4e16\u754c\u4e2d\u62bd\u8c61\u6982\u5ff5\u7684\u56fe\u50cf\u4ee5\u53ca\u7531\u4eba\u7c7b\u6807\u8bb0\u7684\u76f8\u5e94\u95ee\u7b54\u5bf9\u3002\u6211\u4eec\u8868\u660e\uff0cDSG \u663e\u7740\u63d0\u9ad8\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u4e14\u662f\u671d\u7740\u4eba\u7c7b\u4e00\u81f4\u7684\u89c6\u89c9\u62bd\u8c61\u7406\u89e3\u8fc8\u51fa\u7684\u4e00\u6b65\u3002||\n", "2409.07825": "|**2024-09-13**|[A Comprehensive Survey on Deep Multimodal Learning with Missing Modality](http://arxiv.org/abs/2409.07825)|null|\u5728\u591a\u6a21\u6001\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u7531\u4e8e\u4f20\u611f\u5668\u9650\u5236\u3001\u6210\u672c\u9650\u5236\u3001\u9690\u79c1\u95ee\u9898\u3001\u6570\u636e\u4e22\u5931\u4ee5\u53ca\u65f6\u95f4\u548c\u7a7a\u95f4\u56e0\u7d20\uff0c\u6570\u636e\u6837\u672c\u53ef\u80fd\u4f1a\u7f3a\u5c11\u67d0\u4e9b\u6a21\u6001\uff0c\u4ece\u800c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002\u672c\u7efc\u8ff0\u6982\u8ff0\u4e86\u7f3a\u5931\u6a21\u6001\u7684\u591a\u6a21\u6001\u5b66\u4e60 (MLMM) \u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u3002\u5b83\u662f\u7b2c\u4e00\u4e2a\u6db5\u76d6\u5386\u53f2\u80cc\u666f\u548c MLMM \u4e0e\u6807\u51c6\u591a\u6a21\u6001\u5b66\u4e60\u8bbe\u7f6e\u4e4b\u95f4\u533a\u522b\u7684\u7efc\u5408\u6027\u7efc\u8ff0\uff0c\u7136\u540e\u8be6\u7ec6\u5206\u6790\u4e86\u5f53\u524d\u7684 MLMM \u65b9\u6cd5\u3001\u5e94\u7528\u548c\u6570\u636e\u96c6\uff0c\u6700\u540e\u8ba8\u8bba\u4e86\u8be5\u9886\u57df\u7684\u6311\u6218\u548c\u6f5c\u5728\u7684\u672a\u6765\u65b9\u5411\u3002||\n", "2409.07748": "|**2024-09-12**|[Top-down Activity Representation Learning for Video Question Answering](http://arxiv.org/abs/2409.07748)|null|\u4ece\u539f\u5b50\u52a8\u4f5c\uff08\u4f8b\u5982\uff0c\u62ff\u8d77\u4e00\u4e2a\u793c\u7269\uff0c\u79fb\u52a8\u5230\u6c99\u53d1\uff0c\u6253\u5f00\u793c\u7269\uff09\u5230\u4e0a\u4e0b\u6587\u4e8b\u4ef6\uff08\u4f8b\u5982\uff0c\u5e86\u795d\u5723\u8bde\u8282\uff09\u6355\u6349\u590d\u6742\u7684\u5206\u5c42\u4eba\u7c7b\u6d3b\u52a8\u5bf9\u4e8e\u5b9e\u73b0\u9ad8\u6027\u80fd\u89c6\u9891\u95ee\u7b54 (VideoQA) \u81f3\u5173\u91cd\u8981\u3002 \u6700\u8fd1\u7684\u5de5\u4f5c\u5df2\u7ecf\u6269\u5c55\u4e86\u591a\u6a21\u6001\u6a21\u578b\uff08\u4f8b\u5982\uff0cCLIP\uff0cLLaVA\uff09\u6765\u5904\u7406\u8fde\u7eed\u89c6\u9891\u5e8f\u5217\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002 \u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u65e0\u6cd5\u6355\u6349\u53ef\u4ee5\u5206\u89e3\u4e3a\u591a\u4e2a\u539f\u5b50\u52a8\u4f5c\u7684\u4e0a\u4e0b\u6587\u4e8b\u4ef6\uff0c\u8fd9\u4e9b\u52a8\u4f5c\u975e\u8fde\u7eed\u5730\u5206\u5e03\u5728\u76f8\u5bf9\u957f\u671f\u7684\u5e8f\u5217\u4e2d\u3002 \u5728\u672c\u6587\u4e2d\uff0c\u4e3a\u4e86\u5229\u7528 CLIP \u6a21\u578b\u7684\u7a7a\u95f4\u89c6\u89c9\u4e0a\u4e0b\u6587\u8868\u793a\u80fd\u529b\u6765\u83b7\u5f97\u89c6\u9891\u4e2d\u4e0a\u4e0b\u6587\u4e8b\u4ef6\u65b9\u9762\u7684\u975e\u8fde\u7eed\u89c6\u89c9\u8868\u793a\uff0c\u6211\u4eec\u5c06\u957f\u671f\u89c6\u9891\u5e8f\u5217\u8f6c\u6362\u4e3a\u7a7a\u95f4\u56fe\u50cf\u57df\uff0c\u5e76\u9488\u5bf9 VideoQA \u4efb\u52a1\u5fae\u8c03\u591a\u6a21\u6001\u6a21\u578b LLaVA\u3002 \u6211\u4eec\u7684\u65b9\u6cd5\u5728 STAR \u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728 NExTQA \u4efb\u52a1\u4e0a\uff0c\u83b7\u5f97\u4e86 78.4% \u7684\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5f97\u5206 2.8 \u4e2a\u767e\u5206\u70b9\u3002||\n", "2409.07703": "|**2024-09-12**|[DSBench: How Far Are Data Science Agents to Becoming Data Science Experts?](http://arxiv.org/abs/2409.07703)|**[link](https://github.com/liqiangjing/dsbench)**|\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5df2\u7ecf\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u8bed\u8a00/\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u5f15\u53d1\u4e86\u6784\u5efa\u9488\u5bf9\u7279\u5b9a\u5e94\u7528\uff08\u5982\u8d2d\u7269\u52a9\u624b\u6216AI\u8f6f\u4ef6\u5de5\u7a0b\u5e08\uff09\u7684\u4ee3\u7406\u7684\u6700\u65b0\u8d8b\u52bf\u3002\u6700\u8fd1\uff0c\u8bb8\u591a\u6570\u636e\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u88ab\u63d0\u51fa\uff0c\u4ee5\u7814\u7a76\u5176\u5728\u6570\u636e\u79d1\u5b66\u9886\u57df\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u6570\u636e\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u79d1\u5b66\u5e94\u7528\u76f8\u6bd4\u4ecd\u7136\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u4e3a\u5b83\u4eec\u7684\u8bbe\u7f6e\u8fc7\u4e8e\u7b80\u5316\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86 DSBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u5177\u6709\u73b0\u5b9e\u4efb\u52a1\u7684\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u5305\u62ec 466 \u4e2a\u6570\u636e\u5206\u6790\u4efb\u52a1\u548c 74 \u4e2a\u6570\u636e\u5efa\u6a21\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u6765\u81ea Eloquence \u548c Kaggle \u7ade\u8d5b\u3002DSBench \u901a\u8fc7\u5305\u542b\u957f\u4e0a\u4e0b\u6587\u3001\u591a\u6a21\u6001\u4efb\u52a1\u80cc\u666f\u3001\u5bf9\u5927\u578b\u6570\u636e\u6587\u4ef6\u548c\u591a\u8868\u7ed3\u6784\u8fdb\u884c\u63a8\u7406\u4ee5\u53ca\u6267\u884c\u7aef\u5230\u7aef\u6570\u636e\u5efa\u6a21\u4efb\u52a1\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u771f\u5b9e\u7684\u8bbe\u7f6e\u3002\u6211\u4eec\u5bf9\u6700\u5148\u8fdb\u7684 LLM\u3001LVLM \u548c\u4ee3\u7406\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5b83\u4eec\u96be\u4ee5\u5b8c\u6210\u5927\u591a\u6570\u4efb\u52a1\uff0c\u6700\u597d\u7684\u4ee3\u7406\u4ec5\u80fd\u89e3\u51b3 34.12% \u7684\u6570\u636e\u5206\u6790\u4efb\u52a1\uff0c\u5e76\u5b9e\u73b0\u4e86 34.74% \u7684\u76f8\u5bf9\u6027\u80fd\u5dee\u8ddd (RPG)\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u8fdb\u4e00\u6b65\u53d1\u5c55\u66f4\u5b9e\u7528\u3001\u66f4\u667a\u80fd\u3001\u66f4\u81ea\u4e3b\u7684\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u7684\u5fc5\u8981\u6027\u3002||\n", "2409.07683": "|**2024-09-12**|[Open-Vocabulary Remote Sensing Image Semantic Segmentation](http://arxiv.org/abs/2409.07683)|**[link](https://github.com/caoql98/ovrs)**|\u5f00\u653e\u8bcd\u6c47\u56fe\u50cf\u8bed\u4e49\u5206\u5272 (OVS) \u65e8\u5728\u5c06\u56fe\u50cf\u5206\u5272\u6210\u8de8\u5f00\u653e\u7c7b\u522b\u96c6\u7684\u8bed\u4e49\u533a\u57df\u3002\u73b0\u6709\u7684 OVS \u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u57fa\u7840\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5229\u7528\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u6765\u5904\u7406 OVS \u4efb\u52a1\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u81ea\u7136\u56fe\u50cf\u91cf\u8eab\u5b9a\u5236\uff0c\u96be\u4ee5\u5e94\u5bf9\u9065\u611f\u56fe\u50cf\u7684\u72ec\u7279\u7279\u5f81\uff0c\u4f8b\u5982\u5feb\u901f\u53d8\u5316\u7684\u65b9\u5411\u548c\u663e\u8457\u7684\u5c3a\u5ea6\u53d8\u5316\u3002\u8fd9\u4e9b\u6311\u6218\u4f7f\u5730\u7403\u89c6\u89c9\u4e2d\u7684 OVS \u4efb\u52a1\u53d8\u5f97\u590d\u6742\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u96be\u9898\uff0c\u6211\u4eec\u501f\u9274\u4e86\u72ec\u7279\u7684\u9065\u611f\u7279\u5f81\uff0c\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u4e13\u95e8\u4e3a\u9065\u611f\u56fe\u50cf\u8bbe\u8ba1\u7684 OVS \u6846\u67b6\u3002\u7279\u522b\u662f\uff0c\u4e3a\u4e86\u89e3\u51b3\u4e0d\u540c\u7684\u65b9\u5411\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65cb\u8f6c\u805a\u5408\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u751f\u6210\u65b9\u5411\u81ea\u9002\u5e94\u76f8\u4f3c\u5ea6\u56fe\u4f5c\u4e3a\u521d\u59cb\u8bed\u4e49\u56fe\u3002\u968f\u540e\uff0c\u8fd9\u4e9b\u56fe\u4f1a\u5728\u7a7a\u95f4\u548c\u7c7b\u522b\u7ea7\u522b\u8fdb\u884c\u7ec6\u5316\uff0c\u4ee5\u751f\u6210\u66f4\u51c6\u786e\u7684\u8bed\u4e49\u56fe\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u7ba1\u7406\u663e\u8457\u7684\u5c3a\u5ea6\u53d8\u5316\uff0c\u6211\u4eec\u5c06\u591a\u5c3a\u5ea6\u56fe\u50cf\u7279\u5f81\u96c6\u6210\u5230\u4e0a\u91c7\u6837\u8fc7\u7a0b\u4e2d\uff0c\u4ece\u800c\u5f97\u5230\u6700\u7ec8\u7684\u5c3a\u5ea6\u611f\u77e5\u8bed\u4e49\u63a9\u7801\u3002\u4e3a\u4e86\u63a8\u8fdb\u5730\u7403\u89c6\u89c9\u4e2d\u7684 OVS \u5e76\u9f13\u52b1\u53ef\u91cd\u590d\u7814\u7a76\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u7b2c\u4e00\u4e2a\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u7684\u5f00\u6e90 OVS \u57fa\u51c6\uff0c\u5305\u62ec\u56db\u4e2a\u516c\u5171\u9065\u611f\u6570\u636e\u96c6\u3002\u5728\u8fd9\u4e2a\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u96c6\u90fd\u53ef\u4ee5\u5728 https://github.com/caoql98/OVRS \u83b7\u53d6\u3002||\n", "2409.07353": "|**2024-09-11**|[Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks](http://arxiv.org/abs/2409.07353)|**[link](https://github.com/speedlab-git/robust-encoder-against-jailbreak-attack)**|\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6781\u5927\u5730\u63a8\u8fdb\u4e86\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u5c24\u5176\u662f\u8d8a\u72f1\u653b\u51fb\uff0c\u8fd9\u4e9b\u653b\u51fb\u4f1a\u7ed5\u8fc7\u5b89\u5168\u534f\u8bae\uff0c\u5bfc\u81f4\u6a21\u578b\u751f\u6210\u8bef\u5bfc\u6027\u6216\u6709\u5bb3\u7684\u54cd\u5e94\u3002\u8fd9\u79cd\u8106\u5f31\u6027\u6e90\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u56fa\u6709\u7684\u654f\u611f\u6027\u4ee5\u53ca\u89c6\u89c9\u6a21\u6001\u5f15\u5165\u7684\u6269\u5927\u653b\u51fb\u9762\u3002\u6211\u4eec\u63d0\u51fa\u4e86 Sim-CLIP+\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u9632\u5fa1\u673a\u5236\uff0c\u5b83\u5229\u7528 Siamese \u67b6\u6784\u901a\u8fc7\u5bf9\u6297\u6027\u5fae\u8c03 CLIP \u89c6\u89c9\u7f16\u7801\u5668\u3002\u8fd9\u79cd\u65b9\u6cd5\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u4e86\u6270\u52a8\u6837\u672c\u548c\u5e72\u51c0\u6837\u672c\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0c\u589e\u5f3a\u4e86\u5bf9\u5bf9\u6297\u6027\u64cd\u4f5c\u7684\u62b5\u6297\u529b\u3002Sim-CLIP+ \u63d0\u4f9b\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5141\u8bb8\u4f5c\u4e3a\u5f3a\u5927\u7684\u89c6\u89c9\u7f16\u7801\u5668\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7684 LVLM \u67b6\u6784\u4e2d\u3002\u4e0e\u4ee5\u524d\u7684\u9632\u5fa1\u63aa\u65bd\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u9700\u8981\u5bf9 LVLM \u8fdb\u884c\u7ed3\u6784\u4fee\u6539\uff0c\u5e76\u4e14\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\u3002Sim-CLIP+ \u8bc1\u660e\u4e86\u5176\u5bf9\u57fa\u4e8e\u68af\u5ea6\u7684\u5bf9\u6297\u6027\u653b\u51fb\u548c\u5404\u79cd\u8d8a\u72f1\u6280\u672f\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u9488\u5bf9\u4e09\u79cd\u4e0d\u540c\u7684\u8d8a\u72f1\u653b\u51fb\u7b56\u7565\u8bc4\u4f30\u4e86 Sim-CLIP+\uff0c\u5e76\u4f7f\u7528\u6807\u51c6\u4e0b\u6e38\u6570\u636e\u96c6\uff08\u5305\u62ec\u7528\u4e8e\u56fe\u50cf\u5b57\u5e55\u7684 COCO \u548c\u7528\u4e8e\u89c6\u89c9\u95ee\u7b54\u7684 OKVQA\uff09\u6267\u884c\u4e86\u5e72\u51c0\u8bc4\u4f30\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSim-CLIP+ \u5728\u4fdd\u6301\u9ad8\u6e05\u6d01\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u7740\u63d0\u9ad8\u4e86\u5bf9\u57fa\u4e8e\u68af\u5ea6\u7684\u5bf9\u6297\u6027\u653b\u51fb\u548c\u8d8a\u72f1\u6280\u672f\u7684\u9c81\u68d2\u6027\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u5f3a\u5927\u7684\u89c6\u89c9\u7f16\u7801\u5668\u53ef\u5728 https://github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git \u83b7\u53d6\u3002||\n", "2409.07267": "|**2024-09-11**|[MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D Features as Text Tokens for Autonomous Driving](http://arxiv.org/abs/2409.07267)|**[link](https://github.com/emzucas/minidrive)**|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u4f5c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u901a\u7528\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u901a\u8fc7\u95ee\u7b54\u4ea4\u4e92\u6267\u884c\u9884\u6d4b\u3001\u89c4\u5212\u548c\u611f\u77e5\u7b49\u5b50\u4efb\u52a1\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u89c6\u89c9\u7f16\u7801\u5668\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM)\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u96be\u4ee5\u90e8\u7f72\u5728\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u548c\u5b9e\u65f6\u5e94\u7528\u7a0b\u5e8f\u4e2d\u3002\u540c\u65f6\uff0c\u5927\u591a\u6570\u73b0\u6709 VLM \u7f3a\u4e4f\u5904\u7406\u591a\u56fe\u50cf\u7684\u80fd\u529b\uff0c\u96be\u4ee5\u9002\u5e94\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u591a\u6444\u50cf\u5934\u611f\u77e5\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a MiniDrive \u7684\u65b0\u578b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u7279\u5f81\u5de5\u7a0b\u6df7\u5408\u4e13\u5bb6 (FE-MoE) \u6a21\u5757\u548c\u52a8\u6001\u6307\u4ee4\u9002\u914d\u5668 (DI-Adapter)\u3002FE-MoE \u5728\u8f93\u5165\u8bed\u8a00\u6a21\u578b\u4e4b\u524d\uff0c\u5c06 2D \u7279\u5f81\u6709\u6548\u5730\u6620\u5c04\u5230\u89c6\u89c9\u6807\u8bb0\u5d4c\u5165\u4e2d\u3002DI-Adapter \u4f7f\u89c6\u89c9\u6807\u8bb0\u5d4c\u5165\u80fd\u591f\u968f\u6307\u4ee4\u6587\u672c\u5d4c\u5165\u52a8\u6001\u53d8\u5316\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u65b9\u6cd5\u4e2d\u540c\u4e00\u56fe\u50cf\u7684\u9759\u6001\u89c6\u89c9\u6807\u8bb0\u5d4c\u5165\u95ee\u9898\u3002\u4e0e\u4e4b\u524d\u7684\u5de5\u4f5c\u76f8\u6bd4\uff0cMiniDrive \u5728\u53c2\u6570\u5927\u5c0f\u3001\u6d6e\u70b9\u8fd0\u7b97\u548c\u54cd\u5e94\u6548\u7387\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6700\u5c0f\u7248\u672c\u4ec5\u5305\u542b 83M \u53c2\u6570\u3002||\n", "2409.07129": "|**2024-09-11**|[MVLLaVA: An Intelligent Agent for Unified and Flexible Novel View Synthesis](http://arxiv.org/abs/2409.07129)|null|\u672c\u6587\u4ecb\u7ecd\u4e86MVLLaVA\uff0c\u8fd9\u662f\u4e00\u79cd\u4e13\u4e3a\u65b0\u89c6\u89d2\u5408\u6210\u4efb\u52a1\u8bbe\u8ba1\u7684\u667a\u80fd\u4ee3\u7406\u3002MVLLaVA\u5c06\u591a\u4e2a\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b\u4e0e\u5927\u578b\u591a\u6a21\u6001\u6a21\u578bLLaVA\u76f8\u7ed3\u5408\uff0c\u4f7f\u5176\u80fd\u591f\u9ad8\u6548\u5730\u5904\u7406\u5404\u79cd\u4efb\u52a1\u3002MVLLaVA\u4ee3\u8868\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u7edf\u4e00\u7684\u5e73\u53f0\uff0c\u53ef\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u8f93\u5165\u7c7b\u578b\uff0c\u5305\u62ec\u5355\u4e2a\u56fe\u50cf\u3001\u63cf\u8ff0\u6027\u6807\u9898\u6216\u89c6\u89d2\u65b9\u4f4d\u89d2\u7684\u7279\u5b9a\u53d8\u5316\uff0c\u5e76\u4ee5\u8bed\u8a00\u6307\u4ee4\u6307\u5bfc\u89c6\u89d2\u751f\u6210\u3002\u6211\u4eec\u7cbe\u5fc3\u8bbe\u8ba1\u4e86\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u6307\u4ee4\u6a21\u677f\uff0c\u968f\u540e\u7528\u4e8e\u5fae\u8c03LLaVA\u3002\u56e0\u6b64\uff0cMVLLaVA\u83b7\u5f97\u4e86\u6839\u636e\u7528\u6237\u6307\u4ee4\u751f\u6210\u65b0\u89c6\u89d2\u56fe\u50cf\u7684\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u7075\u6d3b\u6027\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MVLLaVA\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5e94\u5bf9\u5404\u79cd\u65b0\u89c6\u89d2\u5408\u6210\u6311\u6218\u65f6\u7684\u5f3a\u5927\u6027\u80fd\u548c\u591a\u529f\u80fd\u6027\u3002||\n", "2409.06945": "|**2024-09-11**|[FSMDet: Vision-guided feature diffusion for fully sparse 3D detector](http://arxiv.org/abs/2409.06945)|null|\u8fd1\u5e74\u6765\uff0c\u5168\u7a00\u758f\u4e09\u7ef4\u76ee\u6807\u68c0\u6d4b\u5f15\u8d77\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6846\u67b6\u4e2d\u7279\u5f81\u7684\u7a00\u758f\u6027\u7531\u4e8e\u6269\u6563\u8fc7\u7a0b\u6709\u9650\uff0c\u5bf9\u5019\u9009\u6846\u7684\u751f\u6210\u63d0\u51fa\u4e86\u6311\u6218\u3002\u6b64\u5916\uff0c\u5bf9\u6548\u7387\u7684\u8ffd\u6c42\u5bfc\u81f4\u5bf9\u89c6\u89c9\u8f85\u52a9\u7684\u5168\u7a00\u758f\u6a21\u578b\u7684\u7814\u7a76\u5f88\u5c11\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86FSMDet\uff08\u5168\u7a00\u758f\u591a\u6a21\u6001\u68c0\u6d4b\uff09\uff0c\u5b83\u4f7f\u7528\u89c6\u89c9\u4fe1\u606f\u6765\u6307\u5bfc\u6fc0\u5149\u96f7\u8fbe\u7279\u5f81\u6269\u6563\u8fc7\u7a0b\uff0c\u540c\u65f6\u4ecd\u7136\u4fdd\u6301\u7ba1\u9053\u7684\u6548\u7387\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5927\u591a\u6570\u5168\u7a00\u758f\u5de5\u4f5c\u90fd\u96c6\u4e2d\u5728\u590d\u6742\u7684\u5b9a\u5236\u4e2d\u5fc3\u878d\u5408\u6269\u6563/\u56de\u5f52\u7b97\u5b50\u4e0a\u3002\u7136\u800c\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u5982\u679c\u6267\u884c\u4e86\u9002\u5f53\u7684\u76ee\u6807\u8865\u5168\uff0c\u5373\u4f7f\u662f\u6700\u7b80\u5355\u7684\u63d2\u503c\u7b97\u5b50\u4e5f\u80fd\u5f97\u5230\u4ee4\u4eba\u6ee1\u610f\u7684\u7ed3\u679c\u3002\u53d7\u6b64\u89c2\u5bdf\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5c06\u89c6\u89c9\u5f15\u5bfc\u7684\u6269\u6563\u8fc7\u7a0b\u5206\u4e3a\u4e24\u4e2a\u6a21\u5757\uff1a\u5f62\u72b6\u6062\u590d\u5c42\uff08SRLayer\uff09\u548c\u81ea\u6269\u6563\u5c42\uff08SDLayer\uff09\u3002\u524d\u8005\u4f7f\u7528RGB\u4fe1\u606f\u6765\u6062\u590d\u7269\u4f53\u53ef\u89c1\u90e8\u5206\u7684\u5f62\u72b6\uff0c\u540e\u8005\u4f7f\u7528\u89c6\u89c9\u5148\u9a8c\u5c06\u7279\u5f81\u8fdb\u4e00\u6b65\u6269\u6563\u5230\u4e2d\u5fc3\u533a\u57df\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6210\u529f\u5730\u63d0\u9ad8\u4e86\u4ee5\u5f80\u4ec5\u4f7f\u7528\u6fc0\u5149\u96f7\u8fbe\u7684\u5168\u7a00\u758f\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u8fbe\u5230\u4e86SOTA\u6027\u80fd\u3002\u540c\u65f6\uff0c\u7531\u4e8e\u91c7\u7528\u4e86\u7a00\u758f\u67b6\u6784\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6bd4\u4ee5\u5f80\u7684SOTA\u65b9\u6cd5\u6548\u7387\u6700\u9ad8\u53ef\u63d0\u9ad85\u500d\u3002||\n", "2409.06853": "|**2024-09-10**|[ExIQA: Explainable Image Quality Assessment Using Distortion Attributes](http://arxiv.org/abs/2409.06853)|null|\u76f2\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30 (BIQA) \u65e8\u5728\u5f00\u53d1\u65e0\u9700\u53c2\u8003\u56fe\u50cf\u5373\u53ef\u4f30\u8ba1\u56fe\u50cf\u8d28\u91cf\u5206\u6570\u7684\u65b9\u6cd5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ece\u5931\u771f\u8bc6\u522b\u89d2\u5ea6\u63a2\u8ba8 BIQA\uff0c\u4e3b\u8981\u76ee\u6807\u662f\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM)\uff08\u5982 CLIP\uff09\u9884\u6d4b\u5931\u771f\u7c7b\u578b\u548c\u5f3a\u5ea6\uff0c\u56e0\u4e3a\u5b83\u4eec\u5177\u6709\u5e7f\u6cdb\u7684\u77e5\u8bc6\u548c\u6cdb\u5316\u80fd\u529b\u3002\u57fa\u4e8e\u8fd9\u4e9b\u9884\u6d4b\u7684\u5931\u771f\uff0c\u6211\u4eec\u7136\u540e\u4f30\u8ba1\u56fe\u50cf\u7684\u8d28\u91cf\u5206\u6570\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c5e\u6027\u5b66\u4e60\u7684\u53ef\u89e3\u91ca\u5931\u771f\u8bc6\u522b\u65b9\u6cd5\u3002\u6211\u4eec\u6ca1\u6709\u4f7f\u7528\u5931\u771f\u540d\u79f0\u63d0\u793a VLM\uff0c\u800c\u662f\u4f7f\u7528\u5931\u771f\u7684\u5c5e\u6027\u6216\u5f71\u54cd\u63d0\u793a\u5b83\u4eec\uff0c\u5e76\u6c47\u603b\u8fd9\u4e9b\u4fe1\u606f\u4ee5\u63a8\u65ad\u5931\u771f\u5f3a\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4e3a\u6bcf\u5f20\u56fe\u50cf\u8003\u8651\u4e86\u591a\u79cd\u5931\u771f\uff0c\u4f7f\u6211\u4eec\u7684\u65b9\u6cd5\u66f4\u5177\u53ef\u6269\u5c55\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u751f\u6210\u4e86\u4e00\u4e2a\u5305\u542b 100,000 \u5f20\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u9ad8\u6548\u8bad\u7ec3\u3002\u6700\u540e\uff0c\u68c0\u7d22\u5c5e\u6027\u6982\u7387\u5e76\u5c06\u5176\u8f93\u5165\u56de\u5f52\u5668\u4ee5\u9884\u6d4b\u56fe\u50cf\u8d28\u91cf\u5206\u6570\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u9664\u4e86\u5177\u6709\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u5916\uff0c\u8fd8\u5728\u591a\u4e2a\u6570\u636e\u96c6\u7684 PLCC \u548c SRCC \u6307\u6807\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb (SOTA) \u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u96f6\u6837\u672c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u3002||\n", "2409.08885": "|**2024-09-13**|[Interactive Masked Image Modeling for Multimodal Object Detection in Remote Sensing](http://arxiv.org/abs/2409.08885)|null|\u9065\u611f\u5f71\u50cf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u5728\u5730\u7403\u89c2\u6d4b\u7684\u5404\u79cd\u5e94\u7528\u4e2d\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u4e0e\u81ea\u7136\u573a\u666f\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u4e0d\u540c\uff0c\u8fd9\u9879\u4efb\u52a1\u7279\u522b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5728\u4e0d\u540c\u7684\u5730\u5f62\u4e2d\u5b58\u5728\u5927\u91cf\u7684\u5c0f\u578b\u4e14\u901a\u5e38\u96be\u4ee5\u5bdf\u89c9\u7684\u76ee\u6807\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u53ef\u4ee5\u4f7f\u7528\u591a\u6a21\u6001\u5b66\u4e60\u6765\u6574\u5408\u6765\u81ea\u4e0d\u540c\u6570\u636e\u6a21\u6001\u7684\u7279\u5f81\uff0c\u4ece\u800c\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002\u7136\u800c\uff0c\u591a\u6a21\u6001\u5b66\u4e60\u7684\u6027\u80fd\u5f80\u5f80\u53d7\u5230\u6807\u8bb0\u6570\u636e\u96c6\u5927\u5c0f\u7684\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528\u63a9\u853d\u56fe\u50cf\u5efa\u6a21\uff08MIM\uff09\u4f5c\u4e3a\u4e00\u79cd\u9884\u8bad\u7ec3\u6280\u672f\uff0c\u5229\u7528\u65e0\u6807\u8bb0\u6570\u636e\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6765\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684MIM\u65b9\u6cd5\uff08\u5982MAE\uff09\u4f7f\u7528\u6ca1\u6709\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u63a9\u853d\u6807\u8bb0\uff0c\u7531\u4e8e\u7f3a\u4e4f\u4e0e\u56fe\u50cf\u5176\u4ed6\u90e8\u5206\u7684\u4ea4\u4e92\uff0c\u96be\u4ee5\u6355\u6349\u5230\u7ec6\u7c92\u5ea6\u7684\u7ec6\u8282\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ea4\u4e92\u5f0fMIM\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u6807\u8bb0\u4e4b\u95f4\u5efa\u7acb\u4ea4\u4e92\uff0c\u8fd9\u5bf9\u4e8e\u9065\u611f\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u7279\u522b\u6709\u5229\u3002\u5927\u91cf\u7684\u6d88\u878d\u7814\u7a76\u548c\u8bc4\u4f30\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002||\n", "2409.08790": "|**2024-09-13**|[A Multimodal Approach for Fluid Overload Prediction: Integrating Lung Ultrasound and Clinical Data](http://arxiv.org/abs/2409.08790)|null|\u7ef4\u6301\u900f\u6790\u60a3\u8005\u7684\u4f53\u6db2\u5e73\u8861\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u7ba1\u7406\u4e0d\u5f53\u4f1a\u5bfc\u81f4\u4e25\u91cd\u5e76\u53d1\u75c7\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u80ba\u90e8\u8d85\u58f0\u56fe\u50cf\u7684\u89c6\u89c9\u7279\u5f81\u548c\u4e34\u5e8a\u6570\u636e\uff0c\u4ee5\u589e\u5f3a\u5bf9\u4f53\u5185\u591a\u4f59\u6db2\u4f53\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u6211\u4eec\u7684\u6846\u67b6\u91c7\u7528\u72ec\u7acb\u7684\u7f16\u7801\u5668\u6765\u63d0\u53d6\u6bcf\u79cd\u6a21\u6001\u7684\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u8de8\u57df\u6ce8\u610f\u529b\u673a\u5236\u5c06\u5b83\u4eec\u7ec4\u5408\u8d77\u6765\uff0c\u4ee5\u6355\u83b7\u4e92\u8865\u4fe1\u606f\u3002\u901a\u8fc7\u5c06\u9884\u6d4b\u6784\u5efa\u4e3a\u5206\u7c7b\u4efb\u52a1\uff0c\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86\u6bd4\u56de\u5f52\u6a21\u578b\u66f4\u597d\u7684\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u591a\u6a21\u6001\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u5355\u6a21\u6001\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5148\u8003\u8651\u8868\u683c\u6570\u636e\u65f6\u3002\u4f2a\u6837\u672c\u751f\u6210\u8fdb\u4e00\u6b65\u6709\u52a9\u4e8e\u7f13\u89e3\u5206\u7c7b\u95ee\u9898\u4e2d\u7684\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86 88.31% \u7684\u6700\u9ad8\u51c6\u786e\u7387\u3002\u8fd9\u9879\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u5bf9\u900f\u6790\u60a3\u8005\u6db2\u4f53\u8d85\u8d1f\u8377\u7ba1\u7406\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6539\u5584\u4e34\u5e8a\u7ed3\u679c\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002||\n", "2409.08582": "|**2024-09-13**|[ChangeChat: An Interactive Model for Remote Sensing Change Analysis via Multimodal Instruction Tuning](http://arxiv.org/abs/2409.08582)|null|\u9065\u611f (RS) \u53d8\u5316\u5206\u6790\u901a\u8fc7\u68c0\u6d4b\u56fe\u50cf\u968f\u65f6\u95f4\u7684\u53d8\u5316\u6765\u76d1\u6d4b\u5730\u7403\u52a8\u6001\u8fc7\u7a0b\uff0c\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u53d8\u70b9\u68c0\u6d4b\u64c5\u957f\u8bc6\u522b\u50cf\u7d20\u7ea7\u7684\u53d8\u5316\uff0c\u4f46\u7f3a\u4e4f\u5c06\u8fd9\u4e9b\u53d8\u5316\u7f6e\u4e8e\u80cc\u666f\u4e2d\u7684\u80fd\u529b\u3002\u867d\u7136\u6700\u8fd1\u5728\u53d8\u5316\u63cf\u8ff0\u65b9\u9762\u7684\u8fdb\u5c55\u63d0\u4f9b\u4e86\u5bf9\u53d8\u5316\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u4f46\u5b83\u4eec\u4e0d\u652f\u6301\u4ea4\u4e92\u5f0f\u7684\u3001\u7528\u6237\u7279\u5b9a\u7684\u67e5\u8be2\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86 ChangeChat\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4e13\u4e3a RS \u53d8\u5316\u5206\u6790\u8bbe\u8ba1\u7684\u53cc\u65f6\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM)\u3002ChangeChat \u5229\u7528\u591a\u6a21\u6001\u6307\u4ee4\u5fae\u8c03\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u67e5\u8be2\uff0c\u4f8b\u5982\u53d8\u5316\u63cf\u8ff0\u3001\u7279\u5b9a\u7c7b\u522b\u7684\u91cf\u5316\u548c\u53d8\u5316\u5b9a\u4f4d\u3002\u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\uff0c\u6211\u4eec\u5f00\u53d1\u4e86 ChangeChat-87k \u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u662f\u4f7f\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u548c GPT \u8f85\u52a9\u6280\u672f\u76f8\u7ed3\u5408\u751f\u6210\u7684\u3002\u5b9e\u9a8c\u8868\u660e\uff0cChangeChat \u4e3a RS \u53d8\u5316\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u3001\u4ea4\u4e92\u5f0f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8fbe\u5230\u751a\u81f3\u4f18\u4e8e\u6700\u5148\u8fdb (SOTA) \u65b9\u6cd5\uff0c\u5e76\u663e\u7740\u8d85\u8fc7\u4e86\u6700\u65b0\u7684\u901a\u7528\u6a21\u578b GPT-4\u3002\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6743\u91cd\u53ef\u5728 https://github.com/hanlinwu/ChangeChat \u83b7\u53d6\u3002||\n", "2409.08468": "|**2024-09-13**|[Generalization Boosted Adapter for Open-Vocabulary Segmentation](http://arxiv.org/abs/2409.08468)|null|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5df2\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u8bc6\u522b\u80fd\u529b\uff0c\u8fd9\u4fc3\u4f7f\u5b83\u4eec\u88ab\u5e94\u7528\u4e8e\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\uff0c\u4f8b\u5982\u5206\u5272\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7f3a\u4e4f\u50cf\u7d20\u7ea7\u7c92\u5ea6\u4ee5\u53ca\u53ef\u7528\u4e8e\u5fae\u8c03\u7684\u6570\u636e\u6709\u9650\uff0c\u76f4\u63a5\u5c06 VLM \u5e94\u7528\u4e8e\u6b64\u7c7b\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u62df\u5408\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6cdb\u5316\u589e\u5f3a\u9002\u914d\u5668 (GBA)\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u9002\u914d\u5668\u7b56\u7565\uff0c\u53ef\u4ee5\u589e\u5f3a VLM \u5bf9\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002GBA \u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u98ce\u683c\u591a\u6837\u5316\u9002\u914d\u5668 (SDA)\uff0c\u5b83\u5c06\u7279\u5f81\u89e3\u8026\u4e3a\u5e45\u5ea6\u548c\u76f8\u4f4d\u5206\u91cf\uff0c\u4ec5\u5bf9\u5e45\u5ea6\u8fdb\u884c\u64cd\u4f5c\u4ee5\u4e30\u5bcc\u7279\u5f81\u7a7a\u95f4\u8868\u793a\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\uff1b(2) \u76f8\u5173\u6027\u7ea6\u675f\u9002\u914d\u5668 (CCA)\uff0c\u5b83\u91c7\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5728\u6587\u672c\u7c7b\u522b\u548c\u76ee\u6807\u533a\u57df\u4e4b\u95f4\u5efa\u7acb\u66f4\u7d27\u5bc6\u7684\u8bed\u4e49\u5173\u8054\uff0c\u6291\u5236\u4e0d\u76f8\u5173\u7684\u4f4e\u9891\u201c\u566a\u58f0\u201d\u4fe1\u606f\u5e76\u907f\u514d\u9519\u8bef\u5173\u8054\u3002\u901a\u8fc7\u6d45\u5c42 SDA \u548c\u6df1\u5c42 CCA \u7684\u534f\u540c\u6548\u5e94\uff0cGBA \u6709\u6548\u5730\u7f13\u89e3\u4e86\u8fc7\u5ea6\u62df\u5408\u95ee\u9898\uff0c\u5e76\u589e\u5f3a\u4e86\u7279\u5f81\u8868\u793a\u7684\u8bed\u4e49\u76f8\u5173\u6027\u3002\u4f5c\u4e3a\u4e00\u4e2a\u7b80\u5355\u3001\u9ad8\u6548\u3001\u5373\u63d2\u5373\u7528\u7684\u7ec4\u4ef6\uff0cGBA \u53ef\u4ee5\u7075\u6d3b\u5730\u96c6\u6210\u5230\u5404\u79cd\u57fa\u4e8e CLIP \u7684\u65b9\u6cd5\u4e2d\uff0c\u5c55\u73b0\u51fa\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002||\n", "2409.08381": "|**2024-09-12**|[Rethinking Prompting Strategies for Multi-Label Recognition with Partial Annotations](http://arxiv.org/abs/2409.08381)|null|\u50cf CLIP \u8fd9\u6837\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5df2\u88ab\u5e94\u7528\u4e8e\u90e8\u5206\u6807\u6ce8\u7684\u591a\u6807\u7b7e\u8bc6\u522b (MLR)\uff0c\u5176\u65b9\u6cd5\u662f\u5229\u7528\u63d0\u793a\u5b66\u4e60\uff0c\u4e3a\u6bcf\u4e2a\u7c7b\u522b\u5b66\u4e60\u6b63\u8d1f\u63d0\u793a\uff0c\u4ee5\u4fbf\u5c06\u5b83\u4eec\u7684\u5d4c\u5165\u4e0e\u5171\u4eab\u89c6\u89c9\u6587\u672c\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u7c7b\u522b\u5b58\u5728\u6216\u4e0d\u5b58\u5728\u76f8\u5173\u8054\u3002\u867d\u7136\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u4f9d\u8d56 VLM \u5148\u9a8c\u4fe1\u606f\u63d0\u9ad8\u4e86 MLR \u6027\u80fd\uff0c\u4f46\u6211\u4eec\u5047\u8bbe\u5b66\u4e60\u8d1f\u9762\u63d0\u793a\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u56e0\u4e3a\u7528\u4e8e\u8bad\u7ec3 VLM \u7684\u6570\u636e\u96c6\u7f3a\u4e4f\u660e\u786e\u5173\u6ce8\u7c7b\u522b\u7f3a\u5931\u7684\u56fe\u50cf-\u6807\u9898\u5bf9\u3002\u4e3a\u4e86\u5206\u6790\u6b63\u8d1f\u63d0\u793a\u5b66\u4e60\u5bf9 MLR \u7684\u5f71\u54cd\uff0c\u6211\u4eec\u5f15\u5165\u4e86 PositiveCoOp \u548c NegativeCoOp\uff0c\u5176\u4e2d\u53ea\u6709\u4e00\u4e2a\u63d0\u793a\u662f\u5728 VLM \u6307\u5bfc\u4e0b\u5b66\u4e60\u7684\uff0c\u800c\u53e6\u4e00\u4e2a\u63d0\u793a\u5219\u88ab\u76f4\u63a5\u5728\u5171\u4eab\u7279\u5f81\u7a7a\u95f4\u4e2d\u5b66\u4e60\u7684\u5d4c\u5165\u5411\u91cf\u6240\u53d6\u4ee3\uff0c\u800c\u4e0d\u4f9d\u8d56\u4e8e\u6587\u672c\u7f16\u7801\u5668\u3002\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u8d1f\u9762\u63d0\u793a\u4f1a\u964d\u4f4e MLR \u6027\u80fd\uff0c\u5e76\u4e14\u4ec5\u5b66\u4e60\u6b63\u9762\u63d0\u793a\u5e76\u7ed3\u5408\u5b66\u4e60\u5230\u7684\u8d1f\u9762\u5d4c\u5165\uff08PositiveCoOp\uff09\u4f18\u4e8e\u53cc\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u91cf\u5316\u4e86\u63d0\u793a\u5b66\u4e60\u76f8\u5bf9\u4e8e\u4ec5\u4f7f\u7528\u89c6\u89c9\u7279\u5f81\u7684\u7b80\u5355\u57fa\u7ebf\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u89c2\u5bdf\u5230\u5f53\u7f3a\u5931\u6807\u7b7e\u7684\u6bd4\u4f8b\u8f83\u4f4e\u65f6\uff0c\u57fa\u7ebf\u8868\u73b0\u51fa\u4e0e\u53cc\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5 (DualCoOp) \u76f8\u5f53\u7684\u5f3a\u52b2\u6027\u80fd\uff0c\u540c\u65f6\u6240\u9700\u7684\u8bad\u7ec3\u8ba1\u7b97\u91cf\u51cf\u5c11\u4e00\u534a\uff0c\u53c2\u6570\u6570\u91cf\u51cf\u5c11 16 \u500d\u3002||\n", "2409.11402": "|**2024-09-17**|[NVLM: Open Frontier-Class Multimodal LLMs](http://arxiv.org/abs/2409.11402)|null|\u6211\u4eec\u63a8\u51fa\u4e86 NVLM 1.0\uff0c\u8fd9\u662f\u4e00\u7cfb\u5217\u524d\u6cbf\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM)\uff0c\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u53ef\u4e0e\u9886\u5148\u7684\u4e13\u6709\u6a21\u578b\uff08\u4f8b\u5982 GPT-4o\uff09\u548c\u5f00\u653e\u8bbf\u95ee\u6a21\u578b\uff08\u4f8b\u5982 Llama 3-V 405B \u548c InternVL 2\uff09\u76f8\u5ab2\u7f8e\u3002 \u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cNVLM 1.0 \u5728\u591a\u6a21\u6001\u8bad\u7ec3\u540e\uff0c\u5176\u7eaf\u6587\u672c\u6027\u80fd\u4f18\u4e8e\u5176 LLM \u9aa8\u5e72\u6a21\u578b\u3002 \u5728\u6a21\u578b\u8bbe\u8ba1\u65b9\u9762\uff0c\u6211\u4eec\u5bf9\u4ec5\u89e3\u7801\u5668\u591a\u6a21\u6001 LLM\uff08\u4f8b\u5982 LLaVA\uff09\u548c\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u6a21\u578b\uff08\u4f8b\u5982 Flamingo\uff09\u8fdb\u884c\u4e86\u5168\u9762\u6bd4\u8f83\u3002 \u57fa\u4e8e\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\uff0c\u53ef\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002 \u6b64\u5916\uff0c\u6211\u4eec\u4e3a\u57fa\u4e8e\u56fe\u5757\u7684\u52a8\u6001\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5f15\u5165\u4e86 1-D \u56fe\u5757\u6807\u8bb0\u8bbe\u8ba1\uff0c\u8fd9\u663e\u7740\u63d0\u9ad8\u4e86\u591a\u6a21\u6001\u63a8\u7406\u548c OCR \u76f8\u5173\u4efb\u52a1\u7684\u6027\u80fd\u3002 \u5173\u4e8e\u8bad\u7ec3\u6570\u636e\uff0c\u6211\u4eec\u7cbe\u5fc3\u7b56\u5212\u5e76\u63d0\u4f9b\u6709\u5173\u6211\u4eec\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\u6570\u636e\u96c6\u7684\u8be6\u7ec6\u4fe1\u606f\u3002 \u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u5728\u6240\u6709\u67b6\u6784\u4e2d\uff0c\u6570\u636e\u96c6\u8d28\u91cf\u548c\u4efb\u52a1\u591a\u6837\u6027\u90fd\u6bd4\u89c4\u6a21\u66f4\u91cd\u8981\u3002 \u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u4e3a NVLM-1.0 \u6a21\u578b\u5f00\u53d1\u4e86\u751f\u4ea7\u7ea7\u591a\u6a21\u6001\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u6539\u8fdb\u4e0e\u5176 LLM \u9aa8\u5e72\u6a21\u578b\u76f8\u6bd4\u7684\u7eaf\u6587\u672c\u6027\u80fd\u3002 \u4e3a\u6b64\uff0c\u6211\u4eec\u5c06\u9ad8\u8d28\u91cf\u7684\u7eaf\u6587\u672c\u6570\u636e\u96c6\u4e0e\u5927\u91cf\u7684\u591a\u6a21\u6001\u6570\u5b66\u548c\u63a8\u7406\u6570\u636e\u4e00\u8d77\u5236\u4f5c\u5e76\u96c6\u6210\u5230\u591a\u6a21\u6001\u8bad\u7ec3\u4e2d\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u8de8\u6a21\u6001\u7684\u6570\u5b66\u548c\u7f16\u7801\u80fd\u529b\u3002 \u4e3a\u4e86\u63a8\u52a8\u8be5\u9886\u57df\u7684  \u7814\u7a76\uff0c\u6211\u4eec\u5c06\u53d1\u5e03\u6a21\u578b\u6743\u91cd\uff0c\u5e76\u5c06\u5f00\u6e90\u4ee3\u7801\u4f9b\u793e\u533a\u4f7f\u7528\uff1ahttps://nvlm-project.github.io/\u3002||\n", "2409.11007": "|**2024-09-17**|[CAST: Cross-modal Alignment Similarity Test for Vision Language Models](http://arxiv.org/abs/2409.11007)|**[link](https://github.com/gautierdag/cast)**|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u901a\u5e38\u901a\u8fc7\u89c6\u89c9\u95ee\u7b54 (VQA) \u4efb\u52a1\u8fdb\u884c\u8bc4\u4f30\uff0c\u8fd9\u4e9b\u4efb\u52a1\u8bc4\u4f30\u6a21\u578b\u5bf9\u573a\u666f\u7684\u7406\u89e3\u3002\u826f\u597d\u7684 VQA \u6027\u80fd\u88ab\u89c6\u4e3a\u8be5\u6a21\u578b\u80fd\u591f\u5728\u9700\u8981\u89c6\u89c9\u548c\u8bed\u8a00\u8f93\u5165\u7684\u66f4\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u7684\u8bc1\u636e\u3002\u7136\u800c\uff0c\u573a\u666f\u611f\u77e5 VQA \u5e76\u4e0d\u80fd\u5b8c\u5168\u6355\u6349\u8f93\u5165\u504f\u5dee\uff0c\u4e5f\u4e0d\u80fd\u8bc4\u4f30\u7531\u6a21\u6001\u4e4b\u95f4\u9519\u4f4d\u5f15\u8d77\u7684\u5e7b\u89c9\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8de8\u6a21\u6001\u5bf9\u9f50\u76f8\u4f3c\u6027\u6d4b\u8bd5 (CAST) \u6765\u63a2\u6d4b VLM \u5728\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7684\u81ea\u6d3d\u6027\u3002\u8be5\u6d4b\u8bd5\u5305\u62ec\u8981\u6c42\u6a21\u578b\u4ec5\u901a\u8fc7\u6587\u672c\u3001\u4ec5\u901a\u8fc7\u56fe\u50cf\u6216\u4e24\u8005\u517c\u7528\u6765\u8bc6\u522b\u4e24\u4e2a\u573a\u666f\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u7136\u540e\u8bc4\u4f30\u5b83\u4eec\u751f\u6210\u7684\u76f8\u4f3c\u6027\u7684\u771f\u5b9e\u6027\u3002\u7531\u4e8e\u6ca1\u6709\u53ef\u4f9b\u6bd4\u8f83\u7684\u771f\u5b9e\u60c5\u51b5\uff0c\u56e0\u6b64\u8be5\u8bc4\u4f30\u7684\u91cd\u70b9\u4e0d\u662f\u5ba2\u89c2\u51c6\u786e\u6027\uff0c\u800c\u662f VLM \u5728\u8f93\u51fa\u65b9\u9762\u662f\u5426\u5185\u90e8\u4e00\u81f4\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u867d\u7136\u5e76\u975e\u6240\u6709\u81ea\u6d3d\u6a21\u578b\u90fd\u5177\u6709\u80fd\u529b\u6216\u51c6\u786e\u6027\uff0c\u4f46\u6240\u6709\u6709\u80fd\u529b\u7684 VLM \u90fd\u5fc5\u987b\u662f\u81ea\u6d3d\u7684\u3002||\n", "2409.10921": "|**2024-09-17**|[KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph](http://arxiv.org/abs/2409.10921)|**[link](https://github.com/yanbei-jiang/artwork-interpretation)**|Exploring the narratives conveyed by fine-art paintings is a challenge in image captioning, where the goal is to generate descriptions that not only precisely represent the visual content but also offer a in-depth interpretation of the artwork's meaning. The task is particularly complex for artwork images due to their diverse interpretations and varied aesthetic principles across different artistic schools and styles. In response to this, we present KALE Knowledge-Augmented vision-Language model for artwork Elaborations), a novel approach that enhances existing vision-language models by integrating artwork metadata as additional knowledge. KALE incorporates the metadata in two ways: firstly as direct textual input, and secondly through a multimodal heterogeneous knowledge graph. To optimize the learning of graph representations, we introduce a new cross-modal alignment loss that maximizes the similarity between the image and its corresponding metadata. Experimental results demonstrate that KALE achieves strong performance (when evaluated with CIDEr, in particular) over existing state-of-the-art work across several artwork datasets. Source code of the project is available at https://github.com/Yanbei-Jiang/Artwork-Interpretation.||\n", "2409.10488": "|**2024-09-16**|[Do Pre-trained Vision-Language Models Encode Object States?](http://arxiv.org/abs/2409.10488)|null|For a vision-language model (VLM) to understand the physical world, such as cause and effect, a first step is to capture the temporal dynamics of the visual world, for example how the physical states of objects evolve over time (e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMs pre-trained on web-scale data learn to encode object states, which can be extracted with zero-shot text prompts. We curate an object state recognition dataset ChangeIt-Frames, and evaluate nine open-source VLMs, including models trained with contrastive and generative objectives. We observe that while these state-of-the-art vision-language models can reliably perform object recognition, they consistently fail to accurately distinguish the objects' physical states. Through extensive experiments, we identify three areas for improvements for VLMs to better encode object states, namely the quality of object localization, the architecture to bind concepts to objects, and the objective to learn discriminative visual and language encoders on object states. Data and code are released.||\n", "2409.10441": "|**2024-09-16**|[CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera](http://arxiv.org/abs/2409.10441)|null|Camera-to-robot calibration is crucial for vision-based robot control and requires effort to make it accurate. Recent advancements in markerless pose estimation methods have eliminated the need for time-consuming physical setups for camera-to-robot calibration. While the existing markerless pose estimation methods have demonstrated impressive accuracy without the need for cumbersome setups, they rely on the assumption that all the robot joints are visible within the camera's field of view. However, in practice, robots usually move in and out of view, and some portion of the robot may stay out-of-frame during the whole manipulation task due to real-world constraints, leading to a lack of sufficient visual features and subsequent failure of these approaches. To address this challenge and enhance the applicability to vision-based robot control, we propose a novel framework capable of estimating the robot pose with partially visible robot manipulators. Our approach leverages the Vision-Language Models for fine-grained robot components detection, and integrates it into a keypoint-based pose estimation network, which enables more robust performance in varied operational conditions. The framework is evaluated on both public robot datasets and self-collected partial-view datasets to demonstrate our robustness and generalizability. As a result, this method is effective for robot pose estimation in a wider range of real-world manipulation scenarios.||\n", "2409.10419": "|**2024-09-16**|[HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping Using Vision-Language Models](http://arxiv.org/abs/2409.10419)|null|\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4e0e\u4eba\u7c7b\u4ea4\u4e92\u7684\u673a\u5668\u4eba\u53ef\u4ee5\u89e3\u9501\u8bb8\u591a\u5e94\u7528\uff0c\u4f8b\u5982\u53c2\u8003\u6293\u53d6\u5408\u6210\uff08RGS\uff09\u3002\u7ed9\u5b9a\u4e00\u4e2a\u6587\u672c\u67e5\u8be2\uff0cRGS\u786e\u5b9a\u4e00\u4e2a\u7a33\u5b9a\u7684\u6293\u53d6\u59ff\u6001\u6765\u64cd\u7eb5\u673a\u5668\u4eba\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u6240\u6307\u7684\u5bf9\u8c61\u3002RGS\u5305\u62ec\u4e24\u4e2a\u6b65\u9aa4\uff1a\u89c6\u89c9\u5b9a\u4f4d\u548c\u6293\u53d6\u59ff\u6001\u4f30\u8ba1\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5229\u7528\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5c06\u81ea\u7531\u6d41\u52a8\u7684\u81ea\u7136\u8bed\u8a00\u89c6\u89c9\u5b9a\u4f4d\u5230\u73b0\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u6267\u884c\u4e2d\u3002\u7136\u800c\uff0c\u5728\u5177\u6709\u591a\u4e2a\u76f8\u540c\u5bf9\u8c61\u5b9e\u4f8b\u7684\u590d\u6742\u3001\u6742\u4e71\u73af\u5883\u4e2d\u7684\u6bd4\u8f83\u4ecd\u7136\u7f3a\u4e4f\u3002\u672c\u6587\u4ecb\u7ecd\u4e86HiFi-CS\uff0c\u5b83\u91c7\u7528\u7279\u5f81\u7ebf\u6027\u8c03\u5236\uff08FiLM\uff09\u7684\u5206\u5c42\u5e94\u7528\u6765\u878d\u5408\u56fe\u50cf\u548c\u6587\u672c\u5d4c\u5165\uff0c\u589e\u5f3a\u4e86\u673a\u5668\u4eba\u6293\u53d6\u4e2d\u9047\u5230\u7684\u590d\u6742\u5c5e\u6027\u4e30\u5bcc\u6587\u672c\u67e5\u8be2\u7684\u89c6\u89c9\u5b9a\u4f4d\u3002\u89c6\u89c9\u5b9a\u4f4d\u5c06\u4e8c\u7ef4/\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u7684\u5bf9\u8c61\u4e0e\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u76f8\u5173\u8054\uff0c\u5e76\u5728\u4e24\u79cd\u60c5\u51b5\u4e0b\u8fdb\u884c\u7814\u7a76\uff1a\u5c01\u95ed\u8bcd\u6c47\u548c\u5f00\u653e\u8bcd\u6c47\u3002HiFi-CS\u5177\u6709\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u89e3\u7801\u5668\uff0c\u7ed3\u5408\u4e86\u4e00\u4e2a\u51bb\u7ed3\u7684VLM\uff0c\u5728\u5c01\u95ed\u8bcd\u6c47\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\uff0c\u540c\u65f6\u5c3a\u5bf8\u7f29\u5c0f\u4e86100\u500d\u3002\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u6307\u5bfc\u50cfGroundedSAM\u8fd9\u6837\u7684\u5f00\u653e\u96c6\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u4ee5\u63d0\u9ad8\u5f00\u653e\u8bcd\u6c47\u6027\u80fd\u3002\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a7\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\uff0c\u901a\u8fc7\u771f\u5b9e\u7684RGS\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u572815\u4e2a\u684c\u9762\u573a\u666f\u4e2d\u5b9e\u73b0\u4e8690.33%\u7684\u89c6\u89c9\u5b9a\u4f4d\u7cbe\u5ea6\u3002\u6211\u4eec\u5728\u8865\u5145\u6750\u6599\u4e2d\u5305\u542b\u4e86\u6211\u4eec\u7684\u4ee3\u7801\u5e93\u3002||\n", "2409.10078": "|**2024-09-19**|[IRIS: Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis](http://arxiv.org/abs/2409.10078)|null|\u5927\u578b\u8bed\u8a00\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u663e\u8457\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u7406\u89e3\uff0c\u7136\u800c\u5c06\u9ad8\u7ea7\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u4e3a\u7cbe\u786e\u76843D\u7a7a\u95f4\u673a\u5668\u4eba\u52a8\u4f5c\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u4ecb\u7ecd\u4e86IRIS\uff08\u4ea4\u4e92\u5f0f\u54cd\u5e94\u667a\u80fd\u5206\u5272\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e3D\u529f\u80fd\u5206\u5272\u7684\u5168\u65b0\u514d\u8bad\u7ec3\u591a\u6a21\u6001\u7cfb\u7edf\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u65e5\u5e38\u73af\u5883\u4e2d\u4ea4\u4e92\u5f0f\u8bed\u8a00\u5f15\u5bfc\u529f\u80fd\u7684\u57fa\u51c6\u3002IRIS\u5c06\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u4e0e\u4e13\u95e8\u76843D\u89c6\u89c9\u7f51\u7edc\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e862D\u548c3D\u89c6\u89c9\u7406\u89e3\u4e0e\u8bed\u8a00\u7406\u89e3\u7684\u65e0\u7f1d\u878d\u5408\u3002\u4e3a\u4e86\u4fbf\u4e8e\u8bc4\u4f30\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b10\u4e2a\u5178\u578b\u5ba4\u5185\u73af\u5883\u7684\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u73af\u5883\u5305\u542b50\u5f20\u6807\u6ce8\u4e86\u7269\u4f53\u52a8\u4f5c\u548c3D\u529f\u80fd\u5206\u5272\u7684\u56fe\u50cf\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cIRIS\u80fd\u591f\u5904\u7406\u5404\u79cd\u73af\u5883\u4e0b\u7684\u4ea4\u4e92\u5f0f3D\u529f\u80fd\u5206\u5272\u4efb\u52a1\uff0c\u5e76\u5728\u5404\u79cd\u6307\u6807\u4e0a\u5747\u5c55\u73b0\u51fa\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u7ed3\u679c\u7a81\u51fa\u4e86IRIS\u5728\u589e\u5f3a\u57fa\u4e8e\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\u529f\u80fd\u7406\u89e3\u7684\u4eba\u673a\u4ea4\u4e92\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u63a8\u8fdb\u4e86\u66f4\u76f4\u89c2\u3001\u66f4\u9ad8\u6548\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u3002||\n", "2409.09845": "|**2024-09-15**|[FSL-LVLM: Friction-Aware Safety Locomotion using Large Vision Language Model in Wheeled Robots](http://arxiv.org/abs/2409.09845)|null|\u8f6e\u817f\u5f0f\u673a\u5668\u4eba\u5728\u79fb\u52a8\u6027\u548c\u591a\u529f\u80fd\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u5728\u6e7f\u6ed1\u5730\u5f62\u4e0a\u8fd0\u884c\u65f6\u9762\u4e34\u7740\u5de8\u5927\u6311\u6218\u3002\u8fd9\u4e9b\u673a\u5668\u4eba\u7684\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\u5668\u5047\u8bbe\u6ca1\u6709\u6ed1\u52a8\u3002\u867d\u7136\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u53ef\u4ee5\u5e2e\u52a9\u56db\u8db3\u673a\u5668\u4eba\u9002\u5e94\u4e0d\u540c\u7684\u8868\u9762\uff0c\u4f46\u4ece\u6ed1\u52a8\u4e2d\u6062\u590d\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u63a5\u89e6\u70b9\u8f83\u5c11\u7684\u7cfb\u7edf\u3002\u4f30\u8ba1\u5730\u9762\u6469\u64e6\u7cfb\u6570\u662f\u53e6\u4e00\u4e2a\u5f00\u653e\u7684\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6469\u64e6\u611f\u77e5\u5b89\u5168\u8fd0\u52a8\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0eRL\u7b56\u7565\u76f8\u7ed3\u5408\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u4f30\u8ba1\u7684\u6469\u64e6\u7cfb\u6570\u660e\u786e\u7eb3\u5165RL\u7b56\u7565\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5728\u5230\u8fbe\u8868\u9762\u4e4b\u524d\u6839\u636e\u8868\u9762\u7c7b\u578b\u63d0\u524d\u8c03\u6574\u5176\u884c\u4e3a\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u201c\u89c6\u89c9\u6469\u64e6\u201d\uff08FFV\uff09\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5229\u7528LLM\u4f30\u8ba1\u5730\u9762\u6469\u64e6\u7cfb\u6570\uff0c\u4ece\u800c\u65e0\u9700\u5927\u578b\u6570\u636e\u96c6\u548c\u5927\u91cf\u8bad\u7ec3\u3002\u8be5\u6846\u67b6\u5728\u5b9a\u5236\u7684\u8f6e\u5f0f\u5012\u7acb\u6446\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u901a\u8fc7\u6839\u636e\u5730\u5f62\u7c7b\u578b\u8c03\u6574\u901f\u5ea6\u6765\u63d0\u9ad8\u5b8c\u6210\u9a7e\u9a76\u4efb\u52a1\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8ddf\u8e2a\u6027\u80fd\u3002\u6211\u4eec\u7684\u6846\u67b6\u53ef\u4ee5\u8f7b\u677e\u5730\u4e0e\u4efb\u4f55\u5176\u4ed6RL\u7b56\u7565\u96c6\u6210\u3002||\n", "2409.09788": "|**2024-09-15**|[Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models](http://arxiv.org/abs/2409.09788)|null|\u5c3d\u7ba1\u8fd1\u671f\u7814\u7a76\u8868\u660e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u80fd\u591f\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u56fe\u50cf\u4e2d\u7684\u590d\u6742\u5173\u7cfb\uff0c\u4f46\u5176\u5bf9\u7269\u4f53\u5927\u5c0f\u548c\u8ddd\u79bb\u8fdb\u884c\u5b9a\u91cf\u63a8\u7406\u7684\u80fd\u529b\u4ecd\u6709\u5f85\u63a2\u7d22\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6 Q-Spatial Bench\uff0c\u5176\u4e2d\u5305\u542b 271 \u4e2a\u8de8\u8d8a\u4e94\u4e2a\u7c7b\u522b\u7684\u3001\u4e13\u4e3a\u5b9a\u91cf\u7a7a\u95f4\u63a8\u7406\u8bbe\u8ba1\u7684\u95ee\u9898\uff0c\u5e76\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u6700\u5148\u8fdb\u7684 VLM \u5728\u8fd9\u9879\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0c\u63a8\u7406\u7269\u4f53\u4e4b\u95f4\u7684\u8ddd\u79bb\u5bf9 SoTA VLM \u6765\u8bf4\u5c24\u5176\u5177\u6709\u6311\u6218\u6027\uff1b\u7136\u800c\uff0c\u4e00\u4e9b VLM \u7684\u6027\u80fd\u660e\u663e\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u8868\u73b0\u6700\u597d\u7684\u4e24\u4e2a\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u8d85\u8fc7 40 \u4e2a\u767e\u5206\u70b9\u7684\u5dee\u8ddd\u3002\u6211\u4eec\u8fd8\u60ca\u5947\u5730\u89c2\u5bdf\u5230\uff0c\u5f53\u54cd\u5e94\u4e2d\u81ea\u7136\u51fa\u73b0\u4f7f\u7528\u53c2\u8003\u5bf9\u8c61\u7684\u63a8\u7406\u8def\u5f84\u65f6\uff0c\u6027\u80fd\u6700\u4f73\u7684 VLM \u7684\u6210\u529f\u7387\u63d0\u9ad8\u4e86 19 \u4e2a\u767e\u5206\u70b9\u3002\u53d7\u6b64\u89c2\u5bdf\u7ed3\u679c\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u63d0\u793a\u6280\u672f SpatialPrompt\uff0c\u8be5\u6280\u672f\u9f13\u52b1 VLM \u4f7f\u7528\u53c2\u8003\u5bf9\u8c61\u4f5c\u4e3a\u89c6\u89c9\u7ebf\u7d22\u6765\u56de\u7b54\u5b9a\u91cf\u7a7a\u95f4\u95ee\u9898\u3002\u901a\u8fc7 SpatialPrompt \u6307\u5bfc VLM \u5728\u5176\u63a8\u7406\u8def\u5f84\u4e2d\u4f7f\u7528\u53c2\u8003\u5bf9\u8c61\uff0cGemini 1.5 Pro\u3001Gemini 1.5 Flash \u548c GPT-4V \u7684\u6210\u529f\u7387\u5206\u522b\u63d0\u9ad8\u4e86 40\u300120 \u548c 30 \u4e2a\u767e\u5206\u70b9\u4ee5\u4e0a\u3002\u6211\u4eec\u5f3a\u8c03\uff0c\u8fd9\u4e9b\u663e\u8457\u7684\u6539\u8fdb\u65e0\u9700\u66f4\u591a\u6570\u636e\u3001\u6a21\u578b\u67b6\u6784\u4fee\u6539\u6216\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u3002||\n", "2409.09721": "|**2024-09-15**|[Finetuning CLIP to Reason about Pairwise Differences](http://arxiv.org/abs/2409.09721)|**[link](https://github.com/dsam99/pc_clip)**|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5982 CLIP \u662f\u901a\u8fc7\u6587\u672c\u548c\u56fe\u50cf\u5bf9\u4e4b\u95f4\u7684\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\u7684\uff0c\u4ece\u800c\u4ea7\u751f\u5bf9\u9f50\u7684\u56fe\u50cf\u548c\u6587\u672c\u5d4c\u5165\uff0c\u8fd9\u5bf9\u8bb8\u591a\u4e0b\u6e38\u4efb\u52a1\u975e\u5e38\u6709\u7528\u3002\u7136\u800c\uff0cCLIP \u7684\u4e00\u4e2a\u663e\u8457\u7f3a\u70b9\u662f\uff0c\u7531\u6b64\u4ea7\u751f\u7684\u5d4c\u5165\u7a7a\u95f4\u4f3c\u4e4e\u7f3a\u4e4f\u5176\u7eaf\u6587\u672c\u66ff\u4ee3\u65b9\u6848\u6240\u5177\u6709\u7684\u4e00\u4e9b\u7ed3\u6784\u3002\u4f8b\u5982\uff0c\u957f\u671f\u4ee5\u6765\uff0c\u4eba\u4eec\u4e00\u76f4\u6ce8\u610f\u5230\u6587\u672c\u5d4c\u5165\u53ef\u4ee5\u4f7f\u7528\u5411\u91cf\u7b97\u672f\u6765\u6ee1\u8db3\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\\emph{\u7c7b\u6bd4}\uff0c\u800c CLIP \u5219\u6ca1\u6709\u8fd9\u79cd\u7279\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u5bf9\u6bd4\u65b9\u5f0f\u539f\u751f\u8bad\u7ec3 CLIP \u7684\u65b9\u6cd5\uff0c\u4ee5\u4fbf\u63a8\u7406\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5dee\u5f02\u3002\u6211\u4eec\u5bf9 CLIP \u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u4ee5\u4fbf\u56fe\u50cf\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5dee\u5f02\u5bf9\u5e94\u4e8e\\emph{\u56fe\u50cf\u5dee\u5f02\u7684\u6587\u672c\u63cf\u8ff0}\uff0c\u6211\u4eec\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf-\u6807\u9898\u914d\u5bf9\u6570\u636e\u96c6\u4e0a\u5408\u6210\u5730\u751f\u6210\u4e86\u8fd9\u4e9b\u63cf\u8ff0\u3002\u6211\u4eec\u9996\u5148\u8bc1\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6309\u7279\u5b9a\u5c5e\u6027\u5bf9\u56fe\u50cf\u8fdb\u884c\u6392\u5e8f\uff08\u4f8b\u5982\uff0c\u5927\u8c61\u6bd4\u732b\u5927\uff09\u65b9\u9762\u4ea7\u751f\u4e86\u663e\u8457\u6539\u8fdb\u7684\u80fd\u529b\uff0c\u8fd9\u5728\u68c0\u7d22\u6216\u6784\u5efa\u57fa\u4e8e\u5c5e\u6027\u7684\u5206\u7c7b\u5668\u4e2d\u975e\u5e38\u6709\u7528\uff0c\u5e76\u4e14\u63d0\u9ad8\u4e86\u8bb8\u591a\u4e0b\u6e38\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u96f6\u6837\u672c\u5206\u7c7b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8fd8\u5b9e\u73b0\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u673a\u5236\uff0c\u6211\u4eec\u5c06\u5176\u79f0\u4e3a\u6bd4\u8f83\u63d0\u793a\uff0c\u5176\u4e2d\u6211\u4eec\u5229\u7528\u5bf9\u611f\u5174\u8da3\u7c7b\u522b\u4e4b\u95f4\u5dee\u5f02\u7684\u6587\u672c\u63cf\u8ff0\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5728\u5206\u7c7b\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5927\u7684\u6027\u80fd\u63d0\u5347\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bf4\u660e\u4e86\u751f\u6210\u7684\u5d4c\u5165\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u9075\u5faa\u66f4\u5927\u7a0b\u5ea6\u7684\u51e0\u4f55\u7279\u6027\uff0c\u4f8b\u5982\u5728\u6587\u672c\u5230\u56fe\u50cf\u7684\u751f\u6210\u4e2d\u3002||\n", "2409.12191": "|**2024-09-18**|[Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution](http://arxiv.org/abs/2409.12191)|**[link](https://github.com/qwenlm/qwen2-vl)**|\u6211\u4eec\u63a8\u51fa\u4e86Qwen2-VL\u7cfb\u5217\uff0c\u8fd9\u662f\u5bf9\u5148\u524dQwen-VL\u6a21\u578b\u7684\u5148\u8fdb\u5347\u7ea7\uff0c\u5b83\u91cd\u65b0\u5b9a\u4e49\u4e86\u89c6\u89c9\u5904\u7406\u4e2d\u4f20\u7edf\u7684\u9884\u5b9a\u5206\u8fa8\u7387\u65b9\u6cd5\u3002Qwen2-VL\u5f15\u5165\u4e86\u6734\u7d20\u52a8\u6001\u5206\u8fa8\u7387\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5c06\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u52a8\u6001\u5904\u7406\u6210\u4e0d\u540c\u6570\u91cf\u7684\u89c6\u89c9\u6807\u8bb0\u3002\u8fd9\u79cd\u65b9\u6cd5\u5141\u8bb8\u6a21\u578b\u751f\u6210\u66f4\u9ad8\u6548\u3001\u66f4\u51c6\u786e\u7684\u89c6\u89c9\u8868\u793a\uff0c\u4e0e\u4eba\u7c7b\u7684\u611f\u77e5\u8fc7\u7a0b\u7d27\u5bc6\u4e00\u81f4\u3002\u8be5\u6a21\u578b\u8fd8\u96c6\u6210\u4e86\u591a\u6a21\u6001\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08M-RoPE\uff09\uff0c\u4fc3\u8fdb\u4e86\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u4e2d\u4f4d\u7f6e\u4fe1\u606f\u7684\u6709\u6548\u878d\u5408\u3002\u6211\u4eec\u91c7\u7528\u7edf\u4e00\u7684\u8303\u5f0f\u6765\u5904\u7406\u56fe\u50cf\u548c\u89c6\u9891\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u3002\u4e3a\u4e86\u63a2\u7d22\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u6f5c\u529b\uff0cQwen2-VL\u7814\u7a76\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7684\u7f29\u653e\u89c4\u5f8b\u3002\u901a\u8fc7\u6269\u5c55\u6a21\u578b\u89c4\u6a21\uff08\u5305\u62ec2B\u30018B\u548c72B\u53c2\u6570\u7684\u7248\u672c\uff09\u548c\u8bad\u7ec3\u6570\u636e\u91cf\uff0cQwen2-VL\u7cfb\u5217\u5b9e\u73b0\u4e86\u6781\u5177\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cQwen2-VL-72B\u6a21\u578b\u5728\u5404\u79cd\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4e0eGPT-4o\u548cClaude3.5-Sonnet\u7b49\u9886\u5148\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u4f18\u4e8e\u5176\u4ed6\u901a\u7528\u6a21\u578b\u3002\u4ee3\u7801\u53ef\u5728\\url{https://github.com/QwenLM/Qwen2-VL}\u83b7\u53d6\u3002||\n", "2409.11941": "|**2024-09-18**|[GauTOAO: Gaussian-based Task-Oriented Affordance of Objects](http://arxiv.org/abs/2409.11941)|null|\u5f53\u60a8\u7684\u673a\u5668\u4eba\u4f7f\u7528\u7075\u5de7\u7684\u624b\u6216\u6293\u624b\u6293\u53d6\u7269\u4f53\u65f6\uff0c\u5b83\u5e94\u8be5\u7406\u89e3\u7269\u4f53\u7684\u9762\u5411\u4efb\u52a1\u7684\u53ef\u64cd\u4f5c\u6027 (TOAO)\uff0c\u56e0\u4e3a\u4e0d\u540c\u7684\u4efb\u52a1\u901a\u5e38\u9700\u8981\u5173\u6ce8\u7269\u4f53\u7684\u7279\u5b9a\u90e8\u5206\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 GauTOAO\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u9ad8\u65af\u7684\u7269\u4f53\u9762\u5411\u4efb\u52a1\u53ef\u64cd\u4f5c\u6027\u6846\u67b6\uff0c\u5b83\u4ee5\u96f6\u6837\u672c\u7684\u65b9\u5f0f\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u7ed9\u5b9a\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7684\u60c5\u51b5\u4e0b\u9884\u6d4b\u7269\u4f53\u4e0a\u4e0e\u53ef\u64cd\u4f5c\u6027\u76f8\u5173\u7684\u533a\u57df\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\uff1a\u201c\u9759\u6001\u76f8\u673a\uff0c\u79fb\u52a8\u7269\u4f53\u201d\uff0c\u4f7f\u673a\u5668\u4eba\u5728\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\u80fd\u591f\u66f4\u597d\u5730\u89c2\u5bdf\u548c\u7406\u89e3\u624b\u4e2d\u7684\u7269\u4f53\u3002GauTOAO \u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u7f3a\u4e4f\u6709\u6548\u7684\u7a7a\u95f4\u5206\u7ec4\uff0c\u5b83\u4f7f\u7528 DINO \u7279\u5f81\u63d0\u53d6\u5b8c\u6574\u7684 3D \u7269\u4f53\u63a9\u7801\u3002\u7136\u540e\uff0c\u8be5\u63a9\u7801\u7528\u4e8e\u6709\u6761\u4ef6\u5730\u67e5\u8be2\u9ad8\u65af\u5206\u5e03\uff0c\u4ece\u800c\u751f\u6210\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u3001\u5728\u7269\u4f53\u4e0a\u7684\u7cbe\u7ec6\u8bed\u4e49\u5206\u5e03\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u63d0\u53d6 TOAO\uff0c\u589e\u5f3a\u673a\u5668\u4eba\u5bf9\u7269\u4f53\u7684\u7406\u89e3\u5e76\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\u3002\u6211\u4eec\u901a\u8fc7\u73b0\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86 GauTOAO \u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5b83\u80fd\u591f\u6cdb\u5316\u5230\u5404\u79cd\u4efb\u52a1\u3002||\n", "2409.11919": "|**2024-09-18**|[LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Foundation Models](http://arxiv.org/abs/2409.11919)|null|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5728\u4f17\u591a\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4e0e\u5176\u4e13\u7528\u6216\u5fae\u8c03\u6a21\u578b\u76f8\u6bd4\uff0c\u5b83\u4eec\u7684\u96f6\u6837\u672c\u80fd\u529b\u53ef\u80fd\u6709\u9650\u3002\u7136\u800c\uff0c\u5fae\u8c03 VLM \u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u5bf9\u6a21\u578b\u67b6\u6784\u548c\u6743\u91cd\u7684\u201c\u767d\u76d2\u201d\u8bbf\u95ee\u6743\u9650\uff0c\u4ee5\u53ca\u8bbe\u8ba1\u5fae\u8c03\u76ee\u6807\u548c\u4f18\u5316\u8d85\u53c2\u6570\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u8fd9\u4e9b\u90fd\u7279\u5b9a\u4e8e\u6bcf\u4e2a VLM \u548c\u4e0b\u6e38\u4efb\u52a1\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 LLM-wrapper\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6765\u63a8\u7406\u5176\u8f93\u51fa\uff0c\u4ee5\u201c\u9ed1\u76d2\u201d\u65b9\u5f0f\u8c03\u6574 VLM \u7684\u65b0\u65b9\u6cd5\u3002\u6211\u4eec\u901a\u8fc7\u6307\u4ee3\u8868\u8fbe\u7406\u89e3 (REC) \u8bc1\u660e\u4e86 LLM-wrapper \u7684\u6709\u6548\u6027\uff0c\u8fd9\u662f\u4e00\u9879\u9700\u8981\u7a7a\u95f4\u548c\u8bed\u4e49\u63a8\u7406\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u5f00\u653e\u8bcd\u6c47\u4efb\u52a1\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u73b0\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e0e\u7ecf\u5178\u5fae\u8c03\u76f8\u6bd4\u83b7\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002||\n", "2409.18125": "|**2024-09-26**|[LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness](http://arxiv.org/abs/2409.18125)|null|\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u8fd1\u671f\u7684\u8fdb\u6b65\u6781\u5927\u5730\u63d0\u9ad8\u4e86\u5176\u5728 2D \u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u548c\u7406\u89e3\u56fe\u50cf\u548c\u89c6\u9891\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21 3D \u89c6\u89c9\u8bed\u8a00\u6570\u636e\u96c6\u548c\u5f3a\u5927\u7684 3D \u7f16\u7801\u5668\uff0c\u5177\u6709 3D \u611f\u77e5\u80fd\u529b\u7684 LMM \u5728 3D \u573a\u666f\u7406\u89e3\u65b9\u9762\u7684\u5f00\u53d1\u4e00\u76f4\u53d7\u5230\u963b\u788d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u6846\u67b6\uff0c\u79f0\u4e3a LLaVA-3D\u3002LLaVA-3D \u5229\u7528 LLaVA \u5f3a\u5927\u7684 2D \u7406\u89e3\u5148\u9a8c\u77e5\u8bc6\uff0c\u6709\u6548\u5730\u5c06 LLaVA \u5e94\u7528\u4e8e 3D \u573a\u666f\u7406\u89e3\uff0c\u800c\u4e0d\u4f1a\u5f71\u54cd\u5176 2D \u7406\u89e3\u80fd\u529b\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u5373 3D Patch\uff0c\u5b83\u5c06 2D CLIP \u56fe\u50cf\u5757\u7279\u5f81\u4e0e\u5176\u5728 3D \u7a7a\u95f4\u4e2d\u7684\u5bf9\u5e94\u4f4d\u7f6e\u8fde\u63a5\u8d77\u6765\u3002\u901a\u8fc7\u5c06 3D Patch \u96c6\u6210\u5230 2D LMM \u4e2d\uff0c\u5e76\u91c7\u7528\u8054\u5408 2D \u548c 3D \u89c6\u89c9\u8bed\u8a00\u6307\u4ee4\u5fae\u8c03\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u4e00\u4e2a\u7528\u4e8e 2D \u56fe\u50cf\u7406\u89e3\u548c 3D \u573a\u666f\u7406\u89e3\u7684\u7edf\u4e00\u67b6\u6784\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728 3D \u89c6\u89c9\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u65f6\uff0cLLaVA-3D \u7684\u6536\u655b\u901f\u5ea6\u6bd4\u73b0\u6709 3D LMM \u5feb 3.5 \u500d\u3002\u6b64\u5916\uff0cLLaVA-3D \u4e0d\u4ec5\u5728\u5404\u79cd 3D \u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u800c\u4e14\u8fd8\u4fdd\u6301\u4e86\u4e0e LLaVA \u76f8\u5f53\u7684 2D \u56fe\u50cf\u7406\u89e3\u548c\u89c6\u89c9\u8bed\u8a00\u5bf9\u8bdd\u80fd\u529b\u3002||\n", "2409.18042": "|**2024-09-26**|[EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions](http://arxiv.org/abs/2409.18042)|null|GPT-4o\uff0c\u4e00\u4e2a\u80fd\u591f\u8fdb\u884c\u5e26\u6709\u4e0d\u540c\u60c5\u611f\u548c\u8bed\u8c03\u7684\u8bed\u97f3\u5bf9\u8bdd\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u6807\u5fd7\u7740\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u4e00\u4e2a\u91cc\u7a0b\u7891\u3002\u7136\u800c\uff0c\u5728\u5f00\u6e90\u793e\u533a\u4e2d\uff0c\u4f7f\u7528\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u8d4b\u4e88\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u611f\u77e5\u548c\u751f\u6210\u56fe\u50cf\u3001\u6587\u672c\u548c\u8bed\u97f3\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u4e8e\u5916\u90e8\u5de5\u5177\u8fdb\u884c\u8bed\u97f3\u5904\u7406\uff0c\u800c\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u4ecd\u7136\u5b58\u5728\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u6709\u9650\u751a\u81f3\u6ca1\u6709\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86EMOVA\uff08\u60c5\u611f\u65e0\u6240\u4e0d\u5728\u7684\u8bed\u97f3\u52a9\u624b\uff09\uff0c\u5b83\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u5907\u7aef\u5230\u7aef\u7684\u8bed\u97f3\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u9886\u5148\u7684\u89c6\u89c9\u8bed\u8a00\u6027\u80fd\u3002\u5229\u7528\u8bed\u4e49-\u58f0\u5b66\u89e3\u8026\u7684\u8bed\u97f3\u6807\u8bb0\u5668\uff0c\u6211\u4eec\u60ca\u5947\u5730\u53d1\u73b0\uff0c\u4e0e\u76f8\u5e94\u7684\u53cc\u6a21\u6001\u5bf9\u9f50\u6a21\u578b\u76f8\u6bd4\uff0c\u591a\u6a21\u6001\u5bf9\u9f50\u53ef\u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u548c\u8bed\u97f3\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u98ce\u683c\u6a21\u5757\uff0c\u7528\u4e8e\u7075\u6d3b\u63a7\u5236\u8bed\u97f3\u98ce\u683c\uff08\u4f8b\u5982\u60c5\u611f\u548c\u97f3\u8c03\uff09\u3002EMOVA\u9996\u6b21\u5728\u89c6\u89c9\u8bed\u8a00\u548c\u8bed\u97f3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u652f\u6301\u5177\u6709\u751f\u52a8\u60c5\u611f\u7684\u591a\u6a21\u6001\u8bed\u97f3\u5bf9\u8bdd\u3002||\n", "2409.18023": "|**2024-09-26**|[DARE: Diverse Visual Question Answering with Robustness Evaluation](http://arxiv.org/abs/2409.18023)|null|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u6269\u5c55\u4e86\u4ec5\u6587\u672c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4ec5\u89c6\u89c9\u6a21\u578b\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u5e76\u4e14\u80fd\u591f\u4ece\u591a\u6a21\u6001\u89c6\u89c9\u6587\u672c\u8f93\u5165\u4e2d\u5b66\u4e60\u548c\u5904\u7406\u3002\u867d\u7136\u73b0\u4ee3 VLM \u5728\u8bb8\u591a\u6807\u51c6\u56fe\u50cf\u5206\u7c7b\u548c\u56fe\u50cf\u6587\u672c\u5339\u914d\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u96be\u4ee5\u5e94\u5bf9\u8bb8\u591a\u5173\u952e\u7684\u89c6\u89c9\u8bed\u8a00 (VL) \u63a8\u7406\u80fd\u529b\uff0c\u4f8b\u5982\u8ba1\u6570\u548c\u7a7a\u95f4\u63a8\u7406\u3002\u6b64\u5916\uff0c\u867d\u7136\u5b83\u4eec\u53ef\u80fd\u5bf9\u6307\u4ee4\u548c/\u6216\u8bc4\u4f30\u534f\u8bae\u7684\u5fae\u5c0f\u53d8\u5316\u975e\u5e38\u8106\u5f31\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u8bc4\u4f30\u5b83\u4eec\u7684\u7a33\u5065\u6027\uff08\u6216\u8005\u66f4\u786e\u5207\u5730\u8bf4\u662f\u7f3a\u4e4f\u7a33\u5065\u6027\uff09\u3002\u4e3a\u4e86\u5c06\u5177\u6709\u6311\u6218\u6027\u7684 VL \u573a\u666f\u4e0e\u5168\u9762\u7684\u7a33\u5065\u6027\u8bc4\u4f30\u76f8\u7ed3\u5408\uff0c\u6211\u4eec\u5f15\u5165\u4e86 DARE\uff0c\u5373\u5177\u6709\u7a33\u5065\u6027\u8bc4\u4f30\u7684\u591a\u6837\u5316\u89c6\u89c9\u95ee\u7b54\uff0c\u8fd9\u662f\u4e00\u4e2a\u7cbe\u5fc3\u521b\u5efa\u548c\u7b56\u5212\u7684\u591a\u9879\u9009\u62e9 VQA \u57fa\u51c6\u3002DARE \u8bc4\u4f30 VLM \u5728\u4e94\u4e2a\u4e0d\u540c\u7c7b\u522b\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u5305\u62ec\u56db\u4e2a\u57fa\u4e8e\u4ee5\u4e0b\u53d8\u5316\u7684\u9762\u5411\u7a33\u5065\u6027\u7684\u8bc4\u4f30\uff1a\u63d0\u793a\u3001\u7b54\u6848\u9009\u9879\u5b50\u96c6\u3001\u8f93\u51fa\u683c\u5f0f\u548c\u6b63\u786e\u7b54\u6848\u7684\u6570\u91cf\u3002\u5728\u4e00\u7cfb\u5217\u5176\u4ed6\u53d1\u73b0\u4e2d\uff0c\u6211\u4eec\u62a5\u544a\u8bf4\uff0c\u6700\u5148\u8fdb\u7684 VLM \u4ecd\u7136\u96be\u4ee5\u56de\u7b54\u5927\u591a\u6570\u7c7b\u522b\u4e2d\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u65e0\u6cd5\u5728\u6d4b\u8bd5\u7684\u7a33\u5065\u6027\u8bc4\u4f30\u4e2d\u59cb\u7ec8\u5982\u4e00\u5730\u63d0\u4f9b\u5176\u5cf0\u503c\u6027\u80fd\u3002\u9009\u9879\u5b50\u96c6\u7684\u6700\u5dee\u60c5\u51b5\u6027\u80fd\u6bd4\u6807\u51c6\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u4f4e 34%\u3002\u8bf8\u5982 LLaVA 1.6 \u548c Idefics2 \u7b49\u5f00\u6e90 VLM \u7684\u7a33\u5065\u6027\u65e0\u6cd5\u4e0e GPT-4 \u548c Gemini \u7b49\u95ed\u6e90\u6a21\u578b\u76f8\u63d0\u5e76\u8bba\uff0c\u4f46\u5373\u4f7f\u662f\u540e\u8005\u4ecd\u7136\u975e\u5e38\u5bb9\u6613\u53d7\u5230\u4e0d\u540c\u53d8\u5316\u7684\u5f71\u54cd\u3002||\n", "2409.17958": "|**2024-09-26**|[The Hard Positive Truth about Vision-Language Compositionality](http://arxiv.org/abs/2409.17958)|**[link](https://github.com/amitakamath/hard_positives)**|\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u5f97\u51fa\u7ed3\u8bba\uff0c\u6211\u4eec\u6700\u597d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u4f8b\u5982 CLIP\uff09\u7f3a\u4e4f\u7ec4\u5408\u6027\u3002\u7ed9\u5b9a\u4e00\u5f20\u56fe\u50cf\uff0c\u8fd9\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u4f1a\u63a2\u6d4b\u6a21\u578b\u4ece\u4e00\u7ec4\u7ec4\u5408\u5e72\u6270\u9879\u4e2d\u8bc6\u522b\u5176\u5173\u8054\u6807\u9898\u7684\u80fd\u529b\u3002\u4f5c\u4e3a\u56de\u5e94\uff0c\u6700\u8fd1\u6d8c\u73b0\u51fa\u5927\u91cf\u63d0\u6848\uff0c\u8868\u660e\u901a\u8fc7\u4f7f\u7528\u5e72\u6270\u9879\u4f5c\u4e3a\u5f3a\u8d1f\u4f8b\u5bf9 CLIP \u8fdb\u884c\u5fae\u8c03\u53ef\u4ee5\u6539\u8fdb\u6a21\u578b\u3002\u6211\u4eec\u7684\u8c03\u67e5\u8868\u660e\uff0c\u8fd9\u4e9b\u6539\u8fdb\u5b9e\u9645\u4e0a\u88ab\u4e25\u91cd\u5938\u5927\u4e86\u2014\u2014\u56e0\u4e3a\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u6ca1\u6709\u63a2\u7a76\u5fae\u8c03\u540e\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5bf9\u5f3a\u6b63\u4f8b\u4fdd\u6301\u4e0d\u53d8\u3002\u901a\u8fc7\u4f7f\u7528 112,382 \u4e2a\u5f3a\u8d1f\u4f8b\u548c\u5f3a\u6b63\u4f8b\u6574\u7406\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u6211\u4eec\u53d1\u73b0\u5305\u542b\u5f3a\u6b63\u4f8b\u4f1a\u4f7f CLIP \u7684\u6027\u80fd\u964d\u4f4e 12.9%\uff0c\u800c\u4eba\u7c7b\u5219\u53ef\u4ee5\u6beb\u4e0d\u8d39\u529b\u5730\u8fbe\u5230 99% \u7684\u51c6\u786e\u7387\u3002\u4f7f\u7528\u5f3a\u8d1f\u4f8b\u5fae\u8c03 CLIP \u4f1a\u5bfc\u81f4\u66f4\u5927\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u9ad8\u8fbe 38.7%\u3002\u57fa\u4e8e\u8fd9\u4e00\u53d1\u73b0\uff0c\u6211\u4eec\u5236\u4f5c\u4e86\u4e00\u4e2a\u5305\u542b 1,775,259 \u4e2a\u56fe\u50cf\u6587\u672c\u7684\u8bad\u7ec3\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u5f3a\u8d1f\u4f8b\u548c\u5f3a\u6b63\u4f8b\u6807\u9898\u3002\u901a\u8fc7\u540c\u65f6\u4f7f\u7528\u4e24\u8005\u8fdb\u884c\u8bad\u7ec3\uff0c\u6211\u4eec\u770b\u5230\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u6027\u80fd\u6709\u6240\u63d0\u9ad8\uff0c\u540c\u65f6\u5f3a\u6b63\u4f8b\u7684\u6027\u80fd\u4e5f\u6709\u6240\u63d0\u9ad8\uff0c\u8fd9\u8868\u660e\u7ec4\u5408\u6027\u5f97\u5230\u4e86\u66f4\u7a33\u5065\u7684\u6539\u8fdb\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u8868\u660e\uff0c\u672a\u6765\u7684\u7814\u7a76\u9700\u8981\u4e25\u683c\u6d4b\u8bd5\u548c\u6539\u8fdb CLIP \u5bf9\u76f8\u5173\u201c\u6b63\u201d\u6982\u5ff5\u4e4b\u95f4\u8bed\u4e49\u5173\u7cfb\u7684\u7406\u89e3\u3002||\n", "2409.17864": "|**2024-09-26**|[A Multimodal Single-Branch Embedding Network for Recommendation in Cold-Start and Missing Modality Scenarios](http://arxiv.org/abs/2409.17864)|null|\u5927\u591a\u6570\u63a8\u8350\u7cfb\u7edf\u91c7\u7528\u534f\u540c\u8fc7\u6ee4 (CF) \u5e76\u6839\u636e\u8fc7\u53bb\u7684\u96c6\u4f53\u4ea4\u4e92\u63d0\u4f9b\u63a8\u8350\u3002\u56e0\u6b64\uff0c\u5f53\u53ef\u7528\u4ea4\u4e92\u5f88\u5c11\u6216\u6ca1\u6709\u4ea4\u4e92\u65f6\uff0cCF \u7b97\u6cd5\u7684\u6027\u80fd\u4f1a\u4e0b\u964d\uff0c\u8fd9\u79cd\u60c5\u51b5\u79f0\u4e3a\u51b7\u542f\u52a8\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4ee5\u524d\u7684\u5de5\u4f5c\u4f9d\u8d56\u4e8e\u5229\u7528\u534f\u4f5c\u6570\u636e\u548c\u7528\u6237\u6216\u9879\u76ee\u8f85\u52a9\u4fe1\u606f\u7684\u6a21\u578b\u3002\u7c7b\u4f3c\u4e8e\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u8fd9\u4e9b\u6a21\u578b\u65e8\u5728\u5c06\u534f\u4f5c\u548c\u5185\u5bb9\u8868\u793a\u7ec4\u5408\u5230\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u63a8\u8350\u6280\u672f\uff0c\u5b83\u4f9d\u8d56\u4e8e\u7528\u4e8e\u63a8\u8350\u7684\u591a\u6a21\u6001\u5355\u5206\u652f\u5d4c\u5165\u7f51\u7edc (SiBraR)\u3002SiBraR \u5229\u7528\u6743\u91cd\u5171\u4eab\uff0c\u5728\u4e0d\u540c\u6a21\u6001\u4e0a\u4f7f\u7528\u76f8\u540c\u7684\u5355\u5206\u652f\u5d4c\u5165\u7f51\u7edc\u5bf9\u4ea4\u4e92\u6570\u636e\u4ee5\u53ca\u591a\u6a21\u6001\u8f85\u52a9\u4fe1\u606f\u8fdb\u884c\u7f16\u7801\u3002\u8fd9\u4f7f\u5f97 SiBraR \u5728\u7f3a\u5c11\u6a21\u6001\u7684\u60c5\u51b5\u4e0b\uff08\u5305\u62ec\u51b7\u542f\u52a8\uff09\u975e\u5e38\u6709\u6548\u3002\u6211\u4eec\u5bf9\u6765\u81ea\u4e09\u4e2a\u4e0d\u540c\u63a8\u8350\u57df\uff08\u97f3\u4e50\u3001\u7535\u5f71\u548c\u7535\u5b50\u5546\u52a1\uff09\u5e76\u63d0\u4f9b\u591a\u6a21\u6001\u5185\u5bb9\u4fe1\u606f\uff08\u97f3\u9891\u3001\u6587\u672c\u3001\u56fe\u50cf\u3001\u6807\u7b7e\u548c\u4ea4\u4e92\uff09\u7684\u5927\u89c4\u6a21\u63a8\u8350\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0cSiBraR \u5728\u51b7\u542f\u52a8\u573a\u666f\u4e0b\u660e\u663e\u4f18\u4e8e CF \u4ee5\u53ca\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u5185\u5bb9\u7684 RS\uff0c\u5e76\u4e14\u5728\u70ed\u542f\u52a8\u573a\u666f\u4e0b\u4e5f\u5177\u6709\u7ade\u4e89\u529b\u3002\u6211\u4eec\u8bc1\u660e\u4e86 SiBraR \u7684\u63a8\u8350\u5728\u7f3a\u5c11\u6a21\u6001\u7684\u60c5\u51b5\u4e0b\u662f\u51c6\u786e\u7684\uff0c\u5e76\u4e14\u8be5\u6a21\u578b\u80fd\u591f\u5c06\u4e0d\u540c\u7684\u6a21\u6001\u6620\u5c04\u5230\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u7684\u540c\u4e00\u533a\u57df\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u6a21\u6001\u5dee\u8ddd\u3002||\n", "2409.17805": "|**2024-09-26**|[Cascade Prompt Learning for Vision-Language Model Adaptation](http://arxiv.org/abs/2409.17805)|**[link](https://github.com/megvii-research/caspl)**|\u63d0\u793a\u5b66\u4e60\u5df2\u6210\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM)\uff08\u5982 CLIP\uff09\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u53ef\u5b66\u4e60\u63d0\u793a\u6807\u8bb0\u4e3b\u8981\u7528\u4e8e\u9002\u5e94\u4efb\u52a1\u7684\u5355\u4e00\u9636\u6bb5\uff08\u5373\uff0c\u8c03\u6574\u63d0\u793a\uff09\uff0c\u5bb9\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\u98ce\u9669\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7ea7\u8054\u63d0\u793a\u5b66\u4e60 CasPL \u6846\u67b6\uff0c\u4f7f\u63d0\u793a\u5b66\u4e60\u80fd\u591f\u540c\u65f6\u670d\u52a1\u4e8e\u901a\u7528\u548c\u7279\u5b9a\u4e13\u4e1a\u77e5\u8bc6\uff08\u5373\uff0c\u589e\u5f3a\u548c\u8c03\u6574\u63d0\u793a\uff09\u3002\u5177\u4f53\u6765\u8bf4\uff0cCasPL \u662f\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u8303\u5f0f\uff0c\u5305\u62ec\u4e24\u4e2a\u4e0d\u540c\u9636\u6bb5\u7684\u53ef\u5b66\u4e60\u63d0\u793a\uff1a\u7b2c\u4e00\u4e2a\u589e\u5f3a\u63d0\u793a\u65e8\u5728\u901a\u8fc7\u4f7f\u7528\u5927\u91cf\u672a\u6807\u8bb0\u7684\u57df\u56fe\u50cf\u5bf9\u9f50\u5176\u9884\u6d4b\u7684 logits\uff0c\u4ece\u9ad8\u7ea7\u66f4\u5927\u7684 CLIP \u6559\u5e08\u6a21\u578b\u4e2d\u63d0\u53d6\u57df\u901a\u7528\u77e5\u8bc6\u3002\u7136\u540e\uff0c\u7b2c\u4e8c\u4e2a\u8c03\u6574\u63d0\u793a\u4e0e\u51bb\u7ed3\u7684\u7b2c\u4e00\u7ec4\u7ea7\u8054\uff0c\u4ee5\u5fae\u8c03\u4e0b\u6e38\u4efb\u52a1\uff0c\u9075\u5faa\u5148\u524d\u7814\u7a76\u4e2d\u91c7\u7528\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0cCasPL \u53ef\u4ee5\u6709\u6548\u5730\u5c06\u57df\u901a\u7528\u548c\u4efb\u52a1\u7279\u5b9a\u8868\u793a\u6355\u83b7\u5230\u660e\u786e\u4e0d\u540c\u7684\u6e10\u8fdb\u63d0\u793a\u7ec4\u4e2d\uff0c\u4ece\u800c\u6f5c\u5728\u5730\u7f13\u89e3\u76ee\u6807\u57df\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cCasPL \u4f5c\u4e3a\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u6a21\u5757\uff0c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u4efb\u4f55\u73b0\u6709\u7684\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u4e2d\u3002CasPL \u5728\u6027\u80fd\u548c\u63a8\u7406\u901f\u5ea6\u4e4b\u95f4\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u597d\u7684\u5e73\u8861\uff0c\u8fd9\u5bf9\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u90e8\u7f72\u8f83\u5c0f\u7684 VLM \u6a21\u578b\u7279\u522b\u6709\u5229\u3002\u4e0e\u5148\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5 PromptSRC \u76f8\u6bd4\uff0cCasPL \u5728 11 \u4e2a\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\uff0c\u57fa\u672c\u7c7b\u522b\u5e73\u5747\u63d0\u9ad8\u4e86 1.85%\uff0c\u65b0\u7c7b\u522b\u5e73\u5747\u63d0\u9ad8\u4e86 3.44%\uff0c\u8c03\u548c\u5e73\u5747\u503c\u5e73\u5747\u63d0\u9ad8\u4e86 2.72%\u3002\u4ee3\u7801\u516c\u5f00\u5730\u5740\uff1ahttps://github.com/megvii-research/CasPL\u3002||\n", "2409.17777": "|**2024-09-26**|[Harnessing Shared Relations via Multimodal Mixup Contrastive Learning for Multimodal Classification](http://arxiv.org/abs/2409.17777)|**[link](https://github.com/RaghavSinghal10/M3CoL)**|\u6df1\u5ea6\u591a\u6a21\u6001\u5b66\u4e60\u901a\u8fc7\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u6765\u6355\u6349\u6a21\u6001\u4e4b\u95f4\u663e\u5f0f\u7684\u4e00\u5bf9\u4e00\u5173\u7cfb\uff0c\u5df2\u7ecf\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6210\u679c\u3002\u7136\u800c\uff0c\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u5f80\u5f80\u8868\u73b0\u51fa\u8d85\u8d8a\u7b80\u5355\u6210\u5bf9\u5173\u8054\u7684\u5171\u4eab\u5173\u7cfb\u3002\u6211\u4eec\u63d0\u51fa\u4e86M3CoL\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u6df7\u5408\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u6355\u6349\u591a\u6a21\u6001\u6570\u636e\u4e2d\u56fa\u6709\u7684\u7ec6\u5fae\u5171\u4eab\u5173\u7cfb\u3002\u6211\u4eec\u7684\u4e3b\u8981\u8d21\u732e\u662f\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u7684\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\uff0c\u5b83\u901a\u8fc7\u5c06\u6765\u81ea\u4e00\u79cd\u6a21\u6001\u7684\u6df7\u5408\u6837\u672c\u4e0e\u5176\u6765\u81ea\u5176\u4ed6\u6a21\u6001\u7684\u5bf9\u5e94\u6837\u672c\u5bf9\u9f50\u6765\u5b66\u4e60\u9c81\u68d2\u7684\u8868\u793a\uff0c\u4ece\u800c\u6355\u6349\u5b83\u4eec\u4e4b\u95f4\u7684\u5171\u4eab\u5173\u7cfb\u3002\u5bf9\u4e8e\u591a\u6a21\u6001\u5206\u7c7b\u4efb\u52a1\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u878d\u5408\u6a21\u5757\u4e0e\u5355\u6a21\u6001\u9884\u6d4b\u6a21\u5757\u76f8\u7ed3\u5408\uff0c\u4ee5\u4fbf\u5728\u8bad\u7ec3\u671f\u95f4\u8fdb\u884c\u8f85\u52a9\u76d1\u7763\uff0c\u5e76\u8f85\u4ee5\u6211\u4eec\u63d0\u51fa\u7684\u57fa\u4e8e\u6df7\u5408\u7684\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\u3002\u901a\u8fc7\u5bf9\u4e0d\u540c\u6570\u636e\u96c6\uff08N24News\u3001ROSMAP\u3001BRCA \u548c Food-101\uff09\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 M3CoL \u53ef\u4ee5\u6709\u6548\u5730\u6355\u6349\u5171\u4eab\u7684\u591a\u6a21\u6001\u5173\u7cfb\u5e76\u5728\u4e0d\u540c\u9886\u57df\u6cdb\u5316\u3002\u5b83\u5728 N24News\u3001ROSMAP \u548c BRCA \u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u5728 Food-101 \u4e0a\u53d6\u5f97\u4e86\u53ef\u6bd4\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u7a81\u51fa\u4e86\u5b66\u4e60\u5171\u4eab\u5173\u7cfb\u5bf9\u4e8e\u9c81\u68d2\u7684\u591a\u6a21\u6001\u5b66\u4e60\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5f00\u8f9f\u4e86\u6709\u5e0c\u671b\u7684\u9014\u5f84\u3002||\n", "2409.17727": "|**2024-09-26**|[Robotic-CLIP: Fine-tuning CLIP on Action Data for Robotic Applications](http://arxiv.org/abs/2409.17727)|null|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e3a\u5404\u79cd\u673a\u5668\u4eba\u5e94\u7528\u63d0\u53d6\u6709\u610f\u4e49\u7684\u7279\u5f81\u65b9\u9762\u53d1\u6325\u4e86\u5173\u952e\u4f5c\u7528\u3002\u5176\u4e2d\uff0c\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3 (CLIP) \u5e7f\u6cdb\u5e94\u7528\u4e8e\u9700\u8981\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u7684\u673a\u5668\u4eba\u4efb\u52a1\u3002\u7136\u800c\uff0cCLIP \u4ec5\u5728\u4e0e\u6587\u672c\u63d0\u793a\u914d\u5bf9\u7684\u9759\u6001\u56fe\u50cf\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5c1a\u672a\u5b8c\u5168\u9002\u5e94\u6d89\u53ca\u52a8\u6001\u52a8\u4f5c\u7684\u673a\u5668\u4eba\u4efb\u52a1\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 Robotic-CLIP \u6765\u589e\u5f3a\u673a\u5668\u4eba\u7684\u611f\u77e5\u80fd\u529b\u3002\u6211\u4eec\u9996\u5148\u6536\u96c6\u548c\u6807\u8bb0\u5927\u89c4\u6a21\u52a8\u4f5c\u6570\u636e\uff0c\u7136\u540e\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u5728 309,433 \u4e2a\u89c6\u9891\uff08\u7ea6 740 \u4e07\u5e27\uff09\u7684\u52a8\u4f5c\u6570\u636e\u4e0a\u5fae\u8c03 CLIP\uff0c\u6784\u5efa\u6211\u4eec\u7684 Robotic-CLIP\u3002\u901a\u8fc7\u5229\u7528\u52a8\u4f5c\u6570\u636e\uff0cRobotic-CLIP \u7ee7\u627f\u4e86 CLIP \u5f3a\u5927\u7684\u56fe\u50cf\u6027\u80fd\uff0c\u540c\u65f6\u83b7\u5f97\u4e86\u7406\u89e3\u673a\u5668\u4eba\u73af\u5883\u4e2d\u52a8\u4f5c\u7684\u80fd\u529b\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684 Robotic-CLIP \u5728\u5404\u79cd\u8bed\u8a00\u9a71\u52a8\u7684\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5176\u4ed6\u57fa\u4e8e CLIP \u7684\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5c55\u793a\u4e86 Robotic-CLIP \u5728\u73b0\u5b9e\u4e16\u754c\u6293\u53d6\u5e94\u7528\u4e2d\u7684\u5b9e\u9645\u6709\u6548\u6027\u3002||\n", "2409.17692": "|**2024-09-26**|[MIO: A Foundation Model on Multimodal Tokens](http://arxiv.org/abs/2409.17692)|null|\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001token\u7684\u65b0\u578b\u57fa\u7840\u6a21\u578bMIO\uff0c\u5b83\u80fd\u591f\u4ee5\u7aef\u5230\u7aef\u3001\u81ea\u56de\u5f52\u7684\u65b9\u5f0f\u7406\u89e3\u548c\u751f\u6210\u8bed\u97f3\u3001\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u3002\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MM-LLM\uff09\u51ed\u501f\u5176\u591a\u529f\u80fd\u6027\u63a8\u52a8\u4e86\u4eba\u5de5\u667a\u80fd\u901a\u7528\u6027\u7684\u8fdb\u6b65\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u7f3a\u4e4f\u771f\u6b63\u7684\u4efb\u610f\u6a21\u6001\u4e4b\u95f4\u7406\u89e3\u548c\u751f\u6210\u7684\u80fd\u529b\u3002\u6700\u8fd1\uff0cGPT-4o\u7684\u53d1\u5e03\u5c55\u793a\u4e86\u4efb\u610f\u6a21\u6001\u4e4b\u95f4LLM\u5728\u5904\u7406\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u5b83\u80fd\u591f\u5b9e\u73b0\u56fe\u50cf\u3001\u8bed\u97f3\u548c\u6587\u672c\u4e4b\u95f4\u7684\u5168\u5411\u8f93\u5165\u548c\u8f93\u51fa\u3002\u7136\u800c\uff0c\u5b83\u662f\u4e00\u4e2a\u95ed\u6e90\u6a21\u578b\uff0c\u5e76\u4e14\u4e0d\u652f\u6301\u751f\u6210\u591a\u6a21\u6001\u4ea4\u9519\u5e8f\u5217\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86MIO\uff0c\u5b83\u4f7f\u7528\u56e0\u679c\u591a\u6a21\u6001\u5efa\u6a21\u5728\u56db\u79cd\u6a21\u6001\u7684\u79bb\u6563token\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002MIO\u7ecf\u5386\u4e86\u56db\u4e2a\u8bad\u7ec3\u9636\u6bb5\uff1a\uff081\uff09\u5bf9\u9f50\u9884\u8bad\u7ec3\uff0c\uff082\uff09\u4ea4\u9519\u9884\u8bad\u7ec3\uff0c\uff083\uff09\u8bed\u97f3\u589e\u5f3a\u9884\u8bad\u7ec3\uff0c\u4ee5\u53ca\uff084\uff09\u9488\u5bf9\u4e0d\u540c\u6587\u672c\u3001\u89c6\u89c9\u548c\u8bed\u97f3\u4efb\u52a1\u7684\u7efc\u5408\u76d1\u7763\u5fae\u8c03\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4e4b\u524d\u7684\u53cc\u6a21\u6001\u57fa\u7ebf\u3001\u4efb\u610f\u6a21\u6001\u4e4b\u95f4\u6a21\u578b\u57fa\u7ebf\uff0c\u751a\u81f3\u662f\u7279\u5b9a\u6a21\u6001\u57fa\u7ebf\u76f8\u6bd4\uff0cMIO\u8868\u73b0\u51fa\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u66f4\u80dc\u4e00\u7b79\u3002\u6b64\u5916\uff0cMIO\u8fd8\u5c55\u793a\u4e86\u5176\u4efb\u610f\u6a21\u6001\u4e4b\u95f4\u529f\u80fd\u6240\u5e26\u6765\u7684\u9ad8\u7ea7\u80fd\u529b\uff0c\u4f8b\u5982\u4ea4\u9519\u89c6\u9891\u6587\u672c\u751f\u6210\u3001\u89c6\u89c9\u601d\u7ef4\u94fe\u63a8\u7406\u3001\u89c6\u89c9\u6307\u5357\u751f\u6210\u3001\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u7b49\u3002||\n", "2409.17634": "|**2024-09-26**|[P4Q: Learning to Prompt for Quantization in Visual-language Models](http://arxiv.org/abs/2409.17634)|null|\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u5404\u79cd\u89c6\u89c9\u548c\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4f46\u7531\u4e8e\u5176\u5bf9\u8bad\u7ec3\u6837\u672c\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u5de8\u5927\u9700\u6c42\uff0c\u5c06VLM\u90e8\u7f72\u5230\u4e0b\u6e38\u5e94\u7528\u5e73\u53f0\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u5bf9VLM\u8fdb\u884c\u5fae\u8c03\u548c\u91cf\u5316\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u6837\u672c\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u8fd9\u65b9\u9762\u7684\u7814\u7a76\u3002\u91cf\u5316\u9886\u57df\u76ee\u524d\u5b58\u5728\u4e24\u79cd\u4e3b\u8981\u8303\u5f0f\uff1a\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u53ef\u4ee5\u6709\u6548\u5730\u91cf\u5316\u5927\u89c4\u6a21VLM\uff0c\u4f46\u4f1a\u4ea7\u751f\u5de8\u5927\u7684\u8bad\u7ec3\u6210\u672c\uff1b\u800c\u4f4e\u6bd4\u7279\u4f4d\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u5219\u5b58\u5728\u660e\u663e\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5e73\u8861\u5fae\u8c03\u548c\u91cf\u5316\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3a\u201c\u91cf\u5316\u63d0\u793a\u201d\uff08P4Q\uff09\uff0c\u5176\u4e2d\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u67b6\u6784\uff0c\u5229\u7528\u5bf9\u6bd4\u635f\u5931\u76d1\u7763\u6765\u589e\u5f3aPTQ\u6a21\u578b\u7684\u8bc6\u522b\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u51cf\u5c11\u7531\u4f4e\u6bd4\u7279\u4f4d\u91cf\u5316\u5f15\u8d77\u7684\u56fe\u50cf\u7279\u5f81\u548c\u6587\u672c\u7279\u5f81\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5176\u65b9\u6cd5\u662f\u57fa\u4e8e\u53ef\u5b66\u4e60\u7684\u63d0\u793a\u6765\u91cd\u7ec4\u6587\u672c\u8868\u793a\uff0c\u5e76\u4f7f\u7528\u4f4e\u6bd4\u7279\u4f4d\u9002\u914d\u5668\u91cd\u65b0\u8c03\u6574\u56fe\u50cf\u548c\u6587\u672c\u7279\u5f81\u7684\u5206\u5e03\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u9884\u6d4b\u7684\u84b8\u998f\u635f\u5931\uff0c\u4ee5\u4f7f\u7528\u5168\u7cbe\u5ea6\u6559\u5e08\u6a21\u578b\u5bf9\u91cf\u5316\u6a21\u578b\u8fdb\u884c\u84b8\u998f\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684P4Q\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u751a\u81f3\u53ef\u4ee5\u8fbe\u5230\u4e0e\u5176\u5168\u7cbe\u5ea6\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u76848\u4f4dP4Q\u7406\u8bba\u4e0a\u53ef\u4ee5\u5c06CLIP-ViT/B-32\u538b\u7f294\u500d\uff0c\u540c\u65f6\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u5b9e\u73b066.94%\u7684Top-1\u51c6\u786e\u7387\uff0c\u6bd4\u53ef\u5b66\u4e60\u63d0\u793a\u5fae\u8c03\u7684\u5168\u7cbe\u5ea6\u6a21\u578b\u9ad8\u51fa2.24%\uff0c\u800c\u989d\u5916\u7684\u53c2\u6570\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u3002||\n", "2409.18674": "|**2024-09-27**|[Image-guided topic modeling for interpretable privacy classification](http://arxiv.org/abs/2409.18674)|**[link](https://github.com/idiap/itm)**|\u7528\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u672f\u8bed\u9884\u6d4b\u548c\u89e3\u91ca\u56fe\u50cf\u4e2d\u5305\u542b\u7684\u9690\u79c1\u4fe1\u606f\u662f\u4e00\u9879\u590d\u6742\u4e14\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\u7684\u7684\u4efb\u52a1\u3002\u5373\u4f7f\u5bf9\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u8bf4\uff0c\u8fd9\u9879\u4efb\u52a1\u4e5f\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u4e86\u4fc3\u8fdb\u5bf9\u9690\u79c1\u51b3\u7b56\u7684\u7406\u89e3\uff0c\u6211\u4eec\u5efa\u8bae\u6839\u636e\u4e00\u7ec4\u81ea\u7136\u8bed\u8a00\u5185\u5bb9\u63cf\u8ff0\u7b26\u6765\u9884\u6d4b\u56fe\u50cf\u9690\u79c1\u3002\u8fd9\u4e9b\u5185\u5bb9\u63cf\u8ff0\u7b26\u4e0e\u9690\u79c1\u5206\u6570\u76f8\u5173\u8054\uff0c\u8fd9\u4e9b\u5206\u6570\u53cd\u6620\u4e86\u4eba\u4eec\u5982\u4f55\u770b\u5f85\u56fe\u50cf\u5185\u5bb9\u3002\u6211\u4eec\u4f7f\u7528\u6211\u4eec\u65b0\u9896\u7684\u56fe\u50cf\u5f15\u5bfc\u4e3b\u9898\u5efa\u6a21\uff08ITM\uff09\u65b9\u6cd5\u751f\u6210\u63cf\u8ff0\u7b26\u3002ITM \u901a\u8fc7\u591a\u6a21\u6001\u5bf9\u9f50\uff0c\u5229\u7528\u6765\u81ea\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u4fe1\u606f\u548c\u56fe\u50cf\u6587\u672c\u63cf\u8ff0\u3002\u6211\u4eec\u4f7f\u7528 ITM \u751f\u6210\u7684\u63cf\u8ff0\u7b26\u6765\u5b66\u4e60\u9690\u79c1\u9884\u6d4b\u5668 Priv\u00d7ITM\uff0c\u5176\u51b3\u7b56\u5728\u8bbe\u8ba1\u4e0a\u662f\u53ef\u89e3\u91ca\u7684\u3002\u6211\u4eec\u7684 Priv\u00d7ITM \u5206\u7c7b\u5668\u5728\u51c6\u786e\u7387\u65b9\u9762\u6bd4\u53c2\u8003\u7684\u53ef\u89e3\u91ca\u65b9\u6cd5\u9ad8\u51fa 5 \u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u4e14\u6027\u80fd\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u4e0d\u53ef\u89e3\u91ca\u6a21\u578b\u76f8\u5f53\u3002||\n", "2409.20429": "|**2024-09-30**|[HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding](http://arxiv.org/abs/2409.20429)|null|\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u5728\u8bb8\u591a\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u4e86\u975e\u51e1\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u4ecd\u7136\u53d7\u5230\u591a\u6a21\u6001\u5e7b\u89c9\u7684\u5f71\u54cd\uff0c\u8fd9\u610f\u5473\u7740\u4f1a\u751f\u6210\u8fdd\u53cd\u56fe\u50cf\u5185\u5bb9\u7684\u5bf9\u8c61\u6216\u5185\u5bb9\u3002\u8bb8\u591a\u73b0\u6709\u5de5\u4f5c\u901a\u8fc7\u76f4\u63a5\u5224\u65ad\u4e00\u4e2a\u5bf9\u8c61\u662f\u5426\u5b58\u5728\u4e8e\u56fe\u50cf\u4e2d\u6765\u68c0\u6d4b\u5e7b\u89c9\uff0c\u800c\u5ffd\u7565\u4e86\u5bf9\u8c61\u4e0e\u8bed\u4e49\u4e4b\u95f4\u7684\u5173\u8054\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u89c6\u89c9\u589e\u5f3a\u60e9\u7f5a\u89e3\u7801\u7684\u5206\u5c42\u53cd\u9988\u5b66\u4e60 (HELPD)\u3002\u8be5\u6846\u67b6\u5728\u5bf9\u8c61\u548c\u53e5\u5b50\u8bed\u4e49\u5c42\u9762\u90fd\u7eb3\u5165\u4e86\u5e7b\u89c9\u53cd\u9988\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5373\u4f7f\u8bad\u7ec3\u7a0b\u5ea6\u4e0d\u9ad8\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e5f\u53ef\u4ee5\u51cf\u5c11 15% \u4ee5\u4e0a\u7684\u5e7b\u89c9\u3002\u540c\u65f6\uff0cHELPD \u6839\u636e\u56fe\u50cf\u6ce8\u610f\u529b\u7a97\u53e3\u60e9\u7f5a\u8f93\u51fa logits\uff0c\u4ee5\u907f\u514d\u8fc7\u5ea6\u53d7\u751f\u6210\u6587\u672c\u7684\u5f71\u54cd\u3002HELPD \u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u4efb\u4f55 LVLMs \u4e2d\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u591a\u4e2a\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4ea7\u751f\u4e86\u826f\u597d\u7684\u7ed3\u679c\u3002\u5b83\u6709\u6548\u5730\u51cf\u8f7b\u4e86\u4e0d\u540c LVLMs \u7684\u5e7b\u89c9\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u5b83\u4eec\u7684\u6587\u672c\u751f\u6210\u8d28\u91cf\u3002||\n", "2409.20353": "|**2024-09-30**|[CableInspect-AD: An Expert-Annotated Anomaly Detection Dataset](http://arxiv.org/abs/2409.20353)|**[link](https://github.com/mila-iqia/cableinspect-ad-code)**|\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6b63\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u3002\u7136\u800c\uff0c\u5173\u4e8e\u5176\u5bf9\u7279\u5b9a\u548c\u5173\u952e\u5e94\u7528\u7684\u53ef\u8fc1\u79fb\u6027\u7684\u7cfb\u7edf\u7814\u7a76\u5728\u7814\u7a76\u6587\u732e\u4e2d\u5374\u9c9c\u6709\u62a5\u9053\u3002\u4e00\u4e2a\u91cd\u8981\u7684\u4f8b\u5b50\u662f\u7528\u4e8e\u673a\u5668\u4eba\u7535\u529b\u7ebf\u5de1\u68c0\u7684\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b (VAD)\u3002\u867d\u7136\u73b0\u6709\u7684 VAD \u65b9\u6cd5\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u73b0\u5b9e\u573a\u666f\u4e2d\u5b58\u5728\u7740\u5f53\u524d\u6570\u636e\u96c6\u65e0\u6cd5\u6355\u6349\u5230\u7684\u5404\u79cd\u610f\u5916\u5f02\u5e38\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63a8\u51fa\u4e86 $\\textit{CableInspect-AD}$\uff0c\u8fd9\u662f\u4e00\u4e2a\u7531\u52a0\u62ff\u5927\u516c\u7528\u4e8b\u4e1a\u516c\u53f8 Hydro-Qu\\'ebec \u7684\u9886\u57df\u4e13\u5bb6\u521b\u5efa\u548c\u6807\u6ce8\u7684\u9ad8\u8d28\u91cf\u3001\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b\u5177\u6709\u6311\u6218\u6027\u7684\u73b0\u5b9e\u4e16\u754c\u5f02\u5e38\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u6db5\u76d6\u4e86\u4e0d\u540c\u4e25\u91cd\u7a0b\u5ea6\u7684\u7f3a\u9677\u3002\u4e3a\u4e86\u89e3\u51b3\u4e3a\u8bbe\u7f6e\u68c0\u6d4b\u9608\u503c\u800c\u6536\u96c6\u5404\u79cd\u5f02\u5e38\u548c\u6b63\u5e38\u6837\u672c\u7684\u6311\u6218\uff0c\u6211\u4eec\u5efa\u8bae\u5bf9\u8457\u540d\u7684 PatchCore \u7b97\u6cd5\u8fdb\u884c\u589e\u5f3a\u3002\u8fd9\u79cd\u589e\u5f3a\u4f7f\u5176\u80fd\u591f\u5728\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4f7f\u7528\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4ea4\u53c9\u9a8c\u8bc1\u7684\u7efc\u5408\u8bc4\u4f30\u65b9\u6848\uff0c\u4ee5\u8bc4\u4f30\u6a21\u578b\u7684\u6027\u80fd\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u6211\u4eec\u7684 $\\textit{Enhanced-PatchCore}$ \u5728\u5c11\u6837\u672c\u548c\u591a\u6837\u672c\u68c0\u6d4b\u65b9\u9762\u7684\u6027\u80fd\uff0c\u4ee5\u53ca\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u68c0\u6d4b\u65b9\u9762\u7684\u6027\u80fd\u3002\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u5f88\u6709\u524d\u666f\uff0c\u4f46\u5b83\u4eec\u96be\u4ee5\u68c0\u6d4b\u6240\u6709\u5f02\u5e38\uff0c\u8fd9\u7a81\u51fa\u4e86\u8be5\u6570\u636e\u96c6\u4f5c\u4e3a\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u5bf9\u66f4\u5e7f\u6cdb\u7814\u7a76\u7fa4\u4f53\u7684\u4ef7\u503c\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://mila-iqia.github.io/cableinspect-ad/\u3002||\n", "2409.20018": "|**2024-09-30**|[Visual Context Window Extension: A New Perspective for Long Video Understanding](http://arxiv.org/abs/2409.20018)|null|\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u5728\u77ed\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5e94\u7528\u4e8e\u957f\u89c6\u9891\u7406\u89e3\u65f6\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u5efa\u6a21\u957f\u6587\u672c\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u73b0\u6709\u5de5\u4f5c\u8bd5\u56fe\u901a\u8fc7\u5728\u8bad\u7ec3\u671f\u95f4\u5f15\u5165\u957f\u89c6\u9891-\u6587\u672c\u5bf9\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u548c\u6570\u636e\u8d44\u6e90\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ece\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u89d2\u5ea6\u6765\u5e94\u5bf9\u957f\u89c6\u9891\u7406\u89e3\u7684\u6311\u6218\uff0c\u65e8\u5728\u5c06 LMM \u5e94\u7528\u4e8e\u957f\u89c6\u9891\u4efb\u52a1\uff0c\u800c\u65e0\u9700\u5728\u957f\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u91cd\u65b0\u8bad\u7ec3\u3002\u6211\u4eec\u9996\u5148\u6df1\u5165\u5206\u6790\u4e86\u9884\u8bad\u7ec3\u7684 LMM \u96be\u4ee5\u7406\u89e3\u957f\u89c6\u9891\u5185\u5bb9\u7684\u539f\u56e0\uff0c\u53d1\u73b0\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\u4e4b\u95f4\u7684\u5dee\u5f02\u5bfc\u81f4\u89c6\u89c9\u548c\u8bed\u8a00\u6807\u8bb0\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e0d\u540c\uff0c\u8fd9\u4f7f\u5f97\u76f4\u63a5\u6269\u5c55\u89c6\u89c9\u6807\u8bb0\u4ee5\u5339\u914d\u8bed\u8a00\u4e0a\u4e0b\u6587\u7a97\u53e3\u53d8\u5f97\u56f0\u96be\u3002\u57fa\u4e8e\u6b64\uff0c\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u6269\u5c55\u89c6\u89c9\u4e0a\u4e0b\u6587\u7a97\u53e3\u6765\u8c03\u6574 LMM \u4ee5\u9002\u5e94\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\uff0c\u4ece\u800c\u65e0\u9700\u5728\u5927\u578b\u957f\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u91cd\u65b0\u8bad\u7ec3\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u51cf\u5c11\u957f\u5e8f\u5217\u5bfc\u81f4\u7684\u5927\u91cf\u5185\u5b58\u6d88\u8017\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u6c60\u5316\u63a8\u7406\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u9009\u62e9\u6027\u5730\u8c03\u6574\u5e27\u5d4c\u5165\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c\u5728\u4fdd\u7559\u91cd\u8981\u7a7a\u95f4\u4fe1\u606f\u7684\u540c\u65f6\u51cf\u5c11\u89c6\u89c9\u6807\u8bb0\u7684\u6570\u91cf\u3002\u5728\u591a\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u968f\u7740\u89c6\u9891\u5e27\u6570\u91cf\u7684\u589e\u52a0\u800c\u6301\u7eed\u63d0\u9ad8\u6027\u80fd\u3002\u5728 MLVU \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e GPT-4o\uff0c\u5373\u4f7f\u6211\u4eec\u7684\u6a21\u578b\u5927\u5c0f\u53ea\u6709 7B\u3002\u6b64\u5916\uff0c\u5728 256 \u5e27\u8bbe\u7f6e\u4e2d\uff0c\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c11\u4e86\u5927\u7ea6 45%\uff0c\u800c\u4e0d\u4f1a\u5bfc\u81f4\u4efb\u4f55\u6027\u80fd\u635f\u5931\u3002||\n", "2409.20012": "|**2024-09-30**|[Towards Robust Multimodal Sentiment Analysis with Incomplete Data](http://arxiv.org/abs/2409.20012)|**[link](https://github.com/haoyu-ha/lnln)**|\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\uff08MSA\uff09\u9886\u57df\u6700\u8fd1\u51fa\u73b0\u4e86\u4e00\u4e2a\u65b0\u5174\u65b9\u5411\uff0c\u65e8\u5728\u89e3\u51b3\u6570\u636e\u4e0d\u5b8c\u6574\u6027\u95ee\u9898\u3002\u8ba4\u8bc6\u5230\u8bed\u8a00\u6a21\u6001\u901a\u5e38\u5305\u542b\u5bc6\u96c6\u7684\u60c5\u611f\u4fe1\u606f\uff0c\u6211\u4eec\u5c06\u5176\u89c6\u4e3a\u4e3b\u8981\u6a21\u6001\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u8bed\u8a00\u4e3b\u5bfc\u6297\u566a\u5b66\u4e60\u7f51\u7edc\uff08LNLN\uff09\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5065\u7684MSA\u3002\u6240\u63d0\u51fa\u7684LNLN\u5177\u6709\u4e3b\u8981\u6a21\u6001\u6821\u6b63\uff08DMC\uff09\u6a21\u5757\u548c\u57fa\u4e8e\u4e3b\u8981\u6a21\u6001\u7684\u591a\u6a21\u6001\u5b66\u4e60\uff08DMML\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u786e\u4fdd\u4e3b\u8981\u6a21\u6001\u8868\u793a\u7684\u8d28\u91cf\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u5404\u79cd\u566a\u58f0\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u9664\u4e86\u65b9\u6cd5\u8bba\u8bbe\u8ba1\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u5728\u968f\u673a\u6570\u636e\u7f3a\u5931\u573a\u666f\u4e0b\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u9a8c\uff0c\u5728\u51e0\u4e2a\u6d41\u884c\u7684\u6570\u636e\u96c6\uff08\u4f8b\u5982MOSI\u3001MOSEI\u548cSIMS\uff09\u4e0a\u4f7f\u7528\u4e86\u591a\u6837\u5316\u4e14\u6709\u610f\u4e49\u7684\u8bbe\u7f6e\uff0c\u4e0e\u6587\u732e\u4e2d\u7684\u73b0\u6709\u8bc4\u4f30\u76f8\u6bd4\uff0c\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u7edf\u4e00\u6027\u3001\u900f\u660e\u5ea6\u548c\u516c\u5e73\u6027\u3002\u6839\u636e\u7ecf\u9a8c\uff0cLNLN\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\uff0c\u5728\u8fd9\u4e9b\u5177\u6709\u6311\u6218\u6027\u548c\u5e7f\u6cdb\u7684\u8bc4\u4f30\u6307\u6807\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002||\n", "2409.19846": "|**2024-09-30**|[Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels](http://arxiv.org/abs/2409.19846)|null|\u50cf CLIP \u8fd9\u6837\u7684\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u7ea7\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\uff0c\u5728\u8bc6\u522b\u7269\u4f53\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u8bed\u4e49\u5206\u5272\u7b49\u50cf\u7d20\u7ea7\u8bc6\u522b\u4efb\u52a1\u4e2d\u5374\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u4efb\u52a1\u8fd8\u9700\u8981\u7406\u89e3\u7269\u4f53\u7684\u4f4d\u7f6e\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a PixelCLIP \u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u4ece SAM \u548c DINO \u7b49\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u751f\u6210\u7684\u672a\u6807\u8bb0\u56fe\u50cf\u548c\u63a9\u7801\u6765\u6307\u5bfc\u6a21\u578b\u8bc6\u522b\u7269\u4f53\u7684\u4f4d\u7f6e\uff0c\u4ece\u800c\u4f7f CLIP \u56fe\u50cf\u7f16\u7801\u5668\u9002\u5e94\u50cf\u7d20\u7ea7\u7406\u89e3\u3002\u4e3a\u4e86\u89e3\u51b3\u5728\u6ca1\u6709\u8bed\u4e49\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u5229\u7528\u63a9\u7801\u7684\u6311\u6218\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4f7f\u7528\u53ef\u5b66\u4e60\u7c7b\u540d\u7684\u5728\u7ebf\u805a\u7c7b\u7b97\u6cd5\u6765\u83b7\u53d6\u4e00\u822c\u7684\u8bed\u4e49\u6982\u5ff5\u3002PixelCLIP \u5728\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u65b9\u9762\u6bd4 CLIP \u663e\u793a\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e14\u4e0e\u5b57\u5e55\u76d1\u7763\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://cvlab-kaist.github.io/PixelCLIP||\n", "2409.19806": "|**2024-09-29**|[PALM: Few-Shot Prompt Learning for Audio Language Models](http://arxiv.org/abs/2409.19806)|null|\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff08ALM\uff09\u6700\u8fd1\u5728\u96f6\u6837\u672c\u97f3\u9891\u8bc6\u522b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u5176\u7075\u611f\u6765\u81ea\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u8fdb\u6b65\uff0c\u5c06\u97f3\u9891\u6ce2\u5f62\u7684\u7279\u5f81\u4e0e\u7279\u5b9a\u7c7b\u522b\u7684\u6587\u672c\u63d0\u793a\u7279\u5f81\u76f8\u5339\u914d\u3002\u9274\u4e8e\u96f6\u6837\u672c\u6027\u80fd\u5bf9\u4eba\u5de5\u8bbe\u8ba1\u6587\u672c\u63d0\u793a\u9009\u62e9\u7684\u654f\u611f\u6027\uff0c\u5df2\u7ecf\u4e3aVLM\u5f00\u53d1\u4e86\u8bb8\u591a\u63d0\u793a\u5b66\u4e60\u6280\u672f\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728ALM\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u63d0\u793a\u5b66\u4e60\u201d\uff08PALM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f18\u5316\u4e86\u6587\u672c\u7f16\u7801\u5668\u5206\u652f\u7684\u7279\u5f81\u7a7a\u95f4\u3002\u4e0e\u5728\u8f93\u5165\u7a7a\u95f4\u4e2d\u5de5\u4f5c\u7684\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8bad\u7ec3\u6548\u7387\u3002\u6211\u4eec\u572811\u4e2a\u97f3\u9891\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u6db5\u76d6\u4e86\u5404\u79cd\u8bed\u97f3\u5904\u7406\u4efb\u52a1\uff0c\u5e76\u5728\u5c11\u6837\u672c\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u5c06\u7ed3\u679c\u4e0e\u4e09\u4e2a\u57fa\u7ebf\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8ba1\u7b97\u91cf\u8f83\u5c0f\u7684\u540c\u65f6\uff0c\u5176\u6027\u80fd\u4e0e\u5176\u4ed6\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u3002\u4ee3\u7801\u53ef\u5728https://asif-hanif.github.io/palm/\u83b7\u53d6\u3002||\n", "2409.19696": "|**2024-09-29**|[Vision-Language Models are Strong Noisy Label Detectors](http://arxiv.org/abs/2409.19696)|**[link](https://github.com/HotanLee/DeFT)**|\u6700\u8fd1\u5173\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u7684\u7814\u7a76\u8868\u660e\uff0c\u5176\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u83b7\u53d6\u51c6\u786e\u6807\u8bb0\u6570\u636e\u7684\u6311\u6218\u7ed9\u5fae\u8c03\u8fc7\u7a0b\u5e26\u6765\u4e86\u91cd\u5927\u969c\u788d\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DeFT \u7684\u53bb\u566a\u5fae\u8c03\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9002\u5e94\u6027\u8bad\u7ec3\u3002DeFT \u5229\u7528\u5728\u6570\u767e\u4e07\u4e2a\u8f85\u52a9\u56fe\u50cf-\u6587\u672c\u5bf9\u4e0a\u9884\u8bad\u7ec3\u7684\u6587\u672c\u548c\u89c6\u89c9\u7279\u5f81\u7684\u9c81\u68d2\u5bf9\u9f50\u6765\u7b5b\u9009\u566a\u58f0\u6807\u7b7e\u3002\u6240\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u5b66\u4e60\u6bcf\u4e2a\u7c7b\u522b\u7684\u6b63\u8d1f\u6587\u672c\u63d0\u793a\u6765\u5efa\u7acb\u566a\u58f0\u6807\u7b7e\u68c0\u6d4b\u5668\u3002\u6b63\u63d0\u793a\u65e8\u5728\u63ed\u793a\u8be5\u7c7b\u522b\u7684\u72ec\u7279\u7279\u5f81\uff0c\u800c\u8d1f\u63d0\u793a\u5219\u4f5c\u4e3a\u53ef\u5b66\u4e60\u7684\u9608\u503c\uff0c\u7528\u4e8e\u533a\u5206\u5e72\u51c0\u6837\u672c\u548c\u566a\u58f0\u6837\u672c\u3002\u6211\u4eec\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\u6765\u8c03\u6574\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u4ee5\u4fc3\u8fdb\u5176\u4e0e\u5b66\u4e60\u5230\u7684\u6587\u672c\u63d0\u793a\u5bf9\u9f50\u3002\u4f5c\u4e3a\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0cDeFT \u53ef\u4ee5\u901a\u8fc7\u5229\u7528\u7cbe\u5fc3\u6311\u9009\u7684\u5e72\u51c0\u6837\u672c\uff0c\u5c06\u8bb8\u591a\u9884\u8bad\u7ec3\u6a21\u578b\u65e0\u7f1d\u5730\u5fae\u8c03\u5230\u4e0b\u6e38\u4efb\u52a1\u3002\u5728\u4e03\u4e2a\u5408\u6210\u548c\u771f\u5b9e\u566a\u58f0\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86 DeFT \u5728\u566a\u58f0\u6807\u7b7e\u68c0\u6d4b\u548c\u56fe\u50cf\u5206\u7c7b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002||\n", "2409.19684": "|**2024-09-29**|[MedViLaM: A multimodal large language model with advanced generalizability and explainability for medical data understanding and generation](http://arxiv.org/abs/2409.19684)|**[link](https://github.com/MedHK23/MedViLaM)**|\u533b\u5b66\u672c\u8d28\u4e0a\u662f\u591a\u6a21\u6001\u548c\u591a\u4efb\u52a1\u7684\uff0c\u5177\u6709\u6db5\u76d6\u6587\u672c\u3001\u5f71\u50cf\u7b49\u591a\u79cd\u6570\u636e\u6a21\u6001\u3002\u7136\u800c\uff0c\u76ee\u524d\u5927\u591a\u6570\u533b\u5b66\u9886\u57df\u6a21\u578b\u90fd\u662f\u5355\u6a21\u6001\u5355\u4efb\u52a1\u7684\uff0c\u7f3a\u4e4f\u826f\u597d\u7684\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86MedViLaM\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u7528\u7684\u533b\u5b66\u6570\u636e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5b83\u53ef\u4ee5\u4f7f\u7528\u76f8\u540c\u7684\u6a21\u578b\u6743\u91cd\u7075\u6d3b\u5730\u7f16\u7801\u548c\u89e3\u91ca\u5404\u79cd\u5f62\u5f0f\u7684\u533b\u5b66\u6570\u636e\uff0c\u5305\u62ec\u4e34\u5e8a\u8bed\u8a00\u548c\u5f71\u50cf\u3002\u4e3a\u4e86\u4fc3\u8fdb\u8fd9\u79cd\u591a\u4efb\u52a1\u6a21\u578b\u7684\u521b\u5efa\uff0c\u6211\u4eec\u7b56\u5212\u4e86MultiMedBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u9762\u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u5305\u542b\u591a\u4e2a\u4e0d\u540c\u7684\u4efb\u52a1\uff0c\u5373\u8fde\u7eed\u95ee\u7b54\u3001\u591a\u6807\u7b7e\u75be\u75c5\u5206\u7c7b\u3001\u75be\u75c5\u5b9a\u4f4d\u3001\u653e\u5c04\u5b66\u62a5\u544a\u7684\u751f\u6210\u548c\u603b\u7ed3\u3002MedViLaM\u5728\u6240\u6709MultiMedBench\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u7ecf\u5e38\u5927\u5e45\u8d85\u8d8a\u5176\u4ed6\u901a\u7528\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u5230\u65b0\u7684\u533b\u5b66\u6982\u5ff5\u548c\u4efb\u52a1\u3001\u8de8\u4e0d\u540c\u4efb\u52a1\u7684\u6709\u6548\u8fc1\u79fb\u5b66\u4e60\u4ee5\u53ca\u96f6\u6837\u672c\u533b\u5b66\u63a8\u7406\u7684\u51fa\u73b0\u3002||\n", "2409.19610": "|**2024-09-29**|[Federated Learning from Vision-Language Foundation Models: Theoretical Analysis and Method](http://arxiv.org/abs/2409.19610)|**[link](https://github.com/PanBikang/PromptFolio)**|\u5c06CLIP\u7b49\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u6574\u5408\u5230\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u4ee5\u589e\u5f3a\u8de8\u4e0d\u540c\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u901a\u5e38\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8054\u90a6\u5b66\u4e60\u91c7\u7528\u63d0\u793a\u5b66\u4e60\u6765\u964d\u4f4e\u901a\u4fe1\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u5373\u57fa\u4e8e\u63d0\u793a\u7684\u8054\u90a6\u5b66\u4e60\u3002\u7136\u800c\uff0c\u76ee\u524d\u5bf9\u57fa\u4e8e\u63d0\u793a\u7684\u8054\u90a6\u5b66\u4e60\u6027\u80fd\u7684\u7406\u8bba\u5206\u6790\u8fd8\u5f88\u6709\u9650\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u7279\u5f81\u5b66\u4e60\u7406\u8bba\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u63d0\u793a\u7684\u8054\u90a6\u5b66\u4e60\u7684\u7406\u8bba\u5206\u6790\u6846\u67b6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u76d1\u63a7\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u8054\u90a6\u5b66\u4e60\u4e2d\u4fe1\u53f7\u5b66\u4e60\u548c\u566a\u58f0\u8bb0\u5fc6\u7684\u6f14\u53d8\uff0c\u8bc1\u660e\u4e86\u53ef\u4ee5\u901a\u8fc7\u4e0e\u4efb\u52a1\u76f8\u5173\u548c\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u7cfb\u6570\u4e4b\u6bd4\u6765\u8bc4\u4f30\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u6536\u76ca\u548c\u98ce\u9669\u4e0e\u7279\u5f81\u5b66\u4e60\u4e2d\u7684\u4efb\u52a1\u76f8\u5173\u548c\u4efb\u52a1\u65e0\u5173\u9879\u8fdb\u884c\u4e86\u7c7b\u6bd4\u3002\u53d7\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u7406\u8bba\u7684\u542f\u53d1\uff0c\u5373\u7ec4\u5408\u4e24\u79cd\u72ec\u7acb\u8d44\u4ea7\u5c06\u4fdd\u6301\u6536\u76ca\uff0c\u540c\u65f6\u964d\u4f4e\u98ce\u9669\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u79cd\u63d0\u793a\uff1a\u5168\u5c40\u63d0\u793a\u548c\u5c40\u90e8\u63d0\u793a\uff0c\u4ee5\u6784\u5efa\u4e00\u4e2a\u63d0\u793a\u7ec4\u5408\u6765\u5e73\u8861\u6cdb\u5316\u6027\u548c\u4e2a\u6027\u5316\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u63d0\u793a\u7ec4\u5408\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e86\u6700\u4f73\u6df7\u5408\u7cfb\u6570\u3002\u8fd9\u4e9b\u7406\u8bba\u4e3b\u5f20\u5f97\u5230\u4e86\u8fdb\u4e00\u6b65\u7684\u5b9e\u8bc1\u5b9e\u9a8c\u7684\u652f\u6301\u3002||\n", "2409.19474": "|**2024-09-28**|[FairPIVARA: Reducing and Assessing Biases in CLIP-Based Multimodal Models](http://arxiv.org/abs/2409.19474)|**[link](https://github.com/hiaac-nlp/fairpivara)**|\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\u5e76\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5f88\u5c11\u6709\u7814\u7a76\u63a2\u8ba8\u5176\u4f26\u7406\u542b\u4e49\u3002\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u800c\u8fd9\u4e9b\u6570\u636e\u5f80\u5f80\u6765\u81ea\u4ed3\u4fc3\u5ba1\u67e5\u7684\u6587\u672c\u548c\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u6570\u636e\u96c6\u9ad8\u5ea6\u5931\u8861\u5e76\u5f15\u53d1\u4f26\u7406\u95ee\u9898\u3002\u6b64\u5916\uff0c\u6700\u521d\u7528\u82f1\u8bed\u8bad\u7ec3\u7684\u6a21\u578b\u7ecf\u5e38\u9488\u5bf9\u5176\u4ed6\u8bed\u8a00\u8fdb\u884c\u5fae\u8c03\uff0c\u4f8b\u5982 CLIP \u6a21\u578b\uff0c\u53ef\u4ee5\u901a\u8fc7\u6dfb\u52a0\u66f4\u591a\u6570\u636e\u6765\u589e\u5f3a\u5176\u529f\u80fd\uff0c\u4f46\u4e5f\u53ef\u80fd\u5f15\u5165\u65b0\u7684\u504f\u5dee\u3002CAPIVARA \u662f\u4e00\u79cd\u57fa\u4e8e CLIP \u6a21\u578b\u5e76\u9002\u7528\u4e8e\u8461\u8404\u7259\u8bed\u7684\u6a21\u578b\uff0c\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u56db\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u6b67\u89c6\u6027\u505a\u6cd5\uff0c\u5e76\u4ecb\u7ecd\u4e86 FairPIVARA\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u8fc7\u79fb\u9664\u7279\u5f81\u5d4c\u5165\u4e2d\u53d7\u5f71\u54cd\u6700\u5927\u7684\u7ef4\u5ea6\u6765\u51cf\u5c11\u8fd9\u4e9b\u505a\u6cd5\u7684\u65b9\u6cd5\u3002FairPIVARA \u7684\u5e94\u7528\u663e\u8457\u51cf\u5c11\u4e86\u9ad8\u8fbe 98% \u7684\u89c2\u5bdf\u5230\u7684\u504f\u5dee\uff0c\u540c\u65f6\u4fc3\u8fdb\u4e86\u6a21\u578b\u4e2d\u66f4\u5e73\u8861\u7684\u8bcd\u8bed\u5206\u5e03\u3002\u6211\u4eec\u7684\u6a21\u578b\u548c\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u53d6\uff1ahttps://github.com/hiaac-nlp/FairPIVARA\u3002||\n", "2410.02763": "|**2024-10-03**|[Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos](http://arxiv.org/abs/2410.02763)|null|\u6700\u8fd1\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u4eba\u8ba4\u4e3a\u73b0\u4ee3\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u5df2\u7ecf\u89e3\u51b3\u4e86\u4e0e\u77ed\u89c6\u9891\u7406\u89e3\u76f8\u5173\u7684\u5927\u591a\u6570\u5173\u952e\u6311\u6218\u3002\u56e0\u6b64\uff0c\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u90fd\u9010\u6e10\u5c06\u6ce8\u610f\u529b\u8f6c\u5411\u7406\u89e3\u957f\u89c6\u9891\u5e26\u6765\u7684\u66f4\u590d\u6742\u6311\u6218\u3002\u7136\u800c\uff0c\u4e8b\u5b9e\u771f\u7684\u5982\u6b64\u5417\uff1f\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u5728\u5904\u7406\u77ed\u89c6\u9891\u65f6\uff0cLMM \u4ecd\u7136\u7f3a\u4e4f\u8bb8\u591a\u57fa\u672c\u7684\u63a8\u7406\u80fd\u529b\u3002\u6211\u4eec\u4ecb\u7ecd\u4e86 Vinoground\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b 1000 \u4e2a\u77ed\u800c\u81ea\u7136\u7684\u89c6\u9891-\u5b57\u5e55\u5bf9\u7684\u65f6\u95f4\u53cd\u4e8b\u5b9e LMM \u8bc4\u4f30\u57fa\u51c6\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u73b0\u6709\u7684 LMM \u5f88\u96be\u533a\u5206\u4e0d\u540c\u52a8\u4f5c\u548c\u5bf9\u8c61\u8f6c\u6362\u4e4b\u95f4\u7684\u65f6\u95f4\u5dee\u5f02\u3002\u4f8b\u5982\uff0c\u6700\u4f73\u6a21\u578b GPT-4o \u5728\u6211\u4eec\u7684\u6587\u672c\u548c\u89c6\u9891\u5f97\u5206\u4e2d\u4ec5\u83b7\u5f97\u7ea6 50% \u7684\u5206\u6570\uff0c\u4e0e\u7ea6 90% \u7684\u4eba\u7c7b\u57fa\u7ebf\u76f8\u6bd4\u5b58\u5728\u8f83\u5927\u5dee\u8ddd\u3002\u6240\u6709\u5f00\u6e90\u591a\u6a21\u6001\u6a21\u578b\u548c\u57fa\u4e8e CLIP \u7684\u6a21\u578b\u8868\u73b0\u66f4\u5dee\uff0c\u4ea7\u751f\u7684\u7ed3\u679c\u5927\u591a\u662f\u968f\u673a\u7684\u3002\u901a\u8fc7\u8fd9\u9879\u5de5\u4f5c\uff0c\u6211\u4eec\u63ed\u793a\u4e86\u77ed\u89c6\u9891\u4e2d\u7684\u65f6\u95f4\u63a8\u7406\u662f\u4e00\u4e2a\u5c1a\u672a\u5b8c\u5168\u89e3\u51b3\u7684\u95ee\u9898\u3002\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u4ee3\u7801\u53ef\u5728 https://vinoground.github.io \u83b7\u53d6\u3002||\n", "2410.02762": "|**2024-10-03**|[Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations](http://arxiv.org/abs/2410.02762)|**[link](https://github.com/nickjiang2378/vl-interp)**|\u6211\u4eec\u7814\u7a76\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u5185\u90e8\u8868\u5f81\uff0c\u4ee5\u89e3\u51b3\u5e7b\u89c9\u95ee\u9898\uff0c\u5c3d\u7ba1\u6a21\u578b\u89c4\u6a21\u548c\u8bad\u7ec3\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u6b65\uff0c\u4f46\u8fd9\u4ecd\u7136\u662f\u4e00\u4e2a\u6301\u7eed\u7684\u6311\u6218\u3002\u6211\u4eec\u5c06 VLM \u7684\u5185\u90e8\u56fe\u50cf\u8868\u5f81\u6295\u5f71\u5230\u5b83\u4eec\u7684\u8bed\u8a00\u8bcd\u6c47\u8868\u4e2d\uff0c\u5e76\u89c2\u5bdf\u5230\u771f\u5b9e\u7269\u4f53\u7684\u8f93\u51fa\u6982\u7387\u6bd4\u5e7b\u89c9\u7269\u4f53\u66f4\u6709\u4fe1\u5fc3\u3002\u6211\u4eec\u8fd8\u4f7f\u7528\u8fd9\u4e9b\u8f93\u51fa\u6982\u7387\u6765\u5bf9\u771f\u5b9e\u7269\u4f53\u8fdb\u884c\u7a7a\u95f4\u5b9a\u4f4d\u3002\u5728\u6b64\u65b9\u6cd5\u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u77e5\u8bc6\u64e6\u9664\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u6b63\u4ea4\u5316\u56fe\u50cf\u7279\u5f81\u548c\u5e7b\u89c9\u7269\u4f53\u7279\u5f81\u6765\u6d88\u9664\u5e7b\u89c9\u3002\u6211\u4eec\u8868\u660e\uff0c\u5bf9\u6a21\u578b\u6f5c\u5728\u8868\u5f81\u7684\u6709\u9488\u5bf9\u6027\u7684\u7f16\u8f91\u53ef\u4ee5\u5c06 COCO2014 \u6570\u636e\u96c6\u4e0a\u7684\u5e7b\u89c9\u51cf\u5c11\u9ad8\u8fbe 25.7%\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u66f4\u6df1\u5165\u5730\u7406\u89e3 VLM \u7684\u6f5c\u5728\u8868\u5f81\u53ef\u4ee5\u589e\u5f3a\u53ef\u9760\u6027\u5e76\u5b9e\u73b0\u65b0\u7684\u529f\u80fd\uff0c\u4f8b\u5982\u96f6\u6837\u672c\u5206\u5272\u3002||\n", "2410.02740": "|**2024-10-03**|[Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models](http://arxiv.org/abs/2410.02740)|null|\u591a\u6a21\u6001\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u7a81\u51fa\u4e86\u91cd\u5199\u56fe\u50cf\u63cf\u8ff0\u5bf9\u4e8e\u63d0\u9ad8\u6027\u80fd\u7684\u4ef7\u503c\uff0c\u4f46\u4e5f\u5b58\u5728\u4e00\u4e9b\u5173\u952e\u6311\u6218\u3002\u4f8b\u5982\uff0c\u867d\u7136\u5408\u6210\u56fe\u50cf\u63cf\u8ff0\u901a\u5e38\u63d0\u4f9b\u66f4\u9ad8\u7684\u8d28\u91cf\u548c\u56fe\u6587\u5bf9\u9f50\u6027\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u5b83\u4eec\u662f\u5426\u53ef\u4ee5\u5b8c\u5168\u66ff\u4ee3 AltTexts\uff1a\u5408\u6210\u56fe\u50cf\u63cf\u8ff0\u7684\u4f5c\u7528\u53ca\u5176\u4e0e\u539f\u59cb\u7f51\u7edc\u6293\u53d6\u7684 AltTexts \u5728\u9884\u8bad\u7ec3\u4e2d\u7684\u4ea4\u4e92\u4f5c\u7528\u4ecd\u4e0d\u6e05\u695a\u3002\u6b64\u5916\uff0c\u4e0d\u540c\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u53ef\u80fd\u5bf9\u7279\u5b9a\u7684\u56fe\u50cf\u63cf\u8ff0\u683c\u5f0f\u6709\u72ec\u7279\u7684\u504f\u597d\uff0c\u4f46\u786e\u5b9a\u6bcf\u4e2a\u6a21\u578b\u7684\u6700\u4f73\u56fe\u50cf\u63cf\u8ff0\u7684\u52aa\u529b\u4ecd\u7136\u6709\u9650\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u53ef\u63a7\u7684\u548c\u53ef\u6269\u5c55\u7684\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u6d41\u7a0b\uff0c\u65e8\u5728\u751f\u6210\u9002\u5408\u5404\u79cd\u591a\u6a21\u6001\u6a21\u578b\u7684\u4e0d\u540c\u56fe\u50cf\u63cf\u8ff0\u683c\u5f0f\u3002\u901a\u8fc7\u4ee5\u7b80\u77ed\u5408\u6210\u56fe\u50cf\u63cf\u8ff0 (SSC) \u548c\u5bc6\u96c6\u5408\u6210\u56fe\u50cf\u63cf\u8ff0 (DSC+) \u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u6211\u4eec\u7cfb\u7edf\u5730\u63a2\u7d22\u4e86\u5b83\u4eec\u5bf9 CLIP\u3001\u591a\u6a21\u6001 LLM \u548c\u6269\u6563\u6a21\u578b\u7b49\u6a21\u578b\u7684\u5f71\u54cd\u4ee5\u53ca\u4e0e AltTexts \u7684\u4ea4\u4e92\u4f5c\u7528\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4fdd\u7559\u5408\u6210\u56fe\u50cf\u63cf\u8ff0\u548c AltTexts \u7684\u6df7\u5408\u65b9\u6cd5\u53ef\u4ee5\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528\u5408\u6210\u56fe\u50cf\u63cf\u8ff0\uff0c\u4ece\u800c\u63d0\u9ad8\u5bf9\u9f50\u6027\u548c\u6027\u80fd\uff0c\u5e76\u4e14\u6bcf\u4e2a\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u5bf9\u7279\u5b9a\u56fe\u50cf\u63cf\u8ff0\u683c\u5f0f\u7684\u504f\u597d\u3002\u8fd9\u79cd\u5168\u9762\u7684\u5206\u6790\u4e3a\u4f18\u5316\u56fe\u50cf\u63cf\u8ff0\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\uff0c\u4ece\u800c\u63a8\u8fdb\u4e86\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u3002||\n", "2410.02730": "|**2024-10-03**|[DivScene: Benchmarking LVLMs for Object Navigation with Diverse Scenes and Objects](http://arxiv.org/abs/2410.02730)|**[link](https://github.com/zhaowei-wang-nlp/divscene)**|\u5728\u672a\u77e5\u73af\u5883\u4e2d\u8fdb\u884c\u7269\u4f53\u5bfc\u822a\u5bf9\u4e8e\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u90e8\u7f72\u5177\u8eab\u4ee3\u7406\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u7531\u4e8e\u5927\u89c4\u6a21\u573a\u666f\u6570\u636e\u96c6\u3001\u66f4\u5feb\u7684\u6a21\u62df\u5668\u548c\u66f4\u5f3a\u5927\u7684\u6a21\u578b\uff0c\u6211\u4eec\u5df2\u7ecf\u76ee\u7779\u4e86\u5de8\u5927\u7684\u8fdb\u6b65\uff0c\u4f46\u4e4b\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6709\u9650\u7684\u573a\u666f\u7c7b\u578b\u548c\u76ee\u6807\u7269\u4f53\u4e0a\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5728\u5927\u91cf\u573a\u666f\u7c7b\u578b\u4e2d\u5bfc\u822a\u5230\u4e0d\u540c\u76ee\u6807\u7269\u4f53\u7684\u65b0\u4efb\u52a1\u3002\u4e3a\u4e86\u5bf9\u8be5\u95ee\u9898\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u573a\u666f\u6570\u636e\u96c6 DivScene\uff0c\u5176\u4e2d\u5305\u542b\u8de8\u8d8a 81 \u79cd\u4e0d\u540c\u7c7b\u578b\u7684 4,614 \u4e2a\u573a\u666f\u3002\u5229\u7528\u8be5\u6570\u636e\u96c6\uff0c\u6211\u4eec\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u5fae\u8c03\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM)\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5177\u8eab\u4ee3\u7406 NatVLM\u3002LVLM \u88ab\u8bad\u7ec3\u7528\u4e8e\u83b7\u53d6\u6765\u81ea\u73af\u5883\u7684\u5148\u524d\u89c2\u5bdf\u7ed3\u679c\u5e76\u751f\u6210\u4e0b\u4e00\u6b65\u52a8\u4f5c\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u52a8\u4f5c\u9884\u6d4b\u7684\u601d\u7ef4\u94fe (CoT) \u89e3\u91ca\u8f68\u8ff9\uff0c\u4ee5\u4fbf\u5728\u8c03\u6574 LVLM \u65f6\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002\u6211\u4eec\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u53d1\u73b0\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5bf9\u7531 BFS \u89c4\u5212\u5668\u6784\u5efa\u7684\u6700\u77ed\u8def\u5f84\u8fdb\u884c\u6a21\u4eff\u5b66\u4e60\u6765\u6784\u5efa\u6027\u80fd\u826f\u597d\u7684\u57fa\u4e8e LVLM \u7684\u4ee3\u7406\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u4eba\u5de5\u76d1\u7763\u3002\u6211\u4eec\u7684\u4ee3\u7406\u5b9e\u73b0\u4e86\u8d85\u8fc7 GPT-4o 20% \u4ee5\u4e0a\u7684\u6210\u529f\u7387\u3002\u540c\u65f6\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u5404\u79cd\u5206\u6790\uff0c\u5c55\u793a\u4e86\u6211\u4eec\u4ee3\u7406\u7684\u6cdb\u5316\u80fd\u529b\u3002||\n", "2410.02713": "|**2024-10-03**|[Video Instruction Tuning With Synthetic Data](http://arxiv.org/abs/2410.02713)|null|\u89c6\u9891\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u7684\u53d1\u5c55\u4e00\u76f4\u53d7\u5230\u4ece\u7f51\u7edc\u83b7\u53d6\u5927\u91cf\u9ad8\u8d28\u91cf\u539f\u59cb\u6570\u636e\u7684\u96be\u5ea6\u7684\u963b\u788d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6cd5\uff0c\u5373\u521b\u5efa\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u89c6\u9891\u6307\u4ee4\u9075\u5faa\u7684\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u96c6\uff0c\u5373 LLaVA-Video-178K\u3002\u8be5\u6570\u636e\u96c6\u5305\u62ec\u5173\u952e\u4efb\u52a1\uff0c\u4f8b\u5982\u8be6\u7ec6\u5b57\u5e55\u3001\u5f00\u653e\u5f0f\u95ee\u7b54 (QA) \u548c\u591a\u9879\u9009\u62e9 QA\u3002\u901a\u8fc7\u7ed3\u5408\u73b0\u6709\u7684\u89c6\u89c9\u6307\u4ee4\u8c03\u6574\u6570\u636e\u5bf9\u8be5\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u6211\u4eec\u63a8\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u89c6\u9891 LLM\uff0c\u5373 LLaVA-Video\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLLaVA-Video \u5728\u5404\u79cd\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u4e86\u51fa\u8272\u7684\u6027\u80fd\uff0c\u7a81\u51fa\u4e86\u6211\u4eec\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u8ba1\u5212\u53d1\u5e03\u6570\u636e\u96c6\u3001\u5176\u751f\u6210\u7ba1\u9053\u548c\u6a21\u578b\u68c0\u67e5\u70b9\u3002||\n", "2410.02712": "|**2024-10-03**|[LLaVA-Critic: Learning to Evaluate Multimodal Models](http://arxiv.org/abs/2410.02712)|null|\u6211\u4eec\u63a8\u51fa\u4e86 LLaVA-Critic\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5f00\u6e90\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM)\uff0c\u5b83\u88ab\u8bbe\u8ba1\u6210\u4e00\u4e2a\u901a\u7528\u7684\u8bc4\u4f30\u5668\uff0c\u7528\u4e8e\u8bc4\u4f30\u5404\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u7684\u6027\u80fd\u3002LLaVA-Critic \u4f7f\u7528\u9ad8\u8d28\u91cf\u7684\u6279\u8bc4\u6307\u4ee4\u9075\u5faa\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u4e0d\u540c\u7684\u8bc4\u4f30\u6807\u51c6\u548c\u573a\u666f\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u6a21\u578b\u5728\u4e24\u4e2a\u5173\u952e\u9886\u57df\u7684\u6709\u6548\u6027\uff1a(1) LMM \u4f5c\u4e3a\u8bc4\u5224\u8005\uff0cLLaVA-Critic \u63d0\u4f9b\u53ef\u9760\u7684\u8bc4\u4f30\u5206\u6570\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0e GPT \u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\uff1b(2) \u504f\u597d\u5b66\u4e60\uff0c\u5b83\u4e3a\u504f\u597d\u5b66\u4e60\u751f\u6210\u5956\u52b1\u4fe1\u53f7\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u9f50\u80fd\u529b\u3002\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u5f00\u6e90 LMM \u5728\u81ea\u6211\u6279\u8bc4\u548c\u8bc4\u4f30\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76 LMM \u53ef\u6269\u5c55\u7684\u3001\u8d85\u4eba\u7684\u5bf9\u9f50\u53cd\u9988\u673a\u5236\u5960\u5b9a\u4e86\u57fa\u7840\u3002||\n", "2410.02681": "|**2024-10-03**|[Understanding and Mitigating Miscalibration in Prompt Tuning for Vision-Language Models](http://arxiv.org/abs/2410.02681)|null|\u7f6e\u4fe1\u5ea6\u6821\u51c6\u5bf9\u4e8e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u50cf CLIP \u8fd9\u6837\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u5fae\u8c03\u4e4b\u540e\uff0c\u5c1a\u672a\u5b8c\u5168\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002 \u672c\u7814\u7a76\u8868\u660e\uff0c\u73b0\u6709\u7684\u63d0\u793a\u5fae\u8c03\u65b9\u6cd5\u901a\u5e38\u4f1a\u5bfc\u81f4\u57fa\u7840\u7c7b\u522b\u548c\u65b0\u7c7b\u522b\u4e4b\u95f4\u6821\u51c6\u7684\u6743\u8861\uff1aCoOp \u4e2d\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u901a\u8fc7\u589e\u52a0\u6587\u672c\u6807\u7b7e\u5dee\u5f02\u5bfc\u81f4\u5bf9\u65b0\u7c7b\u522b\u7684\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u800c KgCoOp \u7684\u6b63\u5219\u5316\u4fdd\u6301\u4e86\u7f6e\u4fe1\u5ea6\u6c34\u5e73\uff0c\u4f46\u7531\u4e8e\u51c6\u786e\u6027\u7684\u63d0\u9ad8\uff0c\u5bfc\u81f4\u5bf9\u57fa\u7840\u7c7b\u522b\u7684\u4e0d\u81ea\u4fe1\u3002 \u53d7\u8fd9\u4e9b\u89c2\u5bdf\u7ed3\u679c\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u52a8\u6001\u5f02\u5e38\u503c\u6b63\u5219\u5316 (DOR) \u6765\u786e\u4fdd\u5fae\u8c03\u540e\u5bf9\u57fa\u7840\u7c7b\u522b\u548c\u65b0\u7c7b\u522b\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u3002 \u7279\u522b\u662f\uff0c\u6211\u4eec\u5efa\u8bae\u6700\u5c0f\u5316\u4ece\u5927\u578b\u8bcd\u6c47\u8868\u4e2d\u91c7\u6837\u7684\u65b0\u6587\u672c\u6807\u7b7e\uff08\u800c\u4e0d\u662f\u57fa\u7840\u7c7b\u522b\uff09\u7684\u7279\u5f81\u504f\u5dee\u3002 \u5b9e\u9645\u4e0a\uff0cDOR \u963b\u6b62\u4e86\u65b0\u6807\u7b7e\u7684\u6587\u672c\u5dee\u5f02\u7684\u589e\u52a0\uff0c\u540c\u65f6\u653e\u5bbd\u4e86\u5bf9\u57fa\u7840\u7c7b\u522b\u7684\u9650\u5236\u3002 \u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDOR \u53ef\u4ee5\u589e\u5f3a\u5f53\u524d\u5fae\u8c03\u65b9\u6cd5\u5728\u57fa\u7840\u7c7b\u522b\u548c\u65b0\u7c7b\u522b\u4e0a\u7684\u6821\u51c6\u6027\u80fd\u3002||\n", "2410.02193": "|**2024-10-03**|[Guiding Long-Horizon Task and Motion Planning with Vision Language Models](http://arxiv.org/abs/2410.02193)|null|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u80fd\u591f\u5728\u88ab\u63d0\u793a\u76ee\u6807\u3001\u4e0a\u4e0b\u6587\u3001\u573a\u666f\u56fe\u50cf\u548c\u4efb\u4f55\u89c4\u5212\u7ea6\u675f\u65f6\u751f\u6210\u770b\u4f3c\u5408\u7406\u7684\u9ad8\u7ea7\u8ba1\u5212\u3002\u4f46\u662f\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u9884\u6d4b\u7684\u52a8\u4f5c\u5bf9\u4e8e\u7279\u5b9a\u7684\u673a\u5668\u4eba\u5b9e\u65bd\u65b9\u6848\u5728\u51e0\u4f55\u548c\u8fd0\u52a8\u5b66\u4e0a\u662f\u53ef\u884c\u7684\u3002\u56e0\u6b64\uff0c\u5728\u4ed6\u4eec\u7684\u8ba1\u5212\u4e2d\uff0c\u8bb8\u591a\u5148\u51b3\u6761\u4ef6\u6b65\u9aa4\uff08\u4f8b\u5982\u6253\u5f00\u62bd\u5c49\u4ee5\u83b7\u53d6\u7269\u4f53\uff09\u7ecf\u5e38\u88ab\u7701\u7565\u3002\u673a\u5668\u4eba\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\u5668\u53ef\u4ee5\u751f\u6210\u5c0a\u91cd\u52a8\u4f5c\u51e0\u4f55\u53ef\u884c\u6027\u7684\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5e76\u63d2\u5165\u7269\u7406\u4e0a\u5fc5\u8981\u7684\u52a8\u4f5c\uff0c\u4f46\u65e0\u6cd5\u6269\u5c55\u5230\u9700\u8981\u5e38\u8bc6\u77e5\u8bc6\u5e76\u6d89\u53ca\u7531\u8bb8\u591a\u53d8\u91cf\u7ec4\u6210\u7684\u5927\u72b6\u6001\u7a7a\u95f4\u7684\u65e5\u5e38\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86 VLM-TAMP\uff0c\u8fd9\u662f\u4e00\u79cd\u5206\u5c42\u89c4\u5212\u7b97\u6cd5\uff0c\u5b83\u5229\u7528 VLM \u751f\u6210\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u4e14\u51cf\u5c11\u8303\u56f4\u7684\u4e2d\u95f4\u5b50\u76ee\u6807\uff0c\u4ece\u800c\u6307\u5bfc\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\u5668\u3002\u5f53\u5b50\u76ee\u6807\u6216\u52a8\u4f5c\u65e0\u6cd5\u7ec6\u5316\u65f6\uff0c\u5c06\u518d\u6b21\u67e5\u8be2 VLM \u4ee5\u8fdb\u884c\u91cd\u65b0\u89c4\u5212\u3002\u6211\u4eec\u5728\u53a8\u623f\u4efb\u52a1\u4e2d\u8bc4\u4f30 VLM-TAMP\uff0c\u5176\u4e2d\u673a\u5668\u4eba\u5fc5\u987b\u5b8c\u6210\u9700\u8981\u6309\u987a\u5e8f\u6267\u884c 30-50 \u4e2a\u52a8\u4f5c\u5e76\u4e0e\u591a\u8fbe 21 \u4e2a\u7269\u4f53\u4ea4\u4e92\u7684\u70f9\u996a\u76ee\u6807\u3002VLM-TAMP \u7684\u6027\u80fd\u5927\u5927\u4f18\u4e8e\u4e25\u683c\u4e14\u72ec\u7acb\u5730\u6267\u884c VLM \u751f\u6210\u7684\u52a8\u4f5c\u5e8f\u5217\u7684\u57fa\u7ebf\uff0c\u65e0\u8bba\u662f\u5728\u6210\u529f\u7387\uff0850% \u5230 100% \u5bf9\u6bd4 0%\uff09\u8fd8\u662f\u5e73\u5747\u4efb\u52a1\u5b8c\u6210\u767e\u5206\u6bd4\uff0872% \u5230 100% \u5bf9\u6bd4 15% \u5230 45%\uff09\u3002\u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u9879\u76ee\u7f51\u7ad9 https://zt-yang.github.io/vlm-tamp-robot/\u3002||\n", "2410.02086": "|**2024-10-02**|[Anchors Aweigh! Sail for Optimal Unified Multi-Modal Representations](http://arxiv.org/abs/2410.02086)|null|\u591a\u6a21\u6001\u5b66\u4e60\u5728\u4f7f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u878d\u5408\u548c\u5229\u7528\u6587\u672c\u3001\u56fe\u50cf\u548c\u97f3\u9891\u7b49\u4e0d\u540c\u6570\u636e\u6e90\u4ee5\u652f\u6301\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u8de8\u5404\u79cd\u6a21\u6001\u7684\u7edf\u4e00\u8868\u793a\u5bf9\u4e8e\u63d0\u9ad8\u6548\u7387\u548c\u6027\u80fd\u5c24\u4e3a\u91cd\u8981\u3002\u6700\u8fd1\u7684\u7ed1\u5b9a\u65b9\u6cd5\uff0c\u5982ImageBind\uff08Girdhar\u7b49\u4eba\uff0c2023\uff09\uff0c\u901a\u5e38\u4f7f\u7528\u56fa\u5b9a\u7684\u951a\u70b9\u6a21\u6001\u6765\u5bf9\u9f50\u951a\u70b9\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u591a\u6a21\u6001\u6570\u636e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5bf9\u56fa\u5b9a\u951a\u70b9\u7ed1\u5b9a\u65b9\u6cd5\u8fdb\u884c\u4e86\u6570\u5b66\u5206\u6790\uff0c\u5e76\u53d1\u73b0\u4e86\u5176\u663e\u8457\u7684\u5c40\u9650\u6027\uff1a\uff081\uff09\u8fc7\u5ea6\u4f9d\u8d56\u4e8e\u951a\u70b9\u6a21\u6001\u7684\u9009\u62e9\uff0c\uff082\uff09\u65e0\u6cd5\u6355\u83b7\u6a21\u6001\u5185\u4fe1\u606f\uff0c\u4ee5\u53ca\uff083\uff09\u65e0\u6cd5\u89e3\u91ca\u975e\u951a\u70b9\u6a21\u6001\u4e4b\u95f4\u7684\u6a21\u6001\u95f4\u76f8\u5173\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86CentroBind\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u800c\u5f3a\u5927\u7684\u65b9\u6cd5\uff0c\u5b83\u6d88\u9664\u4e86\u5bf9\u56fa\u5b9a\u951a\u70b9\u7684\u9700\u6c42\uff1b\u76f8\u53cd\uff0c\u5b83\u91c7\u7528\u4ece\u6240\u6709\u53ef\u7528\u6a21\u6001\u751f\u6210\u7684\u52a8\u6001\u53ef\u8c03\u7684\u57fa\u4e8e\u8d28\u5fc3\u7684\u951a\u70b9\uff0c\u4ece\u800c\u4ea7\u751f\u5e73\u8861\u4e14\u4e30\u5bcc\u7684\u8868\u793a\u7a7a\u95f4\u3002\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u6355\u83b7\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u7684\u4e09\u4e2a\u5173\u952e\u5c5e\u6027\uff1a\u6a21\u6001\u5185\u5b66\u4e60\u3001\u6a21\u6001\u95f4\u5b66\u4e60\u548c\u591a\u6a21\u6001\u5bf9\u9f50\uff0c\u540c\u65f6\u8fd8\u5728\u6240\u6709\u6a21\u6001\u4e2d\u6784\u5efa\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u7edf\u4e00\u8868\u793a\u3002\u6211\u4eec\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u90fd\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u8868\u660e\u52a8\u6001\u951a\u70b9\u65b9\u6cd5\u4f18\u4e8e\u6240\u6709\u56fa\u5b9a\u951a\u70b9\u7ed1\u5b9a\u65b9\u6cd5\uff0c\u56e0\u4e3a\u524d\u8005\u6355\u83b7\u4e86\u66f4\u7ec6\u5fae\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u3002||\n", "2410.02052": "|**2024-10-02**|[Improving Autonomous AI Agents with Reflective Tree Search and Self-Learning](http://arxiv.org/abs/2410.02052)|null|\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u81ea\u52a8\u5316\u590d\u6742\u7684\u591a\u6b65\u51b3\u7b56\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002\u7136\u800c\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u4f8b\u5982GPT-4o\uff0c\u5728\u590d\u6742\u7f51\u7edc\u73af\u5883\u548c\u957f\u671f\u89c4\u5212\u4efb\u52a1\u4e2d\u4ecd\u672a\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u53cd\u5c04\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08R-MCTS\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6d4b\u8bd5\u65f6\u7b97\u6cd5\uff0c\u65e8\u5728\u589e\u5f3a\u4eba\u5de5\u667a\u80fd\u4f53\uff08\u4f8b\u5982\u7531GPT-4o\u9a71\u52a8\u7684\u667a\u80fd\u4f53\uff09\u52a8\u6001\u63a2\u7d22\u51b3\u7b56\u7a7a\u95f4\u7684\u80fd\u529b\u3002R-MCTS\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u6269\u5c55\u4e86\u4f20\u7edf\u7684MCTS\uff1a1\uff09\u7ed3\u5408\u5bf9\u6bd4\u53cd\u5c04\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u4ece\u8fc7\u53bb\u7684\u4ea4\u4e92\u4e2d\u5b66\u4e60\u5e76\u52a8\u6001\u63d0\u9ad8\u5176\u641c\u7d22\u6548\u7387\uff1b2\uff09\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6765\u63d0\u4f9b\u53ef\u9760\u7684\u72b6\u6001\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u81ea\u6211\u5b66\u4e60\u5fae\u8c03GPT-4o\u6765\u63d0\u9ad8\u667a\u80fd\u4f53\u7684\u6027\u80fd\uff0c\u4f7f\u7528R-MCTS\u751f\u6210\u7684\u6811\u904d\u5386\uff0c\u65e0\u9700\u4efb\u4f55\u4eba\u5de5\u63d0\u4f9b\u7684\u6807\u7b7e\u3002\u5728\u5177\u6709\u6311\u6218\u6027\u7684VisualWebArena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u57fa\u4e8eGPT-4o\u7684R-MCTS\u667a\u80fd\u4f53\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u6bd4\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u6280\u672f\u5b9e\u73b0\u4e866%\u523030%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8868\u660e\uff0c\u4ece\u6d4b\u8bd5\u65f6\u641c\u7d22\u4e2d\u83b7\u5f97\u7684\u77e5\u8bc6\u53ef\u4ee5\u901a\u8fc7\u5fae\u8c03\u6709\u6548\u5730\u8f6c\u79fb\u56deGPT-4o\u3002\u7ecf\u8fc7\u5fae\u8c03\u7684GPT-4o\u5728\u6d4b\u8bd5\u65f6\u53ef\u4ee5\u8fbe\u5230R-MCTS\u6027\u80fd\u768497%\uff0c\u540c\u65f6\u8ba1\u7b97\u91cf\u51cf\u5c11\u4e86\u56db\u500d\u3002\u6b64\u5916\uff0c\u5b9a\u6027\u7ed3\u679c\u8868\u660e\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684GPT-4o\u6a21\u578b\u80fd\u591f\u63a2\u7d22\u73af\u5883\u3001\u8bc4\u4f30\u72b6\u6001\uff0c\u5e76\u5728\u68c0\u6d4b\u5230\u5f53\u524d\u72b6\u6001\u65e0\u6cd5\u5bfc\u81f4\u6210\u529f\u65f6\u56de\u6eaf\u5230\u53ef\u884c\u7684\u72b6\u6001\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u5c55\u793a\u4e86\u8bad\u7ec3\uff08\u4f7f\u7528R-MCTS\u6536\u96c6\u6570\u636e\uff09\u548c\u6d4b\u8bd5\u65f6\u7684\u8ba1\u7b97\u6269\u5c55\u7279\u6027\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u901a\u8fc7\u6d4b\u8bd5\u65f6\u641c\u7d22\u548c\u81ea\u6211\u5b66\u4e60\u6765\u589e\u5f3aVLM\u7684\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\uff0c\u4ee5\u7528\u4e8e\u667a\u80fd\u4f53\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u7814\u7a76\u65b9\u5411\u3002||\n", "2410.03189": "|**2024-10-04**|[Generalizable Prompt Tuning for Vision-Language Models](http://arxiv.org/abs/2410.03189)|null|\u9488\u5bf9\u8bf8\u5982 CLIP \u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63d0\u793a\u8c03\u4f18\u6d89\u53ca\u4f18\u5316\u7528\u4e8e\u4e3a\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\u751f\u6210\u56fe\u50cf-\u6587\u672c\u5bf9\u7684\u6587\u672c\u63d0\u793a\u3002\u867d\u7136\u624b\u5de5\u5236\u4f5c\u6216\u57fa\u4e8e\u6a21\u677f\u7684\u63d0\u793a\u901a\u5e38\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u672a\u89c1\u7c7b\u522b\uff0c\u4f46\u5b83\u4eec\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5373\u5df2\u89c1\u7c7b\u522b\uff09\u4e2d\u5f80\u5f80\u8868\u73b0\u4e0d\u4f73\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u53ef\u5b66\u4e60\u7684\u8f6f\u63d0\u793a\u901a\u5e38\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u6cdb\u5316\u6027\u3002\u6b64\u5916\uff0c\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6587\u672c\u6a21\u6001\u4e0a\uff0c\u5f88\u5c11\u6709\u7814\u7a76\u8bd5\u56fe\u4ece\u89c6\u89c9\u6a21\u6001\u63a2\u7d22\u63d0\u793a\u7684\u6cdb\u5316\u6f5c\u529b\u3002\u8003\u8651\u5230\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5982\u4f55\u8fdb\u884c\u63d0\u793a\u8c03\u4f18\u4ee5\u83b7\u5f97\u5177\u6709\u7ade\u4e89\u529b\u7684\u4e0b\u6e38\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5c06\u8f6f\u63d0\u793a\u548c\u624b\u5de5\u63d0\u793a\u89c6\u4e3a\u6587\u672c\u6a21\u6001\u7684\u53cc\u91cd\u89c6\u56fe\uff0c\u5e76\u6700\u5927\u5316\u5b83\u4eec\u7684\u4e92\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u66f4\u597d\u5730\u96c6\u6210\u7279\u5b9a\u4efb\u52a1\u7684\u8bed\u4e49\u4fe1\u606f\u548c\u901a\u7528\u8bed\u4e49\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u751f\u6210\u66f4\u5177\u8868\u8fbe\u529b\u7684\u63d0\u793a\uff0c\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u6765\u81ea\u89c6\u89c9\u6a21\u6001\u7684\u7c7b\u522b\u589e\u5f3a\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u66f4\u5e7f\u6cdb\u7684\u672a\u89c1\u7c7b\u522b\u7684\u9c81\u68d2\u6027\u3002\u5bf9\u591a\u4e2a\u57fa\u51c6\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7279\u5b9a\u4efb\u52a1\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002||\n", "2410.03176": "|**2024-10-04**|[Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models](http://arxiv.org/abs/2410.03176)|**[link](https://github.com/yufang-liu/clip_hallucination)**|\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u5df2\u7ecf\u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u6027\u80fd\uff0c\u4f46\u7814\u7a76\u6307\u51fa\uff0c\u8fd9\u4e9b\u6a21\u578b\u5b58\u5728\u4e25\u91cd\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u8fd9\u4e9b\u5e7b\u89c9\u6e90\u81ea\u6a21\u578b\u7684\u54ea\u4e2a\u90e8\u5206\uff0c\u76ee\u524d\u8fd8\u6ca1\u6709\u660e\u786e\u7684\u7ed3\u8bba\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u6df1\u5165\u7814\u7a76\u4e86 CLIP \u6a21\u578b\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0cCLIP \u6a21\u578b\u662f\u8bb8\u591a\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u7cfb\u7edf\u7684\u652f\u67f1\u3002\u6211\u4eec\u63ed\u793a\u4e86\u5373\u4f7f\u662f\u5355\u72ec\u4f7f\u7528\uff0cCLIP \u6a21\u578b\u4e5f\u5bb9\u6613\u51fa\u73b0\u7269\u4f53\u5e7b\u89c9\uff0c\u8fd9\u8868\u660e\u5e7b\u89c9\u95ee\u9898\u4e0d\u4ec5\u4ec5\u662f\u7531\u4e8e\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\u4e4b\u95f4\u7684\u4ea4\u4e92\u9020\u6210\u7684\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u521b\u5efa\u5177\u6709\u5404\u79cd\u5e7b\u89c9\u95ee\u9898\u7684\u8d1f\u6837\u672c\u6765\u5b9e\u73b0\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u51cf\u8f7b CLIP \u6a21\u578b\u7684\u7269\u4f53\u5e7b\u89c9\uff0c\u5e76\u4e14\u6211\u4eec\u5c55\u793a\u4e86\u589e\u5f3a\u540e\u7684\u6a21\u578b\u53ef\u4ee5\u7528\u4f5c\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u6709\u6548\u5730\u7f13\u89e3\u4e86 LVLMs \u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\u3002||\n", "2410.03051": "|**2024-10-04**|[AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark](http://arxiv.org/abs/2410.03051)|null|\u89c6\u9891\u8be6\u7ec6\u5b57\u5e55\u751f\u6210\u662f\u4e00\u9879\u5173\u952e\u4efb\u52a1\uff0c\u65e8\u5728\u751f\u6210\u5bf9\u89c6\u9891\u5185\u5bb9\u5168\u9762\u800c\u8fde\u8d2f\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u6709\u5229\u4e8e\u89c6\u9891\u7406\u89e3\u548c\u751f\u6210\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 AuroraCap\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u89c6\u9891\u5b57\u5e55\u751f\u6210\u5668\u3002\u6211\u4eec\u9075\u5faa\u6700\u7b80\u5355\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u6ca1\u6709\u4e3a\u65f6\u95f4\u5efa\u6a21\u6dfb\u52a0\u989d\u5916\u7684\u53c2\u6570\u3002\u4e3a\u4e86\u89e3\u51b3\u957f\u89c6\u9891\u5e8f\u5217\u5e26\u6765\u7684\u5f00\u9500\uff0c\u6211\u4eec\u5b9e\u65bd\u4e86\u6807\u8bb0\u5408\u5e76\u7b56\u7565\uff0c\u51cf\u5c11\u4e86\u8f93\u5165\u89c6\u89c9\u6807\u8bb0\u7684\u6570\u91cf\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u6211\u4eec\u53d1\u73b0\u8fd9\u79cd\u7b56\u7565\u51e0\u4e4e\u6ca1\u6709\u9020\u6210\u6027\u80fd\u635f\u5931\u3002AuroraCap \u5728\u5404\u79cd\u89c6\u9891\u548c\u56fe\u50cf\u5b57\u5e55\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f8b\u5982\uff0c\u5728 Flickr30k \u4e0a\u83b7\u5f97\u4e86 88.9 \u7684 CIDEr \u5206\u6570\uff0c\u8d85\u8fc7\u4e86 GPT-4V (55.3) \u548c Gemini-1.5 Pro (82.2)\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u89c6\u9891\u5b57\u5e55\u57fa\u51c6\u6d4b\u8bd5\u53ea\u5305\u542b\u7b80\u5355\u7684\u63cf\u8ff0\uff0c\u7531\u51e0\u5341\u4e2a\u8bcd\u7ec4\u6210\uff0c\u8fd9\u9650\u5236\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5f00\u53d1\u4e86 VDC\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b\u4e00\u5343\u591a\u4e2a\u7cbe\u5fc3\u6807\u6ce8\u7684\u7ed3\u6784\u5316\u5b57\u5e55\u7684\u89c6\u9891\u8be6\u7ec6\u5b57\u5e55\u57fa\u51c6\u6d4b\u8bd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 LLM \u8f85\u52a9\u6307\u6807 VDCscore\uff0c\u7528\u4e8e\u6539\u8fdb\u8bc4\u4f30\uff0c\u8be5\u6307\u6807\u91c7\u7528\u5206\u6cbb\u7b56\u7565\u5c06\u957f\u5b57\u5e55\u8bc4\u4f30\u8f6c\u5316\u4e3a\u591a\u4e2a\u7b80\u77ed\u7684\u95ee\u7b54\u5bf9\u3002\u5728\u4eba\u5de5 Elo \u6392\u540d\u7684\u5e2e\u52a9\u4e0b\uff0c\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u4eba\u7c7b\u5bf9\u89c6\u9891\u8be6\u7ec6\u5b57\u5e55\u8d28\u91cf\u7684\u5224\u65ad\u5177\u6709\u66f4\u597d\u7684\u76f8\u5173\u6027\u3002||\n", "2410.03038": "|**2024-10-03**|[CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification](http://arxiv.org/abs/2410.03038)|null|\u5728\u77ed\u89c6\u9891\u5206\u7c7b\u4e2d\uff0c\u9488\u5bf9\u4e0d\u540c\u4e1a\u52a1\u573a\u666f\u5b9a\u5236\u7684\u5bc6\u96c6\u7279\u5f81\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5b83\u4eec\u7684\u590d\u6742\u6027\u3001\u7279\u5b9a\u7684\u9002\u5e94\u6027\u8981\u6c42\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u4f7f\u5f97\u5b83\u4eec\u5728\u5728\u7ebf\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8d44\u6e90\u5bc6\u96c6\u4e14\u96be\u4ee5\u8bbf\u95ee\u3002\u56e0\u6b64\uff0c\u8fd9\u4e9b\u5bc6\u96c6\u7279\u5f81\u88ab\u79f0\u4e3a\u201c\u7279\u6743\u5bc6\u96c6\u7279\u5f81\u201d\u3002\u540c\u65f6\uff0c\u7aef\u5230\u7aef\u591a\u6a21\u6001\u6a21\u578b\u5728\u4f17\u591a\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u826f\u597d\u7684\u6548\u679c\u3002\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\uff0c\u4f18\u5148\u8003\u8651\u7aef\u5230\u7aef\u591a\u6a21\u6001\u7279\u5f81\u53ef\u4ee5\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u5f80\u5f80\u4f1a\u5bfc\u81f4\u4e22\u5931\u5386\u53f2\u7279\u6743\u5bc6\u96c6\u7279\u5f81\u4e2d\u7684\u5b9d\u8d35\u4fe1\u606f\u3002\u4e3a\u4e86\u5728\u4fdd\u6301\u6548\u7387\u548c\u53ef\u7ba1\u7406\u7684\u8d44\u6e90\u6210\u672c\u7684\u540c\u65f6\u6574\u5408\u8fd9\u4e24\u79cd\u7279\u5f81\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u7279\u6743\u7279\u5f81\u84b8\u998f\uff08CPFD\uff09\uff0c\u5b83\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u5730\u63d0\u53d6\u7279\u6743\u7279\u5f81\u6765\u589e\u5f3a\u7aef\u5230\u7aef\u591a\u6a21\u6001\u6a21\u578b\u7684\u7279\u5f81\u3002\u4e0e\u73b0\u6709\u7684\u7279\u6743\u7279\u5f81\u84b8\u998f\uff08PFD\uff09\u65b9\u6cd5\u4e0d\u540c\uff0cCPFD\u4e0d\u4f1a\u5728\u84b8\u998f\u8fc7\u7a0b\u4e2d\u5bf9\u6240\u6709\u5b9e\u4f8b\u5e94\u7528\u7edf\u4e00\u7684\u6743\u91cd\uff08\u8fd9\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e0d\u540c\u4e1a\u52a1\u573a\u666f\u4e0b\u7684\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u4ee5\u53ca\u6559\u5e08\u6a21\u578b\uff08\u5bc6\u96c6\u7279\u5f81\u589e\u5f3a\u7684\u591a\u6a21\u6001\u6a21\u578bDF-X-VLM\uff09\u548c\u5b66\u751f\u6a21\u578b\uff08\u4ec5\u4f7f\u7528\u591a\u6a21\u6001\u6a21\u578bX-VLM\uff09\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff09\uff0c\u800c\u662f\u5229\u7528\u4ece\u6559\u5e08\u6a21\u578b\u4e2d\u83b7\u5f97\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\u6765\u81ea\u9002\u5e94\u5730\u51cf\u8f7b\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u5dee\u5f02\u3002\u6211\u4eec\u5728\u4e94\u4e2a\u4e0d\u540c\u7684\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u79bb\u7ebf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u7aef\u5230\u7aef\u591a\u6a21\u6001\u6a21\u578b\uff08X-VLM\uff09\u76f8\u6bd4\uff0cCPFD\u5c06\u89c6\u9891\u5206\u7c7b\u7684F1\u5206\u6570\u63d0\u9ad8\u4e866.76%\uff0c\u4e0e\u666e\u901a\u7684PFD\u76f8\u6bd4\u5e73\u5747\u63d0\u9ad8\u4e862.31%\u3002\u5b83\u5c06\u6027\u80fd\u5dee\u8ddd\u7f29\u5c0f\u4e8684.6%\uff0c\u5e76\u53d6\u5f97\u4e86\u4e0e\u6559\u5e08\u6a21\u578bDF-X-VLM\u76f8\u5f53\u7684\u7ed3\u679c\u3002\u5728\u7ebf\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86CPFD\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5df2\u7ecf\u90e8\u7f72\u5230\u751f\u4ea7\u7cfb\u7edf\u4e2d\uff0c\u7528\u4e8e\u5341\u591a\u4e2a\u6a21\u578b\u3002||\n", "2410.03010": "|**2024-10-03**|[MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection](http://arxiv.org/abs/2410.03010)|null|\u591a\u6a21\u6001\u5b66\u4e60\u65e8\u5728\u7ed3\u5408\u6765\u81ea\u591a\u4e2a\u8f93\u5165\u6e90\u7684\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u5982\u679c\u7f3a\u5c11\u67d0\u4e9b\u8f93\u5165\u6a21\u6001\uff0c\u6027\u80fd\u53ef\u80fd\u4f1a\u5927\u5e45\u4e0b\u964d\u3002\u73b0\u6709\u7684\u53ef\u4ee5\u5904\u7406\u7f3a\u5931\u6a21\u6001\u7684\u65b9\u6cd5\u5305\u62ec\u9488\u5bf9\u6bcf\u4e2a\u8f93\u5165\u6a21\u6001\u7ec4\u5408\u8fdb\u884c\u5b9a\u5236\u8bad\u7ec3\u6216\u9002\u5e94\u6b65\u9aa4\u3002\u8fd9\u4e9b\u65b9\u6cd5\u8981\u4e48\u7ed1\u5b9a\u5230\u7279\u5b9a\u7684\u6a21\u6001\uff0c\u8981\u4e48\u968f\u7740\u8f93\u5165\u6a21\u6001\u6570\u91cf\u7684\u589e\u52a0\u800c\u53d8\u5f97\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u63a9\u853d\u6a21\u6001\u6295\u5f71\uff08MMP\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65e8\u5728\u8bad\u7ec3\u5355\u4e2a\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u8be5\u6a21\u578b\u5bf9\u4efb\u4f55\u7f3a\u5931\u6a21\u6001\u573a\u666f\u90fd\u5177\u6709\u9c81\u68d2\u6027\u3002\u6211\u4eec\u901a\u8fc7\u5728\u8bad\u7ec3\u671f\u95f4\u968f\u673a\u63a9\u853d\u4e00\u90e8\u5206\u6a21\u6001\u5e76\u5b66\u4e60\u6295\u5f71\u53ef\u7528\u7684\u8f93\u5165\u6a21\u6001\u6765\u4f30\u8ba1\u63a9\u853d\u6a21\u6001\u7684\u6807\u8bb0\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u5b66\u4e60\u5229\u7528\u6765\u81ea\u53ef\u7528\u6a21\u6001\u7684\u4fe1\u606f\u6765\u8865\u507f\u7f3a\u5931\u7684\u6a21\u6001\uff0c\u4ece\u800c\u589e\u5f3a\u7f3a\u5931\u6a21\u6001\u7684\u9c81\u68d2\u6027\u3002\u6211\u4eec\u4f7f\u7528\u5404\u79cd\u57fa\u7ebf\u6a21\u578b\u548c\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u4e00\u7cfb\u5217\u5b9e\u9a8c\uff0c\u4ee5\u8bc4\u4f30\u8be5\u7b56\u7565\u7684\u6709\u6548\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5bf9\u4e0d\u540c\u7f3a\u5931\u6a21\u6001\u573a\u666f\u7684\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u4e3a\u7f3a\u5931\u6a21\u6001\u6216\u7279\u5b9a\u6a21\u6001\u7ec4\u5408\u8bbe\u8ba1\u7684\u73b0\u6709\u65b9\u6cd5\u3002||\n", "2410.02874": "|**2024-10-03**|[Real-World Cooking Robot System from Recipes Based on Food State Recognition Using Foundation Models and PDDL](http://arxiv.org/abs/2410.02874)|null|\u5c3d\u7ba1\u673a\u5668\u4eba\u70f9\u996a\u884c\u4e3a\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u57fa\u4e8e\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u5bf9\u65b0\u98df\u8c31\u63cf\u8ff0\u7684\u4e00\u7cfb\u5217\u70f9\u996a\u884c\u4e3a\u5c1a\u672a\u5b9e\u73b0\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u548c PDDL \u63cf\u8ff0\u7684\u7ecf\u5178\u89c4\u5212\u7684\u53ef\u6267\u884c\u7684\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u70f9\u996a\u884c\u4e3a\u89c4\u5212\uff0c\u4ee5\u53ca\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u4ece\u5c11\u91cf\u6570\u636e\u4e2d\u5b66\u4e60\u98df\u7269\u6210\u5206\u72b6\u6001\u8bc6\u522b\u3002\u6211\u4eec\u6210\u529f\u5730\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5728\u5b9e\u9a8c\u4e2d\uff0c\u53cc\u81c2\u8f6e\u5f0f\u673a\u5668\u4eba PR2 \u5728\u771f\u5b9e\u73af\u5883\u4e2d\u6839\u636e\u5b89\u6392\u7684\u65b0\u98df\u8c31\u8fdb\u884c\u70f9\u996a\uff0c\u5e76\u786e\u8ba4\u4e86\u6240\u63d0\u51fa\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002||\n", "2410.05270": "|**2024-10-07**|[Fine-Tuning CLIP's Last Visual Projector: A Few-Shot Cornucopia](http://arxiv.org/abs/2410.05270)|**[link](https://github.com/astra-vision/prolip)**|\u6211\u4eec\u7814\u7a76\u4e86\u5982\u4f55\u5c06\u50cf CLIP (Radford et al., 2021) \u8fd9\u6837\u7684\u5bf9\u6bd4\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u5c11\u6837\u672c\u5206\u7c7b\u95ee\u9898\u3002\u73b0\u6709\u6587\u732e\u901a\u8fc7\u5b66\u4e60\u51bb\u7ed3\u89c6\u89c9\u7279\u5f81\u7684\u7ebf\u6027\u5206\u7c7b\u5668\u3001\u4f18\u5316\u8bcd\u5d4c\u5165\u6216\u5b66\u4e60\u5916\u90e8\u7279\u5f81\u9002\u914d\u5668\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65e0\u9700\u6dfb\u52a0\u201c\u5916\u90e8\u201d\u53c2\u6570\u6765\u4f18\u5316 CLIP \u81ea\u9002\u5e94\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u4e0e\u73b0\u6709\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u7b80\u5355\u5730\u5fae\u8c03\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6700\u540e\u4e00\u4e2a\u6295\u5f71\u77e9\u9635\u5c31\u80fd\u83b7\u5f97\u5f3a\u5927\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u901a\u8fc7\u5fae\u8c03\u77e9\u9635\u548c\u9884\u8bad\u7ec3\u77e9\u9635\u4e4b\u95f4\u7684\u8ddd\u79bb\u5bf9\u8bad\u7ec3\u8fdb\u884c\u6b63\u5219\u5316\uff0c\u53ef\u4ee5\u63d0\u9ad8\u901a\u8fc7\u8be5\u5c42\u81ea\u9002\u5e94 CLIP \u7684\u53ef\u9760\u6027\u3002\u4e5f\u8bb8\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u8fd9\u79cd\u88ab\u79f0\u4e3a ProLIP \u7684\u65b9\u6cd5\u5728 11 \u4e2a\u5c11\u6837\u672c\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u3001\u5c11\u6837\u672c\u57df\u6cdb\u5316\u3001\u8de8\u6570\u636e\u96c6\u8fc1\u79fb\u548c\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u9762\u53d6\u5f97\u4e86\u4e0e\u6700\u5148\u8fdb\u6c34\u5e73\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002\u4ee3\u7801\u5c06\u5728 https://github.com/astra-vision/ProLIP \u4e0a\u63d0\u4f9b\u3002||\n", "2410.05261": "|**2024-10-07**|[TextHawk2: A Large Vision-Language Model Excels in Bilingual OCR and Grounding with 16x Fewer Tokens](http://arxiv.org/abs/2410.05261)|null|\u9605\u8bfb\u5bc6\u96c6\u6587\u672c\u548c\u5b9a\u4f4d\u56fe\u50cf\u4e2d\u7684\u7269\u4f53\u662f\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u6267\u884c\u9ad8\u7ea7\u4efb\u52a1\u7684\u57fa\u672c\u80fd\u529b\u3002\u4ee5\u524d\u7684 LVLM\uff0c\u5305\u62ec\u50cf GPT-4o \u8fd9\u6837\u7684\u4f18\u79c0\u4e13\u6709\u6a21\u578b\uff0c\u90fd\u96be\u4ee5\u540c\u65f6\u5728\u8fd9\u4e24\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u6b64\u5916\uff0c\u4ee5\u524d\u5177\u6709\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\u7684 LVLM \u6bcf\u5f20\u56fe\u50cf\u9700\u8981\u6d88\u8017\u6570\u5343\u4e2a\u6807\u8bb0\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u975e\u5e38\u6d88\u8017\u8d44\u6e90\u3002\u6211\u4eec\u63d0\u51fa\u4e86 TextHawk2\uff0c\u8fd9\u662f\u4e00\u79cd\u53cc\u8bed LVLM\uff0c\u5177\u6709\u9ad8\u6548\u7684\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u5728\u901a\u7528\u3001OCR \u548c grounding \u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u56fe\u50cf\u6807\u8bb0\u6570\u91cf\u51cf\u5c11\u4e86 16 \u500d\u3002\u5173\u952e\u6539\u8fdb\u5305\u62ec\uff1a(1) \u6807\u8bb0\u538b\u7f29\uff1aTextHawk2 \u5efa\u7acb\u5728\u5176\u524d\u8eab\u7684\u6709\u6548\u67b6\u6784\u4e4b\u4e0a\uff0c\u5c06\u6bcf\u5f20\u56fe\u50cf\u7684\u6807\u8bb0\u6570\u91cf\u663e\u8457\u51cf\u5c11\u4e86 16 \u500d\uff0c\u4ece\u800c\u80fd\u591f\u4ee5\u6700\u5c11\u7684\u8d44\u6e90\u4fc3\u8fdb TextHawk \u7cfb\u5217\u7684\u8bad\u7ec3\u548c\u90e8\u7f72\u3002(2) \u89c6\u89c9\u7f16\u7801\u5668\u589e\u5f3a\uff1a\u6211\u4eec\u901a\u8fc7 LVLM \u8054\u5408\u8bad\u7ec3\u589e\u5f3a\u4e86\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u4ece\u800c\u91ca\u653e\u4e86\u5176\u5728\u4e2d\u6587 OCR \u548c grounding \u7b49\u4ee5\u524d\u672a\u89c1\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002(3) \u6570\u636e\u591a\u6837\u6027\uff1a\u6211\u4eec\u5728\u4fdd\u6301 1 \u4ebf\u4e2a\u6837\u672c\u7684\u76f8\u5f53\u89c4\u6a21\u7684\u540c\u65f6\uff0c\u4f7f\u9884\u8bad\u7ec3\u6570\u636e\u7684\u6765\u6e90\u591a\u6837\u5316\u3002\u6211\u4eec\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc4\u4f30\u4e86 TextHawk2\uff0c\u5b83\u59cb\u7ec8\u5982\u4e00\u5730\u63d0\u4f9b\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u4f18\u4e8e\u7c7b\u4f3c\u89c4\u6a21\u7684\u95ed\u6e90\u6a21\u578b\uff0c\u4f8b\u5982\u5728 OCRBench \u4e0a\u5b9e\u73b0\u4e86 78.4% \u7684\u51c6\u786e\u7387\uff0c\u5728 ChartQA \u4e0a\u5b9e\u73b0\u4e86 81.4% \u7684\u51c6\u786e\u7387\uff0c\u5728 DocVQA \u4e0a\u5b9e\u73b0\u4e86 89.6% \u7684 ANLS\uff0c\u4ee5\u53ca\u5728 RefCOCOg-test \u4e0a\u5b9e\u73b0\u4e86 88.1% \u7684 accuracy@0.5\u3002||\n", "2410.05239": "|**2024-10-07**|[TuneVLSeg: Prompt Tuning Benchmark for Vision-Language Segmentation Models](http://arxiv.org/abs/2410.05239)|**[link](https://github.com/naamiinepal/tunevlseg)**|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5c06\u5176\u5e94\u7528\u4e8e\u65b0\u9886\u57df\u901a\u5e38\u9700\u8981\u6602\u8d35\u7684\u5fae\u8c03\u3002\u63d0\u793a\u8c03\u6574\u6280\u672f\uff0c\u5305\u62ec\u6587\u672c\u3001\u89c6\u89c9\u548c\u591a\u6a21\u6001\u63d0\u793a\uff0c\u901a\u8fc7\u5229\u7528\u53ef\u5b66\u4e60\u7684\u63d0\u793a\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u89c6\u89c9\u8bed\u8a00\u5206\u5272\u6a21\u578b (VLSM) \u4e2d\u7684\u5e94\u7528\u4ee5\u53ca\u5728\u663e\u8457\u9886\u57df\u8fc1\u79fb\u4e0b\u7684\u8bc4\u4f30\u4ecd\u6709\u5f85\u63a2\u7d22\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6 TuneVLSeg\uff0c\u5c06\u5404\u79cd\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u63d0\u793a\u8c03\u6574\u6280\u672f\u96c6\u6210\u5230 VLSM \u4e2d\uff0c\u4f7f\u5f97\u63d0\u793a\u8c03\u6574\u9002\u7528\u4e8e\u4efb\u4f55\u7c7b\u522b\u6570\u91cf\u7684\u4e0b\u6e38\u5206\u5272\u6570\u636e\u96c6\u3002TuneVLSeg \u5305\u62ec\u5728 2 \u4e2a VLSM \u4e2d\u4f7f\u7528\u7684\u4e0d\u540c\u63d0\u793a\u6df1\u5ea6\u4e0a\u7684 6 \u79cd\u63d0\u793a\u8c03\u6574\u7b56\u7565\uff0c\u603b\u5171 8 \u79cd\u4e0d\u540c\u7684\u7ec4\u5408\u3002\u6211\u4eec\u5728 8 \u4e2a\u4e0d\u540c\u7684\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e86\u5404\u79cd\u63d0\u793a\u8c03\u6574\uff0c\u5305\u62ec 3 \u4e2a\u653e\u5c04\u5b66\u6570\u636e\u96c6\uff08\u4e73\u817a\u80bf\u7624\u3001\u8d85\u58f0\u5fc3\u52a8\u56fe\u3001\u80f8\u90e8 X \u5149\u7247\u75c5\u53d8\uff09\u548c 5 \u4e2a\u975e\u653e\u5c04\u5b66\u6570\u636e\u96c6\uff08\u606f\u8089\u3001\u6e83\u75a1\u3001\u76ae\u80a4\u764c\uff09\uff0c\u4ee5\u53ca\u4e24\u4e2a\u81ea\u7136\u9886\u57df\u5206\u5272\u6570\u636e\u96c6\u3002\u6211\u4eec\u7684\u7814\u7a76\u53d1\u73b0\uff0c\u6587\u672c\u63d0\u793a\u8c03\u6574\u5728\u4ece\u81ea\u7136\u9886\u57df\u56fe\u50cf\u5230\u533b\u5b66\u6570\u636e\u7684\u663e\u8457\u9886\u57df\u8fc1\u79fb\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002\u6b64\u5916\uff0c\u4e0e\u591a\u6a21\u6001\u63d0\u793a\u8c03\u6574\u76f8\u6bd4\uff0c\u89c6\u89c9\u63d0\u793a\u8c03\u6574\u5177\u6709\u66f4\u5c11\u7684\u8d85\u53c2\u6570\uff0c\u901a\u5e38\u53ef\u4ee5\u5b9e\u73b0\u4e0e\u591a\u6a21\u6001\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u6210\u4e3a\u4e00\u79cd\u6709\u4ef7\u503c\u7684\u9996\u6b21\u5c1d\u8bd5\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u4fc3\u8fdb\u4e86\u5bf9\u4e0d\u540c\u63d0\u793a\u8c03\u6574\u6280\u672f\u5728\u9c81\u68d2\u7684\u7279\u5b9a\u9886\u57df\u5206\u5272\u4e2d\u7684\u7406\u89e3\u548c\u9002\u7528\u6027\u3002\u6e90\u4ee3\u7801\u53ef\u5728 https://github.com/naamiinepal/tunevlseg \u83b7\u53d6\u3002||\n", "2410.05191": "|**2024-10-07**|[LADEV: A Language-Driven Testing and Evaluation Platform for Vision-Language-Action Models in Robotic Manipulation](http://arxiv.org/abs/2410.05191)|null|\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u8fdb\u6b65\uff0c\u8fd1\u671f\u7684\u7814\u7a76\u5f15\u5165\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u4f5c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u96c6\u6210\u89e3\u51b3\u65b9\u6848\u3002\u8fd9\u4e9b\u6a21\u578b\u5c06\u76f8\u673a\u56fe\u50cf\u548c\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u6307\u4ee4\u4f5c\u4e3a\u8f93\u5165\uff0c\u76f4\u63a5\u751f\u6210\u673a\u5668\u4eba\u7684\u63a7\u5236\u52a8\u4f5c\u6765\u6267\u884c\u6307\u5b9a\u4efb\u52a1\uff0c\u6781\u5927\u5730\u63d0\u9ad8\u4e86\u51b3\u7b56\u80fd\u529b\u548c\u4e0e\u4eba\u7c7b\u7528\u6237\u7684\u4ea4\u4e92\u3002\u7136\u800c\uff0cVLA\u6a21\u578b\u7684\u6570\u636e\u9a71\u52a8\u7279\u6027\uff0c\u52a0\u4e0a\u5176\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u4f7f\u5f97\u786e\u4fdd\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u6210\u4e3a\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u8fd9\u7a81\u51fa\u4e86\u5bf9\u53ef\u9760\u6d4b\u8bd5\u548c\u8bc4\u4f30\u5e73\u53f0\u7684\u9700\u6c42\u3002\u4e3a\u6b64\uff0c\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86LADEV\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u8bc4\u4f30VLA\u6a21\u578b\u800c\u8bbe\u8ba1\u7684\u7efc\u5408\u9ad8\u6548\u5e73\u53f0\u3002\u6211\u4eec\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u8a00\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6839\u636e\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u81ea\u52a8\u751f\u6210\u4eff\u771f\u73af\u5883\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u624b\u52a8\u8c03\u6574\u7684\u9700\u6c42\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u6548\u7387\u3002\u7136\u540e\uff0c\u4e3a\u4e86\u8fdb\u4e00\u6b65\u8bc4\u4f30\u8bed\u8a00\u8f93\u5165\u5bf9VLA\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u79cd\u91ca\u4e49\u673a\u5236\uff0c\u53ef\u4ee5\u751f\u6210\u4e0d\u540c\u7684\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u6307\u4ee4\u8fdb\u884c\u6d4b\u8bd5\u3002\u6700\u540e\uff0c\u4e3a\u4e86\u52a0\u5feb\u8bc4\u4f30\u8fc7\u7a0b\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u6279\u91cf\u5f0f\u65b9\u6cd5\u6765\u5bf9VLA\u6a21\u578b\u8fdb\u884c\u5927\u89c4\u6a21\u6d4b\u8bd5\u3002\u4f7f\u7528LADEV\uff0c\u6211\u4eec\u5bf9\u51e0\u79cd\u6700\u5148\u8fdb\u7684VLA\u6a21\u578b\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u7684\u5de5\u5177\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cLADEV\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u6548\u7387\uff0c\u800c\u4e14\u4e3a\u8bc4\u4f30VLA\u6a21\u578b\u5efa\u7acb\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u4e3a\u5f00\u53d1\u66f4\u667a\u80fd\u3001\u66f4\u5148\u8fdb\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002||\n", "2410.05051": "|**2024-10-07**|[HE-Drive: Human-Like End-to-End Driving with Vision Language Models](http://arxiv.org/abs/2410.05051)|null|\u672c\u6587\u63d0\u51fa\u4e86HE-Drive\uff1a\u7b2c\u4e00\u4e2a\u4ee5\u7c7b\u4eba\u4e3a\u4e2d\u5fc3\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u4e14\u8212\u9002\u7684\u8f68\u8ff9\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u89c4\u5212\u5668\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u8f68\u8ff9\u8bc4\u5206\u5668\u53ef\u4ee5\u6709\u6548\u5730\u751f\u6210\u548c\u9009\u62e9\u4e0e\u4e13\u5bb6\u6f14\u793a\u975e\u5e38\u76f8\u4f3c\u7684\u7cbe\u786e\u8f68\u8ff9\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u8f68\u8ff9\u89c4\u5212\u5668\u548c\u8bc4\u5206\u5668\u9762\u4e34\u7740\u751f\u6210\u65f6\u95f4\u4e0d\u4e00\u81f4\u548c\u4e0d\u8212\u9002\u8f68\u8ff9\u7684\u56f0\u5883\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u6211\u4eec\u7684HE-Drive\u9996\u5148\u901a\u8fc7\u7a00\u758f\u611f\u77e5\u63d0\u53d6\u5173\u952e\u76843D\u7a7a\u95f4\u8868\u793a\uff0c\u7136\u540e\u5c06\u5176\u4f5c\u4e3a\u57fa\u4e8e\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPMs\uff09\u7684\u8fd0\u52a8\u89c4\u5212\u5668\u7684\u6761\u4ef6\u8f93\u5165\uff0c\u4ee5\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u7684\u591a\u6a21\u6001\u8f68\u8ff9\u3002\u968f\u540e\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5f15\u5bfc\u7684\u8f68\u8ff9\u8bc4\u5206\u5668\u4ece\u8fd9\u4e9b\u5019\u9009\u8f68\u8ff9\u4e2d\u9009\u62e9\u6700\u8212\u9002\u7684\u8f68\u8ff9\u6765\u63a7\u5236\u8f66\u8f86\uff0c\u786e\u4fdd\u7c7b\u4eba\u7684\u7aef\u5230\u7aef\u9a7e\u9a76\u3002\u5b9e\u9a8c\u8868\u660e\uff0cHE-Drive\u4e0d\u4ec5\u5728\u5177\u6709\u6311\u6218\u6027\u7684nuScenes\u548cOpenScene\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff08\u5373\u5c06\u5e73\u5747\u78b0\u649e\u7387\u964d\u4f4e\u4e8671%\u6bd4VAD\uff09\u548c\u6548\u7387\uff08\u5373\u6bd4SparseDrive\u5feb1.9\u500d\uff09\uff0c\u800c\u4e14\u5728\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u4e0a\u63d0\u4f9b\u4e86\u6700\u8212\u9002\u7684\u9a7e\u9a76\u4f53\u9a8c\u3002\u66f4\u591a\u4fe1\u606f\u8bf7\u8bbf\u95ee\u9879\u76ee\u7f51\u7ad9\uff1ahttps://jmwang0117.github.io/HE-Drive/\u3002||\n", "2410.04884": "|**2024-10-07**|[Patch is Enough: Naturalistic Adversarial Patch against Vision-Language Pre-training Models](http://arxiv.org/abs/2410.04884)|null|\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3 (VLP) \u6a21\u578b\u5728\u5404\u4e2a\u9886\u57df\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u653b\u51fb\u3002\u89e3\u51b3\u8fd9\u4e9b\u5bf9\u6297\u6027\u6f0f\u6d1e\u5bf9\u4e8e\u589e\u5f3a\u591a\u6a21\u6001\u5b66\u4e60\u7684\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u4e0a\uff0c\u9488\u5bf9 VLP \u6a21\u578b\u7684\u5bf9\u6297\u6027\u65b9\u6cd5\u6d89\u53ca\u540c\u65f6\u6270\u52a8\u56fe\u50cf\u548c\u6587\u672c\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u9762\u4e34\u7740\u663e\u8457\u7684\u6311\u6218\uff1a\u9996\u5148\uff0c\u5bf9\u6297\u6027\u6270\u52a8\u901a\u5e38\u65e0\u6cd5\u6709\u6548\u5730\u8f6c\u5316\u4e3a\u73b0\u5b9e\u573a\u666f\uff1b\u5176\u6b21\uff0c\u5bf9\u6587\u672c\u7684\u76f4\u63a5\u4fee\u6539\u975e\u5e38\u660e\u663e\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u4e13\u95e8\u4f7f\u7528\u56fe\u50cf\u8865\u4e01\u8fdb\u884c\u653b\u51fb\uff0c\u4ece\u800c\u4fdd\u6301\u539f\u59cb\u6587\u672c\u7684\u5b8c\u6574\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u6765\u81ea\u6269\u6563\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\u6765\u589e\u5f3a\u6270\u52a8\u7684\u771f\u5b9e\u6027\u548c\u81ea\u7136\u6027\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u4f18\u5316\u8865\u4e01\u653e\u7f6e\u5e76\u63d0\u9ad8\u653b\u51fb\u7684\u6548\u7387\uff0c\u6211\u4eec\u5229\u7528\u4e86\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8be5\u673a\u5236\u901a\u8fc7\u751f\u6210\u6ce8\u610f\u529b\u56fe\u6765\u5c01\u88c5\u6a21\u6001\u95f4\u4ea4\u4e92\uff0c\u4ee5\u6307\u5bfc\u6218\u7565\u6027\u8865\u4e01\u653e\u7f6e\u3002\u5728\u56fe\u50cf\u5230\u6587\u672c\u573a\u666f\u7684\u767d\u76d2\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5b9e\u73b0\u4e86 100% \u7684\u653b\u51fb\u6210\u529f\u7387\u3002\u6b64\u5916\uff0c\u5b83\u5728\u6d89\u53ca\u6587\u672c\u5230\u56fe\u50cf\u914d\u7f6e\u7684\u8fc1\u79fb\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa commendable \u7684\u6027\u80fd\u3002||\n", "2410.04107": "|**2024-10-05**|[TUBench: Benchmarking Large Vision-Language Models on Trustworthiness with Unanswerable Questions](http://arxiv.org/abs/2410.04107)|**[link](https://github.com/nlpcode/tubench)**|\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u5728\u89c6\u89c9\u611f\u77e5\u548c\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u5c3d\u7ba1\u5b83\u4eec\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46 LVLM \u4ecd\u7136\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u751f\u6210\u4e0e\u89c6\u89c9\u6216\u6587\u672c\u8f93\u5165\u4e0d\u6b63\u786e\u6216\u4e0d\u5fe0\u5b9e\u7684\u5185\u5bb9\u3002\u4f20\u7edf\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5982 MME \u548c POPE\uff0c\u4f7f\u7528\u53ef\u56de\u7b54\u7684\u95ee\u9898\u5728\u89c6\u89c9\u95ee\u7b54 (VQA) \u8303\u56f4\u5185\u8bc4\u4f30 LVLM \u4e2d\u7684\u5e7b\u89c9\u3002\u7136\u800c\uff0c\u7531\u4e8e\u56fe\u50cf\u4e2d\u4fe1\u606f\u4e0d\u8db3\uff0c\u6709\u4e9b\u95ee\u9898\u65e0\u6cd5\u56de\u7b54\uff0c\u800c LVLM \u5728\u6b64\u7c7b\u65e0\u6cd5\u56de\u7b54\u7684\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u7814\u7a76\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 TUBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u4f7f\u7528\u65e0\u6cd5\u56de\u7b54\u7684\u95ee\u9898\u8bc4\u4f30 LVLM \u53ef\u9760\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002TUBench \u5305\u542b\u5927\u91cf\u9ad8\u8d28\u91cf\u7684\u3001\u65e0\u6cd5\u56de\u7b54\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u662f\u4f7f\u7528\u5341\u79cd\u4e0d\u540c\u7684\u7b56\u7565\u7cbe\u5fc3\u5236\u4f5c\u7684\u3002\u4e3a\u4e86\u5168\u9762\u8bc4\u4f30 LVLM\uff0cTUBench \u4e2d\u7684\u65e0\u6cd5\u56de\u7b54\u7684\u95ee\u9898\u57fa\u4e8e\u6765\u81ea\u56db\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u56fe\u50cf\u4f5c\u4e3a\u89c6\u89c9\u4e0a\u4e0b\u6587\uff1a\u4ee3\u7801\u7247\u6bb5\u7684\u5c4f\u5e55\u622a\u56fe\u3001\u81ea\u7136\u56fe\u50cf\u3001\u51e0\u4f55\u56fe\u5f62\u548c\u7edf\u8ba1\u8868\u7684\u5c4f\u5e55\u622a\u56fe\u3002\u8fd9\u4e9b\u65e0\u6cd5\u56de\u7b54\u7684\u95ee\u9898\u5206\u522b\u7528\u4e8e\u6d4b\u8bd5 LVLM \u5728\u4ee3\u7801\u63a8\u7406\u3001\u5e38\u8bc6\u63a8\u7406\u3001\u51e0\u4f55\u63a8\u7406\u548c\u4e0e\u8868\u683c\u76f8\u5173\u7684\u6570\u5b66\u63a8\u7406\u65b9\u9762\u7684\u53ef\u4fe1\u5ea6\u3002\u6211\u4eec\u5bf9 TUBench \u4e0a\u7684 28 \u4e2a\u9886\u5148\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9a\u91cf\u8bc4\u4f30\uff0c\u5176\u4e2d\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b Gemini-1.5-Pro \u5728\u786e\u5b9a\u95ee\u9898\u662f\u5426\u53ef\u56de\u7b54\u65b9\u9762\u8fbe\u5230\u4e86 69.2% \u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u6392\u540d\u7b2c\u4e09\u7684\u6a21\u578b GPT-4o \u5219\u8fbe\u5230\u4e86 66.7% \u7684\u5e73\u5747\u51c6\u786e\u7387\u3002TUBench \u53ef\u5728 https://github.com/NLPCode/TUBench \u83b7\u53d6\u3002||\n", "2410.04055": "|**2024-10-05**|[Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks](http://arxiv.org/abs/2410.04055)|**[link](https://github.com/ivy3h/SCL)**|\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5728\u89c6\u89c9\u548c\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u975e\u51e1\u7684\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u4e5f\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u4ea7\u751f\u9519\u8bef\u7684\u54cd\u5e94\u3002\u81ea\u6211\u7ea0\u6b63\uff0c\u5373\u6307\u5bfc\u6a21\u578b\u6539\u8fdb\u5176\u8f93\u51fa\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f88\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4ee5\u5f80\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e0a\uff0c\u800c VLM \u7684\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4fe1\u606f\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u5f97\u5230\u68c0\u9a8c\u3002\u672c\u7814\u7a76\u8c03\u67e5\u4e86 VLM \u5728\u63a8\u7406\u548c\u5fae\u8c03\u9636\u6bb5\u7684\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\u3002\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u81ea\u6211\u7ea0\u6b63\u5b66\u4e60 (SCL) \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f VLM \u80fd\u591f\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316 (DPO) \u4ece\u5176\u81ea\u6211\u751f\u6210\u7684\u81ea\u6211\u7ea0\u6b63\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u53cd\u9988\uff0c\u4ece\u800c\u4fc3\u8fdb\u81ea\u6211\u6539\u8fdb\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u6839\u636e\u521d\u59cb\u548c\u6539\u8fdb\u54cd\u5e94\u7684\u6b63\u786e\u6027\u6536\u96c6\u504f\u597d\u548c\u4e0d\u504f\u597d\u7684\u6837\u672c\uff0c\u8fd9\u4e9b\u6837\u672c\u662f\u901a\u8fc7\u5728\u63a8\u7406\u9636\u6bb5\u4f7f\u7528 VLM \u8fdb\u884c\u4e24\u8f6e\u81ea\u6211\u7ea0\u6b63\u83b7\u5f97\u7684\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136 VLM \u5728\u6ca1\u6709\u989d\u5916\u5fae\u8c03\u548c\u5916\u90e8\u53cd\u9988\u7684\u60c5\u51b5\u4e0b\u96be\u4ee5\u5728\u8fed\u4ee3\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6709\u6548\u5730\u8fdb\u884c\u81ea\u6211\u7ea0\u6b63\uff0c\u4f46\u5f53\u5b83\u4eec\u81ea\u6211\u751f\u6210\u7684\u81ea\u6211\u7ea0\u6b63\u6570\u636e\u88ab\u5206\u7c7b\u4e3a\u504f\u597d\u548c\u4e0d\u504f\u597d\u6837\u672c\u65f6\uff0c\u5b83\u4eec\u53ef\u4ee5\u901a\u8fc7\u504f\u597d\u5fae\u8c03\u6765\u63d0\u9ad8\u6027\u80fd\u5e76\u907f\u514d\u4ee5\u524d\u7684\u9519\u8bef\u3002\u8fd9\u9879\u7814\u7a76\u5f3a\u8c03\uff0c\u81ea\u6211\u7ea0\u6b63\u4e0d\u4ec5\u4ec5\u662f\u4e00\u4e2a\u6539\u8fdb\u8fc7\u7a0b\uff1b\u76f8\u53cd\uff0c\u5b83\u5e94\u8be5\u901a\u8fc7\u989d\u5916\u7684\u8bad\u7ec3\u6765\u589e\u5f3a\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u76f4\u63a5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u54cd\u5e94\uff0c\u800c\u65e0\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002||\n", "2410.04038": "|**2024-10-05**|[Gamified crowd-sourcing of high-quality data for visual fine-tuning](http://arxiv.org/abs/2410.04038)|null|\u672c\u6587\u4ecb\u7ecd\u4e86\u6e38\u620f\u5316\u5bf9\u6297\u63d0\u793a (GAP)\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e3a\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u89c6\u89c9\u6307\u4ee4\u5fae\u8c03\u8fdb\u884c\u4f17\u5305\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u6846\u67b6\u3002GAP \u5c06\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\u8f6c\u5316\u4e3a\u5f15\u4eba\u5165\u80dc\u7684\u6e38\u620f\uff0c\u6fc0\u52b1\u73a9\u5bb6\u63d0\u4f9b\u9488\u5bf9\u6a21\u578b\u77e5\u8bc6\u5dee\u8ddd\u7684\u7ec6\u7c92\u5ea6\u3001\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u548c\u7b54\u6848\u3002\u6211\u4eec\u7684\u8d21\u732e\u5305\u62ec (1) \u4e00\u79cd\u4ece\u4eba\u7c7b\u90a3\u91cc\u6355\u83b7\u95ee\u7b54\u5bf9\u7684\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u95ee\u7b54\u5bf9\u76f4\u63a5\u9488\u5bf9\u6a21\u578b\u77e5\u8bc6\u4e2d\u7684\u5f31\u70b9\uff0c(2) \u4e00\u79cd\u8bc4\u4f30\u548c\u5956\u52b1\u73a9\u5bb6\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u6fc0\u52b1\u4ed6\u4eec\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u63d0\u4ea4\u5185\u5bb9\uff0c\u4ee5\u53ca (3) \u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6e38\u620f\u5316\u5e73\u53f0\uff0c\u8be5\u5e73\u53f0\u6210\u529f\u5730\u5728\u51e0\u5468\u5185\u4ece\u8d85\u8fc7 50,000 \u540d\u53c2\u4e0e\u8005\u90a3\u91cc\u6536\u96c6\u4e86\u8fd9\u4e9b\u6570\u636e\u3002\u6211\u4eec\u5bf9 GAP \u7684\u5b9e\u73b0\u663e\u7740\u63d0\u9ad8\u4e86\u5c0f\u578b\u591a\u6a21\u6001\u6a21\u578b MiniCPM-Llama3-V-2.5-8B \u7684\u51c6\u786e\u6027\uff0c\u5c06\u5176\u5728\u6211\u4eec\u6570\u636e\u96c6\u4e0a\u7684 GPT \u5206\u6570\u4ece 0.147 \u63d0\u9ad8\u5230 0.477\uff0c\u63a5\u8fd1\u66f4\u5927\u7684 GPT-4V \u6240\u8bbe\u5b9a\u7684\u57fa\u51c6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u4f7f\u7528 MiniCPM-Llama3-V-2.5-8B \u751f\u6210\u7684\u6570\u636e\u4e5f\u589e\u5f3a\u4e86\u5176\u5728\u5176\u4ed6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u8de8\u6a21\u578b\u7684\u4f18\u52bf\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u76f8\u540c\u7684\u6570\u636e\u63d0\u9ad8\u4e86 QWEN2-VL-2B \u548c QWEN2-VL-7B \u5728\u76f8\u540c\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\u3002||\n", "2410.03955": "|**2024-10-04**|[Model Developmental Safety: A Safety-Centric Method and Applications in Vision-Language Models](http://arxiv.org/abs/2410.03955)|**[link](https://github.com/ganglii/devsafety)**|\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u5b66\u4e60\u578b\u7cfb\u7edf\u901a\u5e38\u4f1a\u7ecf\u5386\u591a\u4e2a\u6a21\u578b\u5f00\u53d1\u5468\u671f\uff0c\u4ee5\u589e\u5f3a\u7cfb\u7edf\u5904\u7406\u56f0\u96be\u6216\u65b0\u51fa\u73b0\u4efb\u52a1\u7684\u80fd\u529b\u3002\u8fd9\u79cd\u6301\u7eed\u7684\u6a21\u578b\u5f00\u53d1\u8fc7\u7a0b\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u5373\u4e3a\u83b7\u53d6\u65b0\u80fd\u529b\u6216\u6539\u8fdb\u73b0\u6709\u80fd\u529b\u800c\u8fdb\u884c\u7684\u6a21\u578b\u5f00\u53d1\u53ef\u80fd\u4f1a\u65e0\u610f\u4e2d\u5931\u53bb\u65e7\u6a21\u578b\u7684\u80fd\u529b\uff0c\u4e5f\u79f0\u4e3a\u707e\u96be\u6027\u9057\u5fd8\u3002\u73b0\u6709\u7684\u6301\u7eed\u5b66\u4e60\u7814\u7a76\u4fa7\u91cd\u4e8e\u901a\u8fc7\u6743\u8861\u5148\u524d\u4efb\u52a1\u548c\u65b0\u4efb\u52a1\u7684\u6027\u80fd\u6765\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4ee5\u786e\u4fdd\u826f\u597d\u7684\u5e73\u5747\u6027\u80fd\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4e0d\u8db3\u4ee5\u7528\u4e8e\u8bb8\u591a\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u56e0\u4e3a\u672a\u80fd\u4e25\u683c\u4fdd\u6301\u65e7\u6a21\u578b\u7684\u6027\u80fd\u4e0d\u4ec5\u4f1a\u5e26\u6765\u5b89\u5168\u98ce\u9669\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd8\u4f1a\u5728\u91cd\u65b0\u6539\u8fdb\u548c\u91cd\u65b0\u9a8c\u8bc1\u73b0\u6709\u5c5e\u6027\u65b9\u9762\u9020\u6210\u5de8\u5927\u5f00\u9500\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6a21\u578b\u5f00\u53d1\u5b89\u5168\u4f5c\u4e3a\u5b66\u4e60\u7cfb\u7edf\u7684\u4fdd\u8bc1\uff0c\u5373\u5728\u6a21\u578b\u5f00\u53d1\u8fc7\u7a0b\u4e2d\uff0c\u65b0\u6a21\u578b\u5e94\u4e25\u683c\u4fdd\u7559\u65e7\u6a21\u578b\u73b0\u6709\u7684\u53d7\u4fdd\u62a4\u80fd\u529b\uff0c\u540c\u65f6\u63d0\u9ad8\u5176\u5728\u76ee\u6807\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u786e\u4fdd\u6a21\u578b\u5f00\u53d1\u5b89\u5168\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee5\u5b89\u5168\u4e3a\u4e2d\u5fc3\u7684\u6846\u67b6\uff0c\u5c06\u6a21\u578b\u5f00\u53d1\u5b89\u5168\u5236\u5b9a\u4e3a\u4f9d\u8d56\u4e8e\u6570\u636e\u7684\u7ea6\u675f\u3002\u5728\u8fd9\u4e2a\u6846\u67b6\u4e0b\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5982\u4f55\u5f00\u53d1\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u53c8\u79f0 CLIP \u6a21\u578b\uff09\uff0c\u4ee5\u83b7\u5f97\u65b0\u7684\u80fd\u529b\u6216\u6539\u8fdb\u73b0\u6709\u7684\u56fe\u50cf\u5206\u7c7b\u80fd\u529b\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u9ad8\u6548\u7ea6\u675f\u4f18\u5316\u7b97\u6cd5\uff0c\u5e76\u5229\u7528\u5176\u89c1\u89e3\u5fae\u8c03\u5177\u6709\u4efb\u52a1\u4f9d\u8d56\u5934\u7684 CLIP \u6a21\u578b\uff0c\u4ee5\u4fc3\u8fdb\u6a21\u578b\u5f00\u53d1\u5b89\u5168\u3002\u6211\u4eec\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u573a\u666f\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\u6539\u8fdb\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002||\n", "2410.08211": "|**2024-10-10**|[LatteCLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts](http://arxiv.org/abs/2410.08211)|null|\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3 (VLP) \u6a21\u578b\uff08\u4f8b\u5982 CLIP\uff09\u4ee5\u5176\u591a\u529f\u80fd\u6027\u800c\u95fb\u540d\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ef\u4ee5\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u5e94\u7528\u4e8e\u5404\u79cd\u5e94\u7528\u3002\u7136\u800c\uff0c\u5f53\u8fd9\u4e9b\u6a21\u578b\u7528\u4e8e\u7279\u5b9a\u9886\u57df\u65f6\uff0c\u7531\u4e8e\u9886\u57df\u5dee\u8ddd\u6216\u8bad\u7ec3\u6570\u636e\u4e2d\u8fd9\u4e9b\u9886\u57df\u7684\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u5b83\u4eec\u7684\u6027\u80fd\u5f80\u5f80\u4e0d\u5c3d\u5982\u4eba\u610f\u3002\u867d\u7136\u5728\u5177\u6709\u4eba\u5de5\u6807\u6ce8\u6807\u7b7e\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u5fae\u8c03 VLP \u6a21\u578b\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u5373\u4f7f\u662f\u6807\u6ce8\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\uff08\u4f8b\u5982\uff0c100k \u4e2a\u6837\u672c\uff09\u4e5f\u53ef\u80fd\u662f\u4e00\u9879\u6602\u8d35\u7684\u5de5\u4f5c\uff0c\u5982\u679c\u4efb\u52a1\u590d\u6742\uff0c\u901a\u5e38\u9700\u8981\u4e13\u5bb6\u6807\u6ce8\u5458\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 LatteCLIP\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u81ea\u5b9a\u4e49\u9886\u57df\u4e2d\u4f7f\u7528\u5df2\u77e5\u7c7b\u540d\u5bf9 CLIP \u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u5fae\u8c03\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u4e3a\u5355\u4e2a\u56fe\u50cf\u548c\u56fe\u50cf\u7ec4\u751f\u6210\u5bcc\u6709\u8868\u73b0\u529b\u7684\u6587\u672c\u63cf\u8ff0\u3002\u8fd9\u4e9b\u4fe1\u606f\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ee5\u6307\u5bfc\u81ea\u5b9a\u4e49\u9886\u57df\u4e2d\u7684\u5fae\u8c03\u8fc7\u7a0b\u3002\u7531\u4e8e LMM \u751f\u6210\u7684\u63cf\u8ff0\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u6216\u7ec6\u8282\u7f3a\u5931\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7b56\u7565\uff0c\u4ec5\u63d0\u53d6\u6709\u7528\u4fe1\u606f\u5e76\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u4ece\u566a\u58f0\u751f\u6210\u7684\u6587\u672c\u548c\u53cc\u91cd\u4f2a\u6807\u7b7e\u4e2d\u5b66\u4e60\u4e30\u5bcc\u7684\u6bcf\u7c7b\u539f\u578b\u8868\u793a\u3002\u6211\u4eec\u5728 10 \u4e2a\u7279\u5b9a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLatteCLIP \u7684\u6027\u80fd\u4f18\u4e8e\u9884\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u5e73\u5747\u63d0\u9ad8\u4e86 +4.74 \u4e2a\u767e\u5206\u70b9\u7684 top-1 \u51c6\u786e\u7387\uff0c\u5e76\u4e14\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65e0\u76d1\u7763\u65b9\u6cd5 +3.45 \u4e2a\u767e\u5206\u70b9\u3002||\n", "2410.08209": "|**2024-10-10**|[Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision](http://arxiv.org/abs/2410.08209)|null|\u5f53\u524d\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u9762\u4e34\u7740 grounding \u7684\u6311\u6218\uff0c grounding \u8981\u6c42\u6a21\u578b\u5c06\u8bed\u8a00\u6210\u5206\u4e0e\u89c6\u89c9\u5b9e\u4f53\u76f8\u5173\u8054\u3002\u4e0e\u4f7f\u7528\u989d\u5916\u7684 grounding \u76d1\u7763\u5fae\u8c03 LMM \u7684\u5e38\u89c1\u505a\u6cd5\u76f8\u53cd\uff0c\u6211\u4eec\u53d1\u73b0 grounding \u80fd\u529b\u5b9e\u9645\u4e0a\u53ef\u4ee5\u5728\u6ca1\u6709\u660e\u786e grounding \u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u8bad\u7ec3\u7684 LMM \u4e2d\u51fa\u73b0\u3002\u4e3a\u4e86\u63ed\u793a\u8fd9\u79cd\u65b0\u5174\u7684 grounding \u80fd\u529b\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u201cattend-and-segment\u201d\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u6765\u81ea\u6807\u51c6 LMM \u7684\u6ce8\u610f\u529b\u56fe\u6765\u6267\u884c\u50cf\u7d20\u7ea7\u5206\u5272\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u589e\u5f3a grounding \u80fd\u529b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DIFFLMM\uff0c\u8fd9\u662f\u4e00\u79cd\u5229\u7528\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u89c9\u7f16\u7801\u5668\uff08\u800c\u4e0d\u662f\u6807\u51c6 CLIP \u89c6\u89c9\u7f16\u7801\u5668\uff09\u7684 LMM\uff0c\u5e76\u4f7f\u7528\u76f8\u540c\u7684\u5f31\u76d1\u7763\u8fdb\u884c\u8bad\u7ec3\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u53d7\u9650\u4e8e grounding \u7279\u5b9a\u76d1\u7763\u6570\u636e\u7684\u504f\u5dee\u548c\u89c4\u6a21\u9650\u5236\uff0c\u56e0\u6b64\u66f4\u5177\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u4e0e grounding LMM \u548c\u901a\u624d LMM \u76f8\u6bd4\uff0c\u6211\u4eec\u5728 grounding \u7279\u5b9a\u548c\u4e00\u822c\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u5728\u6ca1\u6709\u4efb\u4f55 grounding \u76d1\u7763\u7684\u60c5\u51b5\u4e0b\uff0c\u5728 grounded \u5bf9\u8bdd\u751f\u6210\u65b9\u9762\u5b9e\u73b0\u4e86 44.2 \u7684 grounding \u63a9\u7801\u53ec\u56de\u7387\uff0c\u4f18\u4e8e\u7ecf\u8fc7\u5e7f\u6cdb\u76d1\u7763\u7684\u6a21\u578b GLaMM\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://groundLMM.github.io\u3002||\n", "2410.08182": "|**2024-10-10**|[MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models](http://arxiv.org/abs/2410.08182)|null|\u73b0\u6709\u7684\u591a\u6a21\u6001\u68c0\u7d22\u57fa\u51c6\u4e3b\u8981\u4fa7\u91cd\u4e8e\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u80fd\u591f\u68c0\u7d22\u548c\u5229\u7528\u5916\u90e8\u6587\u672c\u77e5\u8bc6\u6765\u56de\u7b54\u95ee\u9898\u3002\u7136\u800c\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u68c0\u7d22\u89c6\u89c9\u4fe1\u606f\u6bd4\u6587\u672c\u6570\u636e\u66f4\u6709\u76ca\u6216\u66f4\u5bb9\u6613\u83b7\u53d6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u57fa\u51c6 MRAG-Bench\uff0c\u5728\u8be5\u57fa\u51c6\u4e2d\uff0c\u6211\u4eec\u7cfb\u7edf\u5730\u8bc6\u522b\u548c\u5206\u7c7b\u4e86\u89c6\u89c9\u589e\u5f3a\u77e5\u8bc6\u4f18\u4e8e\u6587\u672c\u77e5\u8bc6\u7684\u573a\u666f\uff0c\u4f8b\u5982\uff0c\u6765\u81ea\u4e0d\u540c\u89c6\u89d2\u7684\u66f4\u591a\u56fe\u50cf\u3002MRAG-Bench \u7531 16,130 \u5f20\u56fe\u50cf\u548c 1,353 \u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u591a\u9879\u9009\u62e9\u9898\u7ec4\u6210\uff0c\u6db5\u76d6 9 \u4e2a\u4e0d\u540c\u7684\u573a\u666f\u3002\u501f\u52a9 MRAG-Bench\uff0c\u6211\u4eec\u5bf9 10 \u4e2a\u5f00\u6e90\u548c 4 \u4e2a\u4e13\u6709\u7684\u8d85\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6587\u672c\u77e5\u8bc6\u76f8\u6bd4\uff0c\u6240\u6709 LVLM \u5728\u4f7f\u7528\u56fe\u50cf\u589e\u5f3a\u65f6\u90fd\u8868\u73b0\u51fa\u66f4\u5927\u7684\u6539\u8fdb\uff0c\u8fd9\u8bc1\u5b9e\u4e86 MRAG-Bench \u4ee5\u89c6\u89c9\u4e3a\u4e2d\u5fc3\u7684\u7279\u70b9\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4f7f\u7528 MRAG-Bench \u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5206\u6790\uff0c\u4e3a\u4e86\u89e3\u68c0\u7d22\u589e\u5f3a\u578b LVLM \u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b GPT-4o \u5728\u6709\u6548\u5229\u7528\u68c0\u7d22\u5230\u7684\u77e5\u8bc6\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u5728\u4f7f\u7528\u771f\u5b9e\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u4ec5\u5b9e\u73b0\u4e86 5.82% \u7684\u6539\u8fdb\uff0c\u800c\u4eba\u7c7b\u53c2\u4e0e\u8005\u89c2\u5bdf\u5230\u7684\u6539\u8fdb\u4e3a 33.16%\u3002\u8fd9\u4e9b\u53d1\u73b0\u7a81\u51fa\u4e86 MRAG-Bench \u5728\u9f13\u52b1\u793e\u533a\u589e\u5f3a LVLM \u66f4\u6709\u6548\u5730\u5229\u7528\u68c0\u7d22\u5230\u7684\u89c6\u89c9\u77e5\u8bc6\u65b9\u9762\u7684\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002||\n", "2410.08119": "|**2024-10-10**|[Q-VLM: Post-training Quantization for Large Vision-Language Models](http://arxiv.org/abs/2410.08119)|**[link](https://github.com/changyuanwang17/qvlm)**|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLMs) \u7684\u8bad\u7ec3\u540e\u91cf\u5316\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u6a21\u6001\u63a8\u7406\u3002\u4f20\u7edf\u7684\u91cf\u5316\u65b9\u6cd5\u901a\u8fc7\u6700\u5c0f\u5316\u6fc0\u6d3b\u79bb\u6563\u5316\u8bef\u5dee\u6765\u987a\u5e8f\u641c\u7d22\u9010\u5c42\u820d\u5165\u51fd\u6570\uff0c\u8fd9\u79cd\u65b9\u6cd5\u7531\u4e8e\u6ca1\u6709\u8003\u8651\u8de8\u5c42\u4f9d\u8d56\u6027\uff0c\u56e0\u6b64\u65e0\u6cd5\u83b7\u5f97\u6700\u4f73\u91cf\u5316\u7b56\u7565\u3002\u76f8\u53cd\uff0c\u6211\u4eec\u6316\u6398\u4e86\u5bf9\u6574\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u79bb\u6563\u5316\u8bef\u5dee\u6709\u663e\u8457\u5f71\u54cd\u7684\u8de8\u5c42\u4f9d\u8d56\u6027\uff0c\u5e76\u5c06\u8fd9\u79cd\u4f9d\u8d56\u6027\u5d4c\u5165\u5230\u4f4e\u641c\u7d22\u6210\u672c\u7684\u6700\u4f73\u91cf\u5316\u7b56\u7565\u641c\u7d22\u4e2d\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u6fc0\u6d3b\u71b5\u548c\u8de8\u5c42\u4f9d\u8d56\u6027\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u8fd9\u4e0e\u8f93\u51fa\u79bb\u6563\u5316\u8bef\u5dee\u6709\u5173\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u91c7\u7528\u71b5\u4f5c\u4e3a\u4ee3\u7406\u6765\u4f18\u5316\u5206\u533a\u5757\uff0c\u65e8\u5728\u5728\u79bb\u6563\u5316\u8bef\u5dee\u548c\u641c\u7d22\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u4ee4\u4eba\u6ee1\u610f\u7684\u5e73\u8861\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4f18\u5316\u4e86\u89c6\u89c9\u7f16\u7801\u5668\u4ee5\u89e3\u8026\u8de8\u5c42\u4f9d\u8d56\u6027\uff0c\u4ece\u800c\u5bf9\u641c\u7d22\u7a7a\u95f4\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u89e3\uff0c\u4ece\u800c\u5728\u4e0d\u635f\u5bb3\u91cf\u5316\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u8fdb\u4e00\u6b65\u964d\u4f4e\u641c\u7d22\u6210\u672c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e0d\u964d\u4f4e\u5404\u79cd\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u5927\u7ea6 13B LLaVA \u6a21\u578b\u7684\u5185\u5b58\u538b\u7f29\u4e86 2.78 \u500d\uff0c\u5e76\u5c06\u751f\u6210\u901f\u5ea6\u63d0\u9ad8\u4e86 1.44 \u500d\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/ChangyuanWang17/QVLM \u83b7\u53d6\u3002||\n", "2410.07880": "|**2024-10-10**|[Unsupervised Data Validation Methods for Efficient Model Training](http://arxiv.org/abs/2410.07880)|null|\u672c\u6587\u63a2\u8ba8\u4e86\u6539\u8fdb\u4f4e\u8d44\u6e90\u8bed\u8a00\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u6240\u9762\u4e34\u7684\u6311\u6218\u548c\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP)\u3001\u6587\u672c\u5230\u8bed\u97f3 (TTS)\u3001\u8bed\u97f3\u5230\u6587\u672c (STT) \u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u4e2d\u7684\u6700\u65b0\u6a21\u578b\u4e25\u91cd\u4f9d\u8d56\u4e8e\u5927\u578b\u6570\u636e\u96c6\uff0c\u800c\u8fd9\u4e9b\u6570\u636e\u96c6\u901a\u5e38\u4e0d\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5173\u952e\u9886\u57df\uff0c\u4f8b\u5982\u5b9a\u4e49\u201c\u9ad8\u8d28\u91cf\u6570\u636e\u201d\u3001\u5f00\u53d1\u751f\u6210\u9002\u5f53\u6570\u636e\u7684\u65b9\u6cd5\u4ee5\u53ca\u589e\u5f3a\u6a21\u578b\u8bad\u7ec3\u7684\u53ef\u8bbf\u95ee\u6027\u3002\u5bf9\u5f53\u524d\u65b9\u6cd5\u7684\u5168\u9762\u56de\u987e\uff0c\u5305\u62ec\u6570\u636e\u589e\u5f3a\u3001\u591a\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u3001\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u6570\u636e\u9009\u62e9\u6280\u672f\uff0c\u7a81\u51fa\u4e86\u8fdb\u6b65\u548c\u5c40\u9650\u6027\u3002\u786e\u5b9a\u4e86\u51e0\u4e2a\u5f00\u653e\u7684\u7814\u7a76\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u65e8\u5728\u4f18\u5316\u6570\u636e\u5229\u7528\u3001\u51cf\u5c11\u6240\u9700\u6570\u636e\u91cf\u548c\u4fdd\u6301\u9ad8\u8d28\u91cf\u6a21\u578b\u6027\u80fd\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6846\u67b6\u3002\u901a\u8fc7\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u6587\u65e8\u5728\u4f7f\u4f4e\u8d44\u6e90\u8bed\u8a00\u66f4\u5bb9\u6613\u83b7\u5f97\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4ece\u800c\u589e\u5f3a\u5176\u5728\u5404\u4e2a\u9886\u57df\u7684\u6548\u7528\u548c\u5f71\u54cd\u529b\u3002||\n", "2410.07854": "|**2024-10-10**|[HeGraphAdapter: Tuning Multi-Modal Vision-Language Models with Heterogeneous Graph Adapter](http://arxiv.org/abs/2410.07854)|null|\u57fa\u4e8e\u9002\u914d\u5668\u7684\u8c03\u4f18\u65b9\u6cd5\u5728\u5c06\u77e5\u8bc6\u4ece\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fc1\u79fb\u5230\u4e0b\u6e38\u4efb\u52a1\u65b9\u9762\u5df2\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\u3002\u7136\u800c\uff0c\u5728\u56de\u987e\u73b0\u6709\u7684\u9002\u914d\u5668\u540e\uff0c\u6211\u4eec\u53d1\u73b0\u5b83\u4eec\u901a\u5e38\u65e0\u6cd5\u5145\u5206\u63a2\u7d22\u6784\u5efa\u7279\u5b9a\u4efb\u52a1\u77e5\u8bc6\u65f6\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002\u6b64\u5916\uff0c\u73b0\u6709\u5de5\u4f5c\u901a\u5e38\u53ea\u5173\u6ce8\u6b63\u6587\u672c\u63d0\u793a\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u5339\u914d\uff0c\u8fd9\u4f7f\u5f97\u533a\u5206\u5177\u6709\u9ad8\u5ea6\u76f8\u4f3c\u89c6\u89c9\u5185\u5bb9\u7684\u7c7b\u522b\u53d8\u5f97\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f02\u6784\u56fe\u9002\u914d\u5668\u6765\u5b9e\u73b0\u4e0b\u6e38\u4efb\u52a1\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5f02\u6784\u56fe\u6a21\u5f0f\uff0c\u5b83\u5305\u542b i) \u89c6\u89c9\u8282\u70b9\u3001\u6b63\u6587\u672c\u8282\u70b9\u548c\u8d1f\u6587\u672c\u8282\u70b9\uff0c\u4ee5\u53ca ii) \u51e0\u79cd\u7c7b\u578b\u7684\u8fb9\u8fde\u63a5\uff0c\u4ee5\u5168\u9762\u5730\u5bf9\u6a21\u6001\u5185\u3001\u6a21\u6001\u95f4\u548c\u7c7b\u95f4\u7ed3\u6784\u77e5\u8bc6\u8fdb\u884c\u5efa\u6a21\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u91c7\u7528\u7279\u5b9a\u7684\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\u6765\u6316\u6398\u591a\u6a21\u6001\u7ed3\u6784\u77e5\u8bc6\uff0c\u4ee5\u4fbf\u4e3a\u4e0b\u6e38\u4efb\u52a1\u8c03\u6574\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\u3002\u6700\u540e\uff0c\u5728HeGraphAdapter\u4e4b\u540e\uff0c\u6211\u4eec\u540c\u65f6\u6784\u5efa\u57fa\u4e8e\u6587\u672c\u548c\u57fa\u4e8e\u89c6\u89c9\u7684\u5206\u7c7b\u5668\uff0c\u4ee5\u5168\u9762\u63d0\u5347CLIP\u6a21\u578b\u7684\u6027\u80fd\u3002\u5728 11 \u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684 HeGraphAdapter \u7684\u6709\u6548\u6027\u548c\u4f18\u52bf\u3002||\n", "2410.07648": "|**2024-10-10**|[FLIER: Few-shot Language Image Models Embedded with Latent Representations](http://arxiv.org/abs/2410.07648)|null|\u968f\u7740\u50cf\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3 (CLIP) \u8fd9\u6837\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8bb8\u591a\u7c7b\u4f3c CLIP \u7684\u65b9\u6cd5\u5728\u89c6\u89c9\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u3002\u7136\u800c\uff0c\u6211\u4eec\u6ce8\u610f\u5230\u5927\u591a\u6570\u8fd9\u4e9b\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5bf9\u6587\u672c\u548c\u56fe\u50cf\u7f16\u7801\u5668\u8fdb\u884c\u65b0\u7684\u4fee\u6539\u3002\u6700\u8fd1\uff0c\u6f5c\u5728\u6269\u6563\u6a21\u578b (LDM) \u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u826f\u597d\u7684\u80fd\u529b\u3002LDM \u7684\u5f3a\u5927\u80fd\u529b\u5c06\u6211\u4eec\u7684\u6ce8\u610f\u529b\u5f15\u5411\u4e86 UNet \u91c7\u6837\u7684\u6f5c\u5728\u8868\u793a\u3002\u53d7 CoOp \u4e2d\u5b66\u4e60\u5230\u7684\u63d0\u793a\u7f16\u7801\u8d85\u51fa\u73b0\u6709\u8bcd\u6c47\u91cf\u7684\u542b\u4e49\u7684\u731c\u60f3\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5047\u8bbe\uff0c\u5bf9\u4e8e\u6df1\u5ea6\u6a21\u578b\uff0c\u6f5c\u5728\u8868\u793a\u662f\u5bf9\u56fe\u50cf\u7684\u7b80\u6d01\u51c6\u786e\u7684\u7406\u89e3\uff0c\u5176\u4e2d\u62bd\u8c61\u6389\u4e86\u9ad8\u9891\u7684\u3001\u4e0d\u53ef\u611f\u77e5\u7684\u7ec6\u8282\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u6f5c\u5728\u8868\u793a\u7684\u5c11\u6837\u672c\u8bed\u8a00\u56fe\u50cf\u6a21\u578b (FLIER)\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u4e0e CLIP \u7684\u56fe\u50cf\u7f16\u7801\u5668\u8054\u5408\u8bad\u7ec3\u7684\u6f5c\u5728\u7f16\u7801\u5668\u6765\u8fdb\u884c\u56fe\u50cf\u8bc6\u522b\uff0c\u5b83\u7ed3\u5408\u4e86 CLIP \u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u77e5\u8bc6\u548c\u7a33\u5b9a\u6269\u6563\u7684\u6f5c\u5728\u8868\u793a\u3002\u6211\u4eec\u9996\u5148\u901a\u8fc7\u7a33\u5b9a\u6269\u6563\u4f7f\u7528 GPT-3 \u7684\u6587\u672c\u8f93\u5165\u751f\u6210\u56fe\u50cf\u548c\u76f8\u5e94\u7684\u6f5c\u5728\u8868\u793a\u3002\u5c06\u6f5c\u5728\u8868\u793a\u4f5c\u4e3a\u201c\u6a21\u578b\u53ef\u7406\u89e3\u7684\u50cf\u7d20\u201d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u5177\u6709\u4e24\u4e2a\u5377\u79ef\u5c42\u7684\u7075\u6d3b\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u6f5c\u5728\u7f16\u7801\u5668\uff0c\u5b83\u6bd4\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5927\u591a\u6570\u7f16\u7801\u5668\u90fd\u7b80\u5355\u3002\u6f5c\u5728\u7f16\u7801\u5668\u4e0e CLIP \u7684\u56fe\u50cf\u7f16\u7801\u5668\u8054\u5408\u8bad\u7ec3\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u5c06\u9884\u8bad\u7ec3\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u4e0b\u6e38\u4efb\u52a1\u3002\u5728\u5404\u79cd\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u548c\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0cFLIER \u5728\u5927\u591a\u6570\u5c11\u6837\u672c\u5206\u7c7b\u7684 11 \u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002||\n", "2410.07593": "|**2024-10-10**|[A Unified Debiasing Approach for Vision-Language Models across Modalities and Tasks](http://arxiv.org/abs/2410.07593)|null|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u6700\u65b0\u8fdb\u5c55\u4f7f\u5f97\u901a\u8fc7\u540c\u65f6\u5904\u7406\u6587\u672c\u548c\u56fe\u50cf\u6570\u636e\u6765\u5b8c\u6210\u590d\u6742\u7684\u591a\u6a21\u6001\u4efb\u52a1\u6210\u4e3a\u53ef\u80fd\uff0c\u4ece\u800c\u663e\u8457\u589e\u5f3a\u4e86\u4eba\u5de5\u667a\u80fd\u9886\u57df\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u7ecf\u5e38\u8868\u73b0\u51fa\u504f\u5dee\uff0c\u8fd9\u4e9b\u504f\u5dee\u4f1a\u5bfc\u81f4\u8f93\u51fa\u504f\u5411\u793e\u4f1a\u523b\u677f\u5370\u8c61\uff0c\u56e0\u6b64\u9700\u8981\u53bb\u504f\u5dee\u7b56\u7565\u3002\u73b0\u6709\u7684\u53bb\u504f\u5dee\u65b9\u6cd5\u72ed\u9698\u5730\u5173\u6ce8\u7279\u5b9a\u7684\u6a21\u6001\u6216\u4efb\u52a1\uff0c\u5e76\u4e14\u9700\u8981\u5927\u91cf\u7684\u518d\u8bad\u7ec3\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u672c\u6587\u4ecb\u7ecd\u4e86\u7528\u4e8e\u53bb\u504f\u5dee\u7684\u9009\u62e9\u6027\u7279\u5f81\u63d2\u8865 (SFID)\uff0c\u8fd9\u662f\u4e00\u79cd\u96c6\u6210\u4e86\u7279\u5f81\u526a\u679d\u548c\u4f4e\u7f6e\u4fe1\u5ea6\u63d2\u8865 (LCI) \u7684\u65b0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u51cf\u5c11 VLM \u4e2d\u7684\u504f\u5dee\u3002SFID \u5177\u6709\u591a\u79cd\u529f\u80fd\uff0c\u53ef\u4ee5\u4fdd\u6301\u8f93\u51fa\u7684\u8bed\u4e49\u5b8c\u6574\u6027\uff0c\u5e76\u4e14\u901a\u8fc7\u6d88\u9664\u91cd\u65b0\u8bad\u7ec3\u7684\u9700\u8981\u6765\u8282\u7701\u6210\u672c\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86 SFID \u5728\u5404\u79cd VLM \u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u96f6\u6837\u672c\u5206\u7c7b\u3001\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u3001\u56fe\u50cf\u5b57\u5e55\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u901a\u8fc7\u5728\u4e0d\u5f71\u54cd\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u663e\u7740\u51cf\u5c11\u6027\u522b\u504f\u5dee\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u589e\u5f3a\u4e86 VLM \u5e94\u7528\u7684\u516c\u5e73\u6027\uff0c\u800c\u4e14\u8fd8\u4fdd\u7559\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002||\n", "2410.07577": "|**2024-10-10**|[3D Vision-Language Gaussian Splatting](http://arxiv.org/abs/2410.07577)|null|\u8fd1\u5e74\u6765\uff0c\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u4e09\u7ef4\u573a\u666f\u7406\u89e3\u7684\u53d1\u5c55\uff0c\u8fd9\u5728\u673a\u5668\u4eba\u6280\u672f\u3001\u81ea\u52a8\u9a7e\u9a76\u4ee5\u53ca\u865a\u62df/\u589e\u5f3a\u73b0\u5b9e\u4e2d\u5177\u6709\u81f3\u5173\u91cd\u8981\u7684\u5e94\u7528\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u591a\u6a21\u6001\u573a\u666f\u7406\u89e3\u65b9\u6cd5\u7b80\u5355\u5730\u5c06\u8bed\u4e49\u8868\u793a\u5d4c\u5165\u5230\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\u4e2d\uff0c\u800c\u6ca1\u6709\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u8fd9\u5bfc\u81f4\u534a\u900f\u660e\u6216\u53cd\u5c04\u6027\u7269\u4f53\u7684\u8bed\u4e49\u6805\u683c\u5316\u6548\u679c\u4e0d\u7406\u60f3\uff0c\u4ee5\u53ca\u5bf9\u989c\u8272\u6a21\u6001\u7684\u8fc7\u5ea6\u62df\u5408\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5145\u5206\u5904\u7406\u4e0d\u540c\u89c6\u89c9\u548c\u8bed\u4e49\u6a21\u6001\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5373\u7528\u4e8e\u573a\u666f\u7406\u89e3\u7684\u4e09\u7ef4\u89c6\u89c9\u8bed\u8a00\u9ad8\u65af\u6563\u5c04\u6a21\u578b\uff0c\u4ee5\u5f3a\u8c03\u8bed\u8a00\u6a21\u6001\u7684\u8868\u793a\u5b66\u4e60\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8de8\u6a21\u6001\u6805\u683c\u5316\u5668\uff0c\u4f7f\u7528\u6a21\u6001\u878d\u5408\u4ee5\u53ca\u5e73\u6ed1\u8bed\u4e49\u6307\u793a\u5668\u6765\u589e\u5f3a\u8bed\u4e49\u6805\u683c\u5316\u3002\u6211\u4eec\u8fd8\u91c7\u7528\u4e86\u76f8\u673a\u89c6\u56fe\u6df7\u5408\u6280\u672f\u6765\u63d0\u9ad8\u73b0\u6709\u89c6\u56fe\u548c\u5408\u6210\u89c6\u56fe\u4e4b\u95f4\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u6709\u6548\u5730\u51cf\u8f7b\u8fc7\u5ea6\u62df\u5408\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002||\n", "2410.07391": "|**2024-10-09**|[The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks](http://arxiv.org/abs/2410.07391)|null|\u4eba\u4eec\u8d8a\u6765\u8d8a\u5173\u6ce8\u8ffd\u8e2a\u901a\u7528\u4eba\u5de5\u667a\u80fd\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\u3002\u672c\u7814\u7a76\u4ee5\u97e6\u6c0f\u6210\u4eba\u667a\u529b\u91cf\u8868\uff08WAIS-IV\uff09\u4e3a\u57fa\u51c6\uff0c\u5c06\u9886\u5148\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u8868\u73b0\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002WAIS-IV\u662f\u4e00\u79cd\u5168\u9762\u3001\u4ee5\u4eba\u7fa4\u4e3a\u89c4\u8303\u7684\u6f5c\u5728\u4eba\u7c7b\u8ba4\u77e5\u548c\u667a\u529b\u80fd\u529b\u8bc4\u4f30\uff0c\u91cd\u70b9\u5173\u6ce8\u8bed\u8a00\u7406\u89e3\uff08VCI\uff09\u3001\u5de5\u4f5c\u8bb0\u5fc6\uff08WMI\uff09\u548c\u77e5\u89c9\u63a8\u7406\uff08PRI\uff09\u9886\u57df\u3002\u5927\u591a\u6570\u6a21\u578b\u5728\u5b58\u50a8\u3001\u68c0\u7d22\u548c\u5904\u7406\u8bf8\u5982\u5b57\u6bcd\u548c\u6570\u5b57\u7684\u4efb\u610f\u5e8f\u5217\u7b49token\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u80fd\u529b\uff0c\u4e0e\u4eba\u7c7b\u7fa4\u4f53\u89c4\u8303\u80fd\u529b\u76f8\u6bd4\uff0c\u5de5\u4f5c\u8bb0\u5fc6\u6307\u6570\uff08WMI\uff09\u7684\u8868\u73b0\u7b49\u4e8e\u6216\u5927\u4e8e99.5%\u3002\u8bed\u8a00\u7406\u89e3\u6307\u6570\uff08VCI\uff09\u8861\u91cf\u7684\u662f\u5bf9\u83b7\u5f97\u4fe1\u606f\u7684\u68c0\u7d22\uff0c\u4ee5\u53ca\u5bf9\u5355\u8bcd\u542b\u4e49\u53ca\u5176\u76f8\u4e92\u5173\u7cfb\u7684\u8bed\u8a00\u7406\u89e3\uff0c\u5176\u8868\u73b0\u4e5f\u59cb\u7ec8\u4fdd\u6301\u572898%\u6216\u4ee5\u4e0a\u3002\u5c3d\u7ba1\u6709\u8fd9\u4e9b\u5e7f\u6cdb\u7684\u4f18\u52bf\uff0c\u4f46\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u591a\u6a21\u6001\u6a21\u578b\u5728\u77e5\u89c9\u63a8\u7406\u6307\u6570\uff08PRI\uff1b\u8303\u56f40.1-10%\uff09\u4e0a\u7684\u8868\u73b0\u4e00\u76f4\u5f88\u5dee\uff0c\u8fd9\u8868\u660e\u5176\u5728\u89e3\u91ca\u548c\u63a8\u7406\u89c6\u89c9\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u4e0d\u8db3\u3002\u8f83\u5c0f\u548c\u8f83\u65e7\u7684\u6a21\u578b\u7248\u672c\u7684\u8868\u73b0\u59cb\u7ec8\u8f83\u5dee\uff0c\u8fd9\u8868\u660e\u8bad\u7ec3\u6570\u636e\u3001\u53c2\u6570\u6570\u91cf\u548c\u5fae\u8c03\u65b9\u9762\u7684\u8fdb\u6b65\u6b63\u5728\u5bfc\u81f4\u8ba4\u77e5\u80fd\u529b\u7684\u663e\u8457\u8fdb\u6b65\u3002||\n", "2410.08901": "|**2024-10-11**|[SegGrasp: Zero-Shot Task-Oriented Grasping via Semantic and Geometric Guided Segmentation](http://arxiv.org/abs/2410.08901)|null|\u9762\u5411\u4efb\u52a1\u7684\u6293\u53d6\uff0c\u5373\u6839\u636e\u7269\u4f53\u529f\u80fd\u6293\u53d6\u5176\u7279\u5b9a\u90e8\u4f4d\uff0c\u5bf9\u4e8e\u5f00\u53d1\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u5148\u8fdb\u673a\u5668\u4eba\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u514d\u8bad\u7ec3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u8bed\u4e49\u548c\u51e0\u4f55\u5148\u9a8c\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u9762\u5411\u4efb\u52a1\u7684\u6293\u53d6\u751f\u6210\u3002\u6240\u63d0\u51fa\u7684\u6846\u67b6\u540d\u4e3a SegGrasp\uff0c\u9996\u5148\u5229\u7528 GLIP \u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7c97\u5206\u5272\u3002\u7136\u540e\uff0c\u5b83\u4f7f\u7528\u6765\u81ea\u51f8\u5206\u89e3\u7684\u8be6\u7ec6\u51e0\u4f55\u4fe1\u606f\uff0c\u901a\u8fc7\u540d\u4e3a GeoFusion \u7684\u878d\u5408\u7b56\u7565\u6765\u63d0\u9ad8\u5206\u5272\u8d28\u91cf\u3002\u901a\u8fc7\u6539\u8fdb\u5206\u5272\u7684\u6293\u53d6\u7f51\u7edc\u53ef\u4ee5\u751f\u6210\u6709\u6548\u7684\u6293\u53d6\u59ff\u6001\u3002\u6211\u4eec\u5728\u5206\u5272\u57fa\u51c6\u548c\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u6293\u53d6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSegGrasp \u5728\u6293\u53d6\u548c\u5206\u5272\u6027\u80fd\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf 15% \u4ee5\u4e0a\u3002||\n", "2410.08895": "|**2024-10-11**|[Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation](http://arxiv.org/abs/2410.08895)|null|\u57fa\u4e8e\u7f13\u5b58\u7684\u65b9\u6cd5\u5728\u9002\u5e94\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u65b9\u9762\u8868\u73b0\u51fa\u8272\u4e14\u9ad8\u6548\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u7f13\u5b58\u6a21\u578b\u5ffd\u7565\u4e86\u4e09\u4e2a\u5173\u952e\u65b9\u9762\u30021) \u9884\u8bad\u7ec3\u7684 VLM \u4e3b\u8981\u9488\u5bf9\u56fe\u50cf-\u6587\u672c\u76f8\u4f3c\u6027\u8fdb\u884c\u4f18\u5316\uff0c\u5ffd\u7565\u4e86\u56fe\u50cf-\u56fe\u50cf\u76f8\u4f3c\u6027\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u9884\u8bad\u7ec3\u548c\u9002\u5e94\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u30022) \u5f53\u524d\u7684\u7f13\u5b58\u6a21\u578b\u57fa\u4e8e Nadaraya-Watson (N-W) \u4f30\u8ba1\u5668\uff0c\u5b83\u5728\u6784\u5efa\u6743\u91cd\u51fd\u6570\u65f6\u5ffd\u7565\u4e86\u8bad\u7ec3\u6837\u672c\u4e4b\u95f4\u9519\u7efc\u590d\u6742\u7684\u5173\u7cfb\u30023) \u5728\u6837\u672c\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u7f13\u5b58\u6a21\u578b\u751f\u6210\u7684 logits \u5177\u6709\u5f88\u9ad8\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u76f4\u63a5\u4f7f\u7528\u8fd9\u4e9b logits \u800c\u4e0d\u8003\u8651\u7f6e\u4fe1\u5ea6\u53ef\u80fd\u4f1a\u6709\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u6311\u6218\uff0c\u672c\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e09\u4e2a\u6821\u51c6\u6a21\u5757\u3002\u76f8\u4f3c\u6027\u6821\u51c6\u901a\u8fc7\u4f7f\u7528\u672a\u6807\u8bb0\u7684\u56fe\u50cf\u6765\u6539\u8fdb\u56fe\u50cf-\u56fe\u50cf\u76f8\u4f3c\u6027\u3002\u6211\u4eec\u5728 CLIP \u7684\u9884\u8bad\u7ec3\u56fe\u50cf\u7f16\u7801\u5668\u4e4b\u4e0a\u6dfb\u52a0\u4e86\u4e00\u4e2a\u5e26\u6709\u6b8b\u5dee\u8fde\u63a5\u7684\u53ef\u5b66\u4e60\u6295\u5f71\u5c42\uff0c\u5e76\u901a\u8fc7\u6700\u5c0f\u5316\u81ea\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\u6765\u4f18\u5316\u53c2\u6570\u3002\u6743\u91cd\u6821\u51c6\u5728\u6743\u91cd\u51fd\u6570\u4e2d\u5f15\u5165\u4e86\u4e00\u4e2a\u7cbe\u5ea6\u77e9\u9635\uff0c\u4ee5\u5145\u5206\u6a21\u62df\u8bad\u7ec3\u6837\u672c\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5c06\u73b0\u6709\u7684\u7f13\u5b58\u6a21\u578b\u8f6c\u6362\u4e3a\u9ad8\u65af\u8fc7\u7a0b (GP) \u56de\u5f52\u5668\uff0c\u8fd9\u53ef\u80fd\u6bd4 N-W \u4f30\u8ba1\u5668\u66f4\u51c6\u786e\u3002\u7f6e\u4fe1\u5ea6\u6821\u51c6\u5229\u7528 GP \u56de\u5f52\u8ba1\u7b97\u7684\u9884\u6d4b\u65b9\u5dee\u6765\u52a8\u6001\u5730\u91cd\u65b0\u8c03\u6574\u7f13\u5b58\u6a21\u578b\u7684 logits\uff0c\u786e\u4fdd\u7f13\u5b58\u6a21\u578b\u7684\u8f93\u51fa\u6839\u636e\u5176\u7f6e\u4fe1\u5ea6\u8fdb\u884c\u9002\u5f53\u8c03\u6574\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u964d\u4f4e GP \u7684\u9ad8\u590d\u6742\u5ea6\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ec4\u7684\u5b66\u4e60\u7b56\u7565\u3002\u6574\u5408\u4e0a\u8ff0\u8bbe\u8ba1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u514d\u8bad\u7ec3\u548c\u9700\u8981\u8bad\u7ec3\u7684\u4e24\u79cd\u53d8\u4f53\u3002\u5728 11 \u4e2a\u5c11\u6837\u672c\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002||\n", "2410.08876": "|**2024-10-11**|[RoRA-VLM: Robust Retrieval-Augmented Vision Language Models](http://arxiv.org/abs/2410.08876)|null|\u76ee\u524d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u4ecd\u7136\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u96be\u4ee5\u5c06\u89c6\u89c9\u5bf9\u8c61\u548c\u573a\u666f\u4e0e\u5176\u5bf9\u5e94\u7684\u5b9e\u4f53\u548c\u80cc\u666f\u77e5\u8bc6\u4e4b\u95f4\u7684\u6240\u6709\u5173\u8054\u8fdb\u884c\u51c6\u786e\u7f16\u7801\u3002\u867d\u7136\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u96c6\u6210\u5916\u90e8\u77e5\u8bc6\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u5c06\u5176\u6269\u5c55\u5230\u89c6\u89c9\u8bed\u8a00\u9886\u57df\u5b58\u5728\u7740\u72ec\u7279\u7684\u6311\u6218\uff1a(1) \u7531\u4e8e\u591a\u6a21\u6001\u67e5\u8be2\u4e2d\u56fa\u6709\u7684\u5dee\u5f02\uff0c\u96be\u4ee5\u4ece\u5916\u90e8\u6765\u6e90\u51c6\u786e\u68c0\u7d22\u76f8\u5173\u4fe1\u606f\uff1b(2) \u96be\u4ee5\u62b5\u6297\u68c0\u7d22\u5230\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u7247\u6bb5\u4e2d\u5305\u542b\u7684\u65e0\u5173\u3001\u591a\u4f59\u548c\u5608\u6742\u7684\u4fe1\u606f\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 RORA-VLM\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u4e3a VLM \u91cf\u8eab\u5b9a\u5236\u7684\u65b0\u9896\u4e14\u5f3a\u5927\u7684\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u5b83\u5177\u6709\u4e24\u9879\u5173\u952e\u521b\u65b0\uff1a(1) \u4e00\u79cd\u91c7\u7528\u56fe\u50cf\u951a\u5b9a\u6587\u672c\u67e5\u8be2\u6269\u5c55\u7684\u4e24\u9636\u6bb5\u68c0\u7d22\u8fc7\u7a0b\uff0c\u4ee5\u534f\u540c\u7ec4\u5408\u67e5\u8be2\u4e2d\u7684\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\uff0c\u5e76\u68c0\u7d22\u6700\u76f8\u5173\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u7247\u6bb5\uff1b(2) \u4e00\u79cd\u9c81\u68d2\u7684\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u68c0\u7d22\u589e\u5f3a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u5bf9\u6297\u6027\u566a\u58f0\uff0c\u589e\u5f3a VLM \u5bf9\u68c0\u7d22\u5230\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u4e2d\u65e0\u5173\u4fe1\u606f\u7684\u62b5\u6297\u529b\uff0c\u5e76\u901a\u8fc7\u9762\u5411\u67e5\u8be2\u7684\u89c6\u89c9\u6807\u8bb0\u4f18\u5316\u7b56\u7565\u8fc7\u6ee4\u6389\u65e0\u5173\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u4f8b\u5982\u56fe\u50cf\u4e2d\u5448\u73b0\u7684\u65e0\u5173\u5b9e\u4f53\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u4ee5\u9a8c\u8bc1\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e09\u4e2a\u5e7f\u6cdb\u91c7\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u53ea\u9700\u6781\u5c11\u7684\u8bad\u7ec3\u5b9e\u4f8b\uff0cRORA-VLM \u5c31\u53ef\u4ee5\u4f7f\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u68c0\u7d22\u589e\u5f3a VLM\uff0c\u540c\u65f6\u8fd8\u5c55\u73b0\u51fa\u65b0\u9896\u7684\u96f6\u6837\u672c\u57df\u8fc1\u79fb\u80fd\u529b\u3002||\n", "2410.08792": "|**2024-10-11**|[VLM See, Robot Do: Human Demo Video to Robot Action Plan via Vision Language Model](http://arxiv.org/abs/2410.08792)|null|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u8fd1\u671f\u56e0\u5176\u5728\u5e38\u8bc6\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u4f18\u52bf\u88ab\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u9886\u57df\u3002\u73b0\u6709\u5de5\u4f5c\u5df2\u5c06 VLM \u5e94\u7528\u4e8e\u4ece\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u751f\u6210\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u4ee5\u53ca\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u6a21\u62df\u8bad\u7ec3\u6570\u636e\u3002\u5728\u672c\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u4f7f\u7528 VLM \u6765\u89e3\u91ca\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u5e76\u751f\u6210\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u5173\u952e\u5e27\u9009\u62e9\u3001\u89c6\u89c9\u611f\u77e5\u548c VLM \u63a8\u7406\u96c6\u6210\u5230\u4e00\u4e2a\u7ba1\u9053\u4e2d\u3002\u6211\u4eec\u5c06\u5176\u547d\u540d\u4e3a SeeDo\uff0c\u56e0\u4e3a\u5b83\u4f7f VLM \u80fd\u591f\u201c\u770b\u5230\u201d\u4eba\u7c7b\u6f14\u793a\u5e76\u5411\u673a\u5668\u4eba\u89e3\u91ca\u76f8\u5e94\u7684\u8ba1\u5212\uff0c\u4ee5\u4fbf\u673a\u5668\u4eba\u201c\u6267\u884c\u201d\u3002\u4e3a\u4e86\u9a8c\u8bc1\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u6536\u96c6\u4e86\u4e00\u7ec4\u957f\u65f6\u7a0b\u4eba\u7c7b\u89c6\u9891\uff0c\u6f14\u793a\u4e86\u4e09\u79cd\u4e0d\u540c\u7c7b\u522b\u4e2d\u7684\u62fe\u653e\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u5957\u6307\u6807\uff0c\u4ee5\u5168\u9762\u6bd4\u8f83 SeeDo \u4e0e\u51e0\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff08\u5305\u62ec\u6700\u5148\u8fdb\u7684\u89c6\u9891\u8f93\u5165 VLM\uff09\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e SeeDo \u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5728\u4eff\u771f\u73af\u5883\u548c\u771f\u5b9e\u7684\u673a\u5668\u4eba\u624b\u81c2\u4e0a\u90e8\u7f72\u4e86\u751f\u6210\u7684\u7684\u4efb\u52a1\u8ba1\u5212\u3002||\n", "2410.08791": "|**2024-10-11**|[Superpipeline: A Universal Approach for Reducing GPU Memory Usage in Large Models](http://arxiv.org/abs/2410.08791)|**[link](https://github.com/abbasireza/super-pipeline)**|\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\uff0c\u7ed9\u5728\u8d44\u6e90\u6709\u9650\u7684\u786c\u4ef6\u4e0a\u8fd0\u884c\u8fd9\u4e9b\u6a21\u578b\u5e26\u6765\u4e86\u6311\u6218\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 Superpipeline\uff0c\u8fd9\u662f\u4e00\u4e2a\u65e8\u5728\u4f18\u5316\u5927\u578b AI \u6a21\u578b\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5728\u53d7\u9650\u786c\u4ef6\u4e0a\u6267\u884c\u7684\u65b0\u6846\u67b6\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6d89\u53ca\u901a\u8fc7\u5c06\u6a21\u578b\u5212\u5206\u4e3a\u5355\u72ec\u7684\u5c42\u5e76\u6709\u6548\u5730\u5728 GPU \u548c CPU \u5185\u5b58\u4e4b\u95f4\u4f20\u8f93\u8fd9\u4e9b\u5c42\u6765\u52a8\u6001\u7ba1\u7406\u6a21\u578b\u6267\u884c\u3002\u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d\uff0cSuperpipeline \u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u548c\u53ef\u63a5\u53d7\u7684\u5904\u7406\u901f\u5ea6\u7684\u540c\u65f6\uff0c\u5c06 GPU \u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c11\u4e86\u9ad8\u8fbe 60%\u3002\u8fd9\u4f7f\u5f97\u539f\u672c\u4f1a\u8d85\u51fa\u53ef\u7528 GPU \u5185\u5b58\u7684\u6a21\u578b\u80fd\u591f\u6709\u6548\u8fd0\u884c\u3002\u4e0e\u4e3b\u8981\u5173\u6ce8\u63a8\u7406\u6216\u7279\u5b9a\u6a21\u578b\u7c7b\u578b\u7684\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4e0d\u540c\uff0cSuperpipeline \u53ef\u4ee5\u5e94\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM)\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u548c\u57fa\u4e8e\u89c6\u89c9\u7684\u6a21\u578b\u3002\u6211\u4eec\u5728\u5404\u79cd\u6a21\u578b\u548c\u786c\u4ef6\u8bbe\u7f6e\u4e2d\u6d4b\u8bd5\u4e86 Superpipeline \u7684\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u53c2\u6570\uff0c\u5141\u8bb8\u5fae\u8c03 GPU \u5185\u5b58\u4f7f\u7528\u91cf\u548c\u5904\u7406\u901f\u5ea6\u4e4b\u95f4\u7684\u5e73\u8861\u3002\u91cd\u8981\u7684\u662f\uff0cSuperpipeline \u4e0d\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6216\u66f4\u6539\u6a21\u578b\u53c2\u6570\uff0c\u786e\u4fdd\u539f\u59cb\u6a21\u578b\u7684\u8f93\u51fa\u4fdd\u6301\u4e0d\u53d8\u3002Superpipeline \u7684\u7b80\u5355\u6027\u548c\u7075\u6d3b\u6027\u4f7f\u5176\u5bf9\u5728\u6709\u9650\u786c\u4ef6\u4e0a\u4f7f\u7528\u9ad8\u7ea7 AI \u6a21\u578b\u7684\u7814\u7a76\u4eba\u5458\u548c\u4e13\u4e1a\u4eba\u58eb\u975e\u5e38\u6709\u7528\u3002\u5b83\u5141\u8bb8\u5728\u73b0\u6709\u786c\u4ef6\u4e0a\u4f7f\u7528\u66f4\u5927\u7684\u6a21\u578b\u6216\u66f4\u5927\u7684\u6279\u6b21\u5927\u5c0f\uff0c\u4ece\u800c\u6709\u53ef\u80fd\u52a0\u5feb\u8bb8\u591a\u673a\u5668\u5b66\u4e60\u5e94\u7528\u7684\u521b\u65b0\u3002\u8fd9\u9879\u5de5\u4f5c\u6807\u5fd7\u7740\u671d\u7740\u4f7f\u9ad8\u7ea7 AI \u6a21\u578b\u66f4\u6613\u4e8e\u8bbf\u95ee\u5e76\u5728\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e2d\u4f18\u5316\u5176\u90e8\u7f72\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002Superpipeline \u7684\u4ee3\u7801\u53ef\u5728 https://github.com/abbasiReza/super-pipeline \u83b7\u53d6\u3002||\n", "2410.08695": "|**2024-10-11**|[Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping](http://arxiv.org/abs/2410.08695)|null|\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u7b49\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u975e\u51e1\u7684\u80fd\u529b\uff0c\u5728\u5404\u79cd\u591a\u6a21\u6001\u8bc4\u4f30\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u4fdd\u6301\u7740\u9759\u6001\u6027\uff0c\u5e76\u4e14\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u91cd\u53e0\uff0c\u5bfc\u81f4\u590d\u6742\u5ea6\u9650\u5236\u56fa\u5b9a\u548c\u6570\u636e\u6c61\u67d3\u95ee\u9898\u3002\u8fd9\u5f15\u53d1\u4e86\u5bf9\u8bc4\u4f30\u6709\u6548\u6027\u7684\u62c5\u5fe7\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e24\u9879\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u79f0\u4e3a\u89c6\u89c9\u8bed\u8a00\u81ea\u4e3e\uff08VLB\uff09\u7684\u52a8\u6001\u591a\u6a21\u6001\u8bc4\u4f30\u534f\u8bae\u3002VLB \u4e3a LVLM \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u4e14\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u51cf\u5c11\u4e86\u6570\u636e\u6c61\u67d3\uff0c\u5e76\u5177\u6709\u7075\u6d3b\u7684\u590d\u6742\u6027\u3002\u4e3a\u6b64\uff0cVLB \u901a\u8fc7\u591a\u6a21\u6001\u81ea\u4e3e\u6a21\u5757\u52a8\u6001\u751f\u6210\u65b0\u7684\u89c6\u89c9\u95ee\u7b54\u6837\u672c\uff0c\u8be5\u6a21\u5757\u4fee\u6539\u56fe\u50cf\u548c\u8bed\u8a00\uff0c\u540c\u65f6\u901a\u8fc7\u5224\u65ad\u6a21\u5757\u786e\u4fdd\u65b0\u751f\u6210\u7684\u6837\u672c\u4e0e\u539f\u59cb\u6837\u672c\u4fdd\u6301\u4e00\u81f4\u3002\u901a\u8fc7\u7ec4\u5408\u5404\u79cd\u81ea\u4e3e\u7b56\u7565\uff0cVLB \u63d0\u4f9b\u4e86\u5177\u6709\u4e0d\u540c\u590d\u6742\u6027\u7684\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u52a8\u6001\u53d8\u4f53\uff0c\u4f7f\u8bc4\u4f30\u80fd\u591f\u968f\u7740 LVLM \u4e0d\u65ad\u53d1\u5c55\u7684\u80fd\u529b\u800c\u5171\u540c\u53d1\u5c55\u3002\u8de8\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec SEEDBench\u3001MMBench \u548c MME\uff09\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVLB \u663e\u7740\u51cf\u5c11\u4e86\u6570\u636e\u6c61\u67d3\uff0c\u5e76\u66b4\u9732\u4e86 LVLM \u7684\u6027\u80fd\u5c40\u9650\u6027\u3002||\n", "2410.08611": "|**2024-10-11**|[Conjugated Semantic Pool Improves OOD Detection with Pre-trained Vision-Language Models](http://arxiv.org/abs/2410.08611)|**[link](https://github.com/mengyuanchen21/neurips2024-csp)**|\u96f6\u6837\u672c\u5206\u5e03\u5916 (OOD) \u68c0\u6d4b\u7684\u76f4\u63a5 pipeline \u6d89\u53ca\u4ece\u5e7f\u6cdb\u7684\u8bed\u4e49\u5e93\u4e2d\u9009\u62e9\u6f5c\u5728\u7684 OOD \u6807\u7b7e\uff0c\u7136\u540e\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u5206\u5e03\u5185 (ID) \u548c OOD \u6807\u7b7e\u6267\u884c\u5206\u7c7b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u7406\u8bba\uff0c\u8ba4\u4e3a\u63d0\u9ad8\u6027\u80fd\u9700\u8981\u6269\u5c55\u8bed\u4e49\u5e93\uff0c\u540c\u65f6\u589e\u52a0 OOD \u6837\u672c\u6fc0\u6d3b\u6240\u9009 OOD \u6807\u7b7e\u7684\u9884\u671f\u6982\u7387\uff0c\u5e76\u786e\u4fdd\u8fd9\u4e9b OOD \u6807\u7b7e\u7684\u6fc0\u6d3b\u4e4b\u95f4\u76f8\u4e92\u4f9d\u8d56\u6027\u4f4e\u3002\u4e00\u79cd\u81ea\u7136\u7684\u6269\u5c55\u65b9\u5f0f\u662f\u91c7\u7528\u66f4\u5927\u7684\u8bcd\u5e93\uff1b\u7136\u800c\uff0c\u4e0d\u53ef\u907f\u514d\u5730\u5f15\u5165\u5927\u91cf\u540c\u4e49\u8bcd\u548c\u4e0d\u5e38\u7528\u8bcd\u65e0\u6cd5\u6ee1\u8db3\u4e0a\u8ff0\u8981\u6c42\uff0c\u8fd9\u8868\u660e\u53ef\u884c\u7684\u6269\u5c55\u65b9\u5f0f\u4e0d\u4ec5\u4ec5\u662f\u4ece\u8bcd\u5e93\u4e2d\u9009\u62e9\u8bcd\u8bed\u3002\u7531\u4e8e OOD \u68c0\u6d4b\u65e8\u5728\u5c06\u8f93\u5165\u56fe\u50cf\u6b63\u786e\u5206\u7c7b\u5230 ID/OOD \u7c7b\u522b\u7ec4\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u201c\u7f16\u9020\u201dOOD \u6807\u7b7e\u5019\u9009\uff0c\u8fd9\u4e9b\u5019\u9009\u4e0d\u662f\u6807\u51c6\u7c7b\u522b\u540d\u79f0\uff0c\u4f46\u6709\u5229\u4e8e\u8be5\u8fc7\u7a0b\u3002\u89c2\u5bdf\u5230\u539f\u59cb\u8bed\u4e49\u5e93\u7531\u672a\u4fee\u6539\u7684\u7279\u5b9a\u7c7b\u522b\u540d\u79f0\u7ec4\u6210\uff0c\u6211\u4eec\u76f8\u5e94\u5730\u6784\u5efa\u4e86\u4e00\u4e2a\u5171\u8f6d\u8bed\u4e49\u5e93 (CSP)\uff0c\u5b83\u7531\u4fee\u6539\u540e\u7684\u8d85\u7c7b\u522b\u540d\u79f0\u7ec4\u6210\uff0c\u6bcf\u4e2a\u540d\u79f0\u90fd\u5145\u5f53\u8de8\u4e0d\u540c\u7c7b\u522b\u5171\u4eab\u76f8\u4f3c\u5c5e\u6027\u7684\u6837\u672c\u7684\u805a\u7c7b\u4e2d\u5fc3\u3002\u4e0e\u6211\u4eec\u5efa\u7acb\u7684\u7406\u8bba\u4e00\u81f4\uff0c\u4f7f\u7528 CSP \u6269\u5c55 OOD \u6807\u7b7e\u5019\u9009\u6ee1\u8db3\u8981\u6c42\uff0c\u5e76\u4e14\u5728 FPR95 \u4e2d\u7684\u6027\u80fd\u6bd4\u73b0\u6709\u5de5\u4f5c\u63d0\u9ad8\u4e86 7.89%\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/MengyuanChen21/NeurIPS2024-CSP \u4e2d\u83b7\u5f97\u3002||\n", "2410.08584": "|**2024-10-11**|[ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification and KV Cache Compression](http://arxiv.org/abs/2410.08584)|null|\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLMs) \u7684\u6548\u7387\u53d7\u5230\u9884\u586b\u5145\u9636\u6bb5\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\u74f6\u9888\u548c\u89e3\u7801\u9636\u6bb5\u83b7\u53d6\u952e\u503c (KV) \u7f13\u5b58\u7684\u5185\u5b58\u74f6\u9888\u7684\u9650\u5236\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u6216\u89c6\u9891\u7684\u60c5\u51b5\u4e0b\u3002\u89c6\u89c9\u5185\u5bb9\u901a\u5e38\u8868\u73b0\u51fa\u5927\u91cf\u7684\u5197\u4f59\uff0c\u5bfc\u81f4 LVLMs \u4e2d\u7684\u6ce8\u610f\u529b\u56fe\u9ad8\u5ea6\u7a00\u758f\u3002\u53ef\u4ee5\u5229\u7528\u8fd9\u79cd\u7a00\u758f\u6027\uff0c\u901a\u8fc7\u5404\u79cd\u65b9\u6cd5\u6765\u52a0\u901f\u6ce8\u610f\u529b\u8ba1\u7b97\u6216\u538b\u7f29 KV \u7f13\u5b58\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u7814\u7a76\u53ea\u5173\u6ce8\u89e3\u51b3\u8fd9\u4e9b\u74f6\u9888\u4e2d\u7684\u4e00\u4e2a\uff0c\u5e76\u4e14\u6ca1\u6709\u5145\u5206\u652f\u6301\u6839\u636e\u4e0d\u540c\u7684\u5c42\u6216\u4efb\u52a1\u52a8\u6001\u8c03\u6574\u7a00\u758f\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 ZipVL\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e3a LVLMs \u8bbe\u8ba1\u7684\u9ad8\u6548\u63a8\u7406\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u91cd\u8981\u6807\u8bb0\u7684\u52a8\u6001\u6bd4\u7387\u5206\u914d\u7b56\u7565\u6765\u89e3\u51b3\u8ba1\u7b97\u548c\u5185\u5b58\u74f6\u9888\u3002\u8be5\u6bd4\u7387\u662f\u6839\u636e\u7279\u5b9a\u5c42\u7684\u6ce8\u610f\u529b\u5206\u6570\u5206\u5e03\u81ea\u9002\u5e94\u786e\u5b9a\u7684\uff0c\u800c\u4e0d\u662f\u56fa\u5b9a\u7684\u8d85\u53c2\u6570\uff0c\u4ece\u800c\u5728\u8f83\u7b80\u5355\u7684\u4efb\u52a1\u4e2d\u63d0\u9ad8\u6548\u7387\uff0c\u540c\u65f6\u5728\u66f4\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e2d\u4fdd\u6301\u9ad8\u6027\u80fd\u3002\u7136\u540e\u6211\u4eec\u6839\u636e\u5f52\u4e00\u5316\u540e\u7684\u6ce8\u610f\u529b\u5206\u6570\u9009\u62e9\u91cd\u8981\u7684\u6807\u8bb0\uff0c\u5e76\u4ec5\u5bf9\u8fd9\u4e9b\u91cd\u8981\u7684\u6807\u8bb0\u6267\u884c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u52a0\u901f\u9884\u586b\u5145\u9636\u6bb5\u3002\u4e3a\u4e86\u7f13\u89e3\u89e3\u7801\u9636\u6bb5\u7684\u5185\u5b58\u74f6\u9888\uff0c\u6211\u4eec\u5bf9 KV \u7f13\u5b58\u91c7\u7528\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\uff0c\u5176\u4e2d\u5bf9\u91cd\u8981\u6807\u8bb0\u7684\u7f13\u5b58\u4f7f\u7528\u9ad8\u6bd4\u7279\u91cf\u5316\uff0c\u800c\u5bf9\u4e0d\u90a3\u4e48\u91cd\u8981\u7684\u6807\u8bb0\u7684\u7f13\u5b58\u4f7f\u7528\u4f4e\u6bd4\u7279\u91cf\u5316\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cZipVL \u53ef\u4ee5\u5c06\u9884\u586b\u5145\u9636\u6bb5\u7684\u901f\u5ea6\u63d0\u9ad8 2.6 \u500d\uff0c\u5e76\u5c06 GPU \u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c11 50.0%\uff0c\u5728 LongVA-7B \u6a21\u578b\u4e0a\u7684 Video-MME \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u51c6\u786e\u7387\u4ec5\u4e0b\u964d\u4e86 0.2%\uff0c\u6709\u6548\u5730\u63d0\u9ad8\u4e86 LVLMs \u7684\u751f\u6210\u6548\u7387\u3002||\n", "2410.11657": "|**2024-10-15**|[Unveiling the Mystery of Visual Attributes of Concrete and Abstract Concepts: Variability, Nearest Neighbors, and Challenging Categories](http://arxiv.org/abs/2410.11657)|**[link](https://github.com/TarunTater/AbstractConceptsInImages)**|\u4e00\u4e2a\u6982\u5ff5\u7684\u89c6\u89c9\u8868\u5f81\u4f1a\u56e0\u5176\u542b\u4e49\u548c\u51fa\u73b0\u8bed\u5883\u7684\u4e0d\u540c\u800c\u53d1\u751f\u663e\u8457\u53d8\u5316\uff0c\u8fd9\u5bf9\u89c6\u89c9\u548c\u591a\u6a21\u6001\u6a21\u578b\u90fd\u63d0\u51fa\u4e86\u591a\u91cd\u6311\u6218\u3002\u6211\u4eec\u7684\u7814\u7a76\u4fa7\u91cd\u4e8e\u5177\u8c61\u6027\uff0c\u8fd9\u662f\u4e00\u4e2a\u7ecf\u8fc7\u5145\u5206\u7814\u7a76\u7684\u8bcd\u6c47\u8bed\u4e49\u53d8\u91cf\uff0c\u5e76\u4ee5\u6b64\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\u6765\u68c0\u9a8c\u89c6\u89c9\u8868\u5f81\u7684\u53ef\u53d8\u6027\u3002\u6211\u4eec\u4f9d\u8d56\u4e8e\u4ece\u4e24\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\uff08Bing \u548c YFCC\uff09\u4e2d\u63d0\u53d6\u7684\u4e0e\u5927\u7ea6 1000 \u4e2a\u62bd\u8c61\u548c\u5177\u4f53\u6982\u5ff5\u76f8\u5173\u7684\u56fe\u50cf\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\uff1a(i) \u8bc4\u4f30\u6982\u5ff5\u63cf\u8ff0\u4e2d\u7684\u89c6\u89c9\u591a\u6837\u6027\u662f\u5426\u53ef\u4ee5\u53ef\u9760\u5730\u533a\u5206\u5177\u4f53\u6982\u5ff5\u548c\u62bd\u8c61\u6982\u5ff5\uff1b(ii) \u901a\u8fc7\u6700\u8fd1\u90bb\u5206\u6790\u6765\u5206\u6790\u540c\u4e00\u6982\u5ff5\u7684\u591a\u5e45\u56fe\u50cf\u7684\u89c6\u89c9\u7279\u5f81\u7684\u53ef\u53d8\u6027\uff1b(iii) \u901a\u8fc7\u5bf9\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\u548c\u6ce8\u91ca\u6765\u8bc6\u522b\u5bfc\u81f4\u8fd9\u79cd\u53ef\u53d8\u6027\u7684\u6311\u6218\u6027\u56e0\u7d20\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u4e8e\u62bd\u8c61\u6982\u5ff5\u548c\u5177\u4f53\u6982\u5ff5\u56fe\u50cf\u7684\u5206\u7c7b\uff0c\u989c\u8272\u548c\u7eb9\u7406\u7b49\u57fa\u672c\u89c6\u89c9\u7279\u5f81\u7684\u7ec4\u5408\u6bd4\u89c6\u89c9Transformer\uff08ViT\uff09\u7b49\u66f4\u590d\u6742\u6a21\u578b\u63d0\u53d6\u7684\u7279\u5f81\u66f4\u6709\u6548\u3002\u7136\u800c\uff0cViT \u5728\u6700\u8fd1\u90bb\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\uff0c\u8fd9\u5f3a\u8c03\u4e86\u5728\u901a\u8fc7\u6587\u672c\u4ee5\u5916\u7684\u6a21\u6001\u5206\u6790\u6982\u5ff5\u53d8\u91cf\u65f6\uff0c\u9700\u8981\u8c28\u614e\u9009\u62e9\u89c6\u89c9\u7279\u5f81\u3002||\n", "2410.11582": "|**2024-10-15**|[On-the-fly Modulation for Balanced Multimodal Learning](http://arxiv.org/abs/2410.11582)|**[link](https://github.com/gewu-lab/bml_tpami2024)**|\u591a\u6a21\u6001\u5b66\u4e60\u65e8\u5728\u901a\u8fc7\u6574\u5408\u6765\u81ea\u4e0d\u540c\u6a21\u6001\u7684\u4fe1\u606f\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u8054\u5408\u8bad\u7ec3\u7b56\u7565\u5bf9\u6240\u6709\u6a21\u6001\u91c7\u7528\u7edf\u4e00\u76ee\u6807\uff0c\u5bfc\u81f4\u5355\u6a21\u6001\u8868\u5f81\u4e0d\u5e73\u8861\u548c\u6b20\u4f18\u5316\uff0c\u56e0\u6b64\u591a\u6a21\u6001\u5b66\u4e60\u7684\u6f5c\u529b\u5e76\u672a\u5f97\u5230\u5145\u5206\u53d1\u6325\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u6307\u51fa\u901a\u5e38\u5b58\u5728\u5177\u6709\u66f4\u591a\u5224\u522b\u4fe1\u606f\u7684\u6a21\u6001\uff0c\u4f8b\u5982\u8e22\u8db3\u7403\u7684\u89c6\u89c9\u548c\u522e\u98ce\u7684\u542c\u89c9\u3002\u5b83\u4eec\u53ef\u80fd\u5728\u8054\u5408\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\uff0c\u5bfc\u81f4\u5176\u4ed6\u6a21\u6001\u4e25\u91cd\u6b20\u4f18\u5316\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u9996\u5148\u4ece\u4f18\u5316\u7684\u524d\u9988\u548c\u53cd\u5411\u4f20\u64ad\u9636\u6bb5\u5206\u6790\u4e86\u6b20\u4f18\u5316\u73b0\u8c61\u3002\u7136\u540e\uff0c\u63d0\u51fa\u4e86\u52a8\u6001\u9884\u6d4b\u8c03\u5236\uff08OPM\uff09\u548c\u52a8\u6001\u68af\u5ea6\u8c03\u5236\uff08OGM\uff09\u7b56\u7565\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u76d1\u63a7\u6a21\u6001\u95f4\u7684\u5224\u522b\u5dee\u5f02\u6765\u8c03\u8282\u6bcf\u4e2a\u6a21\u6001\u7684\u4f18\u5316\u3002\u5177\u4f53\u800c\u8a00\uff0cOPM\u5728\u524d\u9988\u9636\u6bb5\u901a\u8fc7\u52a8\u6001\u6982\u7387\u4e22\u5f03\u4e3b\u5bfc\u6a21\u6001\u7684\u7279\u5f81\u6765\u524a\u5f31\u5176\u5f71\u54cd\uff0c\u800cOGM\u5728\u53cd\u5411\u4f20\u64ad\u9636\u6bb5\u51cf\u8f7b\u5176\u68af\u5ea6\u3002\u5728\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u76f8\u5f53\u5927\u7684\u6539\u8fdb\u3002\u8fd9\u4e9b\u7b80\u5355\u800c\u6709\u6548\u7684\u7b56\u7565\u4e0d\u4ec5\u589e\u5f3a\u4e86\u666e\u901a\u548c\u9762\u5411\u4efb\u52a1\u7684\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\uff0c\u800c\u4e14\u5728\u66f4\u590d\u6742\u7684\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u5b83\u4eec\u7684\u6709\u6548\u6027\u548c\u7075\u6d3b\u6027\u3002\u6e90\u4ee3\u7801\u53ef\u5728\\url{https://github.com/GeWu-Lab/BML_TPAMI2024}\u83b7\u53d6\u3002||\n", "2410.11403": "|**2024-10-15**|[Enhancing Unimodal Latent Representations in Multimodal VAEs through Iterative Amortized Inference](http://arxiv.org/abs/2410.11403)|null|\u591a\u6a21\u6001\u53d8\u5206\u81ea\u7f16\u7801\u5668 (VAE) \u65e8\u5728\u901a\u8fc7\u6574\u5408\u6765\u81ea\u4e0d\u540c\u6570\u636e\u6a21\u6001\u7684\u4fe1\u606f\u6765\u6355\u83b7\u5171\u4eab\u7684\u6f5c\u5728\u8868\u793a\u3002\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u662f\u5728\u4e0d\u9700\u8981\u4e3a\u6240\u6709\u53ef\u80fd\u7684\u6a21\u6001\u7ec4\u5408\u8bad\u7ec3\u4e0d\u5207\u5b9e\u9645\u6570\u91cf (2^M) \u4e2a\u63a8\u7406\u7f51\u7edc\u7684\u60c5\u51b5\u4e0b\uff0c\u51c6\u786e\u5730\u4ece\u4efb\u4f55\u6a21\u6001\u5b50\u96c6\u63a8\u65ad\u8868\u793a\u3002\u57fa\u4e8e\u6df7\u5408\u7684\u6a21\u578b\u901a\u8fc7\u4ec5\u9700\u8981\u4e0e\u6a21\u6001\u6570\u91cf\u4e00\u6837\u591a\u7684\u63a8\u7406\u6a21\u578b\u6765\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u4ece\u800c\u805a\u5408\u5355\u6a21\u6001\u63a8\u7406\u3002\u7136\u800c\uff0c\u5f53\u6a21\u6001\u7f3a\u5931\u65f6\uff0c\u5b83\u4eec\u4f1a\u906d\u53d7\u4fe1\u606f\u4e22\u5931\u7684\u56f0\u6270\u3002\u57fa\u4e8e\u5bf9\u9f50\u7684 VAE \u901a\u8fc7\u6700\u5c0f\u5316 Kullback-Leibler (KL) \u6563\u5ea6\u5c06\u5355\u6a21\u6001\u63a8\u7406\u6a21\u578b\u4e0e\u591a\u6a21\u6001\u6a21\u578b\u5bf9\u9f50\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u7531\u4e8e\u644a\u9500\u5dee\u8ddd\u5bfc\u81f4\u63a8\u7406\u7cbe\u5ea6\u4e0b\u964d\uff0c\u56e0\u6b64\u9762\u4e34\u7740\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u5728\u591a\u6a21\u6001 VAE \u6846\u67b6\u5185\u5f15\u5165\u4e86\u591a\u6a21\u6001\u8fed\u4ee3\u644a\u9500\u63a8\u7406\uff0c\u8fd9\u662f\u4e00\u79cd\u8fed\u4ee3\u7ec6\u5316\u673a\u5236\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u6240\u6709\u53ef\u7528\u6a21\u6001\u8fed\u4ee3\u5730\u7ec6\u5316\u591a\u6a21\u6001\u63a8\u7406\uff0c\u4ece\u800c\u514b\u670d\u4e86\u7f3a\u5931\u6a21\u6001\u9020\u6210\u7684\u4fe1\u606f\u4e22\u5931\uff0c\u5e76\u6700\u5927\u7a0b\u5ea6\u5730\u51cf\u5c11\u4e86\u644a\u9500\u5dee\u8ddd\u3002\u901a\u8fc7\u5c06\u5355\u6a21\u6001\u63a8\u7406\u4e0e\u8fd9\u79cd\u7ec6\u5316\u7684\u591a\u6a21\u6001\u540e\u9a8c\u5bf9\u9f50\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u5355\u6a21\u6001\u63a8\u7406\uff0c\u8be5\u63a8\u7406\u6709\u6548\u5730\u7ed3\u5408\u4e86\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u540c\u65f6\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4ec5\u9700\u8981\u5355\u6a21\u6001\u8f93\u5165\u3002\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u66f4\u9ad8\u7684\u7ebf\u6027\u5206\u7c7b\u7cbe\u5ea6\u548c\u7ade\u4e89\u6027\u4f59\u5f26\u76f8\u4f3c\u6027\u8bc1\u660e\u4e86\u8fd9\u4e00\u70b9\uff0c\u5e76\u589e\u5f3a\u4e86\u8de8\u6a21\u6001\u751f\u6210\uff0cFID \u5f97\u5206\u8f83\u4f4e\u8868\u660e\u4e86\u8fd9\u4e00\u70b9\u3002\u8fd9\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u589e\u5f3a\u4e86\u4ece\u5355\u6a21\u6001\u8f93\u5165\u63a8\u65ad\u7684\u8868\u793a\u3002||\n", "2410.11366": "|**2024-10-15**|[LargePiG: Your Large Language Model is Secretly a Pointer Generator](http://arxiv.org/abs/2410.11366)|null|\u6700\u8fd1\u5173\u4e8e\u67e5\u8be2\u751f\u6210\u7684\u7814\u7a76\u96c6\u4e2d\u5728\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0a\uff0c\u867d\u7136LLM\u5e26\u6765\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u751f\u6210\u67e5\u8be2\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5c06\u76f8\u5173\u6027\u5e7b\u89c9\u548c\u4e8b\u5b9e\u6027\u5e7b\u89c9\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u7c7b\u578b\u5b66\u6765\u63cf\u8ff0\u57fa\u4e8eLLM\u7684\u67e5\u8be2\u751f\u6210\u5e26\u6765\u7684\u5e7b\u89c9\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5206\u79bbLLM\u751f\u6210\u67e5\u8be2\u4e2d\u7684\u5185\u5bb9\u548c\u5f62\u5f0f\uff0c\u8be5\u65b9\u6cd5\u4fdd\u7559\u4e86\u4ece\u8f93\u5165\u4e2d\u63d0\u53d6\u548c\u96c6\u6210\u7684 factual knowledge\uff0c\u5e76\u5229\u7528LLM\u5f3a\u5927\u7684\u8bed\u8a00\u80fd\u529b\u7f16\u8bd1\u4e86\u53e5\u6cd5\u7ed3\u6784\uff0c\u5305\u62ec\u529f\u80fd\u8bcd\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4e0e\u6a21\u578b\u65e0\u5173\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f6c\u6362\u4e3a\u6307\u9488\u751f\u6210\u5668\uff08LargePiG\uff09\uff0c\u5176\u4e2d\u6307\u9488\u6ce8\u610f\u529b\u5206\u5e03\u5229\u7528\u4e86LLM\u56fa\u6709\u7684\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u5e76\u4e14\u590d\u5236\u6982\u7387\u6e90\u81ea\u6a21\u578b\u9ad8\u5c42\u548c\u6700\u540e\u4e00\u5c42\u7684\u8bcd\u6c47\u5206\u5e03\u5dee\u5f02\u3002\u4e3a\u4e86\u9a8c\u8bc1LargePiG\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e24\u4e2a\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u67e5\u8be2\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u6db5\u76d6\u4e86\u6587\u6863\u548c\u89c6\u9891\u573a\u666f\u3002\u5bf9\u5404\u79cdLLM\u7684\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0cLargePiG\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u90fd\u5177\u6709\u4f18\u8d8a\u6027\u3002\u989d\u5916\u7684\u5b9e\u9a8c\u8fd8\u9a8c\u8bc1\u4e86LargePiG\u53ef\u4ee5\u51cf\u5c11\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\uff0c\u5e76\u63d0\u9ad8\u57fa\u4e8e\u6587\u6863\u7684\u95ee\u7b54\u548c\u4e8b\u5b9e\u6027\u8bc4\u4f30\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002||\n", "2410.11255": "|**2024-10-15**|[CLIP-DFGS: A Hard Sample Mining Method for CLIP in Generalizable Person Re-Identification](http://arxiv.org/abs/2410.11255)|null|\u8fd1\u5e74\u6765\uff0c\u50cfCLIP\u8fd9\u6837\u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\uff0c\u5df2\u7ecf\u663e\u793a\u51fa\u5176\u5728\u884c\u4eba\u91cd\u8bc6\u522b\uff08ReID\uff09\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u901a\u7528\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4ecd\u7136\u6b20\u4f73\u3002CLIP\u9884\u8bad\u7ec3\u4e2d\u4f7f\u7528\u7684\u5927\u89c4\u6a21\u591a\u6837\u5316\u7684\u56fe\u50cf-\u6587\u672c\u5bf9\u53ef\u80fd\u5bfc\u81f4\u67d0\u4e9b\u7ec6\u7c92\u5ea6\u7279\u5f81\u7684\u7f3a\u5931\u6216\u4e0d\u8db3\u3002\u9488\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDFGS\uff08\u6df1\u5ea6\u4f18\u5148\u56fe\u91c7\u6837\u5668\uff09\u7684\u56f0\u96be\u6837\u672c\u6316\u6398\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\uff0c\u65e8\u5728\u63d0\u4f9b\u8db3\u591f\u5177\u6709\u6311\u6218\u6027\u7684\u6837\u672c\uff0c\u4ee5\u589e\u5f3aCLIP\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7279\u5f81\u7684\u80fd\u529b\u3002DFGS\u53ef\u4ee5\u5e94\u7528\u4e8eCLIP\u4e2d\u7684\u56fe\u50cf\u7f16\u7801\u5668\u548c\u6587\u672c\u7f16\u7801\u5668\u3002\u901a\u8fc7\u5229\u7528CLIP\u5f3a\u5927\u7684\u8de8\u6a21\u6001\u5b66\u4e60\u80fd\u529b\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u5e94\u7528DFGS\u65b9\u6cd5\u63d0\u53d6\u5177\u6709\u6311\u6218\u6027\u7684\u6837\u672c\uff0c\u5e76\u5f62\u6210\u5177\u6709\u9ad8\u5224\u522b\u96be\u5ea6\u7684mini-batches\uff0c\u4e3a\u56fe\u50cf\u6a21\u578b\u63d0\u4f9b\u66f4\u6709\u6548\u3001\u66f4\u5177\u6311\u6218\u6027\u7684\u96be\u4ee5\u533a\u5206\u7684\u6837\u672c\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u533a\u5206\u4e2a\u4f53\u7684\u80fd\u529b\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u65b9\u6cd5\u76f8\u6bd4\uff0cDFGS\u6709\u663e\u8457\u7684\u6539\u8fdb\uff0c\u8bc1\u5b9e\u4e86DFGS\u5728\u63d0\u4f9b\u5177\u6709\u6311\u6218\u6027\u7684\u6837\u672c\u4ee5\u589e\u5f3aCLIP\u5728\u901a\u7528\u884c\u4eba\u91cd\u8bc6\u522b\u4e2d\u7684\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002||\n", "2410.11087": "|**2024-10-14**|[Locality Alignment Improves Vision-Language Models](http://arxiv.org/abs/2410.11087)|null|\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5f97\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5e94\u7528\uff0c\u4f46\u8bb8\u591a\u6a21\u578b\u4ecd\u7136\u96be\u4ee5\u89e3\u51b3\u57fa\u672c\u7684 spatial reasoning \u9519\u8bef\u3002\u6211\u4eec\u5047\u8bbe\u8fd9\u662f\u7531\u4e8e VLM \u91c7\u7528\u4e86\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\uff0c\u7279\u522b\u662f\u4f7f\u7528\u56fe\u50cf\u7ea7\u76d1\u7763\u548c\u6700\u5c0f\u5f52\u7eb3\u504f\u5dee\u8bad\u7ec3\u7684\u89c6\u89c9\u53d8\u6362\u5668 (ViT)\u3002\u6b64\u7c7b\u6a21\u578b\u53ef\u80fd\u65e0\u6cd5\u7f16\u7801\u56fe\u50cf\u4e2d\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u7c7b\u522b\u5185\u5bb9\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u901a\u8fc7\u786e\u4fdd\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u6709\u6548\u6355\u83b7\u5c40\u90e8\u548c\u5168\u5c40\u56fe\u50cf\u8bed\u4e49\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002\u6211\u4eec\u7684\u4e3b\u8981\u89c1\u89e3\u662f\uff0c\u6211\u4eec\u4e0d\u9700\u8981\u65b0\u7684\u76d1\u7763\u6765\u5b66\u4e60\u8fd9\u79cd\u80fd\u529b\u2014\u2014\u9884\u8bad\u7ec3\u6a21\u578b\u5305\u542b\u5927\u91cf\u7684\u5c40\u90e8\u8bed\u4e49\u77e5\u8bc6\uff0c\u6211\u4eec\u53ef\u4ee5\u63d0\u53d6\u8fd9\u4e9b\u77e5\u8bc6\u5e76\u5c06\u5176\u7528\u4e8e\u53ef\u6269\u5c55\u7684\u81ea\u76d1\u7763\u3002\u6211\u4eec\u4e3a ViT \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u6548\u7684\u8bad\u7ec3\u540e\u9636\u6bb5\uff0c\u79f0\u4e3a\u5c40\u90e8\u6027\u5bf9\u9f50\uff0c\u4ee5\u53ca\u4e00\u79cd\u65b0\u7684\u5fae\u8c03\u7a0b\u5e8f\uff0c\u79f0\u4e3a MaskEmbed\uff0c\u5b83\u4f7f\u7528\u63a9\u853d\u91cd\u5efa\u635f\u5931\u6765\u5b66\u4e60\u6bcf\u4e2a\u56fe\u50cf\u5757\u7684\u8bed\u4e49\u8d21\u732e\u3002\u6211\u4eec\u9996\u5148\u4f7f\u7528\u4ec5\u89c6\u89c9\u57fa\u51c6\u8bc4\u4f30\u5c40\u90e8\u6027\u5bf9\u9f50\uff0c\u53d1\u73b0\u5b83\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u5757\u7ea7\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4f7f\u7528\u56fe\u50cf-\u6807\u9898\u5bf9\uff08\u4f8b\u5982\uff0cCLIP \u548c SigLIP\uff09\u8bad\u7ec3\u7684\u5f3a\u9aa8\u5e72\u7f51\u7edc\u3002\u7136\u540e\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u7cfb\u5217\u4f7f\u7528\u548c\u4e0d\u4f7f\u7528\u5c40\u90e8\u6027\u5bf9\u9f50\u7684 VLM\uff0c\u5e76\u8868\u660e\u5c40\u90e8\u6027\u5bf9\u9f50\u7684\u9aa8\u5e72\u7f51\u7edc\u63d0\u9ad8\u4e86\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u6d89\u53ca\u7a7a\u95f4\u7406\u89e3\u7684\u57fa\u51c6\u6d4b\u8bd5\uff08\u4f8b\u5982\uff0cRefCOCO\u3001OCID-Ref\u3001TallyQA\u3001VSR\u3001AI2D\uff09\u3002\u603b\u7684\u6765\u8bf4\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5c40\u90e8\u6027\u5bf9\u9f50\u9636\u6bb5\u6709\u6548\u5730\u5b66\u4e60\u5c40\u90e8\u8bed\u4e49\u63d0\u53d6\uff0c\u5e76\u4e14\u6b64\u8fc7\u7a0b\u8865\u5145\u4e86\u4f7f\u7528\u73b0\u6210\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u7684\u73b0\u6709 VLM \u8bad\u7ec3\u65b9\u6cd5\u3002||\n", "2410.10799": "|**2024-10-14**|[Towards Foundation Models for 3D Vision: How Close Are We?](http://arxiv.org/abs/2410.10799)|null|\u6784\u5efa\u7528\u4e8e 3D \u89c6\u89c9\u7684\u57fa\u7840\u6a21\u578b\u662f\u4e00\u4e2a\u5c1a\u672a\u89e3\u51b3\u7684\u590d\u6742\u6311\u6218\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u91cd\u8981\u7684\u662f\u4e86\u89e3\u5f53\u524d\u6a21\u578b\u7684 3D \u63a8\u7406\u80fd\u529b\uff0c\u5e76\u786e\u5b9a\u8fd9\u4e9b\u6a21\u578b\u4e0e\u4eba\u7c7b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684 3D \u89c6\u89c9\u7406\u89e3\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u6db5\u76d6\u4e86\u89c6\u89c9\u95ee\u7b54 (VQA) \u683c\u5f0f\u7684\u57fa\u672c 3D \u89c6\u89c9\u4efb\u52a1\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM)\u3001\u4e13\u95e8\u6a21\u578b\u548c\u4eba\u7c7b\u53d7\u8bd5\u8005\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cVLM \u7684\u6027\u80fd\u666e\u904d\u8f83\u5dee\uff0c\u800c\u4e13\u95e8\u6a21\u578b\u867d\u7136\u51c6\u786e\u4f46\u4e0d\u7a33\u5065\uff0c\u5728\u51e0\u4f55\u6270\u52a8\u4e0b\u4f1a\u5931\u8d25\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4eba\u7c7b\u89c6\u89c9\u4ecd\u7136\u662f\u6700\u53ef\u9760\u7684 3D \u89c6\u89c9\u7cfb\u7edf\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u8bc1\u660e\uff0c\u4e0e\u7ecf\u5178\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u76f8\u6bd4\uff0c\u795e\u7ecf\u7f51\u7edc\u4e0e\u4eba\u7c7b 3D \u89c6\u89c9\u673a\u5236\u7684\u4e00\u81f4\u6027\u66f4\u9ad8\uff0c\u5e76\u4e14\u57fa\u4e8e Transformer \u7684\u7f51\u7edc\uff08\u5982 ViT\uff09\u6bd4 CNN \u4e0e\u4eba\u7c7b 3D \u89c6\u89c9\u673a\u5236\u7684\u4e00\u81f4\u6027\u66f4\u9ad8\u3002\u6211\u4eec\u5e0c\u671b\u6211\u4eec\u7684\u7814\u7a76\u80fd\u591f\u6709\u5229\u4e8e\u672a\u6765 3D \u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u5f00\u53d1\u3002||\n", "2410.10594": "|**2024-10-14**|[VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents](http://arxiv.org/abs/2410.10594)|**[link](https://github.com/openbmb/visrag)**|\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u662f\u4e00\u79cd\u6709\u6548\u7684\u6280\u672f\uff0c\u5b83\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u591f\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u6e90\u8fdb\u884c\u751f\u6210\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684RAG\u7cfb\u7edf\u5b8c\u5168\u57fa\u4e8e\u6587\u672c\uff0c\u65e0\u6cd5\u5229\u7528\u5728\u73b0\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u6587\u6863\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u4f5c\u7528\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u5982\u5e03\u5c40\u548c\u56fe\u50cf\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86VisRAG\uff0c\u5b83\u901a\u8fc7\u5efa\u7acb\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684RAG\u6d41\u7a0b\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u5728\u8fd9\u4e2a\u6d41\u7a0b\u4e2d\uff0c\u4e0d\u662f\u5148\u89e3\u6790\u6587\u6863\u4ee5\u83b7\u53d6\u6587\u672c\uff0c\u800c\u662f\u4f7f\u7528VLM\u5c06\u6587\u6863\u4f5c\u4e3a\u56fe\u50cf\u76f4\u63a5\u5d4c\u5165\uff0c\u7136\u540e\u68c0\u7d22\u4ee5\u589e\u5f3aVLM\u7684\u751f\u6210\u3002\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8e\u6587\u672c\u7684RAG\u76f8\u6bd4\uff0cVisRAG\u6700\u5927\u9650\u5ea6\u5730\u4fdd\u7559\u548c\u5229\u7528\u4e86\u539f\u59cb\u6587\u6863\u4e2d\u7684\u6570\u636e\u4fe1\u606f\uff0c\u6d88\u9664\u4e86\u89e3\u6790\u8fc7\u7a0b\u4e2d\u5f15\u5165\u7684\u4fe1\u606f\u635f\u5931\u3002\u6211\u4eec\u6536\u96c6\u4e86\u5f00\u6e90\u6570\u636e\u548c\u5408\u6210\u6570\u636e\u6765\u8bad\u7ec3VisRAG\u4e2d\u7684\u68c0\u7d22\u5668\uff0c\u5e76\u63a2\u7d22\u4e86\u5404\u79cd\u751f\u6210\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\uff0cVisRAG\u5728\u68c0\u7d22\u548c\u751f\u6210\u9636\u6bb5\u90fd\u4f18\u4e8e\u4f20\u7edf\u7684RAG\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u6587\u672c\u7684RAG\u6d41\u7a0b\uff0c\u5b9e\u73b0\u4e8625%-39%\u7684\u7aef\u5230\u7aef\u6027\u80fd\u63d0\u5347\u3002\u8fdb\u4e00\u6b65\u7684\u5206\u6790\u8868\u660e\uff0cVisRAG\u53ef\u4ee5\u6709\u6548\u5730\u5229\u7528\u8bad\u7ec3\u6570\u636e\u5e76\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8fd9\u4f7f\u5176\u6210\u4e3a\u591a\u6a21\u6001\u6587\u6863\u4e0aRAG\u7684\u4e00\u4e2a\u5f88\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6570\u636e\u53ef\u5728https://github.com/openbmb/visrag \u83b7\u53d6\u3002||\n", "2410.10308": "|**2024-10-14**|[LG-CAV: Train Any Concept Activation Vector with Language Guidance](http://arxiv.org/abs/2410.10308)|null|\u6982\u5ff5\u6fc0\u6d3b\u5411\u91cf\uff08CAV\uff09\u901a\u8fc7\u5c06\u6a21\u578b\u9884\u6d4b\u4f18\u96c5\u5730\u5f52\u56e0\u4e8e\u7279\u5b9a\u6982\u5ff5\uff0c\u5728\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u9886\u57df\u5f15\u8d77\u4e86\u5e7f\u6cdb\u7684\u7814\u7a76\u5174\u8da3\u3002\u7136\u800c\uff0cCAV \u7684\u8bad\u7ec3\u901a\u5e38\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\uff0c\u8fd9\u4e9b\u56fe\u50cf\u7684\u6574\u7406\u6210\u672c\u5f88\u9ad8\uff0c\u56e0\u6b64\u4ec5\u9650\u4e8e\u4e00\u7ec4\u9884\u5b9a\u4e49\u7684\u6982\u5ff5\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8bed\u8a00\u5f15\u5bfc\u7684 CAV\uff08LG-CAV\uff09\uff0c\u4ee5\u5229\u7528\u67d0\u4e9b\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u4f8b\u5982 CLIP\uff09\u4e2d\u4e30\u5bcc\u7684\u6982\u5ff5\u77e5\u8bc6\u3002\u8be5\u65b9\u6cd5\u5141\u8bb8\u5728\u6ca1\u6709\u6807\u8bb0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8bad\u7ec3\u4efb\u4f55 CAV\uff0c\u65b9\u6cd5\u662f\u5229\u7528\u76f8\u5e94\u7684\u6982\u5ff5\u63cf\u8ff0\u4f5c\u4e3a\u6307\u5bfc\u3002\u4e3a\u4e86\u5f25\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u76ee\u6807\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u6211\u4eec\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u4e86\u4e00\u7ec4\u901a\u7528\u56fe\u50cf\uff08\u63a2\u6d4b\u56fe\u50cf\uff09\u4e0a\u6982\u5ff5\u63cf\u8ff0\u7684\u6fc0\u6d3b\u503c\uff0c\u5e76\u5229\u7528\u5b83\u4eec\u4f5c\u4e3a\u8bed\u8a00\u6307\u5bfc\u6765\u8bad\u7ec3 LG-CAV\u3002\u6b64\u5916\uff0c\u5728\u8bad\u7ec3\u4e86\u4e0e\u76ee\u6807\u6a21\u578b\u4e2d\u6240\u6709\u9884\u6d4b\u7c7b\u522b\u76f8\u5173\u7684\u9ad8\u8d28\u91cf LG-CAV \u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6fc0\u6d3b\u6837\u672c\u91cd\u65b0\u52a0\u6743\uff08ASR\uff09\u4f5c\u4e3a\u4e00\u79cd\u6a21\u578b\u6821\u6b63\u6280\u672f\uff0c\u4ee5\u53cd\u8fc7\u6765\u63d0\u9ad8\u76ee\u6807\u6a21\u578b\u7684\u6027\u80fd\u3002\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u8de8\u8d8a\u4e5d\u79cd\u67b6\u6784\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLG-CAV \u5728\u7ed9\u5b9a\u4efb\u4f55\u6982\u5ff5\u7684\u60c5\u51b5\u4e0b\uff0c\u76f8\u8f83\u4e8e\u4ee5\u524d\u7684 CAV \u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8d28\u91cf\u63d0\u5347\uff0c\u5e76\u4e14\u6211\u4eec\u7684\u6a21\u578b\u6821\u6b63\u65b9\u6cd5\u4e0e\u73b0\u6709\u7684\u57fa\u4e8e\u6982\u5ff5\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/hqhQAQ/LG-CAV \u83b7\u53d6\u3002||\n", "2410.10257": "|**2024-10-14**|[Saliency Guided Optimization of Diffusion Latents](http://arxiv.org/abs/2410.10257)|null|\u968f\u7740\u6269\u6563\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4ece\u6587\u672c\u63d0\u793a\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u5df2\u4e0d\u518d\u662f\u6311\u6218\u3002\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u91cd\u70b9\u662f\u5982\u4f55\u4f18\u5316\u751f\u6210\u7ed3\u679c\uff0c\u4f7f\u5176\u66f4\u597d\u5730\u4e0e\u4eba\u7c7b\u610f\u56fe\u6216\u63d0\u793a\u4fdd\u6301\u4e00\u81f4\u3002\u73b0\u6709\u7684\u4f18\u5316\u65b9\u6cd5\u901a\u5e38\u5c06\u6574\u4e2a\u56fe\u50cf\u89c6\u4e3a\u4e00\u4e2a\u6574\u4f53\uff0c\u8fdb\u884c\u5168\u5c40\u4f18\u5316\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e00\u4e2a\u4e8b\u5b9e\uff1a\u5f53\u4eba\u7c7b\u89c2\u5bdf\u56fe\u50cf\u65f6\uff0c\u89c6\u89c9\u7cfb\u7edf\u4f1a\u81ea\u7136\u5730\u5c06\u6ce8\u610f\u529b\u96c6\u4e2d\u5728\u663e\u8457\u533a\u57df\uff0c\u800c\u5ffd\u7565\u4e0d\u592a\u91cd\u8981\u6216\u4e0d\u663e\u8457\u7684\u533a\u57df\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u4eba\u7c7b\u5f88\u53ef\u80fd\u5ffd\u7565\u5bf9\u975e\u663e\u8457\u533a\u57df\u7684\u4f18\u5316\u3002\u56e0\u6b64\uff0c\u5c3d\u7ba1\u5728\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u6307\u5bfc\u4e0b\u8fdb\u884c\u4e86\u6a21\u578b\u5fae\u8c03\uff0c\u4f46\u73b0\u6709\u8fdb\u884c\u5168\u5c40\u4f18\u5316\u7684\u65b9\u6cd5\u5f97\u5230\u7684\u7ed3\u679c\u5e76\u4e0d\u7406\u60f3\u3002\u4e3a\u4e86\u6709\u6548\u4e14\u9ad8\u6548\u5730\u89e3\u51b3\u8fd9\u79cd\u5bf9\u9f50\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u663e\u8457\u6027\u5f15\u5bfc\u7684\u6269\u6563\u6f5c\u5728\u7a7a\u95f4\u4f18\u5316\u65b9\u6cd5\uff08SGOOL\uff09\u3002\u6211\u4eec\u9996\u5148\u4f7f\u7528\u663e\u8457\u6027\u68c0\u6d4b\u5668\u6765\u6a21\u62df\u4eba\u7c7b\u89c6\u89c9\u6ce8\u610f\u529b\u7cfb\u7edf\uff0c\u5e76\u6807\u8bb0\u51fa\u663e\u8457\u533a\u57df\u3002\u4e3a\u4e86\u907f\u514d\u91cd\u65b0\u8bad\u7ec3\u989d\u5916\u7684\u6a21\u578b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u76f4\u63a5\u4f18\u5316\u6269\u6563\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u3002\u6b64\u5916\uff0cSGOOL \u5229\u7528\u4e86\u53ef\u9006\u6269\u6563\u8fc7\u7a0b\uff0c\u5e76\u5177\u6709\u6052\u5b9a\u5185\u5b58\u5b9e\u73b0\u7684\u4f18\u70b9\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6210\u4e3a\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u4e14\u5373\u63d2\u5373\u7528\u7684\u5fae\u8c03\u65b9\u6cd5\u3002\u6211\u4eec\u4f7f\u7528\u591a\u79cd\u6307\u6807\u548c\u4eba\u5de5\u8bc4\u4f30\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSGOOL \u5728\u56fe\u50cf\u8d28\u91cf\u548c\u63d0\u793a\u5bf9\u9f50\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\u3002||\n", "2410.13848": "|**2024-10-17**|[Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](http://arxiv.org/abs/2410.13848)|**[link](https://github.com/deepseek-ai/janus)**|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 Janus\uff0c\u4e00\u4e2a\u7edf\u4e00\u4e86\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u7684\u81ea\u56de\u5f52\u6846\u67b6\u3002\u4e4b\u524d\u7684\u7814\u7a76\u901a\u5e38\u4f9d\u8d56\u4e8e\u5355\u4e00\u89c6\u89c9\u7f16\u7801\u5668\u6765\u5b8c\u6210\u8fd9\u4e24\u9879\u4efb\u52a1\uff0c\u4f8b\u5982 Chameleon\u3002\u7136\u800c\uff0c\u7531\u4e8e\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u6240\u9700\u7684\u4fe1\u606f\u7c92\u5ea6\u4e0d\u540c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u6027\u80fd\u6b20\u4f73\uff0c\u5c24\u5176\u662f\u5728\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5c06\u89c6\u89c9\u7f16\u7801\u5206\u79bb\u6210\u72ec\u7acb\u7684\u8def\u5f84\uff0c\u540c\u65f6\u4ecd\u7136\u5229\u7528\u5355\u4e2a\u7edf\u4e00\u7684 Transformer \u67b6\u6784\u8fdb\u884c\u5904\u7406\u3002\u8fd9\u79cd\u5206\u79bb\u4e0d\u4ec5\u7f13\u89e3\u4e86\u89c6\u89c9\u7f16\u7801\u5668\u5728\u7406\u89e3\u548c\u751f\u6210\u4e2d\u89d2\u8272\u7684\u51b2\u7a81\uff0c\u8fd8\u589e\u5f3a\u4e86\u6846\u67b6\u7684\u7075\u6d3b\u6027\u3002\u4f8b\u5982\uff0c\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u7ec4\u4ef6\u90fd\u53ef\u4ee5\u72ec\u7acb\u9009\u62e9\u6700\u5408\u9002\u7684\u7f16\u7801\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\uff0cJanus \u4f18\u4e8e\u4e4b\u524d\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u5e76\u4e14\u8fbe\u5230\u6216\u8d85\u8fc7\u4e86\u7279\u5b9a\u4efb\u52a1\u6a21\u578b\u7684\u6027\u80fd\u3002Janus \u7684\u7b80\u6d01\u6027\u3001\u9ad8\u7075\u6d3b\u6027\u548c\u6709\u6548\u6027\u4f7f\u5176\u6210\u4e3a\u4e0b\u4e00\u4ee3\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u7684\u6709\u529b\u5019\u9009\u8005\u3002||\n", "2410.13666": "|**2024-10-17**|[VL-GLUE: A Suite of Fundamental yet Challenging Visuo-Linguistic Reasoning Tasks](http://arxiv.org/abs/2410.13666)|**[link](https://github.com/shailaja183/vl-glue)**|\u4ece\u5f02\u6784\u8f93\u5165\uff08\u4f8b\u5982\u56fe\u50cf\u3001\u6587\u672c\u548c\u97f3\u9891\uff09\u4e2d\u63a8\u5bfc\u51fa\u63a8\u7406\u662f\u4eba\u7c7b\u6267\u884c\u65e5\u5e38\u4efb\u52a1\u7684\u4e00\u9879\u91cd\u8981\u6280\u80fd\u3002\u5bf9\u4e8e\u5148\u8fdb\u4eba\u5de5\u667a\u80fd (AI) \u7cfb\u7edf\u7684\u5f00\u53d1\u6765\u8bf4\uff0c\u7c7b\u4f3c\u7684\u80fd\u529b\u4e5f\u662f\u53ef\u53d6\u7684\u3002\u867d\u7136\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u5404\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e0a\u6b63\u5728\u8fc5\u901f\u7f29\u5c0f\u4e0e\u4eba\u7c7b\u6c34\u5e73\u6027\u80fd\u7684\u5dee\u8ddd\uff0c\u4f46\u5b83\u4eec\u96be\u4ee5\u89e3\u51b3\u9700\u8981\u5bf9\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u8fdb\u884c\u8054\u5408\u63a8\u7406\u7684\u4efb\u52a1\u3002\u53d7 GLUE\uff08Wang \u7b49\u4eba\uff0c2018 \u5e74\uff09\u7684\u542f\u53d1\uff0cGLUE \u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u7684\u591a\u4efb\u52a1\u57fa\u51c6\uff0c\u6211\u4eec\u5728\u672c\u6587\u4e2d\u63d0\u51fa\u4e86 VL-GLUE\u3002VL-GLUE \u7531\u8de8\u8d8a\u4e03\u4e2a\u4e0d\u540c\u4efb\u52a1\u7684\u8d85\u8fc7 100,000 \u4e2a\u6837\u672c\u7ec4\u6210\uff0c\u8fd9\u4e9b\u4efb\u52a1\u7684\u6838\u5fc3\u90fd\u9700\u8981\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u5305\u542b\u4e0d\u540c\u7684\u56fe\u50cf\u7c7b\u578b\uff08\u4ece\u5408\u6210\u6e32\u67d3\u7684\u56fe\u5f62\u3001\u65e5\u5e38\u573a\u666f\u5230\u56fe\u8868\u548c\u590d\u6742\u56fe\u8868\uff09\u548c\u5404\u79cd\u7279\u5b9a\u9886\u57df\u7684\u6587\u672c\uff08\u4ece\u70f9\u996a\u3001\u653f\u6cbb\u548c\u4f53\u80b2\u5230\u9ad8\u4e2d\u8bfe\u7a0b\uff09\uff0c\u8bc1\u660e\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u5bf9\u591a\u6a21\u6001\u7406\u89e3\u7684\u9700\u6c42\u3002\u6211\u4eec\u8868\u660e\uff0c\u8fd9\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u4e8e\u73b0\u6709\u7684  \u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6765\u8bf4\u6781\u5177\u6311\u6218\u6027\uff0c\u5e76\u9f13\u52b1\u5f00\u53d1\u5177\u6709\u5f3a\u5927\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u7684\u7cfb\u7edf\u3002||\n", "2410.13611": "|**2024-10-17**|[H2OVL-Mississippi Vision Language Models Technical Report](http://arxiv.org/abs/2410.13611)|null|\u7531\u4e8e\u80fd\u591f\u5728\u6d88\u8d39\u8005\u786c\u4ef6\u4e0a\u9ad8\u6548\u8fd0\u884c\u4ee5\u5904\u7406\u4f01\u4e1a\u5546\u4e1a\u6587\u6863\u548c\u56fe\u50cf\uff0c\u4f53\u79ef\u66f4\u5c0f\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5bf9\u4e8e\u6ce8\u91cd\u9690\u79c1\u7684\u8bbe\u5907\u4e0a\u5e94\u7528\u7a0b\u5e8f\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u8fd9\u4e9b\u6a21\u578b\u9700\u8981\u5f3a\u5927\u7684\u8bed\u8a00\u7406\u89e3\u548c\u89c6\u89c9\u80fd\u529b\u6765\u589e\u5f3a\u4eba\u673a\u4ea4\u4e92\u3002\u4e3a\u4e86\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\uff0c\u6211\u4eec\u63a8\u51fa\u4e86 H2OVL-Mississippi\uff0c\u8fd9\u662f\u4e00\u5bf9\u5c0f\u578b VLM\uff0c\u4f7f\u7528 8 \u4e2a H100 GPU\uff0c\u5728 240 \u5c0f\u65f6\u7684\u8ba1\u7b97\u65f6\u95f4\u5185\uff0c\u5229\u7528 3700 \u4e07\u4e2a\u56fe\u6587\u5bf9\u8fdb\u884c\u8bad\u7ec3\u3002H2OVL-Mississippi-0.8B \u662f\u4e00\u6b3e\u53c2\u6570\u91cf\u4e3a 8 \u4ebf\u7684\u5fae\u578b\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u6587\u672c\u8bc6\u522b\uff0c\u5728 OCRBench \u7684\u6587\u672c\u8bc6\u522b\u90e8\u5206\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u8be5\u9886\u57df\u8d85\u8d8a\u4e86\u8bb8\u591a\u66f4\u5927\u7684\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u53d1\u5e03\u4e86 H2OVL-Mississippi-2B\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b 20 \u4ebf\u4e2a\u53c2\u6570\u7684\u901a\u7528\u6a21\u578b\uff0c\u5728\u5404\u79cd\u5b66\u672f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u51fa\u6781\u5177\u7ade\u4e89\u529b\u7684\u6307\u6807\u3002\u8fd9\u4e24\u4e2a\u6a21\u578b\u90fd\u5efa\u7acb\u5728\u6211\u4eec\u4e4b\u524d\u4f7f\u7528 H2O-Danube \u8bed\u8a00\u6a21\u578b\u7684\u5de5\u4f5c\u57fa\u7840\u4e4b\u4e0a\uff0c\u5c06\u5176\u529f\u80fd\u6269\u5c55\u5230\u89c6\u89c9\u9886\u57df\u3002\u6211\u4eec\u5c06\u5b83\u4eec\u5728 Apache 2.0 \u8bb8\u53ef\u4e0b\u53d1\u5e03\uff0c\u4f7f\u6240\u6709\u4eba\u90fd\u53ef\u4ee5\u4f7f\u7528 VLM\uff0c\u4ece\u800c\u4f7f\u6587\u6863 AI \u548c\u89c6\u89c9 LLM \u6c11\u4e3b\u5316\u3002||\n", "2410.13510": "|**2024-10-17**|[GeoCoder: Solving Geometry Problems by Generating Modular Code through Vision-Language Models](http://arxiv.org/abs/2410.13510)|null|\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u9700\u8981\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u6765\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\u5e76\u6709\u6548\u5730\u5229\u7528\u6570\u5b66\u77e5\u8bc6\u3002\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u5404\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4ecd\u7136\u96be\u4ee5\u89e3\u51b3\u51e0\u4f55\u95ee\u9898\uff0c\u5e76\u4e14\u7531\u4e8e\u65e0\u6cd5\u6267\u884c\u9884\u8bad\u7ec3\u671f\u95f4\u672a\u89c1\u8fc7\u7684\u6570\u5b66\u8fd0\u7b97\uff08\u4f8b\u5982\u8ba1\u7b97\u4efb\u610f\u89d2\u5ea6\u7684\u4f59\u5f26\uff09\u4ee5\u53ca\u96be\u4ee5\u6b63\u786e\u5e94\u7528\u76f8\u5173\u51e0\u4f55\u516c\u5f0f\u800c\u53d7\u5230\u5f88\u5927\u9650\u5236\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 GeoCoder\uff0c\u5b83\u5229\u7528\u6a21\u5757\u5316\u4ee3\u7801\u5fae\u8c03\u6765\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u51e0\u4f55\u51fd\u6570\u5e93\u751f\u6210\u548c\u6267\u884c\u4ee3\u7801\u3002\u901a\u8fc7\u6267\u884c\u4ee3\u7801\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u51c6\u786e\u548c\u786e\u5b9a\u7684\u8ba1\u7b97\uff0c\u4e0e\u81ea\u56de\u5f52\u6807\u8bb0\u9884\u6d4b\u7684\u968f\u673a\u6027\u5f62\u6210\u5bf9\u6bd4\uff0c\u800c\u51fd\u6570\u5e93\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e86\u516c\u5f0f\u4f7f\u7528\u4e2d\u7684\u9519\u8bef\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86 GeoCoder \u7684\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u53d8\u4f53\uff0c\u540d\u4e3a RAG-GeoCoder\uff0c\u5b83\u7ed3\u5408\u4e86\u4e00\u4e2a\u975e\u53c2\u6570\u5185\u5b58\u6a21\u5757\u6765\u4ece\u51e0\u4f55\u5e93\u4e2d\u68c0\u7d22\u51fd\u6570\uff0c\u4ece\u800c\u51cf\u5c11\u5bf9\u53c2\u6570\u5185\u5b58\u7684\u4f9d\u8d56\u3002\u6211\u4eec\u7684\u6a21\u5757\u5316\u4ee3\u7801\u5fae\u8c03\u65b9\u6cd5\u589e\u5f3a\u4e86 VLM \u7684\u51e0\u4f55\u63a8\u7406\u80fd\u529b\uff0c\u4e0e\u5176\u4ed6\u5fae\u8c03\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728 GeomVerse \u6570\u636e\u96c6\u4e0a\u7684\u5404\u79cd\u95ee\u9898\u590d\u6742\u6027\u65b9\u9762\u5e73\u5747\u63d0\u9ad8\u4e86 16% \u4ee5\u4e0a\u3002||\n", "2410.13445": "|**2024-10-17**|[Parameter-efficient Adaptation of Multilingual Multimodal Models for Low-resource ASR](http://arxiv.org/abs/2410.13445)|null|\u7531\u4e8e\u7f3a\u4e4f\u6807\u6ce8\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b (ASR) \u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u548c\u7eaf\u6587\u672c\u81ea\u9002\u5e94\u662f\u4e24\u79cd\u5e38\u7528\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8fd9\u79cd\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u95ee\u9898\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5982\u4f55\u4f7f\u7528\u50cf SeamlessM4T \u8fd9\u6837\u7684\u591a\u8bed\u8a00\u591a\u6a21\u6001\u6a21\u578b\u6709\u6548\u5730\u7ed3\u5408\u8fd9\u4e9b\u6280\u672f\u3002\u591a\u6a21\u6001\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u7eaf\u6587\u672c\u81ea\u9002\u5e94\u5229\u7528\u672a\u6807\u6ce8\u7684\u6587\u672c\uff0c\u5e76\u8fdb\u4e00\u6b65\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u7684 ASR \u5fae\u8c03\uff0c\u4ece\u800c\u63d0\u9ad8 ASR \u6027\u80fd\u3002\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u4ece\u9ad8\u8d44\u6e90\u8bed\u8a00\u8fdb\u884c\u8de8\u8bed\u8a00\u8fc1\u79fb\uff0c\u5728\u6ca1\u6709\u4efb\u4f55\u6807\u6ce8\u8bed\u97f3\u7684\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\uff0c\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u5b9e\u73b0\u4e86\u9ad8\u8fbe 17% \u7684\u8bcd\u9519\u8bef\u7387 (WER) \u964d\u4f4e\u3002||\n", "2410.13321": "|**2024-10-17**|[Mitigating Hallucinations in Large Vision-Language Models via Summary-Guided Decoding](http://arxiv.org/abs/2410.13321)|null|\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u5728\u6839\u636e\u89c6\u89c9\u8f93\u5165\u751f\u6210\u8be6\u7ec6\u4e14\u8fde\u8d2f\u7684\u54cd\u5e94\u65b9\u9762\u8868\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\uff0c\u5b83\u4eec\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u7814\u7a76\u4e86 LVLM \u4e2d\u7684\u8bed\u8a00\u5148\u9a8c\uff0c\u5e76\u5f97\u51fa\u4e24\u4e2a\u5173\u952e\u89c2\u5bdf\u7ed3\u679c\uff1a(1) \u5373\u4f7f\u5728\u9884\u6d4b\u4e0e\u56fe\u50cf\u76f8\u5173\u7684\u8bcd\u6027 (POS) \u76f8\u5173\u7684\u6807\u8bb0\u65f6\uff0c\u968f\u7740\u6807\u8bb0\u5e8f\u5217\u7684\u589e\u957f\uff0c\u6a21\u578b\u8d8a\u6765\u8d8a\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\uff0c\u4ece\u800c\u653e\u5927\u4e86\u5e7b\u89c9\u3002(2) \u76f4\u63a5\u6821\u51c6 LVLM \u7684\u8f93\u51fa\u5206\u5e03\u4ee5\u51cf\u8f7b\u8bed\u8a00\u5148\u9a8c\u7684\u65b9\u6cd5\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6587\u672c\u8d28\u91cf\u4e0b\u964d\uff0c\u751a\u81f3\u52a0\u5267\u5e7b\u89c9\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5373\u6458\u8981\u5f15\u5bfc\u89e3\u7801 (SGD)\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6458\u8981\u51cf\u5c11\u6587\u672c\u4e0a\u4e0b\u6587\uff0c\u81ea\u7136\u5730\u9f13\u52b1\u6a21\u578b\u66f4\u591a\u5730\u5173\u6ce8\u56fe\u50cf\u4fe1\u606f\uff0c\u540c\u65f6\u4ec5\u63a7\u5236\u4e0e\u56fe\u50cf\u76f8\u5173\u7684\u8bcd\u6027\u6807\u8bb0\u4ee5\u4fdd\u6301\u6587\u672c\u8d28\u91cf\u3002\u901a\u8fc7\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 SGD \u5728\u7269\u4f53\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5728\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u7684\u6743\u8861\u65b9\u9762\uff0cSGD \u5728\u73b0\u6709\u65b9\u6cd5\u4e2d\u5b9e\u73b0\u4e86\u5e15\u7d2f\u6258\u6700\u4f18\u3002\u6700\u540e\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u51cf\u5c11\u7269\u4f53\u5e7b\u89c9\u548c\u4fdd\u6301\u6587\u672c\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4f46 SGD \u5728\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u65b9\u9762\u8868\u73b0\u51fa\u7a33\u5065\u6027\u3002||\n", "2410.13146": "|**2024-10-17**|[Mapping Bias in Vision Language Models: Signposts, Pitfalls, and the Road Ahead](http://arxiv.org/abs/2410.13146)|**[link](https://github.com/kuleens/vlmbiaseval)**|\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u516c\u5e73\u6027\u4ecd\u7136\u7f3a\u4e4f\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5206\u6790\u4e86\u4e94\u4e2a\u6a21\u578b\u548c\u516d\u4e2a\u6570\u636e\u96c6\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u5dee\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u50cf UTKFace \u548c CelebA \u8fd9\u6837\u7684\u8096\u50cf\u6570\u636e\u96c6\u662f\u68c0\u6d4b\u504f\u5dee\u7684\u6700\u4f73\u5de5\u5177\uff0c\u53ef\u4ee5\u53d1\u73b0 LLaVa \u548c CLIP \u6a21\u578b\u4e4b\u95f4\u5728\u6027\u80fd\u548c\u516c\u5e73\u6027\u65b9\u9762\u7684\u5dee\u8ddd\u3002\u7136\u800c\uff0c\u50cf PATA\u3001VLStereoSet \u8fd9\u6837\u7684\u573a\u666f\u6570\u636e\u96c6\u7531\u4e8e\u5176\u6784\u5efa\u65b9\u5f0f\uff0c\u65e0\u6cd5\u6210\u4e3a\u6709\u6548\u7684\u504f\u5dee\u57fa\u51c6\u3002\u81f3\u4e8e\u50cf VisoGender \u8fd9\u6837\u7684\u57fa\u4e8e\u4ee3\u8bcd\u7684\u6570\u636e\u96c6\uff0c\u6211\u4eec\u6536\u5230\u4e86\u6df7\u5408\u4fe1\u53f7\uff0c\u56e0\u4e3a\u53ea\u6709\u4e00\u90e8\u5206\u6570\u636e\u5b50\u96c6\u5bf9\u63d0\u4f9b\u89c1\u89e3\u6709\u7528\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u66f4\u96be\u7248\u672c\u7684 VisoGender\uff0c\u4f5c\u4e3a\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u6807\u51c6\u3002\u57fa\u4e8e\u8fd9\u4e9b\u7ed3\u679c\uff0c\u6211\u4eec\u547c\u5401\u5efa\u7acb\u66f4\u6709\u6548\u3001\u8bbe\u8ba1\u66f4\u4ed4\u7ec6\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u786e\u4fdd VLM \u7684\u516c\u5e73\u6027\u548c\u53ef\u9760\u6027\u3002||\n", "2410.13030": "|**2024-10-16**|[Sensitivity of Generative VLMs to Semantically and Lexically Altered Prompts](http://arxiv.org/abs/2410.13030)|null|\u5c3d\u7ba1\u7528\u4e8e\u751f\u6210\u5f0f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u63d0\u793a\u8c03\u6574\u6280\u672f\u5927\u91cf\u6d8c\u73b0\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5bf9\u63d0\u793a\u4e2d\u7684\u8bcd\u6c47\u548c\u8bed\u4e49\u53d8\u5316\u7684\u654f\u611f\u7a0b\u5ea6\u4ecd\u4e0d\u6e05\u695a\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528 SugarCrepe++ \u6570\u636e\u96c6\u8bc4\u4f30\u4e86\u751f\u6210\u5f0f VLM \u7406\u89e3\u6587\u672c\u4e2d\u8bcd\u6c47\u548c\u8bed\u4e49\u53d8\u5316\u7684\u80fd\u529b\u3002\u6211\u4eec\u5206\u6790\u4e86 VLM \u5bf9\u63d0\u793a\u4e2d\u8bcd\u6c47\u53d8\u5316\u7684\u654f\u611f\u6027\uff0c\u800c\u8fd9\u4e9b\u53d8\u5316\u4e0d\u5bf9\u5e94\u4e8e\u8bed\u4e49\u53d8\u5316\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u751f\u6210\u5f0f VLM \u5bf9\u6b64\u7c7b\u66f4\u6539\u9ad8\u5ea6\u654f\u611f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u53d1\u73b0\uff0c\u8fd9\u79cd\u8106\u5f31\u6027\u4f1a\u5f71\u54cd\u65e8\u5728\u5b9e\u73b0\u5176\u8f93\u51fa\u4e00\u81f4\u6027\u7684\u6280\u672f\u6027\u80fd\u3002||\n", "2410.13002": "|**2024-10-16**|[Flex: End-to-End Text-Instructed Visual Navigation with Foundation Models](http://arxiv.org/abs/2410.13002)|null|\u7aef\u5230\u7aef\u5b66\u4e60\u5c06\u611f\u5b98\u8f93\u5165\u76f4\u63a5\u6620\u5c04\u5230\u52a8\u4f5c\uff0c\u4e3a\u590d\u6742\u7684\u673a\u5668\u4eba\u4efb\u52a1\u521b\u5efa\u9ad8\u5ea6\u96c6\u6210\u548c\u9ad8\u6548\u7684\u7b56\u7565\u3002\u7136\u800c\uff0c\u6b64\u7c7b\u6a21\u578b\u96be\u4ee5\u6709\u6548\u8bad\u7ec3\uff0c\u5e76\u4e14\u901a\u5e38\u96be\u4ee5\u6cdb\u5316\u5230\u5176\u8bad\u7ec3\u573a\u666f\u4e4b\u5916\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5bf9\u65b0\u73af\u5883\u3001\u4efb\u52a1\u548c\u6982\u5ff5\u7684\u9002\u5e94\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5728\u770b\u4e0d\u89c1\u7684\u6587\u672c\u6307\u4ee4\u548c\u89c6\u89c9\u5206\u5e03\u53d8\u5316\u4e0b\uff0c\u57fa\u4e8e\u89c6\u89c9\u7684\u63a7\u5236\u7b56\u7565\u5b9e\u73b0\u7a33\u5065\u7684\u95ed\u73af\u6027\u80fd\u6240\u9700\u7684\u6700\u5c0f\u6570\u636e\u8981\u6c42\u548c\u67b6\u6784\u9002\u5e94\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u5177\u6709\u4e0d\u540c\u6570\u636e\u8868\u793a\u4e30\u5bcc\u5ea6\u7684\u6570\u636e\u5e93\uff0c\u901a\u8fc7\u5229\u7528\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7f16\u7801\u5668\u6765\u6539\u8fdb\u7279\u5f81\u63d0\u53d6\u534f\u8bae\uff0c\u5e76\u8bc4\u4f30\u4e0d\u540c\u7b56\u7565\u7f51\u7edc\u5934\u7684\u9002\u7528\u6027\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5728 Flex\uff08Fly-lexically\uff09\u4e2d\u5f97\u5230\u7efc\u5408\uff0c\u8fd9\u662f\u4e00\u4e2a\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4f5c\u4e3a\u51bb\u7ed3\u7684\u9010\u5757\u7279\u5f81\u63d0\u53d6\u5668\u7684\u6846\u67b6\uff0c\u751f\u6210\u6574\u5408\u8bed\u4e49\u548c\u89c6\u89c9\u4fe1\u606f\u7684\u5177\u6709\u7a7a\u95f4\u611f\u77e5\u7684\u5d4c\u5165\u3002\u8fd9\u4e9b\u4e30\u5bcc\u7684\u7279\u5f81\u6784\u6210\u4e86\u8bad\u7ec3\u9ad8\u5ea6\u7a33\u5065\u7684\u4e0b\u6e38\u7b56\u7565\u7684\u57fa\u7840\uff0c\u8fd9\u4e9b\u7b56\u7565\u80fd\u591f\u8de8\u5e73\u53f0\u3001\u73af\u5883\u548c\u6587\u672c\u6307\u5b9a\u7684\u4efb\u52a1\u8fdb\u884c\u6cdb\u5316\u3002\u6211\u4eec\u5c55\u793a\u4e86\u8fd9\u79cd\u65b9\u6cd5\u5728\u56db\u65cb\u7ffc\u98de\u884c\u5668\u98de\u5f80\u76ee\u6807\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5176\u4e2d\u901a\u8fc7\u884c\u4e3a\u514b\u9686\u5728\u5c0f\u578b\u6a21\u62df\u6570\u636e\u5e93\u4e0a\u8bad\u7ec3\u7684\u4ee3\u7406\u6210\u529f\u5730\u6cdb\u5316\u5230\u73b0\u5b9e\u4e16\u754c\u573a\u666f\uff0c\u5904\u7406\u4e0d\u540c\u7684\u65b0\u76ee\u6807\u548c\u547d\u4ee4\u516c\u5f0f\u3002||\n", "2410.12787": "|**2024-10-16**|[The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio](http://arxiv.org/abs/2410.12787)|null|\u8fd1\u5e74\u6765\uff0c\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u7684\u8fdb\u6b65\u663e\u8457\u63d0\u9ad8\u4e86\u5176\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4eba\u4eec\u4e00\u76f4\u5728\u52aa\u529b\u8fdb\u4e00\u6b65\u6574\u5408\u89c6\u9891\u548c\u97f3\u9891\u7b49\u5176\u4ed6\u6a21\u6001\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u7684 LMM \u4ecd\u7136\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\uff0c\u5373\u4e8b\u5b9e\u4e0a\u7684\u591a\u6a21\u6001\u8f93\u5165\u4e0e\u751f\u6210\u7684\u6587\u672c\u8f93\u51fa\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5404\u79cd\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u6d89\u53ca\u4e09\u79cd\u6700\u5e38\u89c1\u6a21\u6001\uff08\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u97f3\u9891\uff09\u7684 LMM \u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002\u6211\u4eec\u7684\u7814\u7a76\u63ed\u793a\u4e86\u5bfc\u81f4\u5e7b\u89c9\u7684\u4e24\u4e2a\u5173\u952e\u56e0\u7d20\uff1a\u8fc7\u5ea6\u4f9d\u8d56\u5355\u6a21\u6001\u5148\u9a8c\u548c\u865a\u5047\u7684\u6a21\u6001\u95f4\u76f8\u5173\u6027\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u591a\u6a21\u6001\u8bc5\u5492 (CMM) \u57fa\u51c6\u6d4b\u8bd5\uff0c\u8be5\u57fa\u51c6\u5168\u9762\u8bc4\u4f30\u4e86 LMM \u4e2d\u7684\u5e7b\u89c9\uff0c\u5e76\u8be6\u7ec6\u5206\u6790\u4e86\u5176\u6839\u672c\u95ee\u9898\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u5173\u952e\u7684\u6f0f\u6d1e\uff0c\u5305\u62ec\u6a21\u6001\u6574\u5408\u7684\u4e0d\u5e73\u8861\u548c\u8bad\u7ec3\u6570\u636e\u7684\u504f\u5dee\uff0c\u5f3a\u8c03\u4e86\u5e73\u8861\u8de8\u6a21\u6001\u5b66\u4e60\u548c\u589e\u5f3a\u5e7b\u89c9\u7f13\u89e3\u7b56\u7565\u7684\u5fc5\u8981\u6027\u3002\u6839\u636e\u6211\u4eec\u7684\u89c2\u5bdf\u548c\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e9b\u6f5c\u5728\u7684\u7814\u7a76\u65b9\u5411\uff0c\u53ef\u4ee5\u63d0\u9ad8 LMM \u7684\u53ef\u9760\u6027\u3002||\n"}, "6DOF Object Pose": {"2409.02581": "|**2024-09-04**|[Object Gaussian for Monocular 6D Pose Estimation from Sparse Views](http://arxiv.org/abs/2409.02581)|null|\u5355\u76ee\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u4eba\u6280\u672f\u4e2d\u7684\u4e00\u9879\u5173\u952e\u4efb\u52a1\uff0c\u4e25\u91cd\u4f9d\u8d56\u4e8e\u7cbe\u786e\u7684 2D-3D \u5bf9\u5e94\u5173\u7cfb\uff0c\u800c\u8fd9\u901a\u5e38\u9700\u8981\u6602\u8d35\u7684 CAD \u6a21\u578b\uff0c\u800c\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u5e76\u4e0d\u5bb9\u6613\u83b7\u5f97\u3002\u7269\u4f53\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u5176\u4e2d\u6700\u8fd1\u5728\u4e09\u7ef4\u9ad8\u65af\u6e32\u67d3 (3DGS) \u65b9\u9762\u7684\u8fdb\u5c55\u63d0\u4f9b\u4e86\u5f15\u4eba\u6ce8\u76ee\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u5b83\u7684\u6027\u80fd\u4ecd\u7136\u4e0d\u5c3d\u7406\u60f3\uff0c\u5e76\u4e14\u5728\u8f93\u5165\u89c6\u56fe\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u5bb9\u6613\u51fa\u73b0\u8fc7\u62df\u5408\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86 SGPose\uff0c\u8fd9\u662f\u4e00\u4e2a\u4f7f\u7528\u57fa\u4e8e\u9ad8\u65af\u65b9\u6cd5\u8fdb\u884c\u7a00\u758f\u89c6\u56fe\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u65b0\u6846\u67b6\u3002\u53ea\u9700\u5341\u4e2a\u89c6\u56fe\uff0cSGPose \u5c31\u53ef\u4ee5\u901a\u8fc7\u4ece\u968f\u673a\u957f\u65b9\u4f53\u521d\u59cb\u5316\u5f00\u59cb\u751f\u6210\u51e0\u4f55\u611f\u77e5\u8868\u793a\uff0c\u4ece\u800c\u907f\u514d\u4f9d\u8d56\u4f20\u7edf 3DGS \u65b9\u6cd5\u6240\u9700\u7684\u8fd0\u52a8\u6062\u590d\u7ed3\u6784 (SfM) \u7ba1\u9053\u884d\u751f\u51e0\u4f55\u3002SGPose \u901a\u8fc7\u56de\u5f52\u7a00\u758f\u8f93\u5165\u548c\u968f\u673a\u521d\u59cb\u5316\u7684\u56fe\u50cf\u4e0e\u91cd\u5efa\u6a21\u578b\u4e4b\u95f4\u7684\u5bc6\u96c6 2D-3D \u5bf9\u5e94\u5173\u7cfb\uff0c\u6d88\u9664\u4e86\u5bf9 CAD \u6a21\u578b\u7684\u4f9d\u8d56\uff0c\u800c\u51e0\u4f55\u4e00\u81f4\u7684\u6df1\u5ea6\u76d1\u7763\u548c\u5728\u7ebf\u5408\u6210\u89c6\u56fe\u626d\u66f2\u662f\u5176\u6210\u529f\u7684\u5173\u952e\u3002\u5728\u5178\u578b\u57fa\u51c6\u6570\u636e\u96c6\uff08\u7279\u522b\u662f\u5728\u906e\u6321 LM-O \u6570\u636e\u96c6\u4e0a\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u7a00\u758f\u89c6\u56fe\u7ea6\u675f\u4e0b\uff0cSGPose \u7684\u6027\u80fd\u4e5f\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd9\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002||\n", "2408.16547": "|**2024-08-29**|[OP-Align: Object-level and Part-level Alignment for Self-supervised Category-level Articulated Object Pose Estimation](http://arxiv.org/abs/2408.16547)|**[link](https://github.com/yc-che/op-align)**|\u7c7b\u522b\u7ea7\u94f0\u63a5\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4fa7\u91cd\u4e8e\u4f30\u8ba1\u5df2\u77e5\u7c7b\u522b\u4e2d\u672a\u77e5\u94f0\u63a5\u7269\u4f53\u7684\u59ff\u6001\u3002\u5c3d\u7ba1\u610f\u4e49\u91cd\u5927\uff0c\u4f46\u8fd9\u9879\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u539f\u56e0\u5728\u4e8e\u7269\u4f53\u7684\u5f62\u72b6\u548c\u59ff\u6001\u5404\u4e0d\u76f8\u540c\uff0c\u6570\u636e\u96c6\u6807\u6ce8\u6210\u672c\u9ad8\u6602\uff0c\u4ee5\u53ca\u73b0\u5b9e\u73af\u5883\u590d\u6742\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u5229\u7528\u5355\u5e27\u70b9\u4e91\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u6211\u4eec\u7684\u6a21\u578b\u59cb\u7ec8\u5982\u4e00\u5730\u751f\u6210\u5177\u6709\u89c4\u8303\u59ff\u6001\u548c\u5173\u8282\u72b6\u6001\u7684\u5b8c\u6574\u8f93\u5165\u5bf9\u8c61\u7684\u91cd\u5efa\uff0c\u5e76\u4f30\u8ba1\u5bf9\u8c61\u7ea7\u59ff\u6001\uff08\u51cf\u5c11\u6574\u4f53\u59ff\u6001\u5dee\u5f02\uff09\u548c\u96f6\u4ef6\u7ea7\u59ff\u6001\uff08\u5c06\u8f93\u5165\u7684\u6bcf\u4e2a\u96f6\u4ef6\u4e0e\u5176\u5bf9\u5e94\u7684\u91cd\u5efa\u96f6\u4ef6\u5bf9\u9f50\uff09\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u4ee5\u5f80\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u4e14\u4e0e\u6700\u5148\u8fdb\u7684\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u3002\u4e3a\u4e86\u8bc4\u4f30\u6211\u4eec\u7684\u6a21\u578b\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u73b0\u5b9e\u4e16\u754c\u94f0\u63a5\u7269\u4f53\u57fa\u51c6\u6570\u636e\u96c6\u3002||\n", "2408.10450": "|**2024-08-19**|[RUMI: Rummaging Using Mutual Information](http://arxiv.org/abs/2408.10450)|null|\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u7ffb\u52a8\u7269\u4f53\u65b9\u6cd5 (RUMI)\uff0c\u8be5\u65b9\u6cd5\u7528\u4e8e\u5728\u7ebf\u751f\u6210\u673a\u5668\u4eba\u5728\u89c6\u89c9\u906e\u6321\u73af\u5883\u4e2d\u6536\u96c6\u5df2\u77e5\u53ef\u52a8\u7269\u4f53\u59ff\u6001\u4fe1\u606f\u7684\u52a8\u4f5c\u5e8f\u5217\u3002\u8be5\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u63a5\u89e6\u4e30\u5bcc\u7684\u7ffb\u52a8\u7269\u4f53\u64cd\u4f5c\uff0c\u5229\u7528\u7269\u4f53\u59ff\u6001\u5206\u5e03\u548c\u673a\u5668\u4eba\u8f68\u8ff9\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u8fdb\u884c\u52a8\u4f5c\u89c4\u5212\u3002\u4ece\u89c2\u5bdf\u5230\u7684\u90e8\u5206\u70b9\u4e91\u51fa\u53d1\uff0cRUMI \u63a8\u65ad\u51fa\u517c\u5bb9\u7684\u7269\u4f53\u59ff\u6001\u5206\u5e03\uff0c\u5e76\u5b9e\u65f6\u5229\u7528\u5de5\u4f5c\u7a7a\u95f4\u5360\u7528\u7387\u8fd1\u4f3c\u5176\u4e92\u4fe1\u606f\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u4fe1\u606f\u589e\u76ca\u6210\u672c\u51fd\u6570\u548c\u53ef\u8fbe\u6027\u6210\u672c\u51fd\u6570\uff0c\u4ee5\u5c06\u7269\u4f53\u4fdd\u6301\u5728\u673a\u5668\u4eba\u7684\u89e6\u53ca\u8303\u56f4\u5185\u3002\u8fd9\u4e9b\u51fd\u6570\u88ab\u96c6\u6210\u5230\u5177\u6709\u968f\u673a\u52a8\u529b\u5b66\u6a21\u578b\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236 (MPC) \u6846\u67b6\u4e2d\uff0c\u5728\u95ed\u73af\u4e2d\u66f4\u65b0\u59ff\u6001\u5206\u5e03\u3002\u4e3b\u8981\u8d21\u732e\u5305\u62ec\u4e00\u79cd\u65b0\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7f6e\u4fe1\u6846\u67b6\u3001\u4e00\u79cd\u9ad8\u6548\u7684\u4fe1\u606f\u589e\u76ca\u8ba1\u7b97\u7b56\u7565\u4ee5\u53ca\u4e00\u79cd\u9c81\u68d2\u7684\u57fa\u4e8e MPC \u7684\u63a7\u5236\u65b9\u6848\u3002\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cRUMI \u5728\u4eff\u771f\u548c\u5b9e\u9645\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002||\n", "2408.08234": "|**2024-08-15**|[Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation](http://arxiv.org/abs/2408.08234)|**[link](https://github.com/varunburde/reconstruction_pose_benchmark)**|\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u5728\u6d89\u53ca\u673a\u5668\u4eba\u64cd\u4f5c\u3001\u5bfc\u822a\u548c\u589e\u5f3a\u73b0\u5b9e\u7684\u8bb8\u591a\u5de5\u4e1a\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u901a\u7528\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u5668\uff0c\u5373\u4e0d\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u7269\u4f53\u8fdb\u884c\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u4f9d\u8d56\u4e8e\u7cbe\u786e\u7684 3D \u6a21\u578b\u3002\u76ee\u524d\u4e3b\u8981\u4f7f\u7528 CAD \u6a21\u578b\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u5f88\u96be\u83b7\u5f97\u3002\u540c\u65f6\uff0c\u83b7\u53d6\u7269\u4f53\u7684\u56fe\u50cf\u901a\u5e38\u662f\u53ef\u884c\u7684\u3002\u81ea\u7136\u800c\u7136\u5730\uff0c\u8fd9\u5c31\u5f15\u51fa\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u4ece\u56fe\u50cf\u91cd\u5efa\u7684 3D \u6a21\u578b\u662f\u5426\u8db3\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff1f\u4e3a\u4e86\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8861\u91cf 3D \u91cd\u5efa\u8d28\u91cf\u5bf9\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u7528\u4e8e\u7269\u4f53\u91cd\u5efa\u7684\u6821\u51c6\u56fe\u50cf\uff0c\u8fd9\u4e9b\u56fe\u50cf\u4e0e YCB-V \u6570\u636e\u96c6\u7684\u6d4b\u8bd5\u56fe\u50cf\u914d\u51c6\uff0c\u7528\u4e8e\u5728 BOP \u57fa\u51c6\u6d4b\u8bd5\u683c\u5f0f\u4e0b\u8fdb\u884c\u59ff\u6001\u8bc4\u4f30\u3002\u5bf9\u591a\u79cd\u6700\u5148\u8fdb\u7684 3D \u91cd\u5efa\u548c\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u8fdb\u884c\u7684\u8be6\u7ec6\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u4ee3\u91cd\u5efa\u65b9\u6cd5\u751f\u6210\u7684\u51e0\u4f55\u7ed3\u6784\u901a\u5e38\u8db3\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u59ff\u6001\u4f30\u8ba1\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u5f97\u51fa\u4e86\u4e00\u4e9b\u6709\u8da3\u7684\u89c2\u5bdf\u7ed3\u679c\uff1a\uff081\uff09\u7528\u4e8e\u8861\u91cf 3D \u91cd\u5efa\u8d28\u91cf\u7684\u6807\u51c6\u6307\u6807\u4e0d\u4e00\u5b9a\u80fd\u53cd\u6620\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u8fd9\u8868\u660e\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f8b\u5982\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\uff082\uff09\u7ecf\u5178\u7684\u3001\u975e\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u53ef\u4ee5\u4e0e\u73b0\u4ee3\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u91cd\u5efa\u6280\u672f\u76f8\u5ab2\u7f8e\uff0c\u751a\u81f3\u53ef\u4ee5\u63d0\u4f9b\u66f4\u597d\u7684\u91cd\u5efa\u65f6\u95f4-\u59ff\u6001\u7cbe\u5ea6\u6743\u8861\u3002\uff083\uff09\u4f7f\u7528\u91cd\u5efa\u6a21\u578b\u548c CAD \u6a21\u578b\u7684\u6027\u80fd\u4e4b\u95f4\u4ecd\u7136\u5b58\u5728\u76f8\u5f53\u5927\u7684\u5dee\u8ddd\u3002\u4e3a\u4e86\u4fc3\u8fdb\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u7684\u7814\u7a76\uff0c\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u5728 https://github.com/VarunBurde/reconstruction_pose_benchmark \u4e0a\u516c\u5f00\u53ef\u7528\u3002||\n", "2407.12207": "|**2024-07-16**|[NeuSurfEmb: A Complete Pipeline for Dense Correspondence-based 6D Object Pose Estimation without CAD Models](http://arxiv.org/abs/2407.12207)|**[link](https://github.com/ethz-asl/neusurfemb)**|\u76ee\u524d\u6700\u5148\u8fdb\u7684 6D \u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u90fd\u5047\u8bbe\u53ef\u4ee5\u4f7f\u7528 CAD \u6a21\u578b\uff0c\u5e76\u4e14\u9700\u8981\u7528\u6237\u624b\u52a8\u8bbe\u7f6e\u57fa\u4e8e\u7269\u7406\u7684\u6e32\u67d3 (PBR) \u6d41\u7a0b\u4ee5\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\u3002\u8fd9\u4e24\u4e2a\u56e0\u7d20\u90fd\u9650\u5236\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0d\u9700\u8981 CAD \u6a21\u578b\u7684\u6d41\u7a0b\uff0c\u5e76\u4e14\u53ea\u9700\u4e00\u5c0f\u7ec4\u771f\u5b9e\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u5373\u53ef\u8bad\u7ec3\u51fa\u6700\u5148\u8fdb\u7684\u59ff\u6001\u4f30\u8ba1\u5668\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u57fa\u4e8e NeuS2 \u5bf9\u8c61\u8868\u793a\uff0c\u6211\u4eec\u901a\u8fc7\u57fa\u4e8e\u8fd0\u52a8\u6062\u590d\u7ed3\u6784 (SfM) \u548c\u5bf9\u8c61\u65e0\u5173\u5206\u5272\u7684\u534a\u81ea\u52a8\u5316\u7a0b\u5e8f\u6765\u5b66\u4e60\u8be5\u8868\u793a\u3002\u6211\u4eec\u5229\u7528 NeuS2 \u7684\u65b0\u89c6\u56fe\u5408\u6210\u80fd\u529b\u548c\u7b80\u5355\u7684\u526a\u5207\u7c98\u8d34\u589e\u5f3a\u529f\u80fd\u6765\u81ea\u52a8\u751f\u6210\u903c\u771f\u7684\u5bf9\u8c61\u6e32\u67d3\u56fe\uff0c\u6211\u4eec\u4f7f\u7528\u8fd9\u4e9b\u6e32\u67d3\u56fe\u6765\u8bad\u7ec3\u57fa\u4e8e\u5bf9\u5e94\u7684 SurfEmb \u59ff\u6001\u4f30\u8ba1\u5668\u3002\u6211\u4eec\u5728 LINEMOD-Occlusion \u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e7f\u6cdb\u7814\u7a76\u4e86\u5176\u5404\u4e2a\u7ec4\u4ef6\u7684\u5f71\u54cd\uff0c\u5e76\u5c55\u793a\u4e86\u76f8\u5bf9\u4e8e\u57fa\u4e8e CAD \u6a21\u578b\u548c PBR \u6570\u636e\u7684\u65b9\u6cd5\u7684\u7ade\u4e89\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u6211\u4eec\u6d41\u7a0b\u5728\u81ea\u6536\u96c6\u7684\u73b0\u5b9e\u4e16\u754c\u5bf9\u8c61\u4e0a\u7684\u6613\u7528\u6027\u548c\u6709\u6548\u6027\uff0c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65e0 CAD \u6a21\u578b\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u597d\u7684\u7cbe\u5ea6\u548c\u5bf9\u8f7b\u5ea6\u906e\u6321\u7684\u9c81\u68d2\u6027\u3002\u4e3a\u4e86\u8ba9\u673a\u5668\u4eba\u793e\u533a\u53d7\u76ca\u4e8e\u8be5\u7cfb\u7edf\uff0c\u6211\u4eec\u5c06\u5728 https://www.github.com/ethz-asl/neusurfemb \u4e0a\u516c\u5f00\u53d1\u5e03\u5b83\u3002||\n", "2406.04316": "|**2024-06-06**|[Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking](http://arxiv.org/abs/2406.04316)|null|6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u4e00\u9879\u81f3\u5173\u91cd\u8981\u4f46\u6781\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5176\u9762\u4e34\u7684\u4e3b\u8981\u95ee\u9898\u662f\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u4e25\u91cd\u7f3a\u4e4f\u3002\u8fd9\u79cd\u7a00\u7f3a\u6027\u963b\u788d\u4e86\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u9650\u5236\u4e86\u7814\u7a76\u8fdb\u5c55\u3002\u6b64\u5916\uff0c\u53ef\u7528\u5b9e\u4f8b\u6216\u7c7b\u522b\u7684\u6570\u91cf\u6709\u9650\u4e5f\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86Omni6DPose\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ee5\u5bf9\u8c61\u7c7b\u522b\u591a\u6837\u6027\u3001\u89c4\u6a21\u5927\u548c\u5bf9\u8c61\u6750\u8d28\u591a\u6837\u6027\u4e3a\u7279\u5f81\u7684\u5927\u578b\u6570\u636e\u96c6\u3002Omni6DPose\u4e3b\u8981\u7531\u4e09\u4e2a\u90e8\u5206\u7ec4\u6210\uff1aROPE\uff08\u771f\u5b9e6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u96c6\uff09\uff0c\u5305\u542b332K\u5f20\u56fe\u50cf\uff0c\u6db5\u76d6149\u4e2a\u7c7b\u522b\u3001581\u4e2a\u5b9e\u4f8b\u7684\u8d85\u8fc7150\u4e07\u4e2a\u6807\u6ce8\uff1bSOPE\uff08\u6a21\u62df6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u96c6\uff09\uff0c\u7531\u6df7\u5408\u73b0\u5b9e\u73af\u5883\u4e2d\u521b\u5efa\u7684475K\u5f20\u56fe\u50cf\u7ec4\u6210\uff0c\u5229\u7528\u6df1\u5ea6\u6a21\u62df\u6280\u672f\u8fdb\u884c\u6807\u6ce8\uff0c\u6db5\u76d6\u4e0eROPE\u76f8\u540c\u7684149\u4e2a\u7c7b\u522b\u30014162\u4e2a\u5b9e\u4f8b\u7684\u8d85\u8fc7500\u4e07\u4e2a\u6807\u6ce8\uff1b\u4ee5\u53ca\u5728ROPE\u548cSOPE\u4e2d\u5747\u4f7f\u7528\u7684\u3001\u7ecf\u8fc7\u624b\u52a8\u5bf9\u9f50\u7684\u771f\u5b9e\u626b\u63cf\u7269\u4f53\u3002\u7531\u4e8e\u5b58\u5728\u5927\u91cf\u7684\u53d8\u5316\u548c\u6a21\u7cca\u6027\uff0cOmni6DPose\u672c\u8eab\u5c31\u5177\u6709\u5f88\u5927\u7684\u6311\u6218\u6027\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86GenPose++\uff0c\u5b83\u662fSOTA\u7c7b\u522b\u7ea7\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\u7684\u589e\u5f3a\u7248\u672c\uff0c\u5b83\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6539\u8fdb\uff1a\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u63d0\u53d6\u548c\u57fa\u4e8e\u805a\u7c7b\u7684\u805a\u5408\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u5206\u6790\uff0c\u4ee5\u8bc4\u4f30\u5148\u524d\u65b9\u6cd5\u5728\u8fd9\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u57286D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u59ff\u6001\u8ddf\u8e2a\u65b9\u9762\u7684\u6027\u80fd\u3002||\n", "2406.02977": "|**2024-06-05**|[Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices](http://arxiv.org/abs/2406.02977)|null|\u968f\u7740\u673a\u5668\u4eba\u548c\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u8d8a\u6765\u8d8a\u4f9d\u8d56\u4e8e\u7cbe\u786e\u9ad8\u6548\u76846D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u6027\u80fd\u5bf9\u4e8e\u5b9e\u73b0\u66f4\u5177\u4ea4\u4e92\u6027\u548c\u54cd\u5e94\u80fd\u529b\u7684\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u63d0\u51fa\u7684\u7a00\u758f\u989c\u8272\u4ee3\u7801\u7f51\u7edc\uff08SCCN\uff09\u4f53\u73b0\u4e86\u4e00\u79cd\u6e05\u6670\u7b80\u6d01\u7684\u6d41\u7a0b\u8bbe\u8ba1\uff0c\u4ee5\u6709\u6548\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u3002SCCN\u5bf9RGB\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u7269\u4f53\u8fdb\u884c\u50cf\u7d20\u7ea7\u9884\u6d4b\uff0c\u5229\u7528\u57fa\u672c\u7269\u4f53\u51e0\u4f55\u7279\u5f81\u7684\u7a00\u758f\u6027\u6765\u52a0\u901fPerspective-n-Point\uff08PnP\uff09\u8ba1\u7b97\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u5b83\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u50cf\u7d20\u7ea7\u51e0\u4f55\u7684\u7269\u4f53\u5bf9\u79f0\u8868\u793a\uff0c\u8be5\u8868\u793a\u4e0e\u521d\u59cb\u59ff\u6001\u9884\u6d4b\u65e0\u7f1d\u96c6\u6210\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5bf9\u79f0\u7269\u4f53\u6b67\u4e49\u95ee\u9898\u3002SCCN\u5728\u82f1\u4f1f\u8fbeJetson AGX Xavier\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86\u5728\u57fa\u51c6LINEMOD\u6570\u636e\u96c6\u548c\u906e\u6321LINEMOD\u6570\u636e\u96c6\u4e0a\u6bcf\u79d219\u5e27\uff08FPS\uff09\u548c6\u5e27\u7684\u4f30\u8ba1\u901f\u7387\uff0c\u540c\u65f6\u5728\u8fd9\u4e9b\u901f\u7387\u4e0b\u59cb\u7ec8\u4fdd\u6301\u8f83\u9ad8\u7684\u4f30\u8ba1\u7cbe\u5ea6\u3002||\n", "2405.07801": "|**2024-05-31**|[Deep Learning-Based Object Pose Estimation: A Comprehensive Survey](http://arxiv.org/abs/2405.07801)|**[link](https://github.com/cnjianliu/awesome-object-pose-estimation)**|\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff0c\u5728\u589e\u5f3a\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u6280\u672f\u4e2d\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u5728\u8fc7\u53bb\u7684\u5341\u5e74\u4e2d\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7531\u4e8e\u5176\u5353\u8d8a\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u8d8a\u6765\u8d8a\u591a\u5730\u53d6\u4ee3\u4e86\u4f9d\u8d56\u4e8e\u5de5\u7a0b\u70b9\u5bf9\u7279\u5f81\u7684\u4f20\u7edf\u7b97\u6cd5\u3002\u7136\u800c\uff0c\u5f53\u4ee3\u65b9\u6cd5\u4ecd\u7136\u5b58\u5728\u82e5\u5e72\u6311\u6218\uff0c\u5305\u62ec\u5b83\u4eec\u5bf9\u6807\u8bb0\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\u6027\u3001\u6a21\u578b\u7d27\u51d1\u6027\u3001\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u65b0\u7269\u4f53\u80fd\u529b\u3002\u76ee\u524d\u7f3a\u4e4f\u4e00\u7bc7\u7efc\u8ff0\u6765\u8ba8\u8bba\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3001\u9762\u4e34\u7684\u6311\u6218\u548c\u672a\u6765\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6db5\u76d6\u4e86\u8be5\u95ee\u9898\u7684\u6240\u6709\u4e09\u79cd\u5f62\u5f0f\uff0c\u5373\u5b9e\u4f8b\u7ea7\u3001\u7c7b\u522b\u7ea7\u548c\u672a\u89c1\u8fc7\u7269\u4f53\u7684\u59ff\u6001\u4f30\u8ba1\u3002\u6211\u4eec\u7684\u7efc\u8ff0\u8fd8\u6db5\u76d6\u4e86\u591a\u79cd\u8f93\u5165\u6570\u636e\u6a21\u6001\u3001\u8f93\u51fa\u59ff\u6001\u7684\u81ea\u7531\u5ea6\u3001\u7269\u4f53\u5c5e\u6027\u548c\u4e0b\u6e38\u4efb\u52a1\uff0c\u4e3a\u8bfb\u8005\u63d0\u4f9b\u4e86\u5bf9\u8be5\u9886\u57df\u7684\u5168\u9762\u7406\u89e3\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u8ba8\u8bba\u4e86\u4e0d\u540c\u9886\u57df\u7684\u8bad\u7ec3\u8303\u5f0f\u3001\u63a8\u7406\u6a21\u5f0f\u3001\u5e94\u7528\u9886\u57df\u3001\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u62a5\u544a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u5728\u8fd9\u4e9b\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\uff0c\u4ece\u800c\u65b9\u4fbf\u8bfb\u8005\u4e3a\u5176\u5e94\u7528\u9009\u62e9\u6700\u5408\u9002\u7684\u65b9\u6cd5\u3002\u6700\u540e\uff0c\u8be5\u7efc\u8ff0\u6307\u51fa\u4e86\u5173\u952e\u6311\u6218\uff0c\u56de\u987e\u4e86\u5f53\u524d\u7684\u8d8b\u52bf\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u5e76\u786e\u5b9a\u4e86\u672a\u6765\u7814\u7a76\u7684\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002\u6211\u4eec\u8fd8\u5728 https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation \u4e0a\u6301\u7eed\u8ddf\u8e2a\u6700\u65b0\u7684\u5de5\u4f5c\u3002||\n", "2403.19527": "|**2024-03-28**|[Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation](http://arxiv.org/abs/2403.19527)|**[link](https://github.com/leeiieeo/ag-pose)**|\u7c7b\u522b\u7ea7 6D \u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65e8\u5728\u4f30\u8ba1\u7279\u5b9a\u7c7b\u522b\u4e2d\u672a\u89c1\u5b9e\u4f8b\u7684\u65cb\u8f6c\u3001\u5e73\u79fb\u548c\u5927\u5c0f\u3002\u5728\u8fd9\u4e00\u9886\u57df\uff0c\u57fa\u4e8e\u5bc6\u96c6\u5bf9\u5e94\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5b83\u4eec\u6ca1\u6709\u660e\u786e\u8003\u8651\u4e0d\u540c\u5b9e\u4f8b\u7684\u5c40\u90e8\u548c\u5168\u5c40\u51e0\u4f55\u4fe1\u606f\uff0c\u5bfc\u81f4\u5bf9\u5f62\u72b6\u53d8\u5316\u663e\u8457\u7684\u672a\u89c1\u5b9e\u4f8b\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5b9e\u4f8b\u81ea\u9002\u5e94\u548c\u51e0\u4f55\u611f\u77e5\u7684\u5173\u952e\u70b9\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u7c7b\u522b\u7ea7 6D \u7269\u4f53\u59ff\u6001\u4f30\u8ba1 (AG-Pose)\uff0c\u5b83\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a\uff081\uff09\u7b2c\u4e00\u4e2a\u8bbe\u8ba1\u662f\u5b9e\u4f8b\u81ea\u9002\u5e94\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u5757\uff0c\u5b83\u53ef\u4ee5\u81ea\u9002\u5e94\u5730\u68c0\u6d4b\u4e00\u7ec4\u7a00\u758f\u7684\u5173\u952e\u70b9\uff0c\u7528\u4e8e\u8868\u793a\u5404\u79cd\u5b9e\u4f8b\u7684\u51e0\u4f55\u7ed3\u6784\u3002(2) \u7b2c\u4e8c\u4e2a\u8bbe\u8ba1\u662f\u51e0\u4f55\u611f\u77e5\u7279\u5f81\u805a\u5408\u6a21\u5757\uff0c\u5b83\u53ef\u4ee5\u6709\u6548\u5730\u5c06\u5c40\u90e8\u548c\u5168\u5c40\u51e0\u4f55\u4fe1\u606f\u6574\u5408\u5230\u5173\u952e\u70b9\u7279\u5f81\u4e2d\u3002\u8fd9\u4e24\u4e2a\u6a21\u5757\u53ef\u4ee5\u534f\u540c\u5de5\u4f5c\uff0c\u4e3a\u672a\u89c1\u5b9e\u4f8b\u5efa\u7acb\u9c81\u68d2\u7684\u5173\u952e\u70b9\u7ea7\u5bf9\u5e94\u5173\u7cfb\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728 CAMERA25 \u548c REAL275 \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 AG-Pose \u5728\u6ca1\u6709\u7c7b\u522b\u7279\u5b9a\u5f62\u72b6\u5148\u9a8c\u7684\u60c5\u51b5\u4e0b\uff0c\u5927\u5927\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002||\n", "2403.18791": "|**2024-06-01**|[Object Pose Estimation via the Aggregation of Diffusion Features](http://arxiv.org/abs/2403.18791)|**[link](https://github.com/tianfu18/diff-feats-pose)**|\u4ece\u56fe\u50cf\u4e2d\u4f30\u8ba1\u7269\u4f53\u59ff\u6001\u662f3D\u573a\u666f\u7406\u89e3\u7684\u5173\u952e\u4efb\u52a1\uff0c\u6700\u8fd1\u7684\u65b9\u6cd5\u5728\u975e\u5e38\u5927\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u53ef\u559c\u7684\u7ed3\u679c\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5904\u7406\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u65f6\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\u3002\u6211\u4eec\u8ba4\u4e3a\u8fd9\u662f\u7531\u4e8e\u56fe\u50cf\u7279\u5f81\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u9020\u6210\u7684\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5bf9\u6269\u6563\u6a21\u578b\uff08\u4f8b\u5982Stable Diffusion\uff09\u7684\u7279\u5f81\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u5bf9\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u5efa\u6a21\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002\u5728\u6b64\u5206\u6790\u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u521b\u65b0\u6027\u5730\u5c06\u8fd9\u4e9b\u6269\u6563\u7279\u5f81\u5f15\u5165\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u67b6\u6784\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u6355\u83b7\u548c\u805a\u5408\u4e0d\u540c\u7c92\u5ea6\u7684\u6269\u6563\u7279\u5f81\uff0c\u6781\u5927\u5730\u63d0\u9ad8\u4e86\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e09\u4e2a\u6d41\u884c\u7684\u57fa\u51c6\u6570\u636e\u96c6LM\u3001O-LM\u548cT-LESS\u4e0a\uff0c\u4ee5\u76f8\u5f53\u5927\u7684\u4f18\u52bf\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u4e0a\u53d6\u5f97\u4e86\u6bd4\u5148\u524d\u6700\u4f73\u7ed3\u679c\u66f4\u9ad8\u7684\u7cbe\u5ea6\uff1a\u5728Unseen LM\u4e0a\u4e3a98.2%\u5bf993.5%\uff0c\u5728Unseen O-LM\u4e0a\u4e3a85.9%\u5bf976.3%\uff0c\u663e\u793a\u4e86\u6211\u4eec\u65b9\u6cd5\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53d1\u5e03\u5728https://github.com/Tianfu18/diff-feats-pose\u3002||\n", "2409.08269": "|**2024-09-12**|[Touch2Touch: Cross-Modal Tactile Generation for Object Manipulation](http://arxiv.org/abs/2409.08269)|null|\u73b0\u4eca\u7684\u89e6\u89c9\u4f20\u611f\u5668\u5f62\u6001\u5404\u5f02\uff0c\u5c3a\u5bf8\u4e0d\u4e00\u3002\u7531\u4e8e\u6a21\u578b\u901a\u5e38\u4e0e\u7279\u5b9a\u7684\u4f20\u611f\u5668\u8bbe\u8ba1\u7ed1\u5b9a\uff0c\u56e0\u6b64\u5f00\u53d1\u901a\u7528\u7684\u89e6\u89c9\u5904\u7406\u65b9\u6cd5\u53d8\u5f97\u6781\u5177\u6311\u6218\u6027\u3002\u6211\u4eec\u901a\u8fc7\u5728\u89e6\u89c9\u4f20\u611f\u5668\u4e4b\u95f4\u8fdb\u884c\u8de8\u6a21\u6001\u9884\u6d4b\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff1a\u7ed9\u5b9a\u6765\u81ea\u4e00\u4e2a\u4f20\u611f\u5668\u7684\u89e6\u89c9\u4fe1\u53f7\uff0c\u6211\u4eec\u4f7f\u7528\u751f\u6210\u6a21\u578b\u6765\u4f30\u8ba1\u53e6\u4e00\u4e2a\u4f20\u611f\u5668\u5982\u4f55\u611f\u77e5\u76f8\u540c\u7684\u7269\u7406\u63a5\u89e6\u3002\u8fd9\u5141\u8bb8\u6211\u4eec\u5c06\u7279\u5b9a\u4e8e\u4f20\u611f\u5668\u7684  \u65b9\u6cd5\u5e94\u7528\u4e8e\u751f\u6210\u7684\u4fe1\u53f7\u3002\u6211\u4eec\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u6269\u6563\u6a21\u578b\u6765\u5b9e\u73b0\u8fd9\u4e2a\u60f3\u6cd5\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u5728\u6d41\u884c\u7684 GelSlim \u548c Soft Bubble \u4f20\u611f\u5668\u4e4b\u95f4\u8fdb\u884c\u8f6c\u6362\u3002\u4f5c\u4e3a\u4e0b\u6e38\u4efb\u52a1\uff0c\u6211\u4eec\u4f7f\u7528 GelSlim \u4f20\u611f\u5668\u6267\u884c\u624b\u6301\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u540c\u65f6\u4f7f\u7528\u4ec5\u5bf9 Soft Bubble \u4fe1\u53f7\u8fdb\u884c\u64cd\u4f5c\u7684\u7b97\u6cd5\u3002\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u53ef\u4ee5\u5728 https://www.mmintlab.com/research/touch2touch/ \u4e0a\u627e\u5230\u3002||\n", "2409.15727": "|**2024-09-24**|[LaPose: Laplacian Mixture Shape Modeling for RGB-Based Category-Level Object Pose Estimation](http://arxiv.org/abs/2409.15727)|null|\u867d\u7136\u57fa\u4e8eRGBD\u7684\u7c7b\u522b\u7ea7\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5f88\u6709\u524d\u666f\uff0c\u4f46\u5b83\u4eec\u5bf9\u6df1\u5ea6\u6570\u636e\u7684\u4f9d\u8d56\u9650\u5236\u4e86\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002\u56e0\u6b64\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u8f6c\u5411\u4e86\u57fa\u4e8eRGB\u7684\u65b9\u6cd5\uff1b\u7136\u800c\uff0c\u7531\u4e8e\u7f3a\u4e4f\u6df1\u5ea6\u4fe1\u606f\uff0c\u5b83\u4eec\u9762\u4e34\u7740\u91cd\u5927\u6311\u6218\u3002\u4e00\u65b9\u9762\uff0c\u6df1\u5ea6\u4fe1\u606f\u7684\u7f3a\u4e4f\u52a0\u5267\u4e86\u5904\u7406\u7c7b\u5185\u5f62\u72b6\u53d8\u5316\u7684\u96be\u5ea6\uff0c\u5bfc\u81f4\u5f62\u72b6\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u589e\u52a0\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u4ec5RGB\u8f93\u5165\u5f15\u5165\u4e86\u56fa\u6709\u7684\u5c3a\u5ea6\u6a21\u7cca\u6027\uff0c\u4f7f\u5f97\u7269\u4f53\u5927\u5c0f\u548c\u5e73\u79fb\u7684\u4f30\u8ba1\u6210\u4e3a\u4e00\u4e2a\u4e0d\u9002\u5b9a\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86LaPose\uff0c\u4e00\u4e2a\u7528\u62c9\u666e\u62c9\u65af\u6df7\u5408\u6a21\u578b\u6765\u5efa\u6a21\u7269\u4f53\u5f62\u72b6\u4ee5\u8fdb\u884c\u59ff\u6001\u4f30\u8ba1\u7684\u65b0\u6846\u67b6\u3002\u901a\u8fc7\u5c06\u6bcf\u4e2a\u70b9\u8868\u793a\u4e3a\u6982\u7387\u5206\u5e03\uff0c\u6211\u4eec\u660e\u786e\u5730\u91cf\u5316\u4e86\u5f62\u72b6\u7684\u4e0d\u786e\u5b9a\u6027\u3002LaPose\u5229\u7528\u5e7f\u4e493D\u4fe1\u606f\u6d41\u548c\u4e13\u95e8\u7684\u7279\u5f81\u6d41\u6765\u72ec\u7acb\u9884\u6d4b\u6bcf\u4e2a\u70b9\u7684\u62c9\u666e\u62c9\u65af\u5206\u5e03\uff0c\u4ece\u800c\u6355\u83b7\u7269\u4f53\u51e0\u4f55\u7684\u4e0d\u540c\u65b9\u9762\u3002\u7136\u540e\uff0c\u8fd9\u4e24\u4e2a\u5206\u5e03\u88ab\u6574\u5408\u4e3a\u4e00\u4e2a\u62c9\u666e\u62c9\u65af\u6df7\u5408\u6a21\u578b\uff0c\u4ee5\u5efa\u7acb2D-3D\u5bf9\u5e94\u5173\u7cfb\uff0c\u8fd9\u4e9b\u5bf9\u5e94\u5173\u7cfb\u88ab\u7528\u4e8e\u901a\u8fc7PnP\u6a21\u5757\u6c42\u89e3\u59ff\u6001\u3002\u4e3a\u4e86\u51cf\u8f7b\u5c3a\u5ea6\u6a21\u7cca\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u7269\u4f53\u5927\u5c0f\u548c\u5e73\u79fb\u7684\u5c3a\u5ea6\u4e0d\u53d8\u8868\u793a\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6574\u4f53\u9c81\u68d2\u6027\u3002\u5728NOCS\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LaPose\u7684\u6709\u6548\u6027\uff0c\u5728\u57fa\u4e8eRGB\u7684\u7c7b\u522b\u7ea7\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4ee3\u7801\u53d1\u5e03\u5728https://github.com/lolrudy/LaPose\u3002||\n", "2409.14592": "|**2024-09-22**|[Tactile Functasets: Neural Implicit Representations of Tactile Datasets](http://arxiv.org/abs/2409.14592)|null|\u73b0\u4ee3\u89e6\u89c9\u4f20\u611f\u5668\u7684\u5316\u8eab\u4f1a\u4ea7\u751f\u9ad8\u7ef4\u5ea6\u7684\u539f\u59cb\u611f\u5b98\u53cd\u9988\uff0c\u4f8b\u5982\u56fe\u50cf\uff0c\u8fd9\u4f7f\u5f97\u8de8\u4f20\u611f\u5668\u9ad8\u6548\u5730\u5b58\u50a8\u3001\u5904\u7406\u548c\u6982\u62ec\u8fd9\u4e9b\u53cd\u9988\u53d8\u5f97\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u4e3a\u89e6\u89c9\u4f20\u611f\u5668\u53cd\u9988\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9690\u51fd\u6570\u8868\u793a\u3002\u6211\u4eec\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u539f\u59cb\u89e6\u89c9\u56fe\u50cf\uff0c\u800c\u662f\u63d0\u51fa\u4e86\u7ecf\u8fc7\u8bad\u7ec3\u7684\u795e\u7ecf\u9690\u51fd\u6570\u6765\u91cd\u5efa\u89e6\u89c9\u6570\u636e\u96c6\uff0c\u4ece\u800c\u751f\u6210\u6355\u83b7\u611f\u5b98\u8f93\u5165\u5e95\u5c42\u7ed3\u6784\u7684\u7d27\u51d1\u8868\u793a\u3002\u4e0e\u539f\u59cb\u8868\u793a\u76f8\u6bd4\uff0c\u8fd9\u4e9b\u8868\u793a\u5177\u6709\u4ee5\u4e0b\u51e0\u4e2a\u4f18\u70b9\uff1a\u5b83\u4eec\u662f\u7d27\u51d1\u7684\uff0c\u80fd\u591f\u8fdb\u884c\u6982\u7387\u53ef\u89e3\u91ca\u7684\u63a8\u7406\uff0c\u5e76\u4e14\u6709\u52a9\u4e8e\u8de8\u4e0d\u540c\u4f20\u611f\u5668\u7684\u6cdb\u5316\u3002\u6211\u4eec\u901a\u8fc7\u624b\u6301\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u4e0b\u6e38\u4efb\u52a1\u8bc1\u660e\u4e86\u8fd9\u79cd\u8868\u793a\u7684\u6709\u6548\u6027\uff0c\u5728\u7b80\u5316\u4e0b\u6e38\u6a21\u578b\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u4e8e\u57fa\u4e8e\u56fe\u50cf\u7684\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u6211\u4eec\u5728https://www.mmintlab.com/tactile-functasets\u4e0a\u53d1\u5e03\u4ee3\u7801\u3001\u6f14\u793a\u548c\u6570\u636e\u96c6\u3002||\n", "2409.12720": "|**2024-09-18**|[FAST GDRNPP: Improving the Speed of State-of-the-Art 6D Object Pose Estimation](http://arxiv.org/abs/2409.12720)|null|6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u6d89\u53ca\u786e\u5b9a\u573a\u666f\u5185\u7269\u4f53\u76f8\u5bf9\u4e8e\u6240\u9009\u5750\u6807\u7cfb\u7684\u4e09\u7ef4\u5e73\u79fb\u548c\u65cb\u8f6c\u3002\u8fd9\u4e2a\u95ee\u9898\u5728\u5de5\u4e1a\u4efb\u52a1\u7684\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u4e2d\u7279\u522b\u91cd\u8981\uff0c\u4f8b\u5982\u8d28\u91cf\u63a7\u5236\u3001\u6599\u7bb1\u62e3\u9009\u548c\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u5728\u8fd9\u4e9b\u5e94\u7528\u4e2d\uff0c\u901f\u5ea6\u548c\u7cbe\u5ea6\u5bf9\u4e8e\u5b9e\u9645\u90e8\u7f72\u90fd\u81f3\u5173\u91cd\u8981\u3002\u65e0\u8bba\u662f\u7ecf\u5178\u6a21\u578b\u8fd8\u662f\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u5f53\u524d\u7684\u6a21\u578b\u5728\u7cbe\u5ea6\u548c\u5ef6\u8fdf\u4e4b\u95f4\u7684\u6743\u8861\u65b9\u9762\u5e38\u5e38\u9047\u5230\u56f0\u96be\u3002\u6211\u4eec\u7684\u7814\u7a76\u91cd\u70b9\u662f\u5728\u4fdd\u6301\u5176\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bGDRNPP\u7684\u901f\u5ea6\u3002\u6211\u4eec\u91c7\u7528\u4e86\u51e0\u79cd\u6280\u672f\u6765\u51cf\u5c0f\u6a21\u578b\u89c4\u6a21\u5e76\u7f29\u77ed\u63a8\u7406\u65f6\u95f4\u3002\u8fd9\u4e9b\u6280\u672f\u5305\u62ec\u4f7f\u7528\u66f4\u5c0f\u66f4\u5feb\u7684\u9aa8\u5e72\u7f51\u7edc\u3001\u4fee\u526a\u4e0d\u5fc5\u8981\u7684\u53c2\u6570\u4ee5\u53ca\u901a\u8fc7\u84b8\u998f\u5c06\u77e5\u8bc6\u4ece\u5927\u578b\u9ad8\u6027\u80fd\u6a21\u578b\u8fc1\u79fb\u5230\u66f4\u5c0f\u3001\u66f4\u9ad8\u6548\u7684\u5b66\u751f\u6a21\u578b\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u914d\u7f6e\u5728\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u65f6\u95f4\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u4e0e\u6700\u5148\u8fdb\u6280\u672f\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002\u8fd9\u4e00\u8fdb\u6b65\u53ef\u4ee5\u5bfc\u81f4\u5728\u5404\u79cd\u5de5\u4e1a\u573a\u666f\u4e2d\u66f4\u6709\u6548\u548c\u5b9e\u7528\u7684\u5e94\u7528\uff0c\u4ece\u800c\u63d0\u9ad86D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u6574\u4f53\u9002\u7528\u6027\u3002||\n", "2410.05996": "|**2024-10-08**|[AIVIO: Closed-loop, Object-relative Navigation of UAVs with AI-aided Visual Inertial Odometry](http://arxiv.org/abs/2410.05996)|null|\u9762\u5411\u5bf9\u8c61\u7684\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u5bf9\u4e8e\u5404\u79cd\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f8b\u5982\u81ea\u4e3b\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u68c0\u67e5\uff0c\u4f46\u8fd9\u9700\u8981\u4ece\u539f\u59cb\u4f20\u611f\u5668\u6570\u636e\u4e2d\u63d0\u53d6\u6709\u5173\u611f\u5174\u8da3\u5bf9\u8c61\u7684\u8bed\u4e49\u4fe1\u606f\u7684\u80fd\u529b\u3002\u867d\u7136\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60 (DL) \u7684\u65b9\u6cd5\u64c5\u957f\u4ece\u56fe\u50cf\u4e2d\u63a8\u65ad\u8bed\u4e49\u5bf9\u8c61\u4fe1\u606f\uff0c\u4f8b\u5982\u7c7b\u522b\u548c\u76f8\u5bf9 6 \u81ea\u7531\u5ea6 (6-DoF) \u4f4d\u59ff\uff0c\u4f46\u5b83\u4eec\u7684\u8ba1\u7b97\u91cf\u5f88\u5927\uff0c\u56e0\u6b64\u901a\u5e38\u4e0d\u9002\u5408\u6709\u6548\u8f7d\u8377\u53d7\u9650\u7684\u79fb\u52a8\u673a\u5668\u4eba\u3002\u5728\u8fd9\u5c01\u4fe1\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5b9e\u65f6\u65e0\u4eba\u673a (UAV) \u7cfb\u7edf\uff0c\u7528\u4e8e\u5bf9\u8c61\u76f8\u5173\u7684\u95ed\u73af\u5bfc\u822a\uff0c\u8be5\u7cfb\u7edf\u5177\u6709\u7531\u60ef\u6027\u6d4b\u91cf\u5355\u5143 (IMU) \u548c RGB \u76f8\u673a\u6784\u6210\u7684\u6700\u5c0f\u4f20\u611f\u5668\u914d\u7f6e\u3002\u5229\u7528\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u5e76\u9488\u5bf9\u914d\u5957\u677f\u90e8\u7f72\u8fdb\u884c\u4f18\u5316\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5bf9\u8c61\u4f4d\u59ff\u4f30\u8ba1\u5668\uff0c\u5c06\u5bf9\u8c61\u76f8\u5173\u4f4d\u59ff\u6d4b\u91cf\u503c\u4e0e IMU \u6570\u636e\u878d\u5408\u4ee5\u6267\u884c\u5bf9\u8c61\u76f8\u5173\u5b9a\u4f4d\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u5b9e\u9a8c\uff0c\u4ee5\u9a8c\u8bc1\u6211\u4eec\u7684\u7cfb\u7edf\u5728\u7535\u7ebf\u6746\u68c0\u67e5\u7684\u6311\u6218\u6027\u7528\u4f8b\u4e2d\u7684\u6027\u80fd\u3002\u8865\u5145\u89c6\u9891\u4e2d\u5c55\u793a\u4e86\u4e00\u4e2a\u95ed\u73af\u98de\u884c\u7684\u793a\u4f8b\u3002||\n"}, "nerf": {"2408.09130": "|**2024-08-20**|[Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark Images Using Gaussian Splatting](http://arxiv.org/abs/2408.09130)|**[link](https://github.com/yec22/Gaussian-DK)**|3D Gaussian Splatting has recently emerged as a powerful representation that can synthesize remarkable novel views using consistent multi-view images as input. However, we notice that images captured in dark environments where the scenes are not fully illuminated can exhibit considerable brightness variations and multi-view inconsistency, which poses great challenges to 3D Gaussian Splatting and severely degrades its performance. To tackle this problem, we propose Gaussian-DK. Observing that inconsistencies are mainly caused by camera imaging, we represent a consistent radiance field of the physical world using a set of anisotropic 3D Gaussians, and design a camera response module to compensate for multi-view inconsistencies. We also introduce a step-based gradient scaling strategy to constrain Gaussians near the camera, which turn out to be floaters, from splitting and cloning. Experiments on our proposed benchmark dataset demonstrate that Gaussian-DK produces high-quality renderings without ghosting and floater artifacts and significantly outperforms existing methods. Furthermore, we can also synthesize light-up images by controlling exposure levels that clearly show details in shadow areas.||\n", "2407.13520": "|**2024-09-05**|[EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian Splatting](http://arxiv.org/abs/2407.13520)|null|3D deblurring reconstruction techniques have recently seen significant advancements with the development of Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although these techniques can recover relatively clear 3D reconstructions from blurry image inputs, they still face limitations in handling severe blurring and complex camera motion. To address these issues, we propose Event-assisted 3D Deblur Reconstruction with Gaussian Splatting (EaDeblur-GS), which integrates event camera data to enhance the robustness of 3DGS against motion blur. By employing an Adaptive Deviation Estimator (ADE) network to estimate Gaussian center deviations and using novel loss functions, EaDeblur-GS achieves sharp 3D reconstructions in real-time, demonstrating performance comparable to state-of-the-art methods.||\n", "2407.07090": "|**2024-07-10**|[3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes](http://arxiv.org/abs/2407.07090)|null|\u57fa\u4e8e\u7c92\u5b50\u7684\u8f90\u5c04\u573a\u8868\u793a\u6cd5\uff0c\u4f8b\u5982 3D \u9ad8\u65af splatting\uff0c\u5728\u590d\u6742\u573a\u666f\u7684\u91cd\u5efa\u548c\u91cd\u65b0\u6e32\u67d3\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\u3002\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5149\u6805\u5316\u6e32\u67d3\u7c92\u5b50\uff0c\u5c06\u5b83\u4eec\u6295\u5f71\u5230\u5c4f\u5e55\u7a7a\u95f4\u56fe\u5757\u4e2d\uff0c\u4ee5\u4fbf\u6309\u6392\u5e8f\u987a\u5e8f\u8fdb\u884c\u5904\u7406\u3002\u800c\u8fd9\u9879\u5de5\u4f5c\u5219\u8003\u8651\u5bf9\u7c92\u5b50\u8fdb\u884c\u5149\u7ebf\u8ffd\u8e2a\uff0c\u6784\u5efa\u8fb9\u754c\u4f53\u79ef\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u4f7f\u7528\u9ad8\u6027\u80fd GPU \u5149\u7ebf\u8ffd\u8e2a\u786c\u4ef6\u4e3a\u6bcf\u4e2a\u50cf\u7d20\u6295\u5c04\u5149\u7ebf\u3002\u4e3a\u4e86\u6709\u6548\u5904\u7406\u5927\u91cf\u534a\u900f\u660e\u7c92\u5b50\uff0c\u6211\u4eec\u63cf\u8ff0\u4e86\u4e00\u79cd\u4e13\u95e8\u7684\u6e32\u67d3\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4f7f\u7528\u8fb9\u754c\u7f51\u683c\u5c01\u88c5\u7c92\u5b50\uff0c\u4ee5\u5229\u7528\u5feb\u901f\u7684\u5149\u7ebf\u4e09\u89d2\u5f62\u76f8\u4ea4\uff0c\u5e76\u6309\u6df1\u5ea6\u987a\u5e8f\u5bf9\u6210\u6279\u7684\u76f8\u4ea4\u8fdb\u884c\u7740\u8272\u3002\u5149\u7ebf\u8ffd\u8e2a\u7684\u4f18\u52bf\u5728\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\u662f\u4f17\u6240\u5468\u77e5\u7684\uff1a\u5904\u7406\u975e\u76f8\u5e72\u5149\u7ebf\u4ee5\u83b7\u5f97\u9634\u5f71\u548c\u53cd\u5c04\u7b49\u4e8c\u6b21\u7167\u660e\u6548\u679c\u3001\u4ece\u673a\u5668\u4eba\u6280\u672f\u4e2d\u5e38\u89c1\u7684\u9ad8\u5ea6\u626d\u66f2\u7684\u76f8\u673a\u8fdb\u884c\u6e32\u67d3\u3001\u968f\u673a\u91c7\u6837\u5149\u7ebf\u7b49\u7b49\u3002\u4f7f\u7528\u6211\u4eec\u7684\u6e32\u67d3\u5668\uff0c\u4e0e\u5149\u6805\u5316\u76f8\u6bd4\uff0c\u8fd9\u79cd\u7075\u6d3b\u6027\u51e0\u4e4e\u6ca1\u6709\u6210\u672c\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u901f\u5ea6\u548c\u51c6\u786e\u6027\uff0c\u4ee5\u53ca\u5728\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u548c\u89c6\u89c9\u65b9\u9762\u7684\u51e0\u79cd\u5e94\u7528\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u5bf9\u57fa\u672c\u9ad8\u65af\u8868\u793a\u7684\u76f8\u5173\u6539\u8fdb\uff0c\u5305\u62ec\u7b80\u5355\u5730\u4f7f\u7528\u5e7f\u4e49\u6838\u51fd\u6570\uff0c\u8fd9\u53ef\u4ee5\u663e\u7740\u51cf\u5c11\u7c92\u5b50\u547d\u4e2d\u6b21\u6570\u3002||\n", "2407.05254": "|**2024-07-07**|[GaussReg: Fast 3D Registration with Gaussian Splatting](http://arxiv.org/abs/2407.05254)|null|\u70b9\u4e91\u914d\u51c6\u662f\u5927\u89c4\u6a21\u4e09\u7ef4\u573a\u666f\u626b\u63cf\u548c\u91cd\u5efa\u7684\u57fa\u672c\u95ee\u9898\u3002\u5728\u6df1\u5ea6\u5b66\u4e60\u7684\u5e2e\u52a9\u4e0b\uff0c\u914d\u51c6\u65b9\u6cd5\u5f97\u5230\u4e86\u663e\u8457\u53d1\u5c55\uff0c\u5df2\u63a5\u8fd1\u6210\u719f\u9636\u6bb5\u3002\u968f\u7740\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u7684\u5f15\u5165\uff0c\u5b83\u51ed\u501f\u5f3a\u5927\u7684\u89c6\u56fe\u5408\u6210\u80fd\u529b\u6210\u4e3a\u6700\u53d7\u6b22\u8fce\u7684\u4e09\u7ef4\u573a\u666f\u8868\u793a\u65b9\u6cd5\u3002\u5bf9\u4e8eNeRF\u8868\u793a\uff0c\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u4e5f\u9700\u8981\u5bf9\u5176\u8fdb\u884c\u914d\u51c6\u3002\u7136\u800c\uff0c\u8fd9\u65b9\u9762\u8fd8\u7f3a\u4e4f\u6df1\u5165\u7684\u63a2\u7d22\u3002\u8fd9\u662f\u56e0\u4e3a\u5bf9\u5177\u6709\u9690\u5f0f\u8868\u793a\u7684\u4e24\u4e2a\u573a\u666f\u4e4b\u95f4\u7684\u51e0\u4f55\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u5b58\u5728\u56fa\u6709\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u9690\u5f0f\u8868\u793a\u8f6c\u6362\u4e3a\u663e\u5f0f\u8868\u793a\u4ee5\u8fdb\u884c\u8fdb\u4e00\u6b65\u914d\u51c6\u3002\u6700\u8fd1\uff0c\u5f15\u5165\u4e86\u9ad8\u65af splatting\uff08GS\uff09\uff0c\u5b83\u91c7\u7528\u663e\u5f0f\u4e09\u7ef4\u9ad8\u65af\u51fd\u6570\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\u6548\u679c\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6e32\u67d3\u901f\u5ea6\u3002\u7ed9\u5b9a\u4e24\u4e2a\u5177\u6709\u663e\u5f0fGS\u8868\u793a\u7684\u573a\u666f\uff0c\u6211\u4eec\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\u63a2\u7d22\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u4e09\u7ef4\u914d\u51c6\u4efb\u52a1\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86GaussReg\uff0c\u4e00\u4e2a\u5feb\u901f\u4e14\u51c6\u786e\u7684\u7531\u7c97\u5230\u7cbe\u7684\u6846\u67b6\u3002\u7c97\u914d\u51c6\u9636\u6bb5\u9075\u5faa\u73b0\u6709\u7684\u70b9\u4e91\u914d\u51c6\u65b9\u6cd5\uff0c\u5e76\u4f30\u8ba1\u6765\u81eaGS\u7684\u70b9\u4e91\u7684\u7c97\u7565\u5bf9\u9f50\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u5f15\u5bfc\u7684\u7cbe\u914d\u51c6\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4eceGS\u6e32\u67d3\u56fe\u50cf\uff0c\u4e3a\u7cbe\u786e\u5bf9\u9f50\u63d0\u4f9b\u66f4\u8be6\u7ec6\u7684\u51e0\u4f55\u4fe1\u606f\u3002\u4e3a\u4e86\u652f\u6301\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u6211\u4eec\u4ed4\u7ec6\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aScanNet-GSReg\u7684\u573a\u666f\u7ea7\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4eceScanNet\u6570\u636e\u96c6\u4e2d\u83b7\u5f97\u76841379\u4e2a\u573a\u666f\uff0c\u5e76\u6536\u96c6\u4e86\u4e00\u4e2a\u540d\u4e3aGSReg\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684GaussReg\u6bd4HLoc\uff08SuperPoint\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0cSuperGlue\u4f5c\u4e3a\u5339\u914d\u5668\uff09\u5feb44\u500d\uff0c\u5e76\u4e14\u5177\u6709\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002||\n", "2407.03923": "|**2024-07-04**|[CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion Blur Images](http://arxiv.org/abs/2407.03923)|null|\u7531\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a (NeRFs) \u80fd\u591f\u9ad8\u8d28\u91cf\u5730\u6e32\u67d3\u65b0\u89c6\u89d2\uff0c\u56e0\u6b64\u5907\u53d7\u5173\u6ce8\uff0c\u8fd9\u4fc3\u4f7f\u4eba\u4eec\u5bf9\u5176\u5728\u5404\u79cd\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u7814\u7a76\u3002\u5176\u4e2d\u4e00\u4e2a\u5173\u952e\u6311\u6218\u662f\u76f8\u673a\u5728\u66dd\u5149\u65f6\u95f4\u5185\u79fb\u52a8\u9020\u6210\u7684\u76f8\u673a\u8fd0\u52a8\u6a21\u7cca\uff0c\u8fd9\u963b\u788d\u4e86\u7cbe\u786e\u7684\u4e09\u7ef4\u573a\u666f\u91cd\u5efa\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8fde\u7eed\u521a\u4f53\u8fd0\u52a8\u611f\u77e5\u9ad8\u65af\u6563\u5c04 (CRiM-GS)\uff0c\u4ee5\u5b9e\u65f6\u6e32\u67d3\u901f\u5ea6\u4ece\u6a21\u7cca\u56fe\u50cf\u4e2d\u91cd\u5efa\u7cbe\u786e\u7684\u4e09\u7ef4\u573a\u666f\u3002\u8003\u8651\u5230\u5b9e\u9645\u7684\u76f8\u673a\u8fd0\u52a8\u6a21\u7cca\u8fc7\u7a0b\u5305\u542b\u590d\u6742\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u6211\u4eec\u57fa\u4e8e\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b (ODEs) \u9884\u6d4b\u76f8\u673a\u7684\u8fde\u7eed\u8fd0\u52a8\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5229\u7528\u521a\u4f53\u53d8\u6362\u6765\u6a21\u62df\u76f8\u673a\u8fd0\u52a8\u5e76\u8fdb\u884c\u9002\u5f53\u7684\u6b63\u5219\u5316\uff0c\u4ee5\u4fdd\u6301\u5bf9\u8c61\u7684\u5f62\u72b6\u548c\u5927\u5c0f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728\\textit{SE(3)} \u573a\u4e2d\u5f15\u5165\u8fde\u7eed\u53ef\u53d8\u5f62\u4e09\u7ef4\u53d8\u6362\uff0c\u901a\u8fc7\u786e\u4fdd\u66f4\u9ad8\u7684\u81ea\u7531\u5ea6\u4f7f\u521a\u4f53\u53d8\u6362\u9002\u5e94\u73b0\u5b9e\u95ee\u9898\u3002\u901a\u8fc7\u91cd\u65b0\u5ba1\u89c6\u57fa\u672c\u76f8\u673a\u7406\u8bba\u5e76\u91c7\u7528\u5148\u8fdb\u7684\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6280\u672f\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u5bf9\u8fde\u7eed\u76f8\u673a\u8f68\u8ff9\u7684\u7cbe\u786e\u5efa\u6a21\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9a\u91cf\u548c\u5b9a\u6027\u5730\u8bc1\u660e\u4e86\u5176\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002||\n", "2406.18214": "|**2024-07-29**|[Trimming the Fat: Efficient Compression of 3D Gaussian Splats through Pruning](http://arxiv.org/abs/2406.18214)|**[link](https://github.com/salmanali96/trimming-the-fat)**|\u8fd1\u5e74\u6765\uff0c\u7531\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\u548c\u6700\u8fd1\u51fa\u73b0\u76843D\u9ad8\u65af\u6837\u6761\u66f2\u7ebf(3DGS)\u6a21\u578b\u63d0\u4f9b\u4e86\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u80fd\u529b\uff0c3D\u6a21\u578b\u7684\u4f7f\u7528\u5f97\u5230\u4e86\u63a8\u5e7f\u3002\u540e\u8005\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u80fd\u591f\u8f7b\u677e\u5730\u5feb\u901f\u6536\u655b\u5e76\u63d0\u4f9b\u5e7f\u6cdb\u7684\u53ef\u7f16\u8f91\u6027\uff0c\u56e0\u6b64\u5177\u6709\u663e\u8457\u7684\u4f18\u52bf\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5173\u4e8e\u8fd9\u4e9b\u6a21\u578b\u53ef\u6269\u5c55\u6027\u7684\u6587\u732e\u4ecd\u5904\u4e8e\u8d77\u6b65\u9636\u6bb5\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u4e3a\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\u91c7\u53d6\u4e86\u4e00\u4e9b\u521d\u6b65\u63aa\u65bd\uff0c\u5c55\u793a\u4e86\u4e00\u79cd\u80fd\u591f\u5b9e\u73b0\u6b64\u7c7b\u6a21\u578b\u5185\u5b58\u548c\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u7684\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u201cTrimming the fat\u201d\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u8fed\u4ee3\u5f0f\u540e\u526a\u679d\u6280\u672f\uff0c\u7528\u4e8e\u6d88\u9664\u6a21\u578b\u4e2d\u7f16\u7801\u7684\u5197\u4f59\u4fe1\u606f\u3002\u6211\u4eec\u5728\u5e7f\u6cdb\u8ba4\u53ef\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8\u57fa\u7ebf\u6027\u80fd\u7684\u540c\u65f6\uff0c\u6700\u591a\u53ef\u4ee5\u79fb\u966475%\u7684\u9ad8\u65af\u51fd\u6570\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5927\u7ea650\u500d\u7684\u538b\u7f29\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u4f3c\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u5c06\u8ba1\u7b97\u901f\u5ea6\u63d0\u9ad8\u5230600 FPS\u3002||\n", "2406.15149": "|**2024-06-21**|[Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks](http://arxiv.org/abs/2406.15149)|null|\u6a21\u62df\u5668\u662f\u81ea\u52a8\u673a\u5668\u4eba\u5b66\u4e60\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ef\u4ee5\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u6570\u636e\u751f\u6210\u3001\u7075\u6d3b\u7684\u8bbe\u8ba1\u548c\u8f68\u8ff9\u4f18\u5316\u3002\u7136\u800c\uff0c\u5c06\u4ece\u6a21\u62df\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u7684\u884c\u4e3a\u8fc1\u79fb\u5230\u73b0\u5b9e\u4e16\u754c\u4e2d\u88ab\u8bc1\u660e\u662f\u56f0\u96be\u7684\uff0c\u901a\u5e38\u9700\u8981\u901a\u8fc7\u8ba1\u7b97\u91cf\u5927\u7684\u57df\u968f\u673a\u5316\u65b9\u6cd5\u6216\u8fdb\u4e00\u6b65\u7684\u6a21\u578b\u5fae\u8c03\u6765\u7f13\u89e3\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\u6765\u63d0\u9ad8\u6a21\u62df\u5230\u771f\u5b9e\u89c6\u89c9\u56db\u65cb\u7ffc\u5bfc\u822a\u4efb\u52a1\u4e2d\u5bf9\u5206\u5e03\u53d8\u5316\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u5c06\u9ad8\u65af splatting \u4e0e\u56db\u65cb\u7ffc\u98de\u884c\u52a8\u529b\u5b66\u76f8\u7ed3\u5408\u6765\u6784\u5efa\u6a21\u62df\u5668\uff0c\u7136\u540e\u4f7f\u7528 Liquid \u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u9c81\u68d2\u7684\u5bfc\u822a\u7b56\u7565\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u6211\u4eec\u83b7\u5f97\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u6a21\u4eff\u5b66\u4e60\u534f\u8bae\uff0c\u5b83\u7ed3\u5408\u4e86 3D \u9ad8\u65af splatting \u8f90\u5c04\u573a\u6e32\u67d3\u7684\u8fdb\u6b65\u3001\u4e13\u5bb6\u6f14\u793a\u8bad\u7ec3\u6570\u636e\u7684\u5de7\u5999\u7f16\u7a0b\u4ee5\u53ca Liquid \u7f51\u7edc\u7684\u4efb\u52a1\u7406\u89e3\u80fd\u529b\u3002\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9a\u91cf\u98de\u884c\u6d4b\u8bd5\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5728\u5355\u4e2a\u6a21\u62df\u573a\u666f\u4e2d\u5b66\u4e60\u5230\u7684\u5bfc\u822a\u6280\u80fd\u53ef\u4ee5\u76f4\u63a5\u7a33\u5065\u5730\u8fc1\u79fb\u5230\u73b0\u5b9e\u4e16\u754c\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5c55\u793a\u4e86\u5728\u5267\u70c8\u7684\u5206\u5e03\u548c\u7269\u7406\u73af\u5883\u53d8\u5316\u4e0b\uff0c\u5728\u8bad\u7ec3\u73af\u5883\u4e4b\u5916\u4fdd\u6301\u6027\u80fd\u7684\u80fd\u529b\u3002\u6211\u4eec\u5b66\u4e60\u7684 Liquid \u7b56\u7565\uff0c\u4ec5\u5728\u4ece\u771f\u5b9e\u611f\u5ba4\u5185\u6a21\u62df\u98de\u884c\u4e2d\u63d0\u53d6\u7684\u5355\u4e2a\u76ee\u6807\u64cd\u4f5c\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u53ef\u4ee5\u6cdb\u5316\u5230\u6237\u5916\u771f\u5b9e\u786c\u4ef6\u5e73\u53f0\u4e0a\u7684\u591a\u6b65\u8fdc\u8db3\u3002||\n", "2406.10373": "|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|\u5728\u975e\u7ed3\u6784\u5316\u7684\u65c5\u6e38\u73af\u5883\u4e2d\u62cd\u6444\u7684\u7167\u7247\u7ecf\u5e38\u8868\u73b0\u51fa\u591a\u53d8\u7684\u5916\u89c2\u548c\u77ed\u6682\u7684\u906e\u6321\uff0c\u8fd9\u5bf9\u51c6\u786e\u7684\u573a\u666f\u91cd\u5efa\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u5e76\u5728\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u5bfc\u81f4\u4e86\u4f2a\u5f71\u3002\u867d\u7136\u5148\u524d\u7684\u65b9\u6cd5\u5df2\u7ecf\u5c06\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u4e0e\u5176\u4ed6\u53ef\u5b66\u4e60\u6a21\u5757\u76f8\u7ed3\u5408\u6765\u5904\u7406\u52a8\u6001\u5916\u89c2\u5e76\u6d88\u9664\u77ac\u6001\u5bf9\u8c61\uff0c\u4f46\u5176\u5927\u91cf\u7684\u8bad\u7ec3\u9700\u6c42\u548c\u7f13\u6162\u7684\u6e32\u67d3\u901f\u5ea6\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u6700\u8fd1\uff0c3D \u9ad8\u65af splatting (3DGS) \u5df2\u6210\u4e3a NeRF \u7684\u4e00\u79cd\u6709\u524d\u9014\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5b83\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u4ee5\u53ca\u66f4\u597d\u7684\u6e32\u67d3\u8d28\u91cf\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 Wild-GS\uff0c\u8fd9\u662f\u4e00\u79cd\u9488\u5bf9\u4e0d\u53d7\u7ea6\u675f\u7684\u7167\u7247\u96c6\u4f18\u5316\u7684 3DGS \u521b\u65b0\u6539\u7f16\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5176\u6548\u7387\u4f18\u52bf\u3002Wild-GS \u901a\u8fc7\u6bcf\u5f20\u56fe\u50cf\u7684\u56fa\u6709\u6750\u8d28\u5c5e\u6027\u3001\u5168\u5c40\u7167\u660e\u548c\u76f8\u673a\u5c5e\u6027\u4ee5\u53ca\u9010\u70b9\u53cd\u5c04\u7387\u7684\u5c40\u90e8\u53d8\u5316\u6765\u786e\u5b9a\u6bcf\u4e2a 3D \u9ad8\u65af\u7684\u5916\u89c2\u3002\u4e0e\u5148\u524d\u5728\u56fe\u50cf\u7a7a\u95f4\u4e2d\u5bf9\u53c2\u8003\u7279\u5f81\u8fdb\u884c\u5efa\u6a21\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cWild-GS \u901a\u8fc7\u5bf9\u4ece\u53c2\u8003\u56fe\u50cf\u4e2d\u63d0\u53d6\u7684\u4e09\u5e73\u9762\u8fdb\u884c\u91c7\u6837\uff0c\u5c06\u50cf\u7d20\u5916\u89c2\u7279\u5f81\u660e\u786e\u5730\u4e0e\u76f8\u5e94\u7684\u5c40\u90e8\u9ad8\u65af\u5bf9\u9f50\u3002\u8fd9\u79cd\u65b0\u9896\u7684\u8bbe\u8ba1\u6709\u6548\u5730\u5c06\u53c2\u8003\u89c6\u56fe\u7684\u9ad8\u9891\u7ec6\u8282\u5916\u89c2\u8f6c\u79fb\u5230 3D \u7a7a\u95f4\uff0c\u5e76\u663e\u7740\u52a0\u5feb\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c2D \u53ef\u89c1\u6027\u56fe\u548c\u6df1\u5ea6\u6b63\u5219\u5316\u5206\u522b\u7528\u4e8e\u51cf\u8f7b\u77ac\u6001\u6548\u5e94\u548c\u7ea6\u675f\u51e0\u4f55\u5f62\u72b6\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cWild-GS \u5728\u6240\u6709\u73b0\u6709\u6280\u672f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6e32\u67d3\u6027\u80fd\u4ee5\u53ca\u6700\u9ad8\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002||\n", "2406.04253": "|**2024-06-06**|[A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation](http://arxiv.org/abs/2406.04253)|null|3D modeling has long been an important area in computer vision and computer graphics. Recently, thanks to the breakthroughs in neural representations and generative models, we witnessed a rapid development of 3D modeling. 3D human modeling, lying at the core of many real-world applications, such as gaming and animation, has attracted significant attention. Over the past few years, a large body of work on creating 3D human avatars has been introduced, forming a new and abundant knowledge base for 3D human modeling. The scale of the literature makes it difficult for individuals to keep track of all the works. This survey aims to provide a comprehensive overview of these emerging techniques for 3D human avatar modeling, from both reconstruction and generation perspectives. Firstly, we review representative methods for 3D human reconstruction, including methods based on pixel-aligned implicit function, neural radiance field, and 3D Gaussian Splatting, etc. We then summarize representative methods for 3D human generation, especially those using large language models like CLIP, diffusion models, and various 3D representations, which demonstrate state-of-the-art performance. Finally, we discuss our reflection on existing methods and open challenges for 3D human avatar modeling, shedding light on future research.||\n", "2406.02720": "|**2024-06-13**|[3D-HGS: 3D Half-Gaussian Splatting](http://arxiv.org/abs/2406.02720)|**[link](https://github.com/lihaolin88/3d-half-gaussian-splatting)**|\u7167\u7247\u7ea7\u903c\u771f\u7684\u4e09\u7ef4\u91cd\u5efa\u662f\u4e09\u7ef4\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\u3002\u7531\u4e8e\u6700\u8fd1\u795e\u7ecf\u6e32\u67d3\u6280\u672f\u7684\u51fa\u73b0\uff0c\u8be5\u9886\u57df\u53d6\u5f97\u4e86\u76f8\u5f53\u5927\u7684\u8fdb\u6b65\u3002\u8fd9\u4e9b\u6280\u672f\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5b66\u4e60\u4e09\u7ef4\u573a\u666f\u7684\u4f53\u79ef\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u6e32\u67d3\u5f97\u5230\u7684\u635f\u5931\u51fd\u6570\u6765\u7ec6\u5316\u8fd9\u4e9b\u8868\u793a\u3002\u5176\u4e2d\uff0c\u4e09\u7ef4\u9ad8\u65af\u6563\u5c04\uff083D-GS\uff09\u5df2\u6210\u4e3a\u4e00\u79cd\u91cd\u8981\u7684\u65b9\u6cd5\uff0c\u5176\u6027\u80fd\u8d85\u8fc7\u4e86\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRFs\uff09\u30023D-GS\u4f7f\u7528\u53c2\u6570\u5316\u7684\u4e09\u7ef4\u9ad8\u65af\u51fd\u6570\u6765\u5efa\u6a21\u7a7a\u95f4\u4f4d\u7f6e\u548c\u989c\u8272\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u56fe\u5757\u7684\u5feb\u901f\u6e32\u67d3\u6280\u672f\u3002\u5c3d\u7ba1\u5176\u6e32\u67d3\u6027\u80fd\u548c\u901f\u5ea6\u90fd\u5f88\u51fa\u8272\uff0c\u4f46\u4f7f\u7528\u4e09\u7ef4\u9ad8\u65af\u6838\u51fd\u6570\u5728\u51c6\u786e\u8868\u793a\u4e0d\u8fde\u7eed\u51fd\u6570\u65b9\u9762\u5b58\u5728\u56fa\u6709\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u5f62\u72b6\u4e0d\u8fde\u7eed\u7684\u8fb9\u7f18\u548c\u89d2\u843d\uff0c\u4ee5\u53ca\u5728\u989c\u8272\u4e0d\u8fde\u7eed\u7684\u4e0d\u540c\u7eb9\u7406\u4e4b\u95f4\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5efa\u8bae\u91c7\u7528\u4e09\u7ef4\u534a\u9ad8\u65af\uff083D-HGS\uff09\u6838\u51fd\u6570\uff0c\u5b83\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u6838\u51fd\u6570\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u4eec\u80fd\u591f\u63d0\u9ad8\u5f53\u524d\u4e0e3D-GS\u76f8\u5173\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u5728\u4e0d\u5f71\u54cd\u6e32\u67d3\u901f\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6e32\u67d3\u6027\u80fd\u3002||\n", "2409.03213": "|**2024-09-05**|[Optimizing 3D Gaussian Splatting for Sparse Viewpoint Scene Reconstruction](http://arxiv.org/abs/2409.03213)|null|\u4e09\u7ef4\u9ad8\u65af splatting (3DGS) \u5df2\u6210\u4e3a\u4e00\u79cd\u5f88\u6709\u524d\u666f\u7684\u4e09\u7ef4\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u4e0e\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u76f8\u6bd4\uff0c\u5b83\u53ef\u4ee5\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002\u7136\u800c\uff0c3DGS \u5bb9\u6613\u51fa\u73b0\u9ad8\u9891\u4f2a\u5f71\uff0c\u5e76\u4e14\u5728\u7a00\u758f\u89c6\u70b9\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5e94\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86 SVS-GS\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u7a00\u758f\u89c6\u70b9\u573a\u666f\u91cd\u5efa\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u96c6\u6210\u4e86\u4e09\u7ef4\u9ad8\u65af\u5e73\u6ed1\u6ee4\u6ce2\u5668\u6765\u6291\u5236\u4f2a\u5f71\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u7ed3\u5408\u4e86\u6df1\u5ea6\u68af\u5ea6\u5256\u9762\u5148\u9a8c (DGPP) \u635f\u5931\u548c\u52a8\u6001\u6df1\u5ea6\u63a9\u7801\u6765\u9510\u5316\u8fb9\u7f18\uff0c\u5e76\u7ed3\u5408\u4e86\u5206\u6570\u84b8\u998f\u91c7\u6837 (SDS) \u635f\u5931\u7684\u4e8c\u7ef4\u6269\u6563\u6765\u589e\u5f3a\u65b0\u89c6\u56fe\u5408\u6210\u4e2d\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u5728 MipNeRF-360 \u548c SeaThru-NeRF \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cSVS-GS \u663e\u7740\u6539\u5584\u4e86\u7a00\u758f\u89c6\u70b9\u4e0b\u7684\u4e09\u7ef4\u91cd\u5efa\uff0c\u4e3a\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u4e2d\u7684\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5065\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002||\n", "2409.06407": "|**2024-09-10**|[Sources of Uncertainty in 3D Scene Reconstruction](http://arxiv.org/abs/2409.06407)|**[link](https://github.com/aaltoml/uncertainty-nerf-gs)**|\u4e09\u7ef4\u573a\u666f\u91cd\u5efa\u8fc7\u7a0b\u4f1a\u53d7\u5230\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u4f17\u591a\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u7684\u5f71\u54cd\u3002\u867d\u7136\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u548c\u4e09\u7ef4\u9ad8\u65af\u6563\u5c04 (GS) \u53ef\u4ee5\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6e32\u67d3\uff0c\u4f46\u5b83\u4eec\u7f3a\u4e4f\u5185\u7f6e\u673a\u5236\u6765\u76f4\u63a5\u89e3\u51b3\u6216\u91cf\u5316\u7531\u566a\u58f0\u3001\u906e\u6321\u3001\u6df7\u6742\u5f02\u5e38\u503c\u548c\u4e0d\u7cbe\u786e\u7684\u76f8\u673a\u59ff\u6001\u8f93\u5165\u5f15\u8d77\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u5206\u7c7b\u6cd5\uff0c\u5bf9\u8fd9\u4e9b\u65b9\u6cd5\u4e2d\u56fa\u6709\u7684\u4e0d\u540c\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u8fdb\u884c\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4f7f\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6280\u672f\u6269\u5c55\u4e86\u57fa\u4e8e NeRF \u548c GS \u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u5b66\u4e60\u4e0d\u786e\u5b9a\u6027\u8f93\u51fa\u548c\u96c6\u6210\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\u6765\u8bc4\u4f30\u5b83\u4eec\u6355\u6349\u91cd\u5efa\u654f\u611f\u6027\u7684\u80fd\u529b\u3002\u6211\u4eec\u7684\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u8bbe\u8ba1\u57fa\u4e8e NeRF/GS \u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\u65f6\uff0c\u9700\u8981\u89e3\u51b3\u5404\u79cd\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u7684\u9700\u6c42\u3002||\n", "2409.17345": "|**2024-09-25**|[SeaSplat: Representing Underwater Scenes with 3D Gaussian Splatting and a Physically Grounded Image Formation Model](http://arxiv.org/abs/2409.17345)|null|\u6211\u4eec\u4ecb\u7ecdSeaSplat\uff0c\u8fd9\u662f\u4e00\u79cd\u5229\u75283D\u8f90\u5c04\u573a\u6700\u65b0\u8fdb\u5c55\u5b9e\u73b0\u6c34\u4e0b\u573a\u666f\u5b9e\u65f6\u6e32\u67d3\u7684\u65b9\u6cd5\u3002\u6c34\u4e0b\u573a\u666f\u662f\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u89c9\u73af\u5883\uff0c\u56e0\u4e3a\u901a\u8fc7\u6c34\u7b49\u4ecb\u8d28\u8fdb\u884c\u6e32\u67d3\u4f1a\u5728\u56fe\u50cf\u6355\u6349\u4e2d\u5f15\u5165\u8ddd\u79bb\u548c\u989c\u8272\u76f8\u5173\u7684\u5f71\u54cd\u3002\u6211\u4eec\u4f7f\u7528\u7269\u7406\u57fa\u7840\u7684\u6c34\u4e0b\u6210\u50cf\u6a21\u578b\u6765\u7ea6\u675f3D\u9ad8\u65af splatting\uff083DGS\uff09\uff0c\u8fd9\u662f\u8f90\u5c04\u573a\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u5b8c\u65743D\u573a\u666f\u7684\u5feb\u901f\u8bad\u7ec3\u548c\u5b9e\u65f6\u6e32\u67d3\u3002\u5c06SeaSplat\u5e94\u7528\u4e8eSeaThru-NeRF\u6570\u636e\u96c6\u4e2d\u7684\u771f\u5b9e\u573a\u666f\u3001\u7531\u7f8e\u5c5e\u7ef4\u5c14\u4eac\u7fa4\u5c9b\u7684\u6c34\u4e0b\u822a\u884c\u5668\u6536\u96c6\u7684\u573a\u666f\u4ee5\u53ca\u6a21\u62df\u9000\u5316\u7684\u771f\u5b9e\u573a\u666f\uff0c\u6211\u4eec\u4e0d\u4ec5\u770b\u5230\u5728\u6e32\u67d3\u5b58\u5728\u4ecb\u8d28\u7684\u573a\u666f\u7684\u65b0\u89c6\u70b9\u65f6\uff0c\u5b9a\u91cf\u6027\u80fd\u6709\u6240\u63d0\u9ad8\uff0c\u800c\u4e14\u8fd8\u80fd\u591f\u6062\u590d\u573a\u666f\u7684\u5e95\u5c42\u771f\u5b9e\u989c\u8272\uff0c\u5e76\u5c06\u6e32\u67d3\u6062\u590d\u5230\u6ca1\u6709\u4e2d\u95f4\u4ecb\u8d28\u7684\u72b6\u6001\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u6c34\u4e0b\u6210\u50cf\u6a21\u578b\u6709\u52a9\u4e8e\u5b66\u4e60\u573a\u666f\u7ed3\u6784\u548c\u66f4\u597d\u7684\u6df1\u5ea6\u56fe\uff0c\u5e76\u8868\u660e\u6211\u4eec\u7684\u6539\u8fdb\u4fdd\u6301\u4e86\u5229\u75283D\u9ad8\u65af\u8868\u793a\u5e26\u6765\u7684\u663e\u8457\u8ba1\u7b97\u6539\u8fdb\u3002||\n", "2409.16915": "|**2024-09-25**|[Let's Make a Splan: Risk-Aware Trajectory Optimization in a Normalized Gaussian Splat](http://arxiv.org/abs/2409.16915)|null|\u795e\u7ecf\u8f90\u5c04\u573a\u548c\u9ad8\u65af splatting \u901a\u8fc7\u5b9e\u73b0\u590d\u6742\u573a\u666f\u7684\u903c\u771f\u8868\u793a\uff0c\u6539\u53d8\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5b83\u4eec\u5728\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u4efb\u52a1\uff08\u5982\u8f68\u8ff9\u4f18\u5316\uff09\u4e2d\u7684\u5e94\u7528\u4ecd\u7136\u6709\u9650\u3002\u9020\u6210\u8fd9\u79cd\u6709\u9650\u6210\u529f\u6709\u4e24\u4e2a\u5173\u952e\u56e0\u7d20\u3002\u9996\u5148\uff0c\u5728\u8f90\u5c04\u6a21\u578b\u4e2d\u63a8\u7406\u78b0\u649e\u5177\u6709\u6311\u6218\u6027\u3002\u5176\u6b21\uff0c\u5f88\u96be\u8db3\u591f\u5feb\u5730\u6267\u884c\u8f90\u5c04\u6a21\u578b\u7684\u63a8\u7406\u4ee5\u8fdb\u884c\u5b9e\u65f6\u8f68\u8ff9\u5408\u6210\u3002\u672c\u6587\u63d0\u51fa\u4e86 SPLANNING\uff0c\u4e00\u79cd\u5728\u9ad8\u65af splatting \u6a21\u578b\u4e2d\u8fd0\u884c\u7684\u98ce\u9669\u611f\u77e5\u8f68\u8ff9\u4f18\u5316\u5668\uff0c\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002\u672c\u6587\u9996\u5148\u63a8\u5bfc\u51fa\u4e86\u4e00\u79cd\u4e25\u683c\u4e0a\u9650\u673a\u5668\u4eba\u4e0e\u8f90\u5c04\u573a\u4e4b\u95f4\u78b0\u649e\u6982\u7387\u7684\u65b9\u6cd5\u3002\u5176\u6b21\uff0c\u672c\u6587\u4ecb\u7ecd\u4e86\u9ad8\u65af splatting \u7684\u5f52\u4e00\u5316\u91cd\u6784\uff0c\u4ece\u800c\u80fd\u591f\u5728\u9ad8\u65af splat \u4e2d\u9ad8\u6548\u8ba1\u7b97\u78b0\u649e\u8fb9\u754c\u3002\u7b2c\u4e09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u907f\u514d\u4e0e\u9ad8\u65af splat \u8868\u793a\u7684\u573a\u666f\u53d1\u751f\u78b0\u649e\u7684\u540c\u65f6\u4f18\u5316\u8f68\u8ff9\u7684\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\uff0cSPLANNING \u5728\u9ad8\u5ea6\u6742\u4e71\u7684\u73af\u5883\u4e2d\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u4e5f\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u673a\u68b0\u624b\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002\u9879\u76ee\u9875\u9762\u4f4d\u4e8e https://roahmlab.github.io/splanning\u3002||\n", "2409.14316": "|**2024-09-22**|[MVPGS: Excavating Multi-view Priors for Gaussian Splatting from Sparse Input Views](http://arxiv.org/abs/2409.14316)|null|\u8fd1\u5e74\u6765\uff0c\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u7684\u8fdb\u6b65\u4fc3\u8fdb\u4e86\u5c11\u6837\u672c\u65b0\u89c6\u89d2\u5408\u6210\uff08NVS\uff09\u7684\u53d1\u5c55\uff0c\u8fd9\u662f\u4e09\u7ef4\u89c6\u89c9\u5e94\u7528\u4e2d\u7684\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u5c3d\u7ba1\u4eba\u4eec\u505a\u4e86\u5f88\u591a\u5c1d\u8bd5\u6765\u51cf\u5c11NeRF\u4e2d\u5bc6\u96c6\u7684\u8f93\u5165\u9700\u6c42\uff0c\u4f46\u5b83\u4ecd\u7136\u9762\u4e34\u7740\u8bad\u7ec3\u548c\u6e32\u67d3\u8fc7\u7a0b\u8017\u65f6\u7684\u96be\u9898\u3002\u6700\u8fd1\uff0c\u4e09\u7ef4\u9ad8\u65af\u6563\u5c04\uff083DGS\uff09\u901a\u8fc7\u4e00\u79cd\u663e\u5f0f\u7684\u57fa\u4e8e\u70b9\u7684\u8868\u793a\u5b9e\u73b0\u4e86\u5b9e\u65f6\u9ad8\u8d28\u91cf\u6e32\u67d3\u3002\u7136\u800c\uff0c\u4e0eNeRF\u7c7b\u4f3c\uff0c\u7531\u4e8e\u7f3a\u4e4f\u7ea6\u675f\uff0c\u5b83\u5f80\u5f80\u4f1a\u5bf9\u8bad\u7ec3\u89c6\u56fe\u8fc7\u62df\u5408\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86MVPGS\uff0c\u4e00\u79cd\u57fa\u4e8e\u4e09\u7ef4\u9ad8\u65af\u6563\u5c04\u6316\u6398\u591a\u89c6\u56fe\u5148\u9a8c\u7684\u5c11\u6837\u672cNVS\u65b9\u6cd5\u3002\u6211\u4eec\u5229\u7528\u6700\u8fd1\u57fa\u4e8e\u5b66\u4e60\u7684\u591a\u89c6\u56fe\u7acb\u4f53\u89c6\u89c9\uff08MVS\uff09\u6765\u63d0\u9ad83DGS\u51e0\u4f55\u521d\u59cb\u5316\u7684\u8d28\u91cf\u3002\u4e3a\u4e86\u51cf\u8f7b\u8fc7\u62df\u5408\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u524d\u5411\u626d\u66f2\u65b9\u6cd5\uff0c\u7528\u4e8e\u6839\u636e\u8ba1\u7b97\u51fa\u7684\u51e0\u4f55\u5f62\u72b6\u5bf9\u573a\u666f\u8fdb\u884c\u989d\u5916\u7684\u5916\u89c2\u7ea6\u675f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u89c6\u56fe\u4e00\u81f4\u6027\u51e0\u4f55\u7ea6\u675f\u6765\u7ea6\u675f\u9ad8\u65af\u53c2\u6570\uff0c\u4ee5\u4fc3\u8fdb\u6b63\u786e\u7684\u4f18\u5316\u6536\u655b\uff0c\u5e76\u5229\u7528\u5355\u76ee\u6df1\u5ea6\u6b63\u5219\u5316\u4f5c\u4e3a\u8865\u507f\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9e\u65f6\u6e32\u67d3\u901f\u5ea6\u4e0b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://zezeaaa.github.io/projects/MVPGS/||\n", "2409.20291": "|**2024-09-30**|[RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for Robotic Manipulation Learning](http://arxiv.org/abs/2409.20291)|null|Sim-to-Real \u6307\u7684\u662f\u5c06\u6a21\u62df\u5b66\u4e60\u5230\u7684\u7b56\u7565\u8fc1\u79fb\u5230\u73b0\u5b9e\u4e16\u754c\u7684\u8fc7\u7a0b\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u73b0\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u6700\u8fd1\u7684 Sim2real \u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5927\u91cf\u7684\u589e\u5f3a\u6570\u636e\uff0c\u8981\u4e48\u4f9d\u8d56\u5927\u578b\u5b66\u4e60\u6a21\u578b\uff0c\u8fd9\u5bf9\u4e8e\u7279\u5b9a\u4efb\u52a1\u6765\u8bf4\u6548\u7387\u4f4e\u4e0b\u3002\u8fd1\u5e74\u6765\uff0c\u57fa\u4e8e\u8f90\u5c04\u573a\u7684\u91cd\u5efa\u65b9\u6cd5\uff0c\u5c24\u5176\u662f 3D Gaussian Splatting \u7684\u51fa\u73b0\uff0c\u4f7f\u5f97\u91cd\u73b0\u771f\u5b9e\u7684\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u6210\u4e3a\u53ef\u80fd\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 real-to-sim-to-real \u5f3a\u5316\u5b66\u4e60\u6846\u67b6 RL-GSBridge\uff0c\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u57fa\u4e8e\u7f51\u683c\u7684 3D Gaussian Splatting \u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u96f6\u6837\u672c sim-to-real \u8fc1\u79fb\u3002\u6211\u4eec\u901a\u8fc7\u4f7f\u7528\u8f6f\u7ed1\u5b9a\u7ea6\u675f\u6539\u8fdb\u4e86\u57fa\u4e8e\u7f51\u683c\u7684 3D GS \u5efa\u6a21\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u7f51\u683c\u6a21\u578b\u7684\u6e32\u67d3\u8d28\u91cf\u3002\u7136\u540e\uff0c\u6211\u4eec\u91c7\u7528 GS \u7f16\u8f91\u65b9\u6cd5\u6765\u540c\u6b65\u6e32\u67d3\u548c\u7269\u7406\u6a21\u62df\u5668\uff0c\u66f4\u51c6\u786e\u5730\u53cd\u6620\u7269\u7406\u673a\u5668\u4eba\u7684\u4ea4\u4e92\u3002\u901a\u8fc7\u4e00\u7cfb\u5217 sim-to-real \u673a\u68b0\u81c2\u5b9e\u9a8c\uff0c\u5305\u62ec\u6293\u53d6\u548c\u62fe\u653e\u4efb\u52a1\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 RL-GSBridge \u5728 sim-to-real \u8fc1\u79fb\u671f\u95f4\u5728\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u5b8c\u6210\u4e2d\u4fdd\u6301\u4e86\u4ee4\u4eba\u6ee1\u610f\u7684\u6210\u529f\u7387\u3002\u6b64\u5916\uff0c\u4e00\u7cfb\u5217\u6e32\u67d3\u6307\u6807\u548c\u53ef\u89c6\u5316\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u57fa\u4e8e\u7f51\u683c\u7684 3D Gaussian \u51cf\u5c11\u4e86\u975e\u7ed3\u6784\u5316\u5bf9\u8c61\u4e2d\u7684\u4f2a\u5f71\uff0c\u5c55\u793a\u4e86\u66f4\u903c\u771f\u7684\u6e32\u67d3\u6027\u80fd\u3002||\n", "2407.11394": "|**2024-10-02**|[DreamCatalyst: Fast and High-Quality 3D Editing via Controlling Editability and Identity Preservation](http://arxiv.org/abs/2407.11394)|**[link](https://github.com/kaist-cvml/dreamcatalyst)**|\u5206\u6570\u84b8\u998f\u91c7\u6837\uff08SDS\uff09\u5df2\u6210\u4e3a\u6587\u672c\u9a71\u52a83D\u7f16\u8f91\u4efb\u52a1\u4e2d\u4e00\u79cd\u6709\u6548\u7684\u6846\u67b6\uff0c\u5b83\u5229\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c3D\u4e00\u81f4\u6027\u7f16\u8f91\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u57fa\u4e8eSDS\u76843D\u7f16\u8f91\u65b9\u6cd5\u5b58\u5728\u8bad\u7ec3\u65f6\u95f4\u957f\u3001\u751f\u6210\u7ed3\u679c\u8d28\u91cf\u4f4e\u7684\u95ee\u9898\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u9020\u6210\u8fd9\u79cd\u6027\u80fd\u4e0b\u964d\u7684\u6839\u672c\u539f\u56e0\u662f\u5b83\u4eec\u4e0e\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u52a8\u529b\u5b66\u76f8\u51b2\u7a81\u3002\u89e3\u51b3\u8fd9\u79cd\u51b2\u7a81\u4f7f\u6211\u4eec\u80fd\u591f\u5c06SDS\u89c6\u4e3a\u901a\u8fc7\u4ece\u6570\u636e\u7a7a\u95f4\u91c7\u6837\u8fdb\u884c3D\u7f16\u8f91\u7684\u6269\u6563\u9006\u8fc7\u7a0b\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u73b0\u6709\u65b9\u6cd5\u7b80\u5355\u5730\u4f7f\u7528\u6269\u6563\u6a21\u578b\u63d0\u53d6\u5206\u6570\u51fd\u6570\u3002\u57fa\u4e8e\u8fd9\u4e9b\u89c1\u89e3\uff0c\u6211\u4eec\u63d0\u51fa\u4e86DreamCatalyst\uff0c\u8fd9\u662f\u4e00\u4e2a\u5728SDS\u6846\u67b6\u4e2d\u8003\u8651\u4e86\u8fd9\u4e9b\u91c7\u6837\u52a8\u529b\u5b66\u7684\u65b0\u6846\u67b6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86DreamCatalyst\u7684\u4f18\u5316\u8fc7\u7a0b\u6765\u903c\u8fd1\u7f16\u8f91\u4efb\u52a1\u4e2d\u7684\u6269\u6563\u9006\u8fc7\u7a0b\uff0c\u4ece\u800c\u4e0e\u6269\u6563\u91c7\u6837\u52a8\u529b\u5b66\u4fdd\u6301\u4e00\u81f4\u3002\u56e0\u6b64\uff0cDreamCatalyst\u6210\u529f\u5730\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u5e76\u63d0\u9ad8\u4e86\u7f16\u8f91\u8d28\u91cf\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e24\u79cd\u6a21\u5f0f\uff1a\uff081\uff09\u5feb\u901f\u6a21\u5f0f\uff0c\u7f16\u8f91\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u573a\u666f\u7684\u901f\u5ea6\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684NeRF\u7f16\u8f91\u65b9\u6cd5\u5feb\u7ea623\u500d\uff1b\uff082\uff09\u9ad8\u8d28\u91cf\u6a21\u5f0f\uff0c\u751f\u6210\u7684\u7ed3\u679c\u6bd4\u8fd9\u4e9b\u65b9\u6cd5\u597d\u7ea68\u500d\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u9ad8\u8d28\u91cf\u6a21\u5f0f\u5728\u901f\u5ea6\u548c\u8d28\u91cf\u65b9\u9762\u90fd\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684NeRF\u7f16\u8f91\u65b9\u6cd5\u3002DreamCatalyst\u8fd8\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u76843D\u9ad8\u65af\u6837\u6761\uff083DGS\uff09\u7f16\u8f91\u65b9\u6cd5\uff0c\u4f7f\u5176\u6210\u4e3a\u4e00\u79cd\u6709\u6548\u4e14\u4e0e\u6a21\u578b\u65e0\u5173\u76843D\u7f16\u8f91\u89e3\u51b3\u65b9\u6848\u3002\u8bf7\u5728\u6211\u4eec\u7684\u9879\u76ee\u9875\u9762\u4e0a\u67e5\u770b\u66f4\u591a\u7ed3\u679c\uff1ahttps://dream-catalyst.github.io\u3002||\n", "2410.06756": "|**2024-10-09**|[DreamMesh4D: Video-to-4D Generation with Sparse-Controlled Gaussian-Mesh Hybrid Representation](http://arxiv.org/abs/2410.06756)|null|\u8fd1\u5e74\u6765\uff0c2D/3D \u751f\u6210\u6280\u672f\u7684\u8fdb\u6b65\u4fc3\u8fdb\u4e86\u4ece\u5355\u76ee\u89c6\u9891\u751f\u6210\u52a8\u6001 3D \u5bf9\u8c61\u3002\u5148\u524d\u7684\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u9690\u5f0f\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u6216\u663e\u5f0f\u9ad8\u65af splatting \u4f5c\u4e3a\u5e95\u5c42\u8868\u793a\uff0c\u5e76\u4e14\u96be\u4ee5\u5b9e\u73b0\u4ee4\u4eba\u6ee1\u610f\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u8868\u9762\u5916\u89c2\u3002\u53d7\u73b0\u4ee3 3D \u52a8\u753b\u6d41\u7a0b\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5f15\u5165\u4e86 DreamMesh4D\uff0c\u8fd9\u662f\u4e00\u79cd\u5c06\u7f51\u683c\u8868\u793a\u4e0e\u51e0\u4f55\u8499\u76ae\u6280\u672f\u76f8\u7ed3\u5408\u7684\u65b0\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u5355\u76ee\u89c6\u9891\u751f\u6210\u9ad8\u8d28\u91cf\u7684 4D \u5bf9\u8c61\u3002\u6211\u4eec\u6ca1\u6709\u4f7f\u7528\u7ecf\u5178\u7684\u7eb9\u7406\u8d34\u56fe\u6765\u8868\u73b0\u5916\u89c2\uff0c\u800c\u662f\u5c06\u9ad8\u65af splat \u7ed1\u5b9a\u5230\u7f51\u683c\u7684\u4e09\u89d2\u5f62\u9762\u4e0a\uff0c\u4ee5\u4fbf\u5bf9\u7eb9\u7406\u548c\u7f51\u683c\u9876\u70b9\u8fdb\u884c\u53ef\u5fae\u5206\u4f18\u5316\u3002\u7279\u522b\u662f\uff0cDreamMesh4D \u4ece\u901a\u8fc7\u56fe\u50cf\u5230 3D \u751f\u6210\u8fc7\u7a0b\u83b7\u5f97\u7684\u7c97\u7565\u7f51\u683c\u5f00\u59cb\u3002\u7136\u540e\uff0c\u5728\u7f51\u683c\u8868\u9762\u5747\u5300\u91c7\u6837\u7a00\u758f\u70b9\uff0c\u5e76\u4f7f\u7528\u8fd9\u4e9b\u70b9\u6784\u5efa\u53d8\u5f62\u56fe\u4ee5\u9a71\u52a8 3D \u5bf9\u8c61\u7684\u8fd0\u52a8\uff0c\u4ece\u800c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u5e76\u63d0\u4f9b\u989d\u5916\u7684\u7ea6\u675f\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u6b65\u9aa4\uff0c\u4f7f\u7528\u53d8\u5f62\u7f51\u7edc\u9884\u6d4b\u7a00\u758f\u63a7\u5236\u70b9\u7684\u53d8\u6362\uff0c\u5e76\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u51e0\u4f55\u8499\u76ae\u7b97\u6cd5\u5bf9\u7f51\u683c\u9876\u70b9\u548c\u8868\u9762\u9ad8\u65af\u8fdb\u884c\u53d8\u5f62\uff0c\u8be5\u7b97\u6cd5\u662f\u4e00\u79cd\u7ed3\u5408\u4e86 LBS\uff08\u7ebf\u6027\u6df7\u5408\u8499\u76ae\uff09\u548c DQS\uff08\u53cc\u56db\u5143\u6570\u8499\u76ae\uff09\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u51cf\u8f7b\u4e86\u8fd9\u4e24\u79cd\u65b9\u6cd5\u76f8\u5173\u7684\u7f3a\u70b9\u3002\u9759\u6001\u8868\u9762\u9ad8\u65af\u548c\u7f51\u683c\u9876\u70b9\u4ee5\u53ca\u53d8\u5f62\u7f51\u7edc\u901a\u8fc7\u53c2\u8003\u89c6\u56fe\u5149\u5ea6\u635f\u5931\u3001\u5206\u6570\u84b8\u998f\u635f\u5931\u4ee5\u53ca\u5176\u4ed6\u6b63\u5219\u5316\u5668\u4ee5\u4e24\u9636\u6bb5\u65b9\u5f0f\u5b66\u4e60\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u73b0\u4ee3\u56fe\u5f62\u6d41\u7a0b\u517c\u5bb9\uff0c\u5c55\u793a\u4e86\u5176\u5728 3D \u6e38\u620f\u548c\u7535\u5f71\u884c\u4e1a\u7684\u6f5c\u529b\u3002||\n", "2410.05772": "|**2024-10-08**|[Comparative Analysis of Novel View Synthesis and Photogrammetry for 3D Forest Stand Reconstruction and extraction of individual tree parameters](http://arxiv.org/abs/2410.05772)|null|\u51c6\u786e\u9ad8\u6548\u5730\u8fdb\u884c\u6811\u6728\u4e09\u7ef4\u91cd\u5efa\u5bf9\u4e8e\u68ee\u6797\u8d44\u6e90\u8bc4\u4f30\u548c\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002\u8fd1\u666f\u6444\u5f71\u6d4b\u91cf\u6cd5 (CRP) \u901a\u5e38\u7528\u4e8e\u91cd\u5efa\u68ee\u6797\u573a\u666f\uff0c\u4f46\u9762\u4e34\u7740\u6548\u7387\u4f4e\u4e0b\u548c\u8d28\u91cf\u4e0d\u4f73\u7b49\u6311\u6218\u3002\u8fd1\u5e74\u6765\uff0c\u5305\u62ec\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u548c 3D \u9ad8\u65af\u6563\u5c04 (3DGS) \u5728\u5185\u7684\u65b0\u578b\u89c6\u56fe\u5408\u6210 (NVS) \u6280\u672f\u5df2\u663e\u793a\u51fa\u5229\u7528\u6709\u9650\u56fe\u50cf\u8fdb\u884c\u690d\u7269\u4e09\u7ef4\u91cd\u5efa\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u679c\u56ed\u4e2d\u7684\u5c0f\u578b\u690d\u7269\u6216\u5355\u4e2a\u6811\u6728\u4e0a\uff0c\u5bf9\u5176\u5728\u66f4\u5927\u3001\u66f4\u590d\u6742\u7684\u6797\u5206\u4e2d\u7684\u5e94\u7528\u5c1a\u4e0d\u786e\u5b9a\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u6536\u96c6\u4e86\u4e0d\u540c\u590d\u6742\u7a0b\u5ea6\u7684\u68ee\u6797\u6837\u5730\u7684\u5e8f\u5217\u56fe\u50cf\uff0c\u5e76\u4f7f\u7528 NeRF \u548c 3DGS \u8fdb\u884c\u4e86\u5bc6\u96c6\u91cd\u5efa\u3002\u5c06\u6240\u5f97\u70b9\u4e91\u4e0e\u6444\u5f71\u6d4b\u91cf\u548c\u6fc0\u5149\u626b\u63cf\u7684\u70b9\u4e91\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7ed3\u679c\u8868\u660e\uff0cNVS \u65b9\u6cd5\u663e\u7740\u63d0\u9ad8\u4e86\u91cd\u5efa\u6548\u7387\u3002\u6444\u5f71\u6d4b\u91cf\u6cd5\u5728\u5904\u7406\u590d\u6742\u6797\u5206\u65f6\u9047\u5230\u56f0\u96be\uff0c\u5bfc\u81f4\u70b9\u4e91\u4e2d\u5b58\u5728\u8fc7\u591a\u7684\u51a0\u5c42\u566a\u58f0\u548c\u6811\u6728\u91cd\u5efa\u9519\u8bef\uff0c\u4f8b\u5982\u6811\u5e72\u91cd\u590d\u3002NeRF \u867d\u7136\u66f4\u9002\u5408\u51a0\u5c42\u533a\u57df\uff0c\u4f46\u5728\u89c6\u91ce\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u53ef\u80fd\u4f1a\u5728\u5730\u9762\u533a\u57df\u4ea7\u751f\u8bef\u5dee\u30023DGS \u65b9\u6cd5\u751f\u6210\u7684\u70b9\u4e91\u8f83\u4e3a\u7a00\u758f\uff0c\u5c24\u5176\u662f\u5728\u6811\u5e72\u533a\u57df\uff0c\u5f71\u54cd\u4e86\u80f8\u5f84 (DBH) \u7684\u7cbe\u5ea6\u3002\u8fd9\u4e09\u79cd\u65b9\u6cd5\u90fd\u53ef\u4ee5\u63d0\u53d6\u6811\u9ad8\u4fe1\u606f\uff0c\u5176\u4e2d NeRF \u7684\u7cbe\u5ea6\u6700\u9ad8\uff1b\u7136\u800c\uff0c\u6444\u5f71\u6d4b\u91cf\u6cd5\u5728\u80f8\u5f84\u7cbe\u5ea6\u65b9\u9762\u4ecd\u7136\u5177\u6709\u4f18\u52bf\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0cNVS \u65b9\u6cd5\u5728\u6797\u5206\u4e09\u7ef4\u91cd\u5efa\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u4e3a\u590d\u6742\u7684\u68ee\u6797\u8d44\u6e90\u6e05\u67e5\u548c\u53ef\u89c6\u5316\u4efb\u52a1\u63d0\u4f9b\u5b9d\u8d35\u652f\u6301\u3002||\n", "2410.11394": "|**2024-10-15**|[MCGS: Multiview Consistency Enhancement for Sparse-View 3D Gaussian Radiance Fields](http://arxiv.org/abs/2410.11394)|null|\u7531\u4e09\u7ef4\u9ad8\u65af\u51fd\u6570\u8868\u793a\u7684\u8f90\u5c04\u573a\u5728\u5408\u6210\u65b0\u89c6\u89d2\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u517c\u5177\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u5feb\u901f\u6e32\u67d3\u901f\u5ea6\u3002\u7136\u800c\uff0c\u5728\u7a00\u758f\u8f93\u5165\u89c6\u89d2\u4e0b\uff0c\u7531\u4e8e\u7f3a\u4e4f\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u4f1a\u5bfc\u81f4\u70b9\u4e91\u521d\u59cb\u5316\u4e0d\u826f\uff0c\u4f18\u5316\u548c\u5bc6\u96c6\u5316\u542f\u53d1\u5f0f\u65b9\u6cd5\u4e0d\u53ef\u9760\uff0c\u6700\u7ec8\u5bfc\u81f4\u6027\u80fd\u6b20\u4f73\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f1a\u7ed3\u5408\u6765\u81ea\u5bc6\u96c6\u4f30\u8ba1\u7f51\u7edc\u7684\u6df1\u5ea6\u5148\u9a8c\uff0c\u4f46\u5ffd\u7565\u4e86\u8f93\u5165\u56fe\u50cf\u4e2d\u56fa\u6709\u7684\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u591a\u89c6\u89d2\u7acb\u4f53\u89c6\u89c9\uff08MVS\uff09\u7684\u521d\u59cb\u5316\uff0c\u8fd9\u9650\u5236\u4e86\u573a\u666f\u8868\u793a\u7684\u6548\u7387\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e09\u7ef4\u9ad8\u65af\u6e32\u67d3\u7684\u89c6\u89d2\u5408\u6210\u6846\u67b6\uff0c\u540d\u4e3aMCGS\uff0c\u80fd\u591f\u4ece\u7a00\u758f\u8f93\u5165\u89c6\u89d2\u5b9e\u73b0\u903c\u771f\u7684\u573a\u666f\u91cd\u5efa\u3002MCGS\u5728\u589e\u5f3a\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u65b9\u9762\u7684\u5173\u952e\u521b\u65b0\u5982\u4e0b\uff1ai\uff09\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u7a00\u758f\u5339\u914d\u5668\u7ed3\u5408\u968f\u673a\u586b\u5145\u7b56\u7565\uff0c\u751f\u6210\u4e00\u7ec4\u7d27\u51d1\u4f46\u5145\u8db3\u7684\u521d\u59cb\u70b9\u3002\u8fd9\u79cd\u65b9\u6cd5\u589e\u5f3a\u4e86\u521d\u59cb\u51e0\u4f55\u5148\u9a8c\uff0c\u4fc3\u8fdb\u4e86\u9ad8\u6548\u7684\u573a\u666f\u8868\u793a\u3002ii\uff09\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u5f15\u5bfc\u7684\u6e10\u8fdb\u5f0f\u526a\u679d\u7b56\u7565\uff0c\u901a\u8fc7\u52a0\u5f3a\u4e00\u81f4\u6027\u548c\u6d88\u9664\u4f4e\u8d21\u732e\u5ea6\u7684\u9ad8\u65af\u51fd\u6570\u6765\u7ec6\u5316\u9ad8\u65af\u573a\u3002\u8fd9\u4e9b\u6a21\u5757\u5316\u3001\u5373\u63d2\u5373\u7528\u7684\u7b56\u7565\u589e\u5f3a\u4e86\u5bf9\u7a00\u758f\u8f93\u5165\u89c6\u89d2\u7684\u9c81\u68d2\u6027\uff0c\u52a0\u901f\u4e86\u6e32\u67d3\u901f\u5ea6\uff0c\u5e76\u51cf\u5c11\u4e86\u5185\u5b58\u6d88\u8017\uff0c\u4f7fMCGS\u6210\u4e3a\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u4e09\u7ef4\u9ad8\u65af\u6e32\u67d3\u6846\u67b6\u3002||\n", "2410.11080": "|**2024-10-14**|[Few-shot Novel View Synthesis using Depth Aware 3D Gaussian Splatting](http://arxiv.org/abs/2410.11080)|**[link](https://github.com/raja-kumar/depth-aware-3dgs)**|\u4e09\u7ef4\u9ad8\u65af splatting \u5728\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u9762\u5df2\u7ecf\u8d85\u8d8a\u4e86\u795e\u7ecf\u8f90\u5c04\u573a\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u548c\u5b9e\u65f6\u9ad8\u8d28\u91cf\u6e32\u67d3\u3002\u867d\u7136\u5b83\u5728\u8f93\u5165\u89c6\u56fe\u8f83\u591a\u65f6\u53ef\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\u7ed3\u679c\uff0c\u4f46\u5728\u4ec5\u6709\u5c11\u91cf\u89c6\u56fe\u53ef\u7528\u65f6\uff0c\u5176\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5c11\u6837\u672c\u65b0\u89c6\u89d2\u5408\u6210\u7684\u6df1\u5ea6\u611f\u77e5\u9ad8\u65af splatting \u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u6211\u4eec\u4f7f\u7528\u5355\u76ee\u6df1\u5ea6\u9884\u6d4b\u4f5c\u4e3a\u5148\u9a8c\uff0c\u4ee5\u53ca\u5c3a\u5ea6\u4e0d\u53d8\u7684\u6df1\u5ea6\u635f\u5931\uff0c\u4ee5\u5728\u4ec5\u6709\u5c11\u91cf\u8f93\u5165\u89c6\u56fe\u7684\u60c5\u51b5\u4e0b\u7ea6\u675f\u4e09\u7ef4\u5f62\u72b6\u3002\u6211\u4eec\u8fd8\u4f7f\u7528\u4f4e\u9636\u7403\u8c10\u51fd\u6570\u5bf9\u989c\u8272\u8fdb\u884c\u5efa\u6a21\uff0c\u4ee5\u907f\u514d\u8fc7\u62df\u5408\u3002\u6b64\u5916\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u50cf\u539f\u59cb\u5de5\u4f5c\u4e2d\u90a3\u6837\u5b9a\u671f\u79fb\u9664\u4e0d\u900f\u660e\u5ea6\u8f83\u4f4e\u7684 splats \u4f1a\u5bfc\u81f4\u70b9\u4e91\u975e\u5e38\u7a00\u758f\uff0c\u4ece\u800c\u964d\u4f4e\u6e32\u67d3\u8d28\u91cf\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u4fdd\u7559\u4e86\u6240\u6709\u7684 splats\uff0c\u4ece\u800c\u5728\u5c11\u91cf\u89c6\u56fe\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u91cd\u5efa\u6548\u679c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u7684\u4e09\u7ef4\u9ad8\u65af splatting \u65b9\u6cd5\uff0c\u5728\u5cf0\u503c\u4fe1\u566a\u6bd4\u3001\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570\u548c\u611f\u77e5\u76f8\u4f3c\u6027\u65b9\u9762\u5206\u522b\u63d0\u9ad8\u4e86 10.5%\u30016% \u548c 14.1%\uff0c\u4ece\u800c\u9a8c\u8bc1\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u4ee3\u7801\u5c06\u5728\u4ee5\u4e0b\u5730\u5740\u5f00\u6e90\uff1ahttps://github.com/raja-kumar/depth-aware-3DGS||\n", "2410.12262": "|**2024-10-16**|[3D Gaussian Splatting in Robotics: A Survey](http://arxiv.org/abs/2410.12262)|null|\u5728\u673a\u5668\u4eba\u9886\u57df\uff0c\u5bf9\u73af\u5883\u8fdb\u884c\u5bc6\u96c6\u7684\u4e09\u7ef4\u8868\u793a\u4e00\u76f4\u662f\u4e00\u4e2a\u957f\u671f\u76ee\u6807\u3002\u867d\u7136\u4ee5\u524d\u57fa\u4e8e\u5750\u6807\u7684\u9690\u5f0f\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u8868\u793a\u6cd5\u4e00\u76f4\u5f88\u6d41\u884c\uff0c\u4f46\u6700\u8fd1\u51fa\u73b0\u76843D\u9ad8\u65af\u6563\u5c04\uff083DGS\uff09\u5728\u5176\u663e\u5f0f\u8f90\u5c04\u573a\u8868\u793a\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u7684\u6f5c\u529b\u3002\u901a\u8fc7\u5229\u75283D\u9ad8\u65af\u57fa\u5143\u8fdb\u884c\u663e\u5f0f\u573a\u666f\u8868\u793a\u5e76\u5b9e\u73b0\u53ef\u5fae\u6e32\u67d3\uff0c3DGS\u5728\u5b9e\u65f6\u6e32\u67d3\u548c\u903c\u771f\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u4e8e\u5176\u4ed6\u8f90\u5c04\u573a\u7684\u663e\u8457\u4f18\u52bf\uff0c\u8fd9\u5bf9\u673a\u5668\u4eba\u5e94\u7528\u975e\u5e38\u6709\u5229\u3002\u5728\u672c\u7efc\u8ff0\u4e2d\uff0c\u6211\u4eec\u5168\u9762\u4ecb\u7ecd\u4e863DGS\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\u3002\u6211\u4eec\u5c06\u76f8\u5173\u5de5\u4f5c\u7684\u8ba8\u8bba\u5206\u4e3a\u4e24\u5927\u7c7b\uff1a3DGS\u7684\u5e94\u7528\u548c3DGS\u6280\u672f\u7684\u8fdb\u6b65\u3002\u5728\u5e94\u7528\u90e8\u5206\uff0c\u6211\u4eec\u63a2\u8ba8\u4e863DGS\u5982\u4f55\u4ece\u573a\u666f\u7406\u89e3\u548c\u4ea4\u4e92\u7684\u89d2\u5ea6\u5e94\u7528\u4e8e\u5404\u79cd\u673a\u5668\u4eba\u4efb\u52a1\u30023DGS\u6280\u672f\u7684\u8fdb\u6b65\u90e8\u5206\u4fa7\u91cd\u4e8e3DGS\u81ea\u8eab\u5c5e\u6027\u5728\u9002\u5e94\u6027\u548c\u6548\u7387\u65b9\u9762\u7684\u6539\u8fdb\uff0c\u65e8\u5728\u63d0\u9ad8\u5176\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u6027\u80fd\u3002\u7136\u540e\uff0c\u6211\u4eec\u603b\u7ed3\u4e86\u673a\u5668\u4eba\u9886\u57df\u6700\u5e38\u7528\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002\u6700\u540e\uff0c\u6211\u4eec\u6307\u51fa\u4e86\u5f53\u524d3DGS\u65b9\u6cd5\u7684\u6311\u6218\u548c\u5c40\u9650\u6027\uff0c\u5e76\u8ba8\u8bba\u4e863DGS\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002||\n"}, "\u5206\u7c7b/\u68c0\u6d4b/\u8bc6\u522b/\u5206\u5272": {"2409.02838": "|**2024-09-04**|[iConFormer: Dynamic Parameter-Efficient Tuning with Input-Conditioned Adaptation](http://arxiv.org/abs/2409.02838)|null|\u57fa\u4e8e\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u5b8c\u6574\u5fae\u8c03\uff08FFT\uff09\u548c\u4efb\u52a1\u7279\u5b9a\u89e3\u7801\u5668\u7684\u8fc1\u79fb\u5b66\u4e60\u968f\u7740\u6df1\u5ea6\u6a21\u578b\u7684\u6307\u6570\u7ea7\u589e\u957f\u800c\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\u3002\u4f7f\u7528\u7531\u5c0f\u578b\u53ef\u5b66\u4e60\u5c42\u7ec4\u6210\u7684\u9002\u914d\u5668\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u5df2\u6210\u4e3a FFT \u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u8bad\u7ec3\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u53ef\u6bd4\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u9002\u914d\u5668\u5bf9\u8f93\u5165\u5b9e\u4f8b\u7684\u4e0d\u7075\u6d3b\u9650\u5236\u4e86\u5176\u5728\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u4fe1\u606f\u7684\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 PEFT \u65b9\u6cd5\uff0c\u5373\u8f93\u5165\u6761\u4ef6\u5316\u7684 Transformer\uff0c\u79f0\u4e3a iConFormer\uff0c\u5b83\u5229\u7528\u4e86\u4ee5\u8f93\u5165\u5b9e\u4f8b\u4e3a\u6761\u4ef6\u7684\u52a8\u6001\u9002\u914d\u5668\u3002\u4e3a\u4e86\u786e\u4fdd\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5bf9\u8f93\u5165\u5b9e\u4f8b\u7684\u7075\u6d3b\u5b66\u4e60\u80fd\u529b\uff0c\u6211\u4eec\u5728\u52a8\u6001\u9002\u914d\u5668\u4e2d\u5f15\u5165\u4e86\u8f93\u5165\u6761\u4ef6\u5316\u7f51\u7edc\uff08iCoN\uff09\uff0c\u4ece\u800c\u5b9e\u73b0\u5b9e\u4f8b\u7ea7\u7279\u5f81\u8f6c\u6362\u3002\u5177\u4f53\u6765\u8bf4\uff0ciCoN \u4e3a\u6bcf\u4e2a\u7279\u5f81\u751f\u6210\u901a\u9053\u7ea7\u7684\u5377\u79ef\u6838\uff0c\u5e76\u4f7f\u7528\u81ea\u9002\u5e94\u5377\u79ef\u8fc7\u7a0b\u5bf9\u5176\u8fdb\u884c\u8f6c\u6362\uff0c\u4ee5\u6709\u6548\u6355\u83b7\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u4efb\u52a1\u7279\u5b9a\u548c\u7ec6\u7c92\u5ea6\u7ec6\u8282\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u4ec5\u8c03\u6574 Transformer \u4e3b\u5e72\u53c2\u6570\u7684 1.6% \u5230 2.8%\uff0ciConFormer \u5728\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u548c\u8bed\u4e49\u5206\u5272\u65b9\u9762\u5b9e\u73b0\u4e86\u4e0e FFT \u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u5b9e\u4f8b\u5206\u5272\u65b9\u9762\u4f18\u4e8e FFT\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6240\u6709\u4e0a\u8ff0\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u8fd1\u7684 PEFT \u65b9\u6cd5\u3002||\n", "2409.02546": "|**2024-09-04**|[Real-Time Dynamic Scale-Aware Fusion Detection Network: Take Road Damage Detection as an example](http://arxiv.org/abs/2409.02546)|null|\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u9053\u8def\u635f\u574f\u68c0\u6d4b (RDD) \u5bf9\u57ce\u5e02\u7684\u65e5\u5e38\u7ef4\u62a4\u548c\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u663e\u8457\u964d\u4f4e\u52b3\u52a8\u529b\u6210\u672c\u65b9\u9762\u3002\u7136\u800c\uff0c\u5f53\u524d\u57fa\u4e8e\u65e0\u4eba\u673a\u7684 RDD \u7814\u7a76\u4ecd\u9762\u4e34\u8bb8\u591a\u6311\u6218\u3002\u4f8b\u5982\uff0c\u5f62\u72b6\u548c\u65b9\u5411\u4e0d\u89c4\u5219\u7684\u635f\u574f\u3001\u80cc\u666f\u5bf9\u635f\u574f\u7684\u906e\u6321\u4ee5\u53ca\u96be\u4ee5\u533a\u5206\u635f\u574f\u548c\u80cc\u666f\uff0c\u8fd9\u4e9b\u56e0\u7d20\u90fd\u663e\u8457\u5f71\u54cd\u4e86\u65e0\u4eba\u673a\u5728\u65e5\u5e38\u5de1\u68c0\u4e2d\u68c0\u6d4b\u9053\u8def\u635f\u574f\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u63d0\u9ad8\u65e0\u4eba\u673a\u5b9e\u65f6\u9053\u8def\u635f\u574f\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u6211\u4eec\u8bbe\u8ba1\u5e76\u63d0\u51fa\u4e86\u4e09\u4e2a\u76f8\u5e94\u7684\u6a21\u5757\uff1a\u4e00\u4e2a\u80fd\u591f\u7075\u6d3b\u9002\u5e94\u5f62\u72b6\u548c\u80cc\u666f\u7684\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff1b\u4e00\u4e2a\u878d\u5408\u591a\u5c3a\u5ea6\u611f\u77e5\u5e76\u9002\u5e94\u5f62\u72b6\u548c\u80cc\u666f\u7684\u6a21\u5757\uff1b\u4e00\u4e2a\u9ad8\u6548\u7684\u4e0b\u91c7\u6837\u6a21\u5757\u3002 \u57fa\u4e8e\u8fd9\u4e9b\u6a21\u5757\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5177\u6709\u81ea\u52a8\u53bb\u9664\u80cc\u666f\u5e72\u6270\u80fd\u529b\u7684\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u9053\u8def\u635f\u574f\u68c0\u6d4b\u6a21\u578b\uff0c\u79f0\u4e3a\u52a8\u6001\u5c3a\u5ea6\u611f\u77e5\u878d\u5408\u68c0\u6d4b\u6a21\u578b (RT-DSAFDet)\u3002\u5728 UAV-PDD2023 \u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b RT-DSAFDet \u7684 mAP50 \u8fbe\u5230\u4e86 54.2%\uff0c\u6bd4\u6700\u65b0\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u6a21\u578b YOLOv10 \u7684\u9ad8\u6548\u53d8\u4f53 YOLOv10-m \u9ad8 11.1%\uff0c\u800c\u53c2\u6570\u91cf\u51cf\u5c11\u5230 1.8M\uff0cFLOPs \u51cf\u5c11\u5230 4.6G\uff0c\u5206\u522b\u964d\u4f4e\u4e86 88% \u548c 93%\u3002\u6b64\u5916\uff0c\u5728\u5927\u578b\u901a\u7528\u76ee\u6807\u68c0\u6d4b\u516c\u5f00\u6570\u636e\u96c6 MS COCO2017 \u4e0a\u4e5f\u5c55\u73b0\u4e86\u6211\u4eec\u6a21\u578b\u7684\u4f18\u8d8a\u6027\uff0c\u5176 mAP50-95 \u4e0e YOLOv9-t \u76f8\u540c\uff0c\u4f46 mAP50 \u9ad8\u51fa 0.5%\uff0c\u53c2\u6570\u91cf\u51cf\u5c11 10%\uff0cFLOPs \u51cf\u5c11 40%\u3002||\n", "2409.02486": "|**2024-09-04**|[Boosting Generalizability towards Zero-Shot Cross-Dataset Single-Image Indoor Depth by Meta-Initialization](http://arxiv.org/abs/2409.02486)|null|\u5ba4\u5185\u673a\u5668\u4eba\u7684\u5bfc\u822a\u6216\u969c\u788d\u7269\u68c0\u6d4b\u7b49\u4efb\u52a1\u4f9d\u8d56\u4e8e\u6df1\u5ea6\u4fe1\u606f\uff0c\u800c\u5355\u56fe\u50cf\u6df1\u5ea6\u4f30\u8ba1\u88ab\u5e7f\u6cdb\u7528\u4e8e\u8f85\u52a9\u611f\u77e5\u3002\u5927\u591a\u6570\u5ba4\u5185\u5355\u56fe\u50cf\u6df1\u5ea6\u9884\u6d4b\u8f83\u5c11\u5173\u6ce8\u6a21\u578b\u5bf9\u672a\u89c1\u6570\u636e\u96c6\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u66f4\u5173\u6ce8\u7cfb\u7edf\u90e8\u7f72\u7684\u91ce\u5916\u9c81\u68d2\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u5229\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u5143\u5b66\u4e60\u5728\u96f6\u6837\u672c\u8de8\u6570\u636e\u96c6\u63a8\u7406\u4e2d\u83b7\u5f97\u66f4\u9ad8\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4e0e\u7814\u7a76\u6700\u591a\u7684\u3001\u4e0e\u663e\u5f0f\u7c7b\u522b\u6807\u7b7e\u76f8\u5173\u7684\u56fe\u50cf\u5206\u7c7b\u5143\u5b66\u4e60\u4e0d\u540c\uff0c\u5bf9\u4e8e\u4e0e\u7269\u4f53\u6392\u5217\u548c\u573a\u666f\u6784\u6210\u65b9\u9762\u9ad8\u5ea6\u53d8\u5316\u7684\u5ba4\u5185\u73af\u5883\u76f8\u5173\u7684\u8fde\u7eed\u6df1\u5ea6\u503c\uff0c\u4e0d\u5b58\u5728\u660e\u786e\u7684\u4efb\u52a1\u8fb9\u754c\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u7ec6\u7c92\u5ea6\u4efb\u52a1\uff0c\u5728\u6211\u4eec\u7684\u5143\u5b66\u4e60\u516c\u5f0f\u4e2d\u5c06\u6bcf\u4e2aRGB-D\u5c0f\u6279\u91cf\u89c6\u4e3a\u4e00\u4e2a\u4efb\u52a1\u3002\u6211\u4eec\u9996\u5148\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6709\u9650\u6570\u636e\u4e0a\u8bf1\u5bfc\u51fa\u66f4\u597d\u7684\u5148\u9a8c\uff08RMSE \u6700\u9ad8\u964d\u4f4e 27.8%\uff09\u3002\u7136\u540e\uff0c\u5728\u5143\u5b66\u4e60\u521d\u59cb\u5316\u4e0a\u8fdb\u884c\u5fae\u8c03\u59cb\u7ec8\u4f18\u4e8e\u6ca1\u6709\u5143\u65b9\u6cd5\u7684\u57fa\u7ebf\u3002\u4e3a\u4e86\u5b9e\u73b0\u6cdb\u5316\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u96f6\u6837\u672c\u8de8\u6570\u636e\u96c6\u534f\u8bae\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7531\u6211\u4eec\u7684\u5143\u521d\u59cb\u5316\u8bf1\u5bfc\u7684\u66f4\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u4f5c\u4e3a\u8bb8\u591a\u73b0\u6709\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u7684\u7b80\u5355\u800c\u6709\u7528\u7684\u63d2\u4ef6\u3002\u6df1\u5ea6\u548c\u5143\u5b66\u4e60\u4ea4\u53c9\u9886\u57df\u7684\u5de5\u4f5c\u6709\u53ef\u80fd\u63a8\u52a8\u8fd9\u4e24\u9879\u7814\u7a76\u66f4\u63a5\u8fd1\u5b9e\u9645\u7684\u673a\u5668\u4eba\u548c\u673a\u5668\u611f\u77e5\u5e94\u7528\u3002||\n", "2409.02329": "|**2024-09-03**|[Site Selection for the Second Flyeye Telescope: A Simulation Study for Optimizing Near-Earth Object Discovery](http://arxiv.org/abs/2409.02329)|null|\u6b27\u6d32\u822a\u5929\u5c40 (ESA) \u6b63\u5728\u5f00\u53d1\u4e00\u4e2a\u540d\u4e3a Flyeye \u7684\u5e7f\u57df\u5de1\u5929\u671b\u8fdc\u955c\u7f51\u7edc\uff0c\u4ee5\u6539\u8fdb\u8fd1\u5730\u5929\u4f53 (NEO) \u7684\u53d1\u73b0\u3002\u8be5\u7f51\u7edc\u4e2d\u7684\u7b2c\u4e00\u4e2a\u671b\u8fdc\u955c\u5c06\u4f4d\u4e8e\u5317\u534a\u7403\u7684\u7a46\u6cd5\u62c9\u5c71\uff08\u610f\u5927\u5229\uff09\uff0c\u800c\u7b2c\u4e8c\u4e2a\u5177\u6709\u589e\u5f3a\u63a2\u6d4b\u80fd\u529b\u7684 Flyeye \u671b\u8fdc\u955c\u521a\u521a\u5f00\u59cb\u5173\u952e\u8bbe\u8ba1\u9636\u6bb5\u3002\u901a\u8fc7\u5bf9\u649e\u51fb\u8f68\u8ff9\u4e0a\u7684\u8fd1\u5730\u5929\u4f53\u8fdb\u884c\u6a21\u62df\uff0c\u7814\u7a76\u4e86\u7b2c\u4e8c\u4e2a Flyeye \u671b\u8fdc\u955c\u7684\u6f5c\u5728\u4f4d\u7f6e\u3002\u5bf9\u5927\u7ea6 3000 \u4e2a\u649e\u51fb\u5c0f\u884c\u661f\uff08\u7edd\u5bf9\u661f\u7b49\u4e3a H=25 \u548c H=28\uff09\u8fdb\u884c\u4e86\u4f20\u64ad\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e3b\u8981\u73b0\u6709\u5de1\u5929\u9879\u76ee\uff08Catalina\u3001Pan-STARRS\u3001ATLAS\uff09\u3001\u5373\u5c06\u6295\u5165\u4f7f\u7528\u7684\u8587\u62c9\u00b7\u9c81\u5bbe\u5929\u6587\u53f0 (LSST) \u4ee5\u53ca Flyeye \u53ef\u80fd\u9009\u5740\u7684\u53ef\u63a2\u6d4b\u6027\u3002 \u8003\u8651\u4e86\u667a\u5229\u3001\u5357\u975e\u548c\u5317\u534a\u7403\u7684\u7b2c\u4e8c\u4e2a\u8bbe\u65bd\u3002\u5bf9\u4e8e\u6bcf\u4e2a\u5929\u6587\u53f0\uff0c\u5728\u6a21\u62df\u4e2d\u90fd\u8003\u8651\u4e86\u5b83\u4eec\u8fc7\u53bb\u6216\u8ba1\u5212\u7684\u6307\u5411\u7b56\u7565\u3002\u5728 LSST \u90e8\u7f72\u4e4b\u524d\uff0c\u5357\u534a\u7403\u7684\u4e00\u4e2a Flyeye \u7684\u6027\u80fd\u4e0e\u5317\u534a\u7403\u7684\u4e00\u4e2a\u671b\u8fdc\u955c\u76f8\u4f3c\u3002\u7ed3\u5408\u8d77\u6765\uff0c\u5728\u5317\u65b9\u548c\u5357\u65b9\u5404\u653e\u7f6e\u4e00\u53f0\u671b\u8fdc\u955c\u53ef\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u63a2\u6d4b\u7387\u548c\u63a2\u6d4b\u5230\u7684\u72ec\u7279\u7269\u4f53\u7684\u6570\u91cf\u3002LSST \u4e4b\u540e\uff0c\u5357\u90e8\u548c\u5317\u90e8\u7684 Flyeye \u671b\u8fdc\u955c\u4ecd\u7136\u662f\u4e92\u8865\u7684\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6a21\u62df\u8868\u660e\uff0c\u65e0\u8bba\u662f\u5728 LSST \u4e4b\u524d\u8fd8\u662f\u4e4b\u540e\uff0c\u4f4d\u4e8e\u5357\u90e8\u7684\u7b2c\u4e8c\u4e2a Flyeye \u90fd\u53ef\u4ee5\u8865\u5145\u4f4d\u4e8e\u5317\u90e8\u7684 Flyeye \u671b\u8fdc\u955c\u3002\u4f4d\u4e8e\u62c9\u897f\u62c9\u7684 Flyeye \u5c06\u5229\u7528\u5176\u4f18\u8d8a\u7684\u5927\u6c14\u6761\u4ef6\uff0c\u540c\u65f6\u5e73\u8861\u5357\u5317\u534a\u7403\u7684\u8d44\u4ea7\u3002||\n", "2409.02281": "|**2024-09-03**|[K-Origins: Better Colour Quantification for Neural Networks](http://arxiv.org/abs/2409.02281)|**[link](https://github.com/lewismmason/Thesis-Public)**|K-Origins\u662f\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u5c42\uff0c\u65e8\u5728\u5728\u5b66\u4e60\u989c\u8272\u6216\u5f3a\u5ea6\u6709\u5229\u65f6\u63d0\u9ad8\u57fa\u4e8e\u56fe\u50cf\u7684\u7f51\u7edc\u6027\u80fd\u3002 \u8d85\u8fc7 250 \u4e2a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5377\u79ef\u7f51\u7edc\u5728 16 \u4f4d\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0cK-Origins \u63d0\u9ad8\u4e86\u8bed\u4e49\u5206\u5272\u7cbe\u5ea6\uff1a\u4f4e\u4fe1\u566a\u6bd4\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u4ee5\u53ca\u5206\u5272\u5f62\u72b6\u76f8\u540c\u4f46\u989c\u8272\u4e0d\u540c\u7684\u591a\u4e2a\u76ee\u6807\u3002 \u5bf9\u4e8e\u6bcf\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570$w_k$\uff0cK-Origins \u901a\u8fc7\u516c\u5f0f $\\textbf{Y}_k = \\textbf{X}-\\textbf{J}\\cdot w_k$ \u4ece\u8f93\u5165\u7279\u5f81 $\\textbf{X}$ \u751f\u6210\u8f93\u51fa\u7279\u5f81\uff0c\u5176\u4e2d $\\textbf{J}$ \u662f\u4e00\u4e2a\u5168 1 \u77e9\u9635\u3002 \u6b64\u5916\uff0c\u8fd8\u8bad\u7ec3\u4e86\u5177\u6709\u4e0d\u540c\u611f\u53d7\u91ce\u7684\u7f51\u7edc\uff0c\u4ee5\u6839\u636e\u76ee\u6807\u7c7b\u522b\u7684\u7ef4\u5ea6\u786e\u5b9a\u6700\u4f73\u7f51\u7edc\u6df1\u5ea6\uff0c\u8fd9\u8868\u660e\u611f\u53d7\u91ce\u957f\u5ea6\u5e94\u8d85\u8fc7\u76ee\u6807\u5927\u5c0f\u3002 \u901a\u8fc7\u786e\u4fdd\u8db3\u591f\u7684\u611f\u53d7\u91ce\u957f\u5ea6\u5e76\u7ed3\u5408 K-Origins\uff0c\u6211\u4eec\u53ef\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u8bed\u4e49\u7f51\u7edc\u6027\u80fd\u3002||\n", "2409.02278": "|**2024-09-03**|[Evaluation and Comparison of Visual Language Models for Transportation Engineering Problems](http://arxiv.org/abs/2409.02278)|null|\u8fd1\u5e74\u6765\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\u5c55\u73b0\u51fa\u5176\u5728\u56fe\u50cf\u7406\u89e3\u76f8\u5173\u5e94\u7528\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u4ea4\u901a\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4f8b\u5982\u56fe\u50cf\u5206\u7c7b\u548c\u76ee\u6807\u68c0\u6d4b\u3002\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u5305\u62ec\u62e5\u5835\u68c0\u6d4b\u548c\u88c2\u7f1d\u8bc6\u522b\uff0c\u800c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u5219\u7528\u4e8e\u8bc6\u522b\u672a\u4f69\u6234\u5934\u76d4\u7684\u884c\u4e3a\u3002\u6211\u4eec\u5e94\u7528\u4e86\u5f00\u6e90\u6a21\u578b\uff08\u5982CLIP\u3001BLIP\u3001OWL-ViT\u3001Llava-Next\uff09\u548c\u95ed\u6e90\u6a21\u578bGPT-4o\uff0c\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4ee5\u5229\u7528\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u6765\u5b8c\u6210\u57fa\u4e8e\u89c6\u89c9\u7684\u4ea4\u901a\u4efb\u52a1\u3002\u8fd9\u4e9b\u4efb\u52a1\u901a\u8fc7\u5bf9VLM\u6a21\u578b\u5e94\u7528\u96f6\u6837\u672c\u63d0\u793a\u6765\u5b8c\u6210\uff0c\u56e0\u4e3a\u96f6\u6837\u672c\u63d0\u793a\u53ef\u4ee5\u5728\u4e0d\u5bf9\u4efb\u52a1\u8fdb\u884c\u4efb\u4f55\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u4efb\u52a1\u3002\u8fd9\u6d88\u9664\u4e86\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u6807\u6ce8\u6570\u636e\u96c6\u6216\u5fae\u8c03\u7684\u9700\u6c42\u3002\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4e0e\u57fa\u51c6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u4f46\u5728\u76ee\u6807\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4ecd\u6709\u6539\u8fdb\u7684\u7a7a\u95f4\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u5bf9\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u7a81\u51fa\u4e86\u8fd9\u4e9b\u6a21\u578b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u672a\u6765\u6539\u8fdb\u548c\u5e7f\u6cdb\u5b9e\u65bd\u7684\u57fa\u51c6\u3002||\n", "2409.02035": "|**2024-09-03**|[A Modern Take on Visual Relationship Reasoning for Grasp Planning](http://arxiv.org/abs/2409.02035)|null|\u4e0e\u73b0\u5b9e\u4e16\u754c\u6742\u4e71\u573a\u666f\u4ea4\u4e92\u5bf9\u673a\u5668\u4eba\u4ee3\u7406\u63d0\u51fa\u4e86\u82e5\u5e72\u6311\u6218\uff0c\u8fd9\u4e9b\u4ee3\u7406\u9700\u8981\u7406\u89e3\u89c2\u5bdf\u5230\u7684\u7269\u4f53\u4e4b\u95f4\u590d\u6742\u7684\u7684\u7a7a\u95f4\u4f9d\u8d56\u6027\uff0c\u4ee5\u786e\u5b9a\u6700\u4f73\u62fe\u53d6\u987a\u5e8f\u6216\u6709\u6548\u7684\u7269\u4f53\u68c0\u7d22\u7b56\u7565\u3002 \u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u7ba1\u7406\u7b80\u5316\u7684\u573a\u666f\uff0c\u5e76\u4fa7\u91cd\u4e8e\u5728\u521d\u59cb\u7269\u4f53\u68c0\u6d4b\u9636\u6bb5\u4e4b\u540e\u9884\u6d4b\u6210\u5bf9\u7269\u4f53\u5173\u7cfb\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u5168\u5c40\u4e0a\u4e0b\u6587\u6216\u96be\u4ee5\u5904\u7406\u5197\u4f59\u548c\u7f3a\u5931\u7684\u7269\u4f53\u5173\u7cfb\u3002 \u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6293\u53d6\u89c4\u5212\u7684\u89c6\u89c9\u5173\u7cfb\u63a8\u7406\u7684\u73b0\u4ee3\u65b9\u6cd5\u3002 \u6211\u4eec\u4ecb\u7ecd\u4e86 D3GD\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5176\u4e2d\u5305\u62ec\u5305\u542b\u6765\u81ea 97 \u4e2a\u4e0d\u540c\u7c7b\u522b\u7684\u591a\u8fbe 35 \u4e2a\u7269\u4f53\u7684\u5206\u62e3\u573a\u666f\u3002 \u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86 D3G\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u7aef\u5230\u7aef transformer \u7684\u4f9d\u8d56\u56fe\u751f\u6210\u6a21\u578b\uff0c\u5b83\u53ef\u4ee5\u540c\u65f6\u68c0\u6d4b\u7269\u4f53\u5e76\u751f\u6210\u8868\u793a\u5176\u7a7a\u95f4\u5173\u7cfb\u7684\u90bb\u63a5\u77e9\u9635\u3002 \u8ba4\u8bc6\u5230\u6807\u51c6\u6307\u6807\u7684\u5c40\u9650\u6027\uff0c\u6211\u4eec\u9996\u6b21\u91c7\u7528\u5173\u7cfb\u5e73\u5747\u7cbe\u5ea6\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u57fa\u51c6\u6d4b\u8bd5\u3002 \u83b7\u5f97\u7684\u7ed3\u679c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u8fd9\u9879\u4efb\u52a1\u7684\u6700\u65b0\u6280\u672f\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002 \u6211\u4eec\u5728 https://paolotron.github.io/d3g.github.io \u4e0a\u516c\u5f00\u53d1\u5e03\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002||\n", "2409.01988": "|**2024-09-03**|[Compressed learning based onboard semantic compression for remote sensing platforms](http://arxiv.org/abs/2409.01988)|**[link](https://github.com/protim1191/glodismo_classifier)**|\u5730\u7403\u89c2\u6d4b (EO) \u5728\u521b\u5efa\u548c\u7ef4\u6301\u4e00\u4e2a\u5177\u6709\u5f39\u6027\u548c\u7e41\u8363\u7684\u793e\u4f1a\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u8fd9\u5bf9\u6240\u6709\u751f\u547d\u548c\u5730\u7403\u672c\u8eab\u90fd\u5177\u6709\u6df1\u8fdc\u7684\u5f71\u54cd\u3002\u536b\u661f\u3001\u822a\u7a7a\u5e73\u53f0\u4ee5\u53ca\u6700\u8fd1\u7684\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u9a7e\u9a76\u98de\u884c\u5668\u7b49\u9065\u611f\u5e73\u53f0\u90fd\u7528\u4e8e EO\u3002\u5b83\u4eec\u6536\u96c6\u5927\u91cf\u6570\u636e\uff0c\u9700\u8981\u5c06\u5176\u4e0b\u4f20\u5230\u5730\u7403\u8fdb\u884c\u8fdb\u4e00\u6b65\u5904\u7406\u548c\u5206\u6790\u3002\u8fd9\u79cd\u9ad8\u541e\u5410\u91cf\u91c7\u96c6\u7684\u74f6\u9888\u662f\u4e0b\u884c\u94fe\u8def\u5e26\u5bbd\u3002\u9700\u8981\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u56fe\u50cf\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u6765\u5e94\u5bf9\u8fd9\u79cd\u6d77\u91cf\u6570\u636e\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u901a\u8fc7\u538b\u7f29\u5b66\u4e60\u6846\u67b6\u7814\u7a76\u4e86\u8bed\u4e49\u538b\u7f29\uff0c\u8be5\u6846\u67b6\u4ec5\u5229\u7528\u5feb\u901f\u548c\u7a00\u758f\u7684\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\u6765\u7f16\u7801\u6570\u636e\u3002\u76f8\u673a\u566a\u58f0\u548c\u901a\u4fe1\u4fe1\u9053\u662f\u9020\u6210\u5931\u771f\u7684\u4e3b\u8981\u6765\u6e90\u3002\u7136\u540e\uff0c\u5b8c\u6574\u7684\u8bed\u4e49\u901a\u4fe1\u7ba1\u9053\u7531\u4e00\u4e2a\u5b66\u4e60\u5230\u7684\u4f4e\u590d\u6742\u5ea6\u538b\u7f29\u77e9\u9635\u7ec4\u6210\uff0c\u8be5\u77e9\u9635\u4f5c\u7528\u4e8e\u566a\u58f0\u76f8\u673a\u8f93\u51fa\uff0c\u4ee5\u5728\u673a\u8f7d\u751f\u6210\u4e00\u4e2a\u89c2\u6d4b\u5411\u91cf\uff0c\u8be5\u5411\u91cf\u901a\u8fc7\u901a\u4fe1\u4fe1\u9053\u4e0b\u884c\u94fe\u8def\u4f20\u8f93\uff0c\u901a\u8fc7\u5c55\u5f00\u7f51\u7edc\u5904\u7406\uff0c\u7136\u540e\u9988\u9001\u5230\u6267\u884c\u5fc5\u8981\u4e0b\u6e38\u4efb\u52a1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1b\u7814\u7a76\u4e86\u56fe\u50cf\u5206\u7c7b\u3002\u901a\u8fc7\u4f7f\u7528\u5c0f\u6ce2\u7a00\u758f\u5148\u9a8c\u5c55\u5f00 NA-ALISTA \u7684\u5c42\u6765\u8865\u507f\u5931\u771f\u3002\u56e0\u6b64\uff0c\u89e3\u7801\u662f\u4e00\u79cd\u6839\u636e\u76f8\u673a/\u73af\u5883\u4fe1\u606f\u548c\u4e0b\u6e38\u4efb\u52a1\u8bbe\u8ba1\u7684\u5373\u63d2\u5373\u7528\u65b9\u6cd5\u3002\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u901a\u8fc7\u7aef\u5230\u7aef\u65b9\u5f0f\u7684\u635f\u5931\u51fd\u6570\u4e0e\u538b\u7f29\u77e9\u9635\u548c\u5c55\u5f00\u7f51\u7edc\u8054\u5408\u5fae\u8c03\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4f4e\u538b\u7f29\u6bd4\u7684\u566a\u58f0\u73af\u5883\u4e2d\uff0c\u6dfb\u52a0\u6062\u590d\u635f\u5931\u4ee5\u53ca\u4efb\u52a1\u76f8\u5173\u635f\u5931\u53ef\u4ee5\u63d0\u9ad8\u4e0b\u6e38\u6027\u80fd\u3002||\n", "2409.01872": "|**2024-09-03**|[Latent Distillation for Continual Object Detection at the Edge](http://arxiv.org/abs/2409.01872)|**[link](https://github.com/pastifra/Continual_Nanodet)**|\u867d\u7136\u5728\u76ee\u6807\u68c0\u6d4b\u6587\u732e\u4e2d\u5b58\u5728\u8bb8\u591a\u6027\u80fd\u5353\u8d8a\u7684\u65b9\u6cd5\uff0c\u4f46\u89e3\u51b3\u6570\u636e\u5206\u5e03\u504f\u79fb\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u4e3a\u8fd9\u4e2a\u95ee\u9898\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u9002\u5e94\u65b0\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5148\u524d\u6570\u636e\u7684\u6027\u80fd\u3002\u8fd9\u5bf9\u4e8e\u8fb9\u7f18\u8bbe\u5907\u5c24\u5176\u91cd\u8981\uff0c\u8fd9\u4e9b\u8bbe\u5907\u5728\u6c7d\u8f66\u548c\u673a\u5668\u4eba\u7b49\u52a8\u6001\u73af\u5883\u4e2d\u5f88\u5e38\u89c1\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u76ee\u6807\u68c0\u6d4b\u6301\u7eed\u5b66\u4e60\uff08CLOD\uff09\u573a\u666f\u4e2d\u8fb9\u7f18\u8bbe\u5907\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u9650\u5236\u3002\u5177\u4f53\u6765\u8bf4\uff0c\uff08i\uff09\u6211\u4eec\u7814\u7a76\u4e86\u4e00\u79cd\u5f00\u6e90\u3001\u8f7b\u91cf\u7ea7\u548c\u5feb\u901f\u7684\u68c0\u6d4b\u5668 NanoDet \u5bf9\u8fb9\u7f18\u8bbe\u5907\u4e0a CLOD \u7684\u9002\u7528\u6027\uff0c\u6539\u8fdb\u4e86\u6587\u732e\u4e2d\u4f7f\u7528\u7684\u8f83\u5927\u67b6\u6784\u3002\u6b64\u5916\uff0c\uff08ii\uff09\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6f5c\u5728\u84b8\u998f\uff08LD\uff09\u7684\u65b0\u578b CL \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u663e\u7740\u5f71\u54cd\u68c0\u6d4b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u4e86\u6700\u5148\u8fdb\u7684 CL \u65b9\u6cd5\u6240\u9700\u7684\u8fd0\u7b97\u6b21\u6570\u548c\u5185\u5b58\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u8457\u540d\u7684 VOC \u548c COCO \u57fa\u51c6\u6d4b\u8bd5\u96c6\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u4e0e\u5176\u4ed6\u84b8\u998f\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6bcf\u6b21\u6a21\u578b\u66f4\u65b0\u53ef\u5c06\u84b8\u998f\u53c2\u6570\u5f00\u9500\u51cf\u5c11 74%\uff0c\u5c06\u6d6e\u70b9\u8fd0\u7b97\uff08FLOPs\uff09\u51cf\u5c11 56%\u3002||\n", "2409.01816": "|**2024-09-03**|[GeoBEV: Learning Geometric BEV Representation for Multi-view 3D Object Detection](http://arxiv.org/abs/2409.01816)|null|\u9e1f\u77b0\u56fe (BEV) \u8868\u793a\u5df2\u6210\u4e3a\u591a\u89c6\u56fe 3D \u5bf9\u8c61\u68c0\u6d4b\u7684\u4e3b\u6d41\u8303\u5f0f\uff0c\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u611f\u77e5\u80fd\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86 BEV \u8868\u793a\u7684\u51e0\u4f55\u8d28\u91cf\uff0c\u4f7f\u5176\u5904\u4e8e\u4f4e\u5206\u8fa8\u7387\u72b6\u6001\uff0c\u65e0\u6cd5\u6062\u590d\u573a\u666f\u771f\u5b9e\u7684\u51e0\u4f55\u4fe1\u606f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u5148\u524d\u65b9\u6cd5\u53d7\u9650\u4e8e\u4f4e BEV \u8868\u793a\u5206\u8fa8\u7387\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u5f84\u5411-\u7b1b\u5361\u5c14 BEV \u91c7\u6837 (RC-Sampling)\uff0c\u4ece\u800c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u5bc6\u96c6 BEV \u8868\u793a\uff0c\u800c\u65e0\u9700\u590d\u6742\u7684\u7b97\u5b50\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u76d2\u5185\u6807\u7b7e\u6765\u66ff\u4ee3\u4ece\u6fc0\u5149\u96f7\u8fbe\u70b9\u751f\u6210\u7684\u4f20\u7edf\u6df1\u5ea6\u6807\u7b7e\u3002\u6b64\u6807\u7b7e\u53cd\u6620\u4e86\u5bf9\u8c61\u7684\u5b9e\u9645\u51e0\u4f55\u7ed3\u6784\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5b83\u4eec\u7684\u8868\u9762\uff0c\u5c06\u73b0\u5b9e\u4e16\u754c\u7684\u51e0\u4f55\u4fe1\u606f\u6ce8\u5165 BEV \u8868\u793a\u4e2d\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u76d2\u5185\u6807\u7b7e\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u8d28\u5fc3\u611f\u77e5\u5185\u90e8\u635f\u5931 (CAI \u635f\u5931) \u6765\u6355\u6349\u5bf9\u8c61\u7684\u7ec6\u7c92\u5ea6\u5185\u90e8\u51e0\u4f55\u7ed3\u6784\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c06\u4e0a\u8ff0\u6a21\u5757\u96c6\u6210\u5230\u4e00\u4e2a\u540d\u4e3a GeoBEV \u7684\u65b0\u578b\u591a\u89c6\u56fe 3D \u5bf9\u8c61\u68c0\u6d4b\u6846\u67b6\u4e2d\u3002\u5728 nuScenes \u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGeoBEV \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7a81\u51fa\u4e86\u5176\u6709\u6548\u6027\u3002||\n", "2409.03530": "|**2024-09-05**|[Use of triplet loss for facial restoration in low-resolution images](http://arxiv.org/abs/2409.03530)|null|\u8fd1\u5e74\u6765\uff0c\u4eba\u8138\u8bc6\u522b (FR) \u6a21\u578b\u5df2\u6210\u4e3a\u5e94\u7528\u6700\u5e7f\u6cdb\u7684\u751f\u7269\u8bc6\u522b\u5de5\u5177\uff0c\u5728\u4f17\u591a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u6210\u679c\u3002\u7136\u800c\uff0c\u786c\u4ef6\u7684\u56fa\u6709\u6311\u6218\u6216\u62cd\u6444\u8ddd\u79bb often \u5bfc\u81f4\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u8fd9\u4f1a\u4e25\u91cd\u5f71\u54cd\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4eba\u4eec\u63d0\u51fa\u4e86\u51e0\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u751f\u6210\u9ad8\u5ea6\u903c\u771f\u7684\u4eba\u8138\u7684\u8d85\u5206\u8fa8\u7387 (SR) \u6a21\u578b\u3002\u5c3d\u7ba1\u505a\u51fa\u4e86\u8fd9\u4e9b\u52aa\u529b\uff0c\u4f46\u4eba\u8138\u8bc6\u522b\u7b97\u6cd5\u5e76\u672a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8d85\u5206\u8fa8\u7387\u6a21\u578b FTLGAN\uff0c\u5b83\u4fa7\u91cd\u4e8e\u751f\u6210\u4fdd\u7559\u4e2a\u4eba\u8eab\u4efd\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\uff0c\u4ece\u800c\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u7684\u6027\u80fd\u3002\u7ed3\u679c\u4ee4\u4eba\u4fe1\u670d\uff0c\u8868\u660e d' \u7684\u5e73\u5747\u503c\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u9ad8\u51fa 21%\uff0c\u5177\u4f53\u800c\u8a00\uff0c14x14 \u50cf\u7d20\u65f6 d' = 1.099\uff0cAUC = 0.78\uff0c28x28 \u50cf\u7d20\u65f6 d' = 2.112\uff0cAUC = 0.92\uff0c56x56 \u50cf\u7d20\u65f6 d' = 3.049\uff0cAUC = 0.98\u3002\u8fd9\u9879\u7814\u7a76\u7684\u8d21\u732e\u5728\u51e0\u4e2a\u5173\u952e\u9886\u57df\u610f\u4e49\u91cd\u5927\u3002\u9996\u5148\uff0c\u5728\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff08\u7279\u522b\u662f 14x14\u300128x28 \u548c 56x56 \u50cf\u7d20\u7684\u5206\u8fa8\u7387\uff09\u4e2d\uff0c\u4eba\u8138\u8bc6\u522b\u6027\u80fd\u53d6\u5f97\u4e86\u663e\u7740\u63d0\u9ad8\u3002\u5176\u6b21\uff0cFTLGAN \u6240\u5c55\u793a\u7684\u589e\u5f3a\u529f\u80fd\u5728\u6240\u6709\u5206\u8fa8\u7387\u4e0b\u90fd\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u54cd\u5e94\uff0c\u4e0e\u5176\u4ed6\u6bd4\u8f83\u6a21\u578b\u4e0d\u540c\uff0c\u5b83\u59cb\u7ec8\u5982\u4e00\u5730\u63d0\u4f9b\u51fa\u8272\u7684\u6027\u80fd\u3002\u7b2c\u4e09\uff0c\u4f7f\u7528\u4e09\u5143\u7ec4\u635f\u5931\u903b\u8f91\u5b9e\u65bd\u4e86\u4e00\u79cd\u521b\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u4ec5\u4f7f\u7528\u771f\u5b9e\u56fe\u50cf\u8bad\u7ec3\u8d85\u5206\u8fa8\u7387\u6a21\u578b\uff0c\u8fd9\u4e0e\u5f53\u524d\u6a21\u578b\u5f62\u6210\u5bf9\u6bd4\uff0c\u5e76\u6269\u5c55\u4e86\u6f5c\u5728\u7684\u73b0\u5b9e\u5e94\u7528\u3002\u6700\u540e\uff0c\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5728\u6a21\u578b\u8bad\u7ec3\u671f\u95f4\u5c06\u4eba\u8138\u8bc6\u522b\u8d28\u91cf\u4f5c\u4e3a\u635f\u5931\u7eb3\u5165\u5176\u4e2d\uff0c\u4e13\u95e8\u89e3\u51b3\u4e86\u63d0\u9ad8\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5206\u7c7b\u6027\u80fd\u7684\u6311\u6218\u3002||\n", "2409.03521": "|**2024-09-05**|[Have Large Vision-Language Models Mastered Art History?](http://arxiv.org/abs/2409.03521)|null|\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u51fa\u73b0\u6700\u8fd1\u5728\u8de8\u591a\u4e2a\u9886\u57df\u7684\u56fe\u50cf\u5206\u7c7b\u65b9\u9762\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002\u7136\u800c\uff0cVLM \u5728\u827a\u672f\u54c1\u5206\u7c7b\u8fd9\u4e00\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u7ed8\u753b\u827a\u672f\u98ce\u683c\u5206\u7c7b\u2014\u2014\u4f20\u7edf\u4e0a\u7531\u827a\u672f\u53f2\u5b66\u5bb6\u638c\u63e1\u7684\u9886\u57df\u2014\u2014\u5c1a\u672a\u5f97\u5230\u63a2\u7d22\u3002\u4e0e\u81ea\u7136\u56fe\u50cf\u76f8\u6bd4\uff0c\u827a\u672f\u54c1\u7531\u4e8e\u5176\u56fa\u6709\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u7ed3\u6784\uff08\u4ee5\u591a\u53d8\u7684\u6784\u56fe\u548c\u98ce\u683c\u4e3a\u7279\u5f81\uff09\u800c\u6784\u6210\u4e86\u72ec\u7279\u7684\u6311\u6218\u3002\u827a\u672f\u53f2\u5b66\u5bb6\u957f\u671f\u4ee5\u6765\u4e00\u76f4\u5728\u7814\u7a76\u827a\u672f\u54c1\u7684\u72ec\u7279\u65b9\u9762\uff0c\u800c\u98ce\u683c\u9884\u6d4b\u662f\u5176\u5b66\u79d1\u7684\u4e00\u4e2a\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u3002\u672c\u6587\u7814\u7a76\u4e86\u96c6\u6210\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u7684\u5927\u578b VLM \u662f\u5426\u53ef\u4ee5\u6709\u6548\u5730\u9884\u6d4b\u7ed8\u753b\u7684\u827a\u672f\u53f2\u5c5e\u6027\u3002\u6211\u4eec\u5bf9\u56db\u79cd VLM\uff08\u5373 CLIP\u3001LLaVA\u3001OpenFlamingo \u548c GPT-4o\uff09\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u91cd\u70b9\u5173\u6ce8\u4f7f\u7528\u4e24\u4e2a\u516c\u5171\u827a\u672f\u54c1\u57fa\u51c6\u5bf9\u827a\u672f\u98ce\u683c\u3001\u4f5c\u8005\u548c\u65f6\u95f4\u6bb5\u8fdb\u884c\u96f6\u6837\u672c\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86 ArTest\uff0c\u8fd9\u662f\u4e00\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u827a\u672f\u54c1\u6d4b\u8bd5\u96c6\uff0c\u5176\u4e2d\u5305\u62ec\u827a\u672f\u53f2\u5b66\u5bb6\u7814\u7a76\u7684\u5173\u952e\u7ed8\u753b\u4f5c\u54c1\u3002||\n", "2409.03516": "|**2024-09-05**|[LMLT: Low-to-high Multi-Level Vision Transformer for Image Super-Resolution](http://arxiv.org/abs/2409.03516)|**[link](https://github.com/jwgdmkj/lmlt)**|\u8fd1\u5e74\u6765\uff0c\u57fa\u4e8e\u89c6\u89c9Transformer (ViT) \u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5b58\u5728\u590d\u6742\u6027\u9ad8\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u63a8\u7406\u65f6\u95f4\u548c\u5185\u5b58\u4f7f\u7528\u91cf\u5927\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u673a\u5236(WSA) \u7684ViT\u6a21\u578b\u5728\u5904\u7406\u7a97\u53e3\u533a\u57df\u5916\u7684\u4fe1\u606f\u65f6\u9762\u4e34\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4f4e\u5230\u9ad8\u591a\u7ea7Transformer (LMLT)\uff0c\u5b83\u5bf9\u6bcf\u4e2a\u5934\u91c7\u7528\u4e0d\u540c\u7279\u5f81\u5927\u5c0f\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002LMLT \u6cbf\u901a\u9053\u7ef4\u5ea6\u5212\u5206\u56fe\u50cf\u7279\u5f81\uff0c\u9010\u6e10\u51cf\u5c0f\u4f4e\u5c42\u5934\u7684\u7a7a\u95f4\u5927\u5c0f\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u5934\u5e94\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3002\u8fd9\u79cd\u65b9\u6cd5\u6709\u6548\u5730\u6355\u83b7\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u3002\u901a\u8fc7\u5c06\u4f4e\u5c42\u5934\u7684\u7ed3\u679c\u6574\u5408\u5230\u9ad8\u5c42\u5934\u4e2d\uff0cLMLT \u514b\u670d\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u7a97\u53e3\u8fb9\u754c\u95ee\u9898\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u4fdd\u6301\u751a\u81f3\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u57fa\u4e8e ViT \u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u7684\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\u548c GPU \u5185\u5b58\u4f7f\u7528\u91cf\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/jwgdmkj/LMLT \u83b7\u53d6\u3002||\n", "2409.03458": "|**2024-09-05**|[Non-Uniform Illumination Attack for Fooling Convolutional Neural Networks](http://arxiv.org/abs/2409.03458)|**[link](https://github.com/Akshayjain97/Non-Uniform_Illumination)**|\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u867d\u7136\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u653b\u51fb\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u4eba\u7c7b\u5bb9\u6613\u8bc6\u522b\u7684\u5fae\u5c0f\u56fe\u50cf\u6270\u52a8\u65f6\u3002\u8fd9\u79cd\u5f31\u70b9\u901a\u5e38\u88ab\u79f0\u4e3a\u201c\u653b\u51fb\u201d\uff0c\u7a81\u663e\u4e86CNN\u7684\u9c81\u68d2\u6027\u6709\u9650\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u589e\u5f3a\u5176\u62b5\u6297\u6b64\u7c7b\u64cd\u7eb5\u7684\u80fd\u529b\u3002\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u975e\u5747\u5300\u7167\u660e\uff08NUI\uff09\u653b\u51fb\u6280\u672f\uff0c\u8be5\u6280\u672f\u4f7f\u7528\u4e0d\u540c\u7684NUI\u63a9\u7801\u5bf9\u56fe\u50cf\u8fdb\u884c\u7ec6\u5fae alteration\u3002\u6211\u4eec\u5728\u5e7f\u6cdb\u63a5\u53d7\u7684\u6570\u636e\u96c6\uff08\u5305\u62ecCIFAR10\u3001TinyImageNet\u548cCalTech256\uff09\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u91cd\u70b9\u5173\u6ce812\u79cd\u4e0d\u540cNUI\u653b\u51fb\u6a21\u578b\u7684\u56fe\u50cf\u5206\u7c7b\u3002\u8bc4\u4f30\u4e86VGG\u3001ResNet\u3001MobilenetV3-small\u548cInceptionV3\u6a21\u578b\u5bf9NUI\u653b\u51fb\u7684\u62b5\u6297\u529b\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cCNN\u6a21\u578b\u5728\u906d\u53d7NUI\u653b\u51fb\u65f6\uff0c\u5206\u7c7b\u7cbe\u5ea6\u5927\u5e45\u4e0b\u964d\uff0c\u8868\u660e\u5b83\u4eec\u5728\u975e\u5747\u5300\u7167\u660e\u4e0b\u7684\u8106\u5f31\u6027\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u79cd\u60c5\u51b5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u5fa1\u7b56\u7565\uff0c\u5c06\u901a\u8fc7\u65b0\u7684NUI\u53d8\u6362\u751f\u6210\u7684NUI\u653b\u51fb\u56fe\u50cf\u5305\u542b\u5230\u8bad\u7ec3\u96c6\u4e2d\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5f53CNN\u6a21\u578b\u9762\u5bf9\u53d7NUI\u653b\u51fb\u5f71\u54cd\u7684\u6270\u52a8\u56fe\u50cf\u65f6\uff0c\u5176\u6027\u80fd\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002\u8be5\u7b56\u7565\u65e8\u5728\u589e\u5f3aCNN\u6a21\u578b\u5bf9NUI\u653b\u51fb\u7684\u62b5\u6297\u529b\u3002||\n", "2409.03377": "|**2024-09-05**|[Raw Speech Enhancement with Deep State Space Modeling](http://arxiv.org/abs/2409.03377)|**[link](https://github.com/Brainchip-Inc/aTENNuate)**|\u6211\u4eec\u63d0\u51fa\u4e86 aTENNuate\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u7684\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u81ea\u7f16\u7801\u5668\uff0c\u4e13\u4e3a\u9ad8\u6548\u7684\u5728\u7ebf\u539f\u59cb\u8bed\u97f3\u589e\u5f3a\u800c\u914d\u7f6e\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u3002\u8be5\u7f51\u7edc\u7684\u6027\u80fd\u4e3b\u8981\u5728\u539f\u59cb\u8bed\u97f3\u53bb\u566a\u65b9\u9762\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5728\u8d85\u5206\u8fa8\u7387\u548c\u53bb\u91cf\u5316\u7b49\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u989d\u5916\u8bc4\u4f30\u3002\u6211\u4eec\u5728 VoiceBank + DEMAND \u548c Microsoft DNS1 \u5408\u6210\u6d4b\u8bd5\u96c6\u4e0a\u5bf9 aTENNuate \u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u8be5\u7f51\u7edc\u5728 PESQ \u5206\u6570\u3001\u53c2\u6570\u6570\u91cf\u3001MAC \u548c\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8e\u4ee5\u524d\u7684\u5b9e\u65f6\u53bb\u566a\u6a21\u578b\u3002\u5373\u4f7f\u4f5c\u4e3a\u539f\u59cb\u6ce2\u5f62\u5904\u7406\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4e5f\u80fd\u4fdd\u6301\u5bf9\u5e72\u51c0\u4fe1\u53f7\u7684\u9ad8\u4fdd\u771f\u5ea6\uff0c\u5e76\u4e14\u53ef\u542c\u89c1\u7684\u4f2a\u5f71\u6781\u5c11\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u5c06\u566a\u58f0\u8f93\u5165\u538b\u7f29\u81f3 4000Hz \u548c 4 \u4f4d\uff0c\u8be5\u6a21\u578b\u4ecd\u80fd\u4fdd\u6301\u826f\u597d\u7684\u6027\u80fd\uff0c\u8fd9\u8868\u660e\u5b83\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u5177\u6709\u4e00\u822c\u7684\u8bed\u97f3\u589e\u5f3a\u80fd\u529b\u3002||\n", "2409.03368": "|**2024-09-05**|[Training-free Conversion of Pretrained ANNs to SNNs for Low-Power and High-Performance Applications](http://arxiv.org/abs/2409.03368)|null|\u8109\u51b2\u795e\u7ecf\u7f51\u7edc (SNN) \u7531\u4e8e\u5176\u63a8\u7406\u901f\u5ea6\u5feb\u3001\u529f\u8017\u4f4e\u7b49\u4f18\u52bf\uff0c\u5df2\u6210\u4e3a\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc (ANN) \u7684\u4e00\u79cd\u5f88\u6709\u524d\u9014\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u8bad\u7ec3\u7b97\u6cd5\u963b\u788d\u4e86\u5b83\u4eec\u7684\u5e7f\u6cdb\u5e94\u7528\u3002\u73b0\u6709\u7684 SNN \u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\u6bd4 ANN \u9700\u8981\u66f4\u591a\u7684\u5185\u5b58\u548c\u65f6\u95f4\u3002\u5373\u4f7f\u662f\u5e38\u7528\u7684 ANN-SNN \u8f6c\u6362\u65b9\u6cd5\u4e5f\u9700\u8981\u91cd\u65b0\u8bad\u7ec3 ANN \u4ee5\u63d0\u9ad8\u8f6c\u6362\u6548\u7387\uff0c\u4ece\u800c\u4ea7\u751f\u989d\u5916\u7684\u8ba1\u7b97\u6210\u672c\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u514d\u8bad\u7ec3 ANN-SNN \u8f6c\u6362\u6d41\u7a0b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u9884\u5148\u8bad\u7ec3\u597d\u7684 ANN \u6a21\u578b\u76f4\u63a5\u8f6c\u6362\u4e3a\u9ad8\u6027\u80fd SNN\uff0c\u65e0\u9700\u989d\u5916\u7684\u8bad\u7ec3\u3002\u8be5\u8f6c\u6362\u6d41\u7a0b\u5305\u62ec\u4e00\u4e2a\u57fa\u4e8e\u5c40\u90e8\u5b66\u4e60\u7684\u9608\u503c\u5e73\u8861\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u8ba1\u7b97\u6700\u4f73\u9608\u503c\u5e76\u901a\u8fc7\u901a\u9053\u7f29\u653e\u5bf9\u9608\u503c\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8c03\u6574\u3002\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u6846\u67b6\u5728\u4e09\u4e2a\u5178\u578b\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u53ef\u6269\u5c55\u6027\uff1a\u56fe\u50cf\u5206\u7c7b\u3001\u8bed\u4e49\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\u3002\u8fd9\u5c55\u793a\u4e86\u5176\u5bf9\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u7684\u9002\u7528\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u8f6c\u6362\u540e\u7684 SNN \u7684\u80fd\u8017\uff0c\u8bc1\u660e\u4e86\u5b83\u4eec\u4e0e\u4f20\u7edf ANN \u76f8\u6bd4\u5177\u6709\u4f18\u8d8a\u7684\u4f4e\u529f\u8017\u4f18\u52bf\u3002\u6211\u4eec\u7684\u514d\u8bad\u7ec3\u7b97\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7a81\u51fa\u4e86\u5176\u5b9e\u7528\u6027\u548c\u6548\u7387\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u5f00\u6e90\u9884\u8bad\u7ec3 ANN \u6a21\u578b\u548c\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u7b80\u5316\u4e86 SNN \u7684\u90e8\u7f72\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u4f4e\u529f\u8017\u7684\u63a8\u7406\uff0c\u5e76\u4e14\u6027\u80fd\u635f\u5931\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u3002||\n", "2409.03320": "|**2024-09-05**|[YOLO-PPA based Efficient Traffic Sign Detection for Cruise Control in Autonomous Driving](http://arxiv.org/abs/2409.03320)|null|\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u9ad8\u6548\u3001\u51c6\u786e\u5730\u68c0\u6d4b\u4ea4\u901a\u6807\u5fd7\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8ddd\u79bb\u8d8a\u8fdc\uff0c\u4ea4\u901a\u6807\u5fd7\u8d8a\u5c0f\u3002\u73b0\u6709\u7684\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u5f88\u96be\u68c0\u6d4b\u5230\u8fd9\u4e9b\u5c0f\u5c3a\u5bf8\u7684\u6807\u5fd7\u3002\u6b64\u5916\uff0c\u8f66\u8f7d\u5d4c\u5165\u5f0f\u8bbe\u5907\u7684\u6027\u80fd\u9650\u5236\u4e86\u68c0\u6d4b\u6a21\u578b\u7684\u89c4\u6a21\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e YOLO PPA \u7684\u4ea4\u901a\u6807\u5fd7\u68c0\u6d4b\u7b97\u6cd5\u3002\u5728 GTSDB \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u539f\u59cb YOLO \u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5c06\u63a8\u7406\u6548\u7387\u63d0\u9ad8\u4e86 11.2%\uff0cmAP 50 \u4e5f\u63d0\u9ad8\u4e86 93.2%\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684 YOLO PPA \u7684\u6709\u6548\u6027\u3002||\n", "2409.03192": "|**2024-09-05**|[PEPL: Precision-Enhanced Pseudo-Labeling for Fine-Grained Image Classification in Semi-Supervised Learning](http://arxiv.org/abs/2409.03192)|null|\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u7684\u51fa\u73b0\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u6b65\u3002\u7136\u800c\uff0c\u8be6\u7ec6\u6807\u6ce8\u7684\u7f3a\u4e4f\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u83b7\u53d6\u9ad8\u8d28\u91cf\u6807\u8bb0\u6570\u636e\u7684\u6210\u672c\u9ad8\u6602\u6216\u8017\u65f6\u7684\u60c5\u51b5\u4e0b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e13\u4e3a\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u5185\u7684\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u8bbe\u8ba1\u7684\u7cbe\u5ea6\u589e\u5f3a\u578b\u4f2a\u6807\u7b7e\uff08PEPL\uff09\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4f2a\u6807\u7b7e\u6765\u5229\u7528\u4e30\u5bcc\u7684\u672a\u6807\u8bb0\u6570\u636e\uff0c\u8fd9\u4e9b\u4f2a\u6807\u7b7e\u901a\u8fc7\u4e24\u4e2a\u5173\u952e\u9636\u6bb5\u9010\u6b65\u7ec6\u5316\uff1a\u521d\u59cb\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u8bed\u4e49\u6df7\u5408\u4f2a\u6807\u7b7e\u751f\u6210\u3002\u8fd9\u4e9b\u9636\u6bb5\u5229\u7528\u7c7b\u6fc0\u6d3b\u56fe\uff08CAM\uff09\u6765\u51c6\u786e\u4f30\u8ba1\u8bed\u4e49\u5185\u5bb9\u5e76\u751f\u6210\u7ec6\u5316\u6807\u7b7e\uff0c\u8fd9\u4e9b\u6807\u7b7e\u6355\u83b7\u4e86\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6240\u9700\u7684\u57fa\u672c\u7ec6\u8282\u3002\u901a\u8fc7\u5173\u6ce8\u8bed\u4e49\u7ea7\u4fe1\u606f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u6807\u51c6\u6570\u636e\u589e\u5f3a\u548c\u56fe\u50cf\u6df7\u5408\u6280\u672f\u5728\u4fdd\u7559\u5173\u952e\u7ec6\u7c92\u5ea6\u7279\u5f81\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002\u6211\u4eec\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u76f8\u5bf9\u4e8e\u73b0\u6709\u534a\u76d1\u7763\u7b56\u7565\u7684\u663e\u8457\u6539\u8fdb\uff0c\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u5728https://github.com/TianSuya/SemiFG\u5f00\u6e90\u3002||\n", "2409.03137": "|**2024-09-05**|[The AdEMAMix Optimizer: Better, Faster, Older](http://arxiv.org/abs/2409.03137)|**[link](https://github.com/apple/ml-ademamix)**|\u57fa\u4e8e\u52a8\u91cf\u7684\u4f18\u5316\u5668\u662f\u4f17\u591a\u673a\u5668\u5b66\u4e60\u5e94\u7528\u7684\u6838\u5fc3\u3002\u8fd9\u4e9b\u4f18\u5316\u5668\u901a\u5e38\u4f9d\u8d56\u4e8e\u68af\u5ea6\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747 (EMA)\uff0c\u5b83\u4f1a\u4ee5\u6307\u6570\u65b9\u5f0f\u8870\u51cf\u65e7\u68af\u5ea6\u5bf9\u5f53\u524d\u68af\u5ea6\u7684\u8d21\u732e\u3002\u8fd9\u662f\u56e0\u4e3a\u68af\u5ea6\u662f\u5c40\u90e8\u7684\u7ebf\u6027\u8fd1\u4f3c\uff0c\u5f53\u8fed\u4ee3\u70b9\u5728\u635f\u5931\u51fd\u6570\u66f2\u9762\u4e0a\u79fb\u52a8\u65f6\uff0c\u65e7\u68af\u5ea6\u7684\u76f8\u5173\u6027\u4f1a\u964d\u4f4e\u3002\u8fd9\u9879\u5de5\u4f5c\u5bf9\u4f7f\u7528\u5355\u4e2a EMA \u6765\u7d2f\u79ef\u8fc7\u53bb\u68af\u5ea6\u7684\u505a\u6cd5\u63d0\u51fa\u4e86\u8d28\u7591\uff0c\u5e76\u901a\u8fc7\u7ecf\u9a8c\u8bc1\u660e\u4e86\u8fd9\u79cd\u9009\u62e9\u53ef\u80fd\u662f\u6b21\u4f18\u7684\uff1a\u5355\u4e2a EMA \u65e0\u6cd5\u540c\u65f6\u5bf9\u6700\u8fd1\u7684\u68af\u5ea6\u8d4b\u4e88\u9ad8\u6743\u91cd\uff0c\u5e76\u5bf9\u8f83\u65e7\u7684\u68af\u5ea6\u8d4b\u4e88\u4e0d\u53ef\u5ffd\u7565\u7684\u6743\u91cd\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c2\u5bdf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 AdEMAMix\uff0c\u5b83\u662f\u5bf9 Adam \u4f18\u5316\u5668\u7684\u4e00\u79cd\u7b80\u5355\u4fee\u6539\uff0c\u5b83\u6df7\u5408\u4e86\u4e24\u4e2a EMA\uff0c\u4ee5\u66f4\u597d\u5730\u5229\u7528\u8fc7\u53bb\u7684\u68af\u5ea6\u3002\u6211\u4eec\u5728\u8bed\u8a00\u5efa\u6a21\u548c\u56fe\u50cf\u5206\u7c7b\u65b9\u9762\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u68af\u5ea6\u5728\u6570\u4e07\u6b65\u5185\u4ecd\u7136\u5177\u6709\u76f8\u5173\u6027\u3002\u5b83\u4eec\u6709\u52a9\u4e8e\u66f4\u5feb\u5730\u6536\u655b\uff0c\u5e76\u4e14\u901a\u5e38\u6536\u655b\u5230\u66f4\u4f4e\u7684\u6700\u5c0f\u503c\uff1a\u4f8b\u5982\uff0c\u4e00\u4e2a\u5728 1010 \u4ebf\u4e2a\u8bcd\u7b26\u4e0a\u8bad\u7ec3\u7684\u5177\u6709 13 \u4ebf\u4e2a\u53c2\u6570\u7684 AdEMAMix LLM \u7684\u6027\u80fd\u4e0e\u5728\u4e00\u4e2a 1970 \u4ebf\u4e2a\u8bcd\u7b26\u4e0a\u8bad\u7ec3\u7684 AdamW \u6a21\u578b\u76f8\u5f53\uff08+95%\uff09\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u51cf\u7f13\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6a21\u578b\u9057\u5fd8\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u9f13\u52b1\u8fdb\u4e00\u6b65\u63a2\u7d22\u5229\u7528\u8fc7\u53bb\u68af\u5ea6\u7684\u4e0d\u540c\u7c7b\u578b\u7684\u51fd\u6570\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f EMA\u3002||\n", "2409.03022": "|**2024-09-04**|[Boundless: Generating Photorealistic Synthetic Data for Object Detection in Urban Streetscapes](http://arxiv.org/abs/2409.03022)|**[link](https://github.com/zk2172-columbia/boundless)**|\u6211\u4eec\u4ecb\u7ecdBoundless\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u5bc6\u96c6\u7684\u57ce\u5e02\u8857\u666f\u4e2d\u5b9e\u73b0\u9ad8\u5ea6\u51c6\u786e\u7684\u76ee\u6807\u68c0\u6d4b\u7684\u903c\u771f\u5408\u6210\u6570\u636e\u751f\u6210\u7cfb\u7edf\u3002Boundless\u53ef\u4ee5\u7528\u81ea\u52a8\u5316\u548c\u53ef\u914d\u7f6e\u7684\u8fc7\u7a0b\u53d6\u4ee3\u5927\u89c4\u6a21\u7684\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u6536\u96c6\u548c\u624b\u52a8\u5730\u9762\u5b9e\u51b5\u76ee\u6807\u6ce8\u91ca\uff08\u6807\u8bb0\uff09\u3002Boundless\u57fa\u4e8e\u865a\u5e7b\u5f15\u64ce5 (UE5) \u57ce\u5e02\u793a\u4f8b\u9879\u76ee\uff0c\u5e76\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u7684\u7167\u660e\u548c\u573a\u666f\u53d8\u5316\u6761\u4ef6\u4e0b\u51c6\u786e\u6536\u96c63D\u8fb9\u754c\u6846\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u5728Boundless\u751f\u6210\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u4ece\u4e2d\u7a7a\u76f8\u673a\u83b7\u53d6\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u63a8\u7406\u65f6\u7684\u6027\u80fd\u3002\u6211\u4eec\u5c06Boundless\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u4e0eCARLA\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u89c2\u5bdf\u52307.8 mAP\u7684\u6539\u8fdb\u3002\u6211\u4eec\u53d6\u5f97\u7684\u7ed3\u679c\u652f\u6301\u4e86\u5408\u6210\u6570\u636e\u751f\u6210\u662f\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u7528\u4e8e\u8bad\u7ec3/\u5fae\u8c03\u7528\u4e8e\u57ce\u5e02\u573a\u666f\u7684\u53ef\u6269\u5c55\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u3002||\n", "2409.05650": "|**2024-09-09**|[Replay Consolidation with Label Propagation for Continual Object Detection](http://arxiv.org/abs/2409.05650)|null|\u76ee\u6807\u68c0\u6d4b\u662f\u4e00\u4e2a\u4e0e\u673a\u5668\u4eba\u6280\u672f\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u8bb8\u591a\u5e94\u7528\u9ad8\u5ea6\u76f8\u5173\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u95ee\u9898\u3002\u6301\u7eed\u5b66\u4e60 (CL) \u8003\u8651\u7684\u662f\u6a21\u578b\u5728\u4fdd\u7559\u5148\u524d\u83b7\u5f97\u7684\u77e5\u8bc6\u7684\u540c\u65f6\u9010\u6b65\u5b66\u4e60\u65b0\u4fe1\u606f\u7684\u8bbe\u7f6e\u3002\u8fd9\u5c24\u5176\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8bad\u7ec3\u65b0\u6570\u636e\u65f6\u5f80\u5f80\u4f1a\u707e\u96be\u6027\u5730\u5fd8\u8bb0\u65e7\u77e5\u8bc6\u3002\u7279\u522b\u662f\uff0c\u4e0e\u7528\u4e8e\u5206\u7c7b\u7684\u6301\u7eed\u5b66\u4e60\u76f8\u6bd4\uff0c\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\u7684\u6301\u7eed\u5b66\u4e60 (CLOD) \u5e26\u6765\u4e86\u989d\u5916\u7684\u56f0\u96be\u3002\u5728 CLOD \u4e2d\uff0c\u6765\u81ea\u5148\u524d\u4efb\u52a1\u7684\u56fe\u50cf\u53ef\u80fd\u5305\u542b\u672a\u77e5\u7684\u7c7b\u522b\uff0c\u8fd9\u4e9b\u7c7b\u522b\u53ef\u80fd\u4f1a\u5728\u672a\u6765\u7684\u4efb\u52a1\u4e2d\u91cd\u65b0\u51fa\u73b0\u5e76\u88ab\u6807\u8bb0\u3002\u8fd9\u4e9b\u7f3a\u5931\u7684\u6ce8\u91ca\u4f1a\u5bfc\u81f4\u57fa\u4e8e\u91cd\u653e\u7684\u65b9\u6cd5\u51fa\u73b0\u4efb\u52a1\u5e72\u6270\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u6587\u732e\u4e2d\u7684\u5927\u591a\u6570\u5de5\u4f5c\u90fd\u96c6\u4e2d\u5728\u57fa\u4e8e\u84b8\u998f\u7684\u65b9\u6cd5\u4e0a\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u53ea\u6709\u5728\u4e0d\u540c\u4efb\u52a1\u4e4b\u95f4\u5b58\u5728\u5f3a\u5927\u7684\u7c7b\u522b\u91cd\u53e0\u65f6\u624d\u6709\u6548\u3002\u4e3a\u4e86\u89e3\u51b3\u5f53\u524d\u65b9\u6cd5\u7684\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3 CLOD \u7684\u65b0\u6280\u672f\uff0c\u79f0\u4e3a\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\u7684\u6807\u7b7e\u4f20\u64ad\u91cd\u653e\u6574\u5408 (RCLPOD)\u3002\u57fa\u4e8e\u91cd\u653e\u65b9\u6cd5\uff0c\u6211\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u901a\u8fc7\u589e\u5f3a\u7f13\u51b2\u533a\u5185\u5b58\u6837\u672c\u6765\u907f\u514d\u4efb\u52a1\u5e72\u6270\u95ee\u9898\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728 CLOD \u6587\u732e\u4e2d\u7684\u73b0\u6709\u6280\u672f\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5176\u5728 VOC \u548c COCO \u7b49\u65e2\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002||\n", "2409.05564": "|**2024-09-09**|[LEROjD: Lidar Extended Radar-Only Object Detection](http://arxiv.org/abs/2409.05564)|**[link](https://github.com/rst-tu-dortmund/lerojd)**|\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u800c\u8a00\uff0c\u7cbe\u786e\u7684\u4e09\u7ef4\u7269\u4f53\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u6fc0\u5149\u96f7\u8fbe\u4f20\u611f\u5668\u975e\u5e38\u9002\u5408\u8fd9\u9879\u4efb\u52a1\uff0c\u4f46\u5b83\u4eec\u4ef7\u683c\u6602\u8d35\uff0c\u5e76\u4e14\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u5b58\u5728\u5c40\u9650\u6027\u30023+1D \u6210\u50cf\u96f7\u8fbe\u4f20\u611f\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u7531\u4e8e\u5176\u5206\u8fa8\u7387\u4f4e\u548c\u6d4b\u91cf\u566a\u58f0\u9ad8\u800c\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u7684 3+1D \u6210\u50cf\u96f7\u8fbe\u6570\u636e\u96c6\u5305\u62ec\u96f7\u8fbe\u548c\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\uff0c\u53ef\u4ee5\u6539\u8fdb\u8de8\u6a21\u6001\u6a21\u578b\u3002\u5c3d\u7ba1\u4e0d\u5e94\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u6fc0\u5149\u96f7\u8fbe\uff0c\u4f46\u5b83\u53ef\u4ee5\u5e2e\u52a9\u8bad\u7ec3\u4ec5\u4f7f\u7528\u96f7\u8fbe\u7684\u7269\u4f53\u68c0\u6d4b\u5668\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u4e24\u79cd\u5c06\u77e5\u8bc6\u4ece\u6fc0\u5149\u96f7\u8fbe\u57df\u8fc1\u79fb\u5230\u96f7\u8fbe\u57df\u548c\u4ec5\u4f7f\u7528\u96f7\u8fbe\u7684\u7269\u4f53\u68c0\u6d4b\u5668\u7684\u7b56\u7565\uff1a1. \u4f7f\u7528\u987a\u5e8f\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u7ec6\u5316\u7684\u591a\u9636\u6bb5\u8bad\u7ec3\uff0c\u4ee5\u53ca 2. \u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u3002\u5728\u591a\u9636\u6bb5\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u4e09\u79cd\u7ec6\u5316\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\uff0c\u5e73\u5747\u7cbe\u5ea6 (mAP) \u663e\u7740\u63d0\u9ad8\u4e86 4.2 \u4e2a\u767e\u5206\u70b9\uff0c\u901a\u8fc7\u4f7f\u7528\u6559\u5e08\u6a21\u578b\u7684\u6743\u91cd\u521d\u59cb\u5316\u5b66\u751f\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\uff0c\u5e73\u5747\u7cbe\u5ea6\u63d0\u9ad8\u4e86 3.9 \u4e2a\u767e\u5206\u70b9\u3002\u8fd9\u4e9b\u65b9\u6cd5\u7684\u4e3b\u8981\u4f18\u70b9\u662f\u5b83\u4eec\u9002\u7528\u4e8e\u5176\u4ed6 3D \u7269\u4f53\u68c0\u6d4b\u7f51\u7edc\uff0c\u800c\u65e0\u9700\u6539\u53d8\u5176\u67b6\u6784\uff0c\u6b63\u5982\u6211\u4eec\u901a\u8fc7\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u7269\u4f53\u68c0\u6d4b\u5668\u4e0a\u8fdb\u884c\u5206\u6790\u6240\u5c55\u793a\u7684\u90a3\u6837\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/rst-tu-dortmund/lerojd \u83b7\u53d6\u3002||\n", "2409.05162": "|**2024-09-08**|[Can OOD Object Detectors Learn from Foundation Models?](http://arxiv.org/abs/2409.05162)|**[link](https://github.com/cvmi-lab/syncood)**|Out-of-distribution (OOD) object detection is a challenging task due to the absence of open-set OOD data. Inspired by recent advancements in text-to-image generative models, such as Stable Diffusion, we study the potential of generative models trained on large-scale open-set data to synthesize OOD samples, thereby enhancing OOD object detection. We introduce SyncOOD, a simple data curation method that capitalizes on the capabilities of large foundation models to automatically extract meaningful OOD data from text-to-image generative models. This offers the model access to open-world knowledge encapsulated within off-the-shelf foundation models. The synthetic OOD samples are then employed to augment the training of a lightweight, plug-and-play OOD detector, thus effectively optimizing the in-distribution (ID)/OOD decision boundaries. Extensive experiments across multiple benchmarks demonstrate that SyncOOD significantly outperforms existing methods, establishing new state-of-the-art performance with minimal synthetic data usage.||\n", "2409.04999": "|**2024-09-08**|[Visual Grounding with Multi-modal Conditional Adaptation](http://arxiv.org/abs/2409.04999)|**[link](https://github.com/mr-bigworth/mmca)**|Visual grounding is the task of locating objects specified by natural language expressions. Existing methods extend generic object detection frameworks to tackle this task. They typically extract visual and textual features separately using independent visual and textual encoders, then fuse these features in a multi-modal decoder for final prediction. However, visual grounding presents unique challenges. It often involves locating objects with different text descriptions within the same image. Existing methods struggle with this task because the independent visual encoder produces identical visual features for the same image, limiting detection performance. Some recently approaches propose various language-guided visual encoders to address this issue, but they mostly rely solely on textual information and require sophisticated designs. In this paper, we introduce Multi-modal Conditional Adaptation (MMCA), which enables the visual encoder to adaptively update weights, directing its focus towards text-relevant regions. Specifically, we first integrate information from different modalities to obtain multi-modal embeddings. Then we utilize a set of weighting coefficients, which generated from the multimodal embeddings, to reorganize the weight update matrices and apply them to the visual encoder of the visual grounding model. Extensive experiments on four widely used datasets demonstrate that MMCA achieves significant improvements and state-of-the-art results. Ablation experiments further demonstrate the lightweight and efficiency of our method. Our source code is available at: https://github.com/Mr-Bigworth/MMCA.||\n", "2409.04979": "|**2024-09-08**|[RCBEVDet++: Toward High-accuracy Radar-Camera Fusion 3D Perception Network](http://arxiv.org/abs/2409.04979)|null|Perceiving the surrounding environment is a fundamental task in autonomous driving. To obtain highly accurate perception results, modern autonomous driving systems typically employ multi-modal sensors to collect comprehensive environmental data. Among these, the radar-camera multi-modal perception system is especially favored for its excellent sensing capabilities and cost-effectiveness. However, the substantial modality differences between radar and camera sensors pose challenges in fusing information. To address this problem, this paper presents RCBEVDet, a radar-camera fusion 3D object detection framework. Specifically, RCBEVDet is developed from an existing camera-based 3D object detector, supplemented by a specially designed radar feature extractor, RadarBEVNet, and a Cross-Attention Multi-layer Fusion (CAMF) module. Firstly, RadarBEVNet encodes sparse radar points into a dense bird's-eye-view (BEV) feature using a dual-stream radar backbone and a Radar Cross Section aware BEV encoder. Secondly, the CAMF module utilizes a deformable attention mechanism to align radar and camera BEV features and adopts channel and spatial fusion layers to fuse them. To further enhance RCBEVDet's capabilities, we introduce RCBEVDet++, which advances the CAMF through sparse fusion, supports query-based multi-view camera perception models, and adapts to a broader range of perception tasks. Extensive experiments on the nuScenes show that our method integrates seamlessly with existing camera-based 3D perception models and improves their performance across various perception tasks. Furthermore, our method achieves state-of-the-art radar-camera fusion results in 3D object detection, BEV semantic segmentation, and 3D multi-object tracking tasks. Notably, with ViT-L as the image backbone, RCBEVDet++ achieves 72.73 NDS and 67.34 mAP in 3D object detection without test-time augmentation or model ensembling.||\n", "2409.04975": "|**2024-09-08**|[PatchAlign:Fair and Accurate Skin Disease Image Classification by Alignment with Clinical Labels](http://arxiv.org/abs/2409.04975)|**[link](https://github.com/aayushmanace/patchalign24)**|\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u76ae\u80a4\u75c5\u53d8\u8bca\u65ad\u81ea\u52a8\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\u3002\u7136\u800c\uff0c\u5728\u90e8\u7f72\u8fd9\u4e9b\u6a21\u578b\u4e4b\u524d\uff0c\u9700\u8981\u89e3\u51b3\u5176\u9884\u6d4b\u4e2d\u5b58\u5728\u7684\u79cd\u65cf\u5dee\u5f02\u95ee\u9898\u3002\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a PatchAlign \u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0e\u76ae\u80a4\u75c5\u4e34\u5e8a\u6587\u672c\u8868\u5f81\u5bf9\u9f50\u6765\u63d0\u9ad8\u76ae\u80a4\u75c5\u56fe\u50cf\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u3002PatchAlign \u4f7f\u7528\u56fe\u6700\u4f18\u4f20\u8f93 (GOT) \u635f\u5931\u4f5c\u4e3a\u6b63\u5219\u5316\u5668\u6765\u6267\u884c\u8de8\u57df\u5bf9\u9f50\u3002\u5373\u4f7f\u5728\u8bad\u7ec3\u6837\u672c\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u83b7\u5f97\u7684\u8868\u5f81\u4e5f\u662f\u7a33\u5065\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u5f88\u597d\u5730\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u80a4\u8272\u3002\u4e3a\u4e86\u51cf\u5c11\u4e34\u5e8a\u76ae\u80a4\u75c5\u56fe\u50cf\u4e2d\u566a\u58f0\u548c\u4f2a\u5f71\u7684\u5f71\u54cd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u63a9\u7801\u56fe\u6700\u4f18\u4f20\u8f93\uff0c\u7528\u4e8e\u8de8\u57df\u5bf9\u9f50\uff0c\u8fdb\u4e00\u6b65\u6539\u5584\u4e86\u516c\u5e73\u6027\u6307\u6807\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u5177\u6709\u4e0d\u540c\u76ae\u80a4\u7c7b\u578b\u7684\u76ae\u80a4\u75c5\u53d8\u6570\u636e\u96c6\u4e0a\u5c06\u6211\u4eec\u7684\u6a21\u578b\u4e0e\u6700\u5148\u8fdb\u7684 FairDisCo \u8fdb\u884c\u4e86\u6bd4\u8f83\uff1aFitzpatrick17k \u548c Diverse Dermatology Images (DDI)\u3002\u4e0e FairDisCo \u76f8\u6bd4\uff0cPatchAlign \u5728 Fitzpatrick17k \u4e0a\u5c06\u76ae\u80a4\u75c5\u56fe\u50cf\u5206\u7c7b\u7684\u51c6\u786e\u6027\u63d0\u9ad8\u4e86 2.8%\uff08\u57df\u5185\uff09\u548c 6.2%\uff08\u8de8\u57df\uff09\uff0c\u5728 DDI \u4e0a\u63d0\u9ad8\u4e86 4.2%\uff08\u57df\u5185\uff09\u3002\u6b64\u5916\uff0c\u5b83\u6301\u7eed\u6539\u5584\u4e86\u4e0d\u540c\u80a4\u8272\u771f\u5b9e\u9633\u6027\u7387\u7684\u516c\u5e73\u6027\u3002\u7528\u4e8e\u5b9e\u73b0\u7684\u6e90\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b GitHub \u5b58\u50a8\u5e93\u4e2d\u83b7\u53d6\uff1ahttps://github.com/aayushmanace/PatchAlign24\uff0c\u53ef\u4ee5\u8f7b\u677e\u590d\u73b0\u548c\u8fdb\u4e00\u6b65\u8bd5\u9a8c\u3002||\n", "2409.04915": "|**2024-09-07**|[Activation Function Optimization Scheme for Image Classification](http://arxiv.org/abs/2409.04915)|**[link](https://github.com/abdurrahman1828/afos)**|Activation function has a significant impact on the dynamics, convergence, and performance of deep neural networks. The search for a consistent and high-performing activation function has always been a pursuit during deep learning model development. Existing state-of-the-art activation functions are manually designed with human expertise except for Swish. Swish was developed using a reinforcement learning-based search strategy. In this study, we propose an evolutionary approach for optimizing activation functions specifically for image classification tasks, aiming to discover functions that outperform current state-of-the-art options. Through this optimization framework, we obtain a series of high-performing activation functions denoted as Exponential Error Linear Unit (EELU). The developed activation functions are evaluated for image classification tasks from two perspectives: (1) five state-of-the-art neural network architectures, such as ResNet50, AlexNet, VGG16, MobileNet, and Compact Convolutional Transformer which cover computationally heavy to light neural networks, and (2) eight standard datasets, including CIFAR10, Imagenette, MNIST, Fashion MNIST, Beans, Colorectal Histology, CottonWeedID15, and TinyImageNet which cover from typical machine vision benchmark, agricultural image applications to medical image applications. Finally, we statistically investigate the generalization of the resultant activation functions developed through the optimization scheme. With a Friedman test, we conclude that the optimization scheme is able to generate activation functions that outperform the existing standard ones in 92.8% cases among 28 different cases studied, and $-x\\cdot erf(e^{-x})$ is found to be the best activation function for image classification generated by the optimization scheme.||\n", "2409.04817": "|**2024-09-07**|[SSFam: Scribble Supervised Salient Object Detection Family](http://arxiv.org/abs/2409.04817)|**[link](https://github.com/liuzywen/ssfam)**|Scribble supervised salient object detection (SSSOD) constructs segmentation ability of attractive objects from surroundings under the supervision of sparse scribble labels. For the better segmentation, depth and thermal infrared modalities serve as the supplement to RGB images in the complex scenes. Existing methods specifically design various feature extraction and multi-modal fusion strategies for RGB, RGB-Depth, RGB-Thermal, and Visual-Depth-Thermal image input respectively, leading to similar model flood. As the recently proposed Segment Anything Model (SAM) possesses extraordinary segmentation and prompt interactive capability, we propose an SSSOD family based on SAM, named SSFam, for the combination input with different modalities. Firstly, different modal-aware modulators are designed to attain modal-specific knowledge which cooperates with modal-agnostic information extracted from the frozen SAM encoder for the better feature ensemble. Secondly, a siamese decoder is tailored to bridge the gap between the training with scribble prompt and the testing with no prompt for the stronger decoding ability. Our model demonstrates the remarkable performance among combinations of different modalities and refreshes the highest level of scribble supervised methods and comes close to the ones of fully supervised methods. https://github.com/liuzywen/SSFam||\n", "2409.04801": "|**2024-09-07**|[SpotActor: Training-Free Layout-Controlled Consistent Image Generation](http://arxiv.org/abs/2409.04801)|null|Text-to-image diffusion models significantly enhance the efficiency of artistic creation with high-fidelity image generation. However, in typical application scenarios like comic book production, they can neither place each subject into its expected spot nor maintain the consistent appearance of each subject across images. For these issues, we pioneer a novel task, Layout-to-Consistent-Image (L2CI) generation, which produces consistent and compositional images in accordance with the given layout conditions and text prompts. To accomplish this challenging task, we present a new formalization of dual energy guidance with optimization in a dual semantic-latent space and thus propose a training-free pipeline, SpotActor, which features a layout-conditioned backward update stage and a consistent forward sampling stage. In the backward stage, we innovate a nuanced layout energy function to mimic the attention activations with a sigmoid-like objective. While in the forward stage, we design Regional Interconnection Self-Attention (RISA) and Semantic Fusion Cross-Attention (SFCA) mechanisms that allow mutual interactions across images. To evaluate the performance, we present ActorBench, a specified benchmark with hundreds of reasonable prompt-box pairs stemming from object detection datasets. Comprehensive experiments are conducted to demonstrate the effectiveness of our method. The results prove that SpotActor fulfills the expectations of this task and showcases the potential for practical applications with superior layout alignment, subject consistency, prompt conformity and background diversity.||\n", "2409.04778": "|**2024-09-07**|[LoCa: Logit Calibration for Knowledge Distillation](http://arxiv.org/abs/2409.04778)|null|Knowledge Distillation (KD), aiming to train a better student model by mimicking the teacher model, plays an important role in model compression. One typical way is to align the output logits. However, we find a common issue named mis-instruction, that the student would be misled when the predictions based on teacher logits do not follow the labels. Meanwhile, there is other useful dark knowledge in the logits such as the class discriminability, which is vital for distillation. In this paper, we propose a simple yet effective Logit Calibration (LoCa) method, which calibrates the logits from the teacher model based on the ground-truth labels. The key insight is to correct the prediction (to address the mis-instruction issue) and maintain useful dark knowledge simultaneously. Our proposed LoCa does not require any additional parameters. Empirical results on image classification and text generation tasks demonstrate that LoCa can effectively improve the performance of baselines.||\n", "2409.06689": "|**2024-09-10**|[A comprehensive study on Blood Cancer detection and classification using Convolutional Neural Network](http://arxiv.org/abs/2409.06689)|null|\u591a\u5e74\u6765\uff0c\u5728\u76ee\u6807\u68c0\u6d4b\u9886\u57df\uff0c\u4e00\u4e9b\u9ad8\u6548\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN)\uff0c\u5982 DenseNet201\u3001InceptionV3\u3001ResNet152v2\u3001SEresNet152\u3001VGG19\u3001Xception \u56e0\u5176\u6027\u80fd\u800c\u5907\u53d7\u5173\u6ce8\u3002\u6b64\u5916\uff0cCNN \u8303\u5f0f\u5df2\u7ecf\u6269\u5c55\u5230\u4ece\u539f\u59cb CNN \u67b6\u6784\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u548c\u96c6\u6210\u6a21\u578b\u3002\u7814\u7a76\u8868\u660e\uff0c\u8fc1\u79fb\u5b66\u4e60\u548c\u96c6\u6210\u6a21\u578b\u80fd\u591f\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60 (DL) \u6a21\u578b\u7684\u51c6\u786e\u6027\u3002\u7136\u800c\uff0c\u5f88\u5c11\u6709\u7814\u7a76\u5229\u7528\u8fd9\u4e9b\u6280\u672f\u5bf9\u8840\u6db2\u6076\u6027\u80bf\u7624\u8fdb\u884c\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7684\u7efc\u5408\u5b9e\u9a8c\u3002\u610f\u8bc6\u5230\u8fd9\u4e00\u5dee\u8ddd\uff0c\u672c\u7814\u7a76\u8fdb\u884c\u4e86\u4e09\u4e2a\u5b9e\u9a8c\uff1b\u5728\u7b2c\u4e00\u4e2a\u5b9e\u9a8c\u4e2d\uff0c\u4f7f\u7528\u4e86\u516d\u4e2a\u539f\u59cb CNN\uff0c\u5728\u7b2c\u4e8c\u4e2a\u5b9e\u9a8c\u4e2d\uff0c\u4f7f\u7528\u4e86\u8fc1\u79fb\u5b66\u4e60\uff0c\u5728\u7b2c\u4e09\u4e2a\u5b9e\u9a8c\u4e2d\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684\u96c6\u6210\u6a21\u578b DIX\uff08DenseNet201\u3001InceptionV3 \u548c Xception\uff09\u6765\u68c0\u6d4b\u548c\u5206\u7c7b\u8840\u764c\u3002\u7edf\u8ba1\u7ed3\u679c\u8868\u660e\uff0cDIX \u7684\u6027\u80fd\u4f18\u4e8e\u539f\u59cb\u6a21\u578b\u548c\u8fc1\u79fb\u5b66\u4e60\uff0c\u51c6\u786e\u7387\u8fbe\u5230 99.12%\u3002\u7136\u800c\uff0c\u8fd9\u9879\u7814\u7a76\u4e5f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u8d1f\u9762\u7ed3\u679c\uff0c\u56e0\u4e3a\u8fc1\u79fb\u5b66\u4e60\u5e76\u6ca1\u6709\u63d0\u9ad8\u539f\u59cb CNN \u7684\u51c6\u786e\u6027\u3002\u4e0e\u8bb8\u591a\u5176\u4ed6\u764c\u75c7\u4e00\u6837\uff0c\u8840\u764c\u75be\u75c5\u9700\u8981\u53ca\u65f6\u8bc6\u522b\uff0c\u624d\u80fd\u5236\u5b9a\u6709\u6548\u7684\u6cbb\u7597\u65b9\u6848\u5e76\u63d0\u9ad8\u751f\u5b58\u673a\u4f1a\u3002\u4f7f\u7528 CNN \u68c0\u6d4b\u548c\u5206\u7c7b\u8840\u764c\u7684\u9ad8\u7cbe\u5ea6\u8868\u660e\uff0cCNN \u6a21\u578b\u5728\u8840\u764c\u68c0\u6d4b\u4e2d\u5f88\u6709\u524d\u666f\u3002\u8fd9\u9879\u7814\u7a76\u5728\u751f\u7269\u533b\u5b66\u5de5\u7a0b\u3001\u8ba1\u7b97\u673a\u8f85\u52a9\u75be\u75c5\u8bca\u65ad\u548c\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u75be\u75c5\u68c0\u6d4b\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002||\n", "2409.06590": "|**2024-09-10**|[Lightweight Multiscale Feature Fusion Super-Resolution Network Based on Two-branch Convolution and Transformer](http://arxiv.org/abs/2409.06590)|null|\u76ee\u524d\uff0c\u6df1\u5ea6\u5b66\u4e60\u4e0b\u7684\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387(SISR)\u7b97\u6cd5\u4e3b\u8981\u6709\u4e24\u5927\u6a21\u578b\uff0c\u4e00\u79cd\u662f\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u578b\uff0c\u53e6\u4e00\u79cd\u662f\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u3002\u524d\u8005\u91c7\u7528\u4e0d\u540c\u5377\u79ef\u6838\u5927\u5c0f\u7684\u5377\u79ef\u5c42\u5806\u53e0\u7684\u65b9\u5f0f\u6765\u8bbe\u8ba1\u6a21\u578b\uff0c\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u63d0\u53d6\u56fe\u50cf\u7684\u5c40\u90e8\u7279\u5f81\uff1b\u540e\u8005\u91c7\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u8bbe\u8ba1\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u53ef\u4ee5\u8ba9\u6a21\u578b\u5efa\u7acb\u56fe\u50cf\u50cf\u7d20\u70b9\u4e4b\u95f4\u7684\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\uff0c\u8fdb\u800c\u66f4\u597d\u5730\u63d0\u53d6\u56fe\u50cf\u7684\u5168\u5c40\u7279\u5f81\u3002\u7136\u800c\uff0c\u4e0a\u8ff0\u4e24\u79cd\u65b9\u6cd5\u90fd\u9762\u4e34\u7740\u81ea\u5df1\u7684\u95ee\u9898\u3002\u57fa\u4e8e\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u5411\u4e92\u8865\u5377\u79ef\u548cTransformer\u7684\u65b0\u578b\u8f7b\u91cf\u7ea7\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u7f51\u7edc\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u53cc\u5206\u652f\u7f51\u7edc\u67b6\u6784\uff0c\u878d\u5408Transformer\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5404\u81ea\u7684\u7279\u70b9\uff0c\u5b9e\u73b0\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\u7684\u76f8\u4e92\u878d\u5408\u3002\u540c\u65f6\uff0c\u8003\u8651\u5230\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u4f4e\u50cf\u7d20\u56fe\u50cf\u9020\u6210\u7684\u5c40\u90e8\u4fe1\u606f\u4e22\u5931\uff0c\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u7279\u5f81\u8865\u5145\u7684\u6a21\u5757\u5316\u8fde\u63a5\u65b9\u5f0f\uff0c\u5c06\u6a21\u578b\u6d45\u5c42\u9636\u6bb5\u63d0\u53d6\u7684\u7279\u5f81\u56fe\u4e0e\u6a21\u578b\u6df1\u5c42\u9636\u6bb5\u63d0\u53d6\u7684\u7279\u5f81\u56fe\u8fdb\u884c\u878d\u5408\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u7279\u5f81\u56fe\u50cf\u4e2d\u4fe1\u606f\u7684\u4e22\u5931\uff0c\u6709\u5229\u4e8e\u56fe\u50cf\u7684\u590d\u539f\uff0c\u4fbf\u4e8e\u83b7\u5f97\u66f4\u9ad8\u8d28\u91cf\u7684\u590d\u539f\u56fe\u50cf\u3002\u6700\u7ec8\u7684\u5b9e\u8df5\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u53c2\u6570\u91cf\u76f8\u540c\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u76f8\u6bd4\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6a21\u578b\u5728\u56fe\u50cf\u6062\u590d\u6027\u80fd\u65b9\u9762\u662f\u6700\u4f18\u7684\u3002||\n", "2409.06584": "|**2024-09-10**|[Transtreaming: Adaptive Delay-aware Transformer for Real-time Streaming Perception](http://arxiv.org/abs/2409.06584)|null|\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u5bf9\u4e8e\u8bb8\u591a\u73b0\u5b9e\u5e94\u7528\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u9632\u649e\u548c\u8def\u5f84\u89c4\u5212\uff09\u7684\u51b3\u7b56\u8fc7\u7a0b\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u5b9e\u65f6\u6d41\u611f\u77e5\u65b9\u6cd5 Transtreaming\uff0c\u5b83\u89e3\u51b3\u4e86\u5177\u6709\u52a8\u6001\u8ba1\u7b97\u5ef6\u8fdf\u7684\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u6311\u6218\u3002Transtreaming \u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u81ea\u9002\u5e94\u5ef6\u8fdf\u611f\u77e5\u8f6c\u6362\u5668\uff0c\u5b83\u53ef\u4ee5\u540c\u65f6\u9884\u6d4b\u591a\u4e2a\u672a\u6765\u5e27\u5e76\u9009\u62e9\u4e0e\u73b0\u5b9e\u4e16\u754c\u5f53\u524d\u65f6\u95f4\u6700\u5339\u914d\u7684\u8f93\u51fa\uff0c\u4ece\u800c\u8865\u507f\u4efb\u4f55\u7cfb\u7edf\u5f15\u8d77\u7684\u8ba1\u7b97\u5ef6\u8fdf\u3002\u5373\u4f7f\u5728\u5355\u5e27\u68c0\u6d4b\u573a\u666f\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u4e5f\u901a\u8fc7\u5229\u7528\u57fa\u4e8e\u8f6c\u6362\u5668\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u5b83\u5728\u4ece\u5f3a\u5927\u7684 V100 \u5230\u9002\u5ea6\u7684 2080Ti \u7684\u5404\u79cd\u8bbe\u5907\u4e0a\u5747\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5728\u6240\u6709\u5e73\u53f0\u4e0a\u90fd\u5b9e\u73b0\u4e86\u6700\u9ad8\u6c34\u5e73\u7684\u611f\u77e5\u7cbe\u5ea6\u3002\u4e0e\u5927\u591a\u6570\u96be\u4ee5\u5728\u529f\u80fd\u8f83\u5f31\u7684\u8bbe\u5907\u4e0a\u5728\u4e00\u5e27\u5185\u5b8c\u6210\u8ba1\u7b97\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u4e0d\u540c\uff0cTranstreaming \u53ef\u4ee5\u6ee1\u8db3\u5404\u79cd\u8bbe\u5907\u4e0a\u7684\u4e25\u683c\u5b9e\u65f6\u5904\u7406\u8981\u6c42\u3002\u5b9e\u9a8c\u7ed3\u679c\u5f3a\u8c03\u4e86\u8be5\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u5176\u663e\u7740\u63d0\u9ad8\u8bb8\u591a\u73b0\u5b9e\u7cfb\u7edf\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\uff09\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u7684\u6f5c\u529b\u3002||\n", "2409.06583": "|**2024-09-10**|[Semi-Supervised 3D Object Detection with Chanel Augmentation using Transformation Equivariance](http://arxiv.org/abs/2409.06583)|null|\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u548c\u673a\u5668\u4eba\u6765\u8bf4\uff0c\u7cbe\u786e\u7684\u4e09\u7ef4\u7269\u4f53\u68c0\u6d4b\u5bf9\u4e8e\u5176\u5b89\u5168\u6709\u6548\u5730\u5bfc\u822a\u548c\u4e0e\u73af\u5883\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002\u540c\u65f6\uff0c\u4e09\u7ef4\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u4f9d\u8d56\u4e8e\u6570\u636e\u89c4\u6a21\u548c\u6807\u6ce8\uff0c\u800c\u8fd9\u901a\u5e38\u6210\u672c\u9ad8\u6602\u3002\u56e0\u6b64\uff0c\u4f7f\u7528\u6709\u9650\u7684\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u672c\u6587\u63a2\u7d22\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5e08\u751f\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u901a\u9053\u589e\u5f3a\u6280\u672f\u8fdb\u884c\u4e09\u7ef4\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u3002\u5e08\u751fSSL\u901a\u5e38\u5bf9\u6559\u5e08\u548c\u5b66\u751f\u5206\u522b\u91c7\u7528\u5f31\u589e\u5f3a\u548c\u5f3a\u589e\u5f3a\u3002\u5728\u672c\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u53d8\u6362\u7b49\u53d8\u68c0\u6d4b\u5668\uff08TED\uff09\u5bf9\u4e24\u4e2a\u7f51\u7edc\u5e94\u7528\u4e86\u591a\u901a\u9053\u589e\u5f3a\u3002TED\u4f7f\u6211\u4eec\u80fd\u591f\u63a2\u7d22\u70b9\u4e91\u4e0a\u589e\u5f3a\u7684\u4e0d\u540c\u7ec4\u5408\uff0c\u5e76\u6709\u6548\u5730\u805a\u5408\u591a\u901a\u9053\u53d8\u6362\u7b49\u53d8\u7279\u5f81\u3002\u539f\u5219\u4e0a\uff0c\u901a\u8fc7\u5bf9\u6559\u5e08\u7f51\u7edc\u91c7\u7528\u56fa\u5b9a\u7684\u901a\u9053\u589e\u5f3a\uff0c\u5b66\u751f\u53ef\u4ee5\u5728\u53ef\u9760\u7684\u4f2a\u6807\u7b7e\u4e0a\u7a33\u5b9a\u5730\u8bad\u7ec3\u3002\u91c7\u7528\u5f3a\u901a\u9053\u589e\u5f3a\u53ef\u4ee5\u4e30\u5bcc\u6570\u636e\u7684\u591a\u6837\u6027\uff0c\u589e\u5f3a\u5bf9\u53d8\u6362\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u9ad8\u5b66\u751f\u7f51\u7edc\u7684\u6cdb\u5316\u6027\u80fd\u3002\u6211\u4eec\u4f7f\u7528SOTA\u5c42\u6b21\u76d1\u7763\u4f5c\u4e3a\u57fa\u7ebf\uff0c\u5e76\u5c06\u5176\u53cc\u9608\u503c\u8c03\u6574\u5230TED\uff0c\u79f0\u4e3a\u901a\u9053IoU\u4e00\u81f4\u6027\u3002\u6211\u4eec\u4f7f\u7528KITTI\u6570\u636e\u96c6\u5bf9\u6211\u4eec\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8d8a\u4e86SOTA\u4e09\u7ef4\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u3002||\n", "2409.06542": "|**2024-09-10**|[Dynamic Decoupling of Placid Terminal Attractor-based Gradient Descent Algorithm](http://arxiv.org/abs/2409.06542)|null|\u68af\u5ea6\u4e0b\u964d (GD) \u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d (SGD) \u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4f17\u591a\u5e94\u7528\u9886\u57df\u3002\u56e0\u6b64\uff0c\u7406\u89e3 GD \u7684\u52a8\u529b\u5b66\u5e76\u63d0\u9ad8\u5176\u6536\u655b\u901f\u5ea6\u4ecd\u7136\u975e\u5e38\u91cd\u8981\u3002\u672c\u6587\u6839\u636e\u68af\u5ea6\u6d41\u4e0d\u540c\u9636\u6bb5\u7684\u7ec8\u7aef\u5438\u5f15\u5b50\uff0c\u4ed4\u7ec6\u5206\u6790\u4e86 GD \u7684\u52a8\u529b\u5b66\u3002\u57fa\u4e8e\u7ec8\u7aef\u6ed1\u6a21\u7406\u8bba\u548c\u7ec8\u7aef\u5438\u5f15\u5b50\u7406\u8bba\uff0c\u8bbe\u8ba1\u4e86\u56db\u79cd\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u3002\u5e76\u901a\u8fc7\u8be6\u7ec6\u7684\u7406\u8bba\u7814\u7a76\u8003\u5bdf\u4e86\u5b83\u4eec\u7684\u6027\u80fd\uff0c\u5e76\u5bf9\u5b66\u4e60\u8fc7\u7a0b\u7684\u8fd0\u884c\u65f6\u95f4\u8fdb\u884c\u4e86\u8bc4\u4f30\u548c\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u8fd8\u8be6\u7ec6\u7814\u7a76\u4e86\u5b83\u4eec\u5b66\u4e60\u8fc7\u7a0b\u7684\u603b\u65f6\u95f4\u3002\u4e3a\u4e86\u8bc4\u4f30\u5176\u6709\u6548\u6027\uff0c\u5728\u51fd\u6570\u903c\u8fd1\u95ee\u9898\u548c\u56fe\u50cf\u5206\u7c7b\u95ee\u9898\u4e0a\u5bf9\u5404\u79cd\u4eff\u771f\u7ed3\u679c\u8fdb\u884c\u4e86\u7814\u7a76\u3002||\n", "2409.06443": "|**2024-09-10**|[Knowledge Distillation via Query Selection for Detection Transformer](http://arxiv.org/abs/2409.06443)|null|Transformer \u901a\u8fc7\u5f15\u5165 DETR \u4e3a\u76ee\u6807\u68c0\u6d4b\u9886\u57df\u5e26\u6765\u4e86\u9769\u547d\u6027\u7684\u53d8\u5316\uff0cDETR \u4ee5\u5176\u7b80\u6d01\u6027\u548c\u6709\u6548\u6027\u800c\u5907\u53d7\u8d5e\u8a89\u3002\u5c3d\u7ba1\u6709\u8fd9\u4e9b\u4f18\u52bf\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u7684\u5e9e\u5927\u89c4\u6a21\u5bf9\u5176\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\uff0c\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u672c\u6587\u5229\u7528\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u89e3\u51b3\u4e86\u538b\u7f29 DETR \u7684\u6311\u6218\uff0c\u8be5\u6280\u672f\u6709\u671b\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c0f\u6a21\u578b\u89c4\u6a21\u3002DETR \u6027\u80fd\u7684\u4e00\u4e2a\u5173\u952e\u65b9\u9762\u662f\u5b83\u4eec\u4f9d\u8d56\u67e5\u8be2\u6765\u51c6\u786e\u89e3\u91ca\u5bf9\u8c61\u8868\u793a\u3002\u4f20\u7edf\u7684\u84b8\u998f\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u901a\u8fc7\u4e8c\u5206\u5339\u914d\u8bc6\u522b\u7684\u6b63\u67e5\u8be2\uff0c\u800c\u5ffd\u7565\u4e86\u786c\u8d1f\u67e5\u8be2\u4e2d\u5b58\u5728\u7684\u4fe1\u606f\u3002\u6211\u4eec\u7684\u89c6\u89c9\u5206\u6790\u8868\u660e\uff0c\u5173\u6ce8\u524d\u666f\u5143\u7d20\u7684\u786c\u8d1f\u67e5\u8be2\u5bf9\u4e8e\u589e\u5f3a\u84b8\u998f\u7ed3\u679c\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7ec4\u67e5\u8be2\u9009\u62e9\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u901a\u8fc7\u6839\u636e\u67e5\u8be2\u4e0e\u771f\u5b9e\u5bf9\u8c61\u7684\u5e7f\u4e49\u4ea4\u5e76\u6bd4 (GIoU) \u5bf9\u67e5\u8be2\u8fdb\u884c\u5206\u6bb5\uff0c\u4ece\u800c\u53d1\u73b0\u6709\u4ef7\u503c\u7684\u786c\u8d1f\u67e5\u8be2\u7528\u4e8e\u84b8\u998f\uff0c\u8fd9\u4e0e DETR \u84b8\u998f\u4e2d\u7684\u4f20\u7edf\u67e5\u8be2\u9009\u62e9\u4e0d\u540c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u67e5\u8be2\u9009\u62e9\u7684 DETR \u77e5\u8bc6\u84b8\u998f (QSKD) \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u6ce8\u610f\u529b\u5f15\u5bfc\u7279\u5f81\u84b8\u998f (AGFD) \u548c\u5c40\u90e8\u5bf9\u9f50\u9884\u6d4b\u84b8\u998f (LAPD)\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u901a\u8fc7\u5173\u6ce8\u6559\u5e08\u6a21\u578b\u4e2d\u95f4\u7279\u5f81\u548c\u8f93\u51fa\u4e2d\u6700\u6709\u4fe1\u606f\u7684\u90e8\u5206\u6765\u4f18\u5316\u84b8\u998f\u8fc7\u7a0b\u3002\u6211\u4eec\u5bf9 MS-COCO \u6570\u636e\u96c6\u7684\u7efc\u5408\u5b9e\u9a8c\u8bc4\u4f30\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u4e0d\u589e\u52a0\u5927\u91cf\u8ba1\u7b97\u6210\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u7740\u63d0\u9ad8\u4e86\u5404\u79cd DETR \u67b6\u6784\u7684\u5e73\u5747\u7cbe\u5ea6 (AP)\u3002\u5177\u4f53\u6765\u8bf4\uff0cConditional DETR ResNet-18 \u7684 AP \u4ece 35.8 \u63d0\u9ad8\u5230 39.9\u3002||\n", "2409.06311": "|**2024-09-10**|[Seam Carving as Feature Pooling in CNN](http://arxiv.org/abs/2409.06311)|null|\u8fd9\u9879\u5de5\u4f5c\u7814\u7a76\u4e86\u5c06\u63a5\u7f1d\u88c1\u526a\u4f5c\u4e3a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u4e2d\u7684\u4e00\u79cd\u7279\u5f81\u6c60\u5316\u6280\u672f\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u6f5c\u529b\u3002\u6211\u4eec\u5efa\u8bae\u7528\u63a5\u7f1d\u88c1\u526a\u64cd\u4f5c\u66ff\u6362\u4f20\u7edf\u7684\u6700\u5927\u6c60\u5316\u5c42\u3002\u6211\u4eec\u5728 Caltech-UCSD Birds 200-2011 \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u63a5\u7f1d\u88c1\u526a\u7684 CNN \u4e0e\u91c7\u7528\u6700\u5927\u6c60\u5316\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548c F1 \u5206\u6570\u7b49\u6307\u6807\u4e0a\u5747\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002\u6211\u4eec\u901a\u8fc7\u7279\u5f81\u56fe\u53ef\u89c6\u5316\u8fdb\u4e00\u6b65\u5206\u6790\u4e86\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u884c\u4e3a\uff0c\u8868\u660e\u63a5\u7f1d\u88c1\u526a\u5728\u6c60\u5316\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u4fdd\u7559\u4e86\u66f4\u591a\u7ed3\u6784\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8ba8\u8bba\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6f5c\u5728\u65b9\u5411\u3002||\n", "2409.06300": "|**2024-09-10**|[An Attribute-Enriched Dataset and Auto-Annotated Pipeline for Open Detection](http://arxiv.org/abs/2409.06300)|null|\u901a\u8fc7\u8bed\u8a00\u68c0\u6d4b\u611f\u5174\u8da3\u7684\u5bf9\u8c61\u7ecf\u5e38\u4f1a\u9047\u5230\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u90a3\u4e9b\u4e0d\u5e38\u89c1\u6216\u96be\u4ee5\u63cf\u8ff0\u7684\u5bf9\u8c61\uff0c\u56e0\u4e3a\u81ea\u52a8\u5316\u6a21\u578b\u548c\u4eba\u7c7b\u6807\u6ce8\u8005\u4e4b\u95f4\u5b58\u5728\u611f\u77e5\u5dee\u5f02\u3002\u8fd9\u4e9b\u6311\u6218\u51f8\u663e\u4e86\u5bf9\u7efc\u5408\u6570\u636e\u96c6\u7684\u9700\u6c42\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u9700\u8981\u8d85\u8d8a\u6807\u51c6\u7684\u5bf9\u8c61\u6807\u7b7e\uff0c\u5e76\u7ed3\u5408\u8be6\u7ec6\u7684\u5c5e\u6027\u63cf\u8ff0\u3002\u4e3a\u4e86\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\uff0c\u6211\u4eec\u5f15\u5165\u4e86 Objects365-Attr \u6570\u636e\u96c6\uff0c\u5b83\u662f\u5bf9\u73b0\u6709 Objects365 \u6570\u636e\u96c6\u7684\u6269\u5c55\uff0c\u5176\u7279\u70b9\u662f\u5177\u6709\u5c5e\u6027\u6807\u6ce8\u3002\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u6574\u5408\u5e7f\u6cdb\u7684\u5c5e\u6027\uff08\u5305\u62ec\u989c\u8272\u3001\u6750\u8d28\u3001\u72b6\u6001\u3001\u7eb9\u7406\u548c\u8272\u8c03\uff09\u6765\u51cf\u5c11\u5bf9\u8c61\u68c0\u6d4b\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u3002\u5b83\u5305\u542b 560 \u4e07\u4e2a\u5bf9\u8c61\u7ea7\u5c5e\u6027\u63cf\u8ff0\u7684\u6269\u5c55\u96c6\u5408\uff0c\u8fd9\u4e9b\u63cf\u8ff0\u5728 140 \u4e07\u4e2a\u8fb9\u754c\u6846\u4e2d\u8fdb\u884c\u4e86\u7cbe\u5fc3\u6807\u6ce8\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5bf9\u4e0d\u540c\u89c4\u6a21\u7684 YOLO-World \u8fdb\u884c\u4e86\u4e25\u683c\u7684\u8bc4\u4f30\uff0c\u6d4b\u91cf\u4e86\u5b83\u4eec\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u6570\u636e\u96c6\u5bf9\u63a8\u8fdb\u5bf9\u8c61\u68c0\u6d4b\u7684\u8d21\u732e\u3002||\n", "2409.07904": "|**2024-09-12**|[FACT: Feature Adaptive Continual-learning Tracker for Multiple Object Tracking](http://arxiv.org/abs/2409.07904)|null|\u591a\u76ee\u6807\u8ddf\u8e2a (MOT) \u6d89\u53ca\u8bc6\u522b\u89c6\u9891\u5e8f\u5217\u4e2d\u7684\u591a\u4e2a\u76ee\u6807\u5e76\u4e3a\u5176\u5206\u914d\u76f8\u5e94\u7684 ID\uff0c\u5176\u4e2d\u7ecf\u5e38\u9047\u5230\u906e\u6321\u3002\u6700\u8fd1\u7684\u65b9\u6cd5\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u6280\u672f\u89e3\u51b3\u906e\u6321\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u9002\u5e94\u6027\uff0c\u6216\u901a\u8fc7\u79bb\u7ebf\u5b66\u4e60\u6280\u672f\u5229\u7528\u89c6\u9891\u4e2d\u7684\u65f6\u95f4\u4fe1\u606f\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u7684\u57fa\u4e8e\u5728\u7ebf\u5b66\u4e60\u7684 MOT \u65b9\u6cd5\u65e0\u6cd5\u4ece\u6240\u6709\u8fc7\u53bb\u7684\u8ddf\u8e2a\u4fe1\u606f\u4e2d\u5b66\u4e60\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u5b9e\u65f6\u8ddf\u8e2a\u901f\u5ea6\u7684\u540c\u65f6\u63d0\u9ad8\u5bf9\u957f\u671f\u906e\u6321\u7684\u9002\u5e94\u6027\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u57fa\u4e8e\u65f6\u95f4\u4fe1\u606f\u7684\u79bb\u7ebf\u5b66\u4e60\u65b9\u6cd5\u7ef4\u62a4\u4e00\u4e2a\u957f\u671f\u8bb0\u5fc6\u6765\u5b58\u50a8\u8fc7\u53bb\u7684\u8ddf\u8e2a\u4fe1\u606f\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u9650\u5236\u4e86\u5b83\u4eec\u5728\u8ddf\u8e2a\u8fc7\u7a0b\u4e2d\u53ea\u80fd\u4f7f\u7528\u5c40\u90e8\u7684\u8fc7\u53bb\u4fe1\u606f\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 MOT \u6846\u67b6\uff0c\u79f0\u4e3a\u7279\u5f81\u81ea\u9002\u5e94\u6301\u7eed\u5b66\u4e60\u8ddf\u8e2a\u5668 (FACT)\uff0c\u5b83\u901a\u8fc7\u5229\u7528\u6240\u6709\u8fc7\u53bb\u7684\u8ddf\u8e2a\u4fe1\u606f\u5b9e\u73b0\u76ee\u6807\u7684\u5b9e\u65f6\u8ddf\u8e2a\u548c\u7279\u5f81\u5b66\u4e60\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u53ef\u4ee5\u4e0e\u5404\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u7279\u5f81\u7684\u8ddf\u8e2a\u5668\u96c6\u6210\uff0c\u4ece\u800c\u63d0\u9ad8\u5b83\u4eec\u7684\u8ddf\u8e2a\u80fd\u529b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u7279\u5f81\u81ea\u9002\u5e94\u6301\u7eed\u5b66\u4e60 (FAC) \u6a21\u5757\uff0c\u8fd9\u662f\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\uff0c\u53ef\u4ee5\u5728\u7ebf\u8bad\u7ec3\u4ee5\u81ea\u9002\u5e94\u5730\u5b66\u4e60\u7279\u5f81\uff0c\u5e76\u5728\u8ddf\u8e2a\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u6240\u6709\u8fc7\u53bb\u7684\u8ddf\u8e2a\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u4e13\u4e3a\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u6301\u7eed\u5b66\u4e60\u7684\u8ddf\u8e2a\u800c\u8bbe\u8ba1\u7684\u4e24\u9636\u6bb5\u5173\u8054\u6a21\u5757\u3002\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728 MOT17 \u548c MOT20 \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5728\u7ebf\u8ddf\u8e2a\u6027\u80fd\u3002\u4ee3\u7801\u5c06\u5728\u63a5\u6536\u540e\u53d1\u5e03\u3002||\n", "2409.07896": "|**2024-09-12**|[Microscopic-Mamba: Revealing the Secrets of Microscopic Images with Just 4M Parameters](http://arxiv.org/abs/2409.07896)|**[link](https://github.com/zs1314/microscopic-mamba)**|\u5728\u533b\u5b66\u663e\u5fae\u56fe\u50cf\u5206\u7c7b (MIC) \u9886\u57df\uff0c\u57fa\u4e8e CNN \u548c Transformer \u7684\u6a21\u578b\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\u3002\u7136\u800c\uff0cCNN \u96be\u4ee5\u5efa\u6a21\u8fdc\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u5176\u5145\u5206\u5229\u7528\u56fe\u50cf\u8bed\u4e49\u4fe1\u606f\u7684\u80fd\u529b\u3002\u76f8\u53cd\uff0cTransformer \u5219\u53d7\u5230\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u6027\u7684\u963b\u788d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e Mamba \u67b6\u6784\u7684\u6a21\u578b\uff1aMicroscopic-Mamba\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u90e8\u5206\u9009\u62e9\u524d\u9988\u7f51\u7edc\uff08PSFFN\uff09\u6765\u66ff\u6362\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\uff08VSSM\uff09\u7684\u6700\u540e\u4e00\u4e2a\u7ebf\u6027\u5c42\uff0c\u589e\u5f3a\u4e86 Mamba \u7684\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u8c03\u5236\u4ea4\u4e92\u7279\u5f81\u805a\u5408\uff08MIFA\uff09\u6a21\u5757\uff0c\u4ee5\u6709\u6548\u5730\u8c03\u5236\u548c\u52a8\u6001\u805a\u5408\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u3002\u6211\u4eec\u8fd8\u7ed3\u5408\u4e86\u5e76\u884c VSSM \u673a\u5236\uff0c\u4ee5\u6539\u5584\u901a\u9053\u95f4\u7684\u4fe1\u606f\u4ea4\u4e92\uff0c\u540c\u65f6\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e94\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/zs1314/Microscopic-Mamba \u83b7\u53d6\u3002||\n", "2409.07813": "|**2024-09-12**|[What is YOLOv9: An In-Depth Exploration of the Internal Features of the Next-Generation Object Detector](http://arxiv.org/abs/2409.07813)|null|\u672c\u7814\u7a76\u5168\u9762\u5206\u6790\u4e86 YOLOv9 \u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u67b6\u6784\u521b\u65b0\u3001\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u53ca\u76f8\u8f83\u4e8e\u5148\u524d\u7248\u672c\u7684\u6027\u80fd\u6539\u8fdb\u3002\u5173\u952e\u7684\u6539\u8fdb\uff0c\u4f8b\u5982\u5e7f\u4e49\u9ad8\u6548\u5c42\u805a\u5408\u7f51\u7edc (GELAN) \u548c\u53ef\u7f16\u7a0b\u68af\u5ea6\u4fe1\u606f (PGI)\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u7279\u5f81\u63d0\u53d6\u548c\u68af\u5ea6\u6d41\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u5377\u79ef\u548c\u8f7b\u91cf\u7ea7 C3Ghost \u67b6\u6784\uff0cYOLOv9 \u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u5728 Microsoft COCO \u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u5b83\u5177\u6709\u4f18\u8d8a\u7684\u5e73\u5747\u7cbe\u5ea6\u5747\u503c (mAP) \u548c\u66f4\u5feb\u7684\u63a8\u7406\u65f6\u95f4\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e YOLOv8\u3002\u8be5\u6a21\u578b\u7684\u591a\u529f\u80fd\u6027\u4f53\u73b0\u5728\u5b83\u53ef\u4ee5\u65e0\u7f1d\u90e8\u7f72\u5230\u4ece\u8fb9\u7f18\u8bbe\u5907\u5230\u9ad8\u6027\u80fd GPU \u7684\u5404\u79cd\u786c\u4ef6\u5e73\u53f0\u4e0a\uff0c\u5e76\u5185\u7f6e\u652f\u6301 PyTorch \u548c TensorRT \u96c6\u6210\u3002\u672c\u6587\u9996\u6b21\u6df1\u5165\u63a2\u8ba8\u4e86 YOLOv9 \u7684\u5185\u90e8\u7279\u5f81\u53ca\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5c06\u5176\u786e\u7acb\u4e3a\u8de8\u884c\u4e1a\u7684\u5b9e\u65f6\u5bf9\u8c61\u68c0\u6d4b\u7684\u6700\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u7269\u8054\u7f51\u8bbe\u5907\u5230\u5927\u578b\u5de5\u4e1a\u5e94\u7528\u3002||\n", "2409.07769": "|**2024-09-12**|[Mesh-based Super-Resolution of Fluid Flows with Multiscale Graph Neural Networks](http://arxiv.org/abs/2409.07769)|null|\u8fd9\u9879\u5de5\u4f5c\u4ecb\u7ecd\u4e86\u4e00\u79cd\u56fe\u795e\u7ecf\u7f51\u7edc (GNN) \u65b9\u6cd5\uff0c\u80fd\u591f\u5bf9\u6d41\u4f53\u6d41\u52a8\u8fdb\u884c\u57fa\u4e8e\u7f51\u683c\u7684\u4e09\u7ef4\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u3002\u5728\u6b64\u6846\u67b6\u4e2d\uff0cGNN \u7684\u8bbe\u8ba1\u4e0d\u662f\u4e00\u6b21\u6027\u5728\u6574\u4e2a\u57fa\u4e8e\u7f51\u683c\u7684\u573a\u4e0a\u8fd0\u884c\uff0c\u800c\u662f\u76f4\u63a5\u5728\u5c40\u90e8\u5143\u7d20\uff08\u6216\u5355\u5143\uff09\u7f51\u683c\u4e0a\u8fd0\u884c\u3002\u4e3a\u4e86\u4ee5\u7c7b\u4f3c\u4e8e\u8c31\uff08\u6216\u6709\u9650\uff09\u5143\u7d20\u79bb\u6563\u5316\u7684\u65b9\u5f0f\u4fc3\u8fdb\u57fa\u4e8e\u7f51\u683c\u7684 GNN \u8868\u793a\uff0c\u4fee\u6539\u4e86\u57fa\u7ebf GNN \u5c42\uff08\u79f0\u4e3a\u6d88\u606f\u4f20\u9012\u5c42\uff0c\u7528\u4e8e\u66f4\u65b0\u5c40\u90e8\u8282\u70b9\u5c5e\u6027\uff09\u4ee5\u8003\u8651\u91cd\u5408\u56fe\u8282\u70b9\u7684\u540c\u6b65\uff0c\u4ece\u800c\u4f7f\u5176\u4e0e\u5e38\u7528\u7684\u57fa\u4e8e\u5143\u7d20\u7684\u7f51\u683c\u8fde\u63a5\u517c\u5bb9\u3002\u8be5\u67b6\u6784\u672c\u8d28\u4e0a\u662f\u591a\u5c3a\u5ea6\u7684\uff0c\u7531\u7c97\u5c3a\u5ea6\u548c\u7ec6\u5c3a\u5ea6\u6d88\u606f\u4f20\u9012\u5c42\u5e8f\u5217\uff08\u79f0\u4e3a\u5904\u7406\u5668\uff09\u7ec4\u5408\u800c\u6210\uff0c\u8fd9\u4e9b\u5e8f\u5217\u4e4b\u95f4\u901a\u8fc7\u56fe\u89e3\u6c60\u5c42\u8fdb\u884c\u5206\u79bb\u3002\u7c97\u5c3a\u5ea6\u5904\u7406\u5668\u4f7f\u7528\u7c97\u5c3a\u5ea6\u540c\u6b65\u6d88\u606f\u4f20\u9012\u5728\u5143\u7d20\u90bb\u57df\u4e0a\u5c06\u67e5\u8be2\u5143\u7d20\uff08\u4ee5\u53ca\u4e00\u7ec4\u76f8\u90bb\u7684\u7c97\u5143\u7d20\uff09\u5d4c\u5165\u5230\u5355\u4e2a\u6f5c\u5728\u56fe\u8868\u793a\u4e2d\uff0c\u800c\u7ec6\u5c3a\u5ea6\u5904\u7406\u5668\u5229\u7528\u6b64\u6f5c\u5728\u56fe\u4e0a\u7684\u5176\u4ed6\u6d88\u606f\u4f20\u9012\u64cd\u4f5c\u6765\u6821\u6b63\u63d2\u503c\u8bef\u5dee\u3002\u4f7f\u7528\u6765\u81ea\u96f7\u8bfa\u6570\u4e3a 1600 \u548c 3200 \u7684\u6cf0\u52d2-\u683c\u6797\u6da1\u6d41\u6a21\u62df\u7684\u516d\u9762\u4f53\u7f51\u683c\u6570\u636e\u8fdb\u884c\u6f14\u793a\u7814\u7a76\u3002\u901a\u8fc7\u5206\u6790\u5168\u5c40\u548c\u5c40\u90e8\u8bef\u5dee\uff0c\u7ed3\u679c\u6700\u7ec8\u8868\u660e\uff0c\u4e0e\u7c97\u5c3a\u5ea6\u548c\u591a\u5c3a\u5ea6\u6a21\u578b\u914d\u7f6e\u4e2d\u7684\u76ee\u6807\u76f8\u6bd4\uff0cGNN \u5982\u4f55\u80fd\u591f\u751f\u6210\u51c6\u786e\u7684\u8d85\u5206\u8fa8\u7387\u573a\u3002\u53d1\u73b0\u56fa\u5b9a\u67b6\u6784\u7684\u91cd\u5efa\u8bef\u5dee\u4e0e\u96f7\u8bfa\u6570\u6210\u6b63\u6bd4\uff0c\u800c\u5305\u542b\u5468\u56f4\u7c97\u5143\u7d20\u90bb\u5c45\u88ab\u53d1\u73b0\u53ef\u4ee5\u6539\u5584 Re=1600 \u65f6\u7684\u9884\u6d4b\uff0c\u4f46\u5728 Re=3200 \u65f6\u5219\u4e0d\u7136\u3002||\n", "2409.07734": "|**2024-09-12**|[DFDG: Data-Free Dual-Generator Adversarial Distillation for One-Shot Federated Learning](http://arxiv.org/abs/2409.07734)|null|\u8054\u90a6\u5b66\u4e60 (FL) \u662f\u4e00\u79cd\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u65b9\u6848\uff0c\u5176\u4e2d\u5ba2\u6237\u7aef\u901a\u8fc7\u5171\u4eab\u6a21\u578b\u4fe1\u606f\u800c\u4e0d\u662f\u5176\u79c1\u6709\u6570\u636e\u96c6\u6765\u5171\u540c\u53c2\u4e0e\u5168\u5c40\u6a21\u578b\u7684\u534f\u4f5c\u8bad\u7ec3\u3002\u8003\u8651\u5230\u4e0e\u901a\u4fe1\u548c\u9690\u79c1\u76f8\u5173\u7684\u62c5\u5fe7\uff0c\u5177\u6709\u4e00\u8f6e\u901a\u4fe1\u7684\u5355\u6b21\u8054\u90a6\u5b66\u4e60\u5df2\u6210\u4e3a\u4e8b\u5b9e\u4e0a\u7684\u6709\u5e0c\u671b\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5355\u6b21\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u516c\u5171\u6570\u636e\u96c6\uff0c\u8981\u4e48\u4fa7\u91cd\u4e8e\u6a21\u578b\u540c\u6784\u8bbe\u7f6e\uff0c\u8981\u4e48\u4ece\u672c\u5730\u6a21\u578b\u4e2d\u63d0\u53d6\u7684\u77e5\u8bc6\u6709\u9650\uff0c\u8fd9\u4f7f\u5f97\u8bad\u7ec3\u9c81\u68d2\u7684\u5168\u5c40\u6a21\u578b\u53d8\u5f97\u56f0\u96be\u751a\u81f3\u4e0d\u5207\u5b9e\u9645\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7528\u4e8e\u5355\u6b21\u8054\u90a6\u5b66\u4e60\u7684\u65e0\u6570\u636e\u53cc\u751f\u6210\u5668\u5bf9\u6297\u84b8\u998f\u65b9\u6cd5 (\u5373 DFDG)\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u8bad\u7ec3\u53cc\u751f\u6210\u5668\u6765\u63a2\u7d22\u66f4\u5e7f\u6cdb\u7684\u672c\u5730\u6a21\u578b\u8bad\u7ec3\u7a7a\u95f4\u3002DFDG \u4ee5\u5bf9\u6297\u65b9\u5f0f\u6267\u884c\uff0c\u5305\u62ec\u4e24\u90e8\u5206\uff1a\u53cc\u751f\u6210\u5668\u8bad\u7ec3\u548c\u53cc\u6a21\u578b\u84b8\u998f\u3002\u5728\u53cc\u751f\u6210\u5668\u8bad\u7ec3\u4e2d\uff0c\u6211\u4eec\u6df1\u5165\u7814\u7a76\u4e86\u6bcf\u4e2a\u751f\u6210\u5668\u5728\u4fdd\u771f\u5ea6\u3001\u53ef\u8fc1\u79fb\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u7684\u5185\u5bb9\uff0c\u4ee5\u786e\u4fdd\u5176\u6548\u7528\uff0c\u5e76\u989d\u5916\u5b9a\u5236\u4e86\u4ea4\u53c9\u6563\u5ea6\u635f\u5931\u4ee5\u51cf\u5c11\u53cc\u751f\u6210\u5668\u8f93\u51fa\u7a7a\u95f4\u7684\u91cd\u53e0\u3002\u5728\u53cc\u6a21\u578b\u84b8\u998f\u4e2d\uff0c\u8bad\u7ec3\u597d\u7684\u53cc\u751f\u6210\u5668\u534f\u540c\u5de5\u4f5c\uff0c\u4e3a\u5168\u5c40\u6a21\u578b\u7684\u66f4\u65b0\u63d0\u4f9b\u8bad\u7ec3\u6570\u636e\u3002\u6700\u540e\uff0c\u6211\u4eec\u5bf9\u5404\u79cd\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e SOTA \u57fa\u7ebf\u76f8\u6bd4\uff0cDFDG \u5728\u51c6\u786e\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u7684\u6027\u80fd\u63d0\u5347\u3002||\n", "2409.07693": "|**2024-09-12**|[Cooperative Inference with Interleaved Operator Partitioning for CNNs](http://arxiv.org/abs/2409.07693)|null|\u5c06\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u90e8\u7f72\u5728\u7269\u8054\u7f51\uff08IoT\uff09\u8bbe\u5907\u4e0a\u901a\u5e38\u4f1a\u9762\u4e34\u5185\u5b58\u8d44\u6e90\u548c\u8ba1\u7b97\u80fd\u529b\u6709\u9650\u7684\u6311\u6218\u3002\u534f\u540c\u63a8\u7406\u662f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u91cd\u8981\u65b9\u6cd5\uff0c\u9700\u8981\u5bf9\u667a\u80fd\u6a21\u578b\u8fdb\u884c\u5206\u533a\u548c\u5206\u5e03\u5f0f\u90e8\u7f72\u3002\u4e3a\u4e86\u6267\u884c\u6c34\u5e73\u5206\u533a\uff0c\u73b0\u6709\u7684\u534f\u540c\u63a8\u7406\u65b9\u6cd5\u8981\u4e48\u91c7\u7528\u7b97\u5b50\u7684\u8f93\u51fa\u901a\u9053\uff0c\u8981\u4e48\u91c7\u7528\u7279\u5f81\u56fe\u7684\u9ad8\u5ea6\u548c\u5bbd\u5ea6\u4f5c\u4e3a\u5206\u533a\u7ef4\u5ea6\u3002\u5728\u8fd9\u79cd\u65b9\u5f0f\u4e0b\uff0c\u7531\u4e8e\u7b97\u5b50\u7684\u6fc0\u6d3b\u662f\u5206\u5e03\u5f0f\u7684\uff0c\u56e0\u6b64\u5fc5\u987b\u5c06\u5b83\u4eec\u8fde\u63a5\u5728\u4e00\u8d77\uff0c\u7136\u540e\u624d\u80fd\u5c06\u5176\u9988\u9001\u5230\u4e0b\u4e00\u4e2a\u7b97\u5b50\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u534f\u540c\u63a8\u7406\u7684\u5ef6\u8fdf\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4e3aCNN\u6a21\u578b\u63d0\u51fa\u4e86\u4ea4\u9519\u7b97\u5b50\u5206\u533a\uff08IOP\uff09\u7b56\u7565\u3002\u901a\u8fc7\u57fa\u4e8e\u8f93\u51fa\u901a\u9053\u7ef4\u5ea6\u5bf9\u4e00\u4e2a\u7b97\u5b50\u8fdb\u884c\u5206\u533a\uff0c\u5e76\u57fa\u4e8e\u8f93\u5165\u901a\u9053\u7ef4\u5ea6\u5bf9\u5176\u540e\u7eed\u7b97\u5b50\u8fdb\u884c\u5206\u533a\uff0c\u53ef\u4ee5\u907f\u514d\u6fc0\u6d3b\u8fde\u63a5\uff0c\u4ece\u800c\u51cf\u5c11\u901a\u4fe1\u8fde\u63a5\u7684\u6570\u91cf\uff0c\u4ece\u800c\u51cf\u5c11\u534f\u540c\u63a8\u7406\u5ef6\u8fdf\u3002\u57fa\u4e8eIOP\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u5206\u5272\u7b97\u6cd5\uff0c\u7528\u4e8e\u6700\u5c0f\u5316\u534f\u540c\u63a8\u7406\u65f6\u95f4\uff0c\u8be5\u7b97\u6cd5\u6839\u636e\u83b7\u5f97\u7684\u63a8\u7406\u5ef6\u8fdf\u6536\u76ca\uff0c\u8d2a\u5a6a\u5730\u9009\u62e9\u7528\u4e8eIOP\u914d\u5bf9\u7684\u7b97\u5b50\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0eCoEdge\u4e2d\u4f7f\u7528\u7684\u6700\u5148\u8fdb\u7684\u5206\u533a\u65b9\u6cd5\u76f8\u6bd4\uff0cIOP\u7b56\u7565\u5728\u4e09\u4e2a\u7ecf\u5178\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e866.39%~16.83%\u7684\u52a0\u901f\uff0c\u5e76\u5c06\u5cf0\u503c\u5185\u5b58\u5360\u7528\u51cf\u5c11\u4e8621.22%~49.98%\u3002||\n", "2409.07582": "|**2024-09-11**|[Minimizing Embedding Distortion for Robust Out-of-Distribution Performance](http://arxiv.org/abs/2409.07582)|null|\u57fa\u4e8e\u5e9e\u5927\u4e14\u591a\u6837\u5316\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u5728\u5404\u79cd\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u8de8\u4e0d\u540c\u9886\u57df\u548c\u5206\u5e03\u6cdb\u5316\u7684\u975e\u51e1\u80fd\u529b\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u89e3\u51b3\u4e86\u5728\u901a\u8fc7\u5fae\u8c03\u4f7f\u57fa\u7840\u6a21\u578b\u9002\u5e94\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\u65f6\uff0c\u5982\u4f55\u4fdd\u7559\u8fd9\u4e9b\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u7684\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u76f8\u4f3c\u6027\u635f\u5931\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u878d\u5165\u5230\u4efb\u4f55\u4efb\u52a1\u7684\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u3002\u901a\u8fc7\u6700\u5c0f\u5316\u5fae\u8c03\u5d4c\u5165\u4e0e\u9884\u8bad\u7ec3\u5d4c\u5165\u4e4b\u95f4\u7684\u626d\u66f2\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u7279\u5b9a\u4efb\u52a1\u9002\u5e94\u548c\u4fdd\u6301\u5e7f\u6cdb\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff1a\u536b\u661f\u56fe\u50cf\u7684\u56fe\u50cf\u5206\u7c7b\u548c\u4eba\u8138\u8bc6\u522b\uff0c\u91cd\u70b9\u5173\u6ce8\u5f00\u653e\u7c7b\u522b\u548c\u9886\u57df\u8fc1\u79fb\u573a\u666f\uff0c\u4ee5\u8bc4\u4f30\u5206\u5e03\u5916 (OOD) \u6027\u80fd\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u5f3a\u5927\u7684\u5206\u5e03\u5185 (ID) \u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86 OOD \u6027\u80fd\u3002||\n", "2409.07541": "|**2024-09-11**|[ENACT: Entropy-based Clustering of Attention Input for Improving the Computational Performance of Object Detection Transformers](http://arxiv.org/abs/2409.07541)|**[link](https://github.com/gsavathrakis/enact)**|Transformer\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u5177\u6709\u7ade\u4e89\u529b\u7684\u7cbe\u5ea6\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6ce8\u610f\u529b\u6743\u91cd\u7684\u5e73\u65b9\u5927\u5c0f\uff0c\u5b83\u4eec\u9700\u8981\u76f8\u5f53\u5927\u7684\u8ba1\u7b97\u8d44\u6e90\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u6839\u636e\u8f93\u5165\u4fe1\u606f\u71b5\u5bf9transformer\u8f93\u5165\u8fdb\u884c\u805a\u7c7b\u3002\u8fd9\u6837\u505a\u7684\u539f\u56e0\u662f\uff0c\u6bcf\u4e2a\u50cf\u7d20\u7684\u81ea\u4fe1\u606f\uff08\u5176\u603b\u548c\u4e3a\u71b5\uff09\u5728\u5bf9\u5e94\u4e8e\u540c\u4e00\u5bf9\u8c61\u7684\u50cf\u7d20\u4e4b\u95f4\u53ef\u80fd\u662f\u76f8\u4f3c\u7684\u3002\u805a\u7c7b\u51cf\u5c11\u4e86\u4f5c\u4e3atransformer\u8f93\u5165\u7684\u6570\u636e\u91cf\uff0c\u56e0\u6b64\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u548cGPU\u5185\u5b58\u4f7f\u7528\u91cf\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u8981\u4f20\u9012\u5230\u7f51\u7edc\u5176\u4f59\u90e8\u5206\u7684\u6709\u610f\u4e49\u4fe1\u606f\u3002\u5efa\u8bae\u7684\u8fc7\u7a0b\u7ec4\u7ec7\u5728\u4e00\u4e2a\u540d\u4e3aENACT\u7684\u6a21\u5757\u4e2d\uff0c\u8be5\u6a21\u5757\u53ef\u4ee5\u63d2\u5165\u4efb\u4f55\u5728\u5176\u7f16\u7801\u5668\u4e2d\u5305\u542b\u591a\u5934\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u7684transformer\u67b6\u6784\u3002\u6211\u4eec\u4f7f\u7528COCO\u76ee\u6807\u68c0\u6d4b\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u68c0\u6d4btransformer\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u83b7\u5f97\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6240\u6709\u6d4b\u8bd5\u6848\u4f8b\u4e2d\uff0c\u6240\u9700\u7684\u8ba1\u7b97\u8d44\u6e90\u90fd\u6301\u7eed\u51cf\u5c11\uff0c\u800c\u68c0\u6d4b\u4efb\u52a1\u7684\u7cbe\u5ea6\u4ec5\u7565\u6709\u4e0b\u964d\u3002ENACT\u6a21\u5757\u7684\u4ee3\u7801\u5c06\u5728https://github.com/GSavathrakis/ENACT\u4e0a\u63d0\u4f9b\u3002||\n", "2409.07387": "|**2024-09-11**|[A Contrastive Symmetric Forward-Forward Algorithm (SFFA) for Continual Learning Tasks](http://arxiv.org/abs/2409.07387)|null|\u6240\u8c13\u7684\u201c\u6b63\u5411-\u6b63\u5411\u7b97\u6cd5\u201d(FFA) \u8fd1\u671f\u4f5c\u4e3a\u4e00\u79cd\u66ff\u4ee3\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u4e2d\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u7684\u65b0\u65b9\u6cd5\u83b7\u5f97\u4e86\u5173\u6ce8\uff0c\u5728\u5404\u79cd\u5efa\u6a21\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u901a\u8fc7\u7528\u4e24\u6b21\u5bf9\u6bd4\u6b63\u5411\u4f20\u9012\u4ee3\u66ff\u68af\u5ea6\u53cd\u5411\u4f20\u64ad\u7684\u53cd\u5411\u4f20\u9012\uff0cFFA \u901a\u8fc7\u542f\u7528\u9010\u5c42\u8bad\u7ec3\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u5176\u524d\u8eab\u6240\u7ecf\u5386\u7684\u51e0\u4e2a\u7f3a\u70b9\uff08\u4f8b\u5982\u68af\u5ea6\u6d88\u5931/\u7206\u70b8\uff09\u3002\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u8fd9\u79cd\u5bf9\u6bd4\u65b9\u6cd5\u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u6709\u6548\u5730\u521b\u5efa\u8f93\u5165\u6570\u636e\u7684\u6f5c\u5728\u7a00\u758f\u8868\u793a\uff0c\u6700\u7ec8\u6709\u5229\u4e8e\u533a\u5206\u6027\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6b63\u8d1f\u6570\u636e\u4e4b\u95f4\u635f\u5931\u51fd\u6570\u7684\u4e0d\u5e73\u8861\uff0cFFA \u8868\u73b0\u51fa\u56fa\u6709\u7684\u4e0d\u5bf9\u79f0\u68af\u5ea6\u884c\u4e3a\uff0c\u8fd9\u4f1a\u5bf9\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u5e76\u5bfc\u81f4\u51c6\u786e\u6027\u4e0b\u964d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u5bf9\u79f0\u6b63\u5411-\u6b63\u5411\u7b97\u6cd5 (SFFA)\uff0c\u8fd9\u662f\u5bf9\u539f\u59cb FFA \u7684\u4e00\u79cd\u65b0\u9896\u6539\u8fdb\uff0c\u5b83\u5c06\u6bcf\u4e00\u5c42\u5212\u5206\u4e3a\u6b63\u795e\u7ecf\u5143\u548c\u8d1f\u795e\u7ecf\u5143\u3002\u8fd9\u5141\u8bb8\u5c06\u5c40\u90e8\u9002\u5e94\u5ea6\u51fd\u6570\u5b9a\u4e49\u4e3a\u6b63\u795e\u7ecf\u5143\u6fc0\u6d3b\u4e0e\u6574\u4f53\u5c42\u6d3b\u52a8\u4e4b\u95f4\u7684\u6bd4\u7387\uff0c\u4ece\u800c\u5728\u8bad\u7ec3\u9636\u6bb5\u4ea7\u751f\u5bf9\u79f0\u7684\u635f\u5931\u60c5\u51b5\u3002\u4e3a\u4e86\u8bc4\u4f30\u6211\u4eec\u65b9\u6cd5\u589e\u5f3a\u7684\u6536\u655b\u6027\uff0c\u6211\u4eec\u4f7f\u7528\u591a\u4e2a\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u8fdb\u884c\u4e86\u591a\u9879\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u4f7f\u7528 SFFA \u8bad\u7ec3\u7684\u6a21\u578b\u4e0e\u5176\u4f7f\u7528 FFA \u8bad\u7ec3\u7684\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002\u4f5c\u4e3a\u8fd9\u79cd\u91cd\u65b0\u8868\u8ff0\u7684\u526f\u4ea7\u54c1\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u5c06\u9010\u5c42\u8bad\u7ec3\u7b97\u6cd5\u7528\u4e8e\u6301\u7eed\u5b66\u4e60 (CL) \u4efb\u52a1\u7684\u4f18\u52bf\u3002\u9010\u5c42\u8bad\u7ec3\u7b97\u6cd5\u5f15\u8d77\u7684\u795e\u7ecf\u5143\u7279\u5316\u53ca\u5176\u6fc0\u6d3b\u7684\u7a00\u758f\u6027\u4f7f\u5f97\u80fd\u591f\u5b9e\u73b0\u6709\u6548\u7684 CL \u7b56\u7565\uff0c\u5c06\u65b0\u77e5\u8bc6\uff08\u7c7b\u522b\uff09\u6574\u5408\u5230\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u540c\u65f6\u9632\u6b62\u707e\u96be\u6027\u5730\u9057\u5fd8\u5148\u524d...||\n", "2409.07322": "|**2024-09-11**|[Three-Dimensional, Multimodal Synchrotron Data for Machine Learning Applications](http://arxiv.org/abs/2409.07322)|**[link](https://github.com/calum-green/xct-xdrct_paper_code)**|Machine learning techniques are being increasingly applied in medical and physical sciences across a variety of imaging modalities; however, an important issue when developing these tools is the availability of good quality training data. Here we present a unique, multimodal synchrotron dataset of a bespoke zinc-doped Zeolite 13X sample that can be used to develop advanced deep learning and data fusion pipelines. Multi-resolution micro X-ray computed tomography was performed on a zinc-doped Zeolite 13X fragment to characterise its pores and features, before spatially resolved X-ray diffraction computed tomography was carried out to characterise the homogeneous distribution of sodium and zinc phases. Zinc absorption was controlled to create a simple, spatially isolated, two-phase material. Both raw and processed data is available as a series of Zenodo entries. Altogether we present a spatially resolved, three-dimensional, multimodal, multi-resolution dataset that can be used for the development of machine learning techniques. Such techniques include development of super-resolution, multimodal data fusion, and 3D reconstruction algorithm development.||\n", "2409.09031": "|**2024-09-13**|[Optically-Validated Microvascular Phantom for Super-Resolution Ultrasound Imaging](http://arxiv.org/abs/2409.09031)|null|\u8d85\u5206\u8fa8\u7387\u8d85\u58f0 (SRUS) \u901a\u8fc7\u5b9a\u4f4d\u548c\u8ddf\u8e2a\u7a7a\u95f4\u9694\u79bb\u7684\u5fae\u6ce1\u9020\u5f71\u5242\uff0c\u53ef\u89c6\u5316\u8d85\u58f0\u884d\u5c04\u6781\u9650\uff08\u6ce2\u957f ($\u03bb$)/2\uff09\u4ee5\u5916\u7684\u5fae\u8840\u7ba1\u7ed3\u6784\u3002SRUS \u6a21\u578b\u901a\u5e38\u7531\u7b80\u5355\u7684\u7ba1\u72b6\u7ed3\u6784\u7ec4\u6210\uff0c\u5176\u4e2d\u76f4\u5f84\u5c0f\u4e8e 100 \u5fae\u7c73\u7684\u901a\u9053\u4e0d\u53ef\u7528\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u6613\u788e\u4e14\u4e0d\u7a33\u5b9a\uff0c\u771f\u503c\u9a8c\u8bc1\u6709\u9650\uff0c\u5e76\u4e14\u5176\u7b80\u5355\u7684\u7ed3\u6784\u9650\u5236\u4e86 SRUS \u7b97\u6cd5\u7684\u8bc4\u4f30\u3002\u4e3a\u4e86\u5e2e\u52a9 SRUS \u7684\u5f00\u53d1\uff0c\u9700\u8981\u5177\u6709\u5df2\u77e5\u4e14\u751f\u7406\u76f8\u5173\u7684\u5fae\u8840\u7ba1\u7ed3\u6784\u7684\u575a\u56fa\u8010\u7528\u7684\u6a21\u578b\uff0c\u4ee5\u4fbf\u8fdb\u884c\u53ef\u91cd\u590d\u7684 SRUS \u6d4b\u8bd5\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u5236\u9020\u8010\u7528\u5fae\u8840\u7ba1\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u8be5\u6a21\u578b\u5141\u8bb8\u8fdb\u884c\u5149\u5b66\u6d4b\u91cf\u4ee5\u8fdb\u884c SRUS \u9a8c\u8bc1\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u5d4c\u5165\u805a\u4e8c\u7532\u57fa\u7845\u6c27\u70f7\u4e2d\u7684\u5fae\u8840\u7ba1\u9634\u6a21\u6765\u5236\u9020\u5fae\u8840\u7ba1\u6a21\u578b\u3002\u5c55\u793a\u4e86\u5177\u6709\u53ef\u53d8\u5fae\u8840\u7ba1\u5bc6\u5ea6\u7684\u5206\u652f\u5fae\u8840\u7ba1\u6a21\u578b\uff0c\u5176\u5149\u5b66\u9a8c\u8bc1\u7684\u8840\u7ba1\u76f4\u5f84\u4f4e\u81f3\u7ea6 60 \u5fae\u7c73\uff08\u03bb/5.8\uff1b\u03bb = \u7ea6 350 \u5fae\u7c73\uff09\u3002\u8fdb\u884c\u4e86 SRUS \u6210\u50cf\u5e76\u901a\u8fc7\u5149\u5b66\u6d4b\u91cf\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u5e73\u5747 SRUS \u8bef\u5dee\u4e3a 15.61 \u5fae\u7c73\uff08\u03bb/22\uff09\uff0c\u6807\u51c6\u504f\u5dee\u8bef\u5dee\u4e3a 11.44 \u5fae\u7c73\u3002\u4e00\u65e6\u5b9a\u4f4d\u7684\u5fae\u6ce1\u6570\u91cf\u8d85\u8fc7\u6bcf\u4e2a\u4f30\u8ba1\u76f4\u5f84 1000 \u4e2a\uff0c\u5e73\u5747\u8bef\u5dee\u964d\u4f4e\u81f3 7.93 \u5fae\u7c73\uff08\u03bb/44\uff09\u3002\u6b64\u5916\uff0c\u5236\u9020\u4e00\u5e74\u540e\u6d4b\u5f97\u7684\u58f0\u5b66\u548c\u5149\u5b66\u7279\u6027\u53d8\u5316\u5c0f\u4e8e 10% \u4ee5\u53ca\u6a21\u578b\u7684\u673a\u68b0\u97e7\u6027\u8bc1\u660e\u4e86\u5176\u957f\u671f\u8010\u7528\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u5236\u9020\u8010\u7528\u4e14\u7ecf\u8fc7\u5149\u5b66\u9a8c\u8bc1\u7684\u590d\u6742\u5fae\u8840\u7ba1\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u8be5\u6a21\u578b\u53ef\u7528\u4e8e\u91cf\u5316 SRUS \u6027\u80fd\u5e76\u4fc3\u8fdb\u5176\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002||\n", "2409.08943": "|**2024-09-13**|[Pushing Joint Image Denoising and Classification to the Edge](http://arxiv.org/abs/2409.08943)|null|\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06\u56fe\u50cf\u5206\u7c7b\u548c\u56fe\u50cf\u53bb\u566a\u76f8\u7ed3\u5408\uff0c\u65e8\u5728\u589e\u5f3a\u4eba\u7c7b\u5bf9\u8fb9\u7f18\u8bbe\u5907\uff08\u5982\u4f4e\u7167\u5ea6\u76d1\u63a7\u6444\u50cf\u5934\uff09\u6240\u62cd\u6444\u566a\u58f0\u56fe\u50cf\u7684\u611f\u77e5\u80fd\u529b\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u91cd\u8981\u7684\u662f\u8981\u4fdd\u7559\u4eba\u7c7b\u9a8c\u8bc1\u81ea\u52a8\u5206\u7c7b\u51b3\u7b56\u7684\u80fd\u529b\uff0c\u4ece\u800c\u8054\u5408\u5bf9\u56fe\u50cf\u8fdb\u884c\u53bb\u566a\u4ee5\u589e\u5f3a\u4eba\u7c7b\u611f\u77e5\u3002\u7531\u4e8e\u8fb9\u7f18\u8bbe\u5907\u8ba1\u7b97\u80fd\u529b\u6709\u9650\uff0c\u6211\u4eec\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u96c6\u6210\u8fd9\u4e24\u9879\u4efb\u52a1\u7684\u65b0\u578b\u67b6\u6784\u6765\u660e\u786e\u4f18\u5316\u6548\u7387\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4fee\u6539\u4e86\u4e00\u79cd\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u641c\u7d22\u5206\u7c7b\u5668\u4ee5\u641c\u7d22\u96c6\u6210\u6a21\u578b\uff0c\u540c\u65f6\u4f18\u5316\u76ee\u6807\u5ef6\u8fdf\u3001\u5206\u7c7b\u7cbe\u5ea6\u548c\u53bb\u566a\u6027\u80fd\u3002NAS \u67b6\u6784\u5728\u53bb\u566a\u548c\u5206\u7c7b\u65b9\u9762\u5747\u4f18\u4e8e\u6211\u4eec\u624b\u52a8\u8bbe\u8ba1\u7684\u65b9\u6848\uff0c\u53ef\u663e\u8457\u6539\u5584\u4eba\u7c7b\u611f\u77e5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u6237\u80fd\u591f\u6784\u5efa\u9488\u5bf9\u533b\u7597\u6210\u50cf\u3001\u76d1\u63a7\u7cfb\u7edf\u548c\u5de5\u4e1a\u68c0\u6d4b\u7b49\u9886\u57df\u7684\u5b9a\u5236\u67b6\u6784\u3002||\n", "2409.08885": "|**2024-09-13**|[Interactive Masked Image Modeling for Multimodal Object Detection in Remote Sensing](http://arxiv.org/abs/2409.08885)|null|\u9065\u611f\u5f71\u50cf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u5728\u5730\u7403\u89c2\u6d4b\u7684\u5404\u4e2a\u5e94\u7528\u4e2d\u90fd\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u4e0e\u81ea\u7136\u573a\u666f\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u4e0d\u540c\uff0c\u7531\u4e8e\u4e0d\u540c\u5730\u5f62\u4e2d\u5b58\u5728\u5927\u91cf\u7684\u5c0f\u578b\u4e14\u901a\u5e38\u96be\u4ee5\u5bdf\u89c9\u7684\u76ee\u6807\uff0c\u8fd9\u9879\u4efb\u52a1\u5c24\u5176\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u53ef\u4ee5\u4f7f\u7528\u591a\u6a21\u6001\u5b66\u4e60\u6765\u6574\u5408\u6765\u81ea\u4e0d\u540c\u6570\u636e\u6a21\u6001\u7684\u7279\u5f81\uff0c\u4ece\u800c\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002\u7136\u800c\uff0c\u591a\u6a21\u6001\u5b66\u4e60\u7684\u6027\u80fd\u5f80\u5f80\u53d7\u5230\u6807\u8bb0\u6570\u636e\u96c6\u5927\u5c0f\u6709\u9650\u7684\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528\u63a9\u853d\u56fe\u50cf\u5efa\u6a21\uff08MIM\uff09\u4f5c\u4e3a\u9884\u8bad\u7ec3\u6280\u672f\uff0c\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6765\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684 MIM \u65b9\u6cd5\uff08\u5982 MAE\uff09\u4f7f\u7528\u4e0d\u5305\u542b\u4efb\u4f55\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u63a9\u7801\u6807\u8bb0\uff0c\u7531\u4e8e\u7f3a\u4e4f\u4e0e\u56fe\u50cf\u5176\u4ed6\u90e8\u5206\u7684\u4ea4\u4e92\uff0c\u96be\u4ee5\u6355\u6349\u5230\u7ec6\u7c92\u5ea6\u7684\u7ec6\u8282\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ea4\u4e92\u5f0f MIM \u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u540c\u6807\u8bb0\u4e4b\u95f4\u5efa\u7acb\u4ea4\u4e92\uff0c\u8fd9\u5bf9\u4e8e\u9065\u611f\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u7279\u522b\u6709\u5229\u3002\u5927\u91cf\u7684\u6d88\u878d\u7814\u7a76\u548c\u8bc4\u4f30\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002||\n", "2409.08840": "|**2024-09-13**|[Direct-CP: Directed Collaborative Perception for Connected and Autonomous Vehicles via Proactive Attention](http://arxiv.org/abs/2409.08840)|null|\u534f\u540c\u611f\u77e5 (CP) \u5229\u7528\u6765\u81ea\u8054\u7f51\u548c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86 (CAV) \u7684\u89c6\u89c9\u6570\u636e\u6765\u589e\u5f3a\u81ea\u8f66\u89c6\u91ce (FoV)\u3002\u5c3d\u7ba1\u6700\u8fd1\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u76ee\u524d\u7684 CP \u65b9\u6cd5\u51e0\u4e4e\u5e73\u7b49\u5730\u6269\u5c55\u4e86\u81ea\u8f66\u7684 360 \u5ea6\u611f\u77e5\u8303\u56f4\uff0c\u8fd9\u9762\u4e34\u7740\u4e24\u4e2a\u5173\u952e\u6311\u6218\u3002\u9996\u5148\uff0c\u5728\u4ea4\u901a\u5206\u5e03\u4e0d\u5747\u5300\u7684\u5730\u533a\uff0c\u5173\u6ce8\u4ea4\u901a\u6d41\u91cf\u5c0f\u7684\u65b9\u5411\u5e26\u6765\u7684\u597d\u5904\u6709\u9650\u3002\u5176\u6b21\uff0c\u5728\u6709\u9650\u7684\u901a\u4fe1\u9884\u7b97\u4e0b\uff0c\u4e3a\u4e0d\u592a\u91cd\u8981\u7684\u65b9\u5411\u5206\u914d\u8fc7\u591a\u7684\u5e26\u5bbd\u4f1a\u964d\u4f4e\u66f4\u91cd\u8981\u533a\u57df\u7684\u611f\u77e5\u7cbe\u5ea6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Direct-CP\uff0c\u4e00\u79cd\u4e3b\u52a8\u4e14\u65b9\u5411\u611f\u77e5\u7684 CP \u7cfb\u7edf\uff0c\u65e8\u5728\u6539\u5584\u7279\u5b9a\u65b9\u5411\u7684 CP\u3002\u6211\u4eec\u7684\u6838\u5fc3\u7406\u5ff5\u662f\u4f7f\u81ea\u8f66\u80fd\u591f\u4e3b\u52a8\u53d1\u51fa\u5176\u611f\u5174\u8da3\u65b9\u5411\u7684\u4fe1\u53f7\uff0c\u5e76\u91cd\u65b0\u8c03\u6574\u5176\u6ce8\u610f\u529b\u4ee5\u589e\u5f3a\u5c40\u90e8\u65b9\u5411\u6027 CP \u6027\u80fd\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u79cd RSU \u8f85\u52a9\u65b9\u5411\u63a9\u853d\u673a\u5236\uff0c\u4ee5\u5e2e\u52a9\u81ea\u8f66\u8bc6\u522b\u91cd\u8981\u65b9\u5411\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b9\u5411\u611f\u77e5\u7684\u9009\u62e9\u6027\u6ce8\u610f\u6a21\u5757\uff0c\u6839\u636e\u81ea\u8f66\u7684\u65b9\u5411\u4f18\u5148\u7ea7\u3001\u901a\u4fe1\u9884\u7b97\u548c CAV \u7684\u4f4d\u7f6e\u6570\u636e\uff0c\u660e\u667a\u5730\u805a\u5408\u76f8\u5173\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u65b9\u5411\u52a0\u6743\u68c0\u6d4b\u635f\u5931 (DWLoss) \u6765\u6355\u6349\u65b9\u5411\u6027 CP \u7ed3\u679c\u4e0e\u771f\u5b9e\u60c5\u51b5\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4ece\u800c\u4fc3\u8fdb\u6709\u6548\u7684\u6a21\u578b\u8bad\u7ec3\u3002\u5728 V2X-Sim 2.0 \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u534f\u4f5c 3D \u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u611f\u5174\u8da3\u65b9\u5411\u7684\u5c40\u90e8\u611f\u77e5\u7cbe\u5ea6\u63d0\u9ad8\u4e86 19.8%\uff0c\u6574\u4f53\u611f\u77e5\u7cbe\u5ea6\u63d0\u9ad8\u4e86 2.5%\u3002||\n", "2409.08667": "|**2024-09-13**|[Test-time Training for Hyperspectral Image Super-resolution](http://arxiv.org/abs/2409.08667)|null|\u9ad8\u5149\u8c31\u56fe\u50cf (HSI) \u8d85\u5206\u8fa8\u7387 (SR) \u7684\u7814\u7a76\u8fdb\u5c55\u4ecd\u7136\u843d\u540e\u4e8e RGB \u56fe\u50cf SR \u7684\u7814\u7a76\u3002HSI \u901a\u5e38\u5177\u6709\u5927\u91cf\u7684\u6ce2\u6bb5\uff0c\u56e0\u6b64\u51c6\u786e\u5730\u6a21\u62df HSI SR \u7684\u6ce2\u6bb5\u95f4\u4ea4\u4e92\u975e\u5e38\u56f0\u96be\u3002\u6b64\u5916\uff0cHSI SR \u7684\u8bad\u7ec3\u6570\u636e\u96be\u4ee5\u83b7\u53d6\uff0c\u56e0\u6b64\u6570\u636e\u96c6\u901a\u5e38\u5f88\u5c0f\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684\u81ea\u8bad\u7ec3\u6846\u67b6\uff0c\u53ef\u4ee5\u751f\u6210\u66f4\u51c6\u786e\u7684\u4f2a\u6807\u7b7e\u548c\u66f4\u51c6\u786e\u7684 LR-HR \u5173\u7cfb\uff0c\u4ee5\u4fbf\u6a21\u578b\u53ef\u4ee5\u4f7f\u7528\u5b83\u4eec\u8fdb\u884c\u8fdb\u4e00\u6b65\u8bad\u7ec3\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u652f\u6301\u6211\u4eec\u7684\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7f51\u7edc\u67b6\u6784\u6765\u5b66\u4e60 HSI SR\uff0c\u800c\u65e0\u9700\u5bf9\u6ce2\u6bb5\u95f4\u4ea4\u4e92\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5 Spectral Mixup\uff0c\u4ee5\u589e\u52a0\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u6570\u636e\u7684\u7684\u591a\u6837\u6027\u3002\u6211\u4eec\u8fd8\u6536\u96c6\u4e86\u4e00\u4e2a\u65b0\u7684 HSI \u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4ece\u98df\u7269\u5230\u690d\u88ab\u3001\u6750\u6599\u548c\u4e00\u822c\u573a\u666f\u7b49\u5404\u79cd\u6709\u8da3\u5bf9\u8c61\u7684\u56fe\u50cf\u3002\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u540e\u663e\u7740\u63d0\u9ad8\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5728 HSI SR \u65b9\u9762\u663e\u7740\u4f18\u4e8e\u7ade\u4e89\u65b9\u6cd5\u3002||\n", "2409.08650": "|**2024-09-13**|[Low Complexity DoA-ToA Signature Estimation for Multi-Antenna Multi-Carrier Systems](http://arxiv.org/abs/2409.08650)|null|\u51c6\u786e\u7684\u65b9\u5411\u4f30\u8ba1 (DoA) \u548c\u5230\u8fbe\u65f6\u95f4 (ToA) \u4f30\u8ba1\u662f\u58f0\u7eb3\u3001\u96f7\u8fbe\u3001\u901a\u4fe1\u548c\u53cc\u529f\u80fd\u96f7\u8fbe\u901a\u4fe1 (DFRC) \u7b49\u591a\u79cd\u65e0\u7ebf\u7cfb\u7edf\u7684\u4e25\u683c\u8981\u6c42\u3002\u7531\u4e8e\u4f7f\u7528\u9ad8\u8f7d\u6ce2\u9891\u7387\u548c\u5e26\u5bbd\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5927\u591a\u6570\u8bbe\u8ba1\u6709\u591a\u4e2a\u5929\u7ebf\u548c\u5b50\u8f7d\u6ce2\u3002\u5c3d\u7ba1\u5927\u9635\u5217\u673a\u5236\u4e0b\u7684\u5206\u8fa8\u7387\u5f88\u9ad8\uff0c\u4f46\u7531\u4e8e\u9891\u8c31\u6cc4\u6f0f\u6548\u5e94\uff0c\u5b9e\u9645\u7684\u7f51\u683c\u4f30\u8ba1\u65b9\u6cd5\u7684 DoA-ToA \u4f30\u8ba1\u7cbe\u5ea6\u4ecd\u7136\u5b58\u5728\u4f30\u8ba1\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u9488\u5bf9\u5177\u6709\u6b63\u4ea4\u9891\u5206\u590d\u7528 (OFDM) \u4fe1\u53f7\u7684\u591a\u5929\u7ebf\u591a\u8f7d\u6ce2\u7cfb\u7edf\u7684 DoA-ToA \u4f30\u8ba1\u65b9\u6cd5\u3002\u5728\u7b2c\u4e00\u79cd\u65b9\u6cd5\u4e2d\uff0c\u6211\u4eec\u5e94\u7528\u4e86\u57fa\u4e8e\u79bb\u6563\u5085\u7acb\u53f6\u53d8\u6362 (DFT) \u7684\u7c97\u7565\u7279\u5f81\u4f30\u8ba1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7684\u591a\u7ea7\u5fae\u8c03\u65b9\u6cd5\uff0c\u4ee5\u6781\u5927\u5730\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u3002\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u57fa\u4e8e\u538b\u7f29\u611f\u77e5\uff0c\u5176\u4e2d\u6211\u4eec\u901a\u8fc7\u91c7\u7528\u6bd4\u5929\u7ebf\u548c\u5b50\u8f7d\u6ce2\u57fa\u6570\u5b9e\u9645\u6570\u91cf\u66f4\u591a\u7684\u4e8c\u7ef4\u8fc7\u5b8c\u5907\u89d2\u5ea6\u5ef6\u8fdf\u5b57\u5178\u6765\u5b9e\u73b0\u8d85\u5206\u8fa8\u7387\u3002\u4e0e\u5411\u91cf\u5316\u4e00\u7ef4\u6b63\u4ea4\u5339\u914d\u8ffd\u8e2a (OMP) \u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u5c06\u4f4e\u590d\u6742\u5ea6\u7684\u4e8c\u7ef4 OMP \u65b9\u6cd5\u5e94\u7528\u4e8e\u77e9\u9635\u6570\u636e\u6a21\u578b\uff0c\u8fd9\u4f7f\u5f97\u5728\u5927\u578b\u9635\u5217\u673a\u5236\u4e2d\u4f7f\u7528\u538b\u7f29\u611f\u77e5\u65b9\u6cd5\u53d8\u5f97\u5207\u5b9e\u53ef\u884c\u3002\u901a\u8fc7\u6570\u503c\u4eff\u771f\uff0c\u6211\u4eec\u8868\u660e\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e0e\u57fa\u4e8e\u5b50\u7a7a\u95f4\u7684\u4e8c\u7ef4\u591a\u91cd\u4fe1\u53f7\u5206\u7c7b (MUSIC) \u65b9\u6cd5\u76f8\u4f3c\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u663e\u7740\u964d\u4f4e\u3002||\n", "2409.08640": "|**2024-09-13**|[Byzantine-Robust and Communication-Efficient Distributed Learning via Compressed Momentum Filtering](http://arxiv.org/abs/2409.08640)|null|\u5206\u5e03\u5f0f\u5b66\u4e60\u5df2\u6210\u4e3a\u8de8\u79c1\u6709\u6570\u636e\u5b64\u5c9b\u8bad\u7ec3\u5927\u89c4\u6a21\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6807\u51c6\u65b9\u6cd5\u3002\u867d\u7136\u5206\u5e03\u5f0f\u5b66\u4e60\u589e\u5f3a\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u4f46\u5b83\u4e5f\u9762\u4e34\u7740\u4e0e\u62dc\u5360\u5ead\u9c81\u68d2\u6027\u548c\u901a\u4fe1\u51cf\u5c11\u76f8\u5173\u7684\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7684\u62dc\u5360\u5ead\u9c81\u68d2\u4e14\u9ad8\u6548\u901a\u4fe1\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6bcf\u6b21\u8fed\u4ee3\u6216\u4ee5\u4e00\u5b9a\u6982\u7387\u5728\u67d0\u4e9b\u8fed\u4ee3\u4e2d\u83b7\u5f97\u5b8c\u6574\u7684\u68af\u5ea6\u4fe1\u606f\uff0c\u5e76\u4e14\u5b83\u4eec\u4ec5\u6536\u655b\u5230\u89e3\u5468\u56f4\u4e00\u4e2a\u4e0d\u5fc5\u8981\u7684\u5927\u7684\u90bb\u57df\u3002\u57fa\u4e8e\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u62dc\u5360\u5ead\u9c81\u68d2\u4e14\u9ad8\u6548\u901a\u4fe1\u7684\u968f\u673a\u5206\u5e03\u5f0f\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5bf9\u6279\u91cf\u5927\u5c0f\u6ca1\u6709\u4efb\u4f55\u8981\u6c42\uff0c\u5e76\u4e14\u6536\u655b\u5230\u6bd4\u6240\u6709\u73b0\u6709\u65b9\u6cd5\u90fd\u66f4\u63a5\u8fd1\u6700\u4f18\u89e3\u7684\u5c0f\u90bb\u57df\uff0c\u4e0e\u7406\u8bba\u4e0b\u754c\u4e00\u81f4\u3002\u6211\u4eec\u7684\u5173\u952e\u521b\u65b0\u662f\u5229\u7528 Polyak \u52a8\u91cf\u6765\u51cf\u8f7b\u7531\u6709\u504f\u538b\u7f29\u5668\u548c\u968f\u673a\u68af\u5ea6\u5f15\u8d77\u7684\u566a\u58f0\uff0c\u4ece\u800c\u5728\u4fe1\u606f\u538b\u7f29\u7684\u60c5\u51b5\u4e0b\u9632\u5fa1\u62dc\u5360\u5ead\u5de5\u4f5c\u8005\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u5728\u975e\u51f8\u5e73\u6ed1\u635f\u5931\u51fd\u6570\u7684\u80cc\u666f\u4e0b\uff0c\u6211\u4eec\u7b97\u6cd5\u7684\u7d27\u590d\u6742\u5ea6\u754c\u9650\u7684\u8bc1\u660e\uff0c\u8bc1\u660e\u8fd9\u4e9b\u754c\u9650\u4e0e\u65e0\u62dc\u5360\u5ead\u573a\u666f\u4e2d\u7684\u4e0b\u754c\u76f8\u5339\u914d\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u4e00\u7cfb\u5217\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7b97\u6cd5\u7684\u5b9e\u9645\u610f\u4e49\uff0c\u5bf9\u4e8c\u8fdb\u5236\u5206\u7c7b\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002||\n", "2409.08551": "|**2024-09-13**|[Think Twice Before You Act: Improving Inverse Problem Solving With MCMC](http://arxiv.org/abs/2409.08551)|null|\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u89e3\u51b3\u9006\u95ee\u9898\u7684\u5f3a\u6709\u529b\u5148\u9a8c\u3002\u4e00\u4e2a\u7a81\u51fa\u7684\u4f8b\u5b50\u662f\u6269\u6563\u540e\u9a8c\u91c7\u6837\uff08DPS\uff09\uff0c\u5b83\u4f7f\u7528Tweedie\u516c\u5f0f\u6765\u8fd1\u4f3c\u7ed9\u5b9a\u6d4b\u91cf\u503c\u7684\u6570\u636e\u540e\u9a8c\u5206\u5e03\u3002\u5c3d\u7ba1DPS\u5728\u89e3\u51b3\u5404\u79cd\u9006\u95ee\u9898\u65f6\u5177\u6709\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u4f18\u70b9\uff0c\u4f46\u7531\u4e8e\u8fd9\u79cd\u540e\u9a8c\u8fd1\u4f3c\u53ef\u80fd\u4e0d\u51c6\u786e\uff0c\u7279\u522b\u662f\u5728\u9ad8\u566a\u58f0\u6c34\u5e73\u4e0b\uff0c\u56e0\u6b64\u5176\u6027\u80fd\u53d7\u5230\u9650\u5236\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6269\u6563\u540e\u9a8cMCMC\uff08DPMC\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u9000\u706bMCMC\u7684\u65b0\u578b\u63a8\u7406\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4f7f\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u9006\u95ee\u9898\u3002\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u7cfb\u5217\u4e2d\u95f4\u5206\u5e03\uff0c\u5176\u7075\u611f\u6765\u81eaDPS\u4f7f\u7528\u7684\u8fd1\u4f3c\u6761\u4ef6\u5206\u5e03\u3002\u901a\u8fc7\u9000\u706bMCMC\u91c7\u6837\uff0c\u6211\u4eec\u9f13\u52b1\u6837\u672c\u5728\u79fb\u52a8\u5230\u566a\u58f0\u6c34\u5e73\u8f83\u4f4e\u7684\u4e0b\u4e00\u4e2a\u5206\u5e03\u4e4b\u524d\uff0c\u66f4\u7d27\u5bc6\u5730\u9075\u5faa\u6bcf\u4e2a\u4e2d\u95f4\u5206\u5e03\uff0c\u4ece\u800c\u51cf\u5c11\u6cbf\u8def\u5f84\u7d2f\u79ef\u7684\u8bef\u5dee\u3002\u6211\u4eec\u5728\u5404\u79cd\u9006\u95ee\u9898\u4e2d\u6d4b\u8bd5\u4e86\u6211\u4eec\u7684\u7b97\u6cd5\uff0c\u5305\u62ec\u8d85\u5206\u8fa8\u7387\u3001\u9ad8\u65af\u53bb\u6a21\u7cca\u3001\u8fd0\u52a8\u53bb\u6a21\u7cca\u3001\u4fee\u590d\u548c\u76f8\u4f4d\u68c0\u7d22\u3002\u6211\u4eec\u7684\u7b97\u6cd5\u5728\u51e0\u4e4e\u6240\u6709\u4efb\u52a1\u4e2d\u90fd\u4f18\u4e8eDPS\uff0c\u5e76\u4e14\u8bc4\u4f30\u6b21\u6570\u66f4\u5c11\uff0c\u5e76\u4e14\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002||\n", "2409.08376": "|**2024-09-12**|[Learned Compression for Images and Point Clouds](http://arxiv.org/abs/2409.08376)|**[link](https://github.com/multimedialabsfu/learned-point-cloud-compression-for-classification)**|\u5728\u8fc7\u53bb\u5341\u5e74\u4e2d\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u6267\u884c\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08\u5305\u62ec\u5206\u7c7b\u3001\u8d85\u5206\u8fa8\u7387\u548c\u98ce\u683c\u8fc1\u79fb\uff09\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u73b0\u5728\uff0c\u6211\u4eec\u5c06\u5176\u5e94\u7528\u4e8e\u6570\u636e\u538b\u7f29\uff0c\u4ee5\u5e2e\u52a9\u6784\u5efa\u4e0b\u4e00\u4ee3\u591a\u5a92\u4f53\u7f16\u89e3\u7801\u5668\u3002\u672c\u8bba\u6587\u5bf9\u8fd9\u4e00\u65b0\u5174\u7684\u5b66\u4e60\u538b\u7f29\u9886\u57df\u505a\u51fa\u4e86\u4e09\u4e2a\u4e3b\u8981\u8d21\u732e\u3002\u9996\u5148\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4f4e\u590d\u6742\u5ea6\u71b5\u6a21\u578b\uff0c\u5b83\u901a\u8fc7\u5c06\u7f16\u7801\u5206\u5e03\u672c\u8eab\u4f5c\u4e3a\u8fb9\u4fe1\u606f\u8fdb\u884c\u538b\u7f29\u548c\u4f20\u8f93\uff0c\u4ece\u800c\u52a8\u6001\u5730\u4f7f\u7f16\u7801\u5206\u5e03\u9002\u5e94\u7279\u5b9a\u7684\u8f93\u5165\u3002\u5176\u6b21\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8f7b\u91cf\u7ea7\u4f4e\u590d\u6742\u5ea6\u70b9\u4e91\u7f16\u89e3\u7801\u5668\uff0c\u8be5\u7f16\u89e3\u7801\u5668\u4e13\u95e8\u9488\u5bf9\u5206\u7c7b\u8fdb\u884c\u4e86\u9ad8\u5ea6\u4f18\u5316\uff0c\u4e0e\u975e\u4e13\u95e8\u7f16\u89e3\u7801\u5668\u76f8\u6bd4\uff0c\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u6bd4\u7279\u7387\u3002\u6700\u540e\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86\u8fde\u7eed\u89c6\u9891\u5e27\u4e4b\u95f4\u8f93\u5165\u57df\u5185\u7684\u8fd0\u52a8\u662f\u5982\u4f55\u4f53\u73b0\u5728\u76f8\u5e94\u7684\u5377\u79ef\u5bfc\u51fa\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u3002||\n", "2409.11235": "|**2024-09-17**|[SLAck: Semantic, Location, and Appearance Aware Open-Vocabulary Tracking](http://arxiv.org/abs/2409.11235)|**[link](https://github.com/siyuanliii/slack)**|Open-vocabulary Multiple Object Tracking (MOT) aims to generalize trackers to novel categories not in the training set. Currently, the best-performing methods are mainly based on pure appearance matching. Due to the complexity of motion patterns in the large-vocabulary scenarios and unstable classification of the novel objects, the motion and semantics cues are either ignored or applied based on heuristics in the final matching steps by existing methods. In this paper, we present a unified framework SLAck that jointly considers semantics, location, and appearance priors in the early steps of association and learns how to integrate all valuable information through a lightweight spatial and temporal object graph. Our method eliminates complex post-processing heuristics for fusing different cues and boosts the association performance significantly for large-scale open-vocabulary tracking. Without bells and whistles, we outperform previous state-of-the-art methods for novel classes tracking on the open-vocabulary MOT and TAO TETA benchmarks. Our code is available at \\href{https://github.com/siyuanliii/SLAck}{github.com/siyuanliii/SLAck}.||\n", "2409.11234": "|**2024-09-17**|[STCMOT: Spatio-Temporal Cohesion Learning for UAV-Based Multiple Object Tracking](http://arxiv.org/abs/2409.11234)|**[link](https://github.com/ydhcg-bobo/stcmot)**|Multiple object tracking (MOT) in Unmanned Aerial Vehicle (UAV) videos is important for diverse applications in computer vision. Current MOT trackers rely on accurate object detection results and precise matching of target reidentification (ReID). These methods focus on optimizing target spatial attributes while overlooking temporal cues in modelling object relationships, especially for challenging tracking conditions such as object deformation and blurring, etc. To address the above-mentioned issues, we propose a novel Spatio-Temporal Cohesion Multiple Object Tracking framework (STCMOT), which utilizes historical embedding features to model the representation of ReID and detection features in a sequential order. Concretely, a temporal embedding boosting module is introduced to enhance the discriminability of individual embedding based on adjacent frame cooperation. While the trajectory embedding is then propagated by a temporal detection refinement module to mine salient target locations in the temporal field. Extensive experiments on the VisDrone2019 and UAVDT datasets demonstrate our STCMOT sets a new state-of-the-art performance in MOTA and IDF1 metrics. The source codes are released at https://github.com/ydhcg-BoBo/STCMOT.||\n", "2409.11175": "|**2024-09-17**|[Vision foundation models: can they be applied to astrophysics data?](http://arxiv.org/abs/2409.11175)|**[link](https://github.com/elastufka/fm4astro)**|Vision foundation models, which have demonstrated significant potential in many multimedia applications, are often underutilized in the natural sciences. This is primarily due to mismatches between the nature of domain-specific scientific data and the typical training data used for foundation models, leading to distribution shifts. Scientific data often differ substantially in structure and characteristics; researchers frequently face the challenge of optimizing model performance with limited labeled data of only a few hundred or thousand images. To adapt foundation models effectively requires customized approaches in preprocessing, data augmentation, and training techniques. Additionally, each vision foundation model exhibits unique strengths and limitations, influenced by differences in architecture, training procedures, and the datasets used for training. In this work, we evaluate the application of various vision foundation models to astrophysics data, specifically images from optical and radio astronomy. Our results show that using features extracted by specific foundation models improves the classification accuracy of optical galaxy images compared to conventional supervised training. Similarly, these models achieve equivalent or better performance in object detection tasks with radio images. However, their performance in classifying radio galaxy images is generally poor and often inferior to traditional supervised training results. These findings suggest that selecting suitable vision foundation models for astrophysics applications requires careful consideration of the model characteristics and alignment with the specific requirements of the downstream tasks.||\n", "2409.11018": "|**2024-09-17**|[Unleashing the Potential of Mamba: Boosting a LiDAR 3D Sparse Detector by Using Cross-Model Knowledge Distillation](http://arxiv.org/abs/2409.11018)|null|The LiDAR-based 3D object detector that strikes a balance between accuracy and speed is crucial for achieving real-time perception in autonomous driving and robotic navigation systems. To enhance the accuracy of point cloud detection, integrating global context for visual understanding improves the point clouds ability to grasp overall spatial information. However, many existing LiDAR detection models depend on intricate feature transformation and extraction processes, leading to poor real-time performance and high resource consumption, which limits their practical effectiveness. In this work, we propose a Faster LiDAR 3D object detection framework, called FASD, which implements heterogeneous model distillation by adaptively uniform cross-model voxel features. We aim to distill the transformer's capacity for high-performance sequence modeling into Mamba models with low FLOPs, achieving a significant improvement in accuracy through knowledge transfer. Specifically, Dynamic Voxel Group and Adaptive Attention strategies are integrated into the sparse backbone, creating a robust teacher model with scale-adaptive attention for effective global visual context modeling. Following feature alignment with the Adapter, we transfer knowledge from the Transformer to the Mamba through latent space feature supervision and span-head distillation, resulting in improved performance and an efficient student model. We evaluated the framework on the Waymo and nuScenes datasets, achieving a 4x reduction in resource consumption and a 1-2\\% performance improvement over the current SoTA methods.||\n", "2409.10901": "|**2024-09-17**|[TrajSSL: Trajectory-Enhanced Semi-Supervised 3D Object Detection](http://arxiv.org/abs/2409.10901)|null|Semi-supervised 3D object detection is a common strategy employed to circumvent the challenge of manually labeling large-scale autonomous driving perception datasets. Pseudo-labeling approaches to semi-supervised learning adopt a teacher-student framework in which machine-generated pseudo-labels on a large unlabeled dataset are used in combination with a small manually-labeled dataset for training. In this work, we address the problem of improving pseudo-label quality through leveraging long-term temporal information captured in driving scenes. More specifically, we leverage pre-trained motion-forecasting models to generate object trajectories on pseudo-labeled data to further enhance the student model training. Our approach improves pseudo-label quality in two distinct manners: first, we suppress false positive pseudo-labels through establishing consistency across multiple frames of motion forecasting outputs. Second, we compensate for false negative detections by directly inserting predicted object tracks into the pseudo-labeled scene. Experiments on the nuScenes dataset demonstrate the effectiveness of our approach, improving the performance of standard semi-supervised approaches in a variety of settings.||\n", "2409.10836": "|**2024-09-17**|[Single-Layer Learnable Activation for Implicit Neural Representation (SL$^{2}$A-INR)](http://arxiv.org/abs/2409.10836)|null|\u9690\u5f0f\u795e\u7ecf\u8868\u793a (INR) \u5229\u7528\u795e\u7ecf\u7f51\u7edc\u5c06\u5750\u6807\u8f93\u5165\u8f6c\u6362\u4e3a\u76f8\u5e94\u7684\u5c5e\u6027\uff0c\u8fd1\u5e74\u6765\u5728\u591a\u4e2a\u89c6\u89c9\u76f8\u5173\u9886\u57df\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\u3002\u7136\u800c\uff0cINR \u7684\u6027\u80fd\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d7\u5176\u591a\u5c42\u611f\u77e5\u5668 (MLP) \u67b6\u6784\u4e2d\u4f7f\u7528\u7684\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u9009\u62e9\u7684\u5f71\u54cd\u3002\u76ee\u524d\u5df2\u7ecf\u7814\u7a76\u4e86\u591a\u79cd\u975e\u7ebf\u6027\u65b9\u6cd5\uff1b\u7136\u800c\uff0c\u5f53\u524d\u7684 INR \u5728\u6355\u83b7\u9ad8\u9891\u5206\u91cf\u3001\u591a\u6837\u4fe1\u53f7\u7c7b\u578b\u548c\u5904\u7406\u9006\u95ee\u9898\u65b9\u9762\u9762\u4e34\u5c40\u9650\u6027\u3002\u6211\u4eec\u5df2\u7ecf\u786e\u5b9a\uff0c\u901a\u8fc7\u5f15\u5165 INR \u7684\u8303\u5f0f\u8f6c\u53d8\u53ef\u4ee5\u5927\u5927\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u5728\u521d\u59cb\u5c42\u5177\u6709\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\u7684\u67b6\u6784\u53ef\u4ee5\u8868\u793a\u5e95\u5c42\u4fe1\u53f7\u4e2d\u7684\u7cbe\u7ec6\u7ec6\u8282\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 SL$^{2}$A-INR\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e INR \u7684\u6df7\u5408\u7f51\u7edc\uff0c\u5177\u6709\u5355\u5c42\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u4f20\u7edf\u57fa\u4e8e ReLU \u7684 MLP \u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u56fe\u50cf\u8868\u793a\u30013D \u5f62\u72b6\u91cd\u5efa\u3001\u56fe\u50cf\u4fee\u590d\u3001\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u3001CT \u91cd\u5efa\u548c\u65b0\u89c6\u56fe\u5408\u6210\u3002\u901a\u8fc7\u7efc\u5408\u5b9e\u9a8c\uff0cSL$^{2}$A-INR \u5728 INR \u7684\u51c6\u786e\u6027\u3001\u8d28\u91cf\u548c\u6536\u655b\u901f\u5ea6\u65b9\u9762\u6811\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002||\n", "2409.10811": "|**2024-09-17**|[Context-Dependent Interactable Graphical User Interface Element Detection for VR Applications](http://arxiv.org/abs/2409.10811)|null|In recent years, Virtual Reality (VR) has emerged as a transformative technology, offering users immersive and interactive experiences across diversified virtual environments. Users can interact with VR apps through interactable GUI elements (IGEs) on the stereoscopic three-dimensional (3D) graphical user interface (GUI). The accurate recognition of these IGEs is instrumental, serving as the foundation of many software engineering tasks, including automated testing and effective GUI search. The most recent IGE detection approaches for 2D mobile apps typically train a supervised object detection model based on a large-scale manually-labeled GUI dataset, usually with a pre-defined set of clickable GUI element categories like buttons and spinners. Such approaches can hardly be applied to IGE detection in VR apps, due to a multitude of challenges including complexities posed by open-vocabulary and heterogeneous IGE categories, intricacies of context-sensitive interactability, and the necessities of precise spatial perception and visual-semantic alignment for accurate IGE detection results. Thus, it is necessary to embark on the IGE research tailored to VR apps. In this paper, we propose the first zero-shot cOntext-sensitive inteRactable GUI ElemeNT dEtection framework for virtual Reality apps, named Orienter. By imitating human behaviors, Orienter observes and understands the semantic contexts of VR app scenes first, before performing the detection. The detection process is iterated within a feedback-directed validation and reflection loop. Specifically, Orienter contains three components, including (1) Semantic context comprehension, (2) Reflection-directed IGE candidate detection, and (3) Context-sensitive interactability classification. Extensive experiments on the dataset demonstrate that Orienter is more effective than the state-of-the-art GUI element detection approaches.||\n", "2409.10775": "|**2024-09-16**|[Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?](http://arxiv.org/abs/2409.10775)|null|\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\uff0c\u5305\u62ec\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\uff0c\u5728\u5404\u79cd\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u90e8\u5206\u906e\u6321\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4f8b\u5982\uff0c\u7269\u4f53\u88ab\u90e8\u5206\u906e\u6321\u5728\u76f8\u673a\u89c6\u91ce\u4e4b\u5916\u7684\u60c5\u51b5\u3002\u5df2\u7ecf\u51fa\u73b0\u4e86\u4e00\u4e9b\u65b9\u6cd5\u6765\u63d0\u9ad8\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\uff0c\u5305\u62ec\u6570\u636e\u589e\u5f3a\u3001\u57fa\u4e8e\u90e8\u5206\u7684\u805a\u7c7b\uff0c\u4ee5\u53ca\u66f4\u5f3a\u5927\u7684\u67b6\u6784\uff0c\u5305\u62ec\u89c6\u89c9Transformer\uff08ViT\uff09\u6a21\u578b\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u5df2\u7ecf\u6839\u636e\u5176\u5728\u90e8\u5206\u906e\u6321\u4e0b\u5bf9\u7269\u4f53\u8fdb\u884c\u5206\u7c7b\u7684\u80fd\u529b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7136\u800c\uff0c\u5bf9\u8fd9\u4e9b\u65b9\u6cd5\u7684\u8bc4\u4f30\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u5305\u542b\u4eba\u5de5\u906e\u6321\u7684\u56fe\u50cf\uff0c\u8fd9\u4e9b\u56fe\u50cf\u901a\u5e38\u662f\u8ba1\u7b97\u673a\u751f\u6210\u7684\uff0c\u56e0\u6b64\u6807\u6ce8\u6210\u672c\u4f4e\u5ec9\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5f88\u5c11\u76f8\u4e92\u6bd4\u8f83\uff0c\u8bb8\u591a\u65b9\u6cd5\u662f\u4e0e\u65e9\u671f\u3001\u73b0\u5728\u5df2\u7ecf\u8fc7\u65f6\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u7684\u3002\u6211\u4eec\u8d21\u732e\u4e86\u906e\u6321\u4e0b\u56fe\u50cf\u8bc6\u522b\uff08IRUO\uff09\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u57fa\u4e8e\u6700\u8fd1\u5f00\u53d1\u7684\u906e\u6321\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\uff08OVIS\uff09\u6570\u636e\u96c6\uff08arXiv:2102.01558\uff09\u3002IRUO\u5229\u7528\u771f\u5b9e\u4e16\u754c\u548c\u4eba\u5de5\u906e\u6321\u7684\u56fe\u50cf\u6765\u6d4b\u8bd5\u548c\u6bd4\u8f83\u9886\u5148\u65b9\u6cd5\u5728\u89c6\u89c9\u8bc6\u522b\u4efb\u52a1\u4e2d\u5bf9\u90e8\u5206\u906e\u6321\u7684\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8d21\u732e\u4e86\u4f7f\u7528IRUO\u56fe\u50cf\u8fdb\u884c\u7684\u4eba\u7c7b\u7814\u7a76\u7684\u8bbe\u8ba1\u548c\u7ed3\u679c\uff0c\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u4eba\u7c7b\u5728\u591a\u4e2a\u7ea7\u522b\u548c\u7c7b\u578b\u7684\u906e\u6321\u4e0b\u7684\u5206\u7c7b\u6027\u80fd\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u4e0e\u65e9\u671f\u7684\u57fa\u4e8eCNN\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u73b0\u4ee3\u57fa\u4e8eCNN\u7684\u6a21\u578b\u5728\u906e\u6321\u56fe\u50cf\u4e0a\u7684\u8bc6\u522b\u7cbe\u5ea6\u6709\u6240\u63d0\u9ad8\uff0c\u5e76\u4e14\u57fa\u4e8eViT\u7684\u6a21\u578b\u5728\u906e\u6321\u56fe\u50cf\u4e0a\u7684\u7cbe\u5ea6\u9ad8\u4e8e\u57fa\u4e8eCNN\u7684\u6a21\u578b\uff0c\u5176\u6027\u80fd\u4ec5\u7565\u4f4e\u4e8e\u4eba\u7c7b\u7cbe\u5ea6\u3002\u6211\u4eec\u8fd8\u53d1\u73b0\uff0c\u67d0\u4e9b\u7c7b\u578b\u7684\u906e\u6321\uff0c\u5305\u62ec\u6f2b\u5c04\u906e\u6321\uff0c\u5373\u76f8\u5173\u7269\u4f53\u901a\u8fc7\u6805\u680f\u548c\u6811\u53f6\u7b49\u906e\u6321\u7269\u4e0a\u7684\u201c\u5b54\u6d1e\u201d\u53ef\u89c1\uff0c\u4e0e\u4eba\u7c7b\u76f8\u6bd4\uff0c\u8fd9\u79cd\u906e\u6321\u4f1a\u5927\u5927\u964d\u4f4e\u6df1\u5ea6\u8bc6\u522b\u6a21\u578b\u7684\u7cbe\u5ea6\uff0c\u5c24\u5176\u662f\u90a3\u4e9b\u5177\u6709CNN\u9aa8\u5e72\u7684\u6a21\u578b\u3002||\n", "2409.10699": "|**2024-09-16**|[CoMamba: Real-time Cooperative Perception Unlocked with State Space Models](http://arxiv.org/abs/2409.10699)|null|Cooperative perception systems play a vital role in enhancing the safety and efficiency of vehicular autonomy. Although recent studies have highlighted the efficacy of vehicle-to-everything (V2X) communication techniques in autonomous driving, a significant challenge persists: how to efficiently integrate multiple high-bandwidth features across an expanding network of connected agents such as vehicles and infrastructure. In this paper, we introduce CoMamba, a novel cooperative 3D detection framework designed to leverage state-space models for real-time onboard vehicle perception. Compared to prior state-of-the-art transformer-based models, CoMamba enjoys being a more scalable 3D model using bidirectional state space models, bypassing the quadratic complexity pain-point of attention mechanisms. Through extensive experimentation on V2X/V2V datasets, CoMamba achieves superior performance compared to existing methods while maintaining real-time processing capabilities. The proposed framework not only enhances object detection accuracy but also significantly reduces processing time, making it a promising solution for next-generation cooperative perception systems in intelligent transportation networks.||\n", "2409.10362": "|**2024-09-16**|[Frequency-Guided Masking for Enhanced Vision Self-Supervised Learning](http://arxiv.org/abs/2409.10362)|null|We present a novel frequency-based Self-Supervised Learning (SSL) approach that significantly enhances its efficacy for pre-training. Prior work in this direction masks out pre-defined frequencies in the input image and employs a reconstruction loss to pre-train the model. While achieving promising results, such an implementation has two fundamental limitations as identified in our paper. First, using pre-defined frequencies overlooks the variability of image frequency responses. Second, pre-trained with frequency-filtered images, the resulting model needs relatively more data to adapt to naturally looking images during fine-tuning. To address these drawbacks, we propose FOurier transform compression with seLf-Knowledge distillation (FOLK), integrating two dedicated ideas. First, inspired by image compression, we adaptively select the masked-out frequencies based on image frequency responses, creating more suitable SSL tasks for pre-training. Second, we employ a two-branch framework empowered by knowledge distillation, enabling the model to take both the filtered and original images as input, largely reducing the burden of downstream tasks. Our experimental results demonstrate the effectiveness of FOLK in achieving competitive performance to many state-of-the-art SSL methods across various downstream tasks, including image classification, few-shot learning, and semantic segmentation.||\n", "2409.12111": "|**2024-09-18**|[Applications of Knowledge Distillation in Remote Sensing: A Survey](http://arxiv.org/abs/2409.12111)|null|\u968f\u7740\u9065\u611f (RS) \u9886\u57df\u6a21\u578b\u590d\u6742\u6027\u7684\u4e0d\u65ad\u63d0\u9ad8\uff0c\u5bf9\u5e73\u8861\u6a21\u578b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u7684\u9700\u6c42\u4e5f\u65e5\u76ca\u589e\u957f\u3002\u77e5\u8bc6\u84b8\u998f (KD) \u5df2\u6210\u4e3a\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u80fd\u591f\u5728\u4e0d\u663e\u8457\u964d\u4f4e\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u77e5\u8bc6\u4ece\u5927\u578b\u590d\u6742\u6a21\u578b\u8fc1\u79fb\u5230\u66f4\u5c0f\u3001\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u3002\u8fd9\u7bc7\u7efc\u8ff0\u6587\u7ae0\u5e7f\u6cdb\u8003\u5bdf\u4e86 KD \u53ca\u5176\u5728\u9065\u611f\u9886\u57df\u7684\u521b\u65b0\u5e94\u7528\u3002KD \u662f\u4e00\u79cd\u5c06\u77e5\u8bc6\u4ece\u590d\u6742\u3001\u901a\u5e38\u7b28\u91cd\u7684\u6a21\u578b\uff08\u6559\u5e08\uff09\u8fc1\u79fb\u5230\u66f4\u7d27\u51d1\u3001\u66f4\u9ad8\u6548\u7684\u6a21\u578b\uff08\u5b66\u751f\uff09\u7684\u6280\u672f\uff0c\u5df2\u7ecf\u5728\u5404\u4e2a\u9886\u57df\u5f97\u5230\u4e86\u663e\u8457\u7684\u53d1\u5c55\u548c\u5e94\u7528\u3002\u9996\u5148\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 KD \u65b9\u6cd5\u7684\u57fa\u672c\u6982\u5ff5\u548c\u5386\u53f2\u8fdb\u7a0b\u3002\u6587\u7ae0\u91cd\u70b9\u4ecb\u7ecd\u4e86\u91c7\u7528 KD \u7684\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u6a21\u578b\u538b\u7f29\u3001\u8ba1\u7b97\u6548\u7387\u63d0\u9ad8\u548c\u6027\u80fd\u6539\u5584\u65b9\u9762\uff0c\u8fd9\u4e9b\u4f18\u52bf\u5bf9\u4e8e RS \u573a\u666f\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u6587\u7ae0\u63d0\u4f9b\u4e86 KD \u6280\u672f\u7684\u5168\u9762\u5206\u7c7b\uff0c\u5176\u4e2d\u6bcf\u4e2a\u7c7b\u522b\u90fd\u7ecf\u8fc7\u4e25\u683c\u5206\u6790\uff0c\u4ee5\u8bc1\u660e\u66ff\u4ee3\u65b9\u6848\u7684\u5e7f\u5ea6\u548c\u6df1\u5ea6\uff0c\u5e76\u901a\u8fc7\u5177\u4f53\u7684\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86 KD \u65b9\u6cd5\u5728 RS \u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u4f8b\u5982\u5b9e\u4f8b\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\u3002\u6b64\u5916\uff0c\u8be5\u7efc\u8ff0\u8fd8\u8ba8\u8bba\u4e86 KD \u5728\u9065\u611f\u9886\u57df\u9762\u4e34\u7684\u6311\u6218\u548c\u5c40\u9650\u6027\uff0c\u5305\u62ec\u5b9e\u9645\u7ea6\u675f\u548c\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\uff0c\u4e3a\u9065\u611f\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6982\u8ff0\u3002\u901a\u8fc7\u8fd9\u79cd\u7ec4\u7ec7\u65b9\u5f0f\uff0c\u672c\u6587\u4e0d\u4ec5\u9610\u660e\u4e86 KD \u7814\u7a76\u7684\u73b0\u72b6\uff0c\u800c\u4e14\u4e3a\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4ece\u800c\u4e3a\u5b66\u672f\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u505a\u51fa\u4e86\u91cd\u5927\u8d21\u732e\u3002||\n", "2409.11995": "|**2024-09-18**|[Unraveling the Hessian: A Key to Smooth Convergence in Loss Function Landscapes](http://arxiv.org/abs/2409.11995)|**[link](https://github.com/kisnikser/landscape-hessian)**|\u795e\u7ecf\u7f51\u7edc\u7684\u635f\u5931\u666f\u89c2\u662f\u5176\u8bad\u7ec3\u7684\u4e00\u4e2a\u5173\u952e\u65b9\u9762\uff0c\u7406\u89e3\u5176\u5c5e\u6027\u5bf9\u4e8e\u63d0\u9ad8\u5176\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5f53\u6837\u672c\u91cf\u589e\u52a0\u65f6\u635f\u5931\u66f2\u9762\u5982\u4f55\u53d8\u5316\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ee5\u524d\u672a\u88ab\u63a2\u7d22\u7684\u95ee\u9898\u3002\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4e86\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u4e2d\u635f\u5931\u666f\u89c2\u7684\u6536\u655b\u6027\uff0c\u5e76\u63a8\u5bfc\u51fa\u5728\u6837\u672c\u4e2d\u6dfb\u52a0\u65b0\u5bf9\u8c61\u65f6\u635f\u5931\u51fd\u6570\u503c\u5dee\u5f02\u7684\u4e0a\u754c\u3002\u6211\u4eec\u7684\u5b9e\u8bc1\u7814\u7a76\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u8bc1\u5b9e\u4e86\u8fd9\u4e9b\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u635f\u5931\u51fd\u6570\u66f2\u9762\u7684\u6536\u655b\u6027\u3002\u6211\u4eec\u7684\u53d1\u73b0\u4e3a\u795e\u7ecf\u635f\u5931\u666f\u89c2\u7684\u5c40\u90e8\u51e0\u4f55\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e76\u5bf9\u6837\u672c\u91cf\u786e\u5b9a\u6280\u672f\u7684\u53d1\u5c55\u5177\u6709\u610f\u4e49\u3002||\n", "2409.11923": "|**2024-09-18**|[Agglomerative Token Clustering](http://arxiv.org/abs/2409.11923)|null|\u6211\u4eec\u63d0\u51fa\u4e86\u805a\u5408\u5f0fToken\u805a\u7c7b\uff08ATC\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684Token\u5408\u5e76\u65b9\u6cd5\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u56fe\u50cf\u5408\u6210\u4ee5\u53ca\u76ee\u6807\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u4ee5\u524d\u7684Token\u5408\u5e76\u548c\u526a\u679d\u65b9\u6cd5\u3002ATC\u901a\u8fc7\u81ea\u4e0b\u800c\u4e0a\u7684\u5c42\u6b21\u805a\u7c7b\u6765\u5408\u5e76\u805a\u7c7b\uff0c\u65e0\u9700\u5f15\u5165\u989d\u5916\u7684\u53ef\u5b66\u4e60\u53c2\u6570\u3002\u6211\u4eec\u53d1\u73b0ATC\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u90fd\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751a\u81f3\u5728\u5e94\u7528\u4e8e\u73b0\u6210\u6a21\u578b\u65f6\uff08\u5373\u65e0\u9700\u5fae\u8c03\uff09\u4e5f\u80fd\u4e0e\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u6280\u672f\u76f8\u5ab2\u7f8e\u3002\u5f53\u5e94\u7528\u4e8e\u4f4e\u4fdd\u7559\u7387\u65f6\uff0cATC\u7279\u522b\u6709\u6548\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u53ea\u6709\u4e00\u5c0f\u90e8\u5206Token\u88ab\u4fdd\u7559\uff0c\u5e76\u4e14\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7279\u522b\u56f0\u96be\u3002||\n", "2409.11867": "|**2024-09-18**|[Distillation-free Scaling of Large SSMs for Images and Videos](http://arxiv.org/abs/2409.11867)|null|State-space models (SSMs), exemplified by S4, have introduced a novel context modeling method by integrating state-space techniques into deep learning. However, they struggle with global context modeling due to their data-independent matrices. The Mamba model addressed this with data-dependent variants via the S6 selective-scan algorithm, enhancing context modeling, especially for long sequences. However, Mamba-based architectures are difficult to scale with respect to the number of parameters, which is a major limitation for vision applications. This paper addresses the scalability issue of large SSMs for image classification and action recognition without requiring additional techniques like knowledge distillation. We analyze the distinct characteristics of Mamba-based and Attention-based models, proposing a Mamba-Attention interleaved architecture that enhances scalability, robustness, and performance. We demonstrate that the stable and efficient interleaved architecture resolves the scalability issue of Mamba-based architectures for images and videos and increases robustness to common artifacts like JPEG compression. Our thorough evaluation on the ImageNet-1K, Kinetics-400 and Something-Something-v2 benchmarks demonstrates that our approach improves the accuracy of state-of-the-art Mamba-based architectures by up to $+1.7$.||\n", "2409.11749": "|**2024-09-18**|[RockTrack: A 3D Robust Multi-Camera-Ken Multi-Object Tracking Framework](http://arxiv.org/abs/2409.11749)|null|\u968f\u77403D\u76ee\u6807\u68c0\u6d4b\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5c24\u5176\u662f\u5728\u7ecf\u6d4e\u9ad8\u6548\u7684\u591a\u76f8\u673a\u8bbe\u7f6e\u4e2d\uff0c3D\u591a\u76ee\u6807\u8ddf\u8e2a\uff08MOT\uff09\u83b7\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u7136\u800c\uff0c\u76ee\u524d\u6d41\u884c\u7684\u7aef\u5230\u7aef\u591a\u76f8\u673a\u8ddf\u8e2a\u5668\u8bad\u7ec3\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u6a21\u578b\u4f9d\u8d56\u4e8e\u7279\u5b9a\u7684\u68c0\u6d4b\u5668\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7684\u901a\u7528\u8ddf\u8e2a\u5668\u5ffd\u7565\u4e86\u591a\u76f8\u673a\u68c0\u6d4b\u5668\u7684\u72ec\u7279\u7279\u5f81\uff0c\u5373\u8fd0\u52a8\u89c2\u6d4b\u7684\u4e0d\u53ef\u9760\u6027\u548c\u89c6\u89c9\u4fe1\u606f\u7684\u53ef\u7528\u6027\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86RockTrack\uff0c\u4e00\u79cd\u9762\u5411\u591a\u76f8\u673a\u68c0\u6d4b\u5668\u76843D MOT\u65b9\u6cd5\u3002RockTrack\u9075\u5faa\u201c\u68c0\u6d4b\u8ddf\u8e2a\u201d\u6846\u67b6\uff0c\u517c\u5bb9\u5404\u79cd\u73b0\u6210\u7684\u68c0\u6d4b\u5668\u3002RockTrack\u5305\u542b\u4e00\u4e2a\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u9884\u5904\u7406\u6a21\u5757\uff0c\u7528\u4e8e\u4ece\u5355\u4e2a\u68c0\u6d4b\u5668\u7684\u4e0d\u540c\u8868\u793a\u7a7a\u95f4\u4e2d\u63d0\u53d6\u53ef\u9760\u7684\u8fd0\u52a8\u548c\u56fe\u50cf\u89c2\u6d4b\u7ed3\u679c\u3002\u7136\u540e\uff0c\u8fd9\u4e9b\u89c2\u6d4b\u7ed3\u679c\u4f1a\u5728\u5173\u8054\u6a21\u5757\u4e2d\u878d\u5408\uff0c\u8be5\u6a21\u5757\u5229\u7528\u51e0\u4f55\u548c\u5916\u89c2\u7ebf\u7d22\u6765\u6700\u5927\u7a0b\u5ea6\u5730\u51cf\u5c11\u9519\u914d\u3002\u6700\u7ec8\u7684\u5339\u914d\u7ed3\u679c\u901a\u8fc7\u5206\u9636\u6bb5\u4f30\u8ba1\u8fc7\u7a0b\u8fdb\u884c\u4f20\u64ad\uff0c\u5f62\u6210\u542f\u53d1\u5f0f\u566a\u58f0\u5efa\u6a21\u7684\u57fa\u7840\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5916\u89c2\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u591a\u76f8\u673a\u8bbe\u7f6e\u4e2d\u660e\u786e\u8868\u5f81\u76ee\u6807\u4eb2\u548c\u5ea6\u3002RockTrack\u5728nuScenes\u4ec5\u89c6\u89c9\u8ddf\u8e2a\u6392\u884c\u699c\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0cAMOTA\u8fbe\u523059.1%\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u60ca\u4eba\u7684\u8ba1\u7b97\u6548\u7387\u3002||\n", "2409.11644": "|**2024-09-18**|[Few-Shot Learning Approach on Tuberculosis Classification Based on Chest X-Ray Images](http://arxiv.org/abs/2409.11644)|null|Tuberculosis (TB) is caused by the bacterium Mycobacterium tuberculosis, primarily affecting the lungs. Early detection is crucial for improving treatment effectiveness and reducing transmission risk. Artificial intelligence (AI), particularly through image classification of chest X-rays, can assist in TB detection. However, class imbalance in TB chest X-ray datasets presents a challenge for accurate classification. In this paper, we propose a few-shot learning (FSL) approach using the Prototypical Network algorithm to address this issue. We compare the performance of ResNet-18, ResNet-50, and VGG16 in feature extraction from the TBX11K Chest X-ray dataset. Experimental results demonstrate classification accuracies of 98.93% for ResNet-18, 98.60% for ResNet-50, and 33.33% for VGG16. These findings indicate that the proposed method outperforms others in mitigating data imbalance, which is particularly beneficial for disease classification applications.||\n", "2409.11542": "|**2024-09-17**|[VALO: A Versatile Anytime Framework for LiDAR-based Object Detection Deep Neural Networks](http://arxiv.org/abs/2409.11542)|**[link](https://github.com/csl-ku/valo)**|This work addresses the challenge of adapting dynamic deadline requirements for LiDAR object detection deep neural networks (DNNs). The computing latency of object detection is critically important to ensure safe and efficient navigation. However, state-of-the-art LiDAR object detection DNNs often exhibit significant latency, hindering their real-time performance on resource-constrained edge platforms. Therefore, a tradeoff between detection accuracy and latency should be dynamically managed at runtime to achieve optimum results.   In this paper, we introduce VALO (Versatile Anytime algorithm for LiDAR Object detection), a novel data-centric approach that enables anytime computing of 3D LiDAR object detection DNNs. VALO employs a deadline-aware scheduler to selectively process input regions, making execution time and accuracy tradeoffs without architectural modifications. Additionally, it leverages efficient forecasting of past detection results to mitigate possible loss of accuracy due to partial processing of input. Finally, it utilizes a novel input reduction technique within its detection heads to significantly accelerate execution without sacrificing accuracy.   We implement VALO on state-of-the-art 3D LiDAR object detection networks, namely CenterPoint and VoxelNext, and demonstrate its dynamic adaptability to a wide range of time constraints while achieving higher accuracy than the prior state-of-the-art. Code is available athttps://github.com/CSL-KU/VALO}{github.com/CSL-KU/VALO.||\n", "2409.11532": "|**2024-09-17**|[Enhancing the Reliability of LiDAR Point Cloud Sampling: A Colorization and Super-Resolution Approach Based on LiDAR-Generated Images](http://arxiv.org/abs/2409.11532)|null|In recent years, Light Detection and Ranging (LiDAR) technology, a critical sensor in robotics and autonomous systems, has seen significant advancements. These improvements include enhanced resolution of point clouds and the capability to provide 360{\\deg} low-resolution images. These images encode various data such as depth, reflectivity, and near-infrared light within the pixels. However, an excessive density of points and conventional point cloud sampling can be counterproductive, particularly in applications such as LiDAR odometry, where misleading points and degraded geometry information may induce drift errors. Currently, extensive research efforts are being directed towards leveraging LiDAR-generated images to improve situational awareness. This paper presents a comprehensive review of current deep learning (DL) techniques, including colorization and super-resolution, which are traditionally utilized in conventional computer vision tasks. These techniques are applied to LiDAR-generated images and are analyzed qualitatively. Based on this analysis, we have developed a novel approach that selectively integrates the most suited colorization and super-resolution methods with LiDAR imagery to sample reliable points from the LiDAR point cloud. This approach aims to not only improve the accuracy of point cloud registration but also avoid mismatching caused by lacking geometry information, thereby augmenting the utility and precision of LiDAR systems in practical applications. In our evaluation, the proposed approach demonstrates superior performance compared to our previous work, achieving lower translation and rotation errors with a reduced number of points.||\n", "2409.11502": "|**2024-09-19**|[Super Resolution On Global Weather Forecasts](http://arxiv.org/abs/2409.11502)|null|Weather forecasting is a vitally important tool for tasks ranging from planning day to day activities to disaster response planning. However, modeling weather has proven to be challenging task due to its chaotic and unpredictable nature. Each variable, from temperature to precipitation to wind, all influence the path the environment will take. As a result, all models tend to rapidly lose accuracy as the temporal range of their forecasts increase. Classical forecasting methods use a myriad of physics-based, numerical, and stochastic techniques to predict the change in weather variables over time. However, such forecasts often require a very large amount of data and are extremely computationally expensive. Furthermore, as climate and global weather patterns change, classical models are substantially more difficult and time-consuming to update for changing environments. Fortunately, with recent advances in deep learning and publicly available high quality weather datasets, deploying learning methods for estimating these complex systems has become feasible. The current state-of-the-art deep learning models have comparable accuracy to the industry standard numerical models and are becoming more ubiquitous in practice due to their adaptability. Our group seeks to improve upon existing deep learning based forecasting methods by increasing spatial resolutions of global weather predictions. Specifically, we are interested in performing super resolution (SR) on GraphCast temperature predictions by increasing the global precision from 1 degree of accuracy to 0.5 degrees, which is approximately 111km and 55km respectively.||\n", "2409.18023": "|**2024-09-26**|[DARE: Diverse Visual Question Answering with Robustness Evaluation](http://arxiv.org/abs/2409.18023)|null|\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u6269\u5c55\u4e86\u4ec5\u6587\u672c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4ec5\u89c6\u89c9\u6a21\u578b\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u5e76\u4e14\u80fd\u591f\u4ece\u591a\u6a21\u6001\u89c6\u89c9\u6587\u672c\u8f93\u5165\u4e2d\u5b66\u4e60\u548c\u5904\u7406\u3002 \u867d\u7136\u73b0\u4ee3 VLM \u5728\u8bb8\u591a\u6807\u51c6\u56fe\u50cf\u5206\u7c7b\u548c\u56fe\u50cf\u6587\u672c\u5339\u914d\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u96be\u4ee5\u5e94\u5bf9\u8bb8\u591a\u5173\u952e\u7684\u89c6\u89c9\u8bed\u8a00 (VL) \u63a8\u7406\u80fd\u529b\uff0c\u4f8b\u5982\u8ba1\u6570\u548c\u7a7a\u95f4\u63a8\u7406\u3002 \u6b64\u5916\uff0c\u867d\u7136\u5b83\u4eec\u53ef\u80fd\u5bf9\u6307\u4ee4\u548c/\u6216\u8bc4\u4f30\u534f\u8bae\u7684\u5fae\u5c0f\u53d8\u5316\u975e\u5e38\u8106\u5f31\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u8bc4\u4f30\u5b83\u4eec\u7684\u7a33\u5065\u6027\uff08\u6216\u8005\u66f4\u786e\u5207\u5730\u8bf4\u662f\u7f3a\u4e4f\u7a33\u5065\u6027\uff09\u3002 \u4e3a\u4e86\u5c06\u5177\u6709\u6311\u6218\u6027\u7684 VL \u573a\u666f\u4e0e\u5168\u9762\u7684\u7a33\u5065\u6027\u8bc4\u4f30\u76f8\u7ed3\u5408\uff0c\u6211\u4eec\u5f15\u5165\u4e86 DARE\uff0c\u5373\u5177\u6709\u7a33\u5065\u6027\u8bc4\u4f30\u7684\u591a\u6837\u5316\u89c6\u89c9\u95ee\u7b54\uff0c\u8fd9\u662f\u4e00\u4e2a\u7cbe\u5fc3\u521b\u5efa\u548c\u7b56\u5212\u7684\u591a\u9879\u9009\u62e9 VQA \u57fa\u51c6\u6d4b\u8bd5\u3002 DARE \u8bc4\u4f30 VLM \u5728\u4e94\u4e2a\u4e0d\u540c\u7c7b\u522b\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u5305\u62ec\u56db\u4e2a\u57fa\u4e8e\u4ee5\u4e0b\u53d8\u5316\u7684\u7a33\u5065\u6027\u8bc4\u4f30\uff1a\u63d0\u793a\u3001\u7b54\u6848\u9009\u9879\u5b50\u96c6\u3001\u8f93\u51fa\u683c\u5f0f\u548c\u6b63\u786e\u7b54\u6848\u7684\u6570\u91cf\u3002 \u5728\u5176\u4ed6\u4e00\u7cfb\u5217\u53d1\u73b0\u4e2d\uff0c\u6211\u4eec\u62a5\u544a\u8bf4\uff0c\u6700\u5148\u8fdb\u7684 VLM \u4ecd\u7136\u96be\u4ee5\u56de\u7b54\u5927\u591a\u6570\u7c7b\u522b\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u65e0\u6cd5\u5728\u6d4b\u8bd5\u7684\u7a33\u5065\u6027\u8bc4\u4f30\u4e2d\u59cb\u7ec8\u5982\u4e00\u5730\u63d0\u4f9b\u5176\u5cf0\u503c\u6027\u80fd\u3002 \u9009\u9879\u5b50\u96c6\u7684\u6700\u574f\u60c5\u51b5\u6027\u80fd\u6bd4\u6807\u51c6\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u4f4e 34%\u3002 LLaVA 1.6 \u548c Idefics2 \u7b49\u5f00\u6e90 VLM \u7684\u7a33\u5065\u6027\u65e0\u6cd5\u4e0e GPT-4 \u548c Gemini \u7b49\u95ed\u6e90\u6a21\u578b\u76f8\u63d0\u5e76\u8bba\uff0c\u4f46\u5373\u4f7f\u662f\u540e\u8005\u4ecd\u7136\u975e\u5e38\u5bb9\u6613\u53d7\u5230\u4e0d\u540c\u53d8\u5316\u7684\u5f71\u54cd\u3002||\n", "2409.17851": "|**2024-09-26**|[A New Dataset for Monocular Depth Estimation Under Viewpoint Shifts](http://arxiv.org/abs/2409.17851)|null|\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u662f\u81ea\u52a8\u9a7e\u9a76\u548c\u8bb8\u591a\u5176\u4ed6\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u7684\u5173\u952e\u4efb\u52a1\u3002\u867d\u7136\u8be5\u9886\u57df\u5df2\u7ecf\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u89c6\u89d2\u53d8\u5316\u5bf9\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u7684\u5f71\u54cd\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u4e0d\u540c\u76f8\u673a\u4f4d\u7f6e\u548c\u65b9\u5411\u5bf9\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6027\u80fd\u7684\u5f71\u54cd\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u5e94\u6027\u4f30\u8ba1\u548c\u76ee\u6807\u68c0\u6d4b\u7684\u771f\u503c\u7b56\u7565\uff0c\u65e0\u9700\u6602\u8d35\u7684\u6fc0\u5149\u96f7\u8fbe\u4f20\u611f\u5668\u3002\u6211\u4eec\u4ece\u591a\u4e2a\u89c6\u70b9\u6536\u96c6\u4e86\u9053\u8def\u573a\u666f\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u5e76\u7528\u5b83\u6765\u8bc4\u4f30\u73b0\u4ee3\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u5bf9\u51e0\u4f55\u504f\u79fb\u7684\u9c81\u68d2\u6027\u3002\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7b56\u7565\u7684\u6709\u6548\u6027\u540e\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u5bf9\u5f53\u524d\u6a21\u578b\u5c40\u9650\u6027\u7684\u5b9d\u8d35\u89c1\u89e3\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8003\u8651\u89c6\u70b9\u53d8\u5316\u7684\u91cd\u8981\u6027\u3002||\n", "2409.17805": "|**2024-09-26**|[Cascade Prompt Learning for Vision-Language Model Adaptation](http://arxiv.org/abs/2409.17805)|**[link](https://github.com/megvii-research/caspl)**|\u63d0\u793a\u5b66\u4e60\u5df2\u6210\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4f8b\u5982CLIP\u3002\u7136\u800c\uff0c\u5f53\u524d\u53ef\u5b66\u4e60\u7684\u63d0\u793a\u6807\u8bb0\u4e3b\u8981\u7528\u4e8e\u9002\u5e94\u4efb\u52a1\u7684\u5355\u4e00\u9636\u6bb5\uff08\u5373\uff0c\u8c03\u6574\u63d0\u793a\uff09\uff0c\u5bb9\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\u98ce\u9669\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7ea7\u8054\u63d0\u793a\u5b66\u4e60CasPL\u6846\u67b6\uff0c\u4f7f\u63d0\u793a\u5b66\u4e60\u80fd\u591f\u540c\u65f6\u670d\u52a1\u4e8e\u901a\u7528\u548c\u7279\u5b9a\u4e13\u4e1a\u77e5\u8bc6\uff08\u5373\uff0c\u589e\u5f3a\u548c\u8c03\u6574\u63d0\u793a\uff09\u3002\u5177\u4f53\u6765\u8bf4\uff0cCasPL\u662f\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u8303\u5f0f\uff0c\u5305\u62ec\u4e24\u4e2a\u4e0d\u540c\u9636\u6bb5\u7684\u53ef\u5b66\u4e60\u63d0\u793a\uff1a\u7b2c\u4e00\u4e2a\u589e\u5f3a\u63d0\u793a\u65e8\u5728\u901a\u8fc7\u4f7f\u7528\u5927\u91cf\u672a\u6807\u8bb0\u7684\u57df\u56fe\u50cf\u5bf9\u9f50\u5176\u9884\u6d4b\u7684logits\uff0c\u4ece\u9ad8\u7ea7\u66f4\u5927\u7684CLIP\u6559\u5e08\u6a21\u578b\u4e2d\u63d0\u53d6\u57df\u4e00\u822c\u77e5\u8bc6\u3002\u7136\u540e\uff0c\u7b2c\u4e8c\u4e2a\u8c03\u6574\u63d0\u793a\u4e0e\u51bb\u7ed3\u7684\u7b2c\u4e00\u7ec4\u7ea7\u8054\uff0c\u4ee5\u5fae\u8c03\u4e0b\u6e38\u4efb\u52a1\uff0c\u9075\u5faa\u5148\u524d\u7814\u7a76\u4e2d\u91c7\u7528\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0cCasPL\u53ef\u4ee5\u6709\u6548\u5730\u5c06\u57df\u4e00\u822c\u8868\u793a\u548c\u4efb\u52a1\u7279\u5b9a\u8868\u793a\u6355\u83b7\u5230\u660e\u786e\u4e0d\u540c\u7684\u6e10\u8fdb\u63d0\u793a\u7ec4\u4e2d\uff0c\u4ece\u800c\u6f5c\u5728\u5730\u7f13\u89e3\u76ee\u6807\u57df\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cCasPL\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u4efb\u4f55\u73b0\u6709\u7684\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u4e2d\u3002CasPL\u5728\u6027\u80fd\u548c\u63a8\u7406\u901f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u663e\u8457\u66f4\u597d\u7684\u5e73\u8861\uff0c\u8fd9\u5bf9\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u90e8\u7f72\u8f83\u5c0f\u7684VLM\u6a21\u578b\u5c24\u5176\u6709\u5229\u3002\u4e0e\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5PromptSRC\u76f8\u6bd4\uff0cCasPL\u572811\u4e2a\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\uff0c\u57fa\u7840\u7c7b\u7684\u5e73\u5747\u6539\u8fdb\u7387\u4e3a1.85%\uff0c\u65b0\u7c7b\u7684\u5e73\u5747\u6539\u8fdb\u7387\u4e3a3.44%\uff0c\u8c03\u548c\u5e73\u5747\u503c\u7684\u5e73\u5747\u6539\u8fdb\u7387\u4e3a2.72%\u3002\u4ee3\u7801\u516c\u5f00\u5730\u5740\uff1ahttps://github.com/megvii-research/CasPL\u3002||\n", "2409.17778": "|**2024-09-26**|[Taming Diffusion Prior for Image Super-Resolution with Domain Shift SDEs](http://arxiv.org/abs/2409.17778)|**[link](https://github.com/qinpengcui/dossr)**|\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387 (SR) \u6a21\u578b\u56e0\u5176\u5f3a\u5927\u7684\u56fe\u50cf\u6062\u590d\u80fd\u529b\u800c\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u901a\u5e38\u96be\u4ee5\u5728\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861\u3002\u5b83\u4eec\u901a\u5e38\u8981\u4e48\u5ffd\u7565\u4e86\u5229\u7528\u73b0\u6709\u5927\u91cf\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u9650\u5236\u4e86\u5176\u751f\u6210\u80fd\u529b\uff0c\u8981\u4e48\u9700\u8981\u4ece\u968f\u673a\u566a\u58f0\u5f00\u59cb\u8fdb\u884c\u6570\u5341\u6b21\u524d\u5411\u4f20\u9012\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u63a8\u7406\u6548\u7387\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DoSSR\uff0c\u4e00\u79cd\u57fa\u4e8e\u57df\u8fc1\u79fb\u6269\u6563\u7684 SR \u6a21\u578b\uff0c\u5b83\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u4ee5\u4f4e\u5206\u8fa8\u7387 (LR) \u56fe\u50cf\u521d\u59cb\u5316\u6269\u6563\u8fc7\u7a0b\u6765\u663e\u8457\u63d0\u9ad8\u6548\u7387\u3002\u6211\u4eec\u65b9\u6cd5\u7684\u6838\u5fc3\u662f\u4e00\u4e2a\u4e0e\u73b0\u6709\u6269\u6563\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\u7684\u57df\u8fc1\u79fb\u65b9\u7a0b\u3002\u8fd9\u79cd\u96c6\u6210\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6269\u6563\u5148\u9a8c\u7684\u5229\u7528\uff0c\u8fd8\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u5c06\u79bb\u6563\u8fc1\u79fb\u8fc7\u7a0b\u8f6c\u6362\u4e3a\u8fde\u7eed\u516c\u5f0f\uff08\u79f0\u4e3a DoS-SDE\uff09\u6765\u63a8\u8fdb\u6211\u4eec\u7684\u65b9\u6cd5\u3002\u8fd9\u4e00\u8fdb\u6b65\u5e26\u6765\u4e86\u5feb\u901f\u4e14\u5b9a\u5236\u5316\u7684\u6c42\u89e3\u5668\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u91c7\u6837\u6548\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4ec5\u9700 5 \u4e2a\u91c7\u6837\u6b65\u9aa4\u3002\u4e0e\u4e4b\u524d\u57fa\u4e8e\u6269\u6563\u5148\u9a8c\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86 5-7 \u500d\u7684\u663e\u8457\u52a0\u901f\uff0c\u8bc1\u660e\u4e86\u5176\u5353\u8d8a\u7684\u6548\u7387\u3002\u4ee3\u7801\uff1ahttps://github.com/QinpengCui/DoSSR\u3002||\n", "2409.17759": "|**2024-09-26**|[LGFN: Lightweight Light Field Image Super-Resolution using Local Convolution Modulation and Global Attention Feature Extraction](http://arxiv.org/abs/2409.17759)|null|\u5149\u573a\uff08LF\uff09\u80fd\u591f\u5c06\u4e09\u7ef4\u573a\u666f\u4fe1\u606f\u7f16\u7801\u6210\u56db\u7ef4\u5149\u573a\u56fe\u50cf\uff0c\u5728\u8bf8\u5982\u540e\u671f\u91cd\u805a\u7126\u548c\u6df1\u5ea6\u611f\u77e5\u7b49\u9886\u57df\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u5149\u573a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u65e8\u5728\u63d0\u5347\u53d7\u9650\u4e8e\u5149\u573a\u76f8\u673a\u4f20\u611f\u5668\u6027\u80fd\u7684\u56fe\u50cf\u5206\u8fa8\u7387\u3002\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u5df2\u7ecf\u53d6\u5f97\u4e86\u53ef\u559c\u7684\u6210\u679c\uff0c\u4f46\u7531\u4e8e\u6a21\u578b\u4e0d\u591f\u8f7b\u91cf\u5316\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLGFN\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u5b83\u96c6\u6210\u4e86\u4e0d\u540c\u89c6\u89d2\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u4ee5\u53ca\u4e0d\u540c\u901a\u9053\u7684\u7279\u5f81\uff0c\u7528\u4e8e\u5149\u573a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u7531\u4e8e\u4e0d\u540c\u5b50\u5b54\u5f84\u56fe\u50cf\u4e2d\u76f8\u540c\u50cf\u7d20\u4f4d\u7f6e\u7684\u76f8\u90bb\u533a\u57df\u8868\u73b0\u51fa\u76f8\u4f3c\u7684\u7ed3\u6784\u5173\u7cfb\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8f7b\u91cf\u7ea7CNN\u7684\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff08DGCE\uff09\uff0c\u901a\u8fc7\u7279\u5f81\u8c03\u5236\u66f4\u597d\u5730\u63d0\u53d6\u5c40\u90e8\u7279\u5f81\u3002\u540c\u65f6\uff0c\u7531\u4e8e\u5149\u573a\u56fe\u50cf\u4e2d\u8d85\u51fa\u8fb9\u754c\u7684\u50cf\u7d20\u4f4d\u7f6e\u5b58\u5728\u8f83\u5927\u5dee\u5f02\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u7a7a\u95f4\u6ce8\u610f\u529b\u6a21\u5757\uff08ESAM\uff09\uff0c\u5b83\u4f7f\u7528\u53ef\u5206\u89e3\u7684\u5927\u6838\u5377\u79ef\u6765\u83b7\u5f97\u66f4\u5927\u7684\u611f\u53d7\u91ce\uff0c\u4ee5\u53ca\u4e00\u4e2a\u9ad8\u6548\u7684\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\uff08ECAM\uff09\u3002\u4e0e\u73b0\u6709\u53c2\u6570\u91cf\u5927\u7684\u5149\u573a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53c2\u6570\u91cf\u4e3a0.45M\uff0cFLOPs\u4e3a19.33G\uff0c\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6548\u679c\u3002\u5927\u91cf\u7684\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728NTIRE2024\u5149\u573a\u8d85\u5206\u8fa8\u7387\u6311\u6218\u8d5b\u7684Track 2\u4fdd\u771f\u5ea6\u548c\u6548\u7387\u8d5b\u9053\u4e2d\u6392\u540d\u7b2c\u4e8c\uff0c\u5728Track 1\u4fdd\u771f\u5ea6\u8d5b\u9053\u4e2d\u6392\u540d\u7b2c\u4e03\u3002||\n", "2409.17720": "|**2024-09-26**|[Scene Understanding in Pick-and-Place Tasks: Analyzing Transformations Between Initial and Final Scenes](http://arxiv.org/abs/2409.17720)|null|\u968f\u7740\u673a\u5668\u4eba\u5728\u65e5\u5e38\u4efb\u52a1\u4e2d\u8d8a\u6765\u8d8a\u591a\u5730\u4e0e\u4eba\u7c7b\u5408\u4f5c\uff0c\u91c7\u53d6\u63aa\u65bd\u4f7f\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u591f\u7406\u89e3\u73af\u5883\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u8fd9\u9879\u5de5\u4f5c\u4fa7\u91cd\u4e8e\u573a\u666f\u7406\u89e3\uff0c\u4ee5\u6839\u636e\u573a\u666f\u7684\u521d\u59cb\u56fe\u50cf\u548c\u6700\u7ec8\u56fe\u50cf\u68c0\u6d4b\u62fe\u53d6\u548c\u653e\u7f6e\u4efb\u52a1\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u6536\u96c6\u4e86\u4e00\u4e2a\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\u548c\u62fe\u53d6\u653e\u7f6e\u4efb\u52a1\u68c0\u6d4b\u7684\u6570\u636e\u96c6\u3002\u968f\u540e\u8bad\u7ec3\u4e86\u4e00\u4e2a YOLOv5 \u7f51\u7edc\u6765\u68c0\u6d4b\u521d\u59cb\u573a\u666f\u548c\u6700\u7ec8\u573a\u666f\u4e2d\u7684\u76ee\u6807\u3002\u7ed9\u5b9a\u68c0\u6d4b\u5230\u7684\u76ee\u6807\u53ca\u5176\u8fb9\u754c\u6846\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\u6765\u68c0\u6d4b\u5c06\u521d\u59cb\u573a\u666f\u8f6c\u6362\u4e3a\u6700\u7ec8\u573a\u666f\u7684\u62fe\u53d6\u548c\u653e\u7f6e\u4efb\u52a1\u3002\u4e00\u79cd\u662f\u51e0\u4f55\u65b9\u6cd5\uff0c\u5b83\u8ddf\u8e2a\u76ee\u6807\u5728\u4e24\u4e2a\u573a\u666f\u4e2d\u7684\u8fd0\u52a8\uff0c\u5e76\u6839\u636e\u573a\u666f\u5185\u79fb\u52a8\u7684\u8fb9\u754c\u6846\u7684\u4ea4\u96c6\u8fdb\u884c\u5de5\u4f5c\u3002\u76f8\u53cd\uff0c\u57fa\u4e8e CNN \u7684\u65b9\u6cd5\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5c06\u5177\u6709\u76f8\u4ea4\u8fb9\u754c\u6846\u7684\u76ee\u6807\u5206\u7c7b\u4e3a 5 \u7c7b\uff0c\u663e\u793a\u76f8\u5173\u76ee\u6807\u4e4b\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\u3002\u7136\u540e\uff0c\u901a\u8fc7\u5206\u6790\u5305\u542b\u8fd9\u4e24\u4e2a\u573a\u666f\u7684\u5b9e\u9a8c\uff0c\u5f97\u51fa\u6267\u884c\u7684\u62fe\u53d6\u548c\u653e\u7f6e\u4efb\u52a1\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\uff0c\u4f7f\u7528 VGG16 \u9aa8\u5e72\u7f51\u7edc\u7684\u57fa\u4e8e CNN \u7684\u65b9\u6cd5\u7684\u6210\u529f\u7387\u6bd4\u51e0\u4f55\u65b9\u6cd5\u9ad8\u51fa\u7ea6 12 \u4e2a\u767e\u5206\u70b9\uff0c\u603b\u4f53\u6210\u529f\u7387\u4e3a 84.3%\u3002||\n", "2409.17597": "|**2024-09-26**|[Unifying Dimensions: A Linear Adaptive Approach to Lightweight Image Super-Resolution](http://arxiv.org/abs/2409.17597)|null|\u57fa\u4e8e\u7a97\u53e3\u7684 Transformer \u7531\u4e8e\u5176\u901a\u8fc7\u5c40\u90e8\u81ea\u6ce8\u610f\u529b\u673a\u5236 (SA) \u8fdb\u884c\u81ea\u9002\u5e94\u5efa\u6a21\u7684\u80fd\u529b\uff0c\u5728\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u4e0e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u76f8\u6bd4\uff0c\u5b83\u4eec\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u63a8\u7406\u5ef6\u8fdf\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u786e\u5b9a Transformer \u7684\u9002\u5e94\u6027\u6e90\u4e8e\u5176\u81ea\u9002\u5e94\u7a7a\u95f4\u805a\u5408\u548c\u5148\u8fdb\u7684\u7ed3\u6784\u8bbe\u8ba1\uff0c\u800c\u5176\u9ad8\u5ef6\u8fdf\u5219\u6e90\u4e8e\u4e0e\u5c40\u90e8 SA \u76f8\u5173\u7684\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u5e03\u5c40\u8f6c\u6362\u3002\u4e3a\u4e86\u6a21\u62df\u8fd9\u79cd\u805a\u5408\u65b9\u6cd5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u57fa\u4e8e\u5377\u79ef\u7684\u7ebf\u6027\u7126\u70b9\u53ef\u5206\u79bb\u6ce8\u610f\u529b\u673a\u5236 (FSA)\uff0c\u5141\u8bb8\u4ee5\u7ebf\u6027\u590d\u6742\u5ea6\u8fdb\u884c\u957f\u8ddd\u79bb\u52a8\u6001\u5efa\u6a21\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u6709\u6548\u7684\u53cc\u5206\u652f\u7ed3\u6784\uff0c\u7ed3\u5408\u8d85\u8f7b\u91cf\u7ea7\u4fe1\u606f\u4ea4\u6362\u6a21\u5757 (IEM)\uff0c\u4ee5\u589e\u5f3a Token Mixer \u5bf9\u4fe1\u606f\u7684\u805a\u5408\u80fd\u529b\u3002\u6700\u540e\uff0c\u5728\u7ed3\u6784\u65b9\u9762\uff0c\u6211\u4eec\u901a\u8fc7\u7ed3\u5408\u81ea\u95e8\u63a7\u673a\u5236\u6765\u4fee\u6539\u73b0\u6709\u7684\u57fa\u4e8e\u7a7a\u95f4\u95e8\u63a7\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u4fdd\u7559\u9ad8\u7ef4\u901a\u9053\u4fe1\u606f\uff0c\u4ece\u800c\u80fd\u591f\u5bf9\u66f4\u590d\u6742\u7684\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u3002\u57fa\u4e8e\u8fd9\u4e9b\u6539\u8fdb\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a\u7ebf\u6027\u81ea\u9002\u5e94\u6df7\u5408\u7f51\u7edc (LAMNet) \u7684\u57fa\u4e8e\u5377\u79ef\u7684 Transformer \u6846\u67b6\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLAMNet \u5728\u4fdd\u6301\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u57fa\u4e8e SA \u7684 Transformer \u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\uff0c\u63a8\u7406\u65f6\u95f4\u53ef\u8fbe \\(3\\times\\) \u52a0\u901f\u3002\u4ee3\u7801\u5c06\u516c\u5f00\u53d1\u5e03\u5728\uff1ahttps://github.com/zononhzy/LAMNet\u3002||\n", "2409.17583": "|**2024-09-26**|[Let the Quantum Creep In: Designing Quantum Neural Network Models by Gradually Swapping Out Classical Components](http://arxiv.org/abs/2409.17583)|**[link](https://github.com/peiyong-addwater/let-the-quantum-creep-in)**|\u4eba\u5de5\u667a\u80fd (AI) \u51ed\u501f\u5176\u4e58\u6570\u6548\u5e94\u548c\u5728\u591a\u4e2a\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u53ef\u80fd\u6210\u4e3a\u91cf\u5b50\u8ba1\u7b97\u7684\u91cd\u8981\u5e94\u7528\u9886\u57df\u3002\u7531\u4e8e\u73b0\u4ee3\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u901a\u5e38\u5efa\u7acb\u5728\u795e\u7ecf\u7f51\u7edc\u4e4b\u4e0a\uff0c\u56e0\u6b64\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u7684\u8bbe\u8ba1\u6210\u4e3a\u5c06\u91cf\u5b50\u8ba1\u7b97\u96c6\u6210\u5230\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002\u4e3a\u4e86\u66f4\u7ec6\u81f4\u5730\u63cf\u8ff0\u91cf\u5b50\u7ec4\u4ef6\u5bf9\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5728\u8be5\u6846\u67b6\u4e2d\uff0c\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u5c42\u9010\u6e10\u88ab\u5177\u6709\u76f8\u540c\u8f93\u5165\u548c\u8f93\u51fa\u7c7b\u578b\u3001\u540c\u65f6\u4fdd\u6301\u5c42\u95f4\u4fe1\u606f\u6d41\u4e0d\u53d8\u7684\u91cf\u5b50\u5c42\u6240\u53d6\u4ee3\uff0c\u8fd9\u4e0d\u540c\u4e8e\u76ee\u524d\u5927\u591a\u6570\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u7684\u7814\u7a76\uff0c\u540e\u8005\u503e\u5411\u4e8e\u7aef\u5230\u7aef\u7684\u91cf\u5b50\u6a21\u578b\u3002\u6211\u4eec\u4ece\u4e00\u4e2a\u6ca1\u6709\u4efb\u4f55\u6807\u51c6\u5316\u5c42\u6216\u6fc0\u6d3b\u51fd\u6570\u7684\u7b80\u5355\u4e09\u5c42\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u5f00\u59cb\uff0c\u9010\u6b65\u5c06\u7ecf\u5178\u5c42\u66f4\u6539\u4e3a\u76f8\u5e94\u7684\u91cf\u5b50\u7248\u672c\u3002\u6211\u4eec\u5bf9 MNIST\u3001FashionMNIST \u548c CIFAR-10 \u7b49\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u6570\u503c\u5b9e\u9a8c\uff0c\u4ee5\u8bc1\u660e\u7cfb\u7edf\u5f15\u5165\u91cf\u5b50\u7ec4\u4ef6\u6240\u5e26\u6765\u7684\u6027\u80fd\u53d8\u5316\u3002\u901a\u8fc7\u8fd9\u4e2a\u6846\u67b6\uff0c\u6211\u4eec\u7684\u7814\u7a76\u4e3a\u672a\u6765\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\uff0c\u5728\u8fd9\u4e9b\u6a21\u578b\u4e2d\uff0c\u5bfb\u627e\u80fd\u591f\u5229\u7528\u7ecf\u5178\u4e16\u754c\u548c\u91cf\u5b50\u4e16\u754c\u4f18\u52bf\u7684\u65b9\u6cd5\u548c\u6846\u67b6\u53ef\u80fd\u66f4\u4e3a\u6709\u5229\u3002||\n", "2409.17564": "|**2024-09-26**|[General Compression Framework for Efficient Transformer Object Tracking](http://arxiv.org/abs/2409.17564)|null|\u57fa\u4e8eTransformer\u7684\u8ddf\u8e2a\u5668\u5728\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002\u867d\u7136\u8fd9\u4e9b\u8ddf\u8e2a\u5668\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\uff0c\u4f46\u7531\u4e8e\u6548\u7387\u4f4e\u4e0b\uff0c\u5b83\u4eec\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u4e86\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5148\u524d\u7684\u65b9\u6cd5\u65e8\u5728\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u8ddf\u8e2a\u5668\u6216\u5c06\u77e5\u8bc6\u4ece\u8f83\u5927\u7684\u6559\u5e08\u6a21\u578b\u63d0\u70bc\u5230\u66f4\u7d27\u51d1\u7684\u5b66\u751f\u6a21\u578b\u4e2d\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u4ee5\u727a\u7272\u7cbe\u5ea6\u4e3a\u4ee3\u4ef7\u6765\u63d0\u9ad8\u901f\u5ea6\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u9ad8\u6548Transformer\u76ee\u6807\u8ddf\u8e2a\u6a21\u578b\u538b\u7f29\u6846\u67b6CompressTracker\uff0c\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u8ddf\u8e2a\u6a21\u578b\u538b\u7f29\u6210\u8f7b\u91cf\u7ea7\u8ddf\u8e2a\u5668\uff0c\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u6027\u80fd\u4e0b\u964d\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9636\u6bb5\u5212\u5206\u7b56\u7565\uff0c\u5c06\u6559\u5e08\u6a21\u578b\u7684Transformer\u5c42\u5212\u5206\u4e3a\u4e0d\u540c\u7684\u9636\u6bb5\uff0c\u4f7f\u5b66\u751f\u6a21\u578b\u80fd\u591f\u66f4\u6709\u6548\u5730\u6a21\u62df\u6bcf\u4e2a\u76f8\u5e94\u7684\u6559\u5e08\u9636\u6bb5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u79cd\u72ec\u7279\u7684\u66ff\u6362\u8bad\u7ec3\u6280\u672f\uff0c\u8be5\u6280\u672f\u6d89\u53ca\u7528\u6559\u5e08\u6a21\u578b\u4e2d\u7684\u76f8\u5e94\u9636\u6bb5\u968f\u673a\u66ff\u6362\u5b66\u751f\u6a21\u578b\u4e2d\u7684\u7279\u5b9a\u9636\u6bb5\uff0c\u800c\u4e0d\u662f\u5b64\u7acb\u5730\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u3002\u66ff\u6362\u8bad\u7ec3\u589e\u5f3a\u4e86\u5b66\u751f\u6a21\u578b\u590d\u5236\u6559\u5e08\u6a21\u578b\u884c\u4e3a\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u8feb\u4f7f\u5b66\u751f\u6a21\u578b\u6a21\u62df\u6559\u5e08\u6a21\u578b\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u9884\u6d4b\u6307\u5bfc\u548c\u9636\u6bb5\u6027\u7279\u5f81\u6a21\u62df\uff0c\u4ee5\u4fbf\u5728\u6559\u5e08\u6a21\u578b\u7684\u538b\u7f29\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u989d\u5916\u7684\u76d1\u7763\u3002\u6211\u4eec\u7684\u6846\u67b6CompressTracker\u5728\u7ed3\u6784\u4e0a\u662f\u4e0d\u53ef\u77e5\u7684\uff0c\u4f7f\u5176\u4e0e\u4efb\u4f55Transformer\u67b6\u6784\u517c\u5bb9\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u4e00\u7cfb\u5217\u5b9e\u9a8c\uff0c\u4ee5\u9a8c\u8bc1CompressTracker\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002\u6211\u4eec\u7684CompressTracker-4\u5177\u67094\u4e2aTransformer\u5c42\uff0c\u5b83\u662f\u4eceOSTrack\u538b\u7f29\u800c\u6765\u7684\uff0c\u5728LaSOT\u4e0a\u4fdd\u7559\u4e86\u7ea696%\u7684\u6027\u80fd\uff0866.1% AUC\uff09\uff0c\u540c\u65f6\u5b9e\u73b0\u4e862.17\u500d\u7684\u52a0\u901f\u3002||\n", "2409.17533": "|**2024-09-26**|[CAMOT: Camera Angle-aware Multi-Object Tracking](http://arxiv.org/abs/2409.17533)|null|\u672c\u6587\u63d0\u51fa\u4e86CAMOT\uff0c\u4e00\u79cd\u7528\u4e8e\u591a\u76ee\u6807\u8ddf\u8e2a\u7684\u7b80\u5355\u76f8\u673a\u89d2\u5ea6\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u4e24\u4e2a\u95ee\u9898\uff1a1\uff09\u906e\u6321\u548c2\uff09\u6df1\u5ea6\u65b9\u5411\u4e0a\u7684\u8ddd\u79bb\u4f30\u8ba1\u4e0d\u51c6\u786e\u3002\u5728\u5047\u8bbe\u6bcf\u4e2a\u89c6\u9891\u5e27\u4e2d\u7684\u591a\u4e2a\u76ee\u6807\u4f4d\u4e8e\u5e73\u9762\u4e0a\uff0cCAMOT \u4f7f\u7528\u76ee\u6807\u68c0\u6d4b\u6765\u4f30\u8ba1\u76f8\u673a\u89d2\u5ea6\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u7ed9\u51fa\u4e86\u6bcf\u4e2a\u76ee\u6807\u7684\u6df1\u5ea6\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u4f2a 3D MOT\u3002\u6211\u4eec\u901a\u8fc7\u5c06\u5176\u6dfb\u52a0\u5230 MOT17 \u548c MOT20 \u6570\u636e\u96c6\u4e0a\u7684\u5404\u79cd 2D MOT \u65b9\u6cd5\u4e2d\u6765\u8bc4\u4f30\u5176\u6027\u80fd\uff0c\u5e76\u786e\u8ba4\u4e86\u5176\u6709\u6548\u6027\u3002\u5c06 CAMOT \u5e94\u7528\u4e8e ByteTrack\uff0c\u6211\u4eec\u5728 MOT17 \u4e2d\u83b7\u5f97\u4e86 63.8% \u7684 HOTA\u300180.6% \u7684 MOTA \u548c 78.5% \u7684 IDF1\uff0c\u8fd9\u4e9b\u90fd\u662f\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u5b83\u7684\u8ba1\u7b97\u6210\u672c\u660e\u663e\u4f4e\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8ddf\u8e2a\u6df1\u5ea6\u4f30\u8ba1\u5668\u3002||\n", "2409.18951": "|**2024-09-27**|[Spectral Wavelet Dropout: Regularization in the Wavelet Domain](http://arxiv.org/abs/2409.18951)|null|\u6b63\u5219\u5316\u6280\u672f\u6709\u52a9\u4e8e\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u4ece\u800c\u63d0\u9ad8\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u7684\u6cdb\u5316\u80fd\u529b\u3002\u8fc7\u62df\u5408\u7684\u539f\u56e0\u4e4b\u4e00\u662f\u7f51\u7edc\u4e0d\u540c\u90e8\u5206\u4e4b\u95f4\u590d\u6742\u7684\u76f8\u4e92\u9002\u5e94\uff0c\u8fd9\u4f7f\u5f97 CNN \u4f9d\u8d56\u4e8e\u5b83\u4eec\u7684\u8054\u5408\u54cd\u5e94\uff0c\u800c\u4e0d\u662f\u9f13\u52b1\u6bcf\u4e2a\u90e8\u5206\u72ec\u7acb\u5b66\u4e60\u6709\u7528\u7684\u7279\u5f81\u8868\u793a\u3002\u9891\u57df\u5904\u7406\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u7b56\u7565\uff0c\u5b83\u5229\u7528\u9891\u7387\u5206\u89e3\u6765\u4fee\u6539\u5177\u6709\u65f6\u95f4\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u7684\u6570\u636e\u3002\u8fd9\u9879\u5de5\u4f5c\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u2014\u2014\u8c31\u5c0f\u6ce2\u4e22\u5f03 (SWD)\uff0c\u5b83\u5305\u62ec\u4e24\u79cd\u53d8\u4f53\uff1a1D-SWD \u548c 2D-SWD\u3002\u8fd9\u4e9b\u53d8\u4f53\u901a\u8fc7\u968f\u673a\u4e22\u5f03\u7279\u5f81\u56fe\u7684\u79bb\u6563\u5c0f\u6ce2\u5206\u89e3\u4e2d\u7684\u8be6\u7ec6\u9891\u5e26\uff0c\u4ece\u800c\u63d0\u9ad8 CNN \u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u533a\u522b\u4e8e\u9884\u5148\u5b58\u5728\u7684\u8c31\u201c\u5085\u7acb\u53f6\u201d\u4e22\u5f03 (2D-SFD)\uff0c\u540e\u8005\u6d88\u9664\u4e86\u5085\u7acb\u53f6\u57df\u4e2d\u7684\u7cfb\u6570\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSWD \u53ea\u9700\u8981\u4e00\u4e2a\u8d85\u53c2\u6570\uff0c\u4e0d\u50cf SFD \u9700\u8981\u4e24\u4e2a\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u5b9e\u73b0\u4e00\u7ef4\u7248\u672c\u7684\u8c31\u201c\u5085\u7acb\u53f6\u201d\u4e22\u5f03 (1D-SFD) \u6765\u6269\u5c55\u6587\u732e\uff0c\u4e3a\u5168\u9762\u6bd4\u8f83\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u76f8\u5bf9\u4e8e 1D-SFD \u548c 2D-SFD\uff0c1D \u548c 2D SWD \u53d8\u4f53\u5728 CIFAR-10/100 \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4e0e 1D/2D-SFD \u76f8\u6bd4\uff0c1D-SWD \u5177\u6709\u663e\u8457\u66f4\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u5728 Pascal VOC \u76ee\u6807\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSWD \u53d8\u4f53\u7684\u6027\u80fd\u4f18\u4e8e 1D-SFD \u548c 2D-SFD\uff0c\u5e76\u4e14\u5728\u8bad\u7ec3\u671f\u95f4\u8868\u73b0\u51fa\u66f4\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002||\n", "2409.18946": "|**2024-09-27**|[Unconditional stability of a recurrent neural circuit implementing divisive normalization](http://arxiv.org/abs/2409.18946)|null|\u9012\u5f52\u795e\u7ecf\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5f00\u53d1\u53ef\u4ee5\u65e0\u7f1d\u8bad\u7ec3\u7684\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684 neurodynamical \u6a21\u578b\u65b9\u9762\u3002\u4f20\u7edf\u7684\u76ae\u8d28\u56de\u8def\u6a21\u578b\u7531\u4e8e\u52a8\u529b\u7cfb\u7edf\u4e2d\u5b58\u5728\u5e7f\u6cdb\u7684\u975e\u7ebf\u6027\uff0c\u56e0\u6b64\u96be\u4ee5\u8bad\u7ec3\uff0c\u5bfc\u81f4\u4f18\u5316\u95ee\u9898\u5177\u6709\u96be\u4ee5\u65bd\u52a0\u7684\u975e\u7ebf\u6027\u7a33\u5b9a\u6027\u7ea6\u675f\u3002\u76f8\u53cd\uff0c\u9012\u5f52\u795e\u7ecf\u7f51\u7edc (RNN) \u5728\u6d89\u53ca\u5e8f\u5217\u6570\u636e\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u751f\u7269\u5b66\u4e0a\u7684\u5408\u7406\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5c06\u52a8\u6001\u9664\u6cd5\u5f52\u4e00\u5316 (DN) \u4e0e ORGaNICs \u7684\u7a33\u5b9a\u6027\u8054\u7cfb\u8d77\u6765\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0cORGaNICs \u662f\u4e00\u79cd\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684\u9012\u5f52\u76ae\u8d28\u56de\u8def\u6a21\u578b\uff0c\u5b83\u53ef\u4ee5\u52a8\u6001\u5730\u5b9e\u73b0 DN\uff0c\u5e76\u4e14\u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u6a21\u62df\u5e7f\u6cdb\u7684\u795e\u7ecf\u751f\u7406\u5b66\u73b0\u8c61\u3002\u901a\u8fc7\u4f7f\u7528 Lyapunov \u7684\u95f4\u63a5\u65b9\u6cd5\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5f53\u9012\u5f52\u6743\u91cd\u77e9\u9635\u662f\u5355\u4f4d\u77e9\u9635\u65f6\uff0c\u4efb\u610f\u7ef4\u5ea6\u7684 ORGaNICs \u7535\u8def\u5177\u6709\u65e0\u6761\u4ef6\u5c40\u90e8\u7a33\u5b9a\u6027\u7684\u663e\u8457\u7279\u6027\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5c06 ORGaNICs \u8fde\u63a5\u5230\u4e00\u4e2a\u8026\u5408\u963b\u5c3c\u8c10\u632f\u5b50\u7684\u7cfb\u7edf\uff0c\u8fd9\u4f7f\u6211\u4eec\u80fd\u591f\u63a8\u5bfc\u51fa\u7535\u8def\u7684\u80fd\u91cf\u51fd\u6570\uff0c\u4ece\u800c\u63d0\u4f9b\u7535\u8def\u548c\u5355\u4e2a\u795e\u7ecf\u5143\u65e8\u5728\u5b9e\u73b0\u7684\u76ee\u6807\u7684\u89c4\u8303\u539f\u5219\u3002\u6b64\u5916\uff0c\u5bf9\u4e8e\u4e00\u822c\u7684\u9012\u5f52\u6743\u91cd\u77e9\u9635\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u4e8c\u7ef4\u6a21\u578b\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u7ecf\u9a8c\u8bc1\u660e\u4e86\u7a33\u5b9a\u6027\u5728\u66f4\u9ad8\u7ef4\u5ea6\u4e0a\u6210\u7acb\u3002\u6700\u540e\uff0c\u6211\u4eec\u8868\u660e ORGaNICs \u53ef\u4ee5\u901a\u8fc7\u65f6\u95f4\u53cd\u5411\u4f20\u64ad\u8fdb\u884c\u8bad\u7ec3\uff0c\u800c\u65e0\u9700\u68af\u5ea6\u88c1\u526a/\u7f29\u653e\uff0c\u8fd9\u5f97\u76ca\u4e8e\u5176\u5185\u5728\u7684\u7a33\u5b9a\u6027\u7279\u6027\u548c\u81ea\u9002\u5e94\u65f6\u95f4\u5e38\u6570\uff0c\u89e3\u51b3\u4e86\u68af\u5ea6\u7206\u70b8\u3001\u6d88\u5931\u548c\u632f\u8361\u7684\u95ee\u9898\u3002\u901a\u8fc7\u8bc4\u4f30\u6a21\u578b\u5728 RNN \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\uff0c\u6211\u4eec\u53d1\u73b0 ORGaNICs \u5728\u9759\u6001\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5176\u4ed6\u795e\u7ecf\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u4e14\u5728\u5e8f\u5217\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u4e0e LSTM \u76f8\u5f53\u3002||\n", "2409.18918": "|**2024-09-27**|[Subspace Preserving Quantum Convolutional Neural Network Architectures](http://arxiv.org/abs/2409.18918)|null|\u5b50\u7a7a\u95f4\u4fdd\u6301\u91cf\u5b50\u7535\u8def\u662f\u4e00\u7c7b\u91cf\u5b50\u7b97\u6cd5\uff0c\u5b83\u4f9d\u8d56\u4e8e\u8ba1\u7b97\u4e2d\u7684\u67d0\u4e9b\u5bf9\u79f0\u6027\uff0c\u53ef\u4ee5\u4e3a\u5176\u8bad\u7ec3\u63d0\u4f9b\u7406\u8bba\u4e0a\u7684\u4fdd\u8bc1\u3002\u8fd9\u4e9b\u7b97\u6cd5\u4e4b\u6240\u4ee5\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u662f\u56e0\u4e3a\u5b83\u4eec\u53ef\u4ee5\u63d0\u4f9b\u591a\u9879\u5f0f\u52a0\u901f\uff0c\u5e76\u4e14\u53ef\u4ee5\u7528\u6765\u6a21\u62df\u7ecf\u5178\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6c49\u660e\u91cd\u91cf\u4fdd\u6301\u91cf\u5b50\u7535\u8def\u7684\u65b0\u578b\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u6a21\u578b\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5377\u79ef\u5c42\u548c\u57fa\u4e8e\u6d4b\u91cf\u7684\u6c60\u5316\u5c42\uff0c\u5b83\u4eec\u5728\u4fdd\u6301\u91cf\u5b50\u6001\u5bf9\u79f0\u6027\u7684\u540c\u65f6\uff0c\u4f7f\u7528\u975e\u5b50\u7a7a\u95f4\u4fdd\u6301\u7684\u95e8\u6765\u5b9e\u73b0\u975e\u7ebf\u6027\u3002\u4e0e\u7ecf\u5178\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6848\u5728\u591a\u9879\u5f0f\u8fd0\u884c\u65f6\u95f4\u4e0a\u5177\u6709\u663e\u8457\u7684\u4f18\u52bf\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7528\u4e8e\u6c49\u660e\u91cd\u91cf\u4fdd\u6301\u91cf\u5b50\u7535\u8def\u7684\u5f00\u6e90\u4eff\u771f\u5e93\uff0c\u53ef\u4ee5\u4f7f\u7528\u9762\u5411GPU\u7684\u5e93\u66f4\u6709\u6548\u5730\u4eff\u771f\u6211\u4eec\u7684\u6280\u672f\u3002\u4f7f\u7528\u6b64\u4ee3\u7801\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e9b\u67b6\u6784\u793a\u4f8b\uff0c\u8fd9\u4e9b\u793a\u4f8b\u7a81\u51fa\u4e86\u5728\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u6709\u9650\u4e14\u53c2\u6570\u5c11\u4e8e\u7ecf\u5178\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u590d\u6742\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u51fa\u8272\u6027\u80fd\u3002||\n", "2409.18866": "|**2024-09-27**|[MCUBench: A Benchmark of Tiny Object Detectors on MCUs](http://arxiv.org/abs/2409.18866)|**[link](https://github.com/deeplite/deeplite-torch-zoo)**|\u6211\u4eec\u63a8\u51fa\u4e86 MCUBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6db5\u76d6\u4e86 100 \u591a\u4e2a\u57fa\u4e8e YOLO \u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728 VOC \u6570\u636e\u96c6\u4e0a\u9488\u5bf9\u4e03\u79cd\u4e0d\u540c\u7684 MCU \u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\u63d0\u4f9b\u4e86\u5404\u79cd\u8f93\u5165\u5206\u8fa8\u7387\u548c\u57fa\u4e8e YOLO \u7684\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\u7684\u5e73\u5747\u7cbe\u5ea6\u3001\u5ef6\u8fdf\u3001RAM \u548c Flash \u4f7f\u7528\u60c5\u51b5\u7684\u8be6\u7ec6\u4fe1\u606f\u3002\u901a\u8fc7\u4f7f\u7528\u56fa\u5b9a\u7684\u8bad\u7ec3\u6d41\u7a0b\u8fdb\u884c\u53d7\u63a7\u6bd4\u8f83\uff0c\u6211\u4eec\u6536\u96c6\u4e86\u5168\u9762\u7684\u6027\u80fd\u6307\u6807\u3002\u6211\u4eec\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u5206\u6790\u8868\u660e\uff0c\u96c6\u6210\u73b0\u4ee3\u68c0\u6d4b\u5934\u548c\u8bad\u7ec3\u6280\u672f\u53ef\u4ee5\u8ba9\u5404\u79cd YOLO \u67b6\u6784\uff08\u5305\u62ec YOLOv3 \u7b49\u4f20\u7edf\u6a21\u578b\uff09\u5728\u5e73\u5747\u7cbe\u5ea6 (mAP) \u548c\u5ef6\u8fdf\u4e4b\u95f4\u5b9e\u73b0\u9ad8\u6548\u7684\u6743\u8861\u3002MCUBench \u662f\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u5bf9\u5f53\u4ee3\u76ee\u6807\u68c0\u6d4b\u5668\u7684 MCU \u6027\u80fd\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u6839\u636e\u7279\u5b9a\u9650\u5236\u6761\u4ef6\u5e2e\u52a9\u8fdb\u884c\u6a21\u578b\u9009\u62e9\u3002||\n", "2409.18686": "|**2024-09-27**|[A Novel Unified Architecture for Low-Shot Counting by Detection and Segmentation](http://arxiv.org/abs/2409.18686)|null|\u5c11\u6837\u672c\u76ee\u6807\u8ba1\u6570\u5668\u53ef\u4ee5\u4f7f\u7528\u5c11\u91cf\u751a\u81f3\u6ca1\u6709\u6807\u6ce8\u6837\u672c\u4f30\u8ba1\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u6570\u91cf\u3002\u76ee\u6807\u5b9a\u4f4d\u901a\u8fc7\u5c06\u76ee\u6807\u4e0e\u539f\u578b\u8fdb\u884c\u5339\u914d\u6765\u5b9e\u73b0\uff0c\u539f\u578b\u662f\u901a\u8fc7\u5bf9\u56fe\u50cf\u8303\u56f4\u5185\u7684\u76ee\u6807\u5916\u89c2\u8fdb\u884c\u65e0\u76d1\u7763\u805a\u5408\u6784\u5efa\u7684\u3002\u7531\u4e8e\u76ee\u6807\u5916\u89c2\u53ef\u80fd\u5b58\u5728\u591a\u6837\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u6cdb\u5316\u548c\u8bef\u62a5\u3002\u6b64\u5916\uff0c\u6027\u80fd\u6700\u4f73\u7684\u65b9\u6cd5\u901a\u8fc7\u9884\u6d4b\u6bcf\u4e2a\u76ee\u6807\u4e2d\u5fc3\u7684\u5355\u4f4d\u9ad8\u65af\u5206\u5e03\u7684\u4ee3\u7406\u635f\u5931\u6765\u8bad\u7ec3\u76ee\u6807\u5b9a\u4f4d\u3002\u8fd9\u79cd\u635f\u5931\u5bf9\u6807\u6ce8\u8bef\u5dee\u548c\u8d85\u53c2\u6570\u5f88\u654f\u611f\uff0c\u5e76\u4e14\u6ca1\u6709\u76f4\u63a5\u4f18\u5316\u68c0\u6d4b\u4efb\u52a1\uff0c\u5bfc\u81f4\u8ba1\u6570\u7ed3\u679c\u6b20\u4f73\u3002\u6211\u4eec\u5f15\u5165\u4e86GeCo\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5c11\u6837\u672c\u8ba1\u6570\u5668\uff0c\u53ef\u4ee5\u5728\u7edf\u4e00\u7684\u67b6\u6784\u4e2d\u5b9e\u73b0\u51c6\u786e\u7684\u76ee\u6807\u68c0\u6d4b\u3001\u5206\u5272\u548c\u8ba1\u6570\u4f30\u8ba1\u3002GeCo \u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u5bc6\u96c6\u76ee\u6807\u67e5\u8be2\u516c\u5f0f\uff0c\u53ef\u4ee5\u7a33\u5065\u5730\u6cdb\u5316\u4e0d\u540c\u76ee\u6807\u5916\u89c2\u7684\u539f\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8ba1\u6570\u635f\u5931\uff0c\u5b83\u76f4\u63a5\u4f18\u5316\u68c0\u6d4b\u4efb\u52a1\uff0c\u907f\u514d\u4e86\u6807\u51c6\u4ee3\u7406\u635f\u5931\u7684\u95ee\u9898\u3002GeCo \u5728\u603b\u8ba1\u6570\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u65b9\u9762\u6bd4\u9886\u5148\u7684\u57fa\u4e8e\u5c11\u6837\u672c\u68c0\u6d4b\u7684\u8ba1\u6570\u5668\u9ad8\u51fa\u7ea6 25%\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u5728\u6240\u6709\u5c11\u6837\u672c\u8ba1\u6570\u8bbe\u7f6e\u4e2d\u90fd\u6811\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002||\n", "2409.18408": "|**2024-09-27**|[Query matching for spatio-temporal action detection with query-based object detector](http://arxiv.org/abs/2409.18408)|null|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u57fa\u4e8e\u67e5\u8be2\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578bDETR\u7684\u65b9\u6cd5\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u65f6\u7a7a\u52a8\u4f5c\u68c0\u6d4b\uff0c\u8be5\u4efb\u52a1\u9700\u8981\u5728\u89c6\u9891\u4e2d\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5c06DETR\u5e94\u7528\u4e8e\u6bcf\u4e00\u5e27\uff0c\u5e76\u4f7f\u7528\u7279\u5f81\u504f\u79fb\u6765\u6574\u5408\u65f6\u95f4\u4fe1\u606f\u3002\u7136\u800c\uff0c\u6bcf\u5e27\u4e2dDETR\u7684\u5bf9\u8c61\u67e5\u8be2\u53ef\u80fd\u5bf9\u5e94\u4e8e\u4e0d\u540c\u7684\u5bf9\u8c61\uff0c\u4f7f\u5f97\u7b80\u5355\u7684\u7279\u5f81\u504f\u79fb\u65e0\u6548\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8de8\u4e0d\u540c\u5e27\u7684\u67e5\u8be2\u5339\u914d\uff0c\u786e\u4fdd\u5bf9\u540c\u4e00\u5bf9\u8c61\u7684\u67e5\u8be2\u80fd\u591f\u5339\u914d\u5e76\u7528\u4e8e\u7279\u5f81\u504f\u79fb\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u4f7f\u7528\u6240\u63d0\u51fa\u7684\u67e5\u8be2\u5339\u914d\u5bf9\u67e5\u8be2\u7279\u5f81\u8fdb\u884c\u504f\u79fb\u65f6\uff0cJHMDB21\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u663e\u8457\u63d0\u9ad8\u3002||\n", "2409.18387": "|**2024-09-27**|[Simpler Gradient Methods for Blind Super-Resolution with Lower Iteration Complexity](http://arxiv.org/abs/2409.18387)|**[link](https://github.com/Jinshengg/SimplerGDs-VHL)**|\u6211\u4eec\u7814\u7a76\u4e86\u76f2\u8d85\u5206\u8fa8\u7387\u95ee\u9898\uff0c\u5b83\u53ef\u4ee5\u901a\u8fc7\u5411\u91cf\u5316\u6c49\u514b\u5c14\u63d0\u5347\uff08VHL\uff09\u516c\u5f0f\u5316\u4e3a\u4e00\u4e2a\u4f4e\u79e9\u77e9\u9635\u6062\u590d\u95ee\u9898\u3002\u5148\u524d\u57fa\u4e8eVHL\u7684\u540d\u4e3aPGD-VHL\u7684\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u989d\u5916\u7684\u6b63\u5219\u5316\uff0c\u4f8b\u5982\u6295\u5f71\u548c\u5e73\u8861\u60e9\u7f5a\uff0c\u8868\u73b0\u51fa\u6b21\u4f18\u7684\u8fed\u4ee3\u590d\u6742\u5ea6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u7b80\u5355\u7684\u65e0\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u65e0\u9700\u4e0a\u8ff0\u4e24\u79cd\u7c7b\u578b\u7684\u6b63\u5219\u5316\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u79cd\u65b0\u7684\u53ef\u8bc1\u68af\u5ea6\u65b9\u6cd5\uff0c\u5206\u522b\u540d\u4e3aVGD-VHL\u548cScalGD-VHL\u3002\u6211\u4eec\u4e3a\u7b97\u6cd5\u7684\u7406\u8bba\u4fdd\u8bc1\u63d0\u4f9b\u4e86\u65b0\u9896\u800c\u6e05\u6670\u7684\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u6bd4PGD-VHL\u5177\u6709\u66f4\u4f4e\u7684\u8fed\u4ee3\u590d\u6742\u5ea6\u3002\u6b64\u5916\uff0cScalGD-VHL\u5177\u6709\u6700\u4f4e\u7684\u8fed\u4ee3\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4e0e\u6761\u4ef6\u6570\u65e0\u5173\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b0\u5206\u6790\u8868\u660e\uff0c\u76f2\u8d85\u5206\u8fa8\u7387\u95ee\u9898\u5bf9\u4e0d\u76f8\u5e72\u6027\u7684\u8981\u6c42\u8f83\u4f4e\uff0c\u4ece\u800c\u65e0\u9700\u4e0d\u76f8\u5e72\u6295\u5f71\u5373\u53ef\u5b9e\u73b0\u7ebf\u6027\u6536\u655b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5b9e\u73b0\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u5f53\u7684\u6062\u590d\u6027\u80fd\u7684\u540c\u65f6\uff0c\u8fd8\u5177\u6709\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u3002||\n", "2409.18314": "|**2024-09-26**|[Realistic Evaluation of Model Merging for Compositional Generalization](http://arxiv.org/abs/2409.18314)|**[link](https://github.com/r-three/realistic_evaluation_of_model_merging_for_compositional_generalization)**|\u6a21\u578b\u878d\u5408\u5df2\u6210\u4e3a\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5c06\u5355\u4e2a\u6a21\u578b\u5ec9\u4ef7\u5730\u7ec4\u5408\u6210\u4e00\u4e2a\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7ee7\u627f\u4e86\u5b83\u4eec\u7684\u6027\u80fd\u5e76\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002\u8fd9\u79cd\u6d41\u884c\u4fc3\u8fdb\u4e86\u8bb8\u591a\u65b0\u878d\u5408\u65b9\u6cd5\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u5728\u4e0d\u540c\u7684\u5b9e\u9a8c\u73af\u5883\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u5e76\u4e14\u7ecf\u5e38\u5728\u5bf9\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u53ef\u7528\u6027\u548c\u8ba1\u7b97\u9884\u7b97\u505a\u51fa\u7684\u5047\u8bbe\u65b9\u9762\u6709\u6240\u4e0d\u540c\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5728\u5171\u4eab\u5b9e\u9a8c\u73af\u5883\u4e2d\u8bc4\u4f30\u4e0d\u540c\u7684\u878d\u5408\u65b9\u6cd5\u5e76\u7cbe\u786e\u8bc6\u522b\u6bcf\u79cd\u65b9\u6cd5\u7684\u5b9e\u9645\u8981\u6c42\uff0c\u6765\u63cf\u8ff0\u5b83\u4eec\u7684\u76f8\u5bf9\u4f18\u70b9\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u8bbe\u7f6e\u4fa7\u91cd\u4e8e\u4f7f\u7528\u878d\u5408\u6765\u5b9e\u73b0\u56fe\u50cf\u5206\u7c7b\u3001\u56fe\u50cf\u751f\u6210\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u529f\u80fd\u7684\u7ec4\u5408\u6cdb\u5316\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u6d4b\u91cf\u4e86\u4e0d\u540c\u878d\u5408\u65b9\u6cd5\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u4ee5\u53ca\u5b83\u4eec\u5728\u6269\u5c55\u878d\u5408\u6a21\u578b\u6570\u91cf\u65f6\u7684\u6027\u80fd\u3002\u603b\u7684\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u9610\u660e\u4e86\u6a21\u578b\u878d\u5408\u9886\u57df\u7684\u73b0\u72b6\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u800c\u4e25\u8c28\u7684\u5b9e\u9a8c\u8bbe\u7f6e\u6765\u6d4b\u8bd5\u65b0\u65b9\u6cd5\u3002||\n", "2409.18286": "|**2024-09-26**|[Advancing Object Detection in Transportation with Multimodal Large Language Models (MLLMs): A Comprehensive Review and Empirical Testing](http://arxiv.org/abs/2409.18286)|null|\u672c\u7814\u7a76\u65e8\u5728\u5168\u9762\u56de\u987e\u548c\u5b9e\u8bc1\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u548c\u5927\u578b\u89c6\u89c9\u6a21\u578b (VLM) \u5728\u4ea4\u901a\u7cfb\u7edf\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\u3002\u9996\u5148\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 MLLM \u5728\u4ea4\u901a\u5e94\u7528\u4e2d\u7684\u6f5c\u5728\u4f18\u52bf\uff0c\u5e76\u5bf9\u4ee5\u5f80\u7814\u7a76\u4e2d\u73b0\u6709\u7684 MLLM \u6280\u672f\u8fdb\u884c\u4e86\u5168\u9762\u56de\u987e\u3002\u6211\u4eec\u91cd\u70b9\u4ecb\u7ecd\u4e86\u5b83\u4eec\u5728\u5404\u79cd\u4ea4\u901a\u573a\u666f\u4e0b\u76ee\u6807\u68c0\u6d4b\u7684\u6709\u6548\u6027\u548c\u5c40\u9650\u6027\u3002\u5176\u6b21\uff0c\u6211\u4eec\u6982\u8ff0\u4e86\u4ea4\u901a\u5e94\u7528\u4e2d\u7aef\u5230\u7aef\u76ee\u6807\u68c0\u6d4b\u7684\u5206\u7c7b\u4ee5\u53ca\u672a\u6765\u65b9\u5411\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5b9e\u8bc1\u5206\u6790\uff0c\u5728\u4e09\u4e2a\u73b0\u5b9e\u4ea4\u901a\u95ee\u9898\u4e0a\u6d4b\u8bd5 MLLM\uff0c\u8fd9\u4e9b\u95ee\u9898\u5305\u62ec\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\uff0c\u5373\u9053\u8def\u5b89\u5168\u5c5e\u6027\u63d0\u53d6\u3001\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\u68c0\u6d4b\u548c\u70ed\u56fe\u50cf\u89c6\u89c9\u63a8\u7406\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u5bf9 MLLM \u6027\u80fd\u7684\u8be6\u7ec6\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u5176\u4f18\u52bf\u548c\u9700\u8981\u6539\u8fdb\u7684\u65b9\u9762\u3002\u6700\u540e\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86 MLLM \u5728\u589e\u5f3a\u4ea4\u901a\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\u7684\u5b9e\u9645\u5c40\u9650\u6027\u548c\u6311\u6218\uff0c\u4ece\u800c\u4e3a\u8be5\u5173\u952e\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u548c\u5f00\u53d1\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\u3002||\n", "2409.20508": "|**2024-09-30**|[NUTRIVISION: A System for Automatic Diet Management in Smart Healthcare](http://arxiv.org/abs/2409.20508)|null|\u901a\u8fc7\u5747\u8861\u996e\u98df\u4fdd\u6301\u5065\u5eb7\u548c\u5f3a\u5065\u4f53\u9b44\u5bf9\u4e8e\u9884\u9632\u5fc3\u810f\u75c5\u3001\u7cd6\u5c3f\u75c5\u548c\u764c\u75c7\u7b49\u975e\u4f20\u67d3\u6027\u75be\u75c5\u81f3\u5173\u91cd\u8981\u3002NutriVision \u5c06\u667a\u80fd\u533b\u7597\u4fdd\u5065\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u4ee5\u5e94\u5bf9\u8425\u517b\u548c\u996e\u98df\u7ba1\u7406\u65b9\u9762\u7684\u6311\u6218\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u53ef\u4ee5\u8bc6\u522b\u98df\u7269\u79cd\u7c7b\uff0c\u4f30\u7b97\u6570\u91cf\uff0c\u5e76\u63d0\u4f9b\u5168\u9762\u7684\u8425\u517b\u4fe1\u606f\u3002NutriVision \u91c7\u7528\u4e86\u57fa\u4e8e Faster Region \u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u8fd9\u662f\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u533a\u57df\u63d0proposals \u5e76\u5bf9\u8fd9\u4e9b\u533a\u57df\u8fdb\u884c\u5206\u7c7b\u6765\u6539\u8fdb\u5bf9\u8c61\u68c0\u6d4b\uff0c\u4f7f\u5176\u5373\u4f7f\u5728\u590d\u6742\u548c\u65e0\u5e8f\u7684\u81b3\u98df\u73af\u5883\u4e2d\u4e5f\u80fd\u9ad8\u6548\u3001\u51c6\u786e\u5730\u8bc6\u522b\u98df\u7269\u3002\u901a\u8fc7\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u7684\u56fe\u50cf\u6355\u6349\uff0cNutriVision \u53ef\u4ee5\u63d0\u4f9b\u5373\u65f6\u8425\u517b\u6570\u636e\uff0c\u5305\u62ec\u5b8f\u91cf\u8425\u517b\u7d20\u5206\u89e3\u3001\u5361\u8def\u91cc\u8ba1\u6570\u548c\u5fae\u91cf\u8425\u517b\u7d20\u8be6\u7ec6\u4fe1\u606f\u3002NutriVision \u7684\u7a81\u51fa\u7279\u70b9\u4e4b\u4e00\u662f\u5176\u4e2a\u6027\u5316\u7684\u8425\u517b\u5206\u6790\u548c\u996e\u98df\u5efa\u8bae\uff0c\u8fd9\u4e9b\u5efa\u8bae\u662f\u6839\u636e\u6bcf\u4e2a\u7528\u6237\u7684\u996e\u98df\u504f\u597d\u3001\u8425\u517b\u9700\u6c42\u548c\u5065\u5eb7\u53f2\u91cf\u8eab\u5b9a\u5236\u7684\u3002\u901a\u8fc7\u63d0\u4f9b\u5b9a\u5236\u5316\u7684\u5efa\u8bae\uff0cNutriVision \u5e2e\u52a9\u7528\u6237\u5b9e\u73b0\u7279\u5b9a\u7684\u5065\u5eb7\u548c\u5065\u8eab\u76ee\u6807\uff0c\u4f8b\u5982\u7ba1\u7406\u996e\u98df\u9650\u5236\u6216\u63a7\u5236\u4f53\u91cd\u3002\u9664\u4e86\u63d0\u4f9b\u7cbe\u786e\u7684\u98df\u7269\u68c0\u6d4b\u548c\u8425\u517b\u8bc4\u4f30\u5916\uff0cNutriVision \u8fd8\u901a\u8fc7\u5c06\u7528\u6237\u6570\u636e\u4e0e\u4fc3\u8fdb\u5747\u8861\u5065\u5eb7\u996e\u98df\u7684\u5efa\u8bae\u76f8\u7ed3\u5408\uff0c\u652f\u6301\u66f4\u660e\u667a\u7684\u996e\u98df\u51b3\u7b56\u3002\u8be5\u7cfb\u7edf\u4e3a\u8425\u517b\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6709\u53ef\u80fd\u663e\u8457\u5f71\u54cd\u4eba\u4eec\u7684\u996e\u98df\u9009\u62e9\u65b9\u5f0f\uff0c\u4fc3\u8fdb\u66f4\u5065\u5eb7\u7684\u996e\u98df\u4e60\u60ef\u548c\u6574\u4f53\u5065\u5eb7\u3002\u672c\u6587\u8ba8\u8bba\u4e86 NutriVision \u7cfb\u7edf\u7684\u8bbe\u8ba1\u3001\u6027\u80fd\u8bc4\u4f30\u548c\u672a\u6765\u5e94\u7528\u3002||\n", "2409.20447": "|**2024-09-30**|[POMONAG: Pareto-Optimal Many-Objective Neural Architecture Generator](http://arxiv.org/abs/2409.20447)|null|\u795e\u7ecf\u67b6\u6784\u641c\u7d22 (NAS) \u81ea\u52a8\u5316\u4e86\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\uff0c\u51cf\u5c11\u4e86\u5bf9\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u7684\u4f9d\u8d56\u3002\u867d\u7136 NAS \u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u4e14\u4f9d\u8d56\u4e8e\u7279\u5b9a\u6570\u636e\u96c6\uff0c\u4f46\u8f85\u52a9\u9884\u6d4b\u5668\u51cf\u5c11\u4e86\u9700\u8981\u8bad\u7ec3\u7684\u6a21\u578b\u6570\u91cf\uff0c\u4ece\u800c\u7f29\u77ed\u4e86\u641c\u7d22\u65f6\u95f4\u3002\u6b64\u7b56\u7565\u7528\u4e8e\u751f\u6210\u6ee1\u8db3\u591a\u4e2a\u8ba1\u7b97\u7ea6\u675f\u7684\u67b6\u6784\u3002\u6700\u8fd1\uff0c\u53ef\u8fc1\u79fb NAS \u5e94\u8fd0\u800c\u751f\uff0c\u5c06\u641c\u7d22\u8fc7\u7a0b\u4ece\u4f9d\u8d56\u4e8e\u6570\u636e\u96c6\u63a8\u5e7f\u5230\u4f9d\u8d56\u4e8e\u4efb\u52a1\u3002\u5728\u8be5\u9886\u57df\uff0cDiffusionNAG \u662f\u4e00\u79cd\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u8fd9\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u7b80\u5316\u4e86\u8ba1\u7b97\uff0c\u751f\u6210\u9488\u5bf9\u672a\u89c1\u6570\u636e\u96c6\u7684\u51c6\u786e\u6027\u8fdb\u884c\u4f18\u5316\u7684\u67b6\u6784\uff0c\u800c\u65e0\u9700\u8fdb\u4e00\u6b65\u8c03\u6574\u3002\u7136\u800c\uff0cDiffusionNAG \u53ea\u5173\u6ce8\u51c6\u786e\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u5176\u4ed6\u5173\u952e\u76ee\u6807\uff0c\u5982\u6a21\u578b\u590d\u6742\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u8fd9\u4e9b\u56e0\u7d20\u5bf9\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u5e15\u7d2f\u6258\u6700\u4f18\u591a\u76ee\u6807\u795e\u7ecf\u67b6\u6784\u751f\u6210\u5668 (POMONAG)\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u6269\u6563\u8fc7\u7a0b\u6269\u5c55\u4e86 DiffusionNAG\u3002POMONAG \u540c\u65f6\u8003\u8651\u51c6\u786e\u6027\u3001\u53c2\u6570\u6570\u91cf\u3001\u4e58\u79ef\u7d2f\u52a0\u8fd0\u7b97 (MAC) \u548c\u63a8\u7406\u5ef6\u8fdf\u3002\u5b83\u96c6\u6210\u4e86\u6027\u80fd\u9884\u6d4b\u5668\u6a21\u578b\u6765\u4f30\u8ba1\u8fd9\u4e9b\u6307\u6807\u5e76\u6307\u5bfc\u6269\u6563\u68af\u5ea6\u3002POMONAG \u7684\u4f18\u5316\u901a\u8fc7\u6269\u5c55\u5176\u8bad\u7ec3\u5143\u6570\u636e\u96c6\u3001\u5e94\u7528\u5e15\u7d2f\u6258\u524d\u6cbf\u8fc7\u6ee4\u548c\u6539\u8fdb\u6761\u4ef6\u751f\u6210\u7684\u5d4c\u5165\u6765\u589e\u5f3a\u3002\u8fd9\u4e9b\u589e\u5f3a\u529f\u80fd\u4f7f POMONAG \u80fd\u591f\u751f\u6210\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u6280\u672f\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u67b6\u6784\u3002\u7ed3\u679c\u5728\u4e24\u4e2a\u641c\u7d22\u7a7a\u95f4\uff08NASBench201 \u548c MobileNetV3\uff09\u4e0a\u5f97\u5230\u9a8c\u8bc1\uff0c\u5e76\u5728 15 \u4e2a\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002||\n", "2409.20329": "|**2024-09-30**|[Fine-Tuning Personalization in Federated Learning to Mitigate Adversarial Clients](http://arxiv.org/abs/2409.20329)|null|\u8054\u90a6\u5b66\u4e60 (FL) \u662f\u4e00\u79cd\u9887\u5177\u5438\u5f15\u529b\u7684\u8303\u5f0f\uff0c\u5b83\u5141\u8bb8\u591a\u53f0\u673a\u5668\uff08\u4e5f\u79f0\u4e3a\u5ba2\u6237\u7aef\uff09\u5728\u4fdd\u6301\u6570\u636e\u672c\u5730\u5316\u7684\u540c\u65f6\u8fdb\u884c\u96c6\u4f53\u5b66\u4e60\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u7684\u5f02\u6784\u6027\uff0c\u4f7f\u7528\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u83b7\u5f97\u7684\u6a21\u578b\u5728\u67d0\u4e9b\u5ba2\u6237\u7aef\u7684\u6570\u636e\u4e0a\u53ef\u80fd\u8868\u73b0\u4e0d\u4f73\u3002\u4e2a\u6027\u5316\u901a\u8fc7\u4f7f\u6bcf\u4e2a\u5ba2\u6237\u7aef\u80fd\u591f\u62e5\u6709\u9488\u5bf9\u81ea\u8eab\u6570\u636e\u5b9a\u5236\u7684\u4e0d\u540c\u6a21\u578b\uff0c\u540c\u65f6\u53d7\u76ca\u4e8e\u5176\u4ed6\u5ba2\u6237\u7aef\u7684\u6570\u636e\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u6211\u4eec\u8003\u8651\u4e86\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\uff0c\u5176\u4e2d\u67d0\u4e9b\u5ba2\u6237\u7aef\u53ef\u80fd\u662f\u5bf9\u6297\u6027\u7684\uff0c\u5e76\u4e14\u6211\u4eec\u63a8\u5bfc\u51fa\u5b8c\u5168\u534f\u4f5c\u5931\u8d25\u7684\u6761\u4ef6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5206\u6790\u4e86\u5728\u5b58\u5728\u5bf9\u6297\u6027\u5ba2\u6237\u7aef\u7684\u60c5\u51b5\u4e0b\u63d2\u503c\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u5e76\u7cbe\u786e\u5730\u63cf\u8ff0\u4e86\u5b8c\u5168\u534f\u4f5c\u7684\u6027\u80fd\u4e25\u683c\u4f4e\u4e8e\u5fae\u8c03\u4e2a\u6027\u5316\u7684\u60c5\u51b5\u3002\u6211\u4eec\u7684\u5206\u6790\u6839\u636e\u6570\u636e\u5f02\u6784\u6027\u548c\u53ef\u5bb9\u5fcd\u7684\u5bf9\u6297\u6027\u5ba2\u6237\u7aef\u6bd4\u4f8b\uff0c\u786e\u5b9a\u4e86\u6211\u4eec\u5e94\u8be5\u5c06\u534f\u4f5c\u7a0b\u5ea6\u964d\u4f4e\u591a\u5c11\u3002\u6211\u4eec\u901a\u8fc7\u5bf9\u5747\u503c\u4f30\u8ba1\u548c\u4e8c\u5143\u5206\u7c7b\u95ee\u9898\u7684\u5b9e\u8bc1\u7ed3\u679c\u6765\u652f\u6301\u6211\u4eec\u7684\u53d1\u73b0\uff0c\u5e76\u8003\u8651\u4e86\u5408\u6210\u548c\u57fa\u51c6\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u3002||\n", "2409.20237": "|**2024-09-30**|[Classroom-Inspired Multi-Mentor Distillation with Adaptive Learning Strategies](http://arxiv.org/abs/2409.20237)|null|\u6211\u4eec\u63d0\u51fa\u4e86ClassroomKD\uff0c\u8fd9\u662f\u4e00\u4e2a\u53d7\u8bfe\u5802\u73af\u5883\u542f\u53d1\u7684\u65b0\u578b\u591a\u5bfc\u5e08\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u65e8\u5728\u589e\u5f3a\u5b66\u751f\u548c\u591a\u4e2a\u5bfc\u5e08\u4e4b\u95f4\u7684\u77e5\u8bc6\u8f6c\u79fb\u3002\u4e0e\u4f9d\u8d56\u56fa\u5b9a\u5bfc\u5e08-\u5b66\u751f\u5173\u7cfb\u7684\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u6846\u67b6\u6839\u636e\u6bcf\u4e2a\u6570\u636e\u6837\u672c\u7684\u6709\u6548\u6027\u52a8\u6001\u9009\u62e9\u548c\u8c03\u6574\u4e0d\u540c\u5bfc\u5e08\u7684\u6559\u5b66\u7b56\u7565\u3002ClassroomKD \u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u6a21\u5757\uff1a\u77e5\u8bc6\u8fc7\u6ee4 (KF) \u6a21\u5757\u548c\u6307\u5bfc\u6a21\u5757\u3002KF \u6a21\u5757\u6839\u636e\u6bcf\u4e2a\u8f93\u5165\u7684\u8868\u73b0\u5bf9\u5bfc\u5e08\u8fdb\u884c\u52a8\u6001\u6392\u540d\uff0c\u4ec5\u6fc0\u6d3b\u9ad8\u8d28\u91cf\u7684\u5bfc\u5e08\uff0c\u4ee5\u6700\u5927\u7a0b\u5ea6\u5730\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef\u5e76\u9632\u6b62\u4fe1\u606f\u4e22\u5931\u3002\u6307\u5bfc\u6a21\u5757\u901a\u8fc7\u6839\u636e\u5b66\u751f\u548c\u5bfc\u5e08\u4e4b\u95f4\u7684\u8868\u73b0\u5dee\u8ddd\u8c03\u6574\u6bcf\u4e2a\u5bfc\u5e08\u7684\u5f71\u54cd\u529b\u6765\u8c03\u6574\u84b8\u998f\u7b56\u7565\uff0c\u4ece\u800c\u6709\u6548\u5730\u8c03\u8282\u5b66\u4e60\u8fdb\u5ea6\u3002\u5728\u56fe\u50cf\u5206\u7c7b\uff08CIFAR-100 \u548c ImageNet\uff09\u548c\u4e8c\u7ef4\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff08COCO Keypoints \u548c MPII Human Pose\uff09\u65b9\u9762\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cClassroomKD \u660e\u663e\u4f18\u4e8e\u73b0\u6709\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5bfc\u5e08\u9009\u62e9\u548c\u6307\u5bfc\u7684\u52a8\u6001\u548c\u81ea\u9002\u5e94\u65b9\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u8f6c\u79fb\uff0c\u4ece\u800c\u901a\u8fc7\u84b8\u998f\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002||\n", "2409.20122": "|**2024-09-30**|[Training a Computer Vision Model for Commercial Bakeries with Primarily Synthetic Images](http://arxiv.org/abs/2409.20122)|null|\u5728\u98df\u54c1\u5de5\u4e1a\u4e2d\uff0c\u91cd\u65b0\u52a0\u5de5\u9000\u56de\u7684\u4ea7\u54c1\u662f\u63d0\u9ad8\u8d44\u6e90\u6548\u7387\u7684\u91cd\u8981\u6b65\u9aa4\u3002[SBB23] \u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u7a0b\u5e8f\uff0c\u53ef\u4ee5\u81ea\u52a8\u8ddf\u8e2a\u9000\u56de\u7684\u5706\u9762\u5305\u3002\u6211\u4eec\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a\u5305\u542b 2432 \u5f20\u56fe\u50cf\u548c\u66f4\u5e7f\u6cdb\u70d8\u7119\u98df\u54c1\u7684\u6269\u5c55\u6570\u636e\u96c6\u6765\u6269\u5c55\u4ed6\u4eec\u7684\u5de5\u4f5c\u3002\u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u6211\u4eec\u4f7f\u7528\u751f\u6210\u6a21\u578b pix2pix \u548c CycleGAN \u6765\u521b\u5efa\u5408\u6210\u56fe\u50cf\u3002\u6211\u4eec\u5728\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8bad\u7ec3\u4e86\u6700\u5148\u8fdb\u7684\u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b YOLOv9 \u548c YOLOv8\u3002\u6211\u4eec\u603b\u4f53\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u5728\u6211\u4eec\u7684\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e86 90.3% \u7684\u5e73\u5747\u7cbe\u5ea6 AP@0.5\u3002||\n", "2409.19983": "|**2024-09-30**|[TSdetector: Temporal-Spatial Self-correction Collaborative Learning for Colonoscopy Video Detection](http://arxiv.org/abs/2409.19983)|null|\u57fa\u4e8eCNN\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u6027\u80fd\u548c\u901f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u5e76\u9010\u6e10\u5e94\u7528\u4e8e\u606f\u8089\u68c0\u6d4b\u4efb\u52a1\u3002\u7136\u800c\uff0c\u7531\u4e8e\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u5e27\u5185\u5e8f\u5217\u5206\u5e03\u5f02\u8d28\u6027\u548c\u7cbe\u5ea6-\u7f6e\u4fe1\u5ea6\u5dee\u5f02\uff0c\u56e0\u6b64\u5728\u590d\u6742\u7684\u7ed3\u80a0\u955c\u89c6\u9891\u573a\u666f\u4e2d\u51c6\u786e\u5b9a\u4f4d\u606f\u8089\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65f6\u7a7a\u81ea\u6821\u6b63\u68c0\u6d4b\u5668\uff08TSdetector\uff09\uff0c\u5b83\u9996\u5148\u6574\u5408\u4e86\u65f6\u95f4\u5c42\u9762\u7684 consistency learning \u548c\u7a7a\u95f4\u5c42\u9762\u7684 reliability learning \u6765\u6301\u7eed\u68c0\u6d4b\u76ee\u6807\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u5c40\u65f6\u95f4\u611f\u77e5\u5377\u79ef\uff0c\u5b83\u6c47\u96c6\u4e86\u5148\u524d\u7684\u4fe1\u606f\uff0c\u4ee5\u52a8\u6001\u5f15\u5bfc\u5f53\u524d\u7684\u5377\u79ef\u6838\u5173\u6ce8\u5e8f\u5217\u4e4b\u95f4\u7684\u5168\u5c40\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5c42\u6b21\u961f\u5217\u96c6\u6210\u673a\u5236\uff0c\u901a\u8fc7\u6e10\u8fdb\u7d2f\u79ef\u7684\u65b9\u5f0f\u7ec4\u5408\u591a\u65f6\u95f4\u7279\u5f81\uff0c\u5145\u5206\u5229\u7528\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u7559\u957f\u5e8f\u5217\u4f9d\u8d56\u7279\u5f81\u3002\u540c\u65f6\uff0c\u5728\u7a7a\u95f4\u5c42\u9762\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4d\u7f6e\u611f\u77e5\u805a\u7c7b\uff0c\u4ee5\u63a2\u7d22\u5019\u9009\u6846\u4e4b\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\uff0c\u4ece\u800c\u81ea\u9002\u5e94\u5730\u91cd\u65b0\u6821\u51c6\u9884\u6d4b\u7f6e\u4fe1\u5ea6\uff0c\u4ece\u800c\u6709\u6548\u5730\u6d88\u9664\u5197\u4f59\u8fb9\u754c\u6846\u3002\u5728\u4e09\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u606f\u8089\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTSdetector \u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u606f\u8089\u68c0\u6d4b\u7387\uff0c\u5e76\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/soleilssss/TSdetector \u83b7\u53d6\u3002||\n", "2409.19972": "|**2024-09-30**|[DAOcc: 3D Object Detection Assisted Multi-Sensor Fusion for 3D Occupancy Prediction](http://arxiv.org/abs/2409.19972)|**[link](https://github.com/alphaplustt/daocc)**|\u591a\u4f20\u611f\u5668\u878d\u5408\u663e\u8457\u63d0\u9ad8\u4e86\u4e09\u7ef4\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u8fd9\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u6280\u672f\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5927\u56fe\u50cf\u5206\u8fa8\u7387\u548c\u590d\u6742\u7f51\u7edc\u6765\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\uff0c\u8fd9\u963b\u788d\u4e86\u5b83\u4eec\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u6b64\u5916\uff0c\u5927\u591a\u6570\u591a\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u6539\u8fdb\u878d\u5408\u7279\u5f81\uff0c\u800c\u5ffd\u7565\u4e86\u5bf9\u8fd9\u4e9b\u7279\u5f81\u7684\u76d1\u7763\u7b56\u7565\u7684\u63a2\u7d22\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DAOcc\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u591a\u4f20\u611f\u5668\u878d\u5408\u5360\u7528\u7f51\u7edc\uff0c\u5b83\u5229\u7528 3D \u76ee\u6807\u68c0\u6d4b\u76d1\u7763\u6765\u5e2e\u52a9\u5b9e\u73b0\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4f7f\u7528\u90e8\u7f72\u53cb\u597d\u7684\u56fe\u50cf\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u548c\u5b9e\u7528\u7684\u8f93\u5165\u56fe\u50cf\u5206\u8fa8\u7387\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86 BEV \u89c6\u57df\u6269\u5c55\u7b56\u7565\u6765\u51cf\u8f7b\u964d\u4f4e\u56fe\u50cf\u5206\u8fa8\u7387\u5e26\u6765\u7684\u4e0d\u5229\u5f71\u54cd\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4f7f\u7528 ResNet50 \u548c 256x704 \u8f93\u5165\u56fe\u50cf\u5206\u8fa8\u7387\u7684 Occ3D-nuScenes \u548c SurroundOcc \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u4ee3\u7801\u5c06\u5728 https://github.com/AlphaPlusTT/DAOcc \u4e0a\u63d0\u4f9b\u3002||\n", "2409.19850": "|**2024-09-30**|[SATA: Spatial Autocorrelation Token Analysis for Enhancing the Robustness of Vision Transformers](http://arxiv.org/abs/2409.19850)|null|\u5728\u8fc7\u53bb\u7684\u51e0\u5e74\u91cc\uff0c\u89c6\u89c9Transformer\uff08ViT\uff09\u5728\u5404\u79cd\u89c6\u89c9\u8bc6\u522b\u4efb\u52a1\u4e2d\u4e00\u76f4\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u589e\u5f3a\u5176\u9c81\u68d2\u6027\u7684\u5c1d\u8bd5\u6536\u6548\u751a\u5fae\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u4e0d\u540c\u7684\u8bad\u7ec3\u7b56\u7565\u3001\u8f93\u5165patch\u589e\u5f3a\u6216\u7f51\u7edc\u7ed3\u6784\u589e\u5f3a\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u6d89\u53ca\u5927\u91cf\u7684\u8bad\u7ec3\u548c\u5fae\u8c03\uff0c\u65e2\u8017\u65f6\u53c8\u8017\u8d39\u8d44\u6e90\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u969c\u788d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a\u7a7a\u95f4\u81ea\u76f8\u5173Token\u5206\u6790\uff08SATA\uff09\u7684\u65b0\u65b9\u6cd5\u3002\u901a\u8fc7\u5229\u7528Token\u7279\u5f81\u4e4b\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\uff0cSATA\u589e\u5f3a\u4e86ViT\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002\u8fd9\u662f\u901a\u8fc7\u5728\u8f93\u5165\u5230\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u524d\u9988\u7f51\u7edc\uff08FFN\uff09\u5757\u4e4b\u524d\uff0c\u6839\u636e\u7a7a\u95f4\u81ea\u76f8\u5173\u5206\u6570\u5bf9Token\u8fdb\u884c\u5206\u6790\u548c\u5206\u7ec4\u6765\u5b9e\u73b0\u7684\u3002\u91cd\u8981\u7684\u662f\uff0cSATA\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7684\u9884\u8bad\u7ec3ViT\u57fa\u7ebf\u4e2d\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u989d\u5916\u7684\u5fae\u8c03\uff0c\u540c\u65f6\u901a\u8fc7\u51cf\u5c11FFN\u5355\u5143\u7684\u8ba1\u7b97\u8d1f\u8f7d\u6765\u63d0\u9ad8\u6548\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ecf\u8fc7SATA\u589e\u5f3a\u7684\u57fa\u7ebfViT\u4e0d\u4ec5\u5728ImageNet-1K\u56fe\u50cf\u5206\u7c7b\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684top-1\u51c6\u786e\u7387\uff0894.9%\uff09\uff0c\u800c\u4e14\u5728\u591a\u4e2a\u9c81\u68d2\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5305\u62ecImageNet-A\uff08top-1=63.6%\uff09\u3001ImageNet-R\uff08top-1=79.2%\uff09\u548cImageNet-C\uff08mCE=13.6%\uff09\uff0c\u6240\u6709\u8fd9\u4e9b\u90fd\u4e0d\u9700\u8981\u5bf9\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u989d\u5916\u7684\u8bad\u7ec3\u6216\u5fae\u8c03\u3002||\n", "2409.19833": "|**2024-09-30**|[HazyDet: Open-source Benchmark for Drone-view Object Detection with Depth-cues in Hazy Scenes](http://arxiv.org/abs/2409.19833)|**[link](https://github.com/grokcv/hazydet)**|\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\u5bf9\u4e8e\u589e\u5f3a\u65e0\u4eba\u673a\u7684\u73af\u5883\u611f\u77e5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u76f8\u5173\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd9\u65b9\u9762\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u5f97\u5230\u63a2\u7d22\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86 HazyDet\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u4e3a\u65e0\u4eba\u673a\u5728\u96fe\u973e\u573a\u666f\u4e2d\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u800c\u8bbe\u8ba1\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002\u5b83\u5305\u542b 383,000 \u4e2a\u771f\u5b9e\u4e16\u754c\u5b9e\u4f8b\uff0c\u8fd9\u4e9b\u5b9e\u4f8b\u662f\u4ece\u81ea\u7136\u96fe\u973e\u73af\u5883\u548c\u5177\u6709\u5408\u6210\u53e0\u52a0\u96fe\u973e\u6548\u679c\u7684\u6b63\u5e38\u573a\u666f\u4e2d\u6536\u96c6\u7684\uff0c\u4ee5\u6a21\u62df\u6076\u52a3\u7684\u5929\u6c14\u6761\u4ef6\u3002\u901a\u8fc7\u89c2\u5bdf\u4e0d\u540c\u6df1\u5ea6\u548c\u96fe\u973e\u6761\u4ef6\u4e0b\u76ee\u6807\u5c3a\u5ea6\u548c\u6e05\u6670\u5ea6\u7684\u663e\u8457\u53d8\u5316\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6df1\u5ea6\u6761\u4ef6\u68c0\u6d4b\u5668 (DeCoDet)\uff0c\u4ee5\u7ed3\u5408\u8fd9\u79cd\u5148\u9a8c\u77e5\u8bc6\u3002DeCoDet \u5177\u6709\u591a\u5c3a\u5ea6\u6df1\u5ea6\u611f\u77e5\u68c0\u6d4b\u5934\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u6df1\u5ea6\u611f\u77e5\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u6df1\u5ea6\u6761\u4ef6\u6838\u6a21\u5757\u5229\u7528\u7531\u6b64\u4ea7\u751f\u7684\u6df1\u5ea6\u7ebf\u7d22\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5c3a\u5ea6\u4e0d\u53d8\u7684\u7ec6\u5316\u635f\u5931\uff0c\u4ee5\u4fc3\u8fdb\u4ece\u4f2a\u6807\u7b7e\u4e2d\u5b66\u4e60\u9c81\u68d2\u7684\u6df1\u5ea6\u7ebf\u7d22\u3002\u5728 HazyDet \u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u8bc4\u4f30\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u7075\u6d3b\u6027\u548c\u6709\u6548\u6027\uff0c\u4ea7\u751f\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u548c\u5de5\u5177\u5305\u53ef\u5728 https://github.com/GrokCV/HazyDet \u83b7\u53d6\u3002||\n", "2409.19703": "|**2024-09-29**|[Applying the Lower-Biased Teacher Model in Semi-Suepervised Object Detection](http://arxiv.org/abs/2409.19703)|null|\u6211\u63d0\u51fa\u4e86\u4f4e\u504f\u5dee\u6559\u5e08\u6a21\u578b\uff0c\u8fd9\u662f\u5bf9\u65e0\u504f\u5dee\u6559\u5e08\u6a21\u578b\u7684\u589e\u5f3a\uff0c\u4e13\u95e8\u9488\u5bf9\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u8fdb\u884c\u4e86\u5b9a\u5236\u3002\u8be5\u6a21\u578b\u7684\u4e3b\u8981\u521b\u65b0\u5728\u4e8e\u5c06\u5b9a\u4f4d\u635f\u5931\u96c6\u6210\u5230\u6559\u5e08\u6a21\u578b\u4e2d\uff0c\u4ece\u800c\u663e\u7740\u63d0\u9ad8\u4e86\u4f2a\u6807\u7b7e\u751f\u6210\u7684\u51c6\u786e\u6027\u3002\u901a\u8fc7\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u8fb9\u754c\u6846\u7cbe\u5ea6\u7b49\u5173\u952e\u95ee\u9898\uff0c\u4f4e\u504f\u5dee\u6559\u5e08\u6a21\u578b\u5728\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u3002\u5728\u591a\u4e2a\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u4f4e\u504f\u5dee\u6559\u5e08\u6a21\u578b\u4e0d\u4ec5\u51cf\u5c11\u4e86\u7531\u7c7b\u522b\u4e0d\u5e73\u8861\u5f15\u8d77\u7684\u4f2a\u6807\u7b7e\u504f\u5dee\uff0c\u800c\u4e14\u8fd8\u51cf\u5c11\u4e86\u7531\u9519\u8bef\u8fb9\u754c\u6846\u5f15\u8d77\u7684\u9519\u8bef\u3002\u56e0\u6b64\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684mAP\u5206\u6570\u548c\u66f4\u53ef\u9760\u7684\u68c0\u6d4b\u7ed3\u679c\u3002\u8fd9\u9879\u7814\u7a76\u5f3a\u8c03\u4e86\u51c6\u786e\u7684\u4f2a\u6807\u7b7e\u751f\u6210\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u534a\u76d1\u7763\u5b66\u4e60\u5728\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u8fdb\u6b65\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\u3002||\n", "2410.02646": "|**2024-10-04**|[Learning 3D Perception from Others' Predictions](http://arxiv.org/abs/2410.02646)|null|\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u8fdb\u884c\u7cbe\u786e\u7684\u4e09\u7ef4\u76ee\u6807\u68c0\u6d4b\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u7684\u6807\u6ce8\u6570\u636e\u3002\u83b7\u53d6\u6b64\u7c7b\u6570\u636e\u7684\u8fc7\u7a0b\u65e2\u4e4f\u5473\u53c8\u6602\u8d35\uff0c\u5e76\u4e14\u5728\u91c7\u7528\u65b0\u4f20\u611f\u5668\u6216\u5c06\u68c0\u6d4b\u5668\u90e8\u7f72\u5230\u65b0\u73af\u5883\u4e2d\u65f6\uff0c\u901a\u5e38\u9700\u8981\u91cd\u590d\u5de5\u4f5c\u3002\u6211\u4eec\u7814\u7a76\u4e86\u4e00\u79cd\u6784\u5efa\u4e09\u7ef4\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u65b0\u65b9\u6848\uff1a\u4ece\u914d\u5907\u7cbe\u786e\u68c0\u6d4b\u5668\u7684\u9644\u8fd1\u5355\u5143\u7684\u9884\u6d4b\u4e2d\u5b66\u4e60\u3002\u4f8b\u5982\uff0c\u5f53\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u8fdb\u5165\u4e00\u4e2a\u65b0\u533a\u57df\u65f6\uff0c\u5b83\u53ef\u4ee5\u4ece\u5176\u4ed6\u4ea4\u901a\u53c2\u4e0e\u8005\u90a3\u91cc\u5b66\u4e60\uff0c\u8fd9\u4e9b\u4ea4\u901a\u53c2\u4e0e\u8005\u7684\u68c0\u6d4b\u5668\u5df2\u7ecf\u9488\u5bf9\u8be5\u533a\u57df\u8fdb\u884c\u4e86\u4f18\u5316\u3002\u8fd9\u79cd\u8bbe\u7f6e\u5177\u6709\u6807\u7b7e\u6548\u7387\u9ad8\u3001\u4f20\u611f\u5668\u65e0\u5173\u6027\u548c\u901a\u4fe1\u6548\u7387\u9ad8\u7684\u7279\u70b9\uff1a\u9644\u8fd1\u7684\u5355\u5143\u53ea\u9700\u8981\u4e0e\u81ea\u6211\u4ee3\u7406\uff08\u4f8b\u5982\uff0c\u6c7d\u8f66\uff09\u5171\u4eab\u9884\u6d4b\u7ed3\u679c\u3002\u7136\u800c\uff0c\u7b80\u5355\u5730\u5c06\u63a5\u6536\u5230\u7684\u9884\u6d4b\u4f5c\u4e3a\u771f\u5b9e\u503c\u6765\u8bad\u7ec3\u81ea\u6211\u8f66\u8f86\u7684\u68c0\u6d4b\u5668\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u6211\u4eec\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u5c06\u89c6\u70b9\u4e0d\u5339\u914d\u548c\u5b9a\u4f4d\u9519\u8bef\uff08\u7531\u4e8e\u540c\u6b65\u548c GPS \u9519\u8bef\uff09\u786e\u5b9a\u4e3a\u4e3b\u8981\u539f\u56e0\uff0c\u8fd9\u4e9b\u539f\u56e0\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u5bfc\u81f4\u8bef\u62a5\u3001\u6f0f\u62a5\u548c\u4e0d\u51c6\u786e\u7684\u4f2a\u6807\u7b7e\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ddd\u79bb\u7684\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u9996\u5148\u4ece\u89c6\u70b9\u76f8\u4f3c\u7684\u8f83\u8fd1\u5355\u5143\u5b66\u4e60\uff0c\u7136\u540e\u901a\u8fc7\u81ea\u6211\u8bad\u7ec3\u9010\u6b65\u63d0\u9ad8\u5176\u4ed6\u5355\u5143\u9884\u6d4b\u7684\u8d28\u91cf\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u8bc1\u660e\uff0c\u53ef\u4ee5\u4f7f\u7528\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u6709\u6548\u7684\u4f2a\u6807\u7b7e\u7ec6\u5316\u6a21\u5757\uff0c\u4ece\u800c\u5927\u5927\u51cf\u5c11\u8bad\u7ec3\u76ee\u6807\u68c0\u6d4b\u5668\u6240\u9700\u7684\u6570\u636e\u91cf\u3002\u6211\u4eec\u5728\u6700\u8fd1\u53d1\u5e03\u7684\u771f\u5b9e\u4e16\u754c\u534f\u540c\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u53c2\u8003\u8f66\u8f86\u7684\u9884\u6d4b\u4f5c\u4e3a\u81ea\u6211\u8f66\u8f86\u7684\u4f2a\u6807\u7b7e\u3002\u5305\u62ec\u591a\u79cd\u573a\u666f\uff08\u4f8b\u5982\uff0c\u4e0d\u540c\u7684\u4f20\u611f\u5668\u3001\u68c0\u6d4b\u5668\u548c\u57df\uff09\u5728\u5185\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u4ece\u5176\u4ed6\u5355\u5143\u7684\u9884\u6d4b\u4e2d\u8fdb\u884c\u6807\u7b7e\u9ad8\u6548\u7684\u4e09\u7ef4\u611f\u77e5\u5b66\u4e60\u3002||\n", "2410.02615": "|**2024-10-03**|[LoGra-Med: Long Context Multi-Graph Alignment for Medical Vision-Language Model](http://arxiv.org/abs/2410.02615)|null|\u5f53\u524d\u6700\u5148\u8fdb\u7684\u533b\u5b66\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08med-MLLM\uff09\uff0c\u5982 LLaVA-Med \u6216 BioMedGPT\uff0c\u5728\u9884\u8bad\u7ec3\u4e2d\u5229\u7528\u4e86\u6307\u4ee4\u9075\u5faa\u6570\u636e\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e3b\u8981\u4fa7\u91cd\u4e8e\u6269\u5927\u6a21\u578b\u89c4\u6a21\u548c\u6570\u636e\u91cf\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u800c\u4e3b\u8981\u4f9d\u8d56\u4e8e\u81ea\u56de\u5f52\u5b66\u4e60\u76ee\u6807\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u6211\u4eec\u53d1\u73b0\u8fd9\u79cd\u5b66\u4e60\u65b9\u6848\u53ef\u80fd\u5bfc\u81f4\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\u4e4b\u95f4\u7684\u5bf9\u9f50\u8f83\u5f31\uff0c\u4f7f\u5f97\u8fd9\u4e9b\u6a21\u578b\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5927\u91cf\u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u2014\u2014\u8fd9\u5728\u533b\u5b66\u9886\u57df\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u9ad8\u8d28\u91cf\u6307\u4ee4\u9075\u5faa\u5b9e\u4f8b\u7684\u6574\u7406\u65e2\u6602\u8d35\u53c8\u8017\u65f6\u3002\u6211\u4eec\u4f7f\u7528 LoGra-Med \u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u591a\u56fe\u5bf9\u9f50\u7b97\u6cd5\uff0c\u53ef\u5728\u56fe\u50cf\u6a21\u6001\u3001\u57fa\u4e8e\u5bf9\u8bdd\u7684\u63cf\u8ff0\u548c\u6269\u5c55\u5b57\u5e55\u4e4b\u95f4\u5f3a\u5236\u6267\u884c\u4e09\u5143\u7ec4\u5173\u8054\u3002\u8fd9\u6709\u52a9\u4e8e\u6a21\u578b\u6355\u6349\u4e0a\u4e0b\u6587\u542b\u4e49\u3001\u5904\u7406\u8bed\u8a00\u53d8\u5f02\u6027\u4ee5\u53ca\u5728\u89c6\u89c9\u548c\u6587\u672c\u4e4b\u95f4\u5efa\u7acb\u8de8\u6a21\u6001\u5173\u8054\u3002\u4e3a\u4e86\u6269\u5c55\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4f7f\u7528\u9ed1\u76d2\u68af\u5ea6\u4f30\u8ba1\u7684\u9ad8\u6548\u7aef\u5230\u7aef\u5b66\u4e60\u65b9\u6848\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u5feb\u7684 LLaMa 7B \u8bad\u7ec3\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cLoGra-Med \u5728 60 \u4e07\u4e2a\u56fe\u50cf-\u6587\u672c\u5bf9\u7684\u533b\u5b66 VQA \u4e0a\u4e0e LLAVA-Med \u7684\u6027\u80fd\u76f8\u5339\u914d\uff0c\u5e76\u4e14\u5728\u63a5\u53d7 10% \u6570\u636e\u8bad\u7ec3\u65f6\u660e\u663e\u4f18\u4e8e\u5b83\u3002\u4f8b\u5982\uff0c\u5728 VQA-RAD \u4e0a\uff0c\u6211\u4eec\u6bd4 LLAVA-Med \u9ad8\u51fa 20.13%\uff0c\u5e76\u4e14\u51e0\u4e4e\u8fbe\u5230\u4e86 100% \u9884\u8bad\u7ec3\u5206\u6570\uff0872.52% \u5bf9\u6bd4 72.64%\uff09\u3002\u6211\u4eec\u8fd8\u5728\u89c6\u89c9\u804a\u5929\u673a\u5668\u4eba\u4e0a\u8d85\u8d8a\u4e86\u50cf BiomedGPT \u8fd9\u6837\u7684 SOTA \u65b9\u6cd5\uff0c\u5e76\u5728\u4f7f\u7528 VQA \u8fdb\u884c\u96f6\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u65b9\u9762\u8d85\u8d8a\u4e86 RadFM\uff0c\u7a81\u51fa\u4e86\u591a\u56fe\u5bf9\u9f50\u7684\u6709\u6548\u6027\u3002||\n", "2410.02547": "|**2024-10-03**|[Personalized Quantum Federated Learning for Privacy Image Classification](http://arxiv.org/abs/2410.02547)|null|\u91cf\u5b50\u8054\u90a6\u5b66\u4e60\u63d0\u9ad8\u4e86\u9690\u79c1\u56fe\u50cf\u5206\u7c7b\u7684\u6548\u679c\uff0c\u4f46\u5ba2\u6237\u7aef\u6a21\u578b\u7f3a\u4e4f\u4e2a\u6027\u5316\u53ef\u80fd\u5bfc\u81f4\u91cf\u5b50\u8054\u90a6\u5b66\u4e60\u7684\u6b21\u4f18\u6027\u3002\u4e3a\u4e86\u589e\u5f3a\u56fe\u50cf\u5206\u5e03\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u5ba2\u6237\u7aef\u6a21\u578b\u7684\u4e2a\u6027\u5316\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9690\u79c1\u56fe\u50cf\u5206\u7c7b\u7684\u4e2a\u6027\u5316\u91cf\u5b50\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u3002\u9996\u5148\uff0c\u6784\u5efa\u4e86\u4e2a\u6027\u5316\u91cf\u5b50\u8054\u90a6\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u5ba2\u6237\u7aef\u6a21\u578b\u4e2d\u8bbe\u7f6e\u4e86\u4e2a\u6027\u5316\u5c42\u4ee5\u7ef4\u62a4\u4e2a\u6027\u5316\u53c2\u6570\u3002\u5176\u6b21\uff0c\u5f15\u5165\u4e86\u4e2a\u6027\u5316\u91cf\u5b50\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u4ee5\u786e\u4fdd\u5ba2\u6237\u7aef\u548c\u670d\u52a1\u5668\u4e4b\u95f4\u4ea4\u6362\u7684\u4fe1\u606f\u5b89\u5168\u3002\u7b2c\u4e09\uff0c\u5c06\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u5e94\u7528\u4e8e FashionMNIST \u6570\u636e\u96c6\u4e0a\u7684\u56fe\u50cf\u5206\u7c7b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u672c\u5730\u8bad\u7ec3\u6837\u672c\u4e0d\u5e73\u8861\u7684\u60c5\u51b5\u4e0b\uff0c\u4e2a\u6027\u5316\u91cf\u5b50\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u4e5f\u80fd\u83b7\u5f97\u6027\u80fd\u4f18\u5f02\u7684\u5168\u5c40\u548c\u5c40\u90e8\u6a21\u578b\u3002\u57288\u4e2a\u5ba2\u6237\u7aef\u548c\u5206\u5e03\u53c2\u6570\u4e3a100\u7684\u60c5\u51b5\u4e0b\uff0c\u670d\u52a1\u5668\u7684\u51c6\u786e\u7387\u8fbe\u5230\u4e86100%\uff0c\u6bd4\u975e\u4e2a\u6027\u5316\u6a21\u578b\u63d0\u9ad8\u4e867%\u3002\u57282\u4e2a\u5ba2\u6237\u7aef\u548c\u5206\u5e03\u53c2\u6570\u4e3a1\u7684\u60c5\u51b5\u4e0b\uff0c\u5ba2\u6237\u7aef\u7684\u5e73\u5747\u51c6\u786e\u7387\u6bd4\u975e\u4e2a\u6027\u5316\u6a21\u578b\u63d0\u9ad8\u4e862.9%\u3002\u4e0e\u4e4b\u524d\u7684\u91cf\u5b50\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u4e2a\u6027\u5316\u91cf\u5b50\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u5728\u4fdd\u62a4\u6a21\u578b\u548c\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\uff0c\u65e0\u9700\u989d\u5916\u7684\u672c\u5730\u8bad\u7ec3\u3002\u8fd9\u53ef\u80fd\u4fc3\u8fdb\u91cf\u5b50\u6280\u672f\u7684\u66f4\u5e7f\u6cdb\u91c7\u7528\u548c\u5e94\u7528\uff0c\u5e76\u4e3a\u66f4\u5b89\u5168\u3001\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u91cf\u5b50\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u94fa\u5e73\u9053\u8def\u3002||\n", "2410.02492": "|**2024-10-03**|[DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking Based on LLM](http://arxiv.org/abs/2410.02492)|null|\u89c6\u89c9\u8bed\u8a00\u8ddf\u8e2a (VLT) \u5df2\u6210\u4e3a\u4e00\u4e2a\u524d\u6cbf\u7814\u7a76\u9886\u57df\uff0c\u5b83\u5229\u7528\u8bed\u8a00\u6570\u636e\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u8f93\u5165\u7b97\u6cd5\uff0c\u5e76\u5c06\u4f20\u7edf\u5355\u76ee\u6807\u8ddf\u8e2a (SOT) \u7684\u8303\u56f4\u6269\u5c55\u5230\u89c6\u9891\u7406\u89e3\u5e94\u7528\u3002 \u5c3d\u7ba1\u5982\u6b64\uff0c\u5927\u591a\u6570 VLT \u57fa\u51c6\u6d4b\u8bd5\u4ecd\u7136\u4f9d\u8d56\u4e8e\u4eba\u5de5\u6807\u6ce8\u7684\u7b80\u6d01\u6587\u672c\u63cf\u8ff0\u6765\u63cf\u8ff0\u6bcf\u4e2a\u89c6\u9891\u3002 \u8fd9\u4e9b\u63cf\u8ff0\u901a\u5e38\u65e0\u6cd5\u6355\u6349\u89c6\u9891\u5185\u5bb9\u52a8\u6001\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u5e76\u4e14\u7f3a\u4e4f\u8bed\u8a00\u98ce\u683c\u53d8\u5316\uff0c\u53d7\u9650\u4e8e\u5176\u7edf\u4e00\u7684\u7ec6\u8282\u6c34\u5e73\u548c\u56fa\u5b9a\u7684\u6807\u6ce8\u9891\u7387\u3002 \u56e0\u6b64\uff0c\u7b97\u6cd5\u503e\u5411\u4e8e\u9ed8\u8ba4\u91c7\u7528\u201c\u8bb0\u4f4f\u7b54\u6848\u201d\u7684\u7b56\u7565\uff0c\u504f\u79bb\u4e86\u6df1\u5165\u7406\u89e3\u89c6\u9891\u5185\u5bb9\u7684\u6838\u5fc3\u76ee\u6807\u3002 \u5e78\u8fd0\u7684\u662f\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u51fa\u73b0\u4f7f\u751f\u6210\u591a\u6837\u5316\u6587\u672c\u6210\u4e3a\u53ef\u80fd\u3002 \u8fd9\u9879\u5de5\u4f5c\u5229\u7528 LLM \u4e3a\u5177\u6709\u4ee3\u8868\u6027\u7684 SOT \u57fa\u51c6\u751f\u6210\u4e0d\u540c\u7684\u8bed\u4e49\u6ce8\u91ca\uff08\u5728\u6587\u672c\u957f\u5ea6\u548c\u7c92\u5ea6\u65b9\u9762\uff09\uff0c\u4ece\u800c\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u57fa\u51c6\u3002 \u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec (1) \u57fa\u4e8e\u4e94\u4e2a\u8457\u540d\u7684 VLT \u548c SOT \u57fa\u51c6\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u5177\u6709\u4e0d\u540c\u6587\u672c\u7684\u89c6\u89c9\u8bed\u8a00\u8ddf\u8e2a\u57fa\u51c6\uff0c\u540d\u4e3a DTVLT\uff0c\u5305\u62ec\u4e09\u4e2a\u5b50\u4efb\u52a1\uff1a\u77ed\u671f\u8ddf\u8e2a\u3001\u957f\u671f\u8ddf\u8e2a\u548c\u5168\u5c40\u5b9e\u4f8b\u8ddf\u8e2a\u3002 (2) \u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u56db\u79cd\u7c92\u5ea6\u7684\u6587\u672c\uff0c\u8003\u8651\u4e86\u8bed\u4e49\u4fe1\u606f\u7684\u8303\u56f4\u548c\u5bc6\u5ea6\u3002 \u6211\u4eec\u9884\u8ba1\u8fd9\u79cd\u591a\u7c92\u5ea6\u751f\u6210\u7b56\u7565\u5c06\u4e3a VLT \u548c\u89c6\u9891\u7406\u89e3\u7814\u7a76\u8425\u9020\u6709\u5229\u7684\u73af\u5883\u3002 (3) \u6211\u4eec\u5bf9 DTVLT \u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u9a8c\u5206\u6790\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540c\u6587\u672c\u5bf9\u8ddf\u8e2a\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5e0c\u671b\u8bc6\u522b\u51fa\u7684\u73b0\u6709\u7b97\u6cd5\u7684\u6027\u80fd\u74f6\u9888\u80fd\u591f\u652f\u6301 VLT \u548c\u89c6\u9891\u7406\u89e3\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002 \u63d0\u51fa\u7684\u57fa\u51c6\u3001\u5b9e\u9a8c\u7ed3\u679c\u548c\u5de5\u5177\u5305\u5c06\u5728 http://videocube.aitestunion.com/ \u4e0a\u9010\u6b65\u53d1\u5e03\u3002||\n", "2410.02423": "|**2024-10-03**|[PnP-Flow: Plug-and-Play Image Restoration with Flow Matching](http://arxiv.org/abs/2410.02423)|**[link](https://github.com/annegnx/PnP-Flow)**|\u672c\u6587\u4ecb\u7ecd\u4e86\u5373\u63d2\u5373\u7528\u6d41\u5339\u914d (PnP Flow Matching)\uff0c\u8fd9\u662f\u4e00\u79cd\u89e3\u51b3\u6210\u50cf\u9006\u95ee\u9898\u7684\u7b97\u6cd5\u3002PnP \u65b9\u6cd5\u5229\u7528\u9884\u8bad\u7ec3\u53bb\u566a\u5668\uff08\u901a\u5e38\u662f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff09\u7684\u4f18\u52bf\uff0c\u5c06\u5b83\u4eec\u96c6\u6210\u5230\u4f18\u5316\u65b9\u6848\u4e2d\u3002\u867d\u7136\u5b83\u4eec\u5728\u5404\u79cd\u6210\u50cf\u9006\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f46 PnP \u65b9\u6cd5\u5728\u4fee\u590d\u7b49\u66f4\u5177\u751f\u6210\u6027\u7684\u4efb\u52a1\u4e2d\u9762\u4e34\u7740\u56fa\u6709\u7684\u5c40\u9650\u6027\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u6d41\u5339\u914d\u7b49\u751f\u6210\u6a21\u578b\u7a81\u7834\u4e86\u56fe\u50cf\u91c7\u6837\u7684\u754c\u9650\uff0c\u4f46\u7f3a\u4e4f\u5728\u56fe\u50cf\u6062\u590d\u4e2d\u6709\u6548\u4f7f\u7528\u7684\u660e\u786e\u65b9\u6cd5\u3002\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u4f7f\u7528\u9884\u8bad\u7ec3\u7684 FM \u6a21\u578b\u5b9a\u4e49\u65f6\u95f4\u76f8\u5173\u7684\u53bb\u566a\u5668\uff0c\u5c06 PnP \u6846\u67b6\u4e0e\u6d41\u5339\u914d (FM) \u76f8\u7ed3\u5408\u3002\u6211\u4eec\u7684\u7b97\u6cd5\u5728\u6570\u636e\u4fdd\u771f\u5ea6\u9879\u4e0a\u7684\u68af\u5ea6\u4e0b\u964d\u6b65\u9aa4\u3001\u5bf9\u5b66\u4e60\u5230\u7684 FM \u8def\u5f84\u7684\u91cd\u65b0\u6295\u5f71\u548c\u53bb\u566a\u4e4b\u95f4\u4ea4\u66ff\u8fdb\u884c\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u5185\u5b58\u53cb\u597d\uff0c\u56e0\u4e3a\u5b83\u907f\u514d\u4e86\u901a\u8fc7 ODE \u7684\u53cd\u5411\u4f20\u64ad\u548c\u8f68\u8ff9\u8ba1\u7b97\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u5176\u5728\u53bb\u566a\u3001\u8d85\u5206\u8fa8\u7387\u3001\u53bb\u6a21\u7cca\u548c\u4fee\u590d\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u4e0e\u73b0\u6709 PnP \u7b97\u6cd5\u548c\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u4f18\u8d8a\u7684\u7ed3\u679c\u3002||\n", "2410.02249": "|**2024-10-03**|[Spiking Neural Network as Adaptive Event Stream Slicer](http://arxiv.org/abs/2410.02249)|null|\u57fa\u4e8e\u4e8b\u4ef6\u7684\u76f8\u673a\u7531\u4e8e\u5176\u4e30\u5bcc\u7684\u8fb9\u7f18\u4fe1\u606f\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u800c\u5907\u53d7\u5173\u6ce8\u3002\u8bb8\u591a\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u4e8b\u4ef6\u7684\u7b97\u6cd5\u4f9d\u8d56\u4e8e\u5c06\u4e8b\u4ef6\u5206\u5272\u6210\u56fa\u5b9a\u7684\u7ec4\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u5173\u952e\u65f6\u95f4\u4fe1\u606f\u7684\u4e22\u5931\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u4e0d\u540c\u7684\u8fd0\u52a8\u573a\u666f\uff08\u4f8b\u5982\uff0c\u9ad8\u901f/\u4f4e\u901f\uff09\u65f6\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SpikeSlicer\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u5373\u63d2\u5373\u7528\u4e8b\u4ef6\u5904\u7406\u65b9\u6cd5\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u5730\u5206\u5272\u4e8b\u4ef6\u6d41\u3002SpikeSlicer\u5229\u7528\u8f7b\u91cf\u7ea7\uff080.41M\uff09\u548c\u4f4e\u80fd\u8017\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u6765\u89e6\u53d1\u4e8b\u4ef6\u5207\u7247\u3002\u4e3a\u4e86\u5f15\u5bfcSNN\u5728\u6700\u4f73\u65f6\u95f4\u6b65\u957f\u89e6\u53d1\u8109\u51b2\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8109\u51b2\u4f4d\u7f6e\u611f\u77e5\u635f\u5931\uff08SPA-Loss\uff09\u6765\u8c03\u8282\u795e\u7ecf\u5143\u7684\u72b6\u6001\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u53cd\u9988\u66f4\u65b0\u8bad\u7ec3\u7b56\u7565\uff0c\u5229\u7528\u6765\u81ea\u4e0b\u6e38\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u7684\u53cd\u9988\u6765\u6539\u8fdb\u5207\u7247\u51b3\u7b56\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u57fa\u4e8e\u4e8b\u4ef6\u7684\u76ee\u6807\u8ddf\u8e2a\u548c\u8bc6\u522b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSpikeSlicer\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u65b0\u7684SNN-ANN\u5408\u4f5c\u8303\u5f0f\uff0c\u5176\u4e2dSNN\u5145\u5f53\u9ad8\u6548\u3001\u4f4e\u80fd\u8017\u7684\u6570\u636e\u5904\u7406\u5668\uff0c\u534f\u52a9ANN\u63d0\u9ad8\u4e0b\u6e38\u6027\u80fd\uff0c\u4e3a\u63a2\u7d22\u65b0\u7684\u89c6\u89d2\u548c\u6f5c\u5728\u9014\u5f84\u6ce8\u5165\u4e86\u6d3b\u529b\u3002||\n", "2410.02077": "|**2024-10-02**|[Kolmogorov-Arnold Network Autoencoders](http://arxiv.org/abs/2410.02077)|**[link](https://github.com/aminmoradixl/kan_ae)**|\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5df2\u7ecf\u5f7b\u5e95\u6539\u53d8\u4e86\u5404\u4e2a\u9886\u57df\uff0c\u5176\u4e2d\u591a\u5c42\u611f\u77e5\u5668 (MLP) \u662f\u6570\u636e\u56de\u5f52\u548c\u56fe\u50cf\u5206\u7c7b\u7b49\u4efb\u52a1\u7684\u57fa\u77f3\u3002\u7136\u800c\uff0c\u6700\u8fd1\u7684\u4e00\u9879\u7814\u7a76\u5f15\u5165\u4e86 Kolmogorov-Arnold \u7f51\u7edc (KAN) \u4f5c\u4e3a MLP \u7684\u6709\u524d\u9014\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5b83\u5229\u7528\u653e\u7f6e\u5728\u8fb9\u800c\u4e0d\u662f\u8282\u70b9\u4e0a\u7684\u6fc0\u6d3b\u51fd\u6570\u3002\u8fd9\u79cd\u7ed3\u6784\u8f6c\u53d8\u4f7f KAN \u4e0e Kolmogorov-Arnold \u8868\u793a\u5b9a\u7406\u7d27\u5bc6\u7ed3\u5408\uff0c\u6709\u53ef\u80fd\u63d0\u9ad8\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86 KAN \u5728\u901a\u8fc7\u81ea\u52a8\u7f16\u7801\u5668\u8fdb\u884c\u6570\u636e\u8868\u793a\u65b9\u9762\u7684\u529f\u6548\uff0c\u5c06\u5b83\u4eec\u5728 MNIST\u3001SVHN \u548c CIFAR-10 \u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u4e0e\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e KAN \u7684\u81ea\u52a8\u7f16\u7801\u5668\u5728\u91cd\u5efa\u7cbe\u5ea6\u65b9\u9762\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u4ece\u800c\u8868\u660e\u5b83\u4eec\u53ef\u4ee5\u4f5c\u4e3a\u6570\u636e\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u5de5\u5177\u3002||\n", "2410.02057": "|**2024-10-02**|[Stochastic Deep Restoration Priors for Imaging Inverse Problems](http://arxiv.org/abs/2410.02057)|null|\u4f5c\u4e3a\u56fe\u50cf\u53bb\u566a\u5668\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u88ab\u5e7f\u6cdb\u7528\u4f5c\u89e3\u51b3\u6210\u50cf\u9006\u95ee\u9898\u7684\u5148\u9a8c\u3002 \u867d\u7136\u9ad8\u65af\u53bb\u566a\u88ab\u8ba4\u4e3a\u8db3\u4ee5\u5b66\u4e60\u56fe\u50cf\u5148\u9a8c\uff0c\u4f46\u6211\u4eec\u8868\u660e\uff0c\u4ece\u9884\u5148\u8bad\u7ec3\u4e3a\u66f4\u901a\u7528\u7684\u6062\u590d\u7b97\u5b50\u7684\u6df1\u5ea6\u6a21\u578b\u4e2d\u83b7\u5f97\u7684\u5148\u9a8c\u53ef\u4ee5\u8868\u73b0\u5f97\u66f4\u597d\u3002 \u6211\u4eec\u5f15\u5165\u4e86\u968f\u673a\u6df1\u5ea6\u6062\u590d\u5148\u9a8c (ShaRP)\uff0c\u8fd9\u662f\u4e00\u79cd\u5229\u7528\u6b64\u7c7b\u6062\u590d\u6a21\u578b\u7684\u96c6\u5408\u6765\u89c4\u8303\u5316\u9006\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\u3002 ShaRP \u901a\u8fc7\u66f4\u597d\u5730\u5904\u7406\u7ed3\u6784\u5316\u4f2a\u5f71\u5e76\u5728\u5373\u4f7f\u6ca1\u6709\u5b8c\u5168\u91c7\u6837\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u8fdb\u884c\u81ea\u76d1\u7763\u8bad\u7ec3\uff0c\u6539\u8fdb\u4e86\u4f7f\u7528\u9ad8\u65af\u53bb\u566a\u5668\u5148\u9a8c\u7684\u65b9\u6cd5\u3002 \u6211\u4eec\u8bc1\u660e\u4e86 ShaRP \u6700\u5c0f\u5316\u4e86\u4e00\u4e2a\u76ee\u6807\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u6d89\u53ca\u4ece\u6700\u5c0f\u5747\u65b9\u8bef\u5dee (MMSE) \u6062\u590d\u7b97\u5b50\u7684\u5f97\u5206\u51fd\u6570\u5bfc\u51fa\u7684\u6b63\u5219\u5316\u5668\uff0c\u5e76\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4e86\u5176\u6536\u655b\u6027\u3002 \u7ecf\u9a8c\u8868\u660e\uff0cShaRP \u5728\u78c1\u5171\u632f\u6210\u50cf\u91cd\u5efa\u548c\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7b49\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8fc7\u4e86\u57fa\u4e8e\u53bb\u566a\u5668\u548c\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002||\n", "2410.01806": "|**2024-10-02**|[Samba: Synchronized Set-of-Sequences Modeling for Multiple Object Tracking](http://arxiv.org/abs/2410.01806)|null|\u5728\u590d\u6742\u573a\u666f\uff08\u4f8b\u5982\uff0c\u534f\u4f5c\u821e\u8e48\u8868\u6f14\u3001\u56e2\u961f\u8fd0\u52a8\u6216\u52a8\u6001\u52a8\u7269\u7fa4\u4f53\uff09\u4e2d\u8fdb\u884c\u591a\u76ee\u6807\u8ddf\u8e2a\u63d0\u51fa\u4e86\u72ec\u7279\u7684\u6311\u6218\u3002\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\uff0c\u76ee\u6807\u7ecf\u5e38\u4ee5\u534f\u8c03\u7684\u6a21\u5f0f\u79fb\u52a8\u3001\u76f8\u4e92\u906e\u6321\u5e76\u5728\u5176\u8f68\u8ff9\u4e2d\u8868\u73b0\u51fa\u957f\u671f\u4f9d\u8d56\u6027\u3002\u7136\u800c\uff0c\u5982\u4f55\u5bf9\u8f68\u8ff9\u5185\u7684\u957f\u671f\u4f9d\u8d56\u6027\u3001\u8f68\u8ff9\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\u4ee5\u53ca\u76f8\u5173\u7684\u65f6\u5e8f\u906e\u6321\u8fdb\u884c\u5efa\u6a21\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u7684\u5f00\u653e\u6027\u7814\u7a76\u95ee\u9898\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86 Samba\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7ebf\u6027\u65f6\u95f4\u5e8f\u5217\u96c6\u6a21\u578b\uff0c\u65e8\u5728\u901a\u8fc7\u540c\u6b65\u7528\u4e8e\u5bf9\u6bcf\u4e2a\u8f68\u8ff9\u5efa\u6a21\u7684\u591a\u4e2a\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6765\u8054\u5408\u5904\u7406\u591a\u4e2a\u8f68\u8ff9\u3002Samba \u81ea\u56de\u5f52\u5730\u9884\u6d4b\u6bcf\u4e2a\u5e8f\u5217\u7684\u672a\u6765\u8f68\u8ff9\u67e5\u8be2\uff0c\u540c\u65f6\u4fdd\u6301\u8de8\u8f68\u8ff9\u540c\u6b65\u7684\u957f\u671f\u8bb0\u5fc6\u8868\u793a\u3002\u901a\u8fc7\u5c06 Samba \u96c6\u6210\u5230\u9010\u4f20\u64ad\u8ddf\u8e2a\u6846\u67b6\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 SambaMOTR\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u6709\u6548\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u7684\u8ddf\u8e2a\u5668\uff0c\u5305\u62ec\u957f\u671f\u4f9d\u8d56\u6027\u3001\u8f68\u8ff9\u76f8\u4e92\u4f9d\u8d56\u6027\u548c\u65f6\u95f4\u906e\u6321\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5904\u7406\u4e0d\u786e\u5b9a\u89c2\u5bdf\u7ed3\u679c\u7684\u6709\u6548\u6280\u672f (MaskObs) \u548c\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u5c06 SambaMOTR \u6269\u5c55\u5230\u66f4\u957f\u7684\u5e8f\u5217\u3002\u901a\u8fc7\u5bf9\u8ddf\u8e2a\u5bf9\u8c61\u4e4b\u95f4\u7684\u957f\u671f\u4f9d\u8d56\u6027\u548c\u4ea4\u4e92\u8fdb\u884c\u5efa\u6a21\uff0cSambaMOTR \u9690\u5f0f\u5730\u5b66\u4e60\u5728\u6ca1\u6709\u4efb\u4f55\u624b\u5de5\u542f\u53d1\u5f0f\u7684\u60c5\u51b5\u4e0b\u51c6\u786e\u5730\u8ddf\u8e2a\u906e\u6321\u4e0b\u7684\u5bf9\u8c61\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728 DanceTrack\u3001BFT \u548c SportsMOT \u6570\u636e\u96c6\u4e0a\u663e\u7740\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002||\n", "2410.01678": "|**2024-10-02**|[Open3DTrack: Towards Open-Vocabulary 3D Multi-Object Tracking](http://arxiv.org/abs/2410.01678)|**[link](https://github.com/ayesha-ishaq/open3dtrack)**|\u4e09\u7ef4\u591a\u76ee\u6807\u8ddf\u8e2a\u901a\u8fc7\u5b9e\u65f6\u76d1\u63a7\u548c\u9884\u6d4b\u591a\u4e2a\u7269\u4f53\u7684\u8fd0\u52a8\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u4f20\u7edf\u7684\u4e09\u7ef4\u8ddf\u8e2a\u7cfb\u7edf\u901a\u5e38\u53d7\u5230\u9884\u5b9a\u4e49\u7269\u4f53\u7c7b\u522b\u7684\u9650\u5236\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5bf9\u52a8\u6001\u73af\u5883\u4e2d\u65b0\u51fa\u73b0\u7684\u3001\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u7684\u9002\u5e94\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5f00\u653e\u8bcd\u6c47\u4e09\u7ef4\u8ddf\u8e2a\uff0c\u5b83\u5c06\u4e09\u7ef4\u8ddf\u8e2a\u7684\u8303\u56f4\u6269\u5c55\u5230\u9884\u5b9a\u4e49\u7c7b\u522b\u4e4b\u5916\u7684\u7269\u4f53\u3002\u6211\u4eec\u5c06\u5f00\u653e\u8bcd\u6c47\u4e09\u7ef4\u8ddf\u8e2a\u95ee\u9898\u8fdb\u884c\u516c\u5f0f\u5316\uff0c\u5e76\u5f15\u5165\u4e86\u65e8\u5728\u8868\u793a\u5404\u79cd\u5f00\u653e\u8bcd\u6c47\u573a\u666f\u7684\u6570\u636e\u96c6\u5212\u5206\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5c06\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\u96c6\u6210\u5230\u4e09\u7ef4\u8ddf\u8e2a\u6846\u67b6\u4e2d\uff0c\u4ece\u800c\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u7c7b\u522b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u7b56\u7565\u6027\u9002\u5e94\u6709\u6548\u5730\u51cf\u5c11\u4e86\u8ddf\u8e2a\u5df2\u77e5\u7269\u4f53\u548c\u65b0\u7269\u4f53\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u5ba4\u5916\u9a7e\u9a76\u573a\u666f\u4e2d\u5177\u6709\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u9879\u5de5\u4f5c\u662f\u7b2c\u4e00\u4e2a\u89e3\u51b3\u5f00\u653e\u8bcd\u6c47\u4e09\u7ef4\u8ddf\u8e2a\u95ee\u9898\u7684\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u81ea\u4e3b\u7cfb\u7edf\u5e26\u6765\u4e86\u91cd\u5927\u8fdb\u6b65\u3002\u4ee3\u7801\u3001\u7ecf\u8fc7\u8bad\u7ec3\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u5212\u5206\u5747\u5df2\u516c\u5f00\u53d1\u5e03\u3002||\n", "2410.03505": "|**2024-10-04**|[Classification-Denoising Networks](http://arxiv.org/abs/2410.03505)|null|\u56fe\u50cf\u5206\u7c7b\u548c\u53bb\u566a\u9762\u4e34\u7740\u7f3a\u4e4f\u9c81\u68d2\u6027\u6216\u90e8\u5206\u5ffd\u7565\u6761\u4ef6\u4fe1\u606f\u7684\u4e92\u8865\u95ee\u9898\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u53ef\u4ee5\u901a\u8fc7 (\u566a\u58f0) \u56fe\u50cf\u548c\u7c7b\u522b\u6807\u7b7e\u7684\u8054\u5408\u6982\u7387\u6a21\u578b\u6765\u7edf\u4e00\u8fd9\u4e24\u4e2a\u4efb\u52a1\uff0c\u4ece\u800c\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\u3002\u5206\u7c7b\u901a\u8fc7\u524d\u5411\u4f20\u9012\u548c\u6761\u4ef6\u5316\u6765\u6267\u884c\u3002\u4f7f\u7528 Tweedie-Miyasawa \u516c\u5f0f\uff0c\u6211\u4eec\u7528\u5206\u6570\u6765\u8bc4\u4f30\u53bb\u566a\u51fd\u6570\uff0c\u8be5\u5206\u6570\u53ef\u4ee5\u901a\u8fc7\u8fb9\u7f18\u5316\u548c\u53cd\u5411\u4f20\u64ad\u6765\u8ba1\u7b97\u3002\u7136\u540e\uff0c\u8bad\u7ec3\u76ee\u6807\u662f\u4ea4\u53c9\u71b5\u635f\u5931\u548c\u5728\u566a\u58f0\u6c34\u5e73\u4e0a\u79ef\u5206\u7684\u53bb\u566a\u5206\u6570\u5339\u914d\u635f\u5931\u7684\u7ec4\u5408\u3002\u5728 CIFAR-10 \u548c ImageNet \u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u53c2\u8003\u6df1\u5ea6\u5377\u79ef\u5206\u7c7b\u5668/\u53bb\u566a\u5668\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u7ade\u4e89\u6027\u7684\u5206\u7c7b\u548c\u53bb\u566a\u6027\u80fd\uff0c\u5e76\u4e14\u4e0e\u4ee5\u524d\u7684\u8054\u5408\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6548\u7387\u663e\u7740\u63d0\u9ad8\u3002\u4e0e\u6807\u51c6\u5224\u522b\u5206\u7c7b\u5668\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5bf9\u5bf9\u6297\u6027\u6270\u52a8\u7684\u9c81\u68d2\u6027\u6709\u6240\u63d0\u9ad8\uff0c\u5e76\u4e14\u53ef\u4ee5\u5c06\u5bf9\u6297\u6027\u68af\u5ea6 novel \u5730\u89e3\u91ca\u4e3a\u53bb\u566a\u5668\u7684\u5dee\u5f02\u3002||\n", "2410.03276": "|**2024-10-04**|[Sm: enhanced localization in Multiple Instance Learning for medical imaging classification](http://arxiv.org/abs/2410.03276)|**[link](https://github.com/franblueee/smmil)**|\u591a\u793a\u4f8b\u5b66\u4e60 (MIL) \u5e7f\u6cdb\u5e94\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\uff0c\u4ee5\u51cf\u5c11\u6807\u6ce8\u5de5\u4f5c\u91cf\u3002\u867d\u7136\u8bad\u7ec3\u65f6\u53ea\u6709\u5305\u6807\u7b7e\u53ef\u7528\uff0c\u4f46\u4eba\u4eec\u901a\u5e38\u4f1a\u5728\u5305\u548c\u5b9e\u4f8b\u7ea7\u522b\u5bfb\u6c42\u9884\u6d4b\uff08\u5206\u522b\u4e3a\u5206\u7c7b\u548c\u5b9a\u4f4d\u4efb\u52a1\uff09\u3002\u65e9\u671f\u7684 MIL \u65b9\u6cd5\u72ec\u7acb\u5730\u5904\u7406\u5305\u4e2d\u7684\u5b9e\u4f8b\u3002\u6700\u8fd1\u7684\u65b9\u6cd5\u8003\u8651\u4e86\u5b9e\u4f8b\u4e4b\u95f4\u7684\u5168\u5c40\u548c\u5c40\u90e8\u4f9d\u8d56\u5173\u7cfb\u3002\u867d\u7136\u5b83\u4eec\u5728\u5206\u7c7b\u65b9\u9762\u53d6\u5f97\u4e86\u5f88\u597d\u7684\u6548\u679c\uff0c\u4f46\u5b83\u4eec\u5728\u5b9a\u4f4d\u65b9\u9762\u7684\u6027\u80fd\u76f8\u5bf9\u6709\u9650\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u8fd9\u4e9b\u6a21\u578b\u7684\u8bbe\u8ba1\u76ee\u6807\u662f\u5206\u7c7b\u4efb\u52a1\uff0c\u800c\u5b9e\u4f8b\u7ea7\u522b\u7684\u542b\u4e49\u5c1a\u672a\u5f97\u5230\u6df1\u5165\u7814\u7a76\u3002\u57fa\u4e8e\u4e00\u4e2a\u7b80\u5355\u7684\u89c2\u5bdf\u7ed3\u679c\u2014\u2014\u76f8\u90bb\u5b9e\u4f8b\u53ef\u80fd\u5177\u6709\u76f8\u540c\u7684\u6807\u7b7e\u2014\u2014\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u3001\u6709\u539f\u5219\u4e14\u7075\u6d3b\u7684\u673a\u5236\u6765\u6a21\u62df\u5c40\u90e8\u4f9d\u8d56\u5173\u7cfb\u3002\u5b83\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528\uff0c\u4e5f\u53ef\u4ee5\u4e0e\u4efb\u4f55\u6a21\u62df\u5168\u5c40\u4f9d\u8d56\u5173\u7cfb\u7684\u673a\u5236\uff08\u4f8b\u5982\uff0cTransformer\uff09\u7ed3\u5408\u4f7f\u7528\u3002\u5168\u9762\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u5757\u5728\u5b9a\u4f4d\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u5206\u7c7b\u65b9\u9762\u4e5f\u5177\u6709\u7ade\u4e89\u529b\u6216\u4f18\u8d8a\u6027\u3002\u6211\u4eec\u7684\u4ee3\u7801\u4f4d\u4e8ehttps://github.com/Franblueee/SmMIL\u3002||\n", "2410.03200": "|**2024-10-04**|[DRAFTS: A Deep Learning-Based Radio Fast Transient Search Pipeline](http://arxiv.org/abs/2410.03200)|**[link](https://github.com/SukiYume/DRAFTS)**|\u5728\u5c04\u7535\u5929\u6587\u5b66\u4e2d\uff0c\u5feb\u901f\u5c04\u7535\u66b4 (FRB) \u7684\u63a2\u6d4b\u662f\u4e00\u9879\u590d\u6742\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u5b83\u9762\u4e34\u7740\u5c04\u9891\u5e72\u6270 (RFI) \u548c\u661f\u9645\u4ecb\u8d28\u4e2d\u4fe1\u53f7\u8272\u6563\u5e26\u6765\u7684\u6311\u6218\u3002\u4f20\u7edf\u7684\u641c\u7d22\u7b97\u6cd5\u901a\u5e38\u6548\u7387\u4f4e\u4e0b\u3001\u8017\u65f6\u4e14\u4f1a\u4ea7\u751f\u5927\u91cf\u7684\u8bef\u62a5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DRAFTS\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5feb\u901f\u5c04\u7535\u77ac\u53d8\u641c\u7d22\u6d41\u7a0b\u3002DRAFTS \u6574\u5408\u4e86\u76ee\u6807\u68c0\u6d4b\u548c\u4e8c\u5143\u5206\u7c7b\u6280\u672f\uff0c\u4ee5\u51c6\u786e\u8bc6\u522b\u5c04\u7535\u6570\u636e\u4e2d\u7684 FRB\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u5927\u578b\u7684\u771f\u5b9e FRB \u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u5bf9 FAST \u771f\u5b9e\u89c2\u6d4b\u6570\u636e\u7684\u641c\u7d22\u6d4b\u8bd5\u8868\u660e\uff0cDRAFTS \u5728\u51c6\u786e\u6027\u3001\u5b8c\u6574\u6027\u548c\u641c\u7d22\u901f\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u5728 FRB 20190520B \u89c2\u6d4b\u6570\u636e\u7684\u641c\u7d22\u4e2d\uff0cDRAFTS \u63a2\u6d4b\u5230\u7684\u7206\u53d1\u6b21\u6570\u662f Heimdall \u7684\u4e09\u500d\u591a\uff0c\u8fd9\u7a81\u51fa\u4e86\u5176\u5728\u672a\u6765 FRB \u63a2\u6d4b\u548c\u5206\u6790\u65b9\u9762\u7684\u6f5c\u529b\u3002||\n", "2410.03021": "|**2024-10-03**|[PixelShuffler: A Simple Image Translation Through Pixel Rearrangement](http://arxiv.org/abs/2410.03021)|**[link](https://github.com/OmarSZamzam/PixelShuffler)**|\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u8f6c\u6362\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u4e00\u4e2a\u8bfe\u9898\uff0c\u5176\u5e94\u7528\u8303\u56f4\u5341\u5206\u5e7f\u6cdb\uff0c\u4ece\u533b\u5b66\u56fe\u50cf\u8f6c\u6362\uff08\u4f8b\u5982\u5c06MRI\u626b\u63cf\u8f6c\u6362\u4e3aCT\u626b\u63cf\u6216\u5176\u4ed6MRI\u5bf9\u6bd4\u5ea6\uff09\u5230\u56fe\u50cf\u7740\u8272\u3001\u8d85\u5206\u8fa8\u7387\u3001\u57df\u9002\u5e94\u4ee5\u53ca\u4ece\u8349\u56fe\u6216\u8bed\u4e49\u56fe\u751f\u6210\u903c\u771f\u56fe\u50cf\u3002\u56fe\u50cf\u98ce\u683c\u8fc1\u79fb\u4e5f\u662f\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u4e2d\u4e00\u4e2a\u88ab\u5e7f\u6cdb\u7814\u7a76\u7684\u5e94\u7528\uff0c\u5176\u76ee\u6807\u662f\u5408\u6210\u4e00\u4e2a\u7ed3\u5408\u4e86\u4e00\u5e45\u56fe\u50cf\u7684\u5185\u5bb9\u548c\u53e6\u4e00\u5e45\u56fe\u50cf\u98ce\u683c\u7684\u56fe\u50cf\u3002\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc\uff08\u5305\u62ec\u6269\u6563\u6a21\u578b\u548c\u8bed\u8a00\u6a21\u578b\uff09\u6765\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u98ce\u683c\u8fc1\u79fb\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u7684\u8ba1\u7b97\u6210\u672c\u53ef\u80fd\u5f88\u9ad8\uff0c\u800c\u4e14\u5b9e\u73b0\u8d77\u6765\u4e5f\u5f88\u590d\u6742\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u50cf\u7d20\u6d17\u724c\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u7684\u4e00\u822c\u95ee\u9898\uff0c\u5e76\u5728\u98ce\u683c\u8fc1\u79fb\u4e2d\u6709\u4e00\u4e2a\u5177\u4f53\u7684\u6f14\u793a\u5e94\u7528\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u98ce\u683c\u56fe\u50cf\u7684\u50cf\u7d20\u8fdb\u884c\u6d17\u724c\u6765\u5b9e\u73b0\u98ce\u683c\u8fc1\u79fb\uff0c\u4ece\u800c\u6700\u5927\u5316\u6d17\u724c\u540e\u7684\u56fe\u50cf\u4e0e\u5185\u5bb9\u56fe\u50cf\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u3002\u8fd9\u79cd\u65b9\u6cd5inherently\u4fdd\u7559\u4e86\u98ce\u683c\u56fe\u50cf\u7684\u989c\u8272\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u5185\u5bb9\u56fe\u50cf\u7684\u7ed3\u6784\u7ec6\u8282\u4fdd\u7559\u5728\u98ce\u683c\u5316\u540e\u7684\u8f93\u51fa\u4e2d\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u8fd9\u79cd\u7b80\u5355\u76f4\u63a5\u7684\u65b9\u6cd5\u4ea7\u751f\u7684\u7ed3\u679c\u53ef\u4e0e\u6700\u5148\u8fdb\u7684\u6280\u672f\u76f8\u5ab2\u7f8e\uff0c\u8fd9\u53ef\u4ee5\u901a\u8fc7\u5b66\u4e60\u611f\u77e5\u56fe\u50cf\u5757\u76f8\u4f3c\u5ea6\uff08LPIPS\uff09\u635f\u5931\uff08\u7528\u4e8e\u5185\u5bb9\u4fdd\u7559\uff09\u548cFr\\'echet\u521d\u59cb\u8ddd\u79bb\uff08FID\uff09\u5206\u6570\uff08\u7528\u4e8e\u98ce\u683c\u76f8\u4f3c\u5ea6\uff09\u6765\u8861\u91cf\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u50cf\u7d20\u6d17\u724c\u65b9\u6cd5\u5728\u663e\u8457\u964d\u4f4e\u590d\u6742\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u7684\u56fe\u50cf\u98ce\u683c\u8fc1\u79fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f88\u6709\u524d\u9014\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u540c\u65f6\u4e5f\u4e3a\u8be5\u65b9\u6cd5\u5728\u4e00\u822c\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u4efb\u52a1\u4e2d\u7684\u53ef\u7528\u6027\u5e26\u6765\u4e86\u5e0c\u671b\u3002||\n", "2410.02935": "|**2024-10-03**|[On Expert Estimation in Hierarchical Mixture of Experts: Beyond Softmax Gating Functions](http://arxiv.org/abs/2410.02935)|null|\u968f\u7740\u6df7\u5408\u4e13\u5bb6\u6a21\u578b (MoE) \u67b6\u6784\u5728\u5f00\u53d1\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5206\u5c42\u6df7\u5408\u4e13\u5bb6\u6a21\u578b (HMoE)\uff0c\u8fd9\u662f MoE \u7684\u4e00\u79cd\u7279\u6b8a\u53d8\u4f53\uff0c\u64c5\u957f\u5904\u7406\u590d\u6742\u8f93\u5165\u548c\u63d0\u9ad8\u76ee\u6807\u4efb\u52a1\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u7814\u7a76\u5f3a\u8c03\u4e86\u4f7f\u7528\u4e0d\u540c\u7684\u95e8\u63a7\u51fd\u6570\u7684\u4f18\u52bf\uff0c\u8d85\u8d8a\u4e86 HMoE \u6846\u67b6\u5185\u7684 softmax \u95e8\u63a7\u3002\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\uff0c\u5373\u4f7f\u4ec5\u5728\u9009\u5b9a\u7684\u5c42\u6b21\u7ea7\u522b\u5e94\u7528\u6700\u4f73\u95e8\u63a7\u51fd\u6570\uff0c\u5bf9\u6bcf\u4e2a\u4e13\u5bb6\u7ec4\u5e94\u7528\u5b9a\u5236\u7684\u95e8\u63a7\u51fd\u6570\u4e5f\u5141\u8bb8 HMoE \u5b9e\u73b0\u7a33\u5065\u7684\u7ed3\u679c\u3002\u8de8\u4e0d\u540c\u573a\u666f\u7684\u7ecf\u9a8c\u9a8c\u8bc1\u652f\u6301\u4e86\u8fd9\u4e9b\u7406\u8bba\u4e3b\u5f20\u3002\u8fd9\u5305\u62ec\u5927\u89c4\u6a21\u591a\u6a21\u6001\u4efb\u52a1\u3001\u56fe\u50cf\u5206\u7c7b\u4ee5\u53ca\u6f5c\u5728\u9886\u57df\u53d1\u73b0\u548c\u9884\u6d4b\u4efb\u52a1\uff0c\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e2d\uff0c\u6211\u4eec\u6539\u8fdb\u7684 HMoE \u6a21\u578b\u663e\u793a\u51fa\u5de8\u5927\u7684\u6027\u80fd\u63d0\u5347\u3002||\n", "2410.05249": "|**2024-10-07**|[LoTLIP: Improving Language-Image Pre-training for Long Text Understanding](http://arxiv.org/abs/2410.05249)|null|\u7406\u89e3\u957f\u6587\u672c\u5728\u5b9e\u8df5\u4e2d\u6709\u7740\u5de8\u5927\u7684\u9700\u6c42\uff0c\u4f46\u8fd9\u8d85\u51fa\u4e86\u5927\u591a\u6570\u8bed\u8a00\u56fe\u50cf\u9884\u8bad\u7ec3 (LIP) \u6a21\u578b\u7684\u80fd\u529b\u8303\u56f4\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5b9e\u8bc1\u8bc1\u5b9e\u4e86\u9020\u6210\u8fd9\u4e2a\u95ee\u9898\u7684\u5173\u952e\u539f\u56e0\u662f\u8bad\u7ec3\u56fe\u50cf\u901a\u5e38\u4e0e\u7b80\u77ed\u7684\u6807\u9898\u914d\u5bf9\uff0c\u5bfc\u81f4\u67d0\u4e9b\u8bcd\u8bed\u5bb9\u6613\u88ab\u7a81\u51fa\u7684\u8bcd\u8bed\u6240\u63a9\u76d6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u6700\u521d\u5c1d\u8bd5\u4f7f\u7528\u957f\u6807\u9898\u91cd\u65b0\u6807\u8bb0\u6570\u636e\uff0c\u4f46\u662f\uff0c\u76f4\u63a5\u4f7f\u7528\u957f\u6807\u9898\u8fdb\u884c\u5b66\u4e60\u53ef\u80fd\u4f1a\u5bfc\u81f4\u7406\u89e3\u77ed\u6587\u672c\u7684\u6027\u80fd\u4e0b\u964d\uff08\u4f8b\u5982\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff09\u3002\u7136\u540e\uff0c\u901a\u8fc7\u7ed3\u5408\u89d2\u70b9\u8bcd\u8bed\u6765\u805a\u5408\u4e0d\u540c\u7684\u6587\u672c\u4fe1\u606f\uff0c\u6211\u4eec\u8bbe\u6cd5\u5e2e\u52a9\u6a21\u578b\u5728\u7406\u89e3\u77ed\u6587\u672c\u65b9\u9762\u8d76\u4e0a\u5176\u539f\u59cb\u6c34\u5e73\uff0c\u540c\u65f6\u5927\u5927\u589e\u5f3a\u5176\u7406\u89e3\u957f\u6587\u672c\u7684\u80fd\u529b\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u7814\u7a76\u4e86\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u4ece\u66f4\u957f\u7684\u6807\u9898\u4e2d\u6301\u7eed\u53d7\u76ca\uff0c\u5e76\u6ce8\u610f\u5230\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u7684\u6743\u8861\u3002\u6700\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u81ea\u5efa\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b 1 \u4ebf\u4e2a\u9762\u5411\u957f\u6807\u9898\u7684\u6587\u672c\u56fe\u50cf\u5bf9\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u957f\u6587\u672c\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0c\u6211\u4eec\u6bd4\u4f7f\u7528\u957f\u6807\u9898\u7684\u7ade\u4e89\u5bf9\u624b\u63d0\u9ad8\u4e86 11.1%\uff08\u5373\u4ece 72.62% \u63d0\u9ad8\u5230 83.72%\uff09\u3002\u6211\u4eec\u5c06\u53d1\u5e03\u4ee3\u7801\u3001\u6a21\u578b\u548c\u65b0\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u548c\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u3002\u9879\u76ee\u9875\u9762\u53ef\u8bbf\u95ee https://wuw2019.github.io/lotlip\u3002||\n", "2410.05063": "|**2024-10-07**|[Control-oriented Clustering of Visual Latent Representation](http://arxiv.org/abs/2410.05063)|null|\u6211\u4eec\u5bf9\u57fa\u4e8e\u56fe\u50cf\u7684\u63a7\u5236\u7ba1\u9053\u4e2d\u89c6\u89c9\u8868\u5f81\u7a7a\u95f4\uff08\u4ece\u89c6\u89c9\u7f16\u7801\u5668\u5230\u52a8\u4f5c\u89e3\u7801\u5668\u7684\u4fe1\u9053\uff09\u7684\u51e0\u4f55\u7ed3\u6784\u8fdb\u884c\u7814\u7a76\uff0c\u8be5\u7ba1\u9053\u901a\u8fc7\u884c\u4e3a\u514b\u9686\u5b66\u4e60\u5f97\u5230\u3002\u53d7\u56fe\u50cf\u5206\u7c7b\u4e2d\u795e\u7ecf\u5143\u5d29\u6e83\uff08NC\uff09\u73b0\u8c61\u7684\u542f\u53d1\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u89c6\u89c9\u8868\u5f81\u7a7a\u95f4\u4e2d\u662f\u5426\u4f1a\u51fa\u73b0\u7c7b\u4f3c\u7684\u805a\u7c7b\u89c4\u5f8b\u3002\u7531\u4e8e\u57fa\u4e8e\u56fe\u50cf\u7684\u63a7\u5236\u662f\u4e00\u9879\u6ca1\u6709\u660e\u786e\u5b9a\u4e49\u7c7b\u522b\u7684\u56de\u5f52\u4efb\u52a1\uff0c\u56e0\u6b64\u95ee\u9898\u7684\u5173\u952e\u5728\u4e8e\u786e\u5b9a\u89c6\u89c9\u7279\u5f81\u6839\u636e\u54ea\u4e9b\u9690\u542b\u7c7b\u522b\u8fdb\u884c\u805a\u7c7b\uff08\u5982\u679c\u5b58\u5728\u8fd9\u79cd\u89c4\u5f8b\uff09\u3002\u6211\u4eec\u4e13\u6ce8\u4e8e\u57fa\u4e8e\u56fe\u50cf\u7684\u5e73\u9762\u63a8\u52a8\u4efb\u52a1\uff0c\u5047\u8bbe\u89c6\u89c9\u8868\u5f81\u5728\u63a7\u5236\u4efb\u52a1\u4e2d\u6700\u91cd\u8981\u4f5c\u7528\u662f\u5411\u52a8\u4f5c\u89e3\u7801\u5668\u4f20\u9012\u76ee\u6807\u3002\u7136\u540e\uff0c\u6211\u4eec\u6839\u636e(a) \u8f93\u5165\u4e2d\u7269\u4f53\u548c\u76ee\u6807\u4e4b\u95f4\u7684\u76f8\u5bf9\u59ff\u6001\u6216(b) \u8f93\u51fa\u4e2d\u4e13\u5bb6\u52a8\u4f5c\u5f15\u8d77\u7684\u7269\u4f53\u7684\u76f8\u5bf9\u59ff\u6001\uff0c\u5c06\u4e13\u5bb6\u6f14\u793a\u7684\u8bad\u7ec3\u6837\u672c\u5206\u4e3a\u516b\u4e2a\u201c\u9762\u5411\u63a7\u5236\u201d\u7684\u7c7b\u522b\uff0c\u5176\u4e2d\u4e00\u4e2a\u7c7b\u522b\u5bf9\u5e94\u4e00\u4e2a\u76f8\u5bf9\u59ff\u6001\u5366\u9650\uff08REPO\uff09\u3002\u5728\u67b6\u6784\u7684\u56db\u79cd\u4e0d\u540c\u5b9e\u4f8b\u4e2d\uff0c\u6211\u4eec\u62a5\u544a\u4e86\u6839\u636e\u516b\u4e2aREPO\uff0c\u89c6\u89c9\u8868\u5f81\u7a7a\u95f4\u4e2d\u666e\u904d\u51fa\u73b0\u4e86\u9762\u5411\u63a7\u5236\u7684\u805a\u7c7b\u3002\u9664\u4e86\u7ecf\u9a8c\u89c2\u5bdf\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u8868\u660e\uff0c\u5f53\u4f7f\u7528\u6709\u9650\u7684\u4e13\u5bb6\u6f14\u793a\u8bad\u7ec3\u7b56\u7565\u65f6\uff0c\u8fd9\u79cd\u805a\u7c7b\u89c4\u5f8b\u53ef\u4ee5\u7528\u4f5c\u7b97\u6cd5\u5de5\u5177\u6765\u63d0\u9ad8\u6d4b\u8bd5\u65f6\u7684\u6027\u80fd\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u4f7f\u7528NC\u4f5c\u4e3a\u6b63\u5219\u5316\u65b9\u6cd5\u5bf9\u89c6\u89c9\u7f16\u7801\u5668\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4ee5\u9f13\u52b1\u89c6\u89c9\u7279\u5f81\u7684\u9762\u5411\u63a7\u5236\u7684\u805a\u7c7b\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u8fd9\u79cd\u7ecf\u8fc7NC\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u7f16\u7801\u5668\u5728\u4f7f\u7528\u52a8\u4f5c\u89e3\u7801\u5668\u8fdb\u884c\u7aef\u5230\u7aef\u5fae\u8c03\u65f6\uff0c\u5728\u4f4e\u6570\u636e\u60c5\u51b5\u4e0b\u5c06\u6d4b\u8bd5\u6027\u80fd\u63d0\u9ad8\u4e8610%\u523035%\u3002\u73b0\u5b9e\u4e16\u754c\u4e2d\u57fa\u4e8e\u89c6\u89c9\u7684\u5e73\u9762\u63a8\u52a8\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u9762\u5411\u63a7\u5236\u7684\u89c6\u89c9\u8868\u5f81\u9884\u8bad\u7ec3\u7684\u60ca\u4eba\u4f18\u52bf\u3002||\n", "2410.05058": "|**2024-10-07**|[Improving Object Detection via Local-global Contrastive Learning](http://arxiv.org/abs/2410.05058)|null|\u89c6\u89c9\u57df\u5dee\u8ddd\u901a\u5e38\u4f1a\u5f71\u54cd\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u8f6c\u6362\u53ef\u4ee5\u51cf\u8f7b\u8fd9\u79cd\u5f71\u54cd\uff0c\u5176\u4e2d\u5bf9\u6bd4\u65b9\u6cd5\u80fd\u591f\u5728\u65e0\u76d1\u7763\u60c5\u51b5\u4e0b\u5b66\u4e60\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u6620\u5c04\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u5904\u7406\u5305\u542b\u591a\u4e2a\u76ee\u6807\u5b9e\u4f8b\u7684\u5185\u5bb9\u4e30\u5bcc\u7684\u573a\u666f\uff0c\u8fd9\u8868\u73b0\u4e3a\u68c0\u6d4b\u6027\u80fd\u4e0d\u7406\u60f3\u3002\u5bf9\u8fd9\u79cd\u5b9e\u4f8b\u7ea7\u5185\u5bb9\u7684\u654f\u611f\u6027\u901a\u5e38\u53ea\u80fd\u901a\u8fc7\u76ee\u6807\u6807\u6ce8\u6765\u83b7\u5f97\uff0c\u800c\u76ee\u6807\u6807\u6ce8\u7684\u83b7\u53d6\u6210\u672c\u53ef\u80fd\u5f88\u9ad8\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u8de8\u57df\u76ee\u6807\u68c0\u6d4b\u3002\u6211\u4eec\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u5236\u5b9a\u4e3a\u4e00\u4e2a\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5177\u6709\u5f52\u7eb3\u5148\u9a8c\uff0c\u901a\u8fc7\u7a7a\u95f4\u6ce8\u610f\u63a9\u7801\u4f18\u5316\u76ee\u6807\u5b9e\u4f8b\u7684\u5916\u89c2\uff0c\u5c06\u573a\u666f\u9690\u5f0f\u5730\u5212\u5206\u4e3a\u4e0e\u76ee\u6807\u76ee\u6807\u5b9e\u4f8b\u76f8\u5173\u7684\u524d\u666f\u533a\u57df\u548c\u80cc\u666f\u975e\u76ee\u6807\u533a\u57df\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u662f\u4f9d\u9760\u76ee\u6807\u6807\u6ce8\u5728\u8f6c\u6362\u8fc7\u7a0b\u4e2d\u660e\u786e\u5730\u8003\u8651\u76ee\u6807\u5b9e\u4f8b\uff0c\u800c\u662f\u901a\u8fc7\u5bf9\u6bd4\u5c40\u90e8-\u5168\u5c40\u4fe1\u606f\u6765\u5b66\u4e60\u8868\u793a\u76ee\u6807\u3002\u8fd9\u4e3a\u63a2\u7d22\u4e00\u9879\u672a\u88ab\u5145\u5206\u6316\u6398\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u53ef\u80fd\uff1a\u5728\u4e0d\u4f9d\u8d56\u76ee\u6807\u6807\u6ce8\u6216\u68c0\u6d4b\u5668\u6a21\u578b\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u57df\u8f6c\u79fb\u4e0b\u83b7\u5f97\u9ad8\u6027\u80fd\u68c0\u6d4b\u3002\u6211\u4eec\u901a\u8fc7\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5bf9\u591a\u4e2a\u8de8\u57df\u76ee\u6807\u68c0\u6d4b\u8bbe\u7f6e\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5e76\u62a5\u544a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://local-global-detection.github.io||\n", "2410.04930": "|**2024-10-07**|[Near-Field ISAC in 6G: Addressing Phase Nonlinearity via Lifted Super-Resolution](http://arxiv.org/abs/2410.04930)|null|\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1 (ISAC) \u662f 6G \u7f51\u7edc\u7684\u4e00\u4e2a\u5f88\u6709\u524d\u666f\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u5b83\u878d\u5408\u4e86\u901a\u4fe1\u548c\u96f7\u8fbe\u6280\u672f\u4ee5\u4fc3\u8fdb\u65b0\u7684\u670d\u52a1\u3002\u6b64\u5916\uff0c\u5728 ISAC \u5171\u7528\u63a5\u6536\u673a\u4e0a\u4f7f\u7528\u8d85\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u5217 (ELLA) \u4e0d\u4ec5\u4fc3\u8fdb\u4e86\u592a\u8d6b\u5179\u7ea7\u901a\u4fe1\u94fe\u8def\uff0c\u800c\u4e14\u8fd8\u663e\u8457\u63d0\u9ad8\u4e86\u96f7\u8fbe\u5e94\u7528\u4e2d\u76ee\u6807\u68c0\u6d4b\u7684\u7cbe\u5ea6\u3002\u5728\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u901a\u4fe1\u6563\u5c04\u4f53\u548c\u96f7\u8fbe\u76ee\u6807\u901a\u5e38\u4f4d\u4e8e\u8ddd\u79bb ISAC \u63a5\u6536\u673a\u5f88\u8fd1\u7684\u4f4d\u7f6e\u3002\u8fd9\u79cd\u60c5\u51b5\uff0c\u518d\u52a0\u4e0a ELLA \u7684\u4f7f\u7528\uff0c\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u4e86\u65e0\u7ebf\u548c\u96f7\u8fbe\u4fe1\u9053\u7684\u7535\u78c1\u7279\u6027\uff0c\u4ece\u8fdc\u573a\u5e73\u9762\u6ce2\u4f20\u64ad\u8f6c\u53d8\u4e3a\u8fd1\u573a\u7403\u9762\u6ce2\u4f20\u64ad\u3002\u5728\u8fdc\u573a\u5e73\u9762\u6ce2\u6a21\u578b\u4e0b\uff0c\u9635\u5217\u54cd\u5e94\u5411\u91cf\u7684\u76f8\u4f4d\u968f\u5929\u7ebf\u7d22\u5f15\u7ebf\u6027\u53d8\u5316\u3002\u76f8\u53cd\uff0c\u5728\u8fd1\u573a\u7403\u9762\u6ce2\u6a21\u578b\u4e2d\uff0c\u8fd9\u79cd\u76f8\u4f4d\u5173\u7cfb\u53d8\u4e3a\u975e\u7ebf\u6027\u3002\u8fd9\u79cd\u8f6c\u53d8\u63d0\u51fa\u4e86\u4e00\u4e2a\u6839\u672c\u6027\u7684\u6311\u6218\uff1a\u5e7f\u6cdb\u4f7f\u7528\u7684\u5085\u7acb\u53f6\u5206\u6790\u4e0d\u80fd\u518d\u76f4\u63a5\u5e94\u7528\u4e8e ISAC \u5171\u7528\u63a5\u6536\u673a\u4e0a\u7684\u76ee\u6807\u68c0\u6d4b\u548c\u901a\u4fe1\u4fe1\u9053\u4f30\u8ba1\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u6765\u89e3\u51b3\u8fd9\u4e2a\u57fa\u672c\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5b58\u5728\u4e00\u4e2a\u9ad8\u7ef4\u7a7a\u95f4\uff0c\u5176\u4e2d\u76f8\u4f4d\u975e\u7ebf\u6027\u53ef\u4ee5\u8868\u793a\u4e3a\u7ebf\u6027\u3002\u5229\u7528\u8fd9\u4e00\u89c1\u89e3\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u63d0\u5347\u7684\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u540c\u65f6\u6267\u884c\u901a\u4fe1\u4fe1\u9053\u4f30\u8ba1\u5e76\u4ee5\u9ad8\u7cbe\u5ea6\u63d0\u53d6\u76ee\u6807\u53c2\u6570\u3002||\n", "2410.04880": "|**2024-10-07**|[Improved detection of discarded fish species through BoxAL active learning](http://arxiv.org/abs/2410.04880)|**[link](https://github.com/pieterblok/boxal)**|\u8fd1\u5e74\u6765\uff0c\u5f3a\u5927\u7684\u6570\u636e\u9a71\u52a8\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5df2\u88ab\u5f00\u53d1\u5e76\u5e94\u7528\u4e8e\u81ea\u52a8\u5316\u6e14\u83b7\u767b\u8bb0\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6807\u8bb0\u6570\u636e\uff0c\u800c\u6807\u8bb0\u6570\u636e\u7684\u6536\u96c6\u975e\u5e38\u8017\u65f6\u3001\u8d39\u529b\u3001\u6602\u8d35\uff0c\u5e76\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a BoxAL \u7684\u4e3b\u52a8\u5b66\u4e60\u6280\u672f\uff0c\u8be5\u6280\u672f\u5305\u62ec\u5bf9 Faster R-CNN \u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u8fdb\u884c\u4f30\u8ba1\u3002\u8be5\u65b9\u6cd5\u5141\u8bb8\u4ece\u672a\u6807\u8bb0\u7684\u56fe\u50cf\u6c60\u4e2d\u9009\u62e9\u6700\u4e0d\u786e\u5b9a\u7684\u8bad\u7ec3\u56fe\u50cf\uff0c\u7136\u540e\u4f7f\u7528\u8fd9\u4e9b\u56fe\u50cf\u6765\u8bad\u7ec3\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u3002\u4e3a\u4e86\u8bc4\u4f30\u8be5\u65b9\u6cd5\uff0c\u6211\u4eec\u4f7f\u7528\u4e86\u4e00\u4e2a\u5f00\u6e90\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u662f\u901a\u8fc7\u4e13\u4e3a\u6355\u635e\u5e95\u5c42\u9c7c\u7c7b\u7684\u5546\u4e1a\u62d6\u7f51\u6e14\u8239\u5f00\u53d1\u7684\u4e13\u7528\u56fe\u50cf\u91c7\u96c6\u7cfb\u7edf\u83b7\u5f97\u7684\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u4f7f\u7528\u6bd4\u968f\u673a\u62bd\u6837\u5c11 400 \u5f20\u6807\u8bb0\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u76f8\u540c\u7684\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5728\u6700\u540e\u4e00\u6b21\u8bad\u7ec3\u8fed\u4ee3\u4e2d\uff0c\u4f7f\u7528 1100 \u5f20\u8bad\u7ec3\u56fe\u50cf\u65f6\uff0c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u91c7\u6837\u548c\u968f\u673a\u91c7\u6837\u7684\u5e73\u5747 AP \u5206\u6570\u5206\u522b\u663e\u7740\u63d0\u9ad8\u5230 39.0\u00b11.6 \u548c 34.8\u00b11.8\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8868\u660e\uff0c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u662f\u4e00\u79cd\u5408\u9002\u7684\u91c7\u6837\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5bf9\u5f53\u524d\u8fed\u4ee3\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u7684\u56fe\u50cf\u8fdb\u884c\u91c7\u6837\u3002\u6211\u4eec\u7684\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u91c7\u6837\u5f97\u5230\u7684\u65b0\u6570\u636e\u6bd4\u5269\u4f59\u7684\u672a\u6807\u8bb0\u6570\u636e\u5bf9\u8bad\u7ec3\u66f4\u6709\u4ef7\u503c\u3002\u6211\u4eec\u7684\u8f6f\u4ef6\u53ef\u5728 https://github.com/pieterblok/boxal \u83b7\u53d6\u3002||\n", "2410.04546": "|**2024-10-06**|[Learning De-Biased Representations for Remote-Sensing Imagery](http://arxiv.org/abs/2410.04546)|**[link](https://github.com/doem97/deblora)**|\u9065\u611f (RS) \u5f71\u50cf\u9700\u8981\u4e13\u95e8\u7684\u536b\u661f\u8fdb\u884c\u91c7\u96c6\uff0c\u800c\u4e14\u6807\u6ce8\u96be\u5ea6\u5927\uff0c\u56e0\u6b64\u5b58\u5728\u6570\u636e\u7a00\u7f3a\u548c\u67d0\u4e9b\u5149\u8c31\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u95ee\u9898\u3002\u7531\u4e8e\u6570\u636e\u7a00\u7f3a\uff0c\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u4efb\u4f55\u5927\u89c4\u6a21 RS \u6a21\u578b\u90fd\u662f\u4e0d\u73b0\u5b9e\u7684\uff0c\u66ff\u4ee3\u65b9\u6848\u662f\u901a\u8fc7\u5fae\u8c03\u6216\u6570\u636e\u6548\u7387\u66f4\u9ad8\u7684 LoRA \u65b9\u6cd5\u6765\u8fc1\u79fb\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u7531\u4e8e\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u8fc1\u79fb\u540e\u7684\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u504f\u5dee\uff0c\u5176\u4e2d\u4e3b\u8981\u7c7b\u522b\u7684\u7279\u5f81\u652f\u914d\u7740\u6b21\u8981\u7c7b\u522b\u7684\u7279\u5f81\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 debLoRA\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u7528\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4e0e\u4efb\u4f55 LoRA \u53d8\u4f53\u4e00\u8d77\u4f7f\u7528\uff0c\u4ee5\u4ea7\u751f\u53bb\u504f\u5dee\u7684\u7279\u5f81\u3002\u5b83\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6839\u636e\u4e0e\u4e3b\u8981\u7c7b\u522b\u5171\u4eab\u7684\u5c5e\u6027\u6765\u5b9e\u73b0\u6b21\u8981\u7c7b\u522b\u7279\u5f81\u7684\u591a\u6837\u5316\uff0c\u5176\u4e2d\u5c5e\u6027\u662f\u901a\u8fc7\u7b80\u5355\u7684\u805a\u7c7b\u6b65\u9aa4\u83b7\u5f97\u7684\u3002\u4e3a\u4e86\u5bf9\u5176\u8fdb\u884c\u8bc4\u4f30\uff0c\u6211\u4eec\u5728 RS \u9886\u57df\u7684\u4e24\u79cd\u8fc1\u79fb\u5b66\u4e60\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff1a\u4ece\u81ea\u7136\u56fe\u50cf\u5230\u5149\u5b66 RS \u56fe\u50cf\uff0c\u4ee5\u53ca\u4ece\u5149\u5b66 RS \u56fe\u50cf\u5230\u591a\u5149\u8c31 RS \u56fe\u50cf\u3002\u6211\u4eec\u5728\u5149\u5b66 RS \u6570\u636e\u96c6 DOTA \u548c SAR \u6570\u636e\u96c6 FUSRS \u4e0a\u6267\u884c\u4e86\u76ee\u6807\u5206\u7c7b\u548c\u9762\u5411\u76ee\u6807\u7684\u68c0\u6d4b\u4efb\u52a1\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684 debLoRA \u5728\u8fd9\u4e9b RS \u9002\u5e94\u6027\u8bbe\u7f6e\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5728\u81ea\u7136\u56fe\u50cf\u5230\u5149\u5b66 RS \u548c\u5149\u5b66 RS \u5230\u591a\u5149\u8c31 RS \u7684\u9002\u5e94\u6027\u65b9\u9762\uff0c\u5c3e\u90e8\u7c7b\u522b\u7684\u6027\u80fd\u5206\u522b\u63d0\u9ad8\u4e86 3.3 \u548c 4.7 \u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5934\u90e8\u7c7b\u522b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u9002\u5e94\u6027\u3002||\n", "2410.04224": "|**2024-10-05**|[Distillation-Free One-Step Diffusion for Real-World Image Super-Resolution](http://arxiv.org/abs/2410.04224)|**[link](https://github.com/jianzeli-114/dfosd)**|\u6269\u6563\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08Real-ISR\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u5f88\u9ad8\u3002\u5f53\u524d\u7684\u65b9\u6cd5\u8bd5\u56fe\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u4ece\u591a\u6b65\u6a21\u578b\u4e2d\u63a8\u5bfc\u51fa\u4e00\u6b65\u6269\u6563\u6a21\u578b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u5927\u91cf\u7684\u8bad\u7ec3\u6210\u672c\uff0c\u5e76\u4e14\u53ef\u80fd\u4f1a\u53d7\u5230\u6559\u5e08\u6a21\u578b\u7684\u9650\u5236\uff0c\u4ece\u800c\u9650\u5236\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86DFOSD\uff0c\u4e00\u79cd\u65e0\u9700\u84b8\u998f\u7684\u4e00\u6b65\u6269\u6563\u6a21\u578b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u566a\u58f0\u611f\u77e5\u9274\u522b\u5668\uff08NAD\uff09\u6765\u53c2\u4e0e\u5bf9\u6297\u8bad\u7ec3\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3a\u751f\u6210\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528\u8fb9\u7f18\u611f\u77e5DISTS\uff08EA-DISTS\uff09\u6539\u8fdb\u4e86\u611f\u77e5\u635f\u5931\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u751f\u6210\u7cbe\u7ec6\u7ec6\u8282\u7684\u80fd\u529b\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4e4b\u524d\u9700\u8981\u6570\u5341\u6b65\u751a\u81f3\u6570\u767e\u6b65\u7684\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684DFOSD\u5728\u5b9a\u91cf\u6307\u6807\u548c\u5b9a\u6027\u8bc4\u4f30\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u7ed3\u679c\u3002\u4e0e\u5176\u4ed6\u4e00\u6b65\u6269\u6563\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684DFOSD\u8fd8\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u6548\u7387\u3002\u6211\u4eec\u5c06\u5728\\url{https://github.com/JianzeLi-114/DFOSD}\u53d1\u5e03\u4ee3\u7801\u548c\u6a21\u578b\u3002||\n", "2410.04205": "|**2024-10-05**|[Exploring Strengths and Weaknesses of Super-Resolution Attack in Deepfake Detection](http://arxiv.org/abs/2410.04205)|null|Image manipulation is rapidly evolving, allowing the creation of credible content that can be used to bend reality. Although the results of deepfake detectors are promising, deepfakes can be made even more complicated to detect through adversarial attacks. They aim to further manipulate the image to camouflage deepfakes' artifacts or to insert signals making the image appear pristine. In this paper, we further explore the potential of super-resolution attacks based on different super-resolution techniques and with different scales that can impact the performance of deepfake detectors with more or less intensity. We also evaluated the impact of the attack on more diverse datasets discovering that the super-resolution process is effective in hiding the artifacts introduced by deepfake generation models but fails in hiding the traces contained in fully synthetic images. Finally, we propose some changes to the detectors' training process to improve their robustness to this kind of attack.||\n", "2410.04173": "|**2024-10-05**|[Fast Object Detection with a Machine Learning Edge Device](http://arxiv.org/abs/2410.04173)|null|\u672c\u673a\u5668\u5b66\u4e60\u7814\u7a76\u8c03\u67e5\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u8fb9\u7f18\u8bbe\u5907\uff0c\u8be5\u8bbe\u5907\u96c6\u6210\u4e86\u4e00\u4e2a\u5177\u6709\u8ba1\u7b97\u673a\u89c6\u89c9\u529f\u80fd\u7684\u5d4c\u5165\u5f0f\u7cfb\u7edf\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u76ee\u6807\u68c0\u6d4b\u548c\u5206\u7c7b\u7684\u63a8\u7406\u65f6\u95f4\u548c\u7cbe\u5ea6\u3002\u672c\u7814\u7a76\u7684\u4e3b\u8981\u76ee\u6807\u662f\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u548c\u964d\u4f4e\u529f\u8017\uff0c\u5e76\u4f7f\u7ade\u8d5b\u7ea7\u81ea\u4e3b\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907\u80fd\u591f\u652f\u6301\u5b9e\u65f6\u76ee\u6807\u8bc6\u522b\u3001\u573a\u666f\u7406\u89e3\u3001\u89c6\u89c9\u5bfc\u822a\u3001\u8fd0\u52a8\u89c4\u5212\u548c\u673a\u5668\u4eba\u7684\u81ea\u4e3b\u5bfc\u822a\u3002\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4e2d\u592e\u5904\u7406\u5668 (CPU)\u3001\u56fe\u5f62\u5904\u7406\u5668 (GPU) \u548c\u5f20\u91cf\u5904\u7406\u5668 (TPU) \u4e4b\u95f4\u7684\u63a8\u7406\u65f6\u95f4\u6027\u80fd\u3002CPU\u3001GPU \u548c TPU \u90fd\u662f\u53ef\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u7684\u5904\u7406\u5668\u3002\u4e3a\u4e86\u652f\u6301\u81ea\u4e3b\u4eba\u5f62\u673a\u5668\u4eba\uff0c\u6211\u4eec\u8fd8\u52aa\u529b\u89c2\u5bdf\u4f7f\u7528\u5177\u6709\u5355\u76ee\u89c6\u89c9\u529f\u80fd\u7684\u76f8\u673a\u4e0e\u7acb\u4f53\u89c6\u89c9\u529f\u80fd\u7684\u76f8\u673a\u662f\u5426\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u672c\u7814\u7a76\u7684 TPU \u63a8\u7406\u65f6\u95f4\u7ed3\u679c\u53cd\u6620\uff0c\u4e0e GPU \u76f8\u6bd4\uff0c\u65f6\u95f4\u7f29\u77ed\u4e86 25%\uff0c\u4e0e CPU \u76f8\u6bd4\uff0c\u63a8\u7406\u65f6\u95f4\u60ca\u4eba\u5730\u7f29\u77ed\u4e86 87.5%\u3002\u672c\u6587\u7684\u8bb8\u591a\u4fe1\u606f\u6709\u52a9\u4e8e\u6700\u7ec8\u9009\u62e9 Google \u7684 Coral \u54c1\u724c Edge TPU \u8bbe\u5907\u3002Arduino Nano 33 BLE Sense Tiny ML \u5957\u4ef6\u4e5f\u88ab\u8003\u8651\u7528\u4e8e\u6bd4\u8f83\uff0c\u4f46\u7531\u4e8e\u521d\u59cb\u4e0d\u517c\u5bb9\u6027\u4ee5\u53ca\u4e3a\u4e86\u53ca\u65f6\u5b8c\u6210\u672c\u7814\u7a76\uff0c\u6211\u4eec\u51b3\u5b9a\u5728\u672a\u6765\u7684\u5b9e\u9a8c\u4e2d\u518d\u5ba1\u67e5\u8be5\u5957\u4ef6\u3002||\n", "2410.04168": "|**2024-10-05**|[Robust Task-Oriented Communication Framework for Real-Time Collaborative Vision Perception](http://arxiv.org/abs/2410.04168)|null|Cooperative perception enhances sensing in multi-robot and vehicular networks by aggregating information from multiple agents, improving perception accuracy and range. However, mobility and non-rigid sensor mounts introduce extrinsic calibration errors, necessitating online calibration, which is complicated by limited overlap in sensing regions. Maintaining fresh information is crucial for timely and accurate sensing. To address calibration errors and ensure both perception accuracy and transmission timeliness, we propose a Robust Task-Oriented Communication framework (R-TOCOM) that optimizes calibration and feature transmission in both deployment and streaming phases. First, we formulate an Age of Perceived Targets (AoPT) minimization problem to capture information freshness. Then, in the deployment phase, we introduce a channel-aware self-calibration technique based on re-identification (Re-ID). This technique adaptively compresses key-point features according to channel capacities, effectively addressing calibration issues via spatial and temporal cross-camera correlations. In the streaming phase, we tackle the trade-off between bandwidth and inference accuracy by integrating an Information Bottleneck (IB)-based encoding method that adjusts video compression rates based on task relevance, thereby reducing communication overhead and latency. To mitigate performance degradation from packet loss, we introduce a priority network that filters corrupted features. Extensive studies demonstrate our framework outperforms five baselines, improving multiple object detection accuracy (MODA) by 25.49% and reducing communication costs by 51.36% under severe channel condition.||\n", "2410.08065": "|**2024-10-10**|[Dynamic Object Catching with Quadruped Robot Front Legs](http://arxiv.org/abs/2410.08065)|null|\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u56db\u8db3\u673a\u5668\u4eba\u7684\u524d\u817f\u5728\u5176\u540e\u817f\u7ad9\u7acb\u65f6\u8fdb\u884c\u52a8\u6001\u7269\u4f53\u6355\u6349\u7684\u6846\u67b6\u3002\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u8f68\u8ff9\u9884\u6d4b\u548c\u817f\u90e8\u63a7\u5236\uff0c\u4f7f\u56db\u8db3\u673a\u5668\u4eba\u80fd\u591f\u4f7f\u7528\u673a\u8f7d\u6444\u50cf\u5934\u89c6\u89c9\u68c0\u6d4b\u3001\u8ddf\u8e2a\u5e76\u6210\u529f\u6355\u6349\u629b\u63b7\u7269\u4f53\u3002\u5229\u7528\u5fae\u8c03\u540e\u7684 YOLOv8 \u6a21\u578b\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\u548c\u57fa\u4e8e\u56de\u5f52\u7684\u8f68\u8ff9\u9884\u6d4b\u6a21\u5757\uff0c\u56db\u8db3\u673a\u5668\u4eba\u8fed\u4ee3\u5730\u8c03\u6574\u5176\u524d\u817f\u4f4d\u7f6e\uff0c\u4ee5\u9884\u6d4b\u548c\u62e6\u622a\u7269\u4f53\u3002\u6355\u6349\u52a8\u4f5c\u5305\u62ec\u8bc6\u522b\u6700\u4f73\u6355\u6349\u4f4d\u7f6e\u3001\u4f7f\u7528\u7b1b\u5361\u5c14 PD \u63a7\u5236\u63a7\u5236\u524d\u817f\u4ee5\u53ca\u5728\u9002\u5f53\u7684\u65f6\u523b\u5408\u62e2\u53cc\u817f\u3002\u6211\u4eec\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e09\u79cd\u9009\u62e9\u6700\u4f73\u6355\u6349\u4f4d\u7f6e\u7684\u4e0d\u540c\u65b9\u6cd5\uff1a1\uff09\u5c06\u9884\u6d4b\u8f68\u8ff9\u4e0e\u5782\u76f4\u5e73\u9762\u76f8\u4ea4\uff1b2\uff09\u9009\u62e9\u9884\u6d4b\u8f68\u8ff9\u4e0a\u4e0e\u673a\u5668\u4eba\u817f\u90e8\u5728\u5176\u6807\u79f0\u4f4d\u7f6e\u7684\u4e2d\u5fc3\u8ddd\u79bb\u6700\u5c0f\u7684\u70b9\uff1b3\uff09\u9009\u62e9\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b (GMM) \u5bf9\u673a\u5668\u4eba\u53ef\u8fbe\u7a7a\u95f4\u5efa\u6a21\u7684\u9884\u6d4b\u8f68\u8ff9\u4e0a\u53ef\u80fd\u6027\u6700\u9ad8\u7684\u70b9\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u7cfb\u7edf\u5728\u5404\u79cd\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6355\u6349\u80fd\u529b\uff0c\u5176\u4e2d GMM \u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u6355\u6349\u6210\u529f\u7387\u8fbe\u5230 80%\u3002\u7cfb\u7edf\u8fd0\u884c\u7684\u89c6\u9891\u6f14\u793a\u53ef\u5728 https://youtu.be/sm7RdxRfIYg  \u627e\u5230\u3002||\n", "2410.07689": "|**2024-10-10**|[When the Small-Loss Trick is Not Enough: Multi-Label Image Classification with Noisy Labels Applied to CCTV Sewer Inspections](http://arxiv.org/abs/2410.07689)|null|\u62e5\u6709\u6570\u767e\u4e07\u516c\u91cc\u7ba1\u9053\u7684\u6c61\u6c34\u7ba1\u7f51\u7ef4\u62a4\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u9ad8\u6548\u7684\u95ed\u8def\u7535\u89c6\uff08CCTV\uff09\u68c0\u67e5\u3002\u8bb8\u591a\u57fa\u4e8e\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\u7684\u6709\u524d\u666f\u7684\u65b9\u6cd5\u90fd\u5229\u7528\u4e86\u5386\u53f2\u68c0\u67e5\u62a5\u544a\u6570\u636e\u5e93\u6765\u81ea\u52a8\u5316\u8fd9\u4e9b\u68c0\u67e5\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6570\u636e\u5e93\u4e2d\u6807\u7b7e\u566a\u58f0\u7684\u663e\u8457\u5b58\u5728\uff0c\u5c3d\u7ba1\u5df2\u4e3a\u4eba\u6240\u77e5\uff0c\u4f46\u5c1a\u672a\u5f97\u5230\u89e3\u51b3\u3002\u867d\u7136\u5927\u91cf\u7814\u7a76\u63a2\u7d22\u4e86\u5355\u6807\u7b7e\u5206\u7c7b\uff08SLC\uff09\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u4f46\u5f88\u5c11\u6709\u4eba\u5173\u6ce8\u591a\u6807\u7b7e\u5206\u7c7b\uff08MLC\uff09\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u9996\u5148\u8c03\u6574\u4e86\u4e09\u79cd\u6837\u672c\u9009\u62e9SLC\u65b9\u6cd5\uff08Co-teaching\u3001CoSELFIE\u548cDISC\uff09\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5df2\u88ab\u8bc1\u660e\u5bf9\u6807\u7b7e\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u57fa\u4e8e\u5c0f\u635f\u5931\u6280\u5de7\u7684\u6837\u672c\u9009\u62e9\u53ef\u4ee5\u5904\u7406\u590d\u6742\u7684\u6807\u7b7e\u566a\u58f0\uff0c\u4f46\u5b83\u4e0d\u662f\u6700\u4f18\u7684\u3002\u5c06\u6df7\u5408\u6837\u672c\u9009\u62e9\u65b9\u6cd5\u5e94\u7528\u4e8e\u566a\u58f0MLC\u4f3c\u4e4e\u662f\u4e00\u79cd\u66f4\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002\u9274\u4e8e\u6b64\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eCoSELFIE\u7684\u65b0\u65b9\u6cd5\uff0c\u79f0\u4e3aMHSS\uff08\u591a\u6807\u7b7e\u6df7\u5408\u6837\u672c\u9009\u62e9\uff09\u3002\u901a\u8fc7\u6df1\u5165\u7684\u6bd4\u8f83\u7814\u7a76\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5904\u7406\u5408\u6210\u590d\u6742\u566a\u58f0\u548c\u771f\u5b9e\u566a\u58f0\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u4ece\u800c\u6709\u52a9\u4e8e\u6301\u7eed\u52aa\u529b\u5b9e\u73b0CCTV\u6c61\u6c34\u7ba1\u9053\u68c0\u67e5\u7684\u6709\u6548\u81ea\u52a8\u5316\u3002||\n", "2410.07663": "|**2024-10-10**|[TDDSR: Single-Step Diffusion with Two Discriminators for Super Resolution](http://arxiv.org/abs/2410.07663)|null|\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u6b63\u8d8a\u6765\u8d8a\u591a\u5730\u9488\u5bf9\u73b0\u5b9e\u4e16\u754c\u548c\u7279\u5b9a\u4eba\u8138\u4efb\u52a1\u8fdb\u884c\u4e13\u95e8\u8bbe\u8ba1\u3002\u7136\u800c\uff0c\u8bb8\u591a\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u8fc7\u4e8e\u7b80\u5316\u7684\u9000\u5316\u6a21\u578b\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u6709\u6548\u5904\u7406\u590d\u6742\u548c\u672a\u77e5\u9000\u5316\u6a21\u5f0f\u7684\u80fd\u529b\u3002\u867d\u7136\u57fa\u4e8e\u6269\u6563\u7684\u8d85\u5206\u8fa8\u7387\u6280\u672f\u6700\u8fd1\u663e\u793a\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u53d7\u5230\u9700\u8981\u5927\u91cf\u63a8\u7406\u6b65\u9aa4\u7684\u9650\u5236\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 TDDSR\uff0c\u4e00\u79cd\u9ad8\u6548\u7684\u5355\u6b65\u6269\u6563\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u4ece\u9884\u8bad\u7ec3\u7684\u6559\u5e08\u6a21\u578b\u4e2d\u63d0\u53d6\uff0c\u5e76\u57fa\u4e8e\u6269\u6563\u7f51\u7edc\uff0c\u53ea\u9700\u4e00\u6b65\u5373\u53ef\u6267\u884c\u8d85\u5206\u8fa8\u7387\u3002\u5b83\u96c6\u6210\u4e86\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u4e0b\u91c7\u6837\u5668\u6765\u6355\u83b7\u4e0d\u540c\u7684\u9000\u5316\u6a21\u5f0f\uff0c\u5e76\u91c7\u7528\u4e86\u4e24\u4e2a\u9274\u522b\u5668\uff08\u4e00\u4e2a\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u4e00\u4e2a\u7528\u4e8e\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff09\u6765\u63d0\u9ad8\u6574\u4f53\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u73b0\u5b9e\u4e16\u754c\u548c\u7279\u5b9a\u4eba\u8138\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5176\u6027\u80fd\u4e0e\u53e6\u4e00\u79cd\u5355\u6b65\u65b9\u6cd5\u3001\u5148\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u548c\u6559\u5e08\u6a21\u578b\u76f8\u5f53\uff0c\u751a\u81f3\u66f4\u597d\u3002||\n", "2410.07613": "|**2024-10-10**|[Explainability of Deep Neural Networks for Brain Tumor Detection](http://arxiv.org/abs/2410.07613)|**[link](https://github.com/sunyoung98/Brain_Tumor_Detection_XAI)**|\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u5bf9\u4e8e\u652f\u6301\u533b\u7597\u4fdd\u5065\u4e13\u4e1a\u4eba\u5458\u8fdb\u884c\u51b3\u7b56\u548c\u57f9\u8bad\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u4f20\u7edf\u4e0a\u4e00\u76f4\u4e3b\u5bfc\u7740\u8be5\u9886\u57df\uff0c\u4f46\u57fa\u4e8e Transformer \u7684\u6a21\u578b\u6b63\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5e94\u7528\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd (XAI) \u6280\u672f\u6765\u8bc4\u4f30\u5404\u79cd\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u533b\u5b66\u6570\u636e\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u786e\u5b9a\u9700\u8981\u6539\u8fdb\u7684\u9886\u57df\u3002\u6211\u4eec\u5c06 VGG-16\u3001ResNet-50 \u548c EfficientNetV2L \u7b49 CNN \u6a21\u578b\u4e0e Transformer \u6a21\u578b ViT-Base-16 \u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u6570\u636e\u589e\u5f3a\u51e0\u4e4e\u6ca1\u6709\u5f71\u54cd\uff0c\u4f46\u8d85\u53c2\u6570\u8c03\u6574\u548c\u9ad8\u7ea7\u5efa\u6a21\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002CNN\uff0c\u7279\u522b\u662f VGG-16 \u548c ResNet-50\uff0c\u4f18\u4e8e ViT-Base-16 \u548c EfficientNetV2L\uff0c\u8fd9\u53ef\u80fd\u662f\u7531\u4e8e\u6570\u636e\u6709\u9650\u5bfc\u81f4\u7684\u6b20\u62df\u5408\u3002LIME \u548c SHAP \u7b49 XAI \u65b9\u6cd5\u8fdb\u4e00\u6b65\u8868\u660e\uff0c\u6027\u80fd\u66f4\u597d\u7684\u6a21\u578b\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u663e\u793a\u80bf\u7624\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u5177\u6709\u8f83\u6d45\u67b6\u6784\u7684 CNN \u5bf9\u4e8e\u5c0f\u578b\u6570\u636e\u96c6\u66f4\u6709\u6548\uff0c\u5e76\u4e14\u53ef\u4ee5\u652f\u6301\u533b\u7597\u51b3\u7b56\u3002||\n", "2410.07514": "|**2024-10-10**|[O1O: Grouping of Known Classes to Identify Unknown Objects as Odd-One-Out](http://arxiv.org/abs/2410.07514)|null|\u5728\u56fa\u5b9a\u5df2\u77e5\u7c7b\u522b\u96c6\u5408\u4e0a\u8bad\u7ec3\u7684\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u68c0\u6d4b\u672a\u77e5\u7c7b\u522b\u7684\u7269\u4f53\u3002\u76ee\u524d\u7684\u4fee\u590d\u65b9\u6cd5\u5305\u62ec\u6dfb\u52a0\u8fd1\u4f3c\u76d1\u7763\uff0c\u4f7f\u7528\u4e0e\u5019\u9009\u7269\u4f53\u4f4d\u7f6e\u76f8\u5bf9\u5e94\u7684\u4f2a\u6807\u7b7e\uff0c\u8fd9\u4e9b\u4f4d\u7f6e\u901a\u5e38\u4ee5\u7c7b\u522b\u65e0\u5173\u7684\u65b9\u5f0f\u83b7\u5f97\u3002\u867d\u7136\u5148\u524d\u7684\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u7269\u4f53\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u4f46\u6211\u4eec\u53d1\u73b0\u51e0\u4f55\u7ebf\u7d22\u53ef\u4ee5\u63d0\u9ad8\u672a\u77e5\u7269\u4f53\u7684\u53ec\u56de\u7387\u3002\u5c3d\u7ba1\u6765\u81ea\u4f2a\u6807\u7b7e\u7684\u989d\u5916\u76d1\u7763\u6709\u52a9\u4e8e\u68c0\u6d4b\u672a\u77e5\u7269\u4f53\uff0c\u4f46\u5b83\u4e5f\u4f1a\u7ed9\u5df2\u77e5\u7c7b\u522b\u5e26\u6765\u6df7\u6dc6\u3002\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u5728\u5b58\u5728\u566a\u58f0\u4f2a\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u68c0\u6d4b\u5df2\u77e5\u7269\u4f53\u7684\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u53d7\u4eba\u7c7b\u8ba4\u77e5\u7814\u7a76\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5efa\u8bae\u5c06\u5df2\u77e5\u7c7b\u522b\u5206\u7ec4\u5230\u8d85\u7c7b\u4e2d\u3002\u901a\u8fc7\u8bc6\u522b\u8d85\u7c7b\u4e2d\u7c7b\u522b\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u201c\u5f02\u7c7b\u6392\u9664\u201d\u8bc4\u5206\u673a\u5236\u8bc6\u522b\u672a\u77e5\u7c7b\u522b\u3002\u6211\u4eec\u5728\u5f00\u653e\u4e16\u754c\u68c0\u6d4b\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u6709\u4efb\u52a1\u7684\u672a\u77e5\u7269\u4f53\u53ec\u56de\u7387\u90fd\u6709\u663e\u8457\u63d0\u9ad8\u3002\u81f3\u5173\u91cd\u8981\u7684\u662f\uff0c\u7531\u4e8e\u901a\u8fc7\u8d85\u7c7b\u66f4\u597d\u5730\u5212\u5206\u4e86\u7279\u5f81\u7a7a\u95f4\uff0c\u6211\u4eec\u5728\u4e0d\u5f71\u54cd\u5df2\u77e5\u7269\u4f53\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u8fd9\u4e00\u70b9\u3002||\n", "2410.07475": "|**2024-10-09**|[Progressive Multi-Modal Fusion for Robust 3D Object Detection](http://arxiv.org/abs/2410.07475)|null|\u591a\u4f20\u611f\u5668\u878d\u5408\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7cbe\u786e\u7684 3D \u7269\u4f53\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u5176\u4e2d\u6444\u50cf\u5934\u548c\u6fc0\u5149\u96f7\u8fbe\u662f\u6700\u5e38\u7528\u7684\u4f20\u611f\u5668\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5c06\u4e24\u79cd\u6a21\u6001\u7684\u7279\u5f81\u6295\u5f71\u5230\u9e1f\u77b0\u56fe (BEV) \u6216\u900f\u89c6\u56fe (PV) \u4e2d\uff0c\u5728\u5355\u4e00\u89c6\u56fe\u4e2d\u8fdb\u884c\u4f20\u611f\u5668\u878d\u5408\uff0c\u4ece\u800c\u727a\u7272\u4e86\u8bf8\u5982\u9ad8\u5ea6\u6216\u51e0\u4f55\u6bd4\u4f8b\u7b49\u8865\u5145\u4fe1\u606f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 ProFusion3D\uff0c\u4e00\u79cd\u6e10\u8fdb\u5f0f\u878d\u5408\u6846\u67b6\uff0c\u5728\u4e2d\u95f4\u548c\u5bf9\u8c61\u67e5\u8be2\u7ea7\u522b\u7ed3\u5408\u4e86 BEV \u548c PV \u4e2d\u7684\u7279\u5f81\u3002\u6211\u4eec\u7684\u67b6\u6784\u5206\u5c42\u878d\u5408\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u589e\u5f3a\u4e86 3D \u7269\u4f53\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u63a9\u7801\u5efa\u6a21\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u4e09\u4e2a\u65b0\u9896\u7684\u76ee\u6807\u6765\u6539\u8fdb\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u548c\u6570\u636e\u6548\u7387\u3002\u5728 nuScenes \u548c Argoverse2 \u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u6700\u7ec8\u8bc1\u660e\u4e86 ProFusion3D \u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0cProFusion3D \u5bf9\u4f20\u611f\u5668\u6545\u969c\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5728\u4ec5\u6709\u4e00\u79cd\u6a21\u6001\u53ef\u7528\u7684\u60c5\u51b5\u4e0b\u4e5f\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u3002||\n", "2410.07442": "|**2024-10-09**|[Self-Supervised Learning for Real-World Object Detection: a Survey](http://arxiv.org/abs/2410.07442)|null|\u81ea\u76d1\u7763\u5b66\u4e60 (SSL) \u5df2\u6210\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u4e00\u79cd\u5f88\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7f51\u7edc\u80fd\u591f\u4ece\u5927\u578b\u672a\u6807\u8bb0\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u6709\u610f\u4e49\u7684\u8868\u793a\u3002SSL \u65b9\u6cd5\u4e3b\u8981\u5206\u4e3a\u4e24\u7c7b\uff1a\u5b9e\u4f8b\u5224\u522b\u548c\u63a9\u7801\u56fe\u50cf\u5efa\u6a21 (MIM)\u3002\u867d\u7136\u5b9e\u4f8b\u5224\u522b\u662f SSL \u7684\u57fa\u7840\uff0c\u4f46\u5b83\u6700\u521d\u662f\u4e3a\u5206\u7c7b\u4efb\u52a1\u8bbe\u8ba1\u7684\uff0c\u5bf9\u4e8e\u76ee\u6807\u68c0\u6d4b\uff0c\u5c24\u5176\u662f\u5c0f\u578b\u76ee\u6807\u68c0\u6d4b\uff0c\u6548\u679c\u53ef\u80fd\u4e0d\u4f73\u3002\u5728\u672c\u7efc\u8ff0\u4e2d\uff0c\u6211\u4eec\u91cd\u70b9\u5173\u6ce8\u4e13\u4e3a\u73b0\u5b9e\u4e16\u754c\u76ee\u6807\u68c0\u6d4b\u800c\u8bbe\u8ba1\u7684 SSL \u65b9\u6cd5\uff0c\u91cd\u70b9\u662f\u5728\u590d\u6742\u73af\u5883\u4e2d\u68c0\u6d4b\u5c0f\u578b\u76ee\u6807\u3002\u4e0e\u4ee5\u5f80\u7684\u7efc\u8ff0\u4e0d\u540c\uff0c\u6211\u4eec\u8be6\u7ec6\u6bd4\u8f83\u4e86 SSL \u7b56\u7565\uff0c\u5305\u62ec\u76ee\u6807\u7ea7\u5b9e\u4f8b\u5224\u522b\u548c MIM \u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e CNN \u548c ViT \u7684\u67b6\u6784\u8bc4\u4f30\u4e86\u5b83\u4eec\u5bf9\u5c0f\u578b\u76ee\u6807\u68c0\u6d4b\u7684\u6709\u6548\u6027\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u662f\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684 COCO \u6570\u636e\u96c6\u4ee5\u53ca\u4e13\u6ce8\u4e8e\u7ea2\u5916\u9065\u611f\u56fe\u50cf\u4e2d\u8f66\u8f86\u68c0\u6d4b\u7684\u4e13\u4e1a\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u3002\u6211\u4eec\u8fd8\u8bc4\u4f30\u4e86\u5728\u81ea\u5b9a\u4e49\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u67d0\u4e9b SSL \u7b56\u7565\u5982\u4f55\u66f4\u9002\u5408\u5904\u7406\u672a\u7ecf\u6574\u7406\u7684\u6570\u636e\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5b9e\u4f8b\u5224\u522b\u65b9\u6cd5\u5728\u57fa\u4e8e CNN \u7684\u7f16\u7801\u5668\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u800c MIM \u65b9\u6cd5\u66f4\u9002\u5408\u57fa\u4e8e ViT \u7684\u67b6\u6784\u548c\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u3002\u672c\u7efc\u8ff0\u4e3a\u9009\u62e9\u6700\u4f73 SSL \u7b56\u7565\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\uff0c\u5e76\u8003\u8651\u4e86\u4e3b\u5e72\u67b6\u6784\u3001\u76ee\u6807\u5927\u5c0f\u548c\u81ea\u5b9a\u4e49\u9884\u8bad\u7ec3\u8981\u6c42\u7b49\u56e0\u7d20\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bc1\u660e\uff0c\u9009\u62e9\u5408\u9002\u7684 SSL \u9884\u8bad\u7ec3\u7b56\u7565\u4ee5\u53ca\u5408\u9002\u7684\u7f16\u7801\u5668\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u73b0\u5b9e\u4e16\u754c\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e2d\u7684\u5c0f\u578b\u76ee\u6807\u68c0\u6d4b\u3002||\n", "2410.07437": "|**2024-10-09**|[Robust infrared small target detection using self-supervised and a contrario paradigms](http://arxiv.org/abs/2410.07437)|null|\u5728\u56fd\u9632\u5e94\u7528\u4e2d\uff0c\u7531\u4e8e\u590d\u6742\u80cc\u666f\u7684\u5b58\u5728\u548c\u76ee\u6807\u7684\u5c0f\u5c3a\u5bf8\uff0c\u7ea2\u5916\u56fe\u50cf\u4e2d\u7684\u5c0f\u76ee\u6807\u68c0\u6d4b\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u4f20\u7edf\u7684\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5f80\u5f80\u96be\u4ee5\u5728\u9ad8\u68c0\u6d4b\u7387\u548c\u4f4e\u8bef\u62a5\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5c0f\u76ee\u6807\u65f6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5c06\u201c\u53cd\u4e8b\u5b9e\u8303\u5f0f\u201d\u4e0e\u81ea\u76d1\u7763\u5b66\u4e60 (SSL) \u76f8\u7ed3\u5408\uff0c\u4ee5\u6539\u8fdb\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b (IRSTD)\u3002\u4e00\u65b9\u9762\uff0c\u5728 YOLO \u68c0\u6d4b\u5934\u4e2d\u96c6\u6210\u201c\u53cd\u4e8b\u5b9e\u51c6\u5219\u201d\u589e\u5f3a\u4e86\u5bf9\u5c0f\u578b\u548c\u610f\u5916\u76ee\u6807\u7684\u7279\u5f81\u56fe\u54cd\u5e94\uff0c\u540c\u65f6\u6709\u6548\u63a7\u5236\u4e86\u8bef\u62a5\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u6211\u4eec\u63a2\u7d22\u4e86 SSL \u6280\u672f\u6765\u514b\u670d IRSTD \u4efb\u52a1\u4e2d\u5e38\u89c1\u7684\u6ce8\u91ca\u6570\u636e\u6709\u9650\u7684\u6311\u6218\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5bf9\u51e0\u79cd\u5177\u6709\u4ee3\u8868\u6027\u7684 SSL \u7b56\u7565\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u4e86\u89e3\u5b83\u4eec\u5728\u63d0\u9ad8\u5c0f\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5b9e\u4f8b\u5224\u522b\u65b9\u6cd5\u5728\u5e94\u7528\u4e8e\u57fa\u4e8e YOLO \u7684\u5c0f\u76ee\u6807\u68c0\u6d4b\u65f6\u4f18\u4e8e\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u7b56\u7565\u3002\u6b64\u5916\uff0c\u201c\u53cd\u4e8b\u5b9e\u8303\u5f0f\u201d\u548c SSL \u8303\u5f0f\u7684\u7ed3\u5408\u5e26\u6765\u4e86\u663e\u7740\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7f29\u5c0f\u4e86\u4e0e\u6700\u5148\u8fdb\u7684\u5206\u5272\u65b9\u6cd5\u7684\u5dee\u8ddd\uff0c\u751a\u81f3\u5728\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e2d\u4e5f\u4f18\u4e8e\u5b83\u4eec\u3002\u8fd9\u79cd\u53cc\u7ba1\u9f50\u4e0b\u7684\u65b9\u6cd5\u4e3a\u63d0\u9ad8 IRSTD \u6027\u80fd\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6761\u4ef6\u4e0b\u3002||\n", "2410.07170": "|**2024-10-09**|[One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation](http://arxiv.org/abs/2410.07170)|**[link](https://github.com/ml-jku/EVA)**|\u57fa\u7840\u6a21\u578b (FM) \u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u9488\u5bf9\u7279\u5b9a\u5e94\u7528\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002\u6700\u6210\u529f\u548c\u6700\u5e38\u7528\u7684\u5fae\u8c03\u65b9\u6cd5\u662f\u901a\u8fc7\u4f4e\u79e9\u81ea\u9002\u5e94 (LoRA) \u66f4\u65b0\u9884\u8bad\u7ec3\u7684\u6743\u91cd\u3002LoRA \u5f15\u5165\u4e86\u65b0\u7684\u6743\u91cd\u77e9\u9635\uff0c\u8fd9\u4e9b\u77e9\u9635\u901a\u5e38\u4f7f\u7528\u8de8\u6a21\u578b\u6743\u91cd\u7684\u5747\u5300\u79e9\u5206\u5e03\u968f\u673a\u521d\u59cb\u5316\u3002\u6700\u8fd1\u7684\u5de5\u4f5c\u96c6\u4e2d\u5728\u6743\u91cd\u9a71\u52a8\u7684\u521d\u59cb\u5316\u6216\u5728\u8bad\u7ec3\u671f\u95f4\u5b66\u4e60\u81ea\u9002\u5e94\u79e9\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u53ea\u662f\u5b64\u7acb\u5730\u8fdb\u884c\u7814\u7a76\uff0c\u5bfc\u81f4\u6536\u655b\u901f\u5ea6\u6162\u6216\u79e9\u5206\u5e03\u5747\u5300\uff0c\u8fdb\u800c\u5bfc\u81f4\u6027\u80fd\u6b20\u4f73\u3002\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u4ee5\u6570\u636e\u9a71\u52a8\u7684\u65b9\u5f0f\u521d\u59cb\u5316\u65b0\u6743\u91cd\u6765\u589e\u5f3a LoRA\uff0c\u65b9\u6cd5\u662f\u5728\u5c0f\u6279\u91cf\u6fc0\u6d3b\u5411\u91cf\u4e0a\u8ba1\u7b97\u5947\u5f02\u503c\u5206\u89e3\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u83b7\u5f97\u7684\u53f3\u5947\u5f02\u5411\u91cf\u521d\u59cb\u5316 LoRA \u77e9\u9635\uff0c\u5e76\u5728\u6240\u6709\u6743\u91cd\u77e9\u9635\u4e4b\u95f4\u91cd\u65b0\u5206\u914d\u79e9\uff0c\u4ee5\u89e3\u91ca\u6700\u5927\u91cf\u7684\u65b9\u5dee\uff0c\u5e76\u7ee7\u7eed\u6807\u51c6\u7684 LoRA \u5fae\u8c03\u8fc7\u7a0b\u3002\u8fd9\u5bfc\u81f4\u4e86\u6211\u4eec\u7684\u65b0\u65b9\u6cd5\uff0c\u79f0\u4e3a\u89e3\u91ca\u65b9\u5dee\u81ea\u9002\u5e94 (EVA)\u3002\u6211\u4eec\u5c06 EVA \u5e94\u7528\u4e8e\u5404\u79cd\u5fae\u8c03\u4efb\u52a1\uff0c\u4ece\u8bed\u8a00\u751f\u6210\u548c\u7406\u89e3\u5230\u56fe\u50cf\u5206\u7c7b\u548c\u5f3a\u5316\u5b66\u4e60\u3002\u4e0e\u7ade\u4e89\u5bf9\u624b\u76f8\u6bd4\uff0cEVA \u8868\u73b0\u51fa\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u5728\u6bcf\u4e2a\u9886\u57df\u7684\u4f17\u591a\u4efb\u52a1\u4e2d\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u5e73\u5747\u5206\u6570\u3002||\n", "2410.07081": "|**2024-10-09**|[JPEG Inspired Deep Learning](http://arxiv.org/abs/2410.07081)|**[link](https://github.com/jpeginspireddl/jpeg-inspired-dl)**|\u5c3d\u7ba1\u4f20\u7edf\u4e0a\u8ba4\u4e3a\u6709\u635f\u56fe\u50cf\u538b\u7f29\uff08\u4f8b\u5982JPEG\u538b\u7f29\uff09\u4f1a\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u7684\u6027\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u4f46\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684JPEG\u538b\u7f29\u5b9e\u9645\u4e0a\u53ef\u4ee5\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u7684\u6027\u80fd\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86JPEG-DL\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u5728\u4efb\u4f55\u5e95\u5c42DNN\u67b6\u6784\u4e4b\u524d\u6dfb\u52a0\u4e86\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684JPEG\u538b\u7f29\u5c42\u3002\u4e3a\u4e86\u4f7fJPEG\u538b\u7f29\u4e2d\u7684\u91cf\u5316\u64cd\u4f5c\u53ef\u8bad\u7ec3\uff0c\u6211\u4eec\u5728JPEG\u5c42\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u5fae\u5206\u8f6f\u91cf\u5316\u5668\uff0c\u7136\u540e\u8054\u5408\u8bad\u7ec3\u91cf\u5316\u64cd\u4f5c\u548c\u5e95\u5c42DNN\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6807\u51c6\u6df1\u5ea6\u5b66\u4e60\u76f8\u6bd4\uff0cJPEG-DL\u5728\u5404\u79cd\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u5747\u53ef\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002\u7279\u522b\u662f\uff0c\u5728\u4e00\u4e9b\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\uff0cJPEG-DL\u53ef\u4ee5\u5c06\u9884\u6d4b\u7cbe\u5ea6\u63d0\u9ad8\u591a\u8fbe20.9%\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728https://github.com/JpegInspiredDl/JPEG-Inspired-DL.git\u83b7\u53d6\u3002||\n", "2410.08920": "|**2024-10-11**|[Efficient Hyperparameter Importance Assessment for CNNs](http://arxiv.org/abs/2410.08920)|null|Hyperparameter selection is an essential aspect of the machine learning pipeline, profoundly impacting models' robustness, stability, and generalization capabilities. Given the complex hyperparameter spaces associated with Neural Networks and the constraints of computational resources and time, optimizing all hyperparameters becomes impractical. In this context, leveraging hyperparameter importance assessment (HIA) can provide valuable guidance by narrowing down the search space. This enables machine learning practitioners to focus their optimization efforts on the hyperparameters with the most significant impact on model performance while conserving time and resources. This paper aims to quantify the importance weights of some hyperparameters in Convolutional Neural Networks (CNNs) with an algorithm called N-RReliefF, laying the groundwork for applying HIA methodologies in the Deep Learning field. We conduct an extensive study by training over ten thousand CNN models across ten popular image classification datasets, thereby acquiring a comprehensive dataset containing hyperparameter configuration instances and their corresponding performance metrics. It is demonstrated that among the investigated hyperparameters, the top five important hyperparameters of the CNN model are the number of convolutional layers, learning rate, dropout rate, optimizer and epoch.||\n", "2410.08769": "|**2024-10-11**|[Efficient Multi-Object Tracking on Edge Devices via Reconstruction-Based Channel Pruning](http://arxiv.org/abs/2410.08769)|null|The advancement of multi-object tracking (MOT) technologies presents the dual challenge of maintaining high performance while addressing critical security and privacy concerns. In applications such as pedestrian tracking, where sensitive personal data is involved, the potential for privacy violations and data misuse becomes a significant issue if data is transmitted to external servers. To mitigate these risks, processing data directly on an edge device, such as a smart camera, has emerged as a viable solution. Edge computing ensures that sensitive information remains local, thereby aligning with stringent privacy principles and significantly reducing network latency. However, the implementation of MOT on edge devices is not without its challenges. Edge devices typically possess limited computational resources, necessitating the development of highly optimized algorithms capable of delivering real-time performance under these constraints. The disparity between the computational requirements of state-of-the-art MOT algorithms and the capabilities of edge devices emphasizes a significant obstacle. To address these challenges, we propose a neural network pruning method specifically tailored to compress complex networks, such as those used in modern MOT systems. This approach optimizes MOT performance by ensuring high accuracy and efficiency within the constraints of limited edge devices, such as NVIDIA's Jetson Orin Nano. By applying our pruning method, we achieve model size reductions of up to 70% while maintaining a high level of accuracy and further improving performance on the Jetson Orin Nano, demonstrating the effectiveness of our approach for edge computing applications.||\n", "2410.08739": "|**2024-10-11**|[MMLF: Multi-modal Multi-class Late Fusion for Object Detection with Uncertainty Estimation](http://arxiv.org/abs/2410.08739)|null|Autonomous driving necessitates advanced object detection techniques that integrate information from multiple modalities to overcome the limitations associated with single-modal approaches. The challenges of aligning diverse data in early fusion and the complexities, along with overfitting issues introduced by deep fusion, underscore the efficacy of late fusion at the decision level. Late fusion ensures seamless integration without altering the original detector's network structure. This paper introduces a pioneering Multi-modal Multi-class Late Fusion method, designed for late fusion to enable multi-class detection. Fusion experiments conducted on the KITTI validation and official test datasets illustrate substantial performance improvements, presenting our model as a versatile solution for multi-modal object detection in autonomous driving. Moreover, our approach incorporates uncertainty analysis into the classification fusion process, rendering our model more transparent and trustworthy and providing more reliable insights into category predictions.||\n", "2410.08645": "|**2024-10-11**|[Boosting Open-Vocabulary Object Detection by Handling Background Samples](http://arxiv.org/abs/2410.08645)|null|Open-vocabulary object detection is the task of accurately detecting objects from a candidate vocabulary list that includes both base and novel categories. Currently, numerous open-vocabulary detectors have achieved success by leveraging the impressive zero-shot capabilities of CLIP. However, we observe that CLIP models struggle to effectively handle background images (i.e. images without corresponding labels) due to their language-image learning methodology. This limitation results in suboptimal performance for open-vocabulary detectors that rely on CLIP when processing background samples. In this paper, we propose Background Information Representation for open-vocabulary Detector (BIRDet), a novel approach to address the limitations of CLIP in handling background samples. Specifically, we design Background Information Modeling (BIM) to replace the single, fixed background embedding in mainstream open-vocabulary detectors with dynamic scene information, and prompt it into image-related background representations. This method effectively enhances the ability to classify oversized regions as background. Besides, we introduce Partial Object Suppression (POS), an algorithm that utilizes the ratio of overlap area to address the issue of misclassifying partial regions as foreground. Experiments on OV-COCO and OV-LVIS benchmarks demonstrate that our proposed model is capable of achieving performance enhancements across various open-vocabulary detectors.||\n", "2410.08582": "|**2024-10-11**|[DeBiFormer: Vision Transformer with Deformable Agent Bi-level Routing Attention](http://arxiv.org/abs/2410.08582)|**[link](https://github.com/maclong01/DeBiFormer)**|Vision Transformers with various attention modules have demonstrated superior performance on vision tasks. While using sparsity-adaptive attention, such as in DAT, has yielded strong results in image classification, the key-value pairs selected by deformable points lack semantic relevance when fine-tuning for semantic segmentation tasks. The query-aware sparsity attention in BiFormer seeks to focus each query on top-k routed regions. However, during attention calculation, the selected key-value pairs are influenced by too many irrelevant queries, reducing attention on the more important ones. To address these issues, we propose the Deformable Bi-level Routing Attention (DBRA) module, which optimizes the selection of key-value pairs using agent queries and enhances the interpretability of queries in attention maps. Based on this, we introduce the Deformable Bi-level Routing Attention Transformer (DeBiFormer), a novel general-purpose vision transformer built with the DBRA module. DeBiFormer has been validated on various computer vision tasks, including image classification, object detection, and semantic segmentation, providing strong evidence of its effectiveness.Code is available at {https://github.com/maclong01/DeBiFormer}||\n", "2410.08534": "|**2024-10-11**|[Quality Prediction of AI Generated Images and Videos: Emerging Trends and Opportunities](http://arxiv.org/abs/2410.08534)|null|The advent of AI has influenced many aspects of human life, from self-driving cars and intelligent chatbots to text-based image and video generation models capable of creating realistic images and videos based on user prompts (text-to-image, image-to-image, and image-to-video). AI-based methods for image and video super resolution, video frame interpolation, denoising, and compression have already gathered significant attention and interest in the industry and some solutions are already being implemented in real-world products and services. However, to achieve widespread integration and acceptance, AI-generated and enhanced content must be visually accurate, adhere to intended use, and maintain high visual quality to avoid degrading the end user's quality of experience (QoE).   One way to monitor and control the visual \"quality\" of AI-generated and -enhanced content is by deploying Image Quality Assessment (IQA) and Video Quality Assessment (VQA) models. However, most existing IQA and VQA models measure visual fidelity in terms of \"reconstruction\" quality against a pristine reference content and were not designed to assess the quality of \"generative\" artifacts. To address this, newer metrics and models have recently been proposed, but their performance evaluation and overall efficacy have been limited by datasets that were too small or otherwise lack representative content and/or distortion capacity; and by performance measures that can accurately report the success of an IQA/VQA model for \"GenAI\". This paper examines the current shortcomings and possibilities presented by AI-generated and enhanced image and video content, with a particular focus on end-user perceived quality. Finally, we discuss open questions and make recommendations for future work on the \"GenAI\" quality assessment problems, towards further progressing on this interesting and relevant field of research.||\n", "2410.08508": "|**2024-10-11**|[Accelerated Distributed Stochastic Non-Convex Optimization over Time-Varying Directed Networks](http://arxiv.org/abs/2410.08508)|null|Distributed stochastic non-convex optimization problems have recently received attention due to the growing interest of signal processing, computer vision, and natural language processing communities in applications deployed over distributed learning systems (e.g., federated learning). We study the setting where the data is distributed across the nodes of a time-varying directed network, a topology suitable for modeling dynamic networks experiencing communication delays and straggler effects. The network nodes, which can access only their local objectives and query a stochastic first-order oracle to obtain gradient estimates, collaborate to minimize a global objective function by exchanging messages with their neighbors. We propose an algorithm, novel to this setting, that leverages stochastic gradient descent with momentum and gradient tracking to solve distributed non-convex optimization problems over time-varying networks. To analyze the algorithm, we tackle the challenges that arise when analyzing dynamic network systems which communicate gradient acceleration components. We prove that the algorithm's oracle complexity is $\\mathcal{O}(1/\\epsilon^{1.5})$, and that under Polyak-$\\L$ojasiewicz condition the algorithm converges linearly to a steady error state. The proposed scheme is tested on several learning tasks: a non-convex logistic regression experiment on the MNIST dataset, an image classification task on the CIFAR-10 dataset, and an NLP classification test on the IMDB dataset. We further present numerical simulations with an objective that satisfies the PL condition. The results demonstrate superior performance of the proposed framework compared to the existing related methods.||\n", "2410.08417": "|**2024-10-10**|[Bilinear MLPs enable weight-based mechanistic interpretability](http://arxiv.org/abs/2410.08417)|**[link](https://github.com/tdooms/bilinear-decomposition)**|A mechanistic understanding of how MLPs do computation in deep neural networks remains elusive. Current interpretability work can extract features from hidden activations over an input dataset but generally cannot explain how MLP weights construct features. One challenge is that element-wise nonlinearities introduce higher-order interactions and make it difficult to trace computations through the MLP layer. In this paper, we analyze bilinear MLPs, a type of Gated Linear Unit (GLU) without any element-wise nonlinearity that nevertheless achieves competitive performance. Bilinear MLPs can be fully expressed in terms of linear operations using a third-order tensor, allowing flexible analysis of the weights. Analyzing the spectra of bilinear MLP weights using eigendecomposition reveals interpretable low-rank structure across toy tasks, image classification, and language modeling. We use this understanding to craft adversarial examples, uncover overfitting, and identify small language model circuits directly from the weights alone. Our results demonstrate that bilinear layers serve as an interpretable drop-in replacement for current activation functions and that weight-based interpretability is viable for understanding deep-learning models.||\n", "2410.08407": "|**2024-10-10**|[What is Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias](http://arxiv.org/abs/2410.08407)|null|Knowledge Distillation is a commonly used Deep Neural Network compression method, which often maintains overall generalization performance. However, we show that even for balanced image classification datasets, such as CIFAR-100, Tiny ImageNet and ImageNet, as many as 41% of the classes are statistically significantly affected by distillation when comparing class-wise accuracy (i.e. class bias) between a teacher/distilled student or distilled student/non-distilled student model. Changes in class bias are not necessarily an undesirable outcome when considered outside of the context of a model's usage. Using two common fairness metrics, Demographic Parity Difference (DPD) and Equalized Odds Difference (EOD) on models trained with the CelebA, Trifeature, and HateXplain datasets, our results suggest that increasing the distillation temperature improves the distilled student model's fairness -- for DPD, the distilled student even surpasses the fairness of the teacher model at high temperatures. This study highlights the uneven effects of Knowledge Distillation on certain classes and its potentially significant role in fairness, emphasizing that caution is warranted when using distilled models for sensitive application domains.||\n", "2410.08365": "|**2024-10-10**|[Are We Ready for Real-Time LiDAR Semantic Segmentation in Autonomous Driving?](http://arxiv.org/abs/2410.08365)|null|Within a perception framework for autonomous mobile and robotic systems, semantic analysis of 3D point clouds typically generated by LiDARs is key to numerous applications, such as object detection and recognition, and scene reconstruction. Scene semantic segmentation can be achieved by directly integrating 3D spatial data with specialized deep neural networks. Although this type of data provides rich geometric information regarding the surrounding environment, it also presents numerous challenges: its unstructured and sparse nature, its unpredictable size, and its demanding computational requirements. These characteristics hinder the real-time semantic analysis, particularly on resource-constrained hardware architectures that constitute the main computational components of numerous robotic applications. Therefore, in this paper, we investigate various 3D semantic segmentation methodologies and analyze their performance and capabilities for resource-constrained inference on embedded NVIDIA Jetson platforms. We evaluate them for a fair comparison through a standardized training protocol and data augmentations, providing benchmark results on the Jetson AGX Orin and AGX Xavier series for two large-scale outdoor datasets: SemanticKITTI and nuScenes.||\n", "2410.11774": "|**2024-10-15**|[Fractal Calibration for long-tailed object detection](http://arxiv.org/abs/2410.11774)|null|\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u9075\u5faa\u4e0d\u5e73\u8861\u7684\u5206\u5e03\uff0c\u8fd9\u5bf9\u7a00\u6709\u7c7b\u522b\u76ee\u6807\u68c0\u6d4b\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u6700\u8fd1\u7684\u7814\u7a76\u901a\u8fc7\u5f00\u53d1\u91cd\u65b0\u52a0\u6743\u548c\u91cd\u65b0\u91c7\u6837\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5229\u7528\u4e86\u6570\u636e\u96c6\u7684\u7c7b\u522b\u9891\u7387\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6280\u672f\u53ea\u5173\u6ce8\u9891\u7387\u7edf\u8ba1\uff0c\u800c\u5ffd\u7565\u4e86\u56fe\u50cf\u7a7a\u95f4\u4e2d\u7c7b\u522b\u7684\u5206\u5e03\uff0c\u4ece\u800c\u9057\u6f0f\u4e86\u91cd\u8981\u4fe1\u606f\u3002\u4e0e\u5b83\u4eec\u4e0d\u540c\u7684\u662f\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5206\u5f62\u6821\u51c6\uff08FRACAL\uff09\uff1a\u4e00\u79cd\u65b0\u7684\u7528\u4e8e\u957f\u5c3e\u76ee\u6807\u68c0\u6d4b\u7684\u540e\u6821\u51c6\u65b9\u6cd5\u3002FRACAL\u8bbe\u8ba1\u4e86\u4e00\u79cdlogit\u8c03\u6574\u65b9\u6cd5\uff0c\u5229\u7528\u5206\u5f62\u7ef4\u6570\u6765\u4f30\u8ba1\u7c7b\u522b\u5728\u56fe\u50cf\u7a7a\u95f4\u4e2d\u7684\u5747\u5300\u5206\u5e03\u7a0b\u5ea6\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u5b83\u4f7f\u7528\u5206\u5f62\u7ef4\u6570\u5bf9\u5747\u5300\u5206\u5e03\u7684\u7c7b\u522b\u9884\u6d4b\u6982\u7387\u8fdb\u884c\u53cd\u5411\u52a0\u6743\uff0c\u4ece\u800c\u5728\u4e24\u4e2a\u8f74\u4e0a\u5b9e\u73b0\u5e73\u8861\uff1a\u9891\u7e41\u7c7b\u522b\u548c\u7a00\u6709\u7c7b\u522b\u4e4b\u95f4\uff0c\u4ee5\u53ca\u5747\u5300\u5206\u5e03\u7c7b\u522b\u548c\u7a00\u758f\u5206\u5e03\u7c7b\u522b\u4e4b\u95f4\u3002FRACAL\u662f\u4e00\u79cd\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u5b83\u4e0d\u9700\u8981\u4efb\u4f55\u8bad\u7ec3\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u8bb8\u591a\u73b0\u6210\u7684\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u4f8b\u5982\u4e00\u7ea7sigmoid\u68c0\u6d4b\u5668\u548c\u4e24\u7ea7\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u3002FRACAL\u5c06\u7a00\u6709\u7c7b\u522b\u7684\u6027\u80fd\u63d0\u9ad8\u4e868.6%\uff0c\u5e76\u5728LVIS\u6570\u636e\u96c6\u4e0a\u8d85\u8fc7\u4e86\u6240\u6709\u4ee5\u524d\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u5176\u4ed6\u6570\u636e\u96c6\uff08\u5982COCO\u3001V3Det\u548cOpenImages\uff09\u4e0a\u4e5f\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4ee3\u7801\u5c06\u88ab\u53d1\u5e03\u3002||\n", "2410.11727": "|**2024-10-15**|[YOLO-ELA: Efficient Local Attention Modeling for High-Performance Real-Time Insulator Defect Detection](http://arxiv.org/abs/2410.11727)|null|\u73b0\u6709\u7684\u65e0\u4eba\u673a\u7edd\u7f18\u5b50\u7f3a\u9677\u68c0\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u80cc\u666f\u548c\u5c0f\u578b\u76ee\u6807\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u5bfc\u81f4\u7cbe\u5ea6\u6b20\u4f73\u548c\u8bef\u62a5\u7387\u9ad8\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u57fa\u4e8e\u5c40\u90e8\u6ce8\u610f\u529b\u5efa\u6a21\u7684\u6982\u5ff5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u57fa\u7840\u67b6\u6784YOLO-ELA\u3002\u8be5\u67b6\u6784\u5728\u5355\u9636\u6bb5YOLOv8\u67b6\u6784\u7684\u9888\u90e8\u6dfb\u52a0\u4e86\u9ad8\u6548\u5c40\u90e8\u6ce8\u610f\u529b\uff08ELA\uff09\u6a21\u5757\uff0c\u5c06\u6a21\u578b\u7684\u6ce8\u610f\u529b\u4ece\u80cc\u666f\u7279\u5f81\u8f6c\u79fb\u5230\u7f3a\u9677\u7edd\u7f18\u5b50\u7279\u5f81\u3002\u91c7\u7528SCYLLA Intersection-Over-Union\uff08SIoU\uff09\u51c6\u5219\u51fd\u6570\u6765\u51cf\u5c11\u68c0\u6d4b\u635f\u5931\uff0c\u52a0\u901f\u6a21\u578b\u6536\u655b\uff0c\u5e76\u63d0\u9ad8\u6a21\u578b\u5bf9\u5c0f\u578b\u7edd\u7f18\u5b50\u7f3a\u9677\u7684\u654f\u611f\u6027\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u9ad8\u7684\u771f\u9633\u6027\u7ed3\u679c\u3002\u7531\u4e8e\u6570\u636e\u96c6\u6709\u9650\uff0c\u6211\u4eec\u5229\u7528\u6570\u636e\u589e\u5f3a\u6280\u672f\u6765\u589e\u52a0\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u3002\u5728\u9ad8\u5206\u8fa8\u7387\u65e0\u4eba\u673a\u56fe\u50cf\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0cmAP0.5\u4e3a96.9%\uff0c\u5b9e\u65f6\u68c0\u6d4b\u901f\u5ea6\u4e3a\u6bcf\u79d274.63\u5e27\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002\u8fd9\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5728\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002||\n", "2410.11666": "|**2024-10-15**|[Degradation Oriented and Regularized Network for Real-World Depth Super-Resolution](http://arxiv.org/abs/2410.11666)|null|\u8fd1\u5e74\u6765\uff0c\u73b0\u6709\u7684RGB\u5f15\u5bfc\u7684\u6df1\u5ea6\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728\u56fa\u5b9a\u548c\u5df2\u77e5\u9000\u5316\uff08\u4f8b\u5982\uff0c\u53cc\u4e09\u6b21\u4e0b\u91c7\u6837\uff09\u7684\u5047\u8bbe\u4e0b\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002 \u7136\u800c\uff0c\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u7531\u4e8e\u4f20\u611f\u5668\u9650\u5236\u548c\u6210\u50cf\u73af\u5883\u7684\u590d\u6742\u6027\uff08\u4f8b\u5982\uff0c\u4f4e\u53cd\u5c04\u8868\u9762\u3001\u7167\u660e\uff09\uff0c\u6355\u83b7\u7684\u6df1\u5ea6\u5f80\u5f80\u4f1a\u51fa\u73b0\u975e\u5e38\u89c4\u548c\u672a\u77e5\u7684\u9000\u5316\u3002 \u5f53\u8fd9\u4e9b\u771f\u5b9e\u9000\u5316\u4e0e\u5176\u5047\u8bbe\u4e0d\u540c\u65f6\uff0c\u5b83\u4eec\u7684\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\u3002 \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u9000\u5316\u548c\u6b63\u5219\u5316\u7684\u7f51\u7edcDORNet\uff0c\u5b83\u66f4\u52a0\u5173\u6ce8\u5b66\u4e60\u4f4e\u5206\u8fa8\u7387\u6df1\u5ea6\u7684\u9000\u5316\u8868\u793a\uff0c\u4ece\u800c\u4e3a\u6df1\u5ea6\u6062\u590d\u63d0\u4f9b\u6709\u9488\u5bf9\u6027\u7684\u6307\u5bfc\u3002 \u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u8bbe\u8ba1\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u9000\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u57fa\u4e8e\u8def\u7531\u9009\u62e9\u7684\u9000\u5316\u6b63\u5219\u5316\u6765\u6a21\u62df\u4f4e\u5206\u8fa8\u7387\u6df1\u5ea6\u7684\u5224\u522b\u6027\u9000\u5316\u8868\u793a\u3002 \u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9000\u5316\u611f\u77e5\u65b9\u6cd5\uff0c\u9012\u5f52\u5730\u8fdb\u884c\u591a\u4e2a\u9762\u5411\u9000\u5316\u7684\u7279\u5f81\u53d8\u6362\uff0c\u6bcf\u4e2a\u53d8\u6362\u90fd\u6839\u636e\u5b66\u4e60\u5230\u7684\u9000\u5316\u8868\u793a\u9009\u62e9\u6027\u5730\u5c06RGB\u4fe1\u606f\u5d4c\u5165\u5230\u6df1\u5ea6\u4e2d\u3002 \u5728\u771f\u5b9e\u6570\u636e\u96c6\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002||\n", "2410.11551": "|**2024-10-15**|[LoKO: Low-Rank Kalman Optimizer for Online Fine-Tuning of Large Models](http://arxiv.org/abs/2410.11551)|null|\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u5177\u6709\u6570\u767e\u4e07\u751a\u81f3\u6570\u5341\u4ebf\u53c2\u6570\u7684\u5927\u578b\u6a21\u578b\u4f1a\u4ea7\u751f\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\u3002\u53c2\u6570\u9ad8\u6548\u5fae\u8c03 (PEFT) \u65b9\u6cd5\uff0c\u7279\u522b\u662f\u4f4e\u79e9\u81ea\u9002\u5e94 (LoRA)\uff0c\u901a\u8fc7\u4ec5\u4f7f\u5c11\u91cf\u53c2\u6570\u9002\u5e94\u57fa\u4e8e\u68af\u5ea6\u4f18\u5316\u5668\u7684\u7279\u5b9a\u4efb\u52a1\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c06 PEFT \u8f6c\u6362\u4e3a\u6700\u4f18\u6ee4\u6ce2/\u72b6\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4f4e\u79e9\u5361\u5c14\u66fc\u4f18\u5316\u5668 (LoKO) \u4ee5\u5728\u7ebf\u65b9\u5f0f\u4f30\u8ba1\u6700\u4f18\u53ef\u8bad\u7ec3\u53c2\u6570\u3002\u6211\u4eec\u5229\u7528 LoRA \u4e2d\u7684\u4f4e\u79e9\u5206\u89e3\u6765\u663e\u7740\u51cf\u5c11\u5361\u5c14\u66fc\u8fed\u4ee3\u4e2d\u7684\u77e9\u9635\u5927\u5c0f\uff0c\u5e76\u8fdb\u4e00\u6b65\u5229\u7528\u534f\u65b9\u5dee\u77e9\u9635\u7684\u5bf9\u89d2\u8fd1\u4f3c\u6765\u6709\u6548\u5730\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u7684\u4e8c\u6b21\u65b9\u964d\u4f4e\u5230\u7ebf\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\u5361\u5c14\u66fc\u7b97\u6cd5\u4e2d\u534f\u65b9\u5dee\u77e9\u9635\u7684\u521d\u59cb\u5316\u548c\u89c2\u6d4b\u566a\u58f0\u534f\u65b9\u5dee\u7684\u51c6\u786e\u4f30\u8ba1\u662f\u8be5\u516c\u5f0f\u7684\u5173\u952e\uff0c\u5e76\u4e14\u6211\u4eec\u63d0\u51fa\u4e86\u5728\u5404\u79cd\u6210\u719f\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u4e2d\u90fd\u80fd\u5f88\u597d\u5730\u5de5\u4f5c\u7684\u9c81\u68d2\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u56fe\u50cf\u5206\u7c7b\u548c\u8bed\u8a00\u4efb\u52a1\u4e2d LoRA \u5e38\u7528\u7684\u4f18\u5316\u5668\u76f8\u6bd4\uff0cLoKO \u4ee5\u66f4\u5c11\u7684\u8fed\u4ee3\u6b21\u6570\u6536\u655b\u5e76\u4ea7\u751f\u66f4\u597d\u7684\u6027\u80fd\u6a21\u578b\u3002\u6211\u4eec\u7684\u7814\u7a76\u5f00\u8f9f\u4e86\u5229\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u4f5c\u4e3a\u5728\u7ebf\u5fae\u8c03\u5927\u578b\u6a21\u578b\u7684\u6709\u6548\u4f18\u5316\u5668\u7684\u53ef\u80fd\u6027\u3002||\n", "2410.11506": "|**2024-10-15**|[Spatio-Temporal Distortion Aware Omnidirectional Video Super-Resolution](http://arxiv.org/abs/2410.11506)|**[link](https://github.com/nichenxingmeng/STDAN)**|\u5168\u5411\u89c6\u9891\uff08ODV\uff09\u53ef\u4ee5\u63d0\u4f9b\u6c89\u6d78\u5f0f\u4f53\u9a8c\uff0c\u5e76\u5e7f\u6cdb\u5e94\u7528\u4e8e\u865a\u62df\u73b0\u5b9e\u548c\u589e\u5f3a\u73b0\u5b9e\u9886\u57df\u3002\u7136\u800c\uff0c\u53d7\u9650\u7684\u91c7\u96c6\u8bbe\u5907\u548c\u4f20\u8f93\u5e26\u5bbd\u5bfc\u81f4ODV\u5206\u8fa8\u7387\u8f83\u4f4e\u3002\u89c6\u9891\u8d85\u5206\u8fa8\u7387\uff08VSR\uff09\u65b9\u6cd5\u88ab\u63d0\u51fa\u7528\u4e8e\u63d0\u9ad8\u89c6\u9891\u7684\u5206\u8fa8\u7387\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u6b64\u7c7b\u65b9\u6cd5\u5e76\u4e0d\u80fd\u5f88\u597d\u5730\u89e3\u51b3\u5e94\u7528\u4e2dODV\u6295\u5f71\u5931\u771f\u95ee\u9898\u3002\u4e3a\u4e86\u83b7\u5f97\u66f4\u597d\u7684\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u8d28\u91cf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411ODV\u7279\u6027\u7684\u65b0\u578b\u65f6\u7a7a\u5931\u771f\u611f\u77e5\u7f51\u7edc\uff08STDAN\uff09\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u65f6\u7a7a\u5931\u771f\u8c03\u5236\u6a21\u5757\uff0c\u4ee5\u6839\u636e\u5e27\u5185\u548c\u5e27\u95f4\u5bf9\u9f50\u6765\u6539\u5584\u7a7a\u95f4ODV\u6295\u5f71\u5931\u771f\u5e76\u5229\u7528\u65f6\u95f4\u76f8\u5173\u6027\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u591a\u5e27\u91cd\u5efa\u548c\u878d\u5408\u673a\u5236\uff0c\u4ee5\u6539\u8fdb\u91cd\u5efaODV\u5e27\u7684\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728\u635f\u5931\u51fd\u6570\u4e2d\u52a0\u5165\u4e86\u7eac\u5ea6\u663e\u8457\u6027\u81ea\u9002\u5e94\u6620\u5c04\uff0c\u4ee5\u4e13\u6ce8\u4e8e\u5177\u6709\u66f4\u9ad8\u7eb9\u7406\u590d\u6742\u5ea6\u548c\u4eba\u7c7b\u89c2\u770b\u5174\u8da3\u7684\u91cd\u8981\u89c6\u70b9\u533a\u57df\u3002\u6b64\u5916\uff0c\u6211\u4eec\u6536\u96c6\u4e86\u4e00\u4e2a\u5305\u542b\u5404\u79cd\u573a\u666f\u7684\u65b0ODV-SR\u6570\u636e\u96c6\u3002\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684STDAN\u5728ODV\u4e0a\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u8d85\u5206\u8fa8\u7387\u6027\u80fd\uff0c\u5e76\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002||\n", "2410.11358": "|**2024-10-15**|[SeaDATE: Remedy Dual-Attention Transformer with Semantic Alignment via Contrast Learning for Multimodal Object Detection](http://arxiv.org/abs/2410.11358)|null|\u591a\u6a21\u6001\u76ee\u6807\u68c0\u6d4b\u5229\u7528\u591a\u79cd\u6a21\u6001\u4fe1\u606f\u6765\u63d0\u9ad8\u68c0\u6d4b\u5668\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u5b66\u4e60\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\uff0cTransformer\u53ef\u4ee5\u5728\u7279\u5f81\u63d0\u53d6\u9636\u6bb5\u6709\u6548\u5730\u878d\u5408\u591a\u6a21\u6001\u7279\u5f81\uff0c\u4ece\u800c\u5927\u5927\u63d0\u9ad8\u591a\u6a21\u6001\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u65b9\u6cd5\u4ec5\u4ec5\u662f\u5806\u53e0Transformer\u5f15\u5bfc\u7684\u878d\u5408\u6280\u672f\uff0c\u800c\u6ca1\u6709\u63a2\u7d22\u5176\u5728\u7f51\u7edc\u4e0d\u540c\u6df1\u5ea6\u5c42\u63d0\u53d6\u7279\u5f81\u7684\u80fd\u529b\uff0c\u4ece\u800c\u9650\u5236\u4e86\u68c0\u6d4b\u6027\u80fd\u7684\u63d0\u5347\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aSeaDATE\u7684\u7cbe\u786e\u9ad8\u6548\u7684\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u91cd\u6ce8\u610f\u529b\u7279\u5f81\u878d\u5408\uff08DTF\uff09\u6a21\u5757\uff0c\u5728Transformer\u7684\u5f15\u5bfc\u4e0b\uff0c\u901a\u8fc7\u53cc\u91cd\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\uff0c\u5229\u7528\u7a7a\u95f4\u548c\u901a\u9053token\u4ece\u6b63\u4ea4\u89d2\u5ea6\u52a0\u5f3a\u6a21\u6001\u7279\u5f81\u7684\u878d\u5408\u3002\u540c\u65f6\uff0c\u6211\u4eec\u7684\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u8868\u660e\uff0c\u5c06\u56fe\u50cf\u89c6\u4e3a\u50cf\u7d20\u5e8f\u5217\u8fdb\u884c\u878d\u5408\u7684Transformer\u5f15\u5bfc\u878d\u5408\u65b9\u6cd5\uff0c\u5728\u6d45\u5c42\u7279\u5f81\u7684\u7ec6\u8282\u4fe1\u606f\u65b9\u9762\u6bd4\u6df1\u5ea6\u8bed\u4e49\u4fe1\u606f\u8868\u73b0\u66f4\u597d\u3002\u9488\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5bf9\u6bd4\u5b66\u4e60\uff08CL\uff09\u6a21\u5757\uff0c\u65e8\u5728\u5b66\u4e60\u591a\u6a21\u6001\u6837\u672c\u7684\u7279\u5f81\uff0c\u5f25\u8865Transformer\u5f15\u5bfc\u878d\u5408\u5728\u63d0\u53d6\u6df1\u5ea6\u8bed\u4e49\u7279\u5f81\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u6709\u6548\u5730\u5229\u7528\u8de8\u6a21\u6001\u4fe1\u606f\u3002\u5728FLIR\u3001LLVIP\u548cM3FD\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u6027\u80fd\u3002||\n", "2410.11233": "|**2024-10-15**|[Representation Similarity: A Better Guidance of DNN Layer Sharing for Edge Computing without Training](http://arxiv.org/abs/2410.11233)|null|\u8fb9\u7f18\u8ba1\u7b97\u5df2\u7ecf\u6210\u4e3a\u4e00\u79cd\u51cf\u5c11\u4f20\u8f93\u548c\u5904\u7406\u5ef6\u8fdf\u5e76\u4fdd\u62a4\u89c6\u9891\u6d41\u9690\u79c1\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u57fa\u4e8e\u89c6\u9891\u7684\u5e94\u7528\u7a0b\u5e8f\uff08\u4f8b\u5982\u76ee\u6807\u68c0\u6d4b\uff09\u4e2d\u4f7f\u7528\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (DNN) \u65e5\u76ca\u590d\u6742\uff0c\u8fd9\u7ed9\u5185\u5b58\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u5e26\u6765\u4e86\u538b\u529b\u3002\u6a21\u578b\u5408\u5e76\u88ab\u63d0\u51fa\u901a\u8fc7\u5728\u5185\u5b58\u4e2d\u4ec5\u4fdd\u7559\u5408\u5e76\u5c42\u6743\u91cd\u7684\u4e00\u4e2a\u526f\u672c\uff0c\u6765\u51cf\u5c11 DNN \u7684\u5185\u5b58\u5360\u7528\u3002\u5728\u73b0\u6709\u7684\u6a21\u578b\u5408\u5e76\u6280\u672f\u4e2d\uff0c(i) \u53ea\u6709\u67b6\u6784\u76f8\u540c\u7684\u5c42\u624d\u80fd\u5171\u4eab\uff1b(ii) \u9700\u8981\u5728\u4e91\u4e2d\u8fdb\u884c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u91cd\u65b0\u8bad\u7ec3\uff1b(iii) \u5047\u8bbe\u53ef\u83b7\u5f97\u7528\u4e8e\u91cd\u65b0\u8bad\u7ec3\u7684\u771f\u5b9e\u6570\u636e\u3002\u7136\u800c\uff0c\u91cd\u65b0\u8bc4\u4f30\u5408\u5e76\u6a21\u578b\u7684\u6027\u80fd\u9700\u8981\u5177\u6709\u771f\u5b9e\u6570\u636e\u7684\u9a8c\u8bc1\u6570\u636e\u96c6\uff0c\u901a\u5e38\u5728\u4e91\u4e2d\u8fd0\u884c\u3002\u6307\u5bfc\u9009\u62e9\u5171\u4eab\u5c42\u7684\u5e38\u7528\u6307\u6807\u5305\u62ec\u5171\u4eab\u5c42\u7684\u5927\u5c0f\u6216\u8ba1\u7b97\u6210\u672c\u6216\u8868\u793a\u5927\u5c0f\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u5408\u5e76\u65b9\u6848\uff0c\u901a\u8fc7\u5728\u8fb9\u7f18\u5171\u4eab\u8868\u793a\uff08\u5373\u5c42\u7684\u8f93\u51fa\uff09\uff0c\u5e76\u4ee5\u8868\u793a\u76f8\u4f3c\u5ea6 S \u4e3a\u6307\u5bfc\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u4e0e\u5176\u4ed6\u6307\u6807\u76f8\u6bd4\uff0cS \u4e0e\u5408\u5e76\u6a21\u578b\u7684\u51c6\u786e\u6027\u5177\u6709\u6781\u9ad8\u7684\u76f8\u5173\u6027\uff0cPearson \u76f8\u5173\u7cfb\u6570|r|\n", "2410.11228": "|**2024-10-15**|[TEOcc: Radar-camera Multi-modal Occupancy Prediction via Temporal Enhancement](http://arxiv.org/abs/2410.11228)|**[link](https://github.com/vdigpku/teocc)**|\u8bed\u4e49\u5360\u7528\u4f5c\u4e3a\u4e00\u79cd\u65b0\u9896\u7684\u4e09\u7ef4\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u53d7\u5230\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5360\u7528\u9884\u6d4b\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u4e8e\u8bbe\u8ba1\u66f4\u597d\u7684\u5360\u7528\u8868\u793a\u65b9\u6cd5\uff0c\u4f8b\u5982\u4e09\u89c6\u89d2\u6216\u795e\u7ecf\u8f90\u5c04\u573a\uff0c\u800c\u5ffd\u7565\u4e86\u5229\u7528\u957f\u671f\u65f6\u95f4\u4fe1\u606f\u7684\u4f18\u52bf\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f7\u8fbe-\u76f8\u673a\u591a\u6a21\u6001\u65f6\u95f4\u589e\u5f3a\u5360\u7528\u9884\u6d4b\u7f51\u7edc\uff0c\u79f0\u4e3aTEOcc\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u53d7\u5230\u4e09\u7ef4\u76ee\u6807\u68c0\u6d4b\u4e2d\u5229\u7528\u65f6\u95f4\u4fe1\u606f\u53d6\u5f97\u6210\u529f\u7684\u542f\u53d1\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65f6\u95f4\u589e\u5f3a\u5206\u652f\u6765\u5b66\u4e60\u65f6\u95f4\u5360\u7528\u9884\u6d4b\u3002\u5728\u8fd9\u4e2a\u5206\u652f\u4e2d\uff0c\u6211\u4eec\u968f\u673a\u4e22\u5f03\u591a\u89c6\u89d2\u76f8\u673a\u7684\u7b2ct-k\u5e27\u8f93\u5165\uff0c\u5e76\u5229\u7528\u5176\u4ed6\u76f8\u90bb\u5e27\u548c\u591a\u6a21\u6001\u8f93\u5165\u7684\u4fe1\u606f\uff0c\u5206\u522b\u901a\u8fc7\u957f\u671f\u548c\u77ed\u671f\u65f6\u95f4\u89e3\u7801\u5668\u9884\u6d4b\u5176\u4e09\u7ef4\u5360\u7528\u60c5\u51b5\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u878d\u5408\u591a\u6a21\u6001\u8f93\u5165\uff0c\u6211\u4eec\u9488\u5bf9\u957f\u671f\u548c\u77ed\u671f\u65f6\u95f4\u89e3\u7801\u5668\u4e13\u95e8\u8bbe\u8ba1\u4e86\u4e09\u7ef4\u5377\u79ef\u5c42\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u8f7b\u91cf\u7ea7\u5360\u7528\u9884\u6d4b\u5934\u662f\u4e00\u4e2a\u5bc6\u96c6\u5206\u7c7b\u5934\uff0c\u6211\u4eec\u5efa\u8bae\u5bf9\u65f6\u95f4\u589e\u5f3a\u5206\u652f\u548c\u4e3b\u5206\u652f\u4f7f\u7528\u5171\u4eab\u7684\u5360\u7528\u9884\u6d4b\u5934\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u65f6\u95f4\u589e\u5f3a\u5206\u652f\u4ec5\u5728\u8bad\u7ec3\u671f\u95f4\u6267\u884c\uff0c\u5728\u63a8\u7406\u671f\u95f4\u88ab\u4e22\u5f03\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTEOcc\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5360\u7528\u9884\u6d4b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u65f6\u95f4\u589e\u5f3a\u5206\u652f\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u6a21\u5757\uff0c\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u96c6\u6210\u5230\u73b0\u6709\u7684\u5360\u7528\u9884\u6d4b\u65b9\u6cd5\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u5360\u7528\u9884\u6d4b\u7684\u6027\u80fd\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5728https://github.com/VDIGPKU/TEOcc\u53d1\u5e03\u3002||\n", "2410.11211": "|**2024-10-15**|[CVCP-Fusion: On Implicit Depth Estimation for 3D Bounding Box Prediction](http://arxiv.org/abs/2410.11211)|**[link](https://github.com/safetylab24/FusionCVCP)**|\u6fc0\u5149\u96f7\u8fbe\u548c\u6444\u50cf\u5934\u89c6\u56fe\u6570\u636e\u7684\u7ed3\u5408\u5df2\u6210\u4e3a3D\u76ee\u6807\u68c0\u6d4b\u7684\u5e38\u7528\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u4ee5\u5f80\u7684\u65b9\u6cd5\u5728\u70b9\u7ea7\u522b\u4e0a\u878d\u5408\u4e24\u79cd\u8f93\u5165\u6d41\uff0c\u4e22\u5f03\u4e86\u4ece\u6444\u50cf\u5934\u7279\u5f81\u4e2d\u63d0\u53d6\u7684\u8bed\u4e49\u4fe1\u606f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8de8\u89c6\u56fe\u4e2d\u5fc3\u70b9\u878d\u5408\uff08Cross-View Center Point-Fusion\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u8fc7\u5728BEV\u7a7a\u95f4\u4e2d\u878d\u5408\u6444\u50cf\u5934\u548c\u6fc0\u5149\u96f7\u8fbe\u884d\u751f\u7279\u5f81\u6765\u6267\u884c3D\u76ee\u6807\u68c0\u6d4b\u7684\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5b83\u5728\u878d\u5408\u6fc0\u5149\u96f7\u8fbe\u7684\u7a7a\u95f4\u6570\u636e\u7684\u540c\u65f6\u4fdd\u7559\u4e86\u6765\u81ea\u6444\u50cf\u5934\u6d41\u7684\u8bed\u4e49\u5bc6\u5ea6\u3002\u6211\u4eec\u7684\u67b6\u6784\u5229\u7528\u4e86\u5148\u524d\u5df2\u5efa\u7acb\u7684\u7b97\u6cd5\uff08\u8de8\u89c6\u56feTransformer\u548cCenterPoint\uff09\u7684\u5404\u4e2a\u65b9\u9762\uff0c\u5e76\u5e76\u884c\u8fd0\u884c\u5b83\u4eec\u7684\u4e3b\u5e72\u7f51\u7edc\uff0c\u4ece\u800c\u5b9e\u73b0\u5b9e\u65f6\u5904\u7406\u548c\u5e94\u7528\u7684\u9ad8\u6548\u8ba1\u7b97\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u867d\u7136\u9690\u5f0f\u8ba1\u7b97\u7684\u6df1\u5ea6\u4f30\u8ba1\u57282D\u5730\u56fe\u89c6\u56fe\u8868\u793a\u4e2d\u53ef\u80fd\u8db3\u591f\u51c6\u786e\uff0c\u4f46\u57283D\u4e16\u754c\u89c6\u56fe\u7a7a\u95f4\u4e2d\u8fdb\u884c\u7cbe\u786e\u7684\u8fb9\u754c\u6846\u9884\u6d4b\u9700\u8981\u663e\u5f0f\u8ba1\u7b97\u7684\u51e0\u4f55\u548c\u7a7a\u95f4\u4fe1\u606f\u3002||\n", "2410.11187": "|**2024-10-15**|[Multiview Scene Graph](http://arxiv.org/abs/2410.11187)|null|\u4e00\u4e2a\u5408\u9002\u7684\u573a\u666f\u8868\u793a\u662f\u5b9e\u73b0\u7a7a\u95f4\u667a\u80fd\u7684\u6838\u5fc3\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u667a\u80fd\u4f53\u53ef\u4ee5\u7a33\u5065\u5730\u91cd\u5efa\u5e76\u6709\u6548\u5730\u7406\u89e3 3D \u573a\u666f\u3002\u573a\u666f\u8868\u793a\u53ef\u4ee5\u662f\u5ea6\u91cf\u7684\uff0c\u4f8b\u5982 3D \u91cd\u5efa\u4e2d\u7684\u5730\u6807\u5730\u56fe\u3001\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684 3D \u8fb9\u754c\u6846\u6216\u5360\u7528\u9884\u6d4b\u4e2d\u7684\u4f53\u7d20\u7f51\u683c\uff0c\u4e5f\u53ef\u4ee5\u662f\u62d3\u6251\u7684\uff0c\u4f8b\u5982 SLAM \u4e2d\u5177\u6709\u95ed\u73af\u7684\u4f4d\u59ff\u56fe\u6216 SfM \u4e2d\u7684\u53ef\u89c1\u6027\u56fe\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u4ece\u65e0\u4f4d\u59ff\u56fe\u50cf\u6784\u5efa\u591a\u89c6\u56fe\u573a\u666f\u56fe (MSG)\uff0c\u4f7f\u7528\u76f8\u4e92\u8fde\u63a5\u7684\u5730\u70b9\u548c\u5bf9\u8c61\u8282\u70b9\u4ee5\u62d3\u6251\u65b9\u5f0f\u8868\u793a\u573a\u666f\u3002\u5bf9\u4e8e\u73b0\u6709\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u6765\u8bf4\uff0c\u6784\u5efa MSG \u7684\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u4ece\u89c6\u91ce\u6709\u9650\u4e14\u53ef\u80fd\u5b58\u5728\u8f83\u5927\u89c6\u89d2\u53d8\u5316\u7684\u56fe\u50cf\u4e2d\u5171\u540c\u89e3\u51b3\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\u3001\u76ee\u6807\u68c0\u6d4b\u548c\u76ee\u6807\u5173\u8054\u95ee\u9898\u3002\u4e3a\u4e86\u8bc4\u4f30\u4efb\u4f55\u89e3\u51b3\u6b64\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u57fa\u4e8e\u516c\u5171 3D \u6570\u636e\u96c6\u5f00\u53d1\u4e86 MSG \u6570\u636e\u96c6\u548c\u6ce8\u91ca\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e MSG \u8fb9\u7f18\u7684\u4ea4\u5e76\u6bd4\u5206\u6570\u7684\u8bc4\u4f30\u6307\u6807\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u6d41\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u65b0\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c06\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\u548c\u76ee\u6807\u5173\u8054\u7ed3\u5408\u5230\u4e00\u4e2a Transformer \u89e3\u7801\u5668\u67b6\u6784\u4e2d\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u7684\u76f8\u5173\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u3002||\n", "2410.13807": "|**2024-10-17**|[ConsisSR: Delving Deep into Consistency in Diffusion-based Image Super-Resolution](http://arxiv.org/abs/2410.13807)|null|\u73b0\u5b9e\u4e16\u754c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387 (Real-ISR) \u65e8\u5728\u4ece\u88ab\u672a\u77e5\u548c\u590d\u6742\u9000\u5316\u7834\u574f\u7684\u4f4e\u8d28\u91cf (LQ) \u8f93\u5165\u4e2d\u6062\u590d\u9ad8\u8d28\u91cf (HQ) \u56fe\u50cf\u3002\u7279\u522b\u662f\uff0c\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf (T2I) \u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u751f\u6210\u5148\u9a8c\uff0c\u4ee5\u91cd\u5efa\u53ef\u4fe1\u548c\u590d\u6742\u7684\u7ec6\u8282\u3002\u7136\u800c\uff0cT2I \u751f\u6210\u4fa7\u91cd\u4e8e\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u800c Real-ISR \u5f3a\u8c03\u50cf\u7d20\u7ea7\u91cd\u5efa\uff0c\u8fd9\u963b\u788d\u4e86\u73b0\u6709\u65b9\u6cd5\u5145\u5206\u5229\u7528\u6269\u6563\u5148\u9a8c\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86 ConsisSR \u6765\u5904\u7406\u8bed\u4e49\u548c\u50cf\u7d20\u7ea7\u7684\u4e00\u81f4\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4e0e\u7c97\u7c92\u5ea6\u7684\u6587\u672c\u63d0\u793a\u76f8\u6bd4\uff0c\u6211\u4eec\u5229\u7528\u66f4\u5f3a\u5927\u7684 CLIP \u56fe\u50cf\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u6211\u4eec\u7684\u6df7\u5408\u63d0\u793a\u9002\u914d\u5668 (HPA) \u6709\u6548\u5730\u5229\u7528\u8fd9\u4e24\u79cd\u6a21\u6001\u6765\u8fdb\u884c\u8bed\u4e49\u6307\u5bfc\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u65f6\u95f4\u611f\u77e5\u6f5c\u5728\u589e\u5f3a (TALA) \u6765\u51cf\u8f7b T2I \u751f\u6210\u548c Real-ISR \u4e00\u81f4\u6027\u8981\u6c42\u4e4b\u95f4\u7684\u56fa\u6709\u5dee\u8ddd\u3002\u901a\u8fc7\u968f\u673a\u6df7\u5408 LQ \u548c HQ \u6f5c\u5728\u8f93\u5165\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4e0d\u4ec5\u53ef\u4ee5\u5904\u7406\u65f6\u95f4\u6b65\u957f\u7279\u5b9a\u7684\u6269\u6563\u566a\u58f0\uff0c\u8fd8\u53ef\u4ee5\u7ec6\u5316\u7d2f\u79ef\u7684\u6f5c\u5728\u8868\u793a\u3002\u6700\u540e\u4f46\u540c\u6837\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u7684 GAN \u5d4c\u5165\u7b56\u7565\u91c7\u7528\u9884\u8bad\u7ec3\u7684 Real-ESRGAN \u6a21\u578b\u6765\u7ec6\u5316\u6269\u6563\u8d77\u70b9\u3002\u8fd9\u5728\u4e0d\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5c06\u63a8\u7406\u8fc7\u7a0b\u52a0\u901f\u5230 10 \u6b65\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u91c7\u6837\u8d28\u91cf\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5168\u5c3a\u5ea6\u548c\u52a0\u901f\u6a21\u578b\u4e2d\u90fd\u8868\u73b0\u51fa\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4ee3\u7801\u5c06\u516c\u5f00\u3002||\n", "2410.13618": "|**2024-10-17**|[LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decomposition for Parameter-Efficient Fine-Tuning](http://arxiv.org/abs/2410.13618)|**[link](https://github.com/skddj/loldu)**|\u6a21\u578b\u89c4\u6a21\u7684\u5feb\u901f\u589e\u957f\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u6765\u8fdb\u884c\u5fae\u8c03\u3002\u73b0\u6709\u7684\u65b9\u6cd5\uff0c\u5982\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\uff0c\u8bd5\u56fe\u89e3\u51b3\u5b8c\u6574\u5fae\u8c03\u4e2d\u5904\u7406\u5927\u91cf\u66f4\u65b0\u53c2\u6570\u7684\u95ee\u9898\u3002\u7136\u800c\uff0cLoRA\u5229\u7528\u968f\u673a\u521d\u59cb\u5316\u548c\u4f4e\u79e9\u77e9\u9635\u4f18\u5316\u6765\u903c\u8fd1\u66f4\u65b0\u6743\u91cd\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u4e0e\u5b8c\u6574\u5fae\u8c03\u76f8\u6bd4\uff0c\u6536\u655b\u901f\u5ea6\u4e0d\u7406\u60f3\u4e14\u7cbe\u5ea6\u5b58\u5728\u5dee\u8ddd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86LoLDU\uff0c\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\uff0c\u4e0e\u5e38\u89c4PEFT\u65b9\u6cd5\u76f8\u6bd4\uff0c\u53ef\u5c06\u53ef\u8bad\u7ec3\u53c2\u6570\u51cf\u5c112600\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u6027\u80fd\u3002LoLDU\u5229\u7528\u4e0b-\u5bf9\u89d2\u7ebf-\u4e0a\u5206\u89e3\uff08LDU\uff09\u6765\u521d\u59cb\u5316\u4f4e\u79e9\u77e9\u9635\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u6b63\u4ea4\u6027\u3002\u6211\u4eec\u4e13\u6ce8\u4e8e\u4f18\u5316\u5bf9\u89d2\u77e9\u9635\u4ee5\u8fdb\u884c\u7f29\u653e\u53d8\u6362\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0cLoLDU\u5728\u6240\u6709PEFT\u65b9\u6cd5\u4e2d\u53c2\u6570\u6700\u5c11\u3002\u6211\u4eec\u5bf94\u4e2a\u6307\u4ee4\u9075\u5faa\u6570\u636e\u96c6\u30016\u4e2a\u81ea\u7136\u8bed\u8a00\u7406\u89e3\uff08NLU\uff09\u6570\u636e\u96c6\u30018\u4e2a\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u548c\u5177\u6709\u591a\u79cd\u6a21\u578b\u7c7b\u578b\uff08LLaMA2\u3001RoBERTa\u3001ViT\u548cStable Diffusion\uff09\u7684\u56fe\u50cf\u751f\u6210\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u63d0\u4f9b\u4e86\u5168\u9762\u800c\u8be6\u7ec6\u7684\u5206\u6790\u3002\u6211\u4eec\u7684\u5f00\u6e90\u4ee3\u7801\u53ef\u5728\\href{https://github.com/SKDDJ/LoLDU}{https://github.com/SKDDJ/LoLDU}\u83b7\u53d6\u3002||\n", "2410.13616": "|**2024-10-17**|[Spatiotemporal Object Detection for Improved Aerial Vehicle Detection in Traffic Monitoring](http://arxiv.org/abs/2410.13616)|null|\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u5f00\u53d1\u65f6\u7a7a\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u5728\u4f7f\u7528\u65e0\u4eba\u673a\u6444\u50cf\u5934\u8fdb\u884c\u591a\u7c7b\u522b\u8f66\u8f86\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\u3002\u8be5\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65f6\u7a7a\u8f66\u8f86\u68c0\u6d4b\u6570\u636e\u96c6\uff08STVD\uff09\uff0c\u5176\u4e2d\u5305\u542b\u7531\u65e0\u4eba\u673a\u6355\u83b7\u7684 6,600 \u5f20\u5e26\u6ce8\u91ca\u7684\u8fde\u7eed\u5e27\u56fe\u50cf\uff0c\u4ece\u800c\u80fd\u591f\u5bf9\u7528\u4e8e\u6574\u4f53\u65f6\u7a7a\u611f\u77e5\u7684\u7b97\u6cd5\u8fdb\u884c\u5168\u9762\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002\u57fa\u4e8e YOLO \u7684\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u5f97\u5230\u4e86\u589e\u5f3a\uff0c\u4ee5\u7eb3\u5165\u65f6\u95f4\u52a8\u6001\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5355\u5e27\u6a21\u578b\u7684\u6027\u80fd\u3002\u5c06\u6ce8\u610f\u529b\u673a\u5236\u96c6\u6210\u5230\u65f6\u7a7a\u6a21\u578b\u4e2d\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u6700\u4f73\u65f6\u7a7a\u6a21\u578b\u6bd4\u5355\u5e27\u6a21\u578b\u63d0\u9ad8\u4e86 16.22%\uff0c\u540c\u65f6\u8bc1\u660e\u6ce8\u610f\u529b\u673a\u5236\u5177\u6709\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u7684\u6f5c\u529b\u3002||\n", "2410.13453": "|**2024-10-17**|[Augmentation Policy Generation for Image Classification Using Large Language Models](http://arxiv.org/abs/2410.13453)|null|\u81ea\u52a8\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u90fd\u662f\u5728\u5e38\u89c1\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4f18\u5316\u7684\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5bf9\u66f4\u591a\u6837\u5316\u6216\u7279\u5b9a\u9886\u57df\u6570\u636e\uff08\u5982\u533b\u5b66\u6570\u636e\u96c6\uff09\u7684\u9002\u7528\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u9ad8\u6548\u589e\u5f3a\u7b56\u7565\u7684\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u53ef\u9488\u5bf9\u4efb\u4f55\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\u7684\u7279\u5b9a\u7279\u5f81\u8fdb\u884c\u5b9a\u5236\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8fed\u4ee3\u5730\u4e0eLLM\u4ea4\u4e92\uff0c\u4ee5\u83b7\u5f97\u5e76\u6839\u636e\u6a21\u578b\u6027\u80fd\u53cd\u9988\u6539\u8fdb\u589e\u5f3a\u7b56\u7565\uff0c\u4ece\u800c\u521b\u5efa\u4e00\u4e2a\u4e0e\u6570\u636e\u96c6\u65e0\u5173\u7684\u6570\u636e\u589e\u5f3a\u7ba1\u9053\u3002\u5728\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u5bf9\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u660e\u663e\u7684\u6539\u8fdb\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002\u867d\u7136\u5b83\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u5b83\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u4f7f\u6d41\u7a0b\u81ea\u52a8\u5316\uff0c\u5e76\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e86\u6a21\u578b\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u7684\u4eba\u5de5\u53c2\u4e0e\u3002||\n", "2410.13437": "|**2024-10-17**|[Temporal-Enhanced Multimodal Transformer for Referring Multi-Object Tracking and Segmentation](http://arxiv.org/abs/2410.13437)|null|\u6307\u4ee3\u6027\u591a\u76ee\u6807\u8ddf\u8e2a\uff08RMOT\uff09\u662f\u4e00\u9879\u65b0\u5174\u7684\u8de8\u6a21\u6001\u4efb\u52a1\uff0c\u65e8\u5728\u5b9a\u4f4d\u89c6\u9891\u4e2d\u7531\u8bed\u8a00\u8868\u8fbe\u5f0f\u6307\u4ee3\u7684\u4efb\u610f\u6570\u91cf\u7684\u76ee\u6807\u5bf9\u8c61\u5e76\u7ef4\u6301\u5176\u8eab\u4efd\u3002\u8fd9\u9879\u590d\u6742\u7684\u4efb\u52a1\u6d89\u53ca\u8bed\u8a00\u548c\u89c6\u89c9\u6a21\u6001\u7684\u63a8\u7406\uff0c\u4ee5\u53ca\u76ee\u6807\u5bf9\u8c61\u7684\u65f6\u95f4\u5173\u8054\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u4ec5\u91c7\u7528\u677e\u6563\u7684\u7279\u5f81\u878d\u5408\uff0c\u5ffd\u7565\u4e86\u5bf9\u8ddf\u8e2a\u76ee\u6807\u7684\u957f\u671f\u4fe1\u606f\u7684\u5229\u7528\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7d27\u51d1\u7684\u57fa\u4e8e Transformer \u7684\u65b9\u6cd5\uff0c\u79f0\u4e3a TenRMOT\u3002\u6211\u4eec\u5728\u7f16\u7801\u548c\u89e3\u7801\u9636\u6bb5\u90fd\u8fdb\u884c\u7279\u5f81\u878d\u5408\uff0c\u4ee5\u5145\u5206\u5229\u7528 Transformer \u67b6\u6784\u7684\u4f18\u52bf\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5728\u7f16\u7801\u9636\u6bb5\u9010\u5c42\u9012\u589e\u5730\u6267\u884c\u8de8\u6a21\u6001\u878d\u5408\u3002\u5728\u89e3\u7801\u9636\u6bb5\uff0c\u6211\u4eec\u5229\u7528\u8bed\u8a00\u5f15\u5bfc\u7684\u67e5\u8be2\u6765\u63a2\u6d4b\u8bb0\u5fc6\u7279\u5f81\uff0c\u4ee5\u51c6\u786e\u9884\u6d4b\u6240\u9700\u7684\u5bf9\u8c61\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u67e5\u8be2\u66f4\u65b0\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u660e\u786e\u5229\u7528\u8ddf\u8e2a\u5bf9\u8c61\u7684\u5148\u524d\u65f6\u95f4\u4fe1\u606f\u6765\u589e\u5f3a\u5176\u8f68\u8ff9\u7684\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u6307\u4ee3\u6027\u591a\u76ee\u6807\u8ddf\u8e2a\u548c\u5206\u5272\uff08RMOTS\uff09\u201d\u7684\u65b0\u4efb\u52a1\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a Ref-KITTI Segmentation \u7684\u65b0\u6570\u636e\u96c6\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u5305\u542b 18 \u4e2a\u89c6\u9891\uff0c\u5171 818 \u4e2a\u8868\u8fbe\u5f0f\uff0c\u6bcf\u4e2a\u8868\u8fbe\u5f0f\u5e73\u5747\u5305\u542b 10.7 \u4e2a\u63a9\u7801\uff0c\u4e0e\u5927\u591a\u6570\u73b0\u6709\u6307\u4ee3\u6027\u89c6\u9891\u5206\u5272\u6570\u636e\u96c6\u4e2d\u5178\u578b\u7684\u5355\u4e2a\u63a9\u7801\u76f8\u6bd4\uff0c\u8fd9\u5e26\u6765\u4e86\u66f4\u5927\u7684\u6311\u6218\u3002TenRMOT \u5728\u6307\u4ee3\u6027\u591a\u76ee\u6807\u8ddf\u8e2a\u548c\u5206\u5272\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002||\n", "2410.13427": "|**2024-10-17**|[Unsupervised Skull Segmentation via Contrastive MR-to-CT Modality Translation](http://arxiv.org/abs/2410.13427)|null|\u4eceCT\u626b\u63cf\u4e2d\u5206\u5272\u9885\u9aa8\u53ef\u4ee5\u770b\u4f5c\u662f\u4e00\u4e2a\u5df2\u7ecf\u89e3\u51b3\u7684\u95ee\u9898\u3002\u7136\u800c\uff0c\u5728MRI\u4e2d\uff0c\u7531\u4e8e\u5b58\u5728\u8f6f\u7ec4\u7ec7\u800c\u4e0d\u662f\u9aa8\u9abc\uff0c\u8fd9\u9879\u4efb\u52a1\u7684\u590d\u6742\u6027\u8981\u5927\u5f97\u591a\u3002\u4ece\u5934\u90e8MRI\u56fe\u50cf\u4e2d\u6355\u83b7\u9aa8\u9abc\u7ed3\u6784\u975e\u5e38\u56f0\u96be\uff0c\u56e0\u4e3a\u5934\u90e8MRI\u7684\u4e3b\u8981\u53ef\u89c6\u5316\u76ee\u6807\u662f\u5927\u8111\u3002\u5c1d\u8bd5\u4f7f\u7528\u9885\u9aa8\u5265\u79bb\u7684\u65b9\u6cd5\u4f3c\u4e4e\u4e0d\u592a\u9002\u5408\u8fd9\u9879\u4efb\u52a1\uff0c\u5e76\u4e14\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u90fd\u5931\u8d25\u4e86\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u4e14\u8017\u65f6\u7684\u9885\u9aa8\u6807\u6ce8\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u56f0\u96be\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u4e0d\u76f4\u63a5\u5bf9MRI\u56fe\u50cf\u8fdb\u884c\u5206\u5272\uff0c\u800c\u662f\u901a\u8fc7MRI\u5230CT\u7684\u8f6c\u6362\u751f\u6210\u5408\u6210CT\u6570\u636e\uff0c\u5e76\u5728\u5176\u4e2d\u8fdb\u884c\u5206\u5272\u3002\u6211\u4eec\u89e3\u51b3\u4e86\u4e0e\u65e0\u76d1\u7763\u9885\u9aa8\u5206\u5272\u76f8\u5173\u7684\u8bb8\u591a\u95ee\u9898\uff0c\u5305\u62ecMRI\u548cCT\u6570\u636e\u96c6\u7684\u4e0d\u914d\u5bf9\u6027\u8d28\uff08\u5bf9\u6bd4\u5b66\u4e60\uff09\u3001\u4f4e\u5206\u8fa8\u7387\u548c\u4f4e\u8d28\u91cf\uff08\u8d85\u5206\u8fa8\u7387\uff09\u4ee5\u53ca\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u9879\u7814\u7a76\u5bf9\u4e8e\u9700\u8981\u4eceMRI\u4f53\u79ef\u6570\u636e\u4e2d\u8fdb\u884c\u9885\u9aa8\u5206\u5272\u7684\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u9885\u9aa8\u5207\u9664\u672f\u6216\u624b\u672f\u8ba1\u5212\uff09\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u5e76\u4e14\u53ef\u4ee5\u88ab\u89c6\u4e3a\u671d\u7740\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u5229\u7528\u5408\u6210\u6570\u636e\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\u3002||\n", "2410.13016": "|**2024-10-16**|[Interpreting and Analyzing CLIP's Zero-Shot Image Classification via Mutual Knowledge](http://arxiv.org/abs/2410.13016)|**[link](https://github.com/fawazsammani/clip-interpret-mutual-knowledge)**|\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3 (CLIP) \u901a\u8fc7\u5c06\u56fe\u50cf\u548c\u6587\u672c\u7c7b\u522b\u8868\u793a\u6620\u5c04\u5230\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u6765\u6267\u884c\u96f6\u6837\u672c\u56fe\u50cf\u5206\u7c7b\uff0c\u7136\u540e\u68c0\u7d22\u6700\u63a5\u8fd1\u56fe\u50cf\u7684\u7c7b\u522b\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4ece\u4e24\u79cd\u6a21\u6001\u4e4b\u95f4\u7684\u4e92\u77e5\u8bc6\u7684\u89d2\u5ea6\u6765\u89e3\u91ca\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u7684 CLIP \u6a21\u578b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4ee5\u4e0b\u95ee\u9898\uff1a\u89c6\u89c9\u548c\u8bed\u8a00 CLIP \u7f16\u7801\u5668\u90fd\u5b66\u4e60\u4e86\u54ea\u4e9b\u5171\u540c\u7684\u6982\u5ff5\uff0c\u8fd9\u4e9b\u6982\u5ff5\u4f1a\u5f71\u54cd\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\uff0c\u5bfc\u81f4\u70b9\u66f4\u8fd1\u6216\u66f4\u8fdc\uff1f\u6211\u4eec\u901a\u8fc7\u57fa\u4e8e\u6587\u672c\u6982\u5ff5\u7684\u89e3\u91ca\u65b9\u6cd5\u6765\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\uff0c\u5c55\u793a\u5176\u6709\u6548\u6027\uff0c\u5e76\u5bf9\u5305\u542b 13 \u4e2a CLIP \u6a21\u578b\u7684\u6c60\u8fdb\u884c\u5206\u6790\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u67b6\u6784\u3001\u89c4\u6a21\u548c\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u65b9\u9762\u5404\u4e0d\u76f8\u540c\u3002\u6211\u4eec\u63a2\u8ba8\u4e86\u4e0e\u4e92\u77e5\u8bc6\u76f8\u5173\u7684\u8fd9\u4e9b\u4e0d\u540c\u65b9\u9762\uff0c\u5e76\u5206\u6790\u4e86\u96f6\u6837\u672c\u9884\u6d4b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c55\u793a\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u4eba\u6027\u5316\u7684\u65b9\u5f0f\u6765\u7406\u89e3 CLIP \u7684\u96f6\u6837\u672c\u5206\u7c7b\u51b3\u7b56\u3002||\n", "2410.12742": "|**2024-10-16**|[PND-Net: Plant Nutrition Deficiency and Disease Classification using Graph Convolutional Network](http://arxiv.org/abs/2410.12742)|null|\u5982\u679c\u80fd\u591f\u5728\u65e9\u671f\u8bc6\u522b\u548c\u68c0\u6d4b\u5404\u79cd\u690d\u7269\u8425\u517b\u7f3a\u4e4f\u75c7\u548c\u75c5\u5bb3\uff0c\u5c31\u53ef\u4ee5\u63d0\u9ad8\u4f5c\u7269\u4ea7\u91cf\uff0c\u4fc3\u8fdb\u519c\u4e1a\u589e\u957f\u3002\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5229\u7528\u53f6\u7247\u89c6\u89c9\u75c7\u72b6\u81ea\u52a8\u68c0\u6d4b\u690d\u7269\u75c5\u5bb3\u548c\u8425\u517b\u7f3a\u4e4f\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5373\u5728\u57fa\u7840\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u7684\u57fa\u7840\u4e0a\uff0c\u4f7f\u7528\u56fe\u5377\u79ef\u7f51\u7edc (GNN) \u5bf9\u690d\u7269\u8425\u517b\u7f3a\u4e4f\u548c\u75c5\u5bb3\u8fdb\u884c\u5206\u7c7b\u3002\u6709\u65f6\uff0c\u5168\u5c40\u7279\u5f81\u63cf\u8ff0\u7b26\u53ef\u80fd\u65e0\u6cd5\u6355\u83b7\u75c5\u53f6\u7684\u5173\u952e\u533a\u57df\uff0c\u4ece\u800c\u5bfc\u81f4\u75be\u75c5\u5206\u7c7b\u4e0d\u51c6\u786e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u533a\u57df\u7279\u5f81\u5b66\u4e60\u5bf9\u4e8e\u6574\u4f53\u7279\u5f81\u805a\u5408\u81f3\u5173\u91cd\u8981\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4f7f\u7528\u7a7a\u95f4\u91d1\u5b57\u5854\u6c60\u5316\u8fdb\u884c\u591a\u5c3a\u5ea6\u533a\u57df\u7279\u5f81\u6c47\u603b\uff0c\u4ee5\u5b9e\u73b0\u5177\u6709\u5224\u522b\u6027\u7684\u7279\u5f81\u8868\u793a\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a GCN\uff0c\u4f7f\u5176\u80fd\u591f\u5b66\u4e60\u66f4\u7cbe\u7ec6\u7684\u7ec6\u8282\uff0c\u4ece\u800c\u5bf9\u690d\u7269\u75c5\u5bb3\u548c\u8425\u517b\u7f3a\u4e4f\u8fdb\u884c\u5206\u7c7b\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u79f0\u4e3a\u690d\u7269\u8425\u517b\u7f3a\u4e4f\u4e0e\u75c5\u5bb3\u7f51\u7edc (PND-Net)\uff0c\u5e76\u5728\u4e24\u4e2a\u8425\u517b\u7f3a\u4e4f\u516c\u5171\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u75c5\u5bb3\u5206\u7c7b\u516c\u5171\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u56db\u79cd CNN \u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u6700\u4f73\u5206\u7c7b\u6027\u80fd\u4e3a\uff1a(a) \u9999\u8549\u8425\u517b\u7f3a\u4e4f\u6570\u636e\u96c6 90.00% \u548c\u5496\u5561\u8425\u517b\u7f3a\u4e4f\u6570\u636e\u96c6 90.54%\uff1b(b) \u4f7f\u7528 Xception \u9aa8\u5e72\u7f51\u7edc\u5728\u9a6c\u94c3\u85af\u75c5\u5bb3\u6570\u636e\u96c6\u4e0a\u8fbe\u5230 96.18%\uff0c\u5728 PlantDoc \u6570\u636e\u96c6\u4e0a\u8fbe\u5230 84.30%\u3002\u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u4e00\u4e9b\u6cdb\u5316\u5b9e\u9a8c\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5373\u4e73\u817a\u764c\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u7c7b\uff08BreakHis 40X\uff1a95.50% \u51c6\u786e\u7387\uff0cBreakHis 100X\uff1a96.79% \u51c6\u786e\u7387\uff09\u548c\u5bab\u9888\u764c\u5206\u7c7b\u5df4\u6c0f\u6d82\u7247\u56fe\u50cf\u4e2d\u7684\u5355\u7ec6\u80de\uff08SIPaKMeD\uff1a99.18% \u51c6\u786e\u7387\uff09\u3002\u6b64\u5916\uff0cPND-Net \u4f7f\u7528\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u4e5f\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002||\n", "2410.12728": "|**2024-10-16**|[Transformer based super-resolution downscaling for regional reanalysis: Full domain vs tiling approaches](http://arxiv.org/abs/2410.12728)|null|\u8d85\u5206\u8fa8\u7387 (SR) \u662f\u4e00\u79cd\u5f88\u6709\u524d\u666f\u7684\u964d\u5c3a\u5ea6\u65b9\u6cd5\uff0c\u53ef\u4ee5\u7ecf\u6d4e\u9ad8\u6548\u5730\u4ece\u8f83\u7c97\u7cd9\u7684\u6c14\u5019\u6570\u636e\u4e2d\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u6c14\u5019\u4fe1\u606f\u3002\u5176\u4e00\u4e2a\u7279\u5b9a\u5e94\u7528\u662f\u4ece\u9a71\u52a8\u5168\u5c40\u5bf9\u5e94\u7269\uff08\u9884\u6d4b\u56e0\u5b50\uff09\u4e2d\u964d\u5c3a\u5ea6\u533a\u57df\u518d\u5206\u6790\u8f93\u51fa\uff08\u9884\u6d4b\u503c\uff09\u3002\u672c\u7814\u7a76\u4ee5 CERRA \u518d\u5206\u6790\uff085.5 \u516c\u91cc\u5206\u8fa8\u7387\uff0c\u7531 ERA5 \u9a71\u52a8\u7684\u533a\u57df\u5927\u6c14\u6a21\u578b\u751f\u6210\uff09\u4e3a\u4f8b\uff0c\u5bf9\u5404\u79cd SR \u964d\u5c3a\u5ea6\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u91cd\u70b9\u5173\u6ce8\u6e29\u5ea6\u3002\u8fd9\u9879\u5de5\u4f5c\u4e2d\u63d0\u51fa\u7684\u65b9\u6cd5\u662f Swin Transformer\uff0c\u5e76\u4f7f\u7528\u4e86\u4e24\u79cd\u66ff\u4ee3\u65b9\u6cd5\u4f5c\u4e3a\u57fa\u51c6\uff08\u5168\u5377\u79ef U-Net \u548c\u5377\u79ef\u548c\u5bc6\u96c6 DeepESD\uff09\u4ee5\u53ca\u7b80\u5355\u7684\u53cc\u4e09\u6b21\u63d2\u503c\u3002\u6211\u4eec\u6bd4\u8f83\u4e86\u4e24\u79cd\u65b9\u6cd5\uff0c\u4e00\u79cd\u662f\u4f7f\u7528\u6574\u4e2a\u57df\u4f5c\u4e3a\u8f93\u5165\u7684\u6807\u51c6\u65b9\u6cd5\uff0c\u53e6\u4e00\u79cd\u662f\u66f4\u5177\u53ef\u6269\u5c55\u6027\u7684\u5207\u7247\u65b9\u6cd5\uff0c\u5c06\u6574\u4e2a\u57df\u5212\u5206\u4e3a\u7528\u4f5c\u8f93\u5165\u7684\u5207\u7247\u3002\u8fd9\u4e9b\u65b9\u6cd5\u7ecf\u8fc7\u8bad\u7ec3\u53ef\u4ee5\u6839\u636e\u6765\u81ea\u9a71\u52a8 ERA5 \u7684\u6e29\u5ea6\u4fe1\u606f\u5bf9 CERRA \u5730\u8868\u6e29\u5ea6\u8fdb\u884c\u964d\u5c3a\u5ea6\uff1b\u6b64\u5916\uff0c\u5207\u7247\u65b9\u6cd5\u8fd8\u5305\u62ec\u9759\u6001\u5730\u5f62\u4fe1\u606f\u3002\u6211\u4eec\u8868\u660e\uff0c\u9700\u8981\u7a7a\u95f4\u53ef\u8fc1\u79fb\u6027\u7684\u5207\u7247\u65b9\u6cd5\u4ee5\u964d\u4f4e\u6027\u80fd\u4e3a\u4ee3\u4ef7\uff08\u5c3d\u7ba1\u5b83\u4f18\u4e8e\u67d0\u4e9b\u5168\u57df\u57fa\u51c6\uff09\uff0c\u4f46\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u5141\u8bb8\u5728\u6cdb\u6b27\u5c3a\u5ea6\u4e0a\u8fdb\u884c SR \u51cf\u5c11\uff0c\u5e76\u4e14\u5bf9\u4e8e\u5b9e\u65f6\u5e94\u7528\u5f88\u6709\u4ef7\u503c\u3002||\n", "2410.12673": "|**2024-10-16**|[MambaBEV: An efficient 3D detection model with Mamba2](http://arxiv.org/abs/2410.12673)|null|\u57fa\u4e8eBEV\u8303\u5f0f\u5e76\u7ed3\u5408\u65f6\u95f4\u4fe1\u606f\u7684\u7a33\u5b9a3D\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5f53\u524d\u4f7f\u7528\u5377\u79ef\u5c42\u6216\u53ef\u53d8\u5f62\u81ea\u6ce8\u610f\u529b\u7684\u65f6\u5e8f\u878d\u5408\u6a21\u578b\u4e0d\u5229\u4e8eBEV\u7a7a\u95f4\u5168\u5c40\u4fe1\u606f\u7684\u4ea4\u6362\uff0c\u5e76\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\u3002\u6700\u8fd1\uff0c\u4e00\u79cd\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u5e8f\u5217\u7684\u65b0\u578b\u57fa\u4e8eMamba\u7684\u6a21\u578b\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba2\u7684BEV 3D\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u540d\u4e3aMambaBEV\u3002\u6211\u4eec\u8fd8\u91c7\u7528\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u9a7e\u9a76\u8303\u5f0f\u6765\u6d4b\u8bd5\u6a21\u578b\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u76f8\u5f53\u4e0d\u9519\u7684\u7ed3\u679c\uff1a\u6211\u4eec\u7684\u57fa\u672c\u7248\u672c\u5b9e\u73b0\u4e8651.7%\u7684NDS\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5c06\u5f88\u5feb\u5f00\u6e90\u3002||\n"}, "\u751f\u6210\u6a21\u578b": {"2409.02919": "|**2024-09-04**|[HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts](http://arxiv.org/abs/2409.02919)|**[link](https://github.com/Liuxinyv/HiPrompt)**|\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u751f\u6210\u66f4\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5728\u5904\u7406\u7269\u4f53\u91cd\u590d\u548c\u7ed3\u6784\u4f2a\u5f71\u65b9\u9762\u5e38\u5e38\u9047\u5230\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u6269\u5c55\u5230 4K \u53ca\u66f4\u9ad8\u5206\u8fa8\u7387\u65f6\u3002\u6211\u4eec\u53d1\u73b0\u95ee\u9898\u5728\u4e8e\uff0c\u5355\u4e2a\u63d0\u793a\u751f\u6210\u591a\u4e2a\u5c3a\u5ea6\u7684\u65b9\u5f0f\u6548\u7387\u4f4e\u4e0b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 HiPrompt\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u987b\u5fae\u8c03\u7684\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u901a\u8fc7\u5f15\u5165\u5206\u5c42\u63d0\u793a\u6765\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002\u5206\u5c42\u63d0\u793a\u63d0\u4f9b\u5168\u5c40\u548c\u5c40\u90e8\u6307\u5bfc\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5168\u5c40\u6307\u5bfc\u6765\u81ea\u63cf\u8ff0\u6574\u4f53\u5185\u5bb9\u7684\u7528\u6237\u8f93\u5165\uff0c\u800c\u5c40\u90e8\u6307\u5bfc\u5219\u5229\u7528\u6765\u81ea MLLM \u7684\u9010\u5757\u63cf\u8ff0\u6765\u7cbe\u5fc3\u6307\u5bfc\u5c40\u90e8\u7ed3\u6784\u548c\u7eb9\u7406\u7684\u751f\u6210\u3002\u6b64\u5916\uff0c\u5728\u9006\u5411\u53bb\u566a\u8fc7\u7a0b\u4e2d\uff0c\u751f\u6210\u7684\u566a\u58f0\u88ab\u5206\u89e3\u4e3a\u4f4e\u9891\u548c\u9ad8\u9891\u7a7a\u95f4\u5206\u91cf\u3002\u8fd9\u4e9b\u5206\u91cf\u4ee5\u591a\u4e2a\u63d0\u793a\u7ea7\u522b\u4e3a\u6761\u4ef6\uff0c\u5305\u62ec\u8be6\u7ec6\u7684\u9010\u5757\u63cf\u8ff0\u548c\u66f4\u5e7f\u6cdb\u7684\u56fe\u50cf\u7ea7\u63d0\u793a\uff0c\u4ece\u800c\u4fc3\u8fdb\u5728\u5206\u5c42\u8bed\u4e49\u6307\u5bfc\u4e0b\u7684\u63d0\u793a\u5f15\u5bfc\u53bb\u566a\u3002\u5b83\u8fdb\u4e00\u6b65\u5141\u8bb8\u751f\u6210\u8fc7\u7a0b\u66f4\u591a\u5730\u5173\u6ce8\u5c40\u90e8\u7a7a\u95f4\u533a\u57df\uff0c\u5e76\u786e\u4fdd\u751f\u6210\u7684\u56fe\u50cf\u5728\u9ad8\u6e05\u6670\u5ea6\u4e0b\u4fdd\u6301\u4e00\u81f4\u7684\u5c40\u90e8\u548c\u5168\u5c40\u8bed\u4e49\u3001\u7ed3\u6784\u548c\u7eb9\u7406\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cHiPrompt \u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u7269\u4f53\u91cd\u590d\u5e76\u63d0\u9ad8\u4e86\u7ed3\u6784\u8d28\u91cf\u3002||\n", "2409.02915": "|**2024-09-04**|[Latent Watermarking of Audio Generative Models](http://arxiv.org/abs/2409.02915)|null|\u97f3\u9891\u751f\u6210\u6a21\u578b\u7684\u8fdb\u6b65\u7ed9\u5176\u8d1f\u8d23\u4efb\u7684\u62ab\u9732\u548c\u6ee5\u7528\u68c0\u6d4b\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u901a\u8fc7\u5bf9\u5176\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u7279\u5b9a\u6c34\u5370\u6765\u6807\u8bb0\u6f5c\u5728\u751f\u6210\u6a21\u578b\u7684\u65b9\u6cd5\u3002\u7531\u6b64\u4ea7\u751f\u7684\u6c34\u5370\u6a21\u578b\u751f\u6210\u7684\u6f5c\u5728\u8868\u793a\uff0c\u5176\u89e3\u7801\u8f93\u51fa\u53ef\u4ee5\u88ab\u9ad8\u7f6e\u4fe1\u5ea6\u5730\u68c0\u6d4b\u5230\uff0c\u800c\u65e0\u8bba\u4f7f\u7528\u4f55\u79cd\u89e3\u7801\u65b9\u6cd5\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e0\u9700\u8fdb\u884c\u4e8b\u540e\u6c34\u5370\u6b65\u9aa4\u5373\u53ef\u68c0\u6d4b\u751f\u6210\u7684\u5185\u5bb9\u3002\u5b83\u4e3a\u5f00\u6e90\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6709\u52a9\u4e8e\u8bc6\u522b\u90a3\u4e9b\u5728\u672a\u9075\u5b88\u8bb8\u53ef\u6761\u6b3e\u7684\u60c5\u51b5\u4e0b\u5bf9\u8fd9\u4e9b\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u6216\u4f7f\u7528\u7684\u884d\u751f\u4f5c\u54c1\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u5bf9\u6f5c\u5728\u751f\u6210\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u540e\uff0c\u751f\u6210\u8f93\u51fa\u7684\u68c0\u6d4b\u7cbe\u5ea6\u4e5f\u80fd\u5728\u5047\u9633\u6027\u7387\u4e3a$10^{-3}$\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230 75% \u4ee5\u4e0a\u3002||\n", "2409.02908": "|**2024-09-04**|[Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling](http://arxiv.org/abs/2409.02908)|null|\u63a9\u7801\u6269\u6563\u6a21\u578b (MDM) \u7531\u4e8e\u5176\u76f8\u8f83\u4e8e\u5176\u4ed6\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5df2\u6210\u4e3a\u79bb\u6563\u6570\u636e\u751f\u6210\u5efa\u6a21\u7684\u70ed\u95e8\u7814\u7a76\u8bfe\u9898\uff0c\u5e76\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u4e0e\u81ea\u56de\u5f52\u6a21\u578b (ARM) \u5c55\u5f00\u7ade\u4e89\u3002\u6700\u8fd1\u7b80\u5316\u63a9\u7801\u6269\u6563\u6846\u67b6\u7684\u52aa\u529b\u8fdb\u4e00\u6b65\u4f7f\u5176\u4e0e\u8fde\u7eed\u7a7a\u95f4\u6269\u6563\u6a21\u578b\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u83b7\u5f97\u4e86\u66f4\u6709\u539f\u5219\u7684\u8bad\u7ec3\u548c\u91c7\u6837\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63ed\u793a\u4e86 MDM \u7684\u8bad\u7ec3\u548c\u91c7\u6837\u5728\u7406\u8bba\u4e0a\u90fd\u53ef\u4ee5\u6446\u8131\u65f6\u95f4\u53d8\u91cf\uff08\u53ef\u4ee5\u8bf4\u662f\u6269\u6563\u6a21\u578b\u7684\u5173\u952e\u7279\u5f81\uff09\uff0c\u5e76\u4e14\u7b49\u6548\u4e8e\u63a9\u7801\u6a21\u578b\u3002\u6211\u4eec\u5728\u91c7\u6837\u65b9\u9762\u7684\u8054\u7cfb\u662f\u901a\u8fc7\u6211\u4eec\u63d0\u51fa\u7684\u9996\u6b21\u547d\u4e2d\u91c7\u6837\u5668 (FHS) \u5efa\u7acb\u7684\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 FHS \u5728\u7406\u8bba\u4e0a\u7b49\u6548\u4e8e MDM \u7684\u539f\u59cb\u751f\u6210\u8fc7\u7a0b\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8017\u65f6\u7684\u5206\u7c7b\u91c7\u6837\uff0c\u5e76\u5b9e\u73b0\u4e86 20 \u500d\u7684\u52a0\u901f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u7814\u7a76\u5bf9\u5148\u524d\u5173\u4e8e MDM \u5728\u751f\u6210\u56f0\u60d1\u5ea6\u65b9\u9762\u53ef\u4ee5\u8d85\u8d8a ARM \u7684\u8bf4\u6cd5\u63d0\u51fa\u4e86\u8d28\u7591\u3002\u6211\u4eec\u9996\u6b21\u53d1\u73b0\u4e86\u4e00\u4e2a\u6f5c\u5728\u7684\u6570\u503c\u95ee\u9898\uff0c\u5373\u4f7f\u4f7f\u7528 32 \u4f4d\u6d6e\u70b9\u7cbe\u5ea6\uff0c\u4e5f\u4f1a\u5bfc\u81f4\u4e0d\u51c6\u786e\u7684\u5206\u7c7b\u91c7\u6837\u3002\u6211\u4eec\u8868\u660e\uff0c\u8be5\u6570\u503c\u95ee\u9898\u5728\u7406\u8bba\u4e0a\u548c\u7ecf\u9a8c\u4e0a\u90fd\u964d\u4f4e\u4e86\u6709\u6548\u6e29\u5ea6\uff0c\u5bfc\u81f4\u5148\u524d\u6587\u732e\u4e2d\u5bf9 MDM \u751f\u6210\u7ed3\u679c\u7684\u8bc4\u4f30\u4e0d\u516c\u5e73\u3002||\n", "2409.02851": "|**2024-09-04**|[Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video Diffusion Models](http://arxiv.org/abs/2409.02851)|**[link](https://github.com/Human-VDM/Human-VDM)**|\u4ece\u5355\u5f20RGB\u56fe\u50cf\u751f\u6210\u903c\u771f3D\u4eba\u4f53\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u7cbe\u786e\u7684\u51e0\u4f55\u5efa\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u7eb9\u7406\u548c\u5408\u7406\u7684\u4e0d\u53ef\u89c1\u90e8\u5206\u751f\u6210\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u8fdb\u884c3D\u4eba\u4f53\u751f\u6210\uff0c\u4f46\u5b83\u4eec\u7ecf\u5e38\u9762\u4e34\u89c6\u89d2\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u8fd9\u963b\u788d\u4e86\u9ad8\u8d28\u91cf3D\u4eba\u4f53\u7684\u751f\u6210\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Human-VDM\uff0c\u4e00\u79cd\u4f7f\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u4ece\u5355\u5f20RGB\u56fe\u50cf\u751f\u62103D\u4eba\u4f53\u7684\u65b0\u65b9\u6cd5\u3002Human-VDM\u4f7f\u7528\u9ad8\u65af\u6e32\u67d3\u4e3a3D\u4eba\u4f53\u751f\u6210\u63d0\u4f9b\u4e86\u65f6\u95f4\u4e0a\u4e00\u81f4\u7684\u89c6\u56fe\u3002\u5b83\u7531\u4e09\u4e2a\u6a21\u5757\u7ec4\u6210\uff1a\u89c6\u56fe\u4e00\u81f4\u7684\u4eba\u4f53\u89c6\u9891\u6269\u6563\u6a21\u5757\u3001\u89c6\u9891\u589e\u5f3a\u6a21\u5757\u548c\u9ad8\u65af\u6e32\u67d3\u6a21\u5757\u3002\u9996\u5148\uff0c\u5c06\u5355\u5f20\u56fe\u50cf\u8f93\u5165\u4eba\u4f53\u89c6\u9891\u6269\u6563\u6a21\u5757\u4ee5\u751f\u6210\u8fde\u8d2f\u7684\u4eba\u4f53\u89c6\u9891\u3002\u63a5\u4e0b\u6765\uff0c\u89c6\u9891\u589e\u5f3a\u6a21\u5757\u5e94\u7528\u8d85\u5206\u8fa8\u7387\u548c\u89c6\u9891\u63d2\u503c\u6765\u589e\u5f3a\u751f\u6210\u89c6\u9891\u7684\u7eb9\u7406\u548c\u51e0\u4f55\u5e73\u6ed1\u5ea6\u3002\u6700\u540e\uff0c3D\u4eba\u4f53\u9ad8\u65af\u6e32\u67d3\u6a21\u5757\u5728\u8fd9\u4e9b\u9ad8\u5206\u8fa8\u7387\u548c\u89c6\u89d2\u4e00\u81f4\u7684\u56fe\u50cf\u7684\u6307\u5bfc\u4e0b\u5b66\u4e60\u903c\u771f\u7684\u4eba\u4f53\u3002\u5b9e\u9a8c\u8868\u660e\uff0cHuman-VDM\u53ef\u4ee5\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u4eba\u4f53\uff0c\u5728\u751f\u6210\u8d28\u91cf\u548c\u6570\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://human-vdm.github.io/Human-VDM/||\n", "2409.02845": "|**2024-09-04**|[Multi-Track MusicLDM: Towards Versatile Music Generation with Latent Diffusion Model](http://arxiv.org/abs/2409.02845)|null|\u6269\u6563\u6a21\u578b\u5728\u6d89\u53ca\u97f3\u9891\u548c\u97f3\u4e50\u7684\u8de8\u6a21\u6001\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f8b\u5982\u6587\u672c\u5230\u58f0\u97f3\u548c\u6587\u672c\u5230\u97f3\u4e50\u7684\u751f\u6210\u3002\u8fd9\u4e9b\u6587\u672c\u63a7\u5236\u7684\u97f3\u4e50\u751f\u6210\u6a21\u578b\u901a\u5e38\u4fa7\u91cd\u4e8e\u901a\u8fc7\u6355\u6349\u5168\u5c40\u97f3\u4e50\u5c5e\u6027\uff08\u5982\u6d41\u6d3e\u548c\u60c5\u7eea\uff09\u6765\u751f\u6210\u97f3\u4e50\u3002\u7136\u800c\uff0c\u97f3\u4e50\u521b\u4f5c\u662f\u4e00\u9879\u590d\u6742\u7684\u591a\u5c42\u6b21\u4efb\u52a1\uff0c\u901a\u5e38\u5c06\u97f3\u4e50\u7f16\u6392\u4f5c\u4e3a\u521b\u4f5c\u8fc7\u7a0b\u7684\u4e00\u4e2a\u7ec4\u6210\u90e8\u5206\u3002\u6b64\u8fc7\u7a0b\u6d89\u53ca\u521b\u4f5c\u6bcf\u4e2a\u4e50\u5668\u90e8\u5206\uff0c\u4f7f\u5176\u5728\u8282\u594f\u3001\u529b\u5ea6\u3001\u548c\u58f0\u548c\u65cb\u5f8b\u65b9\u9762\u4e0e\u73b0\u6709\u90e8\u5206\u4fdd\u6301\u4e00\u81f4\uff0c\u8fd9\u9700\u8981\u6bd4\u6587\u672c\u63d0\u793a\u901a\u5e38\u63d0\u4f9b\u7684\u66f4\u7cbe\u786e\u7684\u97f3\u8f68\u63a7\u5236\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5c06 MusicLDM\uff08\u4e00\u79cd\u7528\u4e8e\u97f3\u4e50\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff09\u6269\u5c55\u4e3a\u591a\u8f68\u751f\u6210\u6a21\u578b\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002\u901a\u8fc7\u5b66\u4e60\u5171\u4eab\u4e0a\u4e0b\u6587\u7684\u97f3\u8f68\u7684\u8054\u5408\u6982\u7387\uff0c\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u8de8\u591a\u4e2a\u97f3\u8f68\u751f\u6210\u5f7c\u6b64\u826f\u597d\u5bf9\u5e94\u7684\u97f3\u4e50\uff0c\u65e0\u8bba\u662f\u6709\u6761\u4ef6\u5730\u8fd8\u662f\u65e0\u6761\u4ef6\u5730\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6a21\u578b\u8fd8\u80fd\u591f\u8fdb\u884c\u7f16\u66f2\u751f\u6210\uff0c\u5176\u4e2d\u6a21\u578b\u53ef\u4ee5\u5728\u7ed9\u5b9a\u5176\u4ed6\u97f3\u8f68\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u4efb\u4f55\u97f3\u8f68\u5b50\u96c6\uff08\u4f8b\u5982\uff0c\u751f\u6210\u4e0e\u7ed9\u5b9a\u8d1d\u65af\u548c\u9f13\u97f3\u8f68\u4e92\u8865\u7684\u94a2\u7434\u97f3\u8f68\uff09\u3002\u6211\u4eec\u5c06\u6211\u4eec\u7684\u6a21\u578b\u4e0e\u73b0\u6709\u7684\u591a\u8f68\u751f\u6210\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u603b\u751f\u6210\u4efb\u52a1\u548c\u7f16\u66f2\u751f\u6210\u4efb\u52a1\u7684\u5ba2\u89c2\u6307\u6807\u4e0a\u90fd\u53d6\u5f97\u4e86\u76f8\u5f53\u5927\u7684\u6539\u8fdb\u3002||\n", "2409.02683": "|**2024-09-04**|[Rethinking HTG Evaluation: Bridging Generation and Recognition](http://arxiv.org/abs/2409.02683)|**[link](https://github.com/koninik/htg_evaluation)**|\u751f\u6210\u6a21\u578b\u5728\u81ea\u7136\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u8bc4\u4f30\u5df2\u5f97\u5230\u5e7f\u6cdb\u7814\u7a76\u3002\u5373\u4f7f\u5728\u8bf8\u5982\u624b\u5199\u751f\u6210\uff08HTG\uff09\u7b49\u5177\u6709\u72ec\u7279\u7279\u6b8a\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u4f7f\u7528\u4e86\u7c7b\u4f3c\u7684\u534f\u8bae\u548c\u6307\u6807\uff0c\u5373\u4f7f\u5b83\u4eec\u53ef\u80fd\u5e76\u975e\u5b8c\u5168\u5408\u9002\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e09\u79cd\u4e13\u4e3a HTG \u8bc4\u4f30\u91cf\u8eab\u5b9a\u5236\u7684\u5ea6\u91cf\u6307\u6807\uff1a$ \\text{HTG}_{\\text{HTR}} $\u3001$ \\text{HTG}_{\\text{style}} $ \u548c $ \\text{HTG}_{\\text{OOV}} $\uff0c\u5e76\u8ba4\u4e3a\u5b83\u4eec\u66f4\u4fbf\u4e8e\u8bc4\u4f30\u751f\u6210\u624b\u5199\u56fe\u50cf\u7684\u8d28\u91cf\u3002\u8fd9\u4e9b\u6307\u6807\u4f9d\u8d56\u4e8e\u624b\u5199\u6587\u672c\u8bc6\u522b\u548c\u4e66\u5199\u8005\u8bc6\u522b\u6a21\u578b\u7684\u8bc6\u522b\u9519\u8bef/\u51c6\u786e\u7387\uff0c\u5e76\u5f3a\u8c03\u4e66\u5199\u98ce\u683c\u3001\u6587\u672c\u5185\u5bb9\u548c\u591a\u6837\u6027\u662f\u7b26\u5408\u624b\u5199\u56fe\u50cf\u5185\u5bb9\u7684\u4e3b\u8981\u65b9\u9762\u3002\u6211\u4eec\u5728 IAM \u624b\u5199\u6570\u636e\u5e93\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u8bf8\u5982 FID \u4e4b\u7c7b\u7684\u5e7f\u6cdb\u4f7f\u7528\u7684\u6307\u6807\u65e0\u6cd5\u6b63\u786e\u91cf\u5316\u751f\u6210\u624b\u5199\u6837\u672c\u7684\u591a\u6837\u6027\u548c\u5b9e\u7528\u6027\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6307\u6807\u4fe1\u606f\u66f4\u4e30\u5bcc\uff0c\u5e76\u5f3a\u8c03\u4e86 HTG \u4e2d\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\u7684\u5fc5\u8981\u6027\u3002\u6240\u63d0\u51fa\u7684\u6307\u6807\u4e3a\u8bc4\u4f30 HTG \u8d28\u91cf\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u3001\u4fe1\u606f\u66f4\u4e30\u5bcc\u7684\u534f\u8bae\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8 HTR \u7684\u6027\u80fd\u3002\u8bc4\u4f30\u534f\u8bae\u7684\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u5f97\uff1ahttps://github.com/koninik/HTG_evaluation\u3002||\n", "2409.02668": "|**2024-09-04**|[Introduction to Machine Learning](http://arxiv.org/abs/2409.02668)|null|\u672c\u4e66\u4ecb\u7ecd\u4e86\u673a\u5668\u5b66\u4e60\u4e2d\u8bb8\u591a\u7b97\u6cd5\u7684\u5f00\u53d1\u548c\u5206\u6790\u6240\u4f9d\u8d56\u7684\u6570\u5b66\u57fa\u7840\u548c\u6280\u672f\u3002\u672c\u4e66\u9996\u5148\u4ecb\u7ecd\u4e86\u8d2f\u7a7f\u5168\u4e66\u7684\u7b26\u53f7\u8868\u793a\uff0c\u5e76\u56de\u987e\u4e86\u5fae\u79ef\u5206\u3001\u7ebf\u6027\u4ee3\u6570\u548c\u6982\u7387\u8bba\u7684\u57fa\u672c\u6982\u5ff5\uff0c\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6d4b\u5ea6\u8bba\u672f\u8bed\uff0c\u53ef\u4f5c\u4e3a\u4f7f\u7528\u8fd9\u4e9b\u5de5\u5177\u7684\u90e8\u5206\u7684\u9605\u8bfb\u6307\u5357\u3002\u5bfc\u8bba\u7ae0\u8282\u8fd8\u63d0\u4f9b\u4e86\u77e9\u9635\u5206\u6790\u548c\u4f18\u5316\u7684\u80cc\u666f\u77e5\u8bc6\u3002\u540e\u9762\u7684\u7ae0\u8282\u4e3a\u672c\u4e66\u4e2d\u4f7f\u7528\u7684\u8bb8\u591a\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5305\u62ec\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u3001\u8fd1\u4f3c\u65b9\u6cd5\u7b49\u3002\u5728\u8ba8\u8bba\u4e86\u7edf\u8ba1\u9884\u6d4b\u7684\u57fa\u672c\u6982\u5ff5\u4e4b\u540e\uff0c\u672c\u4e66\u4ecb\u7ecd\u4e86\u518d\u751f\u6838\u7406\u8bba\u548c\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u6280\u672f\uff0c\u8fd9\u4e9b\u6280\u672f\u5728\u8bb8\u591a\u5730\u65b9\u90fd\u6709\u5e94\u7528\uff0c\u7136\u540e\u4ecb\u7ecd\u4e86\u5404\u79cd\u76d1\u7763\u7edf\u8ba1\u5b66\u4e60\u7b97\u6cd5\uff0c\u5305\u62ec\u7ebf\u6027\u65b9\u6cd5\u3001\u652f\u6301\u5411\u91cf\u673a\u3001\u51b3\u7b56\u6811\u3001boosting\u548c\u795e\u7ecf\u7f51\u7edc\u3002\u63a5\u4e0b\u6765\u8f6c\u5411\u751f\u6210\u65b9\u6cd5\uff0c\u9996\u5148\u4ecb\u7ecd\u4e86\u91c7\u6837\u65b9\u6cd5\u548c\u9a6c\u5c14\u53ef\u592b\u94fe\u7406\u8bba\u3002\u63a5\u4e0b\u6765\u7684\u7ae0\u8282\u63cf\u8ff0\u4e86\u56fe\u6a21\u578b\u7406\u8bba\uff0c\u4ecb\u7ecd\u4e86\u6f5c\u53d8\u91cf\u6a21\u578b\u7684\u53d8\u5206\u65b9\u6cd5\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u751f\u6210\u6a21\u578b\u3002\u63a5\u4e0b\u6765\u7684\u7ae0\u8282\u91cd\u70b9\u4ecb\u7ecd\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ec\u805a\u7c7b\u3001\u56e0\u5b50\u5206\u6790\u548c\u6d41\u5f62\u5b66\u4e60\u3002\u672c\u4e66\u7684\u6700\u540e\u4e00\u7ae0\u504f\u5411\u7406\u8bba\uff0c\u8ba8\u8bba\u4e86\u96c6\u4e2d\u4e0d\u7b49\u5f0f\u548c\u6cdb\u5316\u754c\u3002||\n", "2409.02664": "|**2024-09-04**|[Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection](http://arxiv.org/abs/2409.02664)|null|The proliferation of deepfake faces poses huge potential negative impacts on our daily lives. Despite substantial advancements in deepfake detection over these years, the generalizability of existing methods against forgeries from unseen datasets or created by emerging generative models remains constrained. In this paper, inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach that repurposes a well-trained VLM for general deepfake detection. Motivated by the model reprogramming paradigm that manipulates the model prediction via data perturbations, our method can reprogram a pretrained VLM model (e.g., CLIP) solely based on manipulating its input without tuning the inner parameters. Furthermore, we insert a pseudo-word guided by facial identity into the text prompt. Extensive experiments on several popular benchmarks demonstrate that (1) the cross-dataset and cross-manipulation performances of deepfake detection can be significantly and consistently improved (e.g., over 88% AUC in cross-dataset setting from FF++ to WildDeepfake) using a pre-trained CLIP model with our proposed reprogramming method; (2) our superior performances are at less cost of trainable parameters, making it a promising approach for real-world applications.||\n", "2409.02657": "|**2024-09-04**|[PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for One-Shot Talking Head Generation](http://arxiv.org/abs/2409.02657)|null|While previous audio-driven talking head generation (THG) methods generate head poses from driving audio, the generated poses or lips cannot match the audio well or are not editable. In this study, we propose \\textbf{PoseTalk}, a THG system that can freely generate lip-synchronized talking head videos with free head poses conditioned on text prompts and audio. The core insight of our method is using head pose to connect visual, linguistic, and audio signals. First, we propose to generate poses from both audio and text prompts, where the audio offers short-term variations and rhythm correspondence of the head movements and the text prompts describe the long-term semantics of head motions. To achieve this goal, we devise a Pose Latent Diffusion (PLD) model to generate motion latent from text prompts and audio cues in a pose latent space. Second, we observe a loss-imbalance problem: the loss for the lip region contributes less than 4\\% of the total reconstruction loss caused by both pose and lip, making optimization lean towards head movements rather than lip shapes. To address this issue, we propose a refinement-based learning strategy to synthesize natural talking videos using two cascaded networks, i.e., CoarseNet, and RefineNet. The CoarseNet estimates coarse motions to produce animated images in novel poses and the RefineNet focuses on learning finer lip motions by progressively estimating lip motions from low-to-high resolutions, yielding improved lip-synchronization performance. Experiments demonstrate our pose prediction strategy achieves better pose diversity and realness compared to text-only or audio-only, and our video generator model outperforms state-of-the-art methods in synthesizing talking videos with natural head motions. Project: https://junleen.github.io/projects/posetalk.||\n", "2409.02653": "|**2024-09-04**|[Skip-and-Play: Depth-Driven Pose-Preserved Image Generation for Any Objects](http://arxiv.org/abs/2409.02653)|null|The emergence of diffusion models has enabled the generation of diverse high-quality images solely from text, prompting subsequent efforts to enhance the controllability of these models. Despite the improvement in controllability, pose control remains limited to specific objects (e.g., humans) or poses (e.g., frontal view) due to the fact that pose is generally controlled via camera parameters (e.g., rotation angle) or keypoints (e.g., eyes, nose). Specifically, camera parameters-conditional pose control models generate unrealistic images depending on the object, owing to the small size of 3D datasets for training. Also, keypoint-based approaches encounter challenges in acquiring reliable keypoints for various objects (e.g., church) or poses (e.g., back view). To address these limitations, we propose depth-based pose control, as depth maps are easily obtainable from a single depth estimation model regardless of objects and poses, unlike camera parameters and keypoints. However, depth-based pose control confronts issues of shape dependency, as depth maps influence not only the pose but also the shape of the generated images. To tackle this issue, we propose Skip-and-Play (SnP), designed via analysis of the impact of three components of depth-conditional ControlNet on the pose and the shape of the generated images. To be specific, based on the analysis, we selectively skip parts of the components to mitigate shape dependency on the depth map while preserving the pose. Through various experiments, we demonstrate the superiority of SnP over baselines and showcase the ability of SnP to generate images of diverse objects and poses. Remarkably, SnP exhibits the ability to generate images even when the objects in the condition (e.g., a horse) and the prompt (e.g., a hedgehog) differ from each other.||\n", "2409.03757": "|**2024-09-05**|[Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding](http://arxiv.org/abs/2409.03757)|**[link](https://github.com/yunzeman/lexicon3d)**|\u590d\u6742\u4e09\u7ef4\u573a\u666f\u7406\u89e3\u8fd1\u5e74\u6765\u5907\u53d7\u5173\u6ce8\uff0c\u573a\u666f\u7f16\u7801\u7b56\u7565\u5728\u5176\u4e2d\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u9488\u5bf9\u4e0d\u540c\u573a\u666f\u7684\u6700\u4f73\u573a\u666f\u7f16\u7801\u7b56\u7565\u4ecd\u4e0d\u660e\u786e\uff0c\u7279\u522b\u662f\u4e0e\u57fa\u4e8e\u56fe\u50cf\u7684\u7f16\u7801\u7b56\u7565\u76f8\u6bd4\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5bf9\u7528\u4e8e\u4e09\u7ef4\u573a\u666f\u7406\u89e3\u7684\u5404\u79cd\u89c6\u89c9\u7f16\u7801\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u7814\u7a76\uff0c\u786e\u5b9a\u4e86\u6bcf\u4e2a\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u6db5\u76d6\u4e86\u4e03\u79cd\u89c6\u89c9\u57fa\u7840\u7f16\u7801\u5668\uff0c\u5305\u62ec\u57fa\u4e8e\u56fe\u50cf\u3001\u57fa\u4e8e\u89c6\u9891\u548c\u4e09\u7ef4\u57fa\u7840\u6a21\u578b\u3002\u6211\u4eec\u5728\u56db\u4e2a\u4efb\u52a1\u4e2d\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\uff1a\u89c6\u89c9\u8bed\u8a00\u573a\u666f\u63a8\u7406\u3001\u89c6\u89c9\u5b9a\u4f4d\u3001\u5206\u5272\u548c\u914d\u51c6\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u4fa7\u91cd\u4e8e\u573a\u666f\u7406\u89e3\u7684\u4e0d\u540c\u65b9\u9762\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u5f97\u51fa\u4e86\u4ee5\u4e0b\u4e3b\u8981\u53d1\u73b0\uff1aDINOv2 \u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u89c6\u9891\u6a21\u578b\u5728\u5bf9\u8c61\u7ea7\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6269\u6563\u6a21\u578b\u6709\u5229\u4e8e\u51e0\u4f55\u4efb\u52a1\uff0c\u800c\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u8bed\u8a00\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u610f\u60f3\u4e0d\u5230\u7684\u5c40\u9650\u6027\u3002\u8fd9\u4e9b\u89c1\u89e3\u6311\u6218\u4e86\u4e00\u4e9b\u4f20\u7edf\u8ba4\u77e5\uff0c\u4e3a\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u672a\u6765\u7684\u89c6\u89c9\u8bed\u8a00\u548c\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e2d\u9700\u8981\u66f4\u7075\u6d3b\u7684\u7f16\u7801\u5668\u9009\u62e9\u3002||\n", "2409.03745": "|**2024-09-05**|[ArtiFade: Learning to Generate High-quality Subject from Blemished Images](http://arxiv.org/abs/2409.03745)|null|\u4ee5\u4e3b\u9898\u4e3a\u4e3b\u5bfc\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6280\u672f\u5728\u5b66\u4e60\u548c\u6355\u6349\u4e3b\u9898\u7279\u5f81\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u5373\u4f7f\u53ea\u4f7f\u7528\u6709\u9650\u6570\u91cf\u7684\u56fe\u50cf\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u5f53\u8f93\u5165\u56fe\u50cf\u5b58\u5728\u7455\u75b5\u65f6\uff0c\u53ef\u80fd\u96be\u4ee5\u751f\u6210\u5408\u7406\u7684\u56fe\u50cf\u3002\u8fd9\u4e3b\u8981\u5f52\u56e0\u4e8e\u5f53\u524d\u6280\u672f\u5728\u533a\u5206\u4e3b\u9898\u76f8\u5173\u7279\u5f81\u548c\u5e72\u6270\u6027\u7455\u75b5\u65b9\u9762\u7684\u80fd\u529b\u4e0d\u8db3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86ArtiFade\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u6210\u529f\u5730\u4ece\u6709\u7455\u75b5\u7684\u6570\u636e\u96c6\u4e2d\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u7684\u65e0\u7455\u75b5\u56fe\u50cf\u3002\u5177\u4f53\u6765\u8bf4\uff0cArtiFade\u5229\u7528\u9884\u5148\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5fae\u8c03\u6765\u6d88\u9664\u7455\u75b5\u3002\u901a\u8fc7\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u5305\u542b\u65e0\u7455\u75b5\u56fe\u50cf\u53ca\u5176\u5bf9\u5e94\u7684\u6709\u7455\u75b5\u56fe\u50cf\u7684\u4e13\u95e8\u6570\u636e\u96c6\u6765\u5b9e\u73b0\u7455\u75b5\u7684\u6d88\u9664\u3002ArtiFade\u8fd8\u786e\u4fdd\u4e86\u4fdd\u7559\u6269\u6563\u6a21\u578b\u4e2d\u56fa\u6709\u7684\u539f\u59cb\u751f\u6210\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u4e3b\u9898\u9a71\u52a8\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u548c\u65e0\u7455\u75b5\u56fe\u50cf\u65b9\u9762\u7684\u6574\u4f53\u6027\u80fd\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u4e3a\u8fd9\u9879\u4efb\u52a1\u8bbe\u8ba1\u4e86\u8bc4\u4f30\u57fa\u51c6\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86ArtiFade\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u60c5\u51b5\u4e0b\u90fd\u80fd\u6709\u6548\u53bb\u9664\u7455\u75b5\u7684\u6cdb\u5316\u80fd\u529b\u3002||\n", "2409.03644": "|**2024-09-05**|[RealisHuman: A Two-Stage Approach for Refining Malformed Human Parts in Generated Images](http://arxiv.org/abs/2409.03644)|null|\u8fd1\u5e74\u6765\uff0c\u6269\u6563\u6a21\u578b\u5f7b\u5e95\u6539\u53d8\u4e86\u89c6\u89c9\u751f\u6210\u9886\u57df\uff0c\u5176\u6027\u80fd\u8d85\u8d8a\u4e86\u751f\u6210\u5bf9\u6297\u7f51\u7edc (GANs) \u7b49\u4f20\u7edf\u6846\u67b6\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4eba\u7c7b\u53ca\u5176\u8bed\u4e49\u90e8\u5206\uff08\u5982\u624b\u548c\u8138\uff09\u590d\u6742\u7684\u7ed3\u6784\uff0c\u751f\u6210\u5177\u6709\u771f\u5b9e\u611f\u7684\u4eba\u7c7b\u56fe\u50cf\u4ecd\u7136\u662f\u4e00\u9879\u91cd\u5927\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a RealisHuman \u7684\u65b0\u578b\u540e\u5904\u7406\u89e3\u51b3\u65b9\u6848\u3002RealisHuman \u6846\u67b6\u5206\u4e24\u4e2a\u9636\u6bb5\u8fd0\u884c\u3002\u9996\u5148\uff0c\u5b83\u4f7f\u7528\u539f\u59cb\u7684\u7578\u5f62\u90e8\u5206\u4f5c\u4e3a\u53c2\u8003\uff0c\u751f\u6210\u903c\u771f\u7684\u4eba\u4f53\u90e8\u4f4d\uff08\u5982\u624b\u6216\u8138\uff09\uff0c\u786e\u4fdd\u7ec6\u8282\u4e0e\u539f\u59cb\u56fe\u50cf\u4e00\u81f4\u3002\u5176\u6b21\uff0c\u5b83\u901a\u8fc7\u91cd\u65b0\u7ed8\u5236\u5468\u56f4\u533a\u57df\u5c06\u6821\u6b63\u540e\u7684\u4eba\u4f53\u90e8\u4f4d\u65e0\u7f1d\u5730\u878d\u5165\u5230\u5176\u5bf9\u5e94\u7684\u4f4d\u7f6e\uff0c\u4ee5\u786e\u4fdd\u5e73\u6ed1\u903c\u771f\u7684\u878d\u5408\u3002RealisHuman \u6846\u67b6\u663e\u8457\u589e\u5f3a\u4e86\u4eba\u7c7b\u751f\u6210\u7684\u771f\u5b9e\u611f\uff0c\u8fd9\u53ef\u4ee5\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u6307\u6807\u7684\u663e\u8457\u6539\u8fdb\u5f97\u5230\u8bc1\u660e\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/Wangbenzhi/RealisHuman \u83b7\u53d6\u3002||\n", "2409.03636": "|**2024-09-05**|[DiffEVC: Any-to-Any Emotion Voice Conversion with Expressive Guidance](http://arxiv.org/abs/2409.03636)|null|\u60c5\u611f\u8bed\u97f3\u8f6c\u6362 (EVC) \u901a\u8fc7\u653e\u5927\u79ef\u6781\u7ebf\u7d22\u548c\u51cf\u5c11\u6d88\u6781\u7ebf\u7d22\u6765\u6539\u53d8\u8bed\u97f3\u60c5\u611f\uff0c\u4ece\u800c\u589e\u5f3a\u6c9f\u901a\u3002\u8fd9\u9879\u590d\u6742\u7684\u4efb\u52a1\u6d89\u53ca\u8bed\u97f3\u8d28\u91cf\u3001\u8bf4\u8bdd\u8005\u7279\u5f81\u548c\u5185\u5bb9\u7b49\u7ea0\u7f20\u4e0d\u6e05\u7684\u56e0\u7d20\u3002\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982 GAN \u548c\u81ea\u52a8\u7f16\u7801\u5668\uff09\u901a\u8fc7\u5b66\u4e60\u6620\u5c04\u6216\u89e3\u8026\u7279\u5f81\u5728 EVC \u4e2d\u53d6\u5f97\u4e86\u4e00\u5b9a\u7684\u6210\u529f\uff0c\u4f46\u9762\u4e34\u7740\u4e0d\u7a33\u5b9a\u6027\u548c\u8bed\u97f3\u8d28\u91cf\u4e0b\u964d\u7b49\u6311\u6218\u3002\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u7a33\u5b9a\u7684\u8bad\u7ec3\u548c\u9ad8\u8d28\u91cf\u7684\u751f\u6210\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684 EVC \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u4e92\u4fe1\u606f\u635f\u5931\u548c\u8f85\u52a9\u6a21\u578b\u6765\u89e3\u8026\u60c5\u611f\u548c\u8bf4\u8bdd\u8005\u8eab\u4efd\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u8868\u8fbe\u6027\u5f15\u5bfc\u673a\u5236\uff0c\u4ee5\u6539\u5584\u60c5\u611f\u8f6c\u6362\uff0c\u540c\u65f6\u4fdd\u6301\u8bf4\u8bdd\u8005\u7279\u5f81\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5bf9\u4e8e\u672a\u77e5\u8bf4\u8bdd\u8005\u548c\u60c5\u611f\u7684\u6709\u6548\u6027\uff0c\u5728 EVC \u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002||\n", "2409.03600": "|**2024-09-05**|[TCDiff: Triple Condition Diffusion Model with 3D Constraints for Stylizing Synthetic Faces](http://arxiv.org/abs/2409.03600)|**[link](https://github.com/bovifocr/tcdiff)**|\u4e00\u4e2a\u9c81\u68d2\u7684\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u9700\u8981\u4f7f\u7528\u5305\u542b\u5927\u91cf\u4e2a\u4f53\u4ee5\u53ca\u6bcf\u4e2a\u4e2a\u4f53\u5728\u4e0d\u540c\u6761\u4ef6\uff08\u4f8b\u5982\u59ff\u6001\u3001\u8868\u60c5\u3001\u5e74\u9f84\u3001\u566a\u58f0\u548c\u906e\u6321\uff09\u4e0b\u7684\u5927\u91cf\u6837\u672c\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002\u7531\u4e8e\u4f26\u7406\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5927\u578b\u771f\u5b9e\u4eba\u8138\u6570\u636e\u96c6\uff08\u4f8b\u5982 MS1MV3\uff09\u5df2\u88ab\u505c\u7528\uff0c\u5e76\u4e14\u5df2\u7ecf\u63d0\u51fa\u4e86\u5229\u7528 GAN \u548c\u6269\u6563\u6a21\u578b\u7684\u5408\u6210\u4eba\u8138\u751f\u6210\u5668\uff0c\u4f8b\u5982 SYNFace\u3001SFace\u3001DigiFace-1M\u3001IDiff-Face\u3001DCFace \u548c GANDiffFace\uff0c\u65e8\u5728\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u3002\u5176\u4e2d\u4e00\u4e9b\u65b9\u6cd5\u53ef\u4ee5\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u771f\u5b9e\u4eba\u8138\uff0c\u4f46\u7c7b\u5185\u5dee\u5f02\u8f83\u4f4e\uff0c\u800c\u53e6\u4e00\u4e9b\u65b9\u6cd5\u5219\u751f\u6210\u5177\u6709\u9ad8\u5dee\u5f02\u6027\u4f46\u8eab\u4efd\u4e00\u81f4\u6027\u8f83\u4f4e\u7684\u4eba\u8138\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u91cd\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff08TCDiff\uff09\uff0c\u901a\u8fc7 2D \u548c 3D \u4eba\u8138\u7ea6\u675f\u6765\u6539\u8fdb\u4ece\u771f\u5b9e\u4eba\u8138\u5230\u5408\u6210\u4eba\u8138\u7684\u4eba\u8138\u98ce\u683c\u8fc1\u79fb\uff0c\u5728\u4fdd\u6301\u5fc5\u8981\u7684\u7c7b\u5185\u9ad8\u5dee\u5f02\u6027\u7684\u540c\u65f6\u589e\u5f3a\u4eba\u8138\u8eab\u4efd\u4e00\u81f4\u6027\u3002\u4f7f\u7528\u6211\u4eec\u65b0\u7684\u6570\u636e\u96c6\u7684 1k\u30012k \u548c 5k \u7c7b\u8fdb\u884c\u8bad\u7ec3\u7684\u4eba\u8138\u8bc6\u522b\u5b9e\u9a8c\u5728 LFW\u3001CFP-FP\u3001AgeDB \u548c BUPT \u7b49\u771f\u5b9e\u4eba\u8138\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5408\u6210\u6570\u636e\u96c6\u3002\u6211\u4eec\u7684\u6e90\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u5f97\uff1ahttps://github.com/BOVIFOCR/tcdiff\u3002||\n", "2409.03550": "|**2024-09-05**|[DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any Architecture](http://arxiv.org/abs/2409.03550)|null|\u6269\u6563\u6a21\u578b (DM) \u5728\u5404\u4e2a\u9886\u57df\u90fd\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5176\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u7684\u63a8\u7406\u901f\u5ea6\u6162\u548c\u8ba1\u7b97\u9700\u6c42\u9ad8\u5374\u963b\u788d\u4e86\u5176\u53d1\u5c55\u3002\u52a0\u901fDM\u6700\u5e38\u7528\u7684\u65b9\u6cd5\u662f\u51cf\u5c11\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u53bb\u566a\u6b65\u9aa4\uff0c\u8fd9\u53ef\u4ee5\u901a\u8fc7\u66f4\u5feb\u7684\u91c7\u6837\u6c42\u89e3\u5668\u6216\u77e5\u8bc6\u84b8\u998f (KD) \u6765\u5b9e\u73b0\u3002\u4e0e\u5148\u524d\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5c06\u5927\u578b\u9884\u8bad\u7ec3DM\u7684\u529f\u80fd\u8fc1\u79fb\u5230\u66f4\u5feb\u7684\u67b6\u6784\u4e2d\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u4ee5\u72ec\u7279\u7684\u65b9\u5f0f\u4f7f\u7528KD\uff0c\u901a\u8fc7\u5c06\u751f\u6210\u80fd\u529b\u63d0\u70bc\u5230\u66f4\u5feb\u7684\u53d8\u4f53\u4e2d\u6765\u538b\u7f29DM\u3002\u6b64\u5916\uff0c\u8003\u8651\u5230\u6e90\u6570\u636e\u4e0d\u53ef\u8bbf\u95ee\u6216\u5bf9\u4e8e\u5f53\u524d\u7684\u751f\u6210\u6a21\u578b\u6765\u8bf4\u5b58\u50a8\u91cf\u592a\u5927\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u6e90\u6570\u636e\u84b8\u998f\u8303\u5f0f\uff0c\u79f0\u4e3a\u6269\u6563\u6a21\u578b\u7684\u65e0\u6570\u636e\u77e5\u8bc6\u84b8\u998f (DKDM)\u3002\u901a\u5e38\uff0c\u6211\u4eec\u5efa\u7acb\u7684DKDM\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a1) DKDM\u76ee\u6807\u51fd\u6570\uff0c\u5b83\u4f7f\u7528\u9884\u8bad\u7ec3DM\u751f\u6210\u7684\u5408\u6210\u53bb\u566a\u6570\u636e\u6765\u4f18\u5316\u66f4\u5feb\u7684DM\uff0c\u800c\u65e0\u9700\u6e90\u6570\u636e\uff1b2) \u52a8\u6001\u8fed\u4ee3\u84b8\u998f\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u7075\u6d3b\u5730\u7ec4\u7ec7\u53bb\u566a\u6570\u636e\u7684\u5408\u6210\uff0c\u9632\u6b62\u7531\u4e8e\u751f\u6210\u901f\u5ea6\u6162\u800c\u51cf\u6162\u4f18\u5316\u8fc7\u7a0b\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u6b21\u5c1d\u8bd5\u4f7f\u7528KD\u4ee5\u65e0\u6570\u636e\u7684\u65b9\u5f0f\u5c06DM\u63d0\u70bc\u5230\u4efb\u4f55\u67b6\u6784\u4e2d\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u7684DKDM\u4e0e\u5927\u591a\u6570\u73b0\u6709\u7684\u52a0\u901f\u65b9\u6cd5\uff08\u4f8b\u5982\u51cf\u5c11\u53bb\u566a\u6b65\u9aa4\u3001\u91cf\u5316\u548c\u526a\u679d\uff09\u662f\u6b63\u4ea4\u7684\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684DKDM\u80fd\u591f\u63a8\u5bfc\u51fa\u901f\u5ea6\u63d0\u9ad82\u500d\u7684DM\uff0c\u5176\u6027\u80fd\u4e0e\u57fa\u7ebf\u4fdd\u6301\u4e00\u81f4\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684DKDM\u4f7f\u9884\u8bad\u7ec3\u7684DM\u80fd\u591f\u4f5c\u4e3a\u201c\u6570\u636e\u96c6\u201d\u6765\u8bad\u7ec3\u65b0\u7684DM\u3002||\n", "2409.03514": "|**2024-09-05**|[Blended Latent Diffusion under Attention Control for Real-World Video Editing](http://arxiv.org/abs/2409.03514)|null|\u7531\u4e8e\u7f3a\u4e4f\u5b8c\u5168\u516c\u5f00\u53ef\u7528\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\uff0c\u76ee\u524d\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u503e\u5411\u4e8e\u5efa\u7acb\u5728\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e4b\u4e0a\uff0c\u7136\u800c\uff0c\u5728\u5904\u7406\u5177\u6709\u65f6\u95f4\u4fe1\u606f\u7684\u89c6\u9891\u5c40\u90e8\u7f16\u8f91\u65b9\u9762\uff0c\u5b83\u4eec\u4ecd\u7136\u9762\u4e34\u7740\u5de8\u5927\u7684\u6311\u6218\u3002\u9996\u5148\uff0c\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u8bd5\u56fe\u901a\u8fc7\u9884\u5148\u5b9a\u4e49\u7684\u63a9\u7801\u6765\u5173\u6ce8\u5c40\u90e8\u533a\u57df\u7f16\u8f91\uff0c\u4f46\u7531\u4e8e\u6bcf\u4e00\u5e27\u7684\u7a7a\u95f4\u6574\u4f53\u751f\u6210\uff0c\u5916\u90e8\u533a\u57df\u80cc\u666f\u7684\u4fdd\u7559\u5e76\u4e0d\u7406\u60f3\u3002\u6b64\u5916\uff0c\u7531\u7528\u6237\u4e13\u95e8\u63d0\u4f9b\u63a9\u7801\u662f\u4e00\u9879\u989d\u5916\u7684\u6602\u8d35\u5de5\u4f5c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u96c6\u6210\u5230\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u7684\u81ea\u4e3b\u63a9\u7801\u7b56\u7565\u3002\u6700\u540e\u4f46\u540c\u6837\u91cd\u8981\u7684\u662f\uff0c\u56fe\u50cf\u7ea7\u9884\u8bad\u7ec3\u6a21\u578b\u6ca1\u6709\u5b66\u4e60\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u800c\u8fd9\u5bf9\u4e8e\u8868\u8fbe\u8fd0\u52a8\u548c\u52a8\u6001\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u91c7\u7528\u56fe\u50cf\u7ea7\u6df7\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6765\u6267\u884c\u5c40\u90e8\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5229\u7528 DDIM \u53cd\u6f14\u6765\u83b7\u53d6\u6f5c\u5728\u5411\u91cf\u4f5c\u4e3a\u80cc\u666f\u6f5c\u5728\u5411\u91cf\uff0c\u800c\u4e0d\u662f\u968f\u673a\u566a\u58f0\u7684\u6f5c\u5728\u5411\u91cf\uff0c\u4ee5\u66f4\u597d\u5730\u4fdd\u7559\u8f93\u5165\u89c6\u9891\u7684\u80cc\u666f\u4fe1\u606f\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4ece\u6269\u6563\u6b65\u9aa4\u4e2d\u7684\u4ea4\u53c9\u6ce8\u610f\u56fe\u884d\u751f\u7684\u81ea\u4e3b\u63a9\u7801\u5236\u9020\u673a\u5236\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u5c06 U-Net \u7684\u81ea\u6ce8\u610f\u529b\u5757\u8f6c\u6362\u4e3a\u65f6\u7a7a\u5757\u6765\u589e\u5f3a\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002||\n", "2409.03455": "|**2024-09-05**|[Data-free Distillation with Degradation-prompt Diffusion for Multi-weather Image Restoration](http://arxiv.org/abs/2409.03455)|null|\u591a\u5929\u6c14\u56fe\u50cf\u590d\u539f\u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u8fdb\u5c55\uff0c\u4f46\u6a21\u578b\u5bb9\u91cf\u7684\u589e\u52a0\u548c\u6602\u8d35\u7684\u6570\u636e\u83b7\u53d6\u9650\u5236\u4e86\u5176\u5728\u5185\u5b58\u6709\u9650\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u3002\u65e0\u6570\u636e\u84b8\u998f\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u5141\u8bb8\u4ece\u9884\u8bad\u7ec3\u7684\u6559\u5e08\u6a21\u578b\u4e2d\u5b66\u4e60\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u3002\u73b0\u6709\u7684\u65e0\u6570\u636e\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5229\u7528GAN\u751f\u6210\u7684\u4f2a\u6570\u636e\u6216\u4ece\u4e92\u8054\u7f51\u6536\u96c6\u7684\u771f\u5b9e\u6570\u636e\u6765\u4f18\u5316\u6a21\u578b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u9047\u5230\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6216\u4e0e\u539f\u59cb\u6570\u636e\u5b58\u5728\u57df\u504f\u79fb\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u9000\u5316\u63d0\u793a\u6269\u6563\u7684\u65e0\u6570\u636e\u84b8\u998f\u591a\u5929\u6c14\u56fe\u50cf\u590d\u539f\u6846\u67b6\uff08D4IR\uff09\u3002\u5b83\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u4ee3\u66ffGAN\u4ee5\u907f\u514d\u6a21\u578b\u5d29\u6e83\uff0c\u5e76\u7ed3\u5408\u4e86\u9000\u5316\u611f\u77e5\u63d0\u793a\u9002\u914d\u5668\uff0c\u4ee5\u4fc3\u8fdb\u5185\u5bb9\u9a71\u52a8\u7684\u6761\u4ef6\u6269\u6563\uff0c\u4ece\u800c\u751f\u6210\u4e0e\u57df\u76f8\u5173\u7684\u56fe\u50cf\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u9996\u5148\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u7684\u9000\u5316\u63d0\u793a\u9002\u914d\u5668\uff0c\u7528\u4e8e\u4ece\u7f51\u7edc\u6536\u96c6\u7684\u9000\u5316\u56fe\u50cf\u4e2d\u6355\u83b7\u9000\u5316\u611f\u77e5\u63d0\u793a\u3002\u7136\u540e\uff0c\u5c06\u6536\u96c6\u5230\u7684\u672a\u914d\u5bf9\u7684\u5e72\u51c0\u56fe\u50cf\u6270\u52a8\u5230\u7a33\u5b9a\u6269\u6563\u7684\u6f5c\u5728\u7279\u5f81\u4e2d\uff0c\u5e76\u4ee5\u9000\u5316\u611f\u77e5\u63d0\u793a\u4e3a\u6761\u4ef6\uff0c\u5408\u6210\u65b0\u7684\u57df\u76f8\u5173\u9000\u5316\u56fe\u50cf\uff0c\u7528\u4e8e\u77e5\u8bc6\u84b8\u998f\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u4e0e\u4f7f\u7528\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u84b8\u998f\u7684\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u751a\u81f3\u4f18\u4e8e\u5176\u4ed6\u4e3b\u6d41\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u3002||\n", "2409.03417": "|**2024-09-05**|[Convergence Rates for the Maximum A Posteriori Estimator in PDE-Regression Models with Random Design](http://arxiv.org/abs/2409.03417)|null|\u6211\u4eec\u8003\u8651\u4ece\u9ad8\u65af\u56de\u5f52\u95ee\u9898$Y = \\mathscr{G}(\\theta)(Z)+\\varepsilon$\u4ea7\u751f\u7684\u6570\u636e\u4e2d\u6062\u590d\u53c2\u6570$\\theta\\in H^\\alpha$\u7684\u7edf\u8ba1\u9006\u95ee\u9898\uff0c\u5176\u4e2d$\\mathscr{G}:\\mathbb{L}^2\\to\\mathbb{L}^2$\u662f\u975e\u7ebf\u6027\u6b63\u5411\u6620\u5c04\uff0c$Z$\u662f\u968f\u673a\u8bbe\u8ba1\u70b9\uff0c$\\varepsilon$\u662f\u9ad8\u65af\u566a\u58f0\u3002\u4f30\u8ba1\u7b56\u7565\u57fa\u4e8e$\\Vert\\cdot\\Vert_{H^\\alpha}$-\u7ea6\u675f\u4e0b\u7684\u6700\u5c0f\u4e8c\u4e58\u6cd5\u3002\u6211\u4eec\u5728\u6b63\u5411\u6620\u5c04$\\mathscr{G}$\u6ee1\u8db3Lipschitz\u7c7b\u578b\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\uff0c\u5efa\u7acb\u4e86\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\u91cf$\\hat{\\theta}$\u4f5c\u4e3a\u7ed9\u5b9a\u6cdb\u51fd\u7684\u6700\u5927\u503c\u7684\u5b58\u5728\u6027\u3002\u8bc1\u660e\u4e86\u4e00\u4e2a\u4e00\u822c\u7684\u6d53\u5ea6\u7ed3\u679c\uff0c\u5e76\u7528\u5b83\u6765\u8bc1\u660e\u9884\u6d4b\u8bef\u5dee\u7684\u4e00\u81f4\u6027\u548c\u4e0a\u754c\u3002\u76f8\u5e94\u7684\u6536\u655b\u901f\u5ea6\u4e0d\u4ec5\u53cd\u6620\u4e86\u76ee\u6807\u53c2\u6570\u7684\u5e73\u6ed1\u6027\uff0c\u8fd8\u53cd\u6620\u4e86\u6f5c\u5728\u9006\u95ee\u9898\u7684\u9002\u5b9a\u6027\u3002\u6211\u4eec\u5c06\u4e00\u822c\u6a21\u578b\u5e94\u7528\u4e8e\u8fbe\u897f\u95ee\u9898\uff0c\u5176\u4e2dPDE\u7684\u672a\u77e5\u7cfb\u6570\u51fd\u6570$f$\u7684\u6062\u590d\u662f\u4ee4\u4eba\u611f\u5174\u8da3\u7684\u3002\u5bf9\u4e8e\u8fd9\u4e2a\u4f8b\u5b50\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u9884\u6d4b\u8bef\u5dee\u548c\u4f30\u8ba1\u8bef\u5dee\u7684\u76f8\u5e94\u6536\u655b\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u7b80\u8981\u8ba8\u8bba\u4e86\u8be5\u4e00\u822c\u6a21\u578b\u5bf9\u5176\u4ed6\u95ee\u9898\u7684\u9002\u7528\u6027\u3002||\n", "2409.03403": "|**2024-09-05**|[RoVi-Aug: Robot and Viewpoint Augmentation for Cross-Embodiment Robot Learning](http://arxiv.org/abs/2409.03403)|null|\u6269\u5927\u673a\u5668\u4eba\u5b66\u4e60\u89c4\u6a21\u9700\u8981\u5e9e\u5927\u800c\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\uff0c\u5982\u4f55\u6709\u6548\u5730\u91cd\u590d\u4f7f\u7528\u6536\u96c6\u5230\u7684\u6570\u636e\u5e76\u5c06\u7b56\u7565\u8fc1\u79fb\u5230\u65b0\u7684\u673a\u5668\u4eba\u5e73\u53f0\u4ecd\u7136\u662f\u4e00\u4e2a\u60ac\u800c\u672a\u51b3\u7684\u95ee\u9898\u3002\u8bf8\u5982Open-X Embodiment (OXE) \u9879\u76ee\u7b49\u65b0\u5174\u7814\u7a76\u5df2\u7ecf\u8868\u660e\uff0c\u901a\u8fc7\u7ec4\u5408\u5305\u542b\u4e0d\u540c\u673a\u5668\u4eba\u7684\u6570\u636e\u96c6\u6765\u5229\u7528\u6280\u80fd\u662f\u6709\u5e0c\u671b\u7684\u3002\u7136\u800c\uff0c\u8bb8\u591a\u6570\u636e\u96c6\u4e2d\u673a\u5668\u4eba\u7c7b\u578b\u548c\u76f8\u673a\u89d2\u5ea6\u5206\u5e03\u7684\u4e0d\u5e73\u8861\u4f7f\u5f97\u7b56\u7565\u5bb9\u6613\u8fc7\u62df\u5408\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86RoVi-Aug\uff0c\u5b83\u5229\u7528\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u5408\u6210\u5177\u6709\u4e0d\u540c\u673a\u5668\u4eba\u548c\u76f8\u673a\u89c6\u89d2\u7684\u6f14\u793a\u6765\u589e\u5f3a\u673a\u5668\u4eba\u6570\u636e\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u7269\u7406\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u901a\u8fc7\u5728\u673a\u5668\u4eba\u548c\u89c6\u70b9\u589e\u5f3a\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0cRoVi-Aug \u53ef\u4ee5\u5728\u5177\u6709\u663e\u8457\u4e0d\u540c\u76f8\u673a\u89d2\u5ea6\u7684\u672a\u77e5\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u90e8\u7f72\u3002\u4e0e Mirage \u7b49\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u7b97\u6cd5\u76f8\u6bd4\uff0cRoVi-Aug \u5728\u6d4b\u8bd5\u65f6\u4e0d\u9700\u8981\u989d\u5916\u7684\u5904\u7406\uff0c\u4e0d\u5047\u8bbe\u5df2\u77e5\u76f8\u673a\u89d2\u5ea6\uff0c\u5e76\u4e14\u5141\u8bb8\u7b56\u7565\u5fae\u8c03\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5728\u539f\u59cb\u673a\u5668\u4eba\u6570\u636e\u96c6\u548c\u589e\u5f3a\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\uff0cRoVi-Aug \u53ef\u4ee5\u5b66\u4e60\u591a\u673a\u5668\u4eba\u548c\u591a\u4efb\u52a1\u7b56\u7565\uff0c\u4ece\u800c\u5b9e\u73b0\u673a\u5668\u4eba\u548c\u6280\u80fd\u4e4b\u95f4\u66f4\u6709\u6548\u7684\u8fc1\u79fb\uff0c\u5e76\u5c06\u6210\u529f\u7387\u63d0\u9ad8\u9ad8\u8fbe 30%\u3002||\n", "2409.05798": "|**2024-09-09**|[Enhancing Preference-based Linear Bandits via Human Response Time](http://arxiv.org/abs/2409.05798)|null|Binary human choice feedback is widely used in interactive preference learning for its simplicity, but it provides limited information about preference strength. To overcome this limitation, we leverage human response times, which inversely correlate with preference strength, as complementary information. Our work integrates the EZ-diffusion model, which jointly models human choices and response times, into preference-based linear bandits. We introduce a computationally efficient utility estimator that reformulates the utility estimation problem using both choices and response times as a linear regression problem. Theoretical and empirical comparisons with traditional choice-only estimators reveal that for queries with strong preferences (\"easy\" queries), choices alone provide limited information, while response times offer valuable complementary information about preference strength. As a result, incorporating response times makes easy queries more useful. We demonstrate this advantage in the fixed-budget best-arm identification problem, with simulations based on three real-world datasets, consistently showing accelerated learning when response times are incorporated.||\n", "2409.05790": "|**2024-09-09**|[Predicting Critical Heat Flux with Uncertainty Quantification and Domain Generalization Using Conditional Variational Autoencoders and Deep Neural Networks](http://arxiv.org/abs/2409.05790)|null|Deep generative models (DGMs) have proven to be powerful in generating realistic data samples. Their capability to learn the underlying distribution of a dataset enable them to generate synthetic data samples that closely resemble the original training dataset, thus addressing the challenge of data scarcity. In this work, we investigated the capabilities of DGMs by developing a conditional variational autoencoder (CVAE) model to augment the critical heat flux (CHF) measurement data that was used to generate the 2006 Groeneveld lookup table. To determine how this approach compared to traditional methods, a fine-tuned deep neural network (DNN) regression model was created and evaluated with the same dataset. Both the CVAE and DNN models achieved small mean absolute relative errors, with the CVAE model maintaining more favorable results. To quantify the uncertainty in the model's predictions, uncertainty quantification (UQ) was performed with repeated sampling of the CVAE model and ensembling of the DNN model. Following UQ, the DNN ensemble notably improved performance when compared to the baseline DNN model, while the CVAE model achieved similar results to its non-UQ results. The CVAE model was shown to have significantly less variability and a higher confidence after assessment of the prediction-wise relative standard deviations. Evaluating domain generalization, both models achieved small mean error values when predicting both inside and outside the training domain, with predictions outside the training domain showing slightly larger errors. Overall, the CVAE model was comparable to the DNN regression model in predicting CHF values but with better uncertainty behavior.||\n", "2409.05784": "|**2024-09-09**|[Vector Quantized Diffusion Model Based Speech Bandwidth Extension](http://arxiv.org/abs/2409.05784)|null|\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668 (NAC) \u7684\u6700\u65b0\u8fdb\u5c55\u4e3a\u97f3\u9891\u4fe1\u53f7\u5904\u7406\u89e3\u9501\u4e86\u65b0\u7684\u6f5c\u529b\u3002\u8d8a\u6765\u8d8a\u591a\u7684\u7814\u7a76\u63a2\u7d22\u5229\u7528 NAC \u7684\u6f5c\u5728\u7279\u5f81\u6765\u5b8c\u6210\u5404\u79cd\u8bed\u97f3\u4fe1\u53f7\u5904\u7406\u4efb\u52a1\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u7b2c\u4e00\u79cd\u5229\u7528\u4ece NAC \u83b7\u5f97\u7684\u79bb\u6563\u7279\u5f81\u8fdb\u884c\u8bed\u97f3\u5e26\u5bbd\u6269\u5c55 (BWE) \u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u6062\u590d\u9ad8\u5ea6\u538b\u7f29\u7684\u79bb\u6563\u6807\u8bb0\u4e2d\u7684\u9ad8\u9891\u7ec6\u8282\uff0c\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u8bed\u97f3\u7684\u6e05\u6670\u5ea6\u548c\u81ea\u7136\u5ea6\u3002\u6240\u63d0\u51fa\u7684\u6846\u67b6\u57fa\u4e8e\u77e2\u91cf\u91cf\u5316\u6269\u6563\uff0c\u7ed3\u5408\u4e86\u5148\u8fdb NAC\u3001\u6269\u6563\u6a21\u578b\u548c Mamba-2 \u7684\u4f18\u52bf\uff0c\u4ee5\u91cd\u5efa\u9ad8\u9891\u8bed\u97f3\u6210\u5206\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5bf9\u6570\u8c31\u8ddd\u79bb\u548c ViSQOL \u65b9\u9762\u5747\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u663e\u7740\u63d0\u9ad8\u4e86\u8bed\u97f3\u8d28\u91cf\u3002||\n", "2409.05730": "|**2024-09-09**|[AS-Speech: Adaptive Style For Speech Synthesis](http://arxiv.org/abs/2409.05730)|null|\u8fd1\u5e74\u6765\uff0c\u6587\u672c\u5230\u8bed\u97f3\uff08TTS\uff09\u5408\u6210\u6280\u672f\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u80fd\u591f\u5728\u5e38\u89c1\u573a\u666f\u4e0b\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u3002\u5728\u672a\u77e5\u60c5\u51b5\u4e0b\uff0c\u81ea\u9002\u5e94TTS\u9700\u8981\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u6765\u9002\u5e94\u8bf4\u8bdd\u4eba\u7684\u98ce\u683c\u7279\u5f81\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u53ea\u80fd\u5206\u522b\u63d0\u53d6\u548c\u6574\u5408\u7c97\u7c92\u5ea6\u7684\u97f3\u8272\u6216\u6df7\u5408\u7684\u97f5\u5f8b\u5c5e\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86AS-Speech\uff0c\u4e00\u79cd\u5c06\u8bf4\u8bdd\u4eba\u97f3\u8272\u7279\u5f81\u548c\u97f5\u5f8b\u5c5e\u6027\u6574\u5408\u5230\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u4e2d\u7684\u81ea\u9002\u5e94\u98ce\u683c\u65b9\u6cd5\uff0c\u7528\u4e8e\u6587\u672c\u5230\u8bed\u97f3\u5408\u6210\u3002\u5177\u4f53\u6765\u8bf4\uff0cAS-Speech\u53ef\u4ee5\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u57fa\u4e8e\u6587\u672c\u7684\u97f3\u8272\u7279\u5f81\u548c\u5168\u5c40\u97f5\u5f8b\u4fe1\u606f\u51c6\u786e\u5730\u6a21\u62df\u98ce\u683c\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9ad8\u4fdd\u771f\u8bed\u97f3\u5408\u6210\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4e00\u7cfb\u5217\u81ea\u9002\u5e94TTS\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u751f\u6210\u7684\u8bed\u97f3\u5728\u97f3\u8272\u548c\u97f5\u5f8b\u65b9\u9762\u5177\u6709\u66f4\u9ad8\u7684\u81ea\u7136\u5ea6\u548c\u76f8\u4f3c\u6027\u3002||\n", "2409.05701": "|**2024-09-09**|[pFedGPA: Diffusion-based Generative Parameter Aggregation for Personalized Federated Learning](http://arxiv.org/abs/2409.05701)|null|\u8054\u90a6\u5b66\u4e60 (FL) \u662f\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6570\u636e\u4fdd\u7559\u5728\u672c\u5730\uff0c\u53ea\u6709\u6a21\u578b\u53c2\u6570\u5728\u5ba2\u6237\u7aef\u548c\u4e2d\u5fc3\u670d\u52a1\u5668\u4e4b\u95f4\u5171\u4eab\u3002\u4f20\u7edf\u7684\u8054\u90a6\u5e73\u5747 (FedAvg) \u7b49\u65b9\u6cd5\u5bf9\u8fd9\u4e9b\u901a\u5e38\u5728\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e0a\u8bad\u7ec3\u7684\u53c2\u6570\u8fdb\u884c\u7ebf\u6027\u805a\u5408\uff0c\u8fd9\u53ef\u80fd\u5ffd\u7565\u4e86\u53c2\u6570\u7a7a\u95f4\u590d\u6742\u3001\u9ad8\u7ef4\u7684\u6027\u8d28\uff0c\u5bfc\u81f4\u805a\u5408\u6a21\u578b\u7684\u6027\u80fd\u4e0b\u964d\u3002\u867d\u7136\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u7f13\u89e3\u5f02\u6784\u6570\u636e\u95ee\u9898\uff0c\u4f46\u7ebf\u6027\u805a\u5408\u7684\u5c40\u9650\u6027\u4ecd\u7136\u6ca1\u6709\u89e3\u51b3\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u751f\u6210\u53c2\u6570\u805a\u5408\u6846\u67b6\uff0c\u5373 pFedGPA\u3002\u5728\u8fd9\u4e2a\u6846\u67b6\u4e2d\uff0c\u6211\u4eec\u5728\u670d\u52a1\u5668\u4e0a\u90e8\u7f72\u4e86\u4e00\u4e2a\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u6574\u5408\u4e0d\u540c\u7684\u53c2\u6570\u5206\u5e03\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u53cd\u6f14\u65b9\u6cd5\uff0c\u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u6709\u6548\u5730\u751f\u6210\u4e00\u7ec4\u4e2a\u6027\u5316\u53c2\u6570\u3002\u8fd9\u79cd\u53cd\u6f14\u65b9\u6cd5\u5c06\u4e0a\u4f20\u7684\u53c2\u6570\u8f6c\u6362\u4e3a\u4e00\u4e2a\u6f5c\u5728\u4ee3\u7801\uff0c\u7136\u540e\u901a\u8fc7\u53bb\u566a\u91c7\u6837\u8fdb\u884c\u805a\u5408\uff0c\u751f\u6210\u6700\u7ec8\u7684\u4e2a\u6027\u5316\u53c2\u6570\u3002\u901a\u8fc7\u4f7f\u7528\u9ad8\u5bb9\u91cf\u6269\u6563\u6a21\u578b\u5bf9\u5ba2\u6237\u7aef\u6a21\u578b\u53c2\u6570\u5bf9\u5176\u7279\u5b9a\u6570\u636e\u5206\u5e03\u7684\u4f9d\u8d56\u6027\u8fdb\u884c\u7f16\u7801\uff0cpFedGPA \u53ef\u4ee5\u6709\u6548\u5730\u5c06\u6240\u6709\u5ba2\u6237\u7aef\u6a21\u578b\u53c2\u6570\u7684\u603b\u4f53\u5206\u5e03\u7684\u590d\u6742\u6027\u4e0e\u6bcf\u4e2a\u5ba2\u6237\u7aef\u53c2\u6570\u5206\u5e03\u7684\u590d\u6742\u6027\u89e3\u8026\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\u5730\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u8d85\u8fc7\u4e86\u57fa\u7ebf\u65b9\u6cd5\u3002||\n", "2409.05668": "|**2024-09-09**|[Unlearning or Concealment? A Critical Analysis and Evaluation Metrics for Unlearning in Diffusion Models](http://arxiv.org/abs/2409.05668)|null|\u8fd1\u671f\u7684\u7814\u7a76\u5df2\u7ecf\u770b\u5230\u4eba\u4eec\u5bf9\u6269\u6563\u6a21\u578b\u4e2d\u6982\u5ff5\u53bb\u9664\u548c\u76ee\u6807\u9057\u5fd8\u65b9\u6cd5\u7684\u6d53\u539a\u5174\u8da3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5bf9\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u9057\u5fd8\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7684\u767d\u76d2\u5206\u6790\uff0c\u4ee5\u63ed\u793a\u5176\u5b58\u5728\u7684\u91cd\u5927\u6f0f\u6d1e\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u73b0\u6709\u65b9\u6cd5\u4e2d\u7528\u4e8e\u9057\u5fd8\u7684\u76ee\u6807\u51fd\u6570\u5bfc\u81f4\u4e86\u8981\u9057\u5fd8\u7684\u76ee\u6807\u6982\u5ff5\u4e0e\u76f8\u5e94\u63d0\u793a\u4e4b\u95f4\u7684\u89e3\u8026\u3002\u8fd9\u662f\u4e00\u79cd\u9690\u853d\u884c\u4e3a\uff0c\u800c\u4e0d\u662f\u771f\u6b63\u7684\u9057\u5fd8\uff0c\u800c\u771f\u6b63\u7684\u9057\u5fd8\u624d\u662f\u6700\u521d\u7684\u76ee\u6807\u3002\u5f53\u524d\u65b9\u6cd5\u7684\u65e0\u6548\u6027\u4e3b\u8981\u6e90\u4e8e\u5b83\u4eec\u53ea\u5173\u6ce8\u964d\u4f4e\u7279\u5b9a\u63d0\u793a\u96c6\u7684\u751f\u6210\u6982\u7387\uff0c\u800c\u5ffd\u7565\u4e86\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u7684\u4e2d\u95f4\u5f15\u5bfc\u7684\u591a\u79cd\u5f62\u5f0f\u3002\u672c\u6587\u5bf9\u56db\u79cd\u5e38\u7528\u7684\u6269\u6563\u6a21\u578b\u9057\u5fd8\u6280\u672f\u8fdb\u884c\u4e86\u4e25\u683c\u7684\u7406\u8bba\u548c\u5b9e\u8bc1\u68c0\u9a8c\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u4e2a\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff1a\u6982\u5ff5\u68c0\u7d22\u5206\u6570\uff08CRS\uff09\u548c\u6982\u5ff5\u7f6e\u4fe1\u5ea6\u5206\u6570\uff08CCS\uff09\u3002\u8fd9\u4e9b\u6307\u6807\u57fa\u4e8e\u4e00\u4e2a\u6210\u529f\u7684\u5bf9\u6297\u653b\u51fb\u8bbe\u7f6e\uff0c\u53ef\u4ee5\u4ece\u9057\u5fd8\u7684\u6269\u6563\u6a21\u578b\u4e2d\u6062\u590d\u88ab\u9057\u5fd8\u7684\u6982\u5ff5\u3002CRS \u8861\u91cf\u7684\u662f\u9057\u5fd8\u540e\u7684\u9057\u5fd8\u6a21\u578b\u548c\u5b8c\u5168\u8bad\u7ec3\u6a21\u578b\u7684\u6f5c\u5728\u8868\u793a\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u5b83\u53cd\u6620\u4e86\u968f\u7740\u5f15\u5bfc\u91cf\u589e\u52a0\uff0c\u88ab\u9057\u5fd8\u6982\u5ff5\u7684\u68c0\u7d22\u7a0b\u5ea6\u3002CCS \u91cf\u5316\u4e86\u6a21\u578b\u5c06\u76ee\u6807\u6982\u5ff5\u5206\u914d\u7ed9\u88ab\u64cd\u7eb5\u6570\u636e\u7684\u7f6e\u4fe1\u5ea6\u3002\u5b83\u53cd\u6620\u4e86\u968f\u7740\u5f15\u5bfc\u91cf\u589e\u52a0\uff0c\u672a\u9057\u5fd8\u6a21\u578b\u7684\u751f\u6210\u7ed3\u679c\u4e0e\u539f\u59cb\u9886\u57df\u77e5\u8bc6\u4e00\u81f4\u7684\u6982\u7387\u3002\u6211\u4eec\u4f7f\u7528\u63d0\u51fa\u7684\u9488\u5bf9\u6269\u6563\u6a21\u578b\u7684\u4e25\u683c\u6307\u6807\u5bf9\u73b0\u6709\u7684\u9057\u5fd8\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u771f\u6b63\u9057\u5fd8\u6982\u5ff5\u65b9\u9762\u7684\u91cd\u5927\u7f3a\u9677\u3002\u6e90\u4ee3\u7801\uff1ahttps://respailab.github.io/unlearning-or-concealment||\n", "2409.05622": "|**2024-09-09**|[Forward KL Regularized Preference Optimization for Aligning Diffusion Policies](http://arxiv.org/abs/2409.05622)|null|\u6269\u6563\u6a21\u578b\u901a\u8fc7\u5728\u7b56\u7565\u5b66\u4e60\u4e2d\u5229\u7528\u9ad8\u5ea6\u8868\u8fbe\u7684\u6a21\u578b\u80fd\u529b\uff0c\u5728\u5e8f\u5217\u51b3\u7b56\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\u3002\u5b66\u4e60\u6269\u6563\u7b56\u7565\u7684\u4e00\u4e2a\u6838\u5fc3\u95ee\u9898\u662f\u5982\u4f55\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u4f7f\u7b56\u7565\u8f93\u51fa\u4e0e\u4eba\u7c7b\u610f\u56fe\u4fdd\u6301\u4e00\u81f4\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u5148\u524d\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u56de\u62a5\u6761\u4ef6\u7b56\u7565\u751f\u6210\u6216\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u7b56\u7565\u4f18\u5316\uff0c\u4f46\u5b83\u4eec\u90fd\u4f9d\u8d56\u4e8e\u9884\u5148\u5b9a\u4e49\u7684\u5956\u52b1\u51fd\u6570\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u5373\u7528\u4e8e\u5bf9\u9f50\u6269\u6563\u7b56\u7565\u7684\u524d\u5411 KL \u6b63\u5219\u5316\u504f\u597d\u4f18\u5316\uff0c\u4ee5\u76f4\u63a5\u5c06\u6269\u6563\u7b56\u7565\u4e0e\u504f\u597d\u5bf9\u9f50\u3002\u6211\u4eec\u9996\u5148\u4ece\u79bb\u7ebf\u6570\u636e\u96c6\u4e2d\u8bad\u7ec3\u4e00\u4e2a\u4e0d\u8003\u8651\u504f\u597d\u7684\u6269\u6563\u7b56\u7565\uff0c\u7136\u540e\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5c06\u8be5\u7b56\u7565\u4e0e\u504f\u597d\u6570\u636e\u5bf9\u9f50\u3002\u5728\u5bf9\u9f50\u9636\u6bb5\uff0c\u6211\u4eec\u5728\u6269\u6563\u7b56\u7565\u4e2d\u5236\u5b9a\u4e86\u76f4\u63a5\u504f\u597d\u5b66\u4e60\uff0c\u5176\u4e2d\u5728\u524d\u5411\u504f\u597d\u4f18\u5316\u4e2d\u91c7\u7528\u4e86 KL \u6b63\u5219\u5316\uff0c\u4ee5\u907f\u514d\u751f\u6210\u5206\u5e03\u5916\u52a8\u4f5c\u3002\u6211\u4eec\u5bf9 MetaWorld \u64cd\u4f5c\u548c D4RL \u4efb\u52a1\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u504f\u597d\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\u3002||\n", "2409.05585": "|**2024-09-09**|[Latent 3D Brain MRI Counterfactual](http://arxiv.org/abs/2409.05585)|null|\u7ed3\u6784\u6027\u8111\u90e8MRI\u7814\u7a76\u4e2d\u7684\u6837\u672c\u6570\u91cf\u901a\u5e38\u8fc7\u5c0f\uff0c\u65e0\u6cd5\u5145\u5206\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u751f\u6210\u6a21\u578b\u901a\u8fc7\u6709\u6548\u5b66\u4e60\u6570\u636e\u5206\u5e03\u548c\u751f\u6210\u9ad8\u4fdd\u771fMRI\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5e26\u6765\u4e86\u5e0c\u671b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u96be\u4ee5\u751f\u6210\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u4e4b\u5916\u7684\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u3002\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u4e00\u79cd\u65b9\u6cd5\u662f\u4f7f\u7528\u9488\u5bf93D\u4f53\u79ef\u53cd\u4e8b\u5b9e\u5f00\u53d1\u7684\u56e0\u679c\u6a21\u578b\u3002\u7136\u800c\uff0c\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u51c6\u786e\u5efa\u6a21\u56e0\u679c\u5173\u7cfb\u662f\u4e00\u9879\u6311\u6218\uff0c\u56e0\u6b64\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u751f\u6210\u8d28\u91cf\u8f83\u4f4e\u76843D\u8111\u90e8MRI\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u5185\u6784\u5efa\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\u3002\u5728\u7b2c\u4e00\u9636\u6bb5\uff0c\u6211\u4eec\u91c7\u7528VQ-VAE\u5b66\u4e60MRI\u4f53\u79ef\u7684\u7d27\u51d1\u5d4c\u5165\u3002\u968f\u540e\uff0c\u6211\u4eec\u5c06\u56e0\u679c\u6a21\u578b\u6574\u5408\u5230\u8fd9\u4e2a\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u5e76\u4f7f\u7528\u5c01\u95ed\u5f62\u5f0f\u7684\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\uff08GLM\uff09\u6267\u884c\u4e09\u6b65\u53cd\u4e8b\u5b9e\u7a0b\u5e8f\u3002\u6211\u4eec\u5bf9\u771f\u5b9e\u4e16\u754c\u7684\u9ad8\u5206\u8fa8\u7387MRI\u6570\u636e\uff081mm\uff09\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u76843D MRI\u53cd\u4e8b\u5b9e\u3002||\n", "2409.05583": "|**2024-09-09**|[Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation](http://arxiv.org/abs/2409.05583)|**[link](https://github.com/gmuraleekrishna/sas)**|\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u65e8\u5728\u5f00\u53d1\u80fd\u591f\u7406\u89e3\u548c\u6267\u884c\u4eba\u7c7b\u8bed\u8a00\u6307\u4ee4\u5e76\u4ee5\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u4ea4\u6d41\u7684\u673a\u5668\u4eba\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u751f\u6210\u9ad8\u5ea6\u8be6\u7ec6\u7684\u5bfc\u822a\u6307\u4ee4\u4ee5\u4f9b\u5177\u8eab\u673a\u5668\u4eba\u9075\u5faa\u7684\u4efb\u52a1\u3002\u5c3d\u7ba1\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u4ece\u56fe\u50cf\u5e8f\u5217\u751f\u6210\u9010\u6b65\u6307\u4ee4\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u751f\u6210\u7684\u6307\u4ee4\u5728\u6307\u79f0\u7269\u4f53\u548c\u5730\u6807\u65b9\u9762\u7f3a\u4e4f\u591a\u6837\u6027\u3002\u73b0\u6709\u7684\u8bf4\u8bdd\u8005\u6a21\u578b\u5b66\u4e60\u4e86\u4e00\u4e9b\u7b56\u7565\u6765\u89c4\u907f\u8bc4\u4f30\u6307\u6807\uff0c\u5373\u4f7f\u5bf9\u4e8e\u4f4e\u8d28\u91cf\u7684\u53e5\u5b50\u4e5f\u80fd\u83b7\u5f97\u66f4\u9ad8\u7684\u5206\u6570\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SAS\uff08\u7a7a\u95f4\u611f\u77e5\u8bf4\u8bdd\u8005\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u6307\u4ee4\u751f\u6210\u5668\u6216\u201c\u8bf4\u8bdd\u8005\u201d\u6a21\u578b\uff0c\u5b83\u5229\u7528\u73af\u5883\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u77e5\u8bc6\u6765\u751f\u6210\u66f4\u4e30\u5bcc\u7684\u6307\u4ee4\u3002\u4e3a\u4e86\u8fdb\u884c\u8bad\u7ec3\uff0c\u6211\u4eec\u5728\u5bf9\u6297\u6027\u8bbe\u7f6e\u4e2d\u91c7\u7528\u4e86\u5956\u52b1\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u907f\u514d\u8bed\u8a00\u8bc4\u4f30\u6307\u6807\u5f15\u5165\u7684\u7cfb\u7edf\u6027\u504f\u5dee\u3002\u6839\u636e\u7ecf\u9a8c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u6307\u4ee4\u751f\u6210\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u6807\u51c6\u6307\u6807\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u5f97\uff1ahttps://github.com/gmuraleekrishna/SAS\u3002||\n", "2409.05490": "|**2024-09-09**|[A Taxonomy of Miscompressions: Preparing Image Forensics for Neural Compression](http://arxiv.org/abs/2409.05490)|null|\u795e\u7ecf\u538b\u7f29\u6709\u53ef\u80fd\u5f7b\u5e95\u6539\u53d8\u6709\u635f\u56fe\u50cf\u538b\u7f29\u6280\u672f\u3002\u57fa\u4e8e\u751f\u6210\u6a21\u578b\uff0c\u6700\u8fd1\u7684\u65b9\u6848\u5728\u9ad8\u611f\u77e5\u8d28\u91cf\u4e0b\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u538b\u7f29\u7387\uff0c\u4f46\u727a\u7272\u4e86\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002\u89e3\u538b\u7f29\u56fe\u50cf\u7684\u7ec6\u8282\u53ef\u80fd\u770b\u8d77\u6765\u5728\u89c6\u89c9\u4e0a\u662f\u5b8c\u7f8e\u7684\uff0c\u4f46\u5728\u8bed\u4e49\u4e0a\u4e0e\u539f\u59cb\u56fe\u50cf\u4e0d\u540c\uff0c\u8fd9\u4f7f\u5f97\u538b\u7f29\u9519\u8bef\u96be\u4ee5\u6216\u4e0d\u53ef\u80fd\u88ab\u68c0\u6d4b\u5230\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u8fd9\u4e2a\u95ee\u9898\u7684\u7a7a\u95f4\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6682\u5b9a\u7684\u9519\u8bef\u538b\u7f29\u5206\u7c7b\u6cd5\u3002\u5b83\u5b9a\u4e49\u4e86\u4e09\u79cd\u7c7b\u578b\u7684\u201c\u53d1\u751f\u4e86\u4ec0\u4e48\u201d\uff0c\u5e76\u6709\u4e00\u4e2a\u4e8c\u8fdb\u5236\u7684\u201c\u9ad8\u5f71\u54cd\u201d\u6807\u5fd7\uff0c\u8868\u793a\u6539\u53d8\u7b26\u53f7\u7684\u9519\u8bef\u538b\u7f29\u3002\u6211\u4eec\u8ba8\u8bba\u4e86\u8be5\u5206\u7c7b\u6cd5\u5982\u4f55\u4fc3\u8fdb\u98ce\u9669\u6c9f\u901a\u548c\u7f13\u89e3\u63aa\u65bd\u7684\u7814\u7a76\u3002||\n", "2409.06633": "|**2024-09-10**|[SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse Low-Rank Adaptation](http://arxiv.org/abs/2409.06633)|null|\u8fd1\u5e74\u6765\uff0c\u6269\u6563\u6a21\u578b\u7684\u53d1\u5c55\u63a8\u52a8\u4e86\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4efb\u52a1\u7684\u663e\u8457\u8fdb\u6b65\uff0c\u5176\u4e2d\u50cfStable Diffusion\u7cfb\u5217\u8fd9\u6837\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u53d1\u6325\u4e86\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u53d7\u6a21\u578b\u526a\u679d\u6280\u672f\u7684\u542f\u53d1\uff0c\u8be5\u6280\u672f\u901a\u8fc7\u79fb\u9664\u4e0d\u91cd\u8981\u7684\u53c2\u6570\u6765\u51cf\u8f7b\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8d1f\u62c5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5145\u5206\u5229\u7528\u8fd9\u4e9b\u65e0\u6548\u53c2\u6570\uff0c\u5e76\u4f7f\u9884\u8bad\u7ec3\u6a21\u578b\u5177\u5907\u65b0\u7684\u4efb\u52a1\u7279\u5b9a\u80fd\u529b\u3002\u672c\u7814\u7a76\u9996\u5148\u8c03\u67e5\u4e86\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e2d\u53c2\u6570\u7684\u91cd\u8981\u6027\uff0c\u53d1\u73b0\u6309\u7edd\u5bf9\u503c\u8ba1\u7b97\uff0c\u6700\u5c0f\u768410%\u523020%\u7684\u53c2\u6570\u5bf9\u751f\u6210\u8fc7\u7a0b\u6ca1\u6709\u8d21\u732e\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c2\u5bdf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSaRA\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u91cd\u65b0\u5229\u7528\u8fd9\u4e9b\u6682\u65f6\u65e0\u6548\u7684\u53c2\u6570\uff0c\u76f8\u5f53\u4e8e\u4f18\u5316\u4e00\u4e2a\u7a00\u758f\u6743\u91cd\u77e9\u9635\u6765\u5b66\u4e60\u7279\u5b9a\u4efb\u52a1\u7684\u77e5\u8bc6\u3002\u4e3a\u4e86\u51cf\u8f7b\u8fc7\u62df\u5408\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6838\u8303\u6570\u7684\u4f4e\u79e9\u7a00\u758f\u8bad\u7ec3\u65b9\u6848\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u5fae\u8c03\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u6e10\u8fdb\u5f0f\u53c2\u6570\u8c03\u6574\u7b56\u7565\uff0c\u4ee5\u5145\u5206\u5229\u7528\u91cd\u65b0\u8bad\u7ec3/\u5fae\u8c03\u7684\u53c2\u6570\u3002\u6700\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u975e\u7ed3\u6784\u5316\u53cd\u5411\u4f20\u64ad\u7b56\u7565\uff0c\u53ef\u663e\u8457\u964d\u4f4e\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u5185\u5b58\u6210\u672c\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u589e\u5f3a\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e0b\u6e38\u5e94\u7528\u4e2d\u7684\u751f\u6210\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u4fdd\u6301\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4f18\u4e8eLoRA\u7b49\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u3002\u6211\u4eec\u901a\u8fc7\u5728SD\u6a21\u578b\u4e0a\u7684\u5fae\u8c03\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660eSaRA\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002SaRA\u8fd8\u5177\u6709\u4e00\u4e2a\u5b9e\u9645\u4f18\u52bf\uff0c\u5373\u53ea\u9700\u4fee\u6539\u4e00\u884c\u4ee3\u7801\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65bd\uff0c\u5e76\u4e14\u4e0e\u73b0\u6709\u65b9\u6cd5\u65e0\u7f1d\u517c\u5bb9\u3002||\n", "2409.06620": "|**2024-09-10**|[MVGaussian: High-Fidelity text-to-3D Content Generation with Multi-View Guidance and Surface Densification](http://arxiv.org/abs/2409.06620)|null|\u6587\u672c\u52303D\u5185\u5bb9\u751f\u6210\u9886\u57df\u5728\u751f\u6210\u903c\u771f\u76843D\u5bf9\u8c61\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u50cf\u5206\u6570\u84b8\u998f\u91c7\u6837\uff08SDS\uff09\u8fd9\u6837\u7684\u73b0\u6709\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u6307\u5bfc\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6307\u5bfc\u4e0d\u7cbe\u786e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u7ecf\u5e38\u9047\u5230\u201c\u4e24\u9762\u795e\u201d\u95ee\u9898\u2014\u2014\u591a\u9762\u6b67\u4e49\u3002\u6b64\u5916\uff0c\u867d\u7136\u6700\u8fd13D\u9ad8\u65af\u5206\u88c2\u7684\u8fdb\u6b65\u5df2\u7ecf\u663e\u793a\u51fa\u5176\u5728\u8868\u793a3D\u4f53\u79ef\u65b9\u9762\u7684\u529f\u6548\uff0c\u4f46\u8fd9\u79cd\u8868\u793a\u7684\u4f18\u5316\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u5f97\u5230\u63a2\u7d22\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8e\u6587\u672c\u52303D\u5185\u5bb9\u751f\u6210\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u5173\u952e\u5dee\u8ddd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u591a\u89c6\u56fe\u6307\u5bfc\u8fed\u4ee3\u5f62\u62103D\u6a21\u578b\u7684\u7ed3\u6784\uff0c\u9010\u6b65\u589e\u5f3a\u7ec6\u8282\u548c\u51c6\u786e\u6027\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5bc6\u96c6\u5316\u7b97\u6cd5\uff0c\u4f7f\u9ad8\u65af\u63a5\u8fd1\u8868\u9762\uff0c\u4f18\u5316\u751f\u6210\u6a21\u578b\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u4fdd\u771f\u5ea6\u3002\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u8868\u660e\u5b83\u80fd\u591f\u4ee5\u6700\u5c11\u7684\u65f6\u95f4\u6210\u672c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u89c6\u89c9\u8f93\u51fa\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u534a\u5c0f\u65f6\u7684\u8bad\u7ec3\u65f6\u95f4\u5185\u5c31\u80fd\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\uff0c\u4e0e\u5927\u591a\u6570\u9700\u8981\u6570\u5c0f\u65f6\u8bad\u7ec3\u65f6\u95f4\u624d\u80fd\u83b7\u5f97\u7c7b\u4f3c\u7ed3\u679c\u7684\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6548\u7387\u663e\u8457\u63d0\u9ad8\u3002||\n", "2409.06560": "|**2024-09-10**|[A Primer on Variational Inference for Physics-Informed Deep Generative Modelling](http://arxiv.org/abs/2409.06560)|null|\u53d8\u5206\u63a8\u65ad\uff08VI\uff09\u662f\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u8fd1\u4f3c\u8d1d\u53f6\u65af\u63a8\u65ad\u65b9\u6cd5\u3002\u5b83\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u51c6\u786e\u6027\u548c\u5b9e\u9645\u53ef\u5904\u7406\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002\u7531\u4e8e\u5176\u5185\u7f6e\u7684\u8d1d\u53f6\u65af\u6b63\u5219\u5316\u548c\u7075\u6d3b\u6027\uff0c\u5b83\u5728\u751f\u6210\u5efa\u6a21\u548c\u53cd\u6f14\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fd9\u5bf9\u4e8e\u7269\u7406\u76f8\u5173\u95ee\u9898\u81f3\u5173\u91cd\u8981\u3002\u63a8\u5bfc VI \u7684\u6838\u5fc3\u5b66\u4e60\u76ee\u6807\u901a\u5e38\u5fc5\u987b\u9488\u5bf9\u65b0\u7684\u5b66\u4e60\u4efb\u52a1\u8fdb\u884c\u8c03\u6574\uff0c\u5176\u4e2d\u95ee\u9898\u7684\u6027\u8d28\u51b3\u5b9a\u4e86\u611f\u5174\u8da3\u53d8\u91cf\u4e4b\u95f4\u7684\u6761\u4ef6\u4f9d\u8d56\u6027\uff0c\u4f8b\u5982\u7269\u7406\u95ee\u9898\u4e2d\u51fa\u73b0\u7684\u60c5\u51b5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4e3a\u6b63\u5411\u548c\u53cd\u5411\u95ee\u9898\u63d0\u4f9b\u4e86 VI \u7684\u6613\u4e8e\u7406\u89e3\u4e14\u5168\u9762\u7684\u6280\u672f\u4ecb\u7ecd\uff0c\u5f15\u5bfc\u8bfb\u8005\u4e86\u89e3 VI \u6846\u67b6\u7684\u6807\u51c6\u63a8\u5bfc\u53ca\u5176\u5982\u4f55\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5f97\u5230\u6700\u4f73\u5b9e\u73b0\u3002\u7136\u540e\uff0c\u6211\u4eec\u56de\u987e\u5e76\u7edf\u4e00\u4e86\u6700\u8fd1\u7684\u6587\u732e\uff0c\u8fd9\u4e9b\u6587\u732e\u4f8b\u8bc1\u4e86 VI \u6240\u5141\u8bb8\u7684\u521b\u9020\u6027\u7075\u6d3b\u6027\u3002\u672c\u6587\u9762\u5411\u5e0c\u671b\u89e3\u51b3\u57fa\u4e8e\u7269\u7406\u7684\u95ee\u9898\u5e76\u5f3a\u8c03\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u4e00\u822c\u79d1\u5b66\u53d7\u4f17\u3002||\n", "2409.06550": "|**2024-09-10**|[From LIMA to DeepLIMA: following a new path of interoperability](http://arxiv.org/abs/2409.06550)|null|\u672c\u6587\u63cf\u8ff0\u4e86 LIMA\uff08Libre Multilingual Analyzer\uff09\u6846\u67b6\u7684\u4f53\u7cfb\u7ed3\u6784\u53ca\u5176\u6700\u65b0\u53d1\u5c55\uff0c\u5176\u4e2d\u65b0\u589e\u4e86\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6587\u672c\u5206\u6790\u6a21\u5757\u3002\u6211\u4eec\u5728\u4fdd\u7559\u73b0\u6709\u53ef\u914d\u7f6e\u67b6\u6784\u4ee5\u53ca\u5148\u524d\u5f00\u53d1\u7684\u57fa\u4e8e\u89c4\u5219\u548c\u7edf\u8ba1\u7684\u5206\u6790\u7ec4\u4ef6\u7684\u53ef\u7528\u6027\u7684\u540c\u65f6\uff0c\u6269\u5c55\u4e86 LIMA \u5728\u652f\u6301\u8bed\u8a00\u6570\u91cf\u65b9\u9762\u7684\u529f\u80fd\u3002\u6211\u4eec\u5728 Universal Dependencies 2.5 \u8bed\u6599\u5e93\u3001WikiNer \u8bed\u6599\u5e93\u548c CoNLL-03 \u6570\u636e\u96c6\u4e0a\u9488\u5bf9 60 \u591a\u79cd\u8bed\u8a00\u8bad\u7ec3\u4e86\u6a21\u578b\u3002Universal Dependencies \u5141\u8bb8\u6211\u4eec\u589e\u52a0\u652f\u6301\u7684\u8bed\u8a00\u6570\u91cf\uff0c\u5e76\u751f\u6210\u53ef\u4ee5\u96c6\u6210\u5230\u5176\u4ed6\u5e73\u53f0\u7684\u6a21\u578b\u3002\u8fd9\u79cd\u666e\u904d\u5b58\u5728\u7684\u6df1\u5ea6\u5b66\u4e60\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6a21\u578b\u7684\u96c6\u6210\u4ee5\u53ca\u4f7f\u7528 Universal Dependencies \u7684\u6807\u51c6\u6ce8\u91ca\u96c6\u5408\u7684\u4f7f\u7528\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u65b0\u7684\u4e92\u64cd\u4f5c\u6027\u9014\u5f84\uff0c\u901a\u8fc7\u6a21\u578b\u548c\u6570\u636e\u7684\u89c4\u8303\u5316\uff0c\u4e0e\u66f4\u6807\u51c6\u7684\u6280\u672f\u4e92\u64cd\u4f5c\u6027\u76f8\u8f85\u76f8\u6210\uff0c\u5728 LIMA \u4e2d\u901a\u8fc7 Docker Hub \u4e0a Docker \u5bb9\u5668\u4e2d\u53ef\u7528\u7684\u670d\u52a1\u5b9e\u73b0\u3002||\n", "2409.06451": "|**2024-09-10**|[Enhancing Emotional Text-to-Speech Controllability with Natural Language Guidance through Contrastive Learning and Diffusion Models](http://arxiv.org/abs/2409.06451)|null|\u867d\u7136\u5f53\u524d\u7684\u60c5\u611f\u6587\u672c\u5230\u8bed\u97f3\uff08TTS\uff09\u7cfb\u7edf\u53ef\u4ee5\u751f\u6210\u9ad8\u5ea6\u667a\u80fd\u7684\u60c5\u611f\u8bed\u97f3\uff0c\u4f46\u5728\u8f93\u51fa\u8bed\u97f3\u7684\u60c5\u611f\u6e32\u67d3\u65b9\u9762\u5b9e\u73b0\u7cbe\u7ec6\u63a7\u5236\u4ecd\u7136\u662f\u4e00\u9879\u91cd\u5927\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 ParaEVITS\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u60c5\u611f TTS \u6846\u67b6\uff0c\u5b83\u5229\u7528\u81ea\u7136\u8bed\u8a00\u7684\u7ec4\u5408\u6027\u6765\u589e\u5f3a\u5bf9\u60c5\u611f\u6e32\u67d3\u7684\u63a7\u5236\u3002\u901a\u8fc7\u7ed3\u5408\u53d7 ParaCLAP\uff08\u4e00\u79cd\u7528\u4e8e\u8ba1\u7b97\u8bed\u7528\u5b66\u7684\u5bf9\u6bd4\u6027\u8bed\u8a00-\u97f3\u9891\u9884\u8bad\u7ec3\uff08CLAP\uff09\u6a21\u578b\uff09\u542f\u53d1\u7684\u6587\u672c-\u97f3\u9891\u7f16\u7801\u5668\uff0c\u6211\u4eec\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4ee5\u6839\u636e\u6587\u672c\u60c5\u611f\u98ce\u683c\u63cf\u8ff0\u751f\u6210\u60c5\u611f\u5d4c\u5165\u3002\u6211\u4eec\u7684\u6846\u67b6\u9996\u5148\u4f7f\u7528\u97f3\u9891\u7f16\u7801\u5668\u5728\u53c2\u8003\u97f3\u9891\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u7136\u540e\u5fae\u8c03\u6269\u6563\u6a21\u578b\u4ee5\u5904\u7406\u6765\u81ea ParaCLAP \u6587\u672c\u7f16\u7801\u5668\u7684\u6587\u672c\u8f93\u5165\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u4ec5\u4f7f\u7528\u6587\u672c\u6761\u4ef6\u5c31\u53ef\u4ee5\u64cd\u7eb5\u97f3\u8c03\u3001\u6296\u52a8\u548c\u54cd\u5ea6\u7b49\u8bed\u97f3\u5c5e\u6027\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cParaEVITS \u53ef\u4ee5\u6709\u6548\u5730\u63a7\u5236\u60c5\u611f\u6e32\u67d3\uff0c\u800c\u4e0d\u4f1a\u5f71\u54cd\u8bed\u97f3\u8d28\u91cf\u3002\u8bed\u97f3\u6f14\u793a\u516c\u5f00\u53ef\u7528\u3002||\n", "2409.06442": "|**2024-09-10**|[Prompt2Fashion: An automatically generated fashion dataset](http://arxiv.org/abs/2409.06442)|**[link](https://github.com/georgiarg/prompt2fashion)**|\u5c3d\u7ba1\u8bed\u8a00\u548c\u89c6\u89c9\u751f\u6210\u6a21\u578b\u5728\u5feb\u901f\u53d1\u5c55\u4e14\u6548\u7387\u4e0d\u65ad\u63d0\u9ad8\uff0c\u4f46\u4ecd\u7136\u7f3a\u4e4f\u5c06\u4e2a\u6027\u5316\u65f6\u5c1a\u9700\u6c42\u4e0e\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u8bbe\u8ba1\u8054\u7cfb\u8d77\u6765\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u8fd9\u9650\u5236\u4e86\u771f\u6b63\u5305\u5bb9\u548c\u5b9a\u5236\u5316\u65f6\u5c1a\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5229\u7528\u751f\u6210\u6a21\u578b\u81ea\u52a8\u6784\u5efa\u4e86\u4e00\u4e2a\u65f6\u5c1a\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6839\u636e\u7528\u6237\u7684\u6307\u793a\u9488\u5bf9\u4e0d\u540c\u7684\u573a\u5408\u3001\u98ce\u683c\u548c\u4f53\u578b\u91cf\u8eab\u5b9a\u5236\u3002\u6211\u4eec\u4f7f\u7528\u4e0d\u540c\u7684\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u6a21\u578b\uff08LLM\uff09\u548c\u63d0\u793a\u7b56\u7565\uff0c\u4e3a\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u7528\u6237\u63d0\u4f9b\u5177\u6709\u9ad8\u8d28\u91cf\u5ba1\u7f8e\u3001\u7ec6\u8282\u548c\u76f8\u5173\u6027\u7684\u4e2a\u6027\u5316\u670d\u88c5\uff0c\u5e76\u901a\u8fc7\u5b9a\u6027\u5206\u6790\u8bc1\u660e\u4e86\u8fd9\u4e00\u70b9\u3002\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u751f\u6210\u7684\u670d\u88c5\u7684\u8bc4\u4f30\u4e00\u76f4\u7531\u975e\u4e13\u5bb6\u7684\u4eba\u7c7b\u53d7\u8bd5\u8005\u8fdb\u884c\u3002\u5c3d\u7ba1\u5bf9\u751f\u6210\u7684\u8d28\u91cf\u548c\u76f8\u5173\u6027\u63d0\u4f9b\u4e86\u7ec6\u81f4\u5165\u5fae\u7684\u89c1\u89e3\uff0c\u4f46\u6211\u4eec\u5c31\u4e13\u5bb6\u77e5\u8bc6\u5bf9\u4e8e\u8bc4\u4f30\u6b64\u7c7b\u827a\u672f\u6027\u4eba\u5de5\u667a\u80fd\u751f\u6210\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027\u5c55\u5f00\u4e86\u8fdb\u4e00\u6b65\u7684\u8ba8\u8bba\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u53ef\u5728 GitHub \u4e0a\u516c\u5f00\u83b7\u53d6\uff0c\u7f51\u5740\u4e3a https://github.com/georgiarg/Prompt2Fashion\u3002||\n", "2409.06417": "|**2024-09-10**|[Fast nonparametric inference of network backbones for graph sparsification](http://arxiv.org/abs/2409.06417)|**[link](https://github.com/aleckirkley/mdl-network-backbones)**|\u7f51\u7edc\u9aa8\u5e72\u901a\u8fc7\u4ec5\u4fdd\u7559\u6700\u91cd\u8981\u7684\u94fe\u63a5\u6765\u63d0\u4f9b\u52a0\u6743\u7f51\u7edc\u7684\u6709\u7528\u7a00\u758f\u8868\u793a\uff0c\u4ece\u800c\u5b9e\u73b0\u4e00\u7cfb\u5217\u8ba1\u7b97\u52a0\u901f\u5e76\u7b80\u5316\u590d\u6742\u7684\u7f51\u7edc\u53ef\u89c6\u5316\u3002\u5224\u65ad\u94fe\u63a5\u662f\u5426\u91cd\u8981\u7684\u6807\u51c6\u6709\u5f88\u591a\uff0c\u56e0\u6b64\u5df2\u7ecf\u5f00\u53d1\u4e86\u8bb8\u591a\u7528\u4e8e\u56fe\u7a00\u758f\u5316\u7f51\u7edc\u9aa8\u5e72\u63d0\u53d6\u7684\u65b9\u6cd5\u3002\u8fd9\u4e9b\u65b9\u6cd5\u6839\u636e\u5b83\u4eec\u662f\u5728\u6574\u4e2a\u7f51\u7edc\u8fd8\u662f\u5728\u5355\u4e2a\u8282\u70b9\u90bb\u57df\u7684\u4e0a\u4e0b\u6587\u4e2d\u8bc4\u4f30\u8fb9\u7684\u91cd\u8981\u6027\uff0c\u53ef\u4ee5\u5206\u4e3a\u5168\u5c40\u6216\u5c40\u90e8\u65b9\u6cd5\u3002\u73b0\u6709\u7f51\u7edc\u9aa8\u5e72\u63d0\u53d6\u65b9\u6cd5\u7684\u4e00\u4e2a\u5173\u952e\u9650\u5236\u662f\uff0c\u5b83\u4eec\u8981\u4e48\u4eba\u4e3a\u5730\u5c06\u9aa8\u5e72\u7684\u62d3\u6251\u7ed3\u6784\u9650\u5236\u4e3a\u7279\u5b9a\u5f62\u5f0f\uff08\u4f8b\u5982\u6811\uff09\uff0c\u8981\u4e48\u9700\u8981\u6307\u5b9a\u4e00\u4e2a\u81ea\u7531\u53c2\u6570\uff08\u4f8b\u5982\u663e\u8457\u6027\u6c34\u5e73\uff09\u6765\u786e\u5b9a\u9aa8\u5e72\u4e2d\u8981\u4fdd\u7559\u7684\u8fb9\u6570\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b8c\u5168\u975e\u53c2\u6570\u7684\u6846\u67b6\u6765\u63a8\u65ad\u52a0\u6743\u7f51\u7edc\u7684\u9aa8\u5e72\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4f7f\u7528\u4fe1\u606f\u8bba\u4e2d\u7684\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\uff08MDL\uff09\u539f\u5219\u81ea\u52a8\u9009\u62e9\u4fdd\u7559\u5728\u9aa8\u5e72\u4e2d\u7684\u6700\u4f73\u8fb9\u6570\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e24\u79cd\u7f16\u7801\u65b9\u6848\uff0c\u4f5c\u4e3a\u5168\u5c40\u548c\u5c40\u90e8\u7f51\u7edc\u9aa8\u5e72\u7684\u76ee\u6807\u51fd\u6570\uff0c\u4ee5\u53ca\u6709\u6548\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u4ee5\u6839\u636e\u8fd9\u4e9b\u76ee\u6807\u8bc6\u522b\u6700\u4f73\u9aa8\u5e72\uff0c\u5176\u8fd0\u884c\u65f6\u590d\u6742\u5ea6\u5728\u8fb9\u6570\u4e0a\u662f\u5bf9\u6570\u7ebf\u6027\u7684\u3002\u6211\u4eec\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u4ee5\u4f7f\u7528\u6700\u5927\u540e\u9a8c\uff08MAP\uff09\u4f30\u8ba1\u7a0b\u5e8f\u548c\u6e10\u8fd1\u7b49\u6548\u7684\u8d1d\u53f6\u65af\u9aa8\u5e72\u751f\u6210\u6a21\u578b\u63a8\u5e7f\u5230\u8fb9\u4e0a\u7684\u4efb\u4f55\u79bb\u6563\u6743\u91cd\u5206\u5e03\u3002\u6211\u4eec\u5728\u771f\u5b9e\u548c\u5408\u6210\u7f51\u7edc\u4e0a\u7684\u4e00\u7cfb\u5217\u4efb\u52a1\u4e2d\u5c06\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0e\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002||\n", "2409.06371": "|**2024-09-10**|[Distilling Generative-Discriminative Representations for Very Low-Resolution Face Recognition](http://arxiv.org/abs/2409.06371)|null|\u7531\u4e8e\u5206\u8fa8\u7387\u4e0b\u964d\u4f1a\u5bfc\u81f4\u4fe1\u606f\u4e30\u5bcc\u7684\u9762\u90e8\u7ec6\u8282\u4e25\u91cd\u4e22\u5931\uff0c\u56e0\u6b64\u6781\u4f4e\u5206\u8fa8\u7387\u4eba\u8138\u8bc6\u522b\u6781\u5177\u6311\u6218\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86\u751f\u6210\u8868\u793a\u548c\u8de8\u5206\u8fa8\u7387\u5bf9\u9f50\u77e5\u8bc6\u84b8\u998f\u7684\u751f\u6210-\u5224\u522b\u8868\u793a\u84b8\u998f\u65b9\u6cd5\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u4e24\u4e2a\u84b8\u998f\u6a21\u5757\u8054\u5408\u84b8\u998f\u751f\u6210\u6a21\u578b\u548c\u5224\u522b\u6a21\u578b\uff0c\u4fc3\u8fdb\u4e86\u6781\u4f4e\u5206\u8fa8\u7387\u4eba\u8138\u8bc6\u522b\u3002\u9996\u5148\uff0c\u751f\u6210\u8868\u793a\u84b8\u998f\u5c06\u9884\u5148\u8bad\u7ec3\u7528\u4e8e\u4eba\u8138\u8d85\u5206\u8fa8\u7387\u7684\u6269\u6563\u6a21\u578b\u7684\u7f16\u7801\u5668\u4f5c\u4e3a\u751f\u6210\u6559\u5e08\uff0c\u901a\u8fc7\u7279\u5f81\u56de\u5f52\u6765\u76d1\u7763\u5b66\u751f\u9aa8\u5e72\u7f51\u7edc\u7684\u5b66\u4e60\uff0c\u7136\u540e\u51bb\u7ed3\u5b66\u751f\u9aa8\u5e72\u7f51\u7edc\u3002\u4e4b\u540e\uff0c\u5224\u522b\u8868\u793a\u84b8\u998f\u8fdb\u4e00\u6b65\u8003\u8651\u5c06\u9884\u5148\u8bad\u7ec3\u597d\u7684\u4eba\u8138\u8bc6\u522b\u5668\u4f5c\u4e3a\u5224\u522b\u6559\u5e08\uff0c\u901a\u8fc7\u8de8\u5206\u8fa8\u7387\u5173\u7cfb\u5bf9\u6bd4\u84b8\u998f\u6765\u76d1\u7763\u5b66\u751f\u5934\u90e8\u7684\u5b66\u4e60\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u53ef\u4ee5\u5c06\u901a\u7528\u7684\u9aa8\u5e72\u7f51\u7edc\u8868\u793a\u8f6c\u6362\u4e3a\u5224\u522b\u5934\u90e8\u8868\u793a\uff0c\u4ece\u800c\u5f62\u6210\u4e00\u4e2a\u9c81\u68d2\u7684\u3001\u5177\u6709\u5224\u522b\u529b\u7684\u5b66\u751f\u6a21\u578b\uff0c\u7528\u4e8e\u6781\u4f4e\u5206\u8fa8\u7387\u4eba\u8138\u8bc6\u522b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6539\u8fdb\u4e86\u6781\u4f4e\u5206\u8fa8\u7387\u4eba\u8138\u4e2d\u7f3a\u5931\u7ec6\u8282\u7684\u6062\u590d\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002\u5728\u4eba\u8138\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6781\u4f4e\u5206\u8fa8\u7387\u4eba\u8138\u7684\u8bc6\u522b\u7cbe\u5ea6\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u548c\u9002\u5e94\u6027\u3002||\n", "2409.06364": "|**2024-09-10**|[What happens to diffusion model likelihood when your model is conditional?](http://arxiv.org/abs/2409.06364)|null|Diffusion Models (DMs) iteratively denoise random samples to produce high-quality data. The iterative sampling process is derived from Stochastic Differential Equations (SDEs), allowing a speed-quality trade-off chosen at inference. Another advantage of sampling with differential equations is exact likelihood computation. These likelihoods have been used to rank unconditional DMs and for out-of-domain classification. Despite the many existing and possible uses of DM likelihoods, the distinct properties captured are unknown, especially in conditional contexts such as Text-To-Image (TTI) or Text-To-Speech synthesis (TTS). Surprisingly, we find that TTS DM likelihoods are agnostic to the text input. TTI likelihood is more expressive but cannot discern confounding prompts. Our results show that applying DMs to conditional tasks reveals inconsistencies and strengthens claims that the properties of DM likelihood are unknown. This impact sheds light on the previously unknown nature of DM likelihoods. Although conditional DMs maximise likelihood, the likelihood in question is not as sensitive to the conditioning input as one expects. This investigation provides a new point-of-view on diffusion likelihoods.||\n", "2409.06355": "|**2024-09-10**|[DiffQRCoder: Diffusion-based Aesthetic QR Code Generation with Scanning Robustness Guided Iterative Refinement](http://arxiv.org/abs/2409.06355)|null|With the success of Diffusion Models for image generation, the technologies also have revolutionized the aesthetic Quick Response (QR) code generation. Despite significant improvements in visual attractiveness for the beautified codes, their scannabilities are usually sacrificed and thus hinder their practical uses in real-world scenarios. To address this issue, we propose a novel Diffusion-based QR Code generator (DiffQRCoder) to effectively craft both scannable and visually pleasing QR codes. The proposed approach introduces Scanning-Robust Perceptual Guidance (SRPG), a new diffusion guidance for Diffusion Models to guarantee the generated aesthetic codes to obey the ground-truth QR codes while maintaining their attractiveness during the denoising process. Additionally, we present another post-processing technique, Scanning Robust Manifold Projected Gradient Descent (SR-MPGD), to further enhance their scanning robustness through iterative latent space optimization. With extensive experiments, the results demonstrate that our approach not only outperforms other compared methods in Scanning Success Rate (SSR) with better or comparable CLIP aesthetic score (CLIP-aes.) but also significantly improves the SSR of the ControlNet-only approach from 60% to 99%. The subjective evaluation indicates that our approach achieves promising visual attractiveness to users as well. Finally, even with different scanning angles and the most rigorous error tolerance settings, our approach robustly achieves over 95% SSR, demonstrating its capability for real-world applications.||\n", "2409.08278": "|**2024-09-12**|[DreamHOI: Subject-Driven Generation of 3D Human-Object Interactions with Diffusion Priors](http://arxiv.org/abs/2409.08278)|null|We present DreamHOI, a novel method for zero-shot synthesis of human-object interactions (HOIs), enabling a 3D human model to realistically interact with any given object based on a textual description. This task is complicated by the varying categories and geometries of real-world objects and the scarcity of datasets encompassing diverse HOIs. To circumvent the need for extensive data, we leverage text-to-image diffusion models trained on billions of image-caption pairs. We optimize the articulation of a skinned human mesh using Score Distillation Sampling (SDS) gradients obtained from these models, which predict image-space edits. However, directly backpropagating image-space gradients into complex articulation parameters is ineffective due to the local nature of such gradients. To overcome this, we introduce a dual implicit-explicit representation of a skinned mesh, combining (implicit) neural radiance fields (NeRFs) with (explicit) skeleton-driven mesh articulation. During optimization, we transition between implicit and explicit forms, grounding the NeRF generation while refining the mesh articulation. We validate our approach through extensive experiments, demonstrating its effectiveness in generating realistic HOIs.||\n", "2409.08273": "|**2024-09-12**|[Hand-Object Interaction Pretraining from Videos](http://arxiv.org/abs/2409.08273)|null|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece 3D \u624b-\u7269\u4f53\u4ea4\u4e92\u8f68\u8ff9\u4e2d\u5b66\u4e60\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u5148\u9a8c\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5229\u7528\u91ce\u5916\u89c6\u9891\u751f\u6210\u611f\u89c9\u8fd0\u52a8\u673a\u5668\u4eba\u8f68\u8ff9\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5c06\u4eba\u624b\u548c\u88ab\u64cd\u7eb5\u7269\u4f53\u63d0\u5347\u5230\u5171\u4eab\u7684 3D \u7a7a\u95f4\u4e2d\uff0c\u5e76\u5c06\u4eba\u4f53\u52a8\u4f5c\u91cd\u5b9a\u5411\u5230\u673a\u5668\u4eba\u52a8\u4f5c\u3002\u5bf9\u8fd9\u4e9b\u6570\u636e\u8fdb\u884c\u751f\u6210\u5efa\u6a21\uff0c\u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4e2a\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u57fa\u7840\u7b56\u7565\u3002\u8be5\u7b56\u7565\u6355\u83b7\u4e86\u4e00\u4e2a\u901a\u7528\u800c\u7075\u6d3b\u7684\u64cd\u4f5c\u5148\u9a8c\u3002\u6211\u4eec\u901a\u8fc7\u7ecf\u9a8c\u8bc1\u660e\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60 (RL) \u548c\u884c\u4e3a\u514b\u9686 (BC) \u5bf9\u8be5\u7b56\u7565\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u6837\u672c\u9ad8\u6548\u9002\u5e94\uff0c\u540c\u65f6\u4e0e\u5148\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5b9a\u6027\u5b9e\u9a8c\u7ed3\u679c\u53ef\u89c1\uff1a\\url{https://hgaurav2k.github.io/hop/}\u3002||\n", "2409.08272": "|**2024-09-12**|[Click2Mask: Local Editing with Dynamic Mask Generation](http://arxiv.org/abs/2409.08272)|null|\u751f\u6210\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u5f7b\u5e95\u6539\u53d8\u4e86\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u9886\u57df\uff0c\u4f7f\u975e\u4e13\u4e1a\u4eba\u58eb\u4e5f\u80fd\u8f7b\u677e\u5b8c\u6210\u8fd9\u4e9b\u4efb\u52a1\u3002\u672c\u6587\u91cd\u70b9\u5173\u6ce8\u5c40\u90e8\u56fe\u50cf\u7f16\u8f91\uff0c\u7279\u522b\u662f\u5411\u5927\u81f4\u6307\u5b9a\u533a\u57df\u6dfb\u52a0\u65b0\u5185\u5bb9\u7684\u4efb\u52a1\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u7cbe\u786e\u7684\u63a9\u7801\u6216\u5bf9\u4f4d\u7f6e\u7684\u8be6\u7ec6\u63cf\u8ff0\uff0c\u8fd9\u53ef\u80fd\u65e2\u9ebb\u70e6\u53c8\u5bb9\u6613\u51fa\u9519\u3002\u6211\u4eec\u63d0\u51fa\u4e86 Click2Mask\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5b83\u53ea\u9700\u4e00\u4e2a\u53c2\u8003\u70b9\uff08\u4ee5\u53ca\u5185\u5bb9\u63cf\u8ff0\uff09\u5373\u53ef\u7b80\u5316\u5c40\u90e8\u7f16\u8f91\u8fc7\u7a0b\u3002\u5728\u6df7\u5408\u6f5c\u5728\u6269\u6563 (BLD) \u8fc7\u7a0b\u4e2d\uff0c\u63a9\u7801\u4f1a\u56f4\u7ed5\u8be5\u70b9\u52a8\u6001\u589e\u957f\uff0c\u5e76\u4ee5\u57fa\u4e8e CLIP \u7684\u8bed\u4e49\u635f\u5931\u4e3a\u6307\u5bfc\u3002Click2Mask \u8d85\u8d8a\u4e86\u57fa\u4e8e\u5206\u5272\u548c\u4f9d\u8d56\u5fae\u8c03\u7684\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5bf9\u7528\u6237\u66f4\u53cb\u597d\u4e14\u4e0a\u4e0b\u6587\u66f4\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6839\u636e\u4eba\u7c7b\u5224\u65ad\u548c\u81ea\u52a8\u6307\u6807\uff0c\u4e0e SoTA \u65b9\u6cd5\u76f8\u6bd4\uff0cClick2Mask \u4e0d\u4ec5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e86\u7528\u6237\u7684\u5de5\u4f5c\u91cf\uff0c\u800c\u4e14\u8fd8\u63d0\u4f9b\u4e86\u5177\u6709\u7ade\u4e89\u529b\u6216\u66f4\u4f18\u7684\u5c40\u90e8\u56fe\u50cf\u5904\u7406\u7ed3\u679c\u3002\u4e3b\u8981\u8d21\u732e\u5305\u62ec\u7b80\u5316\u7528\u6237\u8f93\u5165\u3001\u80fd\u591f\u4e0d\u53d7\u73b0\u6709\u5206\u5272\u9650\u5236\u5730\u81ea\u7531\u6dfb\u52a0\u5bf9\u8c61\uff0c\u4ee5\u53ca\u5c06\u6211\u4eec\u7684\u52a8\u6001\u63a9\u7801\u65b9\u6cd5\u96c6\u6210\u5230\u5176\u4ed6\u7f16\u8f91\u65b9\u6cd5\u4e2d\u7684\u6f5c\u529b\u3002||\n", "2409.08271": "|**2024-09-12**|[DreamBeast: Distilling 3D Fantastical Animals with Part-Aware Knowledge Transfer](http://arxiv.org/abs/2409.08271)|null|We present DreamBeast, a novel method based on score distillation sampling (SDS) for generating fantastical 3D animal assets composed of distinct parts. Existing SDS methods often struggle with this generation task due to a limited understanding of part-level semantics in text-to-image diffusion models. While recent diffusion models, such as Stable Diffusion 3, demonstrate a better part-level understanding, they are prohibitively slow and exhibit other common problems associated with single-view diffusion models. DreamBeast overcomes this limitation through a novel part-aware knowledge transfer mechanism. For each generated asset, we efficiently extract part-level knowledge from the Stable Diffusion 3 model into a 3D Part-Affinity implicit representation. This enables us to instantly generate Part-Affinity maps from arbitrary camera views, which we then use to modulate the guidance of a multi-view diffusion model during SDS to create 3D assets of fantastical animals. DreamBeast significantly enhances the quality of generated 3D creatures with user-specified part compositions while reducing computational overhead, as demonstrated by extensive quantitative and qualitative evaluations.||\n", "2409.08269": "|**2024-09-12**|[Touch2Touch: Cross-Modal Tactile Generation for Object Manipulation](http://arxiv.org/abs/2409.08269)|null|\u73b0\u4eca\u7684\u89e6\u89c9\u4f20\u611f\u5668\u5f62\u6001\u5404\u5f02\uff0c\u5c3a\u5bf8\u4e0d\u4e00\u3002\u7531\u4e8e\u6a21\u578b\u901a\u5e38\u4e0e\u7279\u5b9a\u7684\u4f20\u611f\u5668\u8bbe\u8ba1\u7ed1\u5b9a\uff0c\u56e0\u6b64\u5f00\u53d1\u901a\u7528\u7684\u89e6\u89c9\u5904\u7406\u65b9\u6cd5\u53d8\u5f97\u6781\u5177\u6311\u6218\u6027\u3002\u6211\u4eec\u901a\u8fc7\u5728\u89e6\u89c9\u4f20\u611f\u5668\u4e4b\u95f4\u8fdb\u884c\u8de8\u6a21\u6001\u9884\u6d4b\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff1a\u7ed9\u5b9a\u6765\u81ea\u4e00\u4e2a\u4f20\u611f\u5668\u7684\u89e6\u89c9\u4fe1\u53f7\uff0c\u6211\u4eec\u4f7f\u7528\u751f\u6210\u6a21\u578b\u6765\u4f30\u8ba1\u53e6\u4e00\u4e2a\u4f20\u611f\u5668\u5982\u4f55\u611f\u77e5\u76f8\u540c\u7684\u7269\u7406\u63a5\u89e6\u3002\u8fd9\u5141\u8bb8\u6211\u4eec\u5c06\u7279\u5b9a\u4e8e\u4f20\u611f\u5668\u7684\u7b97\u6cd5\u5e94\u7528\u4e8e\u751f\u6210\u7684\u4fe1\u53f7\u3002\u6211\u4eec\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u6269\u6563\u6a21\u578b\u6765\u5b9e\u73b0\u8fd9\u4e2a\u60f3\u6cd5\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u5728\u6d41\u884c\u7684 GelSlim \u548c Soft Bubble \u4f20\u611f\u5668\u4e4b\u95f4\u8fdb\u884c\u8f6c\u6362\u3002\u4f5c\u4e3a\u4e00\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff0c\u6211\u4eec\u4f7f\u7528 GelSlim \u4f20\u611f\u5668\u6267\u884c\u624b\u6301\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u540c\u65f6\u4f7f\u7528\u4ec5\u5bf9 Soft Bubble \u4fe1\u53f7\u8fdb\u884c\u64cd\u4f5c\u7684\u7b97\u6cd5\u3002\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u53ef\u4ee5\u5728 https://www.mmintlab.com/research/touch2touch/ \u4e0a\u627e\u5230\u3002||\n", "2409.08260": "|**2024-09-12**|[Improving Text-guided Object Inpainting with Semantic Pre-inpainting](http://arxiv.org/abs/2409.08260)|**[link](https://github.com/nnn-s/catdiffusion)**|Recent years have witnessed the success of large text-to-image diffusion models and their remarkable potential to generate high-quality images. The further pursuit of enhancing the editability of images has sparked significant interest in the downstream task of inpainting a novel object described by a text prompt within a designated region in the image. Nevertheless, the problem is not trivial from two aspects: 1) Solely relying on one single U-Net to align text prompt and visual object across all the denoising timesteps is insufficient to generate desired objects; 2) The controllability of object generation is not guaranteed in the intricate sampling space of diffusion model. In this paper, we propose to decompose the typical single-stage object inpainting into two cascaded processes: 1) semantic pre-inpainting that infers the semantic features of desired objects in a multi-modal feature space; 2) high-fieldity object generation in diffusion latent space that pivots on such inpainted semantic features. To achieve this, we cascade a Transformer-based semantic inpainter and an object inpainting diffusion model, leading to a novel CAscaded Transformer-Diffusion (CAT-Diffusion) framework for text-guided object inpainting. Technically, the semantic inpainter is trained to predict the semantic features of the target object conditioning on unmasked context and text prompt. The outputs of the semantic inpainter then act as the informative visual prompts to guide high-fieldity object generation through a reference adapter layer, leading to controllable object inpainting. Extensive evaluations on OpenImages-V6 and MSCOCO validate the superiority of CAT-Diffusion against the state-of-the-art methods. Code is available at \\url{https://github.com/Nnn-s/CATdiffusion}.||\n", "2409.08258": "|**2024-09-12**|[Improving Virtual Try-On with Garment-focused Diffusion Models](http://arxiv.org/abs/2409.08258)|null|Diffusion models have led to the revolutionizing of generative modeling in numerous image synthesis tasks. Nevertheless, it is not trivial to directly apply diffusion models for synthesizing an image of a target person wearing a given in-shop garment, i.e., image-based virtual try-on (VTON) task. The difficulty originates from the aspect that the diffusion process should not only produce holistically high-fidelity photorealistic image of the target person, but also locally preserve every appearance and texture detail of the given garment. To address this, we shape a new Diffusion model, namely GarDiff, which triggers the garment-focused diffusion process with amplified guidance of both basic visual appearance and detailed textures (i.e., high-frequency details) derived from the given garment. GarDiff first remoulds a pre-trained latent diffusion model with additional appearance priors derived from the CLIP and VAE encodings of the reference garment. Meanwhile, a novel garment-focused adapter is integrated into the UNet of diffusion model, pursuing local fine-grained alignment with the visual appearance of reference garment and human pose. We specifically design an appearance loss over the synthesized garment to enhance the crucial, high-frequency details. Extensive experiments on VITON-HD and DressCode datasets demonstrate the superiority of our GarDiff when compared to state-of-the-art VTON approaches. Code is publicly available at: \\href{https://github.com/siqi0905/GarDiff/tree/master}{https://github.com/siqi0905/GarDiff/tree/master}.||\n", "2409.08255": "|**2024-09-12**|[LoRID: Low-Rank Iterative Diffusion for Adversarial Purification](http://arxiv.org/abs/2409.08255)|null|This work presents an information-theoretic examination of diffusion-based purification methods, the state-of-the-art adversarial defenses that utilize diffusion models to remove malicious perturbations in adversarial examples. By theoretically characterizing the inherent purification errors associated with the Markov-based diffusion purifications, we introduce LoRID, a novel Low-Rank Iterative Diffusion purification method designed to remove adversarial perturbation with low intrinsic purification errors. LoRID centers around a multi-stage purification process that leverages multiple rounds of diffusion-denoising loops at the early time-steps of the diffusion models, and the integration of Tucker decomposition, an extension of matrix factorization, to remove adversarial noise at high-noise regimes. Consequently, LoRID increases the effective diffusion time-steps and overcomes strong adversarial attacks, achieving superior robustness performance in CIFAR-10/100, CelebA-HQ, and ImageNet datasets under both white-box and black-box settings.||\n", "2409.08251": "|**2024-09-12**|[Dynamic Prompting of Frozen Text-to-Image Diffusion Models for Panoptic Narrative Grounding](http://arxiv.org/abs/2409.08251)|null|Panoptic narrative grounding (PNG), whose core target is fine-grained image-text alignment, requires a panoptic segmentation of referred objects given a narrative caption. Previous discriminative methods achieve only weak or coarse-grained alignment by panoptic segmentation pretraining or CLIP model adaptation. Given the recent progress of text-to-image Diffusion models, several works have shown their capability to achieve fine-grained image-text alignment through cross-attention maps and improved general segmentation performance. However, the direct use of phrase features as static prompts to apply frozen Diffusion models to the PNG task still suffers from a large task gap and insufficient vision-language interaction, yielding inferior performance. Therefore, we propose an Extractive-Injective Phrase Adapter (EIPA) bypass within the Diffusion UNet to dynamically update phrase prompts with image features and inject the multimodal cues back, which leverages the fine-grained image-text alignment capability of Diffusion models more sufficiently. In addition, we also design a Multi-Level Mutual Aggregation (MLMA) module to reciprocally fuse multi-level image and phrase features for segmentation refinement. Extensive experiments on the PNG benchmark show that our method achieves new state-of-the-art performance.||\n", "2409.08240": "|**2024-09-12**|[IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation](http://arxiv.org/abs/2409.08240)|null|While Text-to-Image (T2I) diffusion models excel at generating visually appealing images of individual instances, they struggle to accurately position and control the features generation of multiple instances. The Layout-to-Image (L2I) task was introduced to address the positioning challenges by incorporating bounding boxes as spatial control signals, but it still falls short in generating precise instance features. In response, we propose the Instance Feature Generation (IFG) task, which aims to ensure both positional accuracy and feature fidelity in generated instances. To address the IFG task, we introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhances feature depiction by incorporating additional appearance tokens and utilizing an Instance Semantic Map to align instance-level features with spatial locations. The IFAdapter guides the diffusion process as a plug-and-play module, making it adaptable to various community models. For evaluation, we contribute an IFG benchmark and develop a verification pipeline to objectively compare models' abilities to generate instances with accurate positioning and features. Experimental results demonstrate that IFAdapter outperforms other models in both quantitative and qualitative evaluations.||\n", "2409.09016": "|**2024-09-13**|[Closed-Loop Visuomotor Control with Generative Expectation for Robotic Manipulation](http://arxiv.org/abs/2409.09016)|**[link](https://github.com/OpenDriveLab/CLOVER)**|Despite significant progress in robotics and embodied AI in recent years, deploying robots for long-horizon tasks remains a great challenge. Majority of prior arts adhere to an open-loop philosophy and lack real-time feedback, leading to error accumulation and undesirable robustness. A handful of approaches have endeavored to establish feedback mechanisms leveraging pixel-level differences or pre-trained visual representations, yet their efficacy and adaptability have been found to be constrained. Inspired by classic closed-loop control systems, we propose CLOVER, a closed-loop visuomotor control framework that incorporates feedback mechanisms to improve adaptive robotic control. CLOVER consists of a text-conditioned video diffusion model for generating visual plans as reference inputs, a measurable embedding space for accurate error quantification, and a feedback-driven controller that refines actions from feedback and initiates replans as needed. Our framework exhibits notable advancement in real-world robotic tasks and achieves state-of-the-art on CALVIN benchmark, improving by 8% over previous open-loop counterparts. Code and checkpoints are maintained at https://github.com/OpenDriveLab/CLOVER.||\n", "2409.08947": "|**2024-09-13**|[A Diffusion Approach to Radiance Field Relighting using Multi-Illumination Synthesis](http://arxiv.org/abs/2409.08947)|null|Relighting radiance fields is severely underconstrained for multi-view data, which is most often captured under a single illumination condition; It is especially hard for full scenes containing multiple objects. We introduce a method to create relightable radiance fields using such single-illumination data by exploiting priors extracted from 2D image diffusion models. We first fine-tune a 2D diffusion model on a multi-illumination dataset conditioned by light direction, allowing us to augment a single-illumination capture into a realistic -- but possibly inconsistent -- multi-illumination dataset from directly defined light directions. We use this augmented data to create a relightable radiance field represented by 3D Gaussian splats. To allow direct control of light direction for low-frequency lighting, we represent appearance with a multi-layer perceptron parameterized on light direction. To enforce multi-view consistency and overcome inaccuracies we optimize a per-image auxiliary feature vector. We show results on synthetic and real multi-view data under single illumination, demonstrating that our method successfully exploits 2D diffusion model priors to allow realistic 3D relighting for complete scenes. Project site https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/||\n", "2409.08917": "|**2024-09-13**|[Latent Space Score-based Diffusion Model for Probabilistic Multivariate Time Series Imputation](http://arxiv.org/abs/2409.08917)|**[link](https://github.com/gorgen2020/LSSDM_imputation)**|Accurate imputation is essential for the reliability and success of downstream tasks. Recently, diffusion models have attracted great attention in this field. However, these models neglect the latent distribution in a lower-dimensional space derived from the observed data, which limits the generative capacity of the diffusion model. Additionally, dealing with the original missing data without labels becomes particularly problematic. To address these issues, we propose the Latent Space Score-Based Diffusion Model (LSSDM) for probabilistic multivariate time series imputation. Observed values are projected onto low-dimensional latent space and coarse values of the missing data are reconstructed without knowing their ground truth values by this unsupervised learning approach. Finally, the reconstructed values are fed into a conditional diffusion model to obtain the precise imputed values of the time series. In this way, LSSDM not only possesses the power to identify the latent distribution but also seamlessly integrates the diffusion model to obtain the high-fidelity imputed values and assess the uncertainty of the dataset. Experimental results demonstrate that LSSDM achieves superior imputation performance while also providing a better explanation and uncertainty analysis of the imputation mechanism. The website of the code is \\textit{https://github.com/gorgen2020/LSSDM\\_imputation}.||\n", "2409.08906": "|**2024-09-13**|[Gaussian is All You Need: A Unified Framework for Solving Inverse Problems via Diffusion Posterior Sampling](http://arxiv.org/abs/2409.08906)|null|Diffusion models can generate a variety of high-quality images by modeling complex data distributions. Trained diffusion models can also be very effective image priors for solving inverse problems. Most of the existing diffusion-based methods integrate data consistency steps within the diffusion reverse sampling process. The data consistency steps rely on an approximate likelihood function. In this paper, we show that the existing approximations are either insufficient or computationally inefficient. To address these issues, we propose a unified likelihood approximation method that incorporates a covariance correction term to enhance the performance and avoids propagating gradients through the diffusion model. The correction term, when integrated into the reverse diffusion sampling process, achieves better convergence towards the true data posterior for selected distributions and improves performance on real-world natural image datasets. Furthermore, we present an efficient way to factorize and invert the covariance matrix of the likelihood function for several inverse problems. We present comprehensive experiments to demonstrate the effectiveness of our method over several existing approaches.||\n", "2409.08861": "|**2024-09-13**|[Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control](http://arxiv.org/abs/2409.08861)|null|Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there has not been many theoretically-sound methods for improving these models with reward fine-tuning. In this work, we cast reward fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific memoryless noise schedule must be enforced during fine-tuning, in order to account for the dependency between the noise variable and the generated samples. We also propose a new algorithm named Adjoint Matching which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We find that our approach significantly improves over existing methods for reward fine-tuning, achieving better consistency, realism, and generalization to unseen human preference reward models, while retaining sample diversity.||\n", "2409.08857": "|**2024-09-13**|[InstantDrag: Improving Interactivity in Drag-based Image Editing](http://arxiv.org/abs/2409.08857)|null|Drag-based image editing has recently gained popularity for its interactivity and precision. However, despite the ability of text-to-image models to generate samples within a second, drag editing still lags behind due to the challenge of accurately reflecting user interaction while maintaining image content. Some existing approaches rely on computationally intensive per-image optimization or intricate guidance-based methods, requiring additional inputs such as masks for movable regions and text prompts, thereby compromising the interactivity of the editing process. We introduce InstantDrag, an optimization-free pipeline that enhances interactivity and speed, requiring only an image and a drag instruction as input. InstantDrag consists of two carefully designed networks: a drag-conditioned optical flow generator (FlowGen) and an optical flow-conditioned diffusion model (FlowDiffusion). InstantDrag learns motion dynamics for drag-based image editing in real-world video datasets by decomposing the task into motion generation and motion-conditioned image generation. We demonstrate InstantDrag's capability to perform fast, photo-realistic edits without masks or text prompts through experiments on facial video datasets and general scenes. These results highlight the efficiency of our approach in handling drag-based image editing, making it a promising solution for interactive, real-time applications.||\n", "2409.08850": "|**2024-09-13**|[DX2CT: Diffusion Model for 3D CT Reconstruction from Bi or Mono-planar 2D X-ray(s)](http://arxiv.org/abs/2409.08850)|null|Computational tomography (CT) provides high-resolution medical imaging, but it can expose patients to high radiation. X-ray scanners have low radiation exposure, but their resolutions are low. This paper proposes a new conditional diffusion model, DX2CT, that reconstructs three-dimensional (3D) CT volumes from bi or mono-planar X-ray image(s). Proposed DX2CT consists of two key components: 1) modulating feature maps extracted from two-dimensional (2D) X-ray(s) with 3D positions of CT volume using a new transformer and 2) effectively using the modulated 3D position-aware feature maps as conditions of DX2CT. In particular, the proposed transformer can provide conditions with rich information of a target CT slice to the conditional diffusion model, enabling high-quality CT reconstruction. Our experiments with the bi or mono-planar X-ray(s) benchmark datasets show that proposed DX2CT outperforms several state-of-the-art methods. Our codes and model will be available at: https://www.github.com/intyeger/DX2CT.||\n", "2409.08731": "|**2024-09-13**|[DFADD: The Diffusion and Flow-Matching Based Audio Deepfake Dataset](http://arxiv.org/abs/2409.08731)|**[link](https://github.com/dfadd-dataset/dfadd_demo_pages)**|Mainstream zero-shot TTS production systems like Voicebox and Seed-TTS achieve human parity speech by leveraging Flow-matching and Diffusion models, respectively. Unfortunately, human-level audio synthesis leads to identity misuse and information security issues. Currently, many antispoofing models have been developed against deepfake audio. However, the efficacy of current state-of-the-art anti-spoofing models in countering audio synthesized by diffusion and flowmatching based TTS systems remains unknown. In this paper, we proposed the Diffusion and Flow-matching based Audio Deepfake (DFADD) dataset. The DFADD dataset collected the deepfake audio based on advanced diffusion and flowmatching TTS models. Additionally, we reveal that current anti-spoofing models lack sufficient robustness against highly human-like audio generated by diffusion and flow-matching TTS systems. The proposed DFADD dataset addresses this gap and provides a valuable resource for developing more resilient anti-spoofing models.||\n", "2409.08601": "|**2024-09-13**|[STA-V2A: Video-to-Audio Generation with Semantic and Temporal Alignment](http://arxiv.org/abs/2409.08601)|null|Visual and auditory perception are two crucial ways humans experience the world. Text-to-video generation has made remarkable progress over the past year, but the absence of harmonious audio in generated video limits its broader applications. In this paper, we propose Semantic and Temporal Aligned Video-to-Audio (STA-V2A), an approach that enhances audio generation from videos by extracting both local temporal and global semantic video features and combining these refined video features with text as cross-modal guidance. To address the issue of information redundancy in videos, we propose an onset prediction pretext task for local temporal feature extraction and an attentive pooling module for global semantic feature extraction. To supplement the insufficient semantic information in videos, we propose a Latent Diffusion Model with Text-to-Audio priors initialization and cross-modal guidance. We also introduce Audio-Audio Align, a new metric to assess audio-temporal alignment. Subjective and objective metrics demonstrate that our method surpasses existing Video-to-Audio models in generating audio with better quality, semantic consistency, and temporal alignment. The ablation experiment validated the effectiveness of each module. Audio samples are available at https://y-ren16.github.io/STAV2A.||\n", "2409.08583": "|**2024-09-13**|[LHQ-SVC: Lightweight and High Quality Singing Voice Conversion Modeling](http://arxiv.org/abs/2409.08583)|null|Singing Voice Conversion (SVC) has emerged as a significant subfield of Voice Conversion (VC), enabling the transformation of one singer's voice into another while preserving musical elements such as melody, rhythm, and timbre. Traditional SVC methods have limitations in terms of audio quality, data requirements, and computational complexity. In this paper, we propose LHQ-SVC, a lightweight, CPU-compatible model based on the SVC framework and diffusion model, designed to reduce model size and computational demand without sacrificing performance. We incorporate features to improve inference quality, and optimize for CPU execution by using performance tuning tools and parallel computing frameworks. Our experiments demonstrate that LHQ-SVC maintains competitive performance, with significant improvements in processing speed and efficiency across different devices. The results suggest that LHQ-SVC can meet||\n", "2409.11406": "|**2024-09-17**|[Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion](http://arxiv.org/abs/2409.11406)|null|In 3D modeling, designers often use an existing 3D model as a reference to create new ones. This practice has inspired the development of Phidias, a novel generative model that uses diffusion for reference-augmented 3D generation. Given an image, our method leverages a retrieved or user-provided 3D reference model to guide the generation process, thereby enhancing the generation quality, generalization ability, and controllability. Our model integrates three key components: 1) meta-ControlNet that dynamically modulates the conditioning strength, 2) dynamic reference routing that mitigates misalignment between the input image and 3D reference, and 3) self-reference augmentations that enable self-supervised training with a progressive curriculum. Collectively, these designs result in a clear improvement over existing methods. Phidias establishes a unified framework for 3D generation using text, image, and 3D conditions with versatile applications.||\n", "2409.11401": "|**2024-09-17**|[Teaching dark matter simulations to speak the halo language](http://arxiv.org/abs/2409.11401)|**[link](https://github.com/shivampcosmo/gotham)**|We develop a transformer-based conditional generative model for discrete point objects and their properties. We use it to build a model for populating cosmological simulations with gravitationally collapsed structures called dark matter halos. Specifically, we condition our model with dark matter distribution obtained from fast, approximate simulations to recover the correct three-dimensional positions and masses of individual halos. This leads to a first model that can recover the statistical properties of the halos at small scales to better than 3% level using an accelerated dark matter simulation. This trained model can then be applied to simulations with significantly larger volumes which would otherwise be computationally prohibitive with traditional simulations, and also provides a crucial missing link in making end-to-end differentiable cosmological simulations. The code, named GOTHAM (Generative cOnditional Transformer for Halo's Auto-regressive Modeling) is publicly available at \\url{https://github.com/shivampcosmo/GOTHAM}.||\n", "2409.11380": "|**2024-09-17**|[Ultrasound Image Enhancement with the Variance of Diffusion Models](http://arxiv.org/abs/2409.11380)|**[link](https://github.com/yuxin-zhang-jasmine/ius2024_diffusion)**|Ultrasound imaging, despite its widespread use in medicine, often suffers from various sources of noise and artifacts that impact the signal-to-noise ratio and overall image quality. Enhancing ultrasound images requires a delicate balance between contrast, resolution, and speckle preservation. This paper introduces a novel approach that integrates adaptive beamforming with denoising diffusion-based variance imaging to address this challenge. By applying Eigenspace-Based Minimum Variance (EBMV) beamforming and employing a denoising diffusion model fine-tuned on ultrasound data, our method computes the variance across multiple diffusion-denoised samples to produce high-quality despeckled images. This approach leverages both the inherent multiplicative noise of ultrasound and the stochastic nature of diffusion models. Experimental results on a publicly available dataset demonstrate the effectiveness of our method in achieving superior image reconstructions from single plane-wave acquisitions. The code is available at: https://github.com/Yuxin-Zhang-Jasmine/IUS2024_Diffusion.||\n", "2409.11367": "|**2024-09-17**|[OSV: One Step is Enough for High-Quality Image to Video Generation](http://arxiv.org/abs/2409.11367)|null|Video diffusion models have shown great potential in generating high-quality videos, making them an increasingly popular focus. However, their inherent iterative nature leads to substantial computational and time costs. While efforts have been made to accelerate video diffusion by reducing inference steps (through techniques like consistency distillation) and GAN training (these approaches often fall short in either performance or training stability). In this work, we introduce a two-stage training framework that effectively combines consistency distillation with GAN training to address these challenges. Additionally, we propose a novel video discriminator design, which eliminates the need for decoding the video latents and improves the final performance. Our model is capable of producing high-quality videos in merely one-step, with the flexibility to perform multi-step refinement for further performance enhancement. Our quantitative evaluation on the OpenWebVid-1M benchmark shows that our model significantly outperforms existing methods. Notably, our 1-step performance(FVD 171.15) exceeds the 8-step performance of the consistency distillation based method, AnimateLCM (FVD 184.79), and approaches the 25-step performance of advanced Stable Video Diffusion (FVD 156.94).||\n", "2409.11355": "|**2024-09-17**|[Fine-Tuning Image-Conditional Diffusion Models is Easier than You Think](http://arxiv.org/abs/2409.11355)|**[link](https://github.com/VisualComputingInstitute/diffusion-e2e-ft)**|Recent work showed that large diffusion models can be reused as highly precise monocular depth estimators by casting depth estimation as an image-conditional image generation task. While the proposed model achieved state-of-the-art results, high computational demands due to multi-step inference limited its use in many scenarios. In this paper, we show that the perceived inefficiency was caused by a flaw in the inference pipeline that has so far gone unnoticed. The fixed model performs comparably to the best previously reported configuration while being more than 200$\\times$ faster. To optimize for downstream task performance, we perform end-to-end fine-tuning on top of the single-step model with task-specific losses and get a deterministic model that outperforms all other diffusion-based depth and normal estimation models on common zero-shot benchmarks. We surprisingly find that this fine-tuning protocol also works directly on Stable Diffusion and achieves comparable performance to current state-of-the-art diffusion-based depth and normal estimation models, calling into question some of the conclusions drawn from prior works.||\n", "2409.11340": "|**2024-09-17**|[OmniGen: Unified Image Generation](http://arxiv.org/abs/2409.11340)|**[link](https://github.com/vectorspacelab/omnigen)**|In this work, we introduce OmniGen, a new diffusion model for unified image generation. Unlike popular diffusion models (e.g., Stable Diffusion), OmniGen no longer requires additional modules such as ControlNet or IP-Adapter to process diverse control conditions. OmniGenis characterized by the following features: 1) Unification: OmniGen not only demonstrates text-to-image generation capabilities but also inherently supports other downstream tasks, such as image editing, subject-driven generation, and visual-conditional generation. Additionally, OmniGen can handle classical computer vision tasks by transforming them into image generation tasks, such as edge detection and human pose recognition. 2) Simplicity: The architecture of OmniGen is highly simplified, eliminating the need for additional text encoders. Moreover, it is more user-friendly compared to existing diffusion models, enabling complex tasks to be accomplished through instructions without the need for extra preprocessing steps (e.g., human pose estimation), thereby significantly simplifying the workflow of image generation. 3) Knowledge Transfer: Through learning in a unified format, OmniGen effectively transfers knowledge across different tasks, manages unseen tasks and domains, and exhibits novel capabilities. We also explore the model's reasoning capabilities and potential applications of chain-of-thought mechanism. This work represents the first attempt at a general-purpose image generation model, and there remain several unresolved issues. We will open-source the related resources at https://github.com/VectorSpaceLab/OmniGen to foster advancements in this field.||\n", "2409.11315": "|**2024-09-17**|[fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction](http://arxiv.org/abs/2409.11315)|null|Reconstructing 3D visuals from functional Magnetic Resonance Imaging (fMRI) data, introduced as Recon3DMind in our conference work, is of significant interest to both cognitive neuroscience and computer vision. To advance this task, we present the fMRI-3D dataset, which includes data from 15 participants and showcases a total of 4768 3D objects. The dataset comprises two components: fMRI-Shape, previously introduced and accessible at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape, and fMRI-Objaverse, proposed in this paper and available at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse. fMRI-Objaverse includes data from 5 subjects, 4 of whom are also part of the Core set in fMRI-Shape, with each subject viewing 3142 3D objects across 117 categories, all accompanied by text captions. This significantly enhances the diversity and potential applications of the dataset. Additionally, we propose MinD-3D, a novel framework designed to decode 3D visual information from fMRI signals. The framework first extracts and aggregates features from fMRI data using a neuro-fusion encoder, then employs a feature-bridge diffusion model to generate visual features, and finally reconstructs the 3D object using a generative transformer decoder. We establish new benchmarks by designing metrics at both semantic and structural levels to evaluate model performance. Furthermore, we assess our model's effectiveness in an Out-of-Distribution setting and analyze the attribution of the extracted features and the visual ROIs in fMRI signals. Our experiments demonstrate that MinD-3D not only reconstructs 3D objects with high semantic and spatial accuracy but also deepens our understanding of how human brain processes 3D visual information. Project page at: https://jianxgao.github.io/MinD-3D.||\n", "2409.11308": "|**2024-09-17**|[SpMis: An Investigation of Synthetic Spoken Misinformation Detection](http://arxiv.org/abs/2409.11308)|null|In recent years, speech generation technology has advanced rapidly, fueled by generative models and large-scale training techniques. While these developments have enabled the production of high-quality synthetic speech, they have also raised concerns about the misuse of this technology, particularly for generating synthetic misinformation. Current research primarily focuses on distinguishing machine-generated speech from human-produced speech, but the more urgent challenge is detecting misinformation within spoken content. This task requires a thorough analysis of factors such as speaker identity, topic, and synthesis. To address this need, we conduct an initial investigation into synthetic spoken misinformation detection by introducing an open-source dataset, SpMis. SpMis includes speech synthesized from over 1,000 speakers across five common topics, utilizing state-of-the-art text-to-speech systems. Although our results show promising detection capabilities, they also reveal substantial challenges for practical implementation, underscoring the importance of ongoing research in this critical area.||\n", "2409.11292": "|**2024-09-17**|[DroneDiffusion: Robust Quadrotor Dynamics Learning with Diffusion Models](http://arxiv.org/abs/2409.11292)|null|An inherent fragility of quadrotor systems stems from model inaccuracies and external disturbances. These factors hinder performance and compromise the stability of the system, making precise control challenging. Existing model-based approaches either make deterministic assumptions, utilize Gaussian-based representations of uncertainty, or rely on nominal models, all of which often fall short in capturing the complex, multimodal nature of real-world dynamics. This work introduces DroneDiffusion, a novel framework that leverages conditional diffusion models to learn quadrotor dynamics, formulated as a sequence generation task. DroneDiffusion achieves superior generalization to unseen, complex scenarios by capturing the temporal nature of uncertainties and mitigating error propagation. We integrate the learned dynamics with an adaptive controller for trajectory tracking with stability guarantees. Extensive experiments in both simulation and real-world flights demonstrate the robustness of the framework across a range of scenarios, including unfamiliar flight paths and varying payloads, velocities, and wind disturbances.||\n", "2409.11228": "|**2024-09-17**|[Learning Source Disentanglement in Neural Audio Codec](http://arxiv.org/abs/2409.11228)|null|Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.||\n", "2409.12189": "|**2024-09-18**|[Massively Multi-Person 3D Human Motion Forecasting with Scene Context](http://arxiv.org/abs/2409.12189)|**[link](https://github.com/felixbmuller/sast)**|Forecasting long-term 3D human motion is challenging: the stochasticity of human behavior makes it hard to generate realistic human motion from the input sequence alone. Information on the scene environment and the motion of nearby people can greatly aid the generation process. We propose a scene-aware social transformer model (SAST) to forecast long-term (10s) human motion motion. Unlike previous models, our approach can model interactions between both widely varying numbers of people and objects in a scene. We combine a temporal convolutional encoder-decoder architecture with a Transformer-based bottleneck that allows us to efficiently combine motion and scene information. We model the conditional motion distribution using denoising diffusion models. We benchmark our approach on the Humans in Kitchens dataset, which contains 1 to 16 persons and 29 to 50 objects that are visible simultaneously. Our model outperforms other approaches in terms of realism and diversity on different metrics and in a user study. Code is available at https://github.com/felixbmuller/SAST.||\n", "2409.12140": "|**2024-09-18**|[MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion](http://arxiv.org/abs/2409.12140)|null|We introduce MoRAG, a novel multi-part fusion based retrieval-augmented generation strategy for text-based human motion generation. The method enhances motion diffusion models by leveraging additional knowledge obtained through an improved motion retrieval process. By effectively prompting large language models (LLMs), we address spelling errors and rephrasing issues in motion retrieval. Our approach utilizes a multi-part retrieval strategy to improve the generalizability of motion retrieval across the language space. We create diverse samples through the spatial composition of the retrieved motions. Furthermore, by utilizing low-level, part-specific motion information, we can construct motion samples for unseen text descriptions. Our experiments demonstrate that our framework can serve as a plug-and-play module, improving the performance of motion diffusion models. Code, pretrained models and sample videos will be made available at: https://motion-rag.github.io/||\n", "2409.12099": "|**2024-09-18**|[Brain-Streams: fMRI-to-Image Reconstruction with Multi-modal Guidance](http://arxiv.org/abs/2409.12099)|null|Understanding how humans process visual information is one of the crucial steps for unraveling the underlying mechanism of brain activity. Recently, this curiosity has motivated the fMRI-to-image reconstruction task; given the fMRI data from visual stimuli, it aims to reconstruct the corresponding visual stimuli. Surprisingly, leveraging powerful generative models such as the Latent Diffusion Model (LDM) has shown promising results in reconstructing complex visual stimuli such as high-resolution natural images from vision datasets. Despite the impressive structural fidelity of these reconstructions, they often lack details of small objects, ambiguous shapes, and semantic nuances. Consequently, the incorporation of additional semantic knowledge, beyond mere visuals, becomes imperative. In light of this, we exploit how modern LDMs effectively incorporate multi-modal guidance (text guidance, visual guidance, and image layout) for structurally and semantically plausible image generations. Specifically, inspired by the two-streams hypothesis suggesting that perceptual and semantic information are processed in different brain regions, our framework, Brain-Streams, maps fMRI signals from these brain regions to appropriate embeddings. That is, by extracting textual guidance from semantic information regions and visual guidance from perceptual information regions, Brain-Streams provides accurate multi-modal guidance to LDMs. We validate the reconstruction ability of Brain-Streams both quantitatively and qualitatively on a real fMRI dataset comprising natural image stimuli and fMRI data.||\n", "2409.12080": "|**2024-09-18**|[Design of Ligand-Binding Proteins with Atomic Flow Matching](http://arxiv.org/abs/2409.12080)|null|Designing novel proteins that bind to small molecules is a long-standing challenge in computational biology, with applications in developing catalysts, biosensors, and more. Current computational methods rely on the assumption that the binding pose of the target molecule is known, which is not always feasible, as conformations of novel targets are often unknown and tend to change upon binding. In this work, we formulate proteins and molecules as unified biotokens, and present AtomFlow, a novel deep generative model under the flow-matching framework for the design of ligand-binding proteins from the 2D target molecular graph alone. Operating on representative atoms of biotokens, AtomFlow captures the flexibility of ligands and generates ligand conformations and protein backbone structures iteratively. We consider the multi-scale nature of biotokens and demonstrate that AtomFlow can be effectively trained on a subset of structures from the Protein Data Bank, by matching flow vector field using an SE(3) equivariant structure prediction network. Experimental results show that our method can generate high fidelity ligand-binding proteins and achieve performance comparable to the state-of-the-art model RFDiffusionAA, while not requiring bound ligand structures. As a general framework, AtomFlow holds the potential to be applied to various biomolecule generation tasks in the future.||\n", "2409.12024": "|**2024-09-18**|[LEMON: Localized Editing with Mesh Optimization and Neural Shaders](http://arxiv.org/abs/2409.12024)|null|In practical use cases, polygonal mesh editing can be faster than generating new ones, but it can still be challenging and time-consuming for users. Existing solutions for this problem tend to focus on a single task, either geometry or novel view synthesis, which often leads to disjointed results between the mesh and view. In this work, we propose LEMON, a mesh editing pipeline that combines neural deferred shading with localized mesh optimization. Our approach begins by identifying the most important vertices in the mesh for editing, utilizing a segmentation model to focus on these key regions. Given multi-view images of an object, we optimize a neural shader and a polygonal mesh while extracting the normal map and the rendered image from each view. By using these outputs as conditioning data, we edit the input images with a text-to-image diffusion model and iteratively update our dataset while deforming the mesh. This process results in a polygonal mesh that is edited according to the given text instruction, preserving the geometric characteristics of the initial mesh while focusing on the most significant areas. We evaluate our pipeline using the DTU dataset, demonstrating that it generates finely-edited meshes more rapidly than the current state-of-the-art methods. We include our code and additional results in the supplementary material.||\n", "2409.11920": "|**2024-09-18**|[Generation of Complex 3D Human Motion by Temporal and Spatial Composition of Diffusion Models](http://arxiv.org/abs/2409.11920)|null|In this paper, we address the challenge of generating realistic 3D human motions for action classes that were never seen during the training phase. Our approach involves decomposing complex actions into simpler movements, specifically those observed during training, by leveraging the knowledge of human motion contained in GPTs models. These simpler movements are then combined into a single, realistic animation using the properties of diffusion models. Our claim is that this decomposition and subsequent recombination of simple movements can synthesize an animation that accurately represents the complex input action. This method operates during the inference phase and can be integrated with any pre-trained diffusion model, enabling the synthesis of motion classes not present in the training data. We evaluate our method by dividing two benchmark human motion datasets into basic and complex actions, and then compare its performance against the state-of-the-art.||\n", "2409.11904": "|**2024-09-18**|[Finding the Subjective Truth: Collecting 2 Million Votes for Comprehensive Gen-AI Model Evaluation](http://arxiv.org/abs/2409.11904)|null|Efficiently evaluating the performance of text-to-image models is difficult as it inherently requires subjective judgment and human preference, making it hard to compare different models and quantify the state of the art. Leveraging Rapidata's technology, we present an efficient annotation framework that sources human feedback from a diverse, global pool of annotators. Our study collected over 2 million annotations across 4,512 images, evaluating four prominent models (DALL-E 3, Flux.1, MidJourney, and Stable Diffusion) on style preference, coherence, and text-to-image alignment. We demonstrate that our approach makes it feasible to comprehensively rank image generation models based on a vast pool of annotators and show that the diverse annotator demographics reflect the world population, significantly decreasing the risk of biases.||\n", "2409.11836": "|**2024-09-18**|[NT-ViT: Neural Transcoding Vision Transformers for EEG-to-fMRI Synthesis](http://arxiv.org/abs/2409.11836)|null|This paper introduces the Neural Transcoding Vision Transformer (\\modelname), a generative model designed to estimate high-resolution functional Magnetic Resonance Imaging (fMRI) samples from simultaneous Electroencephalography (EEG) data. A key feature of \\modelname is its Domain Matching (DM) sub-module which effectively aligns the latent EEG representations with those of fMRI volumes, enhancing the model's accuracy and reliability. Unlike previous methods that tend to struggle with fidelity and reproducibility of images, \\modelname addresses these challenges by ensuring methodological integrity and higher-quality reconstructions which we showcase through extensive evaluation on two benchmark datasets; \\modelname outperforms the current state-of-the-art by a significant margin in both cases, e.g. achieving a $10\\times$ reduction in RMSE and a $3.14\\times$ increase in SSIM on the Oddball dataset. An ablation study also provides insights into the contribution of each component to the model's overall effectiveness. This development is critical in offering a new approach to lessen the time and financial constraints typically linked with high-resolution brain imaging, thereby aiding in the swift and precise diagnosis of neurological disorders. Although it is not a replacement for actual fMRI but rather a step towards making such imaging more accessible, we believe that it represents a pivotal advancement in clinical practice and neuroscience research. Code is available at \\url{https://github.com/rom42pla/ntvit}.||\n", "2409.11835": "|**2024-09-18**|[DPI-TTS: Directional Patch Interaction for Fast-Converging and Style Temporal Modeling in Text-to-Speech](http://arxiv.org/abs/2409.11835)|null|In recent years, speech diffusion models have advanced rapidly. Alongside the widely used U-Net architecture, transformer-based models such as the Diffusion Transformer (DiT) have also gained attention. However, current DiT speech models treat Mel spectrograms as general images, which overlooks the specific acoustic properties of speech. To address these limitations, we propose a method called Directional Patch Interaction for Text-to-Speech (DPI-TTS), which builds on DiT and achieves fast training without compromising accuracy. Notably, DPI-TTS employs a low-to-high frequency, frame-by-frame progressive inference approach that aligns more closely with acoustic properties, enhancing the naturalness of the generated speech. Additionally, we introduce a fine-grained style temporal modeling method that further improves speaker style similarity. Experimental results demonstrate that our method increases the training speed by nearly 2 times and significantly outperforms the baseline models.||\n", "2409.11831": "|**2024-09-18**|[RaggeDi: Diffusion-based State Estimation of Disordered Rags, Sheets, Towels and Blankets](http://arxiv.org/abs/2409.11831)|null|Cloth state estimation is an important problem in robotics. It is essential for the robot to know the accurate state to manipulate cloth and execute tasks such as robotic dressing, stitching, and covering/uncovering human beings. However, estimating cloth state accurately remains challenging due to its high flexibility and self-occlusion. This paper proposes a diffusion model-based pipeline that formulates the cloth state estimation as an image generation problem by representing the cloth state as an RGB image that describes the point-wise translation (translation map) between a pre-defined flattened mesh and the deformed mesh in a canonical space. Then we train a conditional diffusion-based image generation model to predict the translation map based on an observation. Experiments are conducted in both simulation and the real world to validate the performance of our method. Results indicate that our method outperforms two recent methods in both accuracy and speed.||\n", "2409.18128": "|**2024-09-26**|[FlowTurbo: Towards Real-time Flow-Based Image Generation with Velocity Refiner](http://arxiv.org/abs/2409.18128)|**[link](https://github.com/shiml20/flowturbo)**|\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u5728\u89c6\u89c9\u751f\u6210\u65b9\u9762\u7684\u6210\u529f\uff0c\u57fa\u4e8e\u6d41\u7684\u6a21\u578b\u4f5c\u4e3a\u53e6\u4e00\u7c7b\u91cd\u8981\u7684\u751f\u6210\u6a21\u578b\u91cd\u65b0\u5174\u8d77\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u4e0e\u4e4b\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002\u901a\u8fc7\u6d41\u5339\u914d\u5b66\u4e60\u901f\u5ea6\u573a\uff0c\u57fa\u4e8e\u6d41\u7684\u6a21\u578b\u503e\u5411\u4e8e\u4ea7\u751f\u66f4\u76f4\u7684\u91c7\u6837\u8f68\u8ff9\uff0c\u8fd9\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u662f\u6709\u5229\u7684\u3002\u7136\u800c\uff0c\u4e0e\u5feb\u901f\u91c7\u6837\u5668\u5df2\u7ecf\u5f97\u5230\u5f88\u597d\u53d1\u5c55\u7684\u6269\u6563\u6a21\u578b\u4e0d\u540c\uff0c\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\u7684\u6709\u6548\u91c7\u6837\u8fd8\u5f88\u5c11\u88ab\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aFlowTurbo\u7684\u6846\u67b6\uff0c\u4ee5\u52a0\u901f\u57fa\u4e8e\u6d41\u7684\u6a21\u578b\u7684\u91c7\u6837\uff0c\u540c\u65f6\u63d0\u9ad8\u91c7\u6837\u8d28\u91cf\u3002\u6211\u4eec\u7684\u4e3b\u8981\u89c2\u5bdf\u7ed3\u679c\u662f\uff0c\u57fa\u4e8e\u6d41\u6a21\u578b\u4e2d\u7684\u901f\u5ea6\u9884\u6d4b\u5668\u8f93\u51fa\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u4f1a\u53d8\u5f97\u7a33\u5b9a\uff0c\u4ece\u800c\u53ef\u4ee5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u901f\u5ea6\u4f18\u5316\u5668\u4f30\u8ba1\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u4e9b\u6280\u672f\uff0c\u5305\u62ec\u4f2a\u6821\u6b63\u5668\u548c\u6837\u672c\u611f\u77e5\u7f16\u8bd1\uff0c\u4ee5\u8fdb\u4e00\u6b65\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u3002\u7531\u4e8eFlowTurbo\u6ca1\u6709\u6539\u53d8\u591a\u6b65\u91c7\u6837\u8303\u5f0f\uff0c\u56e0\u6b64\u53ef\u4ee5\u6709\u6548\u5730\u5e94\u7528\u4e8e\u56fe\u50cf\u7f16\u8f91\u3001\u4fee\u590d\u7b49\u5404\u79cd\u4efb\u52a1\u3002\u901a\u8fc7\u5c06FlowTurbo\u96c6\u6210\u5230\u4e0d\u540c\u7684\u57fa\u4e8e\u6d41\u7684\u6a21\u578b\u4e2d\uff0c\u6211\u4eec\u5728\u7c7b\u522b\u6761\u4ef6\u751f\u6210\u4e0a\u83b7\u5f97\u4e8653.1%$\\sim$58.3%\u7684\u52a0\u901f\u6bd4\uff0c\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e0a\u83b7\u5f97\u4e8629.8%$\\sim$38.5%\u7684\u52a0\u901f\u6bd4\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cFlowTurbo\u5728ImageNet\u4e0a\u5b9e\u73b0\u4e86100 (ms / img)\u65f6FID\u4e3a2.12\uff0c38 (ms / img)\u65f6FID\u4e3a3.93\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u56fe\u50cf\u751f\u6210\uff0c\u5e76\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002\u4ee3\u7801\u53ef\u5728https://github.com/shiml20/FlowTurbo\u83b7\u53d6\u3002||\n", "2409.18124": "|**2024-09-26**|[Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction](http://arxiv.org/abs/2409.18124)|null|\u5229\u7528\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u89c6\u89c9\u5148\u9a8c\u77e5\u8bc6\u4e3a\u589e\u5f3a\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f88\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4e0d\u52a0\u6279\u5224\u5730\u4f7f\u7528\u539f\u59cb\u7684\u6269\u6563\u516c\u5f0f\uff0c\u7531\u4e8e\u5bc6\u96c6\u9884\u6d4b\u548c\u56fe\u50cf\u751f\u6210\u4e4b\u95f4\u7684\u6839\u672c\u5dee\u5f02\uff0c\u8fd9\u53ef\u80fd\u4e0d\u662f\u6700\u4f73\u9009\u62e9\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5bf9\u7528\u4e8e\u5bc6\u96c6\u9884\u6d4b\u7684\u6269\u6563\u516c\u5f0f\u8fdb\u884c\u4e86\u7cfb\u7edf\u5206\u6790\uff0c\u91cd\u70b9\u5173\u6ce8\u8d28\u91cf\u548c\u6548\u7387\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u7528\u4e8e\u56fe\u50cf\u751f\u6210\u7684\u539f\u59cb\u53c2\u6570\u5316\u7c7b\u578b\uff08\u5b66\u4e60\u9884\u6d4b\u566a\u58f0\uff09\u5bf9\u5bc6\u96c6\u9884\u6d4b\u662f\u6709\u5bb3\u7684\uff1b\u591a\u6b65\u52a0\u566a/\u53bb\u566a\u6269\u6563\u8fc7\u7a0b\u4e5f\u662f\u4e0d\u5fc5\u8981\u7684\uff0c\u5e76\u4e14\u96be\u4ee5\u4f18\u5316\u3002\u57fa\u4e8e\u8fd9\u4e9b\u89c1\u89e3\uff0c\u6211\u4eec\u63a8\u51fa\u4e86Lotus\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u5b83\u91c7\u7528\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u5bc6\u96c6\u9884\u6d4b\u9002\u5e94\u534f\u8bae\u3002\u5177\u4f53\u6765\u8bf4\uff0cLotus\u88ab\u8bad\u7ec3\u6210\u76f4\u63a5\u9884\u6d4b\u6ce8\u91ca\u800c\u4e0d\u662f\u566a\u58f0\uff0c\u4ece\u800c\u907f\u514d\u4e86\u6709\u5bb3\u7684\u65b9\u5dee\u3002\u6211\u4eec\u8fd8\u5c06\u6269\u6563\u8fc7\u7a0b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5355\u6b65\u8fc7\u7a0b\uff0c\u7b80\u5316\u4e86\u4f18\u5316\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u79f0\u4e3a\u7ec6\u8282\u4fdd\u7559\u5668\u7684\u65b0\u578b\u8c03\u6574\u7b56\u7565\uff0c\u5b83\u53ef\u4ee5\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u66f4\u7ec6\u7c92\u5ea6\u7684\u9884\u6d4b\u3002\u5728\u4e0d\u6269\u5927\u8bad\u7ec3\u6570\u636e\u6216\u6a21\u578b\u5bb9\u91cf\u7684\u60c5\u51b5\u4e0b\uff0cLotus\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u7684\u96f6\u6837\u672c\u6df1\u5ea6\u548c\u6cd5\u7ebf\u4f30\u8ba1\u65b9\u9762\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5b83\u8fd8\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u6bd4\u5927\u591a\u6570\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u5feb\u6570\u767e\u500d\u3002||\n", "2409.18114": "|**2024-09-26**|[EdgeRunner: Auto-regressive Auto-encoder for Artistic Mesh Generation](http://arxiv.org/abs/2409.18114)|null|\u76ee\u524d\u7684\u81ea\u52a8\u56de\u5f52\u7f51\u683c\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u7740\u8bf8\u5982\u7f51\u683c\u4e0d\u5b8c\u6574\u3001\u7ec6\u8282\u4e0d\u8db3\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7b49\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u56de\u5f52\u81ea\u52a8\u7f16\u7801\u5668\uff08ArAE\uff09\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8fbe4,000\u4e2a\u9762\u7247\u3001\u7a7a\u95f4\u5206\u8fa8\u7387\u4e3a$512^3$\u7684\u9ad8\u8d28\u91cf\u4e09\u7ef4\u7f51\u683c\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7f51\u683c\u6807\u8bb0\u5316\u7b97\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5c06\u4e09\u89d2\u7f51\u683c\u538b\u7f29\u6210\u4e00\u7ef4\u6807\u8bb0\u5e8f\u5217\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5c06\u53d8\u957f\u4e09\u89d2\u7f51\u683c\u538b\u7f29\u6210\u56fa\u5b9a\u957f\u5ea6\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u4ece\u800c\u80fd\u591f\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u70b9\u4e91\u548c\u56fe\u50cf\u6761\u4ef6\u7f51\u683c\u751f\u6210\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002||\n", "2409.18098": "|**2024-09-26**|[StackGen: Generating Stable Structures from Silhouettes via Diffusion](http://arxiv.org/abs/2409.18098)|null|Humans naturally obtain intuition about the interactions between and the stability of rigid objects by observing and interacting with the world. It is this intuition that governs the way in which we regularly configure objects in our environment, allowing us to build complex structures from simple, everyday objects. Robotic agents, on the other hand, traditionally require an explicit model of the world that includes the detailed geometry of each object and an analytical model of the environment dynamics, which are difficult to scale and preclude generalization. Instead, robots would benefit from an awareness of intuitive physics that enables them to similarly reason over the stable interaction of objects in their environment. Towards that goal, we propose StackGen, a diffusion model that generates diverse stable configurations of building blocks matching a target silhouette. To demonstrate the capability of the method, we evaluate it in a simulated environment and deploy it in the real setting using a robotic arm to assemble structures generated by the model.||\n", "2409.18092": "|**2024-09-26**|[DiffSSC: Semantic LiDAR Scan Completion using Denoising Diffusion Probabilistic Models](http://arxiv.org/abs/2409.18092)|null|\u611f\u77e5\u7cfb\u7edf\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u5b83\u7ed3\u5408\u4e86\u591a\u4e2a\u4f20\u611f\u5668\u548c\u76f8\u5e94\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u7b97\u6cd5\u30023D \u6fc0\u5149\u96f7\u8fbe\u4f20\u611f\u5668\u88ab\u5e7f\u6cdb\u7528\u4e8e\u6355\u6349\u8f66\u8f86\u5468\u56f4\u73af\u5883\u7684\u7a00\u758f\u70b9\u4e91\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8fd9\u4e9b\u70b9\u4e91\u7684\u7a00\u758f\u6027\u548c\u7f3a\u4e4f\u8bed\u4e49\u4fe1\u606f\uff0c\u6b64\u7c7b\u7cfb\u7edf\u96be\u4ee5\u611f\u77e5\u906e\u6321\u533a\u57df\u548c\u573a\u666f\u4e2d\u7684\u95f4\u9699\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u8bed\u4e49\u573a\u666f\u8865\u5168 (SSC) \u5728\u7ed9\u5b9a\u539f\u59cb\u6fc0\u5149\u96f7\u8fbe\u6d4b\u91cf\u503c\u7684\u60c5\u51b5\u4e0b\uff0c\u8054\u5408\u9884\u6d4b\u573a\u666f\u4e2d\u672a\u89c2\u5bdf\u5230\u7684\u51e0\u4f55\u5f62\u72b6\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u65e8\u5728\u5b9e\u73b0\u66f4\u5b8c\u6574\u7684\u573a\u666f\u8868\u793a\u3002\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u7684\u826f\u597d\u7ed3\u679c\uff0c\u6211\u4eec\u5efa\u8bae\u5c06\u5176\u6269\u5c55\u5230 SSC\uff0c\u65b9\u6cd5\u662f\u5728\u70b9\u7a7a\u95f4\u548c\u8bed\u4e49\u7a7a\u95f4\u4e2d\u5206\u522b\u5b9e\u73b0\u53bb\u566a\u548c\u52a0\u566a\u6269\u6563\u8fc7\u7a0b\u3002\u4e3a\u4e86\u63a7\u5236\u751f\u6210\u8fc7\u7a0b\uff0c\u6211\u4eec\u91c7\u7528\u8bed\u4e49\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u6b63\u5219\u5316\u635f\u5931\u6765\u7a33\u5b9a\u53bb\u566a\u8fc7\u7a0b\u3002\u6211\u4eec\u5728\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728 SSC \u65b9\u9762\u7684\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002||\n", "2409.18083": "|**2024-09-26**|[Stable Video Portraits](http://arxiv.org/abs/2409.18083)|null|Rapid advances in the field of generative AI and text-to-image methods in particular have transformed the way we interact with and perceive computer-generated imagery today. In parallel, much progress has been made in 3D face reconstruction, using 3D Morphable Models (3DMM). In this paper, we present SVP, a novel hybrid 2D/3D generation method that outputs photorealistic videos of talking faces leveraging a large pre-trained text-to-image prior (2D), controlled via a 3DMM (3D). Specifically, we introduce a person-specific fine-tuning of a general 2D stable diffusion model which we lift to a video model by providing temporal 3DMM sequences as conditioning and by introducing a temporal denoising procedure. As an output, this model generates temporally smooth imagery of a person with 3DMM-based controls, i.e., a person-specific avatar. The facial appearance of this person-specific avatar can be edited and morphed to text-defined celebrities, without any fine-tuning at test time. The method is analyzed quantitatively and qualitatively, and we show that our method outperforms state-of-the-art monocular head avatar methods.||\n", "2409.17996": "|**2024-09-26**|[PhoCoLens: Photorealistic and Consistent Reconstruction in Lensless Imaging](http://arxiv.org/abs/2409.17996)|null|Lensless cameras offer significant advantages in size, weight, and cost compared to traditional lens-based systems. Without a focusing lens, lensless cameras rely on computational algorithms to recover the scenes from multiplexed measurements. However, current algorithms struggle with inaccurate forward imaging models and insufficient priors to reconstruct high-quality images. To overcome these limitations, we introduce a novel two-stage approach for consistent and photorealistic lensless image reconstruction. The first stage of our approach ensures data consistency by focusing on accurately reconstructing the low-frequency content with a spatially varying deconvolution method that adjusts to changes in the Point Spread Function (PSF) across the camera's field of view. The second stage enhances photorealism by incorporating a generative prior from pre-trained diffusion models. By conditioning on the low-frequency content retrieved in the first stage, the diffusion model effectively reconstructs the high-frequency details that are typically lost in the lensless imaging process, while also maintaining image fidelity. Our method achieves a superior balance between data fidelity and visual quality compared to existing methods, as demonstrated with two popular lensless systems, PhlatCam and DiffuserCam. Project website: https://phocolens.github.io/.||\n", "2409.17995": "|**2024-09-26**|[Joint Localization and Planning using Diffusion](http://arxiv.org/abs/2409.17995)|null|Diffusion models have been successfully applied to robotics problems such as manipulation and vehicle path planning. In this work, we explore their application to end-to-end navigation -- including both perception and planning -- by considering the problem of jointly performing global localization and path planning in known but arbitrary 2D environments. In particular, we introduce a diffusion model which produces collision-free paths in a global reference frame given an egocentric LIDAR scan, an arbitrary map, and a desired goal position. To this end, we implement diffusion in the space of paths in SE(2), and describe how to condition the denoising process on both obstacles and sensor observations. In our evaluation, we show that the proposed conditioning techniques enable generalization to realistic maps of considerably different appearance than the training environment, demonstrate our model's ability to accurately describe ambiguous solutions, and run extensive simulation experiments showcasing our model's use as a real-time, end-to-end localization and planning stack.||\n", "2409.17963": "|**2024-09-26**|[CNCA: Toward Customizable and Natural Generation of Adversarial Camouflage for Vehicle Detectors](http://arxiv.org/abs/2409.17963)|null|Prior works on physical adversarial camouflage against vehicle detectors mainly focus on the effectiveness and robustness of the attack. The current most successful methods optimize 3D vehicle texture at a pixel level. However, this results in conspicuous and attention-grabbing patterns in the generated camouflage, which humans can easily identify. To address this issue, we propose a Customizable and Natural Camouflage Attack (CNCA) method by leveraging an off-the-shelf pre-trained diffusion model. By sampling the optimal texture image from the diffusion model with a user-specific text prompt, our method can generate natural and customizable adversarial camouflage while maintaining high attack performance. With extensive experiments on the digital and physical worlds and user studies, the results demonstrate that our proposed method can generate significantly more natural-looking camouflage than the state-of-the-art baselines while achieving competitive attack performance. Our code is available at \\href{https://anonymous.4open.science/r/CNCA-1D54}{https://anonymous.4open.science/r/CNCA-1D54}||\n", "2409.17960": "|**2024-09-26**|[Relativistic diffusion model for hadron production in p-Pb collisions at the LHC](http://arxiv.org/abs/2409.17960)|null|We investigate charged-hadron production in relativistic heavy-ion collisions of asymmetric systems within a nonequilibrium-statistical framework. Calculated centrality-dependent pseudorapidity distributions for p-Pb collisions at sqrt(s_NN)=5.02 and 8.16 TeV are compared with data from the Large Hadron Collider (LHC). Our approach combines a relativistic diffusion model with formulations based on quantum chromodynamics while utilizing numerical solutions of a Fokker-Planck equation to account for the shift and broadening of the fragmentation sources for particle-production with respect to the stopping (net-baryon) rapidity distributions. To represent the centrality dependence of charged-hadron production in asymmetric systems over a broad region of pseudorapidities, the consideration and precise modelling of the fragmentation sources - along with the central gluon-gluon source - is found to be essential. Specifically, this results in an inversion of the particle-production amplitude from backward- to forward-dominance when transitioning from central to peripheral collisions, in agreement with recent ATLAS and ALICE p-Pb data at sqrt(s_NN)=5.02 TeV.||\n", "2409.18959": "|**2024-09-27**|[$O(d/T)$ Convergence Theory for Diffusion Probabilistic Models under Minimal Assumptions](http://arxiv.org/abs/2409.18959)|null|\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u901a\u8fc7\u5b66\u4e60\u9006\u8f6c\u5c06\u76ee\u6807\u5206\u5e03\u6570\u636e\u6270\u52a8\u4e3a\u566a\u58f0\u7684\u6269\u6563\u8fc7\u7a0b\u6765\u751f\u6210\u65b0\u6570\u636e\uff0c\u5df2\u7ecf\u5728\u5404\u79cd\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\u3002\u5c3d\u7ba1\u5b83\u4eec\u5177\u6709\u4f18\u8d8a\u7684\u7ecf\u9a8c\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u7684\u7406\u8bba\u4fdd\u8bc1\u901a\u5e38\u53d7\u5230\u4e25\u683c\u5047\u8bbe\u6216\u6b21\u4f18\u6536\u655b\u901f\u5ea6\u7684\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ee5\u6700\u5c0f\u7684\u5047\u8bbe\u5efa\u7acb\u4e86\u6d41\u884c\u7684\u57fa\u4e8e SDE \u7684\u91c7\u6837\u5668\u7684\u5feb\u901f\u6536\u655b\u7406\u8bba\u3002\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0c\u5982\u679c\u63d0\u4f9b\u5206\u6570\u51fd\u6570\u7684 $\\ell_{2}$ \u7cbe\u5ea6\u4f30\u8ba1\uff0c\u5219\u76ee\u6807\u5206\u5e03\u548c\u751f\u6210\u5206\u5e03\u4e4b\u95f4\u7684\u603b\u53d8\u5dee\u8ddd\u79bb\u7684\u4e0a\u9650\u4e3a $O(d/T)$\uff08\u5ffd\u7565\u5bf9\u6570\u56e0\u5b50\uff09\uff0c\u5176\u4e2d $d$ \u662f\u6570\u636e\u7ef4\u5ea6\uff0c$T$ \u662f\u6b65\u6570\u3002\u8be5\u7ed3\u679c\u9002\u7528\u4e8e\u4efb\u4f55\u5177\u6709\u4e00\u9636\u77e9\u6709\u9650\u7684\u76ee\u6807\u5206\u5e03\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u6539\u8fdb\u4e86\u57fa\u4e8e SDE \u7684\u91c7\u6837\u5668\u548c\u53e6\u4e00\u79cd\u57fa\u4e8e ODE \u7684\u91c7\u6837\u5668\u7684\u73b0\u6709\u6536\u655b\u7406\u8bba\uff0c\u540c\u65f6\u5bf9\u76ee\u6807\u6570\u636e\u5206\u5e03\u548c\u5206\u6570\u4f30\u8ba1\u65bd\u52a0\u4e86\u6700\u5c0f\u5047\u8bbe\u3002\u8fd9\u662f\u901a\u8fc7\u4e00\u7ec4\u65b0\u9896\u7684\u5206\u6790\u5de5\u5177\u5b9e\u73b0\u7684\uff0c\u8be5\u5de5\u5177\u63d0\u4f9b\u4e86\u5bf9\u8bef\u5dee\u5728\u53cd\u5411\u8fc7\u7a0b\u7684\u6bcf\u4e2a\u6b65\u9aa4\u4e2d\u5982\u4f55\u4f20\u64ad\u7684\u7ec6\u7c92\u5ea6\u8868\u5f81\u3002||\n", "2409.18932": "|**2024-09-27**|[ReviveDiff: A Universal Diffusion Model for Restoring Images in Adverse Weather Conditions](http://arxiv.org/abs/2409.18932)|null|\u5728\u8bf8\u5982\u591c\u95f4\u3001\u96fe\u5929\u3001\u96e8\u5929\u548c\u6c34\u4e0b\u7b49\u6311\u6218\u6027\u73af\u5883\u4e2d\u62cd\u6444\u7684\u56fe\u50cf\u7ecf\u5e38\u4f1a\u906d\u53d7\u4e25\u91cd\u7684\u8d28\u91cf\u4e0b\u964d\uff0c\u5bfc\u81f4\u89c6\u89c9\u8d28\u91cf\u5927\u5e45\u964d\u4f4e\u3002\u6709\u6548\u5730\u6062\u590d\u8fd9\u4e9b\u9000\u5316\u7684\u56fe\u50cf\u5bf9\u4e8e\u540e\u7eed\u7684\u89c6\u89c9\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u8bb8\u591a\u73b0\u6709\u65b9\u6cd5\u5df2\u7ecf\u6210\u529f\u5730\u7ed3\u5408\u4e86\u9488\u5bf9\u4e2a\u4efb\u52a1\u7684\u7279\u5b9a\u5148\u9a8c\u77e5\u8bc6\uff0c\u4f46\u8fd9\u4e9b\u5b9a\u5236\u89e3\u51b3\u65b9\u6848\u9650\u5236\u4e86\u5b83\u4eec\u5bf9\u5176\u4ed6\u9000\u5316\u7684\u9002\u7528\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u79f0\u4e3a\u201cReviveDiff\u201d\uff0c\u5b83\u53ef\u4ee5\u89e3\u51b3\u5404\u79cd\u9000\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u589e\u5f3a\u548c\u6062\u590d\u56fe\u50cf\u8d28\u91cf\u4f7f\u5176\u6062\u590d\u751f\u673a\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u53d7\u5230\u4ee5\u4e0b\u89c2\u5bdf\u7ed3\u679c\u7684\u542f\u53d1\uff1a\u4e0e\u8fd0\u52a8\u6216\u7535\u5b50\u95ee\u9898\u9020\u6210\u7684\u9000\u5316\u4e0d\u540c\uff0c\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u8d28\u91cf\u9000\u5316\u4e3b\u8981\u6e90\u4e8e\u81ea\u7136\u4ecb\u8d28\uff08\u5982\u96fe\u3001\u6c34\u548c\u4f4e\u4eae\u5ea6\uff09\uff0c\u8fd9\u4e9b\u4ecb\u8d28\u901a\u5e38\u4fdd\u7559\u4e86\u7269\u4f53\u7684\u539f\u59cb\u7ed3\u6784\u3002\u4e3a\u4e86\u6062\u590d\u6b64\u7c7b\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u6211\u4eec\u5229\u7528\u4e86\u6269\u6563\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u5f00\u53d1\u4e86ReviveDiff\uff0c\u4ece\u5b8f\u89c2\u548c\u5fae\u89c2\u5c42\u9762\u6062\u590d\u56fe\u50cf\u8d28\u91cf\uff0c\u6db5\u76d6\u51b3\u5b9a\u56fe\u50cf\u8d28\u91cf\u7684\u4e00\u4e9b\u5173\u952e\u56e0\u7d20\uff0c\u5982\u6e05\u6670\u5ea6\u3001\u5931\u771f\u3001\u566a\u58f0\u6c34\u5e73\u3001\u52a8\u6001\u8303\u56f4\u548c\u8272\u5f69\u51c6\u786e\u5ea6\u3002\u6211\u4eec\u5728\u6db5\u76d6\u4e94\u79cd\u9000\u5316\u6761\u4ef6\uff08\u96e8\u5929\u3001\u6c34\u4e0b\u3001\u4f4e\u5149\u3001\u70df\u96fe\u548c\u591c\u95f4\u96fe\u973e\uff09\u7684\u4e03\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5bf9ReviveDiff\u8fdb\u884c\u4e86\u4e25\u683c\u8bc4\u4f30\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cReviveDiff\u5728\u5b9a\u91cf\u548c\u89c6\u89c9\u4e0a\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002||\n", "2409.18899": "|**2024-09-27**|[Unsupervised Low-light Image Enhancement with Lookup Tables and Diffusion Priors](http://arxiv.org/abs/2409.18899)|null|\u5f31\u5149\u56fe\u50cf\u589e\u5f3a (LIE) \u65e8\u5728\u7cbe\u786e\u6709\u6548\u5730\u6062\u590d\u5728\u5f31\u5149\u73af\u5883\u4e0b\u964d\u8d28\u7684\u56fe\u50cf\u3002\u6700\u8fd1\u5148\u8fdb\u7684 LIE \u6280\u672f\u6b63\u5728\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u8fd9\u9700\u8981\u5927\u91cf\u7684\u5f31\u5149-\u6b63\u5e38\u5149\u56fe\u50cf\u5bf9\u3001\u7f51\u7edc\u53c2\u6570\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u56e0\u6b64\uff0c\u5b83\u4eec\u7684\u5b9e\u7528\u6027\u53d7\u5230\u9650\u5236\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u5148\u9a8c\u548c\u67e5\u627e\u8868 (DPLUT) \u7684\u65b0\u578b\u65e0\u76d1\u7763 LIE \u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u5f31\u5149\u56fe\u50cf\u6062\u590d\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u5149\u7167\u8c03\u6574\u67e5\u627e\u8868 (LLUT) \u548c\u566a\u58f0\u6291\u5236\u67e5\u627e\u8868 (NLUT)\u3002LLUT \u4f7f\u7528\u4e00\u7ec4\u65e0\u76d1\u7763\u635f\u5931\u8fdb\u884c\u4f18\u5316\u3002\u5b83\u65e8\u5728\u9884\u6d4b\u7279\u5b9a\u56fe\u50cf\u52a8\u6001\u8303\u56f4\u8c03\u6574\u7684\u9010\u50cf\u7d20\u66f2\u7ebf\u53c2\u6570\u3002NLUT \u65e8\u5728\u53bb\u9664\u5149\u7ebf\u53d8\u4eae\u540e\u653e\u5927\u7684\u566a\u58f0\u3002\u7531\u4e8e\u6269\u6563\u6a21\u578b\u5bf9\u566a\u58f0\u5f88\u654f\u611f\uff0c\u56e0\u6b64\u5f15\u5165\u4e86\u6269\u6563\u5148\u9a8c\u4ee5\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u566a\u58f0\u6291\u5236\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002||\n", "2409.18897": "|**2024-09-27**|[Detecting Dataset Abuse in Fine-Tuning Stable Diffusion Models for Text-to-Image Synthesis](http://arxiv.org/abs/2409.18897)|null|\u6587\u56fe\u751f\u6210\u5728\u751f\u6210\u903c\u771f\u548c\u98ce\u683c\u5316\u7684\u56fe\u50cf\u65b9\u9762\u5df2\u7ecf\u53d8\u5f97\u975e\u5e38\u6d41\u884c\uff0c\u8fd9\u901a\u5e38\u9700\u8981\u4f7f\u7528\u7279\u5b9a\u9886\u57df\u7684\u6570\u636e\u5e93\u5bf9\u751f\u6210\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u4ee5\u5b8c\u6210\u4e13\u95e8\u7684\u4efb\u52a1\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6709\u4ef7\u503c\u7684\u6570\u636e\u5e93\u9762\u4e34\u7740\u672a\u7ecf\u6388\u6743\u4f7f\u7528\u548c\u672a\u7ecf\u6279\u51c6\u5171\u4eab\u7684\u98ce\u9669\uff0c\u635f\u5bb3\u4e86\u6240\u6709\u8005\u7684\u6743\u5229\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u5728\u5bf9 Stable Diffusion \u6a21\u578b\u8fdb\u884c\u6587\u56fe\u751f\u6210\u7684\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u6570\u636e\u5e93\u6ee5\u7528\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u636e\u5e93\u6c34\u5370\u6846\u67b6\uff0c\u65e8\u5728\u68c0\u6d4b\u672a\u7ecf\u6388\u6743\u7684\u4f7f\u7528\u5e76\u8ffd\u8e2a\u6570\u636e\u6cc4\u9732\u3002\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u6c34\u5370\u65b9\u6848\u4e2d\u91c7\u7528\u4e86\u4e24\u79cd\u5173\u952e\u7b56\u7565\uff0c\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u5e93\u6388\u6743\u6709\u6548\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u6709\u6548\uff0c\u5bf9\u6570\u636e\u5e93\u7684\u5f71\u54cd\u6700\u5c0f\uff08\u53ea\u9700\u4fee\u6539 2% \u7684\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff09\uff0c\u5e76\u4e14\u80fd\u591f\u8ffd\u8e2a\u6570\u636e\u6cc4\u9732\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8fd8\u7a81\u51fa\u4e86\u8be5\u6846\u67b6\u7684\u9c81\u68d2\u6027\u548c\u53ef\u8fc1\u79fb\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u68c0\u6d4b\u6570\u636e\u5e93\u6ee5\u7528\u65b9\u9762\u7684\u5b9e\u9645\u9002\u7528\u6027\u3002||\n", "2409.18881": "|**2024-09-27**|[Explainable Artifacts for Synthetic Western Blot Source Attribution](http://arxiv.org/abs/2409.18881)|**[link](https://github.com/phillipecardenuto/ai-wblots-detector)**|\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u4f7f\u5f97\u751f\u6210\u6a21\u578b\u80fd\u591f\u751f\u6210\u4e0e\u771f\u5b9e\u56fe\u50cf\u96be\u4ee5\u533a\u5206\u7684\u5408\u6210\u79d1\u5b66\u56fe\u50cf\uff0c\u8fd9\u5bf9\u4e60\u60ef\u4e8e\u5904\u7406\u6b64\u7c7b\u5185\u5bb9\u7684\u4e13\u4e1a\u79d1\u5b66\u5bb6\u4e5f\u6784\u6210\u4e86\u6311\u6218\u3002\u5f53\u88ab\u79f0\u4e3a\u201c\u8bba\u6587\u5de5\u5382\u201d\u7684\u7ec4\u7ec7\u5229\u7528\u8fd9\u4e9b\u6280\u672f\u7cfb\u7edf\u5730\u751f\u6210\u865a\u5047\u6587\u7ae0\u65f6\uff0c\u5b83\u4eec\u53ef\u80fd\u4f1a\u52a9\u957f\u5173\u4e8e\u65e0\u6839\u636e\u79d1\u5b66\u7684\u9519\u8bef\u4fe1\u606f\u7684\u4f20\u64ad\uff0c\u4ece\u800c\u6709\u53ef\u80fd\u7834\u574f\u5bf9\u79d1\u5b66\u7814\u7a76\u7684\u4fe1\u4efb\u3002\u867d\u7136\u4e4b\u524d\u7684\u7814\u7a76\u5df2\u7ecf\u63a2\u7d22\u4e86\u9ed1\u76d2\u89e3\u51b3\u65b9\u6848\uff08\u4f8b\u5982\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff09\u6765\u8bc6\u522b\u5408\u6210\u5185\u5bb9\uff0c\u4f46\u53ea\u6709\u4e00\u90e8\u5206\u7814\u7a76\u89e3\u51b3\u4e86\u8de8\u4e0d\u540c\u6a21\u578b\u8fdb\u884c\u6cdb\u5316\u5e76\u6df1\u5165\u4e86\u89e3\u5408\u6210\u56fe\u50cf\u4e2d\u53ef\u7528\u4e8e\u68c0\u6d4b\u8fc7\u7a0b\u7684\u4eba\u5de5\u75d5\u8ff9\u7684\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc6\u522b\u7531\u6700\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\uff08\u4f8b\u5982\uff0c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u548c\u6269\u6563\u6a21\u578b\uff09\u4ea7\u751f\u7684\u53ef\u89e3\u91ca\u7684\u4eba\u5de5\u75d5\u8ff9\uff0c\u5e76\u5229\u7528\u5b83\u4eec\u8fdb\u884c\u5f00\u653e\u96c6\u8bc6\u522b\u548c\u6765\u6e90\u5f52\u56e0\uff08\u5373\uff0c\u6307\u51fa\u521b\u5efa\u56fe\u50cf\u7684\u6a21\u578b\uff09\u3002||\n", "2409.18869": "|**2024-09-27**|[Emu3: Next-Token Prediction is All You Need](http://arxiv.org/abs/2409.18869)|null|\u867d\u7136\u4e0b\u4e00\u8bcd\u9884\u6d4b\u88ab\u8ba4\u4e3a\u662f\u901a\u5411\u4eba\u5de5\u901a\u7528\u667a\u80fd\u7684\u6709\u5e0c\u671b\u7684\u9014\u5f84\uff0c\u4f46\u5b83\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u4e00\u76f4\u96be\u4ee5\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\uff0c\u800c\u591a\u6a21\u6001\u4efb\u52a1\u4ecd\u7136\u7531\u6269\u6563\u6a21\u578b\uff08\u4f8b\u5982\uff0cStable Diffusion\uff09\u548c\u7ec4\u5408\u65b9\u6cd5\uff08\u4f8b\u5982\uff0cCLIP \u4e0e LLM \u76f8\u7ed3\u5408\uff09\u4e3b\u5bfc\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 Emu3\uff0c\u8fd9\u662f\u4e00\u5957\u5168\u65b0\u7684\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528\u4e0b\u4e00\u8bcd\u9884\u6d4b\u8fdb\u884c\u8bad\u7ec3\u3002\u901a\u8fc7\u5c06\u56fe\u50cf\u3001\u6587\u672c\u548c\u89c6\u9891\u6807\u8bb0\u5316\u4e3a\u79bb\u6563\u7a7a\u95f4\uff0c\u6211\u4eec\u5728\u591a\u6a21\u6001\u5e8f\u5217\u7684\u6df7\u5408\u4e0a\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u5355\u4e2a\u53d8\u6362\u5668\u3002Emu3 \u5728\u751f\u6210\u548c\u611f\u77e5\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u591a\u4e2a\u5b8c\u5584\u7684\u7279\u5b9a\u4efb\u52a1\u6a21\u578b\uff0c\u8d85\u8d8a\u4e86 SDXL \u548c LLaVA-1.6 \u7b49\u65d7\u8230\u6a21\u578b\uff0c\u540c\u65f6\u65e0\u9700\u6269\u6563\u6216\u7ec4\u5408\u67b6\u6784\u3002Emu3 \u8fd8\u80fd\u591f\u901a\u8fc7\u9884\u6d4b\u89c6\u9891\u5e8f\u5217\u4e2d\u7684\u4e0b\u4e00\u4e2a\u6807\u8bb0\u6765\u751f\u6210\u9ad8\u4fdd\u771f\u89c6\u9891\u3002\u6211\u4eec\u901a\u8fc7\u4e13\u6ce8\u4e8e\u5355\u4e00\u7126\u70b9\uff1a\u6807\u8bb0\uff0c\u7b80\u5316\u4e86\u590d\u6742\u7684\u591a\u6a21\u6001\u6a21\u578b\u8bbe\u8ba1\uff0c\u4ece\u800c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u91ca\u653e\u4e86\u5de8\u5927\u7684\u6269\u5c55\u6f5c\u529b\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u4e0b\u4e00\u8bcd\u9884\u6d4b\u662f\u6784\u5efa\u8d85\u8d8a\u8bed\u8a00\u7684\u901a\u7528\u591a\u6a21\u6001\u667a\u80fd\u7684\u6709\u5e0c\u671b\u7684\u9014\u5f84\u3002\u6211\u4eec\u5f00\u6e90\u4e86\u5173\u952e\u6280\u672f\u548c\u6a21\u578b\uff0c\u4ee5\u652f\u6301\u5728\u8be5\u65b9\u5411\u4e0a\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002||\n", "2409.18859": "|**2024-09-27**|[Challenges of Generating Structurally Diverse Graphs](http://arxiv.org/abs/2409.18859)|**[link](https://github.com/Abusagit/Challenges-on-generating-structurally-diverse-graphs)**|\u5bf9\u4e8e\u8bb8\u591a\u4e0e\u56fe\u76f8\u5173\u7684\u95ee\u9898\uff0c\u62e5\u6709\u4e00\u7ec4\u7ed3\u6784\u591a\u6837\u5316\u7684\u56fe\u81f3\u5173\u91cd\u8981\u3002\u4f8b\u5982\uff0c\u6b64\u7c7b\u56fe\u53ef\u7528\u4e8e\u6d4b\u8bd5\u56fe\u7b97\u6cd5\u6216\u5176\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u3002\u7136\u800c\uff0c\u636e\u6211\u4eec\u6240\u77e5\uff0c\u751f\u6210\u7ed3\u6784\u591a\u6837\u5316\u56fe\u7684\u95ee\u9898\u5c1a\u672a\u5728\u6587\u732e\u4e2d\u5f97\u5230\u63a2\u8ba8\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002\u9996\u5148\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u5982\u4f55\u5b9a\u4e49\u4e00\u7ec4\u56fe\u7684\u591a\u6837\u6027\uff0c\u4e3a\u4ec0\u4e48\u8fd9\u9879\u4efb\u52a1\u4e0d\u7b80\u5355\uff0c\u4ee5\u53ca\u5982\u4f55\u9009\u62e9\u5408\u9002\u7684\u5ea6\u91cf\u6807\u51c6\u3002\u7136\u540e\uff0c\u5bf9\u4e8e\u7ed9\u5b9a\u7684\u591a\u6837\u6027\u5ea6\u91cf\u6807\u51c6\uff0c\u6211\u4eec\u63d0\u51fa\u5e76\u6bd4\u8f83\u4e86\u51e0\u79cd\u4f18\u5316\u5b83\u7684\u7b97\u6cd5\uff1a\u6211\u4eec\u8003\u8651\u4e86\u57fa\u4e8e\u6807\u51c6\u968f\u673a\u56fe\u6a21\u578b\u3001\u5c40\u90e8\u56fe\u4f18\u5316\u3001\u9057\u4f20\u7b97\u6cd5\u548c\u795e\u7ecf\u751f\u6210\u6a21\u578b\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u76f8\u8f83\u4e8e\u57fa\u672c\u7684\u968f\u673a\u56fe\u751f\u6210\u5668\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u591a\u6837\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5bf9\u751f\u6210\u56fe\u7684\u5206\u6790\u4f7f\u6211\u4eec\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u56fe\u8ddd\u79bb\u7684\u7279\u6027\uff1a\u6839\u636e\u7528\u4e8e\u4f18\u5316\u7684\u591a\u6837\u6027\u5ea6\u91cf\u6807\u51c6\uff0c\u83b7\u5f97\u7684\u56fe\u53ef\u80fd\u5177\u6709\u975e\u5e38\u4e0d\u540c\u7684\u7ed3\u6784\u7279\u6027\uff0c\u8fd9\u4e3a\u4e86\u89e3\u591a\u6837\u6027\u5ea6\u91cf\u6807\u51c6\u4e2d\u4f7f\u7528\u7684\u56fe\u8ddd\u79bb\u7684\u654f\u611f\u6027\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002||\n", "2409.18804": "|**2024-09-27**|[Convergence of Diffusion Models Under the Manifold Hypothesis in High-Dimensions](http://arxiv.org/abs/2409.18804)|null|\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b (DDPM) \u662f\u4e00\u79cd\u5f3a\u5927\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u9ad8\u7ef4\u6570\u636e\u5206\u5e03\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5e76\u5e7f\u6cdb\u7528\u4e8e\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u751f\u6210\u4ee5\u53ca\u79d1\u5b66\u53ca\u5176\u4ed6\u9886\u57df\u7684\u66f4\u591a\u5e94\u7528\u3002\u6d41\u5f62\u5047\u8bbe\u6307\u51fa\u9ad8\u7ef4\u6570\u636e\u901a\u5e38\u4f4d\u4e8e\u73af\u5883\u7a7a\u95f4\u5185\u7684\u4f4e\u7ef4\u6d41\u5f62\u4e0a\uff0c\u5e76\u4e14\u88ab\u5e7f\u6cdb\u8ba4\u4e3a\u5728\u63d0\u4f9b\u7684\u793a\u4f8b\u4e2d\u6210\u7acb\u3002\u867d\u7136\u6700\u8fd1\u7684\u7ed3\u679c\u4e3a\u4e86\u89e3\u6269\u6563\u6a21\u578b\u5982\u4f55\u9002\u5e94\u6d41\u5f62\u5047\u8bbe\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\uff0c\u4f46\u5b83\u4eec\u6ca1\u6709\u6355\u6349\u5230\u8fd9\u4e9b\u6a21\u578b\u7684\u5de8\u5927\u7ecf\u9a8c\u6210\u529f\uff0c\u8fd9\u4f7f\u5176\u6210\u4e3a\u4e00\u4e2a\u975e\u5e38\u5bcc\u6709\u6210\u679c\u7684\u7814\u7a76\u65b9\u5411\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u6d41\u5f62\u5047\u8bbe\u4e0b\u7684 DDPM\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u4eec\u5728\u5b66\u4e60\u5206\u6570\u65b9\u9762\u5b9e\u73b0\u4e86\u4e0e\u73af\u5883\u7ef4\u5ea6\u65e0\u5173\u7684\u901f\u7387\u3002\u5728\u91c7\u6837\u65b9\u9762\uff0c\u6211\u4eec\u83b7\u5f97\u4e86\u5173\u4e8e Kullback-Leibler \u6563\u5ea6\u7684\u4e0e\u73af\u5883\u7ef4\u5ea6\u65e0\u5173\u7684\u901f\u7387\uff0c\u4ee5\u53ca\u5173\u4e8e Wasserstein \u8ddd\u79bb\u7684  $O(\\sqrt{D})$\u3002\u6211\u4eec\u901a\u8fc7\u5f00\u53d1\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\u6765\u505a\u5230\u8fd9\u4e00\u70b9\uff0c\u8be5\u6846\u67b6\u5c06\u6269\u6563\u6a21\u578b\u8fde\u63a5\u5230\u7ecf\u8fc7\u5145\u5206\u7814\u7a76\u7684\u9ad8\u65af\u8fc7\u7a0b\u6781\u503c\u7406\u8bba\u3002||\n", "2409.18761": "|**2024-09-27**|[Geometric deep learning for galaxy-halo connection: a case study for galaxy intrinsic alignments](http://arxiv.org/abs/2409.18761)|null|\u5373\u5c06\u8fdb\u884c\u7684\u5b87\u5b99\u5b66\u6210\u50cf\u5de1\u5929\uff0c\u4f8b\u5982 Rubin Observatory LSST\uff0c\u9700\u8981\u5305\u542b\u771f\u5b9e\u661f\u7cfb\u7fa4\u7684\u5927\u89c4\u6a21\u6a21\u62df\uff0c\u4ee5\u7528\u4e8e\u5404\u79cd\u79d1\u5b66\u5e94\u7528\u3002\u5176\u4e2d\u4e00\u4e2a\u7279\u522b\u503c\u5f97\u5173\u6ce8\u7684\u73b0\u8c61\u662f\u5185\u7980\u6392\u5217 (IA)\uff0c\u5373\u661f\u7cfb\u503e\u5411\u4e8e\u671d\u5411\u8d85\u5bc6\u5ea6\u533a\u57df\u6392\u5217\uff0c\u5982\u679c\u4e0d\u5bf9\u5176\u8fdb\u884c\u9002\u5f53\u5efa\u6a21\uff0c\u53ef\u80fd\u4f1a\u5728\u5f31\u5f15\u529b\u900f\u955c\u5206\u6790\u4e2d\u5f15\u5165\u663e\u8457\u7684\u7cfb\u7edf\u504f\u5dee\u3002\u7531\u4e8e\u8ba1\u7b97\u9650\u5236\uff0c\u5728\u5e7f\u9614\u7684\u4f53\u79ef\u8303\u56f4\u5185\u6a21\u62df\u4e0e IA \u76f8\u5173\u7684\u661f\u7cfb\u5f62\u6210\u548c\u6f14\u5316\u7684\u590d\u6742\u7ec6\u8282\u662f\u4e0d\u5207\u5b9e\u9645\u7684\u3002\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5728 IllustrisTNG-100 \u6a21\u62df\u4e0a\u8bad\u7ec3\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u5bf9 3D \u661f\u7cfb\u5f62\u72b6\u548c\u65b9\u5411\u8fdb\u884c\u91c7\u6837\uff0c\u4ee5\u51c6\u786e\u5730\u518d\u73b0\u5185\u7980\u6392\u5217\u4ee5\u53ca\u76f8\u5173\u7684\u6807\u91cf\u7279\u5f81\u3002\u6211\u4eec\u5c06\u5b87\u5b99\u7f51\u5efa\u6a21\u4e3a\u4e00\u7ec4\u56fe\uff0c\u6bcf\u4e2a\u56fe\u4ee3\u8868\u4e00\u4e2a\u6655\uff0c\u8282\u70b9\u4ee3\u8868\u5b50\u6655/\u661f\u7cfb\u3002\u8be5\u67b6\u6784\u7531\u4e00\u4e2a SO(3) $\\times$ $\\mathbb{R}^n$ \u6269\u6563\u751f\u6210\u6a21\u578b\u7ec4\u6210\uff0c\u7528\u4e8e\u661f\u7cfb\u65b9\u5411\u548c $n$ \u4e2a\u6807\u91cf\uff0c\u5e76\u4f7f\u7528\u660e\u786e\u9075\u5b88\u5b87\u5b99\u6b27\u51e0\u91cc\u5fb7\u5bf9\u79f0\u6027\u7684 E(3) \u7b49\u53d8\u56fe\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u3002\u8be5\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u548c\u9884\u6d4b\u4e0e\u53c2\u8003\u6a21\u62df\u5728\u7edf\u8ba1\u4e0a\u4e00\u81f4\u7684\u7279\u5f81\uff0c\u4f8b\u5982\u661f\u7cfb\u65b9\u5411\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5c55\u793a\u4e86\u8054\u5408\u5efa\u6a21\u6b27\u51e0\u91cc\u5fb7\u503c\u6807\u91cf\uff08\u661f\u7cfb\u5927\u5c0f\u3001\u5f62\u72b6\u548c\u989c\u8272\uff09\u4ee5\u53ca\u975e\u6b27\u51e0\u91cc\u5fb7\u503c SO(3) \u91cf\uff08\u661f\u7cfb\u65b9\u5411\uff09\u7684\u80fd\u529b\uff0c\u8fd9\u4e9b\u91cf\u53d7\u975e\u7ebf\u6027\u5c3a\u5ea6\u4e0a\u9ad8\u5ea6\u590d\u6742\u7684\u661f\u7cfb\u7269\u7406\u652f\u914d\u3002||\n", "2409.18636": "|**2024-09-27**|[Unsupervised Fingerphoto Presentation Attack Detection With Diffusion Models](http://arxiv.org/abs/2409.18636)|null|\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u7684\u975e\u63a5\u89e6\u5f0f\u6307\u7eb9\u8ba4\u8bc1\u7531\u4e8e\u667a\u80fd\u624b\u673a\u76f8\u673a\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5df2\u6210\u4e3a\u4f20\u7edf\u63a5\u89e6\u5f0f\u6307\u7eb9\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u7684\u53ef\u9760\u66ff\u4ee3\u65b9\u6848\u3002\u5c3d\u7ba1\u5176\u4fbf\u5229\u6027\u5f88\u9ad8\uff0c\u4f46\u901a\u8fc7\u6307\u7eb9\u7167\u7247\u8fdb\u884c\u7684\u6307\u7eb9\u8ba4\u8bc1\u66f4\u5bb9\u6613\u53d7\u5230\u4f2a\u9020\u653b\u51fb\uff0c\u8fd9\u4fc3\u4f7f\u6700\u8fd1\u7684\u7814\u7a76\u5de5\u4f5c\u81f4\u529b\u4e8e\u5f00\u53d1\u6307\u7eb9\u7167\u7247\u5448\u73b0\u653b\u51fb\u68c0\u6d4b (PAD) \u6280\u672f\u3002\u7136\u800c\uff0c\u5148\u524d\u7684 PAD \u65b9\u6cd5\u5229\u7528\u4e86\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u771f\u5b9e\u548c\u653b\u51fb\u6837\u672c\u7684\u6807\u8bb0\u8bad\u7ec3\u6570\u636e\u3002\u8fd9\u53ef\u80fd\u4f1a\u9047\u5230\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u5373 (i)  \u6cdb\u5316\u6027\uff1a\u68c0\u6d4b\u8bad\u7ec3\u6570\u636e\u4e2d\u672a\u89c1\u8fc7\u7684\u5448\u73b0\u653b\u51fb\u5de5\u5177 (PAI)\uff0c\u4ee5\u53ca (ii) \u53ef\u6269\u5c55\u6027\uff1a\u4f7f\u7528\u4e0d\u540c\u7684 PAI \u6536\u96c6\u5927\u578b\u653b\u51fb\u6837\u672c\u6570\u636e\u96c6\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6269\u6563\u6a21\u578b\u7684\u65b0\u578b\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u5373\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b (DDPM)\uff0c\u8be5\u6a21\u578b\u4ec5\u4f7f\u7528\u771f\u5b9e\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u8ba1\u7b97 DDPM \u7684\u8f93\u5165\u548c\u8f93\u51fa\u5bf9\u4e4b\u95f4\u7684\u91cd\u5efa\u76f8\u4f3c\u6027\u6765\u68c0\u6d4b\u5448\u73b0\u653b\u51fb (PA)\u3002\u6211\u4eec\u5c55\u793a\u4e86\u8de8\u4e09\u4e2a PAI \u6570\u636e\u96c6\u7684\u5927\u91cf\u5b9e\u9a8c\uff0c\u4ee5\u6d4b\u8bd5\u6211\u4eec\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u57fa\u7ebf\u65e0\u76d1\u7763\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u57fa\u4e8e DDPM \u7684 PAD \u65b9\u6cd5\u5728\u591a\u4e2a PAI \u7c7b\u522b\u4e0a\u5b9e\u73b0\u4e86\u663e\u7740\u66f4\u597d\u7684\u68c0\u6d4b\u9519\u8bef\u7387\u3002||\n", "2409.20562": "|**2024-09-30**|[SpaceMesh: A Continuous Representation for Learning Manifold Surface Meshes](http://arxiv.org/abs/2409.20562)|null|\u7f51\u683c\u5728\u89c6\u89c9\u8ba1\u7b97\u548c\u6a21\u62df\u4e2d\u65e0\u5904\u4e0d\u5728\uff0c\u4f46\u5927\u591a\u6570\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\u53ea\u80fd\u95f4\u63a5\u5730\u8868\u793a\u7f51\u683c\uff0c\u4f8b\u5982\uff0c\u5c06\u5176\u8868\u793a\u4e3a\u6807\u91cf\u573a\u7684\u6c34\u5e73\u96c6\u6216\u6a21\u677f\u7684\u53d8\u5f62\uff0c\u6216\u8005\u8868\u793a\u4e3a\u7f3a\u4e4f\u5c40\u90e8\u7ed3\u6784\u7684\u65e0\u5e8f\u4e09\u89d2\u5f62\u96c6\u5408\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6848\uff0c\u53ef\u4ee5\u76f4\u63a5\u751f\u6210\u5177\u6709\u590d\u6742\u8fde\u63a5\u6027\u7684\u6d41\u5f62\u591a\u8fb9\u5f62\u7f51\u683c\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u51fa\u3002\u6211\u4eec\u7684\u5173\u952e\u521b\u65b0\u662f\u5728\u6bcf\u4e2a\u7f51\u683c\u9876\u70b9\u5b9a\u4e49\u4e00\u4e2a\u8fde\u7eed\u7684\u6f5c\u5728\u8fde\u63a5\u7a7a\u95f4\uff0c\u8fd9\u610f\u5473\u7740\u79bb\u6563\u7f51\u683c\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u7684\u9876\u70b9\u5d4c\u5165\u5728\u534a\u8fb9\u7f51\u683c\u8868\u793a\u4e2d\u751f\u6210\u5faa\u73af\u90bb\u5c45\u5173\u7cfb\uff0c\u8fd9\u4fdd\u8bc1\u4e86\u8fb9\u7684\u6d41\u5f62\u6027\u548c\u8868\u793a\u4e00\u822c\u591a\u8fb9\u5f62\u7f51\u683c\u7684\u80fd\u529b\u3002\u8fd9\u79cd\u8868\u793a\u975e\u5e38\u9002\u5408\u673a\u5668\u5b66\u4e60\u548c\u968f\u673a\u4f18\u5316\uff0c\u5e76\u4e14\u4e0d\u53d7\u8fde\u901a\u6027\u6216\u62d3\u6251\u7ed3\u6784\u7684\u9650\u5236\u3002\u6211\u4eec\u9996\u5148\u63a2\u7d22\u4e86\u8fd9\u79cd\u8868\u793a\u7684\u57fa\u672c\u5c5e\u6027\uff0c\u7136\u540e\u4f7f\u7528\u5b83\u6765\u62df\u5408\u6765\u81ea\u5927\u578b\u6570\u636e\u96c6\u7684\u7f51\u683c\u5206\u5e03\u3002\u751f\u6210\u7684\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u5177\u6709\u4ece\u6570\u636e\u96c6\u603b\u4f53\u5b66\u4e60\u5230\u7684\u9576\u5d4c\u7ed3\u6784\u7684\u4e0d\u540c\u7f51\u683c\uff0c\u5e76\u5177\u6709\u7b80\u6d01\u7684\u7ec6\u8282\u548c\u9ad8\u8d28\u91cf\u7684\u7f51\u683c\u5143\u7d20\u3002\u5728\u5e94\u7528\u4e2d\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u53ef\u4ee5\u4ece\u751f\u6210\u6a21\u578b\u4e2d\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u8f93\u51fa\uff0c\u8fd8\u53ef\u4ee5\u76f4\u63a5\u5b66\u4e60\u5177\u6709\u6311\u6218\u6027\u7684\u51e0\u4f55\u5904\u7406\u4efb\u52a1\uff0c\u4f8b\u5982\u7f51\u683c\u4fee\u590d\u3002||\n", "2409.20502": "|**2024-09-30**|[COLLAGE: Collaborative Human-Agent Interaction Generation using Hierarchical Latent Diffusion and Language Models](http://arxiv.org/abs/2409.20502)|null|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCOLLAGE\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u534f\u4f5c\u5f0f\u7684\u201c\u4e3b\u4f53-\u5ba2\u4f53-\u4e3b\u4f53\u201d\u4ea4\u4e92\uff0c\u8be5\u6846\u67b6\u5229\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5206\u5c42\u7684\u3001\u9488\u5bf9\u52a8\u4f5c\u7684\u77e2\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VQ-VAE\uff09\u3002\u6211\u4eec\u7684\u6a21\u578b\u901a\u8fc7\u7ed3\u5408LLM\u7684\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u6765\u6307\u5bfc\u751f\u6210\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u7f3a\u4e4f\u4e30\u5bcc\u6570\u636e\u96c6\u7684\u95ee\u9898\u3002\u5206\u5c42VQ-VAE\u67b6\u6784\u5728\u591a\u4e2a\u62bd\u8c61\u7ea7\u522b\u6355\u83b7\u4e0d\u540c\u7684\u52a8\u4f5c\u7279\u5b9a\u7279\u5f81\uff0c\u907f\u514d\u4e86\u5197\u4f59\u6982\u5ff5\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u5206\u8fa8\u7387\u8868\u793a\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fd0\u884c\u7684\u6269\u6563\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u4e86LLM\u751f\u6210\u7684\u8fd0\u52a8\u89c4\u5212\u7ebf\u7d22\u6765\u6307\u5bfc\u53bb\u566a\u8fc7\u7a0b\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u5177\u63a7\u5236\u529b\u548c\u591a\u6837\u6027\u7684\u3001\u9488\u5bf9\u63d0\u793a\u8bcd\u7684\u52a8\u4f5c\u751f\u6210\u3002\u5728CORE-4D\u548cInterHuman\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u751f\u6210\u903c\u771f\u4e14\u591a\u6837\u5316\u7684\u534f\u4f5c\u5f0f\u201c\u4eba-\u7269\u4f53-\u4eba\u201d\u4ea4\u4e92\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u4e3a\u5728\u673a\u5668\u4eba\u3001\u56fe\u5f62\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u5404\u4e2a\u9886\u57df\u5bf9\u590d\u6742\u4ea4\u4e92\u8fdb\u884c\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002||\n", "2409.20500": "|**2024-09-30**|[FreeMask: Rethinking the Importance of Attention Masks for Zero-Shot Video Editing](http://arxiv.org/abs/2409.20500)|null|\u6587\u672c\u5230\u89c6\u9891\u7684\u6269\u6563\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u6b65\u3002\u7531\u4e8e\u5176\u80fd\u591f\u751f\u6210\u65f6\u95f4\u8fde\u8d2f\u7684\u89c6\u9891\uff0c\u4f7f\u7528\u8fd9\u4e9b\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u89c6\u9891\u7f16\u8f91\u7684\u7814\u7a76\u8fc5\u901f\u6269\u5c55\u3002\u4e3a\u4e86\u63d0\u9ad8\u7f16\u8f91\u8d28\u91cf\uff0c\u7ed3\u6784\u5316\u63a7\u5236\u7ecf\u5e38\u88ab\u7528\u4e8e\u89c6\u9891\u7f16\u8f91\u4e2d\u3002\u5728\u8fd9\u4e9b\u6280\u672f\u4e2d\uff0c\u4ea4\u53c9\u6ce8\u610f\u529b\u63a9\u7801\u63a7\u5236\u4ee5\u5176\u6709\u6548\u6027\u548c\u6548\u7387\u800c\u8457\u79f0\u3002\u7136\u800c\uff0c\u5f53\u4ea4\u53c9\u6ce8\u610f\u529b\u63a9\u7801\u88ab\u7b80\u5355\u5730\u5e94\u7528\u4e8e\u89c6\u9891\u7f16\u8f91\u65f6\uff0c\u5b83\u4eec\u4f1a\u5f15\u5165\u8bf8\u5982\u6a21\u7cca\u548c\u95ea\u70c1\u4e4b\u7c7b\u7684\u4f2a\u5f71\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u53d1\u73b0\u4e86\u4e00\u4e2a\u5148\u524d\u89c6\u9891\u7f16\u8f91\u7814\u7a76\u4e2d\u88ab\u5ffd\u89c6\u7684\u5173\u952e\u56e0\u7d20\uff1a\u4ea4\u53c9\u6ce8\u610f\u529b\u63a9\u7801\u5e76\u975e\u59cb\u7ec8\u6e05\u6670\uff0c\u800c\u662f\u968f\u7740\u6a21\u578b\u7ed3\u6784\u548c\u53bb\u566a\u65f6\u95f4\u6b65\u957f\u800c\u53d8\u5316\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5ea6\u91cf\u63a9\u7801\u5339\u914d\u6210\u672c (MMC) \u6765\u91cf\u5316\u8fd9\u79cd\u53ef\u53d8\u6027\uff0c\u5e76\u63d0\u51fa\u4e86 FreeMask\uff0c\u4e00\u79cd\u4e3a\u7279\u5b9a\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u9009\u62e9\u6700\u4f73\u63a9\u7801\u7684\u65b9\u6cd5\u3002\u4f7f\u7528 MMC \u9009\u62e9\u7684\u63a9\u7801\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u6539\u8fdb\u4e86\u5168\u9762\u6ce8\u610f\u529b\u7279\u5f81\uff08\u4f8b\u5982\uff0c\u65f6\u95f4\u3001\u4ea4\u53c9\u548c\u81ea\u6ce8\u610f\u529b\u6a21\u5757\uff09\u4e2d\u7684\u63a9\u7801\u878d\u5408\u673a\u5236\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7684\u96f6\u6837\u672c\u89c6\u9891\u7f16\u8f91\u6846\u67b6\u4e2d\uff0c\u5e76\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\uff0c\u65e0\u9700\u63a7\u5236\u8f85\u52a9\u6216\u53c2\u6570\u5fae\u8c03\uff0c\u4f46\u80fd\u591f\u901a\u8fc7\u63a9\u7801\u7cbe\u5ea6\u63a7\u5236\u81ea\u9002\u5e94\u5730\u89e3\u8026\u672a\u7f16\u8f91\u7684\u8bed\u4e49\u5e03\u5c40\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cFreeMask \u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7f16\u8f91\u8d28\u91cf\u3002||\n", "2409.20346": "|**2024-09-30**|[All-optical autoencoder machine learning framework using diffractive processors](http://arxiv.org/abs/2409.20346)|null|\u884d\u5c04\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (D2NN) \u4ee5\u5176\u9ad8\u901f\u3001\u4f4e\u529f\u8017\u548c\u5f3a\u5927\u7684\u5e76\u884c\u6027\u800c\u95fb\u540d\uff0c\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6a21\u5f0f\u8bc6\u522b\u3001\u56fe\u50cf\u5904\u7406\u548c\u56fe\u50cf\u4f20\u8f93\u7b49\u5404\u4e2a\u9886\u57df\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u7f51\u7edc\u67b6\u6784\u4e3b\u8981\u5173\u6ce8\u539f\u59cb\u57df\u5185\u7684\u6570\u636e\u8868\u793a\uff0c\u5bf9\u6f5c\u5728\u7a7a\u95f4\u7684\u63a2\u7d22\u6709\u9650\uff0c\u4ece\u800c\u9650\u5236\u4e86 D2NN \u7684\u4fe1\u606f\u6316\u6398\u80fd\u529b\u548c\u591a\u529f\u80fd\u96c6\u6210\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u5149\u81ea\u52a8\u7f16\u7801\u5668 (OAE) \u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u5c06\u8f93\u5165\u6ce2\u573a\u7f16\u7801\u5230\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u5148\u9a8c\u5f62\u72b6\u5206\u5e03\uff0c\u5e76\u5c06\u7f16\u7801\u7684\u6a21\u5f0f\u89e3\u7801\u56de\u539f\u59cb\u6ce2\u573a\u3002\u901a\u8fc7\u5229\u7528 D2NN \u7684\u975e\u4e92\u6613\u6027\uff0cOAE \u6a21\u578b\u5728\u4e00\u4e2a\u6ce2\u4f20\u64ad\u65b9\u5411\u4e0a\u5145\u5f53\u7f16\u7801\u5668\uff0c\u800c\u5728\u76f8\u53cd\u65b9\u5411\u4e0a\u5145\u5f53\u89e3\u7801\u5668\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5c06\u8fd9\u4e9b\u6a21\u578b\u5e94\u7528\u4e8e\u4e09\u4e2a\u5173\u952e\u9886\u57df\uff1a\u56fe\u50cf\u53bb\u566a\u3001\u6297\u566a\u58f0\u7684\u53ef\u91cd\u6784\u56fe\u50cf\u5206\u7c7b\u548c\u56fe\u50cf\u751f\u6210\u3002\u5df2\u7ecf\u8fdb\u884c\u4e86\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u9a8c\u4ee5\u9a8c\u8bc1\u6570\u503c\u6a21\u62df\u3002\u6211\u4eec\u7684 OAE \u6846\u67b6\u5145\u5206\u5229\u7528\u4e86\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u7684\u6f5c\u529b\uff0c\u4f7f\u4e00\u7ec4\u884d\u5c04\u5904\u7406\u5668\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u56fe\u50cf\u91cd\u5efa\u3001\u8868\u793a\u548c\u751f\u6210\u3002\u5b83\u53ef\u4ee5\u88ab\u89c6\u4e3a\u7535\u5b50\u81ea\u52a8\u7f16\u7801\u5668\u6a21\u578b\u7684\u5bf9\u5e94\u7269\u548c\u6269\u5c55\u3002\u8fd9\u9879\u5de5\u4f5c\u4e0d\u4ec5\u4e3a\u5149\u5b66\u751f\u6210\u6a21\u578b\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u800c\u4e14\u4e3a\u5f00\u53d1\u548c\u5e94\u7528\u591a\u529f\u80fd\u3001\u9ad8\u5ea6\u96c6\u6210\u548c\u901a\u7528\u7684\u5149\u5b66\u667a\u80fd\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002||\n", "2409.20332": "|**2024-09-30**|[Devil is in Details: Locality-Aware 3D Abdominal CT Volume Generation for Self-Supervised Organ Segmentation](http://arxiv.org/abs/2409.20332)|null|\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9886\u57df\uff0c\u81ea\u76d1\u7763\u5b66\u4e60 (SSL) \u6280\u672f\u5df2\u7ecf\u51fa\u73b0\uff0c\u4ee5\u51cf\u8f7b\u5bf9\u6807\u7b7e\u7684\u9700\u6c42\uff0c\u4f46\u7531\u4e8e\u8d44\u6e90\u9700\u6c42\u4e0d\u65ad\u589e\u52a0\u548c\u9690\u79c1\u9650\u5236\uff0c\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u8bb8\u591a\u52aa\u529b\u90fd\u91c7\u7528\u751f\u6210\u6a21\u578b\u6765\u751f\u6210\u8de8\u8d8a\u4e0d\u540c\u6a21\u6001\u548c\u89e3\u5256\u533a\u57df\u7684\u9ad8\u4fdd\u771f\u3001\u672a\u6807\u8bb0\u7684 3D \u4f53\u79ef\u6570\u636e\u3002\u7136\u800c\uff0c\u4e0e\u5176\u4ed6\u89e3\u5256\u533a\u57df\u76f8\u6bd4\uff0c\u8179\u90e8\u5185\u590d\u6742\u4e14\u96be\u4ee5\u533a\u5206\u7684\u89e3\u5256\u7ed3\u6784\u5bf9\u8179\u90e8 CT \u4f53\u79ef\u751f\u6210\u63d0\u51fa\u4e86\u72ec\u7279\u7684\u6311\u6218\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u88ab\u5ffd\u89c6\u7684\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5c40\u90e8\u611f\u77e5\u6269\u6563 (Lad)\uff0c\u8fd9\u662f\u4e00\u79cd\u4e13\u4e3a\u751f\u6210\u7cbe\u7ec6\u7684 3D \u8179\u90e8 CT \u4f53\u79ef\u6570\u636e\u800c\u8bbe\u8ba1\u7684\u65b0\u65b9\u6cd5\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5c40\u90e8\u635f\u5931\u6765\u7ec6\u5316\u5173\u952e\u7684\u89e3\u5256\u533a\u57df\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6761\u4ef6\u63d0\u53d6\u5668\u5c06\u8179\u90e8\u5148\u9a8c\u4fe1\u606f\u6574\u5408\u5230\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u4ece\u800c\u80fd\u591f\u751f\u6210\u5927\u91cf\u9ad8\u8d28\u91cf\u7684\u8179\u90e8 CT \u4f53\u79ef\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u5bf9\u4e8e SSL \u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u800c\u65e0\u9700\u989d\u5916\u7684\u6807\u7b7e\u6216\u653e\u5c04\u5b66\u62a5\u544a\u7b49\u6570\u636e\u3002\u901a\u8fc7\u6211\u4eec\u7684\u65b9\u6cd5\u751f\u6210\u7684\u4f53\u79ef\u6570\u636e\u5728\u518d\u73b0\u8179\u90e8\u7ed3\u6784\u65b9\u9762\u8868\u73b0\u51fa\u975e\u51e1\u7684\u4fdd\u771f\u5ea6\uff0c\u5728 AbdomenCT-1K \u6570\u636e\u96c6\u4e0a\u5c06 FID \u5206\u6570\u4ece 0.0034 \u964d\u4f4e\u5230 0.0002\uff0c\u4e0e\u771f\u5b9e\u6570\u636e\u975e\u5e38\u63a5\u8fd1\uff0c\u5e76\u4f18\u4e8e\u5f53\u524d\u7684\u65b9\u6cd5\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u81ea\u76d1\u7763\u5668\u5b98\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5728\u4e24\u4e2a\u8179\u90e8\u6570\u636e\u96c6\u4e0a\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u5e73\u5747 Dice \u5206\u6570\u3002\u8fd9\u4e9b\u7ed3\u679c\u5f3a\u8c03\u4e86\u5408\u6210\u6570\u636e\u5728\u63a8\u8fdb\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u9762\u7684\u6f5c\u529b\u3002||\n", "2409.20197": "|**2024-09-30**|[UIR-LoRA: Achieving Universal Image Restoration through Multiple Low-Rank Adaptation](http://arxiv.org/abs/2409.20197)|**[link](https://github.com/justones/uir-lora)**|Existing unified methods typically treat multi-degradation image restoration as a multi-task learning problem. Despite performing effectively compared to single degradation restoration methods, they overlook the utilization of commonalities and specificities within multi-task restoration, thereby impeding the model's performance. Inspired by the success of deep generative models and fine-tuning techniques, we proposed a universal image restoration framework based on multiple low-rank adapters (LoRA) from multi-domain transfer learning. Our framework leverages the pre-trained generative model as the shared component for multi-degradation restoration and transfers it to specific degradation image restoration tasks using low-rank adaptation. Additionally, we introduce a LoRA composing strategy based on the degradation similarity, which adaptively combines trained LoRAs and enables our model to be applicable for mixed degradation restoration. Extensive experiments on multiple and mixed degradations demonstrate that the proposed universal image restoration method not only achieves higher fidelity and perceptual image quality but also has better generalization ability than other unified image restoration models. Our code is available at https://github.com/Justones/UIR-LoRA.||\n", "2409.20175": "|**2024-09-30**|[Ensemble Kalman Diffusion Guidance: A Derivative-free Method for Inverse Problems](http://arxiv.org/abs/2409.20175)|null|\u5728\u89e3\u51b3\u9006\u95ee\u9898\u65f6\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u7684\u5148\u9a8c\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\u3002\u8fd9\u79cd\u6846\u67b6\u53ef\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u524d\u5411\u6a21\u578b\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u3002\u5c3d\u7ba1\u5b83\u4eec\u5728\u8bb8\u591a\u6210\u50cf\u9006\u95ee\u9898\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u90fd\u4f9d\u8d56\u4e8e\u7279\u6743\u4fe1\u606f\uff0c\u4f8b\u5982\u5bfc\u6570\u3001\u4f2a\u9006\u6216\u5173\u4e8e\u524d\u5411\u6a21\u578b\u7684\u5b8c\u6574\u77e5\u8bc6\u3002\u8fd9\u79cd\u4f9d\u8d56\u6027\u6784\u6210\u4e86\u4e00\u4e2a\u91cd\u5927\u9650\u5236\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u65e0\u6cd5\u83b7\u5f97\u6b64\u7c7b\u4fe1\u606f\u7684\u5404\u79cd\u95ee\u9898\u4e2d\u7684\u4f7f\u7528\uff0c\u4f8b\u5982\u5728\u8bb8\u591a\u79d1\u5b66\u5e94\u7528\u4e2d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7528\u4e8e\u6269\u6563\u6a21\u578b\u7684\u96c6\u6210\u5361\u5c14\u66fc\u6269\u6563\u5f15\u5bfc (EnKG)\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u5bfc\u6570\u65b9\u6cd5\uff0c\u53ef\u4ee5\u901a\u8fc7\u4ec5\u8bbf\u95ee\u524d\u5411\u6a21\u578b\u8bc4\u4f30\u548c\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u5148\u9a8c\u6765\u89e3\u51b3\u9006\u95ee\u9898\u3002\u6211\u4eec\u7814\u7a76\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u9006\u95ee\u9898\u4e2d\u7684\u7ecf\u9a8c\u6709\u6548\u6027\uff0c\u5305\u62ec\u79d1\u5b66\u73af\u5883\uff0c\u4f8b\u5982\u63a8\u65ad\u6d41\u4f53\u6d41\u52a8\u548c\u5929\u6587\u7269\u4f53\uff0c\u8fd9\u4e9b\u90fd\u662f\u9ad8\u5ea6\u975e\u7ebf\u6027\u7684\u9006\u95ee\u9898\uff0c\u901a\u5e38\u53ea\u5141\u8bb8\u5bf9\u524d\u5411\u6a21\u578b\u8fdb\u884c\u9ed1\u76d2\u8bbf\u95ee\u3002||\n", "2409.20164": "|**2024-09-30**|[Erase, then Redraw: A Novel Data Augmentation Approach for Free Space Detection Using Diffusion Model](http://arxiv.org/abs/2409.20164)|null|Data augmentation is one of the most common tools in deep learning, underpinning many recent advances including tasks such as classification, detection, and semantic segmentation. The standard approach to data augmentation involves simple transformations like rotation and flipping to generate new images. However, these new images often lack diversity along the main semantic dimensions within the data. Traditional data augmentation methods cannot alter high-level semantic attributes such as the presence of vehicles, trees, and buildings in a scene to enhance data diversity. In recent years, the rapid development of generative models has injected new vitality into the field of data augmentation. In this paper, we address the lack of diversity in data augmentation for road detection task by using a pre-trained text-to-image diffusion model to parameterize image-to-image transformations. Our method involves editing images using these diffusion models to change their semantics. In essence, we achieve this goal by erasing instances of real objects from the original dataset and generating new instances with similar semantics in the erased regions using the diffusion model, thereby expanding the original dataset. We evaluate our approach on the KITTI road dataset and achieve the best results compared to other data augmentation methods, which demonstrates the effectiveness of our proposed development.||\n", "2409.20124": "|**2024-09-30**|[Conditional Diffusion Models are Minimax-Optimal and Manifold-Adaptive for Conditional Distribution Estimation](http://arxiv.org/abs/2409.20124)|null|We consider a class of conditional forward-backward diffusion models for conditional generative modeling, that is, generating new data given a covariate (or control variable). To formally study the theoretical properties of these conditional generative models, we adopt a statistical framework of distribution regression to characterize the large sample properties of the conditional distribution estimators induced by these conditional forward-backward diffusion models. Here, the conditional distribution of data is assumed to smoothly change over the covariate. In particular, our derived convergence rate is minimax-optimal under the total variation metric within the regimes covered by the existing literature. Additionally, we extend our theory by allowing both the data and the covariate variable to potentially admit a low-dimensional manifold structure. In this scenario, we demonstrate that the conditional forward-backward diffusion model can adapt to both manifold structures, meaning that the derived estimation error bound (under the Wasserstein metric) depends only on the intrinsic dimensionalities of the data and the covariate.||\n", "2409.20122": "|**2024-09-30**|[Training a Computer Vision Model for Commercial Bakeries with Primarily Synthetic Images](http://arxiv.org/abs/2409.20122)|null|In the food industry, reprocessing returned product is a vital step to increase resource efficiency. [SBB23] presented an AI application that automates the tracking of returned bread buns. We extend their work by creating an expanded dataset comprising 2432 images and a wider range of baked goods. To increase model robustness, we use generative models pix2pix and CycleGAN to create synthetic images. We train state-of-the-art object detection model YOLOv9 and YOLOv8 on our detection task. Our overall best-performing model achieved an average precision AP@0.5 of 90.3% on our test set.||\n", "2410.02740": "|**2024-10-03**|[Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models](http://arxiv.org/abs/2410.02740)|null|\u8fd1\u5e74\u6765\uff0c\u591a\u6a21\u6001\u6a21\u578b\u7684\u8fdb\u6b65\u51f8\u663e\u4e86\u91cd\u5199\u56fe\u50cf\u63cf\u8ff0\u5bf9\u4e8e\u63d0\u9ad8\u6027\u80fd\u7684\u4ef7\u503c\uff0c\u4f46\u5173\u952e\u6311\u6218\u4f9d\u7136\u5b58\u5728\u3002\u4f8b\u5982\uff0c\u867d\u7136\u5408\u6210\u56fe\u50cf\u63cf\u8ff0\u901a\u5e38\u80fd\u63d0\u4f9b\u66f4\u9ad8\u7684\u8d28\u91cf\u548c\u56fe\u6587\u4e00\u81f4\u6027\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u5b83\u4eec\u662f\u5426\u53ef\u4ee5\u5b8c\u5168\u66ff\u4ee3\u66ff\u4ee3\u6587\u672c\uff1a\u5408\u6210\u56fe\u50cf\u63cf\u8ff0\u7684\u4f5c\u7528\u4ee5\u53ca\u5b83\u4eec\u5728\u9884\u8bad\u7ec3\u4e2d\u4e0e\u539f\u59cb\u7f51\u7edc\u722c\u53d6\u7684\u66ff\u4ee3\u6587\u672c\u7684\u4ea4\u4e92\u4f5c\u7528\u4ecd\u4e0d\u6e05\u695a\u3002\u6b64\u5916\uff0c\u4e0d\u540c\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u53ef\u80fd\u5bf9\u7279\u5b9a\u7684\u56fe\u50cf\u63cf\u8ff0\u683c\u5f0f\u6709\u72ec\u7279\u7684\u504f\u597d\uff0c\u4f46\u8bc6\u522b\u6bcf\u79cd\u6a21\u578b\u6700\u4f73\u56fe\u50cf\u63cf\u8ff0\u7684\u5de5\u4f5c\u4ecd\u7136\u6709\u9650\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u53ef\u63a7\u7684\u3001\u53ef\u6269\u5c55\u7684\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u6d41\u7a0b\uff0c\u65e8\u5728\u751f\u6210\u9488\u5bf9\u5404\u79cd\u591a\u6a21\u6001\u6a21\u578b\u91cf\u8eab\u5b9a\u5236\u7684\u4e0d\u540c\u56fe\u50cf\u63cf\u8ff0\u683c\u5f0f\u3002\u901a\u8fc7\u4ee5\u77ed\u5408\u6210\u56fe\u50cf\u63cf\u8ff0 (SSC) \u548c\u5bc6\u96c6\u5408\u6210\u56fe\u50cf\u63cf\u8ff0 (DSC+) \u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u6211\u4eec\u7cfb\u7edf\u5730\u63a2\u7d22\u4e86\u5b83\u4eec\u5bf9 CLIP\u3001\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u7b49\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5b83\u4eec\u4e0e\u66ff\u4ee3\u6587\u672c\u7684\u4ea4\u4e92\u4f5c\u7528\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4fdd\u7559\u5408\u6210\u56fe\u50cf\u63cf\u8ff0\u548c\u66ff\u4ee3\u6587\u672c\u7684\u6df7\u5408\u65b9\u6cd5\u53ef\u4ee5\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528\u5408\u6210\u56fe\u50cf\u63cf\u8ff0\uff0c\u4ece\u800c\u63d0\u9ad8\u4e00\u81f4\u6027\u548c\u6027\u80fd\uff0c\u5e76\u4e14\u6bcf\u4e2a\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u5bf9\u7279\u5b9a\u56fe\u50cf\u63cf\u8ff0\u683c\u5f0f\u7684\u504f\u597d\u3002\u8fd9\u79cd\u5168\u9762\u7684\u5206\u6790\u4e3a\u4f18\u5316\u56fe\u50cf\u63cf\u8ff0\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e86\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u3002||\n", "2410.02726": "|**2024-10-03**|[A Photonic Parameter-shift Rule: Enabling Gradient Computation for Photonic Quantum Computers](http://arxiv.org/abs/2410.02726)|null|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u6027\u5149\u91cf\u5b50\u8ba1\u7b97\u5e73\u53f0\u4e0a\u5b9e\u73b0\u7684\u91cf\u5b50\u7b97\u6cd5\u4e2d\u8fdb\u884c\u68af\u5ea6\u8ba1\u7b97\u7684\u65b9\u6cd5\u3002\u867d\u7136\u53c2\u6570\u79fb\u4f4d\u89c4\u5219\u5df2\u6210\u4e3a\u57fa\u4e8e\u91cf\u5b50\u6bd4\u7279\u95e8\u7684\u91cf\u5b50\u8ba1\u7b97\u4e2d\u8ba1\u7b97\u68af\u5ea6\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u4f46\u7531\u4e8e\u798f\u514b\u7a7a\u95f4\u4e2d\u5fae\u5206\u76f8\u79fb\u7b97\u7b26\u7684\u975e\u5e7a\u6b63\u6027\uff0c\u5b83\u4eec\u5728\u5149\u5b50\u5e73\u53f0\u4e0a\u7684\u76f4\u63a5\u5e94\u7528\u53d7\u5230\u4e86\u963b\u788d\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u514b\u670d\u4e86\u8fd9\u4e00\u9650\u5236\u7684\u5149\u5b50\u53c2\u6570\u79fb\u4f4d\u89c4\u5219\uff0c\u4e3a\u7ebf\u6027\u5149\u91cf\u5b50\u5904\u7406\u5668\u4e2d\u7684\u68af\u5ea6\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cbe\u786e\u7684\u516c\u5f0f\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u8f93\u5165\u5149\u5b50\u6570\u5448\u7ebf\u6027\u6bd4\u4f8b\uff0c\u5e76\u4e14\u5728\u6bcf\u6b21\u8bc4\u4f30\u4e2d\u4f7f\u7528\u5177\u6709\u79fb\u4f4d\u53c2\u6570\u7684\u76f8\u540c\u53c2\u6570\u5316\u5149\u5b50\u7535\u8def\u3002\u8fd9\u4e00\u8fdb\u6b65\u5f25\u5408\u4e86\u5149\u5b50\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u5dee\u8ddd\uff0c\u4f7f\u5f97\u80fd\u591f\u5728\u8fd1\u671f\u5149\u5b50\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u5bf9\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\u8fdb\u884c\u6709\u6548\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u3002\u6211\u4eec\u901a\u8fc7\u91cf\u5b50\u5316\u5b66\u548c\u751f\u6210\u6a21\u578b\u4efb\u52a1\u4e2d\u7684\u6570\u503c\u6a21\u62df\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e0e\u5176\u4ed6\u57fa\u4e8e\u68af\u5ea6\u548c\u65e0\u68af\u5ea6\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u4f18\u5316\u6027\u80fd\u4ee5\u53ca\u5bf9\u6709\u9650\u91c7\u6837\u548c\u5149\u5b50\u53ef\u5206\u8fa8\u6027\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002||\n", "2410.02710": "|**2024-10-03**|[SteerDiff: Steering towards Safe Text-to-Image Diffusion Models](http://arxiv.org/abs/2410.02710)|null|\u6587\u672c\u5230\u56fe\u50cf (T2I) \u6269\u6563\u6a21\u578b\u56e0\u5176\u80fd\u591f\u751f\u6210\u5177\u6709\u7cbe\u786e\u6587\u672c\u5bf9\u9f50\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u800c\u5907\u53d7\u5173\u6ce8\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e5f\u53ef\u80fd\u88ab\u6ee5\u7528\u4e8e\u5236\u4f5c\u4e0d\u5f53\u5185\u5bb9\u3002\u73b0\u6709\u7684\u5b89\u5168\u63aa\u65bd\u901a\u5e38\u4f9d\u8d56\u4e8e\u6587\u672c\u5206\u7c7b\u5668\u6216\u7c7b\u4f3c ControlNet \u7684\u65b9\u6cd5\uff0c\u4f46\u5f80\u5f80\u4e0d\u591f\u5145\u5206\u3002\u4f20\u7edf\u7684\u6587\u672c\u5206\u7c7b\u5668\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u6807\u8bb0\u6570\u636e\u96c6\uff0c\u5e76\u4e14\u5f88\u5bb9\u6613\u901a\u8fc7\u6539\u5199\u6765\u7ed5\u8fc7\u3002\u968f\u7740\u6269\u6563\u6a21\u578b\u7684\u4e0d\u65ad\u6269\u5c55\uff0c\u5fae\u8c03\u8fd9\u4e9b\u5b89\u5168\u63aa\u65bd\u53d8\u5f97\u8d8a\u6765\u8d8a\u5177\u6709\u6311\u6218\u6027\uff0c\u5e76\u4e14\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002\u6700\u8fd1\u7684\u7ea2\u961f\u653b\u51fb\u7814\u7a76\u8fdb\u4e00\u6b65\u5f3a\u8c03\u4e86\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\u6765\u9632\u6b62\u751f\u6210\u4e0d\u5f53\u5185\u5bb9\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 SteerDiff\uff0c\u8fd9\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u9002\u914d\u5668\u6a21\u5757\uff0c\u65e8\u5728\u5145\u5f53\u7528\u6237\u8f93\u5165\u548c\u6269\u6563\u6a21\u578b\u4e4b\u95f4\u7684\u4e2d\u4ecb\uff0c\u786e\u4fdd\u751f\u6210\u7684\u56fe\u50cf\u7b26\u5408\u9053\u5fb7\u548c\u5b89\u5168\u6807\u51c6\uff0c\u5e76\u4e14\u5bf9\u53ef\u7528\u6027\u7684\u5f71\u54cd\u5fae\u4e4e\u5176\u5fae\u3002SteerDiff \u8bc6\u522b\u5e76\u64cd\u7eb5\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u4e0d\u5f53\u6982\u5ff5\uff0c\u4ee5\u5f15\u5bfc\u6a21\u578b\u8fdc\u79bb\u6709\u5bb3\u8f93\u51fa\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5404\u79cd\u6982\u5ff5\u9057\u5fd8\u4efb\u52a1\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u4ee5\u8bc4\u4f30\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u9488\u5bf9\u591a\u79cd\u7ea2\u961f\u653b\u51fb\u7b56\u7565\u5bf9 SteerDiff \u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u8bc4\u4f30\u5176\u9c81\u68d2\u6027\u3002\u6700\u540e\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86 SteerDiff \u5728\u6982\u5ff5\u9057\u5fd8\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6587\u672c\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u591a\u529f\u80fd\u6027\u3002||\n", "2410.02705": "|**2024-10-03**|[ControlAR: Controllable Image Generation with Autoregressive Models](http://arxiv.org/abs/2410.02705)|**[link](https://github.com/hustvl/controlar)**|\u81ea\u56de\u5f52 (AR) \u6a21\u578b\u5c06\u56fe\u50cf\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u4efb\u52a1\uff0c\u5c55\u73b0\u51fa\u60ca\u4eba\u7684\u6f5c\u529b\uff0c\u5e76\u9010\u6e10\u6210\u4e3a\u6269\u6563\u6a21\u578b\u7684\u6709\u529b\u7ade\u4e89\u8005\u3002\u7136\u800c\uff0c\u7c7b\u4f3c\u4e8e ControlNet \u7684\u63a7\u5236\u5230\u56fe\u50cf\u751f\u6210\u5728 AR \u6a21\u578b\u4e2d\u4ecd\u7136\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u88ab\u63a2\u7d22\u3002\u5c3d\u7ba1\u53d7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u6b65\u7684\u542f\u53d1\uff0c\u4e00\u79cd\u81ea\u7136\u800c\u7136\u7684\u65b9\u6cd5\u662f\u5c06\u63a7\u5236\u56fe\u50cf\u6807\u8bb0\u5316\u4e3a\u6807\u8bb0\uff0c\u5e76\u5728\u89e3\u7801\u56fe\u50cf\u6807\u8bb0\u4e4b\u524d\u5c06\u5b83\u4eec\u9884\u586b\u5145\u5230\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\uff0c\u4f46\u4e0e\u5176 ControlNet \u76f8\u6bd4\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u751f\u6210\u8d28\u91cf\u65b9\u9762\u4ecd\u7136\u4e0d\u8db3\uff0c\u5e76\u4e14\u6548\u7387\u4f4e\u4e0b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86 ControlAR\uff0c\u8fd9\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u6709\u6548\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u7a7a\u95f4\u63a7\u5236\u96c6\u6210\u5230\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u3002\u9996\u5148\uff0c\u6211\u4eec\u63a2\u7d22\u4e86 AR \u6a21\u578b\u7684\u63a7\u5236\u7f16\u7801\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u63a7\u5236\u7f16\u7801\u5668\uff0c\u5c06\u7a7a\u95f4\u8f93\u5165\uff08\u4f8b\u5982\uff0cCanny \u8fb9\u7f18\u6216\u6df1\u5ea6\u56fe\uff09\u8f6c\u6362\u4e3a\u63a7\u5236\u6807\u8bb0\u3002\u7136\u540e\uff0cControlAR \u5229\u7528\u6761\u4ef6\u89e3\u7801\u65b9\u6cd5\uff0c\u6839\u636e\u63a7\u5236\u6807\u8bb0\u548c\u56fe\u50cf\u6807\u8bb0\u4e4b\u95f4\u7684\u6bcf\u4e2a\u6807\u8bb0\u878d\u5408\uff08\u7c7b\u4f3c\u4e8e\u4f4d\u7f6e\u7f16\u7801\uff09\u751f\u6210\u4e0b\u4e00\u4e2a\u56fe\u50cf\u6807\u8bb0\u3002\u4e0e\u9884\u586b\u5145\u6807\u8bb0\u76f8\u6bd4\uff0c\u4f7f\u7528\u6761\u4ef6\u89e3\u7801\u663e\u7740\u589e\u5f3a\u4e86 AR \u6a21\u578b\u7684\u63a7\u5236\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u6548\u7387\u3002\u6b64\u5916\uff0c\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u6240\u63d0\u51fa\u7684 ControlAR \u901a\u8fc7\u6761\u4ef6\u89e3\u7801\u548c\u7279\u5b9a\u63a7\u5236\u4f7f AR \u6a21\u578b\u80fd\u591f\u751f\u6210\u4efb\u610f\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 ControlAR \u80fd\u591f\u5728\u5305\u62ec\u8fb9\u7f18\u3001\u6df1\u5ea6\u548c\u5206\u5272\u63a9\u7801\u5728\u5185\u7684\u4e0d\u540c\u8f93\u5165\u4e0a\u8fdb\u884c\u81ea\u56de\u5f52\u63a7\u5236\u5230\u56fe\u50cf\u751f\u6210\u3002\u6b64\u5916\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u7ed3\u679c\u90fd\u8868\u660e ControlAR \u8d85\u8d8a\u4e86\u5148\u524d\u6700\u5148\u8fdb\u7684\u53ef\u63a7\u6269\u6563\u6a21\u578b\uff0c\u4f8b\u5982 ControlNet++\u3002\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6f14\u793a\u5c06\u5f88\u5feb\u5728 https://github.com/hustvl/ControlAR \u4e0a\u63d0\u4f9b\u3002||\n", "2410.02667": "|**2024-10-03**|[GUD: Generation with Unified Diffusion](http://arxiv.org/abs/2410.02667)|null|\u6269\u6563\u751f\u6210\u6a21\u578b\u901a\u8fc7\u53cd\u8f6c\u5c06\u566a\u58f0\u9010\u6b65\u6dfb\u52a0\u5230\u6570\u636e\u6837\u672c\u7684\u8fc7\u7a0b\uff0c\u5c06\u566a\u58f0\u8f6c\u6362\u4e3a\u6570\u636e\u3002\u53d7\u7269\u7406\u5b66\u4e2d\u91cd\u6574\u5316\u7fa4\u6982\u5ff5\u7684\u542f\u53d1\uff0c\u8be5\u6982\u5ff5\u5206\u6790\u4e0d\u540c\u5c3a\u5ea6\u7684\u7cfb\u7edf\uff0c\u6211\u4eec\u901a\u8fc7\u63a2\u7d22\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\u65b9\u9762\u6765\u91cd\u65b0\u5ba1\u89c6\u6269\u6563\u6a21\u578b\uff1a1\uff09\u6269\u6563\u8fc7\u7a0b\u5728\u5176\u4e0a\u8fd0\u884c\u7684\u8868\u793a\u7684\u9009\u62e9\uff08\u4f8b\u5982\uff0c\u50cf\u7d20\u3001PCA\u3001\u5085\u91cc\u53f6\u6216\u5c0f\u6ce2\u57fa\uff09\uff0c2\uff09\u6570\u636e\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u88ab\u8f6c\u6362\u6210\u5148\u9a8c\u5206\u5e03\uff08\u4f8b\u5982\uff0c\u5177\u6709\u534f\u65b9\u5dee$\\Sigma$\u7684\u9ad8\u65af\u5206\u5e03\uff09\uff0c\u4ee5\u53ca 3\uff09\u5e94\u7528\u4e8e\u6570\u636e\u4e0d\u540c\u90e8\u5206\u7684\u566a\u58f0\u6c34\u5e73\u7684\u8c03\u5ea6\uff0c\u7531\u7ec4\u4ef6\u7ea7\u566a\u58f0\u8c03\u5ea6\u6355\u83b7\u3002\u7ed3\u5408\u8fd9\u4e9b\u9009\u62e9\u7684\u7075\u6d3b\u6027\uff0c\u6211\u4eec\u4e3a\u6269\u6563\u751f\u6210\u6a21\u578b\u5f00\u53d1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u6781\u5927\u5730\u589e\u5f3a\u4e86\u8bbe\u8ba1\u81ea\u7531\u5ea6\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u8f6f\u6761\u4ef6\u6a21\u578b\uff0c\u53ef\u4ee5\u5728\u6807\u51c6\u6269\u6563\u6a21\u578b\u548c\u81ea\u56de\u5f52\u6a21\u578b\uff08\u5728\u4efb\u4f55\u57fa\u7840\u4e0a\uff09\u4e4b\u95f4\u5e73\u6ed1\u63d2\u503c\uff0c\u4ece\u6982\u5ff5\u4e0a\u8fde\u63a5\u4e86\u8fd9\u4e24\u79cd\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u6846\u67b6\u5f00\u8f9f\u4e86\u4e00\u4e2a\u5e7f\u9614\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u6570\u636e\u751f\u6210\uff0c\u5e76\u4e3a\u96c6\u6210\u4e0d\u540c\u751f\u6210\u65b9\u6cd5\u548c\u751f\u6210\u4efb\u52a1\u7684\u65b0\u9896\u67b6\u6784\u94fa\u5e73\u9053\u8def\u3002||\n", "2410.02664": "|**2024-10-03**|[Grounded Answers for Multi-agent Decision-making Problem through Generative World Model](http://arxiv.org/abs/2410.02664)|null|\u751f\u6210\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u4fc3\u8fdb\u4e86\u56fe\u50cf\u751f\u6210\u548c\u804a\u5929\u673a\u5668\u4eba\u7b49\u8bb8\u591a\u9886\u57df\u7684\u91cd\u5927\u521b\u65b0\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u7684\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u95ee\u9898\u65f6\uff0c\u5e38\u5e38\u4f1a\u4ea7\u751f\u7c97\u7565\u4e14\u8bef\u5bfc\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u56e0\u4e3a\u5b83\u4eec\u7f3a\u4e4f\u50cf\u4eba\u7c7b\u4e00\u6837\u7684\u8bd5\u9519\u7ecf\u9a8c\u548c\u63a8\u7406\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4e00\u79cd\u5c06\u8bed\u8a00\u5f15\u5bfc\u7684\u6a21\u62df\u5668\u96c6\u6210\u5230\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7ba1\u9053\u4e2d\u7684\u8303\u5f0f\uff0c\u4ee5\u589e\u5f3a\u751f\u6210\u7684\u7b54\u6848\u8d28\u91cf\u3002\u8be5\u6a21\u62df\u5668\u662f\u4e00\u4e2a\u5206\u522b\u5b66\u4e60\u52a8\u529b\u5b66\u548c\u5956\u52b1\u7684\u4e16\u754c\u6a21\u578b\uff0c\u5176\u4e2d\u52a8\u529b\u5b66\u6a21\u578b\u5305\u62ec\u4e00\u4e2a\u56fe\u50cf\u5206\u8bcd\u5668\u548c\u4e00\u4e2a\u56e0\u679cTransformer\uff0c\u7528\u4e8e\u81ea\u56de\u5f52\u5730\u751f\u6210\u4ea4\u4e92\u8f6c\u6362\uff0c\u800c\u5956\u52b1\u6a21\u578b\u662f\u4e00\u4e2a\u53cc\u5411Transformer\uff0c\u901a\u8fc7\u5728\u8bed\u8a00\u6307\u5bfc\u4e0b\u6700\u5927\u5316\u4e13\u5bb6\u6f14\u793a\u4e2d\u8f68\u8ff9\u7684\u53ef\u80fd\u6027\u6765\u5b66\u4e60\u3002\u7ed9\u5b9a\u5f53\u524d\u72b6\u6001\u7684\u56fe\u50cf\u548c\u4efb\u52a1\u63cf\u8ff0\uff0c\u6211\u4eec\u4f7f\u7528\u4e16\u754c\u6a21\u578b\u6765\u8bad\u7ec3\u8054\u5408\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5728\u52a8\u529b\u5b66\u6a21\u578b\u4e0a\u8fd0\u884c\u6536\u655b\u7684\u7b56\u7565\u6765\u751f\u6210\u56fe\u50cf\u5e8f\u5217\u4f5c\u4e3a\u7b54\u6848\u3002\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u901a\u8fc7\u5728\u661f\u9645\u4e89\u9738\u591a\u667a\u80fd\u4f53\u6311\u6218\u57fa\u51c6\u6d4b\u8bd5\u7684\u8bad\u7ec3\u548c\u672a\u89c1\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u4ece\u800c\u6539\u8fdb\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u95ee\u9898\u7684\u7b54\u6848\u3002\u7279\u522b\u662f\uff0c\u5b83\u53ef\u4ee5\u751f\u6210\u4e00\u81f4\u7684\u4ea4\u4e92\u5e8f\u5217\u548c\u4ea4\u4e92\u72b6\u6001\u4e0b\u53ef\u89e3\u91ca\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4e3a\u672a\u6765\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u5f00\u8f9f\u4e86\u9053\u8def\u3002||\n", "2410.02656": "|**2024-10-03**|[Scalable Simulation-free Entropic Unbalanced Optimal Transport](http://arxiv.org/abs/2410.02656)|null|\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u95ee\u9898\u65e8\u5728\u5bfb\u627e\u4e00\u4e2a\u8fde\u63a5\u4e24\u4e2a\u5206\u5e03\u7684\u4f20\u8f93\u6620\u5c04\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u7ed9\u5b9a\u7684\u6210\u672c\u51fd\u6570\u3002\u5bfb\u627e\u8fd9\u6837\u7684\u4f20\u8f93\u6620\u5c04\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u4f8b\u5982\u751f\u6210\u6a21\u578b\u548c\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u8f6c\u6362\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u65e0\u9700\u6a21\u62df\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u71b5\u975e\u5e73\u8861\u6700\u4f18\u4f20\u8f93\uff08EUOT\uff09\u95ee\u9898\u3002\u6211\u4eec\u63a8\u5bfc\u4e86\u8be5EUOT\u95ee\u9898\u7684\u52a8\u529b\u5b66\u5f62\u5f0f\uff0c\u5b83\u662f\u859b\u5b9a\u8c14\u6865\uff08SB\uff09\u95ee\u9898\u7684\u63a8\u5e7f\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u4ece\u968f\u673a\u6700\u4f18\u63a7\u5236\u7684\u89d2\u5ea6\u63a8\u5bfc\u4e86EUOT\u95ee\u9898\u7684\u5bf9\u5076\u5f62\u5f0f\u548c\u6700\u4f18\u6027\u6761\u4ef6\u3002\u901a\u8fc7\u5229\u7528\u8fd9\u4e9b\u6027\u8d28\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6a21\u62df\u7684\u7b97\u6cd5\u6765\u6c42\u89e3EUOT\uff0c\u79f0\u4e3aSimulation-free EUOT (SF-EUOT)\u3002\u73b0\u6709\u7684SB\u6a21\u578b\u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u9700\u8981\u6602\u8d35\u7684\u6a21\u62df\u6210\u672c\uff0c\u800c\u6211\u4eec\u7684\u6a21\u578b\u5229\u7528\u4e92\u6613\u6027\u5b9e\u73b0\u4e86\u65e0\u9700\u6a21\u62df\u7684\u8bad\u7ec3\u548c\u4e00\u6b65\u751f\u6210\u3002\u4e0e\u4e4b\u524d\u7684SB\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u751f\u6210\u6a21\u578b\u548c\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u663e\u8457\u63d0\u9ad8\u7684\u53ef\u6269\u5c55\u6027\u3002||\n", "2410.02653": "|**2024-10-03**|[Measuring and Improving Persuasiveness of Generative Models](http://arxiv.org/abs/2410.02653)|null|\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6b63\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u6d89\u53ca\u751f\u6210\u4eba\u7c7b\u6d88\u8d39\u5185\u5bb9\uff08\u4f8b\u5982\u8425\u9500\uff09\u4ee5\u53ca\u76f4\u63a5\u4e0e\u4eba\u7c7b\u4e92\u52a8\uff08\u4f8b\u5982\u901a\u8fc7\u804a\u5929\u673a\u5668\u4eba\uff09\u7684\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002\u5f00\u53d1\u80fd\u591f\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u8bf4\u670d\u6027\u4fe1\u606f\u7684\u6b64\u7c7b\u7cfb\u7edf\uff0c\u5bf9\u793e\u4f1a\u6765\u8bf4\u65e2\u6709\u673a\u9047\u4e5f\u6709\u6311\u6218\u3002\u4e00\u65b9\u9762\uff0c\u6b64\u7c7b\u7cfb\u7edf\u53ef\u4ee5\u5bf9\u5e7f\u544a\u548c\u793e\u4f1a\u516c\u76ca\u7b49\u9886\u57df\u4ea7\u751f\u79ef\u6781\u5f71\u54cd\uff0c\u4f8b\u5982\u89e3\u51b3\u836f\u7269\u6210\u763e\u95ee\u9898\uff1b\u53e6\u4e00\u65b9\u9762\uff0c\u5b83\u4eec\u4e5f\u53ef\u80fd\u88ab\u6ee5\u7528\u4e8e\u4f20\u64ad\u9519\u8bef\u4fe1\u606f\u548c\u5851\u9020\u653f\u6cbb\u89c2\u70b9\u3002\u4e3a\u4e86\u5f15\u5bfc LLM \u5bf9\u793e\u4f1a\u7684\u5f71\u54cd\uff0c\u6211\u4eec\u9700\u8981\u5f00\u53d1\u7cfb\u7edf\u6765\u8861\u91cf\u548c\u6bd4\u8f83\u5b83\u4eec\u7684 \u8bf4\u670d\u529b\u3002\u51fa\u4e8e\u8fd9\u79cd\u52a8\u673a\uff0c\u6211\u4eec\u63a8\u51fa\u4e86 PersuasionBench \u548c PersuasionArena\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5305\u542b\u4e00\u7cfb\u5217\u4efb\u52a1\u7684\u5927\u578b\u57fa\u51c6\u548c\u7ade\u6280\u573a\uff0c\u7528\u4e8e\u81ea\u52a8\u8861\u91cf\u751f\u6210\u6a21\u578b\u7684\u8bf4\u670d\u80fd\u529b\u3002\u6211\u4eec\u8c03\u67e5\u4e86 LLM \u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u4e86\u89e3\u548c\u5229\u7528\u4e86\u53ef\u4ee5\u5e2e\u52a9\u5b83\u4eec\u751f\u6210\u66f4\u6709\u8bf4\u670d\u529b\u7684\u8bed\u8a00\u7684\u8bed\u8a00\u6a21\u5f0f\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cLLM \u7684\u8bf4\u670d\u529b\u4e0e\u5176\u6a21\u578b\u89c4\u6a21\u5448\u6b63\u76f8\u5173\uff0c\u4f46\u8f83\u5c0f\u7684\u6a21\u578b\u4e5f\u53ef\u4ee5\u6bd4\u66f4\u5927\u7684\u6a21\u578b\u5177\u6709\u66f4\u9ad8\u7684\u8bf4\u670d\u529b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u548c\u81ea\u7136\u6570\u636e\u96c6\u8fdb\u884c\u7684\u76ee\u6807\u8bad\u7ec3\u663e\u7740\u589e\u5f3a\u4e86\u8f83\u5c0f\u6a21\u578b\u7684\u8bf4\u670d\u80fd\u529b\uff0c\u8fd9\u5bf9\u4f9d\u8d56\u89c4\u6a21\u7684\u5047\u8bbe\u63d0\u51fa\u4e86\u6311\u6218\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5bf9\u6a21\u578b\u5f00\u53d1\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u90fd\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u4f8b\u5982\uff0c\u867d\u7136\u6b27\u76df\u4eba\u5de5\u667a\u80fd\u6cd5\u6848\u548c\u52a0\u5dde\u7684 SB-1047 \u65e8\u5728\u6839\u636e\u6d6e\u70b9\u8fd0\u7b97\u6b21\u6570\u6765\u76d1\u7ba1\u4eba\u5de5\u667a\u80fd\u6a21\u578b\uff0c\u4f46\u6211\u4eec\u8bc1\u660e\uff0c\u4ec5\u51ed\u6b64\u7c7b\u7b80\u5355\u6307\u6807\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u4eba\u5de5\u667a\u80fd\u7684\u793e\u4f1a\u5f71\u54cd\u3002\u6211\u4eec\u9080\u8bf7\u793e\u533a\u63a2\u7d22\u5e76\u8d21\u732e PersuasionArena \u548c PersuasionBench\uff08\u7f51\u5740\u4e3a https://bit.ly/measure-persuasion\uff09\uff0c\u4ee5\u4fc3\u8fdb\u6211\u4eec\u5bf9\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u578b\u8bf4\u670d\u53ca\u5176\u793e\u4f1a\u5f71\u54cd\u7684\u7406\u89e3\u3002||\n", "2410.02596": "|**2024-10-03**|[Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks](http://arxiv.org/abs/2410.02596)|null|\u751f\u6210\u6d41\u7f51\u7edc (GFlowNets) \u662f\u4e00\u7c7b\u65b0\u9896\u7684\u751f\u6210\u6a21\u578b\uff0c\u65e8\u5728\u4ece\u975e\u89c4\u8303\u5316\u5206\u5e03\u4e2d\u91c7\u6837\uff0c\u5e76\u5728\u5404\u79cd\u91cd\u8981\u4efb\u52a1\u4e2d\u5f97\u5230\u5e94\u7528\uff0c\u5176\u8bad\u7ec3\u7b97\u6cd5\u5f15\u8d77\u4e86\u4eba\u4eec\u6781\u5927\u7684\u7814\u7a76\u5174\u8da3\u3002\u901a\u5e38\uff0cGFlowNets \u7684\u8bad\u7ec3\u662f\u901a\u8fc7\u5c06\u91c7\u6837\u7684\u8bad\u7ec3\u5bf9\u8c61\u4e0a\u7684\u524d\u5411\u6d41\u4e0e\u53cd\u5411\u6d41\u8fdb\u884c\u62df\u5408\u6765\u5b9e\u73b0\u7684\u3002\u5148\u524d\u7684\u5de5\u4f5c\u91cd\u70b9\u5173\u6ce8\u8bad\u7ec3\u5bf9\u8c61\u7684\u9009\u62e9\u3001\u53c2\u6570\u5316\u3001\u91c7\u6837\u548c\u91cd\u91c7\u6837\u7b56\u7565\u4ee5\u53ca\u53cd\u5411\u7b56\u7565\uff0c\u65e8\u5728\u589e\u5f3a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u3001\u63a2\u7d22\u6216\u5229\u7528\u3002\u7136\u800c\uff0c\u56de\u5f52\u635f\u5931\u7684\u9009\u62e9\u5374\u88ab\u5ffd\u89c6\u4e86\uff0c\u800c\u5b83\u6781\u5927\u5730\u5f71\u54cd\u4e86\u8bad\u7ec3\u4e0d\u8db3\u7b56\u7565\u7684\u63a2\u7d22\u548c\u5229\u7528\u884c\u4e3a\u3002\u7531\u4e8e\u7f3a\u4e4f\u5bf9\u9009\u62e9\u5408\u9002\u7684\u56de\u5f52\u635f\u5931\u7684\u7406\u8bba\u7406\u89e3\uff0c\u5927\u591a\u6570\u73b0\u6709\u7b97\u6cd5\u901a\u8fc7\u6700\u5c0f\u5316\u5bf9\u6570\u7a7a\u95f4\u4e2d\u524d\u5411\u6d41\u548c\u53cd\u5411\u6d41\u7684\u5e73\u65b9\u8bef\u5dee\u6765\u8bad\u7ec3\u6d41\u7f51\u7edc\uff0c\u5373\u4f7f\u7528\u4e8c\u6b21\u56de\u5f52\u635f\u5931\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4e25\u683c\u8bc1\u660e\u4e86\u4e0d\u540c\u7684\u56de\u5f52\u635f\u5931\u5bf9\u5e94\u4e8e\u7279\u5b9a\u7684\u6563\u5ea6\u5ea6\u91cf\uff0c\u8fd9\u4f7f\u6211\u4eec\u80fd\u591f\u6839\u636e\u76f8\u5e94\u6563\u5ea6\u5ea6\u91cf\u7684\u671f\u671b\u5c5e\u6027\u6765\u8bbe\u8ba1\u548c\u5206\u6790\u56de\u5f52\u635f\u5931\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u4e24\u4e2a\u5173\u952e\u5c5e\u6027\uff1a\u96f6\u5f3a\u5236\u548c\u96f6\u907f\u514d\uff0c\u524d\u8005\u4fc3\u8fdb\u5229\u7528\u548c\u66f4\u9ad8\u7684\u5956\u52b1\uff0c\u800c\u540e\u8005\u9f13\u52b1\u63a2\u7d22\u5e76\u589e\u5f3a\u591a\u6837\u6027\u3002\u57fa\u4e8e\u6211\u4eec\u7684\u7406\u8bba\u6846\u67b6\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e09\u79cd\u65b0\u7684\u56de\u5f52\u635f\u5931\uff0c\u5373 Shifted-Cosh\u3001Linex(1/2) \u548c Linex(1)\u3002\u6211\u4eec\u901a\u8fc7\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u5b83\u4eec\uff1a\u8d85\u7f51\u683c\u3001\u4f4d\u5e8f\u5217\u751f\u6210\u548c\u5206\u5b50\u751f\u6210\u3002\u6211\u4eec\u63d0\u51fa\u7684\u635f\u5931\u51fd\u6570\u4e0e\u5927\u591a\u6570\u73b0\u6709\u8bad\u7ec3\u7b97\u6cd5\u517c\u5bb9\uff0c\u5e76\u5728\u6536\u655b\u901f\u5ea6\u3001\u6837\u672c\u591a\u6837\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u663e\u8457\u63d0\u9ad8\u4e86\u7b97\u6cd5\u7684\u6027\u80fd\u3002||\n", "2410.02548": "|**2024-10-03**|[Local Flow Matching Generative Models](http://arxiv.org/abs/2410.02548)|null|\u6d41\u5339\u914d\uff08FM\uff09\u662f\u4e00\u79cd\u65e0\u9700\u6a21\u62df\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u8fde\u7eed\u4e14\u53ef\u9006\u7684\u6d41\uff0c\u4ee5\u5728\u4e24\u4e2a\u5206\u5e03\u4e4b\u95f4\u8fdb\u884c\u63d2\u503c\uff0c\u7279\u522b\u662f\u5728\u751f\u6210\u5efa\u6a21\u4e2d\u4ece\u566a\u58f0\u751f\u6210\u6570\u636e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u5c40\u90e8\u6d41\u5339\u914d\uff08LFM\uff09\uff0c\u5b83\u5b66\u4e60\u4e00\u7cfb\u5217 FM \u5b50\u6a21\u578b\uff0c\u6bcf\u4e2a\u5b50\u6a21\u578b\u90fd\u5339\u914d\u4e00\u4e2a\u6269\u6563\u8fc7\u7a0b\uff0c\u76f4\u5230\u6570\u636e\u5230\u566a\u58f0\u65b9\u5411\u4e0a\u7684\u6b65\u957f\u65f6\u95f4\u3002\u5728\u6bcf\u4e2a\u6b65\u9aa4\u4e2d\uff0c\u5b50\u6a21\u578b\u8981\u63d2\u503c\u7684\u4e24\u4e2a\u5206\u5e03\u6bd4\u6570\u636e\u4e0e\u566a\u58f0\u66f4\u63a5\u8fd1\uff0c\u8fd9\u4f7f\u5f97\u53ef\u4ee5\u4f7f\u7528\u66f4\u5c0f\u7684\u6a21\u578b\u8fdb\u884c\u66f4\u5feb\u7684\u8bad\u7ec3\u3002LFM \u7684\u9010\u6b65\u7ed3\u6784 naturally lends itself to distillation\uff0c\u5e76\u4e14\u53ef\u4ee5\u91c7\u7528\u4e0d\u540c\u7684\u84b8\u998f\u6280\u672f\u6765\u52a0\u901f\u751f\u6210\u3002\u7406\u8bba\u4e0a\uff0c\u6211\u4eec\u6839\u636e\u751f\u6210\u7684\u548c\u771f\u5b9e\u6570\u636e\u5206\u5e03\u4e4b\u95f4\u7684 $\\chi^2$ \u6563\u5ea6\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u6d41\u6a21\u578b\u7684\u751f\u6210\u4fdd\u8bc1\u3002\u5728\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 LFM \u4e0e FM \u76f8\u6bd4\uff0c\u5728\u8868\u683c\u6570\u636e\u548c\u56fe\u50cf\u6570\u636e\u96c6\u7684\u65e0\u6761\u4ef6\u751f\u6210\u4ee5\u53ca\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6761\u4ef6\u751f\u6210\u65b9\u9762\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u8bad\u7ec3\u6548\u7387\u548c\u66f4\u5177\u7ade\u4e89\u529b\u7684\u751f\u6210\u6027\u80fd\u3002||\n", "2410.03665": "|**2024-10-04**|[Estimating Body and Hand Motion in an Ego-sensed World](http://arxiv.org/abs/2410.03665)|null|\u6211\u4eec\u63d0\u51fa\u4e86EgoAllo\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5934\u6234\u5f0f\u8bbe\u5907\u7684\u4eba\u4f53\u52a8\u4f5c\u4f30\u8ba1\u7cfb\u7edf\u3002EgoAllo\u4ec5\u4f7f\u7528\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684SLAM\u59ff\u6001\u548c\u56fe\u50cf\uff0c\u5f15\u5bfc\u4ece\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4e2d\u91c7\u6837\uff0c\u4ee5\u4f30\u8ba1\u6355\u6349\u4f69\u6234\u8005\u5728\u573a\u666f\u7684\u5168\u5c40\u5750\u6807\u7cfb\u4e2d\u7684\u52a8\u4f5c\u76843D\u8eab\u4f53\u59ff\u6001\u3001\u8eab\u9ad8\u548c\u624b\u90e8\u53c2\u6570\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u7684\u5173\u952e\u89c1\u89e3\u5728\u4e8e\u8868\u793a\uff1a\u6211\u4eec\u63d0\u51fa\u4e86\u7528\u4e8e\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0d\u53d8\u6027\u6807\u51c6\uff0c\u5e76\u7531\u6b64\u63a8\u5bfc\u51fa\u4e00\u79cd\u5934\u90e8\u8fd0\u52a8\u6761\u4ef6\u53c2\u6570\u5316\uff0c\u8be5\u53c2\u6570\u5316\u5c06\u4f30\u8ba1\u7cbe\u5ea6\u63d0\u9ad8\u4e8618%\u3002\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u6211\u4eec\u7cfb\u7edf\u4f30\u8ba1\u7684\u8eab\u4f53\u5982\u4f55\u6539\u8fdb\u624b\u90e8\u4f30\u8ba1\uff1a\u4e0e\u5608\u6742\u7684\u5355\u76ee\u4f30\u8ba1\u76f8\u6bd4\uff0c\u7531\u6b64\u4ea7\u751f\u7684\u8fd0\u52a8\u5b66\u548c\u65f6\u95f4\u7ea6\u675f\u4f7f\u624b\u90e8\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e\u4e8640%\u4ee5\u4e0a\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://egoallo.github.io/||\n", "2410.03655": "|**2024-10-04**|[Geometric Representation Condition Improves Equivariant Molecule Generation](http://arxiv.org/abs/2410.03655)|null|\u8fd1\u5e74\u6765\uff0c\u5206\u5b50\u751f\u6210\u6a21\u578b\u7684\u8fdb\u6b65\u5c55\u73b0\u4e86\u5176\u5728\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u836f\u7269\u8bbe\u8ba1\u9886\u57df\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u5206\u5b50\u65b9\u9762\u7ecf\u5e38\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5fc5\u987b\u6ee1\u8db3\u7279\u5b9a\u5206\u5b50\u7279\u6027\u7684\u6761\u4ef6\u751f\u6210\u573a\u666f\u4e0b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 GeoRCG\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u8fc7\u6574\u5408\u51e0\u4f55\u8868\u793a\u6761\u4ef6\u6765\u589e\u5f3a\u5206\u5b50\u751f\u6210\u6a21\u578b\u6027\u80fd\u7684\u901a\u7528\u6846\u67b6\u3002\u6211\u4eec\u5c06\u5206\u5b50\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u9996\u5148\uff0c\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u7684\u51e0\u4f55\u8868\u793a\uff1b\u5176\u6b21\uff0c\u6839\u636e\u8be5\u8868\u793a\u751f\u6210\u5206\u5b50\u3002\u4e0e\u76f4\u63a5\u751f\u6210\u5206\u5b50\u76f8\u6bd4\uff0c\u5728\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u76f8\u5bf9\u5bb9\u6613\u7684\u8868\u793a\uff0c\u4ee5\u66f4\u76ee\u6807\u5bfc\u5411\u548c\u66f4\u5feb\u7684\u901f\u5ea6\u5f15\u5bfc\u7b2c\u4e8c\u9636\u6bb5\u751f\u6210\u9ad8\u8d28\u91cf\u5206\u5b50\u3002\u5229\u7528 EDM \u4f5c\u4e3a\u57fa\u7840\u751f\u6210\u5668\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684 QM9 \u548c GEOM-DRUG \u6570\u636e\u96c6\u4e0a\u7684\u65e0\u6761\u4ef6\u5206\u5b50\u751f\u6210\u8d28\u91cf\u6709\u663e\u8457\u63d0\u9ad8\u3002\u66f4\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6761\u4ef6\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u6211\u4eec\u7684\u6846\u67b6\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5e73\u5747 31% \u7684\u6027\u80fd\u63d0\u5347\uff0c\u8fd9\u51f8\u663e\u4e86\u4ee5\u8bed\u4e49\u4e30\u5bcc\u7684\u51e0\u4f55\u8868\u793a\u4e3a\u6761\u4ef6\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u4e2d\u4ee5\u5355\u4e2a\u5c5e\u6027\u503c\u4e3a\u6761\u4ef6\u7684\u4f18\u8d8a\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u53d1\u73b0\uff0c\u5728\u8fd9\u79cd\u8868\u793a\u6307\u5bfc\u4e0b\uff0c\u6269\u6563\u6b65\u9aa4\u7684\u6570\u91cf\u53ef\u4ee5\u51cf\u5c11\u5230\u4ec5 100 \u6b65\uff0c\u540c\u65f6\u4fdd\u6301\u6bd4 1000 \u6b65\u66f4\u9ad8\u7684\u751f\u6210\u8d28\u91cf\uff0c\u4ece\u800c\u663e\u8457\u52a0\u901f\u4e86\u751f\u6210\u8fc7\u7a0b\u3002||\n", "2410.03640": "|**2024-10-04**|[Real-World Benchmarks Make Membership Inference Attacks Fail on Diffusion Models](http://arxiv.org/abs/2410.03640)|**[link](https://github.com/caradryanl/copymark)**|\u6269\u6563\u6a21\u578b\u7684\u6210\u5458\u63a8\u65ad\u653b\u51fb (MIA) \u5df2\u6210\u4e3a\u6f5c\u5728\u8bc1\u636e\uff0c\u8868\u660e\u5728\u8bad\u7ec3\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e2d\u5b58\u5728\u672a\u7ecf\u6388\u6743\u7684\u6570\u636e\u4f7f\u7528\u3002\u8fd9\u4e9b\u653b\u51fb\u65e8\u5728\u68c0\u6d4b\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u662f\u5426\u5b58\u5728\u7279\u5b9a\u56fe\u50cf\u3002\u6211\u4eec\u7684\u7814\u7a76\u6df1\u5165\u8bc4\u4f30\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u6700\u5148\u8fdb\u7684 MIA\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709 MIA \u8bc4\u4f30\u4e2d\u7684\u4e25\u91cd\u7f3a\u9677\u548c\u8fc7\u4e8e\u4e50\u89c2\u7684\u6027\u80fd\u4f30\u8ba1\u3002\u6211\u4eec\u4ecb\u7ecd\u4e86 CopyMark\uff0c\u8fd9\u662f\u4e00\u4e2a\u66f4\u73b0\u5b9e\u7684 MIA \u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b83\u901a\u8fc7\u652f\u6301\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u3001\u65e0\u504f\u6570\u636e\u96c6\u548c\u516c\u5e73\u7684\u8bc4\u4f30\u7ba1\u9053\u6765\u533a\u5206\u81ea\u5df1\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5f53\u524d MIA \u65b9\u6cd5\u7684\u6709\u6548\u6027\u5728\u8fd9\u4e9b\u66f4\u5b9e\u9645\u7684\u6761\u4ef6\u4e0b\u4f1a\u663e\u7740\u964d\u4f4e\u3002\u6839\u636e\u6211\u4eec\u7684\u7ed3\u679c\uff0c\u6211\u4eec\u63d0\u9192\uff0cMIA \u76ee\u524d\u7684\u72b6\u6001\u5e76\u4e0d\u662f\u8bc6\u522b\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e2d\u672a\u7ecf\u6388\u6743\u6570\u636e\u4f7f\u7528\u7684\u53ef\u9760\u65b9\u6cd5\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6211\u4eec\u662f\u7b2c\u4e00\u4e2a\u53d1\u73b0 MIA \u5bf9\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\u9ad8\u4f30\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\u4ee5\u8fdb\u884c\u66f4\u73b0\u5b9e\u7684\u8bc4\u4f30\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 GitHub \u4e0a\u83b7\u53d6\uff1a\\url{https://github.com/caradryanl/CopyMark}\u3002||\n", "2410.03634": "|**2024-10-04**|[Conditional Enzyme Generation Using Protein Language Models with Adapters](http://arxiv.org/abs/2410.03634)|null|\u4ee5\u671f\u671b\u7684\u529f\u80fd\u548c/\u6216\u7279\u6027\u4e3a\u6761\u4ef6\u751f\u6210\u86cb\u767d\u8d28\u662f\u751f\u6210\u6a21\u578b\u7684\u5173\u952e\u76ee\u6807\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u63d0\u793a\u7684\u65b9\u6cd5\u53ef\u4ee5\u751f\u6210\u4ee5\u76ee\u6807\u529f\u80fd\uff08\u4f8b\u5982\u6240\u9700\u7684\u9176\u5bb6\u65cf\uff09\u4e3a\u6761\u4ef6\u7684\u86cb\u767d\u8d28\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4ec5\u9650\u4e8e\u7b80\u5355\u7684\u6807\u8bb0\u5316\u6761\u4ef6\uff0c\u5e76\u4e14\u5c1a\u672a\u663e\u793a\u51fa\u5bf9\u672a\u89c1\u529f\u80fd\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 ProCALM\uff08\u86cb\u767d\u8d28\u6761\u4ef6\u81ea\u9002\u5e94\u8bed\u8a00\u6a21\u578b\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u4f7f\u7528\u9002\u914d\u5668\u5bf9\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6761\u4ef6\u751f\u6210\u86cb\u767d\u8d28\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u5bf9 ProCALM \u7684\u5177\u4f53\u5b9e\u73b0\u6d89\u53ca\u5fae\u8c03 ProGen2\uff0c\u4ee5\u7ed3\u5408\u9176\u529f\u80fd\u548c\u5206\u7c7b\u6cd5\u7684\u6761\u4ef6\u8868\u793a\u3002ProCALM \u5728\u6709\u6761\u4ef6\u5730\u4ece\u76ee\u6807\u9176\u5bb6\u65cf\u751f\u6210\u5e8f\u5217\u65b9\u9762\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5339\u914d\u3002\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u662f\uff0c\u5b83\u8fd8\u53ef\u4ee5\u5728\u9176\u529f\u80fd\u548c\u5206\u7c7b\u6cd5\u7684\u8054\u5408\u5206\u5e03\u5185\u751f\u6210\uff0c\u5e76\u4e14\u53ef\u4ee5\u6cdb\u5316\u5230\u7a00\u6709\u548c\u672a\u89c1\u8fc7\u7684\u9176\u5bb6\u65cf\u548c\u5206\u7c7b\u6cd5\u3002\u603b\u7684\u6765\u8bf4\uff0cProCALM \u662f\u4e00\u79cd\u7075\u6d3b\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u9884\u8ba1\u5b83\u53ef\u4ee5\u6269\u5c55\u5230\u5e7f\u6cdb\u7684\u751f\u6210\u8bed\u8a00\u6a21\u578b\u3002||\n", "2410.03601": "|**2024-10-04**|[How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework](http://arxiv.org/abs/2410.03601)|null|\u79bb\u6563\u6269\u6563\u6a21\u578b\u56e0\u5176\u80fd\u591f\u5bf9\u5177\u6709\u6613\u4e8e\u5904\u7406\u7684\u91c7\u6837\u548c\u63a8\u7406\u7684\u590d\u6742\u5206\u5e03\u8fdb\u884c\u5efa\u6a21\u800c\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u8bef\u5dee\u5206\u6790\u4ecd\u7136\u7f3a\u4e4f\u6df1\u5165\u7684\u7406\u89e3\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e L\u00e9vy \u578b\u968f\u673a\u79ef\u5206\u7684\u79bb\u6563\u6269\u6563\u6a21\u578b\u8bef\u5dee\u5206\u6790\u7efc\u5408\u6846\u67b6\u3002\u901a\u8fc7\u5c06\u6cca\u677e\u968f\u673a\u6d4b\u5ea6\u63a8\u5e7f\u5230\u5177\u6709\u65f6\u95f4\u65e0\u5173\u548c\u72b6\u6001\u76f8\u5173\u5f3a\u5ea6\u7684\u6d4b\u5ea6\uff0c\u6211\u4eec\u4e25\u683c\u5efa\u7acb\u4e86\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u968f\u673a\u79ef\u5206\u516c\u5f0f\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5e94\u7684\u6d4b\u5ea6\u53d8\u5316\u5b9a\u7406\uff0c\u8fd9\u4e9b\u5b9a\u7406\u4e0e It\u00f4 \u79ef\u5206\u548c Girsanov \u5b9a\u7406\u53ca\u5176\u8fde\u7eed\u5bf9\u5e94\u7269\u6709\u7740\u60ca\u4eba\u7684\u76f8\u4f3c\u4e4b\u5904\u3002\u6211\u4eec\u7684\u6846\u67b6\u7edf\u4e00\u5e76\u52a0\u5f3a\u4e86\u5f53\u524d\u5173\u4e8e\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u7406\u8bba\u7ed3\u679c\uff0c\u5e76\u83b7\u5f97\u4e86 KL \u6563\u5ea6\u4e2d \u03c4-leaping \u65b9\u6848\u7684\u7b2c\u4e00\u4e2a\u8bef\u5dee\u754c\u3002\u901a\u8fc7\u660e\u786e\u8bc6\u522b\u8bef\u5dee\u6765\u6e90\uff0c\u6211\u4eec\u7684\u5206\u6790\u4e3a\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u6570\u5b66\u6027\u8d28\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u79bb\u6563\u6269\u6563\u6a21\u578b\u5e94\u7528\u7684\u9ad8\u6548\u548c\u51c6\u786e\u7b97\u6cd5\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002||\n", "2410.03558": "|**2024-10-04**|[Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features](http://arxiv.org/abs/2410.03558)|**[link](https://github.com/darkbblue/generic-diffusion-feature)**|\u6269\u6563\u6a21\u578b\u6700\u521d\u662f\u4e3a\u56fe\u50cf\u751f\u6210\u800c\u8bbe\u8ba1\u7684\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5176\u4e3b\u5e72\u5185\u90e8\u7684\u4fe1\u53f7\uff08\u79f0\u4e3a\u6fc0\u6d3b\uff09\u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u5bc6\u96c6\u7279\u5f81\uff0c\u7528\u4e8e\u5404\u79cd\u5224\u522b\u4efb\u52a1\uff0c\u4f8b\u5982\u8bed\u4e49\u5206\u5272\u3002\u5728\u4f17\u591a\u6fc0\u6d3b\u4e2d\uff0c\u9009\u62e9\u4e00\u4e2a\u6709\u6548\u7684\u5c0f\u5b50\u96c6\u662f\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\u3002\u4e3a\u6b64\uff0c\u8be5\u9886\u57df\u7684\u65e9\u671f\u7814\u7a76\u5bf9\u6fc0\u6d3b\u7684\u5224\u522b\u80fd\u529b\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u7684\u5b9a\u91cf\u6bd4\u8f83\u3002\u7136\u800c\uff0c\u6211\u4eec\u53d1\u73b0\u8bb8\u591a\u6f5c\u5728\u7684\u6fc0\u6d3b\u8fd8\u6ca1\u6709\u88ab\u8bc4\u4f30\uff0c\u4f8b\u5982\u7528\u4e8e\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\u7684\u67e5\u8be2\u548c\u952e\u3002\u6b64\u5916\uff0c\u6269\u6563\u67b6\u6784\u7684\u6700\u65b0\u8fdb\u5c55\u5e26\u6765\u4e86\u8bb8\u591a\u65b0\u7684\u6fc0\u6d3b\uff0c\u4f8b\u5982\u5d4c\u5165\u5f0f ViT \u6a21\u5757\u4e2d\u7684\u6fc0\u6d3b\u3002\u4e24\u8005\u7ed3\u5408\u5728\u4e00\u8d77\uff0c\u6fc0\u6d3b\u9009\u62e9\u4ecd\u7136\u662f\u4e00\u4e2a\u5c1a\u672a\u89e3\u51b3\u4f46\u88ab\u5ffd\u89c6\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u66f4\u8fdb\u4e00\u6b65\uff0c\u8bc4\u4f30\u4e86\u66f4\u5e7f\u6cdb\u7684\u6fc0\u6d3b\u3002\u8003\u8651\u5230\u6fc0\u6d3b\u7684\u663e\u8457\u589e\u52a0\uff0c\u5168\u9762\u7684\u5b9a\u91cf\u6bd4\u8f83\u5df2\u4e0d\u518d\u53ef\u884c\u3002\u76f8\u53cd\uff0c\u6211\u4eec\u8bd5\u56fe\u4e86\u89e3\u8fd9\u4e9b\u6fc0\u6d3b\u7684\u5c5e\u6027\uff0c\u4ee5\u4fbf\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u7684\u5b9a\u6027\u8bc4\u4f30\u9884\u5148\u8fc7\u6ee4\u6389\u660e\u663e\u8f83\u5dee\u7684\u6fc0\u6d3b\u3002\u7ecf\u8fc7\u4ed4\u7ec6\u5206\u6790\uff0c\u6211\u4eec\u53d1\u73b0\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\u7684\u4e09\u4e2a\u5c5e\u6027\uff0c\u4f7f\u8fd9\u9879\u7814\u7a76\u80fd\u591f\u8d85\u8d8a\u7279\u5b9a\u7684\u6a21\u578b\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u9488\u5bf9\u51e0\u79cd\u6d41\u884c\u7684\u6269\u6563\u6a21\u578b\u63d0\u51fa\u4e86\u6709\u6548\u7684\u7279\u5f81\u9009\u62e9\u89e3\u51b3\u65b9\u6848\u3002\u6700\u540e\uff0c\u8de8\u591a\u4e2a\u5224\u522b\u4efb\u52a1\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e SOTA \u7ade\u4e89\u5bf9\u624b\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/Darkbblue/generic-diffusion-feature \u83b7\u53d6\u3002||\n", "2410.03535": "|**2024-10-04**|[NRGBoost: Energy-Based Generative Boosted Trees](http://arxiv.org/abs/2410.03535)|null|\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u975e\u7ed3\u6784\u5316\u6570\u636e\u9886\u57df\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u57fa\u4e8e\u6811\u7684\u65b9\u6cd5\uff0c\u5982\u968f\u673a\u68ee\u6797\uff08RF\uff09\u548c\u68af\u5ea6\u63d0\u5347\u51b3\u7b56\u6811\uff08GBDT\uff09\uff0c\u4ecd\u7136\u662f\u5904\u7406\u8868\u683c\u6570\u636e\u5224\u522b\u4efb\u52a1\u7684\u4e3b\u529b\u519b\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u8fd9\u4e9b\u6d41\u884c\u7b97\u6cd5\u7684\u751f\u6210\u5f0f\u6269\u5c55\uff0c\u91cd\u70b9\u662f\u5bf9\u6570\u636e\u5bc6\u5ea6\uff08\u76f4\u5230\u5f52\u4e00\u5316\u5e38\u6570\uff09\u8fdb\u884c\u663e\u5f0f\u5efa\u6a21\uff0c\u4ece\u800c\u652f\u6301\u9664\u91c7\u6837\u4e4b\u5916\u7684\u5176\u4ed6\u5e94\u7528\u3002\u4f5c\u4e3a\u6211\u4eec\u7684\u4e3b\u8981\u8d21\u732e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u7684\u751f\u6210\u5f0f\u63d0\u5347\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u7c7b\u4f3c\u4e8e\u5728 XGBoost \u7b49\u6d41\u884c\u8f6f\u4ef6\u5305\u4e2d\u5b9e\u73b0\u7684\u4e8c\u9636\u63d0\u5347\u3002\u6211\u4eec\u8868\u660e\uff0c\u5c3d\u7ba1\u4ea7\u751f\u4e86\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u4efb\u4f55\u8f93\u5165\u53d8\u91cf\u7684\u63a8\u7406\u4efb\u52a1\u7684\u751f\u6210\u6a21\u578b\uff0c\u4f46\u6211\u4eec\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u8bb8\u591a\u771f\u5b9e\u4e16\u754c\u7684\u8868\u683c\u6570\u636e\u96c6\u4e0a\u53ef\u4ee5\u5b9e\u73b0\u4e0e GBDT \u76f8\u4f3c\u7684\u5224\u522b\u6027\u80fd\uff0c\u4f18\u4e8e\u5176\u4ed6\u751f\u6210\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u6211\u4eec\u4e5f\u5c55\u793a\u4e86\u5b83\u5728\u91c7\u6837\u65b9\u9762\u4e5f\u5177\u6709\u4e0e\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u7ade\u4e89\u529b\u3002||\n", "2410.03494": "|**2024-10-04**|[Generative Artificial Intelligence for Navigating Synthesizable Chemical Space](http://arxiv.org/abs/2410.03494)|**[link](https://github.com/wenhao-gao/synformer)**|\u6211\u4eec\u63a8\u51fa\u4e86 SynFormer\uff0c\u8fd9\u662f\u4e00\u4e2a\u751f\u6210\u5f0f\u5efa\u6a21\u6846\u67b6\uff0c\u65e8\u5728\u6709\u6548\u5730\u63a2\u7d22\u548c\u5bfc\u822a\u53ef\u5408\u6210\u5316\u5b66\u7a7a\u95f4\u3002\u4e0e\u4f20\u7edf\u7684\u5206\u5b50\u751f\u6210\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u4e3a\u5206\u5b50\u751f\u6210\u5408\u6210\u8def\u7ebf\uff0c\u4ee5\u786e\u4fdd\u8bbe\u8ba1\u5177\u6709\u5408\u6210\u53ef\u884c\u6027\u3002\u901a\u8fc7\u7ed3\u5408\u53ef\u6269\u5c55\u7684 Transformer \u67b6\u6784\u548c\u7528\u4e8e\u6784\u5efa\u5757\u9009\u62e9\u7684\u6269\u6563\u6a21\u5757\uff0cSynFormer \u5728\u53ef\u5408\u6210\u5206\u5b50\u8bbe\u8ba1\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u578b\u3002\u6211\u4eec\u901a\u8fc7\u4e24\u4e2a\u5173\u952e\u5e94\u7528\u5c55\u793a\u4e86 SynFormer \u7684\u6709\u6548\u6027\uff1a(1) \u5c40\u90e8\u5316\u5b66\u7a7a\u95f4\u63a2\u7d22\uff0c\u5176\u4e2d\u6a21\u578b\u751f\u6210\u53c2\u8003\u5206\u5b50\u7684\u53ef\u5408\u6210\u7c7b\u4f3c\u7269\uff0c\u4ee5\u53ca (2) \u5168\u5c40\u5316\u5b66\u7a7a\u95f4\u63a2\u7d22\uff0c\u5176\u4e2d\u6a21\u578b\u65e8\u5728\u6839\u636e\u9ed1\u76d2\u6027\u8d28\u9884\u6d4b\u9884\u8a00\u673a\u8bc6\u522b\u6700\u4f73\u5206\u5b50\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u968f\u7740\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90\u53ef\u7528\u800c\u63d0\u9ad8\u6027\u80fd\u6765\u8bc1\u660e\u6211\u4eec\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u3002\u901a\u8fc7\u516c\u5f00\u6211\u4eec\u7684\u4ee3\u7801\u548c\u8bad\u7ec3\u6a21\u578b\uff0c\u6211\u4eec\u5e0c\u671b SynFormer \u80fd\u591f\u5728\u836f\u7269\u53d1\u73b0\u548c\u6750\u6599\u79d1\u5b66\u7684\u5e94\u7528\u4e2d\u5f97\u5230\u5e94\u7528\u3002||\n", "2410.03463": "|**2024-10-04**|[Diffusion State-Guided Projected Gradient for Inverse Problems](http://arxiv.org/abs/2410.03463)|null|\u6269\u6563\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u5728\u5b66\u4e60\u7528\u4e8e\u89e3\u51b3\u53cd\u95ee\u9898\u7684\u5148\u9a8c\u6570\u636e\u65b9\u9762\u975e\u5e38\u6709\u6548\u3002\u5b83\u4eec\u5229\u7528\u6269\u6563\u91c7\u6837\u6b65\u9aa4\u6765\u5f15\u5165\u6570\u636e\u5148\u9a8c\uff0c\u540c\u65f6\u5728\u6bcf\u4e2a\u6b65\u9aa4\u4e2d\u4f7f\u7528\u6d4b\u91cf\u5f15\u5bfc\u68af\u5ea6\u6765\u65bd\u52a0\u6570\u636e\u4e00\u81f4\u6027\u3002\u5bf9\u4e8e\u4e00\u822c\u7684\u53cd\u95ee\u9898\uff0c\u5f53\u4f7f\u7528\u65e0\u6761\u4ef6\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u65f6\uff0c\u7531\u4e8e\u6d4b\u91cf\u4f3c\u7136\u662f\u96be\u4ee5\u5904\u7406\u7684\uff0c\u56e0\u6b64\u9700\u8981\u8fdb\u884c\u8fd1\u4f3c\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u4e0d\u51c6\u786e\u7684\u540e\u9a8c\u91c7\u6837\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u7531\u4e8e\u5b83\u4eec\u7684\u8fd1\u4f3c\u6027\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u65e0\u6cd5\u5728\u7531\u6269\u6563\u5148\u9a8c\u5b9a\u4e49\u7684\u6570\u636e\u6d41\u5f62\u4e0a\u4fdd\u7559\u751f\u6210\u8fc7\u7a0b\uff0c\u4ece\u800c\u5bfc\u81f4\u56fe\u50cf\u6062\u590d\u7b49\u5e94\u7528\u4e2d\u7684\u4f2a\u5f71\u3002\u4e3a\u4e86\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u5728\u89e3\u51b3\u53cd\u95ee\u9898\u65b9\u9762\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6269\u6563\u72b6\u6001\u5f15\u5bfc\u6295\u5f71\u68af\u5ea6\uff08DiffStateGrad\uff09\uff0c\u5b83\u5c06\u6d4b\u91cf\u68af\u5ea6\u6295\u5f71\u5230\u4e00\u4e2a\u5b50\u7a7a\u95f4\u4e0a\uff0c\u8be5\u5b50\u7a7a\u95f4\u662f\u6269\u6563\u8fc7\u7a0b\u4e2d\u95f4\u72b6\u6001\u7684\u4f4e\u79e9\u8fd1\u4f3c\u3002DiffStateGrad\u4f5c\u4e3a\u4e00\u4e2a\u6a21\u5757\uff0c\u53ef\u4ee5\u6dfb\u52a0\u5230\u5404\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u53cd\u6c42\u89e3\u5668\u4e2d\uff0c\u4ee5\u6539\u8fdb\u5bf9\u5148\u9a8c\u6d41\u5f62\u4e0a\u6269\u6563\u8fc7\u7a0b\u7684\u4fdd\u7559\uff0c\u5e76\u6ee4\u9664\u4ea7\u751f\u4f2a\u5f71\u7684\u6210\u5206\u3002\u6211\u4eec\u5f3a\u8c03\uff0cDiffStateGrad\u63d0\u9ad8\u4e86\u6269\u6563\u6a21\u578b\u5728\u6d4b\u91cf\u5f15\u5bfc\u6b65\u957f\u548c\u566a\u58f0\u9009\u62e9\u65b9\u9762\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bc1\u660e\u4e86DiffStateGrad\u5728\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u56fe\u50cf\u6062\u590d\u53cd\u95ee\u9898\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u6c34\u5e73\u3002||\n", "2410.03459": "|**2024-10-04**|[Generative Semantic Communication for Text-to-Speech Synthesis](http://arxiv.org/abs/2410.03459)|null|\u8bed\u4e49\u901a\u4fe1\u662f\u4e00\u79cd\u5f88\u6709\u524d\u666f\u7684\u6280\u672f\uff0c\u5b83\u53ea\u4f20\u8f93\u6e90\u6570\u636e\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u9ad8\u901a\u4fe1\u6548\u7387\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u8bed\u4e49\u901a\u4fe1\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u6570\u636e\u91cd\u5efa\u4efb\u52a1\u4e0a\uff0c\u5bf9\u4e8e\u6587\u672c\u5230\u8bed\u97f3\uff08TTS\uff09\u5408\u6210\u7b49\u65b0\u5174\u7684\u751f\u6210\u4efb\u52a1\u6765\u8bf4\uff0c\u6548\u7387\u53ef\u80fd\u4e0d\u9ad8\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u672c\u6587\u5229\u7528\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684TTS\u5408\u6210\u751f\u6210\u5f0f\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\u3002\u9996\u5148\uff0c\u6211\u4eec\u5229\u7528\u9884\u5148\u8bad\u7ec3\u597d\u7684\u5927\u578b\u8bed\u97f3\u6a21\u578bWavLM\u548c\u6b8b\u5dee\u77e2\u91cf\u91cf\u5316\u65b9\u6cd5\uff0c\u5206\u522b\u5728\u53d1\u9001\u7aef\u548c\u63a5\u6536\u7aef\u6784\u5efa\u4e86\u4e24\u4e2a\u8bed\u4e49\u77e5\u8bc6\u5e93\uff08KB\uff09\u3002\u53d1\u9001\u7aef\u7684KB\u80fd\u591f\u6709\u6548\u5730\u63d0\u53d6\u8bed\u4e49\uff0c\u800c\u63a5\u6536\u7aef\u7684KB\u5219\u6709\u52a9\u4e8e\u903c\u771f\u7684\u8bed\u97f3\u5408\u6210\u3002\u7136\u540e\uff0c\u6211\u4eec\u91c7\u7528Transformer\u7f16\u7801\u5668\u548c\u6269\u6563\u6a21\u578b\u6765\u5b9e\u73b0\u9ad8\u6548\u7684\u8bed\u4e49\u7f16\u7801\uff0c\u800c\u4e0d\u4f1a\u5f15\u5165\u663e\u8457\u7684\u901a\u4fe1\u5f00\u9500\u3002\u6700\u540e\uff0c\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u52a0\u6027\u9ad8\u65af\u767d\u566a\u58f0\u4fe1\u9053\u548c\u745e\u5229\u8870\u843d\u4fe1\u9053\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5728\u751f\u6210\u8bed\u97f3\u7684\u4fdd\u771f\u5ea6\u65b9\u9762\u90fd\u6bd4\u56db\u79cd\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u5f97\u591a\u3002||\n", "2410.05260": "|**2024-10-07**|[DART: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control](http://arxiv.org/abs/2410.05260)|null|\u6587\u672c\u6761\u4ef6\u5316\u4eba\u4f53\u52a8\u4f5c\u751f\u6210\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u4ea4\u4e92\uff0c\u8fd1\u5e74\u6765\u5907\u53d7\u6b22\u8fce\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u5355\u4e2a\u8f93\u5165\u8bed\u53e5\u751f\u6210\u7b80\u77ed\u3001\u5b64\u7acb\u7684\u52a8\u4f5c\u3002\u7136\u800c\uff0c\u4eba\u7c7b\u52a8\u4f5c\u662f\u8fde\u7eed\u7684\uff0c\u53ef\u4ee5\u6301\u7eed\u5f88\u957f\u65f6\u95f4\uff0c\u5e76\u627f\u8f7d\u7740\u4e30\u5bcc\u7684\u8bed\u4e49\u3002\u521b\u9020\u80fd\u591f\u7cbe\u786e\u54cd\u5e94\u6587\u672c\u63cf\u8ff0\u6d41\u7684\u957f\u671f\u3001\u590d\u6742\u52a8\u4f5c\uff0c\u7279\u522b\u662f\u5728\u5728\u7ebf\u548c\u5b9e\u65f6\u73af\u5883\u4e2d\uff0c\u4ecd\u7136\u662f\u4e00\u9879\u91cd\u5927\u6311\u6218\u3002\u6b64\u5916\uff0c\u5c06\u7a7a\u95f4\u7ea6\u675f\u7eb3\u5165\u6587\u672c\u6761\u4ef6\u5316\u52a8\u4f5c\u751f\u6210\u5e26\u6765\u4e86\u989d\u5916\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u5c06\u6587\u672c\u63cf\u8ff0\u6307\u5b9a\u7684\u52a8\u4f5c\u8bed\u4e49\u4e0e\u51e0\u4f55\u4fe1\u606f\uff08\u4f8b\u5982\u76ee\u6807\u4f4d\u7f6e\u548c 3D \u573a\u666f\u51e0\u4f55\u5f62\u72b6\uff09\u5bf9\u9f50\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DART\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u81ea\u56de\u5f52\u8fd0\u52a8\u57fa\u5143\u6a21\u578b\uff0c\u7528\u4e8e\u5b9e\u65f6\u6587\u672c\u9a71\u52a8\u7684\u8fd0\u52a8\u63a7\u5236\u3002\u6211\u4eec\u7684\u6a21\u578b DART \u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u6709\u6548\u5730\u5b66\u4e60\u4e86\u8054\u5408\u4f9d\u8d56\u4e8e\u8fd0\u52a8\u5386\u53f2\u548c\u6587\u672c\u8f93\u5165\u7684\u7d27\u51d1\u8fd0\u52a8\u57fa\u5143\u7a7a\u95f4\u3002\u901a\u8fc7\u6839\u636e\u5148\u524d\u5386\u53f2\u548c\u5f53\u524d\u6587\u672c\u8f93\u5165\u81ea\u56de\u5f52\u5730\u751f\u6210\u8fd0\u52a8\u57fa\u5143\uff0cDART \u53ef\u4ee5\u5b9e\u73b0\u7531\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u9a71\u52a8\u7684\u5b9e\u65f6\u3001\u8fde\u7eed\u52a8\u4f5c\u751f\u6210\u3002\u6b64\u5916\uff0c\u5b66\u4e60\u5230\u7684\u8fd0\u52a8\u57fa\u5143\u7a7a\u95f4\u5141\u8bb8\u7cbe\u786e\u7684\u7a7a\u95f4\u8fd0\u52a8\u63a7\u5236\uff0c\u6211\u4eec\u5c06\u5176\u5236\u5b9a\u4e3a\u6f5c\u5728\u566a\u58f0\u4f18\u5316\u95ee\u9898\u6216\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u3002\u6211\u4eec\u9488\u5bf9\u8fd9\u4e24\u79cd\u65b9\u6cd5\u63d0\u51fa\u4e86\u6709\u6548\u7684\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u6a21\u578b\u5728\u5404\u79cd\u8fd0\u52a8\u5408\u6210\u4efb\u52a1\u4e2d\u7684\u591a\u529f\u80fd\u6027\u548c\u5353\u8d8a\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8fd0\u52a8\u771f\u5b9e\u611f\u3001\u6548\u7387\u548c\u53ef\u63a7\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u3002\u89c6\u9891\u7ed3\u679c\u53ef\u5728\u9879\u76ee\u9875\u9762\u4e0a\u627e\u5230\uff1ahttps://zkf1997.github.io/DART/\u3002||\n", "2410.05259": "|**2024-10-07**|[GS-VTON: Controllable 3D Virtual Try-on with Gaussian Splatting](http://arxiv.org/abs/2410.05259)|null|Diffusion-based 2D virtual try-on (VTON) techniques have recently demonstrated strong performance, while the development of 3D VTON has largely lagged behind. Despite recent advances in text-guided 3D scene editing, integrating 2D VTON into these pipelines to achieve vivid 3D VTON remains challenging. The reasons are twofold. First, text prompts cannot provide sufficient details in describing clothing. Second, 2D VTON results generated from different viewpoints of the same 3D scene lack coherence and spatial relationships, hence frequently leading to appearance inconsistencies and geometric distortions. To resolve these problems, we introduce an image-prompted 3D VTON method (dubbed GS-VTON) which, by leveraging 3D Gaussian Splatting (3DGS) as the 3D representation, enables the transfer of pre-trained knowledge from 2D VTON models to 3D while improving cross-view consistency. (1) Specifically, we propose a personalized diffusion model that utilizes low-rank adaptation (LoRA) fine-tuning to incorporate personalized information into pre-trained 2D VTON models. To achieve effective LoRA training, we introduce a reference-driven image editing approach that enables the simultaneous editing of multi-view images while ensuring consistency. (2) Furthermore, we propose a persona-aware 3DGS editing framework to facilitate effective editing while maintaining consistent cross-view appearance and high-quality 3D geometry. (3) Additionally, we have established a new 3D VTON benchmark, 3D-VTONBench, which facilitates comprehensive qualitative and quantitative 3D VTON evaluations. Through extensive experiments and comparative analyses with existing methods, the proposed \\OM has demonstrated superior fidelity and advanced editing capabilities, affirming its effectiveness for 3D VTON.||\n", "2410.05255": "|**2024-10-07**|[SePPO: Semi-Policy Preference Optimization for Diffusion Alignment](http://arxiv.org/abs/2410.05255)|**[link](https://github.com/dwanzhang-ai/seppo)**|Reinforcement learning from human feedback (RLHF) methods are emerging as a way to fine-tune diffusion models (DMs) for visual generation. However, commonly used on-policy strategies are limited by the generalization capability of the reward model, while off-policy approaches require large amounts of difficult-to-obtain paired human-annotated data, particularly in visual generation tasks. To address the limitations of both on- and off-policy RLHF, we propose a preference optimization method that aligns DMs with preferences without relying on reward models or paired human-annotated data. Specifically, we introduce a Semi-Policy Preference Optimization (SePPO) method. SePPO leverages previous checkpoints as reference models while using them to generate on-policy reference samples, which replace \"losing images\" in preference pairs. This approach allows us to optimize using only off-policy \"winning images.\" Furthermore, we design a strategy for reference model selection that expands the exploration in the policy space. Notably, we do not simply treat reference samples as negative examples for learning. Instead, we design an anchor-based criterion to assess whether the reference samples are likely to be winning or losing images, allowing the model to selectively learn from the generated reference samples. This approach mitigates performance degradation caused by the uncertainty in reference sample quality. We validate SePPO across both text-to-image and text-to-video benchmarks. SePPO surpasses all previous approaches on the text-to-image benchmarks and also demonstrates outstanding performance on the text-to-video benchmarks. Code will be released in https://github.com/DwanZhang-AI/SePPO.||\n", "2410.05234": "|**2024-10-07**|[DiffuseReg: Denoising Diffusion Model for Obtaining Deformation Fields in Unsupervised Deformable Image Registration](http://arxiv.org/abs/2410.05234)|**[link](https://github.com/yutazhuo/diffusereg)**|Deformable image registration aims to precisely align medical images from different modalities or times. Traditional deep learning methods, while effective, often lack interpretability, real-time observability and adjustment capacity during registration inference. Denoising diffusion models present an alternative by reformulating registration as iterative image denoising. However, existing diffusion registration approaches do not fully harness capabilities, neglecting the critical sampling phase that enables continuous observability during the inference. Hence, we introduce DiffuseReg, an innovative diffusion-based method that denoises deformation fields instead of images for improved transparency. We also propose a novel denoising network upon Swin Transformer, which better integrates moving and fixed images with diffusion time step throughout the denoising process. Furthermore, we enhance control over the denoising registration process with a novel similarity consistency regularization. Experiments on ACDC datasets demonstrate DiffuseReg outperforms existing diffusion registration methods by 1.32 in Dice score. The sampling process in DiffuseReg enables real-time output observability and adjustment unmatched by previous deep models.||\n", "2410.05175": "|**2024-10-07**|[Avoiding Deadlocks via Weak Deadlock Sets](http://arxiv.org/abs/2410.05175)|null|A deadlock occurs in a network when two or more items prevent each other from moving and are stalled. In a general model, items are stored at vertices and each vertex $v$ has a buffer with $b(v)$ slots. Given a route for each item toward its destination, the Deadlock Safety Problem asks whether the current state is safe, i.e., it is possible to deliver each item at its destination, or is bound to deadlock, i.e., any sequence of moves will end up with a set of items stalled. While when $b \\geq 2$ the problem is solvable in polynomial time building upon a nice characterization of YES/NO-instances, it is NP-hard on quite simple graphs as grids when $b=1$ and on trees when $b\\leq 3$. We improve on these results by means of two new tools, weak deadlock sets and wise states. We show that for general networks and $b$ a state that is wise and without weak deadlock sets -- this can be recognized in polynomial time -- is safe: this is indeed a strengthening of the result for $b\\geq 2$. We sharpen this result for trees, where we show that a wise state is safe if and only if it has no weak deadlock set. That is interesting in particular in the context of rail transportation where networks are often single-tracked and deadlock detection and avoidance focuses on local sub-networks, mostly with a tree-like structure. We pose some research questions for future investigations.||\n", "2410.05167": "|**2024-10-07**|[Presto! Distilling Steps and Layers for Accelerating Music Generation](http://arxiv.org/abs/2410.05167)|null|Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop a new score-based distribution matching distillation (DMD) method for the EDM-family of diffusion models, the first GAN-based distillation method for TTM. To reduce the cost per step, we develop a simple, but powerful improvement to a recent layer distillation method that improves learning via better preserving hidden state variance. Finally, we combine our step and layer distillation methods together for a dual-faceted approach. We evaluate our step and layer distillation methods independently and show each yield best-in-class performance. Our combined distillation method can generate high-quality outputs with improved diversity, accelerating our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) -- the fastest high-quality TTM to our knowledge. Sound examples can be found at https://presto-music.github.io/web/.||\n", "2410.05163": "|**2024-10-07**|[A Simulation-Free Deep Learning Approach to Stochastic Optimal Control](http://arxiv.org/abs/2410.05163)|null|We propose a simulation-free algorithm for the solution of generic problems in stochastic optimal control (SOC). Unlike existing methods, our approach does not require the solution of an adjoint problem, but rather leverages Girsanov theorem to directly calculate the gradient of the SOC objective on-policy. This allows us to speed up the optimization of control policies parameterized by neural networks since it completely avoids the expensive back-propagation step through stochastic differential equations (SDEs) used in the Neural SDE framework. In particular, it enables us to solve SOC problems in high dimension and on long time horizons. We demonstrate the efficiency of our approach in various domains of applications, including standard stochastic optimal control problems, sampling from unnormalized distributions via construction of a Schr\\\"odinger-F\\\"ollmer process, and fine-tuning of pre-trained diffusion models. In all cases our method is shown to outperform the existing methods in both the computing time and memory efficiency.||\n", "2410.05143": "|**2024-10-07**|[Leveraging Multimodal Diffusion Models to Accelerate Imaging with Side Information](http://arxiv.org/abs/2410.05143)|null|Diffusion models have found phenomenal success as expressive priors for solving inverse problems, but their extension beyond natural images to more structured scientific domains remains limited. Motivated by applications in materials science, we aim to reduce the number of measurements required from an expensive imaging modality of interest, by leveraging side information from an auxiliary modality that is much cheaper to obtain. To deal with the non-differentiable and black-box nature of the forward model, we propose a framework to train a multimodal diffusion model over the joint modalities, turning inverse problems with black-box forward models into simple linear inpainting problems. Numerically, we demonstrate the feasibility of training diffusion models over materials imagery data, and show that our approach achieves superior image reconstruction by leveraging the available side information, requiring significantly less amount of data from the expensive microscopy modality.||\n", "2410.05124": "|**2024-10-07**|[Agnostic Smoothed Online Learning](http://arxiv.org/abs/2410.05124)|null|Classical results in statistical learning typically consider two extreme data-generating models: i.i.d. instances from an unknown distribution, or fully adversarial instances, often much more challenging statistically. To bridge the gap between these models, recent work introduced the smoothed framework, in which at each iteration an adversary generates instances from a distribution constrained to have density bounded by $\\sigma^{-1}$ compared to some fixed base measure $\\mu$. This framework interpolates between the i.i.d. and adversarial cases, depending on the value of $\\sigma$. For the classical online prediction problem, most prior results in smoothed online learning rely on the arguably strong assumption that the base measure $\\mu$ is known to the learner, contrasting with standard settings in the PAC learning or consistency literature. We consider the general agnostic problem in which the base measure is unknown and values are arbitrary. Along this direction, Block et al. showed that empirical risk minimization has sublinear regret under the well-specified assumption. We propose an algorithm R-Cover based on recursive coverings which is the first to guarantee sublinear regret for agnostic smoothed online learning without prior knowledge of $\\mu$. For classification, we prove that R-Cover has adaptive regret $\\tilde O(\\sqrt{dT/\\sigma})$ for function classes with VC dimension $d$, which is optimal up to logarithmic factors. For regression, we establish that R-Cover has sublinear oblivious regret for function classes with polynomial fat-shattering dimension growth.||\n", "2410.05114": "|**2024-10-07**|[Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization](http://arxiv.org/abs/2410.05114)|null|In the realm of dermatological diagnoses, where the analysis of dermatoscopic and microscopic skin lesion images is pivotal for the accurate and early detection of various medical conditions, the costs associated with creating diverse and high-quality annotated datasets have hampered the accuracy and generalizability of machine learning models. We propose an innovative unsupervised augmentation solution that harnesses Generative Adversarial Network (GAN) based models and associated techniques over their latent space to generate controlled semiautomatically-discovered semantic variations in dermatoscopic images. We created synthetic images to incorporate the semantic variations and augmented the training data with these images. With this approach, we were able to increase the performance of machine learning models and set a new benchmark amongst non-ensemble based models in skin lesion classification on the HAM10000 dataset; and used the observed analytics and generated models for detailed studies on model explainability, affirming the effectiveness of our solution.||\n", "2410.08207": "|**2024-10-10**|[DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models](http://arxiv.org/abs/2410.08207)|null|\u79bb\u6563\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u7b49\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u53ef\u63a7\u5185\u5bb9\u7f16\u8f91\u65b9\u9762\u9762\u4e34\u5c40\u9650\u6027\u3002\u6211\u4eec\u5f15\u5165\u4e86 DICE\uff08\u7528\u4e8e\u53ef\u63a7\u7f16\u8f91\u7684\u79bb\u6563\u9006\u63a8\uff09\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u5bf9\u79bb\u6563\u6269\u6563\u6a21\u578b\uff08\u5305\u62ec\u591a\u9879\u5f0f\u6269\u6563\u548c\u63a9\u7801\u751f\u6210\u6a21\u578b\uff09\u8fdb\u884c\u7cbe\u786e\u9006\u63a8\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u5728\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\u4e2d\u8bb0\u5f55\u566a\u58f0\u5e8f\u5217\u548c\u63a9\u7801\u6a21\u5f0f\uff0cDICE \u65e0\u9700\u9884\u5b9a\u4e49\u63a9\u7801\u6216\u6ce8\u610f\u529b\u673a\u5236\u64cd\u4f5c\u5373\u53ef\u5b9e\u73b0\u79bb\u6563\u6570\u636e\u7684\u51c6\u786e\u91cd\u5efa\u548c\u7075\u6d3b\u7f16\u8f91\u3002\u6211\u4eec\u5728\u56fe\u50cf\u548c\u6587\u672c\u9886\u57df\u8bc1\u660e\u4e86 DICE \u7684\u6709\u6548\u6027\uff0c\u5e76\u5728 VQ-Diffusion\u3001Paella \u548c RoBERTa \u7b49\u6a21\u578b\u4e0a\u5bf9\u5176\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0cDICE \u5728\u4fdd\u6301\u9ad8\u6570\u636e\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u7f16\u8f91\u80fd\u529b\uff0c\u4e3a\u79bb\u6563\u7a7a\u95f4\u4e2d\u7684\u7ec6\u7c92\u5ea6\u5185\u5bb9\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\u3002\u9879\u76ee\u7f51\u9875\u8bf7\u8bbf\u95ee https://hexiaoxiao-cs.github.io/DICE/\u3002||\n", "2410.08192": "|**2024-10-10**|[HybridBooth: Hybrid Prompt Inversion for Efficient Subject-Driven Generation](http://arxiv.org/abs/2410.08192)|null|\u8fd1\u5e74\u6765\uff0c\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u4f7f\u7528\u6587\u672c\u63d0\u793a\u8fdb\u884c\u521b\u4f5c\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4f46\u57fa\u4e8e\u7279\u5b9a\u4e3b\u9898\u751f\u6210\u4e2a\u6027\u5316\u5b9e\u4f8b\uff08\u5373\u4e3b\u9898\u9a71\u52a8\u751f\u6210\uff09\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a HybridBooth \u7684\u65b0\u578b\u6df7\u5408\u6846\u67b6\uff0c\u5b83\u878d\u5408\u4e86\u57fa\u4e8e\u4f18\u5316\u548c\u76f4\u63a5\u56de\u5f52\u65b9\u6cd5\u7684\u4f18\u70b9\u3002HybridBooth \u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\u8fd0\u884c\uff1a\u8bcd\u5d4c\u5165\u63a2\u6d4b\u548c\u8bcd\u5d4c\u5165\u7ec6\u5316\u3002\u8bcd\u5d4c\u5165\u63a2\u6d4b\u4f7f\u7528\u5fae\u8c03\u540e\u7684\u7f16\u7801\u5668\u751f\u6210\u7a33\u5065\u7684\u521d\u59cb\u8bcd\u5d4c\u5165\uff1b\u8bcd\u5d4c\u5165\u7ec6\u5316\u901a\u8fc7\u4f18\u5316\u5173\u952e\u53c2\u6570\uff0c\u8fdb\u4e00\u6b65\u4f7f\u7f16\u7801\u5668\u9002\u5e94\u7279\u5b9a\u7684\u4e3b\u9898\u56fe\u50cf\u3002\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4e14\u5feb\u901f\u5730\u5c06\u89c6\u89c9\u6982\u5ff5\u53cd\u8f6c\u4e3a\u6587\u672c\u5d4c\u5165\uff0c\u5373\u4f7f\u53ea\u6709\u4e00\u4e2a\u56fe\u50cf\uff0c\u540c\u65f6\u8fd8\u80fd\u4fdd\u6301\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002||\n", "2410.08188": "|**2024-10-10**|[DifFRelight: Diffusion-Based Facial Performance Relighting](http://arxiv.org/abs/2410.08188)|null|\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u7684\u65b0\u9896\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u7531\u89c6\u70b9\u7684\u4eba\u8138\u8868\u6f14\u91cd\u65b0\u7167\u660e\u3002\u5229\u7528\u5305\u542b\u5728\u5404\u79cd\u7167\u660e\u6761\u4ef6\u4e0b\uff08\u5305\u62ec\u5e73\u9762\u7167\u660e\u548c\u4e00\u6b21\u4e00\u706f (OLAT) \u573a\u666f\uff09\u6355\u83b7\u7684\u591a\u79cd\u9762\u90e8\u8868\u60c5\u7684\u7279\u5b9a\u4e3b\u9898\u6570\u636e\u96c6\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u7528\u4e8e\u7cbe\u786e\u7167\u660e\u63a7\u5236\u7684\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u4ece\u5e73\u9762\u7167\u660e\u8f93\u5165\u4e2d\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u91cd\u65b0\u7167\u660e\u4eba\u8138\u56fe\u50cf\u3002\u6211\u4eec\u7684\u6846\u67b6\u5305\u62ec\u7a7a\u95f4\u5bf9\u9f50\u7684\u5e73\u9762\u7167\u660e\u6355\u83b7\u548c\u968f\u673a\u566a\u58f0\u7684\u8c03\u8282\uff0c\u4ee5\u53ca\u7528\u4e8e\u5168\u5c40\u63a7\u5236\u7684\u96c6\u6210\u7167\u660e\u4fe1\u606f\uff0c\u5229\u7528\u6765\u81ea\u9884\u8bad\u7ec3\u7684\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u7136\u540e\u5c06\u6b64\u6a21\u578b\u5e94\u7528\u4e8e\u5728\u4e00\u81f4\u7684\u5e73\u9762\u7167\u660e\u73af\u5883\u4e2d\u6355\u83b7\u7684\u52a8\u6001\u9762\u90e8\u8868\u6f14\uff0c\u5e76\u4f7f\u7528\u53ef\u6269\u5c55\u7684\u52a8\u6001 3D \u9ad8\u65af\u6e32\u67d3\u65b9\u6cd5\u91cd\u5efa\u4ee5\u8fdb\u884c\u65b0\u9896\u89c6\u56fe\u5408\u6210\uff0c\u4ee5\u4fdd\u6301\u91cd\u65b0\u7167\u660e\u7ed3\u679c\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u5c06\u65b0\u9896\u7684\u533a\u57df\u7167\u660e\u8868\u793a\u4e0e\u5b9a\u5411\u7167\u660e\u76f8\u7ed3\u5408\uff0c\u5f15\u5165\u4e86\u7edf\u4e00\u7684\u7167\u660e\u63a7\u5236\uff0c\u5141\u8bb8\u5bf9\u5149\u7167\u5927\u5c0f\u548c\u65b9\u5411\u8fdb\u884c\u8054\u5408\u8c03\u6574\u3002\u6211\u4eec\u8fd8\u652f\u6301\u4f7f\u7528\u591a\u4e2a\u5b9a\u5411\u5149\u8fdb\u884c\u9ad8\u52a8\u6001\u8303\u56f4\u6210\u50cf (HDRI) \u5408\u6210\uff0c\u4ee5\u5728\u590d\u6742\u7684\u7167\u660e\u6761\u4ef6\u4e0b\u751f\u6210\u52a8\u6001\u5e8f\u5217\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u8bc1\u660e\u4e86\u8be5\u6a21\u578b\u5728\u5b9e\u73b0\u7cbe\u786e\u7167\u660e\u63a7\u5236\u548c\u6cdb\u5316\u5404\u79cd\u9762\u90e8\u8868\u60c5\u65b9\u9762\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u76ae\u80a4\u7eb9\u7406\u548c\u5934\u53d1\u7b49\u7ec6\u8282\u7279\u5f81\u3002\u8be5\u6a21\u578b\u51c6\u786e\u5730\u518d\u73b0\u4e86\u590d\u6742\u7684\u7167\u660e\u6548\u679c\uff0c\u4f8b\u5982\u773c\u775b\u53cd\u5c04\u3001\u6b21\u8868\u9762\u6563\u5c04\u3001\u81ea\u9634\u5f71\u548c\u534a\u900f\u660e\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6211\u4eec\u6846\u67b6\u5185\u7684\u7167\u7247\u771f\u5b9e\u611f\u3002||\n", "2410.08168": "|**2024-10-10**|[ZeroComp: Zero-shot Object Compositing from Image Intrinsics via Diffusion](http://arxiv.org/abs/2410.08168)|null|\u6211\u4eec\u63d0\u51fa\u4e86 ZeroComp\uff0c\u8fd9\u662f\u4e00\u79cd\u6709\u6548\u7684\u96f6\u6837\u672c 3D \u5bf9\u8c61\u5408\u6210\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u4e0d\u9700\u8981\u6210\u5bf9\u7684\u5408\u6210\u573a\u666f\u56fe\u50cf\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528 ControlNet \u4ece\u5185\u8574\u56fe\u50cf\u4e2d\u8fdb\u884c\u6761\u4ef6\u63a7\u5236\uff0c\u5e76\u5c06\u5176\u4e0e Stable Diffusion \u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u5229\u7528\u5176\u573a\u666f\u5148\u9a8c\uff0c\u5171\u540c\u6784\u6210\u4e00\u4e2a\u6709\u6548\u7684\u6e32\u67d3\u5f15\u64ce\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0cZeroComp \u4f7f\u7528\u57fa\u4e8e\u51e0\u4f55\u5f62\u72b6\u3001\u53cd\u7167\u7387\u548c\u906e\u7f69\u9634\u5f71\u7684\u5185\u8574\u56fe\u50cf\uff0c\u800c\u4e0d\u9700\u8981\u5305\u542b\u548c\u4e0d\u5305\u542b\u5408\u6210\u5bf9\u8c61\u7684\u573a\u666f\u7684\u6210\u5bf9\u56fe\u50cf\u3002\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u5b83\u53ef\u4ee5\u5c06\u865a\u62df 3D \u5bf9\u8c61\u65e0\u7f1d\u96c6\u6210\u5230\u573a\u666f\u4e2d\uff0c\u8c03\u6574\u9634\u5f71\u4ee5\u521b\u5efa\u903c\u771f\u7684\u5408\u6210\u56fe\u50cf\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5e76\u8bc1\u660e ZeroComp \u5728\u5b9a\u91cf\u548c\u4eba\u7c7b\u611f\u77e5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u4f7f\u7528\u663e\u5f0f\u5149\u7167\u4f30\u8ba1\u548c\u751f\u6210\u6280\u672f\u7684\u5176\u4ed6\u65b9\u6cd5\u3002\u6b64\u5916\uff0cZeroComp \u8fd8\u53ef\u4ee5\u6269\u5c55\u5230\u771f\u5b9e\u548c\u5ba4\u5916\u56fe\u50cf\u5408\u6210\uff0c\u5373\u4f7f\u4ec5\u5728\u5408\u6210\u5ba4\u5185\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4e5f\u5c55\u793a\u4e86\u5176\u5728\u56fe\u50cf\u5408\u6210\u65b9\u9762\u7684\u6709\u6548\u6027\u3002||\n", "2410.08159": "|**2024-10-10**|[DART: Denoising Autoregressive Transformer for Scalable Text-to-Image Generation](http://arxiv.org/abs/2410.08159)|null|\u6269\u6563\u6a21\u578b\u5df2\u6210\u4e3a\u89c6\u89c9\u751f\u6210\u7684\u4e3b\u5bfc\u65b9\u6cd5\u3002\u5b83\u4eec\u901a\u8fc7\u5bf9\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\u8fdb\u884c\u53bb\u566a\u6765\u8bad\u7ec3\uff0c\u8be5\u8fc7\u7a0b\u9010\u6e10\u5411\u8f93\u5165\u4e2d\u6dfb\u52a0\u566a\u58f0\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u9a6c\u5c14\u53ef\u592b\u6027\u8d28\u9650\u5236\u4e86\u6a21\u578b\u5145\u5206\u5229\u7528\u751f\u6210\u8f68\u8ff9\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6548\u7387\u4f4e\u4e0b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DART\uff0c\u4e00\u79cd\u57fa\u4e8e Transformer \u7684\u6a21\u578b\uff0c\u5b83\u5728\u975e\u9a6c\u5c14\u53ef\u592b\u6846\u67b6\u5185\u7edf\u4e00\u4e86\u81ea\u56de\u5f52 (AR) \u548c\u6269\u6563\u3002DART \u4f7f\u7528\u4e0e\u6807\u51c6\u8bed\u8a00\u6a21\u578b\u76f8\u540c\u67b6\u6784\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u5728\u7a7a\u95f4\u548c\u9891\u8c31\u4e0a\u8fed\u4ee3\u5730\u5bf9\u56fe\u50cf\u5757\u8fdb\u884c\u53bb\u566a\u3002DART \u4e0d\u4f9d\u8d56\u56fe\u50cf\u91cf\u5316\uff0c\u4ece\u800c\u80fd\u591f\u5728\u4fdd\u6301\u7075\u6d3b\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u56fe\u50cf\u5efa\u6a21\u3002\u6b64\u5916\uff0cDART \u53ef\u4ee5\u5728\u7edf\u4e00\u6a21\u578b\u4e2d\u4f7f\u7528\u6587\u672c\u548c\u56fe\u50cf\u6570\u636e\u8fdb\u884c\u65e0\u7f1d\u8bad\u7ec3\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u7c7b\u522b\u6761\u4ef6\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u4e3a\u4f20\u7edf\u7684\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u901a\u8fc7\u8fd9\u79cd\u7edf\u4e00\u7684\u6846\u67b6\uff0cDART \u4e3a\u53ef\u6269\u5c55\u3001\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u5408\u6210\u6811\u7acb\u4e86\u65b0\u7684\u6807\u6746\u3002||\n", "2410.08151": "|**2024-10-10**|[Progressive Autoregressive Video Diffusion Models](http://arxiv.org/abs/2410.08151)|**[link](https://github.com/desaixie/pa_vdm)**|\u5f53\u524d\u524d\u6cbf\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u65b9\u9762\u5df2\u7ecf\u5c55\u73b0\u51fa\u663e\u8457\u6210\u679c\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u8ba1\u7b97\u9650\u5236\uff0c\u5b83\u4eec\u53ea\u80fd\u751f\u6210\u901a\u5e38\u7ea610\u79d2\u6216240\u5e27\u7684\u77ed\u89c6\u9891\u7247\u6bb5\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u73b0\u6709\u6a21\u578b\u53ef\u4ee5\u81ea\u7136\u5730\u6269\u5c55\u5230\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u800c\u65e0\u9700\u6539\u53d8\u67b6\u6784\u3002\u6211\u4eec\u7684\u5173\u952e\u601d\u60f3\u662f\u4e3a\u6f5c\u5728\u5e27\u5206\u914d\u9010\u6e10\u589e\u52a0\u7684\u566a\u58f0\u7ea7\u522b\uff0c\u800c\u4e0d\u662f\u5355\u4e00\u566a\u58f0\u7ea7\u522b\uff0c\u8fd9\u5141\u8bb8\u6f5c\u5728\u5e27\u4e4b\u95f4\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684\u6761\u4ef6\u5316\u4ee5\u53ca\u6ce8\u610f\u529b\u7a97\u53e3\u4e4b\u95f4\u7684\u5927\u91cf\u91cd\u53e0\u3002\u8fd9\u79cd\u6e10\u8fdb\u5f0f\u89c6\u9891\u53bb\u566a\u5141\u8bb8\u6211\u4eec\u7684\u6a21\u578b\u81ea\u56de\u5f52\u5730\u751f\u6210\u89c6\u9891\u5e27\uff0c\u800c\u4e0d\u4f1a\u51fa\u73b0\u8d28\u91cf\u4e0b\u964d\u6216\u573a\u666f\u7a81\u53d8\u3002\u6211\u4eec\u57281\u5206\u949f\u7684\u957f\u89c6\u9891\u751f\u6210\uff0824 FPS\u4e0b1440\u5e27\uff09\u4e0a\u5448\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u672c\u6587\u4e2d\u7684\u89c6\u9891\u53ef\u5728https://desaixie.github.io/pa-vdm/\u4e0a\u83b7\u53d6\u3002||\n", "2410.08134": "|**2024-10-10**|[Steering Masked Discrete Diffusion Models via Discrete Denoising Posterior Prediction](http://arxiv.org/abs/2410.08134)|null|\u79bb\u6563\u6570\u636e\u7684\u751f\u6210\u6a21\u578b\u662f\u8bb8\u591a\u91cd\u8981\u5e94\u7528\u7684\u57fa\u7840\uff0c\u6db5\u76d6\u4e86\u4ece\u57fa\u4e8e\u6587\u672c\u7684\u667a\u80fd\u4f53\uff08\u5982 ChatGPT\uff09\u5230\u86cb\u767d\u8d28\u5e8f\u5217\u4e2d\u751f\u547d\u57fa\u672c\u6784\u5efa\u5757\u7684\u8bbe\u8ba1\u3002\u7136\u800c\uff0c\u5e94\u7528\u9886\u57df\u9700\u8981\u901a\u8fc7\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\uff08\u901a\u5e38\u901a\u8fc7 RLHF\uff09\u6765\u63a7\u5236\u751f\u6210\u7684\u6570\u636e\uff0c\u4ee5\u6ee1\u8db3\u7279\u5b9a\u7684\u5c5e\u6027\u3001\u5956\u52b1\u6216\u4eb2\u548c\u5ea6\u6307\u6807\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5f15\u5bfc\u63a9\u7801\u6269\u6563\u6a21\u578b (MDM) \u7684\u95ee\u9898\uff0cMDM \u662f\u4e00\u7c7b\u65b0\u5174\u7684\u79bb\u6563\u6269\u6563\u6a21\u578b\uff0c\u4e3a\u4f20\u7edf\u7684\u81ea\u56de\u5f52\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f15\u4eba\u6ce8\u76ee\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u6211\u4eec\u5f15\u5165\u4e86\u79bb\u6563\u53bb\u566a\u540e\u9a8c\u9884\u6d4b (DDPP)\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u4ece\u76ee\u6807\u8d1d\u53f6\u65af\u540e\u9a8c\u5206\u5e03\u4e2d\u91c7\u6837\uff0c\u5c06\u5f15\u5bfc\u9884\u8bad\u7ec3 MDM \u7684\u4efb\u52a1\u8f6c\u5316\u4e3a\u6982\u7387\u63a8\u7406\u95ee\u9898\u3002\u6211\u4eec\u7684 DDPP \u6846\u67b6\u4ea7\u751f\u4e86\u4e00\u7cfb\u5217\u4e09\u4e2a\u65b0\u7684\u76ee\u6807\u51fd\u6570\uff0c\u5b83\u4eec\u90fd\u662f\u65e0\u9700\u6a21\u62df\u7684\uff0c\u56e0\u6b64\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u540c\u65f6\u9002\u7528\u4e8e\u4e00\u822c\u7684\u4e0d\u53ef\u5fae\u5956\u52b1\u51fd\u6570\u3002\u5728\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5f15\u5bfc MDM \u6267\u884c\u7c7b\u522b\u6761\u4ef6\u50cf\u7d20\u7ea7\u56fe\u50cf\u5efa\u6a21\u3001\u4f7f\u7528\u57fa\u4e8e\u6587\u672c\u5956\u52b1\u7684 MDM \u7684 RLHF \u5bf9\u9f50\uff0c\u4ee5\u53ca\u5fae\u8c03\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u4ee5\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u4e8c\u7ea7\u7ed3\u6784\u548c\u66f4\u77ed\u7684\u86cb\u767d\u8d28\uff0c\u5b9e\u4f8b\u5316\u4e86 DDPP\u3002\u6211\u4eec\u901a\u8fc7\u6e7f\u5b9e\u9a8c\u5ba4\u9a8c\u8bc1\u8bc1\u5b9e\u4e86\u6211\u4eec\u7684\u8bbe\u8ba1\uff0c\u89c2\u5bdf\u5230\u5956\u52b1\u4f18\u5316\u86cb\u767d\u8d28\u5e8f\u5217\u7684\u77ac\u65f6\u8868\u8fbe\u3002||\n", "2410.08113": "|**2024-10-10**|[Robust AI-Generated Text Detection by Restricted Embeddings](http://arxiv.org/abs/2410.08113)|null|\u4eba\u5de5\u667a\u80fd\u751f\u6210\u6587\u672c\u7684\u6570\u91cf\u548c\u8d28\u91cf\u4e0d\u65ad\u63d0\u9ad8\uff0c\u8fd9\u4f7f\u5f97\u68c0\u6d4b\u6b64\u7c7b\u5185\u5bb9\u53d8\u5f97\u66f4\u52a0\u56f0\u96be\u3002\u5728\u5927\u591a\u6570\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u751f\u6210\u6570\u636e\u7684\u9886\u57df\uff08\u98ce\u683c\u548c\u4e3b\u9898\uff09\u548c\u751f\u6210\u5668\u6a21\u578b\u4e8b\u5148\u5e76\u4e0d\u77e5\u9053\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5173\u6ce8\u57fa\u4e8e\u5206\u7c7b\u5668\u7684 AI \u751f\u6210\u6587\u672c\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027\uff0c\u5373\u5b83\u4eec\u8fc1\u79fb\u5230\u672a\u77e5\u751f\u6210\u5668\u6216\u8bed\u4e49\u9886\u57df\u7684\u80fd\u529b\u3002\u6211\u4eec\u7814\u7a76\u4e86\u57fa\u4e8e Transformer \u7684\u6587\u672c\u7f16\u7801\u5668\u5d4c\u5165\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u8868\u660e\u6e05\u9664\u6709\u5bb3\u7684\u7ebf\u6027\u5b50\u7a7a\u95f4\u6709\u52a9\u4e8e\u8bad\u7ec3\u9c81\u68d2\u7684\u5206\u7c7b\u5668\uff0c\u5ffd\u7565\u7279\u5b9a\u9886\u57df\u7684\u865a\u5047\u7279\u5f81\u3002\u6211\u4eec\u7814\u7a76\u4e86\u51e0\u79cd\u5b50\u7a7a\u95f4\u5206\u89e3\u548c\u7279\u5f81\u9009\u62e9\u7b56\u7565\uff0c\u5e76\u5728\u8de8\u9886\u57df\u548c\u8de8\u751f\u6210\u5668\u8fc1\u79fb\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u663e\u8457\u6539\u8fdb\u3002\u6211\u4eec\u9488\u5bf9\u8bcd\u5934\u548c\u57fa\u4e8e\u5750\u6807\u7684\u5b50\u7a7a\u95f4\u53bb\u9664\u7684\u6700\u4f73\u65b9\u6cd5\u5206\u522b\u5c06 RoBERTa \u548c BERT \u5d4c\u5165\u7684\u5e73\u5747\u5931\u914d\u5206\u5e03 (OOD) \u5206\u7c7b\u5206\u6570\u63d0\u9ad8\u4e86\u9ad8\u8fbe 9% \u548c 14%\u3002\u6211\u4eec\u53d1\u5e03\u4e86\u4ee3\u7801\u548c\u6570\u636e\uff1ahttps://github.com/SilverSolver/RobustATD||\n", "2410.08074": "|**2024-10-10**|[Unstable Unlearning: The Hidden Risk of Concept Resurgence in Diffusion Models](http://arxiv.org/abs/2410.08074)|null|\u6587\u56fe\u751f\u6210\u6269\u6563\u6a21\u578b\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u7f51\u7edc\u6570\u636e\u96c6\u3002\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u8fd9\u4e9b\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u5f00\u53d1\u8005\u901a\u5e38\u66f4\u559c\u6b22\u5bf9\u73b0\u6709\u6a21\u578b\u8fdb\u884c\u589e\u91cf\u66f4\u65b0\u3002\u8fd9\u4e9b\u66f4\u65b0\u901a\u5e38\u5305\u62ec\u5fae\u8c03\u6b65\u9aa4\uff08\u5b66\u4e60\u65b0\u6982\u5ff5\u6216\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff09\u548c\u201c\u9057\u5fd8\u201d\u6b65\u9aa4\uff08\u201c\u5fd8\u8bb0\u201d\u73b0\u6709\u6982\u5ff5\uff0c\u4f8b\u5982\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u4f5c\u54c1\u6216\u9732\u9aa8\u5185\u5bb9\uff09\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u8fd9\u79cd\u8303\u5f0f\u4e2d\u51fa\u73b0\u7684\u4e00\u4e2a\u5173\u952e\u4e14\u4ee5\u524d\u672a\u77e5\u7684\u6f0f\u6d1e\uff1a\u5373\u4f7f\u5728\u826f\u6027\u3001\u975e\u5bf9\u6297\u6027\u6761\u4ef6\u4e0b\uff0c\u5728\u770b\u4f3c\u65e0\u5173\u7684\u56fe\u50cf\u4e0a\u5fae\u8c03\u6587\u56fe\u751f\u6210\u6269\u6563\u6a21\u578b\u4e5f\u4f1a\u5bfc\u81f4\u5176\u201c\u91cd\u65b0\u5b66\u4e60\u201d\u5148\u524d\u5df2\u201c\u9057\u5fd8\u201d\u7684\u6982\u5ff5\u3002\u6211\u4eec\u901a\u8fc7\u4e00\u7cfb\u5217\u5c06\u201c\u5927\u89c4\u6a21\u6982\u5ff5\u64e6\u9664\u201d\uff08\u6587\u56fe\u751f\u6210\u6269\u6563\u6a21\u578b\u4e2d\u9057\u5fd8\u7684\u5f53\u524d\u6280\u672f\u6c34\u5e73\uff08Lu et al., 2024\uff09\uff09\u4e0e\u968f\u540e\u5bf9 Stable Diffusion v1.4 \u8fdb\u884c\u5fae\u8c03\u7684\u5b9e\u9a8c\uff0c\u5168\u9762\u7814\u7a76\u4e86\u8fd9\u79cd\u73b0\u8c61\u7684\u539f\u56e0\u548c\u8303\u56f4\uff0c\u6211\u4eec\u5c06\u8fd9\u79cd\u73b0\u8c61\u79f0\u4e3a\u6982\u5ff5\u590d\u82cf\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u7ec4\u5408\u589e\u91cf\u6a21\u578b\u66f4\u65b0\u7684\u8106\u5f31\u6027\uff0c\u5e76\u5bf9\u5f53\u524d\u786e\u4fdd\u6587\u56fe\u751f\u6210\u6269\u6563\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\u63d0\u51fa\u4e86\u65b0\u7684\u4e25\u91cd\u62c5\u5fe7\u3002||\n", "2410.08053": "|**2024-10-10**|[A Target-Aware Analysis of Data Augmentation for Hate Speech Detection](http://arxiv.org/abs/2410.08053)|null|\u4ec7\u6068\u8a00\u8bba\u662f\u793e\u4ea4\u7f51\u7edc\u5e7f\u6cdb\u4f7f\u7528\u5e26\u6765\u7684\u4e3b\u8981\u5a01\u80c1\u4e4b\u4e00\uff0c\u5c3d\u7ba1\u4eba\u4eec\u52aa\u529b\u9650\u5236\u5b83\u3002\u5c3d\u7ba1\u5df2\u7ecf\u5173\u6ce8\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u7f3a\u4e4f\u4ee5\u80fd\u529b\u6b67\u89c6\u6216\u5e74\u9f84\u6b67\u89c6\u7b49\u9c9c\u5c11\u51fa\u73b0\u7684\u73b0\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u96c6\u548c\u6848\u4f8b\u7814\u7a76\uff0c\u53ef\u80fd\u5bfc\u81f4\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7cfb\u7edf\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8eab\u4efd\u7fa4\u4f53\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u9274\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u65b9\u9762\u7684\u7a7a\u524d\u80fd\u529b\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u4f7f\u7528\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u6269\u5145\u73b0\u6709\u6570\u636e\u7684\u53ef\u80fd\u6027\uff0c\u4ee5\u51cf\u5c11\u76ee\u6807\u4e0d\u5e73\u8861\u3002\u6211\u4eec\u5c1d\u8bd5\u4f7f\u7528 Measuring Hate Speech \u8bed\u6599\u5e93\u4e2d\u7684 1,000 \u4e2a\u5e16\u5b50\u8fdb\u884c\u6269\u5145\uff0c\u8fd9\u662f\u4e00\u4e2a\u6807\u6ce8\u4e86\u76ee\u6807\u8eab\u4efd\u4fe1\u606f\u7684\u82f1\u8bed\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u7b80\u5355\u7684\u6570\u636e\u5e93\u589e\u5f3a\u65b9\u6cd5\u548c\u4e0d\u540c\u7c7b\u578b\u7684\u751f\u6210\u6a21\u578b\u6dfb\u52a0\u4e86\u5927\u7ea6 30,000 \u4e2a\u5408\u6210\u6837\u672c\uff0c\u6bd4\u8f83\u4e86\u81ea\u56de\u5f52\u548c\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u53d1\u73b0\u4f20\u7edf\u7684\u6570\u636e\u5e93\u589e\u5f3a\u65b9\u6cd5\u901a\u5e38\u6bd4\u751f\u6210\u6a21\u578b\u66f4\u53ef\u53d6\uff0c\u4f46\u4e24\u8005\u7ed3\u5408\u5f80\u5f80\u4f1a\u4ea7\u751f\u6700\u597d\u7684\u7ed3\u679c\u3002\u4e8b\u5b9e\u4e0a\uff0c\u5bf9\u4e8e\u67d0\u4e9b\u4ec7\u6068\u7c7b\u522b\uff0c\u4f8b\u5982\u51fa\u8eab\u3001\u5b97\u6559\u548c\u6b8b\u75be\uff0c\u4f7f\u7528\u589e\u5f3a\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u7684\u4ec7\u6068\u8a00\u8bba\u5206\u7c7b\u6bd4\u6ca1\u6709\u589e\u5f3a\u6570\u636e\u7684\u57fa\u7ebf\u63d0\u9ad8\u4e86 10% \u4ee5\u4e0a\u7684 F1 \u503c\u3002\u8fd9\u9879\u5de5\u4f5c\u6709\u52a9\u4e8e\u5f00\u53d1\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7cfb\u7edf\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u4e0d\u4ec5\u6027\u80fd\u66f4\u597d\uff0c\u800c\u4e14\u5bf9\u8fc4\u4eca\u4e3a\u6b62\u88ab\u5ffd\u89c6\u7684\u76ee\u6807\u66f4\u516c\u5e73\u3001\u66f4\u5177\u5305\u5bb9\u6027\u3002||\n", "2410.09049": "|**2024-10-11**|[SceneCraft: Layout-Guided 3D Scene Generation](http://arxiv.org/abs/2410.09049)|**[link](https://github.com/orangesodahub/scenecraft)**|The creation of complex 3D scenes tailored to user specifications has been a tedious and challenging task with traditional 3D modeling tools. Although some pioneering methods have achieved automatic text-to-3D generation, they are generally limited to small-scale scenes with restricted control over the shape and texture. We introduce SceneCraft, a novel method for generating detailed indoor scenes that adhere to textual descriptions and spatial layout preferences provided by users. Central to our method is a rendering-based technique, which converts 3D semantic layouts into multi-view 2D proxy maps. Furthermore, we design a semantic and depth conditioned diffusion model to generate multi-view images, which are used to learn a neural radiance field (NeRF) as the final scene representation. Without the constraints of panorama image generation, we surpass previous methods in supporting complicated indoor space generation beyond a single room, even as complicated as a whole multi-bedroom apartment with irregular shapes and layouts. Through experimental analysis, we demonstrate that our method significantly outperforms existing approaches in complex indoor scene generation with diverse textures, consistent geometry, and realistic visual quality. Code and more results are available at: https://orangesodahub.github.io/SceneCraft||\n", "2410.09046": "|**2024-10-11**|[Linear Convergence of Diffusion Models Under the Manifold Hypothesis](http://arxiv.org/abs/2410.09046)|null|Score-matching generative models have proven successful at sampling from complex high-dimensional data distributions. In many applications, this distribution is believed to concentrate on a much lower $d$-dimensional manifold embedded into $D$-dimensional space; this is known as the manifold hypothesis. The current best-known convergence guarantees are either linear in $D$ or polynomial (superlinear) in $d$. The latter exploits a novel integration scheme for the backward SDE. We take the best of both worlds and show that the number of steps diffusion models require in order to converge in Kullback-Leibler~(KL) divergence is linear (up to logarithmic terms) in the intrinsic dimension $d$. Moreover, we show that this linear dependency is sharp.||\n", "2410.09009": "|**2024-10-11**|[Semantic Score Distillation Sampling for Compositional Text-to-3D Generation](http://arxiv.org/abs/2410.09009)|**[link](https://github.com/yangling0818/semanticsds-3d)**|Generating high-quality 3D assets from textual descriptions remains a pivotal challenge in computer graphics and vision research. Due to the scarcity of 3D data, state-of-the-art approaches utilize pre-trained 2D diffusion priors, optimized through Score Distillation Sampling (SDS). Despite progress, crafting complex 3D scenes featuring multiple objects or intricate interactions is still difficult. To tackle this, recent methods have incorporated box or layout guidance. However, these layout-guided compositional methods often struggle to provide fine-grained control, as they are generally coarse and lack expressiveness. To overcome these challenges, we introduce a novel SDS approach, Semantic Score Distillation Sampling (SemanticSDS), designed to effectively improve the expressiveness and accuracy of compositional text-to-3D generation. Our approach integrates new semantic embeddings that maintain consistency across different rendering views and clearly differentiate between various objects and parts. These embeddings are transformed into a semantic map, which directs a region-specific SDS process, enabling precise optimization and compositional generation. By leveraging explicit semantic guidance, our method unlocks the compositional capabilities of existing pre-trained diffusion models, thereby achieving superior quality in 3D content generation, particularly for complex objects and scenes. Experimental results demonstrate that our SemanticSDS framework is highly effective for generating state-of-the-art complex 3D content. Code: https://github.com/YangLing0818/SemanticSDS-3D||\n", "2410.09002": "|**2024-10-11**|[WaveDiffusion: Exploring Full Waveform Inversion via Joint Diffusion in the Latent Space](http://arxiv.org/abs/2410.09002)|null|Full Waveform Inversion (FWI) is a vital technique for reconstructing high-resolution subsurface velocity maps from seismic waveform data, governed by partial differential equations (PDEs) that model wave propagation. Traditional machine learning approaches typically map seismic data to velocity maps by encoding seismic waveforms into latent embeddings and decoding them into velocity maps. In this paper, we introduce a novel framework that reframes FWI as a joint diffusion process in a shared latent space, bridging seismic waveform data and velocity maps. Our approach has two key components: first, we merge the bottlenecks of two separate autoencoders-one for seismic data and one for velocity maps-into a unified latent space using vector quantization to establish a shared codebook. Second, we train a diffusion model in this latent space, enabling the simultaneous generation of seismic and velocity map pairs by sampling and denoising the latent representations, followed by decoding each modality with its respective decoder. Remarkably, our jointly generated seismic-velocity pairs approximately satisfy the governing PDE without any additional constraint, offering a new geometric interpretation of FWI. The diffusion process learns to score the latent space according to its deviation from the PDE, with higher scores representing smaller deviations from the true solutions. By following this diffusion process, the model traces a path from random initialization to a valid solution of the governing PDE. Our experiments on the OpenFWI dataset demonstrate that the generated seismic and velocity map pairs not only exhibit high fidelity and diversity but also adhere to the physical constraints imposed by the governing PDE.||\n", "2410.08942": "|**2024-10-11**|[Maximizing the Potential of Synthetic Data: Insights from Random Matrix Theory](http://arxiv.org/abs/2410.08942)|null|Synthetic data has gained attention for training large language models, but poor-quality data can harm performance (see, e.g., Shumailov et al. (2023); Seddik et al. (2024)). A potential solution is data pruning, which retains only high-quality data based on a score function (human or machine feedback). Previous work Feng et al. (2024) analyzed models trained on synthetic data as sample size increases. We extend this by using random matrix theory to derive the performance of a binary classifier trained on a mix of real and pruned synthetic data in a high dimensional setting. Our findings identify conditions where synthetic data could improve performance, focusing on the quality of the generative model and verification strategy. We also show a smooth phase transition in synthetic label noise, contrasting with prior sharp behavior in infinite sample limits. Experiments with toy models and large language models validate our theoretical results.||\n", "2410.08924": "|**2024-10-11**|[DiffPO: A causal diffusion model for learning distributions of potential outcomes](http://arxiv.org/abs/2410.08924)|null|Predicting potential outcomes of interventions from observational data is crucial for decision-making in medicine, but the task is challenging due to the fundamental problem of causal inference. Existing methods are largely limited to point estimates of potential outcomes with no uncertain quantification; thus, the full information about the distributions of potential outcomes is typically ignored. In this paper, we propose a novel causal diffusion model called DiffPO, which is carefully designed for reliable inferences in medicine by learning the distribution of potential outcomes. In our DiffPO, we leverage a tailored conditional denoising diffusion model to learn complex distributions, where we address the selection bias through a novel orthogonal diffusion loss. Another strength of our DiffPO method is that it is highly flexible (e.g., it can also be used to estimate different causal quantities such as CATE). Across a wide range of experiments, we show that our method achieves state-of-the-art performance.||\n", "2410.08894": "|**2024-10-11**|[Conditional Generative Models for Contrast-Enhanced Synthesis of T1w and T1 Maps in Brain MRI](http://arxiv.org/abs/2410.08894)|**[link](https://github.com/Janspiry/Palette-Image-to-Image-Diffusion-Models)**|Contrast enhancement by Gadolinium-based contrast agents (GBCAs) is a vital tool for tumor diagnosis in neuroradiology. Based on brain MRI scans of glioblastoma before and after Gadolinium administration, we address enhancement prediction by neural networks with two new contributions. Firstly, we study the potential of generative models, more precisely conditional diffusion and flow matching, for uncertainty quantification in virtual enhancement. Secondly, we examine the performance of T1 scans from quantitive MRI versus T1-weighted scans. In contrast to T1-weighted scans, these scans have the advantage of a physically meaningful and thereby comparable voxel range. To compare network prediction performance of these two modalities with incompatible gray-value scales, we propose to evaluate segmentations of contrast-enhanced regions of interest using Dice and Jaccard scores. Across models, we observe better segmentations with T1 scans than with T1-weighted scans.||\n", "2410.08711": "|**2024-10-11**|[On-Chip Learning via Transformer In-Context Learning](http://arxiv.org/abs/2410.08711)|null|Autoregressive decoder-only transformers have become key components for scalable sequence processing and generation models. However, the transformer's self-attention mechanism requires transferring prior token projections from the main memory at each time step (token), thus severely limiting their performance on conventional processors. Self-attention can be viewed as a dynamic feed-forward layer, whose matrix is input sequence-dependent similarly to the result of local synaptic plasticity. Using this insight, we present a neuromorphic decoder-only transformer model that utilizes an on-chip plasticity processor to compute self-attention. Interestingly, the training of transformers enables them to ``learn'' the input context during inference. We demonstrate this in-context learning ability of transformers on the Loihi 2 processor by solving a few-shot classification problem. With this we emphasize the importance of pretrained models especially their ability to find simple, local, backpropagation free, learning rules enabling on-chip learning and adaptation in a hardware friendly manner.||\n", "2410.08709": "|**2024-10-11**|[Distillation of Discrete Diffusion through Dimensional Correlations](http://arxiv.org/abs/2410.08709)|null|Diffusion models have demonstrated exceptional performances in various fields of generative modeling. While they often outperform competitors including VAEs and GANs in sample quality and diversity, they suffer from slow sampling speed due to their iterative nature. Recently, distillation techniques and consistency models are mitigating this issue in continuous domains, but discrete diffusion models have some specific challenges towards faster generation. Most notably, in the current literature, correlations between different dimensions (pixels, locations) are ignored, both by its modeling and loss functions, due to computational limitations. In this paper, we propose \"mixture\" models in discrete diffusion that are capable of treating dimensional correlations while remaining scalable, and we provide a set of loss functions for distilling the iterations of existing models. Two primary theoretical insights underpin our approach: first, that dimensionally independent models can well approximate the data distribution if they are allowed to conduct many sampling steps, and second, that our loss functions enables mixture models to distill such many-step conventional models into just a few steps by learning the dimensional correlations. We empirically demonstrate that our proposed method for discrete diffusions work in practice, by distilling a continuous-time discrete diffusion model pretrained on the CIFAR-10 dataset.||\n", "2410.08649": "|**2024-10-11**|[E-Motion: Future Motion Simulation via Event Sequence Diffusion](http://arxiv.org/abs/2410.08649)|**[link](https://github.com/p4r4mount/E-Motion)**|Forecasting a typical object's future motion is a critical task for interpreting and interacting with dynamic environments in computer vision. Event-based sensors, which could capture changes in the scene with exceptional temporal granularity, may potentially offer a unique opportunity to predict future motion with a level of detail and precision previously unachievable. Inspired by that, we propose to integrate the strong learning capacity of the video diffusion model with the rich motion information of an event camera as a motion simulation framework. Specifically, we initially employ pre-trained stable video diffusion models to adapt the event sequence dataset. This process facilitates the transfer of extensive knowledge from RGB videos to an event-centric domain. Moreover, we introduce an alignment mechanism that utilizes reinforcement learning techniques to enhance the reverse generation trajectory of the diffusion model, ensuring improved performance and accuracy. Through extensive testing and validation, we demonstrate the effectiveness of our method in various complex scenarios, showcasing its potential to revolutionize motion flow prediction in computer vision applications such as autonomous vehicle guidance, robotic navigation, and interactive media. Our findings suggest a promising direction for future research in enhancing the interpretative power and predictive accuracy of computer vision systems.||\n", "2410.11838": "|**2024-10-15**|[High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion](http://arxiv.org/abs/2410.11838)|null|\u5c3d\u7ba1\u8fd1\u671f\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u73b0\u6709\u7684\u5e27\u63d2\u503c\u65b9\u6cd5\u5728\u5904\u7406\u6781\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u548c\u5904\u7406\u91cd\u590d\u7eb9\u7406\u3001\u7ec6\u5c0f\u7269\u4f53\u548c\u5927\u8fd0\u52a8\u7b49\u6311\u6218\u6027\u6848\u4f8b\u65f6\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u8865\u4e01\u7684\u7ea7\u8054\u50cf\u7d20\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u5e27\u63d2\u503c\uff0c\u540d\u4e3a HiFI\uff0c\u5b83\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u7ea7\u8054\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u4e00\u7cfb\u5217\u4ece\u4f4e\u5206\u8fa8\u7387\u5230\u9ad8\u5206\u8fa8\u7387\u7684\u56fe\u50cf\uff0c\u8fd9\u6709\u52a9\u4e8e\u5904\u7406\u9700\u8981\u5168\u5c40\u4e0a\u4e0b\u6587\u4ee5\u83b7\u5f97\u7c97\u7565\u89e3\u51b3\u65b9\u6848\u4ee5\u53ca\u9700\u8981\u8be6\u7ec6\u4e0a\u4e0b\u6587\u4ee5\u83b7\u5f97\u9ad8\u5206\u8fa8\u7387\u8f93\u51fa\u7684\u5927\u8fd0\u52a8\u6216\u590d\u6742\u8fd0\u52a8\u3002\u7136\u800c\uff0c\u4e0e\u5148\u524d\u5728\u8d8a\u6765\u8d8a\u5927\u7684\u5206\u8fa8\u7387\u4e0a\u6267\u884c\u6269\u6563\u7684\u7ea7\u8054\u6269\u6563\u6a21\u578b\u5de5\u4f5c\u76f8\u53cd\uff0c\u6211\u4eec\u4f7f\u7528\u5355\u4e2a\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u59cb\u7ec8\u4ee5\u76f8\u540c\u7684\u5206\u8fa8\u7387\u6267\u884c\u6269\u6563\uff0c\u5e76\u901a\u8fc7\u5904\u7406\u8f93\u5165\u548c\u5148\u524d\u89e3\u51b3\u65b9\u6848\u7684\u8865\u4e01\u6765\u8fdb\u884c\u4e0a\u91c7\u6837\u3002\u6211\u4eec\u8868\u660e\uff0c\u8fd9\u79cd\u6280\u672f\u5927\u5927\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u7684\u5185\u5b58\u4f7f\u7528\u91cf\uff0c\u5e76\u4e14\u8fd8\u5141\u8bb8\u6211\u4eec\u5728\u6d4b\u8bd5\u65f6\u4f7f\u7528\u5355\u4e2a\u6a21\u578b\uff0c\u540c\u65f6\u89e3\u51b3\u5e27\u63d2\u503c\u548c\u7a7a\u95f4\u4e0a\u91c7\u6837\u95ee\u9898\uff0c\u4ece\u800c\u8282\u7701\u4e86\u8bad\u7ec3\u6210\u672c\u3002\u6211\u4eec\u8bc1\u660e\u4e86 HiFI \u5bf9\u9700\u8981\u5168\u5c40\u4e0a\u4e0b\u6587\u7684\u9ad8\u5206\u8fa8\u7387\u548c\u590d\u6742\u91cd\u590d\u7eb9\u7406\u6709\u5f88\u5927\u5e2e\u52a9\u3002HiFI \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08Vimeo\u3001Xiph\u3001X-Test\u3001SEPE-8K\uff09\u4e0a\u5c55\u793a\u4e86\u4e0e\u6700\u5148\u8fdb\u6280\u672f\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\u3002\u5728\u6211\u4eec\u65b0\u5f15\u5165\u7684\u4e13\u6ce8\u4e8e\u7279\u522b\u5177\u6709\u6311\u6218\u6027\u7684\u6848\u4f8b\u7684\u6570\u636e\u96c6\u4e0a\uff0cHiFI \u5728\u8fd9\u4e9b\u6848\u4f8b\u4e0a\u7684\u8868\u73b0\u4e5f\u660e\u663e\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\u3002\u8bf7\u8bbf\u95ee\u6211\u4eec\u7684\u9879\u76ee\u9875\u9762\u4ee5\u83b7\u53d6\u89c6\u9891\u7ed3\u679c\uff1ahttps://hifi-diffusion.github.io||\n", "2410.11835": "|**2024-10-15**|[On the Effectiveness of Dataset Alignment for Fake Image Detection](http://arxiv.org/abs/2410.11835)|null|\u968f\u7740\u6f5c\u5728\u6269\u6563\u6a21\u578b (LDM) \u4f7f\u56fe\u50cf\u751f\u6210\u80fd\u529b\u5927\u4f17\u5316\uff0c\u5bf9\u865a\u5047\u56fe\u50cf\u68c0\u6d4b\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u4e00\u4e2a\u597d\u7684\u68c0\u6d4b\u5668\u5e94\u8be5\u4e13\u6ce8\u4e8e\u751f\u6210\u6a21\u578b\u7684\u6307\u7eb9\uff0c\u800c\u5ffd\u7565\u56fe\u50cf\u5c5e\u6027\uff0c\u5982\u8bed\u4e49\u5185\u5bb9\u3001\u5206\u8fa8\u7387\u3001\u6587\u4ef6\u683c\u5f0f\u7b49\u3002\u865a\u5047\u56fe\u50cf\u68c0\u6d4b\u5668\u901a\u5e38\u4ee5\u6570\u636e\u9a71\u52a8\u7684\u65b9\u5f0f\u6784\u5efa\uff0c\u5176\u4e2d\u8bad\u7ec3\u6a21\u578b\u4ee5\u533a\u5206\u771f\u5b9e\u56fe\u50cf\u548c\u865a\u5047\u56fe\u50cf\u3002\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u7814\u7a76\u7f51\u7edc\u67b6\u6784\u9009\u62e9\u548c\u8bad\u7ec3\u65b9\u6cd5\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u8ba4\u4e3a\u9664\u4e86\u8fd9\u4e9b\u7b97\u6cd5\u9009\u62e9\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u9700\u8981\u4e00\u4e2a\u826f\u597d\u5bf9\u9f50\u7684\u771f\u5b9e/\u865a\u5047\u56fe\u50cf\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u9c81\u68d2\u7684\u68c0\u6d4b\u5668\u3002\u5bf9\u4e8e LDM \u7cfb\u5217\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u5e38\u7b80\u5355\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff1a\u6211\u4eec\u4f7f\u7528 LDM \u81ea\u52a8\u7f16\u7801\u5668\u91cd\u5efa\u6240\u6709\u771f\u5b9e\u56fe\u50cf\uff0c\u65e0\u9700\u4efb\u4f55\u53bb\u566a\u64cd\u4f5c\u3002\u7136\u540e\uff0c\u6211\u4eec\u8bad\u7ec3\u4e00\u4e2a\u6a21\u578b\u6765\u5c06\u8fd9\u4e9b\u771f\u5b9e\u56fe\u50cf\u4e0e\u5176\u91cd\u5efa\u56fe\u50cf\u533a\u5206\u5f00\u6765\u3002\u4ee5\u8fd9\u79cd\u65b9\u5f0f\u521b\u5efa\u7684\u865a\u5047\u56fe\u50cf\u5728\u51e0\u4e4e\u6240\u6709\u65b9\u9762\uff08\u4f8b\u5982\uff0c\u5927\u5c0f\u3001\u7eb5\u6a2a\u6bd4\u3001\u8bed\u4e49\u5185\u5bb9\uff09\u90fd\u4e0e\u771f\u5b9e\u56fe\u50cf\u6781\u5176\u76f8\u4f3c\uff0c\u8fd9\u8feb\u4f7f\u6a21\u578b\u5bfb\u627e LDM \u89e3\u7801\u5668\u7684\u4f2a\u5f71\u3002\u6211\u4eec\u901a\u8fc7\u7ecf\u9a8c\u8bc1\u660e\uff0c\u8fd9\u79cd\u521b\u5efa\u5bf9\u9f50\u7684\u771f\u5b9e/\u865a\u5047\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff08\u4e5f\u7ed5\u8fc7\u4e86\u8ba1\u7b97\u91cf\u5927\u7684\u53bb\u566a\u8fc7\u7a0b\uff09\u6709\u52a9\u4e8e\u6784\u5efa\u4e00\u4e2a\u8f83\u5c11\u5173\u6ce8\u865a\u5047\u76f8\u5173\u6027\u7684\u68c0\u6d4b\u5668\uff0c\u800c\u73b0\u6709\u7684\u975e\u5e38\u6d41\u884c\u7684\u65b9\u6cd5\u5f88\u5bb9\u6613\u53d7\u5230\u8fd9\u79cd\u76f8\u5173\u6027\u7684\u5f71\u54cd\u3002\u6700\u540e\uff0c\u4e3a\u4e86\u8bc1\u660e\u6570\u636e\u96c6\u4e2d\u5bf9\u9f50\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u4f7f\u7528\u975e\u81ea\u7136\u5bf9\u8c61\u7684\u56fe\u50cf\u6784\u5efa\u4e86\u4e00\u4e2a\u68c0\u6d4b\u5668\uff0c\u5e76\u83b7\u5f97\u4e86\u53ef\u559c\u7684\u7ed3\u679c\u3002\u603b\u7684\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u786e\u5b9a\u4e86\u5728\u8bad\u7ec3\u865a\u5047\u56fe\u50cf\u68c0\u6d4b\u5668\u65f6\u51fa\u73b0\u7684\u7ec6\u5fae\u4f46\u91cd\u8981\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u5ec9\u4ef7\u7684\u89e3\u51b3\u65b9\u6848\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002||\n", "2410.11826": "|**2024-10-15**|[Bayesian Experimental Design via Contrastive Diffusions](http://arxiv.org/abs/2410.11826)|**[link](https://github.com/jcopo/ContrastiveDiffusions)**|\u8d1d\u53f6\u65af\u6700\u4f18\u5b9e\u9a8c\u8bbe\u8ba1 (BOED) \u662f\u4e00\u79cd\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u964d\u4f4e\u8fd0\u884c\u4e00\u7cfb\u5217\u5b9e\u9a8c\u7684\u6210\u672c\u3002\u5f53\u57fa\u4e8e\u9884\u671f\u4fe1\u606f\u589e\u76ca (EIG) \u65f6\uff0c\u8bbe\u8ba1\u4f18\u5316\u5bf9\u5e94\u4e8e\u6700\u5927\u5316\u5148\u9a8c\u5206\u5e03\u548c\u540e\u9a8c\u5206\u5e03\u4e4b\u95f4\u67d0\u4e9b\u96be\u4ee5\u5904\u7406\u7684\u9884\u671f\u201c\u5bf9\u6bd4\u201d\u3002\u7531\u4e8e BOED \u56fa\u6709\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5c06\u8fd9\u79cd\u6700\u5927\u5316\u6269\u5c55\u5230\u9ad8\u7ef4\u548c\u590d\u6742\u7684\u73af\u5883\u4e00\u76f4\u662f\u4e00\u4e2a\u95ee\u9898\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u91c7\u6837\u7279\u6027\u7684\u201c\u9884\u671f\u540e\u9a8c\u201d\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u65b0\u7684 EIG \u68af\u5ea6\u8868\u8fbe\u5f0f\u63d0\u4f9b\u4e86\u5bf9 EIG \u5bf9\u6bd4\u5ea6\u6700\u5927\u5316\u7684\u6613\u5904\u7406\u8bbf\u95ee\u3002\u57fa\u4e8e\u6269\u6563\u7684\u91c7\u6837\u5668\u7528\u4e8e\u8ba1\u7b97\u9884\u671f\u540e\u9a8c\u7684\u52a8\u6001\uff0c\u5e76\u4e14\u5229\u7528\u53cc\u5c42\u4f18\u5316\u7684\u601d\u60f3\u6765\u63a8\u5bfc\u51fa\u9ad8\u6548\u7684\u8054\u5408\u91c7\u6837\u4f18\u5316\u5faa\u73af\uff0c\u800c\u65e0\u9700\u8bc9\u8bf8 EIG \u7684\u4e0b\u754c\u8fd1\u4f3c\u3002\u7531\u6b64\u4ea7\u751f\u7684\u6548\u7387\u63d0\u5347\u5141\u8bb8\u5c06 BOED \u6269\u5c55\u5230\u7ecf\u8fc7\u5145\u5206\u6d4b\u8bd5\u7684\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u3002\u901a\u8fc7\u5c06\u751f\u6210\u6a21\u578b\u7eb3\u5165 BOED \u6846\u67b6\uff0c\u6211\u4eec\u6269\u5c55\u4e86\u5b83\u7684\u8303\u56f4\u53ca\u5176\u5728\u4ee5\u524d\u4e0d\u5207\u5b9e\u9645\u7684\u573a\u666f\u4e2d\u7684\u4f7f\u7528\u3002\u6570\u503c\u5b9e\u9a8c\u548c\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6bd4\u8f83\u663e\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u6f5c\u529b\u3002||\n", "2410.11824": "|**2024-10-15**|[KITTEN: A Knowledge-Intensive Evaluation of Image Generation on Visual Entities](http://arxiv.org/abs/2410.11824)|null|\u6700\u8fd1\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6280\u672f\u7684\u8fdb\u6b65\u663e\u8457\u63d0\u9ad8\u4e86\u5408\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u8fd9\u4e9b\u8fdb\u5c55\uff0c\u4f46\u8bc4\u4f30\u4e3b\u8981\u96c6\u4e2d\u5728\u5ba1\u7f8e\u60c5\u8da3\u6216\u4e0e\u6587\u672c\u63d0\u793a\u7684\u4e00\u81f4\u6027\u4e0a\u3002\u56e0\u6b64\uff0c\u4eba\u4eec\u5bf9\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u80fd\u591f\u51c6\u786e\u5730\u8868\u793a\u5404\u79cd\u73b0\u5b9e\u4e16\u754c\u7684\u89c6\u89c9\u5b9e\u4f53\u2014\u2014\u4e00\u9879\u9700\u8981\u73b0\u5b9e\u4e16\u754c\u77e5\u8bc6\u7684\u4efb\u52a1\u2014\u2014\u77e5\u4e4b\u751a\u5c11\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91cd\u70b9\u8bc4\u4f30\u73b0\u5b9e\u4e16\u754c\u5b9e\u4f53\u7684\u77e5\u8bc6\u5bc6\u96c6\u578b\u56fe\u50cf\u751f\u6210\uff08\u5373 KITTEN\uff09\u3002\u6211\u4eec\u4f7f\u7528 KITTEN \u5bf9\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5b9e\u4f53\u4fdd\u771f\u5ea6\u8fdb\u884c\u4e86\u7cfb\u7edf\u7814\u7a76\uff0c\u91cd\u70b9\u5173\u6ce8\u5b83\u4eec\u751f\u6210\u5404\u79cd\u73b0\u5b9e\u4e16\u754c\u89c6\u89c9\u5b9e\u4f53\u7684\u80fd\u529b\uff0c\u5982\u5730\u6807\u5efa\u7b51\u3001\u98de\u673a\u3001\u690d\u7269\u548c\u52a8\u7269\u3002\u6211\u4eec\u4f7f\u7528\u81ea\u52a8\u6307\u6807\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4eba\u5de5\u8bc4\u4f30\u6765\u8bc4\u4f30\u6700\u65b0\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u548c\u68c0\u7d22\u589e\u5f3a\u5b9a\u5236\u6a21\u578b\uff0c\u91cd\u70b9\u5173\u6ce8\u751f\u6210\u56fe\u50cf\u4e2d\u5b9e\u4f53\u7684\u4fdd\u771f\u5ea6\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e5f\u5e38\u5e38\u65e0\u6cd5\u751f\u6210\u5177\u6709\u51c6\u786e\u89c6\u89c9\u7ec6\u8282\u7684\u5b9e\u4f53\u3002\u5c3d\u7ba1\u68c0\u7d22\u589e\u5f3a\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u5728\u6d4b\u8bd5\u671f\u95f4\u5408\u5e76\u53c2\u8003\u56fe\u50cf\u6765\u589e\u5f3a\u5b9e\u4f53\u7684\u4fdd\u771f\u5ea6\uff0c\u4f46\u5b83\u4eec\u5f80\u5f80\u8fc7\u5ea6\u4f9d\u8d56\u4e8e\u8fd9\u4e9b\u53c2\u8003\uff0c\u5e76\u4e14\u96be\u4ee5\u6839\u636e\u521b\u610f\u6587\u672c\u63d0\u793a\u751f\u6210\u5b9e\u4f53\u7684\u65b0\u9896\u914d\u7f6e\u3002||\n", "2410.11817": "|**2024-10-15**|[Improving Long-Text Alignment for Text-to-Image Diffusion Models](http://arxiv.org/abs/2410.11817)|**[link](https://github.com/luping-liu/longalign)**|\u6587\u672c\u5230\u56fe\u50cf (T2I) \u6269\u6563\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u5176\u80fd\u591f\u6839\u636e\u7ed9\u5b9a\u6587\u672c\u751f\u6210\u524d\u6240\u672a\u6709\u7684\u7ed3\u679c\u3002\u7136\u800c\uff0c\u968f\u7740\u6587\u672c\u8f93\u5165\u53d8\u957f\uff0c\u50cf CLIP \u8fd9\u6837\u7684\u73b0\u6709\u7f16\u7801\u65b9\u6cd5\u9762\u4e34\u5c40\u9650\u6027\uff0c\u5e76\u4e14\u4f7f\u751f\u6210\u7684\u56fe\u50cf\u4e0e\u957f\u6587\u672c\u5bf9\u9f50\u53d8\u5f97\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 LongAlign\uff0c\u5b83\u5305\u62ec\u7528\u4e8e\u5904\u7406\u957f\u6587\u672c\u7684\u5206\u6bb5\u7ea7\u7f16\u7801\u65b9\u6cd5\u548c\u7528\u4e8e\u6709\u6548\u5bf9\u9f50\u8bad\u7ec3\u7684\u5206\u89e3\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u3002\u5bf9\u4e8e\u5206\u6bb5\u7ea7\u7f16\u7801\uff0c\u957f\u6587\u672c\u88ab\u5206\u6210\u591a\u4e2a\u6bb5\u5e76\u5206\u522b\u5904\u7406\u3002\u6b64\u65b9\u6cd5\u514b\u670d\u4e86\u9884\u8bad\u7ec3\u7f16\u7801\u6a21\u578b\u7684\u6700\u5927\u8f93\u5165\u957f\u5ea6\u9650\u5236\u3002\u5bf9\u4e8e\u504f\u597d\u4f18\u5316\uff0c\u6211\u4eec\u63d0\u4f9b\u57fa\u4e8e CLIP \u7684\u5206\u89e3\u504f\u597d\u6a21\u578b\u6765\u5fae\u8c03\u6269\u6563\u6a21\u578b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4e3a\u4e86\u5229\u7528\u57fa\u4e8e CLIP \u7684\u504f\u597d\u6a21\u578b\u8fdb\u884c T2I \u5bf9\u9f50\uff0c\u6211\u4eec\u6df1\u5165\u7814\u7a76\u4e86\u5b83\u4eec\u7684\u8bc4\u5206\u673a\u5236\uff0c\u53d1\u73b0\u504f\u597d\u5206\u6570\u53ef\u4ee5\u5206\u89e3\u4e3a\u4e24\u4e2a\u90e8\u5206\uff1a\u8861\u91cf T2I \u5bf9\u9f50\u7684\u6587\u672c\u76f8\u5173\u90e8\u5206\u548c\u8bc4\u4f30\u4eba\u7c7b\u504f\u597d\u7684\u5176\u4ed6\u89c6\u89c9\u65b9\u9762\u7684\u6587\u672c\u65e0\u5173\u90e8\u5206\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\u6587\u672c\u65e0\u5173\u90e8\u5206\u4f1a\u5bfc\u81f4\u5fae\u8c03\u671f\u95f4\u51fa\u73b0\u5e38\u89c1\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u91cd\u65b0\u52a0\u6743\u7b56\u7565\uff0c\u4e3a\u8fd9\u4e24\u4e2a\u90e8\u5206\u5206\u914d\u4e0d\u540c\u7684\u6743\u91cd\uff0c\u4ece\u800c\u51cf\u5c11\u8fc7\u62df\u5408\u5e76\u589e\u5f3a\u5bf9\u9f50\u3002\u5728\u6211\u4eec\u4f7f\u7528\u8be5\u65b9\u6cd5\u5bf9 $512 \\times 512$ Stable Diffusion (SD) v1.5 \u8fdb\u884c\u7ea6 20 \u5c0f\u65f6\u7684\u5fae\u8c03\u540e\uff0c\u5fae\u8c03\u540e\u7684 SD \u5728 T2I \u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u66f4\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\uff0c\u4f8b\u5982 PixArt-$\\alpha$ \u548c Kandinsky v2.2\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/luping-liu/LongAlign \u83b7\u53d6\u3002||\n", "2410.11815": "|**2024-10-15**|[SGEdit: Bridging LLM with Text2Image Generative Model for Scene Graph-based Image Editing](http://arxiv.org/abs/2410.11815)|null|\u573a\u666f\u56fe\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u7684\u56fe\u50cf\u5c42\u6b21\u8868\u793a\uff0c\u5176\u4e2d\u8282\u70b9\u548c\u8fb9\u5206\u522b\u4ee3\u8868\u5bf9\u8c61\u53ca\u5176\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u5b83\u53ef\u4ee5\u4f5c\u4e3a\u56fe\u50cf\u7f16\u8f91\u7684\u81ea\u7136\u63a5\u53e3\uff0c\u6781\u5927\u5730\u63d0\u9ad8\u7cbe\u5ea6\u548c\u7075\u6d3b\u6027\u3002\u5229\u7528\u8fd9\u4e00\u4f18\u52bf\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e Text2Image \u751f\u6210\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u57fa\u4e8e\u573a\u666f\u56fe\u7684\u56fe\u50cf\u7f16\u8f91\u3002\u8fd9\u79cd\u96c6\u6210\u53ef\u4ee5\u5728\u4e0d\u5f71\u54cd\u6574\u4f53\u56fe\u50cf\u5b8c\u6574\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u5bf9\u8c61\u7ea7\u522b\u7684\u7cbe\u786e\u4fee\u6539\u548c\u573a\u666f\u7684\u521b\u9020\u6027\u91cd\u7ec4\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u4e3b\u8981\u9636\u6bb5\uff1a1\uff09\u5229\u7528 LLM \u9a71\u52a8\u7684\u573a\u666f\u89e3\u6790\u5668\uff0c\u6211\u4eec\u6784\u5efa\u56fe\u50cf\u7684\u573a\u666f\u56fe\uff0c\u6355\u83b7\u5173\u952e\u5bf9\u8c61\u53ca\u5176\u76f8\u4e92\u5173\u7cfb\uff0c\u5e76\u89e3\u6790\u7ec6\u7c92\u5ea6\u5c5e\u6027\uff0c\u5982\u5bf9\u8c61\u63a9\u7801\u548c\u63cf\u8ff0\u3002\u8fd9\u4e9b\u6ce8\u91ca\u6709\u52a9\u4e8e\u4f7f\u7528\u5fae\u8c03\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6982\u5ff5\u5b66\u4e60\uff0c\u7528\u4f18\u5316\u7684\u6807\u8bb0\u548c\u8be6\u7ec6\u7684\u63cf\u8ff0\u63d0\u793a\u6765\u8868\u793a\u6bcf\u4e2a\u5bf9\u8c61\u30022\uff09\u5728\u56fe\u50cf\u7f16\u8f91\u9636\u6bb5\uff0cLLM \u7f16\u8f91\u63a7\u5236\u5668\u5f15\u5bfc\u7f16\u8f91\u7279\u5b9a\u533a\u57df\u3002\u7136\u540e\uff0c\u8fd9\u4e9b\u7f16\u8f91\u7531\u6ce8\u610f\u529b\u8c03\u5236\u7684\u6269\u6563\u7f16\u8f91\u5668\u6267\u884c\uff0c\u5229\u7528\u5fae\u8c03\u6a21\u578b\u6267\u884c\u5bf9\u8c61\u6dfb\u52a0\u3001\u5220\u9664\u3001\u66ff\u6362\u548c\u8c03\u6574\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u6846\u67b6\u5728\u7f16\u8f91\u7cbe\u5ea6\u548c\u573a\u666f\u7f8e\u5b66\u65b9\u9762\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u3002||\n", "2410.11795": "|**2024-10-15**|[Efficient Diffusion Models: A Comprehensive Survey from Principles to Practices](http://arxiv.org/abs/2410.11795)|null|\u4f5c\u4e3a\u8fd1\u5e74\u6765\u6700\u53d7\u6b22\u8fce\u548c\u6700\u53d7\u8ffd\u6367\u7684\u751f\u6210\u6a21\u578b\u4e4b\u4e00\uff0c\u6269\u6563\u6a21\u578b\u51ed\u501f\u5176\u624e\u5b9e\u7684\u7406\u8bba\u57fa\u7840\u548c\u53ef\u9760\u7684\u5e94\u7528\u5b9e\u8df5\uff0c\u5f15\u8d77\u4e86\u4f17\u591a\u7814\u7a76\u8005\u7684\u5174\u8da3\uff0c\u5e76\u5728\u56fe\u50cf\u5408\u6210\u3001\u89c6\u9891\u751f\u6210\u3001\u5206\u5b50\u8bbe\u8ba1\u30013D\u573a\u666f\u6e32\u67d3\u548c\u591a\u6a21\u6001\u751f\u6210\u7b49\u5404\u79cd\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u3002\u8fd9\u4e9b\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6700\u65b0\u7814\u7a76\u6210\u679c\u7684\u663e\u8457\u6210\u529f\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5f52\u529f\u4e8e\u6e10\u8fdb\u5f0f\u8bbe\u8ba1\u539f\u5219\u4ee5\u53ca\u9ad8\u6548\u7684\u67b6\u6784\u3001\u8bad\u7ec3\u3001\u63a8\u7406\u548c\u90e8\u7f72\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u76ee\u524d\u5c1a\u7f3a\u4e4f\u5168\u9762\u6df1\u5165\u7684\u7efc\u8ff0\u6765\u603b\u7ed3\u8fd9\u4e9b\u539f\u5219\u548c\u5b9e\u8df5\uff0c\u4ee5\u5e2e\u52a9\u5feb\u901f\u7406\u89e3\u548c\u5e94\u7528\u6269\u6563\u6a21\u578b\u3002\u5728\u672c\u7efc\u8ff0\u4e2d\uff0c\u6211\u4eec\u4ee5\u6548\u7387\u4e3a\u5bfc\u5411\uff0c\u5bf9\u73b0\u6709\u5de5\u4f5c\u8fdb\u884c\u4e86\u65b0\u7684\u89c6\u89d2\u5ba1\u89c6\uff0c\u4e3b\u8981\u5173\u6ce8\u67b6\u6784\u8bbe\u8ba1\u3001\u6a21\u578b\u8bad\u7ec3\u3001\u5feb\u901f\u63a8\u7406\u548c\u53ef\u9760\u90e8\u7f72\u65b9\u9762\u7684\u6df1\u523b\u539f\u7406\u548c\u9ad8\u6548\u5b9e\u8df5\uff0c\u4ee5\u901a\u4fd7\u6613\u61c2\u7684\u65b9\u5f0f\u6307\u5bfc\u8fdb\u4e00\u6b65\u7684\u7406\u8bba\u7814\u7a76\u3001\u7b97\u6cd5\u8fc1\u79fb\u548c\u6a21\u578b\u5e94\u7528\u5230\u65b0\u7684\u573a\u666f\u4e2d\u3002\\url{https://github.com/ponyzym/Efficient-DMs-Survey}||\n", "2410.11735": "|**2024-10-15**|[Probabilistic Principles for Biophysics and Neuroscience: Entropy Production, Bayesian Mechanics & the Free-Energy Principle](http://arxiv.org/abs/2410.11735)|null|\u672c\u8bba\u6587\u91cd\u70b9\u7814\u7a76\u751f\u7269\u7cfb\u7edf\u7684\u4e09\u4e2a\u57fa\u672c\u65b9\u9762\uff1a\u5373\u71b5\u4ea7\u751f\u3001\u8d1d\u53f6\u65af\u529b\u5b66\u548c\u81ea\u7531\u80fd\u539f\u7406\u3002\u8d21\u732e\u6709\u4e09\u65b9\u9762\uff1a1) \u6211\u4eec\u8ba1\u7b97\u4e86\u6bd4\u4ee5\u5f80\u66f4\u5927\u7c7b\u522b\u7cfb\u7edf\u7684\u71b5\u4ea7\u751f\uff0c\u5305\u62ec\u51e0\u4e4e\u6240\u6709\u7a33\u6001\u6269\u6563\u8fc7\u7a0b\uff0c\u4f8b\u5982\u9a71\u52a8\u566a\u58f0\u4e0d\u4f5c\u7528\u4e8e\u7cfb\u7edf\u6240\u6709\u5750\u6807\u7684\u9000\u5316\u6269\u6563\u3002\u91cd\u8981\u7684\u662f\uff0c\u8fd9\u7c7b\u7cfb\u7edf\u5305\u542b\u4e86\u7531\u6709\u8272\u566a\u58f0\u9a71\u52a8\u7684\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u7684\u9a6c\u5c14\u53ef\u592b\u8fd1\u4f3c\uff0c\u8fd9\u4e00\u70b9\u610f\u4e49\u91cd\u5927\uff0c\u56e0\u4e3a\u5b8f\u89c2\u548c\u4e2d\u5c3a\u5ea6\u751f\u7269\u7cfb\u7edf\u901a\u5e38\u4f1a\u53d7\u5230\u6709\u8272\u566a\u58f0\u7684\u5f71\u54cd\u30022) \u6211\u4eec\u4e3a\u4e0e\u73af\u5883\u76f8\u4e92\u4f5c\u7528\u7684\u751f\u7269\u548c\u7269\u7406\u5b9e\u4f53\u5f00\u53d1\u4e86\u4e00\u79cd\u8d1d\u53f6\u65af\u529b\u5b66\uff0c\u5176\u4e2d\u6211\u4eec\u4e3a\u4e8b\u7269\u7684\u5185\u90e8\u72b6\u6001\u63a8\u65ad\u5176\u5916\u90e8\u72b6\u6001\u63d0\u4f9b\u4e86\u5145\u5206\u5fc5\u8981\u6761\u4ef6\uff0c\u8fd9\u4e0e\u7edf\u8ba1\u5b66\u548c\u7406\u8bba\u795e\u7ecf\u79d1\u5b66\u4e2d\u7684\u53d8\u5206\u8d1d\u53f6\u65af\u63a8\u7406\u4e00\u81f4\u30023) \u6211\u4eec\u6539\u8fdb\u4e86\u5bf9\u8d1d\u53f6\u65af\u529b\u5b66\u7684\u7ea6\u675f\uff0c\u4ee5\u83b7\u5f97\u5bf9\u751f\u7269\u7cfb\u7edf\u66f4\u5177\u4f53\u7684\u63cf\u8ff0\uff0c\u79f0\u4e3a\u81ea\u7531\u80fd\u539f\u7406\u3002\u8fd9\u8868\u660e\u751f\u7269\u7cfb\u7edf\u7684\u6d3b\u52a8\u72b6\u6001\u548c\u5185\u90e8\u72b6\u6001\u662f\u901a\u8fc7\u6700\u5c0f\u5316\u79f0\u4e3a\u81ea\u7531\u80fd\u7684\u91cf\u6765\u5c55\u5f00\u7684\u3002\u8fd9\u91cc\u63d0\u51fa\u7684\u81ea\u7531\u80fd\u539f\u7406\u7684\u6570\u5b66\u57fa\u7840\uff0c\u901a\u8fc7\u5728\u7ed9\u5b9a\u5916\u90e8\u72b6\u6001\u548c\u611f\u89c9\u72b6\u6001\u7684\u751f\u6210\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u6700\u5c0f\u5316\u81ea\u7531\u80fd\uff0c\u4e3a\u795e\u7ecf\u751f\u7269\u5b66\u548c\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u884c\u4e3a\u5efa\u6a21\u548c\u4eff\u771f\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b2c\u4e00\u6027\u539f\u7406\u65b9\u6cd5\u3002||\n", "2410.11730": "|**2024-10-15**|[Patch-Based Diffusion Models Beat Whole-Image Models for Mismatched Distribution Inverse Problems](http://arxiv.org/abs/2410.11730)|null|\u6269\u6563\u6a21\u578b\u7531\u4e8e\u80fd\u591f\u5b66\u4e60\u5f3a\u5927\u7684\u56fe\u50cf\u5148\u9a8c\uff0c\u5728\u89e3\u51b3\u9006\u95ee\u9898\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u529f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7684\u56fe\u50cf\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u56fe\u50cf\u5e94\u8be5\u6765\u81ea\u4e0e\u6d4b\u8bd5\u6570\u636e\u96c6\u76f8\u540c\u7684\u5206\u5e03\u3002\u5f53\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5206\u5e03\u4e0d\u5339\u914d\u65f6\uff0c\u7531\u4e8e\u5148\u9a8c\u4e0d\u6b63\u786e\uff0c\u91cd\u5efa\u56fe\u50cf\u4e2d\u4f1a\u51fa\u73b0\u4f2a\u5f71\u548c\u5e7b\u89c9\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u5206\u5e03\u5916 (OOD) \u95ee\u9898\uff0c\u5176\u4e2d\u9996\u5148\u63d0\u4f9b\u5df2\u77e5\u7684\u8bad\u7ec3\u5206\u5e03\u3002\u6211\u4eec\u9996\u5148\u7814\u7a76\u4e86\u4ec5\u4ece\u672a\u77e5\u6d4b\u8bd5\u5206\u5e03\u83b7\u5f97\u5355\u6b21\u6d4b\u91cf\u7684\u60c5\u51b5\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5c5e\u4e8e\u6d4b\u8bd5\u5206\u5e03\u7684\u975e\u5e38\u5c0f\u7684\u6570\u636e\u6837\u672c\u53ef\u7528\u7684\u60c5\u51b5\uff0c\u6211\u4eec\u7684\u76ee\u6807\u4ecd\u7136\u662f\u4ece\u6765\u81ea\u6d4b\u8bd5\u5206\u5e03\u7684\u6d4b\u91cf\u4e2d\u91cd\u5efa\u56fe\u50cf\u3002\u5728\u8fd9\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u90fd\u4f7f\u7528\u57fa\u4e8e\u8865\u4e01\u7684\u6269\u6563\u5148\u9a8c\uff0c\u5b83\u4ec5\u4ece\u8865\u4e01\u4e2d\u5b66\u4e60\u56fe\u50cf\u5206\u5e03\u3002\u6b64\u5916\uff0c\u5728\u7b2c\u4e00\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u5305\u542b\u4e00\u4e2a\u81ea\u76d1\u7763\u635f\u5931\uff0c\u5e2e\u52a9\u7f51\u7edc\u8f93\u51fa\u4fdd\u6301\u4e0e\u6d4b\u91cf\u7684Consistency\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u8fd9\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0c\u57fa\u4e8e\u8865\u4e01\u7684\u65b9\u6cd5\u90fd\u53ef\u4ee5\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u91cd\u5efa\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u6574\u5e45\u56fe\u50cf\u6a21\u578b\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u53ef\u4ee5\u4f7f\u7528\u5927\u578b\u5206\u5e03\u5185\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u6574\u5e45\u56fe\u50cf\u6a21\u578b\u5982\u4f55\u5bb9\u6613\u51fa\u73b0\u8bb0\u5fc6\u548c\u8fc7\u62df\u5408\uff0c\u4ece\u800c\u5bfc\u81f4\u91cd\u5efa\u4e2d\u7684\u4f2a\u5f71\uff0c\u800c\u57fa\u4e8e\u8865\u4e01\u7684\u6a21\u578b\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002||\n", "2410.11584": "|**2024-10-15**|[DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object Manipulation via Preference-based Action Alignment](http://arxiv.org/abs/2410.11584)|null|In recent years, imitation learning has made progress in the field of robotic manipulation. However, it still faces challenges when dealing with complex long-horizon deformable object tasks, such as high-dimensional state spaces, complex dynamics, and multimodal action distributions. Traditional imitation learning methods often require a large amount of data and encounter distributional shifts and accumulative errors in these tasks. To address these issues, we propose a data-efficient general learning framework (DeformPAM) based on preference learning and reward-guided action selection. DeformPAM decomposes long-horizon tasks into multiple action primitives, utilizes 3D point cloud inputs and diffusion models to model action distributions, and trains an implicit reward model using human preference data. During the inference phase, the reward model scores multiple candidate actions, selecting the optimal action for execution, thereby reducing the occurrence of anomalous actions and improving task completion quality. Experiments conducted on three challenging real-world long-horizon deformable object manipulation tasks demonstrate the effectiveness of this method. Results show that DeformPAM improves both task completion quality and efficiency compared to baseline methods even with limited data. Code and data will be available at https://deform-pam.robotflow.ai.||\n", "2410.13855": "|**2024-10-17**|[Diffusing States and Matching Scores: A New Framework for Imitation Learning](http://arxiv.org/abs/2410.13855)|**[link](https://github.com/ziqian2000/smiling)**|\u5bf9\u6297\u6027\u6a21\u4eff\u5b66\u4e60\u4f20\u7edf\u4e0a\u88ab\u6784\u5efa\u4e3a\u5b66\u4e60\u5668\u548c\u5bf9\u6297\u6027\u9009\u62e9\u7684\u6210\u672c\u51fd\u6570\u4e4b\u95f4\u7684\u4e24\u4eba\u96f6\u548c\u535a\u5f08\uff0c\u56e0\u6b64\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u751f\u6210\u5bf9\u6297\u7f51\u7edc (GAN) \u7684\u987a\u5e8f\u6cdb\u5316\u3002\u8fd9\u79cd\u6846\u67b6\u7684\u4e00\u4e2a\u7a81\u51fa\u4f8b\u5b50\u662f\u751f\u6210\u5bf9\u6297\u6027\u6a21\u4eff\u5b66\u4e60 (GAIL)\u3002\u7136\u800c\uff0c\u8fd1\u5e74\u6765\uff0c\u6269\u6563\u6a21\u578b\u5df2\u6210\u4e3a GAN \u7684\u975e\u5bf9\u6297\u6027\u66ff\u4ee3\u65b9\u6848\uff0c\u5b83\u53ea\u9700\u8981\u901a\u8fc7\u56de\u5f52\u8bad\u7ec3\u4e00\u4e2a\u8bc4\u5206\u51fd\u6570\uff0c\u5c31\u80fd\u4ea7\u751f\u66f4\u9ad8\u8d28\u91cf\u7684\u751f\u6210\u7ed3\u679c\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5982\u4f55\u5c06\u6269\u6563\u6a21\u578b\u7684\u89c1\u89e3\u63d0\u5347\u5230\u5e8f\u5217\u8bbe\u7f6e\u4e2d\u3002\u6211\u4eec\u5efa\u8bae\u6cbf\u7740\u6269\u6563\u72b6\u6001\u5bf9\u72b6\u6001\u8fdb\u884c\u6269\u6563\u5e76\u6267\u884c\u5206\u6570\u5339\u914d\uff0c\u4ee5\u6d4b\u91cf\u4e13\u5bb6\u548c\u5b66\u4e60\u8005\u72b6\u6001\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ea\u9700\u8981\u8bad\u7ec3\u8bc4\u5206\u51fd\u6570\u4ee5\u901a\u8fc7\u6807\u51c6\u56de\u5f52\u6765\u9884\u6d4b\u566a\u58f0\uff0c\u8fd9\u4f7f\u5f97\u5b83\u6bd4\u5bf9\u6297\u6027\u65b9\u6cd5\u66f4\u5bb9\u6613\u8bad\u7ec3\u4e14\u66f4\u7a33\u5b9a\u3002\u7406\u8bba\u4e0a\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5177\u6709\u4e00\u9636\u548c\u4e8c\u9636\u5b9e\u4f8b\u4f9d\u8d56\u754c\u9650\u4e14\u6c34\u5e73\u7ebf\u6027\u7f29\u653e\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u907f\u514d\u4e86\u963b\u788d\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u7684\u590d\u5408\u8bef\u5dee\u3002\u6839\u636e\u7ecf\u9a8c\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u8fde\u7eed\u63a7\u5236\u95ee\u9898\u4e0a\u4f18\u4e8e GAN \u98ce\u683c\u7684\u6a21\u4eff\u5b66\u4e60\u57fa\u7ebf\uff0c\u5305\u62ec\u63a7\u5236\u4eff\u4eba\u673a\u5668\u4eba\u884c\u8d70\u3001\u5750\u4e0b\u548c\u722c\u884c\u7684\u590d\u6742\u4efb\u52a1\u3002||\n", "2410.13850": "|**2024-10-17**|[Influence Functions for Scalable Data Attribution in Diffusion Models](http://arxiv.org/abs/2410.13850)|null|\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u5f0f\u5efa\u6a21\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u5b83\u4eec\u7684\u5e7f\u6cdb\u5e94\u7528\u5bf9\u6570\u636e\u6eaf\u6e90\u548c\u53ef\u89e3\u91ca\u6027\u63d0\u51fa\u4e86\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u901a\u8fc7\u5f00\u53d1\u4e00\u4e2a\\textit{\u5f71\u54cd\u51fd\u6570}\u6846\u67b6\u6765\u5e2e\u52a9\u89e3\u51b3\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6b64\u7c7b\u6311\u6218\u3002\u57fa\u4e8e\u5f71\u54cd\u51fd\u6570\u7684\u6570\u636e\u6eaf\u6e90\u65b9\u6cd5\u8fd1\u4f3c\u4e8e\u5982\u679c\u5220\u9664\u67d0\u4e9b\u8bad\u7ec3\u6570\u636e\uff0c\u6a21\u578b\u7684\u8f93\u51fa\u5c06\u5982\u4f55\u53d8\u5316\u3002\u5728\u76d1\u7763\u5b66\u4e60\u4e2d\uff0c\u8fd9\u901a\u5e38\u7528\u4e8e\u9884\u6d4b\u7279\u5b9a\u6837\u672c\u7684\u635f\u5931\u5c06\u5982\u4f55\u53d8\u5316\u3002\u5bf9\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u6211\u4eec\u4e13\u6ce8\u4e8e\u901a\u8fc7\u51e0\u4e2a\u4ee3\u7406\u6307\u6807\u6765\u9884\u6d4b\u751f\u6210\u7279\u5b9a\u6837\u672c\u7684\u6982\u7387\u53d8\u5316\u3002\u6211\u4eec\u5c55\u793a\u4e86\u5982\u4f55\u4e3a\u6b64\u7c7b\u91cf\u5236\u5b9a\u5f71\u54cd\u51fd\u6570\uff0c\u4ee5\u53ca\u5982\u4f55\u5c06\u5148\u524d\u63d0\u51fa\u7684\u65b9\u6cd5\u89e3\u91ca\u4e3a\u6211\u4eec\u6846\u67b6\u4e2d\u7684\u7279\u5b9a\u8bbe\u8ba1\u9009\u62e9\u3002\u4e3a\u4e86\u786e\u4fdd\u5f71\u54cd\u51fd\u6570\u4e2dHessian\u8ba1\u7b97\u7684\u53ef\u6269\u5c55\u6027\uff0c\u6211\u4eec\u7cfb\u7edf\u5730\u5f00\u53d1\u4e86\u57fa\u4e8e\u5e7f\u4e49\u9ad8\u65af-\u725b\u987f\u77e9\u9635\u7684K-FAC\u8fd1\u4f3c\uff0c\u4e13\u95e8\u9488\u5bf9\u6269\u6563\u6a21\u578b\u91cf\u8eab\u5b9a\u5236\u3002\u6211\u4eec\u5c06\u5148\u524d\u63d0\u51fa\u7684\u65b9\u6cd5\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6211\u4eec\u6846\u67b6\u4e2d\u7684\u7279\u5b9a\u8bbe\u8ba1\u9009\u62e9\uff0c\u5e76\u8868\u660e\u6211\u4eec\u63a8\u8350\u7684\u65b9\u6cd5\u5728\u5e38\u89c1\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u5148\u524d\u7684\u6570\u636e\u6eaf\u6e90\u65b9\u6cd5\uff0c\u4f8b\u5982\u7ebf\u6027\u6570\u636e\u5efa\u6a21\u5206\u6570\uff08LDS\uff09\u6216\u4e0d\u5305\u62ec\u9876\u90e8\u5f71\u54cd\u7684\u91cd\u65b0\u8bad\u7ec3\uff0c\u800c\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u65b9\u6cd5\u8fdb\u884c\u8d85\u53c2\u6570\u8c03\u6574\u3002||\n", "2410.13832": "|**2024-10-17**|[VidPanos: Generative Panoramic Videos from Casual Panning Videos](http://arxiv.org/abs/2410.13832)|null|\u5168\u666f\u56fe\u50cf\u62fc\u63a5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5e7f\u89d2\u573a\u666f\u89c6\u56fe\uff0c\u8d85\u8d8a\u4e86\u76f8\u673a\u7684\u89c6\u91ce\u8303\u56f4\u3002\u5c06\u5e73\u79fb\u89c6\u9891\u7684\u5e27\u62fc\u63a5\u6210\u5168\u666f\u7167\u7247\u5bf9\u4e8e\u9759\u6001\u573a\u666f\u6765\u8bf4\u662f\u4e00\u4e2a\u5f88\u597d\u7406\u89e3\u7684\u95ee\u9898\uff0c\u4f46\u662f\u5f53\u7269\u4f53\u79fb\u52a8\u65f6\uff0c\u9759\u6001\u5168\u666f\u56fe\u65e0\u6cd5\u6355\u6349\u573a\u666f\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u968f\u610f\u62cd\u6444\u7684\u5e73\u79fb\u89c6\u9891\u5408\u6210\u5168\u666f\u89c6\u9891\u7684\u65b9\u6cd5\uff0c\u5c31\u597d\u50cf\u539f\u59cb\u89c6\u9891\u662f\u7528\u5e7f\u89d2\u76f8\u673a\u62cd\u6444\u7684\u4e00\u6837\u3002\u6211\u4eec\u5c06\u5168\u666f\u5408\u6210\u89c6\u4e3a\u4e00\u4e2a\u65f6\u7a7a\u5916\u63a8\u95ee\u9898\uff0c\u76ee\u6807\u662f\u521b\u5efa\u4e00\u4e2a\u4e0e\u8f93\u5165\u89c6\u9891\u957f\u5ea6\u76f8\u540c\u7684\u5b8c\u6574\u5168\u666f\u89c6\u9891\u3002\u65f6\u7a7a\u4f53\u79ef\u7684\u4e00\u81f4\u6027\u5b8c\u6210\u9700\u8981\u5bf9\u89c6\u9891\u5185\u5bb9\u548c\u8fd0\u52a8\u8fdb\u884c\u5f3a\u5927\u800c\u771f\u5b9e\u7684\u5148\u9a8c\uff0c\u4e3a\u6b64\u6211\u4eec\u91c7\u7528\u4e86\u751f\u6210\u5f0f\u89c6\u9891\u6a21\u578b\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u751f\u6210\u5f0f\u6a21\u578b\u5e76\u4e0d\u80fd\u7acb\u5373\u6269\u5c55\u5230\u5168\u666f\u8865\u5168\uff0c\u6b63\u5982\u6211\u4eec\u6240\u5c55\u793a\u7684\u90a3\u6837\u3002\u76f8\u53cd\uff0c\u6211\u4eec\u5c06\u89c6\u9891\u751f\u6210\u4f5c\u4e3a\u5168\u666f\u5408\u6210\u7cfb\u7edf\u7684\u4e00\u4e2a\u7ec4\u6210\u90e8\u5206\uff0c\u5e76\u6f14\u793a\u4e86\u5982\u4f55\u5728\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5176\u5c40\u9650\u6027\u7684\u540c\u65f6\u5229\u7528\u6a21\u578b\u7684\u4f18\u52bf\u3002\u6211\u4eec\u7684\u7cfb\u7edf\u53ef\u4ee5\u4e3a\u5404\u79cd\u91ce\u5916\u573a\u666f\u521b\u5efa\u89c6\u9891\u5168\u666f\u56fe\uff0c\u5305\u62ec\u4eba\u3001\u8f66\u8f86\u548c\u6d41\u52a8\u7684\u6c34\uff0c\u4ee5\u53ca\u9759\u6b62\u7684\u80cc\u666f\u7279\u5f81\u3002||\n", "2410.13823": "|**2024-10-17**|[Deep Generative Models Unveil Patterns in Medical Images Through Vision-Language Conditioning](http://arxiv.org/abs/2410.13823)|**[link](https://github.com/junzhin/dgm-vlc)**|\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u901a\u8fc7\u589e\u5f3a\u6570\u636e\u96c6\u7684\u5927\u5c0f\u548c\u8d28\u91cf\uff0c\u6781\u5927\u5730\u4fc3\u8fdb\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u53d1\u5c55\u3002\u9664\u4e86\u5355\u7eaf\u7684\u6570\u636e\u589e\u5f3a\u4e4b\u5916\uff0c\u6211\u4eec\u7814\u7a76\u7684\u91cd\u70b9\u5728\u4e8e\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7684\u53e6\u4e00\u4e2a\u91cd\u8981\u80fd\u529b\uff1a\u63ed\u793a\u548c\u5c55\u793a\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u6a21\u5f0f\u3002\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u79cd\u5177\u6709\u6df7\u5408\u6761\u4ef6\u7684\u751f\u6210\u7ed3\u6784\uff0c\u7ed3\u5408\u4e34\u5e8a\u6570\u636e\u548c\u5206\u5272\u63a9\u7801\u6765\u6307\u5bfc\u56fe\u50cf\u5408\u6210\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u521b\u65b0\u5730\u5c06\u8868\u683c\u5316\u7684\u4e34\u5e8a\u6570\u636e\u8f6c\u6362\u4e3a\u6587\u672c\u63cf\u8ff0\u3002\u8fd9\u79cd\u65b9\u6cd5\u7b80\u5316\u4e86\u7f3a\u5931\u503c\u7684\u5904\u7406\uff0c\u5e76\u4f7f\u6211\u4eec\u80fd\u591f\u5229\u7528\u5927\u578b\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u4ee5\u7814\u7a76\u72ec\u7acb\u4e34\u5e8a\u6761\u76ee\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u7406\u89e3\u6027\u522b\u548c\u5438\u70df\u72b6\u51b5\u7b49\u4e00\u822c\u672f\u8bed\u3002\u7531\u4e8e\u6211\u4eec\u7684\u4e34\u5e8a\u4fe1\u606f\u4e0e\u56fe\u50cf\u4e4b\u95f4\u7684\u89c6\u89c9\u76f8\u5173\u6027\u8f83\u4f4e\uff0c\u56e0\u6b64\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u540c\u4e8e\u4f20\u7edf\u7684\u533b\u5b66\u62a5\u544a\u6307\u5bfc\u7684\u5408\u6210\uff0c\u5e76\u4e14\u63d0\u51fa\u4e86\u4e00\u9879\u66f4\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u6587\u672c-\u89c6\u89c9\u5d4c\u5165\u673a\u5236\u6765\u52a0\u5f3a\u6761\u4ef6\uff0c\u786e\u4fdd\u7f51\u7edc\u6709\u6548\u5730\u5229\u7528\u6240\u63d0\u4f9b\u7684\u4fe1\u606f\u3002\u6211\u4eec\u7684\u6d41\u7a0b\u53ef\u63a8\u5e7f\u5230\u57fa\u4e8e GAN \u7684\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u3002\u5728\u80f8\u90e8 CT \u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\uff08\u7279\u522b\u5173\u6ce8\u5438\u70df\u72b6\u51b5\uff09\u8868\u660e\uff0c\u80ba\u90e8\u51fa\u73b0\u4e86\u4e00\u81f4\u7684\u5f3a\u5ea6\u53d8\u5316\uff0c\u8fd9\u4e0e\u4e34\u5e8a\u89c2\u5bdf\u7ed3\u679c\u4e00\u81f4\uff0c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u6355\u6349\u548c\u53ef\u89c6\u5316\u7279\u5b9a\u5c5e\u6027\u5bf9\u533b\u5b66\u56fe\u50cf\u6a21\u5f0f\u7684\u5f71\u54cd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e3a\u5229\u7528\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u65e9\u671f\u68c0\u6d4b\u548c\u7cbe\u786e\u53ef\u89c6\u5316\u590d\u6742\u7684\u4e34\u5e8a\u72b6\u51b5\u5f00\u8f9f\u4e86\u65b0\u7684\u9014\u5f84\u3002\u6240\u6709\u4ee3\u7801\u5747\u53ef\u5728 https://github.com/junzhin/DGM-VLC \u83b7\u53d6\u3002||\n", "2410.13807": "|**2024-10-17**|[ConsisSR: Delving Deep into Consistency in Diffusion-based Image Super-Resolution](http://arxiv.org/abs/2410.13807)|null|Real-world image super-resolution (Real-ISR) aims at restoring high-quality (HQ) images from low-quality (LQ) inputs corrupted by unknown and complex degradations. In particular, pretrained text-to-image (T2I) diffusion models provide strong generative priors to reconstruct credible and intricate details. However, T2I generation focuses on semantic consistency while Real-ISR emphasizes pixel-level reconstruction, which hinders existing methods from fully exploiting diffusion priors. To address this challenge, we introduce ConsisSR to handle both semantic and pixel-level consistency. Specifically, compared to coarse-grained text prompts, we exploit the more powerful CLIP image embedding and effectively leverage both modalities through our Hybrid Prompt Adapter (HPA) for semantic guidance. Secondly, we introduce Time-aware Latent Augmentation (TALA) to mitigate the inherent gap between T2I generation and Real-ISR consistency requirements. By randomly mixing LQ and HQ latent inputs, our model not only handle timestep-specific diffusion noise but also refine the accumulated latent representations. Last but not least, our GAN-Embedding strategy employs the pretrained Real-ESRGAN model to refine the diffusion start point. This accelerates the inference process to 10 steps while preserving sampling quality, in a training-free manner.Our method demonstrates state-of-the-art performance among both full-scale and accelerated models. The code will be made publicly available.||\n", "2410.13770": "|**2024-10-17**|[Probing the Latent Hierarchical Structure of Data via Diffusion Models](http://arxiv.org/abs/2410.13770)|null|High-dimensional data must be highly structured to be learnable. Although the compositional and hierarchical nature of data is often put forward to explain learnability, quantitative measurements establishing these properties are scarce. Likewise, accessing the latent variables underlying such a data structure remains a challenge. In this work, we show that forward-backward experiments in diffusion-based models, where data is noised and then denoised to generate new samples, are a promising tool to probe the latent structure of data. We predict in simple hierarchical models that, in this process, changes in data occur by correlated chunks, with a length scale that diverges at a noise level where a phase transition is known to take place. Remarkably, we confirm this prediction in both text and image datasets using state-of-the-art diffusion models. Our results show how latent variable changes manifest in the data and establish how to measure these effects in real data using diffusion models.||\n", "2410.13746": "|**2024-10-17**|[Theory on Score-Mismatched Diffusion Models and Zero-Shot Conditional Samplers](http://arxiv.org/abs/2410.13746)|null|The denoising diffusion model has recently emerged as a powerful generative technique, capable of transforming noise into meaningful data. While theoretical convergence guarantees for diffusion models are well established when the target distribution aligns with the training distribution, practical scenarios often present mismatches. One common case is in zero-shot conditional diffusion sampling, where the target conditional distribution is different from the (unconditional) training distribution. These score-mismatched diffusion models remain largely unexplored from a theoretical perspective. In this paper, we present the first performance guarantee with explicit dimensional dependencies for general score-mismatched diffusion samplers, focusing on target distributions with finite second moments. We show that score mismatches result in an asymptotic distributional bias between the target and sampling distributions, proportional to the accumulated mismatch between the target and training distributions. This result can be directly applied to zero-shot conditional samplers for any conditional model, irrespective of measurement noise. Interestingly, the derived convergence upper bound offers useful guidance for designing a novel bias-optimal zero-shot sampler in linear conditional models that minimizes the asymptotic bias. For such bias-optimal samplers, we further establish convergence guarantees with explicit dependencies on dimension and conditioning, applied to several interesting target distributions, including those with bounded support and Gaussian mixtures. Our findings are supported by numerical studies.||\n", "2410.13738": "|**2024-10-17**|[Improved Convergence Rate for Diffusion Probabilistic Models](http://arxiv.org/abs/2410.13738)|null|Score-based diffusion models have achieved remarkable empirical performance in the field of machine learning and artificial intelligence for their ability to generate high-quality new data instances from complex distributions. Improving our understanding of diffusion models, including mainly convergence analysis for such models, has attracted a lot of interests. Despite a lot of theoretical attempts, there still exists significant gap between theory and practice. Towards to close this gap, we establish an iteration complexity at the order of $d^{1/3}\\varepsilon^{-2/3}$, which is better than $d^{5/12}\\varepsilon^{-1}$, the best known complexity achieved before our work. This convergence analysis is based on a randomized midpoint method, which is first proposed for log-concave sampling (Shen and Lee, 2019), and then extended to diffusion models by Gupta et al. (2024). Our theory accommodates $\\varepsilon$-accurate score estimates, and does not require log-concavity on the target distribution. Moreover, the algorithm can also be parallelized to run in only $O(\\log^2(d/\\varepsilon))$ parallel rounds in a similar way to prior works.||\n", "2410.13735": "|**2024-10-17**|[Optimizing Probabilistic Conformal Prediction with Vectorized Non-Conformity Scores](http://arxiv.org/abs/2410.13735)|null|Generative models have shown significant promise in critical domains such as medical diagnosis, autonomous driving, and climate science, where reliable decision-making hinges on accurate uncertainty quantification. While probabilistic conformal prediction (PCP) offers a powerful framework for this purpose, its coverage efficiency -- the size of the uncertainty set -- is limited when dealing with complex underlying distributions and a finite number of generated samples. In this paper, we propose a novel PCP framework that enhances efficiency by first vectorizing the non-conformity scores with ranked samples and then optimizing the shape of the prediction set by varying the quantiles for samples at the same rank. Our method delivers valid coverage while producing discontinuous and more efficient prediction sets, making it particularly suited for high-stakes applications. We demonstrate the effectiveness of our approach through experiments on both synthetic and real-world datasets.||\n", "2410.13726": "|**2024-10-17**|[DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation](http://arxiv.org/abs/2410.13726)|**[link](https://github.com/hanbo-cheng/dawn-pytorch)**|Talking head generation intends to produce vivid and realistic talking head videos from a single portrait and speech audio clip. Although significant progress has been made in diffusion-based talking head generation, almost all methods rely on autoregressive strategies, which suffer from limited context utilization beyond the current generation step, error accumulation, and slower generation speed. To address these challenges, we present DAWN (Dynamic frame Avatar With Non-autoregressive diffusion), a framework that enables all-at-once generation of dynamic-length video sequences. Specifically, it consists of two main components: (1) audio-driven holistic facial dynamics generation in the latent motion space, and (2) audio-driven head pose and blink generation. Extensive experiments demonstrate that our method generates authentic and vivid videos with precise lip motions, and natural pose/blink movements. Additionally, with a high generation speed, DAWN possesses strong extrapolation capabilities, ensuring the stable production of high-quality long videos. These results highlight the considerable promise and potential impact of DAWN in the field of talking head video generation. Furthermore, we hope that DAWN sparks further exploration of non-autoregressive approaches in diffusion models. Our code will be publicly at https://github.com/Hanbo-Cheng/DAWN-pytorch.||\n"}, "LLM": {"2409.01909": "|**2024-09-03**|[LUK: Empowering Log Understanding with Expert Knowledge from Large Language Models](http://arxiv.org/abs/2409.01909)|**[link](https://github.com/LeaperOvO/LUK)**|Logs play a critical role in providing essential information for system monitoring and troubleshooting. Recently, with the success of pre-trained language models (PLMs) and large language models (LLMs) in natural language processing (NLP), smaller PLMs (such as BERT) and LLMs (like ChatGPT) have become the current mainstream approaches for log analysis. While LLMs possess rich knowledge, their high computational costs and unstable performance make LLMs impractical for analyzing logs directly. In contrast, smaller PLMs can be fine-tuned for specific tasks even with limited computational resources, making them more practical. However, these smaller PLMs face challenges in understanding logs comprehensively due to their limited expert knowledge. To better utilize the knowledge embedded within LLMs for log understanding, this paper introduces a novel knowledge enhancement framework, called LUK, which acquires expert knowledge from LLMs to empower log understanding on a smaller PLM. Specifically, we design a multi-expert collaboration framework based on LLMs consisting of different roles to acquire expert knowledge. In addition, we propose two novel pre-training tasks to enhance the log pre-training with expert knowledge. LUK achieves state-of-the-art results on different log analysis tasks and extensive experiments demonstrate expert knowledge from LLMs can be utilized more effectively to understand logs.||\n", "2409.00702": "|**2024-09-04**|[MARS: Matching Attribute-aware Representations for Text-based Sequential Recommendation](http://arxiv.org/abs/2409.00702)|**[link](https://github.com/junieberry/mars)**|Sequential recommendation aims to predict the next item a user is likely to prefer based on their sequential interaction history. Recently, text-based sequential recommendation has emerged as a promising paradigm that uses pre-trained language models to exploit textual item features to enhance performance and facilitate knowledge transfer to unseen datasets. However, existing text-based recommender models still struggle with two key challenges: (i) representing users and items with multiple attributes, and (ii) matching items with complex user interests. To address these challenges, we propose a novel model, Matching Attribute-aware Representations for Text-based Sequential Recommendation (MARS). MARS extracts detailed user and item representations through attribute-aware text encoding, capturing diverse user intents with multiple attribute-aware representations. It then computes user-item scores via attribute-wise interaction matching, effectively capturing attribute-level user preferences. Our extensive experiments demonstrate that MARS significantly outperforms existing sequential models, achieving improvements of up to 24.43% and 29.26% in Recall@10 and NDCG@10 across five benchmark datasets. Code is available at https://github.com/junieberry/MARS||\n", "2409.00323": "|**2024-08-31**|[From Prediction to Application: Language Model-based Code Knowledge Tracing with Domain Adaptive Pre-Training and Automatic Feedback System with Pedagogical Prompting for Comprehensive Programming Education](http://arxiv.org/abs/2409.00323)|null|Knowledge Tracing (KT) is a critical component in online learning, but traditional approaches face limitations in interpretability and cross-domain adaptability. This paper introduces Language Model-based Code Knowledge Tracing (CodeLKT), an innovative application of Language model-based Knowledge Tracing (LKT) to programming education. CodeLKT leverages pre-trained language models to process learning data, demonstrating superior performance over existing KT and Code KT models. We explore Domain Adaptive Pre-Training (DAPT) and Task Adaptive Pre-Training (TAPT), showing enhanced performance in the coding domain and investigating cross-domain transfer between mathematics and coding. Additionally, we present an theoretically-informed integrated system combining CodeLKT with large language models to generate personalized, in-depth feedback to support students' programming learning. This work advances the field of Code Knowledge Tracing by expanding the knowledge base with language model-based approach and offering practical implications for programming education through data-informed feedback.||\n", "2408.17354": "|**2024-08-30**|[Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage](http://arxiv.org/abs/2408.17354)|null|Fine-tuning large language models on private data for downstream applications poses significant privacy risks in potentially exposing sensitive information. Several popular community platforms now offer convenient distribution of a large variety of pre-trained models, allowing anyone to publish without rigorous verification. This scenario creates a privacy threat, as pre-trained models can be intentionally crafted to compromise the privacy of fine-tuning datasets. In this study, we introduce a novel poisoning technique that uses model-unlearning as an attack tool. This approach manipulates a pre-trained language model to increase the leakage of private data during the fine-tuning process. Our method enhances both membership inference and data extraction attacks while preserving model utility. Experimental results across different models, datasets, and fine-tuning setups demonstrate that our attacks significantly surpass baseline performance. This work serves as a cautionary note for users who download pre-trained models from unverified sources, highlighting the potential risks involved.||\n", "2408.14505": "|**2024-08-24**|[Empowering Pre-Trained Language Models for Spatio-Temporal Forecasting via Decoupling Enhanced Discrete Reprogramming](http://arxiv.org/abs/2408.14505)|null|Spatio-temporal time series forecasting plays a critical role in various real-world applications, such as transportation optimization, energy management, and climate analysis. The recent advancements in Pre-trained Language Models (PLMs) have inspired efforts to reprogram these models for time series forecasting tasks, by leveraging their superior reasoning and generalization capabilities. However, existing approaches fall short in handling complex spatial inter-series dependencies and intrinsic intra-series frequency components, limiting their spatio-temporal forecasting performance. Moreover, the linear mapping of continuous time series to a compressed subset vocabulary in reprogramming constrains the spatio-temporal semantic expressivity of PLMs and may lead to potential information bottleneck. To overcome the above limitations, we propose \\textsc{RePST}, a tailored PLM reprogramming framework for spatio-temporal forecasting. The key insight of \\textsc{RePST} is to decouple the spatio-temporal dynamics in the frequency domain, allowing better alignment with the PLM text space. Specifically, we first decouple spatio-temporal data in Fourier space and devise a structural diffusion operator to obtain temporal intrinsic and spatial diffusion signals, making the dynamics more comprehensible and predictable for PLMs. To avoid information bottleneck from a limited vocabulary, we further propose a discrete reprogramming strategy that selects relevant discrete textual information from an expanded vocabulary space in a differentiable manner. Extensive experiments on four real-world datasets show that our proposed approach significantly outperforms state-of-the-art spatio-temporal forecasting models, particularly in data-scarce scenarios.||\n", "2408.13040": "|**2024-08-23**|[SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks](http://arxiv.org/abs/2408.13040)|null|Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both storage and computation. Additionally, prompting modifies only the LM's inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. This significantly reduces the need for human labor in designing task-specific models. These advantages become even more evident as the number of tasks served by the LM scales up. Motivated by the strengths of prompting, we are the first to explore the potential of prompting speech LMs in the domain of speech processing. Recently, there has been a growing interest in converting speech into discrete units for language modeling. Our pioneer research demonstrates that these quantized speech units are highly versatile within our unified prompting framework. Not only can they serve as class labels, but they also contain rich phonetic information that can be re-synthesized back into speech signals for speech generation tasks. Specifically, we reformulate speech processing tasks into speech-to-unit generation tasks. As a result, we can seamlessly integrate tasks such as speech classification, sequence generation, and speech generation within a single, unified prompting framework. The experiment results show that the prompting method can achieve competitive performance compared to the strong fine-tuning method based on self-supervised learning models with a similar number of trainable parameters. The prompting method also shows promising results in the few-shot setting. Moreover, with the advanced speech LMs coming into the stage, the proposed prompting framework attains great potential.||\n", "2408.12779": "|**2024-08-23**|[Investigating LLM Applications in E-Commerce](http://arxiv.org/abs/2408.12779)|null|The emergence of Large Language Models (LLMs) has revolutionized natural language processing in various applications especially in e-commerce. One crucial step before the application of such LLMs in these fields is to understand and compare the performance in different use cases in such tasks. This paper explored the efficacy of LLMs in the e-commerce domain, focusing on instruction-tuning an open source LLM model with public e-commerce datasets of varying sizes and comparing the performance with the conventional models prevalent in industrial applications. We conducted a comprehensive comparison between LLMs and traditional pre-trained language models across specific tasks intrinsic to the e-commerce domain, namely classification, generation, summarization, and named entity recognition (NER). Furthermore, we examined the effectiveness of the current niche industrial application of very large LLM, using in-context learning, in e-commerce specific tasks. Our findings indicate that few-shot inference with very large LLMs often does not outperform fine-tuning smaller pre-trained models, underscoring the importance of task-specific model optimization.Additionally, we investigated different training methodologies such as single-task training, mixed-task training, and LoRA merging both within domain/tasks and between different tasks. Through rigorous experimentation and analysis, this paper offers valuable insights into the potential effectiveness of LLMs to advance natural language processing capabilities within the e-commerce industry.||\n", "2408.12125": "|**2024-08-22**|[AutoTest: Evolutionary Code Solution Selection with Test Cases](http://arxiv.org/abs/2408.12125)|null|\u968f\u7740\u4ee3\u7801\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4ece\u591a\u4e2a\u5019\u9009\u65b9\u6848\u4e2d\u9009\u62e9\u6b63\u786e\u7684\u4ee3\u7801\u65b9\u6848\u5df2\u6210\u4e3a\u4e00\u9879\u81f3\u5173\u91cd\u8981\u7684\u4efb\u52a1\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAutoTest\u7684\u65b0\u6280\u672f\uff0c\u8be5\u6280\u672f\u5c06\u81ea\u52a8\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u4e0e\u4ee3\u7801\u65b9\u6848\u6267\u884c\u76f8\u7ed3\u5408\uff0c\u5229\u7528\u8fdb\u5316\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u9009\u62e9\u8fc7\u7a0b\u3002\u9996\u5148\uff0cAutoTest\u5229\u7528\u8bf8\u5982codegen-16B\u3001code-davinci-002\u548cincoder-6B\u7b49\u5927\u578b\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u6765\u63d0\u4f9b\u4ee3\u7801\u65b9\u6848\u53ca\u5176\u76f8\u5e94\u7684\u6d4b\u8bd5\u7528\u4f8b\u3002\u7136\u540e\uff0c\u901a\u8fc7\u6267\u884c\u4ee3\u7801\u65b9\u6848\u5e76\u8bc4\u4f30\u5176\u5728\u6d4b\u8bd5\u7528\u4f8b\u4e0a\u7684\u6027\u80fd\uff0c\u5f62\u6210\u5171\u8bc6\u96c6\u3002\u57fa\u4e8e\u8fdb\u5316\u9057\u4f20\u7b97\u6cd5\u7684\u9009\u62e9\u3001\u53d8\u5f02\u548c\u4ea4\u53c9\u673a\u5236\uff0c\u901a\u8fc7\u8c03\u6574alpha\u548cbeta\u53c2\u6570\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6392\u540d\u3002\u6700\u540e\uff0c\u9009\u62e9\u6700\u4f73\u4ee3\u7801\u65b9\u6848\u3002AutoTest\u5728HumanEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002HumanEval\u6570\u636e\u96c6\u5305\u542b164\u4e2a\u7f16\u7a0b\u95ee\u9898\uff0cAutoTest\u5728pass@1\u5206\u6570\u65b9\u9762\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e86\u7ea610%\u3002||\n", "2408.11319": "|**2024-08-24**|[SarcasmBench: Towards Evaluating Large Language Models on Sarcasm Understanding](http://arxiv.org/abs/2408.11319)|null|In the era of large language models (LLMs), the task of ``System I''~-~the fast, unconscious, and intuitive tasks, e.g., sentiment analysis, text classification, etc., have been argued to be successfully solved. However, sarcasm, as a subtle linguistic phenomenon, often employs rhetorical devices like hyperbole and figuration to convey true sentiments and intentions, involving a higher level of abstraction than sentiment analysis. There is growing concern that the argument about LLMs' success may not be fully tenable when considering sarcasm understanding. To address this question, we select eleven SOTA LLMs and eight SOTA pre-trained language models (PLMs) and present comprehensive evaluations on six widely used benchmark datasets through different prompting approaches, i.e., zero-shot input/output (IO) prompting, few-shot IO prompting, chain of thought (CoT) prompting. Our results highlight three key findings: (1) current LLMs underperform supervised PLMs based sarcasm detection baselines across six sarcasm benchmarks. This suggests that significant efforts are still required to improve LLMs' understanding of human sarcasm. (2) GPT-4 consistently and significantly outperforms other LLMs across various prompting methods, with an average improvement of 14.0\\%$\\uparrow$. Claude 3 and ChatGPT demonstrate the next best performance after GPT-4. (3) Few-shot IO prompting method outperforms the other two methods: zero-shot IO and few-shot CoT. The reason is that sarcasm detection, being a holistic, intuitive, and non-rational cognitive process, is argued not to adhere to step-by-step logical reasoning, making CoT less effective in understanding sarcasm compared to its effectiveness in mathematical reasoning tasks.||\n", "2408.10548": "|**2024-08-20**|[Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution](http://arxiv.org/abs/2408.10548)|**[link](https://github.com/lanxiang1017/language-modeling-on-tabular-data-survey)**|Tabular data, a prevalent data type across various domains, presents unique challenges due to its heterogeneous nature and complex structural relationships. Achieving high predictive performance and robustness in tabular data analysis holds significant promise for numerous applications. Influenced by recent advancements in natural language processing, particularly transformer architectures, new methods for tabular data modeling have emerged. Early techniques concentrated on pre-training transformers from scratch, often encountering scalability issues. Subsequently, methods leveraging pre-trained language models like BERT have been developed, which require less data and yield enhanced performance. The recent advent of large language models, such as GPT and LLaMA, has further revolutionized the field, facilitating more advanced and diverse applications with minimal fine-tuning. Despite the growing interest, a comprehensive survey of language modeling techniques for tabular data remains absent. This paper fills this gap by providing a systematic review of the development of language modeling for tabular data, encompassing: (1) a categorization of different tabular data structures and data types; (2) a review of key datasets used in model training and tasks used for evaluation; (3) a summary of modeling techniques including widely-adopted data processing methods, popular architectures, and training objectives; (4) the evolution from adapting traditional Pre-training/Pre-trained language models to the utilization of large language models; (5) an identification of persistent challenges and potential future research directions in language modeling for tabular data analysis. GitHub page associated with this survey is available at: https://github.com/lanxiang1017/Language-Modeling-on-Tabular-Data-Survey.git.||\n", "2409.05197": "|**2024-09-08**|[Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?](http://arxiv.org/abs/2409.05197)|**[link](https://github.com/zawedcvg/are-large-language-models-attentive-readers)**|State-of-the-art Large Language Models (LLMs) are accredited with an increasing number of different capabilities, ranging from reading comprehension, over advanced mathematical and reasoning skills to possessing scientific knowledge. In this paper we focus on their multi-hop reasoning capability: the ability to identify and integrate information from multiple textual sources.   Given the concerns with the presence of simplifying cues in existing multi-hop reasoning benchmarks, which allow models to circumvent the reasoning requirement, we set out to investigate, whether LLMs are prone to exploiting such simplifying cues. We find evidence that they indeed circumvent the requirement to perform multi-hop reasoning, but they do so in more subtle ways than what was reported about their fine-tuned pre-trained language model (PLM) predecessors. Motivated by this finding, we propose a challenging multi-hop reasoning benchmark, by generating seemingly plausible multi-hop reasoning chains, which ultimately lead to incorrect answers. We evaluate multiple open and proprietary state-of-the-art LLMs, and find that their performance to perform multi-hop reasoning is affected, as indicated by up to 45% relative decrease in F1 score when presented with such seemingly plausible alternatives. We conduct a deeper analysis and find evidence that while LLMs tend to ignore misleading lexical cues, misleading reasoning paths indeed present a significant challenge.||\n", "2409.03773": "|**2024-08-21**|[CoPRA: Bridging Cross-domain Pretrained Sequence Models with Complex Structures for Protein-RNA Binding Affinity Prediction](http://arxiv.org/abs/2409.03773)|null|\u51c6\u786e\u6d4b\u91cf\u86cb\u767d\u8d28-RNA\u7ed3\u5408\u4eb2\u548c\u529b\u5728\u8bb8\u591a\u751f\u7269\u8fc7\u7a0b\u548c\u836f\u7269\u8bbe\u8ba1\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u4ee5\u524d\u7684\u86cb\u767d\u8d28-RNA\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u8ba1\u7b97\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5e8f\u5217\u6216\u7ed3\u6784\u7279\u5f81\uff0c\u65e0\u6cd5\u5168\u9762\u6355\u6349\u7ed3\u5408\u673a\u5236\u3002\u6700\u8fd1\u51fa\u73b0\u7684\u5728\u5927\u91cf\u65e0\u76d1\u7763\u86cb\u767d\u8d28\u548cRNA\u5e8f\u5217\u4e0a\u8bad\u7ec3\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u5305\u62ec\u7ed3\u5408\u4f4d\u70b9\u9884\u6d4b\u5728\u5185\u7684\u5404\u79cd\u57df\u5185\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8868\u793a\u80fd\u529b\u3002\u7136\u800c\uff0c\u534f\u540c\u5e94\u7528\u4e0d\u540c\u9886\u57df\u7684\u8bed\u8a00\u6a21\u578b\u6765\u5b8c\u6210\u590d\u6742\u7ea7\u522b\u7684\u4efb\u52a1\u4ecd\u672a\u5f97\u5230\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86CoPRA\uff0c\u901a\u8fc7\u86cb\u767d\u8d28-RNA\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u7684\u590d\u5408\u7269\u7ed3\u6784\uff0c\u5c06\u6765\u81ea\u4e0d\u540c\u751f\u7269\u9886\u57df\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8fde\u63a5\u8d77\u6765\u3002\u6211\u4eec\u9996\u6b21\u8bc1\u660e\u4e86\u8de8\u751f\u7269\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u534f\u540c\u63d0\u9ad8\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2aCo-Former\u6765\u7ed3\u5408\u8de8\u6a21\u6001\u5e8f\u5217\u548c\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8303\u56f4\u9884\u8bad\u7ec3\u7b56\u7565\u6765\u63d0\u9ad8Co-Former\u7684\u4ea4\u4e92\u7406\u89e3\u80fd\u529b\u3002\u540c\u65f6\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u6700\u5927\u7684\u86cb\u767d\u8d28-RNA\u7ed3\u5408\u4eb2\u548c\u529b\u6570\u636e\u96c6PRA310\u7528\u4e8e\u6027\u80fd\u8bc4\u4f30\u3002\u6211\u4eec\u8fd8\u5728\u4e00\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e86\u6211\u4eec\u6a21\u578b\u7684\u7a81\u53d8\u6548\u5e94\u9884\u6d4b\u80fd\u529b\u3002CoPRA\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u7684\u5206\u6790\uff0c\u5e76\u9a8c\u8bc1\u4e86CoPRA\u53ef\u4ee5\uff081\uff09\u51c6\u786e\u9884\u6d4b\u86cb\u767d\u8d28-RNA\u7ed3\u5408\u4eb2\u548c\u529b\uff1b\uff082\uff09\u7406\u89e3\u7531\u7a81\u53d8\u5f15\u8d77\u7684\u7ed3\u5408\u4eb2\u548c\u529b\u53d8\u5316\uff1b\uff083\uff09\u53d7\u76ca\u4e8e\u6570\u636e\u548c\u6a21\u578b\u89c4\u6a21\u7684\u6269\u5927\u3002||\n", "2409.06622": "|**2024-09-10**|[Exploring Italian sentence embeddings properties through multi-tasking](http://arxiv.org/abs/2409.06622)|null|We investigate to what degree existing LLMs encode abstract linguistic information in Italian in a multi-task setting. We exploit curated synthetic data on a large scale -- several Blackbird Language Matrices (BLMs) problems in Italian -- and use them to study how sentence representations built using pre-trained language models encode specific syntactic and semantic information. We use a two-level architecture to model separately a compression of the sentence embeddings into a representation that contains relevant information for a task, and a BLM task. We then investigate whether we can obtain compressed sentence representations that encode syntactic and semantic information relevant to several BLM tasks. While we expected that the sentence structure -- in terms of sequence of phrases/chunks -- and chunk properties could be shared across tasks, performance and error analysis show that the clues for the different tasks are encoded in different manners in the sentence embeddings, suggesting that abstract linguistic notions such as constituents or thematic roles does not seem to be present in the pretrained sentence embeddings.||\n", "2409.05997": "|**2024-09-09**|[TransformerRanker: A Tool for Efficiently Finding the Best-Suited Language Models for Downstream Classification Tasks](http://arxiv.org/abs/2409.05997)|**[link](https://github.com/flairnlp/transformer-ranker)**|Classification tasks in NLP are typically addressed by selecting a pre-trained language model (PLM) from a model hub, and fine-tuning it for the task at hand. However, given the very large number of PLMs that are currently available, a practical challenge is to determine which of them will perform best for a specific downstream task. With this paper, we introduce TransformerRanker, a lightweight library that efficiently ranks PLMs for classification tasks without the need for computationally costly fine-tuning. Our library implements current approaches for transferability estimation (LogME, H-Score, kNN), in combination with layer aggregation options, which we empirically showed to yield state-of-the-art rankings of PLMs (Garbas et al., 2024). We designed the interface to be lightweight and easy to use, allowing users to directly connect to the HuggingFace Transformers and Dataset libraries. Users need only select a downstream classification task and a list of PLMs to create a ranking of likely best-suited PLMs for their task. We make TransformerRanker available as a pip-installable open-source library https://github.com/flairNLP/transformer-ranker.||\n", "2409.08185": "|**2024-09-12**|[Fine-tuning Large Language Models for Entity Matching](http://arxiv.org/abs/2409.08185)|**[link](https://github.com/wbsg-uni-mannheim/tailormatch)**|Generative large language models (LLMs) are a promising alternative to pre-trained language models for entity matching due to their high zero-shot performance and their ability to generalize to unseen entities. Existing research on using LLMs for entity matching has focused on prompt engineering and in-context learning. This paper explores the potential of fine-tuning LLMs for entity matching. We analyze fine-tuning along two dimensions: 1) The representation of training examples, where we experiment with adding different types of LLM-generated explanations to the training set, and 2) the selection and generation of training examples using LLMs. In addition to the matching performance on the source dataset, we investigate how fine-tuning affects the model's ability to generalize to other in-domain datasets as well as across topical domains. Our experiments show that fine-tuning significantly improves the performance of the smaller models while the results for the larger models are mixed. Fine-tuning also improves the generalization to in-domain datasets while hurting cross-domain transfer. We show that adding structured explanations to the training set has a positive impact on the performance of three out of four LLMs, while the proposed example selection and generation methods only improve the performance of Llama 3.1 8B while decreasing the performance of GPT-4o Mini.||\n", "2409.08406": "|**2024-09-12**|[Knowledge Tagging with Large Language Model based Multi-Agent System](http://arxiv.org/abs/2409.08406)|null|Knowledge tagging for questions is vital in modern intelligent educational applications, including learning progress diagnosis, practice question recommendations, and course content organization. Traditionally, these annotations have been performed by pedagogical experts, as the task demands not only a deep semantic understanding of question stems and knowledge definitions but also a strong ability to link problem-solving logic with relevant knowledge concepts. With the advent of advanced natural language processing (NLP) algorithms, such as pre-trained language models and large language models (LLMs), pioneering studies have explored automating the knowledge tagging process using various machine learning models. In this paper, we investigate the use of a multi-agent system to address the limitations of previous algorithms, particularly in handling complex cases involving intricate knowledge definitions and strict numerical constraints. By demonstrating its superior performance on the publicly available math question knowledge tagging dataset, MathKnowCT, we highlight the significant potential of an LLM-based multi-agent system in overcoming the challenges that previous methods have encountered. Finally, through an in-depth discussion of the implications of automating knowledge tagging, we underscore the promising results of deploying LLM-based algorithms in educational contexts.||\n", "2409.10695": "|**2024-09-16**|[Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large Language Models](http://arxiv.org/abs/2409.10695)|null|We introduce Playground v3 (PGv3), our latest text-to-image model that achieves state-of-the-art (SoTA) performance across multiple testing benchmarks, excels in graphic design abilities and introduces new capabilities. Unlike traditional text-to-image generative models that rely on pre-trained language models like T5 or CLIP text encoders, our approach fully integrates Large Language Models (LLMs) with a novel structure that leverages text conditions exclusively from a decoder-only LLM. Additionally, to enhance image captioning quality-we developed an in-house captioner, capable of generating captions with varying levels of detail, enriching the diversity of text structures. We also introduce a new benchmark CapsBench to evaluate detailed image captioning performance. Experimental results demonstrate that PGv3 excels in text prompt adherence, complex reasoning, and accurate text rendering. User preference studies indicate the super-human graphic design ability of our model for common design applications, such as stickers, posters, and logo designs. Furthermore, PGv3 introduces new capabilities, including precise RGB color control and robust multilingual understanding.||\n", "2409.09501": "|**2024-09-14**|[Synthetic4Health: Generating Annotated Synthetic Clinical Letters](http://arxiv.org/abs/2409.09501)|**[link](https://github.com/hecta-uom/synthetic4health)**|Since clinical letters contain sensitive information, clinical-related datasets can not be widely applied in model training, medical research, and teaching. This work aims to generate reliable, various, and de-identified synthetic clinical letters. To achieve this goal, we explored different pre-trained language models (PLMs) for masking and generating text. After that, we worked on Bio\\_ClinicalBERT, a high-performing model, and experimented with different masking strategies. Both qualitative and quantitative methods were used for evaluation. Additionally, a downstream task, Named Entity Recognition (NER), was also implemented to assess the usability of these synthetic letters.   The results indicate that 1) encoder-only models outperform encoder-decoder models. 2) Among encoder-only models, those trained on general corpora perform comparably to those trained on clinical data when clinical information is preserved. 3) Additionally, preserving clinical entities and document structure better aligns with our objectives than simply fine-tuning the model. 4) Furthermore, different masking strategies can impact the quality of synthetic clinical letters. Masking stopwords has a positive impact, while masking nouns or verbs has a negative effect. 5) For evaluation, BERTScore should be the primary quantitative evaluation metric, with other metrics serving as supplementary references. 6) Contextual information does not significantly impact the models' understanding, so the synthetic clinical letters have the potential to replace the original ones in downstream tasks.||\n", "2409.10570": "|**2024-09-14**|[Protecting Copyright of Medical Pre-trained Language Models: Training-Free Backdoor Watermarking](http://arxiv.org/abs/2409.10570)|null|Pre-training language models followed by fine-tuning on specific tasks is standard in NLP, but traditional models often underperform when applied to the medical domain, leading to the development of specialized medical pre-trained language models (Med-PLMs). These models are valuable assets but are vulnerable to misuse and theft, requiring copyright protection. However, no existing watermarking methods are tailored for Med-PLMs, and adapting general PLMs watermarking techniques to the medical domain faces challenges such as task incompatibility, loss of fidelity, and inefficiency. To address these issues, we propose the first training-free backdoor watermarking method for Med-PLMs. Our method uses rare special symbols as trigger words, which do not impact downstream task performance, embedding watermarks by replacing their original embeddings with those of specific medical terms in the Med-PLMs' word embeddings layer. After fine-tuning the watermarked Med-PLMs on various medical downstream tasks, the final models (FMs) respond to the trigger words in the same way they would to the corresponding medical terms. This property can be utilized to extract the watermark. Experiments demonstrate that our method achieves high fidelity while effectively extracting watermarks across various medical downstream tasks. Additionally, our method demonstrates robustness against various attacks and significantly enhances the efficiency of watermark embedding, reducing the embedding time from 10 hours to 10 seconds.||\n", "2409.15890": "|**2024-09-24**|[HLB: Benchmarking LLMs' Humanlikeness in Language Use](http://arxiv.org/abs/2409.15890)|null|As synthetic data becomes increasingly prevalent in training language models, particularly through generated dialogue, concerns have emerged that these models may deviate from authentic human language patterns, potentially losing the richness and creativity inherent in human communication. This highlights the critical need to assess the humanlikeness of language models in real-world language use. In this paper, we present a comprehensive humanlikeness benchmark (HLB) evaluating 20 large language models (LLMs) using 10 psycholinguistic experiments designed to probe core linguistic aspects, including sound, word, syntax, semantics, and discourse (see https://huggingface.co/spaces/XufengDuan/HumanLikeness). To anchor these comparisons, we collected responses from over 2,000 human participants and compared them to outputs from the LLMs in these experiments.   For rigorous evaluation, we developed a coding algorithm that accurately identified language use patterns, enabling the extraction of response distributions for each task. By comparing the response distributions between human participants and LLMs, we quantified humanlikeness through distributional similarity. Our results reveal fine-grained differences in how well LLMs replicate human responses across various linguistic levels. Importantly, we found that improvements in other performance metrics did not necessarily lead to greater humanlikeness, and in some cases, even resulted in a decline. By introducing psycholinguistic methods to model evaluation, this benchmark offers the first framework for systematically assessing the humanlikeness of LLMs in language use.||\n", "2409.14904": "|**2024-09-23**|[DSG-KD: Knowledge Distillation from Domain-Specific to General Language Models](http://arxiv.org/abs/2409.14904)|**[link](https://github.com/josangyeon/dsg-kd)**|The use of pre-trained language models fine-tuned to address specific downstream tasks is a common approach in natural language processing (NLP). However, acquiring domain-specific knowledge via fine-tuning is challenging. Traditional methods involve pretraining language models using vast amounts of domain-specific data before fine-tuning for particular tasks. This study investigates emergency/non-emergency classification tasks based on electronic medical record (EMR) data obtained from pediatric emergency departments (PEDs) in Korea. Our findings reveal that existing domain-specific pre-trained language models underperform compared to general language models in handling N-lingual free-text data characteristics of non-English-speaking regions. To address these limitations, we propose a domain knowledge transfer methodology that leverages knowledge distillation to infuse general language models with domain-specific knowledge via fine-tuning. This study demonstrates the effective transfer of specialized knowledge between models by defining a general language model as the student model and a domain-specific pre-trained model as the teacher model. In particular, we address the complexities of EMR data obtained from PEDs in non-English-speaking regions, such as Korea, and demonstrate that the proposed method enhances classification performance in such contexts. The proposed methodology not only outperforms baseline models on Korean PED EMR data, but also promises broader applicability in various professional and technical domains. In future works, we intend to extend this methodology to include diverse non-English-speaking regions and address additional downstream tasks, with the aim of developing advanced model architectures using state-of-the-art KD techniques. The code is available in https://github.com/JoSangYeon/DSG-KD.||\n", "2409.14810": "|**2024-09-23**|[Pre-trained Language Model and Knowledge Distillation for Lightweight Sequential Recommendation](http://arxiv.org/abs/2409.14810)|null|Sequential recommendation models user interests based on historical behaviors to provide personalized recommendation. Previous sequential recommendation algorithms primarily employ neural networks to extract features of user interests, achieving good performance. However, due to the recommendation system datasets sparsity, these algorithms often employ small-scale network frameworks, resulting in weaker generalization capability. Recently, a series of sequential recommendation algorithms based on large pre-trained language models have been proposed. Nonetheless, given the real-time demands of recommendation systems, the challenge remains in applying pre-trained language models for rapid recommendations in real scenarios. To address this, we propose a sequential recommendation algorithm based on a pre-trained language model and knowledge distillation. The key of proposed algorithm is to transfer pre-trained knowledge across domains and achieve lightweight inference by knowledge distillation. The algorithm operates in two stages: in the first stage, we fine-tune the pre-trained language model on the recommendation dataset to transfer the pre-trained knowledge to the recommendation task; in the second stage, we distill the trained language model to transfer the learned knowledge to a lightweight model. Extensive experiments on multiple public recommendation datasets show that the proposed algorithm enhances recommendation accuracy and provide timely recommendation services.||\n", "2409.14097": "|**2024-09-21**|[Probing Context Localization of Polysemous Words in Pre-trained Language Model Sub-Layers](http://arxiv.org/abs/2409.14097)|null|In the era of high performing Large Language Models, researchers have widely acknowledged that contextual word representations are one of the key drivers in achieving top performances in downstream tasks. In this work, we investigate the degree of contextualization encoded in the fine-grained sub-layer representations of a Pre-trained Language Model (PLM) by empirical experiments using linear probes. Unlike previous work, we are particularly interested in identifying the strength of contextualization across PLM sub-layer representations (i.e. Self-Attention, Feed-Forward Activation and Output sub-layers). To identify the main contributions of sub-layers to contextualisation, we first extract the sub-layer representations of polysemous words in minimally different sentence pairs, and compare how these representations change through the forward pass of the PLM network. Second, by probing on a sense identification classification task, we try to empirically localize the strength of contextualization information encoded in these sub-layer representations. With these probing experiments, we also try to gain a better understanding of the influence of context length and context richness on the degree of contextualization. Our main conclusion is cautionary: BERT demonstrates a high degree of contextualization in the top sub-layers if the word in question is in a specific position in the sentence with a shorter context window, but this does not systematically generalize across different word positions and context sizes.||\n", "2409.13928": "|**2024-09-20**|[Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation](http://arxiv.org/abs/2409.13928)|null|We study the code generation behavior of instruction-tuned models built on top of code pre-trained language models when they could access an auxiliary function to implement a function. We design several ways to provide auxiliary functions to the models by adding them to the query or providing a response prefix to incorporate the ability to utilize auxiliary functions with the instruction-following capability. Our experimental results show the effectiveness of combining the base models' auxiliary function utilization ability with the instruction following ability. In particular, the performance of adopting our approaches with the open-sourced language models surpasses that of the recent powerful proprietary language models, i.e., gpt-4o.||\n", "2409.13561": "|**2024-09-20**|[Demystifying and Extracting Fault-indicating Information from Logs for Failure Diagnosis](http://arxiv.org/abs/2409.13561)|**[link](https://github.com/jun-jie-huang/lofi)**|Logs are imperative in the maintenance of online service systems, which often encompass important information for effective failure mitigation. While existing anomaly detection methodologies facilitate the identification of anomalous logs within extensive runtime data, manual investigation of log messages by engineers remains essential to comprehend faults, which is labor-intensive and error-prone. Upon examining the log-based troubleshooting practices at CloudA, we find that engineers typically prioritize two categories of log information for diagnosis. These include fault-indicating descriptions, which record abnormal system events, and fault-indicating parameters, which specify the associated entities. Motivated by this finding, we propose an approach to automatically extract such faultindicating information from logs for fault diagnosis, named LoFI. LoFI comprises two key stages. In the first stage, LoFI performs coarse-grained filtering to collect logs related to the faults based on semantic similarity. In the second stage, LoFI leverages a pre-trained language model with a novel prompt-based tuning method to extract fine-grained information of interest from the collected logs. We evaluate LoFI on logs collected from Apache Spark and an industrial dataset from CloudA. The experimental results demonstrate that LoFI outperforms all baseline methods by a significant margin, achieving an absolute improvement of 25.8~37.9 in F1 over the best baseline method, ChatGPT. This highlights the effectiveness of LoFI in recognizing fault-indicating information. Furthermore, the successful deployment of LoFI at CloudA and user studies validate the utility of our method. The code and data are available at https://github.com/Jun-jie-Huang/LoFI.||\n", "2409.13501": "|**2024-09-20**|[HUT: A More Computation Efficient Fine-Tuning Method With Hadamard Updated Transformation](http://arxiv.org/abs/2409.13501)|null|Fine-tuning pre-trained language models for downstream tasks has achieved impressive results in NLP. However, fine-tuning all parameters becomes impractical due to the rapidly increasing size of model parameters. To address this, Parameter Efficient Fine-Tuning (PEFT) methods update only a subset of parameters. Most PEFT methods, such as LoRA, use incremental updates, which involve adding learned weight matrix increments to the original parameters. Although effective, these methods face limitations in capturing complex parameter dynamics and do not maintain a strong correlation between the original and updated parameters. To overcome these challenges, we propose the direct Updated Transformation (UT) paradigm, which constructs a transformation directly from the original to the updated parameters. This approach ensures that the correlation between the original and updated parameters is preserved, leveraging the semantic features learned during pre-training. Building on this paradigm, we present the Hadamard Updated Transformation (HUT) method. HUT efficiently updates the original weight matrix using the Hadamard transformation with two low-rank matrices, offering a more expressive and flexible update mechanism. This allows HUT to capture richer parameter features through functional transformations, reducing computational complexity while maintaining or improving model quality. Theoretical analysis and extensive experiments on RoBERTa and GPT-2 validate the effectiveness of HUT. Results show that HUT performs on par with or better than other PEFT methods in terms of model quality, while significantly reducing computational complexity.||\n", "2409.12695": "|**2024-09-19**|[Exploring Large Language Models for Product Attribute Value Identification](http://arxiv.org/abs/2409.12695)|null|Product attribute value identification (PAVI) involves automatically identifying attributes and their values from product information, enabling features like product search, recommendation, and comparison. Existing methods primarily rely on fine-tuning pre-trained language models, such as BART and T5, which require extensive task-specific training data and struggle to generalize to new attributes. This paper explores large language models (LLMs), such as LLaMA and Mistral, as data-efficient and robust alternatives for PAVI. We propose various strategies: comparing one-step and two-step prompt-based approaches in zero-shot settings and utilizing parametric and non-parametric knowledge through in-context learning examples. We also introduce a dense demonstration retriever based on a pre-trained T5 model and perform instruction fine-tuning to explicitly train LLMs on task-specific instructions. Extensive experiments on two product benchmarks show that our two-step approach significantly improves performance in zero-shot settings, and instruction fine-tuning further boosts performance when using training data, demonstrating the practical benefits of using LLMs for PAVI.||\n", "2409.18073": "|**2024-09-26**|[Infer Human's Intentions Before Following Natural Language Instructions](http://arxiv.org/abs/2409.18073)|**[link](https://github.com/simon-wan/fiser)**|For AI agents to be helpful to humans, they should be able to follow natural language instructions to complete everyday cooperative tasks in human environments. However, real human instructions inherently possess ambiguity, because the human speakers assume sufficient prior knowledge about their hidden goals and intentions. Standard language grounding and planning methods fail to address such ambiguities because they do not model human internal goals as additional partially observable factors in the environment. We propose a new framework, Follow Instructions with Social and Embodied Reasoning (FISER), aiming for better natural language instruction following in collaborative embodied tasks. Our framework makes explicit inferences about human goals and intentions as intermediate reasoning steps. We implement a set of Transformer-based models and evaluate them over a challenging benchmark, HandMeThat. We empirically demonstrate that using social reasoning to explicitly infer human intentions before making action plans surpasses purely end-to-end approaches. We also compare our implementation with strong baselines, including Chain of Thought prompting on the largest available pre-trained language models, and find that FISER provides better performance on the embodied social reasoning tasks under investigation, reaching the state-of-the-art on HandMeThat.||\n", "2409.17750": "|**2024-09-26**|[Are Transformers in Pre-trained LM A Good ASR Encoder? An Empirical Study](http://arxiv.org/abs/2409.17750)|null|In this study, we delve into the efficacy of transformers within pre-trained language models (PLMs) when repurposed as encoders for Automatic Speech Recognition (ASR). Our underlying hypothesis posits that, despite being initially trained on text-based corpora, these transformers possess a remarkable capacity to extract effective features from the input sequence. This inherent capability, we argue, is transferrable to speech data, thereby augmenting the acoustic modeling ability of ASR. Through rigorous empirical analysis, our findings reveal a notable improvement in Character Error Rate (CER) and Word Error Rate (WER) across diverse ASR tasks when transformers from pre-trained LMs are incorporated. Particularly, they serve as an advantageous starting point for initializing ASR encoders. Furthermore, we uncover that these transformers, when integrated into a well-established ASR encoder, can significantly boost performance, especially in scenarios where profound semantic comprehension is pivotal. This underscores the potential of leveraging the semantic prowess embedded within pre-trained transformers to advance ASR systems' capabilities.||\n", "2409.18878": "|**2024-09-27**|[Suicide Phenotyping from Clinical Notes in Safety-Net Psychiatric Hospital Using Multi-Label Classification with Pre-Trained Language Models](http://arxiv.org/abs/2409.18878)|null|Accurate identification and categorization of suicidal events can yield better suicide precautions, reducing operational burden, and improving care quality in high-acuity psychiatric settings. Pre-trained language models offer promise for identifying suicidality from unstructured clinical narratives. We evaluated the performance of four BERT-based models using two fine-tuning strategies (multiple single-label and single multi-label) for detecting coexisting suicidal events from 500 annotated psychiatric evaluation notes. The notes were labeled for suicidal ideation (SI), suicide attempts (SA), exposure to suicide (ES), and non-suicidal self-injury (NSSI). RoBERTa outperformed other models using binary relevance (acc=0.86, F1=0.78). MentalBERT (F1=0.74) also exceeded BioClinicalBERT (F1=0.72). RoBERTa fine-tuned with a single multi-label classifier further improved performance (acc=0.88, F1=0.81), highlighting that models pre-trained on domain-relevant data and the single multi-label classification strategy enhance efficiency and performance.   Keywords: EHR-based Phynotyping; Natural Language Processing; Secondary Use of EHR Data; Suicide Classification; BERT-based Model; Psychiatry; Mental Health||\n", "2409.19788": "|**2024-09-29**|[Adversarial Examples for DNA Classification](http://arxiv.org/abs/2409.19788)|null|Pre-trained language models such as DNABERT2 and Nucleotide Transformer, which are trained on DNA sequences, have shown promising performance in DNA sequence classification tasks. The classification ability of these models stems from language models trained on vast amounts of DNA sequence samples, followed by fine-tuning with relatively smaller classification datasets. However, these text-based systems are not robust enough and can be vulnerable to adversarial examples. While adversarial attacks have been widely studied in text classification, there is limited research in DNA sequence classification. In this paper, we adapt commonly used attack algorithms in text classification for DNA sequence classification. We evaluated the impact of various attack methods on DNA sequence classification at the character, word, and sentence levels. Our findings indicate that actual DNA language model sequence classifiers are vulnerable to these attacks.||\n", "2409.19749": "|**2024-09-29**|[NeuroMax: Enhancing Neural Topic Modeling via Maximizing Mutual Information and Group Topic Regularization](http://arxiv.org/abs/2409.19749)|null|Recent advances in neural topic models have concentrated on two primary directions: the integration of the inference network (encoder) with a pre-trained language model (PLM) and the modeling of the relationship between words and topics in the generative model (decoder). However, the use of large PLMs significantly increases inference costs, making them less practical for situations requiring low inference times. Furthermore, it is crucial to simultaneously model the relationships between topics and words as well as the interrelationships among topics themselves. In this work, we propose a novel framework called NeuroMax (Neural Topic Model with Maximizing Mutual Information with Pretrained Language Model and Group Topic Regularization) to address these challenges. NeuroMax maximizes the mutual information between the topic representation obtained from the encoder in neural topic models and the representation derived from the PLM. Additionally, NeuroMax employs optimal transport to learn the relationships between topics by analyzing how information is transported among them. Experimental results indicate that NeuroMax reduces inference time, generates more coherent topics and topic groups, and produces more representative document embeddings, thereby enhancing performance on downstream tasks.||\n", "2410.01946": "|**2024-10-02**|[SciPrompt: Knowledge-augmented Prompting for Fine-grained Categorization of Scientific Topics](http://arxiv.org/abs/2410.01946)|**[link](https://github.com/zhiwenyou103/SciPrompt)**|\u57fa\u4e8e\u63d0\u793a\u7684\u5fae\u8c03\u5df2\u6210\u4e3a\u4ece\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u53d6\u7f16\u7801\u4fe1\u606f\u7684\u91cd\u8981\u65b9\u6cd5\uff0c\u7528\u4e8e\u5404\u79cd\u4efb\u52a1\uff0c\u5305\u62ec\u6587\u672c\u5206\u7c7b\u3002\u5bf9\u4e8e\u591a\u7c7b\u522b\u5206\u7c7b\u4efb\u52a1\uff0c\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\uff0c\u57fa\u4e8e\u63d0\u793a\u7684\u5fae\u8c03\u5df2\u7ecf\u5b9e\u73b0\u4e86\u4e0e\u5b8c\u5168\u5fae\u8c03\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u6c34\u5e73\u3002\u5148\u524d\u7684\u7814\u7a76\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u6a21\u677f\u548c\u8bcd\u8bed\u8f6c\u6362\u5668\uff0c\u5c06\u6807\u7b7e\u8bcd\u7a7a\u95f4\u6620\u5c04\u5230\u7c7b\u522b\u7a7a\u95f4\uff0c\u4ece\u800c\u5c06\u5206\u7c7b\u95ee\u9898\u89e3\u51b3\u4e3a\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u3002\u7136\u800c\uff0c\u5177\u6709\u81ea\u52a8\u4e30\u5bcc\u8bcd\u8bed\u8f6c\u6362\u5668\u7684\u8de8\u9886\u57df\u548c\u7ec6\u7c92\u5ea6\u63d0\u793a\u5fae\u8c03\u4ecd\u7136 unexplored\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u624b\u52a8\u9009\u62e9\u9886\u57df\u6807\u7b7e\u8bcd\u7528\u4e8e\u8bcd\u8bed\u8f6c\u6362\u5668\u5b58\u5728\u56f0\u96be\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u8fd9\u9700\u8981\u5177\u5907\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7684\u4eba\u5458\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86 SciPrompt\uff0c\u8fd9\u662f\u4e00\u4e2a\u65e8\u5728\u81ea\u52a8\u68c0\u7d22\u4e0e\u79d1\u5b66\u4e3b\u9898\u76f8\u5173\u7684\u672f\u8bed\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f4e\u8d44\u6e90\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5728\u79d1\u5b66\u6587\u732e\u7684\u80cc\u666f\u4e0b\u9009\u62e9\u8bed\u4e49\u76f8\u5173\u4e14\u7279\u5b9a\u4e8e\u9886\u57df\u7684\u6807\u7b7e\u8bcd\u8fdb\u884c\u8bcd\u8bed\u8f6c\u6362\u5668\u589e\u5f3a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bcd\u8bed\u8f6c\u6362\u7b56\u7565\uff0c\u4f7f\u7528\u76f8\u5173\u6027\u5f97\u5206\u4f5c\u4e3a\u989d\u5916\u7684\u6743\u91cd\uff0c\u4ee5\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u5728\u6a21\u578b\u5fae\u8c03\u671f\u95f4\u7684\u9884\u6d4b\u6027\u80fd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u79d1\u5b66\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5bf9\u7ec6\u7c92\u5ea6\u548c\u65b0\u5174\u79d1\u5b66\u4e3b\u9898\u8fdb\u884c\u5206\u7c7b\u65b9\u9762\u3002||\n", "2410.00361": "|**2024-10-01**|[PclGPT: A Large Language Model for Patronizing and Condescending Language Detection](http://arxiv.org/abs/2410.00361)|**[link](https://github.com/dut-laowang/emnlp24-PclGPT)**|Disclaimer: Samples in this paper may be harmful and cause discomfort!   Patronizing and condescending language (PCL) is a form of speech directed at vulnerable groups. As an essential branch of toxic language, this type of language exacerbates conflicts and confrontations among Internet communities and detrimentally impacts disadvantaged groups. Traditional pre-trained language models (PLMs) perform poorly in detecting PCL due to its implicit toxicity traits like hypocrisy and false sympathy. With the rise of large language models (LLMs), we can harness their rich emotional semantics to establish a paradigm for exploring implicit toxicity. In this paper, we introduce PclGPT, a comprehensive LLM benchmark designed specifically for PCL. We collect, annotate, and integrate the Pcl-PT/SFT dataset, and then develop a bilingual PclGPT-EN/CN model group through a comprehensive pre-training and supervised fine-tuning staircase process to facilitate implicit toxic detection. Group detection results and fine-grained detection from PclGPT and other models reveal significant variations in the degree of bias in PCL towards different vulnerable groups, necessitating increased societal attention to protect them.||\n", "2410.00249": "|**2024-10-03**|[Enhancing Pre-Trained Language Models for Vulnerability Detection via Semantic-Preserving Data Augmentation](http://arxiv.org/abs/2410.00249)|null|With the rapid development and widespread use of advanced network systems, software vulnerabilities pose a significant threat to secure communications and networking. Learning-based vulnerability detection systems, particularly those leveraging pre-trained language models, have demonstrated significant potential in promptly identifying vulnerabilities in communication networks and reducing the risk of exploitation. However, the shortage of accurately labeled vulnerability datasets hinders further progress in this field. Failing to represent real-world vulnerability data variety and preserve vulnerability semantics, existing augmentation approaches provide limited or even counterproductive contributions to model training. In this paper, we propose a data augmentation technique aimed at enhancing the performance of pre-trained language models for vulnerability detection. Given the vulnerability dataset, our method performs natural semantic-preserving program transformation to generate a large volume of new samples with enriched data diversity and variety. By incorporating our augmented dataset in fine-tuning a series of representative code pre-trained models (i.e., CodeBERT, GraphCodeBERT, UnixCoder, and PDBERT), up to 10.1% increase in accuracy and 23.6% increase in F1 can be achieved in the vulnerability detection task. Comparison results also show that our proposed method can substantially outperform other prominent vulnerability augmentation approaches.||\n", "2410.03470": "|**2024-10-04**|[Vulnerability Detection via Topological Analysis of Attention Maps](http://arxiv.org/abs/2410.03470)|**[link](https://github.com/Snopoff/Vulnerability-Detection-via-Topological-Analysis-of-Attention-Maps)**|Recently, deep learning (DL) approaches to vulnerability detection have gained significant traction. These methods demonstrate promising results, often surpassing traditional static code analysis tools in effectiveness.   In this study, we explore a novel approach to vulnerability detection utilizing the tools from topological data analysis (TDA) on the attention matrices of the BERT model. Our findings reveal that traditional machine learning (ML) techniques, when trained on the topological features extracted from these attention matrices, can perform competitively with pre-trained language models (LLMs) such as CodeBERTa. This suggests that TDA tools, including persistent homology, are capable of effectively capturing semantic information critical for identifying vulnerabilities.||\n", "2410.03278": "|**2024-10-09**|[What do Large Language Models Need for Machine Translation Evaluation?](http://arxiv.org/abs/2410.03278)|**[link](https://github.com/surrey-nlp/LLM4MT_eval)**|Leveraging large language models (LLMs) for various natural language processing tasks has led to superlative claims about their performance. For the evaluation of machine translation (MT), existing research shows that LLMs are able to achieve results comparable to fine-tuned multilingual pre-trained language models. In this paper, we explore what translation information, such as the source, reference, translation errors and annotation guidelines, is needed for LLMs to evaluate MT quality. In addition, we investigate prompting techniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for eight language pairs covering high-, medium- and low-resource languages, leveraging varying LLM variants. Our findings indicate the importance of reference translations for an LLM-based evaluation. While larger models do not necessarily fare better, they tend to benefit more from CoT prompting, than smaller models. We also observe that LLMs do not always provide a numerical score when generating evaluations, which poses a question on their reliability for the task. Our work presents a comprehensive analysis for resource-constrained and training-less LLM-based evaluation of machine translation. We release the accrued prompt templates, code and data publicly for reproducibility.||\n", "2410.03182": "|**2024-10-04**|[Generating bilingual example sentences with large language models as lexicography assistants](http://arxiv.org/abs/2410.03182)|null|We present a study of LLMs' performance in generating and rating example sentences for bilingual dictionaries across languages with varying resource levels: French (high-resource), Indonesian (mid-resource), and Tetun (low-resource), with English as the target language. We evaluate the quality of LLM-generated examples against the GDEX (Good Dictionary EXample) criteria: typicality, informativeness, and intelligibility. Our findings reveal that while LLMs can generate reasonably good dictionary examples, their performance degrades significantly for lower-resourced languages. We also observe high variability in human preferences for example quality, reflected in low inter-annotator agreement rates. To address this, we demonstrate that in-context learning can successfully align LLMs with individual annotator preferences. Additionally, we explore the use of pre-trained language models for automated rating of examples, finding that sentence perplexity serves as a good proxy for typicality and intelligibility in higher-resourced languages. Our study also contributes a novel dataset of 600 ratings for LLM-generated sentence pairs, and provides insights into the potential of LLMs in reducing the cost of lexicographic work, particularly for low-resource languages.||\n", "2410.02992": "|**2024-10-03**|[Guided Stream of Search: Learning to Better Search with Language Models via Optimal Path Guidance](http://arxiv.org/abs/2410.02992)|**[link](https://github.com/symoon11/guided-stream-of-search)**|While language models have demonstrated impressive capabilities across a range of tasks, they still struggle with tasks that require complex planning and reasoning. Recent studies have proposed training language models on search processes rather than optimal solutions, resulting in better generalization performance even though search processes are noisy and even suboptimal. However, these studies overlook the value of optimal solutions, which can serve as step-by-step landmarks to guide more effective search. In this work, we explore how to leverage optimal solutions to enhance the search and planning abilities of language models. To this end, we propose guided stream of search (GSoS), which seamlessly incorporates optimal solutions into the self-generation process in a progressive manner, producing high-quality search trajectories. These trajectories are then distilled into the pre-trained model via supervised fine-tuning. Our approach significantly enhances the search and planning abilities of language models on Countdown, a simple yet challenging mathematical reasoning task. Notably, combining our method with RL fine-tuning yields further improvements, whereas previous supervised fine-tuning methods do not benefit from RL. Furthermore, our approach exhibits greater effectiveness than leveraging optimal solutions in the form of subgoal rewards.||\n", "2410.02915": "|**2024-10-03**|[Does the Order of Fine-tuning Matter and Why?](http://arxiv.org/abs/2410.02915)|null|To improve the performance on a target task, researchers have fine-tuned language models with an intermediate task before the target task of interest. However, previous works have focused on the pre-trained language models and downstream tasks in Natural Language Processing (NLP) and considered only one intermediate task. The effect of fine-tuning multiple intermediate tasks and their ordering on target task performance has not been fully explored in Software Engineering. In this study, we perform the first empirical study on analyzing the impact of task ordering on target task performance. Experimental results show that there is an impact of task ordering on target task performance by up to 6% of performance gain and up to 4% of performance loss. To explain such an impact, we consider a variety of potential factors, including the characteristics of dataset (syntactic similarity and semantic similarity analysis, dataset size), model (probing task and attention analysis), and task (task affinity analysis). Our study provides Software Engineering researchers and practitioners with insights into the effect of task orderings and how to select the one that is cost-effective while achieving the best performance gain.||\n", "2410.04239": "|**2024-10-05**|[Persona Knowledge-Aligned Prompt Tuning Method for Online Debate](http://arxiv.org/abs/2410.04239)|**[link](https://github.com/HKUST-KnowComp/PersonaPrompt)**|Debate is the process of exchanging viewpoints or convincing others on a particular issue. Recent research has provided empirical evidence that the persuasiveness of an argument is determined not only by language usage but also by communicator characteristics. Researchers have paid much attention to aspects of languages, such as linguistic features and discourse structures, but combining argument persuasiveness and impact with the social personae of the audience has not been explored due to the difficulty and complexity. We have observed the impressive simulation and personification capability of ChatGPT, indicating a giant pre-trained language model may function as an individual to provide personae and exert unique influences based on diverse background knowledge. Therefore, we propose a persona knowledge-aligned framework for argument quality assessment tasks from the audience side. This is the first work that leverages the emergence of ChatGPT and injects such audience personae knowledge into smaller language models via prompt tuning. The performance of our pipeline demonstrates significant and consistent improvement compared to competitive architectures.||\n", "2410.04236": "|**2024-10-05**|[Overview of Factify5WQA: Fact Verification through 5W Question-Answering](http://arxiv.org/abs/2410.04236)|null|Researchers have found that fake news spreads much times faster than real news. This is a major problem, especially in today's world where social media is the key source of news for many among the younger population. Fact verification, thus, becomes an important task and many media sites contribute to the cause. Manual fact verification is a tedious task, given the volume of fake news online. The Factify5WQA shared task aims to increase research towards automated fake news detection by providing a dataset with an aspect-based question answering based fact verification method. Each claim and its supporting document is associated with 5W questions that help compare the two information sources. The objective performance measure in the task is done by comparing answers using BLEU score to measure the accuracy of the answers, followed by an accuracy measure of the classification. The task had submissions using custom training setup and pre-trained language-models among others. The best performing team posted an accuracy of 69.56%, which is a near 35% improvement over the baseline.||\n", "2410.04074": "|**2024-10-05**|[On Eliciting Syntax from Language Models via Hashing](http://arxiv.org/abs/2410.04074)|null|Unsupervised parsing, also known as grammar induction, aims to infer syntactic structure from raw text. Recently, binary representation has exhibited remarkable information-preserving capabilities at both lexicon and syntax levels. In this paper, we explore the possibility of leveraging this capability to deduce parsing trees from raw text, relying solely on the implicitly induced grammars within models. To achieve this, we upgrade the bit-level CKY from zero-order to first-order to encode the lexicon and syntax in a unified binary representation space, switch training from supervised to unsupervised under the contrastive hashing framework, and introduce a novel loss function to impose stronger yet balanced alignment signals. Our model shows competitive performance on various datasets, therefore, we claim that our method is effective and efficient enough to acquire high-quality parsing trees from pre-trained language models at a low cost.||\n", "2410.03780": "|**2024-10-03**|[Reward-RAG: Enhancing RAG with Reward Driven Supervision](http://arxiv.org/abs/2410.03780)|null|In this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences. The versatility of our approach allows it to be effectively applied across various domains through domain-specific fine-tuning. We evaluate Reward-RAG on publicly available benchmarks from multiple domains, comparing it to state-of-the-art methods. Our experimental results demonstrate significant improvements in performance, highlighting the effectiveness of Reward-RAG in improving the relevance and quality of generated responses. These findings underscore the potential of integrating reward models with RAG to achieve superior outcomes in natural language generation tasks.||\n", "2410.08198": "|**2024-10-10**|[Adam Exploits $\\ell_\\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity](http://arxiv.org/abs/2410.08198)|**[link](https://github.com/mohamad-amin/adam-coordinate-adaptivity)**|Adam outperforms SGD when training language models. Yet this advantage is not well-understood theoretically -- previous convergence analysis for Adam and SGD mainly focuses on the number of steps $T$ and is already minimax-optimal in non-convex cases, which are both $\\widetilde{O}(T^{-1/4})$. In this work, we argue that the exploitation of nice $\\ell_\\infty$-geometry is the key advantage of Adam over SGD. More specifically, we give a new convergence analysis for Adam under novel assumptions that loss is smooth under $\\ell_\\infty$-geometry rather than the more common $\\ell_2$-geometry, which yields a much better empirical smoothness constant for GPT-2 and ResNet models. Our experiments confirm that Adam performs much worse when the favorable $\\ell_\\infty$-geometry is changed while SGD provably remains unaffected. We also extend the convergence analysis to blockwise Adam under novel blockwise smoothness assumptions.||\n", "2410.07793": "|**2024-10-10**|[Do Current Language Models Support Code Intelligence for R Programming Language?](http://arxiv.org/abs/2410.07793)|null|Recent advancements in developing Pre-trained Language Models for Code (Code-PLMs) have urged many areas of Software Engineering (SE) and brought breakthrough results for many SE tasks. Though these models have achieved the state-of-the-art performance for SE tasks for many popular programming languages, such as Java and Python, the Scientific Software and its related languages like R programming language have rarely benefited or even been evaluated with the Code-PLMs. Research has shown that R has many differences with other programming languages and requires specific techniques. In this study, we provide the first insights for code intelligence for R. For this purpose, we collect and open source an R dataset, and evaluate Code-PLMs for the two tasks of code summarization and method name prediction using several settings and strategies, including the differences in two R styles, Tidy-verse and Base R. Our results demonstrate that the studied models have experienced varying degrees of performance degradation when processing R programming language code, which is supported by human evaluation. Additionally, not all models show performance improvement in R-specific tasks even after multi-language fine-tuning. The dual syntax paradigms in R significantly impact the models' performance, particularly in code summarization tasks. Furthermore, the project-specific context inherent in R codebases significantly impacts the performance when attempting cross-project training.||\n", "2410.07271": "|**2024-10-09**|[Multi-Task Program Error Repair and Explanatory Diagnosis](http://arxiv.org/abs/2410.07271)|null|Program errors can occur in any type of programming, and can manifest in a variety of ways, such as unexpected output, crashes, or performance issues. And program error diagnosis can often be too abstract or technical for developers to understand, especially for beginners. The goal of this paper is to present a novel machine-learning approach for Multi-task Program Error Repair and Explanatory Diagnosis (mPRED). A pre-trained language model is used to encode the source code, and a downstream model is specifically designed to identify and repair errors. Programs and test cases will be augmented and optimized from several perspectives. Additionally, our approach incorporates a \"chain of thoughts\" method, which enables the models to produce intermediate reasoning explanations before providing the final correction. To aid in visualizing and analyzing the program structure, we use a graph neural network for program structure visualization. Overall, our approach offers a promising approach for repairing program errors across different programming languages and providing helpful explanations to programmers.||\n", "2410.06173": "|**2024-10-08**|[Manual Verbalizer Enrichment for Few-Shot Text Classification](http://arxiv.org/abs/2410.06173)|null|With the continuous development of pre-trained language models, prompt-based training becomes a well-adopted paradigm that drastically improves the exploitation of models for many natural language processing tasks. Prompting also shows great performance compared to traditional fine-tuning when adapted to zero-shot or few-shot scenarios where the number of annotated data is limited. In this framework, the role of verbalizers is essential, as an interpretation from masked word distributions into output predictions. In this work, we propose \\acrshort{mave}, an approach for verbalizer construction by enrichment of class labels using neighborhood relation in the embedding space of words for the text classification task. In addition, we elaborate a benchmarking procedure to evaluate typical baselines of verbalizers for document classification in few-shot learning contexts. Our model achieves state-of-the-art results while using significantly fewer resources. We show that our approach is particularly effective in cases with extremely limited supervision data.||\n", "2410.05731": "|**2024-10-08**|[Enhancing SPARQL Generation by Triplet-order-sensitive Pre-training](http://arxiv.org/abs/2410.05731)|**[link](https://github.com/LUMIA-Group/TosT5)**|Semantic parsing that translates natural language queries to SPARQL is of great importance for Knowledge Graph Question Answering (KGQA) systems. Although pre-trained language models like T5 have achieved significant success in the Text-to-SPARQL task, their generated outputs still exhibit notable errors specific to the SPARQL language, such as triplet flips. To address this challenge and further improve the performance, we propose an additional pre-training stage with a new objective, Triplet Order Correction (TOC), along with the commonly used Masked Language Modeling (MLM), to collectively enhance the model's sensitivity to triplet order and SPARQL syntax. Our method achieves state-of-the-art performances on three widely-used benchmarks.||\n", "2410.08905": "|**2024-10-11**|[Lifelong Event Detection via Optimal Transport](http://arxiv.org/abs/2410.08905)|null|Continual Event Detection (CED) poses a formidable challenge due to the catastrophic forgetting phenomenon, where learning new tasks (with new coming event types) hampers performance on previous ones. In this paper, we introduce a novel approach, Lifelong Event Detection via Optimal Transport (LEDOT), that leverages optimal transport principles to align the optimization of our classification module with the intrinsic nature of each class, as defined by their pre-trained language modeling. Our method integrates replay sets, prototype latent representations, and an innovative Optimal Transport component. Extensive experiments on MAVEN and ACE datasets demonstrate LEDOT's superior performance, consistently outperforming state-of-the-art baselines. The results underscore LEDOT as a pioneering solution in continual event detection, offering a more effective and nuanced approach to addressing catastrophic forgetting in evolving environments.||\n", "2410.10454": "|**2024-10-14**|[Improve Meta-learning for Few-Shot Text Classification with All You Can Acquire from the Tasks](http://arxiv.org/abs/2410.10454)|**[link](https://github.com/yvogao/laqda)**|Meta-learning has emerged as a prominent technology for few-shot text classification and has achieved promising performance. However, existing methods often encounter difficulties in drawing accurate class prototypes from support set samples, primarily due to probable large intra-class differences and small inter-class differences within the task. Recent approaches attempt to incorporate external knowledge or pre-trained language models to augment data, but this requires additional resources and thus does not suit many few-shot scenarios. In this paper, we propose a novel solution to address this issue by adequately leveraging the information within the task itself. Specifically, we utilize label information to construct a task-adaptive metric space, thereby adaptively reducing the intra-class differences and magnifying the inter-class differences. We further employ the optimal transport technique to estimate class prototypes with query set samples together, mitigating the problem of inaccurate and ambiguous support set samples caused by large intra-class differences. We conduct extensive experiments on eight benchmark datasets, and our approach shows obvious advantages over state-of-the-art models across all the tasks on all the datasets. For reproducibility, all the datasets and codes are available at https://github.com/YvoGao/LAQDA.||\n", "2410.10181": "|**2024-10-14**|[Scalable Multi-Domain Adaptation of Language Models using Modular Experts](http://arxiv.org/abs/2410.10181)|null|Domain-specific adaptation is critical to maximizing the performance of pre-trained language models (PLMs) on one or multiple targeted tasks, especially under resource-constrained use cases, such as edge devices. However, existing methods often struggle to balance domain-specific performance, retention of general knowledge, and efficiency for training and inference. To address these challenges, we propose Modular Domain Experts (MoDE). MoDE is a mixture-of-experts architecture that augments a general PLMs with modular, domain-specialized experts. These experts are trained independently and composed together via a lightweight training process. In contrast to standard low-rank adaptation methods, each MoDE expert consists of several transformer layers which scale better with more training examples and larger parameter counts. Our evaluation demonstrates that MoDE achieves comparable target performances to full parameter fine-tuning while achieving 1.65% better retention performance. Moreover, MoDE's architecture enables flexible sharding configurations and improves training speeds by up to 38% over state-of-the-art distributed training configurations.||\n", "2410.13201": "|**2024-10-17**|[Meta-DiffuB: A Contextualized Sequence-to-Sequence Text Diffusion Model with Meta-Exploration](http://arxiv.org/abs/2410.13201)|**[link](https://github.com/meta-diffub/meta-diffub)**|The diffusion model, a new generative modeling paradigm, has achieved significant success in generating images, audio, video, and text. It has been adapted for sequence-to-sequence text generation (Seq2Seq) through DiffuSeq, termed S2S Diffusion. Existing S2S-Diffusion models predominantly rely on fixed or hand-crafted rules to schedule noise during the diffusion and denoising processes. However, these models are limited by non-contextualized noise, which fails to fully consider the characteristics of Seq2Seq tasks. In this paper, we propose the Meta-DiffuB framework - a novel scheduler-exploiter S2S-Diffusion paradigm designed to overcome the limitations of existing S2S-Diffusion models. We employ Meta-Exploration to train an additional scheduler model dedicated to scheduling contextualized noise for each sentence. Our exploiter model, an S2S-Diffusion model, leverages the noise scheduled by our scheduler model for updating and generation. Meta-DiffuB achieves state-of-the-art performance compared to previous S2S-Diffusion models and fine-tuned pre-trained language models (PLMs) across four Seq2Seq benchmark datasets. We further investigate and visualize the impact of Meta-DiffuB's noise scheduling on the generation of sentences with varying difficulties. Additionally, our scheduler model can function as a \"plug-and-play\" model to enhance DiffuSeq without the need for fine-tuning during the inference stage.||\n", "2410.12194": "|**2024-10-16**|[Negative-Prompt-driven Alignment for Generative Language Model](http://arxiv.org/abs/2410.12194)|null|Large language models have achieved remarkable capabilities, but aligning their outputs with human values and preferences remains a significant challenge. Existing alignment methods primarily focus on positive examples while overlooking the importance of negative responses in guiding models away from undesirable behaviors. For instance, the widely-used alignment datasets reveals a scarcity of explicit negative examples that contradict human values, hindering its ability to discourage harmful or biased outputs during training. To address this limitation, we propose NEAT, i.e., NEgative-prompt-driven AlignmenT, to introduce negative prompts to generate undesirable responses alongside positive examples during the optimization process. NEAT explicitly penalizes the model for producing harmful outputs, guiding it not only toward desirable behaviors but also steering it away from generating undesirable, biased responses. This dual feedback mechanism enables better alignment with human preferences, crucial in contexts where avoiding harm is paramount. Starting from a pre-trained language model, NEAT performs online alignment by incorporating a ranking loss derived from an expanded preference dataset containing both positive and negative examples. Extensive experiments validate NEAT's effectiveness in significantly enhancing language models' alignment with human values and preferences.||\n", "2410.12096": "|**2024-10-15**|[Bridging Large Language Models and Graph Structure Learning Models for Robust Representation Learning](http://arxiv.org/abs/2410.12096)|null|Graph representation learning, involving both node features and graph structures, is crucial for real-world applications but often encounters pervasive noise. State-of-the-art methods typically address noise by focusing separately on node features with large language models (LLMs) and on graph structures with graph structure learning models (GSLMs). In this paper, we introduce LangGSL, a robust framework that integrates the complementary strengths of pre-trained language models and GSLMs to jointly enhance both node feature and graph structure learning. In LangGSL, we first leverage LLMs to filter noise in the raw data and extract valuable cleaned information as features, enhancing the synergy of downstream models. During the mutual learning phase in LangGSL, the core idea is to leverage the relatively small language model (LM) to process local attributes and generate reliable pseudo-labels and informative node embeddings, which are then integrated into the GSLM's prediction phase. This approach enriches the global context and enhances overall performance. Meanwhile, GSLM refines the evolving graph structure constructed from the LM's output, offering updated labels back to the LM as additional guidance, thus facilitating a more effective mutual learning process. The LM and GSLM work synergistically, complementing each other's strengths and offsetting weaknesses within a variational information-maximizing framework, resulting in enhanced node features and a more robust graph structure. Extensive experiments on diverse graph datasets of varying scales and across different task scenarios demonstrate the scalability and effectiveness of the proposed approach.||\n", "2410.12064": "|**2024-10-15**|[LegalLens Shared Task 2024: Legal Violation Identification in Unstructured Text](http://arxiv.org/abs/2410.12064)|null|This paper presents the results of the LegalLens Shared Task, focusing on detecting legal violations within text in the wild across two sub-tasks: LegalLens-NER for identifying legal violation entities and LegalLens-NLI for associating these violations with relevant legal contexts and affected individuals. Using an enhanced LegalLens dataset covering labor, privacy, and consumer protection domains, 38 teams participated in the task. Our analysis reveals that while a mix of approaches was used, the top-performing teams in both tasks consistently relied on fine-tuning pre-trained language models, outperforming legal-specific models and few-shot methods. The top-performing team achieved a 7.11% improvement in NER over the baseline, while NLI saw a more marginal improvement of 5.7%. Despite these gains, the complexity of legal texts leaves room for further advancements.||\n", "2410.12034": "|**2024-10-15**|[A Survey on Deep Tabular Learning](http://arxiv.org/abs/2410.12034)|null|Tabular data, widely used in industries like healthcare, finance, and transportation, presents unique challenges for deep learning due to its heterogeneous nature and lack of spatial structure. This survey reviews the evolution of deep learning models for tabular data, from early fully connected networks (FCNs) to advanced architectures like TabNet, SAINT, TabTranSELU, and MambaNet. These models incorporate attention mechanisms, feature embeddings, and hybrid architectures to address tabular data complexities. TabNet uses sequential attention for instance-wise feature selection, improving interpretability, while SAINT combines self-attention and intersample attention to capture complex interactions across features and data points, both advancing scalability and reducing computational overhead. Hybrid architectures such as TabTransformer and FT-Transformer integrate attention mechanisms with multi-layer perceptrons (MLPs) to handle categorical and numerical data, with FT-Transformer adapting transformers for tabular datasets. Research continues to balance performance and efficiency for large datasets. Graph-based models like GNN4TDL and GANDALF combine neural networks with decision trees or graph structures, enhancing feature representation and mitigating overfitting in small datasets through advanced regularization techniques. Diffusion-based models like the Tabular Denoising Diffusion Probabilistic Model (TabDDPM) generate synthetic data to address data scarcity, improving model robustness. Similarly, models like TabPFN and Ptab leverage pre-trained language models, incorporating transfer learning and self-supervised techniques into tabular tasks. This survey highlights key advancements and outlines future research directions on scalability, generalization, and interpretability in diverse tabular data applications.||\n"}, "Transformer": {"2409.02727": "|**2024-09-05**|[Pooling And Attention: What Are Effective Designs For LLM-Based Embedding Models?](http://arxiv.org/abs/2409.02727)|**[link](https://github.com/yixuantt/poolingandattn)**|The significant advancements of Large Language Models (LLMs) in generative tasks have led to a growing body of work exploring LLM-based embedding models. While these models, employing different pooling and attention strategies, have achieved state-of-the-art performance on public embedding benchmarks, questions still arise about what constitutes an effective design for LLM-based embedding models. However, these models are often trained on different datasets, using different LLM base models or training settings. Moreover, evaluations on public embedding benchmarks often fail to report statistical significance, making it difficult to determine which designs truly contribute to final performance. This complicates the process for practitioners seeking optimal training recipes for LLM-based embedding models. In this study, we conduct a large-scale experiment by training a series of LLM-based embedding models using the same training data and base model but differing in their pooling and attention strategies. The results show that there is no one-size-fits-all solution: while bidirectional attention and an additional trainable pooling layer outperform in text similarity and information retrieval tasks, they do not significantly surpass simpler designs like EOS-last token pooling and default causal attention in clustering and classification tasks. Furthermore, we propose a new pooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs of all hidden layers, rather than just the last layer, using a cross-attention network. This method proves to be statistically superior in text similarity and retrieval tasks compared to existing pooling methods. Overall, this paper sheds light on effective training strategies for LLM-based embedding models.||\n", "2409.02545": "|**2024-09-04**|[UniTT-Stereo: Unified Training of Transformer for Enhanced Stereo Matching](http://arxiv.org/abs/2409.02545)|null|Unlike other vision tasks where Transformer-based approaches are becoming increasingly common, stereo depth estimation is still dominated by convolution-based approaches. This is mainly due to the limited availability of real-world ground truth for stereo matching, which is a limiting factor in improving the performance of Transformer-based stereo approaches. In this paper, we propose UniTT-Stereo, a method to maximize the potential of Transformer-based stereo architectures by unifying self-supervised learning used for pre-training with stereo matching framework based on supervised learning. To be specific, we explore the effectiveness of reconstructing features of masked portions in an input image and at the same time predicting corresponding points in another image from the perspective of locality inductive bias, which is crucial in training models with limited training data. Moreover, to address these challenging tasks of reconstruction-and-prediction, we present a new strategy to vary a masking ratio when training the stereo model with stereo-tailored losses. State-of-the-art performance of UniTT-Stereo is validated on various benchmarks such as ETH3D, KITTI 2012, and KITTI 2015 datasets. Lastly, to investigate the advantages of the proposed approach, we provide a frequency analysis of feature maps and the analysis of locality inductive bias based on attention maps.||\n", "2409.02056": "|**2024-09-03**|[F2former: When Fractional Fourier Meets Deep Wiener Deconvolution and Selective Frequency Transformer for Image Deblurring](http://arxiv.org/abs/2409.02056)|null|Recent progress in image deblurring techniques focuses mainly on operating in both frequency and spatial domains using the Fourier transform (FT) properties. However, their performance is limited due to the dependency of FT on stationary signals and its lack of capability to extract spatial-frequency properties. In this paper, we propose a novel approach based on the Fractional Fourier Transform (FRFT), a unified spatial-frequency representation leveraging both spatial and frequency components simultaneously, making it ideal for processing non-stationary signals like images. Specifically, we introduce a Fractional Fourier Transformer (F2former), where we combine the classical fractional Fourier based Wiener deconvolution (F2WD) as well as a multi-branch encoder-decoder transformer based on a new fractional frequency aware transformer block (F2TB). We design F2TB consisting of a fractional frequency aware self-attention (F2SA) to estimate element-wise product attention based on important frequency components and a novel feed-forward network based on frequency division multiplexing (FM-FFN) to refine high and low frequency features separately for efficient latent clear image restoration. Experimental results for the cases of both motion deblurring as well as defocus deblurring show that the performance of our proposed method is superior to other state-of-the-art (SOTA) approaches.||\n", "2409.02018": "|**2024-09-03**|[TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation](http://arxiv.org/abs/2409.02018)|null|In healthcare, medical image segmentation is crucial for accurate disease diagnosis and the development of effective treatment strategies. Early detection can significantly aid in managing diseases and potentially prevent their progression. Machine learning, particularly deep convolutional neural networks, has emerged as a promising approach to addressing segmentation challenges. Traditional methods like U-Net use encoding blocks for local representation modeling and decoding blocks to uncover semantic relationships. However, these models often struggle with multi-scale objects exhibiting significant variations in texture and shape, and they frequently fail to capture long-range dependencies in the input data. Transformers designed for sequence-to-sequence predictions have been proposed as alternatives, utilizing global self-attention mechanisms. Yet, they can sometimes lack precise localization due to insufficient granular details. To overcome these limitations, we introduce TransDAE: a novel approach that reimagines the self-attention mechanism to include both spatial and channel-wise associations across the entire feature space, while maintaining computational efficiency. Additionally, TransDAE enhances the skip connection pathway with an inter-scale interaction module, promoting feature reuse and improving localization accuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on the Synaps multi-organ dataset, even without relying on pre-trained weights.||\n", "2409.01557": "|**2024-09-03**|[TASL-Net: Tri-Attention Selective Learning Network for Intelligent Diagnosis of Bimodal Ultrasound Video](http://arxiv.org/abs/2409.01557)|null|In the intelligent diagnosis of bimodal (gray-scale and contrast-enhanced) ultrasound videos, medical domain knowledge such as the way sonographers browse videos, the particular areas they emphasize, and the features they pay special attention to, plays a decisive role in facilitating precise diagnosis. Embedding medical knowledge into the deep learning network can not only enhance performance but also boost clinical confidence and reliability of the network. However, it is an intractable challenge to automatically focus on these person- and disease-specific features in videos and to enable networks to encode bimodal information comprehensively and efficiently. This paper proposes a novel Tri-Attention Selective Learning Network (TASL-Net) to tackle this challenge and automatically embed three types of diagnostic attention of sonographers into a mutual transformer framework for intelligent diagnosis of bimodal ultrasound videos. Firstly, a time-intensity-curve-based video selector is designed to mimic the temporal attention of sonographers, thus removing a large amount of redundant information while improving computational efficiency of TASL-Net. Then, to introduce the spatial attention of the sonographers for contrast-enhanced video analysis, we propose the earliest-enhanced position detector based on structural similarity variation, on which the TASL-Net is made to focus on the differences of perfusion variation inside and outside the lesion. Finally, by proposing a mutual encoding strategy that combines convolution and transformer, TASL-Net possesses bimodal attention to structure features on gray-scale videos and to perfusion variations on contrast-enhanced videos. These modules work collaboratively and contribute to superior performance. We conduct a detailed experimental validation of TASL-Net's performance on three datasets, including lung, breast, and liver.||\n", "2409.01352": "|**2024-09-02**|[Spectron: Target Speaker Extraction using Conditional Transformer with Adversarial Refinement](http://arxiv.org/abs/2409.01352)|**[link](https://github.com/tatban/Spectron)**|Recently, attention-based transformers have become a de facto standard in many deep learning applications including natural language processing, computer vision, signal processing, etc.. In this paper, we propose a transformer-based end-to-end model to extract a target speaker's speech from a monaural multi-speaker mixed audio signal. Unlike existing speaker extraction methods, we introduce two additional objectives to impose speaker embedding consistency and waveform encoder invertibility and jointly train both speaker encoder and speech separator to better capture the speaker conditional embedding. Furthermore, we leverage a multi-scale discriminator to refine the perceptual quality of the extracted speech. Our experiments show that the use of a dual path transformer in the separator backbone along with proposed training paradigm improves the CNN baseline by $3.12$ dB points. Finally, we compare our approach with recent state-of-the-arts and show that our model outperforms existing methods by $4.1$ dB points on an average without creating additional data dependency.||\n", "2409.01193": "|**2024-09-02**|[CLIBE: Detecting Dynamic Backdoors in Transformer-based NLP Models](http://arxiv.org/abs/2409.01193)|**[link](https://github.com/raytsang123/clibe)**|Backdoors can be injected into NLP models to induce misbehavior when the input text contains a specific feature, known as a trigger, which the attacker secretly selects. Unlike fixed words, phrases, or sentences used in the static text trigger, NLP dynamic backdoor attacks design triggers associated with abstract and latent text features, making them considerably stealthier than traditional static backdoor attacks. However, existing research on NLP backdoor detection primarily focuses on defending against static backdoor attacks, while detecting dynamic backdoors in NLP models remains largely unexplored. This paper presents CLIBE, the first framework to detect dynamic backdoors in Transformer-based NLP models. CLIBE injects a \"few-shot perturbation\" into the suspect Transformer model by crafting optimized weight perturbation in the attention layers to make the perturbed model classify a limited number of reference samples as a target label. Subsequently, CLIBE leverages the generalization ability of this few-shot perturbation to determine whether the original model contains a dynamic backdoor. Extensive evaluation on three advanced NLP dynamic backdoor attacks, two widely-used Transformer frameworks, and four real-world classification tasks strongly validates the effectiveness of CLIBE. We also demonstrate the robustness of CLIBE against various adaptive attacks. Furthermore, we employ CLIBE to scrutinize 49 popular Transformer models on Hugging Face and discover one exhibiting a high probability of containing a dynamic backdoor. We have contacted Hugging Face and provided detailed evidence of this model's backdoor behavior. Moreover, we extend CLIBE to detect backdoor text generation models modified to exhibit toxic behavior. To the best of our knowledge, CLIBE is the first framework capable of detecting backdoors in text generation models without access to trigger input test samples.||\n", "2409.01068": "|**2024-09-02**|[Progressive Retinal Image Registration via Global and Local Deformable Transformations](http://arxiv.org/abs/2409.01068)|**[link](https://github.com/lyp-deeplearning/awesome-retinal-registration)**|Retinal image registration plays an important role in the ophthalmological diagnosis process. Since there exist variances in viewing angles and anatomical structures across different retinal images, keypoint-based approaches become the mainstream methods for retinal image registration thanks to their robustness and low latency. These methods typically assume the retinal surfaces are planar, and adopt feature matching to obtain the homography matrix that represents the global transformation between images. Yet, such a planar hypothesis inevitably introduces registration errors since retinal surface is approximately curved. This limitation is more prominent when registering image pairs with significant differences in viewing angles. To address this problem, we propose a hybrid registration framework called HybridRetina, which progressively registers retinal images with global and local deformable transformations. For that, we use a keypoint detector and a deformation network called GAMorph to estimate the global transformation and local deformable transformation, respectively. Specifically, we integrate multi-level pixel relation knowledge to guide the training of GAMorph. Additionally, we utilize an edge attention module that includes the geometric priors of the images, ensuring the deformation field focuses more on the vascular regions of clinical interest. Experiments on two widely-used datasets, FIRE and FLoRI21, show that our proposed HybridRetina significantly outperforms some state-of-the-art methods. The code is available at https://github.com/lyp-deeplearning/awesome-retinal-registration.||\n", "2409.00904": "|**2024-09-02**|[Multi-scale Temporal Fusion Transformer for Incomplete Vehicle Trajectory Prediction](http://arxiv.org/abs/2409.00904)|null|Motion prediction plays an essential role in autonomous driving systems, enabling autonomous vehicles to achieve more accurate local-path planning and driving decisions based on predictions of the surrounding vehicles. However, existing methods neglect the potential missing values caused by object occlusion, perception failures, etc., which inevitably degrades the trajectory prediction performance in real traffic scenarios. To address this limitation, we propose a novel end-to-end framework for incomplete vehicle trajectory prediction, named Multi-scale Temporal Fusion Transformer (MTFT), which consists of the Multi-scale Attention Head (MAH) and the Continuity Representation-guided Multi-scale Fusion (CRMF) module. Specifically, the MAH leverages the multi-head attention mechanism to parallelly capture multi-scale motion representation of trajectory from different temporal granularities, thus mitigating the adverse effect of missing values on prediction. Furthermore, the multi-scale motion representation is input into the CRMF module for multi-scale fusion to obtain the robust temporal feature of the vehicle. During the fusion process, the continuity representation of vehicle motion is first extracted across time steps to guide the fusion, ensuring that the resulting temporal feature incorporates both detailed information and the overall trend of vehicle motion, which facilitates the accurate decoding of future trajectory that is consistent with the vehicle's motion trend. We evaluate the proposed model on four datasets derived from highway and urban traffic scenarios. The experimental results demonstrate its superior performance in the incomplete vehicle trajectory prediction task compared with state-of-the-art models, e.g., a comprehensive performance improvement of more than 39% on the HighD dataset.||\n", "2409.00591": "|**2024-09-01**|[Attention-Guided Multi-scale Interaction Network for Face Super-Resolution](http://arxiv.org/abs/2409.00591)|null|Recently, CNN and Transformer hybrid networks demonstrated excellent performance in face super-resolution (FSR) tasks. Since numerous features at different scales in hybrid networks, how to fuse these multi-scale features and promote their complementarity is crucial for enhancing FSR. However, existing hybrid network-based FSR methods ignore this, only simply combining the Transformer and CNN. To address this issue, we propose an attention-guided Multi-scale interaction network (AMINet), which contains local and global feature interactions as well as encoder-decoder phases feature interactions. Specifically, we propose a Local and Global Feature Interaction Module (LGFI) to promote fusions of global features and different receptive fields' local features extracted by our Residual Depth Feature Extraction Module (RDFE). Additionally, we propose a Selective Kernel Attention Fusion Module (SKAF) to adaptively select fusions of different features within LGFI and encoder-decoder phases. Our above design allows the free flow of multi-scale features from within modules and between encoder and decoder, which can promote the complementarity of different scale features to enhance FSR. Comprehensive experiments confirm that our method consistently performs well with less computational consumption and faster inference.||\n", "2409.03621": "|**2024-09-05**|[Attend First, Consolidate Later: On the Importance of Attention in Different LLM Layers](http://arxiv.org/abs/2409.03621)|null|In decoder-based LLMs, the representation of a given layer serves two purposes: as input to the next layer during the computation of the current token; and as input to the attention mechanism of future tokens. In this work, we show that the importance of the latter role might be overestimated. To show that, we start by manipulating the representations of previous tokens; e.g. by replacing the hidden states at some layer k with random vectors. Our experimenting with four LLMs and four tasks show that this operation often leads to small to negligible drop in performance. Importantly, this happens if the manipulation occurs in the top part of the model-k is in the final 30-50% of the layers. In contrast, doing the same manipulation in earlier layers might lead to chance level performance. We continue by switching the hidden state of certain tokens with hidden states of other tokens from another prompt; e.g., replacing the word \"Italy\" with \"France\" in \"What is the capital of Italy?\". We find that when applying this switch in the top 1/3 of the model, the model ignores it (answering \"Rome\"). However if we apply it before, the model conforms to the switch (\"Paris\"). Our results hint at a two stage process in transformer-based LLMs: the first part gathers input from previous tokens, while the second mainly processes that information internally.||\n", "2409.03516": "|**2024-09-05**|[LMLT: Low-to-high Multi-Level Vision Transformer for Image Super-Resolution](http://arxiv.org/abs/2409.03516)|**[link](https://github.com/jwgdmkj/lmlt)**|Recent Vision Transformer (ViT)-based methods for Image Super-Resolution have demonstrated impressive performance. However, they suffer from significant complexity, resulting in high inference times and memory usage. Additionally, ViT models using Window Self-Attention (WSA) face challenges in processing regions outside their windows. To address these issues, we propose the Low-to-high Multi-Level Transformer (LMLT), which employs attention with varying feature sizes for each head. LMLT divides image features along the channel dimension, gradually reduces spatial size for lower heads, and applies self-attention to each head. This approach effectively captures both local and global information. By integrating the results from lower heads into higher heads, LMLT overcomes the window boundary issues in self-attention. Extensive experiments show that our model significantly reduces inference time and GPU memory usage while maintaining or even surpassing the performance of state-of-the-art ViT-based Image Super-Resolution methods. Our codes are availiable at https://github.com/jwgdmkj/LMLT.||\n", "2409.03514": "|**2024-09-05**|[Blended Latent Diffusion under Attention Control for Real-World Video Editing](http://arxiv.org/abs/2409.03514)|null|\u7531\u4e8e\u7f3a\u4e4f\u5b8c\u5168\u516c\u5f00\u53ef\u7528\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\uff0c\u5f53\u524d\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u503e\u5411\u4e8e\u5efa\u7acb\u5728\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e4b\u4e0a\uff0c\u7136\u800c\uff0c\u5b83\u4eec\u5728\u5904\u7406\u5177\u6709\u65f6\u95f4\u4fe1\u606f\u7684\u89c6\u9891\u5c40\u90e8\u7f16\u8f91\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u9996\u5148\uff0c\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u8bd5\u56fe\u901a\u8fc7\u9884\u5b9a\u4e49\u7684\u63a9\u7801\u4e13\u6ce8\u4e8e\u5c40\u90e8\u533a\u57df\u7f16\u8f91\uff0c\u4f46\u7531\u4e8e\u6bcf\u4e00\u5e27\u7684\u7a7a\u95f4\u6574\u4f53\u751f\u6210\uff0c\u533a\u57df\u5916\u80cc\u666f\u7684\u4fdd\u7559\u5e76\u4e0d\u7406\u60f3\u3002\u6b64\u5916\uff0c\u7528\u6237\u4e13\u95e8\u63d0\u4f9b\u63a9\u7801\u662f\u4e00\u9879\u989d\u5916\u7684\u6602\u8d35\u5de5\u4f5c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u96c6\u6210\u5230\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u7684\u81ea\u4e3b\u63a9\u7801\u7b56\u7565\u3002\u6700\u540e\u4f46\u540c\u6837\u91cd\u8981\u7684\u662f\uff0c\u56fe\u50cf\u7ea7\u9884\u8bad\u7ec3\u6a21\u578b\u6ca1\u6709\u5b66\u4e60\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u800c\u8fd9\u5bf9\u4e8e\u8868\u8fbe\u8fd0\u52a8\u548c\u52a8\u6001\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5efa\u8bae\u91c7\u7528\u56fe\u50cf\u7ea7\u6df7\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6765\u6267\u884c\u5c40\u90e8\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5229\u7528 DDIM \u53cd\u6f14\u6765\u83b7\u53d6\u6f5c\u5728\u4ee3\u7801\u4f5c\u4e3a\u80cc\u666f\u6f5c\u5728\u4ee3\u7801\uff0c\u800c\u4e0d\u662f\u968f\u673a\u566a\u58f0\u7684\u6f5c\u5728\u4ee3\u7801\uff0c\u4ee5\u66f4\u597d\u5730\u4fdd\u7559\u8f93\u5165\u89c6\u9891\u7684\u80cc\u666f\u4fe1\u606f\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4ece\u6269\u6563\u6b65\u9aa4\u4e2d\u7684\u4ea4\u53c9\u6ce8\u610f\u56fe\u6d3e\u751f\u7684\u81ea\u4e3b\u63a9\u7801\u5236\u9020\u673a\u5236\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u5c06 U-Net \u7684\u81ea\u6ce8\u610f\u529b\u5757\u8f6c\u6362\u4e3a\u65f6\u7a7a\u5757\u6765\u589e\u5f3a\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u6709\u6548\u6027\u3002||\n", "2409.03463": "|**2024-09-05**|[Characterizing Massive Activations of Attention Mechanism in Graph Neural Networks](http://arxiv.org/abs/2409.03463)|**[link](https://github.com/msorbi/gnn-ma)**|Graph Neural Networks (GNNs) have become increasingly popular for effectively modeling data with graph structures. Recently, attention mechanisms have been integrated into GNNs to improve their ability to capture complex patterns. This paper presents the first comprehensive study revealing a critical, unexplored consequence of this integration: the emergence of Massive Activations (MAs) within attention layers. We introduce a novel method for detecting and analyzing MAs, focusing on edge features in different graph transformer architectures. Our study assesses various GNN models using benchmark datasets, including ZINC, TOX21, and PROTEINS. Key contributions include (1) establishing the direct link between attention mechanisms and MAs generation in GNNs, (2) developing a robust definition and detection method for MAs based on activation ratio distributions, (3) introducing the Explicit Bias Term (EBT) as a potential countermeasure and exploring it as an adversarial framework to assess models robustness based on the presence or absence of MAs. Our findings highlight the prevalence and impact of attention-induced MAs across different architectures, such as GraphTransformer, GraphiT, and SAN. The study reveals the complex interplay between attention mechanisms, model architecture, dataset characteristics, and MAs emergence, providing crucial insights for developing more robust and reliable graph models.||\n", "2409.03460": "|**2024-09-05**|[LowFormer: Hardware Efficient Design for Convolutional Transformer Backbones](http://arxiv.org/abs/2409.03460)|**[link](https://github.com/altair199797/lowformer)**|Research in efficient vision backbones is evolving into models that are a mixture of convolutions and transformer blocks. A smart combination of both, architecture-wise and component-wise is mandatory to excel in the speedaccuracy trade-off. Most publications focus on maximizing accuracy and utilize MACs (multiply accumulate operations) as an efficiency metric. The latter however often do not measure accurately how fast a model actually is due to factors like memory access cost and degree of parallelism. We analyzed common modules and architectural design choices for backbones not in terms of MACs, but rather in actual throughput and latency, as the combination of the latter two is a better representation of the efficiency of models in real applications. We applied the conclusions taken from that analysis to create a recipe for increasing hardware-efficiency in macro design. Additionally we introduce a simple slimmed-down version of MultiHead Self-Attention, that aligns with our analysis. We combine both macro and micro design to create a new family of hardware-efficient backbone networks called LowFormer. LowFormer achieves a remarkable speedup in terms of throughput and latency, while achieving similar or better accuracy than current state-of-the-art efficient backbones. In order to prove the generalizability of our hardware-efficient design, we evaluate our method on GPU, mobile GPU and ARM CPU. We further show that the downstream tasks object detection and semantic segmentation profit from our hardware-efficient architecture. Code and models are available at https://github.com/ altair199797/LowFormer.||\n", "2409.03332": "|**2024-09-05**|[Masked Sensory-Temporal Attention for Sensor Generalization in Quadruped Locomotion](http://arxiv.org/abs/2409.03332)|null|With the rising focus on quadrupeds, a generalized policy capable of handling different robot models and sensory inputs will be highly beneficial. Although several methods have been proposed to address different morphologies, it remains a challenge for learning-based policies to manage various combinations of proprioceptive information. This paper presents Masked Sensory-Temporal Attention (MSTA), a novel transformer-based model with masking for quadruped locomotion. It employs direct sensor-level attention to enhance sensory-temporal understanding and handle different combinations of sensor data, serving as a foundation for incorporating unseen information. This model can effectively understand its states even with a large portion of missing information, and is flexible enough to be deployed on a physical system despite the long input sequence.||\n", "2409.03223": "|**2024-09-05**|[Why mamba is effective? Exploit Linear Transformer-Mamba Network for Multi-Modality Image Fusion](http://arxiv.org/abs/2409.03223)|null|Multi-modality image fusion aims to integrate the merits of images from different sources and render high-quality fusion images. However, existing feature extraction and fusion methods are either constrained by inherent local reduction bias and static parameters during inference (CNN) or limited by quadratic computational complexity (Transformers), and cannot effectively extract and fuse features. To solve this problem, we propose a dual-branch image fusion network called Tmamba. It consists of linear Transformer and Mamba, which has global modeling capabilities while maintaining linear complexity. Due to the difference between the Transformer and Mamba structures, the features extracted by the two branches carry channel and position information respectively. T-M interaction structure is designed between the two branches, using global learnable parameters and convolutional layers to transfer position and channel information respectively. We further propose cross-modal interaction at the attention level to obtain cross-modal attention. Experiments show that our Tmamba achieves promising results in multiple fusion tasks, including infrared-visible image fusion and medical image fusion. Code with checkpoints will be available after the peer-review process.||\n", "2409.03115": "|**2024-09-04**|[Probing self-attention in self-supervised speech models for cross-linguistic differences](http://arxiv.org/abs/2409.03115)|null|Speech models have gained traction thanks to increase in accuracy from novel transformer architectures. While this impressive increase in performance across automatic speech recognition (ASR) benchmarks is noteworthy, there is still much that is unknown about the use of attention mechanisms for speech-related tasks. For example, while it is assumed that these models are learning language-independent (i.e., universal) speech representations, there has not yet been an in-depth exploration of what it would mean for the models to be language-independent. In the current paper, we explore this question within the realm of self-attention mechanisms of one small self-supervised speech transformer model (TERA). We find that even with a small model, the attention heads learned are diverse ranging from almost entirely diagonal to almost entirely global regardless of the training language. We highlight some notable differences in attention patterns between Turkish and English and demonstrate that the models do learn important phonological information during pretraining. We also present a head ablation study which shows that models across languages primarily rely on diagonal heads to classify phonemes.||\n", "2409.03103": "|**2024-09-04**|[Leveraging Interpretability in the Transformer to Automate the Proactive Scaling of Cloud Resources](http://arxiv.org/abs/2409.03103)|null|\u73b0\u4ee3Web\u670d\u52a1\u91c7\u7528\u4e91\u539f\u751f\u539f\u5219\u6765\u5229\u7528\u5fae\u670d\u52a1\u7684\u4f18\u52bf\u3002\u4e3a\u4e86\u6839\u636e\u670d\u52a1\u7b49\u7ea7\u534f\u8bae\uff08SLA\uff09\u6301\u7eed\u4fdd\u8bc1\u9ad8\u8d28\u91cf\u7684\u670d\u52a1\uff08QoS\uff09\uff0c\u786e\u4fdd\u4ee4\u4eba\u6ee1\u610f\u7684\u7528\u6237\u4f53\u9a8c\u5e76\u6700\u5927\u7a0b\u5ea6\u5730\u964d\u4f4e\u8fd0\u8425\u6210\u672c\uff0c\u5fc5\u987b\u4e3a\u6bcf\u4e2a\u5fae\u670d\u52a1\u914d\u7f6e\u9002\u91cf\u7684\u8d44\u6e90\u3002\u7136\u800c\uff0c\u51c6\u786e\u5730\u4e3a\u5fae\u670d\u52a1\u914d\u7f6e\u5145\u8db3\u7684\u8d44\u6e90\u975e\u5e38\u590d\u6742\uff0c\u5e76\u4e14\u53d6\u51b3\u4e8e\u8bb8\u591a\u56e0\u7d20\uff0c\u5305\u62ec\u5de5\u4f5c\u8d1f\u8f7d\u5f3a\u5ea6\u548c\u5fae\u670d\u52a1\u4e4b\u95f4\u590d\u6742\u7684\u4e92\u8fde\u5173\u7cfb\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u6355\u83b7\u4e86\u7aef\u5230\u7aef\u5ef6\u8fdf\u3001\u524d\u7aef\u7ea7\u522b\u7684\u8bf7\u6c42\u548c\u8d44\u6e90\u5229\u7528\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u5f00\u53d1\u7684\u6a21\u578b\u6765\u9884\u6d4b\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002\u6211\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u5229\u7528\u4e86\u65f6\u95f4\u878d\u5408Transformer\uff08TFT\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5177\u6709\u53ef\u89e3\u91ca\u6027\u7279\u5f81\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u67b6\u6784\u3002\u5f53\u9884\u6d4b\u7ed3\u679c\u8868\u660e\u4e0d\u7b26\u5408SLA\u65f6\uff0c\u6211\u4eec\u4f7f\u7528TFT\u63d0\u4f9b\u7684\u7279\u5f81\u91cd\u8981\u6027\u4f5c\u4e3a\u6838\u5cad\u56de\u5f52\uff08KRR\uff09\u4e2d\u7684\u534f\u53d8\u91cf\uff0c\u5e76\u5c06\u54cd\u5e94\u53d8\u91cf\u8bbe\u7f6e\u4e3a\u671f\u671b\u5ef6\u8fdf\uff0c\u4ee5\u5b66\u4e60\u4e0e\u7279\u5f81\u91cd\u8981\u6027\u76f8\u5173\u7684\u53c2\u6570\u3002\u8fd9\u4e9b\u5b66\u4e60\u5230\u7684\u53c2\u6570\u53cd\u6620\u4e86\u4e3a\u786e\u4fdd\u7b26\u5408SLA\u800c\u9700\u8981\u5bf9\u7279\u5f81\u8fdb\u884c\u7684\u8c03\u6574\u3002\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u57fa\u4e8e\u5fae\u670d\u52a1\u7684\u5e94\u7528\u7a0b\u5e8f\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u90e8\u7f72\u8def\u7ebf\u56fe\u3002||\n", "2409.05749": "|**2024-09-09**|[ReL-SAR: Representation Learning for Skeleton Action Recognition with Convolutional Transformers and BYOL](http://arxiv.org/abs/2409.05749)|null|\u4e3a\u4e86\u63d0\u53d6\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u7684\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u7279\u5f81\uff0c\u901a\u5e38\u9700\u8981\u5927\u91cf\u7cbe\u5fc3\u6807\u6ce8\u7684\u6570\u636e\uff0c\u800c\u6807\u6ce8\u548c\u8ba1\u7b97\u6210\u672c\u7684\u9650\u5236\u4f7f\u5f97\u8fd9\u9879\u4efb\u52a1\u6781\u5177\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u5229\u7528\u65e0\u6807\u7b7e\u9aa8\u67b6\u6570\u636e\u7684\u65e0\u76d1\u7763\u8868\u5f81\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u7528\u4e8e\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u7684\u65e0\u76d1\u7763\u8868\u5f81\u5b66\u4e60\u65b9\u6cd5\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5377\u79efTransformer\u6846\u67b6\uff0c\u540d\u4e3aReL-SAR\uff0c\u5b83\u5229\u7528\u5377\u79ef\u5c42\u548c\u6ce8\u610f\u529b\u5c42\u7684\u4e92\u8865\u6027\u6765\u8054\u5408\u5efa\u6a21\u9aa8\u67b6\u5e8f\u5217\u4e2d\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u7ebf\u7d22\u3002\u6211\u4eec\u8fd8\u5bf9\u9aa8\u67b6\u5173\u8282\u91c7\u7528\u4e86\u9009\u62e9-\u6392\u5217\u7b56\u7565\uff0c\u4ee5\u786e\u4fdd\u4ece\u9aa8\u9abc\u6570\u636e\u4e2d\u83b7\u53d6\u66f4\u591a\u4fe1\u606f\u3002\u6700\u540e\uff0c\u6211\u4eec\u5229\u7528Bootstrap Your Own Latent\uff08BYOL\uff09\u4ece\u65e0\u6807\u7b7e\u9aa8\u67b6\u5e8f\u5217\u6570\u636e\u4e2d\u5b66\u4e60\u9c81\u68d2\u7684\u8868\u5f81\u3002\u6211\u4eec\u5728\u6709\u9650\u5927\u5c0f\u7684\u6570\u636e\u96c6\uff1aMCAD\u3001IXMAS\u3001JHMDB\u548cNW-UCLA\u4e0a\u53d6\u5f97\u4e86\u975e\u5e38\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u76f8\u5bf9\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6709\u6548\u6027\u3002\u4e3a\u4e86\u786e\u4fdd\u53ef\u91cd\u590d\u6027\u548c\u53ef\u590d\u7528\u6027\uff0c\u6211\u4eec\u5728\u4ee5\u4e0b\u94fe\u63a5\u63d0\u4f9b\u4e86\u5305\u542b\u6240\u6709\u5b9e\u73b0\u53c2\u6570\u7684\u6e90\u4ee3\u7801\uff1ahttps://github.com/SafwenNaimi/Representation-Learning-for-Skeleton-Action-Recognition-with-Convolutional-Transformers-and-BYOL||\n", "2409.05587": "|**2024-09-09**|[DSDFormer: An Innovative Transformer-Mamba Framework for Robust High-Precision Driver Distraction Identification](http://arxiv.org/abs/2409.05587)|null|Driver distraction remains a leading cause of traffic accidents, posing a critical threat to road safety globally. As intelligent transportation systems evolve, accurate and real-time identification of driver distraction has become essential. However, existing methods struggle to capture both global contextual and fine-grained local features while contending with noisy labels in training datasets. To address these challenges, we propose DSDFormer, a novel framework that integrates the strengths of Transformer and Mamba architectures through a Dual State Domain Attention (DSDA) mechanism, enabling a balance between long-range dependencies and detailed feature extraction for robust driver behavior recognition. Additionally, we introduce Temporal Reasoning Confident Learning (TRCL), an unsupervised approach that refines noisy labels by leveraging spatiotemporal correlations in video sequences. Our model achieves state-of-the-art performance on the AUC-V1, AUC-V2, and 100-Driver datasets and demonstrates real-time processing efficiency on the NVIDIA Jetson AGX Orin platform. Extensive experimental results confirm that DSDFormer and TRCL significantly improve both the accuracy and robustness of driver distraction detection, offering a scalable solution to enhance road safety.||\n", "2409.05477": "|**2024-09-10**|[Retrofitting Temporal Graph Neural Networks with Transformer](http://arxiv.org/abs/2409.05477)|**[link](https://github.com/qianghuangwhu/tf-tgn)**|Temporal graph neural networks (TGNNs) outperform regular GNNs by incorporating time information into graph-based operations. However, TGNNs adopt specialized models (e.g., TGN, TGAT, and APAN ) and require tailored training frameworks (e.g., TGL and ETC). In this paper, we propose TF-TGN, which uses Transformer decoder as the backbone model for TGNN to enjoy Transformer's codebase for efficient training. In particular, Transformer achieves tremendous success for language modeling, and thus the community developed high-performance kernels (e.g., flash-attention and memory-efficient attention) and efficient distributed training schemes (e.g., PyTorch FSDP, DeepSpeed, and Megatron-LM). We observe that TGNN resembles language modeling, i.e., the message aggregation operation between chronologically occurring nodes and their temporal neighbors in TGNNs can be structured as sequence modeling. Beside this similarity, we also incorporate a series of algorithm designs including suffix infilling, temporal graph attention with self-loop, and causal masking self-attention to make TF-TGN work. During training, existing systems are slow in transforming the graph topology and conducting graph sampling. As such, we propose methods to parallelize the CSR format conversion and graph sampling. We also adapt Transformer codebase to train TF-TGN efficiently with multiple GPUs. We experiment with 9 graphs and compare with 2 state-of-the-art TGNN training frameworks. The results show that TF-TGN can accelerate training by over 2.20 while providing comparable or even superior accuracy to existing SOTA TGNNs. TF-TGN is available at https://github.com/qianghuangwhu/TF-TGN.||\n", "2409.05207": "|**2024-09-08**|[Low Latency Transformer Inference on FPGAs for Physics Applications with hls4ml](http://arxiv.org/abs/2409.05207)|null|This study presents an efficient implementation of transformer architectures in Field-Programmable Gate Arrays(FPGAs) using hls4ml. We demonstrate the strategy for implementing the multi-head attention, softmax, and normalization layer and evaluate three distinct models. Their deployment on VU13P FPGA chip achieved latency less than 2us, demonstrating the potential for real-time applications. HLS4ML compatibility with any TensorFlow-built transformer model further enhances the scalability and applicability of this work. Index Terms: FPGAs, machine learning, transformers, high energy physics, LIGO||\n", "2409.05136": "|**2024-09-08**|[MHS-STMA: Multimodal Hate Speech Detection via Scalable Transformer-Based Multilevel Attention Framework](http://arxiv.org/abs/2409.05136)|null|Social media has a significant impact on people's lives. Hate speech on social media has emerged as one of society's most serious issues recently. Text and pictures are two forms of multimodal data distributed within articles. Unimodal analysis has been the primary emphasis of earlier approaches. Additionally, when doing multimodal analysis, researchers neglect to preserve the distinctive qualities associated with each modality. The present article suggests a scalable architecture for multimodal hate content detection called transformer-based multilevel attention (STMA) to address these shortcomings. This architecture consists of three main parts: a combined attention-based deep learning mechanism, a vision attention mechanism encoder, and a caption attention-mechanism encoder. To identify hate content, each component uses various attention processes and uniquely handles multimodal data. Several studies employing multiple assessment criteria on three hate speech datasets: Hateful memes, MultiOff, and MMHS150K, validate the suggested architecture's efficacy. The outcomes demonstrate that on all three datasets, the suggested strategy performs better than the baseline approaches.||\n", "2409.04940": "|**2024-09-08**|[An Analog and Digital Hybrid Attention Accelerator for Transformers with Charge-based In-memory Computing](http://arxiv.org/abs/2409.04940)|null|The attention mechanism is a key computing kernel of Transformers, calculating pairwise correlations across the entire input sequence. The computing complexity and frequent memory access in computing self-attention put a huge burden on the system especially when the sequence length increases. This paper presents an analog and digital hybrid processor to accelerate the attention mechanism for transformers in 65nm CMOS technology. We propose an analog computing-in-memory (CIM) core, which prunes ~75% of low-score tokens on average during runtime at ultra-low power and delay. Additionally, a digital processor performs precise computations only for ~25% unpruned tokens selected by the analog CIM core, preventing accuracy degradation. Measured results show peak energy efficiency of 14.8 and 1.65 TOPS/W, and peak area efficiency of 976.6 and 79.4 GOPS/mm$^\\mathrm{2}$ in the analog core and the system-on-chip (SoC), respectively.||\n", "2409.04909": "|**2024-09-07**|[Efficient Training of Transformers for Molecule Property Prediction on Small-scale Datasets](http://arxiv.org/abs/2409.04909)|null|\u8840\u8111\u5c4f\u969c\uff08BBB\uff09\u662f\u4e00\u9053\u4fdd\u62a4\u6027\u5c4f\u969c\uff0c\u5c06\u5927\u8111\u4e0e\u5faa\u73af\u7cfb\u7edf\u9694\u5f00\uff0c\u8c03\u8282\u7269\u8d28\u8fdb\u5165\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf\u7684\u901a\u9053\u3002\u8bc4\u4f30\u6f5c\u5728\u836f\u7269\u7684BBB\u6e17\u900f\u6027\u5bf9\u4e8e\u6709\u6548\u7684\u836f\u7269\u9776\u5411\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684BBB\u6e17\u900f\u6027\u6d4b\u91cf\u5b9e\u9a8c\u65b9\u6cd5\u5177\u6709\u6311\u6218\u6027\uff0c\u5e76\u4e14\u5bf9\u4e8e\u5927\u89c4\u6a21\u7b5b\u9009\u6765\u8bf4\u4e0d\u5207\u5b9e\u9645\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u8ba1\u7b97\u65b9\u6cd5\u6765\u9884\u6d4bBBB\u6e17\u900f\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684GPS Transformer\u67b6\u6784\uff0c\u65e8\u5728\u5728\u4f4e\u6570\u636e\u60c5\u51b5\u4e0b\u8868\u73b0\u826f\u597d\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4f7f\u7528BBBP\u6570\u636e\u96c6\u7684BBB\u6e17\u900f\u6027\u9884\u6d4b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u7684ROC-AUC\u4e3a78.8%\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u6c34\u5e73\u63d0\u9ad8\u4e865.5%\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u6807\u51c6\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e0eGPS Transformer\u7ed3\u5408\u4f7f\u7528\u6bd4\u5176\u4ed6\u6ce8\u610f\u529b\u673a\u5236\u53d8\u4f53\u4e0eGPS Transformer\u7ed3\u5408\u4f7f\u7528\u8868\u73b0\u66f4\u597d\u3002||\n", "2409.04803": "|**2024-09-07**|[Cross-attention Inspired Selective State Space Models for Target Sound Extraction](http://arxiv.org/abs/2409.04803)|**[link](https://github.com/WuDH2000/CrossMamba)**|Transformer\u6a21\u578b\uff0c\u7279\u522b\u662f\u5176\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u76ee\u6807\u58f0\u97f3\u63d0\u53d6\u4e2d\u7684\u7279\u5f81\u878d\u5408\uff0c\u8be5\u4efb\u52a1\u57fa\u4e8e\u7ed9\u5b9a\u7684\u7ebf\u7d22\u63d0\u53d6\u611f\u5174\u8da3\u7684\u4fe1\u53f7\u3002\u5c3d\u7ba1\u6709\u6548\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\u8f83\u4f4e\u3002\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u6700\u8fd1\u7684Mamba\u6a21\u578b\uff0c\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e0e\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u7136\u800c\uff0c\u7531\u4e8eMamba\u65e0\u6cd5\u50cf\u4ea4\u53c9\u6ce8\u610f\u529b\u90a3\u6837\u6355\u6349\u4e0d\u540c\u5e8f\u5217\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u56e0\u6b64\u5b83\u5728\u76ee\u6807\u58f0\u97f3\u63d0\u53d6\u4e2d\u7684\u9002\u7528\u6027\u53d7\u5230\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7528\u4e8e\u76ee\u6807\u58f0\u97f3\u63d0\u53d6\u7684CrossMamba\u6a21\u578b\uff0c\u5b83\u5229\u7528Mamba\u7684\u9690\u85cf\u6ce8\u610f\u529b\u673a\u5236\u6765\u8ba1\u7b97\u7ed9\u5b9a\u7ebf\u7d22\u548c\u97f3\u9891\u6df7\u5408\u7269\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002Mamba\u7684\u8ba1\u7b97\u53ef\u4ee5\u5206\u4e3a\u67e5\u8be2\u3001\u952e\u548c\u503c\u3002\u6211\u4eec\u5229\u7528\u7ebf\u7d22\u751f\u6210\u67e5\u8be2\uff0c\u5e76\u5229\u7528\u97f3\u9891\u6df7\u5408\u7269\u5bfc\u51fa\u952e\u548c\u503c\uff0c\u9075\u5faaTransformer\u4e2d\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u7684\u539f\u7406\u3002\u6765\u81ea\u4e24\u79cd\u5177\u6709\u4ee3\u8868\u6027\u7684\u76ee\u6807\u58f0\u97f3\u63d0\u53d6\u65b9\u6cd5\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684CrossMamba\u7684\u6709\u6548\u6027\u3002||\n", "2409.04431": "|**2024-09-06**|[Theory, Analysis, and Best Practices for Sigmoid Self-Attention](http://arxiv.org/abs/2409.04431)|**[link](https://github.com/apple/ml-sigmoid-attention)**|\u6ce8\u610f\u529b\u662f Transformer \u67b6\u6784\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002\u5b83\u662f\u4e00\u79cd\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u6620\u5c04\uff0c\u5c06\u6bcf\u4e2a\u5e8f\u5217\u5143\u7d20\u8f6c\u6362\u4e3a\u503c\u7684\u52a0\u6743\u548c\u3002\u6743\u91cd\u901a\u5e38\u662f\u901a\u8fc7\u952e\u548c\u67e5\u8be2\u4e4b\u95f4\u7684\u70b9\u79ef\u7684 softmax \u83b7\u5f97\u7684\u3002\u6700\u8fd1\u7684\u5de5\u4f5c\u63a2\u7d22\u4e86 Transformer \u4e2d softmax \u6ce8\u610f\u529b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f8b\u5982 ReLU \u548c sigmoid \u6fc0\u6d3b\u51fd\u6570\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6 sigmoid \u6ce8\u610f\u529b\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u6df1\u5165\u7684\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u3002\u7406\u8bba\u4e0a\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5177\u6709 sigmoid \u6ce8\u610f\u529b\u7684 Transformer \u662f\u901a\u7528\u51fd\u6570\u903c\u8fd1\u5668\uff0c\u5e76\u4e14\u4e0e softmax \u6ce8\u610f\u529b\u76f8\u6bd4\uff0c\u5177\u6709\u66f4\u597d\u7684\u6b63\u5219\u6027\u3002\u901a\u8fc7\u8be6\u7ec6\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u5728\u8bad\u7ec3\u7684\u65e9\u671f\u9636\u6bb5\u7a33\u5b9a\u8f83\u5927\u7684\u521d\u59cb\u6ce8\u610f\u529b\u8303\u6570\u662f\u6210\u529f\u8bad\u7ec3\u5177\u6709 sigmoid \u6ce8\u610f\u529b\u6a21\u578b\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u5148\u524d\u7684\u5c1d\u8bd5\u3002\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86 FLASHSIGMOID\uff0c\u8fd9\u662f\u4e00\u79cd\u786c\u4ef6\u611f\u77e5\u4e14\u5185\u5b58\u9ad8\u6548\u7684 sigmoid \u6ce8\u610f\u529b\u5b9e\u73b0\uff0c\u5728 H100 GPU \u4e0a\uff0c\u5176\u63a8\u7406\u5185\u6838\u901f\u5ea6\u6bd4 FLASHATTENTION2 \u63d0\u9ad8\u4e86 17%\u3002\u8de8\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u8bed\u97f3\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ecf\u8fc7\u9002\u5f53\u6807\u51c6\u5316\u7684 sigmoid \u6ce8\u610f\u529b\u5728\u5e7f\u6cdb\u7684\u9886\u57df\u548c\u89c4\u6a21\u4e0a\u4e0e softmax \u6ce8\u610f\u529b\u7684\u5f3a\u5927\u6027\u80fd\u76f8\u5339\u914d\uff0c\u8fd9\u662f\u5148\u524d\u5c1d\u8bd5 sigmoid \u6ce8\u610f\u529b\u6240\u65e0\u6cd5\u5b8c\u5168\u5b9e\u73b0\u7684\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u7edf\u4e00\u4e86\u73b0\u6709\u6280\u672f\uff0c\u5e76\u4e3a sigmoid \u6ce8\u610f\u529b\u4f5c\u4e3a Transformer \u4e2d softmax \u7684\u76f4\u63a5\u66ff\u4ee3\u54c1\u5efa\u7acb\u4e86\u6700\u4f73\u5b9e\u8df5\u3002||\n", "2409.04275": "|**2024-09-09**|[AttentionX: Exploiting Consensus Discrepancy In Attention from A Distributed Optimization Perspective](http://arxiv.org/abs/2409.04275)|null|\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ece\u5206\u5e03\u5f0f\u4f18\u5316\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u5229\u7528\u5171\u8bc6\u5dee\u5f02\u6765\u6269\u5c55Transformer\u4e2d\u7684\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3aAttentionX\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4e58\u5b50\u4ea4\u66ff\u65b9\u5411\u6cd5\uff08PDMM\uff09\\cite{Zhang16PDMM}\u65e8\u5728\u8fed\u4ee3\u5730\u89e3\u51b3\u70b9\u5bf9\u70b9\uff08P2P\uff09\u7f51\u7edc\u4e0a\u7684\u4e00\u5927\u7c7b\u5206\u5e03\u5f0f\u4f18\u5316\u95ee\u9898\uff0c\u5176\u4e2d\u76f8\u90bb\u8282\u70b9\u6839\u636e\u4f18\u5316\u8fc7\u7a0b\u4e2d\u9884\u5b9a\u4e49\u7684\u7ebf\u6027\u8fb9\u7ea6\u675f\u9010\u6e10\u8fbe\u6210\u5171\u8bc6\u3002\u7279\u522b\u662f\u5728PDMM\u7684\u6bcf\u6b21\u8fed\u4ee3\u4e2d\uff0c\u7f51\u7edc\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u9996\u5148\u4ece\u90bb\u5c45\u8282\u70b9\u6536\u96c6\u4fe1\u606f\uff0c\u7136\u540e\u6267\u884c\u672c\u5730\u4fe1\u606f\u878d\u5408\u3002\u4ece\u9ad8\u5c42\u6b21\u6765\u770b\uff0c\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u57fa\u4e8e$KQ$-softmax\u7684$V$\u8868\u793a\u52a0\u6743\u6c42\u548c\u5bf9\u5e94\u4e8e\u4ece\u90bb\u5c45\u8282\u70b9\u6536\u96c6\u4fe1\u606f\uff0c\u800cTransformer\u4e2d\u901a\u8fc7\u524d\u9988\u7f51\u7edc\uff08FFN\uff09\u8fdb\u884c\u7684\u7279\u5f81\u5904\u7406\u5bf9\u5e94\u4e8e\u672c\u5730\u4fe1\u606f\u878d\u5408\u3002PDMM\u5229\u7528\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u4ee5\u7ebf\u6027\u8fb9\u7ea6\u675f\u7684\u6b8b\u5dee\u5f62\u5f0f\u6355\u83b7\u5386\u53f2\u5171\u8bc6\u5dee\u5f02\uff0c\u8fd9\u5bf9\u4e8e\u7b97\u6cd5\u7684\u6536\u655b\u81f3\u5173\u91cd\u8981\u3002\u53d7PDMM\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86AttentionX\uff0c\u5c06\u5171\u8bc6\u5dee\u5f02\u7eb3\u5165\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\u7684\u8f93\u51fa\u66f4\u65b0\u8868\u8fbe\u5f0f\u4e2d\u3002AttentionX\u4e2d\u7684\u5171\u8bc6\u5dee\u5f02\u662f\u6307$V$\u8868\u793a\u7684\u52a0\u6743\u6c42\u548c\u4e0e\u5176\u7f29\u653e\u540e\u7684$V$\u8868\u793a\u672c\u8eab\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u5728ViT\u548cnanoGPT\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u4e86\u5176\u826f\u597d\u7684\u6027\u80fd\u3002||\n", "2409.06603": "|**2024-09-10**|[A Practical Gated Recurrent Transformer Network Incorporating Multiple Fusions for Video Denoising](http://arxiv.org/abs/2409.06603)|null|State-of-the-art (SOTA) video denoising methods employ multi-frame simultaneous denoising mechanisms, resulting in significant delays (e.g., 16 frames), making them impractical for real-time cameras. To overcome this limitation, we propose a multi-fusion gated recurrent Transformer network (GRTN) that achieves SOTA denoising performance with only a single-frame delay. Specifically, the spatial denoising module extracts features from the current frame, while the reset gate selects relevant information from the previous frame and fuses it with current frame features via the temporal denoising module. The update gate then further blends this result with the previous frame features, and the reconstruction module integrates it with the current frame. To robustly compute attention for noisy features, we propose a residual simplified Swin Transformer with Euclidean distance (RSSTE) in the spatial and temporal denoising modules. Comparative objective and subjective results show that our GRTN achieves denoising performance comparable to SOTA multi-frame delay networks, with only a single-frame delay.||\n", "2409.06590": "|**2024-09-10**|[Lightweight Multiscale Feature Fusion Super-Resolution Network Based on Two-branch Convolution and Transformer](http://arxiv.org/abs/2409.06590)|null|The single image super-resolution(SISR) algorithms under deep learning currently have two main models, one based on convolutional neural networks and the other based on Transformer. The former uses the stacking of convolutional layers with different convolutional kernel sizes to design the model, which enables the model to better extract the local features of the image; the latter uses the self-attention mechanism to design the model, which allows the model to establish long-distance dependencies between image pixel points through the self-attention mechanism and then better extract the global features of the image. However, both of the above methods face their problems. Based on this, this paper proposes a new lightweight multi-scale feature fusion network model based on two-way complementary convolutional and Transformer, which integrates the respective features of Transformer and convolutional neural networks through a two-branch network architecture, to realize the mutual fusion of global and local information. Meanwhile, considering the partial loss of information caused by the low-pixel images trained by the deep neural network, this paper designs a modular connection method of multi-stage feature supplementation to fuse the feature maps extracted from the shallow stage of the model with those extracted from the deep stage of the model, to minimize the loss of the information in the feature images that is beneficial to the image restoration as much as possible, to facilitate the obtaining of a higher-quality restored image. The practical results finally show that the model proposed in this paper is optimal in image recovery performance when compared with other lightweight models with the same amount of parameters.||\n", "2409.06443": "|**2024-09-10**|[Knowledge Distillation via Query Selection for Detection Transformer](http://arxiv.org/abs/2409.06443)|null|Transformers have revolutionized the object detection landscape by introducing DETRs, acclaimed for their simplicity and efficacy. Despite their advantages, the substantial size of these models poses significant challenges for practical deployment, particularly in resource-constrained environments. This paper addresses the challenge of compressing DETR by leveraging knowledge distillation, a technique that holds promise for maintaining model performance while reducing size. A critical aspect of DETRs' performance is their reliance on queries to interpret object representations accurately. Traditional distillation methods often focus exclusively on positive queries, identified through bipartite matching, neglecting the rich information present in hard-negative queries. Our visual analysis indicates that hard-negative queries, focusing on foreground elements, are crucial for enhancing distillation outcomes. To this end, we introduce a novel Group Query Selection strategy, which diverges from traditional query selection in DETR distillation by segmenting queries based on their Generalized Intersection over Union (GIoU) with ground truth objects, thereby uncovering valuable hard-negative queries for distillation. Furthermore, we present the Knowledge Distillation via Query Selection for DETR (QSKD) framework, which incorporates Attention-Guided Feature Distillation (AGFD) and Local Alignment Prediction Distillation (LAPD). These components optimize the distillation process by focusing on the most informative aspects of the teacher model's intermediate features and output. Our comprehensive experimental evaluation of the MS-COCO dataset demonstrates the effectiveness of our approach, significantly improving average precision (AP) across various DETR architectures without incurring substantial computational costs. Specifically, the AP of Conditional DETR ResNet-18 increased from 35.8 to 39.9.||\n", "2409.06206": "|**2024-09-10**|[AgileIR: Memory-Efficient Group Shifted Windows Attention for Agile Image Restoration](http://arxiv.org/abs/2409.06206)|null|Image Transformers show a magnificent success in Image Restoration tasks. Nevertheless, most of transformer-based models are strictly bounded by exorbitant memory occupancy. Our goal is to reduce the memory consumption of Swin Transformer and at the same time speed up the model during training process. Thus, we introduce AgileIR, group shifted attention mechanism along with window attention, which sparsely simplifies the model in architecture. We propose Group Shifted Window Attention (GSWA) to decompose Shift Window Multi-head Self Attention (SW-MSA) and Window Multi-head Self Attention (W-MSA) into groups across their attention heads, contributing to shrinking memory usage in back propagation. In addition to that, we keep shifted window masking and its shifted learnable biases during training, in order to induce the model interacting across windows within the channel. We also re-allocate projection parameters to accelerate attention matrix calculation, which we found a negligible decrease in performance. As a result of experiment, compared with our baseline SwinIR and other efficient quantization models, AgileIR keeps the performance still at 32.20 dB on Set5 evaluation dataset, exceeding other methods with tailor-made efficient methods and saves over 50% memory while a large batch size is employed.||\n", "2409.08159": "|**2024-09-12**|[SDformer: Efficient End-to-End Transformer for Depth Completion](http://arxiv.org/abs/2409.08159)|**[link](https://github.com/jamesqian11/sdformer-for-depth-completion)**|Depth completion aims to predict dense depth maps with sparse depth measurements from a depth sensor. Currently, Convolutional Neural Network (CNN) based models are the most popular methods applied to depth completion tasks. However, despite the excellent high-end performance, they suffer from a limited representation area. To overcome the drawbacks of CNNs, a more effective and powerful method has been presented: the Transformer, which is an adaptive self-attention setting sequence-to-sequence model. While the standard Transformer quadratically increases the computational cost from the key-query dot-product of input resolution which improperly employs depth completion tasks. In this work, we propose a different window-based Transformer architecture for depth completion tasks named Sparse-to-Dense Transformer (SDformer). The network consists of an input module for the depth map and RGB image features extraction and concatenation, a U-shaped encoder-decoder Transformer for extracting deep features, and a refinement module. Specifically, we first concatenate the depth map features with the RGB image features through the input model. Then, instead of calculating self-attention with the whole feature maps, we apply different window sizes to extract the long-range depth dependencies. Finally, we refine the predicted features from the input module and the U-shaped encoder-decoder Transformer module to get the enriching depth features and employ a convolution layer to obtain the dense depth map. In practice, the SDformer obtains state-of-the-art results against the CNN-based depth completion models with lower computing loads and parameters on the NYU Depth V2 and KITTI DC datasets.||\n", "2409.07914": "|**2024-09-12**|[InterACT: Inter-dependency Aware Action Chunking with Hierarchical Attention Transformers for Bimanual Manipulation](http://arxiv.org/abs/2409.07914)|null|We present InterACT: Inter-dependency aware Action Chunking with Hierarchical Attention Transformers, a novel imitation learning framework for bimanual manipulation that integrates hierarchical attention to capture inter-dependencies between dual-arm joint states and visual inputs. InterACT consists of a Hierarchical Attention Encoder and a Multi-arm Decoder, both designed to enhance information aggregation and coordination. The encoder processes multi-modal inputs through segment-wise and cross-segment attention mechanisms, while the decoder leverages synchronization blocks to refine individual action predictions, providing the counterpart's prediction as context. Our experiments on a variety of simulated and real-world bimanual manipulation tasks demonstrate that InterACT significantly outperforms existing methods. Detailed ablation studies validate the contributions of key components of our work, including the impact of CLS tokens, cross-segment encoders, and synchronization blocks.||\n", "2409.07793": "|**2024-09-12**|[Lagrange Duality and Compound Multi-Attention Transformer for Semi-Supervised Medical Image Segmentation](http://arxiv.org/abs/2409.07793)|**[link](https://github.com/lzeeorno/lagrange-duality-and-cmaformer)**|Medical image segmentation, a critical application of semantic segmentation in healthcare, has seen significant advancements through specialized computer vision techniques. While deep learning-based medical image segmentation is essential for assisting in medical diagnosis, the lack of diverse training data causes the long-tail problem. Moreover, most previous hybrid CNN-ViT architectures have limited ability to combine various attentions in different layers of the Convolutional Neural Network. To address these issues, we propose a Lagrange Duality Consistency (LDC) Loss, integrated with Boundary-Aware Contrastive Loss, as the overall training objective for semi-supervised learning to mitigate the long-tail problem. Additionally, we introduce CMAformer, a novel network that synergizes the strengths of ResUNet and Transformer. The cross-attention block in CMAformer effectively integrates spatial attention and channel attention for multi-scale feature fusion. Overall, our results indicate that CMAformer, combined with the feature fusion framework and the new consistency loss, demonstrates strong complementarity in semi-supervised learning ensembles. We achieve state-of-the-art results on multiple public medical image datasets. Example code are available at: \\url{https://github.com/lzeeorno/Lagrange-Duality-and-CMAformer}.||\n", "2409.07541": "|**2024-09-11**|[ENACT: Entropy-based Clustering of Attention Input for Improving the Computational Performance of Object Detection Transformers](http://arxiv.org/abs/2409.07541)|**[link](https://github.com/gsavathrakis/enact)**|Transformers demonstrate competitive performance in terms of precision on the problem of vision-based object detection. However, they require considerable computational resources due to the quadratic size of the attention weights. In this work, we propose to cluster the transformer input on the basis of its entropy. The reason for this is that the self-information of each pixel (whose sum is the entropy), is likely to be similar among pixels corresponding to the same objects. Clustering reduces the size of data given as input to the transformer and therefore reduces training time and GPU memory usage, while at the same time preserves meaningful information to be passed through the remaining parts of the network. The proposed process is organized in a module called ENACT, that can be plugged-in any transformer architecture that consists of a multi-head self-attention computation in its encoder. We ran extensive experiments using the COCO object detection dataset, and three detection transformers. The obtained results demonstrate that in all tested cases, there is consistent reduction in the required computational resources, while the precision of the detection task is only slightly reduced. The code of the ENACT module will become available at https://github.com/GSavathrakis/ENACT||\n", "2409.07146": "|**2024-09-11**|[Gated Slot Attention for Efficient Linear-Time Sequence Modeling](http://arxiv.org/abs/2409.07146)|**[link](https://github.com/sustcsonglin/flash-linear-attention)**|Linear attention Transformers and their gated variants, celebrated for enabling parallel training and efficient recurrent inference, still fall short in recall-intensive tasks compared to traditional Transformers and demand significant resources for training from scratch. This paper introduces Gated Slot Attention (GSA), which enhances Attention with Bounded-memory-Control (ABC) by incorporating a gating mechanism inspired by Gated Linear Attention (GLA). Essentially, GSA comprises a two-layer GLA linked via softmax, utilizing context-aware memory reading and adaptive forgetting to improve memory capacity while maintaining compact recurrent state size. This design greatly enhances both training and inference efficiency through GLA's hardware-efficient training algorithm and reduced state size. Additionally, retaining the softmax operation is particularly beneficial in \"finetuning pretrained Transformers to RNNs\" (T2R) settings, reducing the need for extensive training from scratch. Extensive experiments confirm GSA's superior performance in scenarios requiring in-context recall and in T2R settings.||\n", "2409.06985": "|**2024-09-11**|[Enhancing Cross-domain Pre-Trained Decision Transformers with Adaptive Attention](http://arxiv.org/abs/2409.06985)|null|Recently, the pre-training of decision transformers (DT) using a different domain, such as natural language text, has generated significant attention in offline reinforcement learning (Offline RL). Although this cross-domain pre-training approach achieves superior performance compared to training from scratch in environments required short-term planning ability, the mechanisms by which pre-training benefits the fine-tuning phase remain unclear. Furthermore, we point out that the cross-domain pre-training approach hinders the extraction of distant information in environments like PointMaze that require long-term planning ability, leading to performance that is much worse than training DT from scratch. This work first analyzes these issues and found that Markov Matrix, a component that exists in pre-trained attention heads, is the key to explain the significant performance disparity of pre-trained models in different planning abilities. Inspired by our analysis, we propose a general method GPT-DTMA, which equips a pre-trained DT with Mixture of Attention (MoA), to enable adaptive learning and accommodating diverse attention requirements during fine-tuning. Extensive experiments demonstrate that the effectiveness of GPT-DTMA: it achieves superior performance in short-term environments compared to baselines, and in long-term environments, it mitigates the negative impact caused by Markov Matrix, achieving results comparable to those of DT trained from scratch.||\n", "2409.06963": "|**2024-09-11**|[Brain-Inspired Stepwise Patch Merging for Vision Transformers](http://arxiv.org/abs/2409.06963)|null|The hierarchical architecture has become a mainstream design paradigm for Vision Transformers (ViTs), with Patch Merging serving as the pivotal component that transforms a columnar architecture into a hierarchical one. Drawing inspiration from the brain's ability to integrate global and local information for comprehensive visual understanding, we propose a novel technique called Stepwise Patch Merging (SPM), which enhances the subsequent attention mechanism's ability to 'see' better. SPM comprises two critical modules: Multi-Scale Aggregation (MSA) and Guided Local Enhancement (GLE). The MSA module integrates multi-scale features to enrich feature representation, while the GLE module focuses on refining local detail extraction, thus achieving an optimal balance between long-range dependency modeling and local feature enhancement. Extensive experiments conducted on benchmark datasets, including ImageNet-1K, COCO, and ADE20K, demonstrate that SPM significantly improves the performance of various models, particularly in dense prediction tasks such as object detection and semantic segmentation. These results underscore the efficacy of SPM in enhancing model accuracy and robustness across a wide range of computer vision tasks.||\n", "2409.09007": "|**2024-09-13**|[SGFormer: Single-Layer Graph Transformers with Approximation-Free Linear Complexity](http://arxiv.org/abs/2409.09007)|**[link](https://github.com/qitianwu/sgformer)**|Learning representations on large graphs is a long-standing challenge due to the inter-dependence nature. Transformers recently have shown promising performance on small graphs thanks to its global attention for capturing all-pair interactions beyond observed structures. Existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated architectures by stacking deep attention-based propagation layers. In this paper, we attempt to evaluate the necessity of adopting multi-layer attentions in Transformers on graphs, which considerably restricts the efficiency. Specifically, we analyze a generic hybrid propagation layer, comprised of all-pair attention and graph-based propagation, and show that multi-layer propagation can be reduced to one-layer propagation, with the same capability for representation learning. It suggests a new technical path for building powerful and efficient Transformers on graphs, particularly through simplifying model architectures without sacrificing expressiveness. As exemplified by this work, we propose a Simplified Single-layer Graph Transformers (SGFormer), whose main component is a single-layer global attention that scales linearly w.r.t. graph sizes and requires none of any approximation for accommodating all-pair interactions. Empirically, SGFormer successfully scales to the web-scale graph ogbn-papers100M, yielding orders-of-magnitude inference acceleration over peer Transformers on medium-sized graphs, and demonstrates competitiveness with limited labeled data.||\n", "2409.08769": "|**2024-09-13**|[Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry](http://arxiv.org/abs/2409.08769)|**[link](https://github.com/ybkurt/vift)**|In recent years, transformer-based architectures become the de facto standard for sequence modeling in deep learning frameworks. Inspired by the successful examples, we propose a causal visual-inertial fusion transformer (VIFT) for pose estimation in deep visual-inertial odometry. This study aims to improve pose estimation accuracy by leveraging the attention mechanisms in transformers, which better utilize historical data compared to the recurrent neural network (RNN) based methods seen in recent methods. Transformers typically require large-scale data for training. To address this issue, we utilize inductive biases for deep VIO networks. Since latent visual-inertial feature vectors encompass essential information for pose estimation, we employ transformers to refine pose estimates by updating latent vectors temporally. Our study also examines the impact of data imbalance and rotation learning methods in supervised end-to-end learning of visual inertial odometry by utilizing specialized gradients in backpropagation for the elements of SE$(3)$ group. The proposed method is end-to-end trainable and requires only a monocular camera and IMU during inference. Experimental results demonstrate that VIFT increases the accuracy of monocular VIO networks, achieving state-of-the-art results when compared to previous methods on the KITTI dataset. The code will be made available at https://github.com/ybkurt/VIFT.||\n", "2409.08652": "|**2024-09-13**|[SkinFormer: Learning Statistical Texture Representation with Transformer for Skin Lesion Segmentation](http://arxiv.org/abs/2409.08652)|**[link](https://github.com/rongtao-xu/skinformer)**|Accurate skin lesion segmentation from dermoscopic images is of great importance for skin cancer diagnosis. However, automatic segmentation of melanoma remains a challenging task because it is difficult to incorporate useful texture representations into the learning process. Texture representations are not only related to the local structural information learned by CNN, but also include the global statistical texture information of the input image. In this paper, we propose a trans\\textbf{Former} network (\\textbf{SkinFormer}) that efficiently extracts and fuses statistical texture representation for \\textbf{Skin} lesion segmentation. Specifically, to quantify the statistical texture of input features, a Kurtosis-guided Statistical Counting Operator is designed. We propose Statistical Texture Fusion Transformer and Statistical Texture Enhance Transformer with the help of Kurtosis-guided Statistical Counting Operator by utilizing the transformer's global attention mechanism. The former fuses structural texture information and statistical texture information, and the latter enhances the statistical texture of multi-scale features. {Extensive experiments on three publicly available skin lesion datasets validate that our SkinFormer outperforms other SOAT methods, and our method achieves 93.2\\% Dice score on ISIC 2018. It can be easy to extend SkinFormer to segment 3D images in the future.} Our code is available at https://github.com/Rongtao-Xu/SkinFormer.||\n", "2409.08461": "|**2024-09-13**|[VistaFormer: Scalable Vision Transformers for Satellite Image Time Series Segmentation](http://arxiv.org/abs/2409.08461)|**[link](https://github.com/macdonaldezra/VistaFormer)**|We introduce VistaFormer, a lightweight Transformer-based model architecture for the semantic segmentation of remote-sensing images. This model uses a multi-scale Transformer-based encoder with a lightweight decoder that aggregates global and local attention captured in the encoder blocks. VistaFormer uses position-free self-attention layers which simplifies the model architecture and removes the need to interpolate temporal and spatial codes, which can reduce model performance when training and testing image resolutions differ. We investigate simple techniques for filtering noisy input signals like clouds and demonstrate that improved model scalability can be achieved by substituting Multi-Head Self-Attention (MHSA) with Neighbourhood Attention (NA). Experiments on the PASTIS and MTLCC crop-type segmentation benchmarks show that VistaFormer achieves better performance than comparable models and requires only 8% of the floating point operations using MHSA and 11% using NA while also using fewer trainable parameters. VistaFormer with MHSA improves on state-of-the-art mIoU scores by 0.1% on the PASTIS benchmark and 3% on the MTLCC benchmark while VistaFormer with NA improves on the MTLCC benchmark by 3.7%.||\n", "2409.11320": "|**2024-09-17**|[A short trajectory is all you need: A transformer-based model for long-time dissipative quantum dynamics](http://arxiv.org/abs/2409.11320)|**[link](https://github.com/kananenka-group/Transformer-spin-boson)**|In this communication we demonstrate that a deep artificial neural network based on a transformer architecture with self-attention layers can predict the long-time population dynamics of a quantum system coupled to a dissipative environment provided that the short-time population dynamics of the system is known. The transformer neural network model developed in this work predicts the long-time dynamics of spin-boson model efficiently and very accurately across different regimes, from weak system-bath coupling to strong coupling non-Markovian regimes. Our model is more accurate than classical forecasting models, such as recurrent neural networks and is comparable to the state-of-the-art models for simulating the dynamics of quantum dissipative systems, based on kernel ridge regression.||\n", "2409.11250": "|**2024-09-17**|[Linear Recency Bias During Training Improves Transformers' Fit to Reading Times](http://arxiv.org/abs/2409.11250)|null|Recent psycholinguistic research has compared human reading times to surprisal estimates from language models to study the factors shaping human sentence processing difficulty. Previous studies have shown a strong fit between surprisal values from Transformers and reading times. However, standard Transformers work with a lossless representation of the entire previous linguistic context, unlike models of human language processing that include memory decay. To bridge this gap, this paper evaluates a modification of the Transformer model that uses ALiBi (Press et al., 2022), a recency bias added to attention scores. Surprisal estimates with ALiBi show an improved fit to human reading times compared to a standard Transformer baseline. A subsequent analysis of attention heads suggests that ALiBi's mixture of slopes -- which determine the rate of memory decay in each attention head -- may play a role in the improvement by helping models with ALiBi to track different kinds of linguistic dependencies.||\n", "2409.10944": "|**2024-09-17**|[Contrasformer: A Brain Network Contrastive Transformer for Neurodegenerative Condition Identification](http://arxiv.org/abs/2409.10944)|**[link](https://github.com/angusmonroe/contrasformer)**|Understanding neurological disorder is a fundamental problem in neuroscience, which often requires the analysis of brain networks derived from functional magnetic resonance imaging (fMRI) data. Despite the prevalence of Graph Neural Networks (GNNs) and Graph Transformers in various domains, applying them to brain networks faces challenges. Specifically, the datasets are severely impacted by the noises caused by distribution shifts across sub-populations and the neglect of node identities, both obstruct the identification of disease-specific patterns. To tackle these challenges, we propose Contrasformer, a novel contrastive brain network Transformer. It generates a prior-knowledge-enhanced contrast graph to address the distribution shifts across sub-populations by a two-stream attention mechanism. A cross attention with identity embedding highlights the identity of nodes, and three auxiliary losses ensure group consistency. Evaluated on 4 functional brain network datasets over 4 different diseases, Contrasformer outperforms the state-of-the-art methods for brain networks by achieving up to 10.8\\% improvement in accuracy, which demonstrates its efficacy in neurological disorder identification. Case studies illustrate its interpretability, especially in the context of neuroscience. This paper provides a solution for analyzing brain networks, offering valuable insights into neurological disorders. Our code is available at \\url{https://github.com/AngusMonroe/Contrasformer}.||\n", "2409.10870": "|**2024-09-17**|[Adaptive Large Language Models By Layerwise Attention Shortcuts](http://arxiv.org/abs/2409.10870)|null|Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \\textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.||\n", "2409.10792": "|**2024-09-16**|[Recurrent Graph Transformer Network for Multiple Fault Localization in Naval Shipboard Systems](http://arxiv.org/abs/2409.10792)|null|The integration of power electronics building blocks in modern MVDC 12kV Naval ship systems enhances energy management and functionality but also introduces complex fault detection and control challenges. These challenges strain traditional fault diagnostic methods, making it difficult to detect and manage faults across multiple locations while maintaining system stability and performance. This paper proposes a temporal recurrent graph transformer network for fault diagnosis in naval MVDC 12kV shipboard systems. The deep graph neural network uses gated recurrent units to capture temporal features and a multi-head attention mechanism to extract spatial features, enhancing diagnostic accuracy. The approach effectively identifies and evaluates successive multiple faults with high precision. The method is implemented and validated on the MVDC 12kV shipboard system designed by the ESDRC team, incorporating all key components. Results show significant improvements in fault localization accuracy, with a 1-4% increase in performance metrics compared to other machine learning methods.||\n", "2409.10715": "|**2024-09-16**|[Self-Attention Limits Working Memory Capacity of Transformer-Based Models](http://arxiv.org/abs/2409.10715)|null|Recent work on Transformer-based large language models (LLMs) has revealed striking limits in their working memory capacity, similar to what has been found in human behavioral studies. Specifically, these models' performance drops significantly on N-back tasks as N increases. However, there is still a lack of mechanistic interpretability as to why this phenomenon would arise. Inspired by the executive attention theory from behavioral sciences, we hypothesize that the self-attention mechanism within Transformer-based models might be responsible for their working memory capacity limits. To test this hypothesis, we train vanilla decoder-only transformers to perform N-back tasks and find that attention scores gradually aggregate to the N-back positions over training, suggesting that the model masters the task by learning a strategy to pay attention to the relationship between the current position and the N-back position. Critically, we find that the total entropy of the attention score matrix increases as N increases, suggesting that the dispersion of attention scores might be the cause of the capacity limit observed in N-back tasks.||\n", "2409.10653": "|**2024-09-16**|[Logic Synthesis Optimization with Predictive Self-Supervision via Causal Transformers](http://arxiv.org/abs/2409.10653)|null|Contemporary hardware design benefits from the abstraction provided by high-level logic gates, streamlining the implementation of logic circuits. Logic Synthesis Optimization (LSO) operates at one level of abstraction within the Electronic Design Automation (EDA) workflow, targeting improvements in logic circuits with respect to performance metrics such as size and speed in the final layout. Recent trends in the field show a growing interest in leveraging Machine Learning (ML) for EDA, notably through ML-guided logic synthesis utilizing policy-based Reinforcement Learning (RL) methods.Despite these advancements, existing models face challenges such as overfitting and limited generalization, attributed to constrained public circuits and the expressiveness limitations of graph encoders. To address these hurdles, and tackle data scarcity issues, we introduce LSOformer, a novel approach harnessing Autoregressive transformer models and predictive SSL to predict the trajectory of Quality of Results (QoR). LSOformer integrates cross-attention modules to merge insights from circuit graphs and optimization sequences, thereby enhancing prediction accuracy for QoR metrics. Experimental studies validate the effectiveness of LSOformer, showcasing its superior performance over baseline architectures in QoR prediction tasks, where it achieves improvements of 5.74%, 4.35%, and 17.06% on the EPFL, OABCD, and proprietary circuits datasets, respectively, in inductive setup.||\n", "2409.10206": "|**2024-09-16**|[Garment Attribute Manipulation with Multi-level Attention](http://arxiv.org/abs/2409.10206)|null|In the rapidly evolving field of online fashion shopping, the need for more personalized and interactive image retrieval systems has become paramount. Existing methods often struggle with precisely manipulating specific garment attributes without inadvertently affecting others. To address this challenge, we propose GAMMA (Garment Attribute Manipulation with Multi-level Attention), a novel framework that integrates attribute-disentangled representations with a multi-stage attention-based architecture. GAMMA enables targeted manipulation of fashion image attributes, allowing users to refine their searches with high accuracy. By leveraging a dual-encoder Transformer and memory block, our model achieves state-of-the-art performance on popular datasets like Shopping100k and DeepFashion.||\n", "2409.09513": "|**2024-09-14**|[Planning Transformer: Long-Horizon Offline Reinforcement Learning with Planning Tokens](http://arxiv.org/abs/2409.09513)|null|Supervised learning approaches to offline reinforcement learning, particularly those utilizing the Decision Transformer, have shown effectiveness in continuous environments and for sparse rewards. However, they often struggle with long-horizon tasks due to the high compounding error of auto-regressive models. To overcome this limitation, we go beyond next-token prediction and introduce Planning Tokens, which contain high-level, long time-scale information about the agent's future. Predicting dual time-scale tokens at regular intervals enables our model to use these long-horizon Planning Tokens as a form of implicit planning to guide its low-level policy and reduce compounding error. This architectural modification significantly enhances performance on long-horizon tasks, establishing a new state-of-the-art in complex D4RL environments. Additionally, we demonstrate that Planning Tokens improve the interpretability of the model's policy through the interpretable plan visualisations and attention map.||\n", "2409.09266": "|**2024-09-14**|[TransformerMPC: Accelerating Model Predictive Control via Transformers](http://arxiv.org/abs/2409.09266)|null|In this paper, we address the problem of reducing the computational burden of Model Predictive Control (MPC) for real-time robotic applications. We propose TransformerMPC, a method that enhances the computational efficiency of MPC algorithms by leveraging the attention mechanism in transformers for both online constraint removal and better warm start initialization. Specifically, TransformerMPC accelerates the computation of optimal control inputs by selecting only the active constraints to be included in the MPC problem, while simultaneously providing a warm start to the optimization process. This approach ensures that the original constraints are satisfied at optimality. TransformerMPC is designed to be seamlessly integrated with any MPC solver, irrespective of its implementation. To guarantee constraint satisfaction after removing inactive constraints, we perform an offline verification to ensure that the optimal control inputs generated by the MPC solver meet all constraints. The effectiveness of TransformerMPC is demonstrated through extensive numerical simulations on complex robotic systems, achieving up to 35x improvement in runtime without any loss in performance.||\n", "2409.12026": "|**2024-09-18**|[On Vision Transformers for Classification Tasks in Side-Scan Sonar Imagery](http://arxiv.org/abs/2409.12026)|null|Side-scan sonar (SSS) imagery presents unique challenges in the classification of man-made objects on the seafloor due to the complex and varied underwater environments. Historically, experts have manually interpreted SSS images, relying on conventional machine learning techniques with hand-crafted features. While Convolutional Neural Networks (CNNs) significantly advanced automated classification in this domain, they often fall short when dealing with diverse seafloor textures, such as rocky or ripple sand bottoms, where false positive rates may increase. Recently, Vision Transformers (ViTs) have shown potential in addressing these limitations by utilizing a self-attention mechanism to capture global information in image patches, offering more flexibility in processing spatial hierarchies. This paper rigorously compares the performance of ViT models alongside commonly used CNN architectures, such as ResNet and ConvNext, for binary classification tasks in SSS imagery. The dataset encompasses diverse geographical seafloor types and is balanced between the presence and absence of man-made objects. ViT-based models exhibit superior classification performance across f1-score, precision, recall, and accuracy metrics, although at the cost of greater computational resources. CNNs, with their inductive biases, demonstrate better computational efficiency, making them suitable for deployment in resource-constrained environments like underwater vehicles. Future research directions include exploring self-supervised learning for ViTs and multi-modal fusion to further enhance performance in challenging underwater environments.||\n", "2409.16280": "|**2024-09-24**|[MonoFormer: One Transformer for Both Diffusion and Autoregression](http://arxiv.org/abs/2409.16280)|**[link](https://github.com/MonoFormer/MonoFormer)**|Most existing multimodality methods use separate backbones for autoregression-based discrete text generation and diffusion-based continuous visual generation, or the same backbone by discretizing the visual data to use autoregression for both text and visual generation. In this paper, we propose to study a simple idea: share one transformer for both autoregression and diffusion. The feasibility comes from two main aspects: (i) Transformer is successfully applied to diffusion for visual generation, and (ii) transformer training for autoregression and diffusion is very similar, and the difference merely lies in that diffusion uses bidirectional attention mask and autoregression uses causal attention mask. Experimental results show that our approach achieves comparable image generation performance to current state-of-the-art methods as well as maintains the text generation capability. The project is publicly available at https://monoformer.github.io/.||\n", "2409.16214": "|**2024-09-24**|[TE-PINN: Quaternion-Based Orientation Estimation using Transformer-Enhanced Physics-Informed Neural Networks](http://arxiv.org/abs/2409.16214)|null|This paper introduces a Transformer-Enhanced Physics-Informed Neural Network (TE-PINN) designed for accurate quaternion-based orientation estimation in high-dynamic environments, particularly within the field of robotics. By integrating transformer networks with physics-informed learning, our approach innovatively captures temporal dependencies in sensor data while enforcing the fundamental physical laws governing rotational motion. TE-PINN leverages a multi-head attention mechanism to handle sequential data from inertial sensors, such as accelerometers and gyroscopes, ensuring temporal consistency. Simultaneously, the model embeds quaternion kinematics and rigid body dynamics into the learning process, aligning the network's predictions with mechanical principles like Euler's laws of motion. The physics-informed loss function incorporates the dynamics of angular velocity and external forces, enhancing the network's ability to generalize in complex scenarios. Our experimental evaluation demonstrates that TE-PINN consistently outperforms traditional methods such as Extended Kalman Filters (EKF) and LSTM-based estimators, particularly in scenarios characterized by high angular velocities and noisy sensor data. The results show a significant reduction in mean quaternion error and improved gyroscope bias estimation compared to the state-of-the-art. An ablation study further isolates the contributions of both the transformer architecture and the physics-informed constraints, highlighting the synergistic effect of both components in improving model performance. The proposed model achieves real-time performance on embedded systems typical of mobile robots, offering a scalable and efficient solution for orientation estimation in autonomous systems.||\n", "2409.16112": "|**2024-09-24**|[Self-attention as an attractor network: transient memories without backpropagation](http://arxiv.org/abs/2409.16112)|**[link](https://github.com/francill99/self_attention_attractor_network)**|Transformers are one of the most successful architectures of modern neural networks. At their core there is the so-called attention mechanism, which recently interested the physics community as it can be written as the derivative of an energy function in certain cases: while it is possible to write the cross-attention layer as a modern Hopfield network, the same is not possible for the self-attention, which is used in the GPT architectures and other autoregressive models. In this work we show that it is possible to obtain the self-attention layer as the derivative of local energy terms, which resemble a pseudo-likelihood. We leverage the analogy with pseudo-likelihood to design a recurrent model that can be trained without backpropagation: the dynamics shows transient states that are strongly correlated with both train and test examples. Overall we present a novel framework to interpret self-attention as an attractor network, potentially paving the way for new theoretical approaches inspired from physics to understand transformers.||\n", "2409.15869": "|**2024-09-24**|[Whisper in Medusa's Ear: Multi-head Efficient Decoding for Transformer-based ASR](http://arxiv.org/abs/2409.15869)|**[link](https://github.com/aiola-lab/whisper-medusa)**|Large transformer-based models have significant potential for speech transcription and translation. Their self-attention mechanisms and parallel processing enable them to capture complex patterns and dependencies in audio sequences. However, this potential comes with challenges, as these large and computationally intensive models lead to slow inference speeds. Various optimization strategies have been proposed to improve performance, including efficient hardware utilization and algorithmic enhancements. In this paper, we introduce Whisper-Medusa, a novel approach designed to enhance processing speed with minimal impact on Word Error Rate (WER). The proposed model extends the OpenAI's Whisper architecture by predicting multiple tokens per iteration, resulting in a 50% reduction in latency. We showcase the effectiveness of Whisper-Medusa across different learning setups and datasets.||\n", "2409.15553": "|**2024-09-23**|[SOFI: Multi-Scale Deformable Transformer for Camera Calibration with Enhanced Line Queries](http://arxiv.org/abs/2409.15553)|**[link](https://github.com/sebastianjanampa/sofi)**|Camera calibration consists of estimating camera parameters such as the zenith vanishing point and horizon line. Estimating the camera parameters allows other tasks like 3D rendering, artificial reality effects, and object insertion in an image. Transformer-based models have provided promising results; however, they lack cross-scale interaction. In this work, we introduce \\textit{multi-Scale defOrmable transFormer for camera calibratIon with enhanced line queries}, SOFI. SOFI improves the line queries used in CTRL-C and MSCC by using both line content and line geometric features. Moreover, SOFI's line queries allow transformer models to adopt the multi-scale deformable attention mechanism to promote cross-scale interaction between the feature maps produced by the backbone. SOFI outperforms existing methods on the \\textit {Google Street View}, \\textit {Horizon Line in the Wild}, and \\textit {Holicity} datasets while keeping a competitive inference speed.||\n", "2409.15117": "|**2024-09-23**|[Diffusion-based RGB-D Semantic Segmentation with Deformable Attention Transformer](http://arxiv.org/abs/2409.15117)|null|Vision-based perception and reasoning is essential for scene understanding in any autonomous system. RGB and depth images are commonly used to capture both the semantic and geometric features of the environment. Developing methods to reliably interpret this data is critical for real-world applications, where noisy measurements are often unavoidable. In this work, we introduce a diffusion-based framework to address the RGB-D semantic segmentation problem. Additionally, we demonstrate that utilizing a Deformable Attention Transformer as the encoder to extract features from depth images effectively captures the characteristics of invalid regions in depth measurements. Our generative framework shows a greater capacity to model the underlying distribution of RGB-D images, achieving robust performance in challenging scenarios with significantly less training time compared to discriminative methods. Experimental results indicate that our approach achieves State-of-the-Art performance on both the NYUv2 and SUN-RGBD datasets in general and especially in the most challenging of their image data. Our project page will be available at https://diffusionmms.github.io/||\n", "2409.15097": "|**2024-09-24**|[Efficiently Dispatching Flash Attention For Partially Filled Attention Masks](http://arxiv.org/abs/2409.15097)|null|Transformers are widely used across various applications, many of which yield sparse or partially filled attention matrices. Examples include attention masks designed to reduce the quadratic complexity of attention, sequence packing techniques, and recent innovations like tree masking for fast validation in MEDUSA. Despite the inherent sparsity in these matrices, the state-of-the-art algorithm Flash Attention still processes them with quadratic complexity as though they were dense. In this paper, we introduce Binary Block Masking, a highly efficient modification that enhances Flash Attention by making it mask-aware. We further propose two optimizations: one tailored for masks with contiguous non-zero patterns and another for extremely sparse masks. Our experiments on attention masks derived from real-world scenarios demonstrate up to a 9x runtime improvement. The implementation will be publicly released to foster further research and application.||\n", "2409.14906": "|**2024-09-23**|[Kriformer: A Novel Spatiotemporal Kriging Approach Based on Graph Transformers](http://arxiv.org/abs/2409.14906)|null|Accurately estimating data in sensor-less areas is crucial for understanding system dynamics, such as traffic state estimation and environmental monitoring. This study addresses challenges posed by sparse sensor deployment and unreliable data by framing the problem as a spatiotemporal kriging task and proposing a novel graph transformer model, Kriformer. This model estimates data at locations without sensors by mining spatial and temporal correlations, even with limited resources. Kriformer utilizes transformer architecture to enhance the model's perceptual range and solve edge information aggregation challenges, capturing spatiotemporal information effectively. A carefully constructed positional encoding module embeds the spatiotemporal features of nodes, while a sophisticated spatiotemporal attention mechanism enhances estimation accuracy. The multi-head spatial interaction attention module captures subtle spatial relationships between observed and unobserved locations. During training, a random masking strategy prompts the model to learn with partial information loss, allowing the spatiotemporal embedding and multi-head attention mechanisms to synergistically capture correlations among locations. Experimental results show that Kriformer excels in representation learning for unobserved locations, validated on two real-world traffic speed datasets, demonstrating its effectiveness in spatiotemporal kriging tasks.||\n", "2409.14846": "|**2024-09-23**|[A-VL: Adaptive Attention for Large Vision-Language Models](http://arxiv.org/abs/2409.14846)|null|The Large Vision-Language Model (LVLM) integrates computer vision and natural language processing techniques, offering substantial application potential. However, these models demand extensive resources during inference. Adaptive attention techniques can dynamically reduce computational redundancy and thus improve efficiency. Although current adaptive attention methods significantly reduce the memory requirements of Transformer-based language models, they are not tailored for LVLMs. We observe that LVLMs generate responses from both remote image tokens and local text tokens, and different modalities have different attention patterns. This observation inspires us to manage the attention for each modality separately. Specifically, for visual input, we store the cache of potentially useful information but only compute the most critical parts. For language input, we care more about local information. Based on our observation and analysis of vision-language attention patterns, we develop A-VL, a plug-and-play adaptive attention tailored for LVLM inference. Extensive evaluations on three vision-language tasks and five datasets show the effectiveness of our designs. Our approach A-VL outperforms existing adaptive attention methods in reducing memory usage and computational load without compromising performance.||\n", "2409.14829": "|**2024-09-23**|[RoWSFormer: A Robust Watermarking Framework with Swin Transformer for Enhanced Geometric Attack Resilience](http://arxiv.org/abs/2409.14829)|null|In recent years, digital watermarking techniques based on deep learning have been widely studied. To achieve both imperceptibility and robustness of image watermarks, most current methods employ convolutional neural networks to build robust watermarking frameworks. However, despite the success of CNN-based watermarking models, they struggle to achieve robustness against geometric attacks due to the limitations of convolutional neural networks in capturing global and long-range relationships. To address this limitation, we propose a robust watermarking framework based on the Swin Transformer, named RoWSFormer. Specifically, we design the Locally-Channel Enhanced Swin Transformer Block as the core of both the encoder and decoder. This block utilizes the self-attention mechanism to capture global and long-range information, thereby significantly improving adaptation to geometric distortions. Additionally, we construct the Frequency-Enhanced Transformer Block to extract frequency domain information, which further strengthens the robustness of the watermarking framework. Experimental results demonstrate that our RoWSFormer surpasses existing state-of-the-art watermarking methods. For most non-geometric attacks, RoWSFormer improves the PSNR by 3 dB while maintaining the same extraction accuracy. In the case of geometric attacks (such as rotation, scaling, and affine transformations), RoWSFormer achieves over a 6 dB improvement in PSNR, with extraction accuracy exceeding 97\\%.||\n", "2409.17986": "|**2024-09-26**|[Supra-Laplacian Encoding for Transformer on Dynamic Graphs](http://arxiv.org/abs/2409.17986)|null|Fully connected Graph Transformers (GT) have rapidly become prominent in the static graph community as an alternative to Message-Passing models, which suffer from a lack of expressivity, oversquashing, and under-reaching. However, in a dynamic context, by interconnecting all nodes at multiple snapshots with self-attention, GT loose both structural and temporal information. In this work, we introduce Supra-LAplacian encoding for spatio-temporal TransformErs (SLATE), a new spatio-temporal encoding to leverage the GT architecture while keeping spatio-temporal information. Specifically, we transform Discrete Time Dynamic Graphs into multi-layer graphs and take advantage of the spectral properties of their associated supra-Laplacian matrix. Our second contribution explicitly model nodes' pairwise relationships with a cross-attention mechanism, providing an accurate edge representation for dynamic link prediction. SLATE outperforms numerous state-of-the-art methods based on Message-Passing Graph Neural Networks combined with recurrent models (e.g LSTM), and Dynamic Graph Transformers, on 9 datasets. Code and instructions to reproduce our results will be open-sourced.||\n", "2409.17895": "|**2024-09-26**|[Self-supervised Monocular Depth Estimation with Large Kernel Attention](http://arxiv.org/abs/2409.17895)|null|Self-supervised monocular depth estimation has emerged as a promising approach since it does not rely on labeled training data. Most methods combine convolution and Transformer to model long-distance dependencies to estimate depth accurately. However, Transformer treats 2D image features as 1D sequences, and positional encoding somewhat mitigates the loss of spatial information between different feature blocks, tending to overlook channel features, which limit the performance of depth estimation. In this paper, we propose a self-supervised monocular depth estimation network to get finer details. Specifically, we propose a decoder based on large kernel attention, which can model long-distance dependencies without compromising the two-dimension structure of features while maintaining feature channel adaptivity. In addition, we introduce a up-sampling module to accurately recover the fine details in the depth map. Our method achieves competitive results on the KITTI dataset.||\n", "2409.17790": "|**2024-09-26**|[CASPFormer: Trajectory Prediction from BEV Images with Deformable Attention](http://arxiv.org/abs/2409.17790)|null|Motion prediction is an important aspect for Autonomous Driving (AD) and Advance Driver Assistance Systems (ADAS). Current state-of-the-art motion prediction methods rely on High Definition (HD) maps for capturing the surrounding context of the ego vehicle. Such systems lack scalability in real-world deployment as HD maps are expensive to produce and update in real-time. To overcome this issue, we propose Context Aware Scene Prediction Transformer (CASPFormer), which can perform multi-modal motion prediction from rasterized Bird-Eye-View (BEV) images. Our system can be integrated with any upstream perception module that is capable of generating BEV images. Moreover, CASPFormer directly decodes vectorized trajectories without any postprocessing. Trajectories are decoded recurrently using deformable attention, as it is computationally efficient and provides the network with the ability to focus its attention on the important spatial locations of the BEV images. In addition, we also address the issue of mode collapse for generating multiple scene-consistent trajectories by incorporating learnable mode queries. We evaluate our model on the nuScenes dataset and show that it reaches state-of-the-art across multiple metrics||\n", "2409.17746": "|**2024-09-26**|[Paraformer-v2: An improved non-autoregressive transformer for noise-robust speech recognition](http://arxiv.org/abs/2409.17746)|null|Attention-based encoder-decoder, e.g. transformer and its variants, generates the output sequence in an autoregressive (AR) manner. Despite its superior performance, AR model is computationally inefficient as its generation requires as many iterations as the output length. In this paper, we propose Paraformer-v2, an improved version of Paraformer, for fast, accurate, and noise-robust non-autoregressive speech recognition. In Paraformer-v2, we use a CTC module to extract the token embeddings, as the alternative to the continuous integrate-and-fire module in Paraformer. Extensive experiments demonstrate that Paraformer-v2 outperforms Paraformer on multiple datasets, especially on the English datasets (over 14% improvement on WER), and is more robust in noisy environments.||\n", "2409.17677": "|**2024-09-26**|[Optimal Memorization Capacity of Transformers](http://arxiv.org/abs/2409.17677)|null|Recent research in the field of machine learning has increasingly focused on the memorization capacity of Transformers, but how efficient they are is not yet well understood. We demonstrate that Transformers can memorize labels with $\\tilde{O}(\\sqrt{N})$ parameters in a next-token prediction setting for $N$ input sequences of length $n$, which is proved to be optimal up to logarithmic factors. This indicates that Transformers can efficiently perform memorization with little influence from the input length $n$ owing to the benefit of parameter sharing. We also analyze the memorization capacity in the sequence-to-sequence setting, and find that $\\tilde{O}(\\sqrt{nN})$ parameters are not only sufficient, but also necessary at least for Transformers with hardmax. These results suggest that while self-attention mechanisms can efficiently identify input sequences, the feed-forward network becomes a bottleneck when associating a label to each token.||\n", "2409.17625": "|**2024-09-26**|[Benign or Not-Benign Overfitting in Token Selection of Attention Mechanism](http://arxiv.org/abs/2409.17625)|null|Modern over-parameterized neural networks can be trained to fit the training data perfectly while still maintaining a high generalization performance. This \"benign overfitting\" phenomenon has been studied in a surge of recent theoretical work; however, most of these studies have been limited to linear models or two-layer neural networks. In this work, we analyze benign overfitting in the token selection mechanism of the attention architecture, which characterizes the success of transformer models. We first show the existence of a benign overfitting solution and explain its mechanism in the attention architecture. Next, we discuss whether the model converges to such a solution, raising the difficulties specific to the attention architecture. We then present benign overfitting cases and not-benign overfitting cases by conditioning different scenarios based on the behavior of attention probabilities during training. To the best of our knowledge, this is the first study to characterize benign overfitting for the attention mechanism.||\n", "2409.17560": "|**2024-09-26**|[Dynamic Subframe Splitting and Spatio-Temporal Motion Entangled Sparse Attention for RGB-E Tracking](http://arxiv.org/abs/2409.17560)|null|Event-based bionic camera asynchronously captures dynamic scenes with high temporal resolution and high dynamic range, offering potential for the integration of events and RGB under conditions of illumination degradation and fast motion. Existing RGB-E tracking methods model event characteristics utilising attention mechanism of Transformer before integrating both modalities. Nevertheless, these methods involve aggregating the event stream into a single event frame, lacking the utilisation of the temporal information inherent in the event stream.Moreover, the traditional attention mechanism is well-suited for dense semantic features, while the attention mechanism for sparse event features require revolution. In this paper, we propose a dynamic event subframe splitting strategy to split the event stream into more fine-grained event clusters, aiming to capture spatio-temporal features that contain motion cues. Based on this, we design an event-based sparse attention mechanism to enhance the interaction of event features in temporal and spatial dimensions. The experimental results indicate that our method outperforms existing state-of-the-art methods on the FE240 and COESOT datasets, providing an effective processing manner for the event data.||\n", "2409.17546": "|**2024-09-26**|[MASSFormer: Mobility-Aware Spectrum Sensing using Transformer-Driven Tiered Structure](http://arxiv.org/abs/2409.17546)|null|In this paper, we develop a novel mobility-aware transformer-driven tiered structure (MASSFormer) based cooperative spectrum sensing method that effectively models the spatio-temporal dynamics of user movements. Unlike existing methods, our method considers a dynamic scenario involving mobile primary users (PUs) and secondary users (SUs)and addresses the complexities introduced by user mobility. The transformer architecture utilizes an attention mechanism, enabling the proposed method to adeptly model the temporal dynamics of user mobility by effectively capturing long-range dependencies within the input data. The proposed method first computes tokens from the sequence of covariance matrices (CMs) for each SU and processes them in parallel using the SUtransformer network to learn the spatio-temporal features at SUlevel. Subsequently, the collaborative transformer network learns the group-level PU state from all SU-level feature representations. The attention-based sequence pooling method followed by the transformer encoder adjusts the contributions of all tokens. The main goal of predicting the PU states at each SU-level and group-level is to improve detection performance even more. We conducted a sufficient amount of simulations and compared the detection performance of different SS methods. The proposed method is tested under imperfect reporting channel scenarios to show robustness. The efficacy of our method is validated with the simulation results demonstrating its higher performance compared with existing methods in terms of detection probability, sensing error, and classification accuracy.||\n", "2409.17510": "|**2024-09-26**|[NeuroPath: A Neural Pathway Transformer for Joining the Dots of Human Connectomes](http://arxiv.org/abs/2409.17510)|**[link](https://github.com/Chrisa142857/neuro_detour)**|Although modern imaging technologies allow us to study connectivity between two distinct brain regions in-vivo, an in-depth understanding of how anatomical structure supports brain function and how spontaneous functional fluctuations emerge remarkable cognition is still elusive. Meanwhile, tremendous efforts have been made in the realm of machine learning to establish the nonlinear mapping between neuroimaging data and phenotypic traits. However, the absence of neuroscience insight in the current approaches poses significant challenges in understanding cognitive behavior from transient neural activities. To address this challenge, we put the spotlight on the coupling mechanism of structural connectivity (SC) and functional connectivity (FC) by formulating such network neuroscience question into an expressive graph representation learning problem for high-order topology. Specifically, we introduce the concept of topological detour to characterize how a ubiquitous instance of FC (direct link) is supported by neural pathways (detour) physically wired by SC, which forms a cyclic loop interacted by brain structure and function. In the clich\\'e of machine learning, the multi-hop detour pathway underlying SC-FC coupling allows us to devise a novel multi-head self-attention mechanism within Transformer to capture multi-modal feature representation from paired graphs of SC and FC. Taken together, we propose a biological-inspired deep model, coined as NeuroPath, to find putative connectomic feature representations from the unprecedented amount of neuroimages, which can be plugged into various downstream applications such as task recognition and disease diagnosis. We have evaluated NeuroPath on large-scale public datasets including HCP and UK Biobank under supervised and zero-shot learning, where the state-of-the-art performance by our NeuroPath indicates great potential in network neuroscience.||\n", "2409.17335": "|**2024-09-25**|[Non-asymptotic Convergence of Training Transformers for Next-token Prediction](http://arxiv.org/abs/2409.17335)|null|Transformers have achieved extraordinary success in modern machine learning due to their excellent ability to handle sequential data, especially in next-token prediction (NTP) tasks. However, the theoretical understanding of their performance in NTP is limited, with existing studies focusing mainly on asymptotic performance. This paper provides a fine-grained non-asymptotic analysis of the training dynamics of a one-layer transformer consisting of a self-attention module followed by a feed-forward layer. We first characterize the essential structural properties of training datasets for NTP using a mathematical framework based on partial orders. Then, we design a two-stage training algorithm, where the pre-processing stage for training the feed-forward layer and the main stage for training the attention layer exhibit fast convergence performance. Specifically, both layers converge sub-linearly to the direction of their corresponding max-margin solutions. We also show that the cross-entropy loss enjoys a linear convergence rate. Furthermore, we show that the trained transformer presents non-trivial prediction ability with dataset shift, which sheds light on the remarkable generalization performance of transformers. Our analysis technique involves the development of novel properties on the attention gradient and further in-depth analysis of how these properties contribute to the convergence of the training process. Our experiments further validate our theoretical findings.||\n", "2409.18747": "|**2024-09-27**|[Cottention: Linear Transformers With Cosine Attention](http://arxiv.org/abs/2409.18747)|**[link](https://github.com/gmongaras/Cottention_Transformer)**|Attention mechanisms, particularly softmax attention, have been instrumental in the success of transformer-based models such as GPT. However, the quadratic memory complexity of softmax attention with respect to sequence length poses significant challenges for processing longer sequences. We introduce Cottention, a novel attention mechanism that replaces the softmax operation with cosine similarity. By leveraging the properties of cosine similarity and rearranging the attention equation, Cottention achieves native linear memory complexity with respect to sequence length, making it inherently more memory-efficient than softmax attention. We demonstrate that Cottention can be reformulated as a recurrent neural network (RNN) with a finite hidden state, allowing for constant memory usage during inference. We evaluate Cottention on both the bidirectional BERT and causal GPT tasks, demonstrating comparable performance to softmax attention while significantly reducing memory requirements. To ensure efficient computation, we develop a custom CUDA kernel for Cottention. Our results show that Cottention is a promising alternative to softmax attention, enabling the processing of longer sequences without sacrificing performance, due to its native linear memory complexity and ability to maintain a constant memory footprint during inference.||\n", "2409.18523": "|**2024-09-27**|[Token Caching for Diffusion Transformer Acceleration](http://arxiv.org/abs/2409.18523)|null|Diffusion transformers have gained substantial interest in diffusion generative modeling due to their outstanding performance. However, their high computational cost, arising from the quadratic computational complexity of attention mechanisms and multi-step inference, presents a significant bottleneck. To address this challenge, we propose TokenCache, a novel post-training acceleration method that leverages the token-based multi-block architecture of transformers to reduce redundant computations among tokens across inference steps. TokenCache specifically addresses three critical questions in the context of diffusion transformers: (1) which tokens should be pruned to eliminate redundancy, (2) which blocks should be targeted for efficient pruning, and (3) at which time steps caching should be applied to balance speed and quality. In response to these challenges, TokenCache introduces a Cache Predictor that assigns importance scores to tokens, enabling selective pruning without compromising model performance. Furthermore, we propose an adaptive block selection strategy to focus on blocks with minimal impact on the network's output, along with a Two-Phase Round-Robin (TPRR) scheduling policy to optimize caching intervals throughout the denoising process. Experimental results across various models demonstrate that TokenCache achieves an effective trade-off between generation quality and inference speed for diffusion transformers. Our code will be publicly available.||\n", "2409.18158": "|**2024-09-26**|[Decomposable Transformer Point Processes](http://arxiv.org/abs/2409.18158)|null|The standard paradigm of modeling marked point processes is by parameterizing the intensity function using an attention-based (Transformer-style) architecture. Despite the flexibility of these methods, their inference is based on the computationally intensive thinning algorithm. In this work, we propose a framework where the advantages of the attention-based architecture are maintained and the limitation of the thinning algorithm is circumvented. The framework depends on modeling the conditional distribution of inter-event times with a mixture of log-normals satisfying a Markov property and the conditional probability mass function for the marks with a Transformer-based architecture. The proposed method attains state-of-the-art performance in predicting the next event of a sequence given its history. The experiments also reveal the efficacy of the methods that do not rely on the thinning algorithm during inference over the ones they do. Finally, we test our method on the challenging long-horizon prediction task and find that it outperforms a baseline developed specifically for tackling this task; importantly, inference requires just a fraction of time compared to the thinning-based baseline.||\n", "2409.20113": "|**2024-09-30**|[CBAM-SwinT-BL: Small Rail Surface Detect Detection Method Based on Swin Transformer with Block Level CBAM Enhancement](http://arxiv.org/abs/2409.20113)|null|Under high-intensity rail operations, rail tracks endure considerable stresses resulting in various defects such as corrugation and spellings. Failure to effectively detect defects and provide maintenance in time would compromise service reliability and public safety. While advanced models have been developed in recent years, efficiently identifying small-scale rail defects has not yet been studied, especially for categories such as Dirt or Squat on rail surface. To address this challenge, this study utilizes Swin Transformer (SwinT) as baseline and incorporates the Convolutional Block Attention Module (CBAM) for enhancement. Our proposed method integrates CBAM successively within the swin transformer blocks, resulting in significant performance improvement in rail defect detection, particularly for categories with small instance sizes. The proposed framework is named CBAM-Enhanced Swin Transformer in Block Level (CBAM-SwinT-BL). Experiment and ablation study have proven the effectiveness of the framework. The proposed framework has a notable improvement in the accuracy of small size defects, such as dirt and dent categories in RIII dataset, with mAP-50 increasing by +23.0% and +38.3% respectively, and the squat category in MUET dataset also reaches +13.2% higher than the original model. Compares to the original SwinT, CBAM-SwinT-BL increase overall precision around +5% in the MUET dataset and +7% in the RIII dataset, reaching 69.1% and 88.1% respectively. Meanwhile, the additional module CBAM merely extend the model training speed by an average of +0.04s/iteration, which is acceptable compared to the significant improvement in system performance.||\n", "2409.19850": "|**2024-09-30**|[SATA: Spatial Autocorrelation Token Analysis for Enhancing the Robustness of Vision Transformers](http://arxiv.org/abs/2409.19850)|null|Over the past few years, vision transformers (ViTs) have consistently demonstrated remarkable performance across various visual recognition tasks. However, attempts to enhance their robustness have yielded limited success, mainly focusing on different training strategies, input patch augmentation, or network structural enhancements. These approaches often involve extensive training and fine-tuning, which are time-consuming and resource-intensive. To tackle these obstacles, we introduce a novel approach named Spatial Autocorrelation Token Analysis (SATA). By harnessing spatial relationships between token features, SATA enhances both the representational capacity and robustness of ViT models. This is achieved through the analysis and grouping of tokens according to their spatial autocorrelation scores prior to their input into the Feed-Forward Network (FFN) block of the self-attention mechanism. Importantly, SATA seamlessly integrates into existing pre-trained ViT baselines without requiring retraining or additional fine-tuning, while concurrently improving efficiency by reducing the computational load of the FFN units. Experimental results show that the baseline ViTs enhanced with SATA not only achieve a new state-of-the-art top-1 accuracy on ImageNet-1K image classification (94.9%) but also establish new state-of-the-art performance across multiple robustness benchmarks, including ImageNet-A (top-1=63.6%), ImageNet-R (top-1=79.2%), and ImageNet-C (mCE=13.6%), all without requiring additional training or fine-tuning of baseline models.||\n", "2409.19764": "|**2024-09-29**|[Spiking Transformer with Spatial-Temporal Attention](http://arxiv.org/abs/2409.19764)|null|Spiking Neural Networks (SNNs) present a compelling and energy-efficient alternative to traditional Artificial Neural Networks (ANNs) due to their sparse binary activation. Leveraging the success of the transformer architecture, the spiking transformer architecture is explored to scale up dataset size and performance. However, existing works only consider the spatial self-attention in spiking transformer, neglecting the inherent temporal context across the timesteps. In this work, we introduce Spiking Transformer with Spatial-Temporal Attention (STAtten), a simple and straightforward architecture designed to integrate spatial and temporal information in self-attention with negligible additional computational load. The STAtten divides the temporal or token index and calculates the self-attention in a cross-manner to effectively incorporate spatial-temporal information. We first verify our spatial-temporal attention mechanism's ability to capture long-term temporal dependencies using sequential datasets. Moreover, we validate our approach through extensive experiments on varied datasets, including CIFAR10/100, ImageNet, CIFAR10-DVS, and N-Caltech101. Notably, our cross-attention mechanism achieves an accuracy of 78.39 % on the ImageNet dataset.||\n", "2409.19648": "|**2024-09-29**|[OrientedFormer: An End-to-End Transformer-Based Oriented Object Detector in Remote Sensing Images](http://arxiv.org/abs/2409.19648)|**[link](https://github.com/wokaikaixinxin/OrientedFormer)**|Oriented object detection in remote sensing images is a challenging task due to objects being distributed in multi-orientation. Recently, end-to-end transformer-based methods have achieved success by eliminating the need for post-processing operators compared to traditional CNN-based methods. However, directly extending transformers to oriented object detection presents three main issues: 1) objects rotate arbitrarily, necessitating the encoding of angles along with position and size; 2) the geometric relations of oriented objects are lacking in self-attention, due to the absence of interaction between content and positional queries; and 3) oriented objects cause misalignment, mainly between values and positional queries in cross-attention, making accurate classification and localization difficult. In this paper, we propose an end-to-end transformer-based oriented object detector, consisting of three dedicated modules to address these issues. First, Gaussian positional encoding is proposed to encode the angle, position, and size of oriented boxes using Gaussian distributions. Second, Wasserstein self-attention is proposed to introduce geometric relations and facilitate interaction between content and positional queries by utilizing Gaussian Wasserstein distance scores. Third, oriented cross-attention is proposed to align values and positional queries by rotating sampling points around the positional query according to their angles. Experiments on six datasets DIOR-R, a series of DOTA, HRSC2016 and ICDAR2015 show the effectiveness of our approach. Compared with previous end-to-end detectors, the OrientedFormer gains 1.16 and 1.21 AP$_{50}$ on DIOR-R and DOTA-v1.0 respectively, while reducing training epochs from 3$\\times$ to 1$\\times$. The codes are available at https://github.com/wokaikaixinxin/OrientedFormer.||\n", "2409.19345": "|**2024-09-28**|[Unveil Benign Overfitting for Transformer in Vision: Training Dynamics, Convergence, and Generalization](http://arxiv.org/abs/2409.19345)|null|Transformers have demonstrated great power in the recent development of large foundational models. In particular, the Vision Transformer (ViT) has brought revolutionary changes to the field of vision, achieving significant accomplishments on the experimental side. However, their theoretical capabilities, particularly in terms of generalization when trained to overfit training data, are still not fully understood. To address this gap, this work delves deeply into the benign overfitting perspective of transformers in vision. To this end, we study the optimization of a Transformer composed of a self-attention layer with softmax followed by a fully connected layer under gradient descent on a certain data distribution model. By developing techniques that address the challenges posed by softmax and the interdependent nature of multiple weights in transformer optimization, we successfully characterized the training dynamics and achieved generalization in post-training. Our results establish a sharp condition that can distinguish between the small test error phase and the large test error regime, based on the signal-to-noise ratio in the data model. The theoretical results are further verified by experimental simulation.||\n", "2409.19323": "|**2024-09-28**|[Intelligent Fish Detection System with Similarity-Aware Transformer](http://arxiv.org/abs/2409.19323)|**[link](https://github.com/vision4robotics/fishvit)**|Fish detection in water-land transfer has significantly contributed to the fishery. However, manual fish detection in crowd-collaboration performs inefficiently and expensively, involving insufficient accuracy. To further enhance the water-land transfer efficiency, improve detection accuracy, and reduce labor costs, this work designs a new type of lightweight and plug-and-play edge intelligent vision system to automatically conduct fast fish detection with high-speed camera. Moreover, a novel similarity-aware vision Transformer for fast fish detection (FishViT) is proposed to onboard identify every single fish in a dense and similar group. Specifically, a novel similarity-aware multi-level encoder is developed to enhance multi-scale features in parallel, thereby yielding discriminative representations for varying-size fish. Additionally, a new soft-threshold attention mechanism is introduced, which not only effectively eliminates background noise from images but also accurately recognizes both the edge details and overall features of different similar fish. 85 challenging video sequences with high framerate and high-resolution are collected to establish a benchmark from real fish water-land transfer scenarios. Exhaustive evaluation conducted with this challenging benchmark has proved the robustness and effectiveness of FishViT with over 80 FPS. Real work scenario tests validate the practicality of the proposed method. The code and demo video are available at https://github.com/vision4robotics/FishViT.||\n", "2409.19315": "|**2024-09-28**|[Analog In-Memory Computing Attention Mechanism for Fast and Energy-Efficient Large Language Models](http://arxiv.org/abs/2409.19315)|null|Transformer neural networks, driven by self-attention mechanisms, are core components of foundational and Large Language Models. In generative transformers, self-attention uses cache memory to store token projections, avoiding recomputation at each time step. However, GPU-stored projections must be loaded into SRAM for each new generation step, causing latency and energy bottlenecks for long sequences. In this work, we propose a fast and energy-efficient hardware implementation of self-attention using analog in-memory computing based on gain cell memories. Volatile gain cell memories can be efficiently written to store new tokens during sequence generation, while performing analog signed weight multiplications to compute the dot-products required for self-attention. We implement Sliding Window Attention, which keeps memory of a finite set of past steps. A charge-to-pulse converter for array readout eliminates the need for analog-to-digital conversion between self-attention stages. Using a co-designed initialization algorithm to adapt pre-trained weights to gain cell non-idealities, we achieve NLP performance comparable to ChatGPT-2 with minimal training iterations, despite hardware constraints. Our end-to-end hardware design includes digital controls, estimating area, latency, and energy. The system reduces attention latency by up to two orders of magnitude and energy consumption by up to five orders compared to GPUs, marking a significant step toward ultra-fast, low-power sequence generation in Large Language Models.||\n", "2409.19174": "|**2024-09-27**|[Feature Estimation of Global Language Processing in EEG Using Attention Maps](http://arxiv.org/abs/2409.19174)|null|Understanding the correlation between EEG features and cognitive tasks is crucial for elucidating brain function. Brain activity synchronizes during speaking and listening tasks. However, it is challenging to estimate task-dependent brain activity characteristics with methods with low spatial resolution but high temporal resolution, such as EEG, rather than methods with high spatial resolution, like fMRI. This study introduces a novel approach to EEG feature estimation that utilizes the weights of deep learning models to explore this association. We demonstrate that attention maps generated from Vision Transformers and EEGNet effectively identify features that align with findings from prior studies. EEGNet emerged as the most accurate model regarding subject independence and the classification of Listening and Speaking tasks. The application of Mel-Spectrogram with ViTs enhances the resolution of temporal and frequency-related EEG characteristics. Our findings reveal that the characteristics discerned through attention maps vary significantly based on the input data, allowing for tailored feature extraction from EEG signals. By estimating features, our study reinforces known attributes and predicts new ones, potentially offering fresh perspectives in utilizing EEG for medical purposes, such as early disease detection. These techniques will make substantial contributions to cognitive neuroscience.||\n", "2410.02703": "|**2024-10-03**|[Selective Attention Improves Transformer](http://arxiv.org/abs/2410.02703)|null|Unneeded elements in the attention's context degrade performance. We introduce Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention improves language modeling performance in a variety of model sizes and context lengths. For example, a range of transformers trained with the language modeling objective on C4 with selective attention perform equivalently to standard transformers with ~2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attention's context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers with 100M parameters trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity.||\n", "2410.02654": "|**2024-10-03**|[Deconstructing Recurrence, Attention, and Gating: Investigating the transferability of Transformers and Gated Recurrent Neural Networks in forecasting of dynamical systems](http://arxiv.org/abs/2410.02654)|null|Machine learning architectures, including transformers and recurrent neural networks (RNNs) have revolutionized forecasting in applications ranging from text processing to extreme weather. Notably, advanced network architectures, tuned for applications such as natural language processing, are transferable to other tasks such as spatiotemporal forecasting tasks. However, there is a scarcity of ablation studies to illustrate the key components that enable this forecasting accuracy. The absence of such studies, although explainable due to the associated computational cost, intensifies the belief that these models ought to be considered as black boxes. In this work, we decompose the key architectural components of the most powerful neural architectures, namely gating and recurrence in RNNs, and attention mechanisms in transformers. Then, we synthesize and build novel hybrid architectures from the standard blocks, performing ablation studies to identify which mechanisms are effective for each task. The importance of considering these components as hyper-parameters that can augment the standard architectures is exhibited on various forecasting datasets, from the spatiotemporal chaotic dynamics of the multiscale Lorenz 96 system, the Kuramoto-Sivashinsky equation, as well as standard real world time-series benchmarks. A key finding is that neural gating and attention improves the performance of all standard RNNs in most tasks, while the addition of a notion of recurrence in transformers is detrimental. Furthermore, our study reveals that a novel, sparsely used, architecture which integrates Recurrent Highway Networks with neural gating and attention mechanisms, emerges as the best performing architecture in high-dimensional spatiotemporal forecasting of dynamical systems.||\n", "2410.02550": "|**2024-10-03**|[NestedMorph: Enhancing Deformable Medical Image Registration with Nested Attention Mechanisms](http://arxiv.org/abs/2410.02550)|null|Deformable image registration is crucial for aligning medical images in a non-linear fashion across different modalities, allowing for precise spatial correspondence between varying anatomical structures. This paper presents NestedMorph, a novel network utilizing a Nested Attention Fusion approach to improve intra-subject deformable registration between T1-weighted (T1w) MRI and diffusion MRI (dMRI) data. NestedMorph integrates high-resolution spatial details from an encoder with semantic information from a decoder using a multi-scale framework, enhancing both local and global feature extraction. Our model notably outperforms existing methods, including CNN-based approaches like VoxelMorph, MIDIR, and CycleMorph, as well as Transformer-based models such as TransMorph and ViT-V-Net, and traditional techniques like NiftyReg and SyN. Evaluations on the HCP dataset demonstrate that NestedMorph achieves superior performance across key metrics, including SSIM, HD95, and SDlogJ, with the highest SSIM of 0.89, and the lowest HD95 of 2.5 and SDlogJ of 0.22. These results highlight NestedMorph's ability to capture both local and global image features effectively, leading to superior registration performance. The promising outcomes of this study underscore NestedMorph's potential to significantly advance deformable medical image registration, providing a robust framework for future research and clinical applications. The source code and our implementation are available at: https://bit.ly/3zdVqcg||\n", "2410.02367": "|**2024-10-03**|[SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration](http://arxiv.org/abs/2410.02367)|**[link](https://github.com/thu-ml/SageAttention)**|The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of O(N^2), compared to O(N) for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although quantization has proven to be an effective method for accelerating model inference, existing quantization methods primarily focus on optimizing the linear layer. In response, we first analyze the feasibility of quantization in attention detailedly. Following that, we propose SageAttention, a highly efficient and accurate quantization method for attention. The OPS (operations per second) of our approach outperforms FlashAttention2 and xformers by about 2.1 times and 2.7 times, respectively. SageAttention also achieves superior accuracy performance over FlashAttention3. Comprehensive experiments confirm that our approach incurs almost no end-to-end metrics loss across diverse models, including those for large language processing, image generation, and video generation.||\n", "2410.02247": "|**2024-10-03**|[Theoretical Insights into Fine-Tuning Attention Mechanism: Generalization and Optimization](http://arxiv.org/abs/2410.02247)|**[link](https://github.com/chen123CtrlS/LightweightAtt)**|Large Language Models (LLMs), built on Transformer architectures, exhibit remarkable generalization across a wide range of tasks. However, fine-tuning these models for specific tasks remains resource-intensive due to their extensive parameterization. In this paper, we investigate two remarkable phenomena observed during the fine-tuning of LLMs, particularly focusing on the attention mechanism: (1) Different Impact, optimizing the $\\mathbf{W}_v$ matrix significantly improves performance over optimizing the $\\mathbf{W}_k$ matrix. Fine-tuning only the $\\mathbf{W}_q$ and $\\mathbf{W}_v$ matrices is computationally efficient, delivering results that are comparable to, or even better than, fine-tuning all three matrices $\\mathbf{W}_q$, $\\mathbf{W}_k$, and $\\mathbf{W}_v$. (2) Efficient Convergence, employing distinct learning rates for these matrices is crucial for optimal performance, with a higher learning rate for the $\\mathbf{W}_v$ matrix expediting convergence. However, theoretical analyses of these phenomena are still relatively limited. We present a theoretical analysis of these phenomena from two perspectives: (i) Generalization, where we demonstrate that fine-tuning only $\\mathbf{W}_q$ and $\\mathbf{W}_v$ improves generalization bounds, enhances memory efficiency, and (ii) Optimization, where we emphasize that the feature learning of the attention mechanism is efficient, particularly when using distinct learning rates for the matrices, which leads to more effective fine-tuning. Building on these insights, we propose a new strategy that improves fine-tuning efficiency in terms of both storage and time. Experimental results on benchmark datasets validate the effectiveness of this approach, supporting our theoretical findings. Our analysis lays the theoretical groundwork for configuring and improving lightweight algorithms in LLMs fine-tuning.||\n", "2410.02179": "|**2024-10-03**|[HATFormer: Historic Handwritten Arabic Text Recognition with Transformers](http://arxiv.org/abs/2410.02179)|null|Arabic handwritten text recognition (HTR) is challenging, especially for historical texts, due to diverse writing styles and the intrinsic features of Arabic script. Additionally, Arabic handwriting datasets are smaller compared to English ones, making it difficult to train generalizable Arabic HTR models. To address these challenges, we propose HATFormer, a transformer-based encoder-decoder architecture that builds on a state-of-the-art English HTR model. By leveraging the transformer's attention mechanism, HATFormer captures spatial contextual information to address the intrinsic challenges of Arabic script through differentiating cursive characters, decomposing visual representations, and identifying diacritics. Our customization to historical handwritten Arabic includes an image processor for effective ViT information preprocessing, a text tokenizer for compact Arabic text representation, and a training pipeline that accounts for a limited amount of historic Arabic handwriting data. HATFormer achieves a character error rate (CER) of 8.6% on the largest public historical handwritten Arabic dataset, with a 51% improvement over the best baseline in the literature. HATFormer also attains a comparable CER of 4.2% on the largest private non-historical dataset. Our work demonstrates the feasibility of adapting an English HTR method to a low-resource language with complex, language-specific challenges, contributing to advancements in document digitization, information retrieval, and cultural preservation.||\n", "2410.02167": "|**2024-10-03**|[Training Nonlinear Transformers for Chain-of-Thought Inference: A Theoretical Generalization Analysis](http://arxiv.org/abs/2410.02167)|null|Chain-of-Thought (CoT) is an efficient prompting method that enables the reasoning ability of large language models by augmenting the query using multiple examples with multiple intermediate steps. Despite the empirical success, the theoretical understanding of how to train a Transformer to achieve the CoT ability remains less explored. This is primarily due to the technical challenges involved in analyzing the nonconvex optimization on nonlinear attention models. To the best of our knowledge, this work provides the first theoretical study of training Transformers with nonlinear attention to obtain the CoT generalization capability so that the resulting model can inference on unseen tasks when the input is augmented by examples of the new task. We first quantify the required training samples and iterations to train a Transformer model towards CoT ability. We then prove the success of its CoT generalization on unseen tasks with distribution-shifted testing data. Moreover, we theoretically characterize the conditions for an accurate reasoning output by CoT even when the provided reasoning examples contain noises and are not always accurate. In contrast, in-context learning (ICL), which can be viewed as one-step CoT without intermediate steps, may fail to provide an accurate output when CoT does. These theoretical findings are justified through experiments.||\n", "2410.01686": "|**2024-10-02**|[Positional Attention: Out-of-Distribution Generalization and Expressivity for Neural Algorithmic Reasoning](http://arxiv.org/abs/2410.01686)|**[link](https://github.com/opallab/positional_attention)**|There has been a growing interest in the ability of neural networks to solve algorithmic tasks, such as arithmetic, summary statistics, and sorting. While state-of-the-art models like Transformers have demonstrated good generalization performance on in-distribution tasks, their out-of-distribution (OOD) performance is poor when trained end-to-end. In this paper, we focus on value generalization, a common instance of OOD generalization where the test distribution has the same input sequence length as the training distribution, but the value ranges in the training and test distributions do not necessarily overlap. To address this issue, we propose that using fixed positional encodings to determine attention weights-referred to as positional attention-enhances empirical OOD performance while maintaining expressivity. We support our claim about expressivity by proving that Transformers with positional attention can effectively simulate parallel algorithms.||\n", "2410.01637": "|**2024-10-02**|[On The Adaptation of Unlimiformer for Decoder-Only Transformers](http://arxiv.org/abs/2410.01637)|null|One of the prominent issues stifling the current generation of large language models is their limited context length. Recent proprietary models such as GPT-4 and Claude 2 have introduced longer context lengths, 8k/32k and 100k, respectively; however, despite the efforts in the community, most common models, such as LLama-2, have a context length of 4k or less. Unlimiformer (Bertsch et al., 2023) is a recently popular vector-retrieval augmentation method that offloads cross-attention computations to a kNN index. However, its main limitation is incompatibility with decoder-only transformers out of the box. In this work, we explore practical considerations of adapting Unlimiformer to decoder-only transformers and introduce a series of modifications to overcome this limitation. Moreover, we expand the original experimental setup on summarization to include a new task (i.e., free-form Q&A) and an instruction-tuned model (i.e., a custom 6.7B GPT model). Our results showcase the effectiveness of these modifications on summarization, performing on par with a model with 2x the context length. Moreover, we discuss limitations and future directions for free-form Q&A and instruction-tuned models.||\n", "2410.01537": "|**2024-10-02**|[Attention layers provably solve single-location regression](http://arxiv.org/abs/2410.01537)|**[link](https://github.com/pierremarion23/single-location-regression)**|Attention-based models, such as Transformer, excel across various tasks but lack a comprehensive theoretical understanding, especially regarding token-wise sparsity and internal linear representations. To address this gap, we introduce the single-location regression task, where only one token in a sequence determines the output, and its position is a latent random variable, retrievable via a linear projection of the input. To solve this task, we propose a dedicated predictor, which turns out to be a simplified version of a non-linear self-attention layer. We study its theoretical properties, by showing its asymptotic Bayes optimality and analyzing its training dynamics. In particular, despite the non-convex nature of the problem, the predictor effectively learns the underlying structure. This work highlights the capacity of attention mechanisms to handle sparse token information and internal linear structures.||\n", "2410.03462": "|**2024-10-04**|[Linear Transformer Topological Masking with Graph Random Features](http://arxiv.org/abs/2410.03462)|null|When training transformers on graph-structured data, incorporating information about the underlying topology is crucial for good performance. Topological masking, a type of relative position encoding, achieves this by upweighting or downweighting attention depending on the relationship between the query and keys in a graph. In this paper, we propose to parameterise topological masks as a learnable function of a weighted adjacency matrix -- a novel, flexible approach which incorporates a strong structural inductive bias. By approximating this mask with graph random features (for which we prove the first known concentration bounds), we show how this can be made fully compatible with linear attention, preserving $\\mathcal{O}(N)$ time and space complexity with respect to the number of input tokens. The fastest previous alternative was $\\mathcal{O}(N \\log N)$ and only suitable for specific graphs. Our efficient masking algorithms provide strong performance gains for tasks on image and point cloud data, including with $>30$k nodes.||\n", "2410.03364": "|**2024-10-04**|[Error Correction Code Transformer: From Non-Unified to Unified](http://arxiv.org/abs/2410.03364)|null|Channel coding is vital for reliable data transmission in modern wireless systems, and its significance will increase with the emergence of sixth-generation (6G) networks, which will need to support various error correction codes. However, traditional decoders were typically designed as fixed hardware circuits tailored to specific decoding algorithms, leading to inefficiencies and limited flexibility. To address these challenges, this paper proposes a unified, code-agnostic Transformer-based decoding architecture capable of handling multiple linear block codes, including Polar, Low-Density Parity-Check (LDPC), and Bose-Chaudhuri-Hocquenghem (BCH), within a single framework. To achieve this, standardized units are employed to harmonize parameters across different code types, while the redesigned unified attention module compresses the structural information of various codewords. Additionally, a sparse mask, derived from the sparsity of the parity-check matrix, is introduced to enhance the model's ability to capture inherent constraints between information and parity-check bits, resulting in improved decoding accuracy and robustness. Extensive experimental results demonstrate that the proposed unified Transformer-based decoder not only outperforms existing methods but also provides a flexible, efficient, and high-performance solution for next-generation wireless communication systems.||\n", "2410.03171": "|**2024-10-04**|[Selective Transformer for Hyperspectral Image Classification](http://arxiv.org/abs/2410.03171)|null|Transformer has achieved satisfactory results in the field of hyperspectral image (HSI) classification. However, existing Transformer models face two key challenges when dealing with HSI scenes characterized by diverse land cover types and rich spectral information: (1) fixed receptive field representation overlooks effective contextual information; (2) redundant self-attention feature representation. To address these limitations, we propose a novel Selective Transformer (SFormer) for HSI classification. The SFormer is designed to dynamically select receptive fields for capturing both spatial and spectral contextual information, while mitigating the impact of redundant data by prioritizing the most relevant features. This enables a highly accurate classification of the land covers of the HSI. Specifically, a Kernel Selective Transformer Block (KSTB) is first utilized to dynamically select an appropriate receptive field range to effectively extract spatial-spectral features. Furthermore, to capture the most crucial tokens, a Token Selective Transformer Block (TSTB) is introduced, which selects the most relevant tokens based on the ranking of attention scores for each query. Extensive experiments on four benchmark HSI datasets demonstrate that the proposed SFormer outperforms the state-of-the-art HSI classification models. The codes will be released.||\n", "2410.03159": "|**2024-10-04**|[Autoregressive Moving-average Attention Mechanism for Time Series Forecasting](http://arxiv.org/abs/2410.03159)|**[link](https://github.com/ljc-fvnr/arma-attention)**|We propose an Autoregressive (AR) Moving-average (MA) attention structure that can adapt to various linear attention mechanisms, enhancing their ability to capture long-range and local temporal patterns in time series. In this paper, we first demonstrate that, for the time series forecasting (TSF) task, the previously overlooked decoder-only autoregressive Transformer model can achieve results comparable to the best baselines when appropriate tokenization and training methods are applied. Moreover, inspired by the ARMA model from statistics and recent advances in linear attention, we introduce the full ARMA structure into existing autoregressive attention mechanisms. By using an indirect MA weight generation method, we incorporate the MA term while maintaining the time complexity and parameter size of the underlying efficient attention models. We further explore how indirect parameter generation can produce implicit MA weights that align with the modeling requirements for local temporal impacts. Experimental results show that incorporating the ARMA structure consistently improves the performance of various AR attentions on TSF tasks, achieving state-of-the-art results.||\n", "2410.03011": "|**2024-10-03**|[Towards Understanding the Universality of Transformers for Next-Token Prediction](http://arxiv.org/abs/2410.03011)|null|Causal Transformers are trained to predict the next token for a given context. While it is widely accepted that self-attention is crucial for encoding the causal structure of sequences, the precise underlying mechanism behind this in-context autoregressive learning ability remains unclear. In this paper, we take a step towards understanding this phenomenon by studying the approximation ability of Transformers for next-token prediction. Specifically, we explore the capacity of causal Transformers to predict the next token $x_{t+1}$ given an autoregressive sequence $(x_1, \\dots, x_t)$ as a prompt, where $ x_{t+1} = f(x_t) $, and $ f $ is a context-dependent function that varies with each sequence. On the theoretical side, we focus on specific instances, namely when $ f $ is linear or when $ (x_t)_{t \\geq 1} $ is periodic. We explicitly construct a Transformer (with linear, exponential, or softmax attention) that learns the mapping $f$ in-context through a causal kernel descent method. The causal kernel descent method we propose provably estimates $x_{t+1} $ based solely on past and current observations $ (x_1, \\dots, x_t) $, with connections to the Kaczmarz algorithm in Hilbert spaces. We present experimental results that validate our theoretical findings and suggest their applicability to more general mappings $f$.||\n", "2410.02984": "|**2024-10-03**|[Differentiation and Specialization of Attention Heads via the Refined Local Learning Coefficient](http://arxiv.org/abs/2410.02984)|null|We introduce refined variants of the Local Learning Coefficient (LLC), a measure of model complexity grounded in singular learning theory, to study the development of internal structure in transformer language models during training. By applying these \\textit{refined LLCs} (rLLCs) to individual components of a two-layer attention-only transformer, we gain novel insights into the progressive differentiation and specialization of attention heads. Our methodology reveals how attention heads differentiate into distinct functional roles over the course of training, analyzes the types of data these heads specialize to process, and discovers a previously unidentified multigram circuit. These findings demonstrate that rLLCs provide a principled, quantitative toolkit for \\textit{developmental interpretability}, which aims to understand models through their evolution across the learning process. More broadly, this work takes a step towards establishing the correspondence between data distributional structure, geometric properties of the loss landscape, learning dynamics, and emergent computational structures in neural networks.||\n", "2410.02981": "|**2024-10-03**|[GABIC: Graph-based Attention Block for Image Compression](http://arxiv.org/abs/2410.02981)|**[link](https://github.com/EIDOSLAB/GABIC)**|While standardized codecs like JPEG and HEVC-intra represent the industry standard in image compression, neural Learned Image Compression (LIC) codecs represent a promising alternative. In detail, integrating attention mechanisms from Vision Transformers into LIC models has shown improved compression efficiency. However, extra efficiency often comes at the cost of aggregating redundant features. This work proposes a Graph-based Attention Block for Image Compression (GABIC), a method to reduce feature redundancy based on a k-Nearest Neighbors enhanced attention mechanism. Our experiments show that GABIC outperforms comparable methods, particularly at high bit rates, enhancing compression performance.||\n", "2410.05258": "|**2024-10-07**|[Differential Transformer](http://arxiv.org/abs/2410.05258)|null|Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.||\n", "2410.05076": "|**2024-10-07**|[TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention](http://arxiv.org/abs/2410.05076)|**[link](https://github.com/DerrickYLJ/TidalDecode)**|\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5404\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u5176\u4e2d\u957f\u4e0a\u4e0b\u6587\u6a21\u578b\u5728\u5904\u7406\u6269\u5c55\u8f93\u5165\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002\u7136\u800c\uff0cTransformer \u67b6\u6784\u6240\u9700\u7684\u4e0d\u65ad\u6269\u5927\u7684\u952e\u503c\uff08KV\uff09\u7f13\u5b58\u5927\u5c0f\u52a0\u5267\u4e86\u5185\u5b58\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u89e3\u7801\u9636\u6bb5\uff0c\u9020\u6210\u4e86\u663e\u8457\u7684\u74f6\u9888\u3002\u73b0\u6709\u7684\u65e8\u5728\u89e3\u51b3\u6b64\u74f6\u9888\u7684\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u6709\u4e24\u4e2a\u5c40\u9650\u6027\uff1a\uff081\uff09\u5b83\u4eec\u901a\u5e38\u65e0\u6cd5\u53ef\u9760\u5730\u8bc6\u522b\u4e0e\u6ce8\u610f\u529b\u6700\u76f8\u5173\u7684\u6807\u8bb0\uff0c\u4ee5\u53ca\uff082\uff09\u5b83\u4eec\u5ffd\u7565\u4e86\u8de8\u8fde\u7eed Transformer \u5c42\u7684\u6807\u8bb0\u9009\u62e9\u7684\u7a7a\u9593\u4e00\u81f4\u6027\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u6807\u8bb0\u9009\u62e9\u4e2d\u7684\u5927\u91cf\u5f00\u9500\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 TidalDecode\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u7b97\u6cd5\u548c\u7cfb\u7edf\uff0c\u53ef\u901a\u8fc7\u4f4d\u7f6e\u6301\u4e45\u6027\u7a00\u758f\u6ce8\u610f\u529b\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u7684 LLM \u89e3\u7801\u3002TidalDecode \u5229\u7528\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u9009\u62e9\u7684\u6807\u8bb0\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e9b\u6267\u884c\u5b8c\u5168\u6ce8\u610f\u529b\u7684\u6807\u8bb0\u9009\u62e9\u5c42\uff0c\u4ee5\u8bc6\u522b\u5177\u6709\u6700\u9ad8\u6ce8\u610f\u529b\u5206\u6570\u7684\u6807\u8bb0\uff0c\u800c\u6240\u6709\u5176\u4ed6\u5c42\u90fd\u5bf9\u9884\u5148\u9009\u62e9\u7684\u6807\u8bb0\u6267\u884c\u7a00\u758f\u6ce8\u610f\u529b\u3002\u8fd9\u79cd\u8bbe\u8ba1\u4f7f TidalDecode \u80fd\u591f\u5728\u4e0d\u727a\u7272\u751f\u6210\u7ed3\u679c\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u5927\u5e45\u51cf\u5c11\u7a00\u758f\u6ce8\u610f\u529b\u7684\u6807\u8bb0\u9009\u62e9\u5f00\u9500\u3002\u5bf9\u5404\u79cd LLM \u548c\u4efb\u52a1\u7684\u8bc4\u4f30\u8868\u660e\uff0cTidalDecode \u5728\u751f\u6210\u6027\u80fd\u4e0a\u4e0e\u5b8c\u5168\u6ce8\u610f\u529b\u65b9\u6cd5\u975e\u5e38\u63a5\u8fd1\uff0c\u540c\u65f6\u5c06 LLM \u89e3\u7801\u5ef6\u8fdf\u964d\u4f4e\u4e86\u9ad8\u8fbe 2.1 \u500d\u3002||\n", "2410.04870": "|**2024-10-07**|[On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent](http://arxiv.org/abs/2410.04870)|null|The Adam optimizer is widely used for transformer optimization in practice, which makes understanding the underlying optimization mechanisms an important problem. However, due to the Adam's complexity, theoretical analysis of how it optimizes transformers remains a challenging task. Fortunately, Sign Gradient Descent (SignGD) serves as an effective surrogate for Adam. Despite its simplicity, theoretical understanding of how SignGD optimizes transformers still lags behind. In this work, we study how SignGD optimizes a two-layer transformer -- consisting of a softmax attention layer with trainable query-key parameterization followed by a linear layer -- on a linearly separable noisy dataset. We identify four stages in the training dynamics, each exhibiting intriguing behaviors. Based on the training dynamics, we prove the fast convergence but poor generalization of the learned transformer on the noisy dataset. We also show that Adam behaves similarly to SignGD in terms of both optimization and generalization in this setting. Additionally, we find that the poor generalization of SignGD is not solely due to data noise, suggesting that both SignGD and Adam requires high-quality data for real-world tasks. Finally, experiments on synthetic and real-world datasets empirically support our theoretical results.||\n", "2410.04801": "|**2024-10-07**|[Improving Image Clustering with Artifacts Attenuation via Inference-Time Attention Engineering](http://arxiv.org/abs/2410.04801)|null|The goal of this paper is to improve the performance of pretrained Vision Transformer (ViT) models, particularly DINOv2, in image clustering task without requiring re-training or fine-tuning. As model size increases, high-norm artifacts anomaly appears in the patches of multi-head attention. We observe that this anomaly leads to reduced accuracy in zero-shot image clustering. These artifacts are characterized by disproportionately large values in the attention map compared to other patch tokens. To address these artifacts, we propose an approach called Inference-Time Attention Engineering (ITAE), which manipulates attention function during inference. Specifically, we identify the artifacts by investigating one of the Query-Key-Value (QKV) patches in the multi-head attention and attenuate their corresponding attention values inside the pretrained models. ITAE shows improved clustering accuracy on multiple datasets by exhibiting more expressive features in latent space. Our findings highlight the potential of ITAE as a practical solution for reducing artifacts in pretrained ViT models and improving model performance in clustering tasks without the need for re-training or fine-tuning.||\n", "2410.04798": "|**2024-10-07**|[DAPE V2: Process Attention Score as Feature Map for Length Extrapolation](http://arxiv.org/abs/2410.04798)|**[link](https://github.com/chuanyang-zheng/dape)**|The attention mechanism is a fundamental component of the Transformer model, contributing to interactions among distinct tokens, in contrast to earlier feed-forward neural networks. In general, the attention scores are determined simply by the key-query products. However, this work's occasional trial (combining DAPE and NoPE) of including additional MLPs on attention scores without position encoding indicates that the classical key-query multiplication may limit the performance of Transformers. In this work, we conceptualize attention as a feature map and apply the convolution operator (for neighboring attention scores across different heads) to mimic the processing methods in computer vision. Specifically, the main contribution of this paper is identifying and interpreting the Transformer length extrapolation problem as a result of the limited expressiveness of the naive query and key dot product, and we successfully translate the length extrapolation issue into a well-understood feature map processing problem. The novel insight, which can be adapted to various attention-related models, reveals that the current Transformer architecture has the potential for further evolution. Extensive experiments demonstrate that treating attention as a feature map and applying convolution as a processing method significantly enhances Transformer performance.||\n", "2410.04733": "|**2024-10-07**|[PredFormer: Transformers Are Effective Spatial-Temporal Predictive Learners](http://arxiv.org/abs/2410.04733)|**[link](https://github.com/yyyujintang/predformer)**|Spatiotemporal predictive learning methods generally fall into two categories: recurrent-based approaches, which face challenges in parallelization and performance, and recurrent-free methods, which employ convolutional neural networks (CNNs) as encoder-decoder architectures. These methods benefit from strong inductive biases but often at the expense of scalability and generalization. This paper proposes PredFormer, a pure transformer-based framework for spatiotemporal predictive learning. Motivated by the Vision Transformers (ViT) design, PredFormer leverages carefully designed Gated Transformer blocks, following a comprehensive analysis of 3D attention mechanisms, including full-, factorized-, and interleaved- spatial-temporal attention. With its recurrent-free, transformer-based design, PredFormer is both simple and efficient, significantly outperforming previous methods by large margins. Extensive experiments on synthetic and real-world datasets demonstrate that PredFormer achieves state-of-the-art performance. On Moving MNIST, PredFormer achieves a 51.3% reduction in MSE relative to SimVP. For TaxiBJ, the model decreases MSE by 33.1% and boosts FPS from 533 to 2364. Additionally, on WeatherBench, it reduces MSE by 11.1% while enhancing FPS from 196 to 404. These performance gains in both accuracy and efficiency demonstrate PredFormer's potential for real-world applications. The source code will be released at https://github.com/yyyujintang/PredFormer.||\n", "2410.04731": "|**2024-10-07**|[Efficient transformer with reinforced position embedding for language models](http://arxiv.org/abs/2410.04731)|null|In this paper, we propose an efficient transformer architecture that uses reinforced positional embedding to obtain superior performance with half the number of encoder decoder layers. We demonstrate that concatenating positional encoding with trainable token embeddings, normalizing columns in the token embedding matrix, and using the normalized token embedding matrix as the value of the attention layer improve the training and validation loss and the training time in an encoder-decoder Transformer model for a Portuguese-English translation task with 10 epochs or 12 hours of training across 10 trials. Our method, with roughly a threefold parameter reduction compared to the baseline model, yields a mean training loss of 1.21, a mean validation loss of 1.51, and an average training time of 1352.27 seconds per epoch, surpassing the baseline model with the same embedding dimension that employs addition of positional encoding and token embeddings, which achieves a mean training loss of 1.96, a validation loss of 2.18, and an average training time of 4297.79 seconds per epoch. Additionally, we evaluated our proposed architecture and the baseline across 14 diverse translation datasets from TensorFlow. The results indicate that our method consistently achieves lower or comparable training and validation losses, suggesting enhanced learning efficiency.||\n", "2410.04689": "|**2024-10-07**|[Low-Rank Continual Pyramid Vision Transformer: Incrementally Segment Whole-Body Organs in CT with Light-Weighted Adaptation](http://arxiv.org/abs/2410.04689)|null|Deep segmentation networks achieve high performance when trained on specific datasets. However, in clinical practice, it is often desirable that pretrained segmentation models can be dynamically extended to enable segmenting new organs without access to previous training datasets or without training from scratch. This would ensure a much more efficient model development and deployment paradigm accounting for the patient privacy and data storage issues. This clinically preferred process can be viewed as a continual semantic segmentation (CSS) problem. Previous CSS works would either experience catastrophic forgetting or lead to unaffordable memory costs as models expand. In this work, we propose a new continual whole-body organ segmentation model with light-weighted low-rank adaptation (LoRA). We first train and freeze a pyramid vision transformer (PVT) base segmentation model on the initial task, then continually add light-weighted trainable LoRA parameters to the frozen model for each new learning task. Through a holistically exploration of the architecture modification, we identify three most important layers (i.e., patch-embedding, multi-head attention and feed forward layers) that are critical in adapting to the new segmentation tasks, while retaining the majority of the pretrained parameters fixed. Our proposed model continually segments new organs without catastrophic forgetting and meanwhile maintaining a low parameter increasing rate. Continually trained and tested on four datasets covering different body parts of a total of 121 organs, results show that our model achieves high segmentation accuracy, closely reaching the PVT and nnUNet upper bounds, and significantly outperforms other regularization-based CSS methods. When comparing to the leading architecture-based CSS method, our model has a substantial lower parameter increasing rate while achieving comparable performance.||\n", "2410.04514": "|**2024-10-06**|[DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination](http://arxiv.org/abs/2410.04514)|null|Despite the great success of Large Vision-Language Models (LVLMs), they inevitably suffer from hallucination. As we know, both the visual encoder and the Large Language Model (LLM) decoder in LVLMs are Transformer-based, allowing the model to extract visual information and generate text outputs via attention mechanisms. We find that the attention distribution of LLM decoder on image tokens is highly consistent with the visual encoder and both distributions tend to focus on particular background tokens rather than the referred objects in the image. We attribute to the unexpected attention distribution to an inherent flaw in the visual encoder itself, which misguides LLMs to over emphasize the redundant information and generate object hallucination. To address the issue, we propose DAMRO, a novel training-free strategy that $D$ive into $A$ttention $M$echanism of LVLM to $R$educe $O$bject Hallucination. Specifically, our approach employs classification token (CLS) of ViT to filter out high-attention outlier tokens scattered in the background and then eliminate their influence during decoding stage. We evaluate our method on LVLMs including LLaVA-1.5, LLaVA-NeXT and InstructBLIP, using various benchmarks such as POPE, CHAIR, MME and GPT-4V Aided Evaluation. The results demonstrate that our approach significantly reduces the impact of these outlier tokens, thus effectively alleviating the hallucination of LVLMs. The code of our method will be released soon.||\n", "2410.04271": "|**2024-10-05**|[Fundamental Limitations on Subquadratic Alternatives to Transformers](http://arxiv.org/abs/2410.04271)|null|The Transformer architecture is widely deployed in many popular and impactful Large Language Models. At its core is the attention mechanism for calculating correlations between pairs of tokens. Performing an attention computation takes quadratic time in the input size, and had become the time bottleneck for transformer operations. In order to circumvent this, researchers have used a variety of approaches, including designing heuristic algorithms for performing attention computations faster, and proposing alternatives to the attention mechanism which can be computed more quickly. For instance, state space models such as Mamba were designed to replace attention with an almost linear time alternative.   In this paper, we prove that any such approach cannot perform important tasks that Transformer is able to perform (assuming a popular conjecture from fine-grained complexity theory). We focus on document similarity tasks, where one is given as input many documents and would like to find a pair which is (approximately) the most similar. We prove that Transformer is able to perform this task, and we prove that this task cannot be performed in truly subquadratic time by any algorithm. Thus, any model which can be evaluated in subquadratic time - whether because of subquadratic-time heuristics for attention, faster attention replacements like Mamba, or any other reason - cannot perform this task. In other words, in order to perform tasks that (implicitly or explicitly) involve document similarity, one may as well use Transformer and cannot avoid its quadratic running time.||\n", "2410.08024": "|**2024-10-10**|[Pretraining Graph Transformers with Atom-in-a-Molecule Quantum Properties for Improved ADMET Modeling](http://arxiv.org/abs/2410.08024)|**[link](https://github.com/aidd-msca/GraphQPT)**|We evaluate the impact of pretraining Graph Transformer architectures on atom-level quantum-mechanical features for the modeling of absorption, distribution, metabolism, excretion, and toxicity (ADMET) properties of drug-like compounds. We compare this pretraining strategy with two others: one based on molecular quantum properties (specifically the HOMO-LUMO gap) and one using a self-supervised atom masking technique. After fine-tuning on Therapeutic Data Commons ADMET datasets, we evaluate the performance improvement in the different models observing that models pretrained with atomic quantum mechanical properties produce in general better results. We then analyse the latent representations and observe that the supervised strategies preserve the pretraining information after finetuning and that different pretrainings produce different trends in latent expressivity across layers. Furthermore, we find that models pretrained on atomic quantum mechanical properties capture more low-frequency laplacian eigenmodes of the input graph via the attention weights and produce better representations of atomic environments within the molecule. Application of the analysis to a much larger non-public dataset for microsomal clearance illustrates generalizability of the studied indicators. In this case the performances of the models are in accordance with the representation analysis and highlight, especially for the case of masking pretraining and atom-level quantum property pretraining, how model types with similar performance on public benchmarks can have different performances on large scale pharmaceutical data.||\n", "2410.07860": "|**2024-10-11**|[BA-Net: Bridge Attention in Deep Neural Networks](http://arxiv.org/abs/2410.07860)|null|Attention mechanisms, particularly channel attention, have become highly influential in numerous computer vision tasks. Despite their effectiveness, many existing methods primarily focus on optimizing performance through complex attention modules applied at individual convolutional layers, often overlooking the synergistic interactions that can occur across multiple layers. In response to this gap, we introduce bridge attention, a novel approach designed to facilitate more effective integration and information flow between different convolutional layers. Our work extends the original bridge attention model (BAv1) by introducing an adaptive selection operator, which reduces information redundancy and optimizes the overall information exchange. This enhancement results in the development of BAv2, which achieves substantial performance improvements in the ImageNet classification task, obtaining Top-1 accuracies of 80.49% and 81.75% when using ResNet50 and ResNet101 as backbone networks, respectively. These results surpass the retrained baselines by 1.61% and 0.77%, respectively. Furthermore, BAv2 outperforms other existing channel attention techniques, such as the classical SENet101, exceeding its retrained performance by 0.52% Additionally, integrating BAv2 into advanced convolutional networks and vision transformers has led to significant gains in performance across a wide range of computer vision tasks, underscoring its broad applicability.||\n", "2410.07799": "|**2024-10-10**|[Mind the Gap: a Spectral Analysis of Rank Collapse and Signal Propagation in Transformers](http://arxiv.org/abs/2410.07799)|null|Attention layers are the core component of transformers, the current state-of-the-art neural network architecture. However, \\softmaxx-based attention puts transformers' trainability at risk. Even \\textit{at initialisation}, the propagation of signals and gradients through the random network can be pathological, resulting in known issues such as (i) vanishing/exploding gradients and (ii) \\textit{rank collapse}, i.e. when all tokens converge to a single representation \\textit{with depth}. This paper examines signal propagation in \\textit{attention-only} transformers from a random matrix perspective, illuminating the origin of such issues, as well as unveiling a new phenomenon -- (iii) rank collapse \\textit{in width}. Modelling \\softmaxx-based attention at initialisation with Random Markov matrices, our theoretical analysis reveals that a \\textit{spectral gap} between the two largest singular values of the attention matrix causes (iii), which, in turn, exacerbates (i) and (ii). Building on this insight, we propose a novel, yet simple, practical solution to resolve rank collapse in width by removing the spectral gap. Moreover, we validate our findings and discuss the training benefits of the proposed fix through experiments that also motivate a revision of some of the default parameter scaling. Our attention model accurately describes the standard key-query attention in a single-layer transformer, making this work a significant first step towards a better understanding of the initialisation dynamics in the multi-layer case.||\n", "2410.07746": "|**2024-10-10**|[Benign Overfitting in Single-Head Attention](http://arxiv.org/abs/2410.07746)|null|The phenomenon of benign overfitting, where a trained neural network perfectly fits noisy training data but still achieves near-optimal test performance, has been extensively studied in recent years for linear models and fully-connected/convolutional networks. In this work, we study benign overfitting in a single-head softmax attention model, which is the fundamental building block of Transformers. We prove that under appropriate conditions, the model exhibits benign overfitting in a classification setting already after two steps of gradient descent. Moreover, we show conditions where a minimum-norm/maximum-margin interpolator exhibits benign overfitting. We study how the overfitting behavior depends on the signal-to-noise ratio (SNR) of the data distribution, namely, the ratio between norms of signal and noise tokens, and prove that a sufficiently large SNR is both necessary and sufficient for benign overfitting.||\n", "2410.07531": "|**2024-10-10**|[Reducing the Cost of Dropout in Flash-Attention by Hiding RNG with GEMM](http://arxiv.org/abs/2410.07531)|null|Dropout, a network operator, when enabled is likely to dramatically impact the performance of Flash-Attention, which in turn increases the end-to-end training time of Large-Language-Models (LLMs). The main contributor to such performance degradation is the Random Number Generation (RNG) phase that is traditionally fused into the Flash-Attention kernel. As RNG and Attention have the same hardware bottlenecks, RNG latency can hardly be hidden within the Attention kernel.   We propose overlapping RNG with previous GEMM layers in the network to hide RNG runtime and improve end-to-end performance. RNG and GEMM have distinct resource requirements and hardware bottlenecks, so they can run in parallel without compromising each other's performance. Our fine-grained performance model, cross-validated by silicon results, shows 1.14x speedup on one transformer block (including multi-head attention and feed-forward layers) for Llama2, and up to 1.23x speedup when varying workload sizes, on GH100 GPUs with FP8 precision. Further, we extend our theoretical model to different RNG implementations and hardware architectures, and discuss the widely applicable benefits for overlapping RNG with GEMM layers.||\n", "2410.07169": "|**2024-10-09**|[VIRT: Vision Instructed Transformer for Robotic Manipulation](http://arxiv.org/abs/2410.07169)|null|Robotic manipulation, owing to its multi-modal nature, often faces significant training ambiguity, necessitating explicit instructions to clearly delineate the manipulation details in tasks. In this work, we highlight that vision instruction is naturally more comprehensible to recent robotic policies than the commonly adopted text instruction, as these policies are born with some vision understanding ability like human infants. Building on this premise and drawing inspiration from cognitive science, we introduce the robotic imagery paradigm, which realizes large-scale robotic data pre-training without text annotations. Additionally, we propose the robotic gaze strategy that emulates the human eye gaze mechanism, thereby guiding subsequent actions and focusing the attention of the policy on the manipulated object. Leveraging these innovations, we develop VIRT, a fully Transformer-based policy. We design comprehensive tasks using both a physical robot and simulated environments to assess the efficacy of VIRT. The results indicate that VIRT can complete very competitive tasks like ``opening the lid of a tightly sealed bottle'', and the proposed techniques boost the success rates of the baseline policy on diverse challenging tasks from nearly 0% to more than 65%.||\n", "2410.07083": "|**2024-10-09**|[Stanceformer: Target-Aware Transformer for Stance Detection](http://arxiv.org/abs/2410.07083)|null|The task of Stance Detection involves discerning the stance expressed in a text towards a specific subject or target. Prior works have relied on existing transformer models that lack the capability to prioritize targets effectively. Consequently, these models yield similar performance regardless of whether we utilize or disregard target information, undermining the task's significance. To address this challenge, we introduce Stanceformer, a target-aware transformer model that incorporates enhanced attention towards the targets during both training and inference. Specifically, we design a \\textit{Target Awareness} matrix that increases the self-attention scores assigned to the targets. We demonstrate the efficacy of the Stanceformer with various BERT-based models, including state-of-the-art models and Large Language Models (LLMs), and evaluate its performance across three stance detection datasets, alongside a zero-shot dataset. Our approach Stanceformer not only provides superior performance but also generalizes even to other domains, such as Aspect-based Sentiment Analysis. We make the code publicly available.\\footnote{\\scriptsize\\url{https://github.com/kgarg8/Stanceformer}}||\n", "2410.07063": "|**2024-10-09**|[InAttention: Linear Context Scaling for Transformers](http://arxiv.org/abs/2410.07063)|null|VRAM requirements for transformer models scale quadratically with context length due to the self-attention mechanism. In this paper we modify the decoder-only transformer, replacing self-attention with InAttention, which scales linearly with context length during inference by having tokens attend only to initial states. Benchmarking shows that InAttention significantly reduces VRAM usage during inference, enabling handling of long sequences on consumer GPUs. We corroborate that fine-tuning extends context length efficiently, improving performance on long sequences without high training costs. InAttention offers a scalable solution for long-range dependencies in transformer models, paving the way for further optimization.||\n", "2410.06833": "|**2024-10-09**|[Dynamic metastability in the self-attention model](http://arxiv.org/abs/2410.06833)|**[link](https://github.com/hugokoubbi/2024-transformers-dotm)**|We consider the self-attention model - an interacting particle system on the unit sphere, which serves as a toy model for Transformers, the deep neural network architecture behind the recent successes of large language models. We prove the appearance of dynamic metastability conjectured in [GLPR23] - although particles collapse to a single cluster in infinite time, they remain trapped near a configuration of several clusters for an exponentially long period of time. By leveraging a gradient flow interpretation of the system, we also connect our result to an overarching framework of slow motion of gradient flows proposed by Otto and Reznikoff [OR07] in the context of coarsening and the Allen-Cahn equation. We finally probe the dynamics beyond the exponentially long period of metastability, and illustrate that, under an appropriate time-rescaling, the energy reaches its global maximum in finite time and has a staircase profile, with trajectories manifesting saddle-to-saddle-like behavior, reminiscent of recent works in the analysis of training dynamics via gradient descent for two-layer neural networks.||\n", "2410.06746": "|**2024-10-09**|[Cluster-wise Graph Transformer with Dual-granularity Kernelized Attention](http://arxiv.org/abs/2410.06746)|**[link](https://github.com/lumia-group/cluster-wise-graph-transformer)**|In the realm of graph learning, there is a category of methods that conceptualize graphs as hierarchical structures, utilizing node clustering to capture broader structural information. While generally effective, these methods often rely on a fixed graph coarsening routine, leading to overly homogeneous cluster representations and loss of node-level information. In this paper, we envision the graph as a network of interconnected node sets without compressing each cluster into a single embedding. To enable effective information transfer among these node sets, we propose the Node-to-Cluster Attention (N2C-Attn) mechanism. N2C-Attn incorporates techniques from Multiple Kernel Learning into the kernelized attention framework, effectively capturing information at both node and cluster levels. We then devise an efficient form for N2C-Attn using the cluster-wise message-passing framework, achieving linear time complexity. We further analyze how N2C-Attn combines bi-level feature maps of queries and keys, demonstrating its capability to merge dual-granularity information. The resulting architecture, Cluster-wise Graph Transformer (Cluster-GT), which uses node clusters as tokens and employs our proposed N2C-Attn module, shows superior performance on various graph-level tasks. Code is available at https://github.com/LUMIA-Group/Cluster-wise-Graph-Transformer.||\n", "2410.09040": "|**2024-10-11**|[AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation](http://arxiv.org/abs/2410.09040)|**[link](https://github.com/ucsc-vlaa/attngcg-attack)**|This paper studies the vulnerabilities of transformer-based Large Language Models (LLMs) to jailbreaking attacks, focusing specifically on the optimization-based Greedy Coordinate Gradient (GCG) strategy. We first observe a positive correlation between the effectiveness of attacks and the internal behaviors of the models. For instance, attacks tend to be less effective when models pay more attention to system prompts designed to ensure LLM safety alignment. Building on this discovery, we introduce an enhanced method that manipulates models' attention scores to facilitate LLM jailbreaking, which we term AttnGCG. Empirically, AttnGCG shows consistent improvements in attack efficacy across diverse LLMs, achieving an average increase of ~7% in the Llama-2 series and ~10% in the Gemma series. Our strategy also demonstrates robust attack transferability against both unseen harmful goals and black-box LLMs like GPT-3.5 and GPT-4. Moreover, we note our attention-score visualization is more interpretable, allowing us to gain better insights into how our targeted attention manipulation facilitates more effective jailbreaking. We release the code at https://github.com/UCSC-VLAA/AttnGCG-attack.||\n", "2410.08971": "|**2024-10-11**|[Extra Global Attention Designation Using Keyword Detection in Sparse Transformer Architectures](http://arxiv.org/abs/2410.08971)|null|In this paper, we propose an extension to Longformer Encoder-Decoder, a popular sparse transformer architecture. One common challenge with sparse transformers is that they can struggle with encoding of long range context, such as connections between topics discussed at a beginning and end of a document. A method to selectively increase global attention is proposed and demonstrated for abstractive summarization tasks on several benchmark data sets. By prefixing the transcript with additional keywords and encoding global attention on these keywords, improvement in zero-shot, few-shot, and fine-tuned cases is demonstrated for some benchmark data sets.||\n", "2410.08711": "|**2024-10-11**|[On-Chip Learning via Transformer In-Context Learning](http://arxiv.org/abs/2410.08711)|null|Autoregressive decoder-only transformers have become key components for scalable sequence processing and generation models. However, the transformer's self-attention mechanism requires transferring prior token projections from the main memory at each time step (token), thus severely limiting their performance on conventional processors. Self-attention can be viewed as a dynamic feed-forward layer, whose matrix is input sequence-dependent similarly to the result of local synaptic plasticity. Using this insight, we present a neuromorphic decoder-only transformer model that utilizes an on-chip plasticity processor to compute self-attention. Interestingly, the training of transformers enables them to ``learn'' the input context during inference. We demonstrate this in-context learning ability of transformers on the Loihi 2 processor by solving a few-shot classification problem. With this we emphasize the importance of pretrained models especially their ability to find simple, local, backpropagation free, learning rules enabling on-chip learning and adaptation in a hardware friendly manner.||\n", "2410.08626": "|**2024-10-11**|[Small Tunes Transformer: Exploring Macro & Micro-Level Hierarchies for Skeleton-Conditioned Melody Generation](http://arxiv.org/abs/2410.08626)|null|Recently, symbolic music generation has become a focus of numerous deep learning research. Structure as an important part of music, contributes to improving the quality of music, and an increasing number of works start to study the hierarchical structure. In this study, we delve into the multi-level structures within music from macro-level and micro-level hierarchies. At the macro-level hierarchy, we conduct phrase segmentation algorithm to explore how phrases influence the overall development of music, and at the micro-level hierarchy, we design skeleton notes extraction strategy to explore how skeleton notes within each phrase guide the melody generation. Furthermore, we propose a novel Phrase-level Cross-Attention mechanism to capture the intrinsic relationship between macro-level hierarchy and micro-level hierarchy. Moreover, in response to the current lack of research on Chinese-style music, we construct our Small Tunes Dataset: a substantial collection of MIDI files comprising 10088 Small Tunes, a category of traditional Chinese Folk Songs. This dataset serves as the focus of our study. We generate Small Tunes songs utilizing the extracted skeleton notes as conditions, and experiment results indicate that our proposed model, Small Tunes Transformer, outperforms other state-of-the-art models. Besides, we design three novel objective evaluation metrics to evaluate music from both rhythm and melody dimensions.||\n", "2410.08582": "|**2024-10-11**|[DeBiFormer: Vision Transformer with Deformable Agent Bi-level Routing Attention](http://arxiv.org/abs/2410.08582)|**[link](https://github.com/maclong01/DeBiFormer)**|Vision Transformers with various attention modules have demonstrated superior performance on vision tasks. While using sparsity-adaptive attention, such as in DAT, has yielded strong results in image classification, the key-value pairs selected by deformable points lack semantic relevance when fine-tuning for semantic segmentation tasks. The query-aware sparsity attention in BiFormer seeks to focus each query on top-k routed regions. However, during attention calculation, the selected key-value pairs are influenced by too many irrelevant queries, reducing attention on the more important ones. To address these issues, we propose the Deformable Bi-level Routing Attention (DBRA) module, which optimizes the selection of key-value pairs using agent queries and enhances the interpretability of queries in attention maps. Based on this, we introduce the Deformable Bi-level Routing Attention Transformer (DeBiFormer), a novel general-purpose vision transformer built with the DBRA module. DeBiFormer has been validated on various computer vision tasks, including image classification, object detection, and semantic segmentation, providing strong evidence of its effectiveness.Code is available at {https://github.com/maclong01/DeBiFormer}||\n", "2410.08243": "|**2024-10-10**|[Self-Attention Mechanism in Multimodal Context for Banking Transaction Flow](http://arxiv.org/abs/2410.08243)|null|Banking Transaction Flow (BTF) is a sequential data found in a number of banking activities such as marketing, credit risk or banking fraud. It is a multimodal data composed of three modalities: a date, a numerical value and a wording. We propose in this work an application of self-attention mechanism to the processing of BTFs. We trained two general models on a large amount of BTFs in a self-supervised way: one RNN-based model and one Transformer-based model. We proposed a specific tokenization in order to be able to process BTFs. The performance of these two models was evaluated on two banking downstream tasks: a transaction categorization task and a credit risk task. The results show that fine-tuning these two pre-trained models allowed to perform better than the state-of-the-art approaches for both tasks.||\n", "2410.11842": "|**2024-10-15**|[MoH: Multi-Head Attention as Mixture-of-Head Attention](http://arxiv.org/abs/2410.11842)|**[link](https://github.com/skyworkai/moh)**|In this work, we upgrade the multi-head attention mechanism, the core of the Transformer model, to improve efficiency while maintaining or surpassing the previous accuracy level. We show that multi-head attention can be expressed in the summation form. Drawing on the insight that not all attention heads hold equal significance, we propose Mixture-of-Head attention (MoH), a new architecture that treats attention heads as experts in the Mixture-of-Experts (MoE) mechanism. MoH has two significant advantages: First, MoH enables each token to select the appropriate attention heads, enhancing inference efficiency without compromising accuracy or increasing the number of parameters. Second, MoH replaces the standard summation in multi-head attention with a weighted summation, introducing flexibility to the attention mechanism and unlocking extra performance potential. Extensive experiments on ViT, DiT, and LLMs demonstrate that MoH outperforms multi-head attention by using only 50%-90% of the attention heads. Moreover, we demonstrate that pre-trained multi-head attention models, such as LLaMA3-8B, can be further continue-tuned into our MoH models. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14 benchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the attention heads. We believe the proposed MoH is a promising alternative to multi-head attention and provides a strong foundation for developing advanced and efficient attention-based models.||\n", "2410.11720": "|**2024-10-15**|[Light-Weight Fault Tolerant Attention for Large Language Model Training](http://arxiv.org/abs/2410.11720)|null|Large Language Models (LLMs) have demonstrated remarkable performance in various natural language processing tasks. However, the training of these models is computationally intensive and susceptible to faults, particularly in the attention mechanism, which is a critical component of transformer-based LLMs. In this paper, we investigate the impact of faults on LLM training, focusing on INF, NaN, and near-INF values in the computation results with systematic fault injection experiments. We observe the propagation patterns of these errors, which can trigger non-trainable states in the model and disrupt training, forcing the procedure to load from checkpoints.To mitigate the impact of these faults, we propose ATTNChecker, the first Algorithm-Based Fault Tolerance (ABFT) technique tailored for the attention mechanism in LLMs. ATTNChecker is designed based on fault propagation patterns of LLM and incorporates performance optimization to adapt to both system reliability and model vulnerability while providing lightweight protection for fast LLM training. Evaluations on four LLMs show that ATTNChecker on average incurs on average 7% overhead on training while detecting and correcting all extreme errors. Compared with the state-of-the-art checkpoint/restore approach, ATTNChecker reduces recovery overhead by up to 49x.||\n", "2410.11428": "|**2024-10-15**|[CTA-Net: A CNN-Transformer Aggregation Network for Improving Multi-Scale Feature Extraction](http://arxiv.org/abs/2410.11428)|null|Convolutional neural networks (CNNs) and vision transformers (ViTs) have become essential in computer vision for local and global feature extraction. However, aggregating these architectures in existing methods often results in inefficiencies. To address this, the CNN-Transformer Aggregation Network (CTA-Net) was developed. CTA-Net combines CNNs and ViTs, with transformers capturing long-range dependencies and CNNs extracting localized features. This integration enables efficient processing of detailed local and broader contextual information. CTA-Net introduces the Light Weight Multi-Scale Feature Fusion Multi-Head Self-Attention (LMF-MHSA) module for effective multi-scale feature integration with reduced parameters. Additionally, the Reverse Reconstruction CNN-Variants (RRCV) module enhances the embedding of CNNs within the transformer architecture. Extensive experiments on small-scale datasets with fewer than 100,000 samples show that CTA-Net achieves superior performance (TOP-1 Acc 86.76\\%), fewer parameters (20.32M), and greater efficiency (FLOPs 2.83B), making it a highly efficient and lightweight solution for visual tasks on small-scale datasets (fewer than 100,000).||\n", "2410.11396": "|**2024-10-15**|[Implementing Derivations of Definite Logic Programs with Self-Attention Networks](http://arxiv.org/abs/2410.11396)|null|In this paper we propose that a restricted version of logical inference can be implemented with self-attention networks. We are aiming at showing that LLMs (Large Language Models) constructed with transformer networks can make logical inferences. We would reveal the potential of LLMs by analyzing self-attention networks, which are main components of transformer networks. Our approach is not based on semantics of natural languages but operations of logical inference. %point of view. We show that hierarchical constructions of self-attention networks with feed forward networks (FFNs) can implement top-down derivations for a class of logical formulae. We also show bottom-up derivations are also implemented for the same class. We believe that our results show that LLMs implicitly have the power of logical inference.||\n", "2410.11358": "|**2024-10-15**|[SeaDATE: Remedy Dual-Attention Transformer with Semantic Alignment via Contrast Learning for Multimodal Object Detection](http://arxiv.org/abs/2410.11358)|null|Multimodal object detection leverages diverse modal information to enhance the accuracy and robustness of detectors. By learning long-term dependencies, Transformer can effectively integrate multimodal features in the feature extraction stage, which greatly improves the performance of multimodal object detection. However, current methods merely stack Transformer-guided fusion techniques without exploring their capability to extract features at various depth layers of network, thus limiting the improvements in detection performance. In this paper, we introduce an accurate and efficient object detection method named SeaDATE. Initially, we propose a novel dual attention Feature Fusion (DTF) module that, under Transformer's guidance, integrates local and global information through a dual attention mechanism, strengthening the fusion of modal features from orthogonal perspectives using spatial and channel tokens. Meanwhile, our theoretical analysis and empirical validation demonstrate that the Transformer-guided fusion method, treating images as sequences of pixels for fusion, performs better on shallow features' detail information compared to deep semantic information. To address this, we designed a contrastive learning (CL) module aimed at learning features of multimodal samples, remedying the shortcomings of Transformer-guided fusion in extracting deep semantic features, and effectively utilizing cross-modal information. Extensive experiments and ablation studies on the FLIR, LLVIP, and M3FD datasets have proven our method to be effective, achieving state-of-the-art detection performance.||\n", "2410.11261": "|**2024-10-15**|[Beyond Linear Approximations: A Novel Pruning Approach for Attention Matrix](http://arxiv.org/abs/2410.11261)|null|Large Language Models (LLMs) have shown immense potential in enhancing various aspects of our daily lives, from conversational AI to search and AI assistants. However, their growing capabilities come at the cost of extremely large model sizes, making deployment on edge devices challenging due to memory and computational constraints. This paper introduces a novel approach to LLM weight pruning that directly optimizes for approximating the attention matrix, a core component of transformer architectures. Unlike existing methods that focus on linear approximations, our approach accounts for the non-linear nature of the Softmax attention mechanism. We provide theoretical guarantees for the convergence of our Gradient Descent-based optimization method to a near-optimal pruning mask solution. Our preliminary empirical results demonstrate the effectiveness of this approach in maintaining model performance while significantly reducing computational costs. This work establishes a new theoretical foundation for pruning algorithm design in LLMs, potentially paving the way for more efficient LLM inference on resource-constrained devices.||\n", "2410.11189": "|**2024-10-15**|[Rethinking Graph Transformer Architecture Design for Node Classification](http://arxiv.org/abs/2410.11189)|null|Graph Transformer (GT), as a special type of Graph Neural Networks (GNNs), utilizes multi-head attention to facilitate high-order message passing. However, this also imposes several limitations in node classification applications: 1) nodes are susceptible to global noise; 2) self-attention computation cannot scale well to large graphs. In this work, we conduct extensive observational experiments to explore the adaptability of the GT architecture in node classification tasks and draw several conclusions: the current multi-head self-attention module in GT can be completely replaceable, while the feed-forward neural network module proves to be valuable. Based on this, we decouple the propagation (P) and transformation (T) of GNNs and explore a powerful GT architecture, named GNNFormer, which is based on the P/T combination message passing and adapted for node classification in both homophilous and heterophilous scenarios. Extensive experiments on 12 benchmark datasets demonstrate that our proposed GT architecture can effectively adapt to node classification tasks without being affected by global noise and computational efficiency limitations.||\n", "2410.10986": "|**2024-10-14**|[What Does It Mean to Be a Transformer? Insights from a Theoretical Hessian Analysis](http://arxiv.org/abs/2410.10986)|null|The Transformer architecture has inarguably revolutionized deep learning, overtaking classical architectures like multi-layer perceptrons (MLPs) and convolutional neural networks (CNNs). At its core, the attention block differs in form and functionality from most other architectural components in deep learning -- to the extent that Transformers are often accompanied by adaptive optimizers, layer normalization, learning rate warmup, and more, in comparison to MLPs/CNNs. The root causes behind these outward manifestations, and the precise mechanisms that govern them, remain poorly understood. In this work, we bridge this gap by providing a fundamental understanding of what distinguishes the Transformer from the other architectures -- grounded in a theoretical comparison of the (loss) Hessian. Concretely, for a single self-attention layer, (a) we first entirely derive the Transformer's Hessian and express it in matrix derivatives; (b) we then characterize it in terms of data, weight, and attention moment dependencies; and (c) while doing so further highlight the important structural differences to the Hessian of classical networks. Our results suggest that various common architectural and optimization choices in Transformers can be traced back to their highly non-linear dependencies on the data and weight matrices, which vary heterogeneously across parameters. Ultimately, our findings provide a deeper understanding of the Transformer's unique optimization landscape and the challenges it poses.||\n", "2410.10547": "|**2024-10-14**|[Hybrid Transformer for Early Alzheimer's Detection: Integration of Handwriting-Based 2D Images and 1D Signal Features](http://arxiv.org/abs/2410.10547)|null|Alzheimer's Disease (AD) is a prevalent neurodegenerative condition where early detection is vital. Handwriting, often affected early in AD, offers a non-invasive and cost-effective way to capture subtle motor changes. State-of-the-art research on handwriting, mostly online, based AD detection has predominantly relied on manually extracted features, fed as input to shallow machine learning models. Some recent works have proposed deep learning (DL)-based models, either 1D-CNN or 2D-CNN architectures, with performance comparing favorably to handcrafted schemes. These approaches, however, overlook the intrinsic relationship between the 2D spatial patterns of handwriting strokes and their 1D dynamic characteristics, thus limiting their capacity to capture the multimodal nature of handwriting data. Moreover, the application of Transformer models remains basically unexplored. To address these limitations, we propose a novel approach for AD detection, consisting of a learnable multimodal hybrid attention model that integrates simultaneously 2D handwriting images with 1D dynamic handwriting signals. Our model leverages a gated mechanism to combine similarity and difference attention, blending the two modalities and learning robust features by incorporating information at different scales. Our model achieved state-of-the-art performance on the DARWIN dataset, with an F1-score of 90.32\\% and accuracy of 90.91\\% in Task 8 ('L' writing), surpassing the previous best by 4.61% and 6.06% respectively.||\n", "2410.10442": "|**2024-10-14**|[Domain-Conditioned Transformer for Fully Test-time Adaptation](http://arxiv.org/abs/2410.10442)|**[link](https://github.com/yushuntang/dct)**|Fully test-time adaptation aims to adapt a network model online based on sequential analysis of input samples during the inference stage. We observe that, when applying a transformer network model into a new domain, the self-attention profiles of image samples in the target domain deviate significantly from those in the source domain, which results in large performance degradation during domain changes. To address this important issue, we propose a new structure for the self-attention modules in the transformer. Specifically, we incorporate three domain-conditioning vectors, called domain conditioners, into the query, key, and value components of the self-attention module. We learn a network to generate these three domain conditioners from the class token at each transformer network layer. We find that, during fully online test-time adaptation, these domain conditioners at each transform network layer are able to gradually remove the impact of domain shift and largely recover the original self-attention profile. Our extensive experimental results demonstrate that the proposed domain-conditioned transformer significantly improves the online fully test-time domain adaptation performance and outperforms existing state-of-the-art methods by large margins.||\n", "2410.13835": "|**2024-10-17**|[Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs](http://arxiv.org/abs/2410.13835)|null|Practitioners have consistently observed three puzzling phenomena in transformer-based large language models (LLMs): attention sinks, value-state drains, and residual-state peaks, collectively referred to as extreme-token phenomena. These phenomena are characterized by certain so-called \"sink tokens\" receiving disproportionately high attention weights, exhibiting significantly smaller value states, and having much larger residual-state norms than those of other tokens. These extreme tokens give rise to various challenges in LLM inference, quantization, and interpretability.   We elucidate the mechanisms behind extreme-token phenomena. First, we show that these phenomena arise in very simple architectures -- transformers with one to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task. In this setting, we identify an active-dormant mechanism, where attention heads become sinks for specific input domains while remaining non-sinks for others. Our theoretical analysis of the training dynamics reveals that these phenomena are driven by a mutual reinforcement mechanism. Building on these insights, we propose strategies to mitigate extreme-token phenomena during pretraining, including replacing softmax with ReLU and Adam with SGD. Next, we extend our analysis to pretrained LLMs, including Llama and OLMo, showing that many attention heads exhibit a similar active-dormant mechanism as in the BB task, and that the mutual reinforcement mechanism also governs the emergence of extreme-token phenomena during LLM pretraining. Our results reveal that many of the static and dynamic properties of extreme-token phenomena predicted by the BB task align with observations in pretrained LLMs.||\n", "2410.13732": "|**2024-10-17**|[Reducing the Transformer Architecture to a Minimum](http://arxiv.org/abs/2410.13732)|null|Transformers are a widespread and successful model architecture, particularly in Natural Language Processing (NLP) and Computer Vision (CV). The essential innovation of this architecture is the Attention Mechanism, which solves the problem of extracting relevant context information from long sequences in NLP and realistic scenes in CV. A classical neural network component, a Multi-Layer Perceptron (MLP), complements the attention mechanism. Its necessity is frequently justified by its capability of modeling nonlinear relationships. However, the attention mechanism itself is nonlinear through its internal use of similarity measures. A possible hypothesis is that this nonlinearity is sufficient for modeling typical application problems. As the MLPs usually contain the most trainable parameters of the whole model, their omission would substantially reduce the parameter set size. Further components can also be reorganized to reduce the number of parameters. Under some conditions, query and key matrices can be collapsed into a single matrix of the same size. The same is true about value and projection matrices, which can also be omitted without eliminating the substance of the attention mechanism. Initially, the similarity measure was defined asymmetrically, with peculiar properties such as that a token is possibly dissimilar to itself. A possible symmetric definition requires only half of the parameters. We have laid the groundwork by testing widespread CV benchmarks: MNIST and CIFAR-10. The tests have shown that simplified transformer architectures (a) without MLP, (b) with collapsed matrices, and (c) symmetric similarity matrices exhibit similar performance as the original architecture, saving up to 90% of parameters without hurting the classification performance.||\n", "2410.13663": "|**2024-10-17**|[DiRecNetV2: A Transformer-Enhanced Network for Aerial Disaster Recognition](http://arxiv.org/abs/2410.13663)|null|The integration of Unmanned Aerial Vehicles (UAVs) with artificial intelligence (AI) models for aerial imagery processing in disaster assessment, necessitates models that demonstrate exceptional accuracy, computational efficiency, and real-time processing capabilities. Traditionally Convolutional Neural Networks (CNNs), demonstrate efficiency in local feature extraction but are limited by their potential for global context interpretation. On the other hand, Vision Transformers (ViTs) show promise for improved global context interpretation through the use of attention mechanisms, although they still remain underinvestigated in UAV-based disaster response applications. Bridging this research gap, we introduce DiRecNetV2, an improved hybrid model that utilizes convolutional and transformer layers. It merges the inductive biases of CNNs for robust feature extraction with the global context understanding of Transformers, maintaining a low computational load ideal for UAV applications. Additionally, we introduce a new, compact multi-label dataset of disasters, to set an initial benchmark for future research, exploring how models trained on single-label data perform in a multi-label test set. The study assesses lightweight CNNs and ViTs on the AIDERSv2 dataset, based on the frames per second (FPS) for efficiency and the weighted F1 scores for classification performance. DiRecNetV2 not only achieves a weighted F1 score of 0.964 on a single-label test set but also demonstrates adaptability, with a score of 0.614 on a complex multi-label test set, while functioning at 176.13 FPS on the Nvidia Orin Jetson device.||\n", "2410.13566": "|**2024-10-17**|[360U-Former: HDR Illumination Estimation with Panoramic Adapted Vision Transformers](http://arxiv.org/abs/2410.13566)|null|Recent illumination estimation methods have focused on enhancing the resolution and improving the quality and diversity of the generated textures. However, few have explored tailoring the neural network architecture to the Equirectangular Panorama (ERP) format utilised in image-based lighting. Consequently, high dynamic range images (HDRI) results usually exhibit a seam at the side borders and textures or objects that are warped at the poles. To address this shortcoming we propose a novel architecture, 360U-Former, based on a U-Net style Vision-Transformer which leverages the work of PanoSWIN, an adapted shifted window attention tailored to the ERP format. To the best of our knowledge, this is the first purely Vision-Transformer model used in the field of illumination estimation. We train 360U-Former as a GAN to generate HDRI from a limited field of view low dynamic range image (LDRI). We evaluate our method using current illumination estimation evaluation protocols and datasets, demonstrating that our approach outperforms existing and state-of-the-art methods without the artefacts typically associated with the use of the ERP format.||\n", "2410.13314": "|**2024-10-17**|[Precipitation Nowcasting Using Diffusion Transformer with Causal Attention](http://arxiv.org/abs/2410.13314)|null|Short-term precipitation forecasting remains challenging due to the difficulty in capturing long-term spatiotemporal dependencies. Current deep learning methods fall short in establishing effective dependencies between conditions and forecast results, while also lacking interpretability. To address this issue, we propose a Precipitation Nowcasting Using Diffusion Transformer with Causal Attention model. Our model leverages Transformer and combines causal attention mechanisms to establish spatiotemporal queries between conditional information (causes) and forecast results (results). This design enables the model to effectively capture long-term dependencies, allowing forecast results to maintain strong causal relationships with input conditions over a wide range of time and space. We explore four variants of spatiotemporal information interactions for DTCA, demonstrating that global spatiotemporal labeling interactions yield the best performance. In addition, we introduce a Channel-To-Batch shift operation to further enhance the model's ability to represent complex rainfall dynamics. We conducted experiments on two datasets. Compared to state-of-the-art U-Net-based methods, our approach improved the CSI (Critical Success Index) for predicting heavy precipitation by approximately 15% and 8% respectively, achieving state-of-the-art performance.||\n", "2410.13288": "|**2024-10-17**|[DurIAN-E 2: Duration Informed Attention Network with Adaptive Variational Autoencoder and Adversarial Learning for Expressive Text-to-Speech Synthesis](http://arxiv.org/abs/2410.13288)|null|This paper proposes an improved version of DurIAN-E (DurIAN-E 2), which is also a duration informed attention neural network for expressive and high-fidelity text-to-speech (TTS) synthesis. Similar with the DurIAN-E model, multiple stacked SwishRNN-based Transformer blocks are utilized as linguistic encoders and Style-Adaptive Instance Normalization (SAIN) layers are also exploited into frame-level encoders to improve the modeling ability of expressiveness in the proposed the DurIAN-E 2. Meanwhile, motivated by other TTS models using generative models such as VITS, the proposed DurIAN-E 2 utilizes variational autoencoders (VAEs) augmented with normalizing flows and a BigVGAN waveform generator with adversarial training strategy, which further improve the synthesized speech quality and expressiveness. Both objective test and subjective evaluation results prove that the proposed expressive TTS model DurIAN-E 2 can achieve better performance than several state-of-the-art approaches besides DurIAN-E.||\n", "2410.13166": "|**2024-10-17**|[An Evolved Universal Transformer Memory](http://arxiv.org/abs/2410.13166)|**[link](https://github.com/sakanaai/evo-memory)**|Prior methods propose to offset the escalating costs of modern foundation models by dropping specific parts of their contexts with hand-designed rules, while attempting to preserve their original performance. We overcome this trade-off with Neural Attention Memory Models (NAMMs), introducing a learned network for memory management that improves both the performance and efficiency of transformers. We evolve NAMMs atop pre-trained transformers to provide different latent contexts focusing on the most relevant information for individual layers and attention heads.NAMMs are universally applicable to any model using self-attention as they condition exclusively on the values in the produced attention matrices. Learning NAMMs on a small set of problems, we achieve substantial performance improvements across multiple long-context benchmarks while cutting the model's input contexts up to a fraction of the original sizes. We show the generality of our conditioning enables zero-shot transfer of NAMMs trained only on language to entirely new transformer architectures even across input modalities, with their benefits carrying over to vision and reinforcement learning.||\n", "2410.12675": "|**2024-10-16**|[SWIM: An Attention-Only Model for Speech Quality Assessment Under Subjective Variance](http://arxiv.org/abs/2410.12675)|null|Speech quality is best evaluated by human feedback using mean opinion scores (MOS). However, variance in ratings between listeners can introduce noise in the true quality label of an utterance. Currently, deep learning networks including convolutional, recurrent, and attention-based architectures have been explored for quality estimation. This paper proposes an exclusively attention-based model involving a Swin Transformer for MOS estimation (SWIM). Our network captures local and global dependencies that reflect the acoustic properties of an utterance. To counteract subjective variance in MOS labels, we propose a normal distance-based objective that accounts for standard deviation in each label, and we avail a multistage self-teaching strategy to improve generalization further. Our model is significantly more compact than existing attention-based networks for quality estimation. Finally, our experiments on the Samsung Open Mean Opinion Score (SOMOS) dataset show improvement over existing baseline models when trained from scratch.||\n", "2410.12184": "|**2024-10-16**|[ExoTST: Exogenous-Aware Temporal Sequence Transformer for Time Series Prediction](http://arxiv.org/abs/2410.12184)|null|Accurate long-term predictions are the foundations for many machine learning applications and decision-making processes. Traditional time series approaches for prediction often focus on either autoregressive modeling, which relies solely on past observations of the target ``endogenous variables'', or forward modeling, which considers only current covariate drivers ``exogenous variables''. However, effectively integrating past endogenous and past exogenous with current exogenous variables remains a significant challenge. In this paper, we propose ExoTST, a novel transformer-based framework that effectively incorporates current exogenous variables alongside past context for improved time series prediction. To integrate exogenous information efficiently, ExoTST leverages the strengths of attention mechanisms and introduces a novel cross-temporal modality fusion module. This module enables the model to jointly learn from both past and current exogenous series, treating them as distinct modalities. By considering these series separately, ExoTST provides robustness and flexibility in handling data uncertainties that arise from the inherent distribution shift between historical and current exogenous variables. Extensive experiments on real-world carbon flux datasets and time series benchmarks demonstrate ExoTST's superior performance compared to state-of-the-art baselines, with improvements of up to 10\\% in prediction accuracy. Moreover, ExoTST exhibits strong robustness against missing values and noise in exogenous drivers, maintaining consistent performance in real-world situations where these imperfections are common.||\n"}}