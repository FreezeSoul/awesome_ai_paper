---
layout: default
---

## Updated on 2024.07.10
> Usage instructions: [here](./docs/README.md#usage)

## 多模态

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-07-09**|[Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model](http://arxiv.org/abs/2407.07053)|**[link](https://github.com/zwq2018/multi-modal-self-instruct)**|虽然目前大多数大型多模态模型 (LMM) 已经可以理解自然场景和肖像照片，但它们对抽象图像（例如图表、地图或布局）的理解和视觉推理能力仍然相当初级。它们经常难以完成简单的日常任务，例如从时钟读取时间、理解流程图或使用路线图规划路线。鉴于此，我们设计了一种多模态自指示方法，利用大型语言模型及其代码能力，在日常场景中合成大量抽象图像和视觉推理指令。我们的策略轻松创建了一个包含 11,193 条指令的多模态基准测试，涵盖八个视觉场景：图表、表格、模拟地图、仪表盘、流程图、关系图、平面图和视觉谜题。这个由简单的线条和几何元素构建的基准测试暴露了大多数先进的 LMM（如 Claude-3.5-Sonnet 和 GPT-4o）在抽象图像理解、空间关系推理和视觉元素归纳方面的不足。此外，为了验证我们合成数据的质量，我们使用 62,476 条合成的图表、表格和路线图指令微调了一个 LMM。结果表明，图表理解和地图导航性能有所提高，也显示出对其他视觉推理任务的潜在好处。我们的代码可在以下网址获得：https://github.com/zwq2018/Multi-modal-Self-instruct。|
|**2024-07-09**|[Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action Localization](http://arxiv.org/abs/2407.07024)|**[link](https://github.com/hyunjs/stov-tal)**|时序动作定位 (TAL) 中的词汇量受到大规模标注数据集稀缺性的限制。为了解决这个问题，最近的研究结合了强大的预训练视觉语言模型 (VLM)，例如 CLIP，来执行开放词汇表 TAL (OV-TAL)。然而，与在大量图像/视频-文本对上训练的 VLM 不同，现有的 OV-TAL 方法仍然依赖于小型、完全标记的 TAL 数据集来训练动作定位器。在本文中，我们探索了使用未标记的 YouTube 视频进行 OV-TAL 自训练的可扩展性。我们的自训练方法包括两个阶段。首先，在人工标记的 TAL 数据集上训练一个类别无关的动作定位器，并使用它为未标记的视频生成伪标签。其次，将大规模伪标签数据集与人工标记的数据集相结合，以训练定位器。大量实验表明，在自训练中利用网络规模的视频可以显着增强动作定位器的泛化能力。此外，我们重点介绍了现有 OV-TAL 评估方案中存在的问题，并提出了一种新的评估方案。代码发布在 https://github.com/HYUNJS/STOV-TAL|
|**2024-07-09**|[CoLA: Conditional Dropout and Language-driven Robust Dual-modal Salient Object Detection](http://arxiv.org/abs/2407.06780)|**[link](https://github.com/ssecv/CoLA)**|深度/热信息有利于使用传统RGB图像检测显著性目标。然而，在双模态显著性目标检测（SOD）模型中，针对噪声输入和模态缺失的鲁棒性至关重要，但很少被研究。为了解决这个问题，我们引入了条件Dropout和语言驱动（CoLA）框架，该框架包含两个核心组件。1）语言驱动质量评估（LQA）：利用带有提示学习器的预训练视觉语言模型，LQA在不需要额外质量标注的情况下重新校准图像贡献。这种方法有效地减轻了噪声输入的影响。2）条件Dropout（CD）：一种学习方法，用于增强模型在模态缺失场景下的适应性，同时保持其在完整模态下的性能。CD作为一种插件式训练方案，将模态缺失视为条件，增强了各种双模态SOD模型的整体鲁棒性。大量实验表明，所提出的方法在模态完整和模态缺失条件下均优于最先进的双模态SOD模型。代码将在论文被接收后开源。|
|**2024-07-09**|[LVLM-empowered Multi-modal Representation Learning for Visual Place Recognition](http://arxiv.org/abs/2407.06730)|null|视觉位置识别 (VPR) 由于视角变化和外观变化很大，因此仍然具有挑战性。主流工作通过开发各种特征聚合方法将深度特征转换为稳健而紧凑的全局表示来应对这些挑战。不幸的是，在具有挑战性的条件下无法获得令人满意的结果。我们从一个新的角度出发，尝试通过融合图像数据和视觉场景的文本描述来构建具有判别性的全局表示。动机有两个：（1）当前的大型视觉语言模型 (LVLM) 在视觉指令跟随方面表现出非凡的涌现能力，因此提供了一种高效灵活的图像文本描述生成方式；（2）文本描述提供了对场景的高级理解，对环境变化表现出很强的鲁棒性。尽管很有前景，但利用 LVLM 构建多模态 VPR 解决方案在高效的多模态融合方面仍然具有挑战性。此外，LVLM 不可避免地会产生一些不准确的描述，这使得情况变得更加困难。为了应对这些挑战，我们提出了一种新颖的多模态 VPR 解决方案。它首先使预训练的视觉和语言基础模型适应 VPR，以提取图像和文本特征，然后将这些特征输入特征组合器以相互增强。作为主要组件，特征组合器首先提出了一个逐符号注意力块，以根据文本符号与图像数据的相关性自适应地重新校准文本符号，然后开发了一个高效的交叉注意力融合模块，以在不同模态之间传播信息。增强的多模态特征被压缩到特征描述符中以执行检索。实验结果表明，我们的方法在图像描述符维度明显较小的情况下，大大优于最先进的方法。|
|**2024-07-08**|[A Single Transformer for Scalable Vision-Language Modeling](http://arxiv.org/abs/2407.06438)|**[link](https://github.com/yangyi-chen/solo)**|我们提出了 SOLO，一个用于可扩展视觉语言建模的单一 Transformer 模型。目前的大型视觉语言模型 (LVLM)，例如 LLaVA，大多采用异构架构，将预训练的视觉编码器与大型语言模型 (LLM) 连接起来，以促进视觉识别和复杂推理。虽然通过相对轻量级的训练获得了显著的性能，但我们发现了四个主要的扩展性限制：(1) 视觉能力受到预训练视觉编码器的限制，这些编码器通常比 LLM 小一个数量级。(2) 异构架构使已建立的硬件和软件基础设施的使用变得复杂。(3) 对这种架构进行规模法则研究必须考虑三个独立的组件——视觉编码器、连接器和 LLM，这使得分析变得复杂。(4) 使用现有的视觉编码器通常需要遵循预定义的图像输入预处理规范，例如，通过将输入整形为固定分辨率的方形图像，这在处理和训练高分辨率图像或具有不寻常纵横比的图像时会遇到困难。像 SOLO 这样的统一单一 Transformer 架构有效地解决了 LVLMs 中的这些可扩展性问题；然而，它在现代环境中的有限采用可能是由于缺乏可靠的训练方法来平衡两种模态并确保数十亿级模型的稳定训练。在本文中，我们介绍了第一个用于开发 SOLO 的开源训练方法，SOLO 是一个使用中等学术资源的开源 7B LVLM。训练方法包括从 LLM 初始化、在 ImageNet 和网络规模数据上进行顺序预训练，以及在我们策划的高质量数据集上进行指令微调。在广泛的评估中，SOLO 表现出与 LLaVA-v1.5-7B 相当的性能，尤其是在视觉数学推理方面表现出色。|
|**2024-07-08**|[Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision](http://arxiv.org/abs/2407.06189)|**[link](https://github.com/orrzohar/Video-STaR)**|大型视觉语言模型 (LVLM) 的性能取决于其训练数据集的规模和质量。现有的视频指令调整数据集缺乏多样性，因为它们是通过提示大型语言模型使用视频字幕生成问答对而得出的，因此大多是描述性的。同时，存在许多具有不同标签和监督的标记视频数据集——然而，我们发现将它们集成到 LVLM 中并非易事。在此，我们提出了使用增强推理的视频自训练 (Video-STaR)，这是第一个视频自训练方法。Video-STaR 允许利用任何标记的视频数据集进行视频指令调整。在 Video-STaR 中，LVLM 在指令生成和微调之间循环，我们证明这 (I) 提高了一般视频理解能力，并且 (II) 使 LVLM 能够适应现有监督下的新型下游任务。在生成过程中，LVLM 被提示提出答案。然后过滤答案，只保留包含原始视频标签的答案，然后在生成的数据集上重新训练 LVLM。通过仅对包含正确视频标签的生成答案进行训练，Video-STaR 利用这些现有的视频标签作为视频指令调整的弱监督。我们的结果表明，经过 Video-STaR 增强后的 LVLM 在 (I) 常规视频问答（TempCompass 性能提高了 10%）和 (II) 下游任务（Video-STaR 将 Kinetics700-QA 的准确率提高了 20%，并将 FineDiving 上的动作质量评估提高了 15%）中均表现出更好的性能。|
|**2024-07-09**|[HyCIR: Boosting Zero-Shot Composed Image Retrieval with Synthetic Labels](http://arxiv.org/abs/2407.05795)|null|组合图像检索 (CIR) 旨在根据带有文本的查询图像检索图像。当前的零样本 CIR (ZS-CIR) 方法试图在不使用昂贵的三元组标记训练数据集的情况下解决 CIR 任务。然而，ZS-CIR 和三元组监督 CIR 之间的差距仍然很大。在这项工作中，我们提出了混合 CIR (HyCIR)，它使用合成标签来提高 ZS-CIR 的性能。提出了一种新的 CIR 标签合成流程 (SynCir)，其中只需要未标记的图像。首先，根据视觉相似度提取图像对。其次，基于视觉语言模型和 LLM 为每个图像对生成查询文本。第三，基于语义相似度在语言空间中进一步过滤数据。为了提高 ZS-CIR 的性能，我们提出了一种混合训练策略，可以同时使用 ZS-CIR 监督和合成 CIR 三元组。采用了两种对比学习方法。一种是使用大规模未标记图像数据集来学习具有良好泛化能力的图像到文本映射。另一种是使用合成的 CIR 三元组来学习 CIR 任务的更好映射。我们的方法在常见的 CIR 基准测试：CIRR 和 CIRCO 上实现了最先进的零样本性能。|
|**2024-07-07**|[Multimodal Language Models for Domain-Specific Procedural Video Summarization](http://arxiv.org/abs/2407.05419)|null|视频是一种强大的媒介，可以通过长格式教程传达思想、讲述故事和提供详细的说明。此类教程对于按照自己的节奏学习新技能非常有价值，但由于其长度和密集的内容，可能会让人不知所措。观众经常会寻找特定信息，例如精确的测量值或分步执行细节，因此必须有效地提取和总结关键片段。人们非常需要一个能够总结和检测长视频中的亮点的智能、时间敏感的视频助手。多模态大型语言模型的最新进展为开发此类助手提供了有希望的解决方案。我们的研究探索了使用多模态模型来增强特定领域内的视频摘要和分步指令生成。这些模型需要理解跨视频帧的动作之间的时间事件和关系。我们的方法侧重于微调 TimeChat，以提高其在特定领域（烹饪和医疗程序）中的性能。通过在特定领域的数据集（如烹饪领域的 Tasty 和医疗程序领域的 MedVidQA）上训练模型，我们旨在增强其生成简洁、准确的教学视频摘要的能力。我们整理并重构了这些数据集，以创建高质量的以视频为中心的指令数据。我们的研究结果表明，当在特定领域的程序数据上进行微调时，TimeChat 可以显着改善长格式视频中关键指令步骤的提取和总结。这项研究证明了专门的多模式模型通过提供针对每个领域的独特方面量身定制的个性化分步指导来协助完成实际任务的潜力。|
|**2024-07-07**|[Multimodal Prompt Learning with Missing Modalities for Sentiment Analysis and Emotion Recognition](http://arxiv.org/abs/2407.05374)|**[link](https://github.com/zrguo/MPLMM)**|多模态模型的发展显著推进了多模态情感分析和情绪识别。然而，在现实应用中，各种缺失模态情况的存在常常导致模型性能下降。本文提出了一种新颖的使用提示学习的多模态Transformer框架来解决模态缺失问题。我们的方法引入了三种类型的提示：生成提示、缺失信号提示和缺失类型提示。这些提示能够生成缺失的模态特征，并促进模态内和模态间信息的学习。通过提示学习，我们实现了可训练参数数量的大幅减少。我们提出的方法在所有评估指标上都明显优于其他方法。大量的实验和消融研究证明了我们方法的有效性和鲁棒性，展示了其有效处理缺失模态的能力。|
|**2024-07-07**|[WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks](http://arxiv.org/abs/2407.05291)|**[link](https://github.com/servicenow/workarena)**|大型语言模型 (LLM) 模仿人类智能的能力导致了基于 LLM 的自主代理的激增。尽管最近的 LLM 似乎能够根据用户指令进行计划和推理，但它们将这些能力应用于自主任务解决的有效性仍未得到充分探索。在企业环境中尤其如此，因为自动化代理有望产生重大影响。为了填补这一空白，我们提出了 WorkArena++，这是一个包含 682 个任务的新基准，这些任务对应于知识工作者日常执行的现实工作流程。WorkArena++ 旨在评估 Web 代理的计划、解决问题、逻辑/算术推理、检索和上下文理解能力。我们对最先进的 LLM 和视觉语言模型 (VLM) 以及人类工作者的实证研究表明，此类模型要成为工作场所中有用的助手面临着若干挑战。除了基准之外，我们还提供了一种机制，可以毫不费力地生成数千个真实观察/行动轨迹，这些轨迹可用于微调现有模型。总的来说，我们希望这项工作能够成为帮助社区朝着有能力的自主代理方向发展的一种有用资源。该基准可以在 https://github.com/ServiceNow/WorkArena/tree/workarena-plus-plus 找到。|
|**2024-07-05**|[Multimodal Classification via Modal-Aware Interactive Enhancement](http://arxiv.org/abs/2407.04587)|null|由于存在臭名昭著的模态不平衡问题，多模态学习（MML）会导致优化不平衡现象，从而难以达到令人满意的性能。最近，一些具有代表性的方法被提出用于提高性能，主要集中在自适应调整每个模态的优化，以重新平衡主导模态和非主导模态的学习速度。为了更好地促进多模态学习中模型信息的交互，在本文中，我们提出了一种新的多模态学习方法，称为模态感知交互增强（MIE）。具体来说，我们首先利用基于锐度感知最小化（SAM）的优化策略在前向阶段平滑学习目标。然后，借助SAM的几何特性，我们提出了一种梯度修正策略，在反向阶段施加不同模态之间的影响。因此，我们可以提高泛化能力，同时缓解多模态学习中的模态遗忘现象。在广泛使用的数据集上进行的大量实验表明，我们提出的方法可以优于各种最先进的基线，以实现最佳性能。|
|**2024-07-04**|[MiniGPT-Med: Large Language Model as a General Interface for Radiology Diagnosis](http://arxiv.org/abs/2407.04106)|null|近年来，人工智能 (AI) 的快速发展为医疗保健领域带来了重大的突破，特别是在诊断程序的改进方面。然而，以往的研究往往局限于有限的功能。本研究介绍了 MiniGPT-Med，这是一个源于大规模语言模型并专为医疗应用而设计的视觉语言模型。MiniGPT-Med 在各种成像模式（包括 X 光、CT 扫描和 MRI）中均表现出非凡的多功能性，从而增强了其实用性。该模型能够执行医学报告生成、视觉问答 (VQA) 以及医学图像疾病识别等任务。它对图像和文本临床数据的集成处理显著提高了诊断准确性。我们的实证评估证实，MiniGPT-Med 在疾病定位、医学报告生成和 VQA 基准测试中均表现出色，这标志着在缩小放射学实践辅助差距方面迈出了重要一步。此外，它在医学报告生成方面达到了最先进的性能，比之前的最佳模型提高了19%的准确率。MiniGPT-Med 有望成为放射学诊断的通用接口，从而提高各种医学影像应用的诊断效率。|
|**2024-07-04**|[Fully Fine-tuned CLIP Models are Efficient Few-Shot Learners](http://arxiv.org/abs/2407.04003)|null|提示调优通过训练一小部分参数，可以有效地增强预训练视觉语言模型 (VLM) 在下游任务上的性能。然而，当将调优后的模型应用于不同的数据集或领域时，它们往往会牺牲灵活性和适应性。在本文中，我们探索了通过精细微调整个 VLM 来捕获特定任务信息的可能性，同时最大限度地减少参数调整。在有限的监督下对特定任务进行整个 VLM 微调时，过拟合和灾难性遗忘成为事实上的因素。为了缓解这些问题，我们提出了一个名为 CLIP-CITE 的框架，通过设计一个判别性的视觉-文本任务，进一步以监督的方式对齐视觉-文本语义，并集成知识蒸馏技术来保留获得的知识。在少样本学习、基础到新泛化、域泛化和跨域泛化设置下的广泛实验结果表明，我们的方法在有限监督下有效地提高了特定任务的性能，同时保留了 VLM 在其他数据集上的通用性。|
|**2024-07-04**|[Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks](http://arxiv.org/abs/2407.03967)|**[link](https://github.com/amitkparekh/cogelot)**|仅根据多模态模型在分布外数据上的性能来评估其泛化能力，无法捕捉其真正的鲁棒性。本研究引入了一个全面的评估框架，系统地检验了指令和输入在这些模型泛化能力中的作用，并考虑了架构设计、跨语言和视觉模态的输入扰动以及任务复杂性的增加。所提出的框架揭示了多模态模型对极端指令扰动的弹性和它们对观察变化的脆弱性，引发了对过度拟合虚假相关性的担忧。通过在当前基于 Transformer 的机器人操作任务多模态模型上应用此评估框架，我们发现了局限性，并建议未来的改进应侧重于架构和训练创新，以更好地整合多模态输入，通过优先考虑对输入内容的敏感性而不是偶然的相关性来增强模型的泛化能力。|
|**2024-07-04**|[Concept Bottleneck Models Without Predefined Concepts](http://arxiv.org/abs/2407.03921)|null|近年来，可解释的概念型模型，如概念瓶颈模型 (CBM)，引起了人们的广泛兴趣。这类模型首先预测人类可解释的概念，然后将这些概念映射到输出类别。为了减少对人工标注概念的依赖，最近的研究工作已将预训练的黑盒模型后验地转换为可解释的 CBM。然而，这些方法预先定义了一组概念，假设黑盒模型在其表示中编码了哪些概念。在这项工作中，我们通过利用无监督概念发现来自动提取概念，从而消除了这一假设，无需人工标注或预定义的概念集。我们进一步引入了一种依赖于输入的概念选择机制，以确保在所有类别中仅使用一小部分概念。我们证明，我们的方法提高了下游性能，并缩小了与黑盒模型的性能差距，同时在分类中使用的概念要少得多。最后，我们演示了大型视觉语言模型如何干预最终的模型权重以纠正模型错误。|
|**2024-07-04**|[M $\mathbf5$ -- A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks](http://arxiv.org/abs/2407.03791)|null|自ChatGPT发布以来，自然语言处理领域经历了快速发展，特别是在大型语言模型（LLM）及其多模态对应物大型多模态模型（LMM）方面。尽管LLM具有令人印象深刻的能力，但正如各种纯文本基准测试所证明的那样，LLM在不同语言和文化背景下 often 表现出显著的性能差异。然而，目前的研究缺乏针对多模态视觉语言环境的此类基准。为了弥补这一差距，本研究引入了M5，这是第一个旨在评估多语言和多文化背景下不同视觉语言任务的LMM的综合基准。M5包括涵盖五个任务和41种语言的八个数据集，重点关注代表性不足的语言和文化多样化的图像。此外，我们还介绍了两个新的数据集，M5-VGR和M5-VLOD，其中包括一项新的视觉语言异常检测任务，在该任务中，所有评估的开源模型都未能显著超过随机基线。通过广泛的评估和分析，我们重点强调了资源丰富语言和资源匮乏语言之间存在巨大的、与任务无关的性能差异。此外，我们还发现，在多语言环境中，更大的模型不一定优于较小的模型。|
|**2024-07-04**|[Meta-optimized Angular Margin Contrastive Framework for Video-Language Representation Learning](http://arxiv.org/abs/2407.03788)|null|数据质量是决定视频-语言表示学习效果的首要因素。然而，以往数据中的视频-文本对通常不能完美对齐，这可能导致视频-语言表示不能准确反映跨模态语义。此外，以往数据还存在概念分布不均匀的问题，从而影响了在不受欢迎主题上的下游性能。为了解决这些问题，我们提出了一个带有减法角度边际的对比目标函数，以规范跨模态表示，使其达到完美的相似性。此外，为了适应非均匀的概念分布，我们提出了一个多层感知器（MLP）参数化的加权函数，将损失值映射到样本权重，从而能够在整个训练过程中动态调整模型的关注点。在少量无偏元数据的指导下，并通过大型视觉-语言模型生成的视频-文本数据进行增强，我们改进了视频-语言表示，并在常用的视频问答和文本-视频检索数据集上取得了优异的性能。|
|**2024-07-04**|[Visualizing Dialogues: Enhancing Image Selection through Dialogue Understanding with Large Language Models](http://arxiv.org/abs/2407.03615)|null|近年来，对话系统的进步凸显了整合多模态响应的重要性，这种响应能够通过多种模态来传达信息，而不仅仅依赖于基于文本的交互。这种丰富性不仅提高了整体的交流效率，还增强了对话体验的质量。然而，现有的对话到图像检索方法由于预训练视觉语言模型 (VLM) 在准确理解复杂对话方面的局限性而面临挑战。为了解决这个问题，我们提出了一种新方法，利用大型语言模型 (LLM) 强大的推理能力来生成精确的对话相关视觉描述符，从而促进与图像的无缝连接。在基准数据上进行的大量实验验证了我们提出的方法在提取简洁准确的视觉描述符方面的有效性，从而显著提高了对话到图像检索的性能。此外，我们的研究结果证明了该方法在不同视觉线索、各种 LLM 和不同数据集上的泛化能力，突出了其在实际应用中的实用性和潜在影响。|
|**2024-07-04**|[Lateralization LoRA: Interleaved Instruction Tuning with Modality-Specialized Adaptations](http://arxiv.org/abs/2407.03604)|null|视觉语言模型 (VLM) 的最新进展导致了能够理解和生成交错图像和文本的视觉语言通用模型 (VLG) 的发展。尽管取得了这些进步，但 VLG 在遵循用户指令进行交错文本和图像生成方面仍然存在困难。为了解决这个问题，我们引入了 LeafInstruct，这是第一个开源的交错指令调整数据，包含跨 10 多个领域的 30,000 多个高质量实例。由于现有 VLG 的规模庞大，我们选择进行参数高效的调整。然而，我们观察到使用标准 LoRA 调整的 VLG 通常在交错文本图像生成中表现出较差的性能。我们将此问题归因于模态干扰和缺乏模态专用适应性设计。因此，我们提出了一种受大脑偏侧化概念启发的新型模态专用适应方法——Lateralization LoRA。Lateralization LoRA 采用混合方法，结合了传统的线性 LoRA 和用于生成文本和图像的卷积 LoRA，通过利用模态特定的结构和参数集来生成高质量的文本和图像。我们使用 LeafInstruct 数据集对 VLG（即 EMU2）进行 Lateralization LoRA 指令调整。大量实验表明，使用 Lateralization LoRA 调整的 EMU2 实现了最先进的性能，在复杂的交错任务中明显优于基线模型。|
|**2024-07-03**|[HEMM: Holistic Evaluation of Multimodal Foundation Models](http://arxiv.org/abs/2407.03418)|**[link](https://github.com/pliang279/hemm)**|能够全面处理文本、图像、视频、音频和其他感官模态的多模态基础模型正越来越多地应用于各种现实应用中。然而，考虑到可能存在的各种建模决策、任务和领域，描述和研究多模态基础模型的进展具有挑战性。在本文中，我们介绍了多模态模型的整体评估 (HEMM)，以系统地评估多模态基础模型在一组 3 个维度上的能力：基本技能、信息流和现实用例。基本的多模态技能是解决问题所需的内部能力，例如学习跨模态的交互、细粒度对齐、多步骤推理以及处理外部知识的能力。信息流研究多模态内容在任务期间如何通过查询、翻译、编辑和融合发生变化。用例涵盖了现实世界多媒体、情感计算、自然科学、医疗保健和人机交互应用中引入的特定领域挑战。通过对 HEMM 中 30 个任务的全面实验，我们 (1) 确定了对当今模型构成挑战的关键数据集维度（例如，基本技能、信息流和用例），以及 (2) 提炼了关于不同建模维度（例如，规模、预训练数据、多模态对齐、预训练和指令微调目标）如何影响性能的性能趋势。我们关于具有挑战性的多模态交互、用例以及需要推理和外部知识的任务、数据和模型规模的好处以及指令微调的影响的结论，为多模态基础模型的未来工作提供了可操作的见解。|
|**2024-07-03**|[Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation](http://arxiv.org/abs/2407.03056)|**[link](https://github.com/miccunifi/kdpl)**|视觉语言模型 (VLM) 在未见过的任务上表现出非凡的零样本泛化能力，但在有限数据下泛化到下游任务的性能不如监督方法。提示学习正在成为一种参数高效的 VLM 自适应方法，但最先进的方法需要带注释的样本。在本文中，我们提出了一种基于无监督知识蒸馏的新型提示学习方法，该方法从更强大的模型中提取知识。我们的方法称为知识蒸馏提示学习 (KDPL)，可以集成到现有的提示学习技术中，并消除了适应过程中对标记示例的需求。我们对十多个标准基准数据集进行的实验表明，KDPL 在提高学习提示的泛化能力方面非常有效，可以解决零样本域泛化、零样本跨数据集泛化和零样本基础到新类泛化问题。KDPL 不需要用于适应的基本事实标签，此外，我们还表明，即使在没有任何训练类名知识的情况下，它也可以用于有效地迁移知识。代码可在 https://github.com/miccunifi/KDPL 公开获取。|
|**2024-07-03**|[SAFT: Towards Out-of-Distribution Generalization in Fine-Tuning](http://arxiv.org/abs/2407.03036)|null|在机器学习领域，处理训练数据中的分布变化，即所谓的分布外 (OOD) 泛化，是一项重大挑战。虽然像 CLIP 这样的预训练视觉语言模型已经展现出卓越的零样本性能，但模型对下游任务的进一步适应会导致 OOD 数据出现不良的性能下降。在这项工作中，我们引入了用于微调的稀疏适应 (SAFT) 方法，该方法可以防止微调过程中遗忘预训练模型中的通用知识。SAFT 仅更新梯度幅度较大的一小部分重要参数，同时保持其他参数冻结。SAFT 易于实现且概念简单。大量实验表明，仅使用 0.1% 的模型参数，SAFT 就可以显著提高 CLIP 的性能。在多个基准测试中，它始终优于基线方法。在 ImageNet 及其变体的少样本学习基准测试中，在 OOD 设置下，SAFT 比传统的微调方法平均提高了 5.15% 的性能。|
|**2024-07-03**|[Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective](http://arxiv.org/abs/2407.02814)|null|在大型数据集上预训练的视觉语言模型 (VLM) 可能会通过将性别信息与特定对象或场景相关联而无意中学习到偏见。当前的方法侧重于修改输入并监控模型输出概率分数的变化，但往往难以从模型组件的角度全面理解偏见。我们提出了一个结合因果中介分析的框架，用于测量和映射 VLM 内偏见产生和传播的路径。这种方法使我们能够确定干预措施对模型偏差的直接影响，以及干预措施通过不同模型组件介导的对偏差的间接影响。我们的结果表明，图像特征是偏见的主要来源，其影响远高于文本特征，具体而言，在 MSCOCO 和 PASCAL-SENTENCE 数据集中分别占偏见的 32.57% 和 12.63%。值得注意的是，图像编码器的贡献超过了文本编码器和深度融合编码器。进一步的实验表明，语言和视觉模态的贡献是一致且不冲突的。因此，专注于模糊图像编码器中对模型偏见贡献最大的性别表征，可以有效地将 MSCOCO 和 PASCAL-SENTENCE 数据集中的偏见分别减少 22.03% 和 9.04%，而性能损失最小，计算量也没有增加。|
|**2024-07-03**|[MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context](http://arxiv.org/abs/2407.02730)|**[link](https://github.com/dongzizhu/medvh)**|大型视觉语言模型 (LVLM) 最近在自然图像和文本数据的各种任务中取得了优异的性能，这激发了大量关于 LVLM 微调和训练的研究。尽管取得了这些进步，但很少有研究关注这些模型在更小的数据集上微调时对幻觉的鲁棒性。在这项研究中，我们引入了一个新的基准数据集，即医学视觉幻觉测试 (MedVH)，用于评估特定领域 LVLM 的幻觉。 MedVH 包含五项任务，用于评估医学环境中 LVLM 的幻觉，其中包括全面理解文本和视觉输入以及生成长文本响应的任务。我们对通用 LVLM 和医学 LVLM 进行的广泛实验表明，尽管医学 LVLM 在标准医学任务中表现出良好的性能，但它们特别容易受到幻觉的影响，通常比通用模型更容易受到影响，这引发了人们对这些特定领域模型可靠性的严重担忧。为了使医学 LVLM 在实际应用中真正发挥价值，它们不仅必须准确地整合医学知识，还必须保持强大的推理能力以防止幻觉。我们的工作为未来对这些研究的评估铺平了道路。|
|**2024-07-02**|[Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models](http://arxiv.org/abs/2407.02716)|null|对预训练的视觉语言模型 (VLM) 进行微调已在医学图像和文本描述协同作用方面展现出卓越的能力。然而，许多预训练数据集受到患者隐私问题的限制，可能包含会对下游性能产生负面影响的噪声。此外，对多模态生成的日益依赖加剧了这个问题，因为它容易受到对抗性攻击。为了研究在对抗性噪声数据上训练的 VLM 如何在下游医学任务中执行，我们首先使用多模态对抗性攻击来制作噪声上游数据集。通过我们的综合分析，我们揭示了适度的噪声增强了模型的鲁棒性和可迁移性，但增加噪声水平会对下游任务性能产生负面影响。为了缓解这个问题，我们提出了校正对抗性噪声 (RAN) 框架，这是一种旨在有效防御对抗性攻击并在微调期间纠正上游噪声影响的方法。|
|**2024-07-02**|[D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data and eXpert model predictions](http://arxiv.org/abs/2407.02604)|null|大型视觉语言模型（VLM）已经从研究阶段发展到适用于通用用例的阶段，取得了令人难以置信的进步。LLaVA-Med 是一种开创性的生物医学大型语言和视觉助手，可以执行多模态生物医学图像和数据分析，为放射科医生提供自然语言界面。虽然它具有高度的通用性，并且可以处理多模态数据，但它目前受到大型语言模型领域现有挑战的限制。回复中的幻觉和不精确性可能导致误诊，这在目前阻碍了 VLM 的临床适应性。为了在医疗保健领域创建精确、用户友好的模型，我们提出了 D-Rax，这是一种特定领域、对话式的放射学辅助工具，可用于获取有关特定放射图像的见解。在这项研究中，我们增强了胸部 X 光（CXR）图像的对话分析，以支持放射学报告，提供来自医学成像的全面见解，并帮助制定准确的诊断。D-Rax 的实现是通过在我们策划的增强型指令跟随数据上微调 LLaVA-Med 架构来实现的，这些数据包括图像、指令以及从 MIMIC-CXR 成像数据、CXR 相关视觉问答 (VQA) 对和多个专家 AI 模型的预测结果中得出的疾病诊断和人口统计学预测。我们观察到，在对开放式和封闭式对话进行评估时，响应在统计学上都有显著改善。D-Rax 利用最先进的诊断模型与 VLM 相结合的力量，使临床医生能够使用自然语言与医学图像进行交互，这有可能简化他们的决策过程，提高诊断准确性并节省他们的时间。|
|**2024-07-02**|[Understanding Alignment in Multimodal LLMs: A Comprehensive Study](http://arxiv.org/abs/2407.02477)|null|偏好对齐已成为提升大型语言模型 (LLM) 性能的关键组成部分，但其对多模态大型语言模型 (MLLM) 的影响仍相对缺乏研究。与语言模型类似，用于图像理解任务的 MLLM 也面临着诸如幻觉之类的挑战。在 MLLM 中，幻觉不仅可以通过陈述错误的事实发生，还可以通过产生与图像内容不一致的响应来发生。MLLM 对齐的主要目标是鼓励这些模型使响应与图像信息更加一致。最近，多项工作引入了 MLLM 的偏好数据集，并研究了不同的对齐方法，包括直接偏好优化 (DPO) 和近端策略优化 (PPO)。然而，由于数据集、基础模型类型和对齐方法的不同，目前尚不清楚哪些具体因素对这些工作中报告的改进贡献最大。在本文中，我们独立分析了 MLLM 中偏好对齐的各个方面。我们首先将对齐算法分为两组，离线（如 DPO）和在线（如在线 DPO），并表明结合离线和在线方法可以在某些情况下提高模型的性能。我们回顾了各种已发布的多模态偏好数据集，并讨论了其构建细节如何影响模型性能。基于这些见解，我们引入了一种创建多模态偏好数据的新方法，称为偏差驱动幻觉采样 (BDHS)，它既不需要额外的注释也不需要外部模型，并表明它可以在各种基准测试中实现与先前发布的多模态模型对齐工作相当的性能。|
|**2024-07-02**|[Conceptual Codebook Learning for Vision-Language Models](http://arxiv.org/abs/2407.02350)|null|在本文中，我们提出了概念码本学习（CoCoLe），这是一种针对视觉语言模型（VLM）的新型微调方法，旨在解决在少量样本情况下对下游任务进行微调时提高VLM泛化能力的挑战。我们认识到，视觉概念（如纹理、形状和颜色）可以自然地跨域迁移，并且在泛化任务中发挥着至关重要的作用。受这一有趣发现的启发，我们学习了一个由视觉概念作为键、概念提示作为值的概念码本，它充当图像编码器输出和文本编码器输入之间的桥梁。具体来说，对于给定的图像，我们利用码本识别与类嵌入相关的最相关的概念提示，以执行分类。此外，我们还结合了一个手工制作的概念缓存作为正则化，以缓解低样本情况下出现的过拟合问题。我们观察到，这种概念码本学习方法能够增强视觉和语言模态之间的对齐。大量的实验结果表明，我们的CoCoLe方法在各种评估设置（包括从基础到新的泛化、跨数据集评估和域泛化任务）中都明显优于现有的最先进方法。详细的消融研究进一步证实了CoCoLe中每个组件的有效性。|
|**2024-07-02**|[Synthetic Multimodal Question Generation](http://arxiv.org/abs/2407.02233)|null|多模态检索增强生成 (MMRAG) 是一种强大的多模态文档问答方法。评估 MMRAG 的一个关键挑战是缺乏与目标问题风格和模态相匹配的高质量数据集。鉴于此，我们提出了 SMMQG，一个合成数据生成框架。SMMQG 利用检索器、大型语言模型 (LLM) 和大型多模态模型 (LMM) 之间的相互作用，直接从多模态文档中生成问答对，并使问题符合指定的风格和模态。我们使用 SMMQG 从维基百科文档中生成了一个包含 1024 个问题的 MMRAG 数据集，并使用该数据集评估了最先进的模型，揭示了只有通过特定风格和模态的评估数据才能获得的模型性能洞察。接下来，我们通过人工研究来衡量 SMMQG 产生的数据的质量。我们发现，我们的合成数据的质量与众包基准 MMQA 的质量相当，并且使用这两个数据集的下游评估结果非常一致。|
|**2024-07-02**|[Multi-Modal Video Dialog State Tracking in the Wild](http://arxiv.org/abs/2407.02218)|null|我们提出了 MST-MIXER，这是一个基于通用多模态状态跟踪方案的新型视频对话模型。目前声称能够执行多模态状态跟踪的模型在两个主要方面存在不足：(1) 它们要么只跟踪一种模态（主要是视觉输入），要么 (2) 它们针对的是不能反映现实世界复杂性的合成数据集。我们的模型解决了这两个限制，试图弥合这一关键的研究差距。具体来说，MST-MIXER 首先跟踪每个输入模态中最重要的成分。然后，它通过使用一种新颖的多模态图结构学习方法学习局部潜在图，从而预测每个模态所选成分缺失的底层结构。随后，将学习到的局部图和特征一起解析，形成一个在所有模态混合上运行的全局图，从而进一步细化其结构和节点嵌入。最后，利用细粒度的图节点特征来增强骨干视觉语言模型 (VLM) 的隐藏状态。MST-MIXER 在五个具有挑战性的基准测试中取得了新的最先进成果。|

## 6DOF Object Pose

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-06-06**|[Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking](http://arxiv.org/abs/2406.04316)|null|6D物体姿态估计是计算机视觉中一项至关重要但极具挑战性的任务，其面临的主要问题是大规模数据集的缺乏。这种稀缺性阻碍了对模型性能的全面评估，限制了研究进展。此外，可用实例或类别的数量有限也限制了其应用。为了解决这些问题，本文提出了Omni6DPose，这是一个以对象类别多样性、规模大和对象材质多样性为特征的大型数据集。Omni6DPose主要分为三个部分：ROPE（真实6D物体姿态估计数据集），包含332K张图像，涵盖149个类别、581个实例的超过150万个标注；SOPE（模拟6D物体姿态估计数据集），包含在混合现实环境中创建的475K张图像，利用深度模拟技术，对149个类别、4162个实例进行了超过500万个标注；以及在ROPE和SOPE中使用的经过手动对齐的真实扫描物体。由于存在大量的变化和歧义，Omni6DPose本身就极具挑战性。为了应对这一挑战，我们引入了GenPose++，它是SOTA类别级姿态估计框架的增强版本，它包含两项关键改进：语义感知特征提取和基于聚类的聚合。此外，我们还提供了全面的基准分析，以评估先前方法在这个大规模数据集上在6D物体姿态估计和姿态跟踪方面的性能。|
|**2024-06-05**|[Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices](http://arxiv.org/abs/2406.02977)|null|随着机器人和增强现实应用越来越依赖于精确高效的6D物体姿态估计，边缘设备上的实时性能对于实现更具交互性和响应能力的系统至关重要。我们提出的稀疏颜色代码网络（SCCN）体现了一种清晰简洁的流程设计，可以有效地满足这一需求。SCCN对RGB图像中的目标物体进行像素级预测，利用基本物体几何特征的稀疏性来加速透视n点（PnP）计算过程。此外，它引入了一种新颖的基于像素级几何的物体对称表示，该表示与初始姿态预测无缝集成，有效地解决了对称物体的歧义性。SCCN在NVIDIA Jetson AGX Xavier上分别实现了在基准LINEMOD数据集和遮挡LINEMOD数据集上每秒19帧（FPS）和6 FPS的估计速率，同时在这些速率下始终保持较高的估计精度。|
|**2024-05-31**|[Deep Learning-Based Object Pose Estimation: A Comprehensive Survey](http://arxiv.org/abs/2405.07801)|**[link](https://github.com/cnjianliu/awesome-object-pose-estimation)**|物体姿态估计是计算机视觉中的一个基本问题，在增强现实和机器人技术中有着广泛的应用。在过去十年中，深度学习模型由于其卓越的准确性和鲁棒性，已经逐渐取代了依赖于工程点对特征的传统算法。然而，当代方法仍然存在一些挑战，包括它们对标记训练数据的依赖性、模型的紧凑性、在挑战性条件下的鲁棒性以及泛化到新颖未见物体的能力。目前缺少一篇综述来讨论该领域各个方面的进展、突出挑战和未来有希望的方向。为了填补这一空白，我们讨论了基于深度学习的物体姿态估计的最新进展，涵盖了该问题的所有三种形式，即实例级、类别级和未见物体姿态估计。我们的综述还涵盖了多种输入数据模态、输出姿态的自由度、物体属性和下游任务，为读者提供了对该领域的全面理解。此外，它还讨论了不同领域的训练范式、推理模式、应用领域、评估指标和基准数据集，并报告了当前最先进方法在这些基准数据集上的性能，从而方便读者为其应用选择最合适的方法。最后，该综述指出了关键挑战，回顾了当前的趋势及其优缺点，并指出了未来研究的有希望的方向。我们还将继续跟踪https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation上的最新工作。|
|**2024-03-28**|[Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation](http://arxiv.org/abs/2403.19527)|**[link](https://github.com/leeiieeo/ag-pose)**|类别级 6D 物体姿态估计旨在估计特定类别中未见过实例的旋转、平移和大小。在这个领域，基于密集对应的算法已经取得了领先的性能。然而，它们没有明确地考虑不同实例的局部和全局几何信息，导致对具有显著形状变化的未见过实例的泛化能力较差。为了解决这个问题，我们提出了一种新的用于类别级 6D 物体姿态估计的实例自适应和几何感知关键点学习方法 (AG-Pose)，它包括两个关键设计：（1）第一个设计是实例自适应关键点检测模块，它可以自适应地检测一组稀疏关键点来表示不同实例的几何结构。（2）第二个设计是几何感知特征聚合模块，它可以有效地将局部和全局几何信息整合到关键点特征中。这两个模块可以协同工作，为未见过的实例建立鲁棒的关键点级对应关系，从而增强模型的泛化能力。在 CAMERA25 和 REAL275 数据集上的实验结果表明，所提出的 AG-Pose 在没有类别特定形状先验的情况下，大幅度优于现有技术水平的方法。|
|**2024-06-01**|[Object Pose Estimation via the Aggregation of Diffusion Features](http://arxiv.org/abs/2403.18791)|**[link](https://github.com/tianfu18/diff-feats-pose)**|从图像中估计物体姿态是3D场景理解的一项关键任务，最近的方法在非常大的基准数据集上显示出良好的结果。然而，这些方法在处理未见过的物体时性能会显著下降。我们认为这是由于图像特征的泛化能力有限造成的。为了解决这个问题，我们深入分析了扩散模型（如Stable Diffusion）的特征，这些特征在对未见过的物体进行建模方面具有巨大的潜力。基于这一分析，我们创新性地将这些扩散特征引入到物体姿态估计中。为此，我们提出了三种不同的架构，可以有效地捕获和聚合不同粒度的扩散特征，大大提高了物体姿态估计的泛化能力。我们的方法在三个流行的基准数据集LM、O-LM和T-LESS上，以相当大的优势超过了最先进的方法。特别是，我们的方法在未见过的物体上取得了比之前最佳结果更高的准确率：在Unseen LM上为98.2% vs. 93.5%，在Unseen O-LM上为85.9% vs. 76.3%，显示了我们方法强大的泛化能力。我们的代码已发布在https://github.com/Tianfu18/diff-feats-pose。|
|**2024-03-24**|[KITchen: A Real-World Benchmark and Dataset for 6D Object Pose Estimation in Kitchen Environments](http://arxiv.org/abs/2403.16238)|null|尽管最近在用于机器人抓取的6D物体姿态估计方法方面取得了进展，但这些方法在现有数据集上的性能与其在现实世界移动操作任务中的效率之间仍然存在很大差距，尤其是当机器人完全依赖其单目以自我为中心的视野（FOV）时。现有的现实世界数据集主要集中在桌面抓取场景，其中机械臂放置在固定位置，物体集中在固定外部摄像机的视野内。评估此类数据集的性能可能无法准确反映在厨房环境中日常移动操作任务中遇到的挑战，例如从更高的架子、水槽、洗碗机、烤箱、冰箱或微波炉中取回物品。为了解决这一差距，我们提出了KITchen，这是一个专门为估计位于厨房环境中不同位置的物体的6D姿态而设计的新基准。为此，我们记录了一个包含约205k张真实世界RGBD图像的综合数据集，这些图像来自两个不同厨房中的111个厨房物体，利用一个具有人形机器人以自我为中心的视角。随后，我们开发了一个半自动注释管道，以简化此类数据集的标记过程，从而以最少的人力生成2D对象标签、2D对象分割掩码和6D对象姿态。基准、数据集和注释管道可在https://kitchen-dataset.github.io/KITchen获取。|
|**2024-03-22**|[DITTO: Demonstration Imitation by Trajectory Transformation](http://arxiv.org/abs/2403.15203)|null|快速便捷地教会机器人新技能对于机器人系统的广泛应用至关重要。在这项工作中，我们解决了从单个 RGB-D 视频记录的人类演示中进行一次性模仿的问题，该过程分为两个阶段。在第一个离线阶段，我们提取演示的轨迹。这需要分割被操纵的物体并确定它们相对于次要物体（例如容器）的相对运动。随后，在实时在线轨迹生成阶段，我们首先重新检测所有物体，然后将演示轨迹变换到当前场景，最后，我们使用机器人跟踪轨迹。为了完成这些步骤，我们的方法利用了几个辅助模型，包括用于分割、相对物体姿态估计和抓取预测的模型。我们系统地评估了对应关系和重新检测方法的不同组合，以验证我们跨各种任务的设计决策。具体来说，我们收集了十种不同任务的演示，包括拾放任务以及铰接物体操作。最后，我们在真实的机器人系统上进行了广泛的评估，以证明我们的方法在现实世界场景中的有效性和实用性。我们在 http://ditto.cs.uni-freiburg.de 上公开提供代码。|
|**2024-03-21**|[Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation](http://arxiv.org/abs/2403.14559)|null|在二维图像中定位预定义的三维关键点是建立六自由度物体姿态估计的3D-2D对应关系的有效方法。然而，不可见关键点的不可靠定位结果会降低对应关系的质量。在本文中，我们通过定位可见性方面的关键点来解决这个问题。由于关键点可见性信息在当前的数据集收集过程中缺失，我们提出了一种有效的方法，可以从可用的物体级标注中生成二进制可见性标签，用于非对称物体和对称物体的关键点。我们进一步基于PageRank算法从二进制标签中推导出实值的可见性感知重要性。利用我们可见性感知重要性的灵活性，我们通过将可见性感知重要性与最先进的姿态估计算法相结合，并结合额外的positional encoding，构建了VAPO（可见性感知姿态估计器）。我们在流行的姿态估计基准上进行了广泛的实验，包括Linemod、Linemod-Occlusion和YCB-V。结果表明，VAPO改进了关键点对应关系和最终估计的姿态，并明显达到了最先进的性能。|
|**2024-03-18**|[GenFlow: Generalizable Recurrent Flow for 6D Pose Refinement of Novel Objects](http://arxiv.org/abs/2403.11510)|null|尽管基于学习的方法在6D物体姿态估计方面取得了进展，但对于新物体，精度和可扩展性之间的权衡仍然存在。具体来说，以前针对新物体的方法没有很好地利用目标物体的3D形状信息，因为它们侧重于通过间接处理形状来实现泛化，这使得它们效率较低。我们提出了GenFlow，这是一种在目标物体形状的指导下实现对新物体的精度和泛化能力的方法。我们的方法预测渲染图像和观察图像之间的光流，并迭代地细化6D姿态。它通过3D形状约束和从端到端可微系统学习到的可泛化几何知识来提高性能。我们通过设计级联网络架构来进一步改进我们的模型，以利用多尺度相关性和从粗到精的细化。GenFlow在RGB和RGB-D情况下均在未见物体姿态估计基准测试中排名第一。它还实现了与现有最先进的已见物体姿态估计方法相当的性能，而无需任何微调。|
|**2024-03-14**|[MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion](http://arxiv.org/abs/2403.09309)|null|杂乱的料箱拣选环境对姿态估计模型提出了挑战。尽管深度学习取得了令人瞩目的进步，但单视图RGB姿态估计模型在杂乱的动态环境中表现不佳。利用场景视频中包含的丰富时间信息有可能增强模型处理遮挡和环境动态特性的不利影响的能力。此外，联合目标检测和姿态估计模型更适合利用任务的相互依赖性来提高两项任务的准确性。为此，我们提出了一种基于注意力的时序融合方法，用于多目标6D姿态估计，该方法可以在视频序列的多个帧中积累信息。我们的MOTPose方法将一系列图像作为输入，并在一次前向传递中对所有目标执行联合目标检测和姿态估计。它学习使用基于交叉注意力的融合模块在多个时间步长上聚合目标嵌入和目标参数。我们在物理逼真的杂乱料箱拣选数据集SynPick和YCB-Video数据集上评估了我们的方法，并证明了改进的姿态估计精度以及更好的目标检测精度。|

## nerf

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-07-09**|[3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes](http://arxiv.org/abs/2407.07090)|null|基于粒子的辐射场表示方法，例如 3D 高斯 splatting，已经在复杂场景的重建和重新渲染方面取得了巨大成功。大多数现有方法通过光栅化渲染粒子，将它们投影到屏幕空间图块中，以便按排序顺序进行处理。而这项工作则考虑对粒子进行光线追踪，构建边界体积层次结构，并使用高性能 GPU 光线追踪硬件为每个像素投射光线。为了有效处理大量半透明粒子，我们描述了一种专门的渲染算法，该算法使用边界网格封装粒子，以利用快速的光线三角形相交，并按深度顺序对成批的相交进行着色。光线追踪在计算机图形学中的优势是众所周知的：处理用于阴影和反射等二级照明效果的非相干光线、从机器人技术中常见的高度扭曲的相机进行渲染、随机采样光线等等。与光栅化相比，使用我们的渲染器，这种灵活性几乎不需要任何成本。实验证明了我们方法的速度和准确性，以及在计算机图形学和视觉方面的几种应用。我们进一步提出了对基本高斯表示的相关改进，包括简单使用广义核函数，这可以显著减少粒子命中次数。|
|**2024-07-07**|[GaussReg: Fast 3D Registration with Gaussian Splatting](http://arxiv.org/abs/2407.05254)|null|点云配准是大规模三维场景扫描和重建的基本问题。在深度学习的帮助下，配准方法得到了显著的发展，已接近成熟阶段。随着神经辐射场（NeRF）的引入，它因其强大的视图合成能力成为了最受欢迎的三维场景表示方法。对于 NeRF 表示，大规模场景重建也需要对其进行配准。然而，这个主题极度缺乏探索。这是因为对具有隐式表示的两个场景之间的几何关系进行建模存在固有的挑战。现有方法通常将隐式表示转换为显式表示以进行进一步的配准。最近，引入了高斯散射（GS），它采用显式三维高斯函数。这种方法在保持高质量渲染的同时，显著提高了渲染速度。给定两个具有显式 GS 表示的场景，在这项工作中，我们探索了它们之间的三维配准任务。为此，我们提出了 GaussReg，一种快速且准确的由粗到精的框架。粗配准阶段遵循现有的点云配准方法，并估计来自 GS 的点云的粗略对齐。我们进一步提出了一种新的图像引导的精配准方法，该方法从 GS 渲染图像，为精确对齐提供更详细的几何信息。为了支持全面评估，我们仔细构建了一个名为 ScanNet-GSReg 的场景级数据集，其中包含从 ScanNet 数据集中获得的 1379 个场景，并收集了一个名为 GSReg 的真实世界数据集。实验结果表明，我们的方法在多个数据集上实现了最先进的性能。我们的 GaussReg 比 HLoc（SuperPoint 作为特征提取器，SuperGlue 作为匹配器）快 44 倍，并且具有相当的精度。|
|**2024-07-04**|[CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion Blur Images](http://arxiv.org/abs/2407.03923)|null|神经辐射场 (NeRFs) 因其高质量的新视角渲染能力而备受关注，促使研究人员致力于解决各种现实世界案例。其中一个关键挑战是相机在曝光时间内的移动导致的运动模糊，这阻碍了对 3D 场景的准确重建。在本研究中，我们提出了连续刚体运动感知高斯渲染 (CRiM-GS) 方法，以实时渲染速度从模糊图像中重建精确的 3D 场景。考虑到实际相机运动模糊过程包含复杂的运动模式，我们基于神经常微分方程 (ODEs) 预测相机的连续运动。具体来说，我们利用刚体变换对相机运动进行建模，并进行适当的正则化，以保持物体的形状和大小。此外，我们在 \textit{SE(3)} 场中引入了连续可变形 3D 变换，通过确保更高的自由度使刚体变换适应现实世界问题。通过重新审视基本相机理论并采用先进的神经网络训练技术，我们实现了对连续相机轨迹的精确建模。我们进行了广泛的实验，证明了该方法在基准数据集上的定量和定性评估中均达到了最先进的性能。|
|**2024-06-26**|[Trimming the Fat: Efficient Compression of 3D Gaussian Splats through Pruning](http://arxiv.org/abs/2406.18214)|null|近年来，由于神经辐射场和最近出现的3D高斯散射(3DGS)模型提供了端到端训练的能力，3D模型的使用越来越受欢迎。后者具有显著的优势，因为它在训练过程中能够轻松快速地收敛，并具有广泛的可编辑性。然而，尽管发展迅速，但关于这些模型可扩展性的文献仍处于起步阶段。在本研究中，我们针对这一差距采取了一些初步措施，展示了一种能够同时实现此类模型的内存和计算可扩展性的方法。具体而言，我们提出了“Trimming the fat”方法，这是一种基于梯度的后剪枝迭代技术，用于消除模型中编码的冗余信息。我们在广泛认可的基准测试集上的实验结果证明了我们方法的有效性，表明在保持甚至提高基线性能的同时，可以删除高达75%的高斯函数。我们的方法实现了约50倍的压缩，同时保持了与基线模型相似的性能，并且能够将计算速度提高到600帧/秒。|
|**2024-06-21**|[Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks](http://arxiv.org/abs/2406.15149)|null|模拟器是自动机器人学习的强大工具，因为它们提供了可扩展的数据生成、灵活的设计和轨迹优化。然而，将从仿真数据中学习到的行为迁移到现实世界中被证明是困难的，通常需要通过计算量大的域随机化方法或进一步的模型微调来缓解。我们提出了一种方法来提高仿真到真实视觉四旋翼导航任务中对分布变化的泛化能力和鲁棒性。为此，我们首先通过将高斯 splatting 与四旋翼飞行动力学相结合来构建模拟器，然后使用 Liquid 神经网络训练鲁棒的导航策略。通过这种方式，我们获得了一个全栈模仿学习协议，它结合了 3D 高斯 splatting 辐射场渲染的进步、专家演示训练数据的巧妙编程以及 Liquid 网络的任务理解能力。通过一系列定量飞行测试，我们证明了在单个模拟场景中学习到的导航技能可以直接稳健地迁移到现实世界。我们进一步展示了在剧烈的分布和物理环境变化下，在训练环境之外保持性能的能力。我们学习到的 Liquid 策略，仅在从真实感室内模拟飞行中提取的单个目标操作上进行训练，可以推广到户外真实硬件平台上的多步远足。|
|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|在非结构化旅游环境中拍摄的照片经常呈现出多变的外观和短暂的遮挡，这对准确的场景重建提出了挑战，并在新视角合成中导致了伪影。尽管先前的方法已经将神经辐射场 (NeRF) 与其他可学习模块集成以处理动态外观和消除瞬态对象，但其大量的训练需求和缓慢的渲染速度限制了实际部署。最近，3D 高斯 splatting (3DGS) 已成为 NeRF 的一种很有前途的替代方案，它提供了卓越的训练和推理效率以及更好的渲染质量。本文介绍了 Wild-GS，这是一种针对不受约束的照片集优化的 3DGS 创新改编，同时保留了其效率优势。Wild-GS 通过每张图像的固有材质属性、全局照明和相机属性以及逐点反射率的局部方差来确定每个 3D 高斯的外观。与先前在图像空间中对参考特征进行建模的方法不同，Wild-GS 通过对从参考图像中提取的三平面进行采样，将像素外观特征显式对齐到相应的局部高斯。这种新颖的设计有效地将参考视图的高频细节外观转移到 3D 空间，并显着加快了训练过程。此外，利用 2D 可见性图和深度正则化分别减轻瞬态效应和约束几何形状。大量实验表明，Wild-GS 在所有现有技术中实现了最先进的渲染性能以及最高的训练和推理效率。|
|**2024-06-06**|[A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation](http://arxiv.org/abs/2406.04253)|null|三维建模长期以来一直是计算机视觉和计算机图形学中的一个重要领域。近年来，由于神经表示和生成模型的突破，我们见证了三维建模的快速发展。三维人体建模作为游戏和动画等许多现实应用的核心，已经引起了广泛关注。在过去的几年里，出现了大量关于创建三维人体化身的工作，为三维人体建模形成了一个新的、丰富的知识库。文献的规模之大使得个人难以跟踪所有的工作。本次综述旨在从重建和生成的角度全面概述这些新兴的三维人体化身建模技术。首先，我们回顾了三维人体重建的代表性方法，包括基于像素对齐隐式函数、神经辐射场和三维高斯散射等方法。然后，我们总结了三维人体生成的代表性方法，特别是那些使用大型语言模型（如CLIP）、扩散模型和各种三维表示的方法，它们展示了最先进的性能。最后，我们讨论了对现有方法的反思以及三维人体化身建模面临的开放性挑战，为未来的研究指明了方向。|
|**2024-06-13**|[3D-HGS: 3D Half-Gaussian Splatting](http://arxiv.org/abs/2406.02720)|**[link](https://github.com/lihaolin88/3d-half-gaussian-splatting)**|逼真的三维重建是三维计算机视觉中的一个基本问题。由于近年来神经渲染技术的出现，该领域取得了长足的进步。这些技术主要致力于学习三维场景的体积表示，并通过渲染得到的损失函数来优化这些表示。其中，三维高斯散射（3D-GS）已成为一种重要的方法，其性能超过了神经辐射场（NeRF）。3D-GS使用参数化的三维高斯函数来建模空间位置和颜色信息，并结合了基于图块的快速渲染技术。尽管其渲染性能和速度都非常出色，但使用三维高斯核在准确表示不连续函数方面存在固有限制，特别是在形状不连续的边缘和角落，以及颜色不连续的不同纹理之间。为了解决这个问题，我们建议采用三维半高斯（3D-HGS）核，它可以作为一种即插即用的核函数。我们的实验表明，它们能够提高当前与 3D-GS 相关方法的性能，并在不影响渲染速度的情况下，在各种数据集上实现最先进的渲染性能。|
|**2024-06-04**|[FastLGS: Speeding up Language Embedded Gaussians with Feature Grid Mapping](http://arxiv.org/abs/2406.01916)|null|语义交互式辐射场因其具有促进用户友好和自动化的现实世界 3D 场景理解应用的潜力而一直是一项吸引人的任务。 然而，要在辐射场中同时实现高质量、高效率和零样本能力的语义是一项具有挑战性的任务。 在这项工作中，我们提出了 FastLGS，这是一种支持在高分辨率下 3D 高斯渲染 (3DGS) 中进行实时开放词汇查询的方法。 我们提出了语义特征网格来保存基于 Segment Anything Model (SAM) 掩码提取的多视图 CLIP 特征，并将网格映射到低维特征，以便通过 3DGS 进行语义场训练。 训练完成后，我们可以通过渲染特征的特征网格恢复像素对齐的 CLIP 嵌入，用于开放词汇查询。 与其他最先进方法的比较证明，FastLGS 在速度和精度方面均能达到一流的性能，其中 FastLGS 比 LERF 快 98 倍，比 LangSplat 快 4 倍。 同时，实验表明，FastLGS 具有自适应性，并且兼容许多下游任务，例如 3D 分割和 3D 对象修复，可以轻松应用于其他 3D 操作系统。|
|**2024-05-30**|[ $\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving](http://arxiv.org/abs/2405.20323)|**[link](https://github.com/nnanhuang/s3gaussian)**|逼真的街道场景三维重建是开发自动驾驶真实世界模拟器的关键技术。尽管神经辐射场（NeRF）在驾驶场景中非常有效，但三维高斯散射（3DGS）由于其速度更快、表示更明确，正成为一个很有前景的方向。然而，大多数现有的街道3DGS方法都需要跟踪三维车辆边界框来分解静态和动态元素以进行有效的重建，这限制了它们在野外场景中的应用。为了在没有昂贵标注的情况下实现高效的三维场景重建，我们提出了一种自监督的街道高斯（$\textit{S}^3$Gaussian）方法，利用4D一致性来分解动态和静态元素。我们使用三维高斯来表示每个场景以保持其明确性，并进一步使用时空场网络来紧凑地建模4D动态。我们在具有挑战性的Waymo-Open数据集上进行了广泛的实验，以评估我们方法的有效性。我们的$\textit{S}^3$ Gaussian展示了在不使用三维标注的情况下分解静态和动态场景的能力，并实现了最佳性能。代码可在以下网址获取：https://github.com/nnanhuang/S3Gaussian/。|
|**2024-05-28**|[RT-GS2: Real-Time Generalizable Semantic Segmentation for 3D Gaussian Representations of Radiance Fields](http://arxiv.org/abs/2405.18033)|null|高斯渲染技术通过实现实时的高性能渲染，彻底改变了新视角合成的世界。最近，研究重点集中在为下游任务丰富这些3D表示的语义信息。在本文中，我们介绍了RT-GS2，这是第一个采用高斯渲染技术的可泛化语义分割方法。虽然现有的基于高斯渲染的方法依赖于场景特定的训练，但RT-GS2展示了泛化到未见场景的能力。我们的方法采用了一种新方法，首先以自监督的方式提取视图无关的3D高斯特征，然后进行新颖的视图依赖/视图无关（VDVI）特征融合，以增强不同视图之间的语义一致性。在三个不同数据集上的大量实验表明，RT-GS2在语义分割质量方面优于最先进的方法，例如在Replica数据集上的mIoU提高了8.01%。此外，我们的方法实现了27.03 FPS的实时性能，与现有方法相比实现了惊人的901倍加速。据我们所知，这项工作通过引入第一个用于辐射场3D高斯表示的实时可泛化语义分割方法，代表了该领域的重大进步。|
|**2024-05-29**|[PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting](http://arxiv.org/abs/2405.16829)|null|神经辐射场 (NeRFs) 在合成大规模场景的逼真图像方面表现出了非凡的能力。然而，它们经常受到细节丢失和渲染时间长的困扰。三维高斯 splatting 最近被引入作为一种有效的替代方案，可以实现高保真视觉效果和更快的渲染性能。尽管如此，扩展三维高斯 splatting 仍然充满了挑战。具体来说，大规模场景需要整合来自多个尺度和不同视点的对象，这通常会导致效率下降，因为高斯需要在细节级别之间取得平衡。此外，从大规模数据集中通过 COLMAP 生成初始化点不仅计算量大，而且容易导致重建不完整。为了应对这些挑战，我们提出了采用 NeRF 初始化的金字塔式三维高斯 splatting (PyGS)。我们的方法采用以金字塔形式排列的分层高斯集合来表示场景。金字塔的顶层由一些大的高斯函数组成，而随后的每一层都包含更密集的小高斯函数集合。我们通过以不同的频率对快速训练的基于网格的 NeRF 进行采样，从而有效地初始化这些金字塔高斯函数。我们将这些金字塔高斯函数分组到簇中，并使用紧凑的加权网络在渲染过程中动态确定每个簇中每个金字塔级别的影响，同时考虑相机视点。我们的方法在多个大规模数据集上实现了显著的性能飞跃，渲染速度比当前最先进的方法快 400 多倍。|
|**2024-05-11**|[Direct Learning of Mesh and Appearance via 3D Gaussian Splatting](http://arxiv.org/abs/2405.06945)|null|准确重建包含显式几何信息的3D场景既有吸引力又具有挑战性。几何重建可以受益于结合可微分的表观模型，例如神经辐射场和3D高斯 splatting (3DGS)。在这项工作中，我们提出了一个可学习的场景模型，它将3DGS与显式几何表示（即网格）结合起来。我们的模型以端到端的方式学习网格和外观，我们将3D高斯函数绑定到网格面上，并执行3DGS的可微分渲染以获得光度监督。该模型创建了一个有效的信息通路来监督场景学习，包括网格。实验结果表明，学习到的场景模型不仅实现了最先进的渲染质量，而且还支持使用显式网格进行操作。此外，由于网格和外观的端到端学习，我们的模型在适应场景更新方面具有独特优势。|

## 分类/检测/识别/分割

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-07-09**|[CoLA: Conditional Dropout and Language-driven Robust Dual-modal Salient Object Detection](http://arxiv.org/abs/2407.06780)|**[link](https://github.com/ssecv/CoLA)**|深度/热信息有利于利用传统RGB图像检测显著性目标。然而，在双模态显著性目标检测（SOD）模型中，针对噪声输入和模态缺失的鲁棒性至关重要，但很少被研究。为了解决这个问题，我们引入了条件性丢弃和语言驱动（CoLA）框架，该框架包含两个核心组件。1）语言驱动质量评估（LQA）：利用带有提示学习器的预训练视觉语言模型，LQA在不需要额外质量标注的情况下重新校准图像贡献。这种方法有效地减轻了噪声输入的影响。2）条件性丢弃（CD）：一种学习方法，用于增强模型在模态缺失情况下的适应性，同时保持其在完整模态下的性能。CD作为一种插件式训练方案，将模态缺失视为条件，增强了各种双模态SOD模型的整体鲁棒性。大量实验表明，所提出的方法在模态完整和模态缺失两种情况下均优于最先进的双模态SOD模型。我们将开源代码。|
|**2024-07-09**|[Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting Region Captions](http://arxiv.org/abs/2407.06723)|null|人类使用组合性来描述复杂场景，使用带有链接和关系的简单文本描述来丰富描述。虽然视觉语言研究的目标是开发具有组合理解能力的模型，但这还没有反映在现有的数据集中，这些数据集在很大程度上仍然使用纯文本描述图像。在这项工作中，我们提出了一种新的标注策略，即基于图的字幕（GBC），它使用带有各种类型节点的标记图结构来描述图像。GBC 中的节点是通过以下方式创建的：首先，使用对象检测和密集字幕工具递归嵌套以发现和描述实体节点，然后在第二阶段通过使用新型节点、组合和实体之间的关系来突出显示，将它们链接在一起。由于所有 GBC 节点都包含纯文本描述，因此 GBC 保留了自然语言的灵活性，但也可以在其边缘编码分层信息。我们证明了 GBC 可以使用现成的多模态 LLM 和开放词汇检测模型自动生成，方法是构建一个新的数据集 GBC10M，为 CC12M 数据集中的大约 10M 张图像收集 GBC 标注。我们使用 GBC10M 来展示 GBC 发现的大量节点字幕，使用 CLIP 训练进行测量。我们表明，与其他数据集格式相比，使用 GBC 节点的注释（特别是存储在组合和关系节点中的注释）可以显着提高下游模型的性能。为了进一步探索 GBC 提供的机会，我们还提出了一种新的注意力机制，可以利用整个 GBC 图，并获得了令人鼓舞的实验结果，表明了合并图结构的额外好处。我们的数据集发布在 \url{https://huggingface.co/graph-based-captions}。|
|**2024-07-09**|[CTRL-F: Pairing Convolution with Transformer for Image Classification via Multi-Level Feature Cross-Attention and Representation Learning Fusion](http://arxiv.org/abs/2407.06673)|null|Transformer因其强大的容量和全局处理能力，在计算机视觉领域受到越来越多的关注。然而，Transformer是数据密集型的，与卷积神经网络（ConvNets）相比，其泛化能力受到限制，特别是在数据有限的情况下进行训练时，因为它们缺乏ConvNets中存在的内置空间归纳偏差。在本文中，我们致力于将卷积和Transformer的优势结合起来，以完成图像分类任务。为此，我们提出了一种新颖的轻量级混合网络，该网络通过表示学习融合和多级特征交叉注意（CTRL-F）将卷积与Transformer配对。我们的网络包括一个卷积分支和一个名为多级特征交叉注意（MFCA）的新型Transformer模块。MFCA模块对从不同卷积阶段获得的多级特征表示进行操作。它通过两个独立的Transformer分支处理从这些多级特征表示中提取的小块标记和大块标记，其中两个分支通过交叉注意机制进行通信和交换知识。我们使用称为自适应知识融合（AKF）和协作知识融合（CKF）的新型表示融合技术，将从卷积路径获得的局部响应与从MFCA模块获得的全局响应融合在一起。实验表明，我们的CTRL-F变体无论是在大数据上从头开始训练，还是在低数据情况下训练，都能获得最先进的性能。例如，CTRL-F在Oxford-102 Flowers和PlantVillage数据集上从头开始训练时，分别达到了82.24%和99.91%的top-1准确率，超过了最先进的模型，这展示了我们的模型在图像分类任务上的鲁棒性。代码位于：https://github.com/hosamsherif/CTRL-F|
|**2024-07-09**|[NoisyAG-News: A Benchmark for Addressing Instance-Dependent Noise in Text Classification](http://arxiv.org/abs/2407.06579)|null|现有的噪声标签学习研究主要集中在合成标签噪声上。虽然合成噪声具有明确的结构特性，但它往往不能准确地复制现实世界的噪声模式。近年来，人们一直在努力构建用于图像分类的、可泛化和可控的实例相关噪声数据集，这极大地促进了该领域鲁棒噪声学习的发展。然而，关于文本分类中噪声标签学习的研究仍然很少。为了更好地理解现实世界中文本分类环境中的标签噪声，我们通过人工标注构建了基准数据集NoisyAG-News。首先，我们分析了标注数据，以收集关于现实世界噪声的观察结果。我们定性和定量地证明了现实世界的噪声标签遵循实例相关的模式。随后，我们使用预训练语言模型和噪声处理技术，对NoisyAG-News及其相应的合成噪声数据集进行了全面的学习实验。我们的研究结果表明，虽然预训练模型对合成噪声具有鲁棒性，但它们在实例相关噪声面前却表现不佳，不同混淆程度的样本在训练和测试过程中表现出不一致的性能。这些现实世界的噪声模式提出了新的、重大的挑战，促使人们重新评估噪声标签处理方法。我们希望NoisyAG-News能够促进未来噪声标签学习解决方案的开发和评估。|
|**2024-07-09**|[UnmixingSR: Material-aware Network with Unsupervised Unmixing as Auxiliary Task for Hyperspectral Image Super-resolution](http://arxiv.org/abs/2407.06525)|null|基于深度学习 (DL) 的高光谱图像 (HIS) 超分辨率 (SR) 方法取得了显著成果，并在工业界和学术界引起了广泛关注。然而，大多数现有方法都在探索和学习低分辨率 (LR) 和高分辨率 (HR) HSI 之间的映射关系，导致在解决不适定 SR 问题时增加了不可靠性和不合理性。有趣的是，我们发现 LR 成像与混合像元现象相似。传感器阵列中的单个光电探测器接收由多种类别反射的反射信号，导致低空间分辨率和混合像元问题。受此观察的启发，本文提出了一种名为 UnmixingSR 的、组件感知的 HSI SR 网络，其中无监督 HU 作为辅助任务用于感知 HSI 的材料成分。我们将 HU 视为辅助任务，并通过探索 LR 和 HR  丰度之间的约束将其纳入 HSI SR 过程。我们没有仅仅学习 LR 和 HR HSI 之间的映射关系，而是利用 LR 丰度和 HR 丰度之间的联系来提高我们方法在解决 SR 问题时的稳定性。此外，所提出的解混过程可以作为即插即用的辅助任务嵌入到现有的深度 SR 模型中。高光谱实验结果表明，将解混过程作为辅助任务纳入 SR 问题是可行且合理的，并取得了优异的性能。代码可在以下网址获得|
|**2024-07-08**|[Enhancing super-resolution ultrasound localisation through multi-frame deconvolution exploiting spatiotemporal coherence](http://arxiv.org/abs/2407.06373)|null|通过微泡 (MB) 定位和跟踪实现的超分辨率超声成像，也称为超声定位显微镜，可以在动物和人体内对微血管进行非侵入性亚衍射分辨率成像。从获取的对比增强超声 (CEUS) 图像中定位的 MB 数量和定位精度直接影响最终的超分辨率微血管图像的质量。然而，CEUS 图像中不可忽略的噪声会使 MB 定位变得困难。为了提高 MB 定位性能，我们提出了一种多帧反卷积 (MF-Decon) 框架，该框架可以利用 CEUS 数据中固有的时空一致性，并基于总变差 (TV) 和去噪正则化 (RED) 设计新的空间和时间正则化器。基于 MF-Decon 框架，我们引入了两种新方法：具有空间和时间 TV 的 MF-Decon (MF-Decon+3DTV) 和具有空间 RED 和时间 TV 的 MF-Decon (MF-Decon+RED+TV)。计算机模拟结果表明，我们的方法在所有评估指标（包括精度、召回率、 $F_1$ 分数、平均定位误差和标准定位误差）方面均优于两种广泛使用的反卷积或归一化互相关方法。特别是，我们的方法将 MB 定位精度提高了 39%，并将召回率提高了 12%。使用我们的方法在公开可用的体内大鼠大脑数据集上生成的超分辨率微血管图显示出更少的噪声、更好的对比度、更高的分辨率和更多的血管结构。|
|**2024-07-08**|[GeoWATCH for Detecting Heavy Construction in Heterogeneous Time Series of Satellite Images](http://arxiv.org/abs/2407.06337)|null|从多传感器学习是一项挑战，因为存在时空错位以及分辨率和捕获光谱的差异。为此，我们推出了 GeoWATCH，这是一个灵活的框架，用于在来自多个传感器平台的长序列卫星图像上训练模型，该框架旨在处理图像分类、活动识别、物体检测或物体跟踪任务。我们的系统包括一种基于子图同构的新型部分权重加载机制，允许在多个训练周期内持续训练和修改网络。这使我们能够在很长一段时间内训练一系列模型，我们观察到，在我们调整配置的同时保持核心骨干的情况下，性能得到了提高。|
|**2024-07-08**|[Active Label Refinement for Robust Training of Imbalanced Medical Image Classification Tasks in the Presence of High Label Noise](http://arxiv.org/abs/2407.05973)|null|基于监督深度学习的医学图像分类的鲁棒性会被标签噪声显著削弱。为了提高存在噪声标签时的分类性能，目前已经提出了几种方法，但它们面临着一些挑战：1）难以处理类别不平衡的数据集，导致少数类样本经常被误认为是噪声样本；2）仅仅关注于使用噪声数据集最大化性能，而没有结合专家参与主动清理噪声标签。为了应对这些挑战，我们提出了一种结合了噪声标签学习（LNL）和主动学习的两阶段方法。这种方法不仅提高了存在噪声标签时医学图像分类的鲁棒性，而且在有限的标注预算下，通过重新标注重要的错误标签，迭代地提高了数据集的质量。此外，我们在LNL阶段引入了一种新的梯度方差方法，通过对代表性不足的样本进行采样，补充了基于损失的样本选择方法。通过使用两个不平衡的噪声医学分类数据集，我们证明了我们提出的技术在处理类别不平衡方面优于以往的方法，因为它不会将来自少数类的干净样本错误地识别为大部分是噪声样本。|
|**2024-07-08**|[Deform-Mamba Network for MRI Super-Resolution](http://arxiv.org/abs/2407.05969)|null|在本文中，我们提出了一种新的MR图像超分辨率架构，称为Deform-Mamba。不同于传统的CNN或基于Transformer的超分辨率方法，这些方法会遇到与局部感受野或高计算成本相关的挑战，我们的方法旨在有效地探索图像的局部和全局信息。具体来说，我们开发了一个Deform-Mamba编码器，它由两个分支组成：调制变形块和视觉Mamba块。我们还在瓶颈层设计了一个多视图上下文模块，以探索多视图上下文内容。由于编码器提取的特征包括内容自适应的局部信息和高效的全局信息，视觉Mamba解码器最终生成高质量的MR图像。此外，我们引入了一种对比边缘损失来促进边缘和对比度相关内容的重建。在IXI和fastMRI数据集上的定量和定性实验结果表明，我们的方法取得了具有竞争力的性能。|
|**2024-07-08**|[Multi-clue Consistency Learning to Bridge Gaps Between General and Oriented Object in Semi-supervised Detection](http://arxiv.org/abs/2407.05909)|**[link](https://github.com/facias914/sood-mcl)**|虽然现有的半监督目标检测（SSOD）方法在一般场景中表现良好，但它们在处理航空图像中的定向目标时遇到了挑战。我们通过实验发现，在半监督学习中，一般目标检测和定向目标检测之间存在三个差距：1）采样不一致：在从标记数据中选择正标签时，常用的中心采样不适用于长宽比较大的定向目标。2）分配不一致：平衡定向伪框的精度和定位质量带来了更大的挑战，这在从未标记数据中选择正标签时引入了更多噪声。3）置信度不一致：在考虑定向目标时，预测的分类和定位质量之间存在更多不匹配，影响了伪标签的选择。因此，我们提出了一个多线索一致性学习（MCL）框架，以弥合半监督检测中一般目标和定向目标之间的差距。具体来说，考虑到旋转目标的各种形状，我们专门设计了高斯中心分配来从标记数据中选择像素级正标签。然后，我们引入了尺度感知标签分配来选择像素级伪标签而不是不可靠的伪框，这是一种适用于各种尺度目标的分而治之策略。最后采用一致置信度软标签，通过保持预测结果的一致性来进一步提升检测器。在DOTA-v1.5和DOTA-v1.0基准数据集上的综合实验表明，我们提出的MCL方法在半监督定向目标检测任务中可以达到最先进的性能。|
|**2024-07-05**|[SH17: A Dataset for Human Safety and Personal Protective Equipment Detection in Manufacturing Industry](http://arxiv.org/abs/2407.04590)|**[link](https://github.com/ahmadmughees/sh17dataset)**|工作场所事故持续对人类安全构成重大风险，特别是在建筑和制造等行业，因此，有效的个人防护装备 (PPE) 合规性变得越来越重要。我们的研究重点是开发基于目标检测 (OD) 和卷积神经网络 (CNN) 的非侵入性技术，以检测和验证各种类型 PPE 的正确使用，例如安全帽、安全眼镜、口罩和防护服。本研究提出了 SH17 数据集，其中包含从不同工业环境中收集的 8,099 张带注释的图像，这些图像包含 75,994 个 17 个类别的实例，用于训练和验证 OD 模型。我们已经训练了最先进的 OD 模型进行基准测试，初步结果表明，You Only Look Once (YOLO)v9-e 模型变体的 PPE 检测精度超过 70.9%，达到了令人满意的水平。跨域数据集上的模型验证性能表明，集成这些技术可以显着改进安全管理系统，为努力满足人类安全法规和保护员工的行业提供可扩展且高效的解决方案。该数据集可在 https://github.com/ahmadmughees/sh17dataset 获取。|
|**2024-07-05**|[Multi-Branch Auxiliary Fusion YOLO with Re-parameterization Heterogeneous Convolutional for accurate object detection](http://arxiv.org/abs/2407.04381)|**[link](https://github.com/yang-0201/MAF-YOLO)**|由于多尺度特征融合的有效性，路径聚合特征金字塔网络 (PAFPN) 广泛应用于 YOLO 检测器中。然而，它无法同时高效且自适应地融合高级语义信息和低级空间信息。在本文中，我们提出了一个名为 MAF-YOLO 的新模型，它是一个具有名为多分支辅助特征金字塔网络 (MAFPN) 的新型目标检测框架。在 MAFPN 中，浅层辅助融合 (SAF) 模块旨在将主干网络的输出与颈部结合起来，保留最佳级别的浅层信息，以便于后续学习。同时，深度嵌入颈部的高级辅助融合 (AAF) 模块将更多样化的梯度信息传递到输出层。此外，我们提出的重新参数化的异构高效层聚合网络 (RepHELAN) 模块确保了整体模型架构和卷积设计都采用了异构大卷积核。因此，这保证了保留与小目标相关的信息，同时实现了多尺度感受野。最后，以 MAF-YOLO 的纳米版本为例，它在 COCO 数据集上仅用 3.76M 可学习参数和 10.51G FLOPs 即可达到 42.4% 的 AP，性能优于 YOLOv8n 约 5.1%。本研究的源代码可在以下网址获取：https://github.com/yang-0201/MAF-YOLO。|
|**2024-07-05**|[FeatureSORT: Essential Features for Effective Tracking](http://arxiv.org/abs/2407.04249)|null|在这项工作中，我们介绍了一种新颖的跟踪器，该跟踪器专为在线多目标跟踪而设计，其重点在于简单而有效。我们提供了多个特征模块，每个模块代表一个特定的外观信息。通过整合不同的外观特征，包括服装颜色、款式和目标方向，以及用于鲁棒嵌入提取的 ReID 网络，我们的跟踪器显著提高了在线跟踪精度。此外，我们建议结合更强大的检测器，并提供先进的后处理方法，进一步提升跟踪器的性能。在实时操作期间，我们建立测量来跟踪关联距离函数，其中包括 IoU、方向、颜色、样式和 ReID 特征相似性信息，其中每个指标分别计算。通过我们设计的特征相关距离函数，可以跟踪物体更长时间的遮挡，同时保持相对较低的身份切换次数。广泛的实验评估表明，跟踪精度和可靠性显着提高，身份切换减少和遮挡处理增强证明了这一点。这些进步不仅有助于推动目标跟踪领域的最新技术水平，而且为未来需要高精度和可靠性的研究和实际应用开辟了新途径。|
|**2024-07-05**|[AnySR: Realizing Image Super-Resolution as Any-Scale, Any-Resource](http://arxiv.org/abs/2407.04241)|**[link](https://github.com/crispyfeso4/anysr)**|为了提高单图像超分辨率 (SISR) 应用的效率和可扩展性，我们引入了 AnySR，将现有的任意尺度 SR 方法重建为任意尺度、任意资源的实现。与使用相同计算成本解决各种规模的 SR 任务的现成方法相比，我们的 AnySR 创新在于：1) 将任意尺度任务构建为任意资源实现，在不增加额外参数的情况下减少了较小规模的资源需求；2) 以特征交织的方式增强任意尺度性能，将尺度对以规则的间隔插入特征中，并确保正确的特征/尺度处理。我们通过重建大多数现有的任意尺度 SISR 方法并在五个流行的 SISR 测试数据集上进行验证，充分证明了 AnySR 的有效性。结果表明，我们的 AnySR 以计算效率更高的方式实现了 SISR 任务，并且性能与现有的任意尺度 SISR 方法相当。我们首次实现了 SISR 任务，不仅在文献中是任意尺度的，而且是任意资源的。代码可在 https://github.com/CrispyFeSo4/AnySR 获取。|
|**2024-07-05**|[AMD: Automatic Multi-step Distillation of Large-scale Vision Models](http://arxiv.org/abs/2407.04208)|null|基于Transformer的架构因其优越的性能已成为各种视觉任务的标准模型。随着模型规模的不断扩大，模型蒸馏在各种实际应用中变得极其重要，特别是在受计算资源限制的设备上。然而，当教师模型和学生模型之间存在较大的容量差距时，例如10倍的压缩率，现有的知识蒸馏方法效果会下降。在本文中，我们提出了一种名为自动多步蒸馏（AMD）的新方法，用于大规模视觉模型压缩。具体来说，我们的蒸馏过程分为多个步骤。首先，对教师模型进行蒸馏，形成一个中间的助教模型，然后进一步蒸馏到学生模型。我们引入了一个高效且有效的优化框架，来自动识别能够使学生模型性能最大化的最佳助教模型。我们在多个图像分类数据集上进行了广泛的实验，包括CIFAR-10、CIFAR-100和ImageNet。结果一致表明，我们的方法优于几种已建立的基线方法，为未来大规模视觉模型的知识蒸馏方法铺平了道路。|
|**2024-07-04**|[Attention Normalization Impacts Cardinality Generalization in Slot Attention](http://arxiv.org/abs/2407.04170)|null|以对象为中心的场景分解对于计算机视觉和机器人等领域的下游任务非常重要。最近提出的槽位注意力模块已经被一些衍生作品用于图像分割和视频目标跟踪，它是一种深度学习组件，可以在输入图像上执行无监督的以对象为中心的场景分解。它基于一种注意力架构，其中潜在的槽位向量（包含对象的压缩信息）关注来自输入图像的局部感知特征。在本文中，我们发现对注意力架构中聚合值进行归一化的设计决策对槽位注意力泛化到训练期间所见到的更多槽位和对象的能力有相当大的影响。我们认为，原始的槽位注意力归一化方案丢弃了像素先前分配给槽位的概率信息，这损害了其泛化能力。基于这些发现，我们提出并研究了替代的归一化方法，这些方法可以提高槽位注意力对不同槽位和对象数量的泛化能力，从而提高无监督图像分割任务的性能。|
|**2024-07-04**|[Detect Closer Surfaces that can be Seen: New Modeling and Evaluation in Cross-domain 3D Object Detection](http://arxiv.org/abs/2407.04061)|null|目前，域适应技术在自动驾驶三维目标检测领域的性能尚未达到理想水平，这主要是由于车辆尺寸的显著差异以及跨域应用时运行环境的不同。这些因素共同阻碍了从特定数据集中学习到的知识的有效迁移和应用。由于现有的评估指标最初是通过计算预测边界框和真实边界框之间的二维或三维重叠来设计用于单个域上的评估，因此它们经常会遇到由数据集之间的大小差异引起的过拟合问题。这引发了一个与评估三维目标检测模型跨域性能相关的基本问题：我们是否真的需要模型在跨域应用后保持其原始三维边界框的出色性能？从实际应用的角度来看，我们的主要关注点之一实际上是防止车辆与其他障碍物发生碰撞，特别是在跨域场景中，正确预测车辆尺寸要困难得多。换句话说，只要模型能够准确识别出距离自动驾驶车辆最近的表面，就足以有效避开障碍物。在本文中，我们提出了两个指标来衡量三维目标检测模型检测自动驾驶车辆传感器附近表面的能力，这可以用来更全面、更合理地评估其跨域性能。此外，我们提出了一个名为EdgeHead的优化头，用于引导模型更加关注可学习的较近表面，这可以极大地提高现有模型在我们的新指标下以及在原始BEV/3D指标下的跨域性能。|
|**2024-07-04**|[TrackPGD: A White-box Attack using Binary Masks against Robust Transformer Trackers](http://arxiv.org/abs/2407.03946)|null|使用Transformer骨干网络的目标跟踪器在视觉目标跟踪数据集上取得了强大的性能。然而，这些跟踪器的对抗鲁棒性在文献中尚未得到很好的研究。由于骨干网络的差异，为目标跟踪提出的对抗性白盒攻击不能迁移到所有类型的跟踪器。例如，像MixFormerM这样的Transformer跟踪器在黑盒攻击后仍然运行良好，特别是在预测目标二进制掩码方面。我们提出了一种名为TrackPGD的新型白盒攻击，它依靠预测的目标二进制掩码来攻击鲁棒的Transformer跟踪器。这种新攻击通过采用著名的SegPGD分割攻击来关注标注掩码，从而能够成功地对依赖Transformer骨干网络的跟踪器进行白盒攻击。实验结果表明，TrackPGD能够有效攻击基于Transformer的跟踪器，例如MixFormerM、OSTrackSTS和TransT-SEG，并在多个跟踪数据集上取得了成功。|
|**2024-07-04**|[DocXplain: A Novel Model-Agnostic Explainability Method for Document Image Classification](http://arxiv.org/abs/2407.03830)|null|深度学习（DL）彻底改变了文档图像分析领域，在各种任务中展现出超越人类的表现。然而，深度学习模型固有的黑盒性质仍然是其在行业中安全稳健部署的重大挑战。遗憾的是，尽管近年来大量研究致力于开发基于深度学习的文档分析系统，但解决其透明性方面的研究却相对较少。在本文中，我们旨在通过介绍 DocXplain 来弥合这一研究差距，这是一种新颖的模型无关的可解释性方法，专门设计用于为文档图像分类任务生成高可解释性的特征归因图。具体来说，我们的方法涉及将文档的前景和背景特征独立地分割成不同的文档元素，然后消融这些元素以分配特征重要性。我们在文档图像分类的背景下广泛评估了我们提出的方法，利用 4 种不同的评估指标、2 个广泛认可的文档基准数据集和 10 个最先进的文档图像分类模型。通过对 9 种现有的最先进的归因方法进行全面的定量和定性分析，我们证明了我们的方法在保真度和可解释性方面的优越性。据作者所知，这项工作提出了第一个专门针对文档图像量身定制的模型无关的基于归因的可解释性方法。我们预计我们的工作将极大地促进文档图像分类模型的透明度、公平性和鲁棒性研究的进展。|
|**2024-07-04**|[M^3:Manipulation Mask Manufacturer for Arbitrary-Scale Super-Resolution Mask](http://arxiv.org/abs/2407.03695)|null|在图像篡改定位（IML）领域，现有数据集数量少、质量差一直是主要问题。包含各种篡改类型的数据集将极大地提高IML模型的准确性。互联网上的图像（例如百度贴吧PS吧的图像）使用各种技术进行篡改，利用这些图像创建数据集将显著丰富我们数据中的篡改类型。然而，互联网上的图像存在分辨率和清晰度问题，通过简单地从原始图像中减去篡改图像获得的掩码包含各种噪声。这些噪声很难去除，导致掩码无法用于IML模型。受变化检测领域的启发，我们将原始图像和篡改图像视为同一图像随时间的变化，并将数据生成任务视为变化检测任务。然而，由于图像之间的清晰度问题，传统的变化检测模型表现不佳。因此，我们引入了一个超分辨率模块，并提出了篡改掩码生成器（MMM）框架。它增强了原始图像和篡改图像的分辨率，从而改善了图像细节，以便更好地进行比较。同时，该框架将原始图像和篡改图像转换为特征嵌入并进行拼接，有效地对上下文进行建模。此外，我们创建了篡改掩码生成器数据集（MMMD），这是一个涵盖了各种篡改技术的数据集。我们的目标是通过MMM和MMMD提供更真实的篡改数据，为图像取证和篡改检测领域做出贡献。有关MMMD和下载链接的详细信息，请访问：代码和数据集将公开。|
|**2024-07-03**|[Visual Grounding with Attention-Driven Constraint Balancing](http://arxiv.org/abs/2407.03243)|null|不同于目标检测，视觉定位任务需要检测由复杂的自由形式语言描述的对象。为了同时对这种复杂的语义和视觉表示进行建模，最近最先进的研究采用基于 Transformer 的模型来融合来自两种模态的特征，并进一步引入了各种模块来调节视觉特征，使其与语言表达保持一致并消除不相关的冗余信息。然而，它们的损失函数仍然采用常见的目标检测损失，只控制边界框回归输出，无法完全优化上述目标。为了解决这个问题，本文首先分析了基于 Transformer 模型的注意力机制。在此基础上，我们进一步提出了一个名为注意力驱动约束平衡（AttBalance）的新框架，以优化语言相关区域内视觉特征的行为。大量的实验结果表明，我们的方法带来了令人印象深刻的改进。具体来说，我们在四个不同基准上评估的五种不同模型上实现了持续的改进。此外，通过将我们的方法集成到 QRNet 中，我们获得了新的最先进的性能。|
|**2024-07-03**|[Category-Aware Dynamic Label Assignment with High-Quality Oriented Proposal](http://arxiv.org/abs/2407.03205)|null|航拍图像中的物体通常嵌入在复杂的背景中，并呈现任意方向。当使用定向边界框 (OBB) 表示任意方向的物体时，角度的周期性可能导致边界处标签回归值的不连续性，从而导致损失函数出现剧烈波动。为了解决这个问题，在定向检测框架中引入了一种基于复平面的 OBB 表示方法，并提出了一种三角损失函数。此外，利用对复杂背景环境和航拍图像中大型物体显著差异的先验知识，构建了一个 Conformer RPN 头部来预测角度信息。所提出的损失函数和 Conformer RPN 头部共同生成高质量的定向建议。针对仅依靠 IoU 进行建议标签分配的局限性，提出了一种基于预测类别反馈的类别感知动态标签分配方法。该方法使负样本选择更具代表性，确保分类和回归特征之间的一致性。在四个真实的定向检测数据集上进行了实验，结果表明，在参数调整和时间成本最小的情况下，定向目标检测的性能更优。具体而言，在 DOTA-v1.0、DOTA-v1.5、DIOR-R 和 HRSC2016 数据集上分别实现了 82.02%、71.99%、69.87% 和 98.77% 的平均精度均值 (mAP) 分数。|
|**2024-07-03**|[SegVG: Transferring Object Bounding Box to Segmentation for Visual Grounding](http://arxiv.org/abs/2407.03200)|**[link](https://github.com/weitaikang/segvg)**|与目标检测不同，视觉定位（Visual Grounding）旨在为每个文本-图像对检测一个边界框。这种为每个文本-图像数据提供一个边界框的方式提供了稀疏的监督信号。尽管先前的工作取得了令人瞩目的成果，但它们对标注的被动利用，即将边界框标注仅用作回归真值，导致了性能欠佳。在本文中，我们提出了SegVG，这是一种将边界框级标注转换为分割信号的新方法，以便为视觉定位提供额外的像素级监督。具体来说，我们提出了多层多任务编码器-解码器作为目标定位阶段，在该阶段中，我们学习回归查询和多个分割查询，分别通过在每个解码层中对边界框进行回归和分割来定位目标。这种方法使我们能够迭代地利用标注作为边界框级回归和像素级分割的信号。此外，由于骨干网络通常由从单模态任务中学习到的预训练参数初始化，并且用于回归和分割的查询都是静态可学习的嵌入，因此这三种类型的特征之间存在域差异，这会损害后续的目标定位。为了减轻这种差异，我们引入了三重对齐模块，其中查询、文本和视觉标记通过三重注意力机制进行三角更新，以共享相同的空间。在五个广泛使用的数据集上进行的大量实验验证了我们的方法达到了最先进的性能 (SOTA)。|
|**2024-07-03**|[Global Context Modeling in YOLOv8 for Pediatric Wrist Fracture Detection](http://arxiv.org/abs/2407.03163)|**[link](https://github.com/ruiyangju/yolov8_global_context_fracture_detection)**|儿童在日常生活中经常遭受腕部损伤，而骨折损伤放射科医生通常需要在外科医生进行手术治疗之前分析和解释 X 光图像。深度学习的发展使神经网络模型能够作为计算机辅助诊断 (CAD) 工具来帮助医生和专家进行诊断。由于 YOLOv8 模型在目标检测任务中取得了令人满意的成功，因此它已被应用于骨折检测。全局上下文 (GC) 模块以轻量级的方式有效地对全局上下文进行建模，将其融入 YOLOv8 可以极大地提高模型性能。本文提出了用于骨折检测的 YOLOv8+GC 模型，它是具有 GC 模块的 YOLOv8 模型的改进版本。实验结果表明，与原始的 YOLOv8 模型相比，所提出的 YOLOv8-GC 模型在 GRAZPEDWRI-DX 数据集上将交并比阈值为 0.5 时的平均精度均值 (mAP 50) 从 63.58% 提高到 66.32%，达到了最先进的水平 (SOTA)。这项工作的实现代码可在 GitHub 上获取：https://github.com/RuiyangJu/YOLOv8_Global_Context_Fracture_Detection。|
|**2024-07-03**|[Applying Extended Object Tracking for Self-Localization of Roadside Radar Sensors](http://arxiv.org/abs/2407.03084)|null|智能交通系统 (ITS) 可以受益于路边 4D 毫米波雷达传感器，用于大规模交通监控，因为它们具有全天候功能、长感应范围和低制造成本。然而，在城市环境中，使用外部测量设备的定位方法存在局限性。此外，如果传感器安装由于环境影响而出现变化，则在仅在安装期间执行测量时无法对其进行校正。在本文中，我们提出了使用扩展目标跟踪 (EOT) 的路边雷达数据自定位方法。该方法分析传感器观察到的车辆跟踪轨迹和城市街道的航空激光扫描，将“直行”、“左转”、“右转”等驾驶行为标签分配给轨迹段和路段，并执行语义迭代最近点 (SICP) 算法来配准点云。该方法利用下游任务（目标跟踪）的结果进行定位。我们展示了亚米范围内的高精度以及非常低的方位误差。该方法还显示出良好的数据效率。评估在仿真和实际测试中均已完成。|
|**2024-07-03**|[YOLOv5, YOLOv8 and YOLOv10: The Go-To Detectors for Real-time Vision](http://arxiv.org/abs/2407.02988)|null|本文全面回顾了YOLO（You Only Look Once）目标检测算法的演进过程，重点关注YOLOv5、YOLOv8和YOLOv10。我们分析了这些版本在架构改进、性能提升以及边缘部署适用性方面的差异。YOLOv5引入了CSPDarknet骨干网络和Mosaic数据增强等重大创新，实现了速度和精度之间的平衡。YOLOv8在此基础上，通过增强特征提取和无锚框检测，提高了算法的通用性和性能。YOLOv10则凭借无NMS训练、空间通道解耦下采样以及大核卷积等技术实现了跨越式发展，以更低的计算开销实现了最先进的性能。我们的研究结果突出了YOLO算法在精度、效率和实时性能方面的逐步提升，特别强调了其在资源受限环境中的适用性。本综述提供了模型复杂度和检测精度之间权衡的见解，为针对特定边缘计算应用选择最合适的YOLO版本提供了指导。|
|**2024-07-03**|[ShiftAddAug: Augment Multiplication-Free Tiny Neural Network with Hybrid Computation](http://arxiv.org/abs/2407.02881)|null|缺乏乘法运算符（例如移位和加法）因其与硬件的兼容性而受到关注。然而，与具有相同结构的传统神经网络 (NN) 相比，采用这些运算符的神经网络 (NN) 通常表现出较低的精度。ShiftAddAug 使用成本高昂的乘法来增强高效但功能较弱的无乘法运算符，从而在没有任何推理开销的情况下提高性能。它将一个 ShiftAdd 小型神经网络放入一个大型乘法模型中，并鼓励将其训练为子模型以获得额外的监督。为了解决混合运算符之间的权重差异问题，提出了一种新的权重共享方法。此外，一种新颖的两阶段神经架构搜索用于为更小但更强的无乘法小型神经网络获得更好的增强效果。ShiftAddAug 的优越性通过图像分类和语义分割实验得到验证，始终如一地提供显着的增强。值得注意的是，与直接训练的对应模型相比，它在 CIFAR100 上的准确率提高了 4.95%，甚至超过了乘法神经网络的性能。|
|**2024-07-03**|[A Pairwise DomMix Attentive Adversarial Network for Unsupervised Domain Adaptive Object Detection](http://arxiv.org/abs/2407.02835)|null|无监督域自适应目标检测 (DAOD) 可以使在一个源域上训练的模型适应未标记的目标域，以进行目标检测。现有的无监督 DAOD 方法通常执行从目标域到源域的特征对齐。单向域迁移会忽略有关目标样本的信息，并在存在较大域差异时导致欠佳的自适应。因此，我们提出了一种具有域混合 (DomMix) 模块的成对注意力对抗网络，以缓解上述挑战。具体来说，采用深度混合来构建一个中间域，允许来自两个域的特征共享它们的差异。然后，应用成对注意力对抗网络，在不同尺度的图像级和实例级特征上进行注意力编码，并通过对抗学习优化域对齐。这使得网络能够专注于具有不同上下文信息的区域，并学习它们在不同域之间的相似性。在几个基准数据集上进行了广泛的实验，证明了我们提出的方法的优越性。|
|**2024-07-03**|[Data Overfitting for On-Device Super-Resolution with Dynamic Algorithm and Compiler Co-Design](http://arxiv.org/abs/2407.02813)|**[link](https://github.com/coulsonlee/dy-dca-eccv2024)**|深度神经网络 (DNN) 经常被应用于各种计算机视觉应用。如今，当前视频分发系统中一个新兴趋势是利用 DNN 的过拟合特性来执行视频分辨率提升。通过将视频分割成块并应用超分辨率 (SR) 模型对每个块进行过拟合，这种 SR 模型加视频块的方案能够取代传统的视频传输，从而提高视频质量和传输效率。然而，为了保证高性能，需要许多模型和块，这会导致用户端的模型切换和内存占用方面产生巨大的开销。为了解决这些问题，我们提出了一种由内容感知数据处理管道辅助的动态深度神经网络，以将模型数量减少到一个 (Dy-DCA)，这有助于在节省计算资源的同时提高性能。此外，为了在用户端实现真正的加速，我们设计了一个框架来优化 Dy-DCA 中的动态特征（例如，动态形状、大小和控制流），从而实现一系列编译优化，包括融合代码生成、静态执行计划等。通过采用这些技术，我们的方法在现成的手机上实现了更好的 PSNR 和实时性能 (33 FPS)。同时，在我们的编译优化的辅助下，我们实现了 1.7 倍的加速，同时节省了高达 1.61 倍的内存消耗。代码可在 https://github.com/coulsonlee/Dy-DCA-ECCV2024 获取。|
|**2024-07-03**|[Fine-Grained Scene Image Classification with Modality-Agnostic Adapter](http://arxiv.org/abs/2407.02769)|**[link](https://github.com/qunilcs/maa)**|在处理细粒度场景图像分类任务时，以往的大多数工作在进行多模态特征融合时都非常重视全局视觉特征。换句话说，模型的设计是基于对不同模态重要性的先验直觉。在本文中，我们提出了一种新的多模态特征融合方法，称为MAA（模态无关适配器），试图使模型自适应地学习不同模态在不同情况下的重要性，而无需在模型架构中进行先验设置。更具体地说，我们消除了分布中的模态差异，然后使用模态无关的Transformer编码器进行语义级别的特征融合。我们的实验表明，通过应用与以前方法相同的模态，MAA在基准测试中取得了最先进的结果。此外，值得一提的是，在使用MAA时，可以轻松添加新的模态，并进一步提升性能。代码可在https://github.com/quniLcs/MAA获取。|

## 生成模型

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-07-09**|[ConceptExpress: Harnessing Diffusion Models for Single-image Unsupervised Concept Extraction](http://arxiv.org/abs/2407.07077)|**[link](https://github.com/haoosz/conceptexpress)**|虽然个性化文本到图像生成技术已经能够从多张图像中学习单个概念，但更实际但也更具挑战性的场景涉及在单个图像中学习多个概念。然而，现有的解决此场景的工作严重依赖于大量的人工标注。在本文中，我们介绍了一项名为无监督概念提取 (UCE) 的新任务，该任务考虑了没有任何人类概念知识的无监督设置。给定一张包含多个概念的图像，该任务的目标是仅依靠预训练扩散模型的现有知识来提取和重建单个概念。为此，我们提出了 ConceptExpress，它通过从两个方面释放预训练扩散模型的内在能力来解决 UCE。具体来说，概念定位方法通过利用扩散自注意力的空间对应关系来自动定位和分离显著概念；并且基于概念和概念标记之间的查找关联，概念优化过程学习表示每个单独概念的区分标记。最后，我们为 UCE 任务建立了一个定制的评估协议。大量实验表明，ConceptExpress 是 UCE 任务的一个很有前景的解决方案。我们的代码和数据可在以下网址获得：https://github.com/haoosz/ConceptExpress|
|**2024-07-09**|[Latent Space Imaging](http://arxiv.org/abs/2407.07052)|null|传统数字成像系统通常基于对规则网格像素进行暴力测量和处理。另一方面，人类视觉系统对从光感受器到视神经的数据进行了大量压缩，本质上是将图像信息编码成适合人脑处理的低带宽潜在空间表示。在这项工作中，我们建议采用类似的方法来开发人工智能视觉系统。潜在空间成像是一种新范式，它通过光学和软件的结合，将图像信息直接编码到生成模型的语义丰富的潜在空间中，从而大大减少了捕获过程中的带宽和内存需求。我们通过基于单像素相机的初始硬件原型展示了这一新原理。通过设计一种调制幅度以编码到生成模型的潜在空间中的方案，我们在成像过程中实现了 1:100 到 1:1,000 的压缩比，这体现了潜在空间成像在高效成像硬件方面的潜力，使其能够在未来应用于高速成像或硬件复杂度大大降低的特定任务相机。|
|**2024-07-09**|[Generative models of astrophysical fields with scattering transforms on the sphere](http://arxiv.org/abs/2407.07007)|null|散射变换是一种新类型的概括统计量，最近被开发用于研究高度非高斯过程，并已被证明在天文物理研究中非常有前景。 特别是，它们允许人们从有限的数据中构建复杂非线性场的生成模型，并且还被用作新的统计成分分离算法的基础。 在即将进行的宇宙学巡天观测的背景下，例如针对宇宙微波背景偏振的 LiteBIRD 或用于研究宇宙大尺度结构的 Rubin-LSST 和 Euclid，将这些工具扩展到球面数据是必要的。 我们在球面上开发了散射变换，并专注于构建几个天体物理场的最大熵生成模型。 我们从单个目标场构建均匀的天体物理和宇宙学场的生成模型，使用常见的统计数据（功率谱、像素概率密度函数和 Minkowski 泛函）将样本与目标场进行定量比较。 我们的采样场在统计和视觉上都与目标场非常吻合。 因此，这些生成模型为未来的天体物理学和宇宙学研究开辟了广泛的新应用； 特别是那些几乎没有模拟数据的领域。 我们将代码提供给社区，以便这项工作可以轻松复制和进一步开发。|
|**2024-07-09**|[RodinHD: High-Fidelity 3D Avatar Generation with Diffusion Models](http://arxiv.org/abs/2407.06938)|null|我们提出了 RodinHD，它可以从肖像图像生成高保真 3D 头像。现有方法无法捕捉到我们论文中解决的复杂细节，例如发型。我们首先发现了一个被忽视的问题，即在许多头像上顺序拟合三平面时出现的灾难性遗忘问题，这是由 MLP 解码器共享方案引起的。为了克服这个问题，我们提出了一种新颖的数据调度策略和权重整合正则化项，从而提高了解码器渲染更清晰细节的能力。此外，我们通过计算捕获丰富 2D 纹理线索的更细粒度的层次表示，并通过交叉注意力将它们注入到 3D 扩散模型的多个层中，从而优化了肖像图像的引导效果。当使用针对三平面优化的噪声调度对 46K 头像进行训练时，生成的模型可以生成比以前的方法细节明显更好的 3D 头像，并且可以泛化到野外肖像输入。|
|**2024-07-09**|[HumanRefiner: Benchmarking Abnormal Human Generation and Refining with Coarse-to-fine Pose-Reversible Guidance](http://arxiv.org/abs/2407.06937)|**[link](https://github.com/enderfga/humanrefiner)**|文本到图像扩散模型在条件图像生成方面取得了显著进展。然而，这些模型通常难以准确渲染具有人类特征的图像，导致四肢扭曲和其他异常。这个问题主要源于扩散模型对肢体质量的识别和评估不足。为了解决这个问题，我们引入了AbHuman，这是第一个专注于解剖异常的大规模合成人体基准。该基准包含 56K 张合成的人体图像，每张图像都标注了详细的边界框级别标签，识别了 18 个不同类别中的 147K 个人体异常。在此基础上，可以建立人体异常的识别，进而通过负面提示和引导等传统技术增强图像生成。为了进一步促进改进，我们提出了HumanRefiner，这是一种新颖的即插即用方法，用于在文本到图像生成中对人体异常进行从粗到细的细化。具体来说，HumanRefiner 利用自我诊断程序来检测和纠正与粗粒度异常人体姿势和细粒度异常级别相关的问题，促进姿势可逆的扩散生成。在 AbHuman 基准上的实验结果表明，HumanRefiner 显着减少了生成差异，与最先进的开源生成器 SDXL 相比，肢体质量提高了 2.9 倍，在人类评估中比 DALL-E 3 提高了 1.4 倍。我们的数据和代码可在 https://github.com/Enderfga/HumanRefiner 获取。|
|**2024-07-09**|[Maximum stress minimization via data-driven multifidelity topology design](http://arxiv.org/abs/2407.06746)|null|最大应力最小化问题是结构设计中最重要的课题之一。传统的基于梯度的拓扑优化方法需要通过松弛技术将原始问题转化为伪问题。由于其参数对优化有显著影响，因此，在不使用松弛技术的情况下，准确地解决最大应力最小化问题有望获得最佳性能。本文重点关注这一挑战，并研究与基于梯度的拓扑优化获得的解决方案相比，通过解决没有松弛技术的原始最大应力最小化问题，是否可以获得具有更多避免应力集中的设计。我们采用数据驱动的多保真度拓扑设计（MFTD），这是一种基于进化算法的无梯度拓扑优化。基本框架包括通过解决低保真度优化问题来生成候选解，通过高保真度正向分析评估这些解，并在没有灵敏度分析的情况下使用深度生成模型迭代更新它们。在本研究中，数据驱动的MFTD将通过使用p范数应力度量解决基于梯度的拓扑优化问题获得的优化设计纳入初始解，并基于具有贴体网格的高保真度分析来解决原始最大应力最小化问题。我们通过L型支架的基准测试证明了我们提出的方法的有效性。通过使用数据驱动的MFTD解决原始最大应力最小化问题，与初始解相比，在相同的最大应力值下，体积减少了高达22.6%。|
|**2024-07-09**|[Powerful and Flexible: Personalized Text-to-Image Generation via Reinforcement Learning](http://arxiv.org/abs/2407.06642)|**[link](https://github.com/wfanyue/dpg-t2i-personalization)**|个性化文本到图像模型允许用户为一个对象（由一组参考图像指定）生成不同风格的图像（由句子指定）。虽然基于扩散的生成模型已经取得了显著成果，但在扩散过程中，对象的视觉结构和细节经常发生意外的变化。一个主要原因是，这些基于扩散的方法在训练过程中通常采用简单的重建目标，这很难在生成图像和参考图像之间强制实现适当的结构一致性。为此，本文设计了一种新的强化学习框架，利用确定性策略梯度方法进行个性化文本到图像生成，该框架可以轻松地结合各种目标（可微分甚至不可微分）来监督扩散模型，从而提高生成图像的质量。在个性化文本到图像生成基准数据集上的实验结果表明，我们提出的方法在视觉保真度方面明显优于现有的最先进方法，同时保持了文本对齐。我们的代码可在以下网址获得：\url{https://github.com/wfanyue/DPG-T2I-Personalization}。|
|**2024-07-09**|[Ensembled Cold-Diffusion Restorations for Unsupervised Anomaly Detection](http://arxiv.org/abs/2407.06635)|null|无监督异常检测 (UAD) 方法旨在通过将测试样本与从已知无异常的数据集中学习到的规范分布进行比较来识别异常。基于生成模型的方法通过生成无异常版本的测试图像来提供可解释性，但通常无法识别细微的异常。或者，使用特征建模或自监督方法（例如依赖于合成生成的异常的方法）不提供开箱即用的可解释性。在这项工作中，我们提出了一种结合了两种策略的优势的新方法：生成式冷扩散管道（即，使用不基于噪声的损坏的类似扩散的管道），该管道经过训练，目标是将合成损坏的图像恢复到正常、原始外观。为了支持我们的管道，我们引入了一种新的合成异常生成程序，称为 DAG，以及一种新的异常评分，它集成了以不同程度的异常为条件的恢复。我们的方法在三个不同的大脑 MRI 数据集中都超过了先前最先进的无监督异常检测水平。|
|**2024-07-09**|[Mobius: An High Efficient Spatial-Temporal Parallel Training Paradigm for Text-to-Video Generation Task](http://arxiv.org/abs/2407.06617)|null|受文本到图像（T2I）生成任务成功的启发，许多研究人员正致力于文本到视频（T2V）生成任务。大多数 T2V 框架通常继承自 T2I 模型，并添加额外的时序层训练以生成动态视频，这可以视为一项微调任务。然而，传统的 3D-Unet 是一种串行模式，时间层跟随空间层，根据其串行特征流，这将导致高 GPU 内存和训练时间消耗。我们认为，这种串行模式将随着大型扩散模型和海量数据集的出现而带来更高的训练成本，这不符合环保要求，也不利于 T2V 的发展。因此，我们提出了一种高效的时空并行训练范式，用于 T2V 任务，称为 Mobius。在我们提出的 3D-Unet 中，时间层和空间层是并行的，这优化了特征流和反向传播。Mobius 将节省 24% 的 GPU 内存和 12% 的训练时间，这可以极大地改善 T2V 微调任务，并为 AIGC 社区提供新的见解。我们将在未来发布我们的代码。|
|**2024-07-09**|[VQA-Diff: Exploiting VQA and Diffusion for Zero-Shot Image-to-3D Vehicle Asset Generation in Autonomous Driving](http://arxiv.org/abs/2407.06516)|null|从野外观察中生成 3D 车辆模型对于自动驾驶至关重要。现有的图像到 3D 方法不能很好地解决这个问题，因为它们仅仅从图像 RGB 信息中学习生成，而没有更深入地理解野外车辆（例如车型、制造商等）。这导致它们对具有遮挡或棘手视角的真实世界观察结果的零样本预测能力较差。为了解决这个问题，在这项工作中，我们提出了 VQA-Diff，这是一个利用野外车辆图像为自动驾驶创建逼真的 3D 车辆模型的新框架。VQA-Diff 利用视觉问答 (VQA) 模型中大型语言模型继承的真实世界知识进行鲁棒的零样本预测，并利用扩散模型中丰富的图像先验知识进行结构和外观生成。具体来说，我们利用多专家扩散模型策略生成结构信息，并采用主题驱动的结构控制生成机制对外观信息进行建模。因此，VQA-Diff 无需从现实世界中收集的大规模图像到 3D 车辆数据集中学习，仍然具有强大的零样本图像到新颖视图生成能力。我们在 Pascal 3D+、Waymo 和 Objaverse 等各种数据集上进行了实验，结果表明 VQA-Diff 在定性和定量上均优于现有的最先进方法。|
|**2024-07-05**|[Structural Constraint Integration in Generative Model for Discovery of Quantum Material Candidates](http://arxiv.org/abs/2407.04557)|null|已知的有机分子数以十亿计，但已发现的功能性无机材料却只占很小一部分，这对寻找新型量子材料的研究群体来说是一个特别突出的问题。基于机器学习的生成模型，尤其是扩散模型的最新进展，为生成新型稳定材料带来了巨大希望。然而，将几何模式融入材料生成仍然是一项挑战。在此，我们介绍了在生成模型中集成结构约束的方法 (SCIGEN)。我们的方法可以通过在每个扩散步骤之前，用扩散约束结构对去噪结构进行策略性掩蔽，从而将生成引导至约束输出，来修改任何经过训练的生成扩散模型。此外，我们从数学上证明了 SCIGEN 可以有效地从原始分布中执行条件采样，这对于生成稳定的约束材料至关重要。我们使用阿基米德格作为原型约束生成了 800 万种化合物，其中超过 10% 通过了多阶段稳定性预筛选。对 26,000 种存活化合物的 DFT（密度泛函理论）高通量计算表明，超过 50% 的化合物通过了 DFT 级别的结构优化。由于量子材料的性质与几何模式密切相关，我们的结果表明 SCIGEN 为生成量子材料候选材料提供了一个通用框架。|
|**2024-07-05**|[Unified continuous-time q-learning for mean-field game and mean-field control problems](http://arxiv.org/abs/2407.04521)|null|本文从代表性智能体的角度研究了平均场跳扩散模型中的连续时间q学习。为了克服无法直接观察到总体分布时的挑战，我们引入了解耦形式的集成q函数（解耦Iq函数），并建立了其与价值函数的鞅表征，这为平均场博弈（MFG）和平均场控制（MFC）问题提供了统一的策略评估规则。此外，根据求解MFG或MFC问题的任务，我们可以通过不同的方式利用解耦Iq函数来分别学习平均场均衡策略或平均场最优策略。因此，我们利用所有源于平均场交互的测试策略，设计了一种适用于MFG和MFC问题的统一q学习算法。对于跳扩散环境下的几个例子，包括LQ框架内外的例子，我们可以获得解耦Iq函数和价值函数的精确参数化，并从代表性智能体的角度说明我们的算法具有令人满意的性能。|
|**2024-07-05**|[Speed-accuracy trade-off for the diffusion models: Wisdom from nonequlibrium thermodynamics and optimal transport](http://arxiv.org/abs/2407.04495)|null|我们探讨了生成模型（称为扩散模型）与非平衡热力学中用于描述福克-普朗克方程的随机热力学之间的联系。基于随机热力学技术，我们推导出了扩散模型的速度-精度权衡，这是扩散模型中数据生成速度和精度之间的权衡关系。我们的结果表明，正向过程中的熵产生率会影响数据生成的误差。从随机热力学的角度来看，我们的结果为如何在扩散模型中最好地生成数据提供了定量见解。最佳学习方案由随机热力学中的保守力和最优传输理论中 2-Wasserstein 距离的空间测地线引入。我们用数值方法说明了具有不同噪声方案（如余弦方案、条件最优传输和最优传输）的扩散模型的速度-精度权衡的有效性。|
|**2024-07-05**|[PROUD: PaRetO-gUided Diffusion Model for Multi-objective Generation](http://arxiv.org/abs/2407.04493)|null|深度生成模型领域的最新进展集中于生成满足多个期望属性的样本。然而，普遍的方法是独立优化这些属性函数，从而忽略了它们之间的权衡。此外，属性优化通常没有被恰当地整合到生成模型中，导致生成质量（即生成样本的质量）的无必要妥协。为了解决这些问题，我们提出了一个约束优化问题。它寻求在确保生成样本位于多个属性目标的帕累托前沿的同时优化生成质量。这样的公式能够生成在相互冲突的属性函数上无法同时进一步改进的样本，并保持生成样本的良好质量。在此公式的基础上，我们引入了帕累托引导扩散模型 (PROUD)，其中去噪过程中的梯度被动态调整以提高生成质量，同时生成样本遵循帕累托最优性。在图像生成和蛋白质生成任务上的实验评估表明，与各种基线相比，我们的 PROUD 在逼近多个属性函数的帕累托最优性的同时，始终保持着卓越的生成质量。|
|**2024-07-05**|[VCD-Texture: Variance Alignment based 3D-2D Co-Denoising for Text-Guided Texturing](http://arxiv.org/abs/2407.04461)|null|最近关于三维形状纹理合成的研究极大地受益于快速发展的二维文本到图像的扩散模型，包括基于修复和基于优化的方案。然而，这些方法忽略了二维扩散模型和三维物体之间的模态差距，它们主要将三维物体渲染成二维图像并分别对每个图像进行纹理处理。在本文中，我们重新审视了纹理合成，并提出了一个基于方差对齐的三维-二维协同去噪框架，称为VCD-Texture，以解决这些问题。具体来说，我们首先在具有重新投影的三维注意力感受野的扩散自注意力模块中统一了二维和三维潜在特征学习。随后，将去噪后的多视图二维潜在特征聚合到三维空间，然后将其光栅化回二维空间，从而形成更一致的二维预测。然而，光栅化过程存在难以处理的方差偏差，我们提出的方差对齐从理论上解决了这个问题，实现了高保真纹理合成。此外，我们还提出了一种修复细化方法，以进一步改善存在冲突区域的细节。值得注意的是，目前还没有公开可用的基准来评估纹理合成，这阻碍了其发展。因此，我们构建了一个基于三个开源三维数据集的新评估集，并建议使用四个指标来全面验证纹理性能。综合实验表明，VCD-Texture相较于其他方法取得了优越的性能。|
|**2024-07-05**|[Benchmarking structure-based three-dimensional molecular generative models using GenBench3D: ligand conformation quality matters](http://arxiv.org/abs/2407.04424)|null|三维 (3D) 深度分子生成模型提供了基于 3D 依赖属性的目标导向生成的优势，例如结合腔内基于结构的设计的结合亲和力。 为评估 SMILES 或分子图生成器而创建的传统基准，例如 GuacaMol 或 MOSES，由于它们不评估生成的分子构象的质量，因此在评估 3D 生成器方面受到限制。 因此，我们在这项工作中开发了 GenBench3D，它实现了一个用于在结合腔内生成分子的新基准。 我们的主要贡献是 Validity3D 指标，它使用基于剑桥结构数据库中观察到的参考值的键长和价角的可能性来评估构象质量。 对 LiGAN、3D-SBDD、Pocket2Mol、TargetDiff、DiffSBDD 和 ResGen 模型进行了基准测试。 我们发现只有 0% 到 11% 的生成分子具有有效的构象。 对口袋中生成的分子进行局部弛豫，通过至少增加 40% 的 Validity3D，大大提高了所有模型的 Validity3D。 对于 LiGAN、3D-SBDD 或 TargetDiff，有效弛豫分子集显示的平均 Vina 分数（即更差）高于原始生成分子集，表明原始生成分子的结合亲和力可能被高估了。 使用其他评分函数（更重视配体应变）仅在使用有效的弛豫分子时才会产生改进的分数。 使用有效的弛豫分子，TargetDiff 和 Pocket2Mol 显示出比其他模型更好的中位 Vina、Glide 和 Gold PLP 分数。 我们已经在 GitHub 上公开发布了 GenBench3D 以供更广泛地使用：https://github.com/bbaillif/genbench3d|
|**2024-07-05**|[Improving Audio Generation with Visual Enhanced Caption](http://arxiv.org/abs/2407.04416)|null|生成模型在音频生成任务中取得了显著成果，但现有模型难以处理复杂和详细的提示，导致潜在的性能下降。我们假设这个问题源于训练数据的低质量和相对较小的数量。在这项工作中，我们的目标是创建一个带有丰富描述的大规模音频数据集，用于改进音频生成模型。我们开发了一个自动化管道，通过使用大型语言模型 (LLM) 将预测的视觉字幕、音频字幕和标签标签转换为全面的描述，从而为视听数据集生成详细的字幕。我们介绍了 Sound-VECaps，这是一个包含 166 万个高质量音频-字幕对的数据集，其中包含丰富的细节，包括音频事件顺序、发生地点和环境信息。我们证明，使用 Sound-VECaps 进行训练可以显著增强文本到音频生成模型理解和从复杂输入提示生成音频的能力，从而提高整体系统性能。此外，我们对 Sound-VECaps 在多个音频-语言任务中进行了消融研究，表明其在推进音频-文本表示学习方面的潜力。我们的数据集和模型可在网上获得。|
|**2024-07-05**|[Unsupervised Learning of Category-Level 3D Pose from Object-Centric Videos](http://arxiv.org/abs/2407.04384)|**[link](https://github.com/genintel/uns-obj-pose3d)**|类别级三维姿态估计是计算机视觉和机器人技术中的一个基本问题，例如对于具身代理或训练三维生成模型。然而，到目前为止，估计类别级物体姿态的方法要么需要大量的人工标注、CAD模型，要么需要来自RGB-D传感器的输入。相比之下，我们致力于解决仅从随意拍摄的以物体为中心的视频中学习估计类别级三维姿态的问题，无需人工监督。我们提出了一个两步流程：首先，我们引入了一种多视图对齐程序，该程序使用一种新颖且鲁棒的循环距离公式来确定视频之间的规范相机姿态，该公式使用重建的粗网格和DINOv2特征进行几何和外观匹配。其次，规范姿态和重建的网格使我们能够从单个图像中训练用于三维姿态估计的模型。具体来说，我们的模型通过预测二维图像中每个像素的模板网格中对应顶点的特征向量，来学习估计图像和原型三维模板之间的密集对应关系。我们证明，我们的方法在以物体为中心的视频的无监督对齐方面大大优于所有基线，并在实际应用中提供了可靠且鲁棒的预测。我们的代码和数据可在https://github.com/GenIntel/uns-obj-pose3d获取。|
|**2024-07-05**|[A Mapping Strategy for Interacting with Latent Audio Synthesis Using Artistic Materials](http://arxiv.org/abs/2407.04379)|null|本文提出了一种与生成式人工智能模型的潜在空间交互的映射策略。我们的方法涉及使用无监督特征学习对人类控制空间进行编码，并将其映射到音频合成模型的潜在空间。为了演示这种映射策略如何将高维传感器数据转化为深度生成模型的控制机制，我们提出了一个概念验证系统，该系统使用视觉草图来控制音频合成模型。我们借鉴 XAIxArts 中的新兴论述来讨论这种方法如何为艺术和创意环境中的 XAI 做出贡献，我们还讨论了它目前的局限性并提出了未来的研究方向。|
|**2024-07-05**|[MuseBarControl: Enhancing Fine-Grained Control in Symbolic Music Generation through Pre-Training and Counterfactual Loss](http://arxiv.org/abs/2407.04331)|null|自动生成符号音乐——根据特定人类需求量身定制的乐谱——对音乐家和爱好者来说非常有利。最近的研究表明，使用大型数据集和先进的Transformer架构取得了可喜的成果。然而，这些最先进的模型通常只对整首作品的节奏和风格等方面提供基本的控制，缺乏管理更精细细节的能力，例如在单个小节级别的控制。虽然微调预先训练好的符号音乐生成模型似乎是实现这种更精细控制的直接方法，但我们的研究表明这种方法存在挑战。该模型通常无法对新的、细粒度的小节级控制信号做出充分响应。为了解决这个问题，我们提出了两个创新的解决方案。首先，我们引入了一个预训练任务，旨在将控制信号直接与其相应的音乐符号链接起来，这有助于为后续的微调实现更有效的初始化。其次，我们实施了一种新的反事实损失，以促进生成的音乐与控制提示之间更好地保持一致。总的来说，这些技术显著增强了我们在小节级别控制音乐生成的能力，比传统方法提高了13.06%。我们的主观评价也证实，这种增强的控制并没有损害原始预训练生成模型的音乐质量。|
|**2024-07-03**|[DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents](http://arxiv.org/abs/2407.03300)|null|扩散模型 (DM) 为生成式学习带来了革命性的变化。它们利用扩散过程将数据编码为简单的 Gaussian 分布。然而，将复杂且可能具有多模态的数据分布编码为单个连续 Gaussian 分布无疑是一个不必要的具有挑战性的学习问题。我们提出离散-连续潜在变量扩散模型 (DisCo-Diff)，通过引入互补的离散潜在变量来简化此任务。我们使用可学习的离散潜在变量增强 DM，并使用编码器进行推断，并对 DM 和编码器进行端到端训练。DisCo-Diff 不依赖于预先训练的网络，这使得该框架具有普遍适用性。离散潜在变量通过降低 DM 生成 ODE 的曲率，显著简化了学习 DM 复杂噪声到数据映射的过程。一个额外的自回归 Transformer 模型对离散潜在变量的分布进行建模，这是一个简单的步骤，因为 DisCo-Diff 只需要具有少量码本的少量离散变量。我们在玩具数据、多个图像合成任务以及分子对接上验证了 DisCo-Diff，发现引入离散潜在变量始终可以提高模型性能。例如，DisCo-Diff 在使用 ODE 采样器的类条件 ImageNet-64/128 数据集上实现了最先进的 FID 分数。|
|**2024-07-03**|[Improved Noise Schedule for Diffusion Training](http://arxiv.org/abs/2407.03297)|null|扩散模型已成为生成视觉信号的首选方法。然而，训练单个模型来预测不同级别的噪声提出了重大挑战，需要多次迭代并导致巨大的计算成本。为了加快收敛速度，人们引入了各种方法，例如损失加权策略设计和架构改进。在本研究中，我们提出了一种设计噪声调度的新方法，以增强扩散模型的训练。我们的主要见解是，对数信噪比（logSNR）的重要性采样（理论上等效于修改后的噪声调度）对于提高训练效率特别有利，特别是在增加 $\log \text{SNR}=0$ 附近的采样频率时。我们通过经验证明了我们的噪声调度优于标准余弦调度。此外，我们还重点介绍了我们的噪声调度设计在 ImageNet 基准测试中的优势，表明所设计的调度始终有利于不同的预测目标。|
|**2024-07-03**|[Spatio-Temporal Adaptive Diffusion Models for EEG Super-Resolution in Epilepsy Diagnosis](http://arxiv.org/abs/2407.03089)|null|脑电图 (EEG) 技术，特别是高密度脑电图 (HD EEG) 设备，广泛应用于神经科学等领域。HD EEG 设备通过在头皮上放置更多电极来提高 EEG 的空间分辨率，满足癫痫病灶定位等临床诊断应用的要求。然而，该技术面临着采集成本高、使用场景有限等挑战。本文提出了时空自适应扩散模型 (STADM)，率先利用扩散模型实现从低分辨率 (LR, 64 通道或更少) EEG 到高分辨率 (HR, 256 通道) EEG 的空间超分辨率 (SR) 重建。具体而言，设计了一种时空条件模块来提取 LR EEG 的时空特征，然后将其作为条件输入来指导扩散模型的反向去噪过程。此外，构建了一个多尺度 Transformer 去噪模块，利用多尺度卷积块和基于交叉注意力的扩散 Transformer 块进行条件引导，生成自适应于受试者的 SR EEG。实验结果表明，该方法有效提高了 LR EEG 的空间分辨率，并在数量上优于现有方法。此外，STADM 通过将合成的 SR EEG 应用于癫痫患者的分类和源定位任务，证明了其价值，表明其具有显著提高 LR EEG 空间分辨率的潜力。|
|**2024-07-03**|[Artificial Inductive Bias for Synthetic Tabular Data Generation in Data-Scarce Scenarios](http://arxiv.org/abs/2407.03080)|null|虽然使用深度生成模型 (DGM) 生成合成表格数据为数据稀缺和隐私问题提供了一种引人注目的解决方案，但其有效性依赖于大量的训练数据，而这些数据在现实应用中通常不可用。本文提出了一种新颖的方法，用于在有限的真实数据环境中使用 DGM 生成真实可靠的合成表格数据，从而解决了这一挑战。我们的方法提出了几种通过迁移学习和元学习技术在 DGM 中生成人工归纳偏差的方法。我们在该框架内探索并比较了四种不同的方法，证明了预训练和模型平均等迁移学习策略优于模型无关元学习和域随机搜索等元学习方法。我们使用两种最先进的 DGM（变分自动编码器和生成对抗网络）验证了我们的方法，表明我们的人工归纳偏差提高了合成数据的质量（通过 Jensen-Shannon 散度衡量），在使用我们提出的方法时，相对增益高达 50%。这种方法在各种 DGM 和机器学习任务中具有广泛的适用性，特别是在医疗保健和金融等数据稀缺通常是关键问题的领域。|
|**2024-07-03**|[Electromagnetic Property Sensing Based on Diffusion Model in ISAC System](http://arxiv.org/abs/2407.03075)|null|集成传感与通信 (ISAC) 为未来的无线系统开辟了许多颠覆性的机遇。在本文中，我们开发了一种新颖的 ISAC 方案，利用扩散模型来感知预定传感区域中目标的电磁 (EM) 特性。具体来说，我们首先利用从目标反射回来的通信和传感信号来估计传感信道。然后，我们采用扩散模型生成代表目标的点云，从而实现目标电磁特性分布的 3D 可视化。为了最小化真实点云和估计点云之间的平均 Chamfer 距离 (MCD)，我们在最大发射功率和每个用户设备 (UE) 的最小通信可达速率的约束下，进一步设计了通信和传感波束赋形矩阵。仿真结果证明了该方法在实现目标形状、相对介电常数和电导率的高质量重建方面的有效性。此外，该方法可以在传感区域的任何位置有效地感知目标的电磁特性。|
|**2024-07-03**|[Semantic-Aware Power Allocation for Generative Semantic Communications with Foundation Models](http://arxiv.org/abs/2407.03050)|null|扩散模型的最新进展为生成建模带来了重大突破。生成模型与语义通信 (SemCom) 的结合能够以超低速率实现高保真语义信息交换。本文提出了一种用于图像任务的新型生成式 SemCom 框架，其中预训练的基础模型分别充当语义编码器和解码器，用于语义特征提取和图像再生。文章对传输可靠性与再生图像的感知质量以及语义特征的语义值之间的数学关系进行了建模，这些关系是通过对 Kodak 数据集进行数值模拟获得的。我们还研究了语义感知功率分配问题，目标是在保证语义性能的同时最小化总功耗。为了解决这个问题，分别通过约束解耦和二分搜索提出了两种语义感知功率分配方法。数值结果表明，与传统方法相比，所提出的语义感知方法在总功耗方面表现出优越的性能。|
|**2024-07-03**|[SlerpFace: Face Template Protection via Spherical Linear Interpolation](http://arxiv.org/abs/2407.03043)|null|当代人脸识别系统使用从人脸图像中提取的特征模板来识别身份。为了增强隐私性，人脸模板保护技术被广泛用于隐藏存储在模板中的敏感身份和外观信息。本文识别了一种新兴的利用扩散模型的隐私攻击形式，它可以使先前的保护无效，称为反演攻击。这种攻击可以从模板中合成高质量、保留身份的人脸图像，从而暴露人的外貌。基于对扩散模型生成能力的研究，本文提出了一种防御措施来削弱这种攻击，即通过将模板旋转到类似噪声的分布。这是通过在其所在的超球面上对模板进行球面和线性插值（slerp）来有效实现的。为了增强旋转模板的不可逆性，本文进一步提出对模板的特征维度进行分组划分和丢弃。组的划分和每个组内的丢弃以有利于识别的的方式学习。所提出的技术被具体化为一种新的人脸模板保护技术，SlerpFace。大量实验表明，SlerpFace 提供了令人满意的识别精度和全面的隐私保护，可以抵御反演和其他攻击形式，优于现有技术。|
|**2024-07-03**|[An Organism Starts with a Single Pix-Cell: A Neural Cellular Diffusion for High-Resolution Image Synthesis](http://arxiv.org/abs/2407.03018)|null|生成模型旨在逼近真实数据的统计特性，从而能够合成与原始分布非常相似的新数据。生成对抗网络 (GAN) 和去噪扩散概率模型 (DDPM) 代表了生成模型的重大进步，它们分别从博弈论和热力学中汲取灵感。然而，从生物进化的角度探索生成模型在很大程度上仍未得到开发。在本文中，我们介绍了一种称为生成元胞自动机 (GeCA) 的新型模型系列，其灵感来自于生物体从单细胞的进化。GeCA 被评估为一种有效的视网膜疾病分类增强工具，可用于两种成像模式：眼底和光学相干断层扫描 (OCT)。在 OCT 成像中，数据稀缺且类别分布存在固有的偏差，GeCA 显着提高了 11 种不同眼科疾病的表现，与传统基线相比，平均 F1 分数提高了 12%。在类似的参数约束下，GeCA 的性能优于包含 UNet 或基于 Transformer 的最新去噪模型的扩散方法。代码可在以下网址获取：https://github.com/xmed-lab/GeCA。|
|**2024-07-03**|[Frequency-Controlled Diffusion Model for Versatile Text-Guided Image-to-Image Translation](http://arxiv.org/abs/2407.03006)|**[link](https://github.com/xianggao1102/fcdiffusion)**|近年来，大规模文本到图像 (T2I) 扩散模型已成为一种强大的图像到图像转换 (I2I) 工具，允许通过用户提供的文本提示进行开放域图像转换。本文提出了频率控制扩散模型 (FCDiffusion)，这是一个基于扩散的端到端框架，从频域角度为文本引导的 I2I 提供了一种新颖的解决方案。我们框架的核心是一个基于离散余弦变换的特征空间频域滤波模块，它在 DCT 域中滤波源图像的潜在特征，产生具有不同 DCT 谱带的滤波图像特征，作为预训练的潜在扩散模型的不同控制信号。我们发现，不同 DCT 谱带的控制信号在不同的相关性（例如，风格、结构、布局、轮廓等）上桥接了源图像和 T2I 生成的图像，从而使多功能 I2I 应用能够强调不同的 I2I 相关性，包括风格引导的内容创建、图像语义操作、图像场景转换和图像风格转换。与相关方法不同，FCDiffusion 建立了一个统一的文本引导 I2I 框架，只需在推理时切换不同的频率控制分支，即可适用于各种图像转换任务。广泛的定性和定量实验都证明了我们的方法在文本引导 I2I 方面的有效性和优越性。代码公开于：https://github.com/XiangGao1102/FCDiffusion。|
|**2024-07-03**|[Towards a Scalable Reference-Free Evaluation of Generative Models](http://arxiv.org/abs/2407.02961)|**[link](https://github.com/aziksh-ospanov/fkea)**|虽然生成模型的标准评估分数大多是基于参考的，但由于缺乏适用的参考数据集，对生成模型进行依赖参考的评估通常很困难。最近，人们提出了无参考的熵分数 VENDI 和 RKE 来评估生成数据的多样性。然而，从数据中估计这些分数会导致大规模生成模型的计算成本很高。在这项工作中，我们利用随机傅里叶特征框架来降低计算成本，并提出了基于傅里叶的核熵逼近 (FKEA) 方法。我们利用 FKEA 对核矩阵的近似特征谱来有效地估计上述熵分数。此外，我们展示了 FKEA 代理特征向量的应用，以揭示该方法在评估生成样本多样性时识别的模式。我们提供了 FKEA 评估算法的随机实现，其复杂度为 $O(n)$，随样本大小 $n$ 线性增长。我们广泛评估了 FKEA 在标准图像、文本和视频数据集中的数值性能。我们的实验结果表明，该方法应用于大规模生成模型具有可扩展性和可解释性。代码库可在 https://github.com/aziksh-ospanov/FKEA 获取。|

## LLM

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-07-09**|[NoisyAG-News: A Benchmark for Addressing Instance-Dependent Noise in Text Classification](http://arxiv.org/abs/2407.06579)|null|现有的噪声标签学习研究主要集中于合成标签噪声。尽管合成噪声具有定义明确的结构特性，但它往往无法准确地复制现实世界的噪声模式。近年来，人们一直在努力构建用于图像分类的、可泛化且可控的实例依赖噪声数据集，这极大地促进了该领域抗噪声学习的发展。然而，关于文本分类中噪声标签学习的研究仍然很少。为了更好地理解现实世界文本分类环境中的标签噪声，我们通过人工标注构建了基准数据集 NoisyAG-News。首先，我们分析了标注数据，以收集关于现实世界噪声的观察结果。我们定性和定量地证明了现实世界的噪声标签遵循实例依赖模式。随后，我们使用预训练语言模型和噪声处理技术，对 NoisyAG-News 及其相应的合成噪声数据集进行了全面的学习实验。我们的研究结果表明，虽然预训练模型对合成噪声具有鲁棒性，但它们在面对实例依赖噪声时表现不佳，不同混淆程度的样本在训练和测试过程中表现出不一致的性能。这些现实世界的噪声模式带来了新的、重大的挑战，促使人们重新评估噪声标签处理方法。我们希望 NoisyAG-News 将促进未来噪声标签学习解决方案的开发和评估。|
|**2024-07-09**|[Enhancing Low-Resource NMT with a Multilingual Encoder and Knowledge Distillation: A Case Study](http://arxiv.org/abs/2407.06538)|null|神经机器翻译 (NMT) 仍然是一项艰巨的挑战，尤其是在处理低资源语言时。预训练的序列到序列 (seq2seq) 多语言模型，例如 mBART-50，在各种低资源 NMT 任务中表现出令人印象深刻的性能。然而，它们的预训练仅限于 50 种语言，不支持许多低资源语言，尤其是在印度次大陆使用的语言。扩展 mBART-50 的语言支持需要复杂的预训练，由于灾难性遗忘，可能会导致性能下降。考虑到这些不断扩大的挑战，本文探索了一个框架，该框架利用预训练语言模型的优势以及 seq2seq 架构中的知识蒸馏来促进低资源语言的翻译，包括 mBART-50 未涵盖的语言。所提出的框架采用基于多语言编码器的 seq2seq 模型作为基础架构，随后使用互补的知识蒸馏技术来减轻不平衡训练的影响。我们的框架在四个印度语到印度语方向上的三种低资源印度语上进行了评估，与基线相比，BLEU-4 和 chrF 得到了显着提高。此外，我们进行人工评估以确认我们方法的有效性。我们的代码可在 https://github.com/raypretam/Two-step-low-res-NMT 公开获取。|
|**2024-07-07**|[Advancing Prompt Recovery in NLP: A Deep Dive into the Integration of Gemma-2b-it and Phi2 Models](http://arxiv.org/abs/2407.05233)|null|提示恢复是自然语言处理中的一项关键任务，它需要重建语言模型用来将输入文本转换为特定输出的提示或指令。尽管至关重要，但提示的设计和有效性在自然语言处理研究中仍然是一个具有挑战性且相对未被开发的领域。本文深入研究了提示恢复方法，采用了各种预训练语言模型和策略。我们的研究是一个比较分析，旨在衡量各种模型在基准数据集上的有效性，目标是确定最有效的提示恢复方法。通过细致的实验和详细的分析，我们阐明了 Gemma-2b-it + Phi2 模型 + 预训练的出色性能。该模型超越了其他模型，在准确地重建文本转换任务的提示方面展现出卓越的能力。我们的研究结果为现有的提示恢复知识做出了重大贡献，揭示了提示设计的复杂性，并为文本重写和更广泛的自然语言处理领域的未来创新提供了深刻的见解。|
|**2024-07-06**|[The Solution for the AIGC Inference Performance Optimization Competition](http://arxiv.org/abs/2407.04991)|null|近年来，基于Transformer架构的大规模预训练语言模型的快速发展为自然语言处理任务带来了革命性的变化。其中，ChatGPT凭借其接近人类水平的对话能力获得了广泛的用户基础，截至2022年底已拥有超过1亿月活跃用户。与此同时，百度推出的文心大模型也已成功商业化落地，通过AI赋能技术显著提升了营销效率。本文重点研究如何优化文心模型的高性能推理，特别是利用GPU加速和Paddle推理框架进行优化。我们采用了多种技术手段，包括使用Faster Transformer进行高效模型处理、对Embedding层进行剪枝以减少计算开销、以及采用FP16半精度推理以提高计算效率。此外，我们还整合了高效的数据处理策略，利用多进程并行处理来最小化延迟。实验结果表明，相比于标准方法，我们提出的优化方案能够将推理速度提升高达8.96倍，同时保持了具有竞争力的性能表现。|
|**2024-07-03**|[MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language Models](http://arxiv.org/abs/2407.02775)|null|知识蒸馏是一种有效的预训练语言模型压缩技术。虽然现有的知识蒸馏方法对于最典型的模型BERT表现良好，但它们可以在两个方面进一步改进：可以进一步探索关系级知识以提高模型性能；并且学生注意力头数的设置可以更加灵活以减少推理时间。因此，我们提出了一个新的知识蒸馏方法MLKD-BERT，用于在师生框架中提取多级知识。在GLUE基准测试和抽取式问答任务上的大量实验表明，我们的方法优于BERT上的最新知识蒸馏方法。此外，MLKD-BERT可以灵活设置学生注意力头数，从而在性能下降很小的情况下大幅减少推理时间。|
|**2024-07-03**|[Supporting Cross-language Cross-project Bug Localization Using Pre-trained Language Models](http://arxiv.org/abs/2407.02732)|null|自动定位大型代码库中的错误对于开发人员来说仍然是一项重大挑战。现有技术由于依赖于特定于应用程序的数据和大型模型，因此在通用性和部署方面常常遇到困难。本文提出了一种基于预训练语言模型 (PLM) 的新型错误定位技术，该技术超越了项目和语言的界限。我们的方法利用对比学习来增强错误报告和源代码的表示。然后，它利用一种结合提交消息和代码段的新颖排序方法。此外，我们引入了一种知识蒸馏技术，可以在不影响性能的情况下减小模型大小以用于实际部署。本文提出了几个主要优点。通过将代码段和提交消息分析与传统的代码文件级别检查相结合，我们的技术实现了更高的错误定位精度。此外，我们的模型在通用性方面表现出色——在来自各种项目和语言的代码上进行训练，它可以有效地识别看不见的代码库中的错误。为了解决计算限制，我们提出了一种与 CPU 兼容的解决方案。总而言之，我们提出的工作提出了一种高效、通用且高效的错误定位技术，具有实际部署的潜力。|
|**2024-07-02**|[Ensemble of pre-trained language models and data augmentation for hate speech detection from Arabic tweets](http://arxiv.org/abs/2407.02448)|null|如今，从阿拉伯语推文中分类仇恨言论引起了众多研究者的关注。人们已经开发了许多系统和技术来解决这个分类任务。然而，在这方面面临的两个主要挑战是有限的性能和数据不平衡的问题。在本研究中，我们提出了一种利用集成学习和基于先前手动标记的半监督学习的新方法。我们通过将阿拉伯语推文分为 5 个不同的类别：非仇恨、一般仇恨、种族、宗教或性别歧视，在一个基准数据集上进行了实验。实验结果表明：(1) 基于预训练语言模型的集成学习优于现有的相关工作；(2) 我们提出的数据增强方法提高了从阿拉伯语推文中检测仇恨言论的准确率，并优于现有的相关工作。我们的主要贡献是在阿拉伯语仇恨言论检测方面取得了令人鼓舞的结果。|
|**2024-07-02**|[Efficient Nearest Neighbor based Uncertainty Estimation for Natural Language Processing Tasks](http://arxiv.org/abs/2407.02138)|null|深度神经网络 (DNN) 包括预训练语言模型 (PLM) 中的可信预测对于现实世界中安全关键型应用至关重要。然而，DNN 经常会遇到不确定性估计问题，例如校准错误。特别是，需要多次随机推理的方法可以缓解这个问题，但昂贵的推理成本使得它们不切实际。在本研究中，我们提出了 $k$近邻不确定性估计（$k$NN-UE），这是一种利用来自邻居的距离和邻居的标签存在率的不确定性估计方法。在情感分析、自然语言推理和命名实体识别方面的实验表明，我们提出的方法在置信度校准、选择性预测和分布外检测方面优于基线或最近基于密度的方法。此外，我们的分析表明，引入降维或受最近$k$ NN-LM 研究启发的近似最近邻搜索，可以在不显着降低估计性能的情况下降低推理开销。|
|**2024-07-01**|[Bridging the Gap: Transfer Learning from English PLMs to Malaysian English](http://arxiv.org/abs/2407.01374)|null|马来西亚英语是一种低资源的混合语，除了标准英语之外，它还包含马来语、汉语和泰米尔语的元素。命名实体识别 (NER) 模型在从马来西亚英语文本中捕获实体时表现不佳，因为它具有独特的形态句法适应、语义特征和语码转换（混合英语和马来语）。考虑到这些差距，我们引入了 MENmBERT 和 MENBERT，这是一种具有上下文理解能力的预训练语言模型，专为马来西亚英语量身定制。我们使用来自马来西亚英语新闻文章 (MEN) 数据集的手动注释实体和关系微调了 MENmBERT 和 MENBERT。这种微调过程使 PLM 能够学习表示，这些表示捕获与 NER 和 RE 任务相关的马来西亚英语的细微差别。与 bert-base-multilingual-cased 模型相比，MENmBERT 在 NER 和 RE 任务上分别实现了 1.52% 和 26.27% 的改进。尽管 NER 的整体性能没有显着提高，但我们进一步的分析表明，按 12 个实体标签进行评估时，性能有显着提高。这些发现表明，在特定语言和地理位置的语料库上预训练语言模型可能是提高低资源环境中 NER 性能的一种很有前景的方法。本文发布的数据集和代码为专注于马来西亚英语的 NLP 研究工作提供了宝贵的资源。|
|**2024-07-01**|[Language Portability Strategies for Open-domain Dialogue with Pre-trained Language Models from High to Low Resource Languages](http://arxiv.org/abs/2407.01315)|null|本文研究用于开放域对话系统的大型预训练语言模型 (PLM) 在高资源语言中的语言可移植性策略。具体来说，目标低资源语言 (L_T) 将使用法语进行模拟，因为它缺乏特定于任务的资源，并且允许我们进行人工评估，而源语言 (L_S) 为英语。出于显而易见的原因，最近使用此类模型进行开放域对话的工作大多是用英语开发的。然而，为每种可能的目标语言构建特定的 PLM 需要收集新的数据集，而且成本高昂。出于这个原因，我们希望尝试利用 L_S 和 L_T 中的所有现有资源（PLM 和数据），评估使用不同方法在 L_T 中可实现的性能。前两种方法评估了神经机器翻译 (NMT) 在不同级别的使用：TrainOnTarget，其中在 L_T 中微调之前翻译 L_S 数据集，以及 TestOnSource，其中 L_S 模型在推理过程中与 NMT 模块耦合。然后，全球第一个开放获取的多语言大型 PLM BLOOM [2] 的出现，使研究人员能够开发新的方法，旨在不仅利用模型的完全可访问性，还利用其多语言性和翻译能力。在这种情况下，首先在 L_S 中学习任务，然后使用 MAD-X 适配器架构 [16] 适应 L_T。在这两组实验中，模型在口语对话条件下与人类进行评估，并且可以根据感知的交互质量比较策略。|
|**2024-07-01**|[A Fingerprint for Large Language Models](http://arxiv.org/abs/2407.01235)|null|近期研究表明，扩展预训练语言模型可以在许多下游任务上实现最先进的性能，这使得大型语言模型（LLM）成为人工智能领域的热门研究课题。然而，由于从头开始训练LLM需要大量的资源，因此保护LLM的知识产权免遭侵权至关重要且紧迫。这促使本文作者提出了一种新颖的LLM黑盒指纹识别技术，该技术既不需要模型训练也不需要模型微调。我们首先证明LLM的输出跨越与每个模型相关的唯一向量空间。我们将所有权认证问题建模为评估受害模型空间与嫌疑模型输出空间之间相似性的任务。为了解决这个问题，我们提出了两种解决方案，其中第一个解决方案涉及验证可疑大型模型的输出是否与受害模型的输出位于相同的空间中，从而能够快速识别模型侵权；第二个解决方案重建LLM输出和受害模型的向量空间的并集，以解决受害模型遭受参数高效微调（PEFT）攻击的情况。实验结果表明，所提出的技术在所有权验证和抵御PEFT攻击方面取得了优异的性能。这项工作揭示了LLM的固有特性，并为黑盒场景下的LLM所有权验证提供了一种有前景的解决方案，确保了效率、通用性和实用性。|
|**2024-07-01**|[Development of Cognitive Intelligence in Pre-trained Language Models](http://arxiv.org/abs/2407.01047)|null|最近的研究表明，大型预训练语言模型 (PLM) 出现了认知能力。这些模型不断提高的认知一致性使其成为认知科学理论的候选者。先前对 PLM 涌现认知能力的研究很大程度上与模型训练路径无关，即侧重于最终的模型权重而不是中间步骤。然而，使用 PLM 构建合理的人类认知模型将受益于考虑其在训练期间的表现与儿童思维轨迹的发展一致性。在人类智力心理测量测试的指导下，我们选择了四组任务来研究十个流行的 PLM 家族的一致性，并评估它们可用的中间和最终训练步骤。这些任务是数字能力、语言能力、概念理解和流体推理。我们发现了一个惊人的规律：无论模型大小如何，PLM 的发展轨迹始终表现出一个与人类认知发展最大程度一致的窗口。在该窗口之前，训练似乎赋予“空白石板”模型以必要的结构，使其能够从经验中快速学习。在该窗口之后，训练似乎服务于降低损失的工程目标，而不是提高与人类认知一致性的科学目标。|
|**2024-07-01**|[Cross-Modal Attention Alignment Network with Auxiliary Text Description for zero-shot sketch-based image retrieval](http://arxiv.org/abs/2407.00979)|null|本文研究了基于零样本草图的图像检索问题 (ZS-SBIR)。先前的方法在只有类别标签甚至没有文本信息的双模态设置中解决该问题。然而，大规模预训练语言模型 (LLM) 的日益普及，展现出从网络规模数据中学习到的丰富知识，为我们提供了一个总结集体文本信息的机会。我们的主要创新在于使用文本数据作为图像的辅助信息，从而利用语言提供的固有的零样本泛化能力。为此，我们提出了一种名为“基于辅助文本描述的跨模态注意力对齐网络”的方法，用于零样本草图图像检索。该网络由三个部分组成：(i) 描述生成模块，通过使用几个疑问句提示LLM，为每个训练类别生成文本描述；(ii) 特征提取模块，包括用于草图和图像数据的两个ViT、用于提取每个训练类别句子标记的转换器；最后 (iii) 跨模态对齐模块，使用交叉注意力机制交换文本-草图和文本-图像的标记特征，并在局部和全局范围内对齐标记。在三个基准数据集上的大量实验表明，我们的方法优于最先进的 ZS-SBIR 方法。|
|**2024-06-30**|[NAIST Simultaneous Speech Translation System for IWSLT 2024](http://arxiv.org/abs/2407.00826)|null|本文描述了NAIST提交给IWSLT 2024评测活动同步赛道的系统：英语到{德语、日语、汉语}的语音到文本翻译和英语到日语的语音到语音翻译。我们开发了一种多语言端到端语音到文本翻译模型，该模型结合了两个预训练语言模型HuBERT和mBART。我们使用两种解码策略训练该模型：局部一致性（LA）和AlignAtt。提交的模型采用LA策略，因为它在之前的模型中优于AlignAtt策略。我们的语音到语音翻译方法是上述语音到文本模型与增量文本到语音（TTS）模块的级联，该模块包含音素估计模型、并行声学模型和并行WaveGAN声码器。我们通过将采用AlignAtt策略的Transformer架构应用于估计模型来改进增量TTS。结果表明，我们升级后的TTS模块有助于提高系统性能。|

## Transformer

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-07-09**|[CAPformer: Compression-Aware Pre-trained Transformer for Low-Light Image Enhancement](http://arxiv.org/abs/2407.07056)|null|弱光图像增强(LLIE)随着手机摄影需求的激增而取得了进步，但许多现有方法忽略了压缩，而压缩是资源受限的手机摄影的一个关键问题。大多数LLIE方法忽视了这一点，阻碍了它们的有效性。在这项研究中，我们调查了JPEG压缩对弱光图像的影响，并揭示了由于暗区普遍存在低像素值，JPEG压缩导致大量信息丢失。因此，我们提出了压缩感知预训练Transformer (CAPformer)，采用一种新的预训练策略，从未压缩的弱光图像中学习无损信息。此外，提出的亮度引导自注意力(BGSA)机制增强了合理的信息收集。实验表明，我们的方法在减轻压缩对LLIE的影响方面具有优越性，展示了其在资源受限场景下改进LLIE的潜力。|
|**2024-07-09**|[ERQ: Error Reduction for Post-Training Quantization of Vision Transformers](http://arxiv.org/abs/2407.06794)|null|视觉Transformer（ViT）的训练后量化（PTQ）因其在模型压缩方面的效率而备受关注。然而，现有方法通常忽略了量化权重和激活之间错综复杂的相互依赖性，导致了相当大的量化误差。在本文中，我们提出了ERQ，一种精心设计的两步PTQ方法，用于依次减少由激活和权重量化引起的量化误差。ERQ首先引入了激活量化误差减少（Aqer），该方法策略性地将激活量化误差的最小化制定为岭回归问题，并通过使用全精度更新权重来解决该问题。随后，ERQ引入了权重量化误差减少（Wqer），该方法采用迭代方法来减轻由权重量化引起的量化误差。在每次迭代中，采用经验导出的高效代理来细化量化权重的舍入方向，并结合岭回归求解器来减少权重量化误差。实验结果证明了我们方法的有效性。值得注意的是，对于W3A4 ViT-S，ERQ在准确率方面比最先进的GPTQ高出22.36%。|
|**2024-07-09**|[Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules](http://arxiv.org/abs/2407.06677)|null|在Transformer中，是否总是需要从浅层到深层计算token？Vanilla Transformer及其变体的持续成功似乎给出了毫无疑问的“是”。然而，在这项工作中，我们试图打破这种深度有序的惯例，提出了一种名为模块混合（MoM）的新颖架构，其动机是基于一种直觉，即任何层，无论其位置如何，只要具备所需的处理能力，都可以用来计算token。MoM的构建始于一组有限的模块，这些模块由多头注意力机制和前馈网络定义，每个模块都由其独特的参数化来区分。然后，两个路由器迭代地从集合中选择注意力模块和前馈模块来处理token。这种选择在token的前向传递过程中动态扩展计算图，最终形成模块的组合。我们证明了MoM不仅为Transformer及其众多变体提供了一个统一的框架，而且还提供了一种灵活且可学习的方法来减少Transformer参数化中的冗余。我们使用OpenWebText预训练了各种MoM。实验结果表明，不同参数量的MoM在GLUE和XSUM基准测试中始终优于Vanilla Transformer。更有趣的是，在固定的参数预算下，与GPT-2-large相比，MoM-large能够将计算图的深度增加38%以上，从而在GLUE上获得1.4的绝对增益，在XSUM上获得1的绝对增益。另一方面，MoM-large还可以将深度减少60%以上，同时每层包含更多模块，与GPT-2-large相比，TFLOPs减少了16%，内存使用量减少了43%，同时保持了相当的性能。|
|**2024-07-09**|[CTRL-F: Pairing Convolution with Transformer for Image Classification via Multi-Level Feature Cross-Attention and Representation Learning Fusion](http://arxiv.org/abs/2407.06673)|null|Transformer因其强大的容量和全局处理能力，在计算机视觉领域受到越来越多的关注。然而，Transformer是数据密集型的，与卷积神经网络（ConvNets）相比，其泛化能力受到限制，特别是在数据有限的情况下进行训练时，因为它们缺乏ConvNets中存在的内置空间归纳偏差。在本文中，我们致力于将卷积和Transformer的优势结合起来，以完成图像分类任务。为此，我们提出了一种名为CTRL-F的新型轻量级混合网络，该网络通过表示学习融合和多级特征交叉注意力将卷积与Transformer配对。我们的网络包括一个卷积分支和一个名为多级特征交叉注意力（MFCA）的新型Transformer模块。MFCA模块对在不同卷积阶段获得的多级特征表示进行操作。它通过两个独立的Transformer分支处理从这些多级特征表示中提取的小块标记和大块标记，这两个分支通过交叉注意力机制进行通信和知识交换。我们使用称为自适应知识融合（AKF）和协作知识融合（CKF）的新型表示融合技术，将从卷积路径获得的局部响应与从MFCA模块获得的全局响应融合在一起。实验表明，无论是在大数据集上从头开始训练，还是在数据量少的情况下，我们的CTRL-F变体都能达到最先进的性能。例如，CTRL-F在Oxford-102 Flowers和PlantVillage数据集上从头开始训练时，Top-1准确率分别达到82.24%和99.91%，超过了最先进的模型，这体现了我们模型在图像分类任务上的鲁棒性。代码地址：https://github.com/hosamsherif/CTRL-F|
|**2024-07-09**|[Enhancing spatial auditory attention decoding with neuroscience-inspired prototype training](http://arxiv.org/abs/2407.06498)|null|空间听觉注意解码 (Sp-AAD) 技术旨在通过神经记录来确定多说话者场景中的听觉注意方向。尽管最近的 Sp-AAD 算法取得了成功，但其性能受到脑电图数据中特定于试验的特征的阻碍。本研究旨在针对这些特征提高解码性能。神经科学研究表明，空间听觉注意可以反映在不同频带脑电图能量的拓扑分布中。这一见解促使我们提出原型训练，这是一种受神经科学启发的 Sp-AAD 方法。该方法构建了具有增强的能量分布表示和减少的特定于试验的特征的原型，使模型能够更好地捕获听觉注意特征。为了实施原型训练，进一步提出了一种采用脑电图小波变换的 EEGWaveNet。详细的实验表明，采用原型训练的 EEGWaveNet 在各种数据集上均优于其他竞争模型，并且也验证了所提出方法的有效性。作为一种独立于模型架构的训练方法，原型训练为 Sp-AAD 领域提供了新的见解。|
|**2024-07-08**|[FGA: Fourier-Guided Attention Network for Crowd Count Estimation](http://arxiv.org/abs/2407.06110)|null|人群计数在城市规划、人群管理和公共安全等领域越来越具有社会意义。本文介绍了一种新颖的用于人群计数估计的注意力机制——傅里叶引导注意力（FGA），旨在解决现有基于卷积的注意力网络中全局模式捕获效率低下的问题。FGA 通过利用快速傅里叶变换 (FFT) 以及用于全局特征的空间注意力和用于半全局和局部特征的通道注意力卷积，有效地捕获了包括全尺度全局模式在内的多尺度信息。FGA 的架构涉及一种双路径方法：(1) 通过 FFT 处理全尺度全局特征的路径，允许在频域中高效提取信息，以及 (2) 使用传统卷积和通道注意力处理剩余特征图以获取半全局和局部特征的路径。这种双路径架构使 FGA 能够无缝集成频率和空间信息，增强其捕获不同人群模式的能力。我们将 FGA 应用于两个流行的人群计数工作 CSRNet 和 CANNet 的最后几层，以评估模块在基准数据集（如 ShanghaiTech-A、ShanghaiTech-B、UCF-CC-50 和 JHU++ 人群）上的性能。实验表明，基于均方误差 (MSE) 和平均绝对误差 (MAE) 指标，所有数据集都有显着改进，显示出与最近最先进方法相当的性能。此外，我们利用 Grad-CAM 热图，使用定性分析来说明可解释性，以显示 FGA 在捕获人群模式方面的有效性。|
|**2024-07-08**|[HiT-SR: Hierarchical Transformer for Efficient Image Super-Resolution](http://arxiv.org/abs/2407.05878)|null|Transformer 在包括图像超分辨率 (SR) 在内的计算机视觉任务中表现出了良好的性能。然而，流行的基于 Transformer 的 SR 方法通常采用计算复杂度与窗口大小呈二次关系的窗口自注意力机制，导致采用固定的较小窗口，感受野有限。在本文中，我们提出了一种将基于 Transformer 的 SR 网络转换为分层 Transformer (HiT-SR) 的通用策略，在保持高效设计的同时，利用多尺度特征提升 SR 性能。具体来说，我们首先将常用的固定小窗口替换为扩展的分层窗口，以聚合不同尺度的特征并建立远程依赖关系。考虑到大窗口所需的密集计算，我们进一步设计了一种空间-通道相关性方法，该方法对窗口大小具有线性复杂度，可以有效地从分层窗口中收集空间和通道信息。大量实验验证了我们 HiT-SR 的有效性和效率，我们改进后的 SwinIR-Light、SwinIR-NG 和 SRFormer-Light 版本以更少的参数、FLOPs 和更快的速度（约 7 倍）获得了最先进的 SR 结果。|
|**2024-07-08**|[MSTF: Multiscale Transformer for Incomplete Trajectory Prediction](http://arxiv.org/abs/2407.05671)|null|运动预测在自动驾驶系统中起着至关重要的作用，它使车辆能够根据周围车辆的预测执行碰撞警告和合理的局部路径规划。然而，普遍的方法通常假设观察到的轨迹是完整的，而忽略了物体遮挡、范围限制和传感器故障导致的缺失值可能产生的影响。这种疏忽不可避免地会影响轨迹预测的准确性。为了应对这一挑战，我们提出了一个名为多尺度变换器（MSTF）的端到端框架，该框架专为不完整轨迹预测而精心设计。MSTF集成了多尺度注意力头（MAH）和基于信息增量的模式自适应（IIPA）模块。具体来说，MAH组件利用多头注意力机制，从不同的时间粒度同时捕获轨迹序列的多尺度运动表示。这种方法有助于在不同尺度上对运动中的全局依赖关系进行建模，从而减轻缺失值的不利影响。此外，IIPA模块通过分析数据中的缺失模式，自适应地提取跨时间步长的运动连续性表示。连续性表示在更高层次上描绘了运动趋势，引导MSTF生成与运动连续性一致的预测。我们使用两个大规模真实世界数据集评估了我们提出的MSTF模型。实验结果表明，MSTF在不完整轨迹预测任务中优于最先进（SOTA）模型，展示了其在解决自动驾驶系统运动预测中缺失值挑战方面的有效性。|
|**2024-07-08**|[Graph Attention with Random Rewiring](http://arxiv.org/abs/2407.05649)|null|图神经网络 (GNN) 已成为图结构深度学习的基础。现代 GNN 的关键范式包括消息传递、图重连和图 Transformer。本文介绍了具有随机结构的图重连注意力机制 (GRASS)，这是一种结合了这三种范式优点的新型 GNN 架构。GRASS 通过叠加随机正则图来重新连接输入图，增强了远程信息传播，同时保留了输入图的结构特征。它还采用了一种专为图结构数据量身定制的独特加性注意力机制，在保持计算效率的同时提供了图归纳偏差。我们的实证评估表明，GRASS 在多个基准数据集上实现了最先进的性能，证实了其实用性。|
|**2024-07-08**|[On the Power of Convolution Augmented Transformer](http://arxiv.org/abs/2407.05591)|null|Transformer架构在语言建模方面引发了革命性的进步。然而，最近的架构方法，例如状态空间模型，已经弥合了性能差距。受此启发，我们研究了卷积增强型Transformer（CAT）在召回、复制和长度泛化任务中的优势。CAT在注意力层的K/Q/V嵌入中加入了卷积滤波器。通过CAT，我们展示了卷积的局部性与注意力的全局视图协同作用。与诸如Mamba或Transformer等类似架构不同，CAT可以使用单层可证明地解决关联召回（AR）和复制任务，同时还享有保证的长度泛化能力。我们还通过描述卷积如何通过总结上下文窗口和创建突出的摘要标记来参与注意力，从而减轻对完全注意力的需求，从而建立了卷积和注意力之间的计算权衡。对真实数据集的评估证实了我们的发现，并证明CAT及其变体确实增强了语言建模性能。|
|**2024-07-05**|[Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations](http://arxiv.org/abs/2407.04543)|null|模型需要适当的归纳偏置才能有效地从少量数据中学习并在训练分布之外进行系统泛化。 虽然 Transformer 具有高度的通用性和强大的功能，但它们仍然可以从增强的结构性归纳偏置中受益，以完成 seq2seq 任务，尤其是那些涉及句法转换的任务，例如将主动语态转换为被动语态或语义解析。在本文中，我们建议通过中间预训练来增强 Transformer 的结构性归纳偏置，以根据转换的描述对依存树执行合成生成的句法转换。我们的实验表明，这有助于对组块等句法任务进行少样本学习，并且还提高了语义解析的结构泛化能力。我们的分析表明，中间预训练会导致注意力头能够跟踪需要对哪些标记应用哪些句法转换，并且模型可以在下游任务中利用这些注意力头。|
|**2024-07-05**|[LayerShuffle: Enhancing Robustness in Vision Transformers by Randomizing Layer Execution Order](http://arxiv.org/abs/2407.04513)|null|由于人工神经网络的架构和训练方式，它们通常对测试时的剪枝、替换或打乱层顺序的操作缺乏鲁棒性。然而，这些特性对于不同的应用场景非常重要，例如在分布式神经网络架构中，执行顺序无法得到保证，或者网络的某些部分在推理过程中可能发生故障。为了解决这些问题，我们提出了一系列针对视觉Transformer的训练方法，其最重要的组成部分是在训练时随机化注意力模块的执行顺序。我们证明，使用我们提出的方法，假设可以容忍在相同模型规模下精度降低（约20%），视觉Transformer确实能够适应测试时任意层的执行顺序。我们还发现，我们训练的模型可以彼此随机合并，生成功能完备的“科学怪人”模型，并且与源模型相比，性能没有损失。最后，我们在测试时对模型进行层剪枝，发现其性能下降平缓。|
|**2024-07-05**|[Hard-Attention Gates with Gradient Routing for Endoscopic Image Computing](http://arxiv.org/abs/2407.04400)|null|为了解决胃肠道息肉大小评估中的过拟合问题并增强模型泛化能力，我们的研究引入了特征选择门（FSG）或硬注意力门（HAG）以及梯度路由（GR）来进行动态特征选择。该技术旨在通过促进稀疏连接来增强卷积神经网络（CNN）和视觉Transformer（ViT），从而减少过拟合并增强泛化能力。HAG 通过使用可学习权重的稀疏化来实现这一点，作为一种正则化策略。GR 通过独立于主模型的两次前向传递优化 HAG 参数，进一步完善了这一过程，以改进特征重新加权。我们的评估涵盖了多个数据集，包括用于广泛影响评估的 CIFAR-100 和专注于息肉大小估计的专业内窥镜数据集（REAL-Colon、Misawa 和 SUN），涵盖超过 370,000 帧图像中的 200 多个息肉。研究结果表明，我们增强的 HAG 网络大大提高了与息肉大小相关的二分类和三分类任务的性能。具体而言，CNN 在二分类中的 F1 分数提高到 87.8%，而在三分类中，ViT-T 模型的 F1 分数达到 76.5%，优于传统的 CNN 和 ViT-T 模型。为了促进进一步的研究，我们发布了代码库，其中包括 CNN、多流 CNN、ViT 和 HAG 增强变体的实现。该资源旨在标准化内窥镜数据集的使用，为胃肠道息肉大小估计提供公开的训练-验证-测试拆分，以进行可靠和可比较的研究。代码库可在 github.com/cosmoimd/feature-selection-gates 获取。|
|**2024-07-05**|[Batch Transformer: Look for Attention in Batch](http://arxiv.org/abs/2407.04218)|null|人脸表情识别 (FER) 在计算机视觉领域受到了广泛关注，尤其是在人机交互等“自然环境”中。然而，FER 图像存在遮挡、低分辨率、姿态变化、光照变化和主观性等不确定因素，其中包括一些与目标标签不匹配的表情。因此，从一张包含噪声的单个图像中获取的信息很少，而且不可信。这可能会显著降低 FER 任务的性能。为了解决这个问题，我们提出了一种批量转换器 (BT)，它包含所提出的类别批量注意力 (CBA) 模块，通过训练一批图像中反映的特征，而不是来自单个图像的信息，来防止噪声数据过拟合并提取可靠信息。我们还提出了多级注意力 (MLA) 机制，通过捕获每个级别之间的相关性来防止过度拟合特定特征。在本文中，我们提出了一个结合上述方案的批量转换器网络 (BTN)。在各种 FER 基准数据集上的实验结果表明，所提出的 BTN 在 FER 数据集中始终优于最先进的方法。代表性结果证明了所提出的 BTN 在 FER 中的前景。|
|**2024-07-04**|[Adaptive Step-size Perception Unfolding Network with Non-local Hybrid Attention for Hyperspectral Image Reconstruction](http://arxiv.org/abs/2407.04024)|null|深度展开方法和Transformer架构最近在高光谱图像（HSI）重建方面展现出良好的结果。然而，仍然存在两个问题：（1）在数据子问题中，大多数方法使用可学习参数表示步长。然而，对于不同的光谱通道，特征和真实值之间的误差是不相等的。(2) Transformer难以平衡感受野大小和像素级细节信息。为了克服上述缺点，我们提出了一种自适应步长感知展开网络（ASPUN），这是一种基于FISTA算法的深度展开网络，它使用自适应步长感知模块来估计每个光谱通道的更新步长。此外，我们设计了一个非局部混合注意力Transformer（NHAT）模块，用于充分利用Transformer的感受野优势。通过将NLHA插入非局部信息聚合（NLIA）模块，展开网络可以获得更好的重建结果。实验结果表明，我们的ASPUN优于现有的SOTA算法，并取得了最佳性能。|
|**2024-07-03**|[Towards Attention-based Contrastive Learning for Audio Spoof Detection](http://arxiv.org/abs/2407.03514)|null|视觉Transformer（ViT）在计算机视觉分类任务中取得了重大进展。最近，Gong等人（2021）将基于注意力的建模方法引入了几项音频任务。然而，使用ViT进行音频欺骗检测任务的研究相对较少。我们弥合了这一差距，并将ViT引入到这项任务中。基于对SSAST（Gong等人，2022）音频ViT模型进行微调的朴素基线模型实现了次优的等错误率（EER）。为了提高性能，我们提出了一种新颖的基于注意力的对比学习框架（SSAST-CL），该框架使用交叉注意力来辅助表示学习。实验表明，我们的框架成功地 disentangled 了真实和欺骗类别，并有助于学习更好的分类器来完成这项任务。通过适当的数据增强策略，在我们的框架上训练的模型在ASVSpoof 2021挑战赛中取得了具有竞争力的性能。我们提供了比较和消融研究来证明我们的观点。|
|**2024-07-03**|[STF: Sentence Transformer Fine-Tuning For Topic Categorization With Limited Data](http://arxiv.org/abs/2407.03253)|null|如今，从推文中进行主题分类引起了相当多的研究关注。由于这些研究工作，人们提出了不同的分类系统。然而，由于标记数据的数量有限，导致性能指标低下，它们面临着重大挑战。我们提出了句子转换器微调 (STF)，这是一个主题检测系统，它利用预训练的句子转换器模型和微调来准确地对推文主题进行分类。此外，我们还进行了广泛的参数敏感性分析，以针对我们的主题分类任务微调 STF 参数，从而获得最佳性能结果。在两个基准数据集上的实验表明：(1) 所提出的 STF 可以有效地用于对推文主题进行分类，并且优于最新的最先进方法，(2) 所提出的 STF 不需要大量的标记推文就能达到良好的准确性，而这是许多最先进方法的局限性。我们的主要贡献是通过应用预训练的句子转换器语言模型在推文主题分类方面取得了可喜的成果。|
|**2024-07-03**|[Visual Grounding with Attention-Driven Constraint Balancing](http://arxiv.org/abs/2407.03243)|null|与目标检测不同，视觉定位任务需要检测由复杂的自由形式语言描述的对象。为了同时对这种复杂的语义和视觉表示进行建模，最近最先进的研究采用基于Transformer的模型来融合来自两种模态的特征，并进一步引入了各种模块来调制视觉特征，使其与语言表达对齐并消除不相关的冗余信息。然而，它们的损失函数仍然采用常见的目标检测损失，仅仅控制边界框回归输出，未能完全优化上述目标。为了解决这个问题，本文首先分析了基于Transformer模型的注意力机制。在此基础上，我们进一步提出了一个名为注意力驱动约束平衡（AttBalance）的新框架，以优化语言相关区域内视觉特征的行为。大量的实验结果表明，我们的方法带来了令人印象深刻的改进。具体来说，我们在四个不同基准上评估的五种不同模型上都取得了持续的改进。此外，通过将我们的方法集成到QRNet中，我们实现了新的最先进的性能。|
|**2024-07-03**|[Learning Disentangled Representation in Object-Centric Models for Visual Dynamics Prediction via Transformers](http://arxiv.org/abs/2407.03216)|null|最近的研究表明，以对象为中心的表示可以极大地提高学习动力学的准确性，同时也增强了可解释性。在这项工作中，我们将这一想法更进一步，提出了以下问题：“学习解耦表示能否进一步提高以对象为中心的模型中视觉动力学预测的准确性？” 虽然之前已经有一些尝试学习静态图像的解耦表示\citep{nsb}，但据我们所知，我们的工作是第一个尝试在视频的通用环境中做到这一点的工作，而没有对对象可能具有的属性类型做出任何特定假设。我们架构的关键构建块是“块”的概念，其中多个块共同构成一个对象。每个块都表示为给定数量的可学习概念向量的线性组合，并在学习过程中迭代优化。我们模型中的块是以无监督的方式发现的，通过类似于发现槽\citep{slot_attention}的方式关注对象掩码，以学习密集的以对象为中心的表示。我们在发现的块上采用 Transformer 进行自注意力机制来预测下一个状态，从而发现视觉动力学。我们在几个二维和三维基准数据集上进行了一系列实验，证明我们的架构 (1) 可以发现语义上有意义的块 (2) 与最先进的以对象为中心的模型相比，有助于提高动力学预测的准确性 (3) 在训练期间未见过特定属性组合的 OOD 设置中表现明显更好。我们的实验强调了发现解耦表示对于视觉动力学预测的重要性。|
|**2024-07-03**|[Relating CNN-Transformer Fusion Network for Change Detection](http://arxiv.org/abs/2407.03178)|**[link](https://github.com/nust-machine-intelligence-laboratory/rctnet)**|虽然深度学习，特别是卷积神经网络（CNN），已经彻底改变了遥感（RS）变化检测（CD），但现有方法由于忽略了全局上下文和不完整的变化学习，经常遗漏关键特征。此外，transformer 网络难以处理低级细节。RCTNet 通过引入以下内容解决了这些限制：\textbf{(1)} 早期融合骨干网络，用于尽早利用空间和时间特征；\textbf{(2)} 跨阶段聚合（CSA）模块，用于增强时间表示；\textbf{(3)} 多尺度特征融合（MSF）模块，用于在解码器中丰富特征提取；\textbf{(4)} 高效自解密注意力（ESA）模块，利用 Transformer 捕获全局信息和细粒度细节，以实现准确的变化检测。大量实验表明，RCTNet 明显优于传统的遥感图像变化检测方法，显示出显著的改进，并在准确性和计算成本之间取得了最佳平衡。|
|**2024-07-03**|[ISWSST: Index-space-wave State Superposition Transformers for Multispectral Remotely Sensed Imagery Semantic Segmentation](http://arxiv.org/abs/2407.03033)|null|目前，多光谱遥感图像(MSRSI) 语义分割任务面临以下问题：1) 通常只考虑单域特征（即空间域或频率域）；2) 编码器中的下采样操作通常会导致边缘提取精度损失；3) 没有充分考虑 MSRSI 的多通道特征；4) 没有充分利用遥感的先验知识。为了解决上述问题，受量子力学的启发，首次提出了一种用于 MSRSI 语义分割的指标-空间-波态叠加Transformer (ISWSST)，其优势如下：1) 通过自适应投票决策（即集成学习思想）叠加或融合指标、空间和波态来模拟量子叠加，从而成为更强大的分类器并提高分割精度；2) 设计了一种无损小波金字塔编码器-解码器模块，基于小波变换和逆小波变换对图像进行无损重建，模拟量子纠缠，避免边缘提取损失；3) 提出结合多光谱特征（即遥感指数和通道注意力机制）从原始分辨率图像中准确提取地面物体；4) 引入量子力学来解释 ISWSST 的潜在优势。实验表明，ISWSST 在 MSRSI 分割任务中得到了验证，并且优于最先进的架构，有效地提高了分割和边缘提取的精度。代码将在我们的论文被接受后公开。|
|**2024-07-03**|[Graph and Skipped Transformer: Exploiting Spatial and Temporal Modeling Capacities for Efficient 3D Human Pose Estimation](http://arxiv.org/abs/2407.02990)|null|近年来，单目三维人体姿态估计 (HPE) 中的 2D 到 3D 姿态提升引起了广泛的研究兴趣。基于 GNN 的方法和基于 Transformer 的方法由于其先进的空间和时间特征学习能力，已成为主流架构。然而，现有方法通常在空间和时间域中构建关节和帧注意力对齐，导致密集连接，从而引入相当大的局部冗余和计算开销。在本文中，我们采用全局方法来利用时空信息，并通过简洁的图和跳跃 Transformer 架构实现高效的 3D HPE。具体来说，在空间编码阶段，部署粗粒度身体部位以构建具有完全数据驱动自适应拓扑的空间图网络，确保模型在各种姿态下的灵活性和泛化能力。在时间编码和解码阶段，提出了一种简单而有效的跳跃 Transformer 来捕获长期时间依赖性并实现分层特征聚合。还开发了一种直接的数据滚动策略，将动态信息引入 2D 姿态序列。在 Human3.6M、MPI-INF-3DHP 和 Human-Eva 基准测试中进行了广泛的实验。G-SFormer 系列方法与以前的最先进技术相比，仅用大约 10% 的参数就实现了卓越的性能，并显着降低了计算复杂度。此外，G-SFormer 还表现出对检测到的 2D 姿态不准确性的出色鲁棒性。|
|**2024-07-03**|[ADFQ-ViT: Activation-Distribution-Friendly Post-Training Quantization for Vision Transformers](http://arxiv.org/abs/2407.02763)|null|视觉Transformer（ViT）在各种计算机视觉任务中都表现出色，但其庞大的参数量导致内存和计算需求显著增加，阻碍了其在资源受限设备上的有效推理。量化已成为缓解这些挑战的一种很有前景的解决方案，但现有方法在低比特情况下仍然存在显著的精度损失。我们将此问题归因于ViT中LayerNorm后和GELU后激活的独特分布，这使得传统的硬件友好型量化器效率低下，尤其是在低比特情况下。为了解决这个问题，我们提出了一种名为Activation-Distribution-Friendly post-training Quantization for Vision Transformers (ADFQ-ViT)的新颖框架。具体来说，我们引入了Per-Patch Outlier-aware Quantizer来处理LayerNorm后激活中的不规则异常值。该量化器在保持阈值以上最小值子集全精度的情况下，将均匀量化器的粒度细化到每个补丁级别。为了处理GELU后激活在正负区域之间的非均匀分布，我们设计了Shift-Log2 Quantizer，它将所有元素移位到正区域，然后应用log2量化。此外，我们提出了Attention-score enhanced Module-wise Optimization，通过重构误差来调整每个量化器的参数，以进一步减轻量化误差。大量实验表明，ADFQ-ViT在4比特图像分类、目标检测和实例分割任务中比各种基线都有显著改进。具体来说，在将ViT-B模型量化到4比特时，我们在ImageNet数据集上的Top-1准确率提高了10.23%。|
|**2024-07-02**|[A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models](http://arxiv.org/abs/2407.02646)|**[link](https://github.com/dakingrai/awesome-mechanistic-interpretability-lm-papers)**|机械可解释性 (MI) 是可解释性的一个新兴子领域，旨在通过逆向工程神经网络模型的内部计算来理解它。近年来，MI 在解释基于 Transformer 的语言模型 (LM) 方面引起了极大的关注，产生了许多新颖的见解，但也带来了新的挑战。然而，目前还没有工作全面回顾这些见解和挑战，特别是作为该领域新人的指南。为了填补这一空白，我们提供了一份全面的综述，概述了 MI 中的基本研究对象、用于其研究的技术、评估 MI 结果的方法，以及使用 MI 理解 LM 所产生的重要发现和应用。特别是，我们为初学者提供了一个路线图，以帮助他们在该领域导航并利用 MI 为自己谋福利。最后，我们还指出了该领域目前的差距，并讨论了未来可能的发展方向。|
|**2024-07-02**|[On the Anatomy of Attention](http://arxiv.org/abs/2407.02423)|null|我们引入一种范畴论图表形式体系，以便系统地关联和推理机器学习模型。我们的图表以直观的方式呈现架构，但不会丢失必要的细节，其中模型之间的自然关系通过图形变换来捕捉，并且重要的差异和相似性可以一目了然地识别出来。在本文中，我们将重点关注注意力机制：将民间传说转化为数学推导，并构建文献中注意力变体的分类法。作为以我们的形式主义为基础的实证研究的第一个例子，我们确定了注意力机制中反复出现的解剖学成分，我们对其进行了详尽的重组，以探索注意力机制的变化空间。|
|**2024-07-02**|[Efficient Sparse Attention needs Adaptive Token Release](http://arxiv.org/abs/2407.02328)|null|近年来，大型语言模型 (LLM) 在各种以文本为中心的的任务中展现出卓越的能力。然而，其“大”规模带来了巨大的计算和存储挑战，尤其是在管理 Transformer 的键值状态方面，这限制了其更广泛的适用性。因此，我们建议自适应地从缓存中释放资源并重建必要的键值状态。特别是，我们通过一个轻量级的控制器模块来近似理想的top- $K$稀疏注意力来实现这一点。该模块保留具有最高 top-$K$ 注意力权重的标记，并同时重建已丢弃但必要的标记，这些标记可能对未来的解码至关重要。自然语言生成和建模方面的综合实验表明，我们的方法不仅在性能方面与完全注意力机制相比具有竞争力，而且还实现了高达 221.8% 的显著吞吐量提升。复制代码可在 https://github.com/WHUIR/ADORE 上获得。|

