---
layout: default
---

## Updated on 2024.06.17
> Usage instructions: [here](./docs/README.md#usage)

## 多模态

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models**|[2406.10228](http://arxiv.org/abs/2406.10228)|null|**多模态大型模型（MLLM）的快速发展展示了其在处理视觉和语言混合任务方面的强大能力。然而，目前大多数模型和基准测试都集中在视觉和文本上下文范围较窄的场景。当面对复杂的理解任务时，这些模型往往表现不佳，因为这些任务需要在文本和图像形式的大量无关信息和潜在误导性信息中进行导航。为了弥合这一差距，我们引入了一项新的、更具挑战性的任务，称为交错图文理解（IITC）。这项任务要求模型辨别和忽略图像和文本中的多余元素，以准确回答问题，并遵循复杂的指令精确定位相关图像。为了支持这项任务，我们进一步构建了一个新的VEGA数据集，该数据集针对科学内容的IITC任务而定制，并设计了一个子任务，即图文关联（ITA），以完善图文关联能力。我们对四个领先的闭源模型以及使用VEGA的各种开源模型进行了评估，结果突出了IITC任务的严格性。即使是最先进的模型，如Gemini-1.5-pro和GPT4V，也只取得了有限的成功。通过采用多任务、多尺度的后训练策略，我们在IITC任务上为MLLM设定了一个稳健的基线，在图像关联方面达到了85.8%的准确率，Rouge评分达到了0.508。这些结果验证了我们的数据集在提高MLLM对细微图文理解能力方面的有效性。 
**|
|**2024-06-14**|**VideoGUI: A Benchmark for GUI Automation from Instructional Videos**|[2406.10227](http://arxiv.org/abs/2406.10227)|null|**图形用户界面（GUI）自动化在通过辅助计算机任务来提高人类生产力方面具有巨大潜力。现有的任务制定主要集中在可以通过单一语言指令指定的简单任务，例如“插入新幻灯片”。在这项工作中，我们介绍了 VideoGUI，这是一个新颖的多模态基准测试，旨在评估以视觉为中心的 GUI 任务上的 GUI 助手。我们的基准测试源自高质量的网络教学视频，侧重于涉及专业和新颖软件（例如，Adobe Photoshop 或 Stable Diffusion WebUI）以及复杂活动（例如，视频编辑）的任务。VideoGUI 通过分层过程评估 GUI 助手，从而可以识别它们可能失败的特定级别：（i）高级规划：在没有语言描述的情况下，根据视觉条件重建程序性子任务；（ii）中级规划：根据视觉状态（即屏幕截图）和目标生成精确动作叙述的序列；（iii）原子动作执行：执行特定动作，例如准确单击指定元素。对于每个级别，我们设计了跨各个维度的评估指标以提供清晰的信号，例如原子动作执行中单击、拖动、键入和滚动的个体性能。我们对 VideoGUI 的评估表明，即使是最先进的大型多模态模型 GPT4o 在以视觉为中心的 GUI 任务上也表现不佳，尤其是在高级规划方面。 
**|
|**2024-06-14**|**DevBench: A multimodal developmental benchmark for language learning**|[2406.10215](http://arxiv.org/abs/2406.10215)|null|**视觉语言模型和儿童的学习轨迹有多大差异？最近的建模工作试图通过构建使用较少数据（尤其是多模态自然数据）训练的模型来理解模型和人类数据效率之间的差距。然而，此类模型通常在成人水平的基准测试中进行评估，测试的语言能力广度有限，并且没有与行为数据进行直接比较。我们介绍DevBench，这是一个多模态基准测试，包含七项语言评估任务，涵盖词汇、句法和语义能力领域，以及儿童和成人的行为数据。我们针对这些任务评估了一组视觉语言模型，不仅在准确性方面，而且在响应模式方面比较了模型和人类。在各项任务中，模型在接近人类反应模式方面表现出差异，并且在某项任务上表现更好的模型也更接近人类的行为反应。我们还检查了 OpenCLIP 在训练过程中的发展轨迹，发现更多的训练会导致更接近成人反应模式。因此，DevBench 提供了一个用于将模型与人类语言发展进行比较的基准。这些比较突出了模型和人类语言学习过程的不同之处，为了解改进语言模型的切入点提供了见解。 
**|
|**2024-06-14**|**Detecting and Evaluating Medical Hallucinations in Large Vision Language Models**|[2406.10185](http://arxiv.org/abs/2406.10185)|null|**大型视觉语言模型 (LVLM) 在医疗保健应用中越来越不可或缺，包括医学视觉问答和影像报告生成。虽然这些模型继承了基础大型语言模型 (LLM) 的强大功能，但它们也继承了易产生幻觉的倾向——这在容错率极低的医疗环境中是一个重大问题。然而，目前还没有专门针对医学领域幻觉检测和评估的方法或基准。为了弥合这一差距，我们引入了 Med-HallMark，这是第一个专门为医学多模态领域内的幻觉检测和评估而设计的基准。该基准提供多任务幻觉支持、多方面幻觉数据和分层幻觉分类。此外，我们提出了 MediHall Score，这是一种新的医学评估指标，旨在通过考虑幻觉的严重程度和类型的分层评分系统来评估 LVLM 的幻觉，从而能够对潜在的临床影响进行细致的评估。我们还介绍了 MediHallDetector，这是一种为精确检测幻觉而设计的新型医学 LVLM，它采用多任务训练进行幻觉检测。通过广泛的实验评估，我们使用我们的基准为流行的 LVLM 建立了基线。研究结果表明，与传统指标相比，MediHall Score 可以更细致地理解幻觉的影响，并证明了 MediHallDetector 的增强性能。我们希望这项工作能够显着提高 LVLM 在医疗应用中的可靠性。这项工作的所有资源将很快发布。 
**|
|**2024-06-14**|**Multimodal Radiomics Model for Predicting Gold Nanoparticles Accumulation in Mouse Tumors**|[2406.10146](http://arxiv.org/abs/2406.10146)|null|**背景：纳米颗粒可在实体瘤中积累，作为癌症诊断或治疗剂。由于肿瘤积累率低以及肿瘤类型和个体间存在异质性，临床转化具有挑战性。需要用于识别这种异质性和预测纳米颗粒积累的工具。先进的成像技术与放射组学和人工智能相结合可能提供一种解决方案。方法：使用183只小鼠创建了7种皮下肿瘤模型，并通过尾静脉注射了三种尺寸（15nm、40nm、70nm）的金纳米颗粒。使用ICP-OES测量积累量。将数据分为训练集和测试集（7:3）。根据训练集中位数将肿瘤分为高摄取组和低摄取组。注射前，获取多模态成像数据（CT、B型超声、SWE、CEUS），并提取放射组学特征。LASSO和RFE算法构建了放射组学特征。这些特征与肿瘤类型以及CT和SWE的平均值一起，使用SVM构建了最佳模型。对于测试集中的每个肿瘤，放射组学特征预测了金纳米颗粒的摄取。通过AUC评估模型性能。结果：在肿瘤之间观察到金纳米颗粒积累的显着差异（P < 0.001）。训练集中的中位积累量为3.37% ID/g。纳米颗粒尺寸不是摄取的主要决定因素（P > 0.05）。基于放射组学特征的复合模型在训练集（AUC 0.93 vs. 0.68）和测试集（0.78 vs. 0.61）中均优于基本模型。结论：复合模型识别肿瘤异质性并预测金纳米颗粒的高摄取，改善患者分层并支持纳米医学的临床应用。
**|
|**2024-06-14**|**SmartRSD: An Intelligent Multimodal Approach to Real-Time Road Surface Detection for Safe Driving**|[2406.10128](http://arxiv.org/abs/2406.10128)|null|**准确快速地识别路面状况可以让车辆及时调整自身行为，例如改变速度或使用特定的牵引力控制技术，从而降低事故发生概率，减少对驾驶员和行人的潜在危险。然而，现有的路面检测方法大多仅依赖于视觉数据，这在某些情况下可能存在不足，例如道路被碎屑覆盖、光线不足或有雾的情况下。因此，我们引入了一种多模态方法，通过整合音频和图像来自动检测路面状况。我们在收集于各种环境条件和路面类型下的多样化数据集上测试了该方法的鲁棒性。通过广泛的评估，我们证明了多模态方法在实时场景下准确识别路面状况的有效性和可靠性。我们的研究结果突出了整合听觉和视觉线索对于增强道路安全和最小化事故风险的潜力。 
**|
|**2024-06-14**|**Shelf-Supervised Multi-Modal Pre-Training for 3D Object Detection**|[2406.10115](http://arxiv.org/abs/2406.10115)|null|**目前最先进的 3D 物体检测器通常在庞大的标记数据集上进行训练。然而，标注 3D 边界框仍然非常昂贵且耗时，特别是对于激光雷达而言。因此，最近的研究表明，使用未标记数据进行自监督预训练可以提高有限标签情况下的检测精度。当代方法将图像领域自监督学习的最佳实践（例如对比学习）应用于点云。然而，公开可用的 3D 数据集比用于基于图像的自监督学习的数据集要小得多，多样性也差得多，这限制了它们的有效性。然而，我们注意到此类数据是以多模态方式自然收集的，通常与图像配对。我们认为，与其仅使用自监督目标进行预训练，不如使用在互联网规模图像数据上训练的基于图像的基础模型来引导点云表示。具体来说，我们提出了一种“现成监督”方法（例如，使用现成的图像基础模型进行监督），用于从配对的 RGB 和激光雷达数据生成零样本 3D 边界框。使用此类伪标签预训练 3D 检测器比先前的自监督预训练任务产生明显更好的半监督检测精度。重要的是，我们证明了基于图像的现成监督有助于训练仅激光雷达和多模态（RGB + 激光雷达）检测器。我们在 nuScenes 和 WOD 上证明了我们方法的有效性，在有限数据设置下比先前的工作有了显著改进。 
**|
|**2024-06-14**|**Localizing Events in Videos with Multimodal Queries**|[2406.10079](http://arxiv.org/abs/2406.10079)|null|**Video understanding is a pivotal task in the digital era, yet the dynamic and multievent nature of videos makes them labor-intensive and computationally demanding to process. Thus, localizing a specific event given a semantic query has gained importance in both user-oriented applications like video search and academic research into video foundation models. A significant limitation in current research is that semantic queries are typically in natural language that depicts the semantics of the target event. This setting overlooks the potential for multimodal semantic queries composed of images and texts. To address this gap, we introduce a new benchmark, ICQ, for localizing events in videos with multimodal queries, along with a new evaluation dataset ICQ-Highlight. Our new benchmark aims to evaluate how well models can localize an event given a multimodal semantic query that consists of a reference image, which depicts the event, and a refinement text to adjust the images' semantics. To systematically benchmark model performance, we include 4 styles of reference images and 5 types of refinement texts, allowing us to explore model performance across different domains. We propose 3 adaptation methods that tailor existing models to our new setting and evaluate 10 SOTA models, ranging from specialized to large-scale foundation models. We believe this benchmark is an initial step toward investigating multimodal queries in video event localization.**|
|**2024-06-14**|**First Multi-Dimensional Evaluation of Flowchart Comprehension for Multimodal Large Language Models**|[2406.10057](http://arxiv.org/abs/2406.10057)|null|**With the development of multimodal large language models (MLLMs) technology, its general capabilities are increasingly powerful. To evaluate the various abilities of MLLMs, numerous evaluation systems have emerged. But now there is still a lack of a comprehensive method to evaluate MLLMs in the tasks related to flowcharts, which are very important in daily life and work. We propose the first comprehensive method, FlowCE, to assess MLLMs across various dimensions for tasks related to flowcharts. It encompasses evaluating MLLMs' abilities in Reasoning, Localization Recognition, Information Extraction, Logical Verification, and Summarization on flowcharts. However, we find that even the GPT4o model achieves only a score of 56.63. Among open-source models, Phi-3-Vision obtained the highest score of 49.97. We hope that FlowCE can contribute to future research on multimodal large language models (MLLMs) for tasks based on flowcharts. We are open-sourcing this project: \url{https://github.com/360AILAB-NLP/FlowCE}**|
|**2024-06-14**|**Precision Empowers, Excess Distracts: Visual Question Answering With Dynamically Infused Knowledge In Language Models**|[2406.09994](http://arxiv.org/abs/2406.09994)|null|**在多模态任务领域，视觉问答 (VQA) 通过解决基于视觉内容的自然语言问题发挥着至关重要的作用。基于知识的视觉问答 (KBVQA) 通过添加外部知识和图像来回答问题，从而推进了这一概念。我们介绍了一种 KBVQA 的方法，增强了现有的视觉语言转换器编码器-解码器 (OFA) 模型。我们的主要贡献在于使用动态三元组提取方法，通过结合从知识图谱中提取的相关外部知识来增强问题。我们根据回答问题的要求，提供来自知识图谱的灵活数量的三元组作为上下文。我们富含知识的模型在三个不同的 KBVQA 数据集上，相较于现有最佳模型，在精确匹配得分方面平均提高了 4.75%。通过实验和分析，我们证明，与提供固定数量的三元组相比，为每个问题提供可变的三元组可以提高语言模型的推理能力。即使是最近的大型语言模型也证明了这一点。此外，我们通过展示模型在小型数据集上的 SOTA 性能来突出其泛化能力，这是通过简单的微调实现的。 
**|

## 6DOF Object Pose

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-06**|**Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking**|[2406.04316](http://arxiv.org/abs/2406.04316)|null|**6D物体姿态估计是计算机视觉中一项至关重要但极具挑战性的任务，其主要困难在于缺乏大规模数据集。这种稀缺性阻碍了对模型性能的全面评估，限制了研究进展。此外，可用实例或类别的数量有限也限制了其应用。为了解决这些问题，本文提出了Omni6DPose，这是一个以对象类别多样性、规模大和对象材质多样性为特征的大型数据集。Omni6DPose主要由三个部分组成：ROPE（真实6D物体姿态估计数据集），包含332K张图像，涵盖149个类别，581个实例，超过150万个标注；SOPE（模拟6D物体姿态估计数据集），包含475K张在混合现实环境中创建的图像，采用深度模拟技术，涵盖与ROPE相同的149个类别，4162个实例，超过500万个标注；以及在ROPE和SOPE中均使用的手动对齐的真实扫描物体。由于存在大量的变化和歧义，Omni6DPose本身就极具挑战性。为了应对这一挑战，我们引入了GenPose++，它是SOTA类别级姿态估计框架的增强版本，融合了两项关键改进：语义感知特征提取和基于聚类的聚合。此外，我们还提供了全面的基准分析，以评估先前方法在这个大规模数据集上在6D物体姿态估计和姿态跟踪方面的性能。 
**|
|**2024-06-05**|**Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices**|[2406.02977](http://arxiv.org/abs/2406.02977)|null|**随着机器人技术和增强现实应用越来越依赖于精确高效的6D物体姿态估计，边缘设备上的实时性能对于实现更具交互性和响应能力的系统至关重要。我们提出的稀疏颜色代码网络（SCCN）体现了一种清晰简洁的流程设计，以有效满足这一需求。SCCN在RGB图像中对目标物体进行像素级预测，利用基本物体几何特征的稀疏性来加速透视n点（PnP）计算过程。此外，它引入了一种新颖的基于像素级几何的物体对称性表示，该表示与初始姿态预测无缝集成，有效地解决了对称物体歧义问题。值得注意的是，SCCN在NVIDIA Jetson AGX Xavier上分别在基准LINEMOD数据集和遮挡LINEMOD数据集上实现了每秒19帧（FPS）和6 FPS的估计速率，同时在这些速率下始终保持较高的估计精度。 
**|
|**2024-05-19**|**Advancing 6-DoF Instrument Pose Estimation in Variable X-Ray Imaging Geometries**|[2405.11677](http://arxiv.org/abs/2405.11677)|**[link](https://github.com/cviviers/YOLOv5-6D-Pose)**|**在微创手术中，对手术器械进行精确的 6 自由度 (6-DoF) 位姿估计可以显著改进治疗策略并最终提高手术效果。现有的深度学习方法已经取得了准确的结果，但它们需要针对每个对象定制方法，并且需要费力的设置和训练环境（通常扩展到广泛的模拟），同时缺乏实时计算能力。我们提出了一种用于 X 射线系统中 6-DoF 位姿估计任务的通用数据采集方法，一种新颖且通用的 YOLOv5-6D 位姿架构，用于准确、快速地进行对象位姿估计，以及一种在考虑单目锥束 X 射线图像采集几何形状的情况下进行手术螺钉位姿估计的完整方法。所提出的 YOLOv5-6D 位姿模型在公共基准测试中取得了具有竞争力的结果，同时在 GPU 上的速度相当快，达到了 42 FPS。此外，该方法可以泛化到不同的 X 射线采集几何形状和语义图像复杂度，从而能够在不同的领域进行准确的位姿估计。最后，所提出的方法在脊柱手术期间对骨螺钉位姿估计进行了测试，用于计算机辅助引导。该模型的 0.1 ADD-S 指标达到了 92.41%，证明了其在提高手术精度和患者预后方面具有良好的应用前景。YOLOv5-6D 的代码已在 https://github.com/cviviers/YOLOv5-6D-Pose 上公开发布。 
**|
|**2024-05-18**|**PS6D: Point Cloud Based Symmetry-Aware 6D Object Pose Estimation in Robot Bin-Picking**|[2405.11257](http://arxiv.org/abs/2405.11257)|null|**6D object pose estimation holds essential roles in various fields, particularly in the grasping of industrial workpieces. Given challenges like rust, high reflectivity, and absent textures, this paper introduces a point cloud based pose estimation framework (PS6D). PS6D centers on slender and multi-symmetric objects. It extracts multi-scale features through an attention-guided feature extraction module, designs a symmetry-aware rotation loss and a center distance sensitive translation loss to regress the pose of each point to the centroid of the instance, and then uses a two-stage clustering method to complete instance segmentation and pose estimation. Objects from the Sil\'eane and IPA datasets and typical workpieces from industrial practice are used to generate data and evaluate the algorithm. In comparison to the state-of-the-art approach, PS6D demonstrates an 11.5\% improvement in F $_{1_{inst}}$ and a 14.8\% improvement in Recall. The main part of PS6D has been deployed to the software of Mech-Mind, and achieves a 91.7\% success rate in bin-picking experiments, marking its application in industrial pose estimation tasks.**|
|**2024-05-31**|**Deep Learning-Based Object Pose Estimation: A Comprehensive Survey**|[2405.07801](http://arxiv.org/abs/2405.07801)|**[link](https://github.com/cnjianliu/awesome-object-pose-estimation)**|**Object pose estimation is a fundamental computer vision problem with broad applications in augmented reality and robotics. Over the past decade, deep learning models, due to their superior accuracy and robustness, have increasingly supplanted conventional algorithms reliant on engineered point pair features. Nevertheless, several challenges persist in contemporary methods, including their dependency on labeled training data, model compactness, robustness under challenging conditions, and their ability to generalize to novel unseen objects. A recent survey discussing the progress made on different aspects of this area, outstanding challenges, and promising future directions, is missing. To fill this gap, we discuss the recent advances in deep learning-based object pose estimation, covering all three formulations of the problem, \emph{i.e.}, instance-level, category-level, and unseen object pose estimation. Our survey also covers multiple input data modalities, degrees-of-freedom of output poses, object properties, and downstream tasks, providing the readers with a holistic understanding of this field. Additionally, it discusses training paradigms of different domains, inference modes, application areas, evaluation metrics, and benchmark datasets, as well as reports the performance of current state-of-the-art methods on these benchmarks, thereby facilitating the readers in selecting the most suitable method for their application. Finally, the survey identifies key challenges, reviews the prevailing trends along with their pros and cons, and identifies promising directions for future research. We also keep tracing the latest works at https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation.**|
|**2024-05-02**|**IntervenGen: Interventional Data Generation for Robust and Data-Efficient Robot Imitation Learning**|[2405.01472](http://arxiv.org/abs/2405.01472)|null|**模仿学习是训练机器人控制策略的一种很有前景的范式，但这些策略可能会受到分布偏移的影响，即评估时的条件与训练数据中的条件不同。提高策略对分布偏移鲁棒性的一种流行方法是交互式模仿学习（即DAgger及其变体），其中人类操作员在策略执行期间提供纠正性干预。然而，收集足够数量的干预措施以覆盖策略错误的分布对于人类操作员来说可能是一项繁重的任务。我们提出了IntervenGen（I-Gen），这是一种新颖的数据生成系统，可以从少量的人工干预中自主生成大量覆盖状态空间的纠正性干预。我们将 I-Gen 应用于 4 个模拟环境和 1 个具有物体姿态估计误差的物理环境，结果表明，它只需 10 次人工干预即可将策略鲁棒性提高多达 39 倍。视频和更多结果可在 https://sites.google.com/view/intervengen2024 获取。 
**|
|**2024-04-17**|**GeoReF: Geometric Alignment Across Shape Variation for Category-level Object Pose Refinement**|[2404.11139](http://arxiv.org/abs/2404.11139)|null|**物体姿态细化对于稳健的物体姿态估计至关重要。先前的工作在实例级物体姿态细化方面取得了重大进展。然而，由于类别内较大的形状变化以及目标物体与形状先验之间的差异，类别级姿态细化是一个更具挑战性的问题。为了应对这些挑战，我们引入了一种用于类别级物体姿态细化的全新架构。我们的方法集成了 HS 层和可学习的仿射变换，旨在增强几何信息的提取和对齐。此外，我们引入了一种跨点云变换机制，可以有效地融合不同的数据源。最后，我们通过结合形状先验信息进行平移和尺寸误差预测，来突破模型的极限。我们进行了大量的实验来证明所提出框架的有效性。通过广泛的定量实验，我们证明了在所有指标上，我们的方法都比基线方法有显著的改进。 
**|
|**2024-04-08**|**Learning a Category-level Object Pose Estimator without Pose Annotations**|[2404.05626](http://arxiv.org/abs/2404.05626)|null|**三维物体姿态估计是一项具有挑战性的任务。以往的工作总是需要数千张带有标注姿态的物体图像来学习三维姿态对应关系，这对于标注来说既费力又耗时。在本文中，我们提出了一种无需姿态标注即可学习类别级三维物体姿态估计器的方法。我们没有使用手动标注的图像，而是利用扩散模型（例如 Zero-1-to-3）生成一组在受控姿态差异下的图像，并建议使用这些图像来学习我们的物体姿态估计器。直接使用原始扩散模型会导致图像出现姿态噪声和伪影。为了解决这个问题，首先，我们利用从专门设计的对比姿态学习中学习到的图像编码器来过滤不合理的细节并提取图像特征图。此外，我们提出了一种新的学习策略，允许模型从那些生成的图像集中学习物体姿态，而无需知道其规范姿态的对齐方式。实验结果表明，我们的方法能够从单次拍摄设置（作为姿态定义）中进行类别级物体姿态估计，同时在少样本类别级物体姿态估计基准测试中显著优于其他最先进的方法。**|
|**2024-03-28**|**Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation**|[2403.19527](http://arxiv.org/abs/2403.19527)|**[link](https://github.com/leeiieeo/ag-pose)**|**Category-level 6D object pose estimation aims to estimate the rotation, translation and size of unseen instances within specific categories. In this area, dense correspondence-based methods have achieved leading performance. However, they do not explicitly consider the local and global geometric information of different instances, resulting in poor generalization ability to unseen instances with significant shape variations. To deal with this problem, we propose a novel Instance-Adaptive and Geometric-Aware Keypoint Learning method for category-level 6D object pose estimation (AG-Pose), which includes two key designs: (1) The first design is an Instance-Adaptive Keypoint Detection module, which can adaptively detect a set of sparse keypoints for various instances to represent their geometric structures. (2) The second design is a Geometric-Aware Feature Aggregation module, which can efficiently integrate the local and global geometric information into keypoint features. These two modules can work together to establish robust keypoint-level correspondences for unseen instances, thus enhancing the generalization ability of the model.Experimental results on CAMERA25 and REAL275 datasets show that the proposed AG-Pose outperforms state-of-the-art methods by a large margin without category-specific shape priors.**|
|**2024-06-01**|**Object Pose Estimation via the Aggregation of Diffusion Features**|[2403.18791](http://arxiv.org/abs/2403.18791)|**[link](https://github.com/tianfu18/diff-feats-pose)**|**Estimating the pose of objects from images is a crucial task of 3D scene understanding, and recent approaches have shown promising results on very large benchmarks. However, these methods experience a significant performance drop when dealing with unseen objects. We believe that it results from the limited generalizability of image features. To address this problem, we have an in-depth analysis on the features of diffusion models, e.g. Stable Diffusion, which hold substantial potential for modeling unseen objects. Based on this analysis, we then innovatively introduce these diffusion features for object pose estimation. To achieve this, we propose three distinct architectures that can effectively capture and aggregate diffusion features of different granularity, greatly improving the generalizability of object pose estimation. Our approach outperforms the state-of-the-art methods by a considerable margin on three popular benchmark datasets, LM, O-LM, and T-LESS. In particular, our method achieves higher accuracy than the previous best arts on unseen objects: 98.2% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the strong generalizability of our method. Our code is released at https://github.com/Tianfu18/diff-feats-pose.**|

## nerf

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors**|[2406.10111](http://arxiv.org/abs/2406.10111)|null|**从低分辨率输入视图实现高分辨率新视角合成 (HRNVS) 是一项具有挑战性的任务，因为缺乏高分辨率数据。先前的方法从低分辨率输入视图优化高分辨率神经辐射场 (NeRF)，但渲染速度缓慢。在这项工作中，我们基于 3D 高斯样条 (3DGS) 开发方法，因为它能够以更快的渲染速度生成高质量图像。为了缓解更高分辨率合成的数据短缺问题，我们建议利用现成的 2D 扩散先验，通过分数蒸馏采样 (SDS) 将 2D 知识提取到 3D。然而，由于生成先验带来的随机性，将 SDS 直接应用于基于高斯的 3D 超分辨率会导致不希望的和冗余的 3D 高斯基元。为了缓解这个问题，我们引入了两种简单而有效的技术来减少 SDS 引入的随机扰动。具体来说，我们 1) 使用退火策略缩小 SDS 中扩散时间步长的范围；2) 在密集化过程中随机丢弃冗余的高斯基元。大量实验表明，我们提出的 GaussainSR 可以在合成和现实世界数据集上仅使用低分辨率输入即可获得 HRNVS 的高质量结果。项目页面：https://chchnii.github.io/GaussianSR/ 
**|
|**2024-06-14**|**RaNeuS: Ray-adaptive Neural Surface Reconstruction**|[2406.09801](http://arxiv.org/abs/2406.09801)|**[link](https://github.com/wangyida/ra-neus)**|**我们的目标是利用可微分辐射场（例如 NeRF）来重建详细的 3D 表面，以及生成标准的新颖视图渲染。目前已有一些相关方法可以执行此类任务，通常是利用带符号距离场 (SDF)。然而，最先进的方法仍然无法正确重建小规模细节，例如树叶、绳索和纺织品表面。考虑到不同的方法通过全局常数 Eikonal 正则化来制定和优化从 SDF 到辐射场的投影，我们通过射线加权因子进行了改进，以优先考虑渲染和零交叉表面拟合，并在建立完美 SDF 的基础上进行。我们建议自适应地调整带符号距离场上的正则化，以便不令人满意的渲染射线不会强制执行无效的强 Eikonal 正则化，并允许来自具有良好学习辐射的区域的梯度有效地反向传播到 SDF。因此，平衡这两个目标以生成准确和详细的表面。此外，关于 SDF 中的零交叉表面和辐射场中的渲染点之间是否存在几何偏差，投影也变得可调整，这取决于优化期间不同的 3D 位置。我们提出的 RaNeuS 在合成数据集和真实数据集上都进行了广泛评估，在新颖视图合成和几何重建方面均取得了最先进的结果。**|
|**2024-06-13**|**Neural NeRF Compression**|[2406.08943](http://arxiv.org/abs/2406.08943)|null|**神经辐射场 (NeRFs) 已成为通过连续体积表示捕获详细 3D 场景的强大工具。最近的 NeRFs 利用特征网格来提高渲染质量和速度；然而，这些表示引入了大量的存储开销。本文提出了一种有效压缩基于网格的 NeRF 模型的新方法，解决了存储开销问题。我们的方法基于非线性变换编码范式，采用神经压缩来压缩模型的特征网格。由于缺乏涉及许多独立同分布场景的训练数据，我们为单个场景设计了一种无编码器、端到端优化的方案，并使用轻量级解码器。为了利用潜在特征网格的空间不均匀性，我们引入了重要性加权的率失真目标函数和采用掩蔽机制的稀疏熵模型。我们的实验结果表明，我们提出的方法在基于网格的 NeRF 压缩效率和重建质量方面优于现有工作。 
**|
|**2024-06-13**|**OpenMaterial: A Comprehensive Dataset of Complex Materials for 3D Reconstruction**|[2406.08894](http://arxiv.org/abs/2406.08894)|null|**近年来，诸如神经辐射场和隐式神经表示等深度学习技术的进步极大地推动了三维重建领域的发展。然而，由于金属和玻璃等具有复杂光学特性的物体具有独特的镜面反射和透光特性，因此对其进行精确重建仍然是一项艰巨的挑战。为了促进解决这些挑战的方案的开发，我们引入了 OpenMaterial 数据集，该数据集包含 1001 个由 295 种不同材料（包括导体、电介质、塑料及其粗糙变体）制成的物体，并在 723 种不同的照明条件下捕获。为此，我们利用基于物理的渲染技术，结合实验室测量的折射率 (IOR)，生成了高度逼真的多视图图像，逼真地复制了现实世界中的物体。OpenMaterial 提供了全面的注释，包括三维形状、材料类型、相机姿态、深度和物体掩码。它是第一个能够对具有多种多样且具有挑战性的材料的物体进行定量评估现有算法的大规模数据集，从而为开发能够处理复杂材料特性的三维重建算法铺平了道路。**|
|**2024-06-13**|**Gaussian-Forest: Hierarchical-Hybrid 3D Gaussian Splatting for Compressed Scene Modeling**|[2406.08759](http://arxiv.org/abs/2406.08759)|null|**新视角合成领域最近出现了三维高斯 splatting 技术，它以基于点的方式表示场景并通过光栅化进行渲染。与依赖于光线追踪的辐射场相比，该方法展现出更优的渲染质量和速度。然而，三维高斯的显式和非结构化特性带来了巨大的存储挑战，阻碍了其更广泛的应用。为了应对这一挑战，我们引入了高斯森林建模框架，该框架将场景分层表示为混合三维高斯森林。每个混合高斯保留其独特的显式属性，同时与其兄弟高斯共享隐式属性，从而以更少的变量优化参数化。此外，我们还设计了自适应增长和修剪策略，确保在复杂区域的详细表示，并显著减少所需高斯的数量。大量实验表明，高斯森林不仅保持了相当的速度和质量，而且实现了超过10倍的压缩率，标志着高效场景建模的重大进步。代码可在 https://github.com/Xian-Bei/GaussianForest 获取。 
**|
|**2024-06-12**|**OpenObj: Open-Vocabulary Object-Level Neural Radiance Fields with Fine-Grained Understanding**|[2406.08009](http://arxiv.org/abs/2406.08009)|**[link](https://github.com/BIT-DYN/OpenObj)**|**In recent years, there has been a surge of interest in open-vocabulary 3D scene reconstruction facilitated by visual language models (VLMs), which showcase remarkable capabilities in open-set retrieval. However, existing methods face some limitations: they either focus on learning point-wise features, resulting in blurry semantic understanding, or solely tackle object-level reconstruction, thereby overlooking the intricate details of the object's interior. To address these challenges, we introduce OpenObj, an innovative approach to build open-vocabulary object-level Neural Radiance Fields (NeRF) with fine-grained understanding. In essence, OpenObj establishes a robust framework for efficient and watertight scene modeling and comprehension at the object-level. Moreover, we incorporate part-level features into the neural fields, enabling a nuanced representation of object interiors. This approach captures object-level instances while maintaining a fine-grained understanding. The results on multiple datasets demonstrate that OpenObj achieves superior performance in zero-shot semantic segmentation and retrieval tasks. Additionally, OpenObj supports real-world robotics tasks at multiple scales, including global movement and local manipulation.**|
|**2024-06-12**|**Spatial Annealing Smoothing for Efficient Few-shot Neural Rendering**|[2406.07828](http://arxiv.org/abs/2406.07828)|**[link](https://github.com/pulangk97/SANeRF)**|**Neural Radiance Fields (NeRF) with hybrid representations have shown impressive capabilities in reconstructing scenes for view synthesis, delivering high efficiency. Nonetheless, their performance significantly drops with sparse view inputs, due to the issue of overfitting. While various regularization strategies have been devised to address these challenges, they often depend on inefficient assumptions or are not compatible with hybrid models. There is a clear need for a method that maintains efficiency and improves resilience to sparse views within a hybrid framework. In this paper, we introduce an accurate and efficient few-shot neural rendering method named Spatial Annealing smoothing regularized NeRF (SANeRF), which is specifically designed for a pre-filtering-driven hybrid representation architecture. We implement an exponential reduction of the sample space size from an initially large value. This methodology is crucial for stabilizing the early stages of the training phase and significantly contributes to the enhancement of the subsequent process of detail refinement. Our extensive experiments reveal that, by adding merely one line of code, SANeRF delivers superior rendering quality and much faster reconstruction speed compared to current few-shot NeRF methods. Notably, SANeRF outperforms FreeNeRF by 0.3 dB in PSNR on the Blender dataset, while achieving 700x faster reconstruction speed.**|
|**2024-06-11**|**Neural Gaffer: Relighting Any Object via Diffusion**|[2406.07520](http://arxiv.org/abs/2406.07520)|null|**Single-image relighting is a challenging task that involves reasoning about the complex interplay between geometry, materials, and lighting. Many prior methods either support only specific categories of images, such as portraits, or require special capture conditions, like using a flashlight. Alternatively, some methods explicitly decompose a scene into intrinsic components, such as normals and BRDFs, which can be inaccurate or under-expressive. In this work, we propose a novel end-to-end 2D relighting diffusion model, called Neural Gaffer, that takes a single image of any object and can synthesize an accurate, high-quality relit image under any novel environmental lighting condition, simply by conditioning an image generator on a target environment map, without an explicit scene decomposition. Our method builds on a pre-trained diffusion model, and fine-tunes it on a synthetic relighting dataset, revealing and harnessing the inherent understanding of lighting present in the diffusion model. We evaluate our model on both synthetic and in-the-wild Internet imagery and demonstrate its advantages in terms of generalization and accuracy. Moreover, by combining with other generative methods, our model enables many downstream 2D tasks, such as text-based relighting and object insertion. Our model can also operate as a strong relighting prior for 3D tasks, such as relighting a radiance field.**|
|**2024-06-11**|**Active Scout: Multi-Target Tracking Using Neural Radiance Fields in Dense Urban Environments**|[2406.07431](http://arxiv.org/abs/2406.07431)|null|**我们研究了高度遮挡的城市环境（例如城市中的高层建筑）中的追踪-逃避游戏，其中侦察器（四旋翼机）跟踪地面上的多个动态目标。我们证明了可以使用来自不同有利位置的 RGB 和深度图像——在线——构建城市的基于神经辐射场 (NeRF) 的表示。这种表示用于计算信息增益，以探索城市未知区域并跟踪目标——从而为主动跟踪动态目标提供了一种完全基于第一性原理的方法。我们使用基于费城和纽约市开放街道地图数据的定制模拟器进行演示，可以在 300 步内探索和定位 20 个静止目标。这比不使用主动感知的贪婪基线要慢。但对于主动躲避遮挡物的动态目标，我们证明我们的方法最多可以将跟踪误差保持在 200 米以内；贪婪基线的跟踪误差可能高达 600 米。我们观察到侦察策略中的一些有趣特性，例如，它会定期切换注意力以跟踪不同的目标，随着 NeRF 表示质量随时间的提高，侦察在目标跟踪方面也会变得更好。 
**|
|**2024-06-11**|**Cinematic Gaussians: Real-Time HDR Radiance Fields with Depth of Field**|[2406.07329](http://arxiv.org/abs/2406.07329)|null|**辐射场方法代表了从多视图照片重建复杂场景的最新技术。然而，这些重建通常会受到以下一项或两项限制：首先，它们通常以低动态范围 (LDR) 表示场景，这限制了它们在光照均匀的环境中的使用，并阻碍了沉浸式观看体验。其次，它们依赖于针孔相机模型，假设所有场景元素在输入图像中都是对焦的，这带来了实际挑战，并使新视图合成过程中的重新对焦变得复杂。为了解决这些限制，我们提出了一种基于 3D 高斯 splatting 的轻量级方法，该方法利用具有不同曝光时间、光圈和焦距的场景的多视图 LDR 图像作为输入，以重建高动态范围 (HDR) 辐射场。通过结合基于薄透镜相机模型的高斯解析卷积以及色调映射模块，我们的重建能够渲染具有灵活重对焦功能的 HDR 内容。我们证明了我们对 HDR 和景深的组合处理有助于实时电影渲染，其性能优于现有技术水平。 
**|

## 分类/检测/识别/分割

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**SatDiffMoE: A Mixture of Estimation Method for Satellite Image Super-resolution with Latent Diffusion Models**|[2406.10225](http://arxiv.org/abs/2406.10225)|null|**在获取卫星图像时，由于卫星成像系统机载传感器的限制，通常需要在空间分辨率和时间分辨率（采集频率）之间进行权衡。高分辨率卫星图像对于土地作物监测、城市规划、野火管理和各种应用非常重要。实现高时空分辨率的卫星成像是一项重要且具有挑战性的任务。随着扩散模型的出现，我们现在可以学习强大的生成先验来生成具有高分辨率的逼真卫星图像，这也可以用于促进超分辨率任务。在这项工作中，我们提出了一种新的基于扩散的融合算法，称为 SatDiffMoE，它可以将同一位置的任意数量的连续低分辨率卫星图像作为输入，并通过利用和融合来自不同时间点的互补信息，将它们融合成一个更精细的高分辨率重建图像。我们的算法非常灵活，允许对任意数量的低分辨率图像进行训练和推理。实验结果表明，与以前的方法相比，我们提出的 SatDiffMoE 方法不仅在各种数据集上的卫星图像超分辨率任务中取得了优异的性能，而且还通过减少模型参数获得了更高的计算效率。 
**|
|**2024-06-14**|**EFM3D: A Benchmark for Measuring Progress Towards 3D Egocentric Foundation Models**|[2406.10224](http://arxiv.org/abs/2406.10224)|null|**The advent of wearable computers enables a new source of context for AI that is embedded in egocentric sensor data. This new egocentric data comes equipped with fine-grained 3D location information and thus presents the opportunity for a novel class of spatial foundation models that are rooted in 3D space. To measure progress on what we term Egocentric Foundation Models (EFMs) we establish EFM3D, a benchmark with two core 3D egocentric perception tasks. EFM3D is the first benchmark for 3D object detection and surface regression on high quality annotated egocentric data of Project Aria. We propose Egocentric Voxel Lifting (EVL), a baseline for 3D EFMs. EVL leverages all available egocentric modalities and inherits foundational capabilities from 2D foundation models. This model, trained on a large simulated dataset, outperforms existing methods on the EFM3D benchmark.**|
|**2024-06-14**|**YOLOv1 to YOLOv10: A comprehensive review of YOLO variants and their application in the agricultural domain**|[2406.10139](http://arxiv.org/abs/2406.10139)|null|**本综述调查了从 YOLOv1 到最先进的 YOLOv10 的各种 YOLO 变体在农业发展中的变革潜力。主要目标是阐明这些尖端的物体检测模型如何能够重振和优化农业的各个方面，从作物监测到牲畜管理。它旨在实现关键目标，包括识别农业中的当代挑战，详细评估 YOLO 的增量进步，以及探索其在农业中的具体应用。这是最早包含最新 YOLOv10 的调查之一，为人工智能和自动化时代精准农业和可持续农业实践的影响提供了新的视角。此外，该综述对 YOLO 的性能进行了批判性分析，综合了现有研究，并预测了未来趋势。通过仔细研究 YOLO 变体中包含的独特功能及其现实应用，本综述为 YOLO 变体与农业之间不断发展的关系提供了宝贵的见解。这些发现有助于深入理解精准农业和可持续农业实践的潜力，标志着农业领域内先进物体检测技术集成的重大进步。 
**|
|**2024-06-14**|**GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors**|[2406.10111](http://arxiv.org/abs/2406.10111)|null|**Achieving high-resolution novel view synthesis (HRNVS) from low-resolution input views is a challenging task due to the lack of high-resolution data. Previous methods optimize high-resolution Neural Radiance Field (NeRF) from low-resolution input views but suffer from slow rendering speed. In this work, we base our method on 3D Gaussian Splatting (3DGS) due to its capability of producing high-quality images at a faster rendering speed. To alleviate the shortage of data for higher-resolution synthesis, we propose to leverage off-the-shelf 2D diffusion priors by distilling the 2D knowledge into 3D with Score Distillation Sampling (SDS). Nevertheless, applying SDS directly to Gaussian-based 3D super-resolution leads to undesirable and redundant 3D Gaussian primitives, due to the randomness brought by generative priors. To mitigate this issue, we introduce two simple yet effective techniques to reduce stochastic disturbances introduced by SDS. Specifically, we 1) shrink the range of diffusion timestep in SDS with an annealing strategy; 2) randomly discard redundant Gaussian primitives during densification. Extensive experiments have demonstrated that our proposed GaussainSR can attain high-quality results for HRNVS with only low-resolution inputs on both synthetic and real-world datasets. Project page: https://chchnii.github.io/GaussianSR/**|
|**2024-06-14**|**Forgetting Order of Continual Learning: Examples That are Learned First are Forgotten Last**|[2406.09935](http://arxiv.org/abs/2406.09935)|null|**灾难性遗忘是持续学习中的一个重大挑战，模型在学习新数据时经常忘记之前的任务。我们的实证分析表明，灾难性遗忘与样本学习速度之间存在很强的相关性：早期学习的样本很少被遗忘，而后期学习的样本更容易被遗忘。我们证明，基于重放的持续学习方法可以通过关注中等学习速度的样本进行排练来利用这一现象。我们介绍了一种名为“金发姑娘”（Goldilocks）的新型重放缓冲区采样方法，该方法过滤掉学习速度过快或过慢的样本，保留学习速度适中的样本。金发姑娘改进了现有的持续学习算法，在多个图像分类任务中取得了最先进的结果。 
**|
|**2024-06-14**|**Exact Sparse Representation Recovery in Signal Demixing and Group BLASSO**|[2406.09922](http://arxiv.org/abs/2406.09922)|null|**本文简述了(Carioni 和 Del Grande, arXiv:2311.08072, 2023)中提出的关于凸正则化优化问题中稀疏表示恢复的理论。我们关注于未知量属于巴拿赫空间且测量值取自希尔伯特空间的情况，并探讨了此类设置下优化问题解的性质。具体而言，我们分析了一个Tikhonov正则化凸优化问题，其中 $y_0$是测量数据，$w$表示噪声，$\lambda$是正则化参数。通过引入度量非退化源条件(MNDSC)并考虑足够小的$\lambda$和$w$ ，我们为问题建立了精确稀疏表示恢复(ESRR)，这意味着解是唯一的，并且可以精确地恢复原始数据的稀疏表示。然后，我们通过两个新颖的应用：信号分离和基于组稀疏套索的超分辨率，强调了这一理论结果的实际意义。这些应用突出了我们结果的广泛适用性和重要性，展示了其在不同领域的潜力。 
**|
|**2024-06-14**|**Robust compressive tracking via online weighted multiple instance learning**|[2406.09914](http://arxiv.org/abs/2406.09914)|null|**开发强大的目标跟踪器是一项具有挑战性的任务，因为存在诸如遮挡、运动模糊、快速运动、光照变化、旋转、背景杂波、低分辨率以及帧间变形等因素。在文献中，已经提出了许多基于稀疏表示的优秀方法来解决上述问题。然而，大多数算法并不关注稀疏表示的学习。它们只考虑目标外观的建模，因此会因为不精确的训练样本而偏离目标。考虑到上述所有因素，我们提出了一种视觉目标跟踪算法，该算法集成了基于稀疏表示的由粗到精搜索策略和加权多示例学习 (WMIL) 算法。与其他跟踪器相比，由于采用了由粗到精的搜索方法，我们的方法以较低的复杂度获得了更多原始信号的信息，并且还对重要样本进行了加权。因此，它可以很容易地将背景特征与前景特征区分开来。此外，我们还从未遮挡的子区域中选择样本，以有效地开发强大的分类器。因此，我们实现了一个稳定而强大的目标跟踪器来解决所有上述问题。在具有挑战性的基准数据集上进行的定量和定性分析的实验结果表明了我们方法的准确性和效率。 
**|
|**2024-06-14**|**Bayesian Conditioned Diffusion Models for Inverse Problems**|[2406.09768](http://arxiv.org/abs/2406.09768)|null|**扩散模型最近在许多涉及基于正向测量算子的逆问题的图像重建任务中表现出色。一种常见框架是使用与任务无关的无条件模型，这些模型稍后会进行后验条件化以进行重建，但这种方法通常存在任务性能欠佳的问题。虽然也有人提出了特定于任务的条件模型，但当前的方法只是简单地将测量数据作为朴素的输入通道，这会导致采样不准确。在这里，我们解决了扩散模型的最佳条件化问题，以解决图像重建过程中出现的具有挑战性的逆问题。具体来说，我们提出了一种新的基于分数函数的扩散模型贝叶斯条件化技术BCDM，该技术基于给定测量数据的期望图像条件分布的分数函数。我们严格推导了表达和训练条件分数函数的理论。最后，我们展示了该技术在图像去锯齿、去模糊、超分辨率和修复方面的最新性能。 
**|
|**2024-06-14**|**Automated GIS-Based Framework for Detecting Crosswalk Changes from Bi-Temporal High-Resolution Aerial Images**|[2406.09731](http://arxiv.org/abs/2406.09731)|null|**Identification of changes in pavement markings has become crucial for infrastructure monitoring, maintenance, development, traffic management, and safety. Automated extraction of roadway geometry is critical in helping with this, given the increasing availability of high-resolution images and advancements in computer vision and object detection. Specifically, due to the substantial volume of satellite and high-resolution aerial images captured at different time instances, change detection has become a viable solution. In this study, an automated framework is developed to detect changes in crosswalks of Orange, Osceola, and Seminole counties in Florida, utilizing data extracted from high-resolution images obtained at various time intervals. Specifically, for Orange County, crosswalk changes between 2019 and 2021 were manually extracted, verified, and categorized as either new or modified crosswalks. For Seminole County, the developed model was used to automatically extract crosswalk changes between 2018 and 2021, while for Osceola County, changes between 2019 and 2020 were extracted. Findings indicate that Orange County witnessed approximately 2,094 crosswalk changes, with 312 occurring on state roads. In Seminole and Osceola counties, on the other hand, 1,040 and 1,402 crosswalk changes were observed on both local and state roads, respectively. Among these, 340 and 344 were identified on state roads in Seminole and Osceola, respectively. Spatiotemporal changes observed in crosswalks can be utilized to regularly update the existing crosswalk inventories, which is essential for agencies engaged in traffic and safety studies. Data extracted from these crosswalk changes can be combined with traffic and crash data to provide valuable insights to policymakers.**|
|**2024-06-14**|**An alternate approach for estimating grain-growth kinetics**|[2406.09653](http://arxiv.org/abs/2406.09653)|null|**Rate of grain growth, which aides in achieving desired properties in polycrystalline materials, is conventionally estimated by measuring the size of grains and tracking its change in micrographs reflecting the temporal evolution. Techniques adopting this conventional approach demand an absolute distinction between the grains and the interface separating them to yield an accurate result. Edge-detection, segmentation and other deep-learning algorithms are increasingly adopted to expose the network of boundaries and the associated grains precisely. An alternate approach for measuring grain-growth kinetics, that curtails the need for advanced image-processing treatment, is presented in this work. Grain-growth rate in the current technique is ascertained by \textit{counting} the number of triple-( and quadruple-) junctions, and monitoring its change during the microstructural evolution. The shifted focus of this junction-based treatment minimises the significance of a well-defined grain-boundary network, and consequently, the involvement of the sophisticated techniques that expose them. A regression-based object-detection algorithm is extended to realise, and count, the number of junctions in polycrystalline microstructures. By examining the change in the number of junctions with time, the growth rate is subsequently determined.Growth kinetics estimated by the present junction-based approach, across a wide-range of multiphase polycrystalline microstructures, agree convincingly with the outcomes of the conventional treatment.Besides offering a novel technique for grain-growth measurement, the analysis accompanying the current work unravels a trend, compatible with the topological events, in the progressive evolution of the triple-junctions count. The present approach, through its underlying algorithm, provides a promising option for monitoring grain-growth during in-situ investigations.**|

## 模型压缩/优化

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**Self-Knowledge Distillation for Learning Ambiguity**|[2406.09719](http://arxiv.org/abs/2406.09719)|null|**近来的语言模型在自然语言理解 (NLU) 任务中表现出色。然而，当面对可以多重解读的模糊样本时，它们往往表现欠佳，会在未考虑其正确性的情况下过度自信地预测单个标签。为了解决这个问题，我们提出了一种新颖的自我知识蒸馏方法，通过利用从模型低层提取的知识，使模型能够更准确地学习标签分布。这种方法还包括一个学习阶段，根据蒸馏的分布知识，重新校准那些被判定为极其模糊的训练样本的不必要增强的置信度。我们在不同的 NLU 基准数据集上验证了我们的方法，实验结果证明了其在生成更好标签分布方面的有效性。特别是，通过重新校准高度模糊样本的置信度，显著缓解了未见样本预测与其真实标签不匹配时的过度自信问题。这已被证明有助于生成比现有最先进方法更好的分布。此外，与现有方法相比，我们的方法在训练模型方面效率更高，因为它不需要额外的训练过程来细化标签分布。 
**|
|**2024-06-14**|**Frequency-mix Knowledge Distillation for Fake Speech Detection**|[2406.09664](http://arxiv.org/abs/2406.09664)|null|**在电话场景中，用于对抗语音欺骗攻击的假语音检测 (FSD) 任务极具挑战性。数据增强 (DA) 方法被认为是解决电话场景中 FSD 任务的有效手段，通常分为时域和频域两个阶段。虽然每种方法都有其优点，但都可能导致信息丢失。为了解决这个问题，我们提出了一种新颖的 DA 方法，即频率混合 (Freqmix)，并引入了 Freqmix 知识蒸馏 (FKD) 来增强模型的信息提取和泛化能力。具体来说，我们使用 Freqmix 增强的数据作为教师模型的输入，而学生模型的输入则经过时域 DA 方法处理。我们使用多级特征蒸馏方法来恢复信息并提高模型的泛化能力。我们的方法在 ASVspoof 2021 LA 数据集上取得了最先进的结果，与基线相比提高了 31%，并在 ASVspoof 2021 DF 数据集上表现出竞争力。 
**|
|**2024-06-13**|**RobustSAM: Segment Anything Robustly on Degraded Images**|[2406.09627](http://arxiv.org/abs/2406.09627)|null|**图像分割领域中，Segment Anything Model (SAM) 凭借其强大的零样本分割能力和灵活的提示系统，已成为一种变革性的方法。 然而，SAM 在处理低质量图像时性能会下降。为了解决这一局限性，我们提出了 Robust Segment Anything Model (RobustSAM)，它在保留 SAM 提示能力和零样本泛化能力的同时，提高了其在低质量图像上的性能。我们的方法利用预训练的 SAM 模型，仅增加了少量参数和计算需求。RobustSAM 的额外参数可以在 8 个 GPU 上用 30 小时内完成优化，这证明了其对于典型研究实验室的可行性和实用性。我们还介绍了 Robust-Seg 数据集，这是一个包含 688K 对图像-掩码对的集合，这些图像对具有不同的退化程度，旨在优化我们模型的训练和评估。 跨多个分割任务和数据集的大量实验，证实了 RobustSAM 具有优越的性能，尤其是在零样本条件下，突出了其在广泛现实应用中的潜力。此外，我们的方法已被证明可以有效提高基于 SAM 的下游任务的性能，例如单图像去雾和去模糊。 
**|
|**2024-06-13**|**Contextual Distillation Model for Diversified Recommendation**|[2406.09021](http://arxiv.org/abs/2406.09021)|null|**推荐的多样性与准确性在改善用户体验方面同样重要。现有的研究，例如行列式点过程（DPP）和最大边缘相关性（MMR），采用贪婪范式迭代地选择同时优化准确性和多样性的项目。然而，先前的方法通常表现出二次复杂度，将其应用限制在重排序阶段，并且不适用于候选项目池更大的其他推荐阶段，例如预排序和排序阶段。在本文中，我们提出了上下文蒸馏模型 (CDM)，这是一种有效的解决多样性问题的推荐模型，适用于工业推荐流程的所有阶段的部署。具体来说，CDM 利用同一用户请求中的候选项目作为上下文来增强结果的多样性。我们提出了一种对比上下文编码器，它采用注意力机制来对正面和负面上下文进行建模。对于 CDM 的训练，我们将每个目标项目与其上下文嵌入进行比较，并利用知识蒸馏框架来学习 MMR 算法下每个目标项目的获胜概率，其中教师来自 MMR 输出。在推理过程中，排名是通过推荐模型得分和学生模型得分的线性组合来执行的，从而确保了多样性和效率。我们在两个工业数据集上进行了离线评估，并在短视频平台快手上对 CDM 进行了在线 A/B 测试。正如指标所示，推荐质量和多样性方面观察到的显着增强有力地证明了 CDM 的有效性。**|
|**2024-06-12**|**Unveiling Incomplete Modality Brain Tumor Segmentation: Leveraging Masked Predicted Auto-Encoder and Divergence Learning**|[2406.08634](http://arxiv.org/abs/2406.08634)|null|**Brain tumor segmentation remains a significant challenge, particularly in the context of multi-modal magnetic resonance imaging (MRI) where missing modality images are common in clinical settings, leading to reduced segmentation accuracy. To address this issue, we propose a novel strategy, which is called masked predicted pre-training, enabling robust feature learning from incomplete modality data. Additionally, in the fine-tuning phase, we utilize a knowledge distillation technique to align features between complete and missing modality data, simultaneously enhancing model robustness. Notably, we leverage the Holder pseudo-divergence instead of the KLD for distillation loss, offering improve mathematical interpretability and properties. Extensive experiments on the BRATS2018 and BRATS2020 datasets demonstrate significant performance enhancements compared to existing state-of-the-art methods.**|
|**2024-06-14**|**Adaptive Teaching with Shared Classifier for Knowledge Distillation**|[2406.08528](http://arxiv.org/abs/2406.08528)|**[link](https://github.com/random2314235/atsc)**|**知识蒸馏 (KD) 是一种用于将知识从过参数化的教师网络转移到参数较少的学生网络的技术，从而最大限度地减少性能损失。KD 方法可以分为离线和在线两种方法。离线 KD 利用强大的预训练教师网络，而在线 KD 允许动态调整教师网络以提高学生网络的学习效率。最近，人们发现共享教师网络的分类器可以显著提高学生网络的性能，而网络参数只增加了很少一部分。基于这些见解，我们提出了具有共享分类器的自适应教学 (ATSC)。在 ATSC 中，预训练的教师网络会根据学生网络的能力进行自我调整，以便更好地满足其学习需求，而学生网络则受益于共享分类器，从而提高其性能。此外，我们将 ATSC 扩展到具有多个教师的环境。我们进行了大量的实验，证明了所提出的 KD 方法的有效性。我们的方法在单教师和多教师场景下，在 CIFAR-100 和 ImageNet 数据集上都取得了最先进的结果，而所需的模型参数只增加了很少一部分。源代码可在 https://github.com/random2314235/ATSC 公开获取。 
**|
|**2024-06-12**|**DistilDoc: Knowledge Distillation for Visually-Rich Document Applications**|[2406.08226](http://arxiv.org/abs/2406.08226)|null|**This work explores knowledge distillation (KD) for visually-rich document (VRD) applications such as document layout analysis (DLA) and document image classification (DIC). While VRD research is dependent on increasingly sophisticated and cumbersome models, the field has neglected to study efficiency via model compression. Here, we design a KD experimentation methodology for more lean, performant models on document understanding (DU) tasks that are integral within larger task pipelines. We carefully selected KD strategies (response-based, feature-based) for distilling knowledge to and from backbones with different architectures (ResNet, ViT, DiT) and capacities (base, small, tiny). We study what affects the teacher-student knowledge gap and find that some methods (tuned vanilla KD, MSE, SimKD with an apt projector) can consistently outperform supervised student training. Furthermore, we design downstream task setups to evaluate covariate shift and the robustness of distilled DLA models on zero-shot layout-aware document visual question answering (DocVQA). DLA-KD experiments result in a large mAP knowledge gap, which unpredictably translates to downstream robustness, accentuating the need to further explore how to efficiently obtain more semantic document layout awareness.**|
|**2024-06-12**|**Low-Complexity Acoustic Scene Classification Using Parallel Attention-Convolution Network**|[2406.08119](http://arxiv.org/abs/2406.08119)|null|**这项工作是我们提交给 DCASE2023 挑战赛任务 1 的改进系统。我们提出了一种低复杂度的声音场景分类方法，该方法采用并行注意力-卷积网络，该网络由四个模块组成，包括预处理、融合、全局和局部上下文信息提取。所提出的网络在从每个音频片段中捕获全局和局部上下文信息方面具有计算效率。此外，我们将其他技术集成到我们的方法中，例如知识蒸馏、数据增强和自适应残差归一化。在 DCASE2023 挑战赛的官方数据集上进行评估时，我们的方法获得了 56.10% 的最高准确率，参数量为 5.21 千，乘法累加操作次数为 144 万次。它在准确性和复杂度方面超过了 DCASE2023 挑战赛的前两个系统，并获得了最先进的结果。代码位于：https://github.com/Jessytan/Low-complexity-ASC。 
**|
|**2024-06-12**|**Guiding Frame-Level CTC Alignments Using Self-knowledge Distillation**|[2406.07909](http://arxiv.org/abs/2406.07909)|null|**基于连接主义时间分类（CTC）框架的Transformer编码器广泛应用于自动语音识别（ASR）。然而，用于ASR的知识蒸馏（KD）存在师生模型在帧级对齐上的分歧问题，最终阻碍了其对学生模型性能的提升。为了解决这个问题，本文介绍了一种自知识蒸馏（SKD）方法，在训练期间指导帧级对齐。与使用单独的教师和学生模型的传统方法相比，本研究引入了一种简单有效的方法，共享编码器层并将子模型作为学生模型。总的来说，我们的方法可以有效地提高资源效率和性能。我们还对尖峰时间进行了实验分析，以说明所提出的方法通过减少对齐分歧来提高性能。 
**|
|**2024-06-12**|**Small Scale Data-Free Knowledge Distillation**|[2406.07876](http://arxiv.org/abs/2406.07876)|**[link](https://github.com/osvai/ssd-kd)**|**Data-free knowledge distillation is able to utilize the knowledge learned by a large teacher network to augment the training of a smaller student network without accessing the original training data, avoiding privacy, security, and proprietary risks in real applications. In this line of research, existing methods typically follow an inversion-and-distillation paradigm in which a generative adversarial network on-the-fly trained with the guidance of the pre-trained teacher network is used to synthesize a large-scale sample set for knowledge distillation. In this paper, we reexamine this common data-free knowledge distillation paradigm, showing that there is considerable room to improve the overall training efficiency through a lens of ``small-scale inverted data for knowledge distillation". In light of three empirical observations indicating the importance of how to balance class distributions in terms of synthetic sample diversity and difficulty during both data inversion and distillation processes, we propose Small Scale Data-free Knowledge Distillation SSD-KD. In formulation, SSD-KD introduces a modulating function to balance synthetic samples and a priority sampling function to select proper samples, facilitated by a dynamic replay buffer and a reinforcement learning strategy. As a result, SSD-KD can perform distillation training conditioned on an extremely small scale of synthetic samples (e.g., 10X less than the original training data scale), making the overall training efficiency one or two orders of magnitude faster than many mainstream methods while retaining superior or competitive model performance, as demonstrated on popular image classification and semantic segmentation benchmarks. The code is available at https://github.com/OSVAI/SSD-KD.**|

## OCR

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**Enhancing Question Answering on Charts Through Effective Pre-training Tasks**|[2406.10085](http://arxiv.org/abs/2406.10085)|null|**为了完全理解一个文档，仅仅使用文本信息是不够的，理解视觉线索（例如布局和图表）也是必要的。虽然目前最先进的文档理解方法（基于OCR和无OCR）效果很好，但尚未对其功能和局限性进行彻底的分析。因此，在这项工作中，我们解决了当前视觉问答模型在应用于图表时存在的局限性。为了调查最先进模型的缺陷，我们以ChartQA为例，进行了全面的行为分析。我们的研究结果表明，现有模型在回答与图表结构和视觉上下文以及数字信息相关的问题时表现不佳。为了解决这些问题，我们提出了三个简单的预训练任务，从结构视觉知识及其对数字问题的理解两方面强化现有模型。我们在三个图表数据集（包括提取性和抽象性问题数据集）上评估了我们预训练的模型（称为MatCha-v2），观察到它比基线模型平均提高了1.7%。 
**|
|**2024-06-14**|**OSPC: Detecting Harmful Memes with Large Language Model as a Catalyst**|[2406.09779](http://arxiv.org/abs/2406.09779)|null|**Memes, which rapidly disseminate personal opinions and positions across the internet, also pose significant challenges in propagating social bias and prejudice. This study presents a novel approach to detecting harmful memes, particularly within the multicultural and multilingual context of Singapore. Our methodology integrates image captioning, Optical Character Recognition (OCR), and Large Language Model (LLM) analysis to comprehensively understand and classify harmful memes. Utilizing the BLIP model for image captioning, PP-OCR and TrOCR for text recognition across multiple languages, and the Qwen LLM for nuanced language understanding, our system is capable of identifying harmful content in memes created in English, Chinese, Malay, and Tamil. To enhance the system's performance, we fine-tuned our approach by leveraging additional data labeled using GPT-4V, aiming to distill the understanding capability of GPT-4V for harmful memes to our system. Our framework achieves top-1 at the public leaderboard of the Online Safety Prize Challenge hosted by AI Singapore, with the AUROC as 0.7749 and accuracy as 0.7087, significantly ahead of the other teams. Notably, our approach outperforms previous benchmarks, with FLAVA achieving an AUROC of 0.5695 and VisualBERT an AUROC of 0.5561.**|
|**2024-06-12**|**M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation**|[2406.08255](http://arxiv.org/abs/2406.08255)|**[link](https://github.com/amazon-science/m3t-multi-modal-translation-bench)**|**Document translation poses a challenge for Neural Machine Translation (NMT) systems. Most document-level NMT systems rely on meticulously curated sentence-level parallel data, assuming flawless extraction of text from documents along with their precise reading order. These systems also tend to disregard additional visual cues such as the document layout, deeming it irrelevant. However, real-world documents often possess intricate text layouts that defy these assumptions. Extracting information from Optical Character Recognition (OCR) or heuristic rules can result in errors, and the layout (e.g., paragraphs, headers) may convey relationships between distant sections of text. This complexity is particularly evident in widely used PDF documents, which represent information visually. This paper addresses this gap by introducing M3T, a novel benchmark dataset tailored to evaluate NMT systems on the comprehensive task of translating semi-structured documents. This dataset aims to bridge the evaluation gap in document-level NMT systems, acknowledging the challenges posed by rich text layouts in real-world applications.**|
|**2024-06-10**|**VCR: Visual Caption Restoration**|[2406.06462](http://arxiv.org/abs/2406.06462)|**[link](https://github.com/tianyu-z/vcr)**|**我们介绍了视觉字幕修复 (VCR)，这是一项新颖的视觉语言任务，它要求模型利用图像中的像素级提示准确地恢复部分遮挡的文本。这项任务源于以下观察结果：嵌入在图像中的文本与常见的视觉元素和自然语言有着本质的不同，因为它需要对视觉、文本和嵌入在图像中的文本进行模态对齐。虽然许多工作已经将嵌入在图像中的文本整合到视觉问答任务中，但这些任务的方法通常依赖于光学字符识别或掩码语言建模，从而将任务简化为主要基于文本的处理。然而，基于文本的处理在 VCR 中变得无效，因为准确的文本恢复依赖于来自所提供图像、上下文和来自被遮蔽文本的微小暴露区域的细微线索的组合信息。我们开发了一个管道来生成 VCR 任务的合成图像，使用图像-字幕对，并可调整字幕可见性以控制任务难度。利用此管道，我们使用来自维基百科的带字幕的图像构建了一个用于 VCR 的数据集，称为 VCR-Wiki，其中包含简单和困难拆分变体中的 211 万个英文实体和 34.6 万个中文实体。我们的结果表明，当前的视觉语言模型在 VCR 任务中的表现明显落后于人类，仅仅在我们数据集上微调模型并不会带来显著的改进。我们发布了 VCR-Wiki 和数据构建代码，以促进未来的研究。 
**|
|**2024-06-07**|**Scaling Automatic Extraction of Pseudocode**|[2406.04635](http://arxiv.org/abs/2406.04635)|null|**学术论文中的伪代码提供了一种简洁的方式来表达其中实现的算法。伪代码也可以被认为是一种中间表示，有助于弥合编程语言和自然语言之间的差距。拥有大量的伪代码集合可以带来各种好处，从增强算法理解、促进进一步的算法设计，到支持基于 NLP 或计算机视觉的模型，用于自动代码生成和光学字符识别 (OCR) 等任务。我们通过从 arXiv 论文中提取近 320,000 个伪代码示例，创建了一个大型伪代码集合。这个过程涉及扫描超过 220 万篇学术论文，其中 1,000 篇经过人工检查和标记。鉴于集合固有的异质性，我们的方法包括一个为优化覆盖范围而定制的提取机制，以及一个基于随机抽样的验证机制，以检查其准确性和可靠性。此外，我们还提供了对常见伪代码结构的见解，并辅以聚类和统计分析。值得注意的是，这些分析表明伪代码的使用呈指数级增长，突显了它们日益增长的重要性。 
**|
|**2024-06-06**|**CORU: Comprehensive Post-OCR Parsing and Receipt Understanding Dataset**|[2406.04493](http://arxiv.org/abs/2406.04493)|**[link](https://github.com/update-for-integrated-business-ai/coru)**|**在光学字符识别（OCR）和自然语言处理（NLP）领域，集成多语言功能仍然是一项关键挑战，尤其是在考虑阿拉伯语等复杂脚本语言时。本文介绍了综合性OCR后解析和收据理解数据集（CORU），这是一个专门设计用于增强多语言环境（涉及阿拉伯语和英语）下收据的OCR和信息提取的新数据集。CORU包含从超市和服装店等不同零售环境中收集的20,000多张带注释的收据，以及30,000张用于OCR的带注释图像（用于识别每个检测到的行），以及10,000个为详细信息提取而注释的项目。这些注释捕获了基本细节，例如商家名称、商品描述、总价、收据编号和日期。它们的结构支持三种主要的计算任务：目标检测、OCR和信息提取。我们在CORU上建立了一系列模型的基线性能，以评估Tesseract OCR等传统方法以及更先进的基于神经网络的方法的有效性。这些基线对于处理现实世界中收据常见的复杂且嘈杂的文档布局以及推进自动多语言文档处理的现状至关重要。我们的数据集可公开访问（https://github.com/Update-For-Integrated-Business-AI/CORU）。 
**|
|**2024-06-03**|**Generalized Jersey Number Recognition Using Multi-task Learning With Orientation-guided Weight Refinement**|[2406.01033](http://arxiv.org/abs/2406.01033)|null|**球衣号码识别 (JNR) 一直是体育分析中的一项重要任务。由于图像存在模糊、遮挡、变形和低分辨率等问题，提高识别精度仍然是一项持续的挑战。最近的研究已经使用数字定位和光学字符识别来解决这些问题。一些方法将球员识别方案应用于图像序列，而忽略了人体旋转角度对球衣数字识别的影响。通过使用多任务方案来识别每个数字，可以准确预测球衣数字的数量，从而获得更稳健的结果。基于以上考虑，本文提出了一种称为角度-数字优化方案 (ADRS) 的多任务学习方法，该方法结合人体方向角度和数字线索来识别运动球衣号码。根据我们的实验结果，我们的方法增加了推理信息，显着提高了预测精度。与只能处理单一运动类型的现有技术方法相比，所提出的方法产生了更加多样化和实用的 JNR 应用。在我们的数据集中纳入了足球、篮球、排球和棒球等多种类型的团队运动，这极大地促进了体育分析中通用的 JNR。我们的准确率在 Top-1 上达到了 64.07%，在 Top-2 上达到了 89.97%，相应的 F1 分数分别为 67.46% 和 90.64%。 
**|
|**2024-05-30**|**Scaling up archival text analysis with the blockmodeling of n-gram networks -- A case study of Bulgaria's representation in the Osservatore Romano (January-May 1877)**|[2405.20156](http://arxiv.org/abs/2405.20156)|null|**This paper seeks to bridge the gap between archival text analysis and network analysis by applying network clustering methods to analyze the coverage of Bulgaria in 123 issues of the newspaper Osservatore Romano published between January and May 1877. Utilizing optical character recognition and generalized homogeneity blockmodeling, the study constructs networks of relevant keywords. Those including the sets Bulgaria and Russia are rather isomorphic and they largely overlap with those for Germany, Britain, and War. In structural terms, the blockmodel of the two networks exhibits a clear core-semiperiphery-periphery structure that reflects relations between concepts in the newpaper's coverage. The newspaper's lexical choices effectively delegitimised the Bulgarian national revival, highlighting the influence of the Holy See on the newspaper's editorial line.**|
|**2024-05-30**|**Towards Unified Multi-granularity Text Detection with Interactive Attention**|[2405.19765](http://arxiv.org/abs/2405.19765)|null|**现有的OCR引擎或文档图像分析系统通常依赖于针对不同场景和粒度的文本检测训练单独的模型，这会导致巨大的计算复杂性和资源需求。在本文中，我们介绍了“检测任何文本”（DAT），这是一种先进的范式，它将场景文本检测、布局分析和文档页面检测无缝地统一到一个内聚的端到端模型中。这种设计使DAT能够有效地管理不同粒度的文本实例，包括*单词*、*行*、*段落*和*页面*。DAT的一项关键创新是跨粒度交互注意力模块，它通过关联不同文本查询之间的结构信息，显著增强了不同粒度文本实例的表示学习。因此，它使模型能够在多个文本粒度上实现互惠互利的检测性能。此外，基于提示的分割模块改进了对任意曲率和复杂布局文本的检测结果，从而提高了DAT的准确性并扩展了其现实世界的适用性。实验结果表明，DAT在各种文本相关基准测试中均达到了最先进的性能，包括多方向/任意形状场景文本检测、文档布局分析和页面检测任务。 
**|
|**2024-05-28**|**RealitySummary: On-Demand Mixed Reality Document Enhancement using Large Language Models**|[2405.18620](http://arxiv.org/abs/2405.18620)|null|**我们推出了 RealitySummary，这是一种混合现实阅读助手，可以通过按需文本提取、摘要和增强功能来增强任何印刷或数字文档。虽然增强现实阅读工具有望通过叠加数字内容来增强物理阅读体验，但以前的系统通常需要预先处理文档，这限制了它们的通用性和实际用例。在本文中，我们利用大型语言模型探索按需文档增强。为了解不同文档的通用技术，我们首先进行了一项探索性设计研究，确定了五类文档增强功能（摘要、增强、导航、比较和提取）。在此基础上，我们开发了一个概念验证系统，该系统可以使用 Google Cloud OCR 和 GPT-4 自动提取和汇总文本，然后使用 Microsoft Hololens 2 和 Apple Vision Pro 在文档周围嵌入信息。我们演示了六种特定文档增强的实时示例：1）摘要，2）比较表，3）时间线，4）关键词列表，5）摘要突出显示和 6）信息卡。可用性研究 (N=12) 和实地研究 (N=11) 的结果突出了按需 MR 文档增强的潜在优势和未来研究的机会。 
**|

## 生成模型

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**SatDiffMoE: A Mixture of Estimation Method for Satellite Image Super-resolution with Latent Diffusion Models**|[2406.10225](http://arxiv.org/abs/2406.10225)|null|**在获取卫星图像时，由于卫星成像系统机载传感器的限制，空间分辨率和时间分辨率（获取频率）之间通常需要权衡。高分辨率卫星图像对于土地作物监测、城市规划、野火管理和各种应用非常重要。实现高时空分辨率的卫星成像是一项重要且具有挑战性的任务。随着扩散模型的出现，我们现在可以学习强大的生成先验来生成具有高分辨率的真实卫星图像，这也可以用于促进超分辨率任务。在这项工作中，我们提出了一种新的基于扩散的融合算法，称为 SatDiffMoE，它可以将同一位置的任意数量的连续低分辨率卫星图像作为输入，并通过利用和融合来自不同时间点的补充信息，将它们融合成一个更精细的高分辨率重建图像。我们的算法非常灵活，允许对任意数量的低分辨率图像进行训练和推理。实验结果表明，我们提出的 SatDiffMoE 方法不仅在各种数据集上的卫星图像超分辨率任务中取得了优异的性能，而且与以前的方法相比，还通过减少模型参数获得了更高的计算效率。 
**|
|**2024-06-14**|**Universal randomised signatures for generative time series modelling**|[2406.10214](http://arxiv.org/abs/2406.10214)|**[link](https://github.com/niklaswalter/randomised-signature-timeseries-generation)**|**Randomised signature has been proposed as a flexible and easily implementable alternative to the well-established path signature. In this article, we employ randomised signature to introduce a generative model for financial time series data in the spirit of reservoir computing. Specifically, we propose a novel Wasserstein-type distance based on discrete-time randomised signatures. This metric on the space of probability measures captures the distance between (conditional) distributions. Its use is justified by our novel universal approximation results for randomised signatures on the space of continuous functions taking the underlying path as an input. We then use our metric as the loss function in a non-adversarial generator model for synthetic time series data based on a reservoir neural stochastic differential equation. We compare the results of our model to benchmarks from the existing literature.**|
|**2024-06-14**|**DiffusionBlend: Learning 3D Image Prior through Position-aware Diffusion Score Blending for 3D Computed Tomography Reconstruction**|[2406.10211](http://arxiv.org/abs/2406.10211)|null|**扩散模型在现实应用中用于大规模医学图像重建（如 3D 计算机断层扫描 (CT)）时，面临着巨大的挑战。由于对内存、时间和数据的要求很高，很难直接在高维数据的整个体积上训练扩散模型，从而获得高效的 3D 扩散先验。现有的工作利用带有手工制作的跨切片正则化的单个 2D 图像切片上的扩散先验会牺牲 z 轴的一致性，从而导致沿 z 轴出现严重的伪影。在这项工作中，我们提出了一个新颖的框架，该框架能够通过位置感知的 3D 面片扩散分数混合来学习 3D 图像先验，以重建大规模 3D 医学图像。据我们所知，我们是第一个利用 3D 面片扩散先验进行 3D 医学图像重建的。对稀疏视图和有限角度 CT 重建的大量实验表明，我们的 DiffusionBlend 方法明显优于以前的方法，并在具有高维 3D 图像（即 $256 \times 256 \times 500$ ）的现实 CT 重建问题上实现了最先进的性能。与以前的最先进方法相比，我们的算法还具有更好或相当的计算效率。 
**|
|**2024-06-14**|**Make It Count: Text-to-Image Generation with an Accurate Number of Objects**|[2406.10210](http://arxiv.org/abs/2406.10210)|null|**Despite the unprecedented success of text-to-image diffusion models, controlling the number of depicted objects using text is surprisingly hard. This is important for various applications from technical documents, to children's books to illustrating cooking recipes. Generating object-correct counts is fundamentally challenging because the generative model needs to keep a sense of separate identity for every instance of the object, even if several objects look identical or overlap, and then carry out a global computation implicitly during generation. It is still unknown if such representations exist. To address count-correct generation, we first identify features within the diffusion model that can carry the object identity information. We then use them to separate and count instances of objects during the denoising process and detect over-generation and under-generation. We fix the latter by training a model that predicts both the shape and location of a missing object, based on the layout of existing ones, and show how it can be used to guide denoising with correct object count. Our approach, CountGen, does not depend on external source to determine object layout, but rather uses the prior from the diffusion model itself, creating prompt-dependent and seed-dependent layouts. Evaluated on two benchmark datasets, we find that CountGen strongly outperforms the count-accuracy of existing baselines.**|
|**2024-06-14**|**Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual Visual Text Rendering**|[2406.10208](http://arxiv.org/abs/2406.10208)|null|**最近，Glyph-ByT5在图形设计图像中实现了高度准确的视觉文本渲染性能。然而，它仍然只关注英语，并且在视觉吸引力方面表现相对较差。在这项工作中，我们通过提出Glyph-ByT5-v2和Glyph-SDXL-v2来解决这两个基本限制，它们不仅支持10种不同语言的准确视觉文本渲染，而且还实现了更好的审美质量。为了实现这一目标，我们做出了以下贡献：(i) 创建了一个高质量的多语言字形-文本和图形设计数据集，该数据集包含超过100万个字形-文本对和1000万个图形设计图像-文本对，涵盖其他九种语言；(ii) 构建了一个包含1000个提示的多语言视觉段落基准，每种语言100个，用于评估多语言视觉拼写准确性；(iii) 利用最新的步骤感知偏好学习方法来增强视觉审美质量。通过结合这些技术，我们提供了一个强大的定制多语言文本编码器Glyph-ByT5-v2和一个强大的审美图形生成模型Glyph-SDXL-v2，它们可以支持10种不同语言的准确拼写。考虑到最新的DALL-E3和Ideogram 1.0仍然难以完成多语言视觉文本渲染任务，我们认为我们的工作是一个重大进步。 
**|
|**2024-06-14**|**Crafting Parts for Expressive Object Composition**|[2406.10197](http://arxiv.org/abs/2406.10197)|null|**像Stable Diffusion、DALLE-2等大型生成模型的文生图技术，由于其卓越的质量和广泛的知识库，已成为各种任务的通用基础。由于图像合成和生成是创造性的过程，艺术家需要控制生成图像的各个部分。我们发现，仅仅在基本文本提示中添加关于部分的细节，要么会导致完全不同的图像（例如，缺失/不正确的身份），要么会导致额外的部分细节被忽略。为了缓解这些问题，我们引入了PartCraft，它能够根据为基本文本提示中的对象指定的细粒度部分级别细节生成图像。这允许艺术家进行更多控制，并通过组合不同的对象部分实现新颖的对象组合。PartCraft首先通过对特定扩散过程中的对象区域进行去噪来定位对象部分。这使得每个部分标记能够定位到正确的对象区域。在获得部分掩码后，我们根据细粒度的部分描述在每个部分区域运行局部扩散过程，并将它们组合起来生成最终图像。PartCraft的所有阶段都基于对预训练扩散模型的重新利用，这使得它能够跨各种领域进行泛化，而无需训练。我们通过视觉示例定性地展示了PartCraft提供的部分级别控制的有效性，并与当代基线进行了定量比较。 
**|
|**2024-06-14**|**Enhancing Incomplete Multi-modal Brain Tumor Segmentation with Intra-modal Asymmetry and Inter-modal Dependency**|[2406.10175](http://arxiv.org/abs/2406.10175)|null|**近年来，基于深度学习的多模态MRI图像脑肿瘤分割(BTS)模型取得了显著进展。然而，实践中一个常见问题是，由于扫描方案和患者情况的不同，某些模态的图像可能无法获取，这使得从不完整MRI模态进行分割成为一个具有挑战性的问题。以往的方法试图通过融合可获取的多模态特征、利用注意力机制以及使用生成模型合成缺失模态来解决这个问题。然而，这些方法忽略了医学图像分割的内在问题，例如训练样本有限，特别是有肿瘤的情况下。此外，这些方法需要针对每个缺失模态子集训练和部署特定的模型。为了解决这些问题，我们提出了一种新方法，从两个方面增强BTS模型。首先，我们引入了一个预训练阶段，生成一个多样化的预训练数据集，涵盖各种不同的肿瘤形状和脑部解剖结构组合。其次，我们提出了一个后训练阶段，使模型能够在只有部分模态可用的情况下重建预测结果中的缺失模态。为了实现预训练阶段，我们从概念上将MRI图像解耦为两部分：“解剖结构”和“肿瘤”。我们使用从不同训练样本的解剖结构和肿瘤部分生成的合成数据对BTS模型进行预训练。……大量实验表明，我们提出的方法相对于基线方法显著提高了性能，并在三个脑肿瘤分割数据集（BRATS2020、BRATS2018和BRATS2015）上取得了新的最佳结果。 
**|
|**2024-06-14**|**MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers**|[2406.10163](http://arxiv.org/abs/2406.10163)|**[link](https://github.com/buaacyw/meshanything)**|**近来，通过重建和生成技术创建的3D资产在质量上已经可以与人工制作的资产相媲美，突显出其替代后者进行创作的潜力。然而，由于这些资产在实际应用中总是需要转换为网格形式才能用于3D行业应用，而现有的网格提取方法生成的网格质量远低于艺术家创建的网格（AM，即由人类艺术家创建的网格），因此这种潜力在很大程度上尚未实现。具体而言，现有的网格提取方法依赖于密集的面片，忽略了几何特征，导致效率低下、后期处理复杂以及表示质量较低等问题。为了解决这些问题，我们引入了MeshAnything，这是一个将网格提取视为生成问题的模型，可以生成与指定形状一致的AM。通过将任何3D表示形式的3D资产转换为AM，MeshAnything可以与各种3D资产制作方法相结合，从而增强其在3D行业中的应用。MeshAnything的架构包括一个VQ-VAE和一个形状条件解码器Transformer。我们首先使用VQ-VAE学习一个网格词汇表，然后在这个词汇表上训练形状条件解码器Transformer，用于形状条件自回归网格生成。我们的大量实验表明，我们的方法生成的AM面片数量比以前的方法少了几百倍，显著提高了存储、渲染和仿真效率，同时实现了与以前方法相当的精度。 
**|
|**2024-06-14**|**Training-free Camera Control for Video Generation**|[2406.10126](http://arxiv.org/abs/2406.10126)|null|**We propose a training-free and robust solution to offer camera movement control for off-the-shelf video diffusion models. Unlike previous work, our method does not require any supervised finetuning on camera-annotated datasets or self-supervised training via data augmentation. Instead, it can be plugged and played with most pretrained video diffusion models and generate camera controllable videos with a single image or text prompt as input. The inspiration of our work comes from the layout prior that intermediate latents hold towards generated results, thus rearranging noisy pixels in them will make output content reallocated as well. As camera move could also be seen as a kind of pixel rearrangement caused by perspective change, videos could be reorganized following specific camera motion if their noisy latents change accordingly. Established on this, we propose our method CamTrol, which enables robust camera control for video diffusion models. It is achieved by a two-stage process. First, we model image layout rearrangement through explicit camera movement in 3D point cloud space. Second, we generate videos with camera motion using layout prior of noisy latents formed by a series of rearranged images. Extensive experiments have demonstrated the robustness our method holds in controlling camera motion of generated videos. Furthermore, we show that our method can produce impressive results in generating 3D rotation videos with dynamic content. Project page at https://lifedecoder.github.io/CamTrol/.**|
|**2024-06-14**|**Precipitation Nowcasting Using Physics Informed Discriminator Generative Models**|[2406.10108](http://arxiv.org/abs/2406.10108)|null|**Nowcasting leverages real-time atmospheric conditions to forecast weather over short periods. State-of-the-art models, including PySTEPS, encounter difficulties in accurately forecasting extreme weather events because of their unpredictable distribution patterns. In this study, we design a physics-informed neural network to perform precipitation nowcasting using the precipitation and meteorological data from the Royal Netherlands Meteorological Institute (KNMI). This model draws inspiration from the novel Physics-Informed Discriminator GAN (PID-GAN) formulation, directly integrating physics-based supervision within the adversarial learning framework. The proposed model adopts a GAN structure, featuring a Vector Quantization Generative Adversarial Network (VQ-GAN) and a Transformer as the generator, with a temporal discriminator serving as the discriminator. Our findings demonstrate that the PID-GAN model outperforms numerical and SOTA deep generative models in terms of precipitation nowcasting downstream metrics.**|

## LLM

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**Quantifying Variance in Evaluation Benchmarks**|[2406.10229](http://arxiv.org/abs/2406.10229)|null|**评估基准是衡量大型语言模型 (LLM) 能力以及推动这些能力进步的基石。评估基准最初旨在评估完全预训练模型的能力（或缺乏能力），现在也被广泛用于决定各种训练选择。尽管使用广泛，但我们很少量化评估基准的方差，而方差决定了性能差异是否有意义。在这里，我们定义并测量了一系列旨在衡量评估基准方差的指标，包括初始化时的种子方差和训练期间的单调性。通过研究大量模型（包括公开可用的和从头开始预训练的），我们为各种方差指标提供了经验估计，并为实践者提供了考虑因素和建议。我们还评估了连续性能指标与离散性能指标的效用和权衡，并探索了更好地理解和减少这种差异的方案。我们发现，简单的更改（例如将选择任务（如 MMLU）构建为完成任务）通常可以减少较小规模（约 7B）模型的方差，而受人类测试文献启发的更复杂方法（例如项目分析和项目反应理论）难以有意义地减少方差。总的来说，我们的工作提供了对评估基准方差的见解，提出了减少方差的 LM 特定技术，并且更普遍地鼓励实践者在比较模型时仔细考虑方差。**|
|**2024-06-14**|**Short Film Dataset (SFD): A Benchmark for Story-Level Video Understanding**|[2406.10221](http://arxiv.org/abs/2406.10221)|null|**近年来，视觉语言模型的进步显著推动了视频理解领域的发展。然而，现有的数据集和任务存在明显的局限性。大多数数据集仅限于事件有限、叙述内容狭窄的短视频。例如，包含教学和以自我为中心的视频的数据集通常只记录一个人在单个场景中的活动。尽管一些电影数据集提供了更丰富的内容，但它们通常局限于短期任务，缺乏公开可用的视频，并且由于在LLM训练中使用了电影论坛和其他资源，经常遇到数据泄露问题。为了解决上述限制，我们提出了短片数据集（SFD），其中包含1078部公开可用的业余电影、各种类型和最小的数据泄露问题。SFD以多项选择和开放式问答的形式提供面向长期故事的视频任务。我们广泛的实验强调了解决SFD任务需要长期推理。值得注意的是，我们发现电影脚本中有强烈的信号，导致人类和LLM的性能相当。我们还发现，与仅使用视觉数据相比，当前模型的性能明显低于人类。**|
|**2024-06-14**|**Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs**|[2406.10216](http://arxiv.org/abs/2406.10216)|null|**基于人类偏好数据训练的奖励模型已被证明可以有效地将大型语言模型 (LLM) 与人类意图在人类反馈强化学习 (RLHF) 框架内进行对齐。然而，当前奖励模型对未见过的提示和响应的泛化能力有限。这种限制会导致一种称为奖励过度优化的意外现象，即对奖励的过度优化会导致实际性能下降。虽然之前的研究主张限制策略优化，但我们的研究提出了一种新方法，通过正则化隐藏状态来增强奖励模型针对分布变化的泛化能力。具体来说，我们保留了基础模型的语言模型头，并结合了一套文本生成损失来保持隐藏状态的文本生成能力，同时在相同的隐藏状态后面学习奖励头。我们的实验结果表明，引入的正则化技术显着提高了学习到的奖励模型在各种分布外 (OOD) 任务中的准确性，并有效地缓解了 RLHF 中的过度优化问题，从而提供了一种更可靠、更稳健的偏好学习范式。 
**|
|**2024-06-14**|**TRIP-PAL: Travel Planning with Guarantees by Combining Large Language Models and Automated Planners**|[2406.10196](http://arxiv.org/abs/2406.10196)|null|**Travel planning is a complex task that involves generating a sequence of actions related to visiting places subject to constraints and maximizing some user satisfaction criteria. Traditional approaches rely on problem formulation in a given formal language, extracting relevant travel information from web sources, and use an adequate problem solver to generate a valid solution. As an alternative, recent Large Language Model (LLM) based approaches directly output plans from user requests using language. Although LLMs possess extensive travel domain knowledge and provide high-level information like points of interest and potential routes, current state-of-the-art models often generate plans that lack coherence, fail to satisfy constraints fully, and do not guarantee the generation of high-quality solutions. We propose TRIP-PAL, a hybrid method that combines the strengths of LLMs and automated planners, where (i) LLMs get and translate travel information and user information into data structures that can be fed into planners; and (ii) automated planners generate travel plans that guarantee constraint satisfaction and optimize for users' utility. Our experiments across various travel scenarios show that TRIP-PAL outperforms an LLM when generating travel plans.**|
|**2024-06-14**|**CHIRON: Rich Character Representations in Long-Form Narratives**|[2406.10190](http://arxiv.org/abs/2406.10190)|null|**人物是长篇叙事的组成部分，但现有的故事分析和生成系统对其理解甚少。虽然先前的工作已经通过基于图的方法和简短的人物描述简化了人物，但我们的目标是通过从专业作家得到的建议中汲取灵感，更好地解决表示复杂人物的问题。我们提出了CHIRON，这是一种基于“人物卡”的新型表示方法，用于组织和过滤有关人物的文本信息。我们分两步构建CHIRON人物卡：生成模块通过问答提示大型语言模型获取人物信息，验证模块使用自动推理和特定领域蕴涵模型消除关于人物的错误事实。我们通过掩码人物预测的下游任务验证了CHIRON，实验表明CHIRON比类似的基于摘要的基线方法更好、更灵活。我们还表明，从CHIRON派生的指标可用于自动推断故事中的人物中心性，并且这些指标与人类判断一致。 
**|
|**2024-06-14**|**Detecting and Evaluating Medical Hallucinations in Large Vision Language Models**|[2406.10185](http://arxiv.org/abs/2406.10185)|null|**Large Vision Language Models (LVLMs) are increasingly integral to healthcare applications, including medical visual question answering and imaging report generation. While these models inherit the robust capabilities of foundational Large Language Models (LLMs), they also inherit susceptibility to hallucinations-a significant concern in high-stakes medical contexts where the margin for error is minimal. However, currently, there are no dedicated methods or benchmarks for hallucination detection and evaluation in the medical field. To bridge this gap, we introduce Med-HallMark, the first benchmark specifically designed for hallucination detection and evaluation within the medical multimodal domain. This benchmark provides multi-tasking hallucination support, multifaceted hallucination data, and hierarchical hallucination categorization. Furthermore, we propose the MediHall Score, a new medical evaluative metric designed to assess LVLMs' hallucinations through a hierarchical scoring system that considers the severity and type of hallucination, thereby enabling a granular assessment of potential clinical impacts. We also present MediHallDetector, a novel Medical LVLM engineered for precise hallucination detection, which employs multitask training for hallucination detection. Through extensive experimental evaluations, we establish baselines for popular LVLMs using our benchmark. The findings indicate that MediHall Score provides a more nuanced understanding of hallucination impacts compared to traditional metrics and demonstrate the enhanced performance of MediHallDetector. We hope this work can significantly improve the reliability of LVLMs in medical applications. All resources of this work will be released soon.**|
|**2024-06-14**|**Practical offloading for fine-tuning LLM on commodity GPU via learned subspace projectors**|[2406.10181](http://arxiv.org/abs/2406.10181)|null|**微调大型语言模型 (LLM) 需要大量内存，通常超过单个 GPU 的容量。解决此内存挑战的常见解决方案是将计算和数据从 GPU 卸载到 CPU。然而，这种方法受到商用硬件带宽有限的阻碍，这限制了 CPU 和 GPU 之间的通信。在本文中，我们提出了一个卸载框架 LSP_Offload，该框架通过学习的子空间投影仪在商用硬件上实现了接近原生速度的 LLM 微调。我们的数据驱动方法涉及学习一种高效的稀疏压缩器，该压缩器可在将精度损失降至最低的情况下最大程度地减少通信量。此外，我们引入了一种新颖的分层通信调度，以最大限度地提高通信和计算之间的并行度。因此，我们的框架可以在配备 4GB 笔记本电脑 GPU 上微调 13 亿参数模型，在配备 24GB 内存的 NVIDIA RTX 4090 GPU 上微调 70 亿参数模型，与内存不受限制的微调相比，速度仅下降 31%。与最先进的卸载框架相比，我们的方法将微调吞吐量提高了 3.33 倍，并在收敛到相同精度时将端到端微调时间缩短了 33.1%~62.5%。 
**|
|**2024-06-14**|**Datasets for Multilingual Answer Sentence Selection**|[2406.10172](http://arxiv.org/abs/2406.10172)|null|**答案句选择 (AS2) 是设计有效的基于检索的问答 (QA) 系统的一项关键任务。由于缺乏其他语言的标注数据集，AS2 的大多数进展都集中在英语上。这种资源的缺乏阻碍了不同语言下有效的 AS2 模型的训练，导致英语和其他地区的 QA 系统之间存在性能差距。在本文中，我们介绍了五种欧洲语言（法语、德语、意大利语、葡萄牙语和西班牙语）的 AS2 全新高质量数据集，这些数据集是使用大型语言模型 (LLM) 通过对现有英语 AS2 数据集（如 ASNQ、WikiQA 和 TREC-QA）进行监督式自动机器翻译 (AMT) 获得的。我们通过使用不同 Transformer 架构的多次实验来评估我们的方法和翻译数据集的质量。结果表明，我们的数据集对于生成强大的多语言 AS2 模型至关重要，极大地促进了缩小英语和其他语言之间的性能差距。 
**|
|**2024-06-14**|**Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models**|[2406.10162](http://arxiv.org/abs/2406.10162)|**[link](https://github.com/anthropics/sycophancy-to-subterfuge-paper)**|**在强化学习中，规范游戏指的是AI系统学习到由于训练目标设定不当而获得高回报的不良行为。规范游戏的形式多种多样，从简单的阿谀奉承到复杂且有害的行为，比如奖励篡改，即模型直接修改自身的奖励机制。然而，这些更恶劣的行为可能过于复杂，无法通过探索发现。在本文中，我们研究了大型语言模型（LLM）助手在发现易于发现的规范游戏形式后，是否会泛化到执行更罕见、更明目张胆的形式，甚至包括奖励篡改。我们构建了一系列逐渐复杂的、可被利用的环境，发现对早期环境的训练会导致在剩余环境中出现更多的规范游戏行为。令人惊讶的是，在一小部分但不可忽视的情况下，在完整环境中训练的LLM助手能够零样本泛化到直接重写自身的奖励函数。重新训练LLM以使其不在早期环境中进行游戏行为可以减轻但在后期环境中不能消除奖励篡改行为。此外，在我们的可利用环境中添加无害性训练并不能阻止奖励篡改。这些结果表明，LLM可以从常见的规范游戏形式泛化到更恶劣的奖励篡改，并且这种行为可能难以消除。 
**|
|**2024-06-14**|**BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack**|[2406.10149](http://arxiv.org/abs/2406.10149)|null|**In recent years, the input context sizes of large language models (LLMs) have increased dramatically. However, existing evaluation methods have not kept pace, failing to comprehensively assess the efficiency of models in handling long contexts. To bridge this gap, we introduce the BABILong benchmark, designed to test language models' ability to reason across facts distributed in extremely long documents. BABILong includes a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. These tasks are challenging on their own, and even more demanding when the required facts are scattered across long natural text. Our evaluations show that popular LLMs effectively utilize only 10-20\% of the context and their performance declines sharply with increased reasoning complexity. Among alternatives to in-context reasoning, Retrieval-Augmented Generation methods achieve a modest 60\% accuracy on single-fact question answering, independent of context length. Among context extension methods, the highest performance is demonstrated by recurrent memory transformers, enabling the processing of lengths up to 11 million tokens. The BABILong benchmark is extendable to any length to support the evaluation of new upcoming models with increased capabilities, and we provide splits up to 1 million token lengths.**|

## Transformer

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**Let the Poem Hit the Rhythm: Using a Byte-Based Transformer for Beat-Aligned Poetry Generation**|[2406.10174](http://arxiv.org/abs/2406.10174)|null|**诗歌和音乐的交集为计算创造力提供了一个有趣的案例，但仍未得到充分探索。本文从节拍模式的角度探讨了诗歌与音乐的融合，研究了基于字节的语言模型是否能够生成符合诗歌语境下特定节拍模式的词语。借鉴早期的研究，我们开发了一种方法来训练基于字节的变换器模型 ByT5，使其能够将诗歌与节拍模式对齐。结果表明，该模型在保持语义连贯性的同时，实现了高度的节拍对齐。未来的工作将致力于提高模型创作完整的节拍对齐诗歌的能力。 
**|
|**2024-06-14**|**Datasets for Multilingual Answer Sentence Selection**|[2406.10172](http://arxiv.org/abs/2406.10172)|null|**Answer Sentence Selection (AS2) is a critical task for designing effective retrieval-based Question Answering (QA) systems. Most advancements in AS2 focus on English due to the scarcity of annotated datasets for other languages. This lack of resources prevents the training of effective AS2 models in different languages, creating a performance gap between QA systems in English and other locales. In this paper, we introduce new high-quality datasets for AS2 in five European languages (French, German, Italian, Portuguese, and Spanish), obtained through supervised Automatic Machine Translation (AMT) of existing English AS2 datasets such as ASNQ, WikiQA, and TREC-QA using a Large Language Model (LLM). We evaluated our approach and the quality of the translated datasets through multiple experiments with different Transformer architectures. The results indicate that our datasets are pivotal in producing robust and powerful multilingual AS2 models, significantly contributing to closing the performance gap between English and other languages.**|
|**2024-06-14**|**Exact Quantum Electrodynamics in Radiative Photonic Environments**|[2406.10164](http://arxiv.org/abs/2406.10164)|null|**我们提出了一种针对辐射光子器件的全面二次量子化方案。我们通过将连续的光子本征模转换为一组离散的赝模来规范地量化它们，这些赝模提供了与电磁环境相互作用的量子发射器的\textit{完整}和\textit{精确}描述。该方法避免了所有储备近似，并为量子关联提供了新的见解，准确地捕捉了所有非马尔可夫动力学。该方法克服了非厄米系统量子化方面的挑战，适用于各种纳米光子几何结构。 
**|
|**2024-06-14**|**MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers**|[2406.10163](http://arxiv.org/abs/2406.10163)|**[link](https://github.com/buaacyw/meshanything)**|**近年来，通过重建和生成创建的3D资产在质量上已经可以媲美人工制作的资产，突显了其替代的潜力。然而，由于这些资产在3D行业应用中总是需要转换为网格，而现有的网格提取方法生成的网格质量远低于艺术家创建的网格（AM，即由艺术家创建的网格），因此这种潜力在很大程度上尚未实现。具体而言，现有的网格提取方法依赖于密集的面片并忽略了几何特征，导致效率低下、后期处理复杂以及表示质量较低。为了解决这些问题，我们引入了MeshAnything，这是一个将网格提取视为生成问题的模型，可以生成与指定形状对齐的AM。通过将任何3D表示形式的3D资产转换为AM，MeshAnything可以与各种3D资产生产方法集成，从而增强其在3D行业的应用。MeshAnything的架构包括VQ-VAE和形状条件解码器Transformer。我们首先使用VQ-VAE学习网格词汇表，然后在这个词汇表上训练形状条件解码器Transformer，用于形状条件自回归网格生成。我们的大量实验表明，我们的方法生成的AM面片数量减少了数百倍，显著提高了存储、渲染和仿真效率，同时实现了与以前方法相当的精度。 
**|
|**2024-06-14**|**BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack**|[2406.10149](http://arxiv.org/abs/2406.10149)|null|**近年来，大型语言模型 (LLM) 的输入上下文大小急剧增加。然而，现有的评估方法并没有跟上步伐，无法全面评估模型处理长上下文的能力。为了弥合这一差距，我们引入了 BABILong 基准测试，旨在测试语言模型跨极长文档中分布的事实进行推理的能力。BABILong 包含 20 种不同的推理任务，包括事实链、简单归纳、演绎、计数以及处理列表/集合。这些任务本身就具有挑战性，当所需事实分散在很长的自然文本中时，这些任务就更加苛刻。我们的评估表明，流行的 LLM 仅有效利用了 10-20% 的上下文，并且它们的性能随着推理复杂性的增加而急剧下降。在上下文推理的替代方案中，检索增强生成方法在单事实问答中实现了 60% 的适度准确率，且与上下文长度无关。在上下文扩展方法中，循环记忆转换器表现出最佳性能，能够处理高达 1100 万个标记的长度。BABILong 基准测试可以扩展到任何长度，以支持评估具有增强功能的新兴模型，并且我们提供高达 100 万个标记长度的拆分。 
**|
|**2024-06-14**|**The Rise and Fall(?) of Software Engineering**|[2406.10141](http://arxiv.org/abs/2406.10141)|null|**Over the last ten years, the realm of Artificial Intelligence (AI) has experienced an explosion of revolutionary breakthroughs, transforming what seemed like a far-off dream into a reality that is now deeply embedded in our everyday lives. AI's widespread impact is revolutionizing virtually all aspects of human life, and software engineering (SE) is no exception.   As we explore this changing landscape, we are faced with questions about what the future holds for SE and how AI will reshape the roles, duties, and methodologies within the field. The introduction of these groundbreaking technologies highlights the inevitable shift towards a new paradigm, suggesting a future where AI's capabilities may redefine the boundaries of SE, potentially even more than human input.   In this paper, we aim at outlining the key elements that, based on our expertise, are vital for the smooth integration of AI into SE, all while preserving the intrinsic human creativity that has been the driving force behind the field. First, we provide a brief description of SE and AI evolution. Afterward, we delve into the intricate interplay between AI-driven automation and human innovation, exploring how these two components can work together to advance SE practices to new methods and standards.**|
|**2024-06-14**|**YOLOv1 to YOLOv10: A comprehensive review of YOLO variants and their application in the agricultural domain**|[2406.10139](http://arxiv.org/abs/2406.10139)|null|**这篇综述调查了从YOLOv1到最先进的YOLOv10的各种YOLO变体在农业发展中的变革潜力。主要目标是阐明这些尖端的物体检测模型如何能够重振和优化农业的各个方面，从作物监测到牲畜管理。它旨在实现关键目标，包括识别农业中的当代挑战，详细评估YOLO的渐进式进步，以及探索其在农业中的具体应用。这是首批将最新的YOLOv10纳入其中的综述之一，为人工智能和自动化时代精准农业和可持续农业实践的影响提供了新的视角。此外，该综述对YOLO的性能进行了批判性分析，综合了现有研究，并预测了未来趋势。通过仔细研究YOLO变体中包含的独特功能及其现实应用，本综述为YOLO变体与农业之间不断发展的关系提供了宝贵的见解。这些发现有助于深入理解精准农业和可持续农业实践的潜力，标志着先进物体检测技术在农业领域整合方面迈出了重要一步。 
**|
|**2024-06-14**|**Precipitation Nowcasting Using Physics Informed Discriminator Generative Models**|[2406.10108](http://arxiv.org/abs/2406.10108)|null|**Nowcasting leverages real-time atmospheric conditions to forecast weather over short periods. State-of-the-art models, including PySTEPS, encounter difficulties in accurately forecasting extreme weather events because of their unpredictable distribution patterns. In this study, we design a physics-informed neural network to perform precipitation nowcasting using the precipitation and meteorological data from the Royal Netherlands Meteorological Institute (KNMI). This model draws inspiration from the novel Physics-Informed Discriminator GAN (PID-GAN) formulation, directly integrating physics-based supervision within the adversarial learning framework. The proposed model adopts a GAN structure, featuring a Vector Quantization Generative Adversarial Network (VQ-GAN) and a Transformer as the generator, with a temporal discriminator serving as the discriminator. Our findings demonstrate that the PID-GAN model outperforms numerical and SOTA deep generative models in terms of precipitation nowcasting downstream metrics.**|
|**2024-06-14**|**ECGMamba: Towards Efficient ECG Classification with BiSSM**|[2406.10098](http://arxiv.org/abs/2406.10098)|null|**心电图 (ECG) 信号分析是诊断心血管疾病的关键技术。尽管基于 Transformer 的模型在 ECG 分类方面取得了重大进展，但它们在推理阶段表现出效率低下。这个问题主要归因于 Transformer 自注意力机制的二次计算复杂度，尤其是在处理长序列时。为了解决这个问题，我们提出了一种名为 ECGMamba 的新型模型，它采用双向状态空间模型 (BiSSM) 来提高分类效率。ECGMamba 基于创新的 Mamba 块，该块结合了一系列时间序列建模技术，在保持推理效率的同时提高了性能。在两个公开可用的 ECG 数据集上的实验结果表明，ECGMamba 有效地平衡了分类的有效性和效率，实现了具有竞争力的性能。这项研究不仅为 ECG 分类领域做出了贡献，而且为高效、准确的 ECG 信号分析提供了新的研究路径。这对心血管疾病诊断模型的开发具有指导意义。 
**|
|**2024-06-14**|**Whisper-Flamingo: Integrating Visual Features into Whisper for Audio-Visual Speech Recognition and Translation**|[2406.10082](http://arxiv.org/abs/2406.10082)|**[link](https://github.com/roudimit/whisper-flamingo)**|**音频-视觉语音识别 (AVSR) 利用基于嘴唇的视频来提高在噪声环境下的性能。由于视频比音频更难获取，AVSR 模型的视频训练数据通常只有几千小时。相比之下，Whisper 等语音模型使用数十万小时的数据进行训练，因此可以学习到更好的语音到文本解码器。巨大的训练数据差异促使我们调整 Whisper 以处理视频输入。受 Flamingo 将视觉特征注入语言模型的启发，我们提出了 Whisper-Flamingo，它通过门控交叉注意力将视觉特征集成到 Whisper 语音识别和翻译模型中。我们的视听 Whisper-Flamingo 在嘈杂条件下，在英语语音识别和 6 种语言的英文-X 翻译方面优于仅音频的 Whisper。此外，Whisper-Flamingo 是一种多功能模型，可以使用一组参数执行所有这些任务，而先前的方法需要针对每种语言单独训练。 
**|

## 3DGS

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting**|[2406.10219](http://arxiv.org/abs/2406.10219)|null|**近年来，新视角合成技术的进步实现了实时渲染速度和高重建精度。三维高斯 splatting (3D-GS) 是一种基础的基于点的三维场景参数化表示方法，它将场景建模为大量三维高斯函数的集合。复杂的场景可能包含数百万个高斯函数，导致巨大的存储和内存需求，限制了 3D-GS 在资源有限的设备上的可行性。目前，通过修剪高斯函数来压缩这些预训练模型的技术依赖于结合启发式方法来确定要删除哪些高斯函数。在本文中，我们提出了一种基于原则的空间敏感性修剪评分方法，该方法优于这些方法。它被计算为训练视图上的重建误差相对于每个高斯函数的空间参数的二阶近似。此外，我们提出了一个多轮修剪-精炼流程，可以应用于任何预训练的 3D-GS 模型，而无需改变训练流程。在修剪了 88.44% 的高斯函数后，我们观察到，与之前在 Mip-NeRF 360、Tanks & Temples 和 Deep Blending 数据集的场景中应用的修剪技术相比，我们的 PUP 3D-GS 流程将 3D-GS 的平均渲染速度提高了 2.65 倍，同时保留了更多显著的前景信息，并实现了更高的图像质量指标。**|
|**2024-06-14**|**GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors**|[2406.10111](http://arxiv.org/abs/2406.10111)|null|**Achieving high-resolution novel view synthesis (HRNVS) from low-resolution input views is a challenging task due to the lack of high-resolution data. Previous methods optimize high-resolution Neural Radiance Field (NeRF) from low-resolution input views but suffer from slow rendering speed. In this work, we base our method on 3D Gaussian Splatting (3DGS) due to its capability of producing high-quality images at a faster rendering speed. To alleviate the shortage of data for higher-resolution synthesis, we propose to leverage off-the-shelf 2D diffusion priors by distilling the 2D knowledge into 3D with Score Distillation Sampling (SDS). Nevertheless, applying SDS directly to Gaussian-based 3D super-resolution leads to undesirable and redundant 3D Gaussian primitives, due to the randomness brought by generative priors. To mitigate this issue, we introduce two simple yet effective techniques to reduce stochastic disturbances introduced by SDS. Specifically, we 1) shrink the range of diffusion timestep in SDS with an annealing strategy; 2) randomly discard redundant Gaussian primitives during densification. Extensive experiments have demonstrated that our proposed GaussainSR can attain high-quality results for HRNVS with only low-resolution inputs on both synthetic and real-world datasets. Project page: https://chchnii.github.io/GaussianSR/**|
|**2024-06-14**|**GradeADreamer: Enhanced Text-to-3D Generation Using Gaussian Splatting and Multi-View Diffusion**|[2406.09850](http://arxiv.org/abs/2406.09850)|**[link](https://github.com/trapoom555/gradeadreamer)**|**文本到3D生成已经展现出可观的结果，但仍然面临着常见挑战，例如多面 Janus 问题以及高质量资产生成时间过长的问题。在本文中，我们通过引入一种名为 GradeADreamer 的新型三阶段训练流程来解决这些问题。该流程能够仅使用单个 RTX 3090 GPU 在 30 分钟内生成高质量的资产。我们提出的方法采用多视图扩散模型 MVDream 生成高斯球体作为先验，然后使用 StableDiffusion 优化几何形状和纹理。实验结果表明，与之前最先进的方法相比，我们的方法显著减轻了多面 Janus 问题，并实现了最高的平均用户偏好排名。项目代码可在 https://github.com/trapoom555/GradeADreamer 获取。 
**|
|**2024-06-14**|**Unified Gaussian Primitives for Scene Representation and Rendering**|[2406.09733](http://arxiv.org/abs/2406.09733)|null|**Searching for a unified scene representation remains a research challenge in computer graphics. Traditional mesh-based representations are unsuitable for dense, fuzzy elements, and introduce additional complexity for filtering and differentiable rendering. Conversely, voxel-based representations struggle to model hard surfaces and suffer from intensive memory requirement. We propose a general-purpose rendering primitive based on 3D Gaussian distribution for unified scene representation, featuring versatile appearance ranging from glossy surfaces to fuzzy elements, as well as physically based scattering to enable accurate global illumination. We formulate the rendering theory for the primitive based on non-exponential transport and derive efficient rendering operations to be compatible with Monte Carlo path tracing. The new representation can be converted from different sources, including meshes and 3D Gaussian splatting, and further refined via transmittance optimization thanks to its differentiability. We demonstrate the versatility of our representation in various rendering applications such as global illumination and appearance editing, while supporting arbitrary lighting conditions by nature. Additionally, we compare our representation to existing volumetric representations, highlighting its efficiency to reproduce details.**|
|**2024-06-13**|**Modeling Ambient Scene Dynamics for Free-view Synthesis**|[2406.09395](http://arxiv.org/abs/2406.09395)|null|**我们提出了一种新颖的动态自由视点合成方法，可以从单目捕捉中合成环境场景，为观看体验带来沉浸式的质量。我们的方法建立在3D高斯渲染（3DGS）的最新进展之上，该技术可以忠实地重建复杂的静态场景。先前将3DGS扩展到表示动态的尝试仅限于有界场景或需要多相机捕捉，并且通常无法泛化到未见过的运动，从而限制了它们的实际应用。我们的方法通过利用环境运动的周期性来学习运动轨迹模型，并结合仔细的正则化，克服了这些限制。我们还提出了一些重要的实用策略，以提高基线3DGS静态重建的视觉质量，并提高对GPU内存密集型学习至关重要的内存效率。我们展示了几个具有复杂纹理和精细结构元素的环境自然场景的高质量逼真新颖视图合成。 
**|
|**2024-06-13**|**GGHead: Fast and Generalizable 3D Gaussian Heads**|[2406.09377](http://arxiv.org/abs/2406.09377)|null|**从大型二维图像集中学习三维头部先验信息是实现高质量三维感知人体建模的重要步骤。其核心要求是构建一个能够很好地扩展到大型数据集和高分辨率图像的有效架构。遗憾的是，现有的三维生成对抗网络（GAN）由于其训练和渲染速度相对较慢，难以扩展到生成高分辨率样本，并且通常不得不依赖二维超分辨率网络，但这会牺牲全局三维一致性。为了应对这些挑战，我们提出了生成式高斯头部（GGHead）模型，该模型在三维生成对抗网络框架内采用了最新的三维高斯散射表示。为了生成三维表示，我们采用强大的二维卷积神经网络生成器来预测模板头部网格的UV空间中的高斯属性。通过这种方式，GGHead利用了模板UV布局的规律性，极大地促进了预测非结构化三维高斯集这一具有挑战性的任务。我们进一步通过对渲染后的UV坐标采用新的总变异损失来提高生成的三维表示的几何保真度。直观地说，这种正则化鼓励相邻渲染像素应该源于模板UV空间中的相邻高斯分布。总而言之，我们提出的管道可以有效地生成仅从单视图二维图像观察中训练得到的三维头部。我们提出的框架在FFHQ数据集上与现有的三维头部生成对抗网络质量相当，同时速度明显更快且完全保持三维一致性。因此，我们首次展示了以1024x1024分辨率实时生成和渲染高质量、三维一致的头部。 
**|
|**2024-06-14**|**AV-GS: Learning Material and Geometry Aware Priors for Novel View Acoustic Synthesis**|[2406.08920](http://arxiv.org/abs/2406.08920)|null|**新视角声学合成 (NVAS) 旨在在给定三维场景中声源发出的单声道音频的情况下，渲染任意目标视点的双耳音频。现有方法提出了基于 NeRF 的隐式模型，以利用视觉线索作为合成双耳音频的条件。然而，除了 NeRF 渲染效率低下之外，这些方法在表征整个场景环境（如房间几何形状、材料属性以及听者和声源之间的空间关系）方面的能力也有限。为了解决这些问题，我们提出了一种新颖的视听高斯 splatting (AV-GS) 模型。为了获得用于音频合成的材料感知和几何感知条件，我们学习了一种基于点的显式场景表示，并在局部初始化的高斯点上使用音频引导参数，同时考虑了听者和声源的空间关系。为了使视觉场景模型适应音频，我们提出了一种点增密和修剪策略，以优化高斯点的分布，并考虑每个点在声音传播中的贡献（例如，对于无纹理的墙壁表面需要更多点，因为它们会影响声路转向）。大量实验验证了我们的 AV-GS 在真实世界 RWAS 和基于模拟的 SoundSpaces 数据集上优于现有替代方案。**|
|**2024-06-13**|**Gaussian-Forest: Hierarchical-Hybrid 3D Gaussian Splatting for Compressed Scene Modeling**|[2406.08759](http://arxiv.org/abs/2406.08759)|null|**The field of novel-view synthesis has recently witnessed the emergence of 3D Gaussian Splatting, which represents scenes in a point-based manner and renders through rasterization. This methodology, in contrast to Radiance Fields that rely on ray tracing, demonstrates superior rendering quality and speed. However, the explicit and unstructured nature of 3D Gaussians poses a significant storage challenge, impeding its broader application. To address this challenge, we introduce the Gaussian-Forest modeling framework, which hierarchically represents a scene as a forest of hybrid 3D Gaussians. Each hybrid Gaussian retains its unique explicit attributes while sharing implicit ones with its sibling Gaussians, thus optimizing parameterization with significantly fewer variables. Moreover, adaptive growth and pruning strategies are designed, ensuring detailed representation in complex regions and a notable reduction in the number of required Gaussians. Extensive experiments demonstrate that Gaussian-Forest not only maintains comparable speed and quality but also achieves a compression rate surpassing 10 times, marking a significant advancement in efficient scene modeling. Codes are available at https://github.com/Xian-Bei/GaussianForest.**|
|**2024-06-12**|**ICE-G: Image Conditional Editing of 3D Gaussian Splats**|[2406.08488](http://arxiv.org/abs/2406.08488)|null|**近年来，出现了许多创建高质量3D资产和场景的技术。然而，在编辑这些对象时，现有方法要么速度慢，要么质量 compromised，要么无法提供足够的定制化。我们引入了一种新方法，可以从单个参考视图快速编辑3D模型。我们的技术首先分割编辑图像，然后使用 DINO 特征在选定的分割数据集视图中匹配语义上对应的区域。然后，可以以语义合理的方式将编辑图像特定区域的颜色或纹理更改自动应用于其他视图。这些编辑后的视图充当更新后的数据集，以进一步训练和重新样式化3D场景。最终结果是一个经过编辑的3D模型。我们的框架支持各种编辑任务，例如手动局部编辑、基于对应的任意示例图像的风格迁移，以及来自多个示例图像的不同风格的组合。我们使用高斯斑点作为我们的主要3D表示，因为它们的速度快且易于局部编辑，但我们的技术也适用于其他方法，如NeRFs。我们通过多个示例表明，我们的方法可以生成更高质量的结果，同时提供对编辑的细粒度控制。项目页面：ice-gaussian.github.io 
**|
|**2024-06-12**|**Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models**|[2406.08475](http://arxiv.org/abs/2406.08475)|null|**Creating realistic avatars from a single RGB image is an attractive yet challenging problem. Due to its ill-posed nature, recent works leverage powerful prior from 2D diffusion models pretrained on large datasets. Although 2D diffusion models demonstrate strong generalization capability, they cannot provide multi-view shape priors with guaranteed 3D consistency. We propose Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion. Our key insight is that 2D multi-view diffusion and 3D reconstruction models provide complementary information for each other, and by coupling them in a tight manner, we can fully leverage the potential of both models. We introduce a novel image-conditioned generative 3D Gaussian Splats reconstruction model that leverages the priors from 2D multi-view diffusion models, and provides an explicit 3D representation, which further guides the 2D reverse sampling process to have better 3D consistency. Experiments show that our proposed framework outperforms state-of-the-art methods and enables the creation of realistic avatars from a single RGB image, achieving high-fidelity in both geometry and appearance. Extensive ablations also validate the efficacy of our design, (1) multi-view 2D priors conditioning in generative 3D reconstruction and (2) consistency refinement of sampling trajectory via the explicit 3D representation. Our code and models will be released on https://yuxuan-xue.com/human-3diffusion.**|

## 3D/CG

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**A Two-Stage Masked Autoencoder Based Network for Indoor Depth Completion**|[2406.09792](http://arxiv.org/abs/2406.09792)|**[link](https://github.com/kailaisun/indoor-depth-completion)**|**深度图像在三维重建、自动驾驶、增强现实、机器人导航和场景理解等领域有着广泛的应用。商用级深度相机难以感知明亮、光滑、透明和远处表面的深度。尽管现有的深度补全方法已经取得了显著的进展，但当应用于复杂的室内场景时，它们的性能仍然有限。为了解决这些问题，我们提出了一种基于Transformer的两阶段网络用于室内深度补全。与现有的深度补全方法不同，我们采用基于掩码自编码器的自监督预训练编码器来学习缺失深度值的有效潜在表示；然后我们提出了一个基于token融合机制的解码器，从联合RGB和不完整的深度图像中完成（即重建）完整的深度。与现有方法相比，我们提出的网络在Matterport3D数据集上实现了最先进的性能。此外，为了验证深度补全任务的重要性，我们将我们的方法应用于室内三维重建。代码、数据集和演示可在https://github.com/kailaisun/Indoor-Depth-Completion获取。 
**|
|**2024-06-14**|**Grounding Image Matching in 3D with MASt3R**|[2406.09756](http://arxiv.org/abs/2406.09756)|null|**图像匹配是所有高性能三维视觉算法和流程中的核心组成部分。然而，尽管匹配本质上是一个与相机位姿和场景几何密切相关的3D问题，但它通常被视为一个2D问题。这看似合理，因为匹配的目标是在2D像素域之间建立对应关系，但也可能是一个冒险的选择。在这项工作中，我们采取了不同的立场，并建议将匹配视为一项3D任务，使用DUSt3R，一个最近出现的强大的基于Transformer的3D重建框架。这种基于点图回归的方法在匹配具有极端视角变化的视图时表现出令人印象深刻的鲁棒性，但精度有限。我们的目标是在保持其鲁棒性的同时提高这种方法的匹配能力。因此，我们建议使用一个新的头部来增强DUSt3R网络，该头部输出密集的局部特征，并使用额外的匹配损失进行训练。我们进一步解决了密集匹配的二次复杂度问题，如果处理不当，这将导致下游应用程序的速度非常慢。我们引入了一种快速互惠匹配方案，该方案不仅将匹配速度提高了几个数量级，而且还具有理论上的保证，最后还产生了改进的结果。大量实验表明，我们提出的MASt3R方法在多个匹配任务上明显优于现有技术。特别是，在极具挑战性的无地图定位数据集上，它的VCRE AUC比已发表的最佳方法提高了30%（绝对改进）。 
**|
|**2024-06-13**|**ImageNet3D: Towards General-Purpose Object-Level 3D Understanding**|[2406.09613](http://arxiv.org/abs/2406.09613)|**[link](https://github.com/wufeim/imagenet3d)**|**一个具有通用目标级 3D 理解能力的视觉模型应该能够推断自然图像中任意刚性物体的 2D（例如，类别名称和边界框）和 3D 信息（例如，3D 位置和 3D 视点）。这是一项具有挑战性的任务，因为它涉及从 2D 信号推断 3D 信息，最重要的是，要泛化到未知类别的刚性物体。然而，现有的具有目标级 3D 标注的数据集通常受到类别数量或标注质量的限制。在这些数据集上开发的模型成为某些类别或领域的专家，并且无法泛化。在这项工作中，我们提出了 ImageNet3D，这是一个用于通用目标级 3D 理解的大型数据集。ImageNet3D 使用 2D 边界框、3D 姿态、3D 位置标注以及穿插 3D 信息的图像描述，增强了 ImageNet 数据集中的 200 个类别。借助 ImageNet3D 中可用的新标注，我们可以 (i) 分析视觉基础模型的目标级 3D 感知能力，(ii) 研究和开发能够推断自然图像中任意刚性物体的 2D 和 3D 信息的通用模型，以及 (iii) 将统一的 3D 模型与大型语言模型集成，以进行与 3D 相关的推理。除了标准分类和姿态估计之外，我们还考虑了两个新任务，即目标级 3D 感知能力的探测和开放词汇姿态估计。ImageNet3D 上的实验结果证明了我们的数据集在构建具有更强通用目标级 3D 理解能力的视觉模型方面的潜力。 
**|
|**2024-06-13**|**Multiagent Multitraversal Multimodal Self-Driving: Open MARS Dataset**|[2406.09383](http://arxiv.org/abs/2406.09383)|null|**大规模数据集推动了基于人工智能的自动驾驶汽车研究的最新进展。然而，这些数据集通常是从一辆汽车对某个地点的一次性通行中收集的，缺乏多智能体交互或对同一地点的重复遍历。这些信息可以导致自动驾驶汽车在感知、预测和规划能力方面的革命性改进。为了弥合这一差距，我们与自动驾驶公司May Mobility合作，推出了MARS数据集，该数据集统一了支持多智能体、多路线和多模态自动驾驶汽车研究的场景。更具体地说，MARS是通过在特定地理区域内行驶的自动驾驶车队收集的。每辆车都有自己的路线，不同的车辆可能会出现在附近的位置。每辆车都配备了激光雷达和环视RGB摄像头。我们在MARS中策划了两个子集：一个子集通过同时出现在同一地点的多辆汽车促进协同驾驶，另一个子集通过多辆汽车对同一地点的异步遍历实现记忆回溯。我们在地点识别和神经重建方面进行了实验。更重要的是，MARS引入了新的研究机会和挑战，例如多路线三维重建、多智能体感知和无监督目标发现。我们的数据和代码可以在https://ai4ce.github.io/MARS/找到。 
**|
|**2024-06-13**|**LRM-Zero: Training Large Reconstruction Models with Synthesized Data**|[2406.09371](http://arxiv.org/abs/2406.09371)|null|**We present LRM-Zero, a Large Reconstruction Model (LRM) trained entirely on synthesized 3D data, achieving high-quality sparse-view 3D reconstruction. The core of LRM-Zero is our procedural 3D dataset, Zeroverse, which is automatically synthesized from simple primitive shapes with random texturing and augmentations (e.g., height fields, boolean differences, and wireframes). Unlike previous 3D datasets (e.g., Objaverse) which are often captured or crafted by humans to approximate real 3D data, Zeroverse completely ignores realistic global semantics but is rich in complex geometric and texture details that are locally similar to or even more intricate than real objects. We demonstrate that our LRM-Zero, trained with our fully synthesized Zeroverse, can achieve high visual quality in the reconstruction of real-world objects, competitive with models trained on Objaverse. We also analyze several critical design choices of Zeroverse that contribute to LRM-Zero's capability and training stability. Our work demonstrates that 3D reconstruction, one of the core tasks in 3D vision, can potentially be addressed without the semantics of real-world objects. The Zeroverse's procedural synthesis code and interactive visualization are available at: https://desaixie.github.io/lrm-zero/.**|
|**2024-06-13**|**OpenMaterial: A Comprehensive Dataset of Complex Materials for 3D Reconstruction**|[2406.08894](http://arxiv.org/abs/2406.08894)|null|**Recent advances in deep learning such as neural radiance fields and implicit neural representations have significantly propelled the field of 3D reconstruction. However, accurately reconstructing objects with complex optical properties, such as metals and glass, remains a formidable challenge due to their unique specular and light-transmission characteristics. To facilitate the development of solutions to these challenges, we introduce the OpenMaterial dataset, comprising 1001 objects made of 295 distinct materials-including conductors, dielectrics, plastics, and their roughened variants- and captured under 723 diverse lighting conditions. To this end, we utilized physics-based rendering with laboratory-measured Indices of Refraction (IOR) and generated high-fidelity multiview images that closely replicate real-world objects. OpenMaterial provides comprehensive annotations, including 3D shape, material type, camera pose, depth, and object mask. It stands as the first large-scale dataset enabling quantitative evaluations of existing algorithms on objects with diverse and challenging materials, thereby paving the way for the development of 3D reconstruction algorithms capable of handling complex material properties.**|
|**2024-06-12**|**Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models**|[2406.08475](http://arxiv.org/abs/2406.08475)|null|**从单张RGB图像创建逼真的人物化身是一个具有吸引力但极具挑战性的问题。由于其不适定性，近期工作利用在大规模数据集上预训练的2D扩散模型的强大先验。尽管2D扩散模型展现出强大的泛化能力，但它们无法提供具有保证3D一致性的多视图形状先验。我们提出了Human 3Diffusion：通过显式3D一致性扩散创建逼真人像。我们的主要见解是，2D多视图扩散和3D重建模型可以为彼此提供补充信息，通过将它们紧密耦合，我们可以充分利用两种模型的潜力。我们引入了一种新颖的图像条件生成式3D高斯散点重建模型，该模型利用了来自2D多视图扩散模型的先验，并提供了显式的3D表示，进一步引导2D逆向采样过程以获得更好的3D一致性。实验表明，我们提出的框架优于现有技术，能够从单张RGB图像创建逼真的人物化身，在几何形状和外观上均实现了高保真度。大量的消融实验也验证了我们设计的有效性，(1) 生成式3D重建中的多视图2D先验条件和(2) 通过显式3D表示对采样轨迹进行一致性细化。我们的代码和模型将在https://yuxuan-xue.com/human-3diffusion上发布。 
**|
|**2024-06-12**|**Category-level Neural Field for Reconstruction of Partially Observed Objects in Indoor Environment**|[2406.08176](http://arxiv.org/abs/2406.08176)|null|**神经隐式表示法已通过各种成功案例在 3D 重建领域引起了关注。为了实现场景理解或编辑等进一步应用，一些研究已在目标组合重建方面取得进展。尽管它们在已观察区域表现出色，但在重建部分观察到的物体方面，其性能仍然有限。为了更好地解决这个问题，我们引入了类别级神经场，它可以学习场景中属于同一类别的物体之间有意义的常见 3D 信息。我们的关键思想是根据观察到的形状对物体进行子分类，以便更好地训练类别级模型。然后，我们利用神经场通过基于射线的不确定性选择和对齐代表性物体，来执行具有挑战性的部分观察物体配准任务。在仿真和真实世界数据集上的实验表明，我们的方法改进了对多个类别中未观察部分的重建。 
**|
|**2024-06-11**|**M-LRM: Multi-view Large Reconstruction Model**|[2406.07648](http://arxiv.org/abs/2406.07648)|null|**尽管大型重建模型 (LRM) 近期取得了令人瞩目的成果，但当将其输入从单张图像扩展到多张图像时，它表现出效率低下、几何和纹理质量欠佳，以及收敛速度比预期慢等问题。这归因于LRM将三维重建视为一个简单的图像到三维的转换问题，而忽略了输入图像之间强烈的三维一致性。在本文中，我们提出了一种多视图大型重建模型 (M-LRM)，旨在以三维感知的方式从多视图图像中高效地重建高质量的三维形状。具体来说，我们引入了一种多视图一致性交叉注意机制，使M-LRM能够准确地从输入图像中查询信息。此外，我们利用输入多视图图像的三维先验信息来初始化三平面token。与LRM相比，我们提出的M-LRM可以生成分辨率为128×128的三平面NeRF，并生成高保真度的三维形状。实验研究表明，与LRM相比，我们的模型实现了显著的性能提升和更快的训练收敛速度。项目页面：https://murphylmf.github.io/M-LRM/ 
**|
|**2024-06-11**|**NeRSP: Neural 3D Reconstruction for Reflective Objects with Sparse Polarized Images**|[2406.07111](http://arxiv.org/abs/2406.07111)|null|**We present NeRSP, a Neural 3D reconstruction technique for Reflective surfaces with Sparse Polarized images. Reflective surface reconstruction is extremely challenging as specular reflections are view-dependent and thus violate the multiview consistency for multiview stereo. On the other hand, sparse image inputs, as a practical capture setting, commonly cause incomplete or distorted results due to the lack of correspondence matching. This paper jointly handles the challenges from sparse inputs and reflective surfaces by leveraging polarized images. We derive photometric and geometric cues from the polarimetric image formation model and multiview azimuth consistency, which jointly optimize the surface geometry modeled via implicit neural representation. Based on the experiments on our synthetic and real datasets, we achieve the state-of-the-art surface reconstruction results with only 6 views as input.**|

## 各类学习方式

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**Shelf-Supervised Multi-Modal Pre-Training for 3D Object Detection**|[2406.10115](http://arxiv.org/abs/2406.10115)|null|**State-of-the-art 3D object detectors are often trained on massive labeled datasets. However, annotating 3D bounding boxes remains prohibitively expensive and time-consuming, particularly for LiDAR. Instead, recent works demonstrate that self-supervised pre-training with unlabeled data can improve detection accuracy with limited labels. Contemporary methods adapt best-practices for self-supervised learning from the image domain to point clouds (such as contrastive learning). However, publicly available 3D datasets are considerably smaller and less diverse than those used for image-based self-supervised learning, limiting their effectiveness. We do note, however, that such data is naturally collected in a multimodal fashion, often paired with images. Rather than pre-training with only self-supervised objectives, we argue that it is better to bootstrap point cloud representations using image-based foundation models trained on internet-scale image data. Specifically, we propose a shelf-supervised approach (e.g. supervised with off-the-shelf image foundation models) for generating zero-shot 3D bounding boxes from paired RGB and LiDAR data. Pre-training 3D detectors with such pseudo-labels yields significantly better semi-supervised detection accuracy than prior self-supervised pretext tasks. Importantly, we show that image-based shelf-supervision is helpful for training LiDAR-only and multi-modal (RGB + LiDAR) detectors. We demonstrate the effectiveness of our approach on nuScenes and WOD, significantly improving over prior work in limited data settings.**|
|**2024-06-14**|**InstructRL4Pix: Training Diffusion for Image Editing by Reinforcement Learning**|[2406.09973](http://arxiv.org/abs/2406.09973)|null|**Instruction-based image editing has made a great process in using natural human language to manipulate the visual content of images. However, existing models are limited by the quality of the dataset and cannot accurately localize editing regions in images with complex object relationships. In this paper, we propose Reinforcement Learning Guided Image Editing Method(InstructRL4Pix) to train a diffusion model to generate images that are guided by the attention maps of the target object. Our method maximizes the output of the reward model by calculating the distance between attention maps as a reward function and fine-tuning the diffusion model using proximal policy optimization (PPO). We evaluate our model in object insertion, removal, replacement, and transformation. Experimental results show that InstructRL4Pix breaks through the limitations of traditional datasets and uses unsupervised learning to optimize editing goals and achieve accurate image editing based on natural human commands.**|
|**2024-06-14**|**Neural Concept Binder**|[2406.09949](http://arxiv.org/abs/2406.09949)|**[link](https://github.com/ml-research/neuralconceptbinder)**|**基于对象的视觉推理的挑战在于生成具有描述性且独特的概念表示。此外，以无监督的方式进行此操作需要人类用户理解模型学习到的概念并可能修改错误的概念。为了应对这一挑战，我们引入了神经概念绑定器，这是一个用于导出离散概念表示的新框架，我们将其称为“概念槽编码”。这些编码利用了“软绑定”（通过以对象为中心的块槽编码）和“硬绑定”（通过基于检索的推理）。神经概念绑定器有助于直接的概念检查和外部知识的直接整合，例如人类输入或来自其他 AI 模型（如 GPT-4）的见解。此外，我们证明了结合硬绑定机制不会影响性能；相反，它可以无缝集成到神经和符号模块中，以完成复杂的推理任务，这一点在我们新推出的 CLEVR-Sudoku 数据集的评估中得到了证明。 
**|
|**2024-06-14**|**Forgetting Order of Continual Learning: Examples That are Learned First are Forgotten Last**|[2406.09935](http://arxiv.org/abs/2406.09935)|null|**灾难性遗忘是持续学习中的一个重大挑战，模型在学习新数据时经常忘记之前的任务。我们的实证分析揭示了灾难性遗忘与样本学习速度之间的强相关性：早期学习的样本很少被遗忘，而后期学习的样本更容易被遗忘。我们证明，基于回放的持续学习方法可以利用这种现象，通过关注中等学习速度的样本进行复习。我们介绍了一种名为“金发姑娘”（Goldilocks）的新型回放缓冲区采样方法，该方法过滤掉学习太快或太慢的样本，保留那些以中等速度学习的样本。金发姑娘改进了现有的持续学习算法，在多个图像分类任务中取得了最先进的性能。 
**|
|**2024-06-14**|**Exploring the Benefits of Vision Foundation Models for Unsupervised Domain Adaptation**|[2406.09896](http://arxiv.org/abs/2406.09896)|**[link](https://github.com/tue-mps/vfm-uda)**|**Achieving robust generalization across diverse data domains remains a significant challenge in computer vision. This challenge is important in safety-critical applications, where deep-neural-network-based systems must perform reliably under various environmental conditions not seen during training. Our study investigates whether the generalization capabilities of Vision Foundation Models (VFMs) and Unsupervised Domain Adaptation (UDA) methods for the semantic segmentation task are complementary. Results show that combining VFMs with UDA has two main benefits: (a) it allows for better UDA performance while maintaining the out-of-distribution performance of VFMs, and (b) it makes certain time-consuming UDA components redundant, thus enabling significant inference speedups. Specifically, with equivalent model sizes, the resulting VFM-UDA method achieves an 8.4 $\times$ speed increase over the prior non-VFM state of the art, while also improving performance by +1.2 mIoU in the UDA setting and by +6.1 mIoU in terms of out-of-distribution generalization. Moreover, when we use a VFM with 3.6$\times$ more parameters, the VFM-UDA approach maintains a 3.3$\times$ speed up, while improving the UDA performance by +3.1 mIoU and the out-of-distribution performance by +10.3 mIoU. These results underscore the significant benefits of combining VFMs with UDA, setting new standards and baselines for Unsupervised Domain Adaptation in semantic segmentation.**|
|**2024-06-14**|**Federated Learning driven Large Language Models for Swarm Intelligence: A Survey**|[2406.09831](http://arxiv.org/abs/2406.09831)|null|**联邦学习 (FL) 为训练大型语言模型 (LLM) 提供了一个引人注目的框架，同时解决了数据隐私和去中心化挑战。 本文综述了大型语言模型联邦学习的最新进展，特别关注机器遗忘，这是遵守“被遗忘权”等隐私法规的关键方面。 在联邦 LLM 的背景下，机器遗忘涉及从学习模型中系统、安全地删除个人数据贡献，而无需从头开始重新训练。我们探索了各种能够实现有效遗忘的策略，例如扰动技术、模型分解和增量学习，并重点介绍了它们对维护模型性能和数据隐私的影响。此外，我们还考察了近期文献中的案例研究和实验结果，以评估这些方法在现实场景中的有效性和效率。我们的调查显示，人们对开发更健壮、更具可扩展性的联邦遗忘方法的兴趣日益浓厚，这表明人工智能伦理与分布式机器学习技术交叉领域是未来研究的重要方向。 
**|
|**2024-06-14**|**GeoSEE: Regional Socio-Economic Estimation With a Large Language Model**|[2406.09799](http://arxiv.org/abs/2406.09799)|null|**Moving beyond traditional surveys, combining heterogeneous data sources with AI-driven inference models brings new opportunities to measure socio-economic conditions, such as poverty and population, over expansive geographic areas. The current research presents GeoSEE, a method that can estimate various socio-economic indicators using a unified pipeline powered by a large language model (LLM). Presented with a diverse set of information modules, including those pre-constructed from satellite imagery, GeoSEE selects which modules to use in estimation, for each indicator and country. This selection is guided by the LLM's prior socio-geographic knowledge, which functions similarly to the insights of a domain expert. The system then computes target indicators via in-context learning after aggregating results from selected modules in the format of natural language-based texts. Comprehensive evaluation across countries at various stages of development reveals that our method outperforms other predictive models in both unsupervised and low-shot contexts. This reliable performance under data-scarce setting in under-developed or developing countries, combined with its cost-effectiveness, underscores its potential to continuously support and monitor the progress of Sustainable Development Goals, such as poverty alleviation and equitable growth, on a global scale.**|
|**2024-06-14**|**Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity**|[2406.09790](http://arxiv.org/abs/2406.09790)|null|**Semantic Textual Similarity (STS) constitutes a critical research direction in computational linguistics and serves as a key indicator of the encoding capabilities of embedding models. Driven by advances in pre-trained language models and contrastive learning techniques, leading sentence representation methods can already achieved average Spearman's correlation scores of approximately 86 across seven STS benchmarks in SentEval. However, further improvements have become increasingly marginal, with no existing method attaining an average score higher than 87 on these tasks. This paper conducts an in-depth analysis of this phenomenon and concludes that the upper limit for Spearman's correlation scores using contrastive learning is 87.5. To transcend this ceiling, we propose an innovative approach termed Pcc-tuning, which employs Pearson's correlation coefficient as a loss function to refine model performance beyond contrastive learning. Experimental results demonstrate that Pcc-tuning markedly surpasses previous state-of-the-art strategies, raising the Spearman's correlation score to above 90.**|
|**2024-06-14**|**Unsupervised Monocular Depth Estimation Based on Hierarchical Feature-Guided Diffusion**|[2406.09782](http://arxiv.org/abs/2406.09782)|null|**无监督单目深度估计由于其无需真值即可训练的能力而受到广泛关注。在现实场景中，图像可能会由于天气条件和相机固有局限性的影响而变得模糊或噪声。因此，开发稳健的深度估计模型尤为重要。受益于生成网络的训练策略，基于生成的方法通常表现出增强的鲁棒性。鉴于此，我们采用生成网络中一种收敛性良好的扩散模型进行无监督单目深度估计。此外，我们提出了一种分层特征引导的去噪模块。该模块通过充分利用图像特征来指导去噪过程，显著增强了模型学习和解释深度分布的能力。此外，我们还探索了重投影中的隐式深度，并设计了一种隐式深度一致性损失。该损失函数用于增强模型的性能并确保视频序列中深度的尺度一致性。我们在 KITTI、Make3D 和我们自己收集的 SIMIT 数据集上进行了实验。结果表明，我们的方法在基于生成模型中表现突出，同时也展现出卓越的鲁棒性。**|
|**2024-06-14**|**Fine-Grained Urban Flow Inference with Multi-scale Representation Learning**|[2406.09710](http://arxiv.org/abs/2406.09710)|null|**细粒度城市流量推断 (FUFI) 是一项旨在提高交通效率和安全性的关键交通服务。FUFI 可以仅基于观察到的粗粒度数据来推断细粒度的城市交通流量。然而，大多数现有方法侧重于单尺度静态地理信息对 FUFI 的影响，而忽略了城市内不同尺度区域之间交互和动态信息。不同尺度的地理特征可以从相同的空间区域捕获冗余信息。为了有效地学习跨越时空的多尺度信息，我们提出了一种有效的细粒度城市流量推断模型 UrbanMSR，该模型使用自监督对比学习来获得邻域级和城市级地理信息的动态多尺度表示，并融合多尺度表示以提高细粒度精度。多尺度表示的融合增强了细粒度。我们通过对三个真实世界数据集的广泛实验来验证性能。与最先进方法相比，结果证明了所提出模型的优越性。 
**|

