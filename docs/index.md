---
layout: default
---

## Updated on 2025.09.29
> Usage instructions: [here](./docs/README.md#usage)

## 多模态

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2025-09-26**|[See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation](http://arxiv.org/abs/2509.22653)|**[link](https://github.com/Hu-chih-yao/see-point-fly)**|我们提出了视、指、飞 (SPF) 框架，这是一个基于视觉-语言模型 (VLM) 的免训练航空视觉-语言导航 (AVLN) 框架。SPF 能够基于任何形式的自由指令在任何类型的环境中导航到任何目标。与将动作预测视为文本生成任务的现有基于 VLM 的方法不同，我们的关键见解是将用于 AVLN 的动作预测视为一个 2D 空间定位任务。SPF 利用 VLM 将模糊的语言指令分解为在输入图像上的 2D 路点的迭代标注。结合预测的行进距离，SPF 将预测的 2D 路点转换为 3D 位移向量，作为无人机 (UAV) 的动作指令。此外，SPF 还自适应地调整行进距离，以促进更高效的导航。值得注意的是，SPF 以闭环控制方式执行导航，使无人机能够在动态环境中跟踪动态目标。SPF 在 DRL 仿真基准中创造了新的技术水平，其性能比之前最好的方法高出 63% 的绝对优势。在广泛的真实世界评估中，SPF 大幅超越了强大的基线方法。我们还进行了全面的消融研究，以突出我们设计选择的有效性。最后，SPF 对不同的 VLM 展现出显著的泛化能力。项目页面：https://spf-web.pages.dev|
|**2025-09-26**|[VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing](http://arxiv.org/abs/2509.22651)|null|大型语言模型和多模态系统日益增强的能力激发了人们对语音优先AI助手的兴趣，然而现有基准不足以评估这些系统的全部能力。我们引入了VoiceAssistant-Eval，这是一个综合基准，旨在评估AI助手在听觉、口语和视觉方面的能力。VoiceAssistant-Eval包含10,497个精选示例，涵盖13个任务类别。这些任务包括用于听觉的自然声音、音乐和口语对话；用于口语的多轮对话、角色扮演模仿和各种场景；以及用于视觉的高度异构图像。为了展示其效用，我们评估了21个开源模型和GPT-4o-Audio，测量了响应内容和语音的质量及其一致性。结果揭示了三个关键发现：(1) 专有模型并非普遍优于开源模型；(2) 大多数模型擅长口语任务但在音频理解方面滞后；(3) 精心设计的小型模型可以与大得多的模型相媲美。值得注意的是，中型Step-Audio-2-mini (7B) 的听觉准确率是LLaMA-Omni2-32B-Bilingual的两倍多。然而，挑战依然存在：多模态（音频加视觉）输入和角色扮演语音模仿任务对当前模型来说仍然困难，并且在鲁棒性和安全对齐方面仍然存在显著差距。VoiceAssistant-Eval识别了这些差距，并为评估和指导下一代AI助手的开发建立了严格的框架。代码和数据将发布在https://mathllm.github.io/VoiceAssistantEval/。|
|**2025-09-26**|[Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs](http://arxiv.org/abs/2509.22646)|null|人类能否识别AI生成（伪造）的视频并提供有根据的理由？尽管视频生成模型已快速发展，但一个关键维度——人类能否在生成的视频中检测到深度伪造痕迹，即揭示视频为机器生成的时空有据的视觉伪影——却在很大程度上被忽视了。我们引入了DeeptraceReward，首个细粒度、时空感知基准，用于标注人类感知的伪造痕迹以作为视频生成奖励。该数据集包含4.3K条详细标注，涵盖3.3K个高质量生成视频。每条标注都提供了自然语言解释，精确指出包含感知痕迹的边界框区域，并标记了精确的开始和结束时间戳。我们将这些标注整合为9个主要类别的深度伪造痕迹，这些痕迹使人类将视频识别为AI生成，并训练多模态语言模型（LMs）作为奖励模型以模仿人类的判断和定位。在DeeptraceReward上，我们的7B奖励模型在伪造线索识别、定位和解释方面平均优于GPT-5 34.7%。有趣的是，我们观察到一个一致的难度梯度：二元真伪分类比细粒度深度伪造痕迹检测容易得多；在后者中，性能从自然语言解释（最容易）到空间定位，再到时间标注（最难）逐渐下降。通过突出人类感知的深度伪造痕迹，DeeptraceReward提供了一个严谨的测试平台和训练信号，用于实现具有社会意识和值得信赖的视频生成。|
|**2025-09-26**|[WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning](http://arxiv.org/abs/2509.22644)|null|由大型语言模型（LLM）驱动的智能体系统在仓库级别代码生成任务上表现出了令人印象深刻的性能。然而，对于严重依赖视觉效果和用户交互反馈的网站代码库生成等任务，当前的代码智能体仅依赖简单的代码执行进行反馈和验证。这种方法未能捕捉生成代码的实际质量。在本文中，我们提出了WebGen-Agent，这是一种新颖的网站生成智能体，它利用全面多层次的视觉反馈来迭代生成和完善网站代码库。视觉语言模型（VLM）会生成关于网站截图和GUI智能体测试的详细且富有表现力的文本描述和建议，以及量化其质量的分数。截图和GUI智能体分数进一步与回溯和择优机制相结合，从而提升了智能体的性能。利用WebGen-Agent工作流中固有的准确视觉分数，我们进一步引入了带有截图和GUI智能体反馈的Step-GRPO，以提高LLM作为WebGen-Agent推理引擎的能力。通过将每一步的截图和GUI智能体分数用作Step-GRPO中的奖励，我们提供了一个密集且可靠的过程监督信号，有效地提高了模型的网站生成能力。在WebGen-Bench数据集上，WebGen-Agent将Claude-3.5-Sonnet的准确率从26.4%提高到51.9%，并将其外观得分从3.0提高到3.9，超越了之前最先进的智能体系统。此外，我们的Step-GRPO训练方法将Qwen2.5-Coder-7B-Instruct的准确率从38.9%提高到45.4%，并将其外观得分从3.4提高到3.7。|
|**2025-09-26**|[LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision](http://arxiv.org/abs/2509.22631)|null|高质量、领域专用数据集的构建是部署鲁棒视觉系统的主要瓶颈，在探索庞大、未标注的数据湖时，需要在数据质量、多样性和成本之间进行复杂的权衡。我们引入了Labeling Copilot，这是首个用于计算机视觉的数据策划深度研究智能体。一个由大型多模态语言模型驱动的中央编排智能体，利用多步推理执行涵盖三项核心能力的专用工具：(1) 校准发现从大型数据存储库中获取相关的、分布内的数据；(2) 可控合成通过鲁棒过滤为稀有场景生成新数据；(3) 共识标注通过结合非极大值抑制和投票的新颖共识机制编排多个基础模型，生成准确的标签。我们的大规模验证证明了Labeling Copilot各组件的有效性。共识标注模块在目标发现方面表现出色：在密集的COCO数据集上，它平均每图像生成14.2个候选提议——几乎是7.4个真实目标的两倍——最终标注mAP达到37.1%。在网络规模的Open Images数据集上，它解决了极端的类别不平衡问题，发现了903个新的边界框类别，将其能力扩展到总计超过1500个类别。同时，我们的校准发现工具在千万级样本规模上进行测试，采用一种主动学习策略，在样本效率相同的情况下，计算效率比替代方案高出40倍。这些实验验证了结合优化、可扩展工具的智能体工作流为策划工业规模数据集提供了坚实的基础。|
|**2025-09-26**|[Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting](http://arxiv.org/abs/2509.22615)|null|现代视觉语言流水线由在大规模图像文本语料库上训练的RGB视觉编码器驱动。尽管这些流水线实现了令人印象深刻的零样本能力和强大的跨任务迁移，但它们仍然继承了像素域的两种结构性低效：(i) 从边缘设备向云端传输密集RGB图像耗能且成本高昂，以及 (ii) 基于图像块的标记化导致序列长度爆炸式增长，给注意力预算和上下文限制带来压力。我们探索2D高斯溅射(2DGS)作为一种用于对齐的替代视觉基底：它是一种紧凑、空间自适应的表示，通过一组彩色各向异性高斯函数来参数化图像。我们开发了一个采用结构化初始化、亮度感知剪枝和批处理CUDA核函数的可扩展2DGS流水线，相比于之前的实现，实现了超过90倍的拟合速度提升和大约97%的GPU利用率。我们通过重用一个冻结的基于RGB的Transformer主干网络，配合一个轻量级溅射感知输入分支和一个Perceiver重采样器，将对比语言图像预训练(CLIP)进一步适应到2DGS，仅训练了总参数的大约7%。在大型DataComp子集上，GS编码器取得了有意义的ImageNet-1K零样本性能，同时相对于像素压缩输入3到20倍。尽管目前准确性落后于RGB编码器，但我们的结果将2DGS确立为一种可行的多模态基底，明确了架构瓶颈，并为未来在边缘云学习中实现既具有强大语义能力又传输高效的表示开辟了一条道路。|
|**2025-09-26**|[MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data](http://arxiv.org/abs/2509.22573)|null|高效检测人类与普适机器人交互的意图对于有效的人机交互（HRI）和协作至关重要。过去十年中，深度学习在该领域获得了广泛关注，大多数现有方法依赖多模态输入，例如RGB结合深度（RGB-D），将感知数据的时间序列窗口分类为交互或非交互。与此不同的是，我们提出了一种新颖的仅基于RGB的管道，用于以帧级精度预测人类交互意图，从而实现更快的机器人响应和更高的服务质量。意图预测的一个关键挑战是真实世界HRI数据集中固有的类别不平衡，这会阻碍模型的训练和泛化能力。为解决这个问题，我们引入了MINT-RVAE，一种合成序列生成方法，以及新的损失函数和训练策略，从而增强了模型在样本外数据上的泛化能力。我们的方法实现了最先进的性能（AUROC: 0.95），优于现有工作（AUROC: 0.90-0.912），同时仅需要RGB输入并支持精确的帧起始预测。最后，为了支持未来的研究，我们公开了我们的新数据集，其中包含人类交互意图的帧级标注。|
|**2025-09-26**|[UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration](http://arxiv.org/abs/2509.22570)|null|大型多模态模型（LMMs）和云端AI代理的快速进展正在将人机协作转变为双向、多模态的交互。然而，现有编解码器仍针对单模态、单向通信进行优化，导致在传统压缩-传输-重建流水线中反复降质。为解决这一局限性，我们提出了UniMIC，一个统一的基于token的多模态交互编码框架，它连接了边缘设备和云端AI代理。UniMIC不传输原始像素或纯文本，而是采用紧凑的token化表示作为通信介质，实现了高效的低比特率传输，同时保持与LMM的兼容性。为进一步增强压缩，轻量级基于Transformer的熵模型具有场景特定的设计——通用、掩码和文本条件——有效地最小化了token间的冗余。在文本到图像生成、文本引导的图像修复、图像扩展和视觉问答等任务上的大量实验表明，UniMIC实现了显著的比特率节省，并且在超低比特率（<0.05bpp）下仍保持鲁棒性，而不影响下游任务性能。这些结果确立了UniMIC作为下一代多模态交互通信的一种实用且前瞻性的范式。|
|**2025-09-26**|[JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation](http://arxiv.org/abs/2509.22548)|**[link](https://github.com/MIV-XJTU/JanusVLN)**|视觉语言导航（VLN）要求具身智能体在自然语言指令和连续视频流的引导下，在未见环境中进行导航。VLN的最新进展得益于多模态大语言模型（MLLM）强大的语义理解能力。然而，这些方法通常依赖显式语义记忆，例如构建文本认知地图或存储历史视觉帧。这类方法存在空间信息损失、计算冗余和内存膨胀等问题，阻碍了高效导航。受人类导航中隐式场景表示的启发，类似于左脑的语义理解和右脑的空间认知，我们提出了JanusVLN，一种新颖的VLN框架，其特点是采用双隐式神经记忆，将空间几何记忆和视觉语义记忆建模为独立、紧凑且固定大小的神经表示。该框架首先扩展了MLLM，以融入来自空间几何编码器的3D先验知识，从而增强了仅基于RGB输入的模型的空间推理能力。接着，将来自空间几何编码器和视觉语义编码器的历史键值缓存构建成双隐式记忆。通过仅保留初始窗口和滑动窗口中标记的键值对，避免了冗余计算，实现了高效的增量更新。大量实验表明，JanusVLN优于20多种最新方法，取得了最先进的性能。例如，与使用多种数据类型作为输入的方法相比，成功率提高了10.5-35.5；与使用更多RGB训练数据的方法相比，成功率提高了3.6-10.8。这表明所提出的双隐式神经记忆作为一种新颖范式，为未来的VLN研究探索了有前景的新方向。我们的项目页面：https://miv-xjtu.github.io/JanusVLN.github.io/。|
|**2025-09-26**|[Color Names in Vision-Language Models](http://arxiv.org/abs/2509.22524)|null|颜色是人类视觉感知的一个基本维度，也是交流物体和场景的主要方式。随着视觉语言模型（VLMs）日益普及，了解它们是否像人类一样命名颜色对于有效的人机交互至关重要。我们首次对VLMs的颜色命名能力进行了系统性评估，通过使用957个颜色样本在五个代表性模型上复现了经典的颜色命名方法。我们的结果表明，尽管VLMs在经典研究中的原型颜色上取得了高准确度，但在扩展的、非原型颜色集上的性能显著下降。我们识别出在所有模型中一致出现的21个常用颜色词，揭示了两种不同的方法：约束型模型主要使用基本术语，而扩展型模型则采用系统性明度修饰符。对九种语言的跨语言分析表明存在严重的训练不平衡，偏向英语和汉语，其中色相是颜色命名决策的主要驱动因素。最后，消融研究揭示，语言模型架构显著影响颜色命名，且独立于视觉处理能力。|
|**2025-09-25**|[Nova: Real-Time Agentic Vision-Language Model Serving with Adaptive Cross-Stage Parallelization](http://arxiv.org/abs/2509.21301)|null|本文提出 Nova，一个实时调度框架，用于在单张 GPU 上服务代理式视觉语言模型 (VLM)，并在平衡单请求延迟和整体请求处理吞吐量方面表现出色。我们的设计首先通过利用 VLM 在执行过程中异构的资源需求，并引入弹性 GPU 空间分区到视觉编码、LLM 预填充和 LLM 解码阶段之间，实现有效的流水线化，从而最大化利用计算和内存资源。在此基础上，我们引入了一种实时调度算法，该算法根据对延迟-吞吐量权衡的帕累托最优分析，自适应校准各阶段的资源分配，使系统在动态请求负载下保持响应能力和资源效率。为了进一步缓解 GPU 内存压力，我们为视觉编码器设计了一种轻量级权重卸载策略，该策略在最小化内存开销的同时保持推理效率。在合成和真实世界代理工作负载上的广泛评估表明，Nova 始终优于最先进的基线，将最大延迟提高了高达 23.3%，同时保持了有竞争力的吞吐量。|
|**2025-09-25**|[DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding](http://arxiv.org/abs/2509.21287)|null|近期的视觉-语言模型擅长大规模图像-文本对齐，但往往忽视语言的组合结构，导致在依赖词序和谓词-论元结构的任务上表现不佳。我们引入了DisCoCLIP，一个多模态编码器，它结合了冻结的CLIP视觉Transformer和一个新颖的张量网络文本编码器，该编码器显式编码句法结构。句子通过组合范畴语法解析器进行解析，以生成分布词张量，这些张量的收缩反映了句子的语法推导。为了保持模型高效，高阶张量通过张量分解进行因式分解，将参数数量从数千万减少到不到一百万。DisCoCLIP经过自监督对比损失的端到端训练，显著提高了对动词语义和词序的敏感性：它将CLIP在SVO-Probes上的动词准确率从77.6%提高到82.4%，将ARO归因和关系分数分别提高了9%以上和4%以上，并在新引入的SVO-Swap基准测试中达到了93.7%。这些结果表明，通过张量网络嵌入显式语言结构能够产生可解释、参数高效的表示，从而大幅提高视觉-语言任务中的组合推理能力。|
|**2025-09-25**|[MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources](http://arxiv.org/abs/2509.21268)|**[link](https://github.com/LengSicong/MMR1)**|大型多模态推理模型取得了快速进展，但其发展受到两个主要限制的制约：缺乏开放的、大规模的、高质量的长链式思考（CoT）数据，以及强化学习（RL）算法在后训练中的不稳定性。群体相对策略优化（GRPO）作为RL微调的标准框架，在奖励方差较低时容易出现梯度消失，这会削弱优化信号并损害收敛性。本工作做出三项贡献：(1) 我们提出了方差感知采样（VAS），这是一种由方差促进分数（VPS）指导的数据选择策略，它结合了结果方差和轨迹多样性，以促进奖励方差并稳定策略优化。(2) 我们发布了大规模的、精心策划的资源，包含约160万条长CoT冷启动数据和约1.5万对RL问答数据，旨在确保质量、难度和多样性，以及一个完全可复现的端到端训练代码库。(3) 我们开源了一系列多尺度多模态推理模型，为社区建立了标准化基线。在数学推理基准上的实验证明了精心策划的数据和所提出的VAS的有效性。全面的消融研究和分析提供了对每个组件贡献的进一步见解。此外，我们在理论上证实奖励方差是期望策略梯度幅度的下界，而VAS作为实现这一保证的实用机制。我们的代码、数据和检查点可在https://github.com/LengSicong/MMR1获取。|
|**2025-09-25**|[Un-Doubling Diffusion: LLM-guided Disambiguation of Homonym Duplication](http://arxiv.org/abs/2509.21262)|**[link](https://github.com/nagadit/Un-Doubling-Diffusion)**|同形异义词是拼写相同但含义不同的词语，对许多生成模型构成了挑战。当提示中出现同形异义词时，扩散模型可能会同时生成该词的多个含义，这被称为同形异义词重复现象。这个问题因英语中心偏见而变得更加复杂，这种偏见在文本到图像模型流水线之前包含了一个额外的翻译步骤。结果是，即使在原始语言中不是同形异义词的词语，在翻译成英语后也可能变成同形异义词并失去其含义。在本文中，我们介绍了一种测量重复率的方法，并使用利用视觉-语言模型（VLM）的自动评估和人工评估两种方式，对不同的扩散模型进行了评估。此外，我们研究了通过提示扩展来缓解同形异义词重复问题的方法，证明了这种方法也能有效减少与英语中心偏见相关的重复现象。自动评估流水线的代码已公开提供。|
|**2025-09-25**|[Semantic Edge-Cloud Communication for Real-Time Urban Traffic Surveillance with ViT and LLMs over Mobile Networks](http://arxiv.org/abs/2509.21259)|null|实时城市交通监控对于智能交通系统（ITS）至关重要，旨在确保智慧城市中的道路安全、优化交通流量、跟踪车辆轨迹并预防碰撞。在城市环境中部署边缘摄像头是监控路况的标准做法。然而，将这些摄像头与智能模型集成需要对动态交通场景有深入理解，并需要一个响应式接口以供用户交互。尽管多模态大语言模型（LLMs）可以解释交通图像并生成信息丰富的响应，但由于其高计算需求，在边缘设备上部署它们是不可行的。因此，LLM推理必须在云端进行，这需要将视觉数据从边缘传输到云端，但这一过程受限于带宽不足，可能导致延迟，从而损害实时性能。为解决这一挑战，我们提出了一种语义通信框架，该框架显著降低了传输开销。我们的方法包括使用YOLOv11检测感兴趣区域（RoIs），裁剪相关图像片段，并使用视觉Transformer（ViT）将其转换为紧凑的嵌入向量。这些嵌入随后被传输到云端，在云端，图像解码器重建裁剪后的图像。重建后的图像由多模态LLM处理，以生成交通状况描述。与原始裁剪图像的93%准确率相比，该方法实现了99.9%的数据传输量减少，同时对重建的裁剪图像保持89%的LLM响应准确率。我们的结果表明了ViT和LLM辅助的边缘-云语义通信在实时交通监控中的效率和实用性。|
|**2025-09-25**|[Human-like Navigation in a World Built for Humans](http://arxiv.org/abs/2509.21189)|**[link](https://github.com/ReasonNav/ReasonNav)**|在未曾到访过的人造环境中导航时——例如办公楼——人类会采用阅读标志和向他人问路等行为。这些行为通过减少在大片区域中搜索的需要，帮助人类高效地到达目的地。现有的机器人导航系统缺乏执行此类行为的能力，因此在大型环境中导航效率低下。我们提出了ReasonNav，这是一个模块化导航系统，它通过利用视觉语言模型（VLM）的推理能力，集成了这些类人导航技能。我们设计了基于导航地标的紧凑输入和输出抽象，使VLM能够专注于语言理解和推理。我们在真实和模拟导航任务中评估了ReasonNav，并表明该智能体成功地运用了高阶推理，以在大型复杂建筑中高效导航。|
|**2025-09-25**|[Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy](http://arxiv.org/abs/2509.21173)|null|强大的视觉-语言模型（VLM），如CLIP，的零样本泛化能力为安全相关任务（如分布外（OOD）检测）带来了新范式。然而，对CLIP的计算高效和可靠部署至关重要的其他方面仍被忽视。特别是，量化对CLIP性能超出准确性范围的影响仍未得到充分探索。本工作对CLIP模型上的量化进行了大规模评估，不仅评估了分布内准确性，还评估了一整套全面的可靠性指标，并揭示了由预训练来源驱动的反直觉结果。我们证明，量化持续改善了通常置信度不足的预训练模型的校准性，同时经常使其在置信度过高的变体上性能下降。有趣的是，校准性的下降并不排除在其他可靠性指标上取得进展；我们发现，对于这些校准性差的模型，OOD检测仍能得到改善。此外，我们确定了特定的量化感知训练（QAT）方法，这些方法在零样本准确性、校准性和OOD鲁棒性方面实现了同步提升，挑战了严格的效率-性能权衡的观点。这些发现通过超越其传统作用地利用量化，为解决部署高效、可靠和鲁棒VLM的多目标问题提供了重要见解。|
|**2025-09-25**|[Retrieval over Classification: Integrating Relation Semantics for Multimodal Relation Extraction](http://arxiv.org/abs/2509.21151)|null|关系抽取 (RE) 旨在识别非结构化文本中实体间的语义关系。尽管最近的研究将传统的关系抽取扩展到多模态场景，但大多数方法仍采用基于分类的范式，通过融合多模态特征将关系表示为离散标签。这种范式存在两个显著局限性：(1) 它忽略了实体类型和位置线索等结构约束，以及 (2) 它缺乏语义表达能力以实现细粒度关系理解。我们提出了检索优先于分类 (ROC)，这是一种新颖的框架，它将多模态关系抽取重新表述为由关系语义驱动的检索任务。ROC通过多模态编码器集成实体类型和位置信息，使用大型语言模型将关系标签扩展为自然语言描述，并通过基于语义相似度的对比学习对齐实体-关系对。实验表明，我们的方法在基准数据集 MNRE 和 MORE 上取得了最先进的性能，并展现出更强的鲁棒性和可解释性。|
|**2025-09-25**|[CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific Tokenization](http://arxiv.org/abs/2509.21150)|null|计算机辅助设计 (CAD) 是工业原型设计的核心组成部分，其中模型并非由原始坐标定义，而是由草图和拉伸等构建序列定义。这种序列结构能够实现高效的原型初始化和后续编辑。文本引导的CAD原型设计统一了文本到CAD生成和CAD编辑，有望简化整个设计流程。然而，现有研究尚未探索这种设置，这主要是因为标准的大型语言模型 (LLM) 分词器将CAD序列分解为自然语言的词片段，未能捕获基元级别的CAD语义，从而阻碍了注意力模块对几何结构进行建模。我们推测，一种与CAD的基元和结构特性相吻合的多模态分词策略可以提供更有效的表示。为此，我们提出了CAD-Tokenizer，这是一个使用基于序列的VQ-VAE（结合基元级池化和受限解码）的框架，通过模态特定的标记来表示CAD数据。这种设计生成了紧凑的、基元感知的表示，与CAD的结构特性相符。将CAD-Tokenizer应用于统一的文本引导CAD原型设计，显著提高了指令遵循能力和生成质量，在定量和定性性能方面均优于通用LLM和任务特定基线。|
|**2025-09-25**|[Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning](http://arxiv.org/abs/2509.21126)|null|复杂任务中的在线强化学习是耗时的，因为需要大量的交互步骤来学习最优Q函数。视觉-语言动作（VLA）策略代表了解决多样化任务的一个有前景的方向；然而，它们在低层控制上的性能仍然有限，并且有效部署通常需要特定任务的专家演示进行微调。在本文中，我们提出了VARL（VLM作为在线强化学习的动作建议者），这是一个利用视觉-语言模型（VLM）领域知识为强化学习智能体提供动作建议的框架。与以往方法不同，VARL提供的是动作建议而非设计启发式奖励，从而保证了最优性和收敛性不变。所建议的动作增加了样本多样性，并最终提高了样本效率，特别是在稀疏奖励任务中。为了验证VARL的有效性，我们在多样化的环境和智能体设置中对其进行了评估。结果表明，VARL在不引入显著计算开销的情况下大幅提高了样本效率。这些优势使VARL成为一个通用的在线强化学习框架，并使得在现实世界环境中从零开始直接应用强化学习成为可能。|
|**2025-09-23**|[DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture](http://arxiv.org/abs/2509.19274)|null|我们引入了DRISHTIKON，这是一个首个此类专门围绕印度文化的多模态多语言基准，旨在评估生成式人工智能系统的文化理解能力。与现有通用或全球范围的基准不同，DRISHTIKON提供了对印度多样化地区深入、细粒度的覆盖，涵盖15种语言，覆盖所有邦和联邦属地，并整合了超过64,000个对齐的文本-图像对。该数据集捕捉了丰富的文化主题，包括节日、服饰、美食、艺术形式和历史遗产等等。我们评估了广泛的视觉-语言模型（VLM），包括开源的小型和大型模型、专有系统、推理专用VLM以及面向印度的模型，涵盖零样本和思维链设置。我们的结果揭示了当前模型在对文化根植的多模态输入进行推理时的关键局限性，特别是对于低资源语言和记录较少的传统。DRISHTIKON填补了包容性人工智能研究中的一个重要空白，为推动具有文化意识、多模态能力的语言技术发展提供了一个强大的测试平台。|
|**2025-09-23**|[ConViS-Bench: Estimating Video Similarity Through Semantic Concepts](http://arxiv.org/abs/2509.19245)|null|两个视频相似意味着什么？视频如果根据其描绘的动作来判断，可能看起来相似，但如果根据拍摄地点来评估，则可能完全不同。虽然人类在比较视频时会自然地考虑不同方面，但这种能力尚未得到充分研究，并对那些通常依赖于广泛全局相似性分数的模型构成了挑战。具有视频理解能力的大型多模态模型（LMMs）为在视频比较任务中利用自然语言开辟了新机遇。我们引入了基于概念的视频相似性估计（ConViS），这是一项新颖的任务，通过计算一组预定义关键语义概念上的可解释相似性分数来比较视频对。ConViS 允许对视频相似性进行类人推理，并支持诸如概念条件视频检索等新应用。为了支持这项任务，我们还引入了ConViS-Bench，这是一个新的基准，包含跨多个领域的精心标注视频对。每对视频都附带概念级相似性分数以及差异和相似性的文本描述。此外，我们还在ConViS上对多个最先进模型进行了基准测试，深入了解它们与人类判断的一致性。我们的结果揭示了ConViS上显著的性能差异，表明某些概念在估计视频相似性方面提出了更大的挑战。我们相信ConViS-Bench将成为推动语言驱动视频理解研究的宝贵资源。|
|**2025-09-23**|[Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation](http://arxiv.org/abs/2509.19244)|null|我们提出了Lavida-O，一个统一的多模态掩码扩散模型（MDM），能够执行图像理解和生成任务。与现有仅支持简单图像级理解任务和低分辨率图像生成的多模态扩散语言模型（如MMaDa和Muddit）不同，Lavida-O展现了许多新能力，例如目标定位、图像编辑和高分辨率（1024像素）图像合成。它也是第一个利用其理解能力，通过规划和迭代自反思来改进图像生成和编辑结果的统一MDM。为了实现有效和高效的训练和采样，Lavida-O引入了许多新颖技术，例如弹性混合Transformer架构、通用文本条件化和分层采样。我们的模型在RefCOCO目标定位、GenEval文本到图像生成和ImgEdit图像编辑等广泛基准测试中取得了最先进的性能，优于现有的自回归和连续扩散模型（如Qwen2.5-VL和FluxKontext-dev），同时在推理时提供了显著的加速。|
|**2025-09-23**|[Steering Multimodal Large Language Models Decoding for Context-Aware Safety](http://arxiv.org/abs/2509.19212)|null|多模态大语言模型 (MLLMs) 越来越多地部署在实际应用中，但其做出上下文感知安全决策的能力仍然有限。现有方法往往难以平衡过度敏感性（不合理地拒绝良性查询）和欠敏感性（漏报视觉相关的风险），在安全对齐方面留下了一个持续存在的差距。为了解决这个问题，我们引入了安全感知对比解码 (SafeCoDe)，这是一种轻量级且模型无关的解码框架，它根据多模态上下文动态调整 token 生成。SafeCoDe 分为两个阶段运行：(1) 一种对比解码机制，通过对比真实图像和高斯噪声图像来突出对视觉上下文敏感的 token；(2) 一种全局感知 token 调制策略，它将场景级推理与 token 级调整相结合，以根据预测的安全判断调整拒绝行为。跨越不同 MLLM 架构和安全基准（涵盖欠敏感性、过度敏感性和一般安全评估）的大量实验表明，SafeCoDe 在保持模型有用性的同时，持续改进了上下文敏感的拒绝行为。|
|**2025-09-23**|[Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs](http://arxiv.org/abs/2509.19207)|null|对比视觉-语言模型（VLM）在关联视觉和文本信息方面取得了显著进展，但理解长而密集的描述仍然是一个悬而未决的难题。我们假设组合性，即推理对象-属性绑定和对象间关系的能力，是理解更长描述的关键。在本文中，我们研究了组合性与长描述理解之间的相互作用，探讨为一种特性进行训练是否能增强另一种特性。我们训练并评估了一系列针对这些能力的模型。我们的结果揭示了一种双向关系：组合性训练提高了长描述检索的性能，而对长描述的训练促进了组合性。然而，这些收益对数据质量和模型设计很敏感。我们发现，在结构不良的描述上进行训练，或参数更新有限，无法实现泛化。同样地，旨在保持通用对齐的策略，例如冻结位置嵌入，不能提升组合理解能力。总的来说，我们发现组合理解能力和长描述理解能力是相互交织的能力，可以通过对密集、有根据的描述进行训练来共同学习。尽管存在这些挑战，我们表明在高质量长描述数据上训练的模型可以在两项任务中实现强大的性能，为改进VLM的泛化能力提供了实用的指导。|
|**2025-09-23**|[Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions](http://arxiv.org/abs/2509.19203)|**[link](https://github.com/IoannaNti/LexiCLIP)**|对比训练的视觉-语言模型（VLMs），如CLIP，已成为学习判别性视觉-语言表征的标准方法。然而，这些模型常表现出浅层语言理解，呈现词袋行为。其双编码器设计加剧了这些局限，并引入了模态鸿沟。此外，训练过程对大量网络收集数据语料库的依赖使其计算成本高昂，并带来了重大的隐私问题。为解决这些局限性，本研究通过引入一种无视觉、单编码器检索管道，挑战了视觉编码器在检索任务中的必要性。我们摒弃了传统的文本到图像检索范式，在VLLM生成的结构化图像描述的辅助下，转向了文本到文本范式。我们证明了这种范式转变具有显著优势，包括大幅缩小模态鸿沟、提高组合性，以及在短句和长句查询上表现更优，所有这些只需在两块GPU上进行数小时校准即可实现。此外，用文本描述替代原始图像为检索引入了一种更隐私友好的替代方案。为进一步评估泛化能力并解决先前组合性基准的一些不足，我们发布了两个源自Flickr30k和COCO的基准，它们包含由短句组成的各种组合性查询，我们将其命名为subFlickr和subCOCO。我们的无视觉检索器与传统多模态模型表现相当，并常常超越它们。重要的是，我们的方法在多个检索和组合性基准上实现了最先进的零样本性能，所用模型参数量仅为0.3B。代码可在https://github.com/IoannaNti/LexiCLIP获取。|
|**2025-09-23**|[Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models](http://arxiv.org/abs/2509.19191)|null|视觉-语言模型（VLM）在各种实际任务中展现出卓越的性能。然而，现有的VLM通常通过序列化图像来处理视觉信息，这种方法与人类视觉的并行特性显著不同。此外，其不透明的内部机制阻碍了更深入的理解和架构创新。受人类视觉双流假说的启发，该假说区分了“识别什么”和“位于何处”的通路，我们将VLM中的视觉处理解构为物体识别和空间感知进行单独研究。对于物体识别，我们将图像转换为文本标记图，并发现模型对图像内容的感知呈现为从浅层到深层的两阶段过程，始于属性识别，最终实现语义消歧。对于空间感知，我们理论推导并经验验证了VLM中位置表示的底层几何结构。基于这些发现，我们引入了一种基于即插即用视觉解码器的与指令无关的标记压缩算法以提高解码效率，以及一种RoPE缩放技术以增强空间推理能力。通过严格的实验，我们的工作验证了这些分析，提供了对VLM内部机制更深入的理解，并为设计更强大的未来架构提供了清晰的原则。|
|**2025-09-23**|[A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination](http://arxiv.org/abs/2509.19168)|null|本文提出了一种滚动时域、基于采样的规划器，能够对多模态策略分布进行推理。通过使用交叉熵方法在共同成本函数下优化多模态策略，我们的方法增强了对局部最优的鲁棒性，并促进了对解空间的有效探索。我们表明，我们的方法自然地扩展到多机器人无碰撞规划，使智能体能够共享多样化的候选策略以避免死锁，并允许团队在不产生集中式优化的计算复杂性的情况下最小化全局目标。数值模拟表明，采用多模态显著提高了在陷阱环境和多机器人避碰中的成功率。硬件实验进一步验证了该方法的实时可行性和实际性能。|
|**2025-09-23**|[Investigating Traffic Accident Detection Using Multimodal Large Language Models](http://arxiv.org/abs/2509.19096)|null|交通安全仍然是一个重要的全球性问题，及时准确的事故检测对于减少危害和快速应急响应至关重要。基于基础设施的视觉传感器为持续实时监测提供了可扩展且高效的解决方案，促进了直接从捕获图像中自动化检测事故。本研究调查了多模态大语言模型（MLLMs）的零样本能力，利用来自基础设施摄像头的图像检测和描述交通事故，从而最大限度地减少了对大量标注数据集的依赖。主要贡献包括：(1) 使用CARLA模拟的DeepAccident数据集对MLLM进行评估，通过受控模拟明确解决了多样化、真实、基于基础设施的事故数据稀缺问题；(2) 在未经事先微调的情况下，对Gemini 1.5和2.0、Gemma 3以及Pixtral模型在事故识别和描述能力方面进行了比较性能分析；(3) 将先进的视觉分析技术，具体来说，用于目标检测的YOLO、用于多目标跟踪的Deep SORT和用于实例分割的Segment Anything (SAM)，集成到增强的提示中，以提高模型的准确性和可解释性。关键数值结果显示，Pixtral表现最佳，F1分数为0.71，召回率为83%；而Gemini模型通过增强提示提高了精度（例如，Gemini 1.5升至90%），但F1分数和召回率显著下降。Gemma 3提供了最均衡的性能，指标波动最小。这些发现表明，将MLLM与先进视觉分析技术相结合具有巨大的潜力，增强了它们在实际自动化交通监测系统中的适用性。|
|**2025-09-23**|[Data-Free Knowledge Distillation for LiDAR-Aided Beam Tracking in MmWave Systems](http://arxiv.org/abs/2509.19092)|null|多模态感知可减少波束训练开销，但受限于机器学习复杂度和数据集需求。为解决此问题，我们提出一个无数据（DF）知识蒸馏（KD）框架，用于高效的LiDAR辅助毫米波波束跟踪，即预测当前和未来的最佳波束。具体来说，我们提出一个知识反演框架，其中一个生成器在定义于预训练教师模型特征和输出的损失函数引导下，从随机噪声合成LiDAR输入数据。学生模型随后利用合成数据和从教师模型中蒸馏的知识进行训练。生成器损失结合了称为元数据损失、激活损失和熵损失的三项。对于学生训练，除了标准的Kullback-Leibler散度损失外，我们还考虑了教师和学生logit之间的均方误差（MSE）损失。仿真结果表明，所提出的DF-KD在Top-1和Top-5准确率方面（略微）优于教师模型。此外，我们观察到元数据损失对生成器性能有显著贡献，并且学生的MSE损失可以有效替代标准的KD损失，同时需要更少的微调超参数。|
|**2025-09-19**|[MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer](http://arxiv.org/abs/2509.16197)|null|统一多模态大语言模型（LLMs）能够同时理解和生成视觉内容，具有巨大潜力。然而，现有开源模型常常在这些能力之间面临性能权衡。我们提出Manzano，一个简单且可扩展的统一框架，通过结合混合图像分词器和精心设计的训练方案，大幅缓解了这种紧张关系。一个单一的共享视觉编码器驱动两个轻量级适配器，在一个共同的语义空间内，生成用于图像到文本理解的连续嵌入和用于文本到图像生成的离散令牌。一个统一的自回归大语言模型预测以文本和图像令牌形式存在的高级语义，辅以一个辅助扩散解码器随后将图像令牌转化为像素。该架构，结合跨理解和生成数据的统一训练方案，实现了这两种能力的可扩展联合学习。Manzano在统一模型中取得了最先进的结果，并与专业模型具有竞争力，尤其是在文本丰富的评估中。我们的研究表明任务冲突最小，并且随着模型规模的扩大，性能持续提升，验证了我们混合分词器的设计选择。|
|**2025-09-19**|[Are Multimodal Foundation Models All That Is Needed for Emofake Detection?](http://arxiv.org/abs/2509.16193)|null|在这项工作中，我们研究了用于情感伪造检测（EFD）的多模态基础模型（MFM），并假设它们将优于音频基础模型（AFM）。MFM由于其跨模态预训练，从多种模态中学习情感模式，而AFM仅依赖于音频。因此，MFM能更好地识别被操纵音频中不自然的情感转变和不一致性，使其在区分真实与虚假情感表达方面更有效。为了验证我们的假设，我们对最先进（SOTA）的MFM（例如LanguageBind）以及AFM（例如WavLM）进行了全面的比较分析。我们的实验证实MFM在EFD方面优于AFM。除了单个基础模型（FM）的性能之外，我们还探索了FM融合，这受到了合成语音检测和语音情感识别等相关研究领域发现的启发。为此，我们提出了SCAR，一个用于有效融合的新颖框架。SCAR引入了一种嵌套的交叉注意力机制，其中来自FM的表示在两个阶段顺序交互，以精炼信息交换。此外，一个自注意力精炼模块通过强化重要的跨FM线索同时抑制噪声，进一步增强了特征表示。通过SCAR与MFM的协同融合，我们实现了SOTA性能，超越了独立FM、传统融合方法以及之前在EFD方面的研究。|
|**2025-09-19**|[Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks](http://arxiv.org/abs/2509.16163)|null|视觉语言模型 (VLM) 在多模态理解方面表现出色，但容易受到对抗性攻击。现有防御方法通常需要昂贵的再训练或显著的架构修改。我们引入了一种利用张量分解的轻量级防御方法，适用于任何预训练的VLM，无需再训练。通过分解和重构视觉编码器表示，它能过滤对抗性噪声，同时保留语义信息。在COCO和Flickr30K数据集上对CLIP进行的实验表明鲁棒性得到改善。在Flickr30K上，它恢复了因攻击损失的12.3%的性能，将Recall@1准确率从7.5%提高到19.8%。在COCO上，它恢复了8.1%的性能，将准确率从3.8%提高到11.9%。分析表明，低秩 (8-32) 和低残差强度 ( $\alpha=0.1-0.2$ ) 的张量链分解是最优的。该方法是一种实用的即插即用解决方案，对现有VLM具有最小开销。|
|**2025-09-19**|[Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal Large Language Models](http://arxiv.org/abs/2509.16149)|**[link](https://github.com/ustc-mkh/MLLM_Sycophancy)**|多模态大语言模型（MLLM）在基于图像输入进行对话方面展现出非凡的能力。然而，我们观察到MLLM表现出一种显著的视觉逢迎行为。尽管在基于文本的大语言模型（LLM）中也注意到类似行为，但当MLLM处理图像输入时，这种行为变得更为突出。我们将这种现象称为“逢迎模态鸿沟”。为了更好地理解这个问题，我们进一步分析了导致这种鸿沟加剧的因素。为了缓解视觉逢迎行为，我们首先尝试使用朴素的监督微调，以帮助MLLM抵制用户误导性指令。然而，我们发现这种方法也使MLLM对纠正性指令过度抵制（即，即使错了也固执己见）。为了缓解这种权衡，我们提出了逢迎反思调优（SRT），它使MLLM能够进行反思性推理，使其能够在得出结论之前判断用户的指令是误导性的还是纠正性的。应用SRT后，我们观察到对误导性指令的逢迎行为显著减少，同时在接收纠正性指令时也不会导致过度固执。|
|**2025-09-19**|[Reward Evolution with Graph-of-Thoughts: A Bi-Level Language Model Framework for Reinforcement Learning](http://arxiv.org/abs/2509.16136)|null|设计有效的奖励函数在强化学习（RL）中仍然是一个主要挑战，通常需要大量人类专业知识和迭代改进。最近的进展利用大型语言模型（LLMs）进行自动化奖励设计，但这些方法受到幻觉、对人类反馈的依赖以及处理复杂多步任务的挑战的限制。在这项工作中，我们引入了基于思维图的奖励演化（RE-GoT），这是一种新颖的双层框架，它通过结构化图基推理增强了LLMs，并整合了视觉语言模型（VLMs）用于自动化 rollout 评估。RE-GoT首先将任务分解为文本属性图，从而实现全面分析和奖励函数生成，然后利用来自VLMs的视觉反馈迭代地改进奖励，无需人类干预。在10个RoboGen和4个ManiSkill2任务上的大量实验表明，RE-GoT始终优于现有的基于LLM的基线。在RoboGen上，我们的方法将平均任务成功率提高了32.25%，在复杂多步任务上取得了显著的提升。在ManiSkill2上，RE-GoT在四个多样化的操作任务中实现了93.73%的平均成功率，显著超越了先前的基于LLM的方法，甚至超过了专家设计的奖励。我们的结果表明，结合LLMs和VLMs并配合思维图推理，为强化学习中的自主奖励演化提供了一种可扩展且有效的解决方案。|
|**2025-09-19**|[BaseReward: A Strong Baseline for Multimodal Reward Model](http://arxiv.org/abs/2509.16127)|null|多模态大语言模型（MLLMs）的快速发展使得使其与人类偏好对齐成为一个关键挑战。奖励模型（RMs）是实现这一目标的核心技术，但在学术界和工业界，目前均缺乏构建最先进多模态奖励模型（MRMs）的系统性指南。通过详尽的实验分析，本文旨在为构建高性能MRM提供一份明确的“秘籍”。我们系统地研究了MRM开发流程中的每一个关键组成部分，包括奖励建模范式（例如，朴素奖励模型、基于评论员的奖励模型和生成式奖励模型）、奖励头架构、训练策略、数据精选（涵盖十余种多模态和纯文本偏好数据集）、主干模型和模型规模，以及集成方法。基于这些实验洞察，我们引入了BaseReward，一个强大而高效的多模态奖励建模基线。BaseReward采用了一种简单而有效的架构，基于Qwen2.5-VL主干模型构建，具有优化的两层奖励头，并在精心整理的高质量多模态和纯文本偏好数据混合集上进行训练。我们的结果表明，BaseReward在MM-RLHF-Reward Bench、VL-Reward Bench和Multimodal Reward Bench等主要基准测试上建立了新的SOTA，优于之前的模型。此外，为了验证其超越静态基准测试的实用性，我们将BaseReward集成到真实的强化学习流程中，成功提升了多模态大语言模型在各种感知、推理和对话任务中的性能。这项工作不仅提供了一个顶级的MRM，更重要的是，为社区提供了一份清晰的、有实证支持的指南，用于开发下一代多模态大语言模型的稳健奖励模型。|
|**2025-09-19**|[Randomized Smoothing Meets Vision-Language Models](http://arxiv.org/abs/2509.16088)|null|随机平滑（RS）是确保机器学习模型正确性的突出技术之一，通过它可以解析推导出逐点鲁棒性证书。尽管RS在分类任务中已得到充分理解，但其在生成模型中的应用尚不明确，因为生成模型的输出是序列而非标签。我们通过将生成模型的输出与一个预言机分类任务联系起来解决了这个问题，并表明RS仍然可以启用：最终响应可以被分类为离散动作（例如，VLA中的服务机器人指令）、有害与无害（VLM中的内容审核或毒性检测），甚至可以应用预言机将答案聚类为语义等价的类别。假设预言机分类器比较的错误率有界，我们建立了将样本数量与相应鲁棒性半径关联起来的理论。我们进一步解析推导出了改进的缩放定律，将认证半径和准确性与样本数量关联起来，表明即使在较弱的假设下，早期结果（即减少2到3个数量级的样本量即可满足需求且损失最小）仍然有效。总而言之，这些进展使得鲁棒性认证对于最先进的VLM而言既明确又在计算上可行，并已针对近期越狱式对抗攻击进行了验证。|
|**2025-09-19**|[See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model](http://arxiv.org/abs/2509.16087)|null|我们引入了SEE&TREK，这是首个免训练提示框架，旨在增强多模态大语言模型（MLLM）在仅视觉约束下的空间理解能力。尽管先前的工作已通过引入深度或点云等模态来提升空间推理，但纯视觉空间理解仍未得到充分探索。SEE&TREK通过关注两个核心原则来解决这一空白：增加视觉多样性和运动重建。对于视觉多样性，我们采用最大语义丰富度采样，利用现成的感知模型提取能够捕捉场景结构的语义丰富的关键帧。对于运动重建，我们模拟视觉轨迹并将相对空间位置编码到关键帧中，以同时保持空间关系和时间连贯性。我们的方法免训练且无需GPU，仅需一次前向传播，并且可以无缝集成到现有MLLM中。在VSI-B ENCH和STI-B ENCH上进行的大量实验表明，SEE&TREK持续提升了各类MLLM在各种空间推理任务上的性能，最高提升达3.5%，为实现更强的空间智能提供了一条有前景的道路。|
|**2025-09-19**|[I-FailSense: Towards General Robotic Failure Detection with Vision-Language Models](http://arxiv.org/abs/2509.16072)|null|开放世界环境中语言条件下的机器人操作不仅需要准确的任务执行，还需要检测故障的能力，以便在真实世界环境中进行鲁棒部署。尽管视觉-语言模型（VLM）的最新进展显著提升了机器人的空间推理和任务规划能力，但它们在识别自身故障方面的能力仍然有限。特别是一个关键但尚未充分探索的挑战在于检测语义错位错误，即机器人执行的任务在语义上有意义但与给定指令不一致。为此，我们提出了一种从现有语言条件下的操作数据集中构建针对语义错位故障检测的数据集的方法。我们还提出了I-FailSense，一个具备接地仲裁能力的开源VLM框架，专门为故障检测而设计。我们的方法依赖于对一个基础VLM进行后训练，然后训练连接到VLM不同内部层的轻量级分类头（称为FS块），并通过集成机制聚合它们的预测。实验表明，I-FailSense在检测语义错位错误方面优于最先进的VLM，无论是在规模相当还是更大的模型上。值得注意的是，尽管I-FailSense仅在语义错位检测上进行训练，但它能泛化到更广泛的机器人故障类别，并通过零样本或少量后训练有效地迁移到其他模拟环境和真实世界。相关数据集和模型已在HuggingFace上公开（网页：https://clemgris.github.io/I-FailSense/）。|
|**2025-09-19**|[Language-Instructed Reasoning for Group Activity Detection via Multimodal Large Language Model](http://arxiv.org/abs/2509.16054)|null|群体活动检测（GAD）旨在同时识别视频序列中的群体成员并对其集体活动进行分类。现有的基于深度学习的方法开发了专门的架构（例如，Transformer网络）来建模个体角色的动态以及个体与群体之间的语义依赖关系。然而，它们仅依赖于从视觉特征中进行的隐式模式识别，并在上下文推理和可解释性方面面临困难。在这项工作中，我们提出了LIR-GAD，这是一个通过多模态大语言模型（MLLM）实现GAD的语言指导推理的新颖框架。我们的方法通过引入一个活动级别的<ACT> token和多个特定于群体的<GROUP> token来扩展MLLM的原始词汇表。我们处理视频帧，同时结合两个专门设计的token和语言指令，这些随后被集成到MLLM中。嵌入在MLLM中的预训练常识知识使得<ACT> token和<GROUP> token能够分别有效捕获集体活动的语义信息并学习不同群体的独特表示特征。此外，我们引入了一种多标签分类损失，以进一步增强<ACT> token学习判别性语义表示的能力。接着，我们设计了一个多模态双对齐融合（MDAF）模块，该模块将MLLM中对应于所设计token的隐藏嵌入与视觉特征进行集成，显著提升了GAD的性能。定量和定性实验都证明了我们所提出的方法在GAD任务中的优越性能。|
|**2025-09-18**|[Calibration-Aware Prompt Learning for Medical Vision-Language Models](http://arxiv.org/abs/2509.15226)|null|医用视觉-语言模型 (Med-VLM) 通过利用大规模图像-文本预训练，在各种医学影像任务中表现出卓越的性能。然而，它们的置信度校准在很大程度上尚未被探索，因此仍然是一个重大挑战。因此，未校准的预测可能导致过度自信的错误，从而损害临床信任和决策可靠性。为解决这个问题，我们引入了CalibPrompt，这是首个在提示微调期间校准Med-VLM的框架。CalibPrompt在标记数据稀缺的情况下，通过精心设计的校准目标，优化了一组少量的可学习提示。首先，我们研究了一种旨在将平滑准确度与预测模型置信度对齐的正则化器。其次，我们引入了一种角分离损失，以最大化文本特征接近度，从而提高多模态Med-VLM置信度估计的可靠性。在四个公开可用的Med-VLM和五个不同的医学影像数据集上进行的大量实验表明，CalibPrompt在不显著影响原始准确率的情况下持续改进了校准。我们的代码可在 https://github.com/iabh1shekbasu/CalibPrompt 获取。|
|**2025-09-18**|[Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation](http://arxiv.org/abs/2509.15225)|null|我们引入VocAlign，这是一种专为开放词汇语义分割中的视觉语言模型（VLM）设计的新型无源域适应框架。我们的方法采用师生范式，并辅以词汇对齐策略，通过引入额外的类别概念来改进伪标签生成。为确保效率，我们使用低秩适应（LoRA）来微调模型，在保留其原有能力的同时最小化计算开销。此外，我们为学生模型提出了一种Top-K类别选择机制，该机制显著降低了内存需求，同时进一步提升了适应性能。我们的方法在CityScapes数据集上实现了显著的6.11 mIoU提升，并在零样本分割基准上展现出卓越性能，为开放词汇设置下的无源域适应树立了新标杆。|
|**2025-09-18**|[ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data](http://arxiv.org/abs/2509.15221)|**[link](https://github.com/YangYzzzz/ScaleCUAWebCrawler)**|视觉-语言模型（VLMs）使得能够自主操作图形用户界面（GUIs）的计算机使用智能体（CUAs）成为可能，展现出巨大潜力，然而，进展受限于缺乏大规模、开源的计算机使用数据和基础模型。在这项工作中，我们介绍了ScaleCUA，这是迈向扩展开源CUA的一步。它提供了一个大规模数据集，涵盖6个操作系统和3个任务领域，该数据集通过结合自动化智能体和人类专家的闭环管道构建而成。经过这些扩展数据的训练，ScaleCUA能够跨平台无缝操作。具体而言，它在基线上取得了显著提升（WebArena-Lite-v2上提升26.6，ScreenSpot-Pro上提升10.7），并创造了新的SOTA（最先进）结果（MMBench-GUI L1-Hard上达到94.4%，OSWorld-G上达到60.6%，WebArena-Lite-v2上达到47.4%）。这些发现强调了数据驱动扩展对于通用计算机使用智能体而言的强大能力。我们将发布数据、模型和代码以促进未来的研究：https://github.com/OpenGVLab/ScaleCUA。|
|**2025-09-18**|[Generalizable Geometric Image Caption Synthesis](http://arxiv.org/abs/2509.15217)|null|多模态大语言模型具有多种实际应用，这些应用要求强大的推理能力。尽管取得了最新进展，这些模型在解决复杂几何问题方面仍然面临困难。一个关键挑战源于缺乏用于理解几何图像的高质量图像-文本对数据集。此外，大多数基于模板的数据合成流程通常无法泛化到超出其预定义模板的问题。在本文中，我们通过将可验证奖励强化学习（RLVR）这一互补过程引入数据生成流程，弥合了这一差距。通过采用RLVR来细化从50种基本几何关系合成的几何图像的描述，并利用源自数学问题解决任务的奖励信号，我们的流程成功捕获了几何问题解决的关键特征。这使得任务泛化能力更强，并带来了显著的改进。此外，即使在分布外场景中，所生成的数据集也增强了多模态大语言模型的通用推理能力，在MathVista和MathVerse数据集中使用非几何输入图像的统计、算术、代数和数值任务中，准确率提高了2.8%-4.8%，同时在MMMU数据集的艺术、设计、技术和工程任务中，准确率提高了2.4%-3.9%。|
|**2025-09-18**|[What's the Best Way to Retrieve Slides? A Comparative Study of Multimodal, Caption-Based, and Hybrid Retrieval Techniques](http://arxiv.org/abs/2509.15211)|null|幻灯片演示文稿作为弥合演示幻灯片和书面文档之间鸿沟的数字报告，是学术和企业环境中普遍的信息传达媒介。它们结合了文本、图像和图表的多模态特性，给检索增强生成系统带来了挑战，其中检索质量直接影响下游性能。传统的幻灯片检索方法通常涉及对不同模态进行单独索引，这会增加复杂性并可能丢失上下文信息。本文研究了各种有效的幻灯片检索方法，包括ColPali等视觉后期交互嵌入模型、视觉重排序器的使用，以及将密集检索与BM25结合并通过文本重排序器和倒数排名融合等融合方法进一步增强的混合检索技术。本文还评估了一种新颖的基于视觉-语言模型（VLM）的字幕生成流水线，该流水线与视觉后期交互技术相比，显著降低了嵌入存储需求，同时保持了可比的检索性能。我们的分析扩展到这些方法的实际方面，评估它们的运行时性能、存储需求以及检索效率，从而为选择和开发用于实际应用的高效鲁棒幻灯片检索系统提供了实用指导。|
|**2025-09-18**|[Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding](http://arxiv.org/abs/2509.15178)|null|时空视频定位（STVG）旨在根据输入的文本查询来定位视频中的时空管。在本文中，我们利用多模态大语言模型（MLLMs）来探索STVG中的零样本解决方案。我们揭示了关于MLLMs的两个关键见解：(1) MLLMs倾向于动态分配被称为“定位token”的特殊token，用于定位文本查询；(2) MLLMs经常由于无法充分整合文本查询中的线索（例如，属性、动作）进行推理而遭受次优的定位性能。基于这些见解，我们提出了一种基于MLLM的STVG零样本框架，其中包含新颖的分解时空高亮（DSTH）和时序增强组装（TAS）策略，以释放MLLMs的推理能力。DSTH策略首先将原始查询解耦为属性和动作子查询，用于在空间和时间上查询目标的存在。它随后使用一个新颖的logit引导重注意力（LRA）模块，通过正则化每个子查询的token预测，学习潜在变量作为空间和时间提示。这些提示分别突出属性和动作线索，引导模型的注意力到可靠的空间和时间相关的视觉区域。此外，由于属性子查询的空间定位应具有时间一致性，我们引入了TAS策略，利用原始视频帧和时序增强帧作为输入来组装预测，以帮助提高时间一致性。我们在各种MLLMs上评估了我们的方法，并表明它在三个常见的STVG基准测试中超越了SOTA方法。代码将可在https://github.com/zaiquanyang/LLaVA_Next_STVG获取。|
|**2025-09-18**|[An Evaluation-Centric Paradigm for Scientific Visualization Agents](http://arxiv.org/abs/2509.15160)|null|多模态大语言模型（MLLMs）的最新进展使得日益复杂的自主可视化智能体能够将用户意图转化为数据可视化。然而，衡量进展和比较不同智能体仍然具有挑战性，尤其是在科学可视化（SciVis）领域，因为缺乏用于评估实际能力的全面、大规模基准。本立场论文探讨了SciVis智能体所需的各种评估类型，概述了相关挑战，提供了一个简单的概念验证评估示例，并讨论了评估基准如何促进智能体的自我改进。我们倡导更广泛的合作，以开发一个SciVis智能体评估基准，该基准不仅能评估现有能力，而且能推动创新并激发该领域的未来发展。|
|**2025-09-18**|[Exploring How Audio Effects Alter Emotion with Foundation Models](http://arxiv.org/abs/2509.15151)|null|音频效果（FX），如混响、失真、调制和动态范围处理，在音乐聆听过程中塑造情感反应方面发挥着关键作用。虽然先前的研究已检验了低级音频特征与情感感知之间的联系，但音频FX对情感的系统性影响仍未得到充分探索。这项工作研究了如何利用基础模型——即在多模态数据上预训练的大规模神经网络架构——来分析这些效果。这些模型编码了音乐结构、音色和情感意义之间丰富的关联，为探究声音设计技术的情感影响提供了一个强大的框架。通过对深度学习模型中的嵌入应用各种探测方法，我们检验了音频FX与估计情感之间复杂、非线性的关系，揭示了与特定效果相关的模式，并评估了基础音频模型的鲁棒性。我们的发现旨在增进对音频制作实践感知影响的理解，对音乐认知、表演和情感计算具有启示意义。|
|**2025-09-18**|[From Pixels to Urban Policy-Intelligence: Recovering Legacy Effects of Redlining with a Multimodal LLM](http://arxiv.org/abs/2509.15132)|null|本文展示了多模态大语言模型 (MLLM) 如何拓展城市测量能力并支持基于地点的政策干预措施的跟踪。GPT-4o 利用结构化的“先推理后估计”流程在街景图像上推断出社区贫困和树冠覆盖，我们将其嵌入准实验设计中以评估20世纪30年代“红线政策”的遗留影响。GPT-4o 再现了“红线政策”预期的不利社会环境遗留影响，其估计结果与权威来源统计上无显著差异，并且它优于传统的基于像素的分割基线，这与整体场景推理能够提取超越单纯对象计数的更高阶信息的观点相符。这些结果将 MLLM 定位为政策级别的社区测量工具，并促使在更广泛的政策评估场景中进行验证。|
|**2025-09-18**|[Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models](http://arxiv.org/abs/2509.15076)|null|空气污染仍然是对公众健康和环境可持续性的严峻威胁，然而传统监测系统常受限于有限的空间覆盖范围和可及性。本文提出了一种人工智能驱动的智能体，该智能体基于天空图像预测环境空气污染水平，并利用生成建模合成逼真的污染情景可视化效果。我们的方法结合了统计纹理分析与监督学习进行污染分类，并利用视觉-语言模型（VLM）引导的图像生成来生成空气质量状况的可解释表示。生成的视觉效果模拟了不同程度的污染，为用户界面提供了基础，以提高透明度并支持明智的环境决策。这些输出可以无缝集成到智能应用中，旨在增强态势感知并鼓励基于实时预测的行为响应。我们使用城市天空图像数据集验证了我们的方法，并证明了其在污染水平估计和语义一致的视觉合成方面的有效性。系统设计进一步融入了以人为中心的用户体验原则，以确保空气质量预测的可访问性、清晰度和公众参与。为支持可扩展和节能的部署，未来的迭代将整合一种通过基于FPGA的增量学习进行增强的绿色CNN架构，从而实现边缘平台上的实时推理。|

## 大模型PEFT

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2025-09-26**|[AxLLM: accelerator architecture for large language models with computation reuse capability](http://arxiv.org/abs/2509.22512)|null|大语言模型需要巨大的计算能力和内存资源，这对高效部署带来了巨大的挑战。尽管量化已被广泛探索以减小模型尺寸和计算量，但本文论证了一个额外的好处：量化增加了参数局部性，为计算复用创造了机会。基于这一洞察，我们提出了AxLLM，一种专为量化模型设计的硬件加速器架构。AxLLM引入了一种新颖的冗余消除技术，该技术缓存并复用重复权重值的乘法结果，大幅减少了冗余操作。该架构具有双重乘法与复用流水线，无需修改参数、重新训练或离线预处理，即可有效支持基础模型和LoRA微调模型。实验结果表明，AxLLM的计算量减少高达90%，能耗降低28%，相较于基线执行实现了1.7倍的加速。这些结果突出了AxLLM是用于在专用硬件上加速大语言模型的一种可扩展且高效的解决方案。|
|**2025-09-26**|[Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting](http://arxiv.org/abs/2509.22195)|null|在机器人远程操作数据上微调视觉-语言模型（VLM）以创建视觉-语言-动作（VLA）模型是训练通用策略的一种有前景的范式，但它存在一个根本性权衡：学习生成动作常常削弱VLM的基础推理和多模态理解能力，阻碍其泛化到新颖场景、遵循指令和语义理解。我们认为这种灾难性遗忘是由于VLM的互联网规模预训练语料库与机器人微调数据之间存在分布不匹配造成的。受此观察启发，我们引入了VLM2VLA：一种VLA训练范式，它首先在数据层面通过用自然语言表示低级动作来解决这种不匹配。这种对齐使得训练VLA仅使用低秩适应（LoRA）成为可能，从而最大限度地减少对VLM骨干的修改并避免灾难性遗忘。结果是，VLM可以在机器人远程操作数据上进行微调，而无需根本性地改变底层架构，也无需在互联网规模的VLM数据集上进行昂贵的协同训练。通过广泛的视觉问答（VQA）研究和800多次真实世界机器人实验，我们证明VLM2VLA保留了VLM的核心能力，从而实现对需要开放世界语义推理和多语言指令遵循的新颖任务的零样本泛化。|
|**2025-09-26**|[Task-Adaptive Parameter-Efficient Fine-Tuning for Weather Foundation Models](http://arxiv.org/abs/2509.22020)|null|机器学习的最新进展为天气基础模型 (WFMs) 赋予了在各种下游任务中强大的泛化能力，但其规模扩大带来的计算需求不断增长，日益阻碍了实际部署。当前的参数高效微调 (PEFT) 方法专为视觉或语言任务设计，未能解决天气下游任务的独特挑战，例如变量异构性、分辨率多样性和时空覆盖变化，导致应用于 WFMs 时性能次优。为了弥合这一差距，我们提出了 WeatherPEFT，这是一种新颖的适用于 WFMs 的 PEFT 框架，结合了两种协同创新。首先，在前向传播过程中，任务自适应动态提示 (TADP) 通过内部和外部模式提取，将编码器内部的嵌入权重动态注入到预训练主干网络的输入标记中，从而实现针对特定下游任务的上下文感知特征重校准。此外，在反向传播过程中，随机费雪引导自适应选择 (SFAS) 不仅利用费雪信息来识别和更新最关键的任务参数，从而保留不变的预训练知识，而且引入了随机性以稳定选择过程。我们在三个下游任务上展示了 WeatherPEFT 的有效性和效率，在这些任务中，现有 PEFT 方法与完全微调 (Full-Tuning) 相比存在显著差距，而 WeatherPEFT 使用更少的训练参数实现了与完全微调相当的性能。本工作的代码将发布。|
|**2025-09-26**|[Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations](http://arxiv.org/abs/2509.21870)|null|低秩适配 (LoRA) 是一种广泛应用于大语言模型的参数高效微调方法。然而，其线性本质限制了表达能力。我们提出了 LoRAN，它是 LoRA 的一种非线性扩展，将轻量级变换应用于低秩更新。我们进一步引入了 Sinter，它是一种基于正弦的激活函数，可在不增加参数数量的情况下增加结构化扰动。在文本摘要和分类任务中的实验表明，LoRAN 始终优于 QLoRA。消融研究揭示 Sinter 优于 Sigmoid、ReLU 和 Tanh 等标准激活函数，强调了激活函数设计在低秩微调中的重要性。|
|**2025-09-25**|[MORPH: Shape-agnostic PDE Foundation Models](http://arxiv.org/abs/2509.21670)|null|我们引入了MORPH，一个形状无关的、自回归的偏微分方程（PDEs）基础模型。MORPH建立在卷积视觉Transformer骨干网络之上，能够无缝处理具有不同数据维度（1D-3D）、不同分辨率、以及具有混合标量和矢量分量的多个场的异构时空数据集。该架构结合了（i）分量级卷积，它联合处理标量和矢量通道以捕获局部相互作用；（ii）场间交叉注意力，它在不同物理场之间建模并选择性地传播信息；（iii）轴向注意力，它沿单独的空间和时间轴分解完整的时空自注意力，从而在保持表达能力的同时减少计算负担。我们在一系列多样化的异构PDE数据集上预训练了多个模型变体，并评估了其在各种下游预测任务上的迁移能力。通过全模型微调和参数高效的低秩适配器（LoRA），MORPH在零样本和全样本泛化方面均优于从头开始训练的模型。在广泛的评估中，MORPH达到或超越了强大的基线模型和最新的最先进模型。总的来说，这些能力为从科学观测的异构和多模态性质中学习提供了一个灵活而强大的骨干网络，为可扩展且数据高效的科学机器学习开辟了一条道路。|
|**2025-09-25**|[Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven Framework](http://arxiv.org/abs/2509.21241)|null|低秩适应（LoRA）的广泛采用使得大型语言模型（LLMs）能够以显著的效率获取领域特定知识。然而，理解这种微调机制如何改变模型的结构化推理和语义行为仍然是一个开放性挑战。本工作引入了一个新颖的框架，通过基于知识图谱的反事实来解释微调后的LLMs。具体而言，我们构建了BioToolKG，这是一个生物信息学工具领域的特定异构知识图谱，并设计了一个基于反事实的微调LLMs解释器（CFFTLLMExplainer），该解释器学习图节点和边上的软掩码，以生成最小的结构扰动，从而引起最大的语义分歧。我们的方法联合优化了结构稀疏性和语义分歧，同时施加了保持可解释性的约束，例如熵正则化和边平滑性。我们将此框架应用于一个基于LLaMA的微调LLM，并揭示反事实掩码暴露了模型的结构依赖性，并与LoRA引起的参数偏移对齐。这项工作为微调LLMs的内部机制提供了新见解，并强调反事实图是可解释人工智能的潜在工具。|
|**2025-09-25**|[SiNGER: A Clearer Voice Distills Vision Transformers Further](http://arxiv.org/abs/2509.20986)|null|视觉Transformer被广泛用作视觉基础模型的主干网络，但已知它们会产生降低表示质量的高范数伪影。当知识蒸馏将这些特征传递给学生模型时，高范数伪影主导了目标函数，导致学生模型过度拟合伪影并低估了信息丰富的信号，从而削弱了从更大模型中获得的收益。先前工作曾尝试去除伪影，但在伪影抑制和保留教师模型的信息信号之间遇到了一个固有的权衡。为了解决这个问题，我们引入了奇异零空间引导的能量重分配 (SiNGER)，这是一种新颖的蒸馏框架，能够在抑制伪影的同时保留信息信号。其核心思想是原则性的教师特征精炼：在精炼过程中，我们利用零空间引导的扰动，在抑制伪影的同时保留信息。随后，精炼后的教师特征被蒸馏到学生模型。我们使用基于LoRA的适配器高效实现了这一扰动，仅需要最小的结构修改。大量实验表明，SiNGER持续改进了学生模型，在多个下游任务中取得了最先进的性能，并产生了更清晰、更可解释的表示。|
|**2025-09-25**|[MemLens: Uncovering Memorization in LLMs with Activation Trajectories](http://arxiv.org/abs/2509.20909)|null|大语言模型（LLMs）通常在AIME和Math500等具有挑战性的基准上进行评估，这些基准容易受到数据污染并存在模型记忆的风险。现有的检测方法主要依赖于表面词汇重叠和困惑度，泛化能力较差，在遇到隐式污染数据时会显著退化。在本文中，我们提出了MemLens（一种用于记忆检测的激活透镜），通过分析生成过程中数字标记的概率轨迹来检测模型记忆。我们的方法揭示，污染样本表现出“捷径”行为，在模型的早期层中以高置信度锁定答案，而干净样本则在模型的整个深度中显示出更渐进的证据积累。我们观察到，污染样本和干净样本表现出明显且良好分离的推理轨迹。为了进一步验证这一点，我们通过LoRA微调将精心设计的样本注入模型，并观察到与自然污染数据中相同的轨迹模式。这些结果提供了强有力的证据，表明MemLens捕获了真正的记忆信号而非虚假关联。|
|**2025-09-25**|[DAC-LoRA: Dynamic Adversarial Curriculum for Efficient and Robust Few-Shot Adaptation](http://arxiv.org/abs/2509.20792)|null|视觉-语言模型（VLMs）是自动驾驶、医疗诊断和内容审核等关键应用的基础。虽然像LoRA这样的参数高效微调（PEFT）方法使其能够高效适应专业任务，但这些模型仍然容易受到可能危及安全关键决策的对抗性攻击。CLIP作为众多下游VLM的骨干模型，是一个高价值目标，其脆弱性可能在多模态AI生态系统中产生连锁反应。我们提出了动态对抗课程DAC-LoRA，这是一个将对抗训练整合到PEFT中的新颖框架。我们方法的核心原理，即逐步增强攻击难度的智能课程，具有通用性，并可潜在地应用于任何迭代攻击方法。在第一阶平稳条件（FOSC）和受TRADES启发的损失函数的指导下，DAC-LoRA在对抗鲁棒性方面取得了显著提升，而没有显著损害干净准确性。我们的工作提出了一种有效、轻量且广泛适用的方法，旨在证明DAC-LoRA框架可以轻松集成到标准PEFT流程中以显著增强鲁棒性。|
|**2025-09-25**|[SFT Doesn't Always Hurt General Capabilities: Revisiting Domain-Specific Fine-Tuning in LLMs](http://arxiv.org/abs/2509.20758)|null|有监督微调（SFT）在领域特定数据集上是使大语言模型（LLMs）适应专门任务的常用方法，但通常认为这会损害其通用能力。在这项工作中，我们重新审视了这种权衡，并提出了经验和理论见解。首先，我们表明SFT并非总是有害：使用较小的学习率可以大幅缓解通用性能下降，同时保持可比的目标领域性能。随后，我们提供了一项理论分析，解释了这些现象，并进一步推动了一种新方法——词元自适应损失重加权（TALR）。在此基础上，鉴于仅靠较小的学习率并不能完全消除所有情况下的通用性能下降，我们评估了一系列减少通用能力损失的策略，包括L2正则化、LoRA、模型平均、FLOW以及我们提出的TALR。实验结果表明，虽然没有哪种方法能完全消除这种权衡，但TALR在平衡领域特定收益和通用能力方面始终优于这些基线方法。最后，我们将研究结果提炼成将大语言模型适应到新领域的实用指南：(i) 使用较小的学习率以实现有利的权衡；(ii) 当需要更强的平衡时，采用TALR作为一种有效策略。|
|**2025-09-24**|[TianHui: A Domain-Specific Large Language Model for Diverse Traditional Chinese Medicine Scenarios](http://arxiv.org/abs/2509.19834)|null|中医领域特定大型语言模型在研究环境中面临适应性受限、评估数据集不足和计算资源有限等局限。本研究介绍了天汇（TianHui），一个通过上下文数据整合和领域知识融合构建的专门中医大型语言模型。我们构建了一个大规模中医语料库（包含0.97GB无监督数据和611,312个问答对），并采用了结合QLoRA、DeepSpeed Stage 2和Flash Attention 2的两阶段训练策略。在12个基准测试中的评估显示，天汇在六个数据集（APQ、TCMCD、HFR、HCCA、DHPE、TLAW）的所有指标上均位列前三，并在另外六个数据集（TCMEE、APR、GCPMI、TCMKQA、TCMRC、ADTG）中取得了领先结果。最佳配置被确定为LoRA秩=128，alpha=256，迭代周期=4，dropout=0.2，最大长度=2048。天汇实现了中医知识的系统化保存和可扩展应用。所有资源均已开源。|
|**2025-09-23**|[Analysis on distribution and clustering of weight](http://arxiv.org/abs/2509.19122)|**[link](https://github.com/sayantann11/all-classification-templetes-for-ML)**|大语言模型架构和参数特性的研究仍然是当前的热点。本文关注权重的特性，并以此分析模型之间的相关性和差异。提出了标准差向量和聚类向量两种向量来描述模型的特征。在第一种情况下，假设权重服从正态分布。将投影矩阵的标准差值进行归一化，形成标准差向量，用以表示模型的分布特性。在第二种情况下，从每个权重投影矩阵中提取奇异值，并通过K-Means算法进行分组。将同类型矩阵的分组数据组合成聚类向量，用以表示模型权重的相关特性。研究表明，这两种向量能有效区分不同模型，并清楚地显示同一系列模型之间的相似性。此外，在使用不同数据集和模型进行LoRA微调后发现，标准差向量表示的权重分布直接受数据集影响，但聚类向量表示的不同权重之间的相关性不受影响，并与预训练模型保持高度一致。|
|**2025-09-23**|[Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning](http://arxiv.org/abs/2509.18942)|**[link](https://github.com/zzm-black/DEAL-Continuous-Low-Rank-Fine-Tuning)**|大语言模型（LLM）的最新进展强调了微调（FT）技术在使LLM适应特定任务中的关键作用，尤其是在从头开始重新训练在计算上不可行时。微调使LLM能够利用任务或领域特定数据，从而生成能更有效满足目标应用需求的模型。然而，传统的微调方法通常面临灾难性遗忘和次优数据效率的问题，这限制了它们的实际应用性。为解决这些挑战，本文提出DEAL，一个将低秩适应（LoRA）与连续微调策略相结合的新颖框架。通过引入知识保留和自适应参数更新模块，该框架缓解了现有微调方法的局限性，同时在隐私保护设置中保持了效率。在15个不同数据集上的实验表明，DEAL持续优于基线方法，在任务准确性和资源效率方面取得了显著提升。这些发现表明我们的方法通过提高任务性能同时提升资源效率，从而推动LLM中持续适应的潜力。|
|**2025-09-23**|[Memory in Large Language Models: Mechanisms, Evaluation and Evolution](http://arxiv.org/abs/2509.18868)|null|在统一操作定义下，我们将大语言模型（LLM）记忆定义为在预训练、微调或推理期间写入的持久状态，该状态随后可被寻址并稳定地影响输出。我们提出了一种四部分分类法（参数式、上下文式、外部式、程序式/情景式）和一个记忆四元组（位置、持久性、写入/访问路径、可控性）。我们通过“写入 -> 读取 -> 抑制/更新”链条将机制、评估和治理联系起来。为避免在异构设置中出现扭曲的比较，我们采用了一种三设置协议（仅参数式、离线检索、在线检索），该协议将能力与相同数据和时间线上的信息可用性解耦。在此基础上，我们构建了一个分层评估体系：参数式（闭卷回忆、编辑差异、记忆/隐私）、上下文式（位置曲线和序列中部下降）、外部式（答案正确性与片段归因/忠实度）以及程序式/情景式（跨会话一致性和时间线重放，E MARS+）。该框架集成了时间治理和泄露审计（新鲜度命中、过时答案、拒绝响应片段），并通过评估者间一致性以及带有多重比较校正的配对测试来报告不确定性。针对更新和遗忘，我们提出了DMM Gov：协调DAPT/TAPT、PEFT、模型编辑（ROME、MEND、MEMIT、SERAC）和RAG，以形成一个可审计的循环，涵盖准入阈值、部署、监控、回滚和变更审计，并明确了及时性、冲突处理和长期一致性的规范。最后，我们提出了四个可测试命题：最小可识别性；最小评估卡；具有可验证遗忘的因果约束编辑；以及小窗口重放检索何时优于超长上下文阅读。这为研究和部署提供了一个可复现、可比较、可治理的坐标系。|
|**2025-09-23**|[HyperAdapt: Simple High-Rank Adaptation](http://arxiv.org/abs/2509.18629)|null|基础模型在各种任务中表现出色，但将它们适应到专用应用通常需要微调，这种方法是内存和计算密集型的。参数高效微调（PEFT）方法通过仅更新一小部分权重来缓解这一问题。在本文中，我们引入了HyperAdapt，这是一种参数高效微调方法，与LoRA等最先进方法相比，它显著减少了可训练参数的数量。具体而言，HyperAdapt通过对预训练权重矩阵应用行向和列向缩放（通过对角矩阵）来适应模型，从而产生高秩更新，而对于一个 $n \times m$矩阵，仅需要$n+m$ 个可训练参数。理论上，我们建立了HyperAdapt更新的秩的上限，经验上，我们证实它在模型层中始终能产生高秩变换。在GLUE、算术推理和常识推理基准上，使用高达140亿参数的模型进行的实验表明，HyperAdapt的性能与完全微调和最先进的PEFT方法相当或接近，同时使用的可训练参数数量少了几个数量级。|
|**2025-09-22**|[SEQR: Secure and Efficient QR-based LoRA Routing](http://arxiv.org/abs/2509.18093)|null|低秩适配（LoRA）已成为大语言模型参数高效微调的标准技术，使得为特定任务或领域创建大型LoRA库成为可能。然而，对于给定输入如何有效选择正确的LoRA适配器仍然是一个挑战，尤其是在安全环境中，路由器的监督训练可能引发隐私问题。受先前方法的启发，我们将无监督LoRA路由的目标形式化为激活范数最大化，并为此提供了一个理论分析框架。我们证明了激活范数的鉴别能力，并引入了SEQR，这是一种旨在最大化效率同时提供严格路由保证的无监督LoRA路由算法。SEQR可证明地以显著更高的效率识别范数最大化适配器，使其成为动态LoRA组合的高度可扩展和有效的解决方案。我们通过实验验证了我们的结果，这些实验证明了多任务性能和效率的提升。|
|**2025-09-22**|[MapCoder-Lite: Squeezing Multi-Agent Coding into a Single Small LLM](http://arxiv.org/abs/2509.17489)|null|大语言模型 (LLMs) 已将代码生成从单函数任务推进到编程竞赛问题，但现有的多智能体解决方案要么依赖于昂贵的大规模 (超过300亿参数) 模型，要么在缩小到小型开源模型时性能崩溃。我们提出了 MapCoder-Lite，它仅使用秩为32、特定于角色的LoRA适配器 (额外参数少于3%)，将一个70亿参数模型升级为四个角色专业化智能体：检索器、规划器、编码器和调试器。三种轻量级技术使其成为可能：(i) 从强大LLM中进行的轨迹蒸馏解决了检索和调试中的格式脆弱性问题；(ii) 监督者引导的纠正增强了规划和编码智能体；(iii) 智能体级别的LoRA微调实现了内存高效的专业化。在xCodeEval、APPS和CodeContests上的综合评估表明，MapCoder-Lite 将xCodeEval准确率提高了一倍以上 (从13.2%提升至28.3%)，消除了所有格式错误，并将与一个320亿参数基线的差距缩小到六个百分点之内，同时将GPU内存和令牌生成时间减少了四倍。这些结果表明，仔细的智能体级别微调在小型语言模型上释放了高质量的多智能体编码能力。|
|**2025-09-23**|[QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models](http://arxiv.org/abs/2509.17428)|null|大语言模型（LLMs）高效部署的需求推动了对量化（可降低推理成本）和参数高效微调（PEFT，可减少训练开销）的关注。这促使了量化感知型PEFT的开发，以生成准确而高效的量化模型。在这种背景下，在微调之前减少量化误差对于实现高模型精度至关重要。然而，现有依赖低秩适应的方法存在表示能力有限的问题。近期基于傅里叶相关变换（FT）的适配器比低秩适配器提供了更强的表示能力，但将其直接集成到量化模型中往往会导致误差减少效果不佳并增加计算开销。为了克服这些局限性，我们提出了QWHA，该方法通过采用Walsh-Hadamard变换（WHT）作为变换核，并结合一种新颖的适配器初始化方案（该方案包含自适应参数选择和值细化），将基于FT的适配器集成到量化模型中。我们证明了QWHA能有效缓解量化误差，同时便于微调，并且其设计大幅降低了计算成本。实验结果表明，QWHA在低比特量化精度方面持续优于基线方法，并相对于现有基于FT的适配器实现了显著的训练加速。代码已在https://github.com/vantaa89/qwha提供。|
|**2025-09-22**|[EpiCache: Episodic KV Cache Management for Long Conversational Question Answering](http://arxiv.org/abs/2509.17396)|null|大语言模型（LLMs）的近期进展扩展了上下文长度，使助手能够维持长对话历史，以生成连贯、个性化的回复。然而，这种能力依赖于键值（KV）缓存，其内存随对话长度线性增长，并在严格的资源限制下迅速占据主导地位。减少这种开销的一个活跃研究方向是KV缓存压缩，它旨在限制缓存大小同时保持准确性。然而，现有方法面临两个主要限制：(i) 在全上下文预填充后驱逐条目会导致无限制的峰值内存，以及 (ii) 依赖于查询的驱逐将缓存范围缩小到单个查询，导致多轮对话中准确性下降。我们引入了EpiCache，这是一个在固定内存预算下用于长对话问答（LongConvQA）的无需训练的KV缓存管理框架。EpiCache通过块级预填充限制缓存增长，并通过情景式KV压缩保留主题相关的上下文，该压缩将对话历史聚类成连贯的情景并应用情景特定的KV缓存驱逐。我们进一步设计了一种自适应的逐层预算分配策略，该策略衡量每个层对驱逐的敏感性，并相应地在各层之间分配内存预算。在三个LongConvQA基准测试中，EpiCache将准确性比近期基线提高了高达40%，在4-6倍压缩下保持接近完整的KV准确性，并将延迟和内存分别降低了高达2.4倍和3.5倍，从而在严格的资源限制下实现了高效的多轮交互。|
|**2025-09-21**|[Parameter-efficient fine-tuning (PEFT) of Vision Foundation Models for Atypical Mitotic Figure Classification](http://arxiv.org/abs/2509.16935)|null|非典型有丝分裂像（AMFs）是罕见的异常细胞分裂，与肿瘤侵袭性和不良预后相关。由于微妙的形态学线索、类别不平衡以及病理学家之间观察者间差异，它们的检测仍然是一个重大挑战。MIDOG 2025挑战赛引入了一个专门用于非典型有丝分裂分类的赛道，从而能够系统地评估深度学习方法。在本研究中，我们探索了使用大型视觉基础模型（包括Virchow、Virchow2和UNI），并结合低秩适应（LoRA）进行参数高效微调。我们通过不同LoRA秩以及随机和基于组的数据划分进行了大量实验，以分析在不同条件下的鲁棒性。我们的最佳方法是结合LoRA秩为8的Virchow模型和三折交叉验证集成，在初步测试集上实现了88.37%的平衡准确率，在挑战赛排行榜上并列第9位。这些结果突显了基础模型结合高效适应策略在非典型有丝分裂分类方面的潜力，同时也强调了在特异性和域泛化方面进行改进的必要性。|
|**2025-09-19**|[BEFT: Bias-Efficient Fine-Tuning of Language Models](http://arxiv.org/abs/2509.15974)|null|对所有偏置项进行微调在各种参数高效微调（PEFT）技术中脱颖而出，这归因于其开箱即用性和具有竞争力的性能，尤其是在低数据量场景下。仅微调偏置项有潜力实现前所未有的参数效率。然而，微调不同偏置项（即查询、键或值投影中的偏置项）与下游性能之间的联系仍不明确。现有方法，例如基于偏置变化幅度或经验费雪信息的方法，为选择特定的偏置项以实现有效微调提供的指导有限。在本文中，我们提出了一种选择要微调的偏置项的方法，构成了我们偏置高效微调（BEFT）的基础。我们广泛评估了我们的偏置高效方法，并将其与其他偏置选择方法进行了对比，涵盖了从1.1亿到67亿参数的、跨越仅编码器和仅解码器架构的各种大型语言模型（LLMs）。我们的结果表明，在包括分类、多项选择和生成任务在内的多种下游任务上，我们的偏置高效方法具有有效性和优越性。|
|**2025-09-19**|[Distribution-Aligned Decoding for Efficient LLM Task Adaptation](http://arxiv.org/abs/2509.15888)|null|即使使用参数高效微调（PEFT），将亿参数语言模型适配到下游任务仍然成本高昂。我们将任务适配重新定义为输出分布对齐：目标是在解码过程中直接将输出分布引导至任务分布，而不是通过权重更新间接实现。基于这种观点，我们引入了引导向量解码（SVD），这是一种轻量级、兼容PEFT且具有理论基础的方法。我们首先进行一个简短的热启动微调，并从热启动模型和预训练模型输出分布之间的库尔巴克-莱布勒（KL）散度梯度中提取一个任务感知的引导向量。随后，该引导向量被用于引导解码过程，以将模型的输出分布引导至任务分布。我们理论上证明了SVD与全量微调的梯度步长一阶等价，并推导出了引导向量强度的全局最优解。在三个任务和九个基准测试中，SVD与四种标准PEFT方法结合，将多项选择准确率提高了多达5个百分点，将开放式真实性提高了2个百分点，并在常识数据集上取得了类似的提升（1-2个百分点），且除了PEFT适配器之外不增加任何可训练参数。因此，SVD为大语言模型提供了一条轻量级、有理论基础的途径，以实现更强的任务适配。|
|**2025-09-19**|[Mamba-2 audio captioning: design space exploration and analysis](http://arxiv.org/abs/2509.15680)|null|我们提出了一种基于Mamba-2大语言模型骨干的音频字幕生成模型，Mamba-2是一种最先进（SOTA）的状态空间模型（SSM）。我们系统地探索了设计空间，包括LLM尺寸、LoRA秩和连接器设计，这些设计利用了Mamba-2在序列长度方面的线性时间复杂度。在各项基准测试中，与在相同数据集上训练的更大语言模型相比，我们的模型在使用了更少参数的情况下，仍实现了强大的字幕生成性能。我们首次深入分析了LLM参数数量、音频编码器微调策略、音频特征多样性以及不同的特征降维或扩展技术如何影响性能。|
|**2025-09-19**|[UNIV: Unified Foundation Model for Infrared and Visible Modalities](http://arxiv.org/abs/2509.15642)|null|联合可见光和红外感知的需求正在快速增长，尤其是在各种天气条件下实现鲁棒性能方面。尽管针对可见光和红外数据的预训练模型在各自领域表现出色，但在多模态场景中（例如配备这两种传感器的自动驾驶汽车）往往表现不佳。为解决这一挑战，我们提出了一种受生物学启发的红外与可见光模态统一基础模型 (UNIV)，该模型具有两项关键创新。首先，我们引入了逐块跨模态对比学习 (PCCL)，这是一种注意力引导的蒸馏框架，它模仿视网膜水平细胞的侧向抑制作用，能够在与任何基于Transformer的架构兼容的同时，实现有效的跨模态特征对齐。其次，我们的双知识保留机制模拟视网膜双极细胞的信号路由——结合LoRA适配器（增加2%的参数）和同步蒸馏以防止灾难性遗忘，从而复制视网膜的明视（锥体细胞驱动）和暗视（杆体细胞驱动）功能。为支持跨模态学习，我们引入了MVIP数据集，这是迄今为止最全面的可见光-红外基准。它包含98,992对精确对齐的图像，涵盖各种场景。大量实验表明，UNIV在红外任务上表现优越（语义分割中mIoU提升1.7，目标检测中mAP提升0.7），同时在可见光RGB任务上保持了99%以上的基线性能。我们的代码可在 https://github.com/fangyuanmao/UNIV 获取。|
|**2025-09-18**|[Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding](http://arxiv.org/abs/2509.15476)|null|讽刺检测在自然语言理解中仍然是一个挑战，因为讽刺意图通常依赖于文本、语音和视觉等微妙的跨模态线索。尽管先前的工作主要集中在文本或视觉-文本讽刺上，但全面的音频-视觉-文本讽刺理解仍未得到充分探索。在本文中，我们系统地评估了大语言模型 (LLMs) 和多模态大语言模型在零样本、少样本和LoRA微调设置下，对英语 (MUStARD++) 和中文 (MCSD 1.0) 讽刺检测的性能。除了直接分类，我们还探索将模型用作特征编码器，并通过协同门控融合模块整合它们的表示。实验结果表明，基于音频的模型实现了最强的单模态性能，而文本-音频和音频-视觉组合则优于单模态和三模态模型。此外，Qwen-Omni等多模态大语言模型展现出有竞争力的零样本和微调性能。我们的研究结果强调了多模态大语言模型在跨语言、音频-视觉-文本讽刺理解方面的潜力。|
|**2025-09-18**|[Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation](http://arxiv.org/abs/2509.15225)|null|我们提出VocAlign，一种专门为开放词汇语义分割中的视觉语言模型（VLMs）设计的无源域适应新框架。我们的方法采用学生-教师范式，并辅以词汇对齐策略，通过引入额外的类别概念来改进伪标签生成。为确保效率，我们使用低秩适应（LoRA）来微调模型，在保留其原始能力的同时最大限度地减少计算开销。此外，我们为学生模型提出了一种Top-K类别选择机制，该机制显著减少了内存需求，同时进一步提高了适应性能。我们的方法在CityScapes数据集上实现了mIoU显著的6.11点提升，并在零样本分割基准测试中表现出卓越性能，为开放词汇设置下的无源适应设定了新标准。|
|**2025-09-18**|[Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning](http://arxiv.org/abs/2509.15087)|null|大型语言模型（LLMs）在各种任务中展现出令人印象深刻的能力，但为领域特定应用微调LLMs通常需要大量可能分布在多个组织中的领域特定数据。联邦学习（FL）提供了一种隐私保护解决方案，但在应用于LLMs时面临计算约束的挑战。低秩适应（LoRA）作为一种参数高效的微调方法应运而生，但单个LoRA模块在处理不同领域的异构数据时往往表现不佳。本文解决了联邦LoRA微调中的两个关键挑战：1. 确定异构客户端之间LoRA专家的最佳数量和分配，以及2. 使客户端能够根据其特定数据特征选择性地利用这些专家。我们提出了FedLEASE（联邦自适应LoRA专家分配与选择），这是一种新颖的框架，它根据表示相似性自适应地聚类客户端，以分配和训练领域特定的LoRA专家。它还引入了一种自适应的top- $M$ 专家混合机制，允许每个客户端选择所利用专家的最佳数量。我们在各种基准数据集上进行的大量实验表明，FedLEASE在异构客户端设置中显著优于现有的联邦微调方法，同时保持了通信效率。|
|**2025-09-18**|[Explicit vs. Implicit Biographies: Evaluating and Adapting LLM Information Extraction on Wikidata-Derived Texts](http://arxiv.org/abs/2509.14943)|null|文本隐含性一直是自然语言处理 (NLP) 中的一个难题，传统方法依赖于显式陈述来识别实体及其关系。例如，从句子“Zuhdi attends church every Sunday”中，Zuhdi 与基督教之间的关系对人类读者来说是显而易见的，但当需要自动推断时，这便提出了挑战。大语言模型 (LLMs) 已被证明在文本理解和信息抽取 (IE) 等NLP下游任务中表现出色。本研究考察了文本隐含性如何影响预训练LLM（包括 LLaMA 2.3、DeepSeekV1 和 Phi1.5）在IE任务中的表现。我们生成了两个包含1万条隐含和显式传记信息表达的合成数据集，以衡量其对LLM性能的影响，并分析对隐含数据进行微调是否能提高其在隐含推理任务中的泛化能力。本研究展示了一项关于LLM在IE中内部推理过程的实验，特别是在处理隐含和显式上下文方面。结果表明，使用LoRA（低秩适应）对LLM模型进行微调可以提高其从隐含文本中抽取信息的性能，从而有助于提升模型的解释性和可靠性。|
|**2025-09-18**|[LLM4MG: Adapting Large Language Model for Multipath Generation via Synesthesia of Machines](http://arxiv.org/abs/2509.14711)|null|基于机器联觉 (SoM)，大语言模型 (LLM) 首次被用于多径生成 (LLM4MG)。考虑到典型的第六代 (6G) 车-基础设施 (V2I) 场景，本文构建了一个新的多模态感知-通信数据集，命名为 SynthSoM-V2I，其中包括信道多径信息、毫米波 (mmWave) 雷达感知数据、RGB-D 图像以及光探测与测距 (LiDAR) 点云。基于 SynthSoM-V2I 数据集，所提出的 LLM4MG 利用大语言模型 Meta AI (LLaMA) 3.2 通过多模态感知数据进行多径生成。所提出的 LLM4MG 通过特征提取和融合网络，将多模态特征空间与 LLaMA 语义空间对齐。为进一步实现从预训练 LLaMA 到通过多模态感知数据进行多径生成的通用知识迁移，本文采用了低秩适应 (LoRA) 参数高效微调和传播感知提示工程。仿真结果表明，所提出的 LLM4MG 在视距 (LoS)/非视距 (NLoS) 分类方面优于传统的基于深度学习的方法，准确率达到 92.76%；在多径功率/时延生成精度方面，归一化均方误差 (NMSE) 分别为 0.099/0.032；并且在跨车辆交通密度 (VTD)、跨频段和跨场景泛化方面也表现出色。通过真实世界泛化验证了所提出的 LLM4MG 的实用性。通过信道容量比较，也证明了高精度多径生成对于系统设计的必要性。|
|**2025-09-18**|[Catch Me If You Can? Not Yet: LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors](http://arxiv.org/abs/2509.14543)|**[link](https://github.com/jaaack-wang/llms-implicit-writing-styles-imitation)**|随着大型语言模型（LLM）日益融入个人写作工具，一个关键问题浮现：LLM能否仅凭少量示例忠实模仿个体的写作风格？个人风格通常是微妙且隐性的，这使得通过提示（prompt）难以明确指定，但对于用户对齐的生成至关重要。本工作对最先进LLM模仿个人写作风格的能力进行了全面评估，通过少量用户原创样本进行上下文学习（in-context learning）。我们引入了一套互补的指标——包括作者归属、作者验证、风格匹配和AI检测——以稳健地评估风格模仿能力。我们的评估涵盖每个模型超过40000次生成，跨越新闻、电子邮件、论坛和博客等领域，包含了来自400多位真实作者的写作样本。结果表明，尽管LLM可以在新闻和电子邮件等结构化格式中近似用户风格，但它们在博客和论坛中处理细致入微、非正式的写作时表现不佳。对各种提示策略（例如演示数量）的进一步分析揭示了有效个性化中的关键局限。我们的发现突出了个性化LLM适应方面的一个根本性差距，以及对改进技术以支持隐式、风格一致生成的需求。为了促进未来研究和可复现性，我们开源了数据和代码。|
|**2025-09-18**|[CLAIP-Emo: Parameter-Efficient Adaptation of Language-supervised models for In-the-Wild Audiovisual Emotion Recognition](http://arxiv.org/abs/2509.14527)|null|真实场景下的视听情感识别（AVER）仍受姿态变化、遮挡和背景噪声的阻碍。现有方法主要依赖于大规模领域特定预训练，这成本高昂且通常与真实世界的情感数据不匹配。为解决此问题，我们提出了CLAIP-Emo，一个模块化框架，它将真实场景下的AVER重新定义为语言监督基础模型（CLIP/CLAP）的参数高效适应。具体而言，它（i）通过冻结CLIP/CLAP骨干网络并通过LoRA进行情感导向适应（更新总参数的\ensuremath{\le}4.0%）来保留语言监督先验知识，（ii）非对称地分配时间建模，采用轻量级Transformer处理视觉动态，同时对音频韵律应用均值池化，以及（iii）应用一个简单的融合头进行预测。在DFEW和MAFW数据集上，CLAIP-Emo (ViT-L/14) 仅用8M训练参数就达到了80.14%和61.18%的加权平均召回率，创造了新的最先进水平。我们的发现表明，语言监督基础模型的参数高效适应为真实场景下的AVER提供了一种可扩展的替代方案，以替代领域特定预训练。代码和模型将在此处提供：\href{https://github.com/MSA-LMC/CLAIP-Emo}{https://github.com/MSA-LMC/CLAIP-Emo}。|
|**2025-09-17**|[Large Language Model-Empowered Decision Transformer for UAV-Enabled Data Collection](http://arxiv.org/abs/2509.13934)|null|部署无人机 (UAV) 从空间分布设备进行可靠且节能的数据收集，在支持多样化的物联网 (IoT) 应用方面具有巨大潜力。然而，无人机有限的续航能力和通信范围使得智能轨迹规划成为必要。尽管强化学习 (RL) 已被广泛探索用于无人机轨迹优化，但其交互性在真实世界环境中带来了高成本和高风险。离线 RL 缓解了这些问题，但仍易受不稳定训练影响，并高度依赖专家质量数据集。为解决这些挑战，我们提出了一个无人机轨迹规划与资源分配联合问题，以最大化数据收集的能源效率。资源分配子问题首先被转化为等价的线性规划公式，并以多项式时间复杂度得到最优解。随后，我们提出了一个大型语言模型 (LLM) 赋能的批评者正则化决策 Transformer (DT) 框架，称之为 LLM-CRDT，以学习有效的无人机控制策略。在 LLM-CRDT 中，我们整合了批评者网络来正则化 DT 模型训练，从而将 DT 的序列建模能力与基于批评者的价值指导相结合，以实现从次优数据集中学习有效策略。此外，为缓解 Transformer 模型对数据的高需求特性，我们采用预训练 LLM 作为 DT 模型的 Transformer 主干，并采纳参数高效微调策略 LoRA，从而在小规模数据集和低计算开销下实现对无人机控制任务的快速适应。大量仿真表明，LLM-CRDT 优于基准在线和离线 RL 方法，与当前最先进的 DT 方法相比，能源效率提高高达 36.7%。|
|**2025-09-17**|[Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection](http://arxiv.org/abs/2509.13878)|null|Wav2Vec2等基础模型在语音任务（包括音频深度伪造检测）中的表征学习方面表现出色。然而，在对一组固定的真实和伪造音频片段进行微调后，它们通常无法泛化到训练中未出现的新颖深度伪造方法。为解决此问题，我们提出了一种LoRA专家混合方法，该方法将多个低秩适配器（LoRA）集成到模型的注意力层中。一种路由机制选择性地激活专门的专家，从而增强了对不断演变的深度伪造攻击的适应性。实验结果表明，我们的方法在域内和域外场景中均优于标准微调，相对于基线模型降低了等错误率。值得注意的是，我们最佳的MoE-LoRA模型将平均域外EER从8.55%降低到6.08%，证明了其在实现可泛化的音频深度伪造检测方面的有效性。|
|**2025-09-18**|[Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications](http://arxiv.org/abs/2509.13775)|null|本文探讨了我们对阿拉伯方言识别 (ADI) 中不同数据高效和参数高效方法的探索。具体来说，我们研究了各种软提示策略，包括 prefix-tuning、prompt-tuning、P-tuning 和 P-tuning V2，以及 LoRA 重参数化。对于数据高效策略，我们分析了结合零样本和少样本推理的硬提示，以分析大型语言模型 (LLMs) 的方言识别能力。对于参数高效的 PEFT 方法，我们使用阿拉伯语专用的编码器模型在几个主要数据集上进行了实验。我们还在开源的仅解码器模型、一个通用多语言模型 (Phi-3.5) 和一个阿拉伯语专用模型 (SILMA) 上分析了 n-shot 推理。我们观察到，LLMs 在少样本或零样本设置中通常难以区分方言细微差别。软提示编码器变体表现更好，而基于 LoRA 的微调模型表现最佳，甚至超越了完全微调。|
|**2025-09-17**|[Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning](http://arxiv.org/abs/2509.13624)|null|大语言模型正越来越多地部署到各种应用中。这通常包括大语言模型在训练期间未曾遇到的任务。这意味着枚举并获取所有任务的高质量训练数据是不可行的。因此，我们通常需要依赖于使用具有不同特征的数据集的迁移学习，并预测分布外请求。受此实际需求的启发，我们提出了一个分析框架，通过构建迁移学习矩阵和降维来剖析这些跨任务交互。我们训练并分析了10个模型，以识别潜在能力（例如，推理、情感分类、自然语言理解、算术）并发现迁移学习的副作用。我们的发现揭示，性能提升往往难以用基于表层数据集相似性或源数据质量的解释来阐明。相反，源数据集的隐藏统计因素，例如类别分布和生成长度倾向性，以及特定的语言特征，实际上更具影响力。这项工作为理解迁移学习的复杂动态提供了见解，为更可预测和更有效的大语言模型适应铺平了道路。|

## 大模型强化学习

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2025-09-26**|[CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning](http://arxiv.org/abs/2509.22647)|**[link](https://github.com/InternLM/CapRL)**|图像字幕生成是一项连接视觉和语言领域的基本任务，在预训练大型视觉-语言模型（LVLMs）中扮演着关键角色。当前最先进的字幕生成模型通常通过有监督微调（SFT）进行训练，这种范式依赖于由人类或专有模型标注的昂贵且不可扩展的数据。这种方法经常导致模型记忆特定的真实答案，限制了它们的泛化能力以及生成多样化、创意性描述的能力。为了克服SFT的局限性，我们提出将可验证奖励强化学习（RLVR）范式应用于开放式图像字幕生成任务。然而，一个主要挑战是为构成“好”字幕的固有主观性设计一个客观的奖励函数。我们引入了字幕强化学习（CapRL），这是一种新颖的训练框架，通过其效用重新定义字幕质量：一个高质量的字幕应该使非视觉语言模型能够准确回答关于对应图像的问题。CapRL采用解耦的两阶段流程，其中一个LVLM生成字幕，客观奖励则来源于一个独立的、无视觉的LLM仅基于该字幕回答多项选择题的准确性。作为首次将RLVR应用于主观图像字幕生成任务的研究，我们证明CapRL显著提升了多种设置下的性能。在由CapRL-3B标注的CapRL-5M字幕数据集上进行预训练，在12个基准测试中带来了显著的提升。此外，在用于字幕质量评估的Prism框架内，CapRL取得了与Qwen2.5-VL-72B相当的性能，同时平均超越基线8.4%。代码可在此处获取：https://github.com/InternLM/CapRL。|
|**2025-09-26**|[WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning](http://arxiv.org/abs/2509.22644)|null|基于大型语言模型（LLM）的智能体系统在仓库级别的代码生成任务上展现了令人印象深刻的性能。然而，对于诸如网站代码库生成这类严重依赖视觉效果和用户交互反馈的任务，当前的代码智能体仅依赖简单的代码执行进行反馈和验证。这种方法未能捕捉到所生成代码的实际质量。在本文中，我们提出了WebGen-Agent，这是一种新颖的网站生成智能体，它利用全面且多层次的视觉反馈来迭代地生成和完善网站代码库。视觉语言模型（VLM）会生成关于网站屏幕截图和GUI智能体测试的详细且富有表达力的文本描述和建议，并提供量化其质量的分数。屏幕截图和GUI智能体分数进一步与回溯和择优机制相结合，从而提升了智能体的性能。利用WebGen-Agent工作流程中固有的准确视觉分数，我们进一步引入了带有屏幕截图和GUI智能体反馈的Step-GRPO，以提高LLM作为WebGen-Agent推理引擎的能力。通过将每一步的屏幕截图和GUI智能体分数作为Step-GRPO中的奖励，我们提供了一个密集且可靠的过程监督信号，这有效地提高了模型的网站生成能力。在WebGen-Bench数据集上，WebGen-Agent将Claude-3.5-Sonnet的准确率从26.4%提高到51.9%，并将其外观分数从3.0提高到3.9，超过了此前最先进的智能体系统。此外，我们的Step-GRPO训练方法将Qwen2.5-Coder-7B-Instruct的准确率从38.9%提高到45.4%，并将外观分数从3.4提高到3.7。|
|**2025-09-26**|[Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback](http://arxiv.org/abs/2509.22633)|null|人类反馈强化学习 (RLHF) 通过从人类偏好数据中学习奖励模型，然后优化策略以偏好期望的响应，已成为使大型语言模型 (LLM) 与人类偏好对齐的核心范式。在本文中，我们研究了在线RLHF的探索原则，其目标是以数据高效的方式自适应地收集新的偏好数据，以改进奖励模型和策略。通过检查现有的基于乐观主义的探索算法，我们发现其采样协议存在一个缺陷：它们倾向于收集未能减少奖励差异中最具信息量的不确定性的比较，并且我们证明了下界，表明此类方法在指数级长的周期内可能导致线性遗憾。受此启发，我们提出了一种新的探索方案，该方案将偏好查询导向减少与策略改进最相关的奖励差异中的不确定性。在RLHF的多臂老虎机模型下，我们建立了阶数为 $T^{(\beta+1)/(\beta+2)}$ 的遗憾界，其中 $\beta>0$ 是一个用于平衡奖励最大化与减轻分布漂移的超参数。据我们所知，这是第一个遗憾随所有模型参数呈多项式增长的在线RLHF算法。|
|**2025-09-26**|[UML-CoT: Structured Reasoning and Planning with Unified Modeling Language for Robotic Room Cleaning](http://arxiv.org/abs/2509.22628)|null|思维链 (CoT) 提示可提升大型语言模型 (LLMs) 的推理能力，但其对非结构化文本的依赖限制了在具身任务中的可解释性和可执行性。先前工作探索了使用场景图或逻辑图的结构化思维链，但这些方法仍存在根本性限制：它们仅建模低阶关系，缺乏继承或行为抽象等构造，并且未提供序列式或条件式规划的标准化语义。我们提出了 UML-CoT，一个结构化推理与规划框架，它利用统一建模语言 (UML) 来生成符号化思维链和可执行动作规划。UML 类图捕获组合式对象语义，而活动图建模过程控制流。我们的三阶段训练流程结合了监督微调与群体相对策略优化 (GRPO)，包括从仅含答案的数据中进行奖励学习。我们在 MRoom-30k 上评估了 UML-CoT，这是一个新的杂乱房间清理场景基准。UML-CoT 在可解释性、规划连贯性和执行成功率方面均优于非结构化思维链，突出了 UML 作为一种更具表现力且更具可操作性的结构化推理形式化。|
|**2025-09-26**|[SPARK: Synergistic Policy And Reward Co-Evolving Framework](http://arxiv.org/abs/2509.22624)|**[link](https://github.com/InternLM/Spark)**|近年来，大型语言模型（LLMs）和大型视觉-语言模型（LVLMs）越来越多地采用强化学习（RL）进行预训练后微调，例如针对客观任务的可验证奖励强化学习（RLVR）和针对主观任务的人类反馈强化学习（RLHF）。然而，RLHF由于依赖人类偏好而导致成本高昂且可能存在奖励-策略不匹配问题，而RLVR在每次更新后都会丢弃轨迹和正确性信号，从而浪费了监督信息。为应对这些挑战，我们引入了协同策略与奖励协同进化框架（SPARK），这是一种基于RLVR的高效、在线策略且稳定的方法。SPARK没有丢弃轨迹和正确性数据，而是回收这些有价值的信息，同时训练模型本身作为一个生成式奖励模型。这种辅助训练采用了多种目标，例如逐点奖励分数、成对比较以及基于进一步反思响应的评估，以教授模型评估和改进自身响应的能力。我们的方法消除了对独立奖励模型和昂贵的人类偏好数据的需求。SPARK创建了一个积极的协同进化反馈循环：改进的奖励准确性产生更好的策略梯度，这反过来又生成更高质量的轨迹，进一步完善奖励模型。我们的统一框架支持通过自反思进行测试时扩展，无需外部奖励模型及其相关成本。我们表明SPARK在多个LLM和LVLM模型以及多个推理、奖励模型和通用基准测试上取得了显著的性能提升。例如，SPARK-VL-7B在7个推理基准测试上平均实现了9.7%的提升，在2个奖励基准测试上实现了12.1%的提升，在8个通用基准测试上实现了1.5%的提升，相较于基线模型，这展示了其鲁棒性和广泛的泛化能力。|
|**2025-09-26**|[Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective](http://arxiv.org/abs/2509.22613)|null|最近的强化学习（RL）方法大幅提升了大语言模型（LLMs）的规划能力，然而其有效性的理论基础仍然难以捉摸。在这项工作中，我们通过一种易于处理的基于图的抽象来研究RL的优点和局限性，重点关注策略梯度（PG）和Q-学习方法。我们的理论分析表明，有监督微调（SFT）可能会引入基于共现的虚假解，而RL主要通过探索实现正确的规划，强调了探索在实现更好泛化能力中的作用。然而，我们也表明PG存在多样性崩溃问题，即在训练过程中输出多样性下降，即使在达到完美准确度后仍然存在。相比之下，Q-学习提供了两个关键优势：离策略学习和收敛时的多样性保持。我们进一步证明，精心设计的奖励对于防止Q-学习中的奖励作弊是必要的。最后，将我们的框架应用于真实世界规划基准Blocksworld，我们证实这些行为在实践中也表现出来。|
|**2025-09-26**|[Quantile Advantage Estimation for Entropy-Safe Reasoning](http://arxiv.org/abs/2509.22611)|**[link](https://github.com/junkangwu/QAE)**|可验证奖励强化学习（RLVR）能增强大型语言模型（LLM）的推理能力，但其训练过程常在熵塌缩和熵爆炸之间振荡。我们将这两种风险归因于无价值强化学习（如GRPO和DAPO）中使用的均值基线，该基线在奖励异常值下会不当地惩罚负优势样本。我们提出了分位数优势估计（QAE），用组内K分位数基线取代了均值基线。QAE引入了一个响应级别的双机制门控：对于困难查询（p <= 1 - K），它强化罕见的成功案例；对于简单查询（p > 1 - K），它针对剩余的失败案例。在一阶softmax更新下，我们证明了双边熵安全性，给出了单步熵变化的下限和上限，从而遏制了熵爆炸并防止了熵塌缩。经验上，这一微小修改稳定了熵，稀疏化了信用分配（在调整后的K值下，大约80%的响应获得了零优势），并在AIME 2024/2025和AMC 2023数据集上，为Qwen3-8B/14B-Base模型带来了持续的pass@1性能提升。这些结果表明，基线设计——而非token级别的启发式方法——是扩展RLVR的主要机制。|
|**2025-09-26**|[Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning](http://arxiv.org/abs/2509.22601)|null|强化学习（RL）是提升大语言模型（LLMs）在长周期、稀疏奖励智能体任务中战略性工具使用能力的主导范式，但它面临着探索-利用困境这一根本性挑战。现有研究通过策略熵来促进探索，但这种机械式的熵最大化由于多轮分布偏移，容易导致RL训练不稳定。在本文中，我们旨在智能体自身经验的指导下实现渐进式探索-利用平衡，避免陷入熵崩溃或失控发散。我们提出SPEAR，一种基于课程的自我模仿学习（SIL）方法，用于训练智能体大语言模型。它扩展了香草版SIL框架，在该框架中，经验回放缓冲区存储自我生成的有前景轨迹用于离策略更新，通过在不同阶段逐步将策略演化引导至一个良好平衡的熵范围。具体而言，我们的方法引入了一个课程来管理探索过程，利用内在奖励培养技能层面探索，并通过SIL促进动作层面探索。最初，辅助工具调用奖励在工具使用技能的积累中扮演关键角色，使得智能体能够广泛接触环境反馈的陌生分布，并呈现出熵上升趋势。随着训练的进行，自我模仿得到强化，利用回放经验中现有的成功模式进行比较性的动作层面探索，从而加速解决方案迭代，同时避免无限熵增长。为了进一步稳定训练，我们重新校准了经验回放缓冲区中经验的优势值，以解决潜在的策略漂移问题。我们引入了正则化，例如对概率与优势之间具有高协方差的token进行裁剪，以进行轨迹层面的熵控制，从而抑制过度自信。|
|**2025-09-26**|[EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning](http://arxiv.org/abs/2509.22576)|null|在稀疏奖励的多轮交互环境中训练大语言模型智能体，其中完成单个任务在一个回合内需要30多个交互轮次，这对强化学习提出了一个基本挑战。我们识别出该设置特有的一个关键失效模式：探索-利用级联失效。这种级联始于早期策略过早收敛，即稀疏反馈导致智能体采纳有缺陷的低熵策略。随后，智能体进入晚期策略崩溃，此时传统的熵正则化变得适得其反，促进了混乱的探索，从而破坏了训练的稳定性。我们提出了熵正则化策略优化 (EPO)，这是一个通过三种协同机制打破这种失效循环的通用框架：(1) 在多轮设置中采用熵正则化以增强探索，(2) 一种熵平滑正则化器，它将策略熵限制在历史平均值范围内以防止突然波动，以及 (3) 一种自适应的基于阶段的权重分配，它在整个训练过程中平衡了探索与利用。我们的分析证明，EPO保证了熵方差的单调递减，同时保持了收敛性。EPO在ScienceWorld上实现了高达152%的性能提升，在ALFWorld上实现了高达19.8%的性能提升。我们的工作表明，多轮稀疏奖励设置需要与传统强化学习根本不同的熵控制，这对大语言模型智能体训练具有广泛影响。|
|**2025-09-26**|[StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models](http://arxiv.org/abs/2509.22558)|**[link](https://github.com/0xzhouchenyu/StepORLM)**|大语言模型（LLMs）在解决运筹学（OR）问题方面展现出巨大的潜力。尽管强化学习是LLM在运筹学问题训练上的强大范式，但现有工作普遍面临两个主要局限。首先，结果奖励存在信用分配问题，即正确的最终答案可能会强化有缺陷的推理。其次，传统的判别式过程监督是短视的，未能整体评估运筹学建模中相互依赖的步骤。为此，我们引入了StepORLM，这是一种采用生成式过程监督的新颖自演化框架。StepORLM的核心是一个协同演化循环，其中策略模型和生成式过程奖励模型（GenPRM）相互迭代改进。该循环由双重反馈机制驱动：来自外部求解器的明确的、基于结果的验证，以及来自GenPRM的细致入微的、整体的过程评估。组合信号用于通过加权直接偏好优化（W-DPO）来校准策略，并同时改进GenPRM。我们得到的80亿参数StepORLM在六个基准测试中建立了新的最先进水平，显著超越了规模大得多的通用模型、智能体方法和专用基线。此外，协同演化的GenPRM能够作为一个强大且普适的过程验证器，大幅提升了我们自己的模型以及其他现有LLM的推理扩展性能。|
|**2025-09-25**|[SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines](http://arxiv.org/abs/2509.21320)|null|我们提出了一个科学推理基础模型，它将自然语言与异构科学表示对齐。该模型在包含2060亿个词元的语料库上进行预训练，涵盖科学文本、纯序列和序列-文本对，然后通过在4000万条指令上进行SFT（监督微调）、采用退火冷启动自举法以引出长格式思维链，以及使用带有任务特定奖励塑形的强化学习进行对齐，从而灌输审慎的科学推理能力。它支持四大能力族，涵盖跨工作流的103项任务：(i) 文本与科学格式之间的忠实翻译，(ii) 文本/知识提取，(iii) 属性预测，(iv) 属性分类，(v) 无条件和有条件的序列生成与设计。与专业系统相比，我们的方法拓宽了指令覆盖范围，提高了跨领域泛化能力，并增强了保真度。我们详细介绍了数据整理和训练过程，并表明跨学科学习能增强迁移能力和下游可靠性。该模型、指令微调数据集和评估代码已开源，位于 https://huggingface.co/SciReason 和 https://github.com/open-sciencelab/SciReason。|
|**2025-09-25**|[RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards](http://arxiv.org/abs/2509.21319)|null|人类反馈强化学习 (RLHF) 和可验证奖励强化学习 (RLVR) 是大语言模型后训练中使用的主要强化学习范式，各具优势。然而，RLHF在可解释性和奖励作弊方面面临挑战，因为它依赖通常缺乏明确标准的人类判断；而RLVR的适用范围受到限制，因为它侧重于基于正确性的验证器。我们提出了二元灵活反馈强化学习 (RLBFF)，它结合了人类驱动偏好的多功能性与基于规则验证的精确性，使奖励模型能够捕获响应质量超越单纯正确性的细微方面。RLBFF从自然语言反馈中提取可以以二元方式回答的原则 (例如，信息准确性：是，或代码可读性：否)。这些原则随后可用于将奖励模型训练转化为蕴含任务 (即，响应是否满足任意原则)。我们展示了以这种方式训练的奖励模型在数据匹配的情况下可以超越Bradley-Terry模型，并在RM-Bench (86.2%) 和 JudgeBench (81.4%，截至2025年9月24日位列排行榜第一) 上取得了顶尖性能。此外，与Bradley-Terry模型不同，用户可以在推理时指定感兴趣的原则，以定制我们奖励模型的关注点。最后，我们提出了一个完全开源的方案 (包括数据)，使用RLBFF和我们的奖励模型对Qwen3-32B进行对齐，以匹配或超越o3-mini和DeepSeek R1在MT-Bench、WildBench和Arena Hard v2等通用对齐基准上的性能 (推理成本低于5%)。|
|**2025-09-25**|[It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL](http://arxiv.org/abs/2509.21282)|null|使用PPO和GRPO等强化学习（RL）方法训练大语言模型（LLM）通常依赖于比例裁剪来稳定更新。尽管裁剪能有效防止不稳定，但它会丢弃信息并引入梯度不连续性。我们提出了概率平滑策略优化（PSPO），它在计算重要性比率之前，将当前策略的概率向旧（行为）策略进行平滑，类似于标签平滑。与裁剪不同，PSPO保留了梯度信号，而向旧策略的插值创建了一个软信任区域，从而阻止了大规模、不稳定的更新，并提供了形式化保证。我们在GRPO中实例化了PSPO（GR-PSPO），并在GSM8K上微调了Qwen2.5-0.5B和Qwen2.5-1.5B模型，评估了它们在GSM8K测试集上的性能以及在SVAMP、ASDiv和MATH-500上的跨数据集泛化能力。相对于未裁剪的GRPO（单次迭代；无数据重用，比例始终为1），GR-PSPO取得了相似的性能，但改进了推理能力，从而产生了更清晰、更简洁且更具逻辑性的响应。与裁剪的GRPO相比，GR-PSPO显著提升了0.5B和1.5B模型的性能，在GSM8K上提升了20%以上（0.5B模型从17.6%提升到39.7%，1.5B模型从37.8%提升到59.4%）。|
|**2025-09-25**|[Tree Search for LLM Agent Reinforcement Learning](http://arxiv.org/abs/2509.21240)|**[link](https://github.com/AMAP-ML/Tree-GRPO)**|强化学习（RL）的最新进展显著增强了大型语言模型（LLM）的智能体能力。在长期和多轮智能体任务中，现有仅由结果奖励驱动的方法常常面临稀疏监督问题。为了解决这一挑战，我们提出了基于树的组相对策略优化（Tree-GRPO），这是一种基于树搜索的分组智能体强化学习方法，其中每个树节点代表完整的智能体交互步骤。通过共享共同前缀，树搜索采样在固定令牌或工具调用预算内增加了可实现的轨迹数量。此外，我们发现树状结构轨迹即使仅使用结果奖励，也能自然地构建逐步过程监督信号。基于此，Tree-GRPO在树内和树间两个层面估计了分组相对优势。通过理论分析，我们证明了树内层面组相对策略优化的目标等价于步级直接偏好学习的目标。在11个数据集和3种问答任务上的实验证明了所提出的基于树的强化学习方法相较于基于链的强化学习方法的优越性。|
|**2025-09-25**|[GRPO is Secretly a Process Reward Model](http://arxiv.org/abs/2509.21154)|**[link](https://github.com/coli-saar/grpo-prm)**|我们理论上证明，在关于跨补全的token序列组内重叠的某些假设下，GRPO强化学习算法会诱导一个非平凡的过程奖励模型（PRM）。然后我们经验性地表明，这些假设在真实世界条件下得到满足：GRPO确实诱导了一个非平凡的PRM。利用“GRPO即PRM”的框架，我们发现了GRPO目标函数中的一个缺陷：非均匀分布的过程步骤在不同条件下会阻碍探索和利用。我们提出了一个简单的算法修改来缓解这一缺陷（ $\lambda$-GRPO），并表明使用$\lambda$ -GRPO训练的大语言模型比使用标准GRPO训练的模型在验证准确率和下游推理任务表现上更高，并且更快地达到峰值性能。我们的结果质疑了昂贵、显式定义的PRM对GRPO的优势：我们展示了可以转而利用原始GRPO算法中隐藏的、内置的PRM结构来提升模型性能，同时对训练时间和成本的影响可忽略不计。|
|**2025-09-25**|[ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective](http://arxiv.org/abs/2509.21134)|null|大语言模型（LLM）已被用于在复杂场景中做出决策，这些场景要求模型进行深入思考、逻辑推理和明智决策。许多现有研究仅关注社交任务或模拟环境中的多轮对话，而忽略了不同类型的决策及其相互依赖性。当前的强化学习方法在训练过程中难以考虑其他个体的策略。为解决这些问题，我们首先定义了一个包含两种类型决策及其时间依赖性的战略决策问题。此外，我们提出了心智理论策略优化（ToMPO）算法，以优化对其他个体策略和博弈局势趋势的感知。与群体相对策略优化（GRPO）算法相比，ToMPO主要通过以下方式增强了LLM的战略决策能力：1) 基于对其他个体策略的推理生成轨迹，2) 在图级别和样本级别估计优势，以及3) 平衡全局和局部奖励。ToMPO算法在模型输出符合度和合作结果方面比GRPO方法高出35%。此外，与参数规模大100倍的模型相比，其性能提升了18%。这表明ToMPO算法在增强模型战略决策能力方面的有效性。|
|**2025-09-25**|[RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs](http://arxiv.org/abs/2509.21128)|null|大语言模型 (LLMs) 通常通过带可验证奖励的强化学习 (RLVR) 和基于推理轨迹的监督微调 (SFT) 进行训练，以提高其推理能力。然而，这些方法如何塑造推理能力仍然很大程度上不清楚。本文超越了仅基于准确性来调查这两个组件如何塑造推理过程的方法，引入了一种新颖的分析框架，该框架量化了推理路径并捕捉了它们在每种训练过程下（使用在数学领域拥有1.5B、7B和14B参数的模型）的定性变化。具体而言，我们从两个粒度级别研究推理过程：轨迹级别，该级别检查完整的推理输出；以及步骤级别，该级别分析推理图，其节点对应于单个推理步骤。值得注意的是，对独特推理轨迹的聚类显示出互补效应：RL压缩了不正确的轨迹，而SFT扩展了正确的轨迹。步骤级别分析表明，RL使推理图中节点访问频率、度数和中介中心性分布的衰减率变得更陡峭（约2.5倍），而SFT则使其趋于平坦（减少到约三分之一）。这表明RL将推理功能集中到一小部分步骤中，而SFT则将其均匀分布到许多步骤中。此外，通过从多个角度评估推理图的拓扑结构，我们描绘了RL和SFT的共享和独特特征。我们的工作提出了一种新颖的推理路径视角，解释了为什么当前SFT后接RL的两阶段训练最佳实践是成功的，并为数据构建和更有效的学习方法提供了实际启示。|
|**2025-09-25**|[Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning](http://arxiv.org/abs/2509.21126)|null|在线强化学习在复杂任务中耗时巨大，因为需要大量的交互步骤来学习最优Q函数。视觉-语言动作（VLA）策略代表了解决各种任务的一个有前景的方向；然而，它们在低层控制上的性能仍然有限，并且有效的部署通常需要任务特定的专家演示来进行微调。在本文中，我们提出了VARL（VLM作为在线强化学习的动作建议者），一个利用视觉-语言模型（VLM）领域知识为强化学习智能体提供动作建议的框架。与以往方法不同，VARL提供动作建议而非设计启发式奖励，从而保证了最优性和收敛性不变。这些建议动作增加了样本多样性，最终提高了样本效率，尤其是在稀疏奖励任务中。为了验证VARL的有效性，我们在各种环境和智能体设置中对其进行了评估。结果表明，VARL大幅提高了样本效率，而没有引入显著的计算开销。这些优势使得VARL成为一个用于在线强化学习的通用框架，并使其能够将强化学习从零开始直接应用于真实世界环境。|
|**2025-09-25**|[Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns](http://arxiv.org/abs/2509.21124)|null|大型推理模型在具有挑战性的数学推理方面的最新进展得益于强化学习（RL）。在中期训练中融入长链式思考（CoT）数据，也被证明能够大幅提升推理深度。然而，当前方法常常不加区分地使用CoT数据，这使得哪种数据类型能最有效地增强模型推理能力这一关键问题悬而未决。在本文中，我们首次将基础模型的推理潜力定义为正确回答问题所需的独立尝试次数的倒数，这与最终模型性能密切相关。随后，我们提出利用富含高价值推理模式的多样化数据来扩展推理潜力。具体而言，我们从CoT序列中抽象出以通用性和归纳能力为特征的原子推理模式，并利用它们构建了一个富含宝贵推理模式的核心参考集。此外，我们提出了一种涉及推理模式链和token熵的双粒度算法，以有效从与核心集对齐的数据池中选择高价值CoT数据（CoTP），从而训练模型有效掌握推理。仅100亿token的CoTP数据使85A6B专家混合（MoE）模型在具有挑战性的AIME 2024和2025上提升了9.58%，并将下游RL性能的上限提高了7.81%。|
|**2025-09-25**|[MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning](http://arxiv.org/abs/2509.21113)|null|视频推理已成为多模态大语言模型（MLLMs）的一项关键能力，要求模型超越静态感知，实现对复杂场景中时间动态性的连贯理解。然而，现有MLLMs常表现出过程不一致性，即便是最终答案正确，中间推理也可能偏离视频动态，从而损害了可解释性和鲁棒性。为解决此问题，我们引入了MOSS-ChatV，一个采用基于动态时间规整（DTW）的过程奖励的强化学习框架。这种基于规则的奖励将推理轨迹与时间上基础的参考对齐，从而无需辅助奖励模型即可实现高效的过程监督。我们进一步将动态状态预测确定为视频推理的关键衡量标准，并构建了MOSS-Video，这是一个带有标注推理轨迹的基准数据集，其中训练集用于微调MOSS-ChatV，预留集用于评估。MOSS-ChatV在MOSS-Video（测试集）上取得了87.2%的成绩，并提高了在MVBench和MMVU等通用视频基准上的性能。该框架在Qwen2.5-VL和Phi-2等不同架构上持续取得增益，证实了其广泛适用性。使用GPT-4o作为评判者的评估进一步表明，MOSS-ChatV生成了更一致和稳定的推理轨迹。|
|**2025-09-23**|[Reinforcement Learning on Pre-Training Data](http://arxiv.org/abs/2509.19249)|**[link](https://github.com/synlp/ChiMed-GPT)**|计算资源的指数级扩展与高质量文本数据的有限增长之间日益扩大的差距，目前限制了大型语言模型（LLMs）的传统扩展方法。为了应对这一挑战，我们引入了预训练数据上的强化学习（RLPT），这是一种用于优化LLMs的新型训练时扩展范式。与以往主要通过监督学习扩展训练的方法不同，RLPT使策略能够自主探索有意义的轨迹，从而从预训练数据中学习并通过强化学习（RL）提高其能力。尽管现有的强化学习策略，例如基于人类反馈的强化学习（RLHF）和可验证奖励的强化学习（RLVR），依赖人工标注来构建奖励，但RLPT通过直接从预训练数据中获取奖励信号消除了这种依赖。具体而言，它采用了一种下一片段推理目标，奖励策略基于前文语境准确预测后续文本片段。这种表述使得强化学习可以在预训练数据上进行扩展，鼓励在更广泛的语境中探索更丰富的轨迹，从而培养更具泛化性的推理能力。在通用领域和数学推理基准上对多个模型进行的大量实验验证了RLPT的有效性。例如，当应用于Qwen3-4B-Base时，RLPT在MMLU、MMLU-Pro、GPQA-Diamond、KOR-Bench、AIME24和AIME25上分别带来了3.0、5.1、8.1、6.0、6.6和5.3的绝对提升。结果进一步表明了良好的扩展行为，预示着在更多计算资源下有持续提升的巨大潜力。此外，RLPT提供了一个坚实的基础，扩展了LLMs的推理边界并增强了RLVR的性能。|
|**2025-09-23**|[Online Process Reward Leanring for Agentic Reinforcement Learning](http://arxiv.org/abs/2509.19199)|null|大型语言模型 (LLM) 越来越多地通过强化学习 (RL) 进行训练，成为在交互式环境中进行长期推理和行动的自主智能体。然而，稀疏且有时不可验证的奖励使得时间信用分配极具挑战性。最近的工作尝试将过程监督整合到智能体学习中，但存在标注偏差、奖励欺骗、过细粒度信号导致的高方差，或在状态重叠罕见时失效等问题。因此，我们引入了在线过程奖励学习 (OPRL)，这是一种通用的智能体强化学习信用分配策略，它与标准在轨策略算法无缝集成，无需额外回溯或显式步骤标签。在 OPRL 中，我们通过基于轨迹的 DPO 目标，交替优化隐式过程奖励模型 (PRM) 和智能体策略，将轨迹偏好转换为隐式步骤奖励。这些步骤奖励随后被用于计算步骤级优势，并与来自结果奖励的回合级优势相结合以更新策略，从而形成一个自我强化的循环。理论研究结果保证所学到的步骤奖励与轨迹偏好一致，并作为基于势函数的塑形奖励，提供有界梯度以稳定训练。在实证方面，我们在三个不同的智能体基准上评估了 OPRL，包括 WebShop 和 VisualSokoban，以及 SOTOPIA 中具有不可验证奖励的开放式社交互动。至关重要的是，OPRL 在各个领域都表现出优于前沿大型语言模型和强大的强化学习基线模型的性能，以更高的样本效率和更低的训练方差实现了最先进的结果。进一步分析还表明，OPRL 使用更少的动作实现了高效探索，这突显了其在现实世界场景中进行智能体学习的潜力。|
|**2025-09-23**|[Soft Tokens, Hard Truths](http://arxiv.org/abs/2509.19170)|**[link](https://github.com/rprokap/entremanure)**|在推理大型语言模型（LLMs）的思维链（CoT）阶段使用连续而非离散的token近来受到关注，其直觉是离散token的连续混合可以同时模拟多个推理路径的叠加。理论结果已正式证明连续token具有更强的表达能力，并能更高效地解决特定问题。然而，连续token的实际应用受限于强大的训练难度：之前的工作要么仅在推理时将连续token应用于预训练的离散token模型，要么必须从真实离散CoT中蒸馏出连续CoT，并面临计算成本，使得CoT仅限于极少数token。这是首次引入一种可扩展方法，通过强化学习（RL）来学习连续CoT，无需从参考离散CoT中进行蒸馏。我们使用“软”token：token的混合以及输入嵌入上的噪声，以提供RL探索。计算开销最小，使我们能够学习包含数百个token的连续CoT。在高达8B参数的Llama和Qwen模型上的数学推理基准测试中，使用连续CoT进行训练，在pass@1指标上与离散token CoT持平，并在pass@32指标上超越它们，表明CoT的多样性更强。在系统比较中，表现最佳的场景是使用连续CoT token进行训练，然后在推理时使用离散token，这意味着“软”模型可以以标准方式部署。最后，我们展示了连续CoT强化学习训练能更好地保留基础模型在域外任务上的预测，从而为基础模型提供了更温和的干预。|
|**2025-09-23**|[PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio](http://arxiv.org/abs/2509.19128)|null|强化学习 (RL) 正越来越多地被用于增强大语言模型 (LLM) 的推理能力。然而，有效扩展这些RL方法面临显著挑战，这主要是因为在不生成损害常见RL算法的陈旧离策略数据的前提下，难以维持高AI加速器利用率。本文介绍了一种名为PipelineRL的方法，旨在为LLM训练实现硬件效率和数据在策略性之间的卓越权衡。PipelineRL采用并发异步数据生成和模型训练，其特点是新颖的飞行中权重更新。这种机制允许LLM生成引擎在生成token序列期间以最小中断接收更新的模型权重，从而最大化加速器利用率和训练数据的新鲜度。使用128个H100 GPU在长文本推理任务上进行的实验表明，与传统RL基线相比，PipelineRL实现了大约2倍的学习速度提升，同时保持了高度在策略的训练数据。作为一项关键贡献，PipelineRL的一个可扩展且模块化的开源实现也已发布。|
|**2025-09-23**|[DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment](http://arxiv.org/abs/2509.19104)|**[link](https://github.com/sharansahu/distributionally_robust_rebel)**|人类反馈强化学习（RLHF）对于使大语言模型（LLM）与人类意图对齐至关重要。然而，现有离线RLHF方法存在过度优化问题，即模型对奖励误设定过拟合，并偏离训练中观察到的偏好行为。我们引入了DRO-REBEL，这是一个统一的鲁棒REBEL更新族，包含 $p$型Wasserstein、KL和$\chi^2$模糊集。利用Fenchel对偶性，每个更新都简化为简单的相对奖励回归，从而保持可扩展性并避免PPO风格的裁剪或辅助价值网络。在标准线性奖励和对数线性策略类以及数据覆盖条件下，我们建立了$O(n^{-1/4})$的估计界限，其常数比先前的DRO-DPO方法更紧密，并通过局部Rademacher复杂度分析恢复了极小极大最优的$O(n^{-1/2})$速率。同样的分析弥补了Wasserstein-DPO和KL-DPO的差距，表明两者也达到了最优参数速率。我们为所有三种散度推导了实用的SGD算法：梯度正则化（Wasserstein）、重要性采样（KL）和快速一维对偶求解（$\chi^2$）。在情感对齐、大规模ArmoRM多目标基准和HH对齐上的实验证明了在未见偏好混合、模型规模和数据规模下强大的最坏情况鲁棒性，其中$\chi^2$-REBEL持续显示出强大的经验性能。一项受控的半径-覆盖率研究验证了“没有免费午餐”的权衡：比经验散度集中速率收缩更快的半径能够达到极小极大最优参数速率但牺牲了覆盖率，而保证覆盖率的半径则会导致$O(n^{-1/4})$ 的速率。|
|**2025-09-23**|[Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards](http://arxiv.org/abs/2509.19003)|**[link](https://github.com/baaivision/CoS)**|思维链推理在大语言模型中取得了显著成功，但其在视觉-语言推理中的应用仍然是一个开放性挑战，且最佳实践尚不明确。现有尝试通常采用粗粒度推理链，这难以执行细粒度结构化推理，更重要的是，难以评估中间推理的奖励和质量。在这项工作中，我们深入研究了视觉-语言模型的步骤链推理，从而能够精确评估推理步骤质量，并利用细粒度奖励实现有效的强化学习和推理时扩展。我们提出了一个简单、有效且完全透明的框架，包括步骤级推理数据、过程奖励模型（PRM）和强化学习训练。凭借所提出的方法，我们的模型在具有挑战性的视觉-语言基准上建立了强大的基线，并取得了持续改进。更重要的是，我们进行了彻底的实证分析和消融研究，揭示了每个组件的影响以及推理时扩展的一些有趣特性。我们相信本文可作为视觉-语言模型的基线，并为更复杂的多模态推理提供见解。我们的数据集、PRM和代码将可在https://github.com/baaivision/CoS获取。|
|**2025-09-23**|[Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling](http://arxiv.org/abs/2509.18864)|null|用户画像作为用户理解的核心技术，旨在从用户信息中推断结构化属性。大语言模型（LLMs）为用户画像提供了有前景的途径，然而，进展受到缺乏全面基准的阻碍。为了弥补这一空白，我们提出了ProfileBench，这是一个源自真实世界视频平台的工业基准，它包含异构用户数据和结构良好的画像分类体系。然而，由于难以收集大规模真实标签，以及异构且嘈杂的用户信息可能会损害LLMs的可靠性，画像任务仍然具有挑战性。为了实现无标签且可靠的用户画像，我们提出了一个置信度驱动的画像推理框架Conf-Profile，它采用两阶段范式。我们首先利用带有置信度提示的先进LLMs合成高质量标签，接着通过置信度加权投票提高准确性，并通过置信度校准实现平衡分布。多个画像结果、理由和置信度分数被聚合并蒸馏到一个轻量级LLM中。我们通过置信度引导的无监督强化学习进一步增强了推理能力，该学习利用置信度进行难度过滤、准真实标签投票和奖励加权。实验结果表明，Conf-Profile通过两阶段训练提供了显著的性能，在Qwen3-8B上将F1提高了13.97。|
|**2025-09-23**|[NGRPO: Negative-enhanced Group Relative Policy Optimization](http://arxiv.org/abs/2509.18851)|null|RLVR增强了大语言模型（LLMs）在各种任务中的推理能力。然而，作为一种代表性的RLVR算法，GRPO存在一个关键局限：当一个组内的所有响应要么完全正确，要么完全不正确时，模型无法从这些同质响应中学习。这对于同质不正确组尤其成问题，因为GRPO的优势函数会产生零值，导致零梯度并丢失有价值的学习信号。为了克服这个问题，我们提出了NGRPO（负增强组相对策略优化），这是一种旨在将同质错误转化为鲁棒学习信号的算法。首先，NGRPO引入了优势校准。该机制假设在优势计算过程中存在一个虚拟的最大奖励样本，从而改变组内奖励的均值和方差，并确保同质不正确样本的优势不再为零。其次，NGRPO采用了非对称裁剪，该方法放宽了对正样本的更新幅度，同时对负样本施加了更严格的约束。这有助于稳定由优势校准引入的探索压力。我们在Qwen2.5-Math-7B上的实验表明，NGRPO在MATH500、AMC23和AIME2025等数学基准测试中显著优于PPO、GRPO、DAPO和PSR-NSR等基线方法。这些结果验证了NGRPO从同质错误中学习的能力，从而在数学推理方面带来稳定且显著的改进。我们的代码可在https://github.com/nangongrui-ngr/NGRPO获取。|
|**2025-09-23**|[MAPO: Mixed Advantage Policy Optimization](http://arxiv.org/abs/2509.18849)|**[link](https://github.com/WenkeHuang/MAPO)**|针对基础模型的强化学习（例如组相对策略优化GRPO）的最新进展，显著提升了基础模型在推理任务上的性能。值得注意的是，优势函数在GRPO中作为对轨迹重要性进行排序的核心机制。然而，现有探索遇到了优势反转和优势镜像问题，这阻碍了在不同查询样本之间进行合理的优势分配。在这项工作中，我们提出了一种简单但有效的GRPO策略：混合优势策略优化 (MAPO)。我们揭示了轨迹以不同的确定性出现，并针对高确定性轨迹的样本提出了优势百分比偏差。此外，我们动态地重新加权具有不同轨迹确定性的样本的优势函数，从而自适应地配置优势函数以考虑样本特有特性。与相关最先进方法的比较，以及针对不同优势变体的消融研究，验证了我们方法的有效性。|
|**2025-09-23**|[Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions](http://arxiv.org/abs/2509.18847)|null|工具增强型大型语言模型（LLM）通常通过监督模仿学习或优化单次工具调用的粗粒度强化学习进行训练。当前的自我反思实践依赖于启发式提示或单向推理：模型被敦促“多思考”，而不是学习错误诊断和修复。这在多轮交互中是脆弱的；在失败后，模型往往会重复相同的错误。我们提出了结构化反思，它将从错误到修复的路径转化为一个显式、可控且可训练的动作。智能体生成一个简短而精确的反思：它利用前一步的证据诊断故障，然后提出一个正确且可执行的后续调用。为了进行训练，我们将DAPO和GSPO目标与针对工具使用定制的奖励机制相结合，优化“反思-调用-结束”的逐步策略。为了评估，我们引入了Tool-Reflection-Bench，这是一个轻量级基准，它通过编程方式检查结构有效性、可执行性、参数正确性和结果一致性。任务被构建为错误调用、反思和纠正调用的微型轨迹，具有不相交的训练集和测试集划分。在BFCL v3和Tool-Reflection-Bench上的实验表明，多轮工具调用成功率和错误恢复能力显著提高，并且冗余调用减少。这些结果表明，使反思显式化并直接优化它，可以提高工具交互的可靠性，并为智能体从失败中学习提供了可复现的路径。|
|**2025-09-19**|[Reward Evolution with Graph-of-Thoughts: A Bi-Level Language Model Framework for Reinforcement Learning](http://arxiv.org/abs/2509.16136)|null|设计有效的奖励函数仍然是强化学习（RL）中的一个主要挑战，通常需要大量人类专业知识和迭代优化。最近的进展利用大语言模型（LLMs）进行自动化奖励设计，但这些方法受限于幻觉、对人类反馈的依赖以及处理复杂多步骤任务的挑战。在这项工作中，我们引入了基于思绪图的奖励演化（RE-GoT），这是一个新颖的双层框架，它通过结构化图基推理增强LLMs，并集成视觉语言模型（VLMs）以进行自动化轨迹评估。RE-GoT首先将任务分解为文本属性图，从而实现全面分析和奖励函数生成，然后利用来自VLMs的视觉反馈迭代优化奖励，无需人工干预。在10个RoboGen任务和4个ManiSkill2任务上进行的大量实验表明，RE-GoT持续优于现有的基于LLM的基线。在RoboGen上，我们的方法将平均任务成功率提高了32.25%，在复杂多步骤任务上取得了显著提升。在ManiSkill2上，RE-GoT在四个多样化操作任务中实现了93.73%的平均成功率，显著超越了先前的基于LLM的方法，甚至超过了专家设计的奖励。我们的结果表明，将LLMs和VLMs与思绪图推理相结合，为RL中的自主奖励演化提供了一种可扩展且有效的解决方案。|
|**2025-09-19**|[BaseReward: A Strong Baseline for Multimodal Reward Model](http://arxiv.org/abs/2509.16127)|null|多模态大语言模型（MLLMs）的快速发展使得使其与人类偏好对齐成为一个严峻挑战。奖励模型（RMs）是实现此目标的核心技术，然而，目前学术界和工业界都缺乏构建最先进多模态奖励模型（MRMs）的系统性指南。本文旨在通过详尽的实验分析，为构建高性能MRMs提供一个清晰的“秘籍”。我们系统性地研究了MRM开发流程中的每个关键组成部分，包括奖励建模范式（例如，朴素-RM、基于评论员的RM和生成式RM）、奖励头架构、训练策略、数据筛选（涵盖十余个多模态和纯文本偏好数据集）、主干模型和模型规模以及集成方法。基于这些实验见解，我们引入了BaseReward，一个强大而高效的多模态奖励建模基线。BaseReward采用了一种简单而有效的架构，它建立在Qwen2.5-VL主干之上，具有优化的两层奖励头，并在一组精心筛选的高质量多模态和纯文本偏好数据混合上进行训练。我们的结果表明，BaseReward在MM-RLHF-奖励基准、VL-奖励基准和多模态奖励基准等主要基准上建立了新的SOTA，优于以往模型。此外，为了验证其超越静态基准的实际效用，我们将BaseReward集成到一个真实的强化学习流程中，成功提升了MLLM在各种感知、推理和对话任务中的性能。这项工作不仅交付了一个顶级的MRM，更重要的是，为社区提供了一个清晰、有经验支持的指南，用于开发下一代MLLMs的稳健奖励模型。|
|**2025-09-19**|[Rethinking Molecule Synthesizability with Chain-of-Reaction](http://arxiv.org/abs/2509.16084)|null|分子生成模型的一个众所周知的不足是它们不能保证生成可合成的分子。尽管已为此问题进行了大量尝试，但鉴于可合成分子指数级大的组合空间，现有方法在空间覆盖率和分子优化性能方面表现不佳。为解决这些问题，我们引入了ReaSyn，这是一个用于可合成投影的生成框架，模型通过生成能产生可合成类似物的路径，来探索给定分子在可合成空间中的邻域。为了充分利用合成路径中包含的化学知识，我们提出了一种新颖的视角，将合成路径类比于大语言模型（LLM）中的推理路径。具体而言，受LLM中思维链（CoT）推理的启发，我们引入了反应链（CoR）表示法，该表示法明确说明了路径中每一步的反应物、反应类型和中间产物。借助CoR表示法，ReaSyn可以在每个反应步骤中获得密集监督，从而在监督训练期间明确学习化学反应规则并执行逐步推理。此外，为了进一步增强ReaSyn的推理能力，我们提出了基于强化学习（RL）的微调和专为可合成投影定制的目标导向测试时计算扩展。ReaSyn在可合成分子重建中实现了最高的重建率和路径多样性，在可合成目标导向分子优化中实现了最高的优化性能，并在可合成命中扩展方面显著优于先前的可合成投影方法。这些结果突出了ReaSyn在探索组合学上巨大的可合成化学空间方面的卓越能力。|
|**2025-09-19**|[AI Methods for Permutation Circuit Synthesis Across Generic Topologies](http://arxiv.org/abs/2509.16020)|null|本文研究了用于在通用拓扑结构上合成和转译置换电路的人工智能 (AI) 方法。我们的方法采用强化学习 (RL) 技术，实现了多达 25 量子比特置换电路的近乎最优合成。我们没有为单个拓扑结构开发专用模型，而是在通用矩形格子上训练了一个基础模型，并采用掩码机制在合成过程中动态选择拓扑结构的子集。这使得置换电路能够在任何可嵌入矩形格子的拓扑结构上进行合成，而无需重新训练模型。本文展示了 5x5 格子的结果，并将其与之前面向拓扑的AI模型和经典方法进行比较，结果表明它们优于经典启发式方法，与之前的专用AI模型性能相当，甚至能够对训练中未见过的拓扑结构进行合成。我们进一步表明，该模型可以通过微调来增强对特定感兴趣拓扑结构的性能。这种方法使得单一训练模型能够高效地在不同拓扑结构上合成电路，从而使其能够实际集成到转译工作流程中。|
|**2025-09-19**|[Enhancing Generative Auto-bidding with Offline Reward Evaluation and Policy Search](http://arxiv.org/abs/2509.15927)|null|自动出价是广告主提升广告效果的重要工具。最新进展表明，AI生成式出价 (AIGB) 将自动出价建模为轨迹生成任务，并在离线数据上训练基于条件扩散模型的规划器，相比于典型的基于离线强化学习 (RL) 的自动出价方法，取得了卓越且稳定的性能。然而，现有的AIGB方法由于忽视了细粒度的生成质量评估以及无法探索静态数据集之外的内容，仍然面临性能瓶颈。为解决此问题，我们提出了AIGB-Pearl (意为通过RL进行评估器引导的规划)，这是一种融合了生成式规划和策略优化的新颖方法。AIGB-Pearl的关键在于构建一个非自举的轨迹评估器，用于分配奖励并指导策略搜索，从而使规划器能够通过交互迭代地优化其生成质量。此外，为提升离线设置中轨迹评估器的准确性，我们引入了三项关键技术：(i) 基于大语言模型 (LLM) 的架构以获得更好的表示能力，(ii) 混合点式和对式损失以获得更好的分数学习，以及 (iii) 专家反馈的自适应集成以获得更好的泛化能力。在模拟和真实世界的广告系统上进行的大量实验证明了我们方法的最先进性能。|
|**2025-09-19**|[Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds](http://arxiv.org/abs/2509.15915)|null|尽管从零开始的强化学习在利用高效模拟器解决序列决策任务方面取得了令人瞩目的成果，但现实世界中交互成本高昂的应用需要更具样本效率的智能体。基础模型（FM）因其广泛的知识和推理能力，自然成为提高样本效率的候选者，但目前尚不清楚如何有效地将它们整合到强化学习框架中。在本文中，我们预测并（最重要的是）评估了两种有前景的策略。首先，我们考虑使用基础世界模型（FWM），它们利用FM的先验知识，从而能够通过模拟交互来训练和评估智能体。其次，我们考虑使用基础智能体（FA），它们利用FM的推理能力进行决策。我们在一系列适合当前一代大型语言模型（LLM）的网格世界环境中凭经验评估了这两种方法。我们的结果表明，LLM的改进已经转化为更好的FWM和FA；基于当前LLM的FA已经能够为足够简单的环境提供优秀的策略；并且FWM与强化学习智能体的结合对于具有部分可观察性和随机元素的更复杂环境非常有前景。|
|**2025-09-19**|[CCrepairBench: A High-Fidelity Benchmark and Reinforcement Learning Framework for C++ Compilation Repair](http://arxiv.org/abs/2509.15690)|null|C++编译错误的自动化修复是一个重大挑战，其解决对于提高开发者生产力至关重要。该领域的进展受两个主要因素的限制：大规模、高保真数据集的稀缺性，以及传统监督方法的局限性，这些方法往往无法生成语义正确的补丁。本文通过引入一个具有三个核心贡献的综合框架来解决这些空白。首先，我们提出了CCrepair，这是一个通过精密生成与验证流程构建的新颖、大规模C++编译错误数据集。其次，我们提出了一种由混合奖励信号引导的强化学习（RL）范式，将重点从仅仅是可编译性转移到修复的语义质量。最后，我们建立了一个提供此信号的鲁棒两阶段评估系统，其核心是一个“LLM作为判官”的模型，该模型的可靠性已根据人类专家小组的集体判断进行了严格验证。这种集成方法使训练目标与生成高质量、非平凡且语法和语义都正确的补丁相一致。我们的方法通过实验证明了其有效性。我们经过RL训练的Qwen2.5-1.5B-Instruct模型达到了与Qwen2.5-14B-Instruct模型相当的性能，验证了我们训练范式的效率。我们的工作为研究社区提供了一个有价值的新数据集和一种更有效的训练与评估鲁棒编译修复模型的范式，为开发更实用和可靠的自动化编程助手铺平了道路。|
|**2025-09-19**|[PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models](http://arxiv.org/abs/2509.15607)|null|基于偏好的强化学习 (PbRL) 已成为一种无需奖励工程即可教会机器人复杂行为的有前景的范式。然而，其有效性常受限于两个关键挑战：对大量人类输入的依赖以及在奖励学习过程中解决查询歧义和信用分配的固有困难。本文引入 PRIMT，一个旨在通过利用基础模型 (FMs) 进行多模态合成反馈和轨迹合成来克服这些挑战的 PbRL 框架。与以往依赖单模态基础模型评估的方法不同，PRIMT 采用一种分层神经符号融合策略，整合了大语言模型和视觉语言模型在评估机器人行为方面的互补优势，以获得更可靠、更全面的反馈。PRIMT 还结合了预见性轨迹生成，通过用引导样本预热轨迹缓冲区来减少早期查询歧义；以及回溯性轨迹增强，它结合因果辅助损失实现了反事实推理，以改进信用分配。我们在各种基准上对 PRIMT 进行了 2 个运动任务和 6 个操作任务的评估，证明了其性能优于基于基础模型和脚本的基线。|
|**2025-09-19**|[BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent](http://arxiv.org/abs/2509.15566)|null|在AI驱动的人机图形界面交互自动化领域，尽管多模态大语言模型和强化微调技术的快速发展取得了显著进步，但一个根本挑战依然存在：它们的交互逻辑显著偏离了自然的人机图形界面通信模式。为了弥补这一空白，我们提出了“闪视-思考-连接”（Blink-Think-Link，简称BTL）框架，这是一个受大脑启发的、模仿用户与图形界面之间人类认知过程的人机图形界面交互框架。该系统将交互分解为三个生物学上合理的阶段：（1）闪视（Blink）——快速检测并关注相关屏幕区域，类似于眼跳运动；（2）思考（Think）——高级推理和决策，映射认知规划；（3）连接（Link）——生成用于精确运动控制的可执行命令，模拟人类的行动选择机制。此外，我们为BTL框架引入了两项关键技术创新：（1）闪视数据生成（Blink Data Generation）——一个专门为闪视数据优化的自动化标注流程，以及（2）BTL奖励（BTL Reward）——首个基于规则的奖励机制，能够驱动由过程和结果共同决定的强化学习。基于此框架，我们开发了一个名为BTL-UI的图形界面智能体模型，该模型在综合基准测试中，于静态GUI理解和动态交互任务中均展示了持续的最先进性能。这些结果为该框架在开发高级GUI智能体方面的有效性提供了确凿的经验验证。|
|**2025-09-19**|[Reward Hacking Mitigation using Verifiable Composite Rewards](http://arxiv.org/abs/2509.15557)|**[link](https://github.com/healthylaife/Composite-LLM-Reward-Model)**|可验证奖励强化学习（RLVR）最近表明，大型语言模型（LLMs）无需直接监督即可发展出自己的推理能力。然而，在医疗领域的应用，特别是问答任务中，推理阶段容易受到严重的奖励欺骗。我们的工作解决了这种行为的两种主要形式：一是提供最终答案而无前置推理，二是采用非标准推理格式来利用奖励机制。为了缓解这些问题，我们引入了一种复合奖励函数，对这些行为施加了特定的惩罚。我们的实验表明，将RLVR与我们提出的奖励模型结合，能够产生格式更规范的推理，减少了奖励欺骗，并与基线相比具有良好的准确性。这种方法标志着向减少奖励欺骗和增强利用RLVR的模型的可靠性迈进了一步。|
|**2025-09-18**|[Generalizable Geometric Image Caption Synthesis](http://arxiv.org/abs/2509.15217)|null|多模态大语言模型拥有各种要求强大推理能力的实际应用。尽管最近取得了进展，这些模型在解决复杂几何问题方面仍然面临挑战。一个关键挑战源于缺乏用于理解几何图像的高质量图像-文本对数据集。此外，大多数基于模板的数据合成管道通常无法泛化到超出其预定义模板的问题。在本文中，我们通过将可验证奖励强化学习 (RLVR) 这一互补过程引入数据生成管道来弥补这一空白。通过采用RLVR来细化从50种基本几何关系合成的几何图像的描述，并利用源自数学问题解决任务的奖励信号，我们的管道成功捕捉了几何问题解决的关键特征。这使得任务泛化能力得到提升，并带来了显著改进。此外，即使在分布外场景中，所生成的数据集也增强了多模态大语言模型的通用推理能力，在MathVista和MathVerse数据集中针对非几何输入图像的统计、算术、代数和数值任务中，准确率提高了2.8%-4.8%，同时在MMMU的艺术、设计、技术和工程任务中也实现了2.4%-3.9%的改进。|
|**2025-09-18**|[FlowRL: Matching Reward Distributions for LLM Reasoning](http://arxiv.org/abs/2509.15207)|null|我们提出FlowRL：在大语言模型（LLM）强化学习（RL）中，通过流平衡来匹配完整的奖励分布，而非最大化奖励。近期的先进推理模型采用奖励最大化方法（例如，PPO和GRPO），这些方法倾向于过度优化主导奖励信号，同时忽视不那么频繁但有效的推理路径，从而降低了多样性。相比之下，我们将标量奖励转换为使用可学习配分函数的归一化目标分布，然后最小化策略与目标分布之间的逆KL散度。我们将这一思想实现为一种流平衡优化方法，该方法促进多样化探索和可泛化的推理轨迹。我们在数学和代码推理任务上进行了实验：FlowRL在数学基准测试中，相较于GRPO平均实现了10.0%的显著提升，相较于PPO平均实现了5.1%的显著提升，并在代码推理任务上表现持续更好。这些结果突出表明，奖励分布匹配是迈向LLM强化学习中高效探索和多样化推理的关键一步。|
|**2025-09-18**|[Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation](http://arxiv.org/abs/2509.15194)|**[link](https://github.com/YujunZhou/EVOL-RL)**|大型语言模型（LLM）正越来越多地通过基于可验证奖励的强化学习（RLVR）进行训练，然而，实际部署需要模型能够在没有标签或外部评判的情况下进行自我改进。现有的无标签方法，如置信度最小化、自我一致性或多数投票目标，能稳定学习但会逐渐减少探索，导致熵坍塌：生成内容变得更短、多样性降低且脆弱。与先前方法（例如测试时强化学习（TTRL），其主要使模型适应当前的无标签数据集）不同，我们的目标更广阔：在不牺牲模型固有的探索能力和泛化能力（即进化）的情况下实现普遍改进。我们形式化了这个问题，并提出了面向进化的无标签强化学习（EVOL-RL），它是一种在无标签设置下将稳定性与变异性相结合的简单规则。EVOL-RL将多数投票答案作为稳定锚点（选择），同时添加了新颖性感知奖励，该奖励偏好推理与已生成内容不同（变异）的响应，并在语义空间中进行衡量。EVOL-RL使用GRPO实现，还利用非对称裁剪来保留强信号和熵正则化器来维持探索。这种多数选择+新颖变异的设计可以防止坍塌，保持更长、信息量更大的思维链，并提高pass@1和pass@n。EVOL-RL持续优于仅基于多数投票的TTRL基线；例如，在无标签AIME24上训练，将Qwen3-4B-Base模型在AIME25上的pass@1从TTRL的4.6%提升到16.4%，pass@16从18.5%提升到37.9%。EVOL-RL不仅防止了多样性坍塌，还实现了更强的跨领域泛化能力（例如，GPQA）。此外，我们证明EVOL-RL在RLVR设置中也能提升性能，突显了其广泛适用性。|
|**2025-09-18**|[Self-Improving Embodied Foundation Models](http://arxiv.org/abs/2509.15155)|**[link](https://github.com/self-improving-efms/self-improving-efms.github.io)**|基于海量网络数据训练的基础模型彻底改变了机器人技术，但其在低级控制中的应用仍主要局限于行为克隆。借鉴大型语言模型微调中强化学习阶段的成功经验，我们提出了一种用于机器人的两阶段后训练方法。第一阶段为监督微调（SFT），它利用行为克隆和剩余步数预测目标来微调预训练的基础模型。在第二阶段，即自我改进阶段，剩余步数预测使得能够提取形态良好的奖励函数和鲁棒的成功检测器，从而使机器人集群能够在最少人工监督下自主练习下游任务。通过在真实世界和模拟机器人实体上进行的大量实验，我们新颖的后训练方案在具身基础模型上取得了显著成果。首先，我们证明了SFT和自我改进的结合比扩展监督学习的模仿数据收集效率显著更高，并能带来成功率显著提升的策略。进一步的消融实验强调，海量网络预训练和自我改进的结合是实现这种样本效率的关键。其次，我们证明了我们提出的组合独特地解锁了一种当前方法无法实现的能力：自主练习和获取新技能，这些技能的泛化能力远超训练中使用的模仿学习数据集中观察到的行为。这些发现突出了将预训练基础模型与在线自我改进相结合，以实现在机器人技术中自主技能获取的变革性潜力。我们的项目网站可在 https://self-improving-efms.github.io 找到。|
|**2025-09-18**|[Stochastic Bilevel Optimization with Heavy-Tailed Noise](http://arxiv.org/abs/2509.14952)|null|本论文研究了平滑双层优化问题，其中下层问题是强凸的，上层问题可能为非凸。我们关注随机设置，即算法可以访问带有重尾噪声的无偏随机梯度评估，这在许多机器学习应用中普遍存在，例如训练大型语言模型和强化学习。我们提出了一种嵌套循环归一化随机双层近似（N $^2$SBA）算法，用于找到一个$\epsilon$-驻点，其随机一阶预言机（SFO）复杂度为$\tilde{\mathcal{O}}\big(\kappa^{\frac{7p-3}{p-1}} \sigma^{\frac{p}{p-1}} \epsilon^{-\frac{4 p - 2}{p-1}}\big)$，其中$\kappa$是条件数，$p\in(1,2]$是噪声中心矩的阶数，$\sigma$是噪声水平。此外，我们将我们的思想专门应用于求解非凸-强凹极小极大优化问题，实现了$\epsilon$-驻点，其SFO复杂度为$\tilde{\mathcal O}\big(\kappa^{\frac{2p-1}{p-1}} \sigma^{\frac{p}{p-1}} \epsilon^{-\frac{3p-2}{p-1}}\big)$。上述所有上限在有界方差设置的特殊情况（即$p=2$ ）下均与已知最佳结果相匹配。|
|**2025-09-18**|[Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support](http://arxiv.org/abs/2509.14851)|null|同理心对于有效的心理健康支持至关重要，尤其是在处理长篇咨询文本（LCTs）时。然而，现有的大型语言模型（LLMs）通常生成的回复在语义上很流畅，但缺乏真正心理支持所需的结构化推理能力，尤其是在中文语境下。为了弥合这一鸿沟，我们引入了Empathy-R1，这是一个新颖的框架，它将同理心链（CoE）推理过程与强化学习（RL）相结合，以提升LCTs的回复质量。受认知行为疗法的启发，我们的CoE范式引导模型顺序推理求助者的情绪、原因和意图，使其思维过程既透明又可解释。我们的框架得益于一个新的大规模中文数据集Empathy-QA和两阶段训练过程。首先，监督微调灌输了CoE的推理结构。随后，在专用奖励模型的指导下，强化学习优化了最终回复的治疗相关性和上下文适宜性。实验表明，Empathy-R1在关键自动指标上取得了强大性能。更重要的是，人工评估证实了其优越性，显示出对强大基线的明显偏好，并在我们的新基准上实现了44.30%的Win@1比率。通过实现可解释且上下文细致入微的回复，Empathy-R1代表了在开发负责任且真正有益于心理健康支持的人工智能方面的一项重大进展。|
|**2025-09-18**|[ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based Tool Learning](http://arxiv.org/abs/2509.14718)|null|虽然强化学习（RL）越来越多地用于基于LLM的工具学习，但其效率常受到过多的简单样本的阻碍，这些样本随着训练的进行提供的学习价值逐渐递减。现有的动态采样技术不适用于工具学习固有的多任务结构和细粒度奖励机制。本文提出了带有课程学习的动态采样（DSCL）框架，该框架专门设计用于解决这一挑战，通过针对工具学习的独特特点：其多个相互依赖的子任务和多值奖励函数。DSCL包含两个核心组件：基于奖励的动态采样，它利用多维奖励统计数据（均值和方差）来优先处理有价值的数据；以及基于任务的动态课程学习，它自适应地将训练重点放在掌握程度较低的子任务上。通过广泛的实验，我们证明DSCL相较于强大的基线方法显著提升了训练效率和模型性能，在BFCLv3基准上取得了3.29%的提升。我们的方法提供了一种定制的解决方案，有效利用了工具学习中复杂的奖励信号和子任务动态，以取得卓越的成果。|
|**2025-09-18**|[RationAnomaly: Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning](http://arxiv.org/abs/2509.14693)|null|日志构成了表明软件系统运行状态的一种证据形式。自动化日志异常检测对于确保现代软件系统的可靠性至关重要。然而，现有方法面临显著局限性：传统深度学习模型缺乏可解释性和泛化能力，而利用大型语言模型的方法常常因不可靠性和事实不准确性而受阻。为解决这些问题，我们提出了RationAnomaly，一个新颖的框架，它通过协同思维链（CoT）微调与强化学习来增强日志异常检测。我们的方法首先使用CoT引导的监督微调灌输专家级的推理模式，该微调基于通过严格的专家驱动过程校正的高质量数据集。随后，一个采用多方面奖励函数的强化学习阶段优化了准确性和逻辑一致性，有效缓解了幻觉现象。实验结果表明，RationAnomaly优于最先进的基线，在关键基准测试中实现了更高的F1分数，同时提供透明、分步的分析输出。我们已发布相应的资源，包括代码和数据集。|
|**2025-09-18**|[LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning](http://arxiv.org/abs/2509.14680)|null|多智能体强化学习（MARL）在复杂环境中的智能决策方面具有巨大的潜力。然而，随着智能体数量的增加，它面临着协调性和可扩展性瓶颈。为了解决这些问题，我们提出了用于多智能体强化学习的基于大型语言模型赋能的专家演示框架（LEED）。LEED由两个组件组成：一个演示生成（DG）模块和一个策略优化（PO）模块。具体而言，DG模块利用大型语言模型生成与环境交互的指令，从而产生高质量的演示。PO模块采用去中心化训练范式，其中每个智能体利用生成的演示来构建专家策略损失，并将其与自身的策略损失相结合。这使得每个智能体能够基于专家知识和个体经验有效地个性化和优化其局部策略。实验结果表明，与最先进的基线相比，LEED在样本效率、时间效率和鲁棒的可扩展性方面表现出卓越的性能。|
|**2025-09-17**|[Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents](http://arxiv.org/abs/2509.14480)|null|有效的交互式工具使用要求智能体掌握工具集成推理（TIR）：这是一个涉及多轮规划和长上下文对话管理的复杂过程。为了训练智能体应对这一动态过程，特别是在多模态场景中，我们引入了一个支持交错语音-文本推演的强化学习（RL）沙盒环境。我们的核心策略，轮次级裁决强化学习（TARL），通过采用大型语言模型（LLM）作为评判者提供轮次级评估，解决了长周期任务中信用分配的挑战。为了增强探索，我们将混合任务训练课程与数学推理问题相结合。这种统一方法使基于文本的 $\tau$ -bench 上的任务通过率比强大的强化学习基线提高了6%以上。关键的是，我们展示了我们的框架适用于为智能体任务微调多模态基础模型。通过在交错语音-文本推演上训练基础多模态LLM，我们赋予其工具使用能力，为更自然、语音驱动的交互式智能体铺平了道路。|

## 大模型持续学习

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2025-09-26**|[Hierarchical Representation Matching for CLIP-based Class-Incremental Learning](http://arxiv.org/abs/2509.22645)|null|类增量学习（CIL）旨在使模型具备持续适应不断演进的数据流的能力。预训练视觉-语言模型（例如CLIP）的最新进展为此任务提供了强大基础。然而，现有方法通常依赖于“一张[类别]的照片”等简单模板，这忽略了视觉概念的层级性。例如，“猫”与“汽车”的识别依赖于粗粒度线索，而区分“猫”与“狮子”则需要细粒度细节。同样，CLIP中当前的特征映射仅依赖于最后一层的表示，忽略了早期层中包含的层级信息。在这项工作中，我们为基于CLIP的CIL引入了层级表示匹配（HERMAN）方法。我们的方法利用大型语言模型（LLMs）递归生成判别性文本描述符，从而通过显式层级线索扩充语义空间。这些描述符被匹配到语义层级的不同级别，并根据任务特定要求进行自适应路由，从而在增量任务中实现精确判别并缓解灾难性遗忘。在多个基准上进行的大量实验表明，我们的方法持续达到了最先进的性能。|
|**2025-09-26**|[We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong](http://arxiv.org/abs/2509.22510)|null|大型语言模型（LLM）在有益性、无害性和诚实性（HHH）等多个目标上的对齐对于其安全可靠的部署至关重要。先前的工作使用引导向量——注入到隐藏状态中的小型控制信号——来指导LLM的输出，通常通过一对一（1-to-1）Transformer解码器实现。在这种设置下，优化单一对齐目标可能会无意中覆盖为其他目标学习到的表示，从而导致灾难性遗忘。最近的方法通过一对多（1-to-N）Transformer解码器扩展了引导向量。虽然这缓解了灾难性遗忘，但朴素的多分支设计独立优化每个目标，这可能导致推理碎片化——HHH目标之间的输出可能变得不一致。我们提出了自适应多分支引导（AMBS），这是一个两阶段的1-to-N框架，用于统一高效的多目标对齐。在第一阶段，Transformer层的注意力后隐藏状态被计算一次以形成共享表示。在第二阶段，该表示被克隆到并行分支中，并通过策略-参考机制进行引导，从而在保持跨目标一致性的同时实现目标特定控制。在Alpaca、BeaverTails和TruthfulQA上的经验评估表明，AMBS在多个7B LLM骨干模型上持续改进了HHH对齐。例如，在DeepSeek-7B上，与朴素的1-to-N基线相比，AMBS将平均对齐分数提高了32.4%，并将不安全输出减少了11.0%，同时与最先进的方法保持竞争力。|
|**2025-09-26**|[Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting](http://arxiv.org/abs/2509.22195)|null|将视觉语言模型（VLM）在机器人远程操作数据上进行微调以创建视觉语言动作（VLA）模型，是训练通用策略的一个有前景的范式，但它面临一个根本性权衡：学习生成动作通常会削弱VLM的基础推理和多模态理解能力，从而阻碍其泛化到新颖场景、指令遵循和语义理解。我们认为这种灾难性遗忘是由于VLM的互联网规模预训练语料库与机器人微调数据之间存在分布不匹配。受此观察启发，我们引入了VLM2VLA：一种VLA训练范式，它首先通过自然语言表示低级动作，在数据层面解决这种不匹配。这种对齐使得仅使用低秩适应（LoRA）来训练VLA成为可能，从而最大限度地减少对VLM主干的修改并避免灾难性遗忘。因此，VLM可以在机器人远程操作数据上进行微调，而无需从根本上改变底层架构，也无需在互联网规模的VLM数据集上进行昂贵的联合训练。通过大量的视觉问答（VQA）研究和超过800次真实世界机器人实验，我们证明VLM2VLA保留了VLM的核心能力，实现了对需要开放世界语义推理和多语言指令遵循的新颖任务的零样本泛化。|
|**2025-09-26**|[ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation](http://arxiv.org/abs/2509.21730)|null|随着大语言模型（LLMs）日益融入日常生活，对不仅能够响应、还能主动提供个性化服务的AI助手的需求日益增长。尽管近期进展分别推动了主动性和个性化的发展，但它们的结合仍未得到充分探索。为弥补这一差距，我们引入了ProPerSim，这是一个新的任务与模拟框架，用于开发能够在真实的家庭场景中提供及时、个性化推荐的助手。在我们的模拟环境中，一个具有丰富人设的用户代理与助手交互，并根据每条建议与其偏好和情境的契合度提供评分。助手的目标是利用这些评分进行学习和适应，以随着时间推移获得更高的分数。基于ProPerSim，我们提出了ProPerAssistant，这是一个结合检索增强、偏好对齐的助手，能够通过用户反馈持续学习和适应。在32种不同人设上的实验表明，ProPerAssistant调整其策略并稳步提升用户满意度，突显了将主动性和个性化结合起来的巨大潜力。|
|**2025-09-24**|[A co-evolving agentic AI system for medical imaging analysis](http://arxiv.org/abs/2509.20279)|**[link](https://github.com/zhihuanglab/TissueLab)**|智能体AI在医疗保健和生物医学研究领域正迅速发展。然而，在医学图像分析中，由于缺乏强大的生态系统、不足的工具集以及实时交互式专家反馈的缺失，其性能和应用仍受限制。本文我们介绍“TissueLab”，一个协同进化的智能体AI系统，它允许研究人员直接提问，自动规划并生成可解释的工作流，并进行实时分析，专家可以在其中可视化中间结果并对其进行优化。TissueLab整合了病理学、放射学和空间组学领域的工具工厂。通过标准化各种工具的输入、输出和功能，该系统能够确定何时以及如何调用它们以解决研究和临床问题。在涉及具有临床意义的量化（为分期、预后和治疗计划提供信息）的各种任务中，与端到端视觉-语言模型（VLM）以及GPT-5等其他智能体AI系统相比，TissueLab均实现了最先进的性能。此外，TissueLab不断向临床医生学习，演化出改进的分类器和更有效的决策策略。通过主动学习，它能在数分钟内在新疾病背景下提供准确结果，而无需大规模数据集或冗长的再训练。作为可持续的开源生态系统发布，TissueLab旨在加速医学成像领域的计算研究和转化应用，同时为下一代医疗AI奠定基础。|
|**2025-09-24**|[CollaPipe: Adaptive Segment-Optimized Pipeline Parallelism for Collaborative LLM Training in Heterogeneous Edge Networks](http://arxiv.org/abs/2509.19855)|null|智能移动应用日益增长的需求使得多智能体协作结合基于Transformer的大型语言模型（LLMs）在移动边缘计算（MEC）网络中变得至关重要。然而，在此类环境中训练LLMs仍面临挑战，原因在于计算量大、高端到端延迟以及模型泛化能力有限。我们引入了CollaPipe，这是一种混合分布式学习框架，它集成了协作流水线并行与联邦聚合，以支持自演进智能网络。在CollaPipe中，编码器部分被自适应地划分为可变大小的段并部署在移动设备上用于流水线并行训练，而解码器则部署在边缘服务器上以处理生成任务。接着我们通过联邦聚合执行全局模型更新。为了提高训练效率，我们提出了一个联合优化问题，该问题自适应地分配模型段、微批次、带宽和传输功率。我们推导并利用了一个闭式收敛界来设计一个基于Lyapunov优化的动态段调度和资源分配（DSSDA）算法，确保了长期约束下的系统稳定性。在使用Transformer和BERT模型进行下游任务的广泛实验表明，CollaPipe将计算效率提高了高达15.09%，将端到端延迟降低了至少48.98%，并将单设备内存使用量削减了一半以上，从而在异构和动态通信环境中实现了在线学习。|
|**2025-09-23**|[Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning](http://arxiv.org/abs/2509.18942)|**[link](https://github.com/zzm-black/DEAL-Continuous-Low-Rank-Fine-Tuning)**|大语言模型（LLMs）的最新进展强调了微调（FT）技术在使LLMs适应特定任务方面的关键作用，尤其是在从头开始重新训练计算上不可行时。微调使LLMs能够利用任务或领域特定数据，生成更有效地满足目标应用需求的模型。然而，传统微调方法通常存在灾难性遗忘和次优数据效率的问题，限制了它们的实际应用性。为解决这些挑战，本文提出了DEAL，一个将低秩适应（LoRA）与持续微调策略相结合的新颖框架。通过引入知识保留和自适应参数更新模块，该框架缓解了现有微调方法的局限性，同时在隐私保护设置中保持了效率。在15个不同数据集上进行的实验表明，DEAL始终优于基线方法，在任务准确性和资源效率方面取得了显著提升。这些发现证明了我们方法在通过提高任务性能同时提升资源效率来推动LLMs持续适应方面的潜力。|
|**2025-09-24**|[COLT: Enhancing Video Large Language Models with Continual Tool Usage](http://arxiv.org/abs/2509.18754)|null|大语言模型（LLMs）的成功显著推动了视频理解的研究。为了利用训练有素的专家模型（即工具）的优势，视频大语言模型优先探索工具使用能力。现有方法要么提示闭源大语言模型，要么采用指令微调范式进行工具使用微调。然而，这些方法假设存在一个固定的工具库，并且难以泛化到工具数据不断演进和涌入的真实世界环境。为此，我们提出通过持续工具使用（简称COLT）来增强开源视频大语言模型，使其在连续的工具流中自动获取工具使用能力，而不会遭受对过去学习工具的“灾难性遗忘”。具体来说，我们的COLT集成了一个可学习的工具码本作为工具专用记忆系统。然后，根据用户指令与码本中工具特征之间的相似性，动态选择相关工具。为了释放视频大语言模型的工具使用潜力，我们收集了一个以视频为中心的工具使用指令微调数据集VideoToolBench。在先前的视频大语言模型基准和工具使用专用VideoToolBench数据集上进行的大量实验证明了我们提出的COLT的最先进性能。|
|**2025-09-23**|[COLT: Enhancing Video Large Language Models with Continual Tool Usage](http://arxiv.org/abs/2509.18754)|null|大语言模型（LLMs）的成功显著推动了视频理解研究。为了利用训练有素的专家模型（即工具）的优势，视频大语言模型优先探索工具使用能力。现有方法要么提示闭源大语言模型，要么采用指令微调范式进行工具使用微调。然而，这些方法假设存在一个固定的工具库，难以泛化到工具数据不断演变和涌入的真实世界环境。为此，我们提出通过持续工具使用能力（简称COLT）来增强开源视频大语言模型，使其能够在连续的工具流中自动获取工具使用能力，而不会遭受对过去学习工具的“灾难性遗忘”。具体而言，我们的COLT引入了一个可学习的工具码本作为特定于工具的记忆系统。然后，根据用户指令与码本中工具特征之间的相似性动态选择相关工具。为了释放视频大语言模型的工具使用潜力，我们收集了一个以视频为中心的工具使用指令微调数据集VideoToolBench。在之前的视频大语言模型基准和特定于工具使用的VideoToolBench数据集上进行的大量实验证明了我们提出的COLT具有最先进的性能。|
|**2025-09-25**|[Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills](http://arxiv.org/abs/2509.18597)|**[link](https://github.com/Ghiara/LYRA)**|基于大型语言模型（LLMs）的机器人操作代码生成近期展现出潜力，能将人类指令直接转化为可执行代码，但现有方法仍存在噪声，受限于固定的基元和有限的上下文窗口，并且难以应对长周期任务。尽管已探索闭环反馈，但纠正后的知识通常以不当格式存储，限制了泛化能力并导致灾难性遗忘，这突显了学习可复用技能的必要性。此外，仅依赖LLM指导的方法在极长周期场景中经常失败，原因在于LLM在机器人领域推理能力有限，而此类问题对人类而言通常很容易识别。为解决这些挑战，我们提出了一种人机协作框架，该框架将纠正编码为可复用技能，并由外部记忆和带有提示机制的检索增强生成（Retrieval-Augmented Generation, RAG）支持以实现动态复用。在Ravens、Franka Kitchen和MetaWorld以及真实世界环境中的实验表明，我们的框架实现了0.93的成功率（比基线高出多达27%），并在纠正轮次中效率提高了42%。它能鲁棒地解决极长周期任务，例如“建造房屋”，这需要对20多个基元进行规划。|
|**2025-09-23**|[Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills](http://arxiv.org/abs/2509.18597)|**[link](https://github.com/Ghiara/LYRA)**|基于大型语言模型（LLMs）的机器人操作代码生成最近通过将人类指令直接转换为可执行代码而展现出潜力，但现有方法仍然存在噪声，受限于固定的基元和有限的上下文窗口，并且难以处理长程任务。尽管已探索闭环反馈，但纠正后的知识通常以不当格式存储，限制了泛化能力并导致灾难性遗忘，这突出了学习可重用技能的必要性。此外，仅依赖LLM指导的方法在超长程场景中经常失败，原因在于LLM在机器人领域推理能力的局限性，而这些问题对人类来说通常很容易识别。为应对这些挑战，我们提出了一种人机协作（human-in-the-loop）框架，该框架将纠正编码为可重用技能，并由外部记忆以及带有提示机制的检索增强生成（Retrieval-Augmented Generation）提供支持以实现动态重用。在Ravens、Franka Kitchen和MetaWorld以及真实世界环境中的实验表明，我们的框架实现了0.93的成功率（比基线高出27%），并在纠正轮次中实现了42%的效率提升。它能够稳健地解决超长程任务，例如“建造房屋”，这需要对20多个基元进行规划。|
|**2025-09-22**|[AIMMerging: Adaptive Iterative Model Merging Using Training Trajectories for Language Model Continual Learning](http://arxiv.org/abs/2509.17348)|null|持续学习（CL）对于在动态的现实世界环境中部署大型语言模型（LLMs）至关重要，且无需昂贵的再训练。最近基于模型合并的方法引起了广泛关注，但它们在学习新知识和防止遗忘之间的权衡管理上仍面临挑战，这主要源于次优的合并次数和合并频率。在本文中，我们提出了一种新颖的持续学习框架——自适应迭代模型合并（AimMerging），它利用来自训练轨迹的学习和遗忘信号来动态监测模型的训练状态。在动态监测的指导下，训练轨迹引导的合并控制器自适应地确定迭代融合的时机和频率，而基于排练的知识融合模块则计算合并权重并执行融合。在三个不同模型尺寸（从770M到13B）的CL基准上进行的全面实验表明，AimMerging相较于现有最先进方法取得了显著的性能提升，在FWT和BWT上分别实现了80%和59%的平均相对提升。源代码已提供以供复现。|
|**2025-09-21**|[LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization](http://arxiv.org/abs/2509.17183)|null|对齐在大型语言模型（LLMs）中发挥着关键作用，使其与人类在特定任务/领域的偏好保持一致。传统的对齐方法面临灾难性遗忘问题，即模型在适应新偏好或领域时会丢失先前获得的知识。我们引入了LifeAlign，一个用于终身对齐的新颖框架，它使LLMs能够在顺序学习任务中保持一致的人类偏好对齐，而不会遗忘先前学到的知识。我们的方法包含两项关键创新。首先，我们提出了一种聚焦式偏好优化策略，该策略使LLMs与新偏好对齐，同时防止先前任务中获得的知识受到侵蚀。其次，我们开发了一种从短期到长期的记忆巩固机制，该机制利用内在降维将去噪的短期偏好表示融合到稳定的长期记忆中，从而实现跨多样化领域的对齐模式的高效存储和检索。我们在涵盖不同领域和偏好类型的多个顺序对齐任务中评估了LifeAlign。实验结果表明，与现有终身学习方法相比，我们的方法在保持偏好对齐质量和知识保留方面均取得了卓越的性能。代码和数据集将在GitHub上发布。|
|**2025-09-21**|[MCTS-EP: Empowering Embodied Planning with Online Preference Optimization](http://arxiv.org/abs/2509.17116)|null|本文介绍了MCTS-EP，一个结合大语言模型（LLM）与蒙特卡洛树搜索（MCTS）来训练具身智能体的在线学习框架。MCTS-EP集成了三个关键组件：用于偏好数据收集的MCTS引导探索、高效的多模态推理机制，以及基于偏好优化的迭代训练流程。我们理论上证明了当损失函数为强凸时，MCTS-EP实现了优于传统在线策略算法的性能界限，并展示了它可以被表述为GAIL的一种搜索增强变体。MCTS-EP在多个基准测试中取得了最先进的性能。在ALFWorld中，它对于文本任务和视觉任务分别达到了92%和87%的成功率。在WebShop中，它达到了0.81的平均奖励。MTCS-EP还将视觉ALFWorld中的平均交互步数从18.7/19.5步减少到10.2/9.9步。代码可在以下网址获取：https://github.com/xuhang-2/Embodied-Agent-Planning|
|**2025-09-23**|[K-DeCore: Facilitating Knowledge Transfer in Continual Structured Knowledge Reasoning via Knowledge Decoupling](http://arxiv.org/abs/2509.16929)|null|持续结构化知识推理（CSKR）专注于训练模型处理顺序任务，其中每个任务都涉及将自然语言问题转化为基于结构化知识的结构化查询。现有通用持续学习方法在应用于此任务时面临显著挑战，包括对异构结构化知识的泛化能力差以及随着任务增加而参数增长导致的推理效率低下。为解决这些局限性，我们提出了一种新颖的CSKR框架K-DeCore，它在固定数量的可调参数下运行。与现有方法不同，K-DeCore引入了一种知识解耦机制，将推理过程解耦为任务特定和任务无关阶段，有效弥合了不同任务之间的差距。在此基础上，K-DeCore集成了一种用于不同阶段的双视角记忆巩固机制，并引入了一种结构引导的伪数据合成策略，以进一步增强模型的泛化能力。在四个基准数据集上进行的大量实验证明，利用各种骨干大型语言模型，K-DeCore在多项指标上均优于现有持续学习方法。|
|**2025-09-21**|[AdaptiveGuard: Towards Adaptive Runtime Safety for LLM-Powered Software](http://arxiv.org/abs/2509.16861)|null|护栏对于大型语言模型（LLM）驱动的软件的安全部署至关重要。与具有有限、预定义输入-输出空间、本质上限制不安全行为的传统基于规则的系统不同，LLM实现了开放式、智能的交互——这为通过用户输入发起的越狱攻击打开了大门。护栏作为保护层，在不安全的提示到达LLM之前对其进行过滤。然而，先前的研究表明，即使是面对GPT-4o等高级模型，越狱攻击的成功率仍然超过70%。尽管LlamaGuard等护栏报告的准确率高达95%，但我们的初步分析显示，当面对未见攻击时，它们的性能会急剧下降——低至12%。这凸显了一个日益增长的软件工程挑战：如何构建一个能够动态适应新出现的威胁的部署后护栏？为解决此问题，我们提出了AdaptiveGuard，这是一种自适应护栏，它将新型越狱攻击检测为分布外（OOD）输入，并通过持续学习框架学习防御这些攻击。通过经验评估，AdaptiveGuard实现了96%的OOD检测准确率，仅用两个更新步骤即可适应新攻击，并在适应后在分布内数据上保持超过85%的F1分数，优于其他基线。这些结果表明，AdaptiveGuard是一种能够在部署后响应新出现的越狱策略而演变的护栏。我们已在https://github.com/awsm-research/AdaptiveGuard发布了我们的AdaptiveGuard和所研究的数据集，以支持进一步的研究。|
|**2025-09-19**|[Towards Robust Visual Continual Learning with Multi-Prototype Supervision](http://arxiv.org/abs/2509.16011)|null|语言引导监督利用来自预训练语言模型 (PLM) 的冻结语义目标，已成为视觉持续学习 (CL) 的一个有前景的范式。然而，依赖单一目标引入了两个关键限制：1) 语义模糊性，即多义类别名称会导致冲突的视觉表示；2) 类内视觉多样性，即单一原型无法捕捉类内丰富的视觉外观多样性。为此，我们提出了 MuproCL，一个用多个上下文感知原型取代单一目标的新颖框架。具体而言，我们采用一个轻量级大型语言模型 (LLM) 代理来执行类别消歧和视觉模态扩展，以生成一组鲁棒的语义原型。LogSumExp 聚合机制允许视觉模型对于给定图像自适应地与最相关的原型对齐。在各种持续学习 (CL) 基线上进行的大量实验表明，MuproCL 持续提升了性能和鲁棒性，为语言引导的持续学习开辟了一条更有效的路径。|
|**2025-09-19**|[UNIV: Unified Foundation Model for Infrared and Visible Modalities](http://arxiv.org/abs/2509.15642)|null|联合RGB可见光与红外感知的需求迅速增长，尤其是在各种天气条件下实现鲁棒性能。尽管用于RGB可见光和红外数据的预训练模型在各自领域表现出色，但在多模态场景（如配备两种传感器的自动驾驶汽车）中它们往往表现不佳。为解决这一挑战，我们提出了一种受生物学启发，用于红外和可见光模态的统一基础模型（UNIV），该模型具有两项关键创新。首先，我们引入了逐块跨模态对比学习（PCCL），这是一种注意力引导的蒸馏框架，它模仿视网膜水平细胞的侧向抑制，能够在实现有效跨模态特征对齐的同时，与任何基于Transformer的架构兼容。其次，我们的双知识保持机制模拟了视网膜双极细胞的信号路由——结合LoRA适配器（增加2%参数）与同步蒸馏以防止灾难性遗忘，从而复刻了视网膜的明视（视锥细胞驱动）和暗视（视杆细胞驱动）功能。为支持跨模态学习，我们引入了MVIP数据集，这是迄今为止最全面的可见光-红外基准，它包含98,992对精确对齐的图像，涵盖多种场景。大量实验证明，UNIV在红外任务上表现优越（语义分割mIoU提升1.7，目标检测mAP提升0.7），同时在可见光RGB任务上保持了99%以上的基线性能。我们的代码可在https://github.com/fangyuanmao/UNIV获取。|
|**2025-09-18**|[The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning](http://arxiv.org/abs/2509.15097)|null|深度学习日益增长的计算和能源需求，特别是在基础模型和大语言模型（LLM）等大规模架构中，对可持续性构成了严峻挑战。传统的基于梯度的训练方法效率低下，需要大量的迭代更新和高功耗。为解决这些局限性，我们提出了一种混合框架，将分层分解与基于FPGA的直接方程求解和增量学习相结合。我们的方法将神经网络分为两个功能层级：较低层通过FPGA上的单步方程求解进行优化，以实现高效且可并行化的特征提取；而较高层则采用自适应增量学习，以支持持续更新而无需完全重新训练。在此基础上，我们引入了复合LLM框架，该框架明确地在两个层级中部署了LLM模块。较低层LLM以最小的能源开销处理可重用表示学习，而较高层LLM则通过能源感知更新执行自适应决策。这种集成设计增强了可扩展性，减少了冗余计算，并符合可持续AI的原则。理论分析和架构见解表明，我们的方法显著降低了计算成本，同时保持了高模型性能，使其非常适合在能源受限环境中进行边缘部署和实时适应。|
|**2025-09-18**|[Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models](http://arxiv.org/abs/2509.15076)|null|空气污染仍然是对公众健康和环境可持续性构成严重威胁，然而传统监测系统常受限于有限的空间覆盖和可及性。本文提出一种人工智能驱动的代理，该代理利用天空图像预测环境空气污染水平，并使用生成式建模合成逼真的污染场景可视化。我们的方法将统计纹理分析与监督学习相结合进行污染分类，并利用视觉-语言模型（VLM）引导的图像生成来产生可解释的空气质量状况表示。生成的视觉效果模拟不同程度的污染，为面向用户的界面提供基础，从而提高透明度并支持知情的环境决策。这些输出可以无缝集成到旨在增强态势感知并鼓励基于实时预测的行为响应的智能应用中。我们使用城市天空图像数据集验证了我们的方法，并证明了其在污染水平估计和语义一致的视觉合成方面的有效性。系统设计还进一步融合了以人为中心的用户体验原则，以确保空气质量预测的可及性、清晰度和公众参与度。为支持可扩展和节能的部署，未来的迭代将整合一种绿色CNN架构，该架构通过基于FPGA的增量学习得到增强，从而实现在边缘平台上的实时推理。|
|**2025-09-18**|[Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via Cross-Modal Geometric Rectification](http://arxiv.org/abs/2509.14958)|null|3D数字内容的快速增长要求针对开放世界场景的可扩展识别系统。然而，现有的3D类别增量学习方法在极端数据稀缺下，由于几何不对齐和纹理偏置，表现不佳。尽管最近的方法将3D数据与2D基础模型（例如CLIP）相结合，但它们存在由纹理偏置投影和几何-纹理线索不加区分的融合导致的语义模糊问题，从而导致不稳定的决策原型和灾难性遗忘。为解决这些问题，我们提出了跨模态几何校正（CMGR）框架，该框架通过利用CLIP的层次空间语义来增强3D几何保真度。具体而言，我们引入了一个结构感知几何校正模块，通过注意力驱动的几何融合，将3D部件结构与CLIP的中间空间先验进行分层对齐。此外，一个纹理放大模块合成最小但具有区分性的纹理，以抑制噪声并增强跨模态一致性。为进一步稳定增量原型，我们采用一个基类-新类判别器来隔离几何变异。大量实验表明，我们的方法显著改善了3D小样本类别增量学习，在跨域和域内设置中均实现了卓越的几何一致性和对纹理偏置的鲁棒性。|
|**2025-09-18**|[Cross-Modal Knowledge Distillation for Speech Large Language Models](http://arxiv.org/abs/2509.14930)|null|在这项工作中，我们首次系统性评估了语音大语言模型中的灾难性遗忘和模态不一致性，结果表明，引入语音能力即使在输入仍为文本时，也会损害知识和推理能力，并且当查询为语音时，性能会进一步下降。为了解决这些挑战，我们提出了一个跨模态知识蒸馏框架，该框架利用文本到文本和语音到文本两种通道，将知识从基于文本的教师模型迁移到语音大语言模型。在对话和音频理解任务上进行的广泛实验验证了我们方法在保留文本知识、改善跨模态对齐以及增强基于语音交互中的推理能力方面的有效性。|
|**2025-09-18**|[Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications](http://arxiv.org/abs/2509.14921)|null|CLIP等基础模型在多种视觉任务中展现了卓越的零样本和少样本迁移能力。然而，当针对人脸识别（FR）、形态攻击检测（MAD）和演示攻击检测（PAD）等高度专业化的生物识别任务进行微调时，这些模型可能会出现过度专业化，从而失去其基础优势之一——跨领域泛化能力。在这项工作中，我们通过评估三个针对FR、MAD和PAD任务微调的CLIP模型实例，系统地量化了这些权衡。我们评估了每个经过适应的模型以及原始的CLIP基线模型在14个通用视觉数据集上的零样本和线性探测协议下的性能，同时也在常见的FR、MAD和PAD基准测试中进行了评估。我们的结果表明，微调模型存在过度专业化问题，尤其是在针对复杂的人脸识别任务进行微调时。此外，我们的结果指出，任务复杂性和分类头设计（多类别FR与二元类别MAD和PAD）与灾难性遗忘的程度相关。采用ViT-L骨干网络的FRoundation模型在大型FR基准测试IJB-C上优于其他方法，实现了高达58.52%的改进。然而，它在ImageNetV2上经历了显著的性能下降，仅达到51.63%，而基线CLIP模型达到了69.84%。此外，较大的CLIP架构始终比小型变体保留了更多的模型原始泛化能力，表明增加模型容量可能有助于减轻过度专业化。|
|**2025-09-18**|[OnlineMate: An LLM-Based Multi-Agent Companion System for Cognitive Support in Online Learning](http://arxiv.org/abs/2509.14803)|null|在在线学习环境中，学生通常缺乏个性化的同伴互动，而这种互动在支持认知发展和学习投入方面起着关键作用。尽管之前的研究已经利用大语言模型（LLMs）为学生模拟交互式动态学习环境，但这些互动仍然局限于对话交流，缺乏对学习者个性化学习和认知状态的洞察和适应。结果是，学生与AI学习伙伴进行讨论的兴趣不高，并且他们难以从这些互动中获得启发。为了解决这一挑战，我们提出了OnlineMate，一个由LLMs驱动并整合了心智理论（ToM）的多智能体学习伙伴系统。OnlineMate能够模拟同伴般的智能体角色，在协作讨论中适应学习者的认知状态，并推断他们的心理状态，例如误解、困惑或动机。通过整合心智理论能力，该系统可以动态调整其交互策略，以支持高阶思维和认知的发展。在模拟学习场景中的实验结果表明，OnlineMate有效地促进了深度学习和讨论，同时增强了在线教育环境中的认知投入。|
|**2025-09-18**|[AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production](http://arxiv.org/abs/2509.14647)|null|随着大语言模型（LLMs）在自动化复杂多智能体工作流中的日益普及，组织面临着来自错误、涌现行为和系统性故障的日益增长的风险，而这些是当前评估方法未能捕捉到的。我们提出了AgentCompass，这是首个专门为智能体工作流的部署后监控和调试设计的评估框架。AgentCompass通过一个结构化的多阶段分析管道模拟专家调试员的推理过程，包括：错误识别和分类、主题聚类、定量评分和策略性总结。该框架通过一个双记忆系统——情景记忆和语义记忆——得到进一步增强，从而实现了跨执行的持续学习。通过与设计伙伴的合作，我们展示了该框架在真实世界部署中的实用性，并在公开可用的TRAIL基准上确立了其有效性。AgentCompass在关键指标上取得了最先进的结果，同时揭示了人工标注中遗漏的关键问题，强调了其作为一种强大且以开发者为中心的工具，在生产环境中对智能体系统进行可靠监控和改进的作用。|
|**2025-09-17**|[CL $^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction](http://arxiv.org/abs/2509.13672)|null|自动化写作辅助在不同学术领域日益增长的需求，凸显了对能够跨学科适应的鲁棒中文语法纠错（CGEC）系统的需求。然而，现有的CGEC研究很大程度上缺乏用于多学科的学术写作的专用基准，忽视了持续学习（CL）作为处理领域特定语言变异和防止灾难性遗忘的一种有前景的解决方案。为了填补这一关键空白，我们引入了CL$^2$GEC，这是首个用于中文文献语法纠错的持续学习基准，旨在评估跨多个学术领域的自适应CGEC。我们的基准包含10,000个人工标注的句子，涵盖10个学科，每个学科都展现出独特的语言风格和错误模式。CL$^2$ GEC专注于在持续学习设置下评估语法纠错，模拟顺序暴露于不同的学术学科，以反映真实的编辑动态。我们在顺序微调、参数高效适应和四种代表性CL算法下评估了大型语言模型，使用了标准GEC指标和适应任务级别变化的持续学习指标。实验结果表明，基于正则化的方法比基于回放或朴素顺序的方法能更有效地缓解遗忘。我们的基准为未来在跨不同学术领域的自适应语法纠错研究提供了坚实的基础。|
|**2025-09-10**|[A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs](http://arxiv.org/abs/2509.09727)|null|问答 (QA) 在金融教育中扮演核心角色，然而现有的大型语言模型 (LLM) 方法往往未能捕捉到解决金融问题所需的细致入微且专业化的推理。金融领域要求多步定量推理、熟悉领域特定术语以及理解现实世界情景。我们提出了一个多智能体框架，该框架利用基于角色的提示 (role-based prompting) 来提高在领域特定问答 (QA) 上的性能。我们的框架包含一个基础生成器 (Base Generator)、一个证据检索器 (Evidence Retriever) 和一个专家评审员智能体 (Expert Reviewer)，它们通过单次迭代 (single-pass iteration) 协同工作以生成一个精炼的答案。我们使用来自在线学习平台 Study.com 的 3,532 个专家设计的金融教育问题对我们的框架进行了评估。我们利用检索增强生成 (RAG) 从 6 本金融教科书中获取上下文证据，并为领域专家评审员设计了提示策略。我们的实验表明，基于批判的精炼 (critique-based refinement) 将答案准确率相较于零样本思维链 (zero-shot Chain-of-Thought) 基线提高了 6.6-8.3%，其中 Gemini-2.0-Flash 取得了最高性能。此外，我们的方法使 GPT-4o-mini 能够达到与经过金融领域微调的 FinGPT-mt_Llama3-8B_LoRA 模型相当的性能。我们的结果展示了一种提升金融问答 (QA) 的成本效益方法，并为多智能体金融大型语言模型 (LLM) 系统的进一步研究提供了见解。|
|**2025-09-10**|[Ubiquitous Intelligence Via Wireless Network-Driven LLMs Evolution](http://arxiv.org/abs/2509.08400)|null|我们引入泛在智能作为一种范式，其中大语言模型(LLMs)在无线网络驱动的生态系统中演进。与静态模型部署不同，这种方法通过网络和LLMs之间的协同，实现了可扩展和持续的智能提升。无线网络支持系统编排的终身学习，而LLMs则推动了更具适应性和响应性的下一代网络发展。这种协同演进凸显了向自我完善系统的转变，在多样化和资源受限的环境中持续提升能力。|

## Transformer

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2025-09-26**|[RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformer](http://arxiv.org/abs/2509.22323)|null|扩散Transformer (DiT) 在视觉生成方面表现出色，但仍受限于缓慢的采样速度。现有的免训练加速器——如步长缩减、特征缓存和稀疏注意力——能够提升推理速度，但通常依赖于对所有图像采用统一的启发式方法或手动设计的自适应策略，牺牲了一定的生成质量。另一方面，动态神经网络提供了每图像自适应加速，但其高昂的微调成本限制了更广泛的应用。为解决这些局限性，我们引入了RAPID3：用于扩散Transformer的三级强化加速策略，该框架在不对基础生成器进行任何更新的情况下实现了图像级加速。具体而言，三个轻量级策略头——步长跳过、缓存重用和稀疏注意力——观察当前的去噪状态，并在每个时间步独立决定其相应的加速策略。所有策略参数通过组相对策略优化 (GRPO) 进行在线训练，同时生成器保持冻结。同时，一个对抗学习的判别器增强了奖励信号，仅当生成的样本与原始模型的分布保持接近时才提升回报，从而阻止奖励作弊。在包括Stable Diffusion 3和FLUX在内的最先进DiT骨干网络上，RAPID3实现了近3倍的采样加速，且保持了具有竞争力的生成质量。|
|**2025-09-26**|[Statistical Advantage of Softmax Attention: Insights from Single-Location Regression](http://arxiv.org/abs/2509.21936)|null|大型语言模型依赖于采用 softmax 激活的注意力机制。然而，softmax 相对于替代方案（例如逐分量或线性）的优势仍知之甚少，并且许多理论研究侧重于更易于分析的线性化注意力。在这项工作中，我们通过对单位置回归任务进行一项有原则的研究来弥补这一空白，其中输出取决于随机位置处单个输入 token 的线性变换。借鉴统计物理学的思想，我们开发了在高维极限下对基于注意力的预测器的分析，其中泛化性能由一小部分序参数捕获。在总体层面，我们表明 softmax 达到了贝叶斯风险，而线性注意力则根本不足。然后我们检查其他激活函数，以确定哪些属性对于最优性能是必要的。最后，我们分析了有限样本机制：我们提供了测试误差的渐近表征，并表明，虽然 softmax 不再是贝叶斯最优的，但它始终优于线性注意力。我们讨论了与基于梯度的算法优化的联系。|
|**2025-09-26**|[SynerGen: Contextualized Generative Recommender for Unified Search and Recommendation](http://arxiv.org/abs/2509.21777)|null|大规模推荐系统中主流的“检索-然后-排序”流水线由于其架构分离和不同的优化目标，面临校准不当和工程开销问题。尽管近期的生成式序列模型通过自回归地生成排序后的项目，在统一检索和排序方面展现出潜力，但现有解决方案通常只解决个性化搜索或无查询推荐中的一个，并且在尝试统一两者时常常表现出性能权衡。我们提出了SynerGen，这是一种新颖的生成式推荐模型，通过为个性化搜索和推荐提供单一的生成式骨干，弥补了这一关键空白，同时在检索和排序任务中表现出色。我们的仅解码器Transformer在行为序列上进行训练，利用InfoNCE进行检索的联合优化和混合点对损失进行排序，从而使来自搜索的语义信号能够改善推荐，反之亦然。我们还提出了一种新颖的时间感知旋转位置嵌入，以有效地将时间信息整合到注意力机制中。与强大的生成式推荐器和联合搜索推荐基线模型相比，SynerGen在广泛采用的推荐和搜索基准上取得了显著改进。这项工作证明了单一生成式基础模型在工业规模统一信息访问方面的可行性。|
|**2025-09-25**|[Decoupled-Value Attention for Prior-Data Fitted Networks: GP Inference for Physical Equations](http://arxiv.org/abs/2509.20950)|null|先验数据拟合网络 (PFNs) 是耗时的高斯过程 (GP) 推断的一种有前途的替代方案，可用于创建物理系统的快速替代模型。PFN通过将GP中的贝叶斯推断替换为学习到的预测模型的单次前向传播，减轻了GP训练的计算负担。然而，使用标准的Transformer注意力机制，PFNs在高维回归任务上表现出有限的有效性。我们引入了解耦值注意力机制 (DVA)，其灵感来源于GP的特性，即函数空间完全由输入上的核函数刻画，且预测均值是训练目标的加权和。DVA仅从输入计算相似度，并仅通过值传播标签。因此，所提出的DVA模拟了高斯过程的更新，同时保持无核化。我们证明，扩展PFNs的关键因素是注意力规则而不是架构本身。具体而言，我们的结果表明 (a) 局部注意力机制在不同维度设置下的PFNs中持续降低了样本外验证损失，在五维和十维情况下，验证损失降低了50%以上，并且 (b) 注意力机制的作用比骨干架构的选择更具决定性，表明基于CNN的PFNs可以与基于Transformer的PFNs表现相当。所提出的PFNs提供了64维潮流方程近似，平均绝对误差约为1E-3，同时比精确GP推断快80多倍。|
|**2025-09-25**|[Why Attention Fails: The Degeneration of Transformers into MLPs in Time Series Forecasting](http://arxiv.org/abs/2509.20942)|null|基于Transformer的架构在自然语言处理和计算机视觉领域取得了高性能，然而许多研究表明，它们在时间序列预测中并未展现出明显优势，甚至在某些情况下表现不如简单的线性基线。然而，这些研究大多未能彻底探究Transformer失败背后的原因。为了更好地理解时间序列Transformer (TST)，我们设计了一系列实验，逐步将Transformer修改为多层感知机(MLP)以探究注意力机制的影响。令人惊讶的是，在现有的时间序列Transformer中，Transformer块经常退化为简单的多层感知机。我们设计了一个可解释的数据集来探究注意力机制失败背后的原因，并揭示了注意力机制并未以预期方式工作。我们从理论上分析了这种现象背后的原因，表明当前的嵌入方法未能使Transformer在结构良好的潜在空间中发挥作用，并进一步分析了嵌入失败更深层次的根本原因。|
|**2025-09-25**|[DENet: Dual-Path Edge Network with Global-Local Attention for Infrared Small Target Detection](http://arxiv.org/abs/2509.20701)|null|红外小目标检测对于灾害预警和海上监视等遥感应用至关重要。然而，由于缺乏独特的纹理和形态特征，红外小目标极易融入杂乱和嘈杂的背景中。为该任务设计深度模型的一个根本挑战在于，捕获微小目标的高分辨率空间细节与提取较大目标的鲁棒语义上下文之间存在内在冲突，这通常会导致特征错位和次优性能。现有方法通常依赖于固定梯度算子或简单的注意力机制，这些方法不足以在低对比度和高噪声条件下准确提取目标边缘。在本文中，我们提出了一种新颖的双路径边缘网络，通过将边缘增强和语义建模解耦为两个互补的处理路径来明确解决这一挑战。第一条路径采用双向交互模块，该模块使用局部自注意力和全局自注意力来捕获多尺度局部和全局特征依赖性。基于Transformer架构的全局注意力机制整合了长距离语义关系和上下文信息，确保了鲁棒的场景理解。第二条路径引入了多边缘细化器，该细化器使用级联的泰勒有限差分算子在多个尺度上增强细粒度边缘细节。这种数学方法结合注意力驱动的门控机制，实现了对不同尺寸目标的精确边缘定位和特征增强，同时有效抑制噪声。我们的方法为精确的红外小目标检测和定位提供了一个有前景的解决方案，在一个统一的框架中结合了结构语义和边缘细化。|
|**2025-09-25**|[From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training](http://arxiv.org/abs/2509.20072)|null|大语言模型（LLMs）的近期进展引起了将LLMs能力扩展到多模态场景的广泛关注，特别是对于语音到语音对话系统。然而，现有处理交错音频和文本的多模态模型依赖于自回归方法，忽略了文本依赖于目标-目标关系，而音频主要依赖于源-目标关系。在这项工作中，我们提出了Text-to-Talk（TtT），一个统一的音频-文本框架，它将自回归（AR）文本生成与非自回归（NAR）音频扩散集成在一个Transformer中。通过利用吸收离散扩散的任意顺序自回归特性，我们的方法为文本和音频提供了一个统一的训练目标。为了支持这种混合生成范式，我们设计了一种模态感知注意力机制，该机制对文本强制执行因果解码，同时允许在音频片段内进行双向建模，并进一步引入了三种训练策略以减少训练-测试差异。在推理过程中，TtT采用块级扩散以并行合成音频，同时灵活处理可变长度输出。在音频问答（Audio-QA）和自动语音识别（ASR）任务上的大量实验证明了我们方法的有效性，并通过详细的消融研究验证了每个提出的组件。我们将开源我们的模型、数据和代码，以促进该方向的未来研究。|
|**2025-09-23**|[Mamba Modulation: On the Length Generalization of Mamba](http://arxiv.org/abs/2509.19633)|null|Transformer模型中注意力机制的二次复杂度促使了具有次二次缩放特性的替代架构（如状态空间模型）的发展。其中，Mamba已成为一种领先架构，在一系列语言建模任务中取得了最先进的结果。然而，当Mamba应用于比预训练时更长的上下文时，其性能会显著下降，这揭示了其对上下文长度扩展的显著敏感性。通过详细分析，我们将这一局限性归因于其状态空间动态的分布外行为，特别是在状态转移矩阵 $\mathbf{A}$的参数化中。与近期将这种敏感性归因于离散化时间步长累积消失（即$\exp(-\sum_{t=1}^N\Delta_t)$）的工作不同，我们建立了输入长度趋于无穷大时状态收敛行为与转移矩阵$\mathbf{A}$的谱之间的联系，为$\mathbf{A}$在长度扩展中的作用提供了充分依据的解释。接下来，为了克服这一挑战，我们提出了一种方法，通过选择性地调制每一层中$\mathbf{A}$矩阵的谱，将谱缩放应用于预训练的Mamba模型，以实现鲁棒的长上下文泛化。我们表明，在仅调制$\Delta_t$ 会失败的设置中，我们的方法可以显著提高性能，从而验证了我们的见解，并为具有结构化转移矩阵的状态空间模型实现更好的长度泛化提供了途径。|
|**2025-09-23**|[Circuit Complexity From Physical Constraints: Scaling Limitations of Attention](http://arxiv.org/abs/2509.19161)|null|我们认为，源自 $NC, AC, TC$的标准电路复杂性度量提供的实用信息有限，并且现在不足以进一步区分模型表达能力。为了解决这些新限制，我们定义了一种新颖的局部一致性概念，以及一个捕捉扩展物理电路基本约束的电路复杂性类别$RC(\cdot)$家族。借助于$RC(\cdot)$的视角，我们表明运行时为$\omega(n^{3/2})$ 的注意力机制无法扩展以适应日益复杂数据集的熵。我们的结果同时为定义Transformer表达能力的有意义界限提供了一种方法，并自然地揭示了注意力机制有限的适用性。|
|**2025-09-23**|[BiGraspFormer: End-to-End Bimanual Grasp Transformer](http://arxiv.org/abs/2509.19142)|null|双手抓取对于机器人操作大型复杂物体至关重要。然而，现有方法要么仅专注于单臂抓取，要么采用独立的抓取生成和双手评估阶段，导致了包括碰撞风险和受力不均在内的协调问题。为解决这些局限性，我们提出了BiGraspFormer，一个统一的端到端Transformer框架，可以直接从物体点云中生成协调的双手抓取。我们的核心思想是单臂引导双手 (SGB) 策略，该策略首先使用Transformer解码器生成多样化的单臂抓取候选，然后通过专门的注意力机制利用它们学到的特征，联合预测双手姿态和质量分数。这种条件策略降低了12自由度搜索空间的复杂性，同时确保了协调的双手操作。综合仿真实验和真实世界验证表明，BiGraspFormer持续优于现有方法，同时保持了高效的推理速度 (<0.05秒)，证实了我们框架的有效性。代码和补充材料可在 https://sites.google.com/bigraspformer 获取。|
|**2025-09-23**|[Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model](http://arxiv.org/abs/2509.19028)|null|本文提出了一种用于食物图像的弱监督语义分割方法，该方法利用了Segment Anything模型（SAM）的零样本能力和可提示性，以及视觉Transformer（ViT）的注意力机制。具体而言，我们使用来自ViT的类激活图（CAM）为SAM生成提示，从而得到适用于食物图像分割的掩码。该ViT模型（Swin Transformer）仅使用图像级标注进行训练，消除了训练过程中对像素级标注的需求。此外，为了提高SAM生成掩码的质量，我们研究了结合图像预处理技术以及单掩码和多掩码SAM生成策略的使用。该方法在FoodSeg103数据集上进行了评估，平均每张图像生成2.4个掩码（不包括背景），并在多掩码情景下实现了0.54的平均交并比（mIoU）。我们设想所提出的方法可作为加速食物图像标注任务的工具，或作为食物和营养追踪应用中的集成组件。|
|**2025-09-23**|[Pay More Attention To Audio: Mitigating Imbalance of Cross-Modal Attention in Large Audio Language Models](http://arxiv.org/abs/2509.18816)|**[link](https://github.com/MyParadise21/MATA)**|大型音频语言模型（LALMs）常面临音频-文本注意力不平衡问题，尤其是在Transformer架构的多模态融合层中，模型倾向于优先处理文本信息而非声学信息。这种偏差阻碍了LALMs充分利用声学线索的能力，导致其在音频推理任务上性能欠佳。为缓解此问题，我们提出MATA，这是一种新颖的免训练方法，能动态地促使LALMs在自注意力机制中更多地关注音频token。具体而言，MATA在原始注意力得分计算后进行干预，仅针对中间层的最后一个token，且不引入额外参数或计算开销。在MMAU和MMAR基准上的实验证实了MATA的有效性，并带来了持续的性能提升。值得注意的是，在MMAR上，MATA首次使一个开源模型超越了专有的Gemini 2.0 Flash。我们的工作为缓解注意力偏差提供了一种有效的解决方案，并为增强多模态模型的音频处理能力开辟了新的研究方向。|
|**2025-09-23**|[Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification](http://arxiv.org/abs/2509.18692)|null|随着社会的快速发展和科学技术的不断进步，食品工业对生产质量和效率的要求越来越高。食品图像分类在实现生产线上的自动化质量控制、支持食品安全监管和推动智慧农业生产方面发挥着至关重要的作用。然而，由于 Vision Transformer 模型参数量大、计算复杂度高，这项任务面临挑战。为了解决这些问题，我们提出了一种融合窗口多头注意力机制（WMHAM）和空间注意力机制（SAM）的轻量级食品图像分类算法。WMHAM 通过高效的窗口划分捕获局部和全局上下文特征，从而降低了计算成本，而 SAM 则自适应地强调关键空间区域，以提高判别性特征表示。在 Food-101 和 Vireo Food-172 数据集上进行的实验表明，我们的模型分别达到了 95.24% 和 94.33% 的准确率，同时与基线方法相比显著减少了参数量和 FLOPs。这些结果证实，所提出的方法在计算效率和分类性能之间实现了有效平衡，使其非常适合部署在资源受限的环境中。|
|**2025-09-22**|[GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting](http://arxiv.org/abs/2509.18457)|null|本文提出了GluMind，一种基于Transformer的多模态框架，专为连续和长期血糖预测而设计。GluMind设计了两种注意力机制，包括交叉注意力（cross-attention）和多尺度注意力（multi-scale attention），它们并行运行并提供了准确的预测性能。交叉注意力有效地整合了血糖数据与其他生理和行为信号，例如活动、压力和心率，解决了与采样率变化相关的挑战以及它们对鲁棒预测的不利影响。此外，多尺度注意力机制捕获了长程时间依赖性。为了减轻灾难性遗忘，GluMind将一种知识保留技术融入基于Transformer的预测模型中。知识保留模块不仅增强了模型保留先验知识的能力，而且提升了其整体预测性能。我们在最近发布的AIREADI数据集上评估了GluMind，该数据集包含来源于健康人、糖尿病前期患者和2型糖尿病患者的行为和生理数据。我们研究了GluMind在引入新患者队列时进行持续学习的性能稳定性和适应性。实验结果表明，GluMind持续优于其他最先进的预测模型，在均方根误差（RMSE）和平均绝对误差（MAE）方面分别实现了约15%和9%的改进。|
|**2025-09-22**|[Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image Diffusion Transformers](http://arxiv.org/abs/2509.18096)|**[link](https://github.com/cvlab-kaist/Seg4Diff)**|文本到图像扩散模型通过其跨模态注意力机制隐式地关联文本概念，擅长将语言提示翻译成逼真的图像。近期的多模态扩散Transformer通过在拼接的图像和文本token上引入联合自注意力，扩展了这一能力，实现了更丰富、更具扩展性的跨模态对齐。然而，关于这些注意力图如何以及在何处对图像生成做出贡献的详细理解仍然有限。在本文中，我们引入了Seg4Diff（Diffusion的分割），这是一个用于分析MM-DiT注意力结构的系统框架，重点关注特定层如何将语义信息从文本传播到图像。通过全面的分析，我们识别出一个语义关联专家层，这是一个特定的MM-DiT模块，能够持续地将文本token与空间上连贯的图像区域对齐，自然地生成高质量的语义分割掩码。我们进一步证明，应用一种使用带有掩码标注的图像数据的轻量级微调方案，可以增强这些层的语义分组能力，从而提高了分割性能和生成图像的保真度。我们的研究结果表明，语义分组是扩散Transformer的一种涌现特性，并且可以被选择性地放大，以提升分割和生成性能，为弥合视觉感知和生成之间鸿沟的统一模型铺平道路。|
|**2025-09-22**|[M3ET: Efficient Vision-Language Learning for Robotics based on Multimodal Mamba-Enhanced Transformer](http://arxiv.org/abs/2509.18005)|null|近年来，多模态学习在机器人视觉和信息融合中已变得至关重要，尤其是在理解复杂环境中人类行为方面。然而，当前方法难以充分利用文本模态，它们依赖于监督预训练模型，这限制了在无监督机器人环境中进行语义提取，尤其是在存在显著模态损失的情况下。这些方法也往往是计算密集型的，导致在实际应用中资源消耗较高。为了应对这些挑战，我们提出了多模态Mamba增强型Transformer (M3ET)，这是一种轻量级模型，旨在实现高效多模态学习，特别是在移动平台上。通过结合Mamba模块和一种基于语义的自适应注意力机制，M3ET优化了特征融合、对齐和模态重建。我们的实验表明，M3ET提升了跨任务性能，预训练推理速度提高了2.3倍。具体而言，M3ET在核心VQA任务上的准确率保持在0.74，而模型参数量减少了0.67。尽管在EQA任务上的性能有限，但M3ET的轻量级设计使其非常适合部署在资源受限的机器人平台上。|
|**2025-09-22**|[Training-free Truthfulness Detection via Value Vectors in LLMs](http://arxiv.org/abs/2509.17932)|null|大型语言模型经常生成事实不准确的输出，这促使人们努力检测其内容的真实性。大多数现有方法依赖于对内部激活进行训练探针，但这些方法存在可扩展性和泛化性问题。一种近期无需训练的方法NoVo通过利用模型本身的统计模式来解决这一挑战。然而，它只专注于注意力机制，可能忽略了多层感知机（MLP）模块——Transformer模型中一个已知支持事实回忆的核心组件。在本文中，我们展示了MLP模块中某些值向量表现出与真实性相关的统计模式。基于这一发现，我们提出TruthV，一种简单且可解释的无需训练方法，通过利用这些值向量来检测内容的真实性。在NoVo基准测试中，TruthV显著优于NoVo和对数似然基线，这表明MLP模块——尽管在之前的无需训练工作中被忽视——编码了丰富而有用的真实性检测信号。这些发现为真实性在大型语言模型中是如何内部表示的提供了新见解，并推动了对可扩展和可解释的真实性检测的进一步研究。|
|**2025-09-22**|[Conv-like Scale-Fusion Time Series Transformer: A Multi-Scale Representation for Variable-Length Long Time Series](http://arxiv.org/abs/2509.17845)|null|时间序列分析在处理变长数据和实现鲁棒泛化方面面临严峻挑战。尽管基于Transformer的模型推动了时间序列任务的发展，但它们常常面临特征冗余和有限泛化能力的问题。借鉴经典CNN架构的金字塔结构，我们提出了一种基于类卷积尺度融合Transformer的多尺度表示学习框架。我们的方法引入了一种类似时间卷积的结构，将分块操作与多头注意力相结合，从而实现了渐进式时间维度压缩和特征通道扩展。我们还进一步开发了一种新颖的跨尺度注意力机制，用于在不同时间尺度上进行有效的特征融合，以及一种用于变长序列的对数空间归一化方法。大量实验表明，与最先进的方法相比，我们的框架在预测和分类任务中实现了卓越的特征独立性、降低了冗余并获得了更优的性能。|
|**2025-09-19**|[Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers](http://arxiv.org/abs/2509.16058)|null|注意力机制已成为人工智能中不可或缺的一部分，通过借鉴人类认知，显著提升了模型性能和可扩展性。与此同时，认知科学中的注意力图式理论（AST）提出，个体通过构建注意力自身的模型来管理注意力，从而有效分配认知资源。受AST启发，我们引入了ASAC（基于注意力图式的注意力控制），将注意力图式概念整合到人工神经网络中。我们的初步实验专注于将ASAC模块嵌入Transformer架构中。该模块采用矢量量化变分自编码器（VQVAE）作为注意力抽象器和控制器，促进精确的注意力管理。通过显式建模注意力分配，我们的方法旨在提高系统效率。我们证明了ASAC在视觉和自然语言处理（NLP）领域均有效，强调了其提高分类准确性和加快学习过程的能力。我们对视觉Transformer在各种数据集上的实验表明，注意力控制器不仅提高了分类准确性，还加速了学习。此外，我们还证明了模型在噪声和分布外数据集上的鲁棒性和泛化能力。另外，我们展示了在多任务设置中的性能提升。快速实验表明，基于注意力图式的模块增强了对对抗攻击的韧性，优化注意力以提高学习效率，并促进了有效的迁移学习和少样本学习。这些有前景的结果建立了认知科学与机器学习之间的联系，揭示了AI系统中注意力机制的有效利用。|
|**2025-09-19**|[Interplay Between Belief Propagation and Transformer: Differential-Attention Message Passing Transformer](http://arxiv.org/abs/2509.15637)|null|基于Transformer的神经网络译码器已成为纠错编码的一种有前景的方法，它结合了数据驱动的适应性与长程依赖的有效建模。本文提出了一种新颖的译码器架构，将经典的信念传播原理与Transformer设计相结合。我们引入了一个利用全局码本结构的可微分伴随式损失函数，以及一个优化比特和伴随式嵌入交互的差分注意力机制。实验结果表明，与现有基于Transformer的译码器相比，性能有持续改进，我们的方法在短到中等长度的LDPC码上超越了传统的信念传播译码器。|
|**2025-09-18**|[Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems](http://arxiv.org/abs/2509.15448)|null|Transformer模型及其注意力机制在机器学习领域具有革命性意义。虽然最初提出用于语言数据，但它们很快被应用于图像、视频、图等具有各种信号几何结构的数据模态。尽管具有这种多功能性，将注意力机制泛化到数据以不同尺度、可能来自不同模态呈现的场景并非易事。尝试在Transformer中整合层次结构和多模态主要基于特设启发式方法，这些方法无法无缝泛化到具有潜在不同结构的类似问题。为解决此问题，在本文中，我们采取了一种根本不同的方法：我们首先提出了一种数学构造来表示多模态、多尺度数据。然后，我们从熵最小化的第一性原理出发，数学推导了所提出的构造的神经注意力机制。我们表明，所推导的公式在与标准Softmax注意力最接近的意义上是最佳的，同时整合了源于问题层次/几何信息的归纳偏置。我们进一步提出了一种基于动态规划的高效算法来计算我们推导出的注意力机制。通过将其整合到Transformer中，我们表明所提出的层次注意力机制不仅可以用于从头开始训练层次/多模态设置下的Transformer模型，而且还可以用于在训练后向经典的、预训练的Transformer模型注入层次信息，从而以零样本方式获得更高效的模型。|
|**2025-09-18**|[SPH-Net: A Co-Attention Hybrid Model for Accurate Stock Price Prediction](http://arxiv.org/abs/2509.15414)|null|预测股票价格走势在金融分析中构成严峻挑战，这归因于市场数据固有的波动性、非平稳性和非线性特征。本文介绍了SPH-Net（股票价格预测混合神经网络），这是一种创新的深度学习框架，旨在提高金融市场时间序列预测的准确性。所提出的架构采用一种新颖的协同注意力机制，该机制首先通过Vision Transformer处理时间模式，随后通过注意力机制进行精炼的特征提取，从而捕获市场数据中的全局和局部依赖关系。为了严格评估模型的性能，我们在八个多样化的股票数据集上进行了全面的实验：AMD、Ebay、Facebook、FirstService Corp、Tesla、Google、Mondi ADR和Matador Resources。每个数据集都使用六个基本市场指标进行标准化：开盘价、最高价、最低价、收盘价、调整后收盘价和成交量，代表了一整套用于全面市场分析的特征。实验结果表明，SPH-Net在所有评估指标上始终优于现有的股票预测模型。该模型的卓越性能源于其有效捕获复杂时间模式的能力，同时保持对市场噪声的鲁棒性。通过显著提高金融时间序列分析中的预测准确性，SPH-Net为投资者和金融分析师提供了宝贵的决策支持能力，有可能在波动的市场条件下实现更明智的投资策略和风险评估。|
|**2025-09-18**|[Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering](http://arxiv.org/abs/2509.15024)|null|注意力机制已成为现代神经网络的核心，推动了各个领域的突破性进展。然而，它们在图结构数据（其中捕获拓扑连接至关重要）上的应用仍探索不足且性能欠佳，尤其是在图聚类任务中，相较于图神经网络（GNNs）。GNN 倾向于过分强调邻域聚合，导致节点表示的同质化。相反，Transformer 倾向于过度关注全局，突出远距离节点却牺牲了有意义的局部模式。这种对立提出了一个关键问题：注意力机制对于无监督图学习是否本质上是多余的？为了解决这个问题，我们进行了一项全面的实证分析，揭示了 GNN 和 Transformer 在图聚类中的互补弱点。受这些见解的启发，我们提出了注意力图聚类网络（AGCN），这是一种新颖的架构，重新诠释了“图即注意力”这一理念。AGCN 直接将注意力机制嵌入到图结构中，从而实现有效的全局信息提取，同时保持对局部拓扑线索的敏感。我们的框架结合了理论分析，以对比 AGCN 与 GNN 和 Transformer 的行为，并引入了两项创新：(1) 一种 KV 缓存机制，以提高计算效率；(2) 一种成对间隔对比损失，以提升注意力空间的判别能力。广泛的实验结果表明，AGCN 的性能优于现有最先进的方法。|
|**2025-09-18**|[Exploring the Global-to-Local Attention Scheme in Graph Transformers: An Empirical Study](http://arxiv.org/abs/2509.14863)|null|图形Transformer (GT) 在图表示学习中展现出巨大潜力。GT 的架构通常将图神经网络 (GNN) 与全局注意力机制并行集成或作为注意力机制的前置，从而形成局部-全局或局部到全局的注意力方案。然而，由于全局注意力机制主要捕获节点间的长程依赖关系，这些集成方案可能会遭受信息损失，即 GNN 学习到的局部邻域信息可能会被注意力机制稀释。因此，我们提出了 G2LFormer，它采用了一种新颖的全局到局部注意力方案，其中浅层网络层使用注意力机制捕获全局信息，而深层网络层则采用 GNN 模块学习局部结构信息，从而防止节点忽略其直接邻居。为使局部层能够保留来自全局层的有益信息并减轻信息损失，我们引入了一种有效的跨层信息融合策略，同时在可扩展性方面实现了可接受的权衡。为了验证全局到局部注意力方案的可行性，我们在节点级和图级任务上将 G2LFormer 与最先进的线性 GT 和 GNN 进行了比较。结果表明，G2LFormer 表现出优异的性能，同时保持了线性复杂度。|
|**2025-09-18**|[Stochastic Clock Attention for Aligning Continuous and Ordered Sequences](http://arxiv.org/abs/2509.14678)|null|我们为连续且有序的序列提出了一种明确地作为对齐模型发挥作用的注意力机制，该机制是许多序列到序列任务的核心。标准的缩放点积注意力依赖于位置编码和掩码，但它不强制连续性或单调性，而这对于帧同步目标至关重要。我们提出了针对源和目标学习的非负“时钟”，并将注意力建模为这些时钟的相遇概率；路径积分推导得到一个封闭形式的、类高斯的评分规则，该规则具有对因果、平滑、近对角线对齐的内在偏置，无需外部位置正则化器。该框架支持两种互补的模式：当全局长度可用时，用于并行解码的归一化时钟，以及用于自回归解码的未归一化时钟——两者都是几乎无参数的、可直接替换的方案。在Transformer文本到语音测试平台中，这种构造产生了更稳定的对齐，并提高了对全局时间尺度变化的鲁棒性，同时与缩放点积基线相比，准确性持平或有所提高。我们推测它适用于其他连续目标，包括视频和时序信号建模。|
|**2025-09-18**|[SpeechMLC: Speech Multi-label Classification](http://arxiv.org/abs/2509.14677)|null|本文提出一个多标签分类框架，用于检测语音样本中的多种说话风格。与以往主要关注识别单一目标风格的研究不同，我们的框架能在一个统一的结构中有效捕获多种说话者特征，使其适用于广义的人机交互应用。所提出的框架在Transformer解码器内部整合了交叉注意力机制，以从输入语音中提取与每个目标标签相关的显著特征。为了缓解多标签语音数据集中固有的数据不平衡问题，我们采用了一种基于语音生成模型的数据增强技术。我们通过在已知语料库和未知语料库上的多项客观评估，验证了我们模型的有效性。此外，我们通过考虑人类标注一致性对模型性能的影响，分析了人类感知对分类准确性的影响。|
|**2025-09-17**|[White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation](http://arxiv.org/abs/2509.13907)|null|少样本三维点云分割（FS-PCS）旨在仅给定少量标记样本的情况下，预测未标记点云的每个点的标签。为了从有限的支持集中提取判别性表示，现有方法使用最远点采样等传统算法构建原型。然而，我们指出其初始随机性显著影响FS-PCS性能，并且原型生成过程尽管普遍存在但仍未得到充分探索。这促使我们研究一种基于注意力机制的先进原型生成方法。尽管注意力机制有其潜力，我们发现朴素模块存在可学习原型tokens与支持特征之间的分布差异问题。为了克服这一问题，我们提出了白化聚合与恢复模块（WARM），该模块通过将交叉注意力置于白化和着色变换之间来解决错位问题。具体来说，白化操作在注意力处理之前将支持特征与原型tokens对齐，随后着色操作恢复经过注意力处理的tokens的原始分布。这种简单而有效的设计实现了鲁棒的注意力，从而通过捕获支持特征之间的语义关系生成具有代表性的原型。我们的方法在多个FS-PCS基准上以显著优势实现了最先进的性能，并通过广泛的实验证明了其有效性。|
|**2025-09-17**|[ST-LINK: Spatially-Aware Large Language Models for Spatio-Temporal Forecasting](http://arxiv.org/abs/2509.13753)|null|交通预测在智能交通系统中是一个关键问题。在近期研究中，大型语言模型（LLMs）已成为一种有前景的方法，但其主要为序列化标记处理而设计的内在结构，在有效捕捉空间依赖性方面带来了显著挑战。具体而言，LLMs在建模空间关系方面的固有局限性及其与图结构空间数据在架构上的不兼容性，在很大程度上仍未得到解决。为克服这些局限性，我们引入了ST-LINK，这是一个新颖的框架，旨在增强大型语言模型捕捉时空依赖性的能力。其关键组成部分是空间增强注意力（SE-Attention）和记忆检索前馈网络（MRFFN）。SE-Attention扩展了旋转位置嵌入，将空间相关性作为直接的旋转变换整合到注意力机制中。这种方法在最大化空间学习的同时，保留了LLM固有的序列处理结构。同时，MRFFN动态检索并利用关键历史模式，以捕捉复杂的时序依赖性并提高长期预测的稳定性。在基准数据集上的综合实验表明，ST-LINK超越了传统的深度学习和LLM方法，并能有效捕捉常规交通模式和突发变化。|
|**2025-09-16**|[SAGA: Selective Adaptive Gating for Efficient and Expressive Linear Attention](http://arxiv.org/abs/2509.12817)|null|尽管Transformer架构在建模长距离依赖方面表现出色，使其在视觉任务中得到广泛应用，但基于softmax的注意力机制的二次复杂度带来了主要瓶颈，尤其是在处理高分辨率图像时。线性注意力通过将注意力计算从 $(QK)V$重新表述为$Q(KV)$，从而将复杂度从$\mathcal{O}(N^2)$降低到$\mathcal{O}(N)$，同时保留了全局感受野，提供了一种有前景的替代方案。然而，大多数现有方法均匀地压缩历史键值（KV）信息，这可能导致特征冗余以及与查询（Q）的方向对齐丢失。这种均匀压缩导致低秩$KV$特征图，从而导致与softmax注意力相比的性能差距。为了缓解这一局限性，我们提出了用于高效且富有表现力的线性注意力的选择性自适应门控（SAGA），该方法引入了输入自适应的可学习门控，以选择性地调节信息聚合到$KV$特征图中。这些门控增强了语义多样性，并缓解了传统线性注意力中固有的低秩约束。此外，我们提出了一种高效的Hadamard积分解方法用于门控计算，该方法不引入额外的内存开销。实验表明，SAGA在分辨率为$1280 \times 1280$ 时，与PVT-T相比，在吞吐量方面实现了1.76倍的提升，在峰值GPU内存方面实现了2.69倍的降低。此外，它在ImageNet数据集上的top-1准确率提高了高达4.4%，证明了计算效率和模型有效性。|
|**2025-09-16**|[BATR-FST: Bi-Level Adaptive Token Refinement for Few-Shot Transformers](http://arxiv.org/abs/2509.12768)|null|视觉Transformer (ViT) 在计算机视觉应用中展现出巨大潜力。然而，它们在小样本学习中的性能受限于细化token级交互、难以处理有限训练数据以及建立强大归纳偏置等挑战。现有方法常依赖不灵活的token匹配或基本相似性度量，这限制了全局上下文的有效融合和局部特征的细化。为解决这些挑战，我们提出针对小样本Transformer的双层自适应Token细化 (BATR-FST)，这是一种两阶段方法，能逐步改进token表示并为小样本分类保持鲁棒的归纳偏置。在预训练阶段，掩码图像建模 (MIM) 通过重建被掩码的图像区域，为视觉Transformer (ViT) 提供可迁移的块级表示，为后续适应奠定鲁棒基础。在元微调阶段，BATR-FST 融合了一个双层自适应Token细化模块，该模块利用Token聚类来捕获局部交互，通过不确定性感知Token加权优先处理可靠特征，并采用双层注意力机制来平衡簇内和簇间关系，从而促进彻底的token细化。此外，图Token传播确保了支持集和查询集实例之间的语义一致性，而类别分离惩罚则保持了不同类别边界，增强了判别能力。在三个基准小样本数据集上进行的大量实验表明，BATR-FST 在1-shot和5-shot场景中均取得了优异结果，并改进了基于Transformer的小样本分类。|
|**2025-09-15**|[Dynamic Relational Priming Improves Transformer in Multivariate Time Series](http://arxiv.org/abs/2509.12196)|null|Transformer模型中的标准注意力机制采用静态的token表示，这些表示在每一层的所有成对计算中保持不变。这限制了它们在表示上与每对token交互中潜在的多元关系动态的对齐。虽然它们在关系相对同质的领域表现出色，但标准注意力机制的静态关系学习难以捕捉多元时间序列（MTS）数据中多样化、异构的通道间依赖关系——在单个系统中，不同通道对之间的交互可能受完全不同的物理定律或时间动态支配。为了更好地调整注意力机制以适应此类领域现象，我们提出了带有动态关系预置（prime attention）的注意力机制。与标准注意力机制不同，在标准注意力机制中，每个token在其所有成对交互中都呈现相同的表示，而prime attention通过可学习的调制动态地（或按每次交互）调整每个token，以最好地捕捉每对token独特的关联动态，从而为该特定关系优化每次成对交互。prime attention的这种表示可塑性使得在MTS中有效提取关系特定信息成为可能，同时保持与标准注意力机制相同的渐近计算复杂度。我们的结果表明，prime attention在各项基准测试中始终优于标准注意力机制，实现了高达6.5%的预测准确性提升。此外，我们发现与标准注意力机制相比，prime attention在使用减少高达40%的序列长度时，取得了相当或更优的性能，进一步证明了其卓越的关系建模能力。|
|**2025-09-14**|[Length-Aware Rotary Position Embedding for Text-Speech Alignment](http://arxiv.org/abs/2509.11084)|null|许多近期文本到语音（TTS）系统基于Transformer架构，并采用交叉注意力机制用于文本-语音对齐。在这些系统中，旋转位置编码（RoPE）常被用于编码文本和语音表示中的位置信息。在这项工作中，我们引入了长度感知RoPE（LARoPE），作为RoPE的一个简单而有效的扩展，能够改善文本-语音对齐。与依赖绝对索引的RoPE不同，LARoPE使用长度归一化索引计算查询（query）和键（key）位置之间的相对距离。实验结果表明，LARoPE持续优于RoPE，提供了更快的损失收敛、更准确的文本-语音对齐和更高的整体TTS质量。此外，LARoPE对发音时长变化表现出更强的鲁棒性，并在长达30秒的扩展语音生成中保持稳定性能，而RoPE则出现显著性能下降。值得注意的是，我们的方法在标准零样本TTS基准测试上，词错误率达到了最先进水平。|

## 生成模型

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2025-09-26**|[Pixel Motion Diffusion is What We Need for Robot Control](http://arxiv.org/abs/2509.22652)|null|我们提出了DAWN (Diffusion is All We Need for robot control)，这是一个统一的基于扩散的框架，用于语言条件下的机器人操作，它通过结构化的像素运动表示桥接了高级运动意图和低级机器人动作。在DAWN中，高级和低级控制器都被建模为扩散过程，从而产生了一个完全可训练的端到端系统，具有可解释的中间运动抽象。DAWN在具有挑战性的CALVIN基准测试中取得了最先进的结果，展示了强大的多任务性能，并进一步在MetaWorld上验证了其有效性。尽管模拟与现实之间存在显著的领域差距且现实世界数据有限，我们仍展示了仅需少量微调即可实现可靠的真实世界迁移，阐明了基于扩散的运动抽象在机器人控制中的实际可行性。我们的结果表明，将扩散建模与以运动为中心的表示相结合，可作为可扩展和鲁棒机器人学习的强大基线。项目页面：https://nero1342.github.io/DAWN/|
|**2025-09-26**|[RefAM: Attention Magnets for Zero-Shot Referral Segmentation](http://arxiv.org/abs/2509.22650)|null|目前大多数指代分割方法仅通过微调或组合多个预训练模型来实现强大性能，这通常以额外训练和架构修改为代价。同时，大规模生成扩散模型编码了丰富的语义信息，使其作为通用特征提取器具有吸引力。在这项工作中，我们引入了一种新方法，直接利用来自扩散Transformer的特征（即注意力分数）进行下游任务，既不需要架构修改也不需要额外训练。为了系统地评估这些特征，我们通过涵盖图像和视频的视觉-语言基础任务扩展了基准。我们的关键见解是停用词充当注意力磁铁：它们累积过剩注意力，并且可以通过过滤来减少噪声。此外，我们识别出在更深层出现的全局注意力汇聚点（GAS），并表明它们可以安全地被抑制或重定向到辅助词元，从而得到更清晰、更准确的基础图。我们进一步提出了一种注意力再分配策略，其中添加的停用词将背景激活划分成更小的簇，产生更清晰、更局部的热图。基于这些发现，我们开发了RefAM，一个简单的免训练基础框架，它结合了交叉注意力图、GAS处理和再分配。在零样本指代图像和视频分割基准上，我们的方法始终优于现有方法，在无需微调或额外组件的情况下建立了新的领先水平。|
|**2025-09-26**|[Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs](http://arxiv.org/abs/2509.22646)|null|人类能否识别AI生成（伪造）视频并提供有根据的理由？尽管视频生成模型发展迅速，但一个关键维度——即人类能否在生成的视频中检测到深度伪造痕迹（也就是揭示视频为机器生成的时空有依据的视觉伪影）——却在很大程度上被忽视了。我们引入了DeeptraceReward，这是首个细粒度、空间感知和时间感知的基准，用于标注人类感知到的伪造痕迹，以作为视频生成奖励。该数据集包含4.3K条详细标注，涵盖3.3K个高质量生成视频。每条标注都提供自然语言解释，精确指出包含感知痕迹的边界框区域，并标记精确的起始和结束时间戳。我们将这些标注整合为9个主要类别的深度伪造痕迹，这些痕迹使人类能够识别视频为AI生成，并训练多模态语言模型（LMs）作为奖励模型，以模仿人类的判断和定位。在DeeptraceReward上，我们的7B奖励模型在伪造线索识别、定位和解释方面平均比GPT-5高出34.7%。有趣的是，我们观察到一个一致的难度梯度：二元真假分类明显比细粒度深度伪造痕迹检测更容易；在后者中，性能从自然语言解释（最容易）到空间定位，再到时间标注（最难）逐渐下降。通过突出人类感知的深度伪造痕迹，DeeptraceReward为实现具有社会意识和值得信赖的视频生成提供了一个严谨的测试平台和训练信号。|
|**2025-09-26**|[Language Models Can Learn from Verbal Feedback Without Scalar Rewards](http://arxiv.org/abs/2509.22638)|**[link](https://github.com/sail-sg/feedback-conditional-policy)**|大型语言模型（LLMs）通常通过人类或AI反馈进行强化学习（RL）训练，然而此类方法通常将细致的反馈压缩成标量奖励，丢弃了其大部分丰富性并导致尺度不平衡。我们提出将语言反馈视为条件信号。受文本到图像生成中语言先验的启发（这使得从未见过的提示也能生成新颖的输出），我们引入了反馈条件策略（FCP）。FCP直接从响应-反馈对中学习，通过离线数据的最大似然训练来近似反馈条件后验。我们进一步开发了一个在线自举阶段，在该阶段中，策略在积极条件下生成并接收新的反馈以完善自身。这重新定义了反馈驱动学习为条件生成而非奖励优化，为LLMs直接从语言反馈中学习提供了一种更具表达力的方式。我们的代码可在https://github.com/sail-sg/feedback-conditional-policy获取。|
|**2025-09-26**|[Scale-Wise VAR is Secretly Discrete Diffusion](http://arxiv.org/abs/2509.22636)|null|自回归（AR）Transformer已成为视觉生成领域的一种强大范式，这主要归因于它们的可扩展性、计算效率以及与语言和视觉统一的架构。其中，下一尺度预测视觉自回归生成（VAR）最近展现出卓越性能，甚至超越了基于扩散的模型。在这项工作中，我们重新审视了VAR，并揭示了一个理论见解：当配备马尔可夫注意力掩码时，VAR在数学上等同于离散扩散。我们将这种重新解释命名为基于离散扩散的可扩展视觉细化（SRDD），从而在AR Transformer与扩散模型之间建立了一个原则性桥梁。利用这一新视角，我们展示了如何直接引入扩散模型的优势，例如迭代细化，并减少VAR中的架构低效性，从而带来更快的收敛、更低的推理成本以及改进的零样本重建。在多个数据集上，我们表明VAR的基于扩散的视角在效率和生成方面带来了持续的提升。|
|**2025-09-26**|[Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance](http://arxiv.org/abs/2509.22635)|null|小样本图像分类由于标注样本数量有限而仍具挑战。近期方法探索了使用文本到图像扩散模型生成合成训练数据，但通常需要大量的模型微调或外部信息源。我们提出了一种新颖的免训练方法DIPSY，它利用IP-Adapter进行图像到图像翻译，仅使用现有的小样本生成高度判别性的合成图像。DIPSY引入了三项关键创新：(1)一种扩展的无分类器引导方案，能够独立控制正向和负向图像条件；(2)一种基于类别相似度的采样策略，用于识别有效的对比样本；以及(3)一个简单而有效的流程，无需模型微调或外部字幕生成和过滤。在十个基准数据集上的实验表明，我们的方法达到了最先进或可比的性能，同时消除了对生成模型适应或对用于字幕生成和图像过滤的外部工具的依赖。我们的结果突出了利用双图像提示结合正负向引导在生成类别判别性特征方面的有效性，特别是对于细粒度分类任务。|
|**2025-09-26**|[A Theoretical Analysis of Discrete Flow Matching Generative Models](http://arxiv.org/abs/2509.22623)|null|我们对端到端训练的离散流匹配（DFM）生成模型提供了理论分析。DFM是一种有前景的离散生成建模框架，它通过训练神经网络来近似变换速度场，从而学习底层的生成动力学。我们的分析通过分解最终的分布估计误差，建立了一系列明确的保证。我们首先证明，生成分布与目标分布之间的全变差距离受所学速度场的风险控制。接着，我们通过分析该风险的两个主要来源来对其进行约束：(i) 近似误差，我们量化了Transformer架构表示真实速度的能力；以及 (ii) 估计误差，我们推导了统计收敛速度，以限制在有限数据集上训练所产生的误差。通过综合这些结果，我们首次提供了正式证明，表明随着训练集规模的增加，经过训练的DFM模型生成的分布可证明地收敛到真实数据分布。|
|**2025-09-26**|[LongLive: Real-time Interactive Long Video Generation](http://arxiv.org/abs/2509.22622)|null|我们提出了LongLive，一个帧级自回归（AR）框架，用于实时交互式长视频生成。长视频生成在效率和质量方面都面临挑战。扩散模型和强制扩散模型可以生成高质量视频，但由于双向注意力机制而效率低下。因果注意力AR模型支持KV缓存以加速推理，但由于长视频训练期间的内存挑战，在长视频上质量往往下降。此外，除了静态提示生成之外，交互能力（例如流式提示输入）对于动态内容创建至关重要，使用户能够实时引导叙事。这种交互式需求显著增加了复杂性，尤其是在确保提示转换期间的视觉一致性和语义连贯性方面。为了解决这些挑战，LongLive采用了因果的帧级AR设计，该设计集成了KV重缓存机制（该机制用新提示刷新缓存状态，以实现平滑、一致的切换）、流式长时微调（以支持长视频训练并对齐训练与推理，即“训练长，测试长”）以及短窗口注意力机制与帧级注意力槽（简称“帧槽”）相结合（在实现更快生成的同时保持长程一致性）。凭借这些关键设计，LongLive仅需32个GPU日就将一个13亿参数的短片段模型微调至分钟级视频生成。在推理时，LongLive在单个NVIDIA H100上保持20.7 FPS，并在短视频和长视频的VBench测试中都取得了优异性能。LongLive在单个H100 GPU上支持长达240秒的视频。LongLive进一步支持INT8量化推理，且仅有微小的质量损失。|
|**2025-09-26**|[Transport Based Mean Flows for Generative Modeling](http://arxiv.org/abs/2509.22592)|null|流匹配生成模型已成为连续数据生成的一种强大范式，在图像、3D形状和点云等领域取得了最先进的结果。尽管它们取得了成功，但这些模型由于需要大量顺序采样步骤而面临推理速度慢的问题。近期工作旨在通过减少采样步骤数量来加速推理。特别是，均值流提供了一种一步生成方法，在保持强大生成性能的同时显著提高了速度。然而，在许多连续域中，均值流未能忠实地近似原始多步流匹配过程的行为。在这项工作中，我们通过将基于最优传输的采样策略整合到均值流框架中来解决这一局限性，从而使一步生成器能够更好地保留原始多步流过程的真实性和多样性。在受控低维设置以及图像生成、图像到图像翻译和点云生成等高维任务上的实验表明，我们的方法在一步生成建模中实现了卓越的推理准确性。|
|**2025-09-26**|[EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation](http://arxiv.org/abs/2509.22578)|null|基于模仿学习的策略在机器人操作中表现良好，但当它们从单一自我中心视角训练时，在自我中心视角偏移下性能通常会下降。为解决此问题，我们提出了EgoDemoGen，这是一个通过在新自我中心帧中重定向动作并使用我们提出的生成式视频修复模型EgoViewTransfer合成相应自我中心观察视频来生成成对新自我中心视演示的框架，该模型以新视角重投影的场景视频和从重定向关节动作渲染的仅包含机器人的视频为条件。EgoViewTransfer使用自监督双重重投影策略从预训练的视频生成模型进行微调。我们在仿真（RoboTwin2.0）和真实世界机器人上评估了EgoDemoGen。在使用EgoDemoGen生成的新自我中心视演示和原始标准自我中心视演示的混合数据进行训练后，策略成功率在标准自我中心视角下绝对提高了+17.0%，并在仿真中，在新自我中心视角下绝对提高了+17.7%。在真实世界机器人上，绝对改进分别为+18.3%和+25.8%。此外，随着EgoDemoGen生成演示比例的增加，性能持续提升，但回报递减。这些结果表明EgoDemoGen为实现对自我中心视角鲁棒的机器人操作提供了一条实用的途径。|
|**2025-09-25**|[SD3.5-Flash: Distribution-Guided Distillation of Generative Flows](http://arxiv.org/abs/2509.21318)|null|我们提出了SD3.5-Flash，这是一个高效的少步蒸馏框架，将高质量图像生成普及到消费级设备。我们的方法通过专门针对少步生成重新设计的分布匹配目标，蒸馏了计算成本高昂的修正流模型。我们引入了两项关键创新：“时间步共享”以减少梯度噪声，以及“分步时间步微调”以提高提示对齐。结合文本编码器重构和专用量化等全面的流水线优化，我们的系统实现了跨不同硬件配置的快速生成和内存高效部署。这使得从手机到台式电脑等全系列设备都能普及访问。通过包括大规模用户研究在内的广泛评估，我们证明SD3.5-Flash始终优于现有少步方法，使先进的生成式AI真正可用于实际部署。|
|**2025-09-25**|[NewtonGen: Physics-Consistent and Controllable Text-to-Video Generation via Neural Newtonian Dynamics](http://arxiv.org/abs/2509.21309)|null|当今大规模文本到视频生成的主要瓶颈是物理一致性和可控性。尽管近期取得了进展，但最先进的模型通常会产生不真实的运动，例如物体向上坠落，或速度和方向的突然变化。此外，这些模型缺乏精确的参数控制，难以在不同的初始条件下生成物理一致的动力学。我们认为，这一根本性局限源于当前模型仅从外观学习运动分布，而缺乏对底层动力学的理解。在这项工作中，我们提出了NewtonGen，一个将数据驱动合成与可学习物理原理相结合的框架。其核心是可训练的神经牛顿动力学（NND），它能够建模和预测多种牛顿运动，从而将潜在的动力学约束注入视频生成过程。通过联合利用数据先验和动力学指导，NewtonGen能够实现具有精确参数控制的物理一致视频合成。|
|**2025-09-25**|[Taxonomy-aware Dynamic Motion Generation on Hyperbolic Manifolds](http://arxiv.org/abs/2509.21281)|null|机器人的人形动作生成通常借鉴生物力学研究，这些研究常将复杂的人体动作归类到层级分类体系中。尽管这些分类法提供了关于动作之间如何相互关联的丰富结构信息，但这种信息在动作生成模型中经常被忽视，导致生成的动作与其底层层级结构之间存在脱节。本文介绍了GPHDM，这是一种新颖的方法，它学习潜在表示，同时保留动作的层级结构和时间动态性，以确保物理一致性。我们的模型通过将高斯过程动力学模型（GPDM）的动力学先验扩展到双曲流形，并将其与分类法感知的归纳偏置相结合来实现这一点。基于这种几何和分类法感知框架，我们提出了三种新颖的机制，用于生成既具有分类结构又物理一致的动作：两种概率递归方法和一种基于回拉度量测地线的方法。在手抓取分类法上生成逼真动作序列的实验表明，所提出的GPHDM忠实地编码了底层分类法和时间动态性，并生成了新颖的物理一致轨迹。|
|**2025-09-25**|[Does FLUX Already Know How to Perform Physically Plausible Image Composition?](http://arxiv.org/abs/2509.21278)|null|图像合成旨在将用户指定对象无缝插入新场景，但现有模型难以处理复杂光照（例如，精确阴影、水面反射）以及多样化、高分辨率的输入。现代文本到图像扩散模型（例如，SD3.5、FLUX）已编码基本的物理和分辨率先验知识，但缺乏一个框架来释放它们，而无需诉诸于潜在空间反演（这常将物体姿态锁定在上下文不合适的方向上）或脆弱的注意力操作。我们提出了SHINE，一个用于消除误差的无缝高保真插入的免训练框架。SHINE引入了流形引导锚点损失，利用预训练定制适配器（例如，IP-Adapter）来引导潜在表示，以实现忠实的主体表示，同时保持背景完整性。我们还提出了降级抑制引导和自适应背景融合，以进一步消除低质量输出和可见接缝。为解决缺乏严格基准的问题，我们引入了ComplexCompo，它具有多样化的分辨率和挑战性条件，例如低光照、强光照、复杂阴影和反射表面。在ComplexCompo和DreamEditBench上的实验表明，SHINE在标准度量（例如，DINOv2）和与人类感知一致的分数（例如，DreamSim、ImageReward、VisionReward）上均表现出最先进的性能。代码和基准将在发布时公开可用。|
|**2025-09-25**|[Un-Doubling Diffusion: LLM-guided Disambiguation of Homonym Duplication](http://arxiv.org/abs/2509.21262)|**[link](https://github.com/nagadit/Un-Doubling-Diffusion)**|同形异义词是指拼写相同但意义不同的词语，它们对许多生成模型构成了挑战。当提示中出现同形异义词时，扩散模型可能会同时生成该词的多种含义，这被称为同形异义词重复问题。这个问题因盎格鲁中心偏见而进一步复杂化，这种偏见在文本到图像模型流程之前包含一个额外的翻译步骤。结果是，即使在原始语言中并非同形异义词的词语，在翻译成英语后也可能变成同形异义词并失去其原义。在本文中，我们介绍了一种衡量重复率的方法，并使用基于视觉-语言模型（VLM）的自动评估和人工评估两种方式，对不同的扩散模型进行了评估。此外，我们研究了通过提示扩展来缓解同形异义词重复问题的方法，证明了这种方法也能有效减少与盎格鲁中心偏见相关的重复。自动评估流程的代码已公开可用。|
|**2025-09-25**|[Hallucination as an Upper Bound: A New Perspective on Text-to-Image Evaluation](http://arxiv.org/abs/2509.21257)|null|在语言模型和视觉-语言模型中，幻觉被广义地理解为模型基于其先验知识或偏见而非给定输入生成的内容。尽管这种现象已在这些领域得到研究，但尚未为文本到图像（T2I）生成模型明确界定。现有评估主要关注对齐性，检查提示中指定的元素是否出现，但忽视了模型在提示之外生成的内容。我们主张将T2I中的幻觉定义为偏见驱动的偏差，并提出了一个包含三类别的分类法：属性幻觉、关系幻觉和对象幻觉。这种界定方式为评估引入了上限并揭示了隐藏的偏见，为T2I模型更丰富的评估提供了基础。|
|**2025-09-25**|[Federated Flow Matching](http://arxiv.org/abs/2509.21250)|**[link](https://github.com/scnu-kevinkong/FedFlow)**|当今数据是去中心化的，在各种设备和机构中生成和存储，而隐私、所有权和法规阻碍了数据的集中化。这促使了对无需中央聚合，直接从本地分布式数据中训练生成模型的需求。在本文中，我们介绍了联邦流匹配 (FFM)，一个在隐私约束下训练流匹配模型的框架。具体来说，我们首先研究了FFM-vanilla，其中每个客户端使用独立的源和目标耦合在本地训练，保持了隐私但产生了弯曲的流，这减慢了推理速度。接着我们开发了FFM-LOT，它采用局部最优传输耦合以改善每个客户端内的流的直度，但在异构数据下缺乏全局一致性。最后，我们提出了FFM-GOT，一种基于最优传输半对偶公式的联邦策略，其中一个共享的全局势函数协调了跨客户端的耦合。在合成数据集和图像数据集上的实验表明，FFM 实现了隐私保护的训练，同时在联邦设置中提高了流的直度和样本质量，性能可与集中式基线相媲美。|
|**2025-09-25**|[Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets](http://arxiv.org/abs/2509.21245)|**[link](https://github.com/Tencent-Hunyuan/Hunyuan3D-Omni)**|原生3D生成模型的最新进展加速了游戏、电影和设计领域的资产创建。然而，大多数方法仍然主要依赖于图像或文本条件，并且缺乏细粒度的跨模态控制，这限制了其可控性和实际应用。为了弥补这一空白，我们提出了Hunyuan3D-Omni，一个基于Hunyuan3D 2.1构建的、用于细粒度可控3D资产生成的统一框架。除了图像，Hunyuan3D-Omni还接受点云、体素、边界框和骨骼姿态先验作为条件信号，从而实现了对几何、拓扑和姿态的精确控制。与为每个模态设置独立头部不同，我们的模型在一个单一的跨模态架构中统一了所有信号。我们采用一种渐进式、难度感知采样策略进行训练，该策略在每个样本中选择一种控制模态，并偏向于更难的信号（例如骨骼姿态）采样，同时降低较容易信号（例如点云）的权重，这促进了鲁棒的多模态融合以及对缺失输入的优雅处理。实验表明，这些额外的控制提高了生成精度，实现了几何感知变换，并增强了生产工作流程的鲁棒性。|
|**2025-09-25**|[Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation](http://arxiv.org/abs/2509.21227)|null|文本到图像生成已取得快速进展，但评估输出是否真正捕捉到提示中描述的对象、属性和关系仍然是一个核心挑战。这一领域的评估严重依赖自动化指标，然而这些指标通常是出于惯例或流行度而被采用，而非经过人类判断的验证。由于该领域的评估和报告的进展直接依赖于这些指标，了解它们在多大程度上反映了人类偏好至关重要。为了解决这个问题，我们对广泛用于组合式文本-图像评估的指标进行了一项广泛研究。我们的分析超越了简单的相关性，考察了它们在各种组合挑战中的表现，并比较了不同指标家族与人类判断的一致性。结果表明，没有单一指标在所有任务中表现一致：性能随组合问题的类型而变化。值得注意的是，基于VQA的指标尽管流行，但并非普遍优越，而某些基于嵌入的指标在特定情况下表现更强。正如预期，仅基于图像的指标对组合式评估贡献甚微，因为它们是为感知质量而非对齐而设计的。这些发现强调了仔细和透明地选择指标的重要性，既为了可信赖的评估，也为了将它们用作生成中的奖励模型。项目页面可在该网址获取：\href{https://amirkasaei.com/eval-the-evals/}{this URL}。|
|**2025-09-25**|[MeanSE: Efficient Generative Speech Enhancement with Mean Flows](http://arxiv.org/abs/2509.21214)|null|语音增强（SE）改善了降质语音的质量，其中流匹配等生成模型因其卓越的感知质量而受到关注。然而，基于流的模型需要多次函数评估（NFEs）才能实现稳定和令人满意的性能，导致计算负载高且单次函数评估（1-NFE）性能较差。在本文中，我们提出了 MeanSE，一种使用均值流的高效生成式语音增强模型，它对平均速度场进行建模，以实现高质量的单次函数评估（1-NFE）增强。实验结果表明，我们提出的 MeanSE 在单次函数评估（NFE）下显著优于流匹配基线，展现出极佳的域外泛化能力。|
|**2025-09-23**|[CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching](http://arxiv.org/abs/2509.19300)|null|条件生成建模旨在从包含数据-条件对的样本中学习条件数据分布。为此，扩散和基于流的方法已取得引人注目的结果。这些方法使用学习到的（流）模型将忽略条件的初始标准高斯噪声传输到条件数据分布。因此，模型需要学习质量传输和条件注入。为了减轻模型的需求，我们提出了流匹配的条件感知重参数化（CAR-Flow）——一种轻量级的、学习到的偏移，其对源分布、目标分布或两者进行条件化。通过重新定位这些分布，CAR-Flow缩短了模型必须学习的概率路径，从而在实践中实现更快的训练。在低维合成数据上，我们可视化并量化了CAR的效果。在更高维度的自然图像数据（ImageNet-256）上，为SiT-XL/2配备CAR-Flow将FID从2.07降低到1.68，同时引入不到0.6%的额外参数。|
|**2025-09-23**|[Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation](http://arxiv.org/abs/2509.19296)|**[link](https://github.com/nv-tlabs/lyra)**|生成虚拟环境的能力对于从游戏到机器人学、自动驾驶和工业AI等物理AI领域的应用至关重要。当前基于学习的3D重建方法依赖于捕获的真实世界多视角数据，而这些数据并非总是容易获得。视频扩散模型在最近的进展展示了卓越的想象能力，然而它们的2D本质限制了应用，仅限于机器人需要在环境中导航和交互的仿真场景。在本文中，我们提出了一种自蒸馏框架，旨在将视频扩散模型中的隐式3D知识蒸馏到显式3D高斯溅射（3DGS）表示中，从而消除了对多视角训练数据的需求。具体来说，我们用一个3DGS解码器增强了典型的RGB解码器，该解码器由RGB解码器的输出进行监督。在这种方法中，3DGS解码器可以纯粹地用视频扩散模型生成的合成数据进行训练。在推理时，我们的模型可以从文本提示或单张图像合成3D场景，用于实时渲染。我们的框架进一步扩展到从单目输入视频生成动态3D场景。实验结果表明，我们的框架在静态和动态3D场景生成中取得了最先进的性能。|
|**2025-09-23**|[OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps](http://arxiv.org/abs/2509.19282)|null|尽管布局到图像生成取得了稳步进展，但当前方法在处理包含显著边界框重叠的布局时仍然面临挑战。我们确定了两个主要挑战：(1) 大面积重叠区域和 (2) 语义区分度极小的重叠实例。通过定性示例和定量分析，我们证明了这些因素如何降低生成质量。为了系统地评估这个问题，我们引入了 OverLayScore，一个量化重叠边界框复杂性的新颖度量指标。我们的分析表明，现有基准偏向于 OverLayScore 值较低的简单案例，从而限制了它们在更具挑战性条件下评估模型性能的有效性。为了弥补这一差距，我们提出了 OverLayBench，一个具有高质量标注并在不同 OverLayScore 水平上呈现平衡分布的新基准。作为改进复杂重叠性能的初步步骤，我们还提出了 CreatiLayout-AM，一个在精选的非模态掩码数据集上微调的模型。综合来看，我们的贡献为在真实和具有挑战性的场景下实现更鲁棒的布局到图像生成奠定了基础。项目链接：https://mlpc-ucsd.github.io/OverLayBench。|
|**2025-09-23**|[A Gradient Flow Approach to Solving Inverse Problems with Latent Diffusion Models](http://arxiv.org/abs/2509.19276)|null|求解病态逆问题需要强大而灵活的先验。我们提出通过一种名为扩散正则化 Wasserstein 梯度流 (DWGF) 的新型免训练方法，利用预训练的潜在扩散模型来完成此任务。具体而言，我们将后验采样问题表述为潜在空间中 Kullback-Leibler 散度的正则化 Wasserstein 梯度流。我们使用 StableDiffusion (Rombach et al., 2022) 作为先验，在标准基准测试上展示了我们方法的性能。|
|**2025-09-23**|[Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps](http://arxiv.org/abs/2509.19252)|null|连续人体运动理解因其高维度和固有的冗余性，在计算机视觉领域仍然是一个核心挑战。高效的压缩和表示对于分析复杂运动动态至关重要。在这项工作中，我们引入了一个对抗性精炼的VQ-GAN框架，该框架采用密集运动标记化技术来压缩时空热图，同时保留人体运动的细粒度轨迹。我们的方法结合了密集运动标记化与对抗性精炼，消除了非对抗性基线中观察到的重建伪影，例如运动拖影和时间错位。我们在CMU Panoptic数据集上的实验提供了我们方法优越性的确凿证据，其SSIM指标优于dVAE基线9.31%，并降低了37.1%的时间不稳定性。此外，我们的密集标记化策略实现了一种新颖的运动复杂性分析，揭示了2D运动可以用紧凑的128个标记词汇表进行最佳表示，而3D运动的复杂性则需要一个大得多的1024个标记码本才能实现忠实重建。这些结果确立了该方法在各种运动分析应用中的实际部署可行性。本工作的代码库可在https://github.com/TeCSAR-UNCC/Pose-Quantization获取。|
|**2025-09-23**|[Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation](http://arxiv.org/abs/2509.19244)|null|我们提出了Lavida-O，一个统一的多模态掩码扩散模型（MDM），能够执行图像理解和生成任务。与现有仅支持简单图像级理解任务和低分辨率图像生成的多模态扩散语言模型（如MMaDa和Muddit）不同，Lavida-O展现了许多新能力，例如目标定位、图像编辑和高分辨率（1024像素）图像合成。它也是第一个利用其理解能力，通过规划和迭代自反思来改进图像生成和编辑结果的统一MDM。为了实现有效和高效的训练和采样，Lavida-O引入了许多新颖技术，例如弹性混合Transformer架构、通用文本条件化和分层采样。我们在RefCOCO目标定位、GenEval文本到图像生成和ImgEdit图像编辑等广泛基准测试中取得了最先进的性能，优于现有的自回归和连续扩散模型（如Qwen2.5-VL和FluxKontext-dev），同时在推理时提供了显著的加速。|
|**2025-09-23**|[Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data](http://arxiv.org/abs/2509.19208)|null|在热成像中进行精确的植物分割仍然是高通量田间表型分析面临的重大挑战，尤其是在室外环境中，植物与杂草之间对比度低以及频繁的遮挡阻碍了性能。为此，我们提出了一个框架，该框架利用合成RGB图像、有限的真实标注集以及基于GAN的跨模态对齐来增强热图像中的语义分割。我们使用1,128张包含作物和杂草植物复杂混合的合成图像训练了模型，以生成作物和杂草植物的图像分割掩膜。我们还评估了在训练过程中使用不同采样策略整合少至五张真实、手动分割的田间图像所带来的益处。当将所有合成图像与少量标注的真实图像结合时，与完整的真实数据基线相比，我们观察到杂草类别最大相对改进为22%，植物类别为17%。通过使用CycleGAN-turbo将RGB图像转换为热图像实现了跨模态对齐，从而实现了无需校准的鲁棒模板匹配。结果表明，将合成数据与有限的手动标注以及通过生成模型进行的跨域转换相结合，可以显著提升复杂田间环境中多模态图像的分割性能。|
|**2025-09-23**|[GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding](http://arxiv.org/abs/2509.19135)|null|人类出行轨迹，通常记录为签到序列，为短期访问模式和持久生活规律提供了独特的视角。在这项工作中，我们引入了GSTM-HMU，一个生成式时空框架，旨在通过明确建模人类运动的语义和时间复杂性来推进出行分析。该框架包含四项关键创新。首先，时空概念编码器（STCE）将地理位置、POI类别语义和周期性时间节奏整合到统一的向量表示中。其次，认知轨迹记忆（CTM）自适应地过滤历史访问，强调近期和行为显著的事件，以更有效地捕捉用户意图。第三，生活方式概念库（LCB）贡献了结构化的人类偏好线索，例如活动类型和生活模式，以增强可解释性和个性化。最后，面向任务的生成头将学习到的表示转化为多个下游任务的预测。我们在Gowalla、WeePlace、Brightkite和FourSquare等四个广泛使用的真实世界数据集上进行了广泛实验，并在三个基准任务上评估了性能：下一位置预测、轨迹-用户识别和时间估计。结果表明，与强大的基线相比，性能有持续且显著的提升，证实了GSTM-HMU在从复杂出行数据中提取语义规律的有效性。除了原始性能提升外，我们的发现还表明生成式建模为构建更鲁棒、可解释和可推广的人类出行智能系统提供了有前景的基础。|
|**2025-09-23**|[World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation](http://arxiv.org/abs/2509.19080)|null|机器人操作策略通常通过模仿学习进行初始化，但其性能受限于专家数据的稀缺性和覆盖范围狭窄。强化学习能够改进策略以缓解这一局限，然而真实机器人训练成本高昂且不安全，而在模拟器中训练则存在从模拟到真实世界的鸿沟。生成模型在近期取得的进展在真实世界模拟中展现出卓越的能力，扩散模型尤其擅长生成。这引出了一个问题：如何结合基于扩散模型的世界模型来增强机器人操作中的预训练策略。在这项工作中，我们提出了World4RL，一个利用基于扩散的世界模型作为高保真模拟器，完全在想象环境中改进机器人操作预训练策略的框架。与以往主要利用世界模型进行规划的工作不同，我们的框架实现了直接的端到端策略优化。World4RL围绕两个原则设计：预训练一个能够在多任务数据集中捕获多样化动力学的扩散世界模型，以及完全在一个冻结的世界模型中改进策略以避免在线的真实世界交互。我们进一步设计了一种针对机器人操作的two-hot动作编码方案，并采用扩散骨干网络以提高建模保真度。大量的模拟和真实世界实验表明，World4RL提供了高保真环境建模并实现了持续的策略改进，取得了比模仿学习和其他基线方法显著更高的成功率。更多可视化结果可在https://world4rl.github.io/获取。|
|**2025-09-23**|[WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction](http://arxiv.org/abs/2509.19073)|null|3D高斯泼溅 (3DGS) 已成为一种强大的基于图像的对象重建表示方法，但在稀疏视角设置下其性能急剧下降。先前工作通过采用扩散模型修复受损渲染来解决这一局限性，随后将其用作后续优化的伪真值。尽管有效，此类方法在扩散模型微调和修复步骤中带来了巨大的计算开销。我们提出了WaveletGaussian，一个用于更高效稀疏视角3D高斯对象重建的框架。我们的核心思想是将扩散转移到小波域：扩散仅应用于低分辨率LL子带，而高频子带通过轻量级网络进行精炼。我们进一步提出了一种高效的在线随机掩码策略来构建扩散模型微调的训练对，取代了常用的但效率低下的留一法策略。在Mip-NeRF 360和OmniObject3D这两个基准数据集上的实验表明，WaveletGaussian在实现具有竞争力的渲染质量的同时，大幅缩短了训练时间。|
|**2025-09-19**|[MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer](http://arxiv.org/abs/2509.16197)|null|能够理解和生成视觉内容的统一多模态大语言模型（LLM）具有巨大的潜力。然而，现有的开源模型往往在这些能力之间面临性能权衡。我们提出了Manzano，一个简单且可扩展的统一框架，通过结合混合图像分词器和精心设计的训练方案，显著缓解了这种矛盾。一个单一的共享视觉编码器驱动两个轻量级适配器，在共同的语义空间内生成用于图像到文本理解的连续嵌入和用于文本到图像生成的离散token。一个统一的自回归LLM以文本和图像token的形式预测高层次语义，随后一个辅助扩散解码器将图像token转换为像素。该架构，结合涵盖理解和生成数据的统一训练方案，实现了这两种能力的可扩展联合学习。Manzano在统一模型中取得了最先进的结果，并与专业模型具有竞争力，尤其是在富文本评估方面。我们的研究表明，任务冲突极小，并且随着模型规模的扩大能持续获得性能提升，从而验证了我们选择混合分词器的设计决策。|
|**2025-09-19**|[Quantum Generative Adversarial Autoencoders: Learning latent representations for quantum data generation](http://arxiv.org/abs/2509.16186)|null|本工作中，我们引入了量子生成对抗自编码器 (QGAA)，这是一种用于生成量子数据的量子模型。QGAA 由两个组成部分构成：(a) 用于压缩量子态的量子自编码器 (QAE)，以及 (b) 用于学习已训练 QAE 潜在空间的量子生成对抗网络 (QGAN)。这种方法赋予 QAE 生成能力。QGAA 的效用在两个代表性场景中得到证明：(a) 纯纠缠态的生成，以及 (b) H $_2$ 和 LiH 的参数化分子基态的生成。在多达 6 量子比特的模拟中，经训练的 QGAA 估计的能量平均误差对于 H$_2$ 为 0.02 Ha，对于 LiH 为 0.06 Ha。这些结果表明了 QGAA 在量子态生成、量子化学和近期量子机器学习应用方面的潜力。|
|**2025-09-19**|[AcT2I: Evaluating and Improving Action Depiction in Text-to-Image Models](http://arxiv.org/abs/2509.16141)|null|文生图（T2I）模型近期在根据文本描述生成图像方面取得了显著成功。然而，在准确渲染以动作和交互为主要语义焦点的复杂场景时，挑战依然存在。本文的关键观察是，T2I模型经常难以捕捉动作描绘中固有的微妙且通常隐含的属性，导致生成的图像缺乏关键的上下文细节。为了实现系统性评估，我们引入了AcT2I，这是一个旨在评估T2I模型在根据以动作为中心的提示生成图像方面的性能的基准。我们通过实验验证，主流T2I模型在AcT2I上表现不佳。我们进一步假设这一缺点源于现有T2I模型训练语料库中固有属性和上下文依赖的表示不完整。我们在此基础上开发了一种无需训练的知识蒸馏技术，利用大型语言模型来解决这一局限性。具体而言，我们通过整合三个维度上的密集信息来增强提示，观察到向提示注入时间细节显著提高了图像生成准确性，我们的最佳模型实现了72%的增长。我们的发现突出了当前T2I方法在生成需要复杂推理的图像方面的局限性，并证明以系统方式整合语言知识可以显著促进细致入微且上下文准确的图像生成。|
|**2025-09-19**|[Dynamic Classifier-Free Diffusion Guidance via Online Feedback](http://arxiv.org/abs/2509.16131)|null|无分类器引导 (CFG) 是文生图扩散模型的基石，然而其有效性受限于静态引导尺度的使用。这种“一刀切”的方法未能适应不同提示词的多样化需求；此外，先前诸如基于梯度的校正或固定启发式调度等解决方案引入了额外复杂性且难以泛化。在这项工作中，我们通过引入一个用于动态CFG调度的框架来挑战这种静态范式。我们的方法利用来自一系列通用和专用的小规模潜在空间评估（例如，用于对齐的CLIP、用于保真度的判别器以及人类偏好奖励模型）的在线反馈，以在逆向扩散过程的每一步评估生成质量。基于此反馈，我们执行贪婪搜索以选择每个时间步的最佳CFG尺度，从而为每个提示词和样本创建独特的量身定制的引导调度。我们在小规模模型和最先进的Imagen 3上证明了我们方法的有效性，显示出在文本对齐、视觉质量、文本渲染和数值推理方面取得了显著改进。值得注意的是，与默认的Imagen 3基线相比，我们的方法在整体偏好方面实现了高达53.8%的人类偏好胜率，在针对文本渲染等特定能力的提示词上，这一数字增加到55.5%。我们的工作确立了最佳引导调度本质上是动态且依赖于提示词的，并提供了一个高效且可泛化的框架来实现它。|
|**2025-09-19**|[DiffusionNFT: Online Diffusion Reinforcement with Forward Process](http://arxiv.org/abs/2509.16117)|null|在线强化学习 (RL) 对语言模型的后训练至关重要，但由于难以处理的似然，其在扩散模型上的扩展仍然充满挑战。最近的工作离散化了逆向采样过程以实现GRPO风格的训练，然而它们却继承了根本性的缺点，包括求解器限制、前向-逆向不一致性以及与无分类器指导 (CFG) 的复杂集成。我们引入了扩散负样本感知微调 (DiffusionNFT)，这是一种新的在线强化学习范式，它通过流匹配直接在前向过程中优化扩散模型。DiffusionNFT对比正负生成来定义一个隐式的策略改进方向，自然地将强化信号融入监督学习目标中。这种公式化方法使得能够使用任意黑盒求解器进行训练，消除了似然估计的需要，并且只需要干净图像而非采样轨迹来进行策略优化。在直接对比中，DiffusionNFT比FlowGRPO效率提升高达25倍，同时无需CFG。例如，DiffusionNFT在1k步内将GenEval分数从0.24提高到0.98，而FlowGRPO在超过5k步和额外CFG使用下达到0.95。通过利用多个奖励模型，DiffusionNFT显著提升了SD3.5-Medium在所有测试的基准中的性能。|
|**2025-09-19**|[PRISM: Probabilistic and Robust Inverse Solver with Measurement-Conditioned Diffusion Prior for Blind Inverse Problems](http://arxiv.org/abs/2509.16106)|null|扩散模型现已普遍用于解决计算成像中的逆问题。然而，大多数基于扩散的逆求解器需要完全了解前向算子才能使用。在这项工作中，我们引入了一种新颖的、带有测量条件扩散先验（PRISM）的概率且鲁棒的逆求解器，以有效解决盲逆问题。相较于现有方法，PRISM通过将强大的测量条件扩散模型整合到理论上严谨的后验采样方案中，提供了一项技术进步。在盲图像去模糊上的实验验证了所提出方法的有效性，证明了PRISM在图像和模糊核恢复方面均优于最先进的基线方法。|
|**2025-09-19**|[Blind-Spot Guided Diffusion for Self-supervised Real-World Denoising](http://arxiv.org/abs/2509.16091)|null|本文提出盲点引导扩散（Blind-Spot Guided Diffusion），一种用于真实世界图像去噪的新颖自监督框架。我们的方法解决了两个主要挑战：一是盲点网络（BSN）的局限性，这类网络由于空间独立性假设，常牺牲局部细节并引入像素不连续性；二是将扩散模型应用于自监督去噪的困难。我们提出了一种双分支扩散框架，它结合了一个基于BSN的扩散分支（用于生成半干净图像）和一个捕获底层噪声分布的传统扩散分支。为了在没有配对数据的情况下实现有效训练，我们利用基于BSN的分支来引导采样过程，在捕获噪声结构的同时保留局部细节。在SIDD和DND数据集上进行的广泛实验证明了最先进的性能，确立了我们的方法作为一种高效的真实世界去噪自监督解决方案。代码和预训练模型已发布于：https://github.com/Sumching/BSGD。|
|**2025-09-19**|[Randomized Smoothing Meets Vision-Language Models](http://arxiv.org/abs/2509.16088)|null|随机平滑（RS）是确保机器学习模型正确性的突出技术之一，通过它可以解析推导出逐点鲁棒性认证。尽管RS在分类领域已得到充分理解，但其在生成模型中的应用尚不明确，因为生成模型的输出是序列而非标签。我们通过将生成输出与一个预言机分类任务联系起来解决了这个问题，并表明RS仍然可以启用：最终响应可以被分类为离散动作（例如，视觉语言行动模型VLAs中的服务机器人指令），或分类为有害与无害（视觉语言模型VLMs中的内容审核或有害信息检测），甚至可以应用预言机将答案聚类为语义等价的类别。假设预言机分类器比较的错误率有界，我们发展了将样本数量与相应鲁棒性半径关联起来的理论。我们进一步解析推导了改进的缩放定律，将认证半径和准确性与样本数量关联起来，表明早期结果，即减少2到3个数量级的样本量即可满足要求且损失最小，在更弱的假设下仍然有效。总之，这些进展使鲁棒性认证对于最先进的视觉语言模型VLMs既有明确定义又在计算上可行，这一点已通过针对最近的越狱式对抗性攻击的验证得到证实。|
|**2025-09-19**|[Rethinking Molecule Synthesizability with Chain-of-Reaction](http://arxiv.org/abs/2509.16084)|null|分子生成模型的一个众所周知的缺陷是它们不能保证生成可合成的分子。为解决此问题已进行了大量尝试，但考虑到可合成分子指数级大的组合空间，现有方法在空间覆盖方面表现出局限性，并且分子优化性能不佳。为解决这些问题，我们引入了ReaSyn，这是一个用于可合成投影的生成框架，模型通过生成合成路径来探索给定分子在可合成空间中的邻域，从而得到可合成类似物。为了充分利用合成路径中包含的化学知识，我们提出了一种新颖的视角，将合成路径视为大型语言模型 (LLM) 中的推理路径。具体而言，受LLM中思维链 (CoT) 推理的启发，我们引入了反应链 (CoR) 表示法，该表示法明确说明了路径中每一步的反应物、反应类型和中间产物。借助CoR表示法，ReaSyn可以在每个反应步骤中获得密集监督，从而在监督训练期间明确学习化学反应规则并执行逐步推理。此外，为了进一步增强ReaSyn的推理能力，我们提出了基于强化学习 (RL) 的微调以及专为可合成投影定制的目标导向的测试时计算扩展。ReaSyn在可合成分子重建中实现了最高的重建率和路径多样性，在可合成目标导向分子优化中实现了最高的优化性能，并在可合成命中扩展方面显著优于先前的可合成投影方法。这些结果突显了ReaSyn在组合规模庞大的可合成化学空间中导航的卓越能力。|
|**2025-09-19**|[Generating Detailed Character Motion from Blocking Poses](http://arxiv.org/abs/2509.16064)|null|我们专注于使用生成扩散模型解决运动细节化任务：将以稀疏的、姿态粗糙且时间不精确的阻挡姿态表示的角色动画粗略版本，转换为细节丰富、自然逼真的角色动画。当前的扩散模型可以解决校正时间不精确姿态的时间问题，但我们发现，目前尚无好的解决方案可以利用扩散先验来为稀疏的阻挡姿态集增加额外的姿态细节。我们通过一个简单的推理时技巧克服了这一挑战。在特定的扩散步骤中，我们使用每个阻挡姿态的容差权重，将无条件扩散模型的输出与输入的阻挡姿态约束进行融合，并将此结果作为输入条件传递给一个预先存在的运动重定时模型。我们发现，这种方法显著优于现有尝试通过融合模型输出或将阻挡姿态约束表示为引导来添加细节的方法。结果是首个能够鲁棒地将阻挡级别姿态转换为合理细节化角色动画的扩散模型。|
|**2025-09-18**|[Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model](http://arxiv.org/abs/2509.15220)|null|为了从标定图像重建三维几何，基于学习的多视图立体（MVS）方法通常执行多视图深度估计，然后将深度图融合为网格或点云。为了提高计算效率，许多方法会初始化一个粗糙深度图，然后逐步以更高分辨率对其进行细化。最近，扩散模型在生成任务中取得了巨大成功。扩散模型从随机噪声开始，通过迭代去噪过程逐步恢复样本。在本文中，我们提出了一种新颖的MVS框架，将扩散模型引入MVS。具体来说，我们将深度细化公式化为一个条件扩散过程。考虑到深度估计的判别性特征，我们设计了一个条件编码器来指导扩散过程。为了提高效率，我们提出了一种结合轻量级2D U-Net和卷积GRU的新颖扩散网络。此外，我们提出了一种新颖的基于置信度的采样策略，以根据扩散模型估计的置信度自适应地采样深度假设。基于我们新颖的MVS框架，我们提出了两种新颖的MVS方法：DiffMVS和CasDiffMVS。DiffMVS在运行时间和GPU内存方面达到了最先进的效率，并取得了有竞争力的性能。CasDiffMVS在DTU、Tanks & Temples和ETH3D数据集上实现了最先进的性能。代码可在以下网址获取：https://github.com/cvg/diffmvs。|
|**2025-09-18**|[RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation](http://arxiv.org/abs/2509.15212)|**[link](https://github.com/alibaba-damo-academy/RynnVLA-001)**|本文提出了RynnVLA-001，一个基于人类演示的大规模视频生成式预训练构建的视觉-语言-动作（VLA）模型。我们提出了一种新颖的两阶段预训练方法。第一阶段，即以自我为中心的视频生成式预训练，利用1200万个以自我为中心的操作视频训练一个图像到视频模型，以初始帧和语言指令为条件预测未来帧。第二阶段，即以人为中心的轨迹感知建模，通过联合预测未来的关键点轨迹来扩展此方法，从而有效地弥合了视觉帧预测与动作预测之间的鸿沟。此外，为了增强动作表示，我们提出了ActionVAE，这是一个变分自编码器，它将动作序列压缩成紧凑的潜在嵌入，从而降低了VLA输出空间的复杂性。在相同的下游机器人数据集上进行微调时，RynnVLA-001取得了优于最先进基线方法的卓越性能，证明了所提出的预训练策略为VLA模型提供了更有效的初始化。|
|**2025-09-18**|[Fair-GPTQ: Bias-Aware Quantization for Large Language Models](http://arxiv.org/abs/2509.15206)|null|大型生成语言模型的高内存需求使得量化技术备受关注，该技术通过将模型权重映射到低精度整数来降低计算成本、内存使用和延迟。GPTQ等方法能有效最小化量化过程中的输入-权重乘积误差；然而，最近的实证研究表明，它们可能增加有偏输出并降低公平性基准测试上的性能，目前尚不清楚是哪些特定权重导致了这个问题。在这项工作中，我们通过在量化目标中添加明确的群体公平性约束，在量化与模型公平性之间建立了新的联系，并引入了Fair-GPTQ，这是第一个明确设计用于减少大型语言模型中不公平性的量化方法。所添加的约束指导舍入操作的学习，以实现对受保护群体偏见更少的文本生成。具体来说，我们关注涉及职业偏见的刻板印象生成以及跨越性别、种族和宗教的歧视性语言。Fair-GPTQ对性能影响极小，在零样本基准测试上至少能保持90%的基线准确率，相对于半精度模型减少了不公平性，并保留了4比特量化的内存和速度优势。我们还将Fair-GPTQ的性能与现有去偏方法进行了比较，发现在种族刻板印象基准测试上，其性能与迭代零空间投影去偏方法相当。总的来说，这些结果验证了我们针对带有群体偏置项的量化问题的理论解决方案，突出了其在生成模型量化时减少群体偏置的适用性，并表明我们的方法还可以进一步用于分析量化过程中通道和权重层面对公平性的贡献。|
|**2025-09-18**|[Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning](http://arxiv.org/abs/2509.15188)|null|自回归（AR）语言模型逐个token生成文本，这限制了它们的推理速度。基于扩散的语言模型提供了一种有前景的替代方案，因为它们可以并行解码多个token。然而，我们发现了当前扩散语言模型中的一个关键瓶颈：长解码窗口问题，即远离输入上下文生成的token经常变得不相关或重复。像半自回归这样的先前解决方案通过将窗口分成块来解决这个问题，但这牺牲了速度和双向性，从而消除了扩散模型的主要优势。为了克服这个问题，我们提出了卷积解码（Conv），这是一种基于归一化的方法，它在不进行硬性分段的情况下缩小了解码窗口，从而带来了更好的流畅性和灵活性。此外，我们引入了基于拒绝规则的微调（R2FT），这是一种事后训练方案，可以更好地对齐远离上下文位置的token。我们的方法在开放式生成基准测试（例如，AlpacaEval）中，在扩散语言模型基线中取得了最先进的结果，且步长显著低于先前工作，证明了速度和质量的双重提升。|
|**2025-09-18**|[Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation](http://arxiv.org/abs/2509.15185)|null|近期研究证明了高质量视觉表征在图像生成中的重要性，并突出了生成模型在图像理解方面的局限性。作为一种最初为自然语言设计的生成范式，自回归模型也面临着类似的挑战。在这项工作中，我们首次系统地研究了将下一个token预测范式应用于视觉领域的机制。我们确定了三个阻碍高层视觉语义学习的关键特性：局部和条件依赖性、步间语义不一致性以及空间不变性不足。我们表明，通过在训练过程中引入自监督目标，这些问题可以得到有效解决，从而形成了一种新颖的训练框架：自回归模型自引导训练（ST-AR）。ST-AR无需依赖预训练表征模型，显著增强了自回归模型的图像理解能力，并提升了生成质量。具体而言，ST-AR在LlamaGen-L上实现了约42%的FID提升，在LlamaGen-XL上实现了49%的FID提升，同时保持了相同的采样策略。|
|**2025-09-18**|[A Race Bias Free Face Aging Model for Reliable Kinship Verification](http://arxiv.org/abs/2509.15177)|**[link](https://github.com/bardiya2254kariminia/Age-Transformation-Without-Racial-Bias-Kinship-Verification)**|亲属关系验证中的年龄差距问题是指父母和子女照片之间的时间差。此外，他们的同龄照片通常难以获得，且人脸老化模型存在种族偏见，这会影响照片的逼真度。因此，我们提出了一种人脸老化GAN模型RA-GAN，它由两个新模块RACEpSp和特征混合器组成，旨在生成无种族偏见的图像。这些无偏见的合成照片被用于亲属关系验证，以研究验证同龄父母-子女图像的效果。实验表明，在种族准确性方面，我们的RA-GAN在所有年龄组中平均优于SAM-GAN 13.14%，并在60岁以上年龄组中优于CUSP-GAN 9.1%。此外，RA-GAN在所有年龄组中保留主体身份的能力优于SAM-GAN和CUSP-GAN。此外，我们证明将KinFaceW-I和KinFaceW-II数据集中的父母和子女图像转换为同龄可以提高所有年龄组的验证准确性。在KinFaceW-I数据集上，使用我们的RA-GAN，父子、父女、母子和母女等亲属关系的准确性分别提高了5.22%、5.12%、1.63%和0.41%。此外，在KinFaceW-II数据集上，父女、父子和母子关系的准确性分别提高了2.9%、0.39%和1.6%。代码可在Github上获取。|
|**2025-09-18**|[Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting](http://arxiv.org/abs/2509.15170)|null|射频指纹识别（RFFI）通过无线设备模拟电路的微小差异来区分它们，从而避免繁重的密码认证。尽管基于频谱图的深度学习提高了准确性，但模型仍然容易受到复制、篡改和规避的攻击。我们提出了一个更强大的RFFI系统，它结合了用于所有权证明的水印技术和用于发现可疑输入的异常检测。使用基于对数梅尔频谱图的ResNet-34，我们嵌入了三种水印：一个简单触发器、一个对噪声和滤波具有鲁棒性的对抗性训练触发器，以及一个隐藏的梯度/权重签名。一个带有Kullback-Leibler（KL）热启动和free-bits的卷积变分自编码器（VAE）用于标记离分布查询。在LoRa数据集上，我们的系统实现了94.6%的准确率、98%的水印成功率和0.94的AUROC，提供了可验证、防篡改的认证。|
|**2025-09-18**|[AnoF-Diff: One-Step Diffusion-Based Anomaly Detection for Forceful Tool Use](http://arxiv.org/abs/2509.15153)|null|多元时间序列异常检测对于识别意外事件至关重要，在机器学习领域已被探索了几十年。然而，将这些方法直接应用于强力工具使用任务的数据具有挑战性，因为现实世界中的流式传感器数据本质上是嘈杂的，表现出非平稳行为，并且在不同任务和工具之间存在差异。为了解决这些挑战，我们提出了一种名为AnoF-Diff的方法，该方法基于扩散模型从时间序列数据中提取力矩特征，并利用力矩特征来检测异常。我们将我们的方法与四项强力工具使用任务上的其他最先进方法在F1分数和受试者工作特征曲线下面积（AUROC）方面进行了比较，结果表明我们的方法具有更好的性能，并且对噪声数据集更鲁棒。我们还提出了一种基于一步扩散的并行异常分数评估方法，并展示了我们的方法如何在多项强力工具使用实验中用于在线异常检测。|
|**2025-09-18**|[WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance](http://arxiv.org/abs/2509.15130)|null|近期视频扩散模型因其丰富的潜在世界先验知识，在空间智能任务中展现出强大潜力。然而，这种潜力受到其有限的可控性和几何不一致性的阻碍，在它们的强大先验知识与3D/4D任务中的实际应用之间造成了差距。因此，现有方法通常依赖于重新训练或微调，这可能导致预训练知识退化并产生高昂的计算成本。为此，我们提出了WorldForge，一个免训练、推理时框架，由三个紧密耦合的模块组成。步内递归细化在推理过程中引入了一种递归细化机制，该机制在每个去噪步骤中反复优化网络预测，以实现精确的轨迹注入。流门控潜在融合利用光流相似性，在潜在空间中将运动与外观解耦，并选择性地将轨迹引导注入到运动相关通道中。双路径自校正引导比较有引导和无引导的去噪路径，以自适应地纠正由噪声或未对齐的结构信号引起的轨迹漂移。这些组件共同作用，在无需训练的情况下注入细粒度、轨迹对齐的引导，实现了精确的运动控制和逼真的内容生成。在各种基准上进行的大量实验验证了我们方法在真实感、轨迹一致性和视觉保真度方面的优越性。这项工作引入了一种新颖的即插即用范式用于可控视频合成，为利用生成先验知识进行空间智能提供了一个新视角。|
|**2025-09-18**|[Learning Mechanistic Subtypes of Neurodegeneration with a Physics-Informed Variational Autoencoder Mixture Model](http://arxiv.org/abs/2509.15124)|**[link](https://github.com/sanpinnawala/BrainPhys)**|建模神经退行性疾病的潜在机制需要能够从稀疏、高维神经影像数据中捕捉异质且空间变化的动态的方法。将基于偏微分方程（PDE）的物理知识与机器学习相结合，相较于经典数值方法，提供了增强的可解释性和实用性。然而，当前物理集成机器学习方法仅限于考虑单个PDE，这严重限制了它们在多种机制导致不同组（即亚型）疾病中的应用，并加剧了模型误设定和退化的问题。在本文中，我们提出了一种深度生成模型，用于学习由基于物理的PDEs控制的潜在动态模型混合，超越了假设单一PDE结构的传统方法。我们的方法将反应扩散PDEs集成到变分自编码器（VAE）混合模型框架中，支持从神经影像数据中推断可解释潜在变量（例如扩散系数和反应速率）的亚型。我们在合成基准上评估了我们的方法，并展示了其在从正电子发射断层扫描（PET）数据中揭示阿尔茨海默病进展的机制亚型方面的潜力。|

