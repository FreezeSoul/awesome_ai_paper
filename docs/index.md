---
layout: default
---

## Updated on 2024.08.24
> Usage instructions: [here](./docs/README.md#usage)

## 多模态

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-08-22**|[Adapt CLIP as Aggregation Instructor for Image Dehazing](http://arxiv.org/abs/2408.12317)|null|大多数去雾方法都存在感受野有限的问题，并且没有探索视觉语言模型中丰富的语义先验，而视觉语言模型已被证明在下游任务中非常有效。在本文中，我们介绍了 CLIPHaze，这是一个开创性的混合框架，它将 Mamba 高效的全局建模与 CLIP 的先验知识和零样本能力相结合，以同时解决这两个问题。具体来说，我们的方法采用并行状态空间模型和基于窗口的自注意力机制来分别获取全局上下文依赖关系和局部细粒度感知。为了无缝聚合来自两条路径的信息，我们引入了 CLIP 指导的聚合模块 (CAM)。对于非均匀雾和均匀雾，CAM 利用零样本估计的雾密度图和没有退化信息的高质量图像嵌入，为每个像素明确和隐式地确定最佳神经操作范围，从而自适应地融合具有不同感受野的两条路径。在各种基准数据集上的大量实验表明，CLIPHaze 取得了最先进的结果，特别是在非均匀雾的情况下。代码将在论文被接收后公开。||
|**2024-08-22**|[TRRG: Towards Truthful Radiology Report Generation With Cross-modal Disease Clue Enhanced Large Language Model](http://arxiv.org/abs/2408.12141)|null|多模态大型语言模型的视觉语言建模能力引起了社区的广泛关注。然而，在医学领域，由于放射报告中大量否定描述导致的数据分布不平衡，以及放射报告和射线照片之间粗略对齐等问题，使用视觉语言模型生成放射报告仍然面临着重大挑战。在本文中，我们提出了一种真实的放射报告生成框架，即 TRRG，该框架基于阶段式训练，用于将跨模态疾病线索注入大型语言模型。在预训练阶段，采用对比学习来增强视觉编码器感知细粒度疾病细节的能力。在微调阶段，我们提出的线索注入模块通过有效地结合鲁棒的零样本疾病感知，显著增强了大型语言模型的面向疾病的感知能力。最后，通过跨模态线索交互模块，我们的模型有效地实现了视觉嵌入和任意数量疾病线索嵌入的多粒度交互。这显著增强了多模态大型语言模型在放射报告生成领域的报告生成能力和临床效果。实验结果表明，我们提出的预训练和微调框架在 IU-Xray 和 MIMIC-CXR 等数据集上的放射报告生成方面达到了最先进的性能。进一步的分析表明，我们提出的方法可以有效地增强模型对疾病的感知能力，并提高其临床效果。||
|**2024-08-22**|[RoVRM: A Robust Visual Reward Model Optimized via Auxiliary Textual Preference Data](http://arxiv.org/abs/2408.12109)|null|大型视觉语言模型 (LVLM) 通常无法与人类偏好保持一致，从而导致出现诸如在没有适当视觉上下文的情况下生成误导性内容（也称为幻觉）等问题。解决此问题的一个有希望的方案是使用人类偏好对齐技术，例如 n 个最佳样本和强化学习。然而，这些技术面临着视觉偏好数据稀缺带来的挑战，而这些数据是训练视觉奖励模型 (VRM) 所必需的。在这项工作中，我们延续了这一研究方向。我们提出了一种鲁棒的视觉奖励模型 (RoVRM)，它改进了 LVLM 的人类偏好对齐。RoVRM 通过三个阶段的渐进式训练和基于最优传输的偏好数据选择来利用辅助文本偏好数据，从而有效地缓解了视觉偏好数据的稀缺性。我们基于 LLaVA-1.5-7B 和 -13B 模型，在常用的视觉语言任务上对 RoVRM 进行了实验。实验结果表明，RoVRM 的性能始终优于传统的 VRM。此外，我们的三阶段渐进式训练和偏好数据选择方法可以比基于排序的对齐技术（例如直接偏好优化）产生一致的性能提升。||
|**2024-08-22**|[Integrating Audio, Visual, and Semantic Information for Enhanced Multimodal Speaker Diarization](http://arxiv.org/abs/2408.12102)|null|说话人 diarization，即将音频流或转录的语音内容分割成基于说话人身份的同质分区的过程，在人类语音的解释和分析中起着至关重要的作用。大多数现有的说话人 diarization 系统完全依赖于单模态的声学信息，由于音频信号固有的模糊性，这使得该任务特别具有挑战性。最近的研究在视听或音频语义建模方面做出了巨大的努力，以提高性能。然而，即使结合了多达两种模态，也常常无法解决自发和非结构化对话的复杂性。为了利用更有意义的对话模式，我们提出了一种新的多模态方法，该方法联合利用音频、视觉和语义线索来增强说话人 diarization。我们的方法优雅地将多模态建模制定为一个约束优化问题。首先，我们深入了解活跃说话人之间的视觉联系和口语内容中的语义交互，从而建立丰富的成对约束。然后，我们引入了一种联合成对约束传播算法，以根据这些视觉和语义约束对说话人进行聚类。这种集成有效地利用了不同模态的互补优势，改进了单个说话人嵌入之间的亲和力估计。在多个多模态数据集上进行的大量实验表明，我们的方法始终优于最先进的说话人 diarization 方法。||
|**2024-08-21**|[GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models](http://arxiv.org/abs/2408.11817)|null|大型多模态模型 (LMM) 在许多视觉任务中都表现出色。尽管存在许多众所周知的基准来评估模型性能，但它们的提升空间越来越小。因此，迫切需要新一代的基准测试，以挑战下一代 LMM。LMM 显示出潜力的一个领域是图形分析，特别是分析师在解释图形时通常执行的任务，例如估计函数和数据序列的均值、截距或相关性。在这项工作中，我们介绍了 GRAB，这是一个图形分析基准，适用于当前和未来的前沿 LMM。我们的基准测试完全是合成的，确保了高质量、无噪声的问题。GRAB 由 2170 个问题组成，涵盖四个任务和 23 个图形属性。我们在 GRAB 上评估了 20 个 LMM，发现它是一个具有挑战性的基准，性能最佳的模型仅获得 21.7% 的分数。最后，我们进行了各种消融实验，以研究模型的成功和失败之处。我们发布 GRAB 是为了鼓励在这个重要且不断发展的领域取得进展。||
|**2024-08-21**|[CluMo: Cluster-based Modality Fusion Prompt for Continual Learning in Visual Question Answering](http://arxiv.org/abs/2408.11742)|**[link](https://github.com/yuliangcai2022/clumo)**|大型视觉语言模型 (VLM) 在各个应用领域都表现出显著的性能提升。然而，采用它们来处理多个顺序遇到的任务一直具有挑战性，因为在任务上微调 VLM 通常会导致其泛化能力和学习新任务的能力下降，以及对先前学习的任务造成灾难性遗忘。在多模态持续学习 (CL) 设置中使用 VLM 可以帮助解决这种情况。为了提高泛化能力并防止灾难性遗忘，我们提出了一种新的基于提示的 VLM CL 方法，即基于簇的模态融合提示 (CluMo)。我们设计了一种新颖的“关键-关键-提示”对，其中每个提示都与一个视觉提示关键和一个文本提示关键相关联。我们采用两阶段训练策略。在第一阶段，通过 $K$ -均值聚类算法训练单模态关键，以帮助选择语义匹配最佳的提示。在第二阶段，提示关键被冻结，选定的提示被附加到输入中，用于在 CL 场景中训练 VLM。在两个基准数据集上的实验表明，我们的方法实现了最先进的性能。||
|**2024-08-21**|[MSCPT: Few-shot Whole Slide Image Classification with Multi-scale and Context-focused Prompt Tuning](http://arxiv.org/abs/2408.11505)|null|多实例学习 (MIL) 已成为弱监督全切片图像 (WSI) 分类的标准范式。然而，这种范式依赖于使用大量标记的 WSI 进行训练。训练数据的缺乏和罕见疾病的存在给这些方法带来了重大挑战。提示调整与预训练的视觉语言模型 (VLM) 相结合是解决少样本弱监督 WSI 分类 (FSWC) 任务的有效方案。然而，将为自然图像设计的提示调整方法应用于 WSI 面临着三个重大挑战：1) 这些方法未能充分利用 VLM 文本模态的先验知识；2) 它们忽略了 WSI 中必不可少的的多尺度和上下文信息，导致结果不理想；3) 它们缺乏对实例聚合方法的探索。为了解决这些问题，我们提出了一种用于 FSWC 任务的多尺度和上下文相关的提示调整 (MSCPT) 方法。具体来说，MSCPT 采用冻结的大型语言模型在多尺度上生成病理视觉语言先验知识，指导分层提示调整。此外，我们设计了一个图提示调整模块来学习 WSI 中的关键上下文信息，最后引入了一个非参数交叉引导实例聚合模块来获取 WSI 级别的特征。基于两个 VLM，在三个数据集上的大量实验和可视化证明了我们 MSCPT 的强大性能。||
|**2024-08-21**|[Enabling Small Models for Zero-Shot Classification through Model Label Learning](http://arxiv.org/abs/2408.11449)|null|像 CLIP 这样的视觉语言模型 (VLM) 在图像分类任务中通过对齐文本和图像表现出令人印象深刻的零样本能力，但在性能上却逊于特定任务的专家模型。相反，专家模型在其专业领域表现出色，但缺乏针对新任务的零样本能力。如何兼具专家模型的高性能和零样本能力是一个重要的研究方向。在本文中，我们试图证明，通过构建模型中心并使用模型标签将模型与其功能对齐，可以通过有效地选择和重用中心中的模型以零样本的方式解决新任务。我们引入了一种新的范式，即模型标签学习 (MLL)，它通过语义有向无环图 (SDAG) 弥合模型与其功能之间的差距，并利用分类头组合优化 (CHCO) 算法为新任务选择合适的模型。与基础模型范式相比，它成本更低，可扩展性更强，即零样本能力随着模型中心规模的增长而增长。在七个真实世界数据集上的实验验证了 MLL 的有效性和效率，证明专家模型可以有效地重用于零样本任务。我们的代码将公开发布。||
|**2024-08-21**|[Swarm Intelligence in Geo-Localization: A Multi-Agent Large Vision-Language Model Collaborative Framework](http://arxiv.org/abs/2408.11312)|null|视觉地理定位需要深入的知识和先进的推理能力，才能将图像与现实世界的地理位置精确关联起来。通常，基于数据匹配的传统方法受到存储全球地标的充足视觉记录不切实际的阻碍。最近，大型视觉语言模型 (LVLM) 已经展示了通过视觉问答 (VQA) 进行地理定位的能力，从而提供了一种不需要外部地理标记图像记录的解决方案。然而，单个 LVLM 的性能仍然受到其内在知识和推理能力的限制。为此，我们在本文中介绍了一种名为 \name\ 的新型视觉地理定位框架，该框架通过代理间通信集成了多个 LVLM 代理的固有知识，以实现图像的有效地理定位。此外，我们的框架采用动态学习策略来优化代理之间的通信模式，减少代理之间不必要的讨论并提高框架的效率。为了验证所提出框架的有效性，我们构建了 GeoGlobe，一个用于视觉地理定位任务的新数据集。对数据集的广泛测试表明，我们的方法明显优于最先进的方法。||
|**2024-08-21**|[Making Large Vision Language Models to be Good Few-shot Learners](http://arxiv.org/abs/2408.11297)|null|少样本分类 (FSC) 是计算机视觉中一项基础但具有挑战性的任务，涉及从有限的数据中识别新类别。虽然先前的方法侧重于增强视觉特征或结合其他模态，但大型视觉语言模型 (LVLM) 由于其丰富的知识和强大的视觉感知能力，提供了一种很有前景的替代方案。然而，在 FSC 任务中，LVLM 可能会学习特定的响应格式，而不是有效地从支持数据中提取有用信息。在本文中，我们研究了 LVLM 在 FSC 中的性能，并确定了关键问题，例如学习不足和存在严重的positional biases。为了应对上述挑战，我们采用元学习策略来教模型“学会学习”。通过构建一组丰富的元任务进行指令微调，LVLM 增强了从少样本支持数据中提取信息进行分类的能力。此外，我们通过在微调和推理阶段分别进行标签增强和候选选择，进一步提升了 LVLM 的少样本学习能力。标签增强是通过字符扰动策略实现的，以确保模型关注支持信息。候选选择利用属性描述来过滤掉不可靠的候选者并简化任务。大量实验表明，我们的方法在通用和细粒度数据集上均取得了优异的性能。此外，我们的候选选择策略已被证明有利于无需训练的 LVLM。||
|**2024-08-20**|[TDS-CLIP: Temporal Difference Side Network for Image-to-Video Transfer Learning](http://arxiv.org/abs/2408.10688)|**[link](https://github.com/BBYL9413/TDS-CLIP)**|近年来，大型预训练视觉语言模型（例如CLIP）因其强大的表示能力而备受关注。这启发了研究人员将这些大型预训练模型中的知识迁移到其他特定任务模型，例如视频动作识别（VAR）模型，特别是通过利用侧网络来提高参数高效微调（PEFT）的效率。然而，当前VAR中的迁移方法倾向于以最小的成本将冻结的知识从大型预训练模型直接迁移到动作识别网络，而不是利用动作识别模型本身的时间建模能力。因此，在本文中，我们提出了一种内存高效的时间差分侧网络（TDS-CLIP），以平衡知识迁移和时间建模，避免在冻结参数模型中进行反向传播。具体来说，我们引入了一个时间差分适配器（TD-Adapter），它可以有效地捕捉运动特征中的局部时间差异，从而增强模型的全局时间建模能力。此外，我们设计了一个侧运动增强适配器（SME-Adapter），以指导所提出的侧网络有效地学习视频中的丰富运动信息，从而提高侧网络捕捉和学习运动信息的能力。我们在三个基准数据集上进行了广泛的实验，包括Something-Something V1\&V2和Kinetics-400。实验结果表明，我们的方法取得了具有竞争力的性能。||
|**2024-08-19**|[CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs](http://arxiv.org/abs/2408.10433)|null|尽管大型视觉语言模型（LVLM）最近取得了一些成功，但它们容易出现细节上的幻觉，例如物体及其属性或关系，这限制了它们在现实世界中的部署。为了解决这个问题并提高其鲁棒性，我们提出了CLIP-DPO，这是一种偏好优化方法，它利用对比预训练的视觉语言（VL）嵌入模型（如CLIP）对LVLM进行基于DPO的优化。与之前解决LVLM幻觉的工作不同，我们的方法不依赖于付费API，也不需要额外的训练数据或部署其他外部LVLM。相反，我们从初始的有监督微调数据池开始，生成一组不同的预测，根据其CLIP图像-文本相似度对其进行排序，然后使用强大的基于规则的方法对其进行过滤，以获得一组用于基于DPO训练的正负样本对。我们将CLIP-DPO微调应用于MobileVLM-v2系列模型和LlaVA-1.5，在所有情况下都观察到在减少幻觉方面相对于基线模型有显著改进。我们还观察到零样本分类的性能更好，表明基础能力有所提高，并验证了在标准LVLM基准测试中的原始性能总体上得到了保留。||
|**2024-08-18**|[NoRA: Nested Low-Rank Adaptation for Efficient Fine-Tuning Large Models](http://arxiv.org/abs/2408.10280)|null|在本文中，我们介绍了嵌套低秩适应 (NoRA)，这是一种新的参数高效微调方法，扩展了低秩适应 (LoRA) 技术的功能。普通的 LoRA 忽略了预训练权重的继承，并且仍然需要微调大量参数。为了解决这些问题，我们的 NoRA 采用具有奇异值分解 (SVD) 的双层嵌套结构，有效地利用原始矩阵知识，同时减少可调参数。具体来说，NoRA 冻结外部 LoRA 权重并利用内部 LoRA 设计，从而增强对模型优化的控制。这种方法允许模型在保持紧凑参数空间的同时更精确地适应特定任务。通过冻结外部 LoRA 权重并使用内部 LoRA 设计，NoRA 能够通过紧凑的参数空间实现精确的任务适应。对大型语言模型的常识推理、视觉语言模型的微调和主题驱动的生成等任务的评估表明，NoRA 优于 LoRA 及其变体。值得注意的是，与 LLaMA-3 8B 上的 LoRA 相比，NoRA 将微调参数|训练时间|
|**2024-08-19**|[Cross-composition Feature Disentanglement for Compositional Zero-shot Learning](http://arxiv.org/abs/2408.09786)|null|对基本元素（即属性和物体）视觉特征的解耦在组合式零样本学习（CZSL）中表现出卓越的效果。然而，由于属性（或物体）在与不同物体（或属性）组合时存在特征差异，学习跨不同组合的通用解耦基本元素特征具有一定挑战性。为此，我们提出了跨组合特征解耦的解决方案，该方案将多个共享基本元素的组合作为输入，并约束解耦的基本元素特征在这些组合中通用。具体而言，我们利用组合图来定义组合之间整体的基本元素共享关系，并基于近期成功的预训练大型视觉语言模型（VLM）CLIP构建特定任务架构，将双重跨组合解耦适配器（称为L-Adapter和V-Adapter）分别插入CLIP冻结的文本和图像编码器中。在三个流行的CZSL基准测试集上的评估结果表明，我们提出的解决方案显著提高了CZSL的性能，并且其组件已通过可靠的消融实验得到验证。||
|**2024-08-18**|[PA-LLaVA: A Large Language-Vision Assistant for Human Pathology Image Understanding](http://arxiv.org/abs/2408.09530)|**[link](https://github.com/ddw2aigroup2cqupt/pa-llava)**|病理图像理解领域的先前进展主要集中在开发针对特定任务的模型。最近的研究表明，大型视觉语言模型可以提高医学图像理解中各种下游任务的性能。在本研究中，我们开发了一种用于病理图像理解的领域特定大型语言视觉助手 (PA-LLaVA)。具体来说，(1) 我们首先通过清理公共医学图像文本数据来构建人类病理图像文本数据集，以实现特定领域的语义对齐；(2) 利用提出的图像文本数据，我们首先训练一个病理语言图像预训练 (PLIP) 模型作为病理图像的专门视觉编码器，然后我们开发了尺度不变连接器来避免图像缩放造成的信息损失；(3) 我们采用两阶段学习来训练 PA-LLaVA，第一阶段用于领域对齐，第二阶段用于端到端的视觉问答 (VQA) 任务。在实验中，我们在监督和零样本 VQA 数据集上评估了我们的 PA-LLaVA，我们的模型在类似规模的多模态模型中取得了最佳的总体性能。消融实验也证实了我们设计的有效性。我们认为，我们的 PA-LLaVA 模型和这项工作中提供的数据集可以促进计算病理学领域的研究。所有代码均可在以下网址获取：https://github.com/ddw2AIGROUP2CQUPT/PA-LLaVA||
|**2024-08-16**|[AdaRank: Disagreement Based Module Rank Prediction for Low-rank Adaptation](http://arxiv.org/abs/2408.09015)|**[link](https://github.com/google-research/google-research)**|随着语言和多模态模型规模的不断扩大，预训练一个通用的基础模型并将其适配到下游任务已成为一种常见做法。为此，考虑到模型规模庞大，适配效率可能成为一个关键瓶颈，因此 LoRA 等高效微调方法开始流行。然而，LoRA 通常在所有模型层中使用相同的秩，尽管迁移学习文献中越来越多的证据表明，在微调过程中，后面的层与预训练权重的差异更大。受特征学习和模块关键性相关理论和观察的启发，我们开发了一种基于模型差异的简单技术来预测给定模块相对于其他模块的秩。根据经验，在相同参数数量的情况下，AdaRank 在未见过的数据上的泛化能力明显优于使用统一秩的方法。与先前的工作相比，AdaRank 具有独特的优势，它可以完全保持预训练和适配阶段的完整性：不需要任何额外的目标函数或正则化器，这些可能会影响适配的准确性和性能。我们的代码在 https://github.com/google-research/google-research/tree/master/adaptive_low_rank 公开可用。||
|**2024-08-16**|[xGen-MM (BLIP-3): A Family of Open Large Multimodal Models](http://arxiv.org/abs/2408.08872)|null|本报告介绍了xGen-MM（也称为BLIP-3），这是一个用于开发大型多模态模型（LMM）的框架。该框架包括精心策划的数据集、训练方法、模型架构以及由此产生的一套LMM。xGen-MM是xGen-MultiModal的缩写，它是Salesforce xGen基础人工智能模型计划的扩展。我们的模型在一系列任务中经过了严格的评估，包括单图像和多图像基准测试。我们预训练的基础模型表现出强大的上下文学习能力，并且经过指令微调的模型在类似规模的开源LMM中展现出具有竞争力的性能。此外，我们还介绍了一种使用DPO进行安全微调的模型，旨在减少幻觉等有害行为并提高安全性。我们开源了我们的模型、策划的大规模数据集和微调代码库，以促进LMM研究的进一步发展。相关资源将在我们上面的项目页面上提供。||
|**2024-08-16**|[DPA: Dual Prototypes Alignment for Unsupervised Adaptation of Vision-Language Models](http://arxiv.org/abs/2408.08855)|null|视觉语言模型（VLM），例如 CLIP，在零样本图像分类方面已展现出巨大潜力。然而，将这些模型适配到新领域仍然具有挑战性，尤其是在没有标记数据的无监督环境中。最近的研究提出了伪标签方法，使用未标记的目标数据以无监督方式适配 CLIP。尽管如此，由于 CLIP 的视觉和文本表示之间的错位导致的噪声伪标签，这些方法的效果并不理想。本研究介绍了一种用于 VLM 的无监督域适配方法 DPA。DPA 引入了双重原型的概念，充当不同的分类器，并对其输出进行凸组合，从而实现准确的伪标签构建。接下来，它对伪标签进行排序以促进稳健的自我训练，特别是在训练初期。最后，它通过将文本原型与图像原型对齐来解决视觉-文本错位问题，从而进一步提高适配性能。在 13 个下游视觉任务上的实验表明，DPA 明显优于零样本 CLIP 和最先进的无监督适配基线。||
|**2024-08-16**|[Beyond the Hype: A dispassionate look at vision-language models in medical scenario](http://arxiv.org/abs/2408.08704)|null|大型视觉语言模型 (LVLM) 近期的进步展现出其在多种任务中的出色能力，引起了人工智能界的广泛关注。然而，它们在医学等专业领域的性能和可靠性尚未得到充分评估。特别是，大多数评估过度集中于基于多模态数据的简单视觉问答 (VQA) 来评估 VLM，而忽略了 LVLMs 的深度特性。在本研究中，我们介绍了 RadVUQA，这是一个新颖的放射学视觉理解和问答基准，用于全面评估现有的 LVLMs。RadVUQA 主要从五个维度验证 LVLMs：1）解剖理解，评估模型在视觉上识别生物结构的能力；2）多模态理解，包括解释语言和视觉指令以产生预期结果的能力；3）定量和空间推理，评估模型的空间感知能力以及将定量分析与视觉和语言信息相结合的能力；4）生理知识，衡量模型理解器官和系统功能和机制的能力；5）鲁棒性，评估模型针对不和谐和合成数据的能力。结果表明，通用 LVLMs 和医学专用 LVLMs 都存在严重缺陷，多模态理解和定量推理能力较弱。我们的研究结果揭示了现有 LVLMs 与临床医生之间的巨大差距，突出了对更强大、更智能的 LVLMs 的迫切需求。代码和数据集将在论文被接受后公开。||
|**2024-08-16**|[\textit{MMJ-Bench}: A Comprehensive Study on Jailbreak Attacks and Defenses for Vision Language Models](http://arxiv.org/abs/2408.08464)|null|随着深度学习的进步，大型语言模型 (LLM) 及其多模态对应模型，视觉语言模型 (VLM)，在许多现实任务中表现出卓越的性能。然而，VLM 面临着重大的安全挑战，例如越狱攻击，攻击者试图绕过模型的安全对齐以 elicit 有害的响应。VLM 越狱攻击的威胁源于 LLM 固有的漏洞以及 VLM 处理的多个信息通道。虽然已经提出了各种攻击和防御方法，但由于每种方法都是在不同的数据集和指标上进行评估的，因此在统一和全面的评估方面存在显着差距，这使得无法比较每种方法的有效性。为了解决这一差距，我们引入了 \textit{MMJ-Bench}，这是一个用于评估 VLM 越狱攻击和防御技术的统一管道。通过广泛的实验，我们评估了各种攻击方法对 SoTA VLM 的有效性，并评估了防御机制对防御有效性和模型在正常任务中的效用的影响。我们的全面评估通过提供统一且系统的评估框架和第一个公开可用的 VLM 越狱研究基准，为该领域做出了贡献。我们还展示了一些有见地的发现，这些发现突出了未来研究的方向。||
|**2024-08-15**|[Penny-Wise and Pound-Foolish in Deepfake Detection](http://arxiv.org/abs/2408.08412)|**[link](https://github.com/iamwangyabin/poundnet)**|深度伪造技术的普及引发了人们对其在各个领域潜在滥用的严重担忧，因此迫切需要强大的检测方法。尽管技术有所进步，但许多现有方法都优先考虑短期收益，而牺牲了长期有效性。本文批判了过度专业化的方法，即仅利用单个深度伪造数据集上的“小聪明”目标对预训练模型进行微调，而忽略了泛化和知识保留的“大智慧”平衡。为了解决这个“ penny-wise and pound-foolish ”的问题，我们提出了一种新的学习框架（PoundNet），用于在预训练的视觉语言模型上泛化深度伪造检测。PoundNet 结合了可学习的提示设计和平衡的目标，以保留上游任务（对象分类）的广泛知识，同时增强下游任务（深度伪造检测）的泛化能力。我们按照文献中的常见做法，在标准的单个深度伪造数据集上训练 PoundNet。然后，我们使用 5 个主要评估指标在 10 个公共大型深度伪造数据集上评估其性能，形成了据我们所知用于评估深度伪造检测模型泛化能力的最大基准测试集。综合基准评估表明，所提出的 PoundNet 明显不那么“penny-wise and pound-foolish ”，与最先进的方法相比，深度伪造检测性能提高了 19%，同时在其他深度伪造检测模型往往无效的对象分类任务上保持了 63% 的强大性能。代码和数据在 https://github.com/iamwangyabin/PoundNet 上开源。||
|**2024-08-15**|[MathScape: Evaluating MLLMs in multimodal Math Scenarios through a Hierarchical Benchmark](http://arxiv.org/abs/2408.07543)|**[link](https://github.com/PKU-Baichuan-MLSystemLab/MathScape)**|随着多模态大语言模型 (MLLM) 的发展，在数学问题背景下评估多模态模型已成为一个有价值的研究领域。多模态视觉-文本数学推理是评估 MLLM 理解和复杂多步骤定量推理能力的关键指标。然而，以往的多模态数学基准测试没有充分整合视觉和文本信息。为了弥补这一差距，我们提出了 MathScape，这是一个强调对视觉和文本信息组合理解和应用的新基准。MathScape 旨在评估基于照片的数学问题场景，通过分类分层方法评估 MLLM 的理论理解和应用能力。我们对 11 种先进的 MLLM 进行了多维评估，结果表明，即使对于最复杂的模型，我们的基准测试也具有挑战性。通过分析评估结果，我们发现了 MLLM 的局限性，为提高模型性能提供了宝贵的见解。||
|**2024-08-14**|[Modality Invariant Multimodal Learning to Handle Missing Modalities: A Single-Branch Approach](http://arxiv.org/abs/2408.07445)|null|多模态网络相较于单模态网络表现出了显著的性能提升。现有的多模态网络采用多分支设计，由于依赖于融合策略，如果缺少一种或多种模态，性能会下降。在这项工作中，我们提出了一种模态不变的多模态学习方法，该方法对缺失模态的影响不太敏感。它由一个跨多个模态共享权重的单分支网络组成，以学习模态间表示，从而最大限度地提高性能以及对缺失模态的鲁棒性。我们在四个具有挑战性的数据集上进行了广泛的实验，包括文本-视觉（UPMC Food-101、Hateful Memes、Ferramenta）和视听模态（VoxCeleb1）。与现有的最先进方法相比，我们提出的方法在所有模态都存在以及在训练或测试期间缺少模态的情况下都实现了优越的性能。||
|**2024-08-14**|[Robust Semi-supervised Multimodal Medical Image Segmentation via Cross Modality Collaboration](http://arxiv.org/abs/2408.07341)|**[link](https://github.com/med-air/cmc)**|多模态学习利用来自不同模态的互补信息，从而提高医学图像分割的性能。然而，现有的多模态学习方法严重依赖于来自各种模态的大量标注良好的数据来实现准确的分割性能。由于此类数据的可用性有限，这种依赖性在临床环境中常常构成挑战。此外，不同成像模态之间固有的解剖错位进一步增加了提高分割性能的难度。为了解决这个问题，我们提出了一种新的半监督多模态分割框架，该框架对稀缺的标记数据和错位的模态具有鲁棒性。我们的框架采用了一种新的跨模态协作策略来提取与每种模态 inherently 相关的模态独立知识，并将这些信息整合到一个统一的融合层中，以进行特征融合。通过通道语义一致性损失，我们的框架确保了跨模态从特征角度对齐模态独立信息，从而增强了其在多模态场景中对错位的抵抗力。此外，我们的框架有效地集成了对比一致性学习来规范解剖结构，促进了半监督分割任务中未标记数据的解剖学预测对齐。与其他多模态方法相比，我们的方法在心脏、腹部多器官和甲状腺相关眼眶病变分割这三项任务中均取得了具有竞争力的性能。它还展示了在涉及稀缺标记数据和错位模态的场景中的出色鲁棒性。||
|**2024-08-14**|[Enhancing Visual Question Answering through Ranking-Based Hybrid Training and Multimodal Fusion](http://arxiv.org/abs/2408.07303)|null|视觉问答 (VQA) 是一项具有挑战性的任务，它要求系统根据图像内容提供准确的答案。由于在有效捕获和整合多模态信息方面的局限性，当前的 VQA 模型在处理复杂问题时遇到困难。为了应对这些挑战，我们提出了 Rank VQA 模型，该模型利用一种受排序启发的混合训练策略来增强 VQA 性能。Rank VQA 模型集成了使用 Faster R-CNN 模型提取的高质量视觉特征和从预训练的 BERT 模型中获得的丰富语义文本特征。这些特征通过采用多头自注意力机制的复杂多模态融合技术进行融合。此外，模型中还加入了一个排序学习模块，用于优化答案的相对排序，从而提高答案的准确性。混合训练策略结合了分类和排序损失，增强了模型在不同数据集上的泛化能力和鲁棒性。实验结果证明了 Rank VQA 模型的有效性。我们的模型在标准 VQA 数据集（包括 VQA v2.0 和 COCO-QA）上，在准确性和平均倒数排名 (MRR) 方面均显著优于现有的最先进模型。Rank VQA 在处理需要理解细微差别并从图像和文本中进行复杂推理的复杂问题方面的能力，证明了其优越的性能。这项工作突出了基于排序的混合训练策略在提高 VQA 性能方面的有效性，并为多模态学习方法的进一步研究奠定了基础。||
|**2024-08-13**|[PathInsight: Instruction Tuning of Multimodal Datasets and Models for Intelligence Assisted Diagnosis in Histopathology](http://arxiv.org/abs/2408.07037)|null|病理诊断仍然是识别肿瘤的最终标准。多模态大型模型的兴起简化了图像分析与文本描述相结合的过程。尽管取得了这一进展，但训练和部署这些复杂的多模态模型所需的巨额成本，以及高质量训练数据集的稀缺，在尖端技术与其在临床环境中的应用之间造成了巨大的鸿沟。我们精心整理了一个包含大约 45,000 个病例的数据集，涵盖 6 多项任务，包括器官组织分类、生成病理报告描述以及解决病理相关问题和答案。我们已经使用该数据集微调了多模态大型模型，特别是 LLaVA、Qwen-VL、InternLM，以增强基于指令的性能。我们对基础模型和微调模型在特定数据集上执行图像描述和分类任务的能力进行了定性评估。评估结果表明，微调后的模型在解决典型病理问题方面表现出色。我们希望通过公开提供我们的模型和数据集，它们可以对医学和研究界产生价值。||
|**2024-08-13**|[Do Vision-Language Foundational models show Robust Visual Perception?](http://arxiv.org/abs/2408.06781)|**[link](https://github.com/shivam-chandhok/cpsc-540-project)**|近年来，视觉语言基础模型的进步使得开发能够执行视觉理解和推理任务的系统成为可能。然而，尚不清楚这些模型是否对分布变化具有鲁棒性，以及它们的性能和泛化能力在数据分布变化下如何变化。在本项目中，我们试图回答这个问题：“视觉语言基础模型是否像人类感知一样对分布变化具有鲁棒性？”具体来说，我们考虑了各种各样的视觉语言模型，并比较了这些系统的性能如何受到实际现实世界场景中常见的基于损坏的分布变化（例如\textit{运动模糊、雾、雪、高斯噪声}）的影响。我们定性和定量地分析了上述分布变化下零样本图像分类任务的泛化能力。我们的代码将在\url{https://github.com/shivam-chandhok/CPSC-540-Project}提供。||
|**2024-08-13**|[Response Wide Shut: Surprising Observations in Basic Vision Language Model Capabilities](http://arxiv.org/abs/2408.06721)|null|视觉语言模型 (VLM) 已成为解决各种复杂计算机视觉问题的通用工具。此类模型已被证明功能强大，但同时也缺乏一些基本的视觉理解能力。在本文中，我们着手通过构建一系列测试来了解 SoTA VLM 在基本视觉任务上的局限性：物体分类、理解空间排列和描绘单个物体实例的能力（通过计数），这些测试探究了设计中哪些组件可能缺乏，特别是哪些组件。重要的是，我们远远超出了当前仅仅衡量 VLM 最终性能的基准，还将其与直接在从视觉编码器获得的特征（图像嵌入）上训练的探针的性能，以及用于桥接图像的中间视觉语言投影的性能进行了比较和对比-编码器和 LLM-解码器输出在许多 SoTA 模型中（例如，LLaVA、BLIP、InstructBLIP）。通过这样做，我们发现了 VLM 响应中新出现的缺点，并提出了许多重要的观察结果，这些观察结果可以帮助在未来训练和开发更有效的 VLM 模型。||
|**2024-08-13**|[EditScribe: Non-Visual Image Editing with Natural Language Verification Loops](http://arxiv.org/abs/2408.06632)|null|图像编辑是一个迭代过程，需要精确的视觉评估和操作才能使输出结果与编辑意图相匹配。然而，现有的图像编辑工具无法为盲人和低视力人士提供可访问的交互和足够的反馈，以实现这种程度的控制。为了解决这个问题，我们开发了 EditScribe，这是一个原型系统，它利用大型多模态模型驱动的自然语言验证循环，使图像编辑变得可访问。使用 EditScribe，用户首先通过初始的一般描述和对象描述来理解图像内容，然后使用开放式自然语言提示指定编辑操作。EditScribe 执行图像编辑，并提供四种类型的验证反馈，供用户验证执行的编辑，包括视觉变化摘要、人工智能判断以及更新后的一般描述和对象描述。在执行另一次编辑之前，用户可以提出后续问题，以澄清和探究编辑或验证反馈。在一项针对十名盲人或低视力用户的研究中，我们发现 EditScribe 支持参与者以非视觉方式执行和验证图像编辑操作。我们观察到参与者不同的提示策略，以及他们对各种类型验证反馈的看法。最后，我们讨论了利用自然语言验证循环使非视觉用户能够进行视觉创作的意义。||
|**2024-08-13**|[CROME: Cross-Modal Adapters for Efficient Multimodal LLM](http://arxiv.org/abs/2408.06610)|null|多模态大型语言模型 (MLLM) 表现出卓越的图像-语言能力，但其广泛应用面临着成本效益高且适应性强的训练挑战。现有方法通常需要昂贵的语言模型再训练且适应性有限。此外，当前对零样本性能改进的关注不足以指导特定任务的调整。我们提出了 CROME，一个高效的视觉-语言指令微调框架。它具有一个新颖的门控跨模态适配器，可以在输入冻结的 LLM 之前有效地组合视觉和文本表示。这种轻量级适配器只需极少的参数即可训练，从而实现高效的跨模态理解。值得注意的是，CROME 在标准视觉问答和指令遵循基准测试中表现出优异的零样本性能。此外，它通过出色的参数效率实现了微调，可与特定任务的专业最先进方法相媲美。CROME 展示了预 LLM 对齐在构建可扩展、适应性强且参数高效的多模态模型方面的潜力。||
|**2024-08-12**|[VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents](http://arxiv.org/abs/2408.06327)|**[link](https://github.com/thudm/visualagentbench)**|大型多模态模型 (LMM) 开启了人工智能的新纪元，将语言和视觉能力融合在一起，形成了高度 capable 的视觉基础智能体。这些智能体被认为在各种任务中表现出色，有可能接近通用人工智能。然而，现有的基准测试未能充分挑战或展示 LMM 在复杂、现实环境中的全部潜力。为了弥合这一差距，我们引入了 VisualAgentBench (VAB)，这是一个全面且开创性的基准测试，专门用于在不同场景下训练和评估 LMM 作为视觉基础智能体，包括具身、图形用户界面和视觉设计，并制定了探究 LMM 理解和交互能力深度的任务。通过对 9 个专有 LMM API 和 8 个开源模型的严格测试，我们证明了这些模型相当可观但仍在发展的智能体能力。此外，VAB 通过程序化求解器、LMM 智能体引导和人类演示等混合方法构建了一个轨迹训练集，通过行为克隆促进了 LMM 的性能大幅提升。我们的工作不仅旨在对现有模型进行基准测试，还为未来视觉基础智能体的开发奠定了坚实的基础。代码、训练和测试数据以及部分微调的开源 LMM 可在 \url{https://github.com/THUDM/VisualAgentBench} 获取。||
|**2024-08-12**|[Adapting a Foundation Model for Space-based Tasks](http://arxiv.org/abs/2408.05924)|null|基础模型，例如大型语言模型，拥有人工智能的属性，有望赋予机器人必要的上下文理解能力，以在野外导航复杂、非结构化的任务。在太空机器人的未来，我们看到了三个核心挑战，这些挑战推动了适用于太空应用的基础模型的使用：1）地面环路操作的可扩展性；2）将先验知识推广到新环境；3）任务和传感器数据的多模态性。因此，作为构建用于太空应用的基础模型的第一步，我们自动标记 AI4Mars 数据集，以策划一个包含视觉问答元组的语言注释数据集。我们在这个数据集上微调了一个预训练的 LLaVA 检查点，以赋予视觉语言模型在火星表面执行空间推理和导航的能力。在这项工作中，我们证明了 1）现有的视觉语言模型在基于太空的应用中是缺乏视觉推理能力的，以及 2）在即使只有几千个样本的有限训练数据集上，在 extraterrestrial 数据上微调视觉语言模型也能显著提高响应质量。||
|**2024-08-13**|[Weakly Supervised Video Anomaly Detection and Localization with Spatio-Temporal Prompts](http://arxiv.org/abs/2408.05905)|null|目前的弱监督视频异常检测 (WSVAD) 任务的目标是在只有粗粒度的视频级标注可用的情况下实现帧级异常事件检测。现有工作通常涉及从全分辨率视频帧中提取全局特征，并训练帧级分类器来检测时间维度上的异常。然而，大多数异常事件往往发生在局部空间区域，而不是整个视频帧，这意味着现有的基于帧级特征的工作可能会被占主导地位的背景信息误导，并且缺乏对检测到的异常的解释。为了解决这一难题，本文介绍了一种名为 STPrompt 的新方法，该方法基于预训练的视觉语言模型 (VLM) 学习用于弱监督视频异常检测和定位 (WSVADL) 的时空提示嵌入。我们提出的方法采用双流网络结构，其中一个流专注于时间维度，另一个流主要关注空间维度。通过利用从预训练的 VLM 中学习到的知识并结合原始视频中的自然运动先验，我们的模型学习与视频的时空区域（例如，单个帧的补丁）对齐的提示嵌入，以识别异常的特定局部区域，从而能够在减轻背景信息影响的同时准确地检测视频异常。在不依赖详细的时空标注或辅助对象检测/跟踪的情况下，我们的方法在 WSVADL 任务的三个公共基准测试中实现了最先进的性能。||
|**2024-08-12**|[GlyphPattern: An Abstract Pattern Recognition for Vision-Language Models](http://arxiv.org/abs/2408.05894)|**[link](https://github.com/Wellesley-EASEL-lab/GlyphPattern)**|基于强大的大型语言模型的视觉语言模型 (VLM) 在跨视觉和文本数据进行推理方面取得了快速进展。虽然 VLM 在其训练的视觉任务上表现良好，但我们的结果突出了抽象模式识别方面的关键挑战。我们提出了 GlyphPattern，这是一个包含 954 个项目的dataset，将来自 40 种书写系统的 318 个人工编写的视觉模式描述与三种视觉呈现样式配对。GlyphPattern 评估 VLM 中的抽象模式识别，要求模型理解和判断视觉模式的自然语言描述。GlyphPattern 模式源自对人类书写系统的大规模认知科学调查；因此，它们在空间参考和组合性方面非常丰富。我们的实验表明，GlyphPattern 对最先进的 VLM 极具挑战性（GPT-4o 仅达到 55% 的准确率），并且从少样本提示中获得的收益微乎其微。我们详细的错误分析揭示了多个层面的挑战，包括视觉处理、自然语言理解和模式泛化。||
|**2024-08-11**|[HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes](http://arxiv.org/abs/2408.05794)|null|随着大型多模态模型 (LMM) 的兴起及其在生成和解释复杂内容方面的广泛应用，传播带有偏见和有害模因的风险仍然很大。当前的安全措施通常无法检测到“混淆模因”中巧妙整合的仇恨内容。为了解决这个问题，我们引入了 \textsc{HateSieve}，这是一个旨在加强对模因中仇恨元素的检测和分割的新框架。\textsc{HateSieve} 具有新颖的对比模因生成器，可以创建语义配对的模因，用于对比学习的定制三元组数据集，以及生成上下文感知嵌入以进行准确模因分割的图像-文本对齐模块。在仇恨模因数据集上的实证实验表明，\textsc{HateSieve} 不仅在性能上优于现有的 LMM，而且可训练参数更少，而且还提供了一种精确识别和隔离仇恨内容的强大机制。（注意：包含对仇恨言论的学术讨论；建议观众谨慎观看。）||
|**2024-08-09**|[Hyperbolic Learning with Multimodal Large Language Models](http://arxiv.org/abs/2408.05097)|null|双曲嵌入已在各种深度学习任务（包括图像分割和主动学习）中证明了其在捕捉不确定性和层次关系方面的有效性。然而，它们在现代视觉语言模型 (VLM) 中的应用仍然有限。一个值得注意的例外是 MERU，它利用 CLIP ViT-large 模型中双曲空间的层次属性，该模型包含数亿个参数。在我们的工作中，我们使用 BLIP-2 架构解决了在参数（数十亿）和训练复杂性方面将多模态双曲模型扩展几个数量级的挑战。尽管双曲嵌入提供了对欧几里得嵌入中不存在的不确定性的潜在洞察力，但我们的分析表明，扩展这些模型特别困难。我们为 BLIP-2 的双曲版本提出了一种新的训练策略，该策略允许实现与其欧几里得对应物相当的性能，同时在整个训练过程中保持稳定性，并通过每个嵌入显示出有意义的不确定性指示。||
|**2024-08-09**|[Avoid Wasted Annotation Costs in Open-set Active Learning with Pre-trained Vision-Language Model](http://arxiv.org/abs/2408.04917)|**[link](https://github.com/dsba-lab/openal)**|主动学习 (AL) 旨在通过选择性地收集信息量最大的数据来提高模型性能，从而最大程度地降低标注成本。 然而，在实际场景中，未标记的数据可能包含分布外 (OOD) 样本，如果选择了错误的数据，会导致标注成本浪费。 最近的研究探索了将 AL 应用于开放集数据的方法，但这些方法通常需要或产生不可避免的成本损失才能将其降至最低。 为了应对这些挑战，我们提出了一种新的选择策略，即用于 AL 的 CLIPN (CLIPNAL)，它可以在不需要 OOD 样本的情况下最大程度地减少成本损失。 CLIPNAL 依次评估数据的纯度和信息量。 首先，它利用预训练的视觉语言模型，通过利用分布内 (ID) 数据的语言和视觉信息来检测和排除 OOD 数据，而无需额外训练。 其次，它从剩余的 ID 数据中选择信息量最大的数据，然后由人类专家对所选样本进行标注。 在具有各种开放集条件的数据集上的实验结果表明，CLIPNAL 在所有场景中均实现了最低的成本损失和最高的性能。 代码可在 https://github.com/DSBA-Lab/OpenAL 获取。||
|**2024-08-08**|[Enhancing Journalism with AI: A Study of Contextualized Image Captioning for News Articles using LLMs and LMMs](http://arxiv.org/abs/2408.04331)|null|大型语言模型 (LLM) 和大型多模态模型 (LMM) 对人工智能社区、行业和各个经济领域产生了重大影响。在新闻领域，人工智能的整合带来了独特的挑战和机遇，特别是在提高新闻报道的质量和效率方面。本研究探讨了 LLM 和 LMM 如何通过为新闻文章中的图片生成上下文相关的标题来辅助新闻实践。我们使用 GoodNews 数据集进行了实验，以评估 LMM（BLIP-2、GPT-4v 或 LLaVA）整合两种类型上下文的能力：完整的新闻文章或提取的命名实体。此外，我们还将它们的性能与由字幕模型（BLIP-2、OFA 或 ViT-GPT2）和使用 LLM（GPT-4 或 LLaMA）进行事后语境化的两阶段流水线进行了比较。我们评估了各种模型，发现虽然语境化模型的选择对于两阶段流水线来说是一个重要因素，但在 LMM 中并非如此，在 LMM 中，与专有的、基于 GPT 的模型相比，更小型的开源模型表现良好。此外，我们发现控制提供的上下文数量可以提高性能。这些结果突出了全自动方法的局限性，并强调了交互式人工参与策略的必要性。||
|**2024-08-07**|[How Well Can Vision Language Models See Image Details?](http://arxiv.org/abs/2408.03940)|null|基于大型语言模型的视觉语言模型（LLM-based VLMs）在各种视觉语言理解任务中表现出色。然而，这些VLM在语义层面之外对图像细节的感知能力仍不清楚。在我们的研究中，我们引入了一个像素值预测任务（PVP）来探讨“视觉语言模型对图像细节的感知程度”，并帮助VLM感知更多细节。通常，这些模型包含一个冻结的CLIP视觉编码器、一个大型语言模型和一个连接模块。在对PVP任务进行微调后，我们发现：1）现有的VLM仅通过微调连接模块和LLM难以预测精确的像素值；2）当视觉编码器也进行调整时，预测精度显著提高。此外，我们的研究表明，将像素值预测作为VLM预训练任务之一，并进行视觉编码器适应，可以显著提高VLM在下游需要精细图像感知的图像语言理解任务中的性能，例如参考图像分割（平均cIoU提升+10.19）和视频游戏决策（在两个游戏中的平均得分分别提升+80.34和+70.54）。||
|**2024-08-07**|[Teach CLIP to Develop a Number Sense for Ordinal Regression](http://arxiv.org/abs/2408.03574)|**[link](https://github.com/xmed-lab/numclip)**|序数回归是计算机视觉领域的一个基本问题，它针对特定任务使用定制的经过良好训练的模型。虽然预训练的视觉语言模型 (VLM) 在各种视觉任务中都表现出令人印象深刻的性能，但它们在序数回归方面的潜力却很少被探索。在这项研究中，我们首先研究了 CLIP 在序数回归方面的潜力，我们期望该模型可以推广到不同的序数回归任务和场景。不幸的是，vanilla CLIP 在这项任务中失败了，因为当前的 VLM 存在一个有据可查的局限性，即封装了数字感知等组合概念。我们提出了一种简单而有效的方法，称为 NumCLIP，以提高 VLM 的定量理解能力。我们将精确的图像到数字的文本匹配问题分解为粗略分类和精细预测阶段。我们使用通用语言概念对每个数字区间进行离散化和表述，以更好地利用 CLIP 中可用的预训练对齐。为了考虑序数回归固有的连续性，我们提出了一种新颖的基于细粒度跨模态排序的正则化损失，专门设计用于在 CLIP 的特征空间中保持语义和序数对齐。在三个通用序数回归任务上的实验结果证明了 NumCLIP 的有效性，在历史图像年代测定和图像美学评估任务上的准确率分别提高了 10% 和 3.83%。代码可在 https://github.com/xmed-lab/NumCLIP 公开获取。||
|**2024-08-06**|[LLaVA-OneVision: Easy Visual Task Transfer](http://arxiv.org/abs/2408.03326)|**[link](https://github.com/evolvinglmms-lab/lmms-eval)**|我们推出了 LLaVA-OneVision，这是一个开放的大型多模态模型 (LMM) 系列，它融合了我们在 LLaVA-NeXT 博客系列中对数据、模型和视觉表示的见解。我们的实验结果表明，LLaVA-OneVision 是第一个能够同时在三个重要的计算机视觉场景（单图像、多图像和视频场景）中突破开放 LMM 性能界限的单一模型。重要的是，LLaVA-OneVision 的设计允许在不同模态/场景之间进行强大的迁移学习，从而产生新的涌现能力。特别是，通过从图像到视频的任务迁移，展示了强大的视频理解能力和跨场景能力。||
|**2024-08-06**|[Multitask and Multimodal Neural Tuning for Large Models](http://arxiv.org/abs/2408.03001)|null|近年来，大规模多模态模型在各个领域展现出令人印象深刻的能力。然而，如何使这些模型能够同时有效地执行多个多模态任务仍然是一项重大挑战。为了解决这个问题，我们引入了一种称为神经调优的新型调优方法，旨在同时处理不同的多模态任务，包括推理分割、参考分割、图像描述和文本到图像生成。神经调优模拟了人类大脑中的稀疏分布式表示，其中每个任务只激活特定子集的神经元。此外，我们提出了一个新的基准测试MMUD，其中每个样本都标注了多个任务标签。通过在MMUD基准测试上对预训练的大型模型应用神经调优，我们以一种简化且高效的方式实现了同时处理多个任务。所有模型、代码和数据集将在发布后公开，以促进该领域的进一步研究和发展。||
|**2024-08-06**|[Body of Her: A Preliminary Study on End-to-End Humanoid Agent](http://arxiv.org/abs/2408.02879)|null|交互式虚拟人形代理是与物理世界交互的关键接口。一个相对完整的人形代理首先需要拥有面部和身体，然后具备口头和非口头（如眼神交流、面部表情、嘴唇运动、手势和操作）能力，最后，它能够进行实时双向交流，例如主动打断对话的能力。大多数现有系统通常只考虑这些元素的一个子集，与真实的人形代理之间存在差距。在这项工作中，我们提出了一个实时的、双向的、交互式的端到端网络，能够对真实代理的行为进行建模，包括语音、用于说话、响应、空闲和操作的全身运动。该系统是一个多模态模型，集成了音频和视觉输入，从预先训练的大型语言模型（LLM）扩展而来。我们收集了大约200,000小时的音频，大约130,000小时的视频数据，以及大约20,000个对齐样本，用于构建模型。最终的模型展示了以前系统难以实现的功能，例如广义物体操作。这项工作对该领域的端到端方法进行了初步探索，旨在激发进一步的研究，以扩大规模。||
|**2024-08-05**|[Pre-trained Encoder Inference: Revealing Upstream Encoders In Downstream Machine Learning Services](http://arxiv.org/abs/2408.02814)|**[link](https://github.com/fshp971/encoder-inference)**|虽然预训练编码器可以很容易地在网上获得，以便快速构建下游机器学习 (ML) 服务，但人们已经设计出各种攻击来破坏这些编码器的安全性和隐私性。虽然大多数攻击的目标是上游的编码器，但当编码器部署在下游 ML 服务中时，它会受到怎样的威胁仍然未知。本文揭示了一个新的漏洞：预训练编码器推理 (PEI) 攻击，它对隐藏在下游 ML 服务背后的编码器构成隐私威胁。通过仅提供对目标下游服务的 API 访问权限和一组候选编码器，PEI 攻击可以根据候选编码器推断出目标服务秘密使用了哪个编码器。我们评估了 PEI 攻击在图像分类、文本分类和文本到图像生成这三项下游任务中针对现实世界编码器的攻击性能。实验表明，PEI 攻击在大多数情况下都能成功揭示隐藏的编码器，即使隐藏的编码器不在候选集中也很少出错。我们还对最新的视觉语言模型之一 LLaVA 进行了案例研究，以说明 PEI 攻击有助于其他 ML 攻击，如对抗性攻击。代码可在 https://github.com/fshp971/encoder-inference 获取。||
|**2024-08-05**|[MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models](http://arxiv.org/abs/2408.02718)|null|大型视觉语言模型 (LVLM) 处理多图像的能力对于其发展对场景更全面和细致的理解至关重要。最近，多图像 LVLM 开始满足这一需求。然而，它们的评估并没有跟上其发展的步伐。为了填补这一空白，我们引入了多模态多图像理解 (MMIU) 基准测试，这是一个全面的评估套件，旨在评估 LVLM 在各种多图像任务中的性能。MMIU 包含 7 种多图像关系类型、52 项任务、7.7 万张图像和 1.1 万个精心策划的选择题，使其成为同类基准测试中规模最大的一个。我们对 24 种流行 LVLM（包括开源和专有模型）的评估表明，多图像理解方面存在重大挑战，特别是在涉及空间理解的任务中。即使是最先进的模型，如 GPT-4o，在 MMIU 上的准确率也只有 55.7%。通过多方面的分析实验，我们确定了关键的性能差距和局限性，为未来模型和数据的改进提供了宝贵的见解。我们的目标是让 MMIU 推动 LVLM 研究和开发的前沿，使我们朝着实现复杂的多模态多图像用户交互的目标迈进。||
|**2024-08-05**|[Cross-Domain Semantic Segmentation on Inconsistent Taxonomy using VLMs](http://arxiv.org/abs/2408.02261)|null|无监督域适应 (UDA) 中的语义分割挑战不仅源于源图像和目标图像之间的域偏移，还源于跨域的类别分类差异。传统的 UDA 研究假设源域和目标域之间具有一致的分类，从而限制了它们识别和适应目标域分类的能力。本文介绍了一种新方法，即使用视觉语言模型 (CSI) 对不一致分类进行跨域语义分割，即使在源目标类别不匹配的情况下也能有效地执行域自适应语义分割。CSI 利用视觉语言模型 (VLM) 的语义泛化潜力，与以前的 UDA 方法协同作用。它利用通过传统 UDA 方法获得的分割推理，结合 VLM 中嵌入的丰富语义知识，重新标记目标域中的新类别。这种方法允许有效地适应扩展的分类，而无需任何目标域的基本事实标签。我们的方法已被证明在不一致分类设置（从粗到细的分类和开放分类）的各种基准测试中是有效的，并且在与以前的最先进的 UDA 方法集成时表现出一致的协同效应。该实现可在 http://github.com/jkee58/CSI 获得。||
|**2024-08-05**|[REVISION: Rendering Tools Enable Spatial Fidelity in Vision-Language Models](http://arxiv.org/abs/2408.02231)|null|文本到图像 (T2I) 和多模态大型语言模型 (MLLM) 已被应用于解决多种计算机视觉和多模态学习任务。然而，人们发现此类视觉语言模型缺乏对空间关系进行正确推理的能力。为了解决这一缺陷，我们开发了 REVISION 框架，该框架提高了视觉语言模型中的空间保真度。REVISION 是一个基于 3D 渲染的管道，可以在给定文本提示的情况下生成空间准确的合成图像。REVISION 是一个可扩展的框架，目前支持 100 多个 3D 资源、11 种空间关系，所有这些都具有不同的相机视角和背景。利用来自 REVISION 的图像作为免训练的额外指导，可以持续提高 T2I 模型在所有空间关系中的空间一致性，从而在 VISOR 和 T2I-CompBench 基准测试中取得了具有竞争力的性能。我们还设计了 RevQA，这是一个问答基准，用于评估 MLLM 的空间推理能力，并发现最先进的模型在对抗性设置下对复杂的空间推理不具有鲁棒性。我们的结果和发现表明，利用基于渲染的框架是开发空间感知生成模型的有效方法。||
|**2024-08-04**|[AdaCBM: An Adaptive Concept Bottleneck Model for Explainable and Accurate Diagnosis](http://arxiv.org/abs/2408.02001)|**[link](https://github.com/AIML-MED/AdaCBM)**|将视觉语言模型（如 CLIP 和概念瓶颈模型 (CBM)）相结合，为解释深度神经网络 (DNN) 决策提供了一种很有前景的方法，可以使用人类可理解的概念来解释，从而解决了 DNN 的黑盒问题。虽然 CLIP 提供了可解释性和零样本分类能力，但它在通用图像和文本数据上的预训练可能会限制其分类精度和在医学图像诊断任务中的适用性，从而产生迁移学习问题。为了保持可解释性并解决迁移学习需求，CBM 方法通常在瓶颈模块之后设计后处理模块。然而，这种方法一直效果不佳。本文采用了一种非传统的方法，通过将 CBM 框架视为一个简单的线性分类系统的几何表示来重新审视它。分析发现，CBM 后的微调模块只是重新缩放和平移了系统的分类结果，未能充分利用系统的学习潜力。我们在 CLIP 和 CBM 之间引入了一个自适应模块，以弥合源域和下游域之间的差距。这种简单而有效的方法在保持框架提供的可解释性的同时提高了分类性能。我们的工作提供了一个涵盖从概念发现到模型训练的完整流程的综合解决方案，为利用 GPT、CLIP 和 CBM 的优势提供了一个完整的方案。||
|**2024-08-04**|[Visual Grounding for Object-Level Generalization in Reinforcement Learning](http://arxiv.org/abs/2408.01942)|**[link](https://github.com/pku-rl/copl)**|泛化是遵循自然语言指令的智能体面临的关键挑战。为了实现这一目标，我们利用视觉语言模型 (VLM) 进行视觉基础，并将其视觉语言知识迁移到以对象为中心的强化学习 (RL) 任务中，这使得智能体能够对未见过的对象和指令进行零样本泛化。通过视觉基础，我们获得了指令中指示的目标对象的基于对象的置信度图。基于此图，我们介绍了将 VLM 知识迁移到 RL 的两种途径。首先，我们提出了一种从置信度图导出的基于对象的内在奖励函数，以更有效地引导智能体朝向目标对象。其次，与语言嵌入相比，置信度图为智能体策略提供了更统一、更易于访问的任务表示。这使得智能体能够通过易于理解的视觉置信度图处理未见过的对象和指令，从而促进零样本对象级泛化。单任务实验表明，我们的内在奖励显着提高了挑战性技能学习的性能。在多任务实验中，通过对训练集以外的任务进行测试，我们表明，当智能体获得置信度图作为任务表示时，它比基于语言的条件化方法具有更好的泛化能力。代码可在 https://github.com/PKU-RL/COPL 获取。||
|**2024-08-03**|[Is Generative Communication between Embodied Agents Good for Zero-Shot ObjectNav?](http://arxiv.org/abs/2408.01877)|null|在零样本目标导航 (Zero-Shot ObjectNav) 中，我们期望一个具身化的地面智能体能够根据自然语言标签指定的的目标对象进行导航，而无需任何特定于环境的微调。考虑到地面智能体有限的视野及其独立的探索行为，这是一项具有挑战性的任务。为了解决这些问题，我们考虑在配备有限全局视野的辅助空中智能体与地面智能体协同工作，并提出了两种用于明智探索的协调导航方案。我们通过实验证明，配备视觉语言模型 (VLM) 的具身智能体之间的生成式沟通 (GC) 有助于改善零样本目标导航，与无辅助设置相比，地面智能体找到目标对象的能力提高了 10%。我们进一步分析了生成式沟通的独特特征，量化了幻觉和合作的存在。特别地，我们发现了一种特定于我们具身化环境的独特特征，称为“先发制人的幻觉”，即空中智能体会假设地面智能体已经在对话中执行了一个动作，而实际上它还没有移动。最后，我们使用生成式沟通进行了真实世界的推断，并展示了一些定性示例，说明通过提示微调来对抗先发制人的幻觉可以提高真实世界中的目标导航性能。||
|**2024-08-02**|[GraphAge: Unleashing the power of Graph Neural Network to Decode Epigenetic Aging](http://arxiv.org/abs/2408.00984)|null|DNA甲基化是一种重要的表观遗传标记，被用于各种时钟来预测表观遗传年龄。然而，许多现有的时钟未能考虑到关于CpG位点及其相互关系的关键信息，例如共甲基化模式。我们提出了一种新的方法，将甲基化数据表示为图形，使用甲基化值和关于CpG位点的相关信息作为节点，并将共甲基化、相同基因和相同染色体等关系作为边。然后，我们使用图神经网络 (GNN) 来预测年龄。因此，我们的模型GraphAge利用结构和位置信息进行预测以及更好的解释。尽管我们不得不在受限的计算环境中进行训练，但GraphAge仍然表现出具有竞争力的性能，平均绝对误差 (MAE) 为 3.207，均方误差 (MSE) 为 25.277，略优于当前最先进的水平。也许更重要的是，我们利用 GNN 解释器进行解释，并能够发现有趣的见解（例如，关键 CpG 位点、通路及其在衰老背景下通过甲基化调控网络的关系），如果没有利用 GraphAge “编码”各种结构关系的独特能力，这些见解是不可能被“解码”的。GraphAge 有可能消耗和利用与个体衰老复杂过程相关的所有相关信息（如果有的话）。因此，从这个意义上说，它是同类中的第一个，可以被视为多模态模型的第一个基准，该模型可以整合所有这些信息，以缩小我们对衰老真实本质理解上的差距。||
|**2024-08-01**|[Towards Zero-Shot Annotation of the Built Environment with Vision-Language Models (Vision Paper)](http://arxiv.org/abs/2408.00932)|null|公平的城市交通应用需要对建筑环境进行高保真度的数字化呈现：不仅包括街道和人行道，还包括自行车道、标记和未标记的人行横道、路缘坡道和路缘切口、障碍物、交通信号灯、标识、街道标记、坑洼等等。直接检查和人工标注在大规模情况下成本高昂。传统的机器学习方法需要大量的标注训练数据才能获得足够的性能。在本文中，我们将视觉语言模型视为一种从卫星图像中标注不同城市特征的机制，减少了对人工标注的依赖，从而生成大型训练集。虽然这些模型在描述从人类视角拍摄的图像中的常见物体方面取得了令人印象深刻的结果，但它们的训练集不太可能包含建筑环境中特殊特征的强信号，因此它们在这些环境中的性能尚不清楚。我们展示了一种概念验证，它结合了最先进的视觉语言模型和提示策略的变体，该策略要求模型独立于原始图像考虑分割元素。对两种城市特征（停止线和抬高台）的实验表明，虽然直接零样本提示几乎无法正确标注任何图像，但预分割策略可以标注图像，其交并比精度接近 40%。我们描述了这些结果如何为建筑环境自动标注的新研究议程提供信息，以在更大范围内和不同环境中提高公平性、可访问性和安全性。||
|**2024-08-01**|[Collaborative Vision-Text Representation Optimizing for Open-Vocabulary Segmentation](http://arxiv.org/abs/2408.00744)|**[link](https://github.com/jiaosiyu1999/MAFT-Plus)**|预训练的视觉语言模型，例如 CLIP，由于其良好对齐的视觉文本嵌入空间，已被越来越多地用于解决具有挑战性的开放词汇分割 (OVS) 任务。典型的解决方案包括在训练期间冻结 CLIP 以单方面保持其零样本能力，或微调 CLIP 视觉编码器以实现对局部区域的感知敏感性。然而，很少有人将视觉文本协同优化结合起来。基于此，我们提出了Content-Dependent Transfer，通过与输入图像交互来自适应地增强每个文本嵌入，这提供了一种参数高效的方式来优化文本表示。此外，我们还引入了表示补偿策略，将原始 CLIP-V 表示作为补偿进行审查，以保持 CLIP 的零样本能力。通过这种方式，CLIP 的视觉和文本表示得到协同优化，增强了视觉文本特征空间的对齐。据我们所知，我们是第一个在 OVS 领域内建立视觉文本协同优化机制的。大量实验表明，我们的方法在流行的 OVS 基准测试中取得了优异的性能。在开放词汇语义分割中，我们的方法在 A-847、A-150、PC-459、PC-59 和 PAS-20 上分别比之前的最先进方法高出 +0.5、+2.3、+3.4、+0.4 和 +1.1 mIoU。此外，在 ADE20K 的全景设置中，我们实现了 27.1 PQ、73.5 SQ 和 32.9 RQ 的性能。代码将在 https://github.com/jiaosiyu1999/MAFT-Plus.git 上提供。||
|**2024-08-01**|[Are Bigger Encoders Always Better in Vision Large Models?](http://arxiv.org/abs/2408.00620)|null|近年来，多模态大语言模型（MLLM）在现实应用中展现出巨大潜力。得益于其理解多模态信息以及强大的认知和推理能力，MLLM正处于快速发展阶段。在众多MLLM中，视觉语言模型（VLM）因其理解视觉信息的能力而备受瞩目。然而，在当前主流范式下，VLM的规模化趋势尚未得到充分研究。我们是否可以通过训练更大的模型来获得更好的性能仍尚不清楚。为了解决这个问题，我们对MLLM的预训练阶段进行了实验。我们使用不同的编码器大小和大语言模型（LLM）大小进行了实验。我们的研究结果表明，仅仅增加编码器的规模并不一定会提高VLM的性能。此外，我们还分析了LLM骨干参数大小和数据质量对预训练结果的影响。此外，我们还探讨了LLM和VLM之间缩放法则的差异。||
|**2024-08-01**|[GalleryGPT: Analyzing Paintings with Large Multimodal Models](http://arxiv.org/abs/2408.00491)|**[link](https://github.com/steven640pixel/gallerygpt)**|艺术品分析是艺术欣赏中一项重要且基础的技能，它可以丰富个人的审美感受并促进批判性思维能力的发展。然而，由于艺术作品的主观性、多样化的解读方式以及复杂的视觉元素，理解艺术品极具挑战性，需要艺术史、文化背景和美学理论方面的专业知识。受限于数据收集和模型能力，以往的艺术品自动分析工作主要集中在分类、检索等简单任务上，这与人工智能的目标相去甚远。为了促进研究进展，本文以大型多模态模型卓越的感知和生成能力为灵感，进一步探索了生成综合分析的方法。具体而言，我们首先提出了一项为艺术作品（本文以绘画为例）撰写段落分析的任务，该任务仅关注视觉特征，旨在更全面地理解艺术作品。为了支持形式分析研究，我们收集了一个名为PaintingForm的大型数据集，其中包含约1.9万张绘画图像和5万段分析文本。我们进一步介绍了一种用于绘画分析生成的优秀大型多模态模型，名为GalleryGPT，它是在LLaVA架构的基础上，利用我们收集的数据进行微调和优化的。我们跨多个数据集进行了形式分析生成和零样本实验，以评估我们模型的能力。结果表明，与强大的基线语言模型相比，我们的模型取得了显著的性能提升，展示了其在艺术分析和泛化方面的出色能力。\textcolor{blue}{代码和模型可在以下网址获取：https://github.com/steven640pixel/GalleryGPT。}||
|**2024-08-01**|[DECIDER: Leveraging Foundation Model Priors for Improved Model Failure Detection and Explanation](http://arxiv.org/abs/2408.00331)|null|可靠地检测部署的机器学习模型何时可能在给定输入上失败对于确保安全操作至关重要。在这项工作中，我们提出了 DECIDER（去偏差分类器以可靠地识别错误），这是一种利用大型语言模型 (LLM) 和视觉语言模型 (VLM) 的先验来检测图像分类模型中的故障的新方法。DECIDER 利用 LLM 指定与任务相关的核心属性，并通过使用 VLM 将其视觉特征与这些核心属性对齐来构建分类器的“去偏差”版本，并通过测量原始模型和去偏差模型之间的差异来检测潜在故障。除了主动识别模型可能失败的样本外，DECIDER 还通过一种新颖的属性消融策略为失败提供人类可解释的解释。通过跨越子群体偏移（虚假相关性、类别不平衡）和协变量偏移（合成损坏、域偏移）的不同基准的广泛实验，DECIDER 始终如一地实现了最先进的故障检测性能，在整体马修斯相关系数方面显着优于基线以及失败和成功召回。我们的代码可以在~\url{https://github.com/kowshikthopalli/DECIDER/}访问。||
|**2024-08-01**|[OmniParser for Pure Vision Based GUI Agent](http://arxiv.org/abs/2408.00203)|null|大型视觉语言模型最近的成功显示出其在驱动基于用户界面的代理系统方面的巨大潜力。然而，我们认为，由于缺乏强大的屏幕解析技术，像 GPT-4V 这样强大的多模态模型作为跨不同应用程序的多个操作系统上的通用代理的能力被大大低估了，这种技术能够：1）可靠地识别用户界面中的可交互图标，以及 2）理解屏幕截图中各种元素的语义，并将预期动作与屏幕上的相应区域准确关联起来。为了填补这些空白，我们引入了 \textsc{OmniParser}，这是一种将用户界面屏幕截图解析为结构化元素的综合方法，它显著增强了 GPT-4V 生成可以准确地基于界面相应区域的动作的能力。我们首先使用流行网页和图标描述数据集整理了一个可交互图标检测数据集。这些数据集被用于微调专门的模型：一个用于解析屏幕上可交互区域的检测模型和一个用于提取检测到的元素的功能语义的描述模型。\textsc{OmniParser} 显着提高了 GPT-4V 在 ScreenSpot 基准测试中的性能。在 Mind2Web 和 AITW 基准测试中，仅使用屏幕截图输入的 \textsc{OmniParser} 优于需要屏幕截图以外的额外信息的 GPT-4V 基线。||
|**2024-07-31**|[Vision-Language Model Based Handwriting Verification](http://arxiv.org/abs/2407.21788)|null|笔迹鉴定在文件检验中至关重要。基于深度学习的方法由于缺乏可解释性以及依赖于大量的训练数据和手工特征，经常受到法医文件检验员的质疑。本文探讨了使用视觉语言模型 (VLM)，例如 OpenAI 的 GPT-4o 和 Google 的 PaliGemma，来应对这些挑战。通过利用其视觉问答能力和零样本思维链 (CoT) 推理，我们的目标是为模型决策提供清晰易懂的解释。我们对 CEDAR 笔迹数据集的实验表明，VLM 增强了可解释性，减少了对大型训练数据集的需求，并且能够更好地适应不同的笔迹风格。然而，结果表明，基于 CNN 的 ResNet-18 架构的性能优于采用零样本 CoT 提示工程方法的 GPT-4o（准确率：70%）和经过监督微调的 PaliGemma（准确率：71%），在 CEDAR AND 数据集上达到了 84% 的准确率。这些发现突出了 VLM 在生成人类可理解的决策方面的潜力，同时也强调了需要进一步改进以匹配专门深度学习模型的性能。||
|**2024-07-31**|[Open-Vocabulary Audio-Visual Semantic Segmentation](http://arxiv.org/abs/2407.21721)|null|音视语义分割 (AVSS) 旨在利用声音线索对视频中的发声对象进行分割和分类。然而，大多数方法都是在封闭集假设下运行的，只能识别训练数据中预定义的类别，缺乏在实际应用中检测新类别的泛化能力。在本文中，我们介绍了一项新任务：开放词汇音视语义分割，将 AVSS 任务扩展到标注标签空间之外的开放世界场景。这是一项更具挑战性的任务，需要识别所有类别，即使是那些在训练中从未见过或从未听过的类别。此外，我们提出了第一个开放词汇 AVSS 框架，OV-AVSS，它主要由两部分组成：1）一个通用的声源定位模块，用于执行视听融合并定位所有潜在的发声对象，以及 2）一个开放词汇分类模块，用于借助大规模预训练视觉语言模型的先验知识来预测类别。为了正确评估开放词汇 AVSS，我们基于 AVSBench-semantic 基准拆分了零样本训练和测试子集，即 AVSBench-OV。大量实验表明，我们的模型对所有类别都具有强大的分割能力和零样本泛化能力。在 AVSBench-OV 数据集上，OV-AVSS 在基本类别上实现了 55.43% 的 mIoU，在新类别上实现了 29.14% 的 mIoU，分别超过了最先进的零样本方法 41.88%/20.61% 和开放词汇方法 10.2%/11.6%。代码可在 https://github.com/ruohaoguo/ovavss 获取。||
|**2024-08-01**|[Defending Jailbreak Attack in VLMs via Cross-modality Information Detector](http://arxiv.org/abs/2407.21659)|null|视觉语言模型 (VLM) 扩展了大型语言模型 (LLM) 全面理解视觉信息的能力，在许多以视觉为中心的的任务中取得了显著成果。尽管如此，最近的研究表明，这些模型容易受到越狱攻击，这是一种利用技术，恶意用户可以破坏目标模型的安全对齐，并生成误导性和有害的答案。这种潜在威胁是由 LLM 固有的脆弱性以及视觉输入带来的更大攻击范围共同造成的。为了增强 VLM 对抗越狱攻击的安全性，研究人员开发了各种防御技术。然而，这些方法要么需要修改模型的内部结构，要么在推理阶段需要大量的计算资源。多模态信息是一把双刃剑。虽然它增加了攻击的风险，但它也提供了可以增强安全保障的额外数据。受此启发，我们提出了跨模态信息检测器 ( $\textit{CIDER}$)，这是一种即插即用的越狱检测器，旨在利用有害查询和对抗性图像之间的跨模态相似性来识别恶意扰动的图像输入。这种简单而有效的跨模态信息检测器 $\textit{CIDER}$ 独立于目标 VLM，并且需要较少的计算成本。大量的实验结果证明了 $\textit{CIDER}$ 的有效性和效率，以及它对白盒和黑盒 VLM 的可迁移性。||
|**2024-07-31**|[MTA-CLIP: Language-Guided Semantic Segmentation with Mask-Text Alignment](http://arxiv.org/abs/2407.21654)|null|最近的研究表明，像 CLIP 这样的大规模视觉语言模型可以提高语义分割的性能。这些方法通常旨在实现像素级的视觉语言对齐，但往往依赖于来自 CLIP 的低分辨率图像特征，导致边界上的类别模糊。此外，CLIP 文本嵌入中的全局场景表示与局部和详细的像素级特征没有直接关联，使得有意义的对齐更加困难。为了解决这些限制，我们引入了 MTA-CLIP，这是一个采用掩码级视觉语言对齐的新框架。具体来说，我们首先提出了掩码-文本解码器，它使用 CLIP 语言模型和丰富的文本数据来增强掩码表示。随后，它使用掩码到文本的对比学习将掩码表示与文本嵌入对齐。此外，我们还引入了掩码文本提示学习，利用多个特定于上下文的提示来嵌入文本，以捕获跨掩码的不同类别表示。总的来说，MTA-CLIP 实现了最先进的性能，在标准基准数据集 ADE20k 和 Cityscapes 上，分别比之前的工作平均提高了 2.8% 和 1.3%。||
|**2024-07-31**|[Prompting Medical Large Vision-Language Models to Diagnose Pathologies by Visual Question Answering](http://arxiv.org/abs/2407.21368)|null|近年来，大型视觉语言模型（LVLM）取得了重大成功，并已扩展到医学领域。尽管在医学视觉问答（VQA）任务中表现出令人满意的性能，但医学 LVLM（MLVLM）存在幻觉问题，这使得它们无法诊断复杂的病理。此外，由于训练数据不平衡，它们很难学习少数病理。我们提出了两种针对 MLVLM 的提示策略，以减少幻觉并提高 VQA 性能。在第一个策略中，我们提供了所查询病理的详细解释。在第二个策略中，我们微调了一个成本低廉的弱学习器，以在特定指标上实现高性能，并以文本形式将其判断提供给 MLVLM。在 MIMIC-CXR-JPG 和 Chexpert 数据集上进行测试，我们的方法显着提高了诊断 F1 分数，最高增幅为 0.27。我们还证明了我们的提示策略可以扩展到一般的 LVLM 领域。根据 POPE 指标，它有效地抑制了现有 LVLM 的假阴性预测，并将召回率提高了约 0.07。||
|**2024-07-31**|[SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving](http://arxiv.org/abs/2407.21293)|null|大型语言模型（LLM）的快速发展可以让许多领域受益。由于LLM支持越来越多的模态，端到端自动驾驶（e2eAD）是面临新机遇的典型领域之一。在这里，我们利用视觉语言模型（VLM）提出了一种名为SimpleLLM4AD的e2eAD方法。在我们的方法中，e2eAD任务被分为四个阶段，即感知、预测、规划和行为。每个阶段都包含多个视觉问答（VQA）对，并且VQA对相互连接构成一个称为图VQA（GVQA）的图。通过VLM逐阶段推理GVQA中的每个VQA对，我们的方法可以使用语言实现端到端驾驶。在我们的方法中，视觉变换器（ViT）模型用于处理nuScenes视觉数据，而VLM则用于解释和推理从视觉输入中提取的信息。在感知阶段，系统从驾驶环境中识别和分类物体。预测阶段涉及预测这些物体的潜在运动。规划阶段利用收集到的信息制定驾驶策略，确保自动驾驶汽车的安全性和效率。最后，行为阶段将计划好的动作转换为车辆可执行的命令。我们的实验表明，SimpleLLM4AD在复杂的驾驶场景中实现了具有竞争力的性能。||
|**2024-07-30**|[Evolver: Chain-of-Evolution Prompting to Boost Large Multimodal Models for Hateful Meme Detection](http://arxiv.org/abs/2407.21004)|null|最近的研究进展表明，双流方法在仇恨模因检测方面取得了优异的性能。然而，随着新的模因通过融合进步的文化理念而不断涌现，仇恨模因也在不断演变，这使得现有的方法变得过时或无效。在这项工作中，我们探索了大型多模态模型 (LMM) 在仇恨模因检测中的潜力。为此，我们提出了 Evolver，它通过进化链 (CoE) 提示将 LMMs 结合起来，整合了模因的进化属性和上下文信息。具体来说，Evolver 模拟了模因的演变和表达过程，并通过 LMMs 逐步进行推理。首先，进化对挖掘模块在外部策展模因集中检索与输入模因最相似的 top-k 个模因。其次，设计了一个进化信息提取器来总结配对模因之间的语义规律，用于提示。最后，上下文相关性放大器增强了上下文仇恨信息，以促进对进化过程的搜索。在公共 FHM、MAMI 和 HarM 数据集上的大量实验表明，CoE 提示可以融入现有的 LMMs 中以提高其性能。更令人鼓舞的是，它可以作为一种解释工具来促进对社交模因演变的理解。||
|**2024-07-30**|[GABInsight: Exploring Gender-Activity Binding Bias in Vision-Language Models](http://arxiv.org/abs/2407.21001)|null|视觉语言模型 (VLM) 被广泛应用于许多下游任务，包括需要评估图像中出现的人物身份的任务。虽然 VLM 在简单的单人场景中表现良好，但在现实应用中，我们经常会遇到复杂的情况，例如不同性别的人在进行不同的活动。我们发现，在这种情况下，VLM 倾向于将预期性别（根据模型中根深蒂固的性别刻板印象或其他形式的样本选择偏差）的个体识别为活动的执行者。我们将这种将活动与其在图像或文本中的实际执行者的性别相关联的偏差称为性别-活动绑定 (GAB) 偏差，并分析了这种偏差是如何在 VLM 中内化的。为了评估这种偏差，我们引入了 GAB 数据集，其中包含大约 5500 张 AI 生成的图像，这些图像代表了各种活动，解决了某些场景下现实世界图像稀缺的问题。为了进行广泛的质量控制，我们对生成的图像的多样性、质量和真实性进行了评估。我们在文本到图像和图像到文本检索的背景下，在这个数据集上测试了 12 个著名的预训练 VLM，以衡量这种偏差对其预测的影响。此外，我们还进行了一些补充实验，以量化 VLM 文本编码器中的偏差，并评估 VLM 识别活动的能力。我们的实验表明，当遇到性别-活动绑定偏差时，VLM 的平均性能下降约为 13.2%。||
|**2024-07-30**|[UniProcessor: A Text-induced Unified Low-level Image Processor](http://arxiv.org/abs/2407.20928)|**[link](https://github.com/intmegroup/uniprocessor)**|图像处理，包括图像修复、图像增强等，涉及从退化的输入生成高质量的清晰图像。基于深度学习的方法在单任务条件下对各种图像处理任务表现出优越的性能。然而，它们需要针对不同的退化类型和级别训练单独的模型，这限制了这些模型的泛化能力，并限制了它们在现实世界中的应用。在本文中，我们提出了一种文本引导的统一图像处理器，用于低级视觉任务，称为UniProcessor，它可以有效地处理各种退化类型和级别，并支持多模态控制。具体来说，我们的UniProcessor使用主题提示对特定退化信息进行编码，并使用操作提示处理退化。这些上下文控制特征通过交叉注意力注入到UniProcessor主干中，以控制处理过程。为了自动生成主题提示，我们进一步构建了一个视觉语言模型，用于通过指令微调技术进行通用的低级退化感知。我们的UniProcessor涵盖了30种退化类型，广泛的实验表明，我们的UniProcessor可以很好地处理这些退化，而无需额外的训练或微调，并且优于其他竞争方法。此外，在退化感知上下文控制的帮助下，我们的UniProcessor首次展示了在具有多种退化的图像中单独处理单个失真的能力。||
|**2024-07-30**|[SSPA: Split-and-Synthesize Prompting with Gated Alignments for Multi-Label Image Recognition](http://arxiv.org/abs/2407.20920)|null|多标签图像识别是计算机视觉中的一项基本任务。近年来，视觉语言模型（VLM）在这一领域取得了显著进展。然而，以往的方法未能有效地利用语言模型中的丰富知识，并且经常将标签语义单向地融入视觉特征中。为了克服这些问题，我们提出了一种带有门控对齐的分裂合成提示（SSPA）框架，以放大VLM的潜力。具体来说，我们开发了一种上下文学习方法来关联来自大型语言模型（LLM）的固有知识。然后，我们提出了一种新颖的分裂合成提示（SSP）策略，首先分别对通用知识和下游标签语义进行建模，然后通过四元数网络将它们仔细地聚合起来。此外，我们提出了门控双模态对齐（GDMA），以双向交互视觉和语言模态，同时消除冗余的跨模态信息，从而实现更高效的区域级对齐。我们提出了一种软聚合器，共同考虑所有图像区域的结果，而不是像以前的工作那样以一种急剧的方式做出最终预测。在灵活的提示和门控对齐的帮助下，SSPA可以推广到特定的领域。在三个领域（即自然、行人属性和遥感）的九个数据集上进行的大量实验证明了SSPA的最先进性能。进一步的分析验证了SSP的有效性和GDMA的可解释性。代码将公开。||
|**2024-07-30**|[Effectively Leveraging CLIP for Generating Situational Summaries of Images and Videos](http://arxiv.org/abs/2407.20642)|**[link](https://github.com/LUNAProject22/CLIPSitu)**|情境识别是指代理根据可用信息和感官输入识别和理解各种情境或上下文的能力。它涉及解释来自环境的数据以确定正在发生的事情、涉及哪些因素以及哪些行为导致了这些情境的认知过程。在基于计算机视觉的情境识别中，这种对情境的解释被表述为语义角色标注问题。图像和视频中描述的情境包含关键信息，这对于图像和视频字幕、多媒体检索、自主系统和事件监控等各种应用至关重要。然而，现有方法在生成有意义和准确的预测方面，往往难以应对歧义和缺乏上下文的问题。利用CLIP等多模态模型，我们提出了ClipSitu，它避免了完全微调的需要，并在情境识别和定位任务中取得了最先进的结果。ClipSitu利用基于CLIP的图像、动词和角色嵌入来预测名词，从而实现对所描述场景的全面理解。通过交叉注意力Transformer，ClipSitu XTF增强了语义角色查询和视觉标记表示之间的联系，从而在情境识别方面实现了卓越的性能。我们还提出了一个具有近乎完美准确率的动词角色预测模型，以创建一个端到端的框架，用于生成域外图像的情境摘要。我们证明，与通用字幕相比，情境摘要使我们的ClipSitu模型能够生成结构化描述，并减少歧义。最后，我们将ClipSitu扩展到视频情境识别，以展示其多功能性，并产生与最先进方法相当的性能。||
|**2024-07-30**|[Autonomous Improvement of Instruction Following Skills via Foundation Models](http://arxiv.org/abs/2407.20635)|null|能够从自主收集的经验中学习改进的智能指令跟随机器人在机器人学习领域拥有巨大潜力：它们无需收集成本高昂的遥操作演示数据，而是可以通过大规模部署机器人队伍来快速收集大量的自主数据，从而共同提高其性能。然而，自主改进需要解决两个关键问题：(i) 如何完全自动化可扩展的数据收集程序，以收集多样且语义上有意义的机器人数据；(ii) 如何从未经人工标注的非最优自主数据中学习。为此，我们提出了一种解决这些挑战的新方法，使指令跟随策略能够在没有人为监督的情况下从自主收集的数据中学习改进。我们的框架利用视觉语言模型在新环境中收集和评估语义上有意义的经验，然后将指令跟随任务分解为（语义）语言条件图像生成和（非语义）目标达成，这使得从这些自主收集的数据中学习改进变得更加实用，而无需任何人工标注。我们在现实世界中进行了大量的实验，以证明我们方法的有效性，并发现机器人在面对一系列未知环境时，其策略可以通过自主收集的数据得到显著改进。我们将开源我们语义自主改进管道的代码，以及我们在五个桌面环境中收集的包含 3.05 万条轨迹的自主数据集。||
|**2024-07-29**|[FlexAttention for Efficient High-Resolution Vision-Language Models](http://arxiv.org/abs/2407.20228)|null|目前高分辨率的视觉语言模型将图像编码为高分辨率图像token，并穷举所有这些token来计算注意力，这显著增加了计算成本。为了解决这个问题，我们提出了FlexAttention，一种用于高效高分辨率视觉语言模型的灵活注意力机制。具体来说，一张高分辨率图像被编码为高分辨率token和低分辨率token，其中只有低分辨率token和少数选定的高分辨率token被用于计算注意力图，这大大降低了计算成本。高分辨率token的选择是通过一个高分辨率选择模块完成的，该模块可以根据输入的注意力图检索相关区域的token。然后将选定的高分辨率token与低分辨率token和文本token连接起来，并输入到一个分层的自注意力层，该层会生成一个注意力图，用于下一步的高分辨率token选择。分层自注意力过程和高分辨率token选择过程在每个注意力层迭代执行。在多模态基准测试上的实验结果证明，我们的FlexAttention优于现有的高分辨率视觉语言模型（例如，在V* Bench上相对提高约9%，在TextVQA上相对提高约7%），同时显著降低了近40%的计算成本。||
|**2024-07-29**|[Adversarial Robustness in RGB-Skeleton Action Recognition: Leveraging Attention Modality Reweighter](http://arxiv.org/abs/2407.19981)|null|深度神经网络 (DNN) 已被应用于许多计算机视觉任务并取得了最先进的 (SOTA) 性能。然而，当 DNN 预测通过向自然样本添加人眼难以察觉的对抗性噪声而创建的对抗性样本时，就会发生错误分类。这限制了 DNN 在安全关键领域的应用。为了增强模型的鲁棒性，之前的研究主要集中在单模态领域，例如图像识别和视频理解。尽管多模态学习在各种任务（例如动作识别）中取得了先进的性能，但关于 RGB 骨架动作识别模型鲁棒性的研究却很少。在本文中，我们系统地研究了如何提高 RGB 骨架动作识别模型的鲁棒性。我们最初对不同模态的鲁棒性进行了实证分析，并观察到骨架模态比 RGB 模态更鲁棒。基于这一观察结果，我们提出了基于注意力的模态重加权器 (AMR)，它利用注意力层对两种模态进行重新加权，使模型能够学习更鲁棒的特征。我们的 AMR 即插即用，可以轻松地与多模态模型集成。为了证明 AMR 的有效性，我们对各种数据集进行了广泛的实验。例如，与 SOTA 方法相比，在 NTU-RGB+D 60 数据集上，AMR 对 PGD20 攻击的改进率为 43.77%。此外，它有效地平衡了不同模态之间鲁棒性的差异。||
|**2024-07-29**|[Normality Addition via Normality Detection in Industrial Image Anomaly Detection Models](http://arxiv.org/abs/2407.19849)|null|图像异常检测 (IAD) 的任务旨在识别图像数据中与正常情况的偏差。这些异常是与 IAD 模型在训练期间从数据中学习到的模式显著不同的模式。然而，在现实场景中，构成正常情况的标准经常发生变化，因此需要将以前异常的实例重新分类为正常。为了应对这一挑战，我们提出了一种称为“正常性添加”的新场景，涉及在训练后调整决策边界以纳入新的正常情况。为了应对这一挑战，我们提出了一种称为通过正常性检测进行正常性添加 (NAND) 的方法，该方法利用了视觉语言模型。NAND 执行正常性检测，根据文本描述检测图像中与预期正常性相关的模式。然后，我们修改预训练的 IAD 模型的结果以实现这种正常性添加。我们使用 IAD 中的基准数据集 MVTec AD，建立了正常性添加任务的评估方案，并通过经验证明了 NAND 方法的有效性。||
|**2024-07-29**|[ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2](http://arxiv.org/abs/2407.19832)|null|多模态大型语言模型 (MLLM) 因其多功能性而备受关注。然而，传统的 Transformer 架构由于其二次计算复杂度而导致 significant overhead。为了解决这个问题，我们引入了 ML-Mamba，一种利用最新高效的 Mamba-2 模型进行推理的多模态语言模型。Mamba-2 以其线性扩展和对长序列的快速处理而闻名。我们用预训练的 Mamba-2 模型替换了基于 Transformer 的骨干网络，并探索了将 2D 视觉选择性扫描机制集成到多模态学习中的方法。我们还尝试了各种视觉编码器和 Mamba-2 模型变体。我们在各种多模态基准测试中进行了广泛的实验，证明了 ML-Mamba 的竞争性能，并突出了状态空间模型在多模态任务中的潜力。实验结果表明：(1) ML-Mamba 通过其线性序列建模实现了与 TinyLaVA 和 MobileVLM v2 等最先进方法相当的性能，同时具有更快的推理速度；(2) ML-Mamba 在封闭集基准测试中的视觉幻觉和空间关系判断方面表现出色；(3) ML-Mamba 在将参数数量减少 40% 的情况下实现了与 LLaVA 相当的性能。(4) 与使用原始 Mamba 模型的多模态模型相比，基于 Mamba-2 的大规模多模态语言模型具有更强的推理性能和效果。||
|**2024-07-26**|[Wolf: Captioning Everything with a World Summarization Framework](http://arxiv.org/abs/2407.18908)|null|我们提出了Wolf，一个用于准确视频字幕生成的WOrLd摘要框架。Wolf是一个自动字幕生成框架，采用混合专家方法，利用视觉语言模型 (VLM) 的互补优势。通过利用图像和视频模型，我们的框架捕获不同级别的信息并对其进行有效总结。我们的方法可用于增强视频理解、自动标记和字幕生成。为了评估字幕质量，我们引入了 CapScore，这是一种基于 LLM 的指标，用于评估与真实字幕相比生成的字幕的相似性和质量。我们进一步构建了三个领域（自动驾驶、一般场景和机器人技术）的四个人工标注数据集，以促进全面比较。我们表明，与来自研究社区 (VILA1.5, CogAgent) 和商业解决方案 (Gemini-Pro-1.5, GPT-4V) 的最先进方法相比，Wolf 实现了卓越的字幕生成性能。例如，与 GPT-4V 相比，在具有挑战性的驾驶视频中，Wolf 在质量方面将 CapScore 提高了 55.6%，在相似性方面提高了 77.4%。最后，我们建立了视频字幕生成的基准并引入了排行榜，旨在加速视频理解、字幕生成和数据对齐方面的进步。排行榜：https://wolfv0.github.io/leaderboard.html。||
|**2024-07-26**|[Unifying Visual and Semantic Feature Spaces with Diffusion Models for Enhanced Cross-Modal Alignment](http://arxiv.org/abs/2407.18854)|null|图像分类模型在实际应用中经常表现出不稳定的性能，这是因为图像信息的变化，以及受试对象的视觉视角差异和光照差异的影响。为了缓解这些挑战，现有研究通常会结合与视觉数据相匹配的附加模态信息来规范模型的学习过程，从而能够从复杂的图像区域中提取高质量的视觉特征。具体而言，在多模态学习领域，跨模态对齐被认为是一种有效的策略，它通过学习视觉和语义特征的域一致潜在特征空间来协调不同的模态信息。然而，由于多模态信息之间的异质性，例如特征分布和结构的差异，这种方法可能面临局限性。为了解决这个问题，我们引入了多模态对齐和重建网络（MARNet），旨在增强模型对视觉噪声的抵抗力。重要的是，MARNet 包含一个跨模态扩散重建模块，用于平滑稳定地融合不同域的信息。在 Vireo-Food172 和 Ingredient-101 这两个基准数据集上进行的实验表明，MARNet 有效地提高了模型提取的图像信息的质量。它是一个即插即用的框架，可以快速集成到各种图像分类框架中，从而提高模型性能。||
|**2024-07-26**|[ChatSchema: A pipeline of extracting structured information with Large Multimodal Models based on schema](http://arxiv.org/abs/2407.18716)|null|目的：本研究介绍了一种名为ChatSchema的有效方法，该方法结合使用大型多模态模型（LMM）和基于模式的光学字符识别（OCR）技术，从医学论文报告的非结构化数据中提取和构建信息。通过集成预定义模式，我们旨在使LMM能够根据模式规范直接提取和标准化信息，从而促进进一步的数据输入。方法：我们的方法涉及分类和提取两个阶段的过程，用于对报告场景进行分类并构建信息结构。我们建立并注释了一个数据集来验证ChatSchema的有效性，并使用精确率、召回率、F1分数和准确率指标评估了关键信息的提取效果。在关键信息提取的基础上，我们进一步评估了价值信息的提取效果。我们对两种LMM进行了消融研究，以说明不同输入模态和方法对结构化信息提取的改进。结果：我们分析了来自北京大学第一医院的100份医学报告，并建立了一个包含2,945个键值对的真实数据集。我们使用GPT-4o和Gemini 1.5 Pro对ChatSchema进行了评估，发现GPT-4o的整体性能更高。结果如下：对于关键信息提取的结果，关键信息精确率为98.6%，关键信息召回率为98.5%，关键信息F1分数为98.6%。对于基于正确关键信息提取的价值信息提取结果，总体准确率为97.2%，精确率为95.8%，召回率为95.8%，F1分数为95.8%。一项消融研究表明，与基线相比，ChatSchema实现了更高的键值提取总体准确率和总体F1分数，分别提高了26.9%和27.4%。||
|**2024-07-25**|[Cross-Vendor Reproducibility of Radiomics-based Machine Learning Models for Computer-aided Diagnosis](http://arxiv.org/abs/2407.18060)|null|背景：机器学习模型在前列腺癌检测中跨不同 MRI 厂商的再现性仍然是一项重大挑战。方法：本研究调查了使用 Pyradiomics 和 MRCradiomics 库从 T2 加权 MRI 图像中提取的放射组学特征训练的支持向量机 (SVM) 和随机森林 (RF) 模型。使用最大相关最小冗余 (MRMR) 技术进行特征选择。我们的目标是通过多模态学习和特征融合来增强临床决策支持。结果：我们的 SVM 模型利用来自 Pyradiomics 和 MRCradiomics 的组合特征，在 Multi-Improd 数据集（西门子扫描仪）上实现了 0.74 的 AUC，但在飞利浦测试集上降至 0.60。RF 模型显示出类似的趋势，仅使用 Pyradiomics 特征的模型具有显著的稳健性（飞利浦上的 AUC 为 0.78）。结论：这些发现证明了多模态特征整合在提高机器学习模型的稳健性和泛化性方面，以支持前列腺癌检测的临床决策。这项研究标志着朝着开发可靠的 AI 驱动诊断工具迈出的重要一步，该工具可在各种成像平台上保持有效性。||
|**2024-07-25**|[Enhancing Model Performance: Another Approach to Vision-Language Instruction Tuning](http://arxiv.org/abs/2407.17813)|null|大型语言模型 (LLM) 与视觉语言 (VL) 任务的集成是人工智能领域的一项变革性发展，突出了 LLM 作为一种通用聊天机器人的多功能潜力。然而，这种演变的当前趋势集中于视觉和语言的集成，以创建能够在更多样化和现实世界环境中运行的模型。我们提出了一种称为瓶颈适配器的新方法，专门用于增强这些复杂模型的多模态功能，通过称为多模态模型调整 (MMT) 的过程实现整个多模态 LLM 框架的联合优化。我们的方法利用轻量级适配器将图像编码器和 LLM 连接起来，无需大型、复杂的神经网络。与传统的模块化训练方案不同，我们的方法采用端到端优化机制，结合适配器后，可以使用更小的参数集促进联合优化。我们的方法表现出强大的性能，准确率达到 90.12%，优于人类水平 (88.4%) 和 LaVIN-7B (89.41%)。||
|**2024-07-25**|[KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models](http://arxiv.org/abs/2407.17773)|**[link](https://github.com/ey242/kiva)**|本文研究了大型多模态模型 (LMM) 与人类成人和儿童在视觉类比推理方面的差异。“视觉类比”是指从一幅图像中推断出的抽象规则并应用于另一幅图像。虽然存在用于测试 LMM 视觉推理能力的基准，但它们需要高级技能，并且忽略了即使是幼儿也能做到的基本视觉类比。受发展心理学的启发，我们提出了一个包含 1,400 个日常物体视觉变换的新基准，用于测试 LMM 的视觉类比推理能力，并将它们与儿童和成人进行比较。我们将评估分为三个阶段：识别变化的内容（例如，颜色、数量等），变化的方式（例如，添加一个物体），以及将规则应用于新场景。我们的研究结果表明，虽然像 GPT-4V、LLaVA-1.5 和 MANTIS 这样的模型可以有效地识别“什么”发生了变化，但它们难以量化“如何”变化并将此规则外推到新物体上。相比之下，儿童和成人在所有三个阶段都表现出更强的类比推理能力。此外，测试中最强的模型 GPT-4V 在涉及颜色和大小等简单视觉属性的任务中表现更好，这与人类成人的更快反应时间相关。相反，更复杂的任务，例如数量、旋转和反射，需要对 3D 物理世界进行广泛的认知处理和理解，则提出了更重大的挑战。总而言之，这些发现突出了在主要由 2D 图像和文本组成的数据上训练模型的局限性。||
|**2024-07-24**|[Testing Large Language Models on Driving Theory Knowledge and Skills for Connected Autonomous Vehicles](http://arxiv.org/abs/2407.17211)|null|处理长尾极端情况是自动驾驶汽车 (AV) 面临的主要挑战。虽然大型语言模型 (LLM) 具有强大的泛化能力和解释能力，在处理极端情况方面具有巨大潜力，并且在自动驾驶应用方面越来越受到研究兴趣，但仍有一些技术障碍需要克服，例如严格的模型性能和对巨大计算资源的需求。在本文中，我们研究了一种应用远程或边缘 LLM 来支持自动驾驶的新方法。这种 LLM 辅助驾驶系统的一个关键问题是评估 LLM 对驾驶理论和技能的理解，确保它们有资格为 CAV 执行安全关键型驾驶辅助任务。我们针对几种专有 LLM 模型（OpenAI GPT 模型、百度文心一言和阿里通义千问）和开源 LLM 模型（清华智谱 MiniCPM-2B 和 MiniCPM-Llama3-V2.5）设计并运行了包含 500 多道多项选择题的驾驶理论测试。实验测量了模型的准确性、成本和处理延迟。实验结果表明，模型 GPT-4 通过了测试，并具有改进的领域知识，文心一言的准确率为 85%（略低于 86% 的及格线），而包括 GPT-3.5 在内的其他 LLM 模型均未通过测试。对于包含图像的测试题，多模态模型 GPT4-o 的准确率高达 96%，MiniCPM-Llama3-V2.5 的准确率达到 76%。虽然 GPT-4 在 CAV 驾驶辅助应用方面具有更大的潜力，但使用 GPT4 模型的成本要高得多，几乎是使用 GPT3.5 的 50 倍。这些结果有助于为 CAV 应用选择合适的现有 LLM 提供决策依据，并在模型性能和成本之间取得平衡。||
|**2024-07-26**|[Selective Vision-Language Subspace Projection for Few-shot CLIP](http://arxiv.org/abs/2407.16977)|**[link](https://github.com/zhuhsingyuu/ssp)**|像CLIP这样的视觉语言模型能够将不同模态的数据映射到一个统一的特征空间，从而可以通过测量给定图像和文本的相似性来实现零样本/少样本推理。然而，大多数现有方法忽略了CLIP编码特征中的模态差异，这表现为文本和图像特征彼此相距甚远，导致分类性能有限。为了解决这个问题，我们引入了一种称为选择性视觉语言子空间投影（SSP）的方法，它结合了局部图像特征并利用它们作为桥梁来增强图像-文本对之间的对齐。具体来说，我们的SSP框架包含两个并行模块：视觉投影器和语言投影器。两个投影器都利用局部图像特征来跨越图像和文本各自的子空间，从而将图像和文本特征投影到各自的子空间以实现对齐。此外，我们的方法只需要进行无需训练的矩阵计算，并且可以无缝集成到先进的基于CLIP的少样本学习框架中。在11个数据集上的大量实验表明，SSP具有优越的文本-图像对齐能力，优于最先进的对齐方法。代码可在https://github.com/zhuhsingyuu/SSP获取。||
|**2024-07-23**|[Imperfect Vision Encoders: Efficient and Robust Tuning for Vision-Language Models](http://arxiv.org/abs/2407.16526)|null|视觉语言模型 (VLM) 在视觉问答和图像描述方面表现出令人印象深刻的能力，成为视觉模型和语言模型之间的关键纽带。然而，现有的开源 VLM 严重依赖于预训练和冻结的视觉编码器（例如 CLIP）。尽管 CLIP 在不同领域具有鲁棒性，但它仍然表现出不可忽视的图像理解错误。这些错误会传播到 VLM 响应中，导致性能欠佳。在我们的工作中，我们提出了一种高效且鲁棒的方法来更新 VLM 中的视觉编码器。我们的方法有选择地、局部地更新编码器，从而在先前出现错误的数据上显着提高性能，同时保持整体鲁棒性。此外，我们还证明了我们的方法在持续少量样本更新期间的有效性。我们的方法的特点是理论基础扎实、通用性强和计算效率高。||
|**2024-07-23**|[Chameleon: Images Are What You Need For Multimodal Learning Robust To Missing Modalities](http://arxiv.org/abs/2407.16243)|null|多模态学习相较于单模态架构已展现出显著的性能提升。然而，如果缺少一种或多种模态，多模态学习方法通常表现出性能下降。这可能归因于常用的多分支设计，其中包含特定于模态的流，使得模型依赖于完整模态集的可用性。在这项工作中，我们提出了一种鲁棒的文本-视觉多模态学习方法，称为Chameleon，它完全不同于传统的多分支设计。为了实现这一点，我们提出通过将文本模态编码为视觉表示来将输入模态统一为一种格式。因此，我们的方法不需要特定于模态的分支来学习独立于模态的多模态表示，使其对缺失的模态具有鲁棒性。我们在四个流行的具有挑战性的数据集上进行了广泛的实验，包括Hateful Memes、UPMC Food-101、MM-IMDb和Ferramenta。Chameleon不仅在训练/测试时所有模态都存在的情况下实现了优越的性能，而且在模态缺失的情况下也表现出显著的鲁棒性。||
|**2024-07-22**|[AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot Anomaly Detection](http://arxiv.org/abs/2407.15795)|**[link](https://github.com/caoyunkang/adaclip)**|零样本异常检测 (ZSAD) 的目标是在来自任意新类别的图像中识别异常。本研究介绍了用于 ZSAD 任务的 AdaCLIP，它利用了预训练的视觉语言模型 (VLM) CLIP。AdaCLIP 将可学习的提示语纳入 CLIP，并通过在辅助标注的异常检测数据上进行训练来优化它们。提出了两种类型的可学习提示语：静态和动态。静态提示语在所有图像之间共享，用于初步调整 CLIP 以适应 ZSAD。相反，动态提示语针对每个测试图像生成，为 CLIP 提供动态适应能力。静态和动态提示语的组合被称为混合提示语，可以提高 ZSAD 性能。在来自工业和医疗领域的 14 个真实世界异常检测数据集上进行的大量实验表明，AdaCLIP 优于其他 ZSAD 方法，并且可以更好地泛化到不同的类别甚至领域。最后，我们的分析强调了多样化的辅助数据和优化的提示语对于增强泛化能力的重要性。代码可在 https://github.com/caoyunkang/AdaCLIP 获取。||
|**2024-07-22**|[CLIP with Generative Latent Replay: a Strong Baseline for Incremental Learning](http://arxiv.org/abs/2407.15793)|**[link](https://github.com/aimagelab/mammoth)**|随着Transformer和视觉语言模型（VLM，例如CLIP）的出现，大型预训练模型已成为增强持续学习场景中性能的常用策略。这导致了许多提示策略的发展，这些策略可以有效地微调基于Transformer的模型，而不会屈服于灾难性遗忘。然而，这些方法难以将模型专门用于与预训练显着不同的领域，并难以保持其零样本能力。在这项工作中，我们提出了用于增量提示学习的持续生成训练，这是一种减轻遗忘并适应VLM的新方法，它利用生成式重放来使提示与任务保持一致。我们还引入了一个新的指标来评估CL基准测试中的零样本能力。通过对不同领域的广泛实验，我们证明了我们的框架在适应新任务的同时提高零样本能力的有效性。进一步的分析表明，我们的方法可以弥合与联合提示调整的差距。代码库可在https://github.com/aimagelab/mammoth获取。||
|**2024-07-22**|[Concept-Based Interpretable Reinforcement Learning with Limited to No Human Labels](http://arxiv.org/abs/2407.15786)|null|近来，强化学习（RL）的进展主要利用基于神经网络的策略进行决策，但这些模型通常缺乏可解释性，对利益相关者的理解和信任构成挑战。概念瓶颈模型通过将人类可理解的概念整合到神经网络中，提供了一种可解释的替代方案。然而，先前工作的一个重大限制是假设在训练期间这些概念的人工标注很容易获得，需要人工标注员持续实时输入。为了克服这一限制，我们引入了一种新的训练方案，使强化学习算法能够通过仅查询人类标记一小部分数据，或者在极端情况下，在没有任何人工标记的情况下，有效地学习基于概念的策略。我们的算法LICORICE包含三个主要贡献：将概念学习和强化学习训练交织在一起，使用概念集成来主动选择信息丰富的数据点进行标记，以及使用简单策略去除概念数据的相关性。我们展示了LICORICE如何在三种环境中将人工标记工作量减少到500个或更少的概念标签。最后，我们提出了一个初步研究，以探索如何使用强大的视觉语言模型从原始视觉输入中推断概念，而无需明确的标签，并且对性能的影响最小。||
|**2024-07-22**|[LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding](http://arxiv.org/abs/2407.15754)|**[link](https://github.com/longvideobench/longvideobench)**|大型多模态模型 (LMM) 正在处理越来越长、越来越丰富的输入。尽管取得了进展，但很少有公共基准可以衡量这种发展。为了弥补这一差距，我们引入了 LongVideoBench，这是一个问答基准测试，其特点是视频和语言交错输入，时长可达一小时。我们的基准测试包括 3,763 个不同长度的网络收集视频及其字幕，涵盖各种主题，旨在全面评估 LMM 对长期多模态理解的能力。为此，我们将主要挑战理解为从长输入中准确检索和推理详细的多模态信息。因此，我们构建了一个名为“指称推理”的新型视频问答任务。具体而言，作为问题的一部分，它包含一个指称查询，该查询引用相关的视频上下文，称为指称上下文。然后，模型需要根据指称上下文对相关的视频细节进行推理。遵循指称推理的范式，我们策划了 17 个细粒度类别中 6,678 个人工标注的多选题，建立了最全面的长格式视频理解基准测试之一。评估表明，即使是最先进的专有模型（例如 GPT-4o、Gemini-1.5-Pro、GPT-4-Turbo），LongVideoBench 也提出了重大挑战，而开源模型的性能差距更大。此外，我们的结果表明，只有当模型能够处理更多帧时，它们在基准测试中的性能才会提高，这使得 LongVideoBench 成为评估未来一代长上下文 LMM 的宝贵基准。||
|**2024-07-22**|[Zero-Shot Embeddings Inform Learning and Forgetting with Vision-Language Encoders](http://arxiv.org/abs/2407.15731)|null|尽管大型视觉语言基础模型大量涌现，但对这些模型微调后学习和遗忘结果的估计在很大程度上仍未得到探索。受对比双编码器中模态差距重要性研究的启发，我们提出了模态间内度量（IIMM）。IIMM结合了量化图像嵌入之间相似性和错误图像与标签嵌入对之间相似性的项，可以有效预测微调后的性能变化。我们对四种最先进的视觉语言模型（CLIP、SigLIP、CoCa、EVA-02-CLIP）和五种微调技术（全微调、BitFit、注意力权重调整、LoRA、CLIP-Adapter）进行了广泛的实证分析，结果表明存在强烈的、统计学上显著的线性关系：在IIMM得分较高的任务上进行微调会产生更大的域内性能提升，但也会导致更严重的域外性能下降，一些参数高效的微调（PEFT）方法表现出极端的遗忘现象。我们将我们的度量与来自最先进模型选择方法的迁移分数进行了比较，结果表明IIMM对准确率提升的预测能力明显更强。只需对目标数据进行一次前向传递，实践者就可以利用这一关键见解，启发式地评估模型在微调后预期改进的程度。如果进一步了解模型在一些不同任务上的性能，这种启发式方法还可以进一步演变为在训练新任务时预测预期性能变化的强有力工具。||
|**2024-07-22**|[SAM2CLIP2SAM: Vision Language Model for Segmentation of 3D CT Scans for Covid-19 Detection](http://arxiv.org/abs/2407.15728)|null|本文提出了一种能够集成到任何模型和方法中的有效图像分割新方法；我们选择的范例是医学图像（3D 胸部 CT 扫描）分类以进行 Covid-19 检测。我们的方法包括结合视觉语言模型来分割 CT 扫描，然后将其输入到名为 RACNet 的深度神经网络架构中以进行 Covid-19 检测。特别是，引入了一种名为 SAM2CLIP2SAM 的新型框架用于分割，该框架利用 Segment Anything Model (SAM) 和 Contrastive Language-Image Pre-Training (CLIP) 的优势来准确分割 CT 扫描中的左右肺，随后将这些分割后的输出输入 RACNet 以对 COVID-19 和非 COVID-19 病例进行分类。首先，SAM 为 CT 扫描中的每个切片生成多个基于部分的分割掩码；然后 CLIP 仅选择与感兴趣区域 (ROI) 相关的掩码，即左右肺；最后，将这些 ROI 作为提示提供给 SAM，并生成肺部的最终分割掩码。在两个 Covid-19 标注数据库中进行了实验，结果表明，当我们的方法用于 CT 扫描分割时，性能得到了提高。||
|**2024-07-22**|[HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning](http://arxiv.org/abs/2407.15680)|**[link](https://github.com/google/haloquest)**|幻觉一直是大语言模型的一个主要问题，并且在多模态方面仍然是一个关键挑战，因为视觉语言模型 (VLM) 不仅要处理文本输入，还要处理视觉输入。尽管 VLM 取得了快速进展，但用于评估和解决多模态幻觉的资源仍然有限，并且主要集中在评估上。这项工作介绍了 HaloQuest，这是一个新颖的视觉问答数据集，它捕获了多模态幻觉的各个方面，例如错误前提、上下文不足和视觉挑战。HaloQuest 的一个新颖之处在于利用合成图像（除了真实图像）来实现大规模数据集创建。HaloQuest 包含超过 7.7K 个示例，涵盖各种类别，旨在成为 VLM 的挑战性基准和推进多模态推理的微调数据集。我们的实验表明，当前的模型难以处理 HaloQuest，所有开源 VLM 的准确率都低于 36%。另一方面，在 HaloQuest 上进行微调可以显着降低幻觉率，同时保持标准推理任务的性能。我们的结果发现，使用生成图像进行基准测试与真实图像高度相关（r=0.97）。最后但同样重要的是，我们提出了一种新的自动评估机制 Auto-Eval，该机制与人类评估员高度相关（r=0.99），用于评估 VLM。总而言之，这项工作在理解、评估和减轻 VLM 中的幻觉方面取得了具体进展，朝着未来更可靠的多模态人工智能系统迈出了重要一步。||
|**2024-07-22**|[In-Context Learning Improves Compositional Understanding of Vision-Language Models](http://arxiv.org/abs/2407.15487)|**[link](https://github.com/hoezey/vlm-compositionality)**|视觉语言模型 (VLM) 在大量下游任务中表现出非凡的能力。然而，由于训练数据中存在对象偏差，组合图像理解仍然是一项相当困难的任务。在这项工作中，我们通过对 VLM 中的组合理解进行广泛的基准测试来调查造成这种能力不足的原因。我们比较了对比模型和生成模型，并分析了它们在架构、预训练数据、训练任务和损失方面的差异。此外，我们利用上下文学习 (ICL) 作为一种方法，来提高 VLM 在给定图像的情况下执行更复杂推理和理解的能力。我们广泛的实验表明，我们提出的方法在多个组合理解数据集上的表现优于基线模型。||
|**2024-07-19**|[Multimodal Misinformation Detection using Large Vision-Language Models](http://arxiv.org/abs/2407.14321)|null|misinformation的日益泛滥及其惊人影响促使工业界和学术界都致力于开发 misinformation 检测和事实核查的方法。大型语言模型 (LLM) 的最新进展在各种任务中都表现出色，但 LLM 是否以及如何帮助 misinformation 检测仍有待探索。大多数现有的最先进方法要么不考虑证据而只关注与声明相关的特征，要么假设证据是提供的。很少有方法将证据检索视为 misinformation 检测的一部分，而是依赖于微调模型。在本文中，我们研究了 LLM 在零样本设置下进行 misinformation 检测的潜力。我们将证据检索组件纳入流程中，因为从各种来源收集相关信息对于检测声明的真实性至关重要。为此，我们提出了一种使用 LLM 和大型视觉语言模型 (LVLM) 进行多模态证据检索的新型重新排序方法。检索到的证据样本（图像和文本）用作基于 LVLM 的多模态事实验证方法 (LVLM4FV) 的输入。为了进行公平的评估，我们通过为图像和文本检索注释更完整的证据样本集，解决了现有证据检索数据集中证据样本的ground truth 不完整的问题。我们在两个数据集上的实验结果证明了所提出的方法在证据检索和事实验证任务方面的优越性，以及与监督基线相比更好的跨数据集泛化能力。||
|**2024-07-19**|[Patch-based Intuitive Multimodal Prototypes Network (PIMPNet) for Alzheimer's Disease classification](http://arxiv.org/abs/2407.14277)|**[link](https://github.com/desantilisa/PIMPNet3D/blob/main/README.md)**|类似结构性磁共振成像 (sMRI) 等体积神经影像学检查通常用于支持阿尔茨海默病 (AD) 等痴呆症的临床诊断。神经放射学家检查 3D sMRI 以检测和监测由 AD 引起的脑形态异常，例如整体和/或局部脑萎缩以及特征结构的形状改变。基于深度学习 (DL) 模型开发用于分析 sMRI 以诊断 AD 的诊断系统存在强烈的研究兴趣。然而，需要将从 sMRI 检查中提取的解剖信息与患者年龄一起解释，才能区分 AD 模式与正常衰老过程中的规律改变。在这种情况下，部分原型神经网络在“设计可解释”的架构中集成了深度学习的计算优势，并在医学影像应用中显示出可观的结果。我们介绍了 PIMPNet，这是第一个用于 3D 图像和人口统计数据的可解释多模态模型，应用于根据 3D sMRI 和患者年龄对 AD 进行二元分类。尽管与单模态模型相比，年龄原型没有提高预测性能，但这为模型设计和多模态原型训练过程的未来工作奠定了基础。||
|**2024-07-19**|[Words2Contact: Identifying Support Contacts from Verbal Instructions Using Foundation Models](http://arxiv.org/abs/2407.14229)|null|本文介绍了 Words2Contact，这是一个利用大型语言模型和视觉语言模型进行语言引导的多接触点放置流程。我们的方法是语言辅助遥操作和人机合作的关键组成部分，在这种情况下，人类操作员可以使用自然语言在全身伸展或操作之前指示机器人放置支撑接触点的位置。Words2Contact 将人类操作员的口头指令转换为接触点放置预测；它还处理迭代修正，直到人类对机器人在其视野中识别的接触位置感到满意为止。我们对最先进的 LLM 和 VLM 的规模和性能进行了基准测试，以进行接触预测。我们证明了迭代校正过程的有效性，表明即使是新手用户也可以快速学习如何指示系统获得准确的位置。最后，我们使用 Talos 人形机器人在现实世界的实验中验证了 Words2Contact，该机器人由人类操作员指示将支撑接触点放置在不同的位置和表面上，以避免在伸手去拿远处物体时跌倒。||
|**2024-07-19**|[Multi-modal Relation Distillation for Unified 3D Representation Learning](http://arxiv.org/abs/2407.14007)|null|近年来，面向三维点云的多模态预训练通过对齐三维形状及其对应的二维图像和语言描述中的异构特征，取得了令人瞩目的成果。然而，当前简单直接的解决方案往往忽略了样本之间复杂的结构关系，可能限制了多模态学习的全部潜力。为了解决这个问题，我们引入了多模态关系蒸馏（MRD），这是一个三模态预训练框架，旨在有效地将可靠的大型视觉语言模型（VLM）蒸馏到三维骨干网络中。MRD旨在捕获每个模态内的内部关系以及不同模态之间的交叉关系，并生成更具判别性的三维形状表示。值得注意的是，MRD在下游零样本分类任务和跨模态检索任务中取得了显著改进，实现了新的最先进性能。||
|**2024-07-18**|[Which objects help me to act effectively? Reasoning about physically-grounded affordances](http://arxiv.org/abs/2407.13811)|null|为了与开放世界进行有效的交互，机器人应该理解与已知和未知物体的交互如何帮助它们实现目标。这种理解的一个关键方面在于检测物体的可供性，它表示通过以各种方式操纵物体可以实现的潜在效果。我们的方法利用大型语言模型（LLM）和视觉语言模型（VLM）的对话来实现开放世界可供性检测。给定开放词汇描述的预期动作和效果，可以找到环境中有用的物体。通过将我们的系统基于物理世界，我们考虑了机器人的具身性和它遇到的物体的内在属性。在我们的实验中，我们已经证明，我们的方法可以根据不同的具身性或预期效果产生定制的输出。该方法能够从一组干扰项中选择一个有用的物体。微调 VLM 的物理属性提高了整体性能。这些结果强调了将可供性搜索基于物理世界的重要性，要考虑到机器人的具身性和物体的物理属性。||
|**2024-07-18**|[Visual Haystacks: Answering Harder Questions About Sets of Images](http://arxiv.org/abs/2407.13766)|**[link](https://github.com/visual-haystacks/vhs_benchmark)**|近年来，大型多模态模型 (LMM) 在单图像视觉问答领域取得了重大进展。然而，当面对需要跨越大量图像集合的查询时，这些模型面临着巨大的挑战，类似于现实世界中的场景，如在大型相册中搜索、在互联网上查找特定信息或通过卫星图像监测环境变化。本文探讨了多图像视觉问答 (MIQA) 任务：给定一组大型图像和一个自然语言查询，任务是生成一个相关且有依据的答案。我们提出了一个新的公共基准测试集，称为“视觉草垛 (VHs)”，专门用于评估 LMM 在非相关图像集上进行视觉检索和推理的能力，我们在该基准测试集上进行了全面的评估，结果表明，即使是强大的闭源模型也难以胜任。为了解决这些缺陷，我们引入了 MIRAGE（多图像检索增强生成），这是一种专为 LMM 量身定制的新型检索/问答框架，它以显著的效率和准确率提升来应对 MIQA 的挑战，超越了基线方法。我们的评估表明，MIRAGE 在 VHs 基准测试集上的表现优于闭源 GPT-4o 模型高达 11%，并且与以文本为中心的多阶段方法相比，效率提升高达 3.4 倍。||
|**2024-07-18**|[BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models](http://arxiv.org/abs/2407.13442)|null|视觉语言模型 (VLM) 通过视觉编码器和大型语言模型 (LLM) 的组合来感知世界。视觉编码器在大规模视觉文本数据集上进行预训练，提供对视觉数据的零样本泛化能力，而 LLM 则赋予 VLM 高推理能力。这使得 VLM 无需微调即可在广泛的基准测试中实现高性能，展现出零样本或少样本学习能力。然而，最近的研究表明，VLM 容易出现幻觉。这种不良行为会降低可靠性和可信度，从而使用户无法完全信任 VLM 的输出。为了增强可信度并更好地解决 VLM 的幻觉问题，我们整理了一个新的评估数据集，称为前后幻觉数据集 (BEAF)，并引入了新的指标：真实理解 (TU)、无知 (IG)、固执 (SB) 和犹豫不决 (ID)。与之前只关注构建问答的工作不同，我们基准测试的关键思想是通过图像编辑模型操纵视觉场景信息，并根据场景变化设计指标。这使我们能够通过观察感知变化的能力来清楚地评估 VLM 是否正确理解了给定的场景。我们还凭借我们的双轴视图（视觉和文本）可视化了图像级的对象关系。通过使用我们的数据集评估 VLM，我们观察到我们的指标揭示了 VLM 幻觉的不同方面，这些方面以前从未被报道过。项目页面：\url{https://beafbench.github.io/}||
|**2024-07-17**|[R+X: Retrieval and Execution from Everyday Human Videos](http://arxiv.org/abs/2407.12957)|null|我们提出了R+X，这是一个框架，它使机器人能够从人类执行日常任务的长篇幅、未标记的第一人称视频中学习技能。在接收到人类的语言指令后，R+X首先检索包含相关行为的短视频片段，然后通过在这种行为上调整上下文内模仿学习方法来执行技能。通过利用视觉语言模型 (VLM) 进行检索，R+X 不需要对视频进行任何手动注释，并且通过利用上下文内学习进行执行，机器人可以立即执行指令的技能，而无需在检索到的视频上进行一段时间的训练。对一系列日常家居任务的研究表明，R+X 成功地将未标记的人类视频转化为强大的机器人技能，并且 R+X 的性能优于最近的几种替代方法。视频可在 https://www.robot-learning.uk/r-plus-x 上获取。||
|**2024-07-16**|[ChatBCG: Can AI Read Your Slide Deck?](http://arxiv.org/abs/2407.12875)|null|像 GPT4o 和 Gemini Flash 这样的多模态模型在推理和摘要任务方面表现出色，其性能接近人类水平。然而，我们发现，当被要求执行非常具体的“阅读和估计”任务时，尤其是在商务演示文稿中的视觉图表环境下，这些模型的表现不如人类。本文评估了 GPT 4o 和 Gemini Flash-1.5 在回答关于标记图表（数据在图表上清晰标注）和未标记图表（数据未清晰标注，必须从 X 轴和 Y 轴推断）数据的简单问题时的准确性。我们得出结论，如果演示文稿包含任何复杂或未标记的图表，这些模型目前无法准确地端到端地阅读演示文稿。即使用户创建的演示文稿仅包含标记图表，该模型也只能完美地端到端地阅读 15 个标记图表中的 7-8 个。有关幻灯片演示文稿中的所有图表，请访问 https://www.repromptai.com/chat_bcg||
|**2024-07-17**|[E5-V: Universal Embeddings with Multimodal Large Language Models](http://arxiv.org/abs/2407.12580)|**[link](https://github.com/kongds/e5-v)**|多模态大型语言模型 (MLLM) 在通用视觉和语言理解方面已展现出巨大的潜力。然而，使用 MLLM 进行多模态信息表示的研究仍然相对较少。在这项工作中，我们介绍了一个名为 E5-V 的新框架，旨在使 MLLM 适应实现通用的多模态嵌入。我们的研究结果表明，与先前的方法相比，MLLM 在表示多模态输入方面具有巨大潜力。通过利用带有提示的 MLLM，E5-V 有效地弥合了不同类型输入之间的模态差距，即使在没有微调的情况下也能在多模态嵌入方面表现出色。我们为 E5-V 提出了一种单模态训练方法，其中模型仅在文本对上进行训练。这种方法与传统的图文对多模态训练相比有了显著改进，同时将训练成本降低了约 95%。此外，这种方法还消除了对昂贵的多模态训练数据收集的需求。跨四种类型任务的大量实验证明了 E5-V 的有效性。作为一个通用的多模态模型，E5-V 不仅在每项任务中都达到了最先进的性能，而且往往还超越了它，尽管它只在单一模态上进行了训练。||
|**2024-07-17**|[VisionTrap: Vision-Augmented Trajectory Prediction Guided by Textual Descriptions](http://arxiv.org/abs/2407.12345)|null|预测其他道路参与者的未来轨迹对于自动驾驶汽车来说是一项至关重要的任务。现有的轨迹预测方法主要使用由检测和跟踪系统生成的代理轨迹和高清地图作为输入。在这项工作中，我们提出了一种新方法，该方法还结合了来自环视摄像头的视觉输入，允许模型利用人类凝视和手势、道路状况、车辆转向信号等视觉线索，这些线索在先前的方法中通常对模型是隐藏的。此外，我们使用由视觉语言模型 (VLM) 生成并由大型语言模型 (LLM) 细化的文本描述作为训练期间的监督，以指导模型从输入数据中学习什么。尽管使用了这些额外的输入，我们的方法实现了 53 毫秒的延迟，使其可用于实时处理，这明显快于具有类似性能的先前单代理预测方法。我们的实验表明，视觉输入和文本描述都有助于提高轨迹预测性能，我们的定性分析强调了模型如何能够利用这些额外的输入。最后，在这项工作中，我们创建并发布了nuScenes-Text数据集，该数据集通过为每个场景添加丰富的文本注释来增强已建立的nuScenes数据集，证明了利用VLM对轨迹预测的积极影响。我们的项目页面是https://moonseokha.github.io/VisionTrap/||
|**2024-07-17**|[ModalChorus: Visual Probing and Alignment of Multi-modal Embeddings via Modal Fusion Map](http://arxiv.org/abs/2407.12315)|null|多模态嵌入是视觉语言模型的基础，例如 CLIP 嵌入，这是使用最广泛的文本图像嵌入。然而，这些嵌入容易受到跨模态特征细微错位的 ảnh hưởng，导致模型性能下降和泛化能力减弱。为了解决这个问题，我们设计了 ModalChorus，一个用于视觉探测和多模态嵌入对齐的交互式系统。ModalChorus 主要提供一个两阶段的过程：1）使用模态融合映射（MFM）进行嵌入探测，这是一种新颖的参数化降维方法，它集成了度量和非度量目标以增强模态融合；2）嵌入对齐，允许用户交互式地表达点集和集合对齐的意图。将 CLIP 嵌入与现有降维（例如，t-SNE 和 MDS）和数据融合（例如，数据上下文图）方法进行定量和定性比较，证明了 MFM 在展示常见视觉语言数据集上的跨模态特征方面的优势。案例研究表明，ModalChorus 可以促进直观地发现错位并在从零样本分类到跨模态检索和生成的场景中进行有效的重新对齐。||
|**2024-07-17**|[VCP-CLIP: A visual context prompting model for zero-shot anomaly segmentation](http://arxiv.org/abs/2407.12276)|**[link](https://github.com/xiaozhen228/vcp-clip)**|近年来，诸如CLIP之类的大规模视觉语言模型在零样本异常分割（ZSAS）任务中展现出巨大潜力，它们利用统一模型，通过精心设计的文本提示直接检测任何未见过产品的异常。然而，现有方法通常假设待检测产品的类别是已知的，从而设置特定于产品的文本提示，这在数据隐私场景中难以实现。此外，即使是同一类型的产品，由于特定组件和生产过程的差异，也会表现出显著差异，这对文本提示的设计提出了重大挑战。为此，我们提出了一种基于CLIP的视觉上下文提示模型（VCP-CLIP），用于ZSAS任务。VCP-CLIP背后的见解是利用视觉上下文提示来激活CLIP的异常语义感知能力。具体来说，我们首先设计了一个Pre-VCP模块，将全局视觉信息嵌入到文本提示中，从而消除了对特定于产品的提示的需求。然后，我们提出了一个新颖的Post-VCP模块，利用图像的细粒度特征调整文本嵌入。在对10个真实工业异常分割数据集进行的广泛实验中，VCP-CLIP在ZSAS任务中取得了最先进的性能。代码可在https://github.com/xiaozhen228/VCP-CLIP获取。||
|**2024-07-16**|[Reflective Instruction Tuning: Mitigating Hallucinations in Large Vision-Language Models](http://arxiv.org/abs/2407.11422)|null|大型视觉语言模型 (LVLM) 在各种视觉语言任务中都表现出良好的性能。然而，它们仍然容易产生幻觉，生成与视觉内容或指令不一致的输出。虽然已经提出了各种缓解策略，但它们往往忽略了导致幻觉的一个关键因素：训练过程中缺乏细粒度推理监督。如果没有中间推理步骤，模型可能会在指令和响应之间建立肤浅的捷径，无法内化固有的推理逻辑。为了解决这一挑战，我们提出了反思指令调整，它将原理学习融入到视觉指令调整中。与以往仅从响应中学习的方法不同，我们的方法需要模型预测能够证明响应正确或错误的原因。这促使模型更深入地参与每个响应背后的细粒度推理，从而提高模型的推理能力。为了促进这种方法，我们提出了 REVERIE，这是第一个具有反思性原理注释的大规模指令调整数据集。REVERIE 包含 115k 个机器生成的推理指令，每个指令都经过精心注释，对应一对正确和混淆的响应，以及解释每个响应正确性或错误性背后的理由的全面原理。在多个 LVLM 基准测试上的实验结果表明，使用 REVERIE 数据集进行反思指令调整比基线模型产生了显著的性能提升，证明了从原理中反思的有效性。项目页面位于 https://zjr2000.github.io/projects/reverie。||
|**2024-07-16**|[Mask-Free Neuron Concept Annotation for Interpreting Neural Networks in Medical Domain](http://arxiv.org/abs/2407.11375)|**[link](https://github.com/ailab-kyunghee/mammi)**|近年来，深度神经网络的进步为辅助疾病诊断和医疗决策带来了希望。 然而，为了确保符合法规的AI模型决策过程透明，需要全面了解模型的内部运作机制。 然而，以往的方法严重依赖于昂贵的像素级标注数据集来解释模型，这在医学领域是一个很大的缺陷。 在本文中，我们提出了一种新的医学神经元概念标注方法，称为无掩码医学模型解释（MAMMI），以应对这些挑战。 通过使用视觉语言模型，我们的方法不再需要像素级掩码来进行神经元概念标注。 与其他解释方法相比，MAMMI 取得了优越的性能，证明了其在为医学图像分析中的神经元提供丰富表示方面的功效。 我们在 NIH 胸部 X 光片训练的模型上进行的实验验证了 MAMMI 的有效性，展示了其在医学领域实现透明临床决策的潜力。 代码可在 https://github.com/ailab-kyunghee/MAMMI 获取。||
|**2024-07-16**|[LaMI-DETR: Open-Vocabulary Detection with Language Model Instruction](http://arxiv.org/abs/2407.11335)|**[link](https://github.com/eternaldolphin/lami-detr)**|现有的开放词汇目标检测方法通过利用视觉语言模型 (VLM)（例如 CLIP）强大的开放词汇识别能力来增强性能。然而，出现了两个主要挑战：(1) 概念表示的缺陷，其中 CLIP 文本空间中的类别名称缺乏文本和视觉知识。(2) 对基本类别的过度拟合趋势，在从 VLM 到检测器的迁移过程中，开放词汇知识偏向于基本类别。为了应对这些挑战，我们提出了语言模型指令 (LaMI) 策略，该策略利用视觉概念之间的关系，并在一种简单而有效的类 DETR 检测器（称为 LaMI-DETR）中应用它们。LaMI 利用 GPT 来构建视觉概念，并利用 T5 来研究跨类别的视觉相似性。这些类别间关系改进了概念表示，并避免了对基本类别的过度拟合。全面的实验验证了我们的方法在相同的严格设置下优于现有方法，并且不依赖于外部训练资源。LaMI-DETR 在 OV-LVIS 上实现了 43.4 的罕见框 AP，超过了之前的最佳结果 7.8 个罕见框 AP。||
|**2024-07-16**|[Large Vision-Language Models as Emotion Recognizers in Context Awareness](http://arxiv.org/abs/2407.11300)|null|上下文感知情绪识别 (CAER) 是一项复杂且重要的任务，需要从各种上下文线索中感知情绪。先前的方法主要侧重于设计复杂的架构来从图像中提取情绪线索。然而，它们的知识仅限于特定的训练数据集，并且可能反映了标注者主观的情绪偏差。此外，在现实应用中获取大量标记数据通常具有挑战性。在本文中，我们系统地探索了利用大型视觉语言模型 (LVLM) 从三个范式赋能 CAER 任务的潜力：1) 我们在两个 CAER 数据集上微调 LVLM，这是将大型模型迁移到下游任务的最常见方式。2) 我们设计了零样本和少样本模式来评估 LVLM 在数据有限甚至完全不可见的情况下的性能。在这种情况下，我们提出了一个免训练框架来充分利用 LVLM 的上下文学习 (ICL) 能力。具体来说，我们开发了一种基于图像相似度的排序算法来检索示例；随后，将指令、检索到的示例和测试示例组合起来输入 LVLM 以获得相应的情绪判断。3) 为了利用 LVLM 丰富的知识库，我们将思维链 (CoT) 纳入我们的框架，以增强模型的推理能力并提供可解释的结果。广泛的实验和分析表明，LVLM 在不同范式下的 CAER 任务中均取得了具有竞争力的性能。值得注意的是，少样本设置下的优越性能表明 LVLM 在无需大量训练的情况下完成特定任务的可行性。||
|**2024-07-15**|[OpenPSG: Open-set Panoptic Scene Graph Generation via Large Multimodal Models](http://arxiv.org/abs/2407.11213)|null|全景场景图生成 (PSG) 旨在分割对象并识别它们之间的关系，从而实现对图像的结构化理解。先前的方法侧重于预测预定义的对象和关系类别，因此限制了它们在开放世界场景中的应用。随着大型多模态模型 (LMM) 的快速发展，开放集目标检测和分割取得了重大进展，但 PSG 中的开放集关系预测仍未得到探索。在本文中，我们专注于与预训练的开放集全景分割模型相结合的开放集关系预测任务，以实现真正的开放集全景场景图生成 (OpenPSG)。我们的 OpenPSG 利用 LMM 以自回归的方式实现开放集关系预测。我们引入了一个关系查询转换器来有效地提取对象对的视觉特征并估计它们之间关系的存在。后者可以通过过滤不相关的对来提高预测效率。最后，我们设计了生成和判断指令，以在 PSG 中以自回归的方式执行开放集关系预测。据我们所知，我们是第一个提出开放集 PSG 任务的人。大量实验表明，我们的方法在开放集关系预测和全景场景图生成方面实现了最先进的性能。代码可在 \url{https://github.com/franciszzj/OpenPSG} 获取。||
|**2024-07-15**|[Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?](http://arxiv.org/abs/2407.10956)|**[link](https://github.com/xlang-ai/spider2-v)**|数据科学和工程工作流程通常跨越多个阶段，从仓储到编排，使用BigQuery、dbt和Airbyte等工具。随着视觉语言模型（VLM）在多模态理解和代码生成方面的进步，基于VLM的代理可以通过生成SQL查询、Python代码和GUI操作来自动化这些工作流程。这种自动化可以提高专家的生产力，同时使大规模数据分析更容易获得。在本文中，我们介绍了Spider2-V，这是第一个专注于专业数据科学和工程工作流程的多模态代理基准测试，它包含494个真实计算机环境中的真实任务，并整合了20个企业级专业应用程序。这些任务源自现实世界的用例，评估多模态代理通过在企业数据软件系统中编写代码和管理GUI来执行数据相关任务的能力。为了平衡真实模拟和评估的简单性，我们致力于开发任务设置的自动配置，并为每个任务精心设计评估指标。此外，我们还为多模态代理提供了这些企业数据软件系统的全面文档。我们的实证评估表明，现有的最先进的基于LLM/VLM的代理无法可靠地自动化完整的数据工作流程（成功率为14.0%）。即使在逐步指导下，这些代理在需要细粒度、知识密集型GUI操作（16.2%）和涉及远程云托管工作区（10.6%）的任务中仍然表现不佳。我们希望Spider2-V能够为自主多模态代理转变数据科学和工程工作流程的自动化铺平道路。我们的代码和数据可在https://spider2-v.github.io获取。||
|**2024-07-15**|[Benchmarking Vision Language Models for Cultural Understanding](http://arxiv.org/abs/2407.10920)|null|Foundation models and vision-language pre-training have notably advanced Vision Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their performance has been typically assessed on general scene understanding - recognizing objects, attributes, and actions - rather than cultural comprehension. This study introduces CulturalVQA, a visual question-answering benchmark aimed at assessing VLM's geo-diverse cultural understanding. We curate a collection of 2,378 image-question pairs with 1-5 answers per question representing cultures from 11 countries across 5 continents. The questions probe understanding of various facets of culture such as clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on CulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of cultural understanding across regions, with strong cultural understanding capabilities for North America while significantly lower performance for Africa. We observe disparity in their performance across cultural facets too, with clothing, rituals, and traditions seeing higher performances than food and drink. These disparities help us identify areas where VLMs lack cultural understanding and demonstrate the potential of CulturalVQA as a comprehensive evaluation set for gauging VLM progress in understanding diverse cultures.||
|**2024-07-15**|[GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via VLM](http://arxiv.org/abs/2407.10870)|null|大型视觉语言模型（LVLM），例如生成式预训练 Transformer 4-omni（GPT-4o），是新兴的多模态基础模型，在医疗保健、工业和学术领域等无数应用中，作为强大的人工智能（AI）辅助工具具有巨大潜力。 尽管此类基础模型在各种通用任务中表现良好，但在没有微调的情况下，它们在专业任务中的能力通常有限。 然而，由于巨大的计算/内存/数据集要求，对大型基础模型进行全面微调具有挑战性。 我们证明，即使没有微调，GPT-4o 也可以解码来自前臂超声数据的  手势，并且可以通过少量样本的上下文学习得到改进。||
|**2024-07-15**|[FabGPT: An Efficient Large Multimodal Model for Complex Wafer Defect Knowledge Queries](http://arxiv.org/abs/2407.10810)|null|智能是推进集成电路（IC）制造的关键。大型多模态模型（LMM）的最新突破解锁了图像和文本理解的空前能力，促进了智能制造的发展。我们利用LMM的强大功能，推出了FabGPT，这是一种用于晶圆缺陷知识查询的定制化IC制造大型多模态模型。FabGPT在扫描电子显微镜（SEM）图像缺陷检测、根本原因分析以及提供有关制造工艺的专家问答（Q&A）方面表现出色。FabGPT匹配增强的多模态特征，自动检测复杂晶圆背景下的微小缺陷，并减少手动阈值设置的主观性。此外，所提出的调制模块和交互式语料库训练策略将晶圆缺陷知识嵌入到预训练模型中，有效地平衡了与缺陷知识和原始知识相关的问答查询，并减轻了模态偏差问题。对内部晶圆厂数据（SEM-WaD）的实验表明，我们的FabGPT在晶圆缺陷检测和知识查询方面实现了显著的性能提升。||
|**2024-07-16**|[Qwen2 Technical Report](http://arxiv.org/abs/2407.10671)|**[link](https://github.com/qwenlm/qwen2)**|This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning.   The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach.   To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.||
|**2024-07-12**|[Open Vocabulary Multi-Label Video Classification](http://arxiv.org/abs/2407.09073)|null|预训练的视觉语言模型（VLM）在开放词汇计算机视觉任务（例如图像分类、目标检测和图像分割）方面取得了重大进展。最近的一些工作集中于将VLM扩展到视频中的开放词汇单标签动作分类。然而，先前的方法在整体视频理解方面存在不足，整体视频理解需要在开放词汇环境中同时识别多个动作和实体（例如视频中的对象）的能力。我们将此问题表述为开放词汇多标签视频分类，并提出了一种方法来调整预训练的VLM（例如CLIP）以解决此任务。我们利用大型语言模型（LLM）为VLM提供关于类别标签的语义指导，通过两个关键贡献来提高其开放词汇性能。首先，我们提出了一种端到端可训练架构，该架构学习提示LLM为CLIP文本编码器生成软属性，使其能够识别新类别。其次，我们将时间建模模块集成到CLIP的视觉编码器中，以有效地对视频概念的时空动态进行建模，并提出了一种新颖的正则化微调技术，以确保在视频域中强大的开放词汇分类性能。我们广泛的实验结果证明了我们的方法在多个基准数据集上的有效性。||
|**2024-07-12**|[LAPT: Label-driven Automated Prompt Tuning for OOD Detection with Vision-Language Models](http://arxiv.org/abs/2407.08966)|**[link](https://github.com/ybzh/lapt)**|分布外 (OOD) 检测对于模型可靠性至关重要，因为它可以识别来自未知类的样本并减少由于意外输入造成的错误。视觉语言模型 (VLM)（例如 CLIP）通过集成多模态信息，正在成为用于 OOD 检测的强大工具。然而，此类系统的实际应用受到手动提示工程的挑战，这需要领域专业知识并且对语言细微差别很敏感。在本文中，我们介绍了标签驱动的自动提示调整 (LAPT)，这是一种减少手动提示工程需求的新型 OOD 检测方法。我们使用自动挖掘的分布内 (ID) 类名和负标签来开发感知分布的提示。通过图像合成和检索方法自动收集链接到这些类标签的训练样本，从而无需手动即可进行提示学习。我们利用简单的交叉熵损失进行提示优化，并采用跨模态和跨分布混合策略分别减少图像噪声和探索分布之间的中间空间。LAPT 框架自主运行，只需要 ID 类名作为输入，无需人工干预。通过大量实验，LAPT 始终优于手动制作的提示，为 OOD 检测树立了新标准。此外，LAPT 不仅增强了 ID 和 OOD 样本之间的区别，还提高了 ID 分类精度并增强了对协变量偏移的泛化鲁棒性，从而在具有挑战性的全谱 OOD 检测任务中获得了出色的性能。代码可在 \url{https://github.com/YBZh/LAPT} 获取。||
|**2024-07-11**|[CXR-Agent: Vision-language models for chest X-ray interpretation with uncertainty aware radiology reporting](http://arxiv.org/abs/2407.08811)|null|近年来，大型视觉语言模型在解释复杂图像和使用高级推理生成自然语言描述方面展现出潜力。医学本质上是多模态的，它结合了扫描图像和基于文本的病史来撰写报告，这使得它有利于从人工智能能力的飞跃中受益。我们评估了几个数据集和基准上公开可用的、最先进的、基础视觉语言模型在胸部 X 光片解释方面的性能。我们使用线性探针来评估各种组件的性能，包括 CheXagent 的视觉转换器和 Q-former，它们在许多不同的数据集上的表现优于行业标准 Torch X-ray Vision 模型，显示出强大的泛化能力。重要的是，我们发现视觉语言模型经常会自信地产生幻觉语言，这会减慢临床解释的速度。基于这些发现，我们开发了一种基于代理的视觉语言方法，用于报告生成，使用 CheXagent 的线性探针和 BioViL-T 的短语 grounding 工具，根据病理的可能性生成具有不确定性感知的放射学报告，并对其进行定位和描述。我们通过开发一个评估平台，与呼吸系统专家进行用户研究，使用 NLP 指标、胸部 X 光片基准和临床评估，彻底评估了我们的视觉语言代理。我们的结果表明，人工智能生成的报告在准确性、可解释性和安全性方面都有相当大的改进。我们强调分别分析正常和异常扫描结果的重要性。最后，我们强调需要更大的配对（扫描和报告）数据集以及数据增强，以解决在这些大型视觉语言模型中出现的过拟合问题。||
|**2024-07-11**|[HiRes-LLaVA: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models](http://arxiv.org/abs/2407.08706)|null|高分辨率输入使大型视觉语言模型 (LVLM) 能够识别更精细的视觉细节，从而增强其理解能力。为了降低高分辨率输入带来的训练和计算成本，一种有前景的方向是使用滑动窗口将输入切片成均匀的块，每个块都与经过良好训练的视觉编码器的输入大小相匹配。虽然这种切片策略效率很高，但它会导致原始输入的碎片化，即跨块丢失上下文信息和空间几何的连续性，从而对跨块上下文感知和位置特定任务的性能产生负面影响。为了克服这些缺点，我们引入了 HiRes-LLaVA，这是一种新颖的框架，旨在有效处理任何大小的高分辨率输入，而不会改变原始的上下文和几何信息。HiRes-LLaVA 包含两个创新组件：(i) SliceRestore 适配器，它将切片的块重建为其原始形式，通过下上采样和卷积层有效地提取全局和局部特征，以及 (ii) 自挖掘采样器，用于根据自身压缩视觉标记，在保留原始上下文和位置信息的同时减少训练开销。为了评估处理上下文碎片的能力，我们构建了一个新的基准测试 EntityGrid-QA，其中包含与边缘相关和与位置相关的任务。我们的综合实验表明 HiRes-LLaVA 在现有公共基准测试和 EntityGrid-QA 上的优越性，尤其是在面向文档的任务上，为处理高分辨率输入树立了新标准。||
|**2024-07-11**|[Robotic Control via Embodied Chain-of-Thought Reasoning](http://arxiv.org/abs/2407.08693)|null|学习机器人控制策略的一个关键限制是它们无法在训练数据之外进行泛化。最近关于视觉语言动作模型 (VLA) 的研究表明，使用大型互联网预训练视觉语言模型作为学习机器人策略的支柱可以显著提高其鲁棒性和泛化能力。然而，大型视觉语言模型在其他领域最令人兴奋的能力之一是它们能够通过复杂问题进行迭代推理。这种能力能否应用于机器人领域，使策略能够在采取行动之前通过推理给定任务来提高性能？由于可用的训练示例相对简单，因此简单地使用“思维链”(CoT) 风格的提示对于标准 VLA 的效果要差得多。此外，纯粹对子任务进行语义推理（这在常规 CoT 中很常见）对于需要根据感官观察和机器人状态进行推理的机器人策略来说是不够的。为此，我们为 VLA 引入了具身思维链推理 (ECoT)，其中我们训练 VLA 在预测机器人动作之前对计划、子任务、动作以及视觉基础特征（如对象边界框和末端执行器位置）执行多个推理步骤。我们设计了一个可扩展的管道，用于在大型机器人数据集上为 ECoT 生成合成训练数据。我们证明，ECoT 将 OpenVLA（当前最强大的开源 VLA 策略）在具有挑战性的泛化任务中的绝对成功率提高了 28%，而无需任何额外的机器人训练数据。此外，ECoT 使人类更容易解释策略的失败并使用自然语言纠正其行为。||
|**2024-07-11**|[Bootstrapping Vision-language Models for Self-supervised Remote Physiological Measurement](http://arxiv.org/abs/2407.08507)|null|基于面部视频的远程生理测量是一个很有前景的研究领域，它以非接触方式检测人体生命体征（例如心率、呼吸频率）。传统方法大多是有监督学习，需要收集大量的面部视频和同步记录的光电容积脉搏波 (PPG) 信号。为了解决这个问题，自监督学习最近受到了关注；然而，由于缺乏真实PPG信号，其性能有限。在本文中，我们提出了一种新颖的自监督框架，成功地将流行的视觉语言模型（VLM）集成到远程生理测量任务中。给定一段面部视频，我们首先使用不同的rPPG信号频率增强其正负视频样本。接下来，我们引入了一种面向频率的视觉文本对生成方法，通过从正负样本中仔细创建对比时空图，并设计适当的文本提示来描述它们信号频率的相对比率。我们采用预训练的VLM来提取这些形成的视觉文本对的特征，然后估计rPPG信号。我们开发了一系列生成性和对比性学习机制来优化VLM，包括文本引导的视觉地图重建任务、视觉文本对比学习任务以及频率对比和排序任务。总的来说，我们的方法首次将VLM应用于视觉和文本模态中频率相关知识的提取和对齐。在四个基准数据集上的大量实验表明，它明显优于现有的自监督方法。||
|**2024-07-11**|[Specialist vision-language models for clinical ophthalmology](http://arxiv.org/abs/2407.08410)|**[link](https://github.com/robbieholland/specialistvlms)**|临床医生需要花费大量时间查看医学影像，并以文本形式记录他们对患者诊断、转诊和治疗的发现。视觉语言模型 (VLM) 可以自动解读图像并将其发现总结为文本，在减轻临床工作量和增加患者获得高质量医疗服务的机会方面具有巨大潜力。虽然基础模型引起了医学界的极大兴趣，但尚不清楚它们的一般能力是否能转化为现实世界的临床效用。在这项工作中，我们发现，与执业眼科医生相比，基础 VLM 在对老年性黄斑变性 (AMD) 患者的护理至关重要的专家任务中表现明显不佳。为了解决这个问题，我们首先确定了基于图像的临床决策所需的基本能力，然后制定了一个课程，以有选择地训练 VLM 掌握这些技能。由此产生的模型 RetinaVLM 可以被指示编写报告，该报告在疾病分期（F1 分数为 0.63 对 0.11）和患者转诊（0.67 对 0.39）方面明显优于领先的基础医学 VLM 编写的报告，并且接近初级眼科医生的诊断性能（他们在各自任务上的得分分别为 0.77 和 0.78）。此外，在一项由两位具有长达 32 年经验的资深眼科医生参与的读者研究中，RetinaVLM 的报告被认为与具有长达 10 年经验的初级眼科医生编写的报告同样正确（78.6% 对 82.1%）和完整（均为 78.6%）。这些结果表明，我们基于课程的方法为将通用的基础医学 VLM 专业化以处理现实世界的临床任务提供了一个蓝图。||
|**2024-07-11**|[Enhancing Robustness of Vision-Language Models through Orthogonality Learning and Cross-Regularization](http://arxiv.org/abs/2407.08374)|null|像 CLIP 这类视觉语言模型 (VLM)  如何进行有效的微调以适应特定的下游任务正受到越来越多的关注。先前的工作主要集中在使用 prompt learning 来使 CLIP 适应各种下游任务，然而，当在小数据集上进行微调时，这种方法容易出现过拟合问题。在本文中，我们介绍了一种正交微调方法，可以有效地更新预训练权重，增强鲁棒性和泛化能力，同时进一步利用交叉正则化策略来保持 VLM 零样本泛化能力的稳定性，我们称之为 \textbf{\textit{OrthCR}}。具体来说，我们在 Transformer 架构中无缝地注入了可训练的正交矩阵，并使用 Cayley 参数化来强制执行正交约束，受益于范数保持特性，从而实现稳定和更快的收敛。为了减轻训练过程中与正交约束的偏差，我们进一步采用了一种交叉正则化策略，以旁路的方式利用初始预训练权重。此外，为了丰富下游任务的样本多样性，我们首先探索了 Cutout 数据增强技术，以促进高效的微调，并从正交学习的角度理解我们的方法如何提高特定下游任务的性能并保持泛化能力。除了现有的 prompt learning 技术之外，我们还进行了广泛的实验，以证明我们的方法能够明确地引导预训练权重空间来表示特定于任务的知识，并在\textit{base-to-base/base-to-new}、\textit{跨数据集迁移}和\textit{领域泛化}评估中表现出具有竞争力的泛化能力。||
|**2024-07-11**|[Explore the Potential of CLIP for Training-Free Open Vocabulary Semantic Segmentation](http://arxiv.org/abs/2407.08268)|**[link](https://github.com/leaves162/cliptrase)**|作为一种视觉语言模型，CLIP凭借其零样本能力极大地推进了开放词汇语义分割（OVSS）的发展。尽管取得了成功，但由于其最初的图像级对齐训练，其在OVSS中的应用面临着挑战，这影响了其在需要详细局部上下文的任务中的性能。我们的研究深入探讨了CLIP的[CLS]标记对图像块特征相关性的影响，揭示了“全局”图像块的主导地位阻碍了局部特征的区分。为了克服这个问题，我们提出了CLIPtrase，这是一种新颖的无需训练的语义分割策略，通过重新校准图像块之间的自相关性来增强局部特征感知。这种方法在分割精度和保持对象之间语义连贯性方面表现出显著的改进。实验表明，我们在9个分割基准测试中平均领先CLIP 22.3%，优于现有的最先进的无需训练方法。代码已在以下地址公开：https://github.com/leaves162/CLIPtrase。||
|**2024-07-11**|[AddressCLIP: Empowering Vision-Language Models for City-wide Image Address Localization](http://arxiv.org/abs/2407.08156)|**[link](https://github.com/xsx1001/addressclip)**|本研究介绍了社交媒体和摄影新闻带来的一个新问题，称为图像地址定位（IAL），旨在预测拍摄图像的可读文本地址。现有的两阶段方法涉及预测地理坐标并将其转换为人类可读的地址，这可能会导致歧义并且资源密集。相比之下，我们提出了一个名为 AddressCLIP 的端到端框架来解决语义更丰富的问题，它包含两个关键要素：i）图像-文本对齐，通过对比学习将图像与地址和场景描述对齐，以及 ii）图像-地理匹配，根据流形学习在空间距离方面约束图像特征。此外，我们还构建了三个来自匹兹堡和旧金山的不同规模的数据集，专门用于 IAL 问题。实验表明，我们的方法在所提出的数据集上取得了令人信服的性能，并且优于视觉语言模型的代表性迁移学习方法。此外，广泛的消融和可视化展示了所提出方法的有效性。数据集和源代码可在 https://github.com/xsx1001/AddressCLIP 获取。||
|**2024-07-10**|[RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization](http://arxiv.org/abs/2407.08044)|**[link](https://github.com/huangowen/rolora)**|低秩适应 (LoRA) 作为一种典型的参数高效微调 (PEFT) 方法，通过仅更新大型语言模型 (LLM) 中的一小部分权重，显著提高了训练效率。近年来，仅权重量化技术也被应用于 LoRA 方法，以减少微调的内存占用。然而，将权重-激活量化应用于 LoRA 流程的研究还不够充分，我们观察到性能大幅下降，这主要是由于激活异常值的存在。在这项工作中，我们提出了 RoLoRA，这是第一个基于 LoRA 的有效权重-激活量化方案。RoLoRA 利用旋转来消除异常值，并提出了旋转感知微调，以在旋转后的 LLM 中保留无异常值的特性。实验结果表明，在权重-激活设置中，RoLoRA 持续提高了低比特 LoRA 的收敛性和训练后量化的鲁棒性。我们在 LLaMA2-7B/13B、LLaMA3-8B 模型上评估了 RoLoRA，与 LoRA 基线相比，在常识推理任务上，4 位权重-激活量化的 LLaMA2-13B 的绝对精度提高了 29.5%。我们进一步证明了其在大型多模态模型 (LLaVA-1.5-7B) 上的有效性。代码可在 https://github.com/HuangOwen/RoLoRA 获取。||
|**2024-07-10**|[LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models](http://arxiv.org/abs/2407.07895)|**[link](https://github.com/LLaVA-VL/LLaVA-NeXT)**|视觉指令微调在增强大型多模态模型 (LMM) 的能力方面取得了重大进展。然而，现有的开放 LMM 主要集中在单图像任务上，它们在多图像场景中的应用仍有待探索。此外，先前的 LMM 研究分别解决了不同的场景，使得无法通过新兴功能泛化跨场景。为此，我们引入了 LLaVA-NeXT-Interleave，它在 LMM 中同时处理多图像、多帧（视频）、多视图（3D）和多patch（单图像）场景。为了实现这些功能，我们将交错数据格式视为通用模板，并使用 1,177.6k 个样本编译了 M4-Instruct 数据集，涵盖 4 个主要领域，包括 14 个任务和 41 个数据集。我们还策划了 LLaVA-Interleave Bench 以全面评估 LMM 的多图像性能。通过大量实验，LLaVA-NeXT-Interleave 在多图像、视频和 3D 基准测试中取得了领先的结果，同时保持了单图像任务的性能。此外，我们的模型还展现出一些新兴功能，例如跨不同设置和模态迁移任务。代码可在 https://github.com/LLaVA-VL/LLaVA-NeXT 获取。||
|**2024-07-10**|[Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs](http://arxiv.org/abs/2407.07775)|null|导航研究的一个难以实现的目标是构建一个能够理解包括自然语言和图像在内的多模态指令并执行有用导航的智能体。为了实现这一目标，我们研究了一类广泛适用的导航任务，我们称之为带有演示路径的多模态指令导航（MINT），其中环境先验是通过先前记录的演示视频提供的。视觉语言模型（VLM）的最新进展为实现这一目标指明了一条充满希望的道路，因为它展示了感知和推理多模态输入的能力。然而，VLM 通常被训练用于预测文本输出，如何最好地将其用于导航是一个开放的研究问题。为了解决 MINT 问题，我们提出了 Mobility VLA，这是一种分层的视觉-语言-动作（VLA）导航策略，它结合了长上下文 VLM 的环境理解和常识推理能力，以及基于拓扑图的鲁棒的低级导航策略。高级策略由一个长上下文 VLM 组成，它将演示路径视频和多模态用户指令作为输入，以在路径视频中找到目标帧。接下来，低级策略使用目标帧和离线构建的拓扑图在每个时间步长生成机器人动作。我们在一个 836 平方米的真实世界环境中评估了 Mobility VLA，结果表明，Mobility VLA 在以前未解决的多模态指令（例如“我应该把这个还到哪里？”）上具有很高的端到端成功率，同时手持一个塑料箱。||
|**2024-07-09**|[Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model](http://arxiv.org/abs/2407.07053)|**[link](https://github.com/zwq2018/multi-modal-self-instruct)**|虽然目前大多数大型多模态模型 (LMM) 已经可以理解自然场景和肖像照片，但它们对抽象图像（例如图表、地图或布局）的理解和视觉推理能力仍然相当初级。它们经常难以完成简单的日常任务，例如从时钟读取时间、理解流程图或使用路线图规划路线。鉴于此，我们设计了一种多模态自指示方法，利用大型语言模型及其代码能力，在日常场景中合成大量抽象图像和视觉推理指令。我们的策略轻松创建了一个包含 11,193 条指令的多模态基准测试，涵盖八个视觉场景：图表、表格、模拟地图、仪表盘、流程图、关系图、平面图和视觉谜题。这个由简单的线条和几何元素构建的基准测试暴露了大多数先进的 LMM（如 Claude-3.5-Sonnet 和 GPT-4o）在抽象图像理解、空间关系推理和视觉元素归纳方面的不足。此外，为了验证我们合成数据的质量，我们使用 62,476 条合成的图表、表格和路线图指令微调了一个 LMM。结果表明，图表理解和地图导航性能有所提高，也显示出对其他视觉推理任务的潜在好处。我们的代码可在以下网址获得：https://github.com/zwq2018/Multi-modal-Self-instruct。||
|**2024-07-09**|[Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action Localization](http://arxiv.org/abs/2407.07024)|**[link](https://github.com/hyunjs/stov-tal)**|时序动作定位 (TAL) 中的词汇量受到大规模标注数据集稀缺性的限制。为了解决这个问题，最近的研究结合了强大的预训练视觉语言模型 (VLM)，例如 CLIP，来执行开放词汇表 TAL (OV-TAL)。然而，与在大量图像/视频-文本对上训练的 VLM 不同，现有的 OV-TAL 方法仍然依赖于小型、完全标记的 TAL 数据集来训练动作定位器。在本文中，我们探索了使用未标记的 YouTube 视频进行 OV-TAL 自训练的可扩展性。我们的自训练方法包括两个阶段。首先，在人工标记的 TAL 数据集上训练一个类别无关的动作定位器，并使用它为未标记的视频生成伪标签。其次，将大规模伪标签数据集与人工标记的数据集相结合，以训练定位器。大量实验表明，在自训练中利用网络规模的视频可以显着增强动作定位器的泛化能力。此外，我们重点介绍了现有 OV-TAL 评估方案中存在的问题，并提出了一种新的评估方案。代码发布在 https://github.com/HYUNJS/STOV-TAL||
|**2024-07-09**|[CoLA: Conditional Dropout and Language-driven Robust Dual-modal Salient Object Detection](http://arxiv.org/abs/2407.06780)|**[link](https://github.com/ssecv/CoLA)**|深度/热信息有利于使用传统RGB图像检测显著性目标。然而，在双模态显著性目标检测（SOD）模型中，针对噪声输入和模态缺失的鲁棒性至关重要，但很少被研究。为了解决这个问题，我们引入了条件Dropout和语言驱动（CoLA）框架，该框架包含两个核心组件。1）语言驱动质量评估（LQA）：利用带有提示学习器的预训练视觉语言模型，LQA在不需要额外质量标注的情况下重新校准图像贡献。这种方法有效地减轻了噪声输入的影响。2）条件Dropout（CD）：一种学习方法，用于增强模型在模态缺失场景下的适应性，同时保持其在完整模态下的性能。CD作为一种插件式训练方案，将模态缺失视为条件，增强了各种双模态SOD模型的整体鲁棒性。大量实验表明，所提出的方法在模态完整和模态缺失条件下均优于最先进的双模态SOD模型。代码将在论文被接收后开源。||
|**2024-07-09**|[LVLM-empowered Multi-modal Representation Learning for Visual Place Recognition](http://arxiv.org/abs/2407.06730)|null|视觉位置识别 (VPR) 由于视角变化和外观变化很大，因此仍然具有挑战性。主流工作通过开发各种特征聚合方法将深度特征转换为稳健而紧凑的全局表示来应对这些挑战。不幸的是，在具有挑战性的条件下无法获得令人满意的结果。我们从一个新的角度出发，尝试通过融合图像数据和视觉场景的文本描述来构建具有判别性的全局表示。动机有两个：（1）当前的大型视觉语言模型 (LVLM) 在视觉指令跟随方面表现出非凡的涌现能力，因此提供了一种高效灵活的图像文本描述生成方式；（2）文本描述提供了对场景的高级理解，对环境变化表现出很强的鲁棒性。尽管很有前景，但利用 LVLM 构建多模态 VPR 解决方案在高效的多模态融合方面仍然具有挑战性。此外，LVLM 不可避免地会产生一些不准确的描述，这使得情况变得更加困难。为了应对这些挑战，我们提出了一种新颖的多模态 VPR 解决方案。它首先使预训练的视觉和语言基础模型适应 VPR，以提取图像和文本特征，然后将这些特征输入特征组合器以相互增强。作为主要组件，特征组合器首先提出了一个逐符号注意力块，以根据文本符号与图像数据的相关性自适应地重新校准文本符号，然后开发了一个高效的交叉注意力融合模块，以在不同模态之间传播信息。增强的多模态特征被压缩到特征描述符中以执行检索。实验结果表明，我们的方法在图像描述符维度明显较小的情况下，大大优于最先进的方法。||
|**2024-07-08**|[A Single Transformer for Scalable Vision-Language Modeling](http://arxiv.org/abs/2407.06438)|**[link](https://github.com/yangyi-chen/solo)**|我们提出了 SOLO，一个用于可扩展视觉语言建模的单一 Transformer 模型。目前的大型视觉语言模型 (LVLM)，例如 LLaVA，大多采用异构架构，将预训练的视觉编码器与大型语言模型 (LLM) 连接起来，以促进视觉识别和复杂推理。虽然通过相对轻量级的训练获得了显著的性能，但我们发现了四个主要的扩展性限制：(1) 视觉能力受到预训练视觉编码器的限制，这些编码器通常比 LLM 小一个数量级。(2) 异构架构使已建立的硬件和软件基础设施的使用变得复杂。(3) 对这种架构进行规模法则研究必须考虑三个独立的组件——视觉编码器、连接器和 LLM，这使得分析变得复杂。(4) 使用现有的视觉编码器通常需要遵循预定义的图像输入预处理规范，例如，通过将输入整形为固定分辨率的方形图像，这在处理和训练高分辨率图像或具有不寻常纵横比的图像时会遇到困难。像 SOLO 这样的统一单一 Transformer 架构有效地解决了 LVLMs 中的这些可扩展性问题；然而，它在现代环境中的有限采用可能是由于缺乏可靠的训练方法来平衡两种模态并确保数十亿级模型的稳定训练。在本文中，我们介绍了第一个用于开发 SOLO 的开源训练方法，SOLO 是一个使用中等学术资源的开源 7B LVLM。训练方法包括从 LLM 初始化、在 ImageNet 和网络规模数据上进行顺序预训练，以及在我们策划的高质量数据集上进行指令微调。在广泛的评估中，SOLO 表现出与 LLaVA-v1.5-7B 相当的性能，尤其是在视觉数学推理方面表现出色。||
|**2024-07-08**|[Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision](http://arxiv.org/abs/2407.06189)|**[link](https://github.com/orrzohar/Video-STaR)**|大型视觉语言模型 (LVLM) 的性能取决于其训练数据集的规模和质量。现有的视频指令调整数据集缺乏多样性，因为它们是通过提示大型语言模型使用视频字幕生成问答对而得出的，因此大多是描述性的。同时，存在许多具有不同标签和监督的标记视频数据集——然而，我们发现将它们集成到 LVLM 中并非易事。在此，我们提出了使用增强推理的视频自训练 (Video-STaR)，这是第一个视频自训练方法。Video-STaR 允许利用任何标记的视频数据集进行视频指令调整。在 Video-STaR 中，LVLM 在指令生成和微调之间循环，我们证明这 (I) 提高了一般视频理解能力，并且 (II) 使 LVLM 能够适应现有监督下的新型下游任务。在生成过程中，LVLM 被提示提出答案。然后过滤答案，只保留包含原始视频标签的答案，然后在生成的数据集上重新训练 LVLM。通过仅对包含正确视频标签的生成答案进行训练，Video-STaR 利用这些现有的视频标签作为视频指令调整的弱监督。我们的结果表明，经过 Video-STaR 增强后的 LVLM 在 (I) 常规视频问答（TempCompass 性能提高了 10%）和 (II) 下游任务（Video-STaR 将 Kinetics700-QA 的准确率提高了 20%，并将 FineDiving 上的动作质量评估提高了 15%）中均表现出更好的性能。||
|**2024-07-09**|[HyCIR: Boosting Zero-Shot Composed Image Retrieval with Synthetic Labels](http://arxiv.org/abs/2407.05795)|null|组合图像检索 (CIR) 旨在根据带有文本的查询图像检索图像。当前的零样本 CIR (ZS-CIR) 方法试图在不使用昂贵的三元组标记训练数据集的情况下解决 CIR 任务。然而，ZS-CIR 和三元组监督 CIR 之间的差距仍然很大。在这项工作中，我们提出了混合 CIR (HyCIR)，它使用合成标签来提高 ZS-CIR 的性能。提出了一种新的 CIR 标签合成流程 (SynCir)，其中只需要未标记的图像。首先，根据视觉相似度提取图像对。其次，基于视觉语言模型和 LLM 为每个图像对生成查询文本。第三，基于语义相似度在语言空间中进一步过滤数据。为了提高 ZS-CIR 的性能，我们提出了一种混合训练策略，可以同时使用 ZS-CIR 监督和合成 CIR 三元组。采用了两种对比学习方法。一种是使用大规模未标记图像数据集来学习具有良好泛化能力的图像到文本映射。另一种是使用合成的 CIR 三元组来学习 CIR 任务的更好映射。我们的方法在常见的 CIR 基准测试：CIRR 和 CIRCO 上实现了最先进的零样本性能。||
|**2024-07-07**|[Multimodal Language Models for Domain-Specific Procedural Video Summarization](http://arxiv.org/abs/2407.05419)|null|视频是一种强大的媒介，可以通过长格式教程传达思想、讲述故事和提供详细的说明。此类教程对于按照自己的节奏学习新技能非常有价值，但由于其长度和密集的内容，可能会让人不知所措。观众经常会寻找特定信息，例如精确的测量值或分步执行细节，因此必须有效地提取和总结关键片段。人们非常需要一个能够总结和检测长视频中的亮点的智能、时间敏感的视频助手。多模态大型语言模型的最新进展为开发此类助手提供了有希望的解决方案。我们的研究探索了使用多模态模型来增强特定领域内的视频摘要和分步指令生成。这些模型需要理解跨视频帧的动作之间的时间事件和关系。我们的方法侧重于微调 TimeChat，以提高其在特定领域（烹饪和医疗程序）中的性能。通过在特定领域的数据集（如烹饪领域的 Tasty 和医疗程序领域的 MedVidQA）上训练模型，我们旨在增强其生成简洁、准确的教学视频摘要的能力。我们整理并重构了这些数据集，以创建高质量的以视频为中心的指令数据。我们的研究结果表明，当在特定领域的程序数据上进行微调时，TimeChat 可以显着改善长格式视频中关键指令步骤的提取和总结。这项研究证明了专门的多模式模型通过提供针对每个领域的独特方面量身定制的个性化分步指导来协助完成实际任务的潜力。||
|**2024-07-07**|[Multimodal Prompt Learning with Missing Modalities for Sentiment Analysis and Emotion Recognition](http://arxiv.org/abs/2407.05374)|**[link](https://github.com/zrguo/MPLMM)**|多模态模型的发展显著推进了多模态情感分析和情绪识别。然而，在现实应用中，各种缺失模态情况的存在常常导致模型性能下降。本文提出了一种新颖的使用提示学习的多模态Transformer框架来解决模态缺失问题。我们的方法引入了三种类型的提示：生成提示、缺失信号提示和缺失类型提示。这些提示能够生成缺失的模态特征，并促进模态内和模态间信息的学习。通过提示学习，我们实现了可训练参数数量的大幅减少。我们提出的方法在所有评估指标上都明显优于其他方法。大量的实验和消融研究证明了我们方法的有效性和鲁棒性，展示了其有效处理缺失模态的能力。||
|**2024-07-07**|[WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks](http://arxiv.org/abs/2407.05291)|**[link](https://github.com/servicenow/workarena)**|大型语言模型 (LLM) 模仿人类智能的能力导致了基于 LLM 的自主代理的激增。尽管最近的 LLM 似乎能够根据用户指令进行计划和推理，但它们将这些能力应用于自主任务解决的有效性仍未得到充分探索。在企业环境中尤其如此，因为自动化代理有望产生重大影响。为了填补这一空白，我们提出了 WorkArena++，这是一个包含 682 个任务的新基准，这些任务对应于知识工作者日常执行的现实工作流程。WorkArena++ 旨在评估 Web 代理的计划、解决问题、逻辑/算术推理、检索和上下文理解能力。我们对最先进的 LLM 和视觉语言模型 (VLM) 以及人类工作者的实证研究表明，此类模型要成为工作场所中有用的助手面临着若干挑战。除了基准之外，我们还提供了一种机制，可以毫不费力地生成数千个真实观察/行动轨迹，这些轨迹可用于微调现有模型。总的来说，我们希望这项工作能够成为帮助社区朝着有能力的自主代理方向发展的一种有用资源。该基准可以在 https://github.com/ServiceNow/WorkArena/tree/workarena-plus-plus 找到。||
|**2024-07-05**|[Multimodal Classification via Modal-Aware Interactive Enhancement](http://arxiv.org/abs/2407.04587)|null|由于存在臭名昭著的模态不平衡问题，多模态学习（MML）会导致优化不平衡现象，从而难以达到令人满意的性能。最近，一些具有代表性的方法被提出用于提高性能，主要集中在自适应调整每个模态的优化，以重新平衡主导模态和非主导模态的学习速度。为了更好地促进多模态学习中模型信息的交互，在本文中，我们提出了一种新的多模态学习方法，称为模态感知交互增强（MIE）。具体来说，我们首先利用基于锐度感知最小化（SAM）的优化策略在前向阶段平滑学习目标。然后，借助SAM的几何特性，我们提出了一种梯度修正策略，在反向阶段施加不同模态之间的影响。因此，我们可以提高泛化能力，同时缓解多模态学习中的模态遗忘现象。在广泛使用的数据集上进行的大量实验表明，我们提出的方法可以优于各种最先进的基线，以实现最佳性能。||
|**2024-07-04**|[MiniGPT-Med: Large Language Model as a General Interface for Radiology Diagnosis](http://arxiv.org/abs/2407.04106)|**[link](https://github.com/vision-cair/minigpt-med)**|近年来，人工智能 (AI) 的快速发展为医疗保健领域带来了重大的突破，特别是在诊断程序的改进方面。然而，以往的研究往往局限于有限的功能。本研究介绍了 MiniGPT-Med，这是一个源于大规模语言模型并专为医疗应用而设计的视觉语言模型。MiniGPT-Med 在各种成像模式（包括 X 光、CT 扫描和 MRI）中均表现出非凡的多功能性，从而增强了其实用性。该模型能够执行医学报告生成、视觉问答 (VQA) 以及医学图像疾病识别等任务。它对图像和文本临床数据的集成处理显著提高了诊断准确性。我们的实证评估证实，MiniGPT-Med 在疾病定位、医学报告生成和 VQA 基准测试中均表现出色，这标志着在缩小放射学实践辅助差距方面迈出了重要一步。此外，它在医学报告生成方面达到了最先进的性能，比之前的最佳模型提高了19%的准确率。MiniGPT-Med 有望成为放射学诊断的通用接口，从而提高各种医学影像应用的诊断效率。||
|**2024-07-04**|[Fully Fine-tuned CLIP Models are Efficient Few-Shot Learners](http://arxiv.org/abs/2407.04003)|null|提示调优通过训练一小部分参数，可以有效地增强预训练视觉语言模型 (VLM) 在下游任务上的性能。然而，当将调优后的模型应用于不同的数据集或领域时，它们往往会牺牲灵活性和适应性。在本文中，我们探索了通过精细微调整个 VLM 来捕获特定任务信息的可能性，同时最大限度地减少参数调整。在有限的监督下对特定任务进行整个 VLM 微调时，过拟合和灾难性遗忘成为事实上的因素。为了缓解这些问题，我们提出了一个名为 CLIP-CITE 的框架，通过设计一个判别性的视觉-文本任务，进一步以监督的方式对齐视觉-文本语义，并集成知识蒸馏技术来保留获得的知识。在少样本学习、基础到新泛化、域泛化和跨域泛化设置下的广泛实验结果表明，我们的方法在有限监督下有效地提高了特定任务的性能，同时保留了 VLM 在其他数据集上的通用性。||
|**2024-07-04**|[Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks](http://arxiv.org/abs/2407.03967)|**[link](https://github.com/amitkparekh/cogelot)**|仅根据多模态模型在分布外数据上的性能来评估其泛化能力，无法捕捉其真正的鲁棒性。本研究引入了一个全面的评估框架，系统地检验了指令和输入在这些模型泛化能力中的作用，并考虑了架构设计、跨语言和视觉模态的输入扰动以及任务复杂性的增加。所提出的框架揭示了多模态模型对极端指令扰动的弹性和它们对观察变化的脆弱性，引发了对过度拟合虚假相关性的担忧。通过在当前基于 Transformer 的机器人操作任务多模态模型上应用此评估框架，我们发现了局限性，并建议未来的改进应侧重于架构和训练创新，以更好地整合多模态输入，通过优先考虑对输入内容的敏感性而不是偶然的相关性来增强模型的泛化能力。||
|**2024-07-04**|[Concept Bottleneck Models Without Predefined Concepts](http://arxiv.org/abs/2407.03921)|null|近年来，可解释的概念型模型，如概念瓶颈模型 (CBM)，引起了人们的广泛兴趣。这类模型首先预测人类可解释的概念，然后将这些概念映射到输出类别。为了减少对人工标注概念的依赖，最近的研究工作已将预训练的黑盒模型后验地转换为可解释的 CBM。然而，这些方法预先定义了一组概念，假设黑盒模型在其表示中编码了哪些概念。在这项工作中，我们通过利用无监督概念发现来自动提取概念，从而消除了这一假设，无需人工标注或预定义的概念集。我们进一步引入了一种依赖于输入的概念选择机制，以确保在所有类别中仅使用一小部分概念。我们证明，我们的方法提高了下游性能，并缩小了与黑盒模型的性能差距，同时在分类中使用的概念要少得多。最后，我们演示了大型视觉语言模型如何干预最终的模型权重以纠正模型错误。||
|**2024-07-04**|[M $\mathbf5$ -- A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks](http://arxiv.org/abs/2407.03791)|null|自ChatGPT发布以来，自然语言处理领域经历了快速发展，特别是在大型语言模型（LLM）及其多模态对应物大型多模态模型（LMM）方面。尽管LLM具有令人印象深刻的能力，但正如各种纯文本基准测试所证明的那样，LLM在不同语言和文化背景下 often 表现出显著的性能差异。然而，目前的研究缺乏针对多模态视觉语言环境的此类基准。为了弥补这一差距，本研究引入了M5，这是第一个旨在评估多语言和多文化背景下不同视觉语言任务的LMM的综合基准。M5包括涵盖五个任务和41种语言的八个数据集，重点关注代表性不足的语言和文化多样化的图像。此外，我们还介绍了两个新的数据集，M5-VGR和M5-VLOD，其中包括一项新的视觉语言异常检测任务，在该任务中，所有评估的开源模型都未能显著超过随机基线。通过广泛的评估和分析，我们重点强调了资源丰富语言和资源匮乏语言之间存在巨大的、与任务无关的性能差异。此外，我们还发现，在多语言环境中，更大的模型不一定优于较小的模型。||
|**2024-07-04**|[Meta-optimized Angular Margin Contrastive Framework for Video-Language Representation Learning](http://arxiv.org/abs/2407.03788)|null|数据质量是决定视频-语言表示学习效果的首要因素。然而，以往数据中的视频-文本对通常不能完美对齐，这可能导致视频-语言表示不能准确反映跨模态语义。此外，以往数据还存在概念分布不均匀的问题，从而影响了在不受欢迎主题上的下游性能。为了解决这些问题，我们提出了一个带有减法角度边际的对比目标函数，以规范跨模态表示，使其达到完美的相似性。此外，为了适应非均匀的概念分布，我们提出了一个多层感知器（MLP）参数化的加权函数，将损失值映射到样本权重，从而能够在整个训练过程中动态调整模型的关注点。在少量无偏元数据的指导下，并通过大型视觉-语言模型生成的视频-文本数据进行增强，我们改进了视频-语言表示，并在常用的视频问答和文本-视频检索数据集上取得了优异的性能。||
|**2024-07-04**|[Visualizing Dialogues: Enhancing Image Selection through Dialogue Understanding with Large Language Models](http://arxiv.org/abs/2407.03615)|**[link](https://github.com/MiuLab/VisualDialog)**|近年来，对话系统的进步凸显了整合多模态响应的重要性，这种响应能够通过多种模态来传达信息，而不仅仅依赖于基于文本的交互。这种丰富性不仅提高了整体的交流效率，还增强了对话体验的质量。然而，现有的对话到图像检索方法由于预训练视觉语言模型 (VLM) 在准确理解复杂对话方面的局限性而面临挑战。为了解决这个问题，我们提出了一种新方法，利用大型语言模型 (LLM) 强大的推理能力来生成精确的对话相关视觉描述符，从而促进与图像的无缝连接。在基准数据上进行的大量实验验证了我们提出的方法在提取简洁准确的视觉描述符方面的有效性，从而显著提高了对话到图像检索的性能。此外，我们的研究结果证明了该方法在不同视觉线索、各种 LLM 和不同数据集上的泛化能力，突出了其在实际应用中的实用性和潜在影响。||
|**2024-07-04**|[Lateralization LoRA: Interleaved Instruction Tuning with Modality-Specialized Adaptations](http://arxiv.org/abs/2407.03604)|null|视觉语言模型 (VLM) 的最新进展导致了能够理解和生成交错图像和文本的视觉语言通用模型 (VLG) 的发展。尽管取得了这些进步，但 VLG 在遵循用户指令进行交错文本和图像生成方面仍然存在困难。为了解决这个问题，我们引入了 LeafInstruct，这是第一个开源的交错指令调整数据，包含跨 10 多个领域的 30,000 多个高质量实例。由于现有 VLG 的规模庞大，我们选择进行参数高效的调整。然而，我们观察到使用标准 LoRA 调整的 VLG 通常在交错文本图像生成中表现出较差的性能。我们将此问题归因于模态干扰和缺乏模态专用适应性设计。因此，我们提出了一种受大脑偏侧化概念启发的新型模态专用适应方法——Lateralization LoRA。Lateralization LoRA 采用混合方法，结合了传统的线性 LoRA 和用于生成文本和图像的卷积 LoRA，通过利用模态特定的结构和参数集来生成高质量的文本和图像。我们使用 LeafInstruct 数据集对 VLG（即 EMU2）进行 Lateralization LoRA 指令调整。大量实验表明，使用 Lateralization LoRA 调整的 EMU2 实现了最先进的性能，在复杂的交错任务中明显优于基线模型。||
|**2024-07-03**|[HEMM: Holistic Evaluation of Multimodal Foundation Models](http://arxiv.org/abs/2407.03418)|**[link](https://github.com/pliang279/hemm)**|能够全面处理文本、图像、视频、音频和其他感官模态的多模态基础模型正越来越多地应用于各种现实应用中。然而，考虑到可能存在的各种建模决策、任务和领域，描述和研究多模态基础模型的进展具有挑战性。在本文中，我们介绍了多模态模型的整体评估 (HEMM)，以系统地评估多模态基础模型在一组 3 个维度上的能力：基本技能、信息流和现实用例。基本的多模态技能是解决问题所需的内部能力，例如学习跨模态的交互、细粒度对齐、多步骤推理以及处理外部知识的能力。信息流研究多模态内容在任务期间如何通过查询、翻译、编辑和融合发生变化。用例涵盖了现实世界多媒体、情感计算、自然科学、医疗保健和人机交互应用中引入的特定领域挑战。通过对 HEMM 中 30 个任务的全面实验，我们 (1) 确定了对当今模型构成挑战的关键数据集维度（例如，基本技能、信息流和用例），以及 (2) 提炼了关于不同建模维度（例如，规模、预训练数据、多模态对齐、预训练和指令微调目标）如何影响性能的性能趋势。我们关于具有挑战性的多模态交互、用例以及需要推理和外部知识的任务、数据和模型规模的好处以及指令微调的影响的结论，为多模态基础模型的未来工作提供了可操作的见解。||
|**2024-07-03**|[Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation](http://arxiv.org/abs/2407.03056)|**[link](https://github.com/miccunifi/kdpl)**|视觉语言模型 (VLM) 在未见过的任务上表现出非凡的零样本泛化能力，但在有限数据下泛化到下游任务的性能不如监督方法。提示学习正在成为一种参数高效的 VLM 自适应方法，但最先进的方法需要带注释的样本。在本文中，我们提出了一种基于无监督知识蒸馏的新型提示学习方法，该方法从更强大的模型中提取知识。我们的方法称为知识蒸馏提示学习 (KDPL)，可以集成到现有的提示学习技术中，并消除了适应过程中对标记示例的需求。我们对十多个标准基准数据集进行的实验表明，KDPL 在提高学习提示的泛化能力方面非常有效，可以解决零样本域泛化、零样本跨数据集泛化和零样本基础到新类泛化问题。KDPL 不需要用于适应的基本事实标签，此外，我们还表明，即使在没有任何训练类名知识的情况下，它也可以用于有效地迁移知识。代码可在 https://github.com/miccunifi/KDPL 公开获取。||
|**2024-07-03**|[SAFT: Towards Out-of-Distribution Generalization in Fine-Tuning](http://arxiv.org/abs/2407.03036)|null|在机器学习领域，处理训练数据中的分布变化，即所谓的分布外 (OOD) 泛化，是一项重大挑战。虽然像 CLIP 这样的预训练视觉语言模型已经展现出卓越的零样本性能，但模型对下游任务的进一步适应会导致 OOD 数据出现不良的性能下降。在这项工作中，我们引入了用于微调的稀疏适应 (SAFT) 方法，该方法可以防止微调过程中遗忘预训练模型中的通用知识。SAFT 仅更新梯度幅度较大的一小部分重要参数，同时保持其他参数冻结。SAFT 易于实现且概念简单。大量实验表明，仅使用 0.1% 的模型参数，SAFT 就可以显著提高 CLIP 的性能。在多个基准测试中，它始终优于基线方法。在 ImageNet 及其变体的少样本学习基准测试中，在 OOD 设置下，SAFT 比传统的微调方法平均提高了 5.15% 的性能。||
|**2024-07-03**|[Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective](http://arxiv.org/abs/2407.02814)|null|在大型数据集上预训练的视觉语言模型 (VLM) 可能会通过将性别信息与特定对象或场景相关联而无意中学习到偏见。当前的方法侧重于修改输入并监控模型输出概率分数的变化，但往往难以从模型组件的角度全面理解偏见。我们提出了一个结合因果中介分析的框架，用于测量和映射 VLM 内偏见产生和传播的路径。这种方法使我们能够确定干预措施对模型偏差的直接影响，以及干预措施通过不同模型组件介导的对偏差的间接影响。我们的结果表明，图像特征是偏见的主要来源，其影响远高于文本特征，具体而言，在 MSCOCO 和 PASCAL-SENTENCE 数据集中分别占偏见的 32.57% 和 12.63%。值得注意的是，图像编码器的贡献超过了文本编码器和深度融合编码器。进一步的实验表明，语言和视觉模态的贡献是一致且不冲突的。因此，专注于模糊图像编码器中对模型偏见贡献最大的性别表征，可以有效地将 MSCOCO 和 PASCAL-SENTENCE 数据集中的偏见分别减少 22.03% 和 9.04%，而性能损失最小，计算量也没有增加。||
|**2024-07-03**|[MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context](http://arxiv.org/abs/2407.02730)|**[link](https://github.com/dongzizhu/medvh)**|大型视觉语言模型 (LVLM) 最近在自然图像和文本数据的各种任务中取得了优异的性能，这激发了大量关于 LVLM 微调和训练的研究。尽管取得了这些进步，但很少有研究关注这些模型在更小的数据集上微调时对幻觉的鲁棒性。在这项研究中，我们引入了一个新的基准数据集，即医学视觉幻觉测试 (MedVH)，用于评估特定领域 LVLM 的幻觉。 MedVH 包含五项任务，用于评估医学环境中 LVLM 的幻觉，其中包括全面理解文本和视觉输入以及生成长文本响应的任务。我们对通用 LVLM 和医学 LVLM 进行的广泛实验表明，尽管医学 LVLM 在标准医学任务中表现出良好的性能，但它们特别容易受到幻觉的影响，通常比通用模型更容易受到影响，这引发了人们对这些特定领域模型可靠性的严重担忧。为了使医学 LVLM 在实际应用中真正发挥价值，它们不仅必须准确地整合医学知识，还必须保持强大的推理能力以防止幻觉。我们的工作为未来对这些研究的评估铺平了道路。||
|**2024-07-02**|[Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models](http://arxiv.org/abs/2407.02716)|null|对预训练的视觉语言模型 (VLM) 进行微调已在医学图像和文本描述协同作用方面展现出卓越的能力。然而，许多预训练数据集受到患者隐私问题的限制，可能包含会对下游性能产生负面影响的噪声。此外，对多模态生成的日益依赖加剧了这个问题，因为它容易受到对抗性攻击。为了研究在对抗性噪声数据上训练的 VLM 如何在下游医学任务中执行，我们首先使用多模态对抗性攻击来制作噪声上游数据集。通过我们的综合分析，我们揭示了适度的噪声增强了模型的鲁棒性和可迁移性，但增加噪声水平会对下游任务性能产生负面影响。为了缓解这个问题，我们提出了校正对抗性噪声 (RAN) 框架，这是一种旨在有效防御对抗性攻击并在微调期间纠正上游噪声影响的方法。||
|**2024-07-02**|[D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data and eXpert model predictions](http://arxiv.org/abs/2407.02604)|null|大型视觉语言模型（VLM）已经从研究阶段发展到适用于通用用例的阶段，取得了令人难以置信的进步。LLaVA-Med 是一种开创性的生物医学大型语言和视觉助手，可以执行多模态生物医学图像和数据分析，为放射科医生提供自然语言界面。虽然它具有高度的通用性，并且可以处理多模态数据，但它目前受到大型语言模型领域现有挑战的限制。回复中的幻觉和不精确性可能导致误诊，这在目前阻碍了 VLM 的临床适应性。为了在医疗保健领域创建精确、用户友好的模型，我们提出了 D-Rax，这是一种特定领域、对话式的放射学辅助工具，可用于获取有关特定放射图像的见解。在这项研究中，我们增强了胸部 X 光（CXR）图像的对话分析，以支持放射学报告，提供来自医学成像的全面见解，并帮助制定准确的诊断。D-Rax 的实现是通过在我们策划的增强型指令跟随数据上微调 LLaVA-Med 架构来实现的，这些数据包括图像、指令以及从 MIMIC-CXR 成像数据、CXR 相关视觉问答 (VQA) 对和多个专家 AI 模型的预测结果中得出的疾病诊断和人口统计学预测。我们观察到，在对开放式和封闭式对话进行评估时，响应在统计学上都有显著改善。D-Rax 利用最先进的诊断模型与 VLM 相结合的力量，使临床医生能够使用自然语言与医学图像进行交互，这有可能简化他们的决策过程，提高诊断准确性并节省他们的时间。||
|**2024-07-02**|[Understanding Alignment in Multimodal LLMs: A Comprehensive Study](http://arxiv.org/abs/2407.02477)|null|偏好对齐已成为提升大型语言模型 (LLM) 性能的关键组成部分，但其对多模态大型语言模型 (MLLM) 的影响仍相对缺乏研究。与语言模型类似，用于图像理解任务的 MLLM 也面临着诸如幻觉之类的挑战。在 MLLM 中，幻觉不仅可以通过陈述错误的事实发生，还可以通过产生与图像内容不一致的响应来发生。MLLM 对齐的主要目标是鼓励这些模型使响应与图像信息更加一致。最近，多项工作引入了 MLLM 的偏好数据集，并研究了不同的对齐方法，包括直接偏好优化 (DPO) 和近端策略优化 (PPO)。然而，由于数据集、基础模型类型和对齐方法的不同，目前尚不清楚哪些具体因素对这些工作中报告的改进贡献最大。在本文中，我们独立分析了 MLLM 中偏好对齐的各个方面。我们首先将对齐算法分为两组，离线（如 DPO）和在线（如在线 DPO），并表明结合离线和在线方法可以在某些情况下提高模型的性能。我们回顾了各种已发布的多模态偏好数据集，并讨论了其构建细节如何影响模型性能。基于这些见解，我们引入了一种创建多模态偏好数据的新方法，称为偏差驱动幻觉采样 (BDHS)，它既不需要额外的注释也不需要外部模型，并表明它可以在各种基准测试中实现与先前发布的多模态模型对齐工作相当的性能。||
|**2024-07-02**|[Conceptual Codebook Learning for Vision-Language Models](http://arxiv.org/abs/2407.02350)|null|在本文中，我们提出了概念码本学习（CoCoLe），这是一种针对视觉语言模型（VLM）的新型微调方法，旨在解决在少量样本情况下对下游任务进行微调时提高VLM泛化能力的挑战。我们认识到，视觉概念（如纹理、形状和颜色）可以自然地跨域迁移，并且在泛化任务中发挥着至关重要的作用。受这一有趣发现的启发，我们学习了一个由视觉概念作为键、概念提示作为值的概念码本，它充当图像编码器输出和文本编码器输入之间的桥梁。具体来说，对于给定的图像，我们利用码本识别与类嵌入相关的最相关的概念提示，以执行分类。此外，我们还结合了一个手工制作的概念缓存作为正则化，以缓解低样本情况下出现的过拟合问题。我们观察到，这种概念码本学习方法能够增强视觉和语言模态之间的对齐。大量的实验结果表明，我们的CoCoLe方法在各种评估设置（包括从基础到新的泛化、跨数据集评估和域泛化任务）中都明显优于现有的最先进方法。详细的消融研究进一步证实了CoCoLe中每个组件的有效性。||
|**2024-07-02**|[Synthetic Multimodal Question Generation](http://arxiv.org/abs/2407.02233)|null|多模态检索增强生成 (MMRAG) 是一种强大的多模态文档问答方法。评估 MMRAG 的一个关键挑战是缺乏与目标问题风格和模态相匹配的高质量数据集。鉴于此，我们提出了 SMMQG，一个合成数据生成框架。SMMQG 利用检索器、大型语言模型 (LLM) 和大型多模态模型 (LMM) 之间的相互作用，直接从多模态文档中生成问答对，并使问题符合指定的风格和模态。我们使用 SMMQG 从维基百科文档中生成了一个包含 1024 个问题的 MMRAG 数据集，并使用该数据集评估了最先进的模型，揭示了只有通过特定风格和模态的评估数据才能获得的模型性能洞察。接下来，我们通过人工研究来衡量 SMMQG 产生的数据的质量。我们发现，我们的合成数据的质量与众包基准 MMQA 的质量相当，并且使用这两个数据集的下游评估结果非常一致。||
|**2024-07-02**|[Multi-Modal Video Dialog State Tracking in the Wild](http://arxiv.org/abs/2407.02218)|null|我们提出了 MST-MIXER，这是一个基于通用多模态状态跟踪方案的新型视频对话模型。目前声称能够执行多模态状态跟踪的模型在两个主要方面存在不足：(1) 它们要么只跟踪一种模态（主要是视觉输入），要么 (2) 它们针对的是不能反映现实世界复杂性的合成数据集。我们的模型解决了这两个限制，试图弥合这一关键的研究差距。具体来说，MST-MIXER 首先跟踪每个输入模态中最重要的成分。然后，它通过使用一种新颖的多模态图结构学习方法学习局部潜在图，从而预测每个模态所选成分缺失的底层结构。随后，将学习到的局部图和特征一起解析，形成一个在所有模态混合上运行的全局图，从而进一步细化其结构和节点嵌入。最后，利用细粒度的图节点特征来增强骨干视觉语言模型 (VLM) 的隐藏状态。MST-MIXER 在五个具有挑战性的基准测试中取得了新的最先进成果。||

## 6DOF Object Pose

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-08-19**|[RUMI: Rummaging Using Mutual Information](http://arxiv.org/abs/2408.10450)|null|本文介绍了一种名为“基于互信息的翻动物体方法”(RUMI) 的在线机器人动作序列生成方法，用于在视觉遮挡环境中收集有关已知可移动物体姿态的信息。该方法专注于接触丰富的翻动物体任务，利用物体姿态分布和机器人轨迹之间的互信息进行动作规划。RUMI 从观察到的部分点云推断出兼容的物体姿态分布，并实时利用工作空间占用率近似其互信息。在此基础上，我们开发了一种信息增益成本函数和可达性成本函数，以将物体保持在机器人的可达范围内。这些函数被集成到具有随机动力学模型的模型预测控制 (MPC) 框架中，以闭环方式更新姿态分布。主要贡献包括一种新的物体姿态估计置信度框架、一种高效的信息增益计算策略以及一种鲁棒的基于 MPC 的控制方案。与基线方法相比，RUMI 在仿真和真实任务中均表现出优越的性能。||
|**2024-08-15**|[Comparative Evaluation of 3D Reconstruction Methods for Object Pose Estimation](http://arxiv.org/abs/2408.08234)|**[link](https://github.com/varunburde/reconstruction_pose_benchmark)**|物体姿态估计对于许多涉及机器人操作、导航和增强现实的工业应用至关重要。当前通用的物体姿态估计器，即不需要针对每个物体进行训练的方法，依赖于精确的3D模型。目前主要使用CAD模型，但在实践中很难获得。同时，获取物体的图像通常是可行的。自然而然地，这就引出了一个问题：从图像重建的3D模型是否足以实现准确的物体姿态估计？为了回答这个问题，我们提出了一个新的基准测试，用于测量3D重建质量对姿态估计精度的影响。我们的基准测试提供了用于物体重建的校准图像，这些图像与YCB-V数据集的测试图像配准，用于在BOP基准测试格式下进行姿态评估。对多种最先进的3D重建和物体姿态估计方法进行的详细实验表明，现代重建方法生成的几何结构通常足以实现准确的姿态估计。我们的实验得出了一些有趣的观察结果：（1）用于测量3D重建质量的标准指标不一定能指示姿态估计精度，这表明需要专门的基准测试，例如我们的基准测试。（2）经典的、非基于学习的方法可以与现代的基于学习的重建技术相媲美，甚至可以提供更好的重建时间-姿态精度权衡。（3）使用重建模型和CAD模型的性能之间仍然存在相当大的差距。为了促进缩小这一差距的研究，我们的基准测试在https://github.com/VarunBurde/reconstruction_pose_benchmark公开可用。||
|**2024-07-16**|[NeuSurfEmb: A Complete Pipeline for Dense Correspondence-based 6D Object Pose Estimation without CAD Models](http://arxiv.org/abs/2407.12207)|**[link](https://github.com/ethz-asl/neusurfemb)**|目前最先进的 6D 物体姿态估计方法都假设可以使用 CAD 模型，并且需要用户手动建立基于物理的渲染 (PBR) 流程来生成合成训练数据。这两个因素都限制了这些方法在现实场景中的应用。在这项工作中，我们提出了一个不需要 CAD 模型的流程，并且只需输入一小部分真实图像就可以训练出最先进的姿态估计器。我们的方法基于 NeuS2 对象表示，我们通过基于运动恢复结构 (SfM) 和对象无关分割的半自动程序来学习这种表示。我们利用 NeuS2 的新视角合成能力和简单的剪切粘贴增强功能来自动生成逼真的物体渲染图，我们使用这些渲染图来训练基于对应的 SurfEmb 姿态估计器。我们在 LINEMOD-Occlusion 数据集上评估了我们的方法，广泛研究了其各个组件的影响，并展示了其相对于基于 CAD 模型和 PBR 数据的方法的竞争性能。我们还展示了我们流程在自收集的现实世界对象上的易用性和有效性，表明我们的方法优于最先进的无 CAD 模型方法，具有更好的精度和对轻度遮挡的鲁棒性。为了让机器人界能够从该系统中受益，我们将在 https://www.github.com/ethz-asl/neusurfemb 上公开发布它。||
|**2024-06-06**|[Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking](http://arxiv.org/abs/2406.04316)|null|6D物体姿态估计是计算机视觉中一项至关重要但极具挑战性的任务，其主要难点在于缺乏大规模数据集。这种稀缺性阻碍了对模型性能的全面评估，限制了研究进展。此外，可用实例或类别的数量有限也限制了其应用。为了解决这些问题，本文提出了Omni6DPose，这是一个以对象类别多样性、规模大和对象材质多样性为特征的大型数据集。Omni6DPose主要由三个部分组成：ROPE（真实6D物体姿态估计数据集），包含332K张图像，涵盖149个类别、581个实例，超过150万个标注；SOPE（模拟6D物体姿态估计数据集），包含475K张在混合现实环境中创建并进行深度模拟的图像，涵盖与ROPE相同的149个类别、4162个实例，超过500万个标注；以及在ROPE和SOPE中均使用的手动对齐的真实扫描物体。由于存在大量变化和歧义，Omni6DPose本身就极具挑战性。为了应对这一挑战，我们引入了GenPose++，它是SOTA类别级姿态估计框架的增强版本，它包含两个关键改进：语义感知特征提取和基于聚类的聚合。此外，我们还提供了全面的基准测试分析，以评估先前方法在这个大规模数据集上在6D物体姿态估计和姿态跟踪方面的性能。||
|**2024-06-05**|[Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices](http://arxiv.org/abs/2406.02977)|null|随着机器人和增强现实应用越来越依赖于精确高效的 6D 物体姿态估计，边缘设备上的实时性能对于实现更具交互性和响应能力的系统至关重要。我们提出的稀疏颜色代码网络 (SCCN) 体现了一种清晰简洁的管道设计，可以有效地满足这一需求。SCCN 对 RGB 图像中的目标物体执行像素级预测，利用基本物体几何特征的稀疏性来加速透视 n 点 (PnP) 计算过程。此外，它引入了一种新颖的基于像素级几何的物体对称表示，可以与初始姿态预测无缝集成，有效地解决了对称物体的歧义问题。值得注意的是，SCCN 在 NVIDIA Jetson AGX Xavier 上分别在基准 LINEMOD 数据集和遮挡 LINEMOD 数据集上实现了每秒 19 帧 (FPS) 和 6 FPS 的估计速率，同时在这些速率下始终保持较高的估计精度。||
|**2024-05-31**|[Deep Learning-Based Object Pose Estimation: A Comprehensive Survey](http://arxiv.org/abs/2405.07801)|**[link](https://github.com/cnjianliu/awesome-object-pose-estimation)**|目标姿态估计是计算机视觉中的一个基本问题，在增强现实和机器人领域有着广泛的应用。在过去的十年中，深度学习模型由于其优越的准确性和鲁棒性，越来越多地取代了依赖于工程点对特征的传统算法。然而，当代方法仍然存在若干挑战，包括它们对标记训练数据的依赖性、模型紧凑性、在挑战性条件下的鲁棒性以及泛化到新颖的未见过对象的能力。目前缺乏一项关于该领域不同方面取得的进展、突出挑战和未来有希望的方向的最新综述。为了填补这一空白，我们讨论了基于深度学习的目标姿态估计的最新进展，涵盖了该问题的所有三种形式，即实例级、类别级和未见过目标姿态估计。我们的综述还涵盖了多种输入数据模态、输出姿态的自由度、目标属性和下游任务，为读者提供了对该领域的全面理解。此外，它还讨论了不同领域的训练范式、推理模式、应用领域、评估指标和基准数据集，并报告了当前最先进方法在这些基准上的性能，从而方便读者为其应用选择最合适的方法。最后，该综述指出了关键挑战，回顾了当前的趋势及其优缺点，并指出了未来研究的有希望的方向。我们还将继续跟踪https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation上的最新工作。||
|**2024-03-28**|[Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation](http://arxiv.org/abs/2403.19527)|**[link](https://github.com/leeiieeo/ag-pose)**|类别级 6D 对象姿态估计旨在估计特定类别中未见实例的旋转、平移和大小。在这个领域，基于密集对应的方法已经取得了领先的性能。然而，它们没有明确地考虑不同实例的局部和全局几何信息，导致对具有显著形状变化的未见实例的泛化能力较差。为了解决这个问题，我们提出了一种新的用于类别级 6D 对象姿态估计的实例自适应和几何感知关键点学习方法 (AG-Pose)，它包括两个关键设计：(1) 第一个设计是实例自适应关键点检测模块，它可以自适应地检测一组稀疏关键点，用于表示各种实例的几何结构。(2) 第二个设计是几何感知特征聚合模块，它可以有效地将局部和全局几何信息整合到关键点特征中。这两个模块可以协同工作，为未见过的实例建立鲁棒的关键点级对应关系，从而增强模型的泛化能力。在 CAMERA25 和 REAL275 数据集上的实验结果表明，所提出的 AG-Pose 在没有类别特定形状先验的情况下，大大优于现有技术方法。||
|**2024-06-01**|[Object Pose Estimation via the Aggregation of Diffusion Features](http://arxiv.org/abs/2403.18791)|**[link](https://github.com/tianfu18/diff-feats-pose)**|从图像中估计物体姿态是3D场景理解的关键任务，最近的方法在非常大的基准数据集上显示出良好的结果。然而，这些方法在处理未见过物体时性能会显著下降。我们认为这是由于图像特征的泛化能力有限造成的。为了解决这个问题，我们深入分析了扩散模型（如Stable Diffusion）的特征，这些模型在对未见过物体建模方面具有巨大潜力。基于这一分析，我们创新性地将这些扩散特征引入物体姿态估计。为此，我们提出了三种不同的架构，可以有效地捕获和聚合不同粒度的扩散特征，大大提高了物体姿态估计的泛化能力。我们的方法在三个流行的基准数据集LM、O-LM和T-LESS上，以相当大的优势优于最先进的方法。特别是，我们的方法在未见过物体上取得了比之前最佳结果更高的准确率：在Unseen LM上为98.2% vs. 93.5%，在Unseen O-LM上为85.9% vs. 76.3%，显示了我们方法强大的泛化能力。我们的代码已发布在https://github.com/Tianfu18/diff-feats-pose。||
|**2024-07-29**|[KITchen: A Real-World Benchmark and Dataset for 6D Object Pose Estimation in Kitchen Environments](http://arxiv.org/abs/2403.16238)|null|尽管最近在用于机器人抓取的6D物体姿态估计方法方面取得了进展，但这些方法在现有数据集上的能力与其在现实世界抓取和移动操作任务中的效率之间仍然存在巨大的性能差距，特别是当机器人完全依赖于其单目以自我为中心的视野（FOV）时。现有的现实世界数据集主要集中在桌面抓取场景，其中机械臂放置在固定位置，物体集中在固定外部摄像机的视野内。评估此类数据集的性能可能无法准确反映在厨房环境中日常抓取和移动操作任务中遇到的挑战，例如从更高的架子、水槽、洗碗机、烤箱、冰箱或微波炉中检索物体。为了解决这一差距，我们提出了KITchen，这是一种专门为估计厨房环境中不同位置的物体的6D姿态而设计的新型基准。为此，我们记录了一个综合数据集，其中包含使用具有人形机器人以自我为中心的视角在两个不同的厨房中捕获的111个厨房物体的约205k个真实世界RGBD图像。随后，我们开发了一个半自动注释管道，以简化此类数据集的标记过程，从而以最少的人工生成2D对象标签、2D对象分割掩码和6D对象姿态。基准、数据集和注释管道将在https://kitchen-dataset.github.io/KITchen上公开发布。||
|**2024-03-22**|[DITTO: Demonstration Imitation by Trajectory Transformation](http://arxiv.org/abs/2403.15203)|null|快速便捷地教会机器人新技能对于机器人系统的广泛应用至关重要。在这项工作中，我们提出了一种两阶段方法，用于解决从单个RGB-D视频记录的人类演示中进行一次性模仿的问题。在第一阶段（离线阶段），我们提取演示的轨迹。这需要分割被操作物体并确定它们相对于次要物体（如容器）的相对运动。随后，在实时在线轨迹生成阶段，我们首先重新检测所有物体，然后将演示轨迹映射到当前场景，最后控制机器人跟踪该轨迹。为了完成这些步骤，我们的方法利用了几个辅助模型，包括用于分割、相对物体姿态估计和抓取预测的模型。我们系统地评估了对应方法和重新检测方法的不同组合，以验证我们跨多种任务的设计决策。具体来说，我们收集了十种不同任务的演示，包括拾放任务以及铰接物体操作。最后，我们在真实的机器人系统上进行了广泛的评估，以证明我们的方法在现实场景中的有效性和实用性。我们在http://ditto.cs.uni-freiburg.de上公开了代码。||
|**2024-03-21**|[Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation](http://arxiv.org/abs/2403.14559)|null|在2D图像中定位预定义的3D关键点是为6DoF物体姿态估计建立3D-2D对应关系的有效方法。然而，不可见关键点的不可靠定位结果会降低对应关系的质量。在本文中，我们通过定位可见性方面的关键点来解决这个问题。由于关键点可见性信息在当前的数据集收集过程中缺失，我们提出了一种有效的方法，可以从可用的物体级标注中生成二进制可见性标签，用于非对称物体和对称物体的关键点。我们进一步基于PageRank算法从二进制标签中导出实值可见性感知重要性。利用我们可见性感知重要性的灵活性，我们通过将可见性感知重要性与最先进的姿态估计算法以及附加的位置编码相结合，构建了VAPO（可见性感知姿态估计器）。我们在流行的姿态估计基准上进行了广泛的实验，包括Linemod、Linemod-Occlusion和YCB-V。结果表明，VAPO改进了关键点对应关系和最终估计的姿态，并明显达到了最先进的性能。||
|**2024-03-18**|[GenFlow: Generalizable Recurrent Flow for 6D Pose Refinement of Novel Objects](http://arxiv.org/abs/2403.11510)|null|尽管基于学习的方法在6D物体姿态估计方面取得了进展，但对于新物体，精度和可扩展性之间的权衡仍然存在。具体来说，以前针对新物体的方法没有很好地利用目标物体的3D形状信息，因为它们侧重于通过间接处理形状来实现泛化，从而降低了效率。我们提出了GenFlow，这是一种在目标物体形状的指导下实现精度和对新物体泛化的方。我们的方法预测渲染图像和观察图像之间的光流，并迭代地细化6D姿态。它通过3D形状约束和从端到端可微系统学习到的可泛化几何知识来提高性能。我们通过设计级联网络架构来进一步改进我们的模型，以利用多尺度相关性和从粗到精的细化。GenFlow在RGB和RGB-D情况下均在未见物体姿态估计基准测试中排名第一。它还在没有任何微调的情况下，在可见物体姿态估计方面实现了与现有最先进方法相当的性能。||
|**2024-03-14**|[MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion](http://arxiv.org/abs/2403.09309)|null|杂乱的料箱拣选环境对姿态估计模型提出了挑战。尽管深度学习取得了令人瞩目的进步，但单视图 RGB 姿态估计模型在杂乱的动态环境中表现不佳。利用视频场景中包含的丰富时间信息有可能增强模型处理遮挡和环境动态性的不利影响的能力。此外，联合目标检测和姿态估计模型更适合利用任务的相互依赖性来提高两项任务的准确性。为此，我们提出了一种基于注意力的多目标 6D 姿态估计时间融合方法，该方法可以在视频序列的多个帧中积累信息。我们的 MOTPose 方法将一系列图像作为输入，并在一次前向传递中对所有目标执行联合目标检测和姿态估计。它学习使用基于交叉注意力的融合模块在多个时间步长上聚合目标嵌入和目标参数。我们在物理逼真的杂乱料箱拣选数据集 SynPick 和 YCB-Video 数据集上评估了我们的方法，并证明了改进的姿态估计精度以及更好的目标检测精度。||

## nerf

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-08-20**|[Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark Images Using Gaussian Splatting](http://arxiv.org/abs/2408.09130)|**[link](https://github.com/yec22/Gaussian-DK)**|三维高斯体积渲染技术近年来发展迅速，可以使用一致的多视图图像作为输入，合成出非凡的新视角。然而，我们注意到，在场景未被完全照亮的黑暗环境中拍摄的图像可能会出现明显的亮度变化和多视图不一致性，这对三维高斯体积渲染技术提出了巨大的挑战，并严重降低了其性能。为了解决这个问题，我们提出了 Gaussian-DK。观察到不一致性主要是由相机成像引起的，我们使用一组各向异性的三维高斯函数来表示现实世界中一致的辐射场，并设计了一个相机响应模块来补偿多视图不一致性。我们还引入了一种基于步长的梯度缩放策略，以约束靠近相机的高斯函数（结果证明是漂浮物）的拆分和克隆。在我们提出的基准数据集上的实验表明，Gaussian-DK 可以生成高质量的渲染结果，没有重影和漂浮物伪影，并且性能明显优于现有方法。此外，我们还可以通过控制曝光水平来合成亮度更高的图像，清晰地显示阴影区域的细节。||
|**2024-07-18**|[EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian Splatting](http://arxiv.org/abs/2407.13520)|null|近年来，随着神经辐射场（NeRF）和3D高斯散射（3DGS）的发展，3D去模糊重建技术取得了显著进展。 尽管这些技术可以从模糊图像输入中恢复相对清晰的3D重建，但它们在处理严重模糊和复杂相机运动方面仍然面临局限性。 为了解决这些问题，我们提出了事件辅助3D高斯散射去模糊重建（EaDeblur-GS），它集成了事件相机数据，以增强3DGS对运动模糊的鲁棒性。 通过采用自适应偏差估计器（ADE）网络估计高斯中心偏差和使用新颖的损失函数，EaDeblur-GS实时实现了清晰的3D重建，其性能可与最先进的方法相媲美。||
|**2024-07-10**|[3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes](http://arxiv.org/abs/2407.07090)|null|基于粒子的辐射场表示法，例如 3D 高斯 splatting，已经在复杂场景的重建和重新渲染方面取得了巨大成功。大多数现有方法通过光栅化渲染粒子，将它们投影到屏幕空间图块中，以便按排序顺序进行处理。而这项工作考虑对粒子进行光线追踪，构建边界体积层次结构，并使用高性能 GPU 光线追踪硬件为每个像素投射光线。为了有效处理大量半透明粒子，我们描述了一种专门的渲染算法，该算法使用边界网格封装粒子以利用快速的光线三角形相交，并按深度顺序对成批的相交点进行着色。光线追踪的优势在计算机图形学中是众所周知的：处理非相干光线以获得阴影和反射等二次照明效果，从机器人技术中常见的高度扭曲的相机进行渲染，对光线进行随机采样等等。使用我们的渲染器，与光栅化相比，这种灵活性几乎不需要任何成本。实验证明了我们方法的速度和准确性，以及在计算机图形学和视觉方面的若干应用。我们进一步提出了对基本高斯表示的相关改进，包括简单使用广义核函数，这显著减少了粒子命中次数。||
|**2024-07-07**|[GaussReg: Fast 3D Registration with Gaussian Splatting](http://arxiv.org/abs/2407.05254)|null|点云配准是大规模三维场景扫描和重建的基本问题。在深度学习的帮助下，配准方法得到了显著发展，已接近成熟阶段。随着神经辐射场（NeRF）的引入，它凭借强大的视图合成能力成为了最流行的三维场景表示方法。对于NeRF表示，大规模场景重建同样需要对其进行配准。然而，该主题极度缺乏探索。这是因为对具有隐式表示的两个场景之间的几何关系进行建模存在固有挑战。现有方法通常将隐式表示转换为显式表示以进行进一步配准。最近，引入了高斯 splatting（GS），它采用显式三维高斯函数。该方法在保持高质量渲染的同时，显著提高了渲染速度。给定两个具有显式 GS 表示的场景，我们在这项工作中探索它们之间的三维配准任务。为此，我们提出了 GaussReg，一种快速且准确的从粗到精的框架。粗配准阶段遵循现有的点云配准方法，并估计来自 GS 的点云的粗略对齐。我们还提出了一种新的图像引导的精配准方法，该方法从 GS 渲染图像，为精确对齐提供更详细的几何信息。为了支持全面评估，我们仔细构建了一个名为 ScanNet-GSReg 的场景级数据集，其中包含从 ScanNet 数据集中获得的 1379 个场景，并收集了一个名为 GSReg 的真实世界数据集。实验结果表明，我们的方法在多个数据集上实现了最先进的性能。我们的 GaussReg 比 HLoc（SuperPoint 作为特征提取器，SuperGlue 作为匹配器）快 44 倍，并且精度相当。||
|**2024-07-04**|[CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion Blur Images](http://arxiv.org/abs/2407.03923)|null|神经辐射场 (NeRF) 因其高质量的新视角渲染能力而备受关注，促使研究人员致力于解决各种现实世界案例。其中一个关键挑战是相机在曝光时间内移动造成的相机运动模糊，这阻碍了准确的 3D 场景重建。在本研究中，我们提出了连续刚体运动感知高斯散射 (CRiM-GS)，以实时渲染速度从模糊图像中重建精确的 3D 场景。考虑到实际的相机运动模糊过程包含复杂的运动模式，我们基于神经常微分方程 (ODE) 预测相机的连续运动。具体而言，我们利用刚体变换对相机运动进行建模，并进行适当的正则化，以保持对象的形状和大小。此外，我们在 \textit{SE(3)} 场中引入了连续可变形 3D 变换，通过确保更高的自由度使刚体变换适应现实世界问题。通过重新审视基本相机理论并采用先进的神经网络训练技术，我们实现了对连续相机轨迹的精确建模。我们进行了广泛的实验，证明了在基准数据集上，无论是在定量还是定性方面，都达到了最先进的性能。||
|**2024-07-29**|[Trimming the Fat: Efficient Compression of 3D Gaussian Splats through Pruning](http://arxiv.org/abs/2406.18214)|**[link](https://github.com/salmanali96/trimming-the-fat)**|近年来，由于神经辐射场和最近出现的3D高斯渲染(3DGS)模型提供了端到端训练的能力，3D模型的使用得到了推广。后者具有显著优势，因为它本身就易于在训练期间快速收敛，并提供了广泛的可编辑性。然而，尽管发展迅速，但关于这些模型可扩展性的文献仍处于起步阶段。在本研究中，我们采取了一些初步步骤来解决这一差距，展示了一种能够实现此类模型的内存和计算可扩展性的方法。具体来说，我们提出了“Trimming the fat”，这是一种事后梯度信息的迭代剪枝技术，用于消除模型中编码的冗余信息。我们在广泛认可的基准测试上的实验结果证明了我们方法的有效性，表明在保持甚至改进基线性能的同时，可以移除高达75%的高斯函数。我们的方法实现了约50倍的压缩，同时保持了与基线模型相似的性能，并且能够将计算速度提高到600 FPS。||
|**2024-06-21**|[Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks](http://arxiv.org/abs/2406.15149)|null|仿真器是自主机器人学习的强大工具，因为它们可以提供可扩展的数据生成、灵活的设计和轨迹优化。然而，将从仿真数据中学习到的行为迁移到现实世界中被证明是困难的，通常需要通过计算量大的域随机化方法或进一步的模型微调来缓解。我们提出了一种改进泛化能力和鲁棒性的方法，以应对仿真到真实视觉四旋翼导航任务中的分布偏移。为此，我们首先通过将高斯 splatting 与四旋翼飞行动力学相结合来构建仿真器，然后使用 Liquid 神经网络训练鲁棒的导航策略。通过这种方式，我们获得了一个全栈模仿学习协议，它结合了 3D 高斯 splatting 辐射场渲染的进步、专家演示训练数据的巧妙编程以及 Liquid 网络的任务理解能力。通过一系列定量飞行测试，我们证明了在单个模拟场景中学习到的导航技能可以直接稳健地迁移到现实世界。我们进一步展示了在剧烈的分布和物理环境变化下，在训练环境之外保持性能的能力。我们学习到的 Liquid 策略，仅在从真实感室内模拟飞行中挑选的单目标机动上进行训练，可以泛化到户外真实硬件平台上的多步远足。||
|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|在非结构化的旅游环境中拍摄的照片经常呈现出多变的外观和短暂的遮挡，这对精确的场景重建提出了挑战，并在新视角合成中导致了伪影。尽管先前的方法已经将神经辐射场 (NeRF) 与其他可学习模块相结合来处理动态外观并消除瞬态对象，但其大量的训练需求和缓慢的渲染速度限制了实际部署。最近，3D 高斯 splatting (3DGS) 已成为 NeRF 的一种很有前途的替代方案，它提供了卓越的训练和推理效率以及更好的渲染质量。本文介绍了 Wild-GS，这是一种针对不受约束的照片集优化的 3DGS 创新改编版，同时保留了其效率优势。Wild-GS 通过每张图像的固有材质属性、全局照明和相机属性以及逐点反射率的局部方差来确定每个 3D 高斯函数的外观。与先前在图像空间中对参考特征进行建模的方法不同，Wild-GS 通过对从参考图像中提取的三平面进行采样，将像素外观特征与相应的局部高斯函数明确对齐。这种新颖的设计有效地将参考视图的高频细节外观转移到 3D 空间，并显着加快了训练过程。此外，利用 2D 可见性图和深度正则化分别减轻瞬态效应和约束几何形状。大量实验表明，Wild-GS 在所有现有技术中实现了最先进的渲染性能以及最高的训练和推理效率。||
|**2024-06-06**|[A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation](http://arxiv.org/abs/2406.04253)|null|三维建模一直是计算机视觉和计算机图形学的重要领域。近年来，由于神经表示和生成模型的突破，我们见证了三维建模的快速发展。三维人体建模作为游戏和动画等许多现实世界应用程序的核心，已经引起了极大的关注。在过去的几年里，出现了大量关于创建三维人体化身的工作，为三维人体建模形成了一个新的、丰富的知识库。文献的规模之大，使得个人难以跟踪所有的工作。本次综述旨在从重建和生成两个角度，全面概述这些新兴的三维人体化身建模技术。首先，我们回顾了三维人体重建的代表性方法，包括基于像素对齐隐函数、神经辐射场和三维高斯散射等方法。然后，我们总结了三维人体生成的代表性方法，特别是那些使用大型语言模型（如CLIP）、扩散模型和各种三维表示的方法，这些方法展示了最先进的性能。最后，我们讨论了对现有方法的反思以及三维人体化身建模面临的开放性挑战，为未来的研究指明了方向。||
|**2024-06-13**|[3D-HGS: 3D Half-Gaussian Splatting](http://arxiv.org/abs/2406.02720)|**[link](https://github.com/lihaolin88/3d-half-gaussian-splatting)**|逼真的三维重建是三维计算机视觉中的一个基本问题。由于最近神经渲染技术的出现，该领域取得了相当大的进步。这些技术主要旨在专注于学习三维场景的体积表示，并通过渲染得到的损失函数来细化这些表示。其中，三维高斯 splatting (3D-GS) 已成为一种重要方法，其性能超过了神经辐射场 (NeRFs)。3D-GS 使用参数化的三维高斯模型来建模空间位置和颜色信息，并结合基于图块的快速渲染技术。尽管其渲染性能和速度都非常出色，但使用三维高斯核在准确表示不连续函数方面存在固有限制，特别是在形状不连续的边缘和角落，以及跨不同纹理的颜色不连续处。为了解决这个问题，我们建议采用三维半高斯 (3D-HGS) 核，它可以作为即插即用的核使用。我们的实验表明，它们能够在不影响渲染速度的情况下，提高当前与 3D-GS 相关方法的性能，并在各种数据集上实现最先进的渲染性能。||
|**2024-08-11**|[FastLGS: Speeding up Language Embedded Gaussians with Feature Grid Mapping](http://arxiv.org/abs/2406.01916)|null|语义交互式辐射场因其在促进用户友好和自动化的现实世界 3D 场景理解应用方面的潜力而一直是一项吸引人的任务。然而，在辐射场中同时实现高质量、高效率和零样本能力的语义是一项具有挑战性的任务。在这项工作中，我们提出了 FastLGS，这是一种支持在高分辨率下 3D 高斯渲染 (3DGS) 中进行实时开放词汇查询的方法。我们提出了语义特征网格来保存基于 Segment Anything Model (SAM)  掩码提取的多视图 CLIP 特征，并通过 3DGS 将网格映射到低维特征以进行语义场训练。训练完成后，我们可以通过渲染特征的特征网格恢复像素对齐的 CLIP 嵌入，用于开放词汇查询。与其他最先进方法的比较证明，FastLGS 在速度和精度方面均能达到最佳性能，其中 FastLGS 比 LERF 快 98 倍，比 LangSplat 快 4 倍。同时，实验表明 FastLGS 具有适应性和兼容性，可用于许多下游任务，例如 3D 分割和 3D 对象修复，并且可以轻松应用于其他 3D 操作系统。||
|**2024-05-30**|[ $\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving](http://arxiv.org/abs/2405.20323)|**[link](https://github.com/nnanhuang/s3gaussian)**|逼真的街道场景三维重建是开发自动驾驶真实世界模拟器的关键技术。尽管神经辐射场 (NeRF) 对驾驶场景的重建效果很好，但三维高斯散射 (3DGS) 凭借其更快的速度和更明确的表示，成为一个很有前途的方向。然而，大多数现有的街道 3DGS 方法都需要跟踪的三维车辆边界框来分解静态和动态元素以进行有效重建，这限制了它们在野外场景中的应用。为了在无需昂贵标注的情况下实现高效的三维场景重建，我们提出了一种自监督的街道高斯（$\textit{S}^3$Gaussian）方法，利用四维一致性来分解动态和静态元素。我们使用三维高斯函数来表示每个场景以保持其清晰度，并进一步结合时空场网络来紧凑地建模四维动态。我们在具有挑战性的 Waymo-Open 数据集上进行了大量实验，以评估我们方法的有效性。我们的 $\textit{S}^3$ Gaussian 展示了在不使用三维标注的情况下分解静态和动态场景的能力，并取得了最佳性能。代码可在以下网址获得：https://github.com/nnanhuang/S3Gaussian/。||
|**2024-05-28**|[RT-GS2: Real-Time Generalizable Semantic Segmentation for 3D Gaussian Representations of Radiance Fields](http://arxiv.org/abs/2405.18033)|**[link](https://github.com/mbjurca/RT_GS2)**|高斯渲染技术通过实现实时的高性能渲染，彻底改变了新视角合成的世界。最近，研究重点集中在为下游任务丰富这些3D表示的语义信息。在本文中，我们介绍了RT-GS2，这是第一个采用高斯渲染技术的可泛化语义分割方法。虽然现有的基于高斯渲染的方法依赖于场景特定的训练，但RT-GS2展示了泛化到未见场景的能力。我们的方法采用了一种新方法，首先以自监督的方式提取视图无关的3D高斯特征，然后进行新颖的视图依赖/视图无关（VDVI）特征融合，以增强不同视图之间的语义一致性。在三个不同数据集上的大量实验表明，RT-GS2在语义分割质量方面优于最先进的方法，例如在Replica数据集上的mIoU提高了8.01%。此外，我们的方法实现了27.03 FPS的实时性能，与现有方法相比实现了惊人的901倍加速。据我们所知，这项工作通过引入第一个用于辐射场3D高斯表示的实时可泛化语义分割方法，代表了该领域的重大进步。||
|**2024-05-29**|[PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting](http://arxiv.org/abs/2405.16829)|null|神经辐射场 (NeRFs) 在合成大规模场景的逼真图像方面表现出了非凡的能力。然而，它们经常受到细节丢失和渲染时间长的困扰。三维高斯 splatting 最近被引入作为一种有效的替代方案，可以实现高保真视觉效果和更快的渲染性能。尽管如此，扩展三维高斯 splatting 仍然充满了挑战。具体来说，大规模场景需要整合来自多个尺度和不同视点的对象，这通常会导致效率下降，因为高斯需要在细节级别之间取得平衡。此外，从大规模数据集中通过 COLMAP 生成初始化点不仅计算量大，而且容易导致重建不完整。为了应对这些挑战，我们提出了采用 NeRF 初始化的金字塔式三维高斯 splatting (PyGS)。我们的方法采用以金字塔形式排列的分层高斯集合来表示场景。金字塔的顶层由一些大的高斯函数组成，而随后的每一层都包含更密集的小高斯函数集合。我们通过以不同的频率对快速训练的基于网格的 NeRF 进行采样，从而有效地初始化这些金字塔高斯函数。我们将这些金字塔高斯函数分组到簇中，并使用紧凑的加权网络在渲染过程中动态确定每个簇中每个金字塔级别的影响，同时考虑相机视点。我们的方法在多个大规模数据集上实现了显著的性能飞跃，渲染速度比当前最先进的方法快 400 多倍。||
|**2024-05-11**|[Direct Learning of Mesh and Appearance via 3D Gaussian Splatting](http://arxiv.org/abs/2405.06945)|null|准确重建包含显式几何信息的3D场景既有吸引力又具有挑战性。几何重建可以受益于结合可微分的表观模型，例如神经辐射场和3D高斯 splatting (3DGS)。在这项工作中，我们提出了一个可学习的场景模型，它将3DGS与显式几何表示（即网格）结合起来。我们的模型以端到端的方式学习网格和外观，我们将3D高斯函数绑定到网格面上，并执行3DGS的可微分渲染以获得光度监督。该模型创建了一个有效的信息通路来监督场景学习，包括网格。实验结果表明，学习到的场景模型不仅实现了最先进的渲染质量，而且还支持使用显式网格进行操作。此外，由于网格和外观的端到端学习，我们的模型在适应场景更新方面具有独特优势。||

## 分类/检测/识别/分割

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-08-22**|[ssProp: Energy-Efficient Training for Convolutional Neural Networks with Scheduled Sparse Back Propagation](http://arxiv.org/abs/2408.12561)|**[link](https://github.com/lujiazho/ssprop)**|近年来，深度学习取得了显著进步，尤其是在生成模型方面，例如大型语言模型和概率扩散模型。然而，训练这些模型通常需要大量的计算资源，需要数十亿petaFLOPs的计算量。这种高资源消耗导致大量的能源消耗和碳排放，引发了严重的环境问题。反向传播（BP）是深度学习模型训练过程中计算开销的主要来源。为了推进节能训练的研究，并使任何机器和设备都能进行稀疏学习，我们提出了一种通用的、节能的卷积模块，可以无缝集成到任何深度学习架构中。具体来说，我们在反向传播过程中引入了通道稀疏性以及额外的梯度选择调度器，其基于以下假设：反向传播通常是密集且低效的，这可能导致过拟合和高计算量。我们的实验表明，我们的方法在图像分类和生成任务上验证了可以减少40%的计算量，同时有可能提高模型性能。这种减少可以显著节省能源，并降低大规模人工智能系统研发阶段的碳足迹。此外，我们的方法以不同于Dropout的方式减轻了过拟合，使其能够与Dropout结合使用，进一步提高模型性能并减少计算资源的使用。大量实验验证了我们的方法可以推广到各种数据集和任务，并且与各种深度学习架构和模块兼容。代码公开发布于https://github.com/lujiazho/ssProp。||
|**2024-08-22**|[Enhanced Infield Agriculture with Interpretable Machine Learning Approaches for Crop Classification](http://arxiv.org/abs/2408.12426)|null|近年来，人工智能的日益普及引发了人们对图像分类的兴趣激增，尤其是在农业领域。在计算机视觉、机器学习和深度学习的帮助下，农业领域发生了重大变革，导致了田间作物分类新技术的开发。尽管对各种图像分类技术进行了广泛的研究，但大多数技术都存在局限性，例如准确度低、数据使用有限以及缺乏对模型大小和预测的报告。所有限制中最重要的是模型可解释性的需求。本研究评估了四种不同的作物分类方法，即使用 SIFT、ORB 和颜色直方图等手工特征提取方法的传统机器学习；自定义设计的 CNN 和已建立的深度学习架构（如 AlexNet）；在使用 ImageNet 预训练的五个模型（如 EfficientNetV2、ResNet152V2、Xception、Inception-ResNetV2、MobileNetV3）上进行迁移学习；以及尖端的基石模型，如 YOLOv8 和自监督视觉Transformer 模型 DINOv2。所有模型都表现良好，但 Xception 在泛化方面优于所有模型，在测试数据上达到了 98% 的准确率，模型大小为 80.03 MB，预测时间为 0.0633 秒。本研究的一个关键方面是应用可解释人工智能来提供所有模型的可解释性。本期刊使用 LIME、SHAP 和 GradCAM 展示了 Xception 模型的可解释性，确保了模型预测的透明度和可信度。本研究强调了根据特定任务需求选择正确模型的重要性。它还强调了可解释性在农业中部署人工智能方面的重要作用，它提供了有助于增强人工智能驱动的作物管理策略的有见地的信息。||
|**2024-08-22**|[Class-balanced Open-set Semi-supervised Object Detection for Medical Images](http://arxiv.org/abs/2408.12355)|null|现实世界中的医学图像数据集通常是未标记和不平衡的，半监督目标检测 (SSOD) 可以利用未标记的数据来改进目标检测器。然而，现有方法主要假设未标记数据和测试数据不包含分布外 (OOD) 类别。现有的少数开放集半监督目标检测方法有两个弱点：首先，没有考虑类别不平衡；其次，OOD 实例在伪标签过程中被区分出来并简单地丢弃。在本文中，我们考虑开放集半监督目标检测问题，该问题利用包含 OOD 类别的未标记数据来改进医学图像的目标检测。我们的研究包含两项关键创新：类别控制嵌入 (CCE) 和分布外检测融合分类器 (OODFC)。CCE 旨在通过构建前景信息库来解决数据集不平衡问题，而 OODFC 通过将“未知”信息集成到基本伪标签中来应对开放集挑战。我们的方法优于最先进的 SSOD 性能，在公共寄生虫数据集上实现了 4.25 mAP 的改进。||
|**2024-08-22**|[AT-SNN: Adaptive Tokens for Vision Transformer on Spiking Neural Network](http://arxiv.org/abs/2408.12293)|null|在脉冲神经网络 (SNN) 的训练和推理中，直接训练和轻量计算方法一直是正交发展的，旨在降低功耗。然而，由于 SNN-based vision transformers (ViTs) 最初是为卷积神经网络 (CNN) 设计的，因此只有少数方法同时应用了这两种机制，并且未能充分利用 SNN-based ViTs 的优势。在本文中，我们提出了 AT-SNN，旨在通过直接训练动态调整基于 SNN 的 ViTs 在推理过程中处理的token数量，其中功耗与token数量成正比。我们首先证明了自适应计算时间 (ACT)（以前仅限于 RNN 和 ViTs）对基于 SNN 的 ViTs 的适用性，并增强了它以选择性地丢弃信息量较少的空间token。此外，我们提出了一种新的token合并机制，它依赖于token的相似性，可以进一步减少token的数量，同时提高准确性。我们将 AT-SNN 应用于 Spikformer，并展示了 AT-SNN 在图像分类任务 CIFAR10、CIFAR-100 和 TinyImageNet 上与最先进方法相比在实现高能效和准确性方面的有效性。例如，我们的方法使用的token数量比 CIFAR-100 上现有的最佳性能方法少 42.4%，同时保持了更高的准确性。||
|**2024-08-22**|[BihoT: A Large-Scale Dataset and Benchmark for Hyperspectral Camouflaged Object Tracking](http://arxiv.org/abs/2408.12232)|null|高光谱目标跟踪（HOT）在各种应用中展现出潜力，特别是在目标伪装的场景中。由于现有HOT数据集存在偏差，大多数目标往往具有区别性的视觉外观而非光谱特征，因此现有的跟踪器可以通过波段重组有效地检索目标。这种偏差使得跟踪器可以直接使用由高光谱图像生成的伪彩色图像获得的视觉特征，而无需提取光谱特征。为了解决这种偏差，我们发现当目标外观不可靠时，跟踪器应该关注光谱信息。因此，我们提供了一个名为高光谱伪装目标跟踪（HCOT）的新任务，并精心构建了一个名为 BihoT 的大规模 HCOT 数据集，该数据集包含 41,912 张高光谱图像，涵盖 49 个视频序列。该数据集涵盖了各种人工伪装场景，其中目标具有相似的外观、不同的光谱和频繁的遮挡，这使其成为 HCOT 的一个非常具有挑战性的数据集。此外，本文还提出了一个简单但有效的基线模型，称为基于光谱提示的分心感知网络（SPDAN），该模型包括一个光谱嵌入网络（SEN）、一个基于光谱提示的骨干网络（SPBN）和一个分心感知模块（DAM）。具体而言，SEN 通过 3D 和 2D 卷积提取光谱空间特征。然后，SPBN 使用光谱提示微调强大的 RGB 跟踪器，并缓解训练样本不足的问题。此外，DAM 利用一种新的统计量来捕获由目标和背景遮挡引起的分心因素。大量实验表明，我们提出的 SPDAN 在所提出的 BihoT 和其他 HOT 数据集上实现了最先进的性能。||
|**2024-08-22**|[A Unified Plug-and-Play Algorithm with Projected Landweber Operator for Split Convex Feasibility Problems](http://arxiv.org/abs/2408.12100)|null|近年来，即插即用(PnP)方法通过用去噪器代替近端算子，在逆成像问题中取得了最先进的表现。基于近端梯度方法，PnP的一些理论结果已经出现，其中适当的步长对于收敛性分析至关重要。然而，在实际应用中，应用具有理论保证步长的PnP方法是困难的，并且这些算法仅限于高斯噪声。本文从分裂凸可行性问题(SCFP)的角度出发，提出了一种具有投影Landweber算子的自适应PnP算法(PnP-PLO)来解决这些问题。图像去模糊、超分辨率和压缩感知MRI的数值实验表明，具有理论保证的PnP-PLO优于RED和RED-PRO等最先进的方法。||
|**2024-08-21**|[Approaching Deep Learning through the Spectral Dynamics of Weights](http://arxiv.org/abs/2408.11804)|**[link](https://github.com/dyunis/spectral_dynamics)**|我们提出了一种以权重谱动力学（即优化过程中奇异值和奇异向量的行为）为中心的经验方法，以统一和阐明深度学习中的几种现象。我们从小型“顿悟”实验到诸如使用卷积神经网络进行图像分类、使用 U-Net 进行图像生成、使用 LSTM 进行语音识别以及使用 Transformer 进行语言建模等大型任务，在各种实验中发现了优化过程中存在的一致偏差。我们还证明，即使在实际系统中，权重衰减也能增强这种偏差，使其超出其作为范数正则化器的作用。此外，我们表明，这些谱动力学将记忆网络与泛化网络区分开来，为这个长期存在的难题提供了新的视角。此外，我们利用谱动力学来探索高性能稀疏子网络（彩票假设）的出现以及通过线性模式连接的损失曲面结构。我们的研究结果表明，谱动力学提供了一个连贯的框架，可以更好地理解神经网络在不同环境下的行为。||
|**2024-08-21**|[SBDet: A Symmetry-Breaking Object Detector via Relaxed Rotation-Equivariance](http://arxiv.org/abs/2408.11760)|null|引入群等变卷积（GConv）使模型能够探索隐藏在视觉数据中的对称性，从而提高模型性能。然而，在现实世界场景中，物体或场景通常表现出对称系统的扰动，特别是与对称结构的偏差，这可以用对称群的非平凡作用来表征，称为对称性破缺。传统的 GConv 方法受到群空间中严格运算规则的限制，只能确保特征在有限的群变换下保持严格等变，因此难以适应对称性破缺或非刚性变换。为此，我们引入了具有我们定义的松弛旋转等变群 $\mathbf{R}_4$ 的新型松弛旋转 GConv (R2GConv)。此外，我们提出了松弛旋转等变网络 (R2Net) 作为骨干网络，并进一步开发了基于此网络的用于二维目标检测的对称性破缺目标检测器 (SBDet)。实验表明，我们提出的 R2GConv 在自然图像分类任务中是有效的，并且 SBDet 在目标检测任务中取得了优异的性能，具有更好的泛化能力和鲁棒性。||
|**2024-08-21**|[MambaCSR: Dual-Interleaved Scanning for Compressed Image Super-Resolution With SSMs](http://arxiv.org/abs/2408.11758)|null|我们提出了 MambaCSR，这是一个基于 Mamba 的简单而有效的框架，用于处理具有挑战性的压缩图像超分辨率 (CSR) 任务。 特别是，尽管 Mamba 依赖于所有标记的选择性状态空间建模，但其扫描策略对于在恢复过程中进行有效的上下文知识建模至关重要。 在这项工作中，我们为 CSR 提出了一种高效的双重交错扫描范式 (DIS)，它由两种扫描策略组成：(i) 分层交错扫描旨在通过同时利用基于局部窗口的扫描方法和顺序扫描方法，全面捕获和利用图像中最潜在的上下文信息；(ii) 水平到垂直交错扫描旨在通过减少不同方向扫描之间的冗余来降低计算成本。 为了克服非均匀压缩伪影，我们还提出了位置对齐的跨尺度扫描来对多尺度上下文信息进行建模。 在多个基准数据集上的实验结果表明，我们的 MambaCSR 在压缩图像超分辨率任务中具有出色的性能。 该代码将很快在~\textcolor{magenta}{\url{https://github.com/renyulin-f/MambaCSR}} 中提供。||
|**2024-08-21**|[Low-Light Object Tracking: A Benchmark](http://arxiv.org/abs/2408.11463)|null|近年来，随着大规模训练数据集的应用，视觉跟踪领域取得了显著进展。这些数据集支持了复杂算法的开发，提高了视觉目标跟踪的精度和稳定性。然而，大多数研究主要集中在良好的照明环境下，而忽略了在弱光环境下跟踪的挑战。在弱光场景中，光照可能会发生剧烈变化，目标可能缺乏明显的纹理特征，并且在某些情况下，目标可能无法直接观察到。这些因素会导致跟踪性能严重下降。为了解决这个问题，我们引入了 LLOT，这是一个专为弱光目标跟踪设计的基准测试。LLOT 包含 269 个具有挑战性的序列，总计超过 132K 帧，每个序列都用边界框进行了仔细标注。这个专门设计的数据集旨在促进弱光条件下目标跟踪技术的创新和进步，以应对现有基准测试未充分涵盖的挑战。为了评估现有方法在 LLOT 上的性能，我们对 39 种最先进的跟踪算法进行了广泛的测试。结果突出了弱光跟踪性能的巨大差距。为此，我们提出了 H-DCPT，这是一种结合了历史和黑暗线索提示的新型跟踪器，以设定更强的基线。在我们的实验中，H-DCPT 的性能优于所有 39 种评估方法，证明了其显著的改进。我们希望我们的基准测试和 H-DCPT 将促进开发新颖且准确的方法来跟踪弱光条件下的目标。LLOT 和代码可在 https://github.com/OpenCodeGithub/H-DCPT 获取。||
|**2024-08-20**|[A Closer Look at Data Augmentation Strategies for Finetuning-Based Low/Few-Shot Object Detection](http://arxiv.org/abs/2408.10940)|null|目前的低样本和少样本目标检测方法主要集中在提高模型检测目标的性能。实现这一目标的一种常见方法是将模型微调与数据增强策略相结合。然而，在数据稀缺的情况下，这些方法的能效却很少受到关注。本文旨在进行一项全面的实证研究，考察轻量级目标检测器结合自定义数据增强和自动数据增强选择策略时的模型性能和能效。在三个不同的基准数据集上，从性能和能耗方面对这些方法进行了评估，并使用效率因子来深入了解它们在性能和效率方面的有效性。结果表明，在许多情况下，数据增强策略带来的性能提升被其增加的能耗所掩盖，因此需要开发更节能的数据增强策略来解决数据稀缺问题。||
|**2024-08-20**|[Aligning Object Detector Bounding Boxes with Human Preference](http://arxiv.org/abs/2408.10844)|**[link](https://github.com/ombretta/humans-vs-detectors)**|先前的工作表明，在相同交并比（IoU）的情况下，人类倾向于更大的边界框，而不是更小的边界框。然而，我们在这里展示了常用的目标检测器预测大框和小框的频率相同。在这项工作中，我们研究了如何使自动检测到的目标框与人类偏好一致，并研究这是否能提高人类的质量感知。我们通过用户研究（N = 123）评估了三种常用目标检测器的性能。我们发现，即使相应的AP接近于0，人们也更喜欢放大1.5倍或2倍的目标检测结果。基于这一结果，我们提出了一种非对称边界框回归损失，它鼓励预测更大的边界框，而不是更小的边界框。我们的评估研究表明，使用非对称损失微调的目标检测器更符合人类的偏好，并且优于固定的缩放因子。定性评估表明，人类的偏好可能受到某些物体特征的影响，例如物体形状。||
|**2024-08-20**|[Just a Hint: Point-Supervised Camouflaged Object Detection](http://arxiv.org/abs/2408.10777)|null|伪装目标检测 (COD) 要求模型快速准确地区分隐藏在环境中的物体。由于细微的差异和模糊的边界，COD 不仅对模型来说是一项极具挑战性的任务，对人工标注员也是如此，需要付出巨大的努力才能提供像素级标注。为了减轻繁重的标注负担，我们建议仅借助一个点监督来完成这项任务。具体来说，通过快速点击每个对象，我们首先自适应地将原始的基于点的注释扩展到合理的提示区域。然后，为了避免在判别部分周围出现部分定位，我们提出了一种注意力调节器，通过部分掩盖标记区域将模型注意力分散到整个目标。此外，为了解决仅基于点注释下伪装对象特征表示不稳定的问题，我们基于不同增强的图像对（例如，改变颜色或进行转换）执行无监督对比学习。在三个主流 COD 基准测试中，实验结果表明，我们的模型在各种指标上都大幅优于几种弱监督方法。||
|**2024-08-20**|[Detection of Intracranial Hemorrhage for Trauma Patients](http://arxiv.org/abs/2408.10768)|null|全身CT扫描用于多发伤患者的全身损伤搜索。由于初步评估需要快速进行，并且需要对全身进行病灶搜索，因此分配给特定解剖结构检查的时间非常少。特别是颅内出血仍然会被遗漏，尤其是在临床实习生中。在这项工作中，我们提出了一种深度学习方法来突出显示此类病灶，以提高诊断准确性。虽然大多数关于颅内出血的工作都进行分割，但检测只需要边界框来定位出血点。在本文中，我们提出了一种新的体素完全IoU（VC-IoU）损失函数，它鼓励网络学习边界框的三维纵横比，并实现更精确的检测。我们使用公开可用的数据集对脑出血检测进行了广泛的实验，并在一个私人队列中进行了验证，分别达到了0.877 AR30、0.728 AP30和0.653 AR30、0.514 AP30的成绩。与其他损失函数相比，这些结果表明两个数据集的平均召回率都有相对+5%的提高。最后，由于目前公开可用的三维目标检测数据很少，而且临床环境中的标注资源有限，我们评估了不同标注方法的成本，以及训练数据中不精确边界框对检测性能的影响。||
|**2024-08-20**|[SAM-COD: SAM-guided Unified Framework for Weakly-Supervised Camouflaged Object Detection](http://arxiv.org/abs/2408.10760)|null|大多数伪装目标检测 (COD) 方法严重依赖于掩码标注，而获取这些标注非常耗时且劳动强度大。与全监督方法相比，现有的弱监督 COD 方法表现出明显较差的性能，并且难以同时支持所有现有类型的伪装目标标签，包括涂鸦、边界框和点。即使对于 Segment Anything Model (SAM) 来说，处理弱监督 COD 仍然存在问题，它通常会遇到涂鸦标签的提示兼容性、极端响应、语义错误响应和特征表示不稳定等挑战，从而在伪装场景中产生不令人满意的结果。为了缓解这些问题，我们在本文中提出了一种统一的 COD 框架，称为 SAM-COD，它能够支持任意的弱监督标签。我们的 SAM-COD 采用了一种提示适配器，以基于 SAM 处理涂鸦作为提示。同时，我们引入了响应过滤器和语义匹配器模块，以提高 SAM 在 COD 提示下获得的掩码的质量。为了减轻不准确掩码预测的负面影响，我们采用了一种新的提示自适应知识蒸馏策略，以确保可靠的特征表示。为了验证我们方法的有效性，我们对三个主流的 COD 基准进行了广泛的实验。结果表明，我们的方法优于最先进的弱监督方法，甚至优于全监督方法。||
|**2024-08-20**|[Leveraging Temporal Contexts to Enhance Vehicle-Infrastructure Cooperative Perception](http://arxiv.org/abs/2408.10531)|null|安装在高处的基础设施传感器提供更广泛的感知范围，并减少遮挡。通过 V2X 通信集成基础设施和自动驾驶车辆数据，称为车路协同，在增强感知能力和解决单车自动驾驶遇到的极端情况方面显示出相当大的优势。然而，协同感知仍然面临着许多挑战，包括通信带宽有限和实际通信中断。在本文中，我们提出了 CTCE，一种用于协同三维目标检测的新框架。该框架传输具有时间上下文增强的查询，有效地平衡了传输效率和性能，以适应实际通信条件。此外，我们提出了一个时间引导融合模块，以进一步提高性能。路边时间增强和车辆端时空融合共同构成了一个多级时间上下文集成机制，充分利用时间信息来增强性能。此外，引入了一个运动感知重建模块，以恢复由于通信中断而丢失的路边查询。在 V2X-Seq 和 V2X-Sim 数据集上的实验结果表明，CTCE 优于基线 QUEST，mAP 分别提高了 3.8% 和 1.3%。通信中断条件下的实验验证了 CTCE 对通信中断的鲁棒性。||
|**2024-08-20**|[Cervical Cancer Detection Using Multi-Branch Deep Learning Model](http://arxiv.org/abs/2408.10498)|null|宫颈癌是全球女性面临的一个重大健康问题，持续感染高危人乳头瘤病毒 (HPV) 是其主要诱因，仍然是一个全球性健康挑战，年轻女性的发病率在过去三十年中从 10% 激增至 40%。虽然巴氏涂片检查是一种普遍的诊断方法，但视觉图像分析可能耗时长且容易出错。早期发现该疾病有助于显著改善患者预后。近几十年来，许多研究人员采用机器学习技术，在基于医学图像的宫颈癌检测过程中取得了可喜的成果。近年来，许多研究人员采用各种深度学习技术，在检测宫颈癌方面取得了较高的准确率，但仍面临着各种挑战。本研究提出了一种利用多头自注意力机制 (MHSA) 和卷积神经网络 (CNN) 进行宫颈癌图像分类的创新方法。该方法利用 MHSA 机制和 CNN 的优势，在两个流中有效地捕获宫颈图像中的局部和全局特征。MHSA 促进模型聚焦于相关的感兴趣区域，而 CNN 则提取有助于准确分类的层次特征。最后，我们将两个流的特征结合起来，输入到分类模块中，以优化特征和分类。为了评估所提出方法的性能，我们使用了 SIPaKMeD 数据集，该数据集将宫颈细胞分为五类。我们的模型取得了 98.522% 的显著准确率。该性能在医学图像分类方面具有很高的识别精度，并有望应用于其他医学图像识别任务。||
|**2024-08-19**|[HaSPeR: An Image Repository for Hand Shadow Puppet Recognition](http://arxiv.org/abs/2408.10360)|**[link](https://github.com/Starscream-11813/HaSPeR)**|手影戏，也被称为皮影戏或阴影戏，是一种戏剧艺术和讲故事的形式，表演者通过将手影投射到平面上来创造生物的幻觉。熟练的表演者通过手部定位、手指运动和灵巧的手势来创造这些剪影，使其像动物和物体的影子。由于缺乏从业者和人们娱乐标准的巨大转变，这种艺术形式正处于灭绝的边缘。为了促进其保存并将其推广到更广泛的受众，我们引入了 ${\rm H{\small A}SP{\small E}R}$ ，这是一个由8,340张手影木偶图像组成的新数据集，涵盖从专业和业余手影木偶师视频中提取的11个类别。我们提供了数据集的详细统计分析，并采用了一系列预训练的图像分类模型来建立基线。我们的研究结果表明，传统的卷积模型的性能优于基于注意力的Transformer架构。我们还发现，适用于移动应用程序和嵌入式设备的轻量级模型（如MobileNetV2）表现良好。我们推测，这种低延迟架构可用于开发手影教学工具，并且我们创建了一个原型应用程序来探索这一推测。我们将性能最佳的模型InceptionV3置于聚光灯下，对其进行全面的特征空间、可解释性和错误分析，以深入了解其决策过程。据我们所知，这是第一个记录在案的数据集和研究工作，旨在利用计算机视觉方法为子 generations 代保留这门濒临灭绝的艺术。我们的代码和数据是公开的。||
|**2024-08-19**|[Leveraging Superfluous Information in Contrastive Representation Learning](http://arxiv.org/abs/2408.10292)|null|对比表征学习旨在通过最大化未标记数据的不同视图之间的互信息来学习它们之间的共享信息，这已在面向下游任务的自监督学习中展现出强大的能力。然而，最近的研究表明，更多估计的互信息并不能保证在不同的下游任务中获得更好的性能。这些工作启发我们推测，学习到的表征不仅保留了来自未标记数据的任务相关信息，还携带了对下游任务来说多余的任务无关信息，从而导致性能下降。在本文中，我们证明了在传统的对比学习框架中确实存在多余的信息，并进一步设计了一个新的目标函数，即SuperInfo，通过预测信息和多余信息的线性组合来学习鲁棒的表征。此外，我们注意到，根据我们的SuperInfo损失，可以调整引入损失的系数以丢弃任务无关信息，同时保留部分非共享的任务相关信息。我们证明，使用我们的损失函数进行学习通常可以在图像分类、目标检测和实例分割任务上优于传统的对比学习方法，并取得显著改进。||
|**2024-08-19**|[SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models](http://arxiv.org/abs/2408.10174)|**[link](https://github.com/tanganke/fusion_bench)**|深度模型在大规模数据集上的训练成本越来越高，这促使人们广泛采用深度模型融合技术来利用已有模型的知识。从简单的权重平均到更复杂的方法（如 AdaMerging），模型融合有效地提高了模型性能并加速了新模型的开发。然而，各个模型参数之间潜在的干扰以及融合过程缺乏可解释性仍然是重大挑战。现有方法通常试图通过评估参数的属性（例如其大小或符号）或通过参数剪枝来解决参数干扰问题。在本研究中，我们首先通过子空间分析的视角来研究线性层的微调，并将参数干扰明确定义为一个优化问题，以阐明这一主题。随后，我们介绍了一种创新的模型融合方法，称为零样本低秩专家稀疏混合（SMILE）构建，该方法允许在没有额外数据或进一步训练的情况下将源模型升级为 MoE 模型。我们的方法基于以下观察结果：微调主要保留了预训练中的重要部分，但它使用不太重要或未使用的区域来适应新任务。此外，参数干扰问题在原始参数空间中本质上是难以处理的，可以通过扩展维度来解决。我们针对不同的场景（例如图像分类和文本生成任务）进行了广泛的实验，使用全微调和 LoRA 微调，并将我们的方法应用于大型语言模型（CLIP 模型、Flan-T5 模型和 Mistral-7B 模型），突出了 SMILE 的适应性和可扩展性。代码可在 https://github.com/tanganke/fusion_bench 获取。||
|**2024-08-16**|[SAM2-UNet: Segment Anything 2 Makes Strong Encoder for Natural and Medical Image Segmentation](http://arxiv.org/abs/2408.08870)|**[link](https://github.com/wzh0120/sam2-unet)**|图像分割在视觉理解中起着至关重要的作用。近年来，新兴的视觉基础模型在各种任务上不断取得优异性能。基于这一成功，本文证明了Segment Anything Model 2 (SAM2) 可以作为U型分割模型的强大编码器。我们提出了一个简单但有效的框架，称为SAM2-UNet，用于通用的图像分割。具体来说，SAM2-UNet采用SAM2的Hiera骨干网络作为编码器，而解码器则使用经典的U型设计。此外，编码器中插入了适配器，以实现参数高效的微调。在各种下游任务上的初步实验，如伪装目标检测、显著性目标检测、海洋动物分割、镜子检测和息肉分割，表明我们的SAM2-UNet可以简单地超越现有的专门化的最先进方法，而无需任何额外技巧。项目页面：\url{https://github.com/WZH0120/SAM2-UNet}。||
|**2024-08-16**|[DPA: Dual Prototypes Alignment for Unsupervised Adaptation of Vision-Language Models](http://arxiv.org/abs/2408.08855)|null|视觉语言模型（VLM），例如 CLIP，在零样本图像分类方面已展现出巨大潜力。然而，将这些模型应用于新领域仍然具有挑战性，尤其是在缺乏标记数据的无监督环境下。最近的研究提出了伪标签方法，使用未标记的目标数据以无监督的方式调整 CLIP。然而，由于 CLIP 的视觉和文本表示之间的错位导致的噪声伪标签，这些方法的效果并不理想。本研究介绍了 DPA，一种用于 VLM 的无监督域适应方法。DPA 引入了双重原型的概念，充当不同的分类器，并对其输出进行凸组合，从而实现准确的伪标签构建。接下来，它对伪标签进行排序以促进稳健的自训练，尤其是在训练初期。最后，它通过将文本原型与图像原型对齐来解决视觉-文本错位问题，从而进一步提高适应性能。在 13 个下游视觉任务上的实验表明，DPA 明显优于零样本 CLIP 和最先进的无监督适应基线。||
|**2024-08-16**|[LEVIS: Large Exact Verifiable Input Spaces for Neural Networks](http://arxiv.org/abs/2408.08824)|null|神经网络的鲁棒性在安全关键型应用中至关重要。虽然目前大多数鲁棒性验证方法都是在假设输入空间已知的情况下评估最坏情况下的输出，但识别一个可验证的输入空间 $\mathcal{C}$（其中不存在对抗性样本）对于有效的模型选择、鲁棒性评估和开发可靠的控制策略至关重要。为了应对这一挑战，我们引入了一个名为$\texttt{LEVIS}$的新框架，它包含$\texttt{LEVIS}$-$\alpha$和$\texttt{LEVIS}$-$\beta$。$\texttt{LEVIS}$-$\alpha$在$\mathcal{C}$中心区域内定位尽可能大的可验证球，该球体与至少两个边界相交。相比之下，$\texttt{LEVIS}$-$\beta$集成了多个可验证球，以全面封装整个可验证空间。我们的贡献有三方面：(1) 我们提出了配备了三种开创性技术的$\texttt{LEVIS}$，用于识别沿共线或正交方向的最大可验证球和最近的对抗点。(2) 我们提供了理论分析，阐述了通过$\texttt{LEVIS}$-$\alpha$和$\texttt{LEVIS}$-$\beta$ 获得的可验证球的特性。(3) 我们在各种应用中验证了我们的方法，包括电力流回归和图像分类，展示了性能增强和搜索特性的可视化。||
|**2024-08-16**|[Xpikeformer: Hybrid Analog-Digital Hardware Acceleration for Spiking Transformers](http://arxiv.org/abs/2408.08794)|null|本文介绍了Xpikeformer，这是一种混合模拟-数字硬件架构，旨在加速基于脉冲神经网络（SNN）的Transformer模型。通过将SNN的能效和时间动态特性与Transformer强大的序列建模能力相结合，Xpikeformer利用混合模拟-数字计算技术来提高性能和能效。该架构集成了用于前馈和全连接层的模拟内存计算（AIMC）和用于高效注意力机制的随机脉冲注意力（SSA）引擎。我们详细介绍了Xpikeformer的设计、实现和评估，证明了其在能耗和计算效率方面的显著改进。通过图像分类任务和无线通信符号检测任务，我们证明了Xpikeformer可以实现与软件相当的推理精度。能量评估表明，与最先进的数字ANN Transformer相比，Xpikeformer的能耗降低了17.8到19.2倍，与全数字SNN Transformer相比，能耗降低了5.9到6.8倍。与脉冲Transformer的GPU实现相比，Xpikeformer还实现了12.0倍的速度提升。||
|**2024-08-16**|[Task-Aware Dynamic Transformer for Efficient Arbitrary-Scale Image Super-Resolution](http://arxiv.org/abs/2408.08736)|null|任意尺度超分辨率 (ASSR) 旨在学习单个模型以实现任意放大尺度的图像超分辨率。现有的 ASSR 网络通常包含一个通用的尺度无关特征提取器和一个任意尺度上采样器。这些特征提取器通常使用固定的网络架构来处理不同的 ASSR 推理任务，每个任务都以输入图像和上采样尺度为特征。然而，这忽略了超分辨率在不同推理场景下的难度差异，其中简单的图像或小的超分辨率尺度可以用比困难图像或大的超分辨率尺度更少的计算量来解决。为了解决这种难度可变性问题，在本文中，我们提出了一种任务感知动态变换器 (TADT) 作为一种输入自适应特征提取器，用于高效的图像 ASSR。我们的 TADT 由一个建立在多尺度变换器块 (MSTB) 组上的多尺度特征提取主干和一个任务感知路由控制器 (TARC) 组成。TARC 预测特征提取主干内的推理路径，特别是根据输入图像和超分辨率尺度选择 MSTB。推理路径的预测由一个新的损失函数引导，以权衡超分辨率精度和效率。实验表明，当与三个流行的任意尺度上采样器一起工作时，与主流特征提取器相比，我们的 TADT 实现了最先进的 ASSR 性能，但计算成本相对较低。代码将公开发布。||
|**2024-08-16**|[Multimodal Relational Triple Extraction with Query-based Entity Object Transformer](http://arxiv.org/abs/2408.08709)|null|多模态关系抽取对于构建灵活且真实的知识图谱至关重要。最近的研究侧重于提取不同模态中存在的实体对之间的关系类型，例如文本中的一个实体和图像中的另一个实体。然而，现有方法需要预先给出实体和对象，这既昂贵又不切实际。为了解决这一局限性，我们提出了一个新的任务，即多模态实体-对象关系三元组抽取，旨在从图像-文本对中抽取所有三元组（实体跨度、关系、对象区域）。为了促进这项研究，我们修改了包含 21 种关系类型的多模态关系抽取数据集 MORE，创建了一个包含 20,264 个三元组的新数据集，平均每个图像-文本对包含 5.75 个三元组。此外，我们提出了 QEOT，一个基于查询的模型，具有选择性注意机制，可以动态地探索文本和视觉信息的交互和融合。特别是，所提出的方法可以通过一组查询同时完成实体提取、关系分类和目标检测。我们的方法适用于下游应用，并且由于采用流水线式方法，减少了错误积累。大量的实验结果表明，我们提出的方法优于现有的基线 8.06%，达到了最先进的性能。||
|**2024-08-16**|[QMambaBSR: Burst Image Super-Resolution with Query State Space Model](http://arxiv.org/abs/2408.08665)|null|突发超分辨率旨在通过融合多帧突发低分辨率图像中的亚像素信息来重建具有更高质量和更丰富细节的高分辨率图像。在突发超分辨率中，关键挑战在于提取基准帧内容的互补亚像素细节，同时抑制高频噪声干扰。现有方法试图通过逐帧建模帧间关系来提取亚像素，而忽略了多帧之间的相互关联以及帧内交互，导致用于基准帧超分辨率的亚像素不准确且带有噪声。此外，现有方法主要采用固定参数的静态上采样来提高所有场景的空间分辨率，无法感知多帧之间亚像素分布的差异，也不能平衡不同帧的融合权重，导致细节过度平滑和伪影。为了解决这些限制，我们引入了一种新颖的查询曼巴突发超分辨率 (QMambaBSR) 网络，它结合了查询状态空间模型 (QSSM) 和自适应上采样模块 (AdaUp)。具体来说，基于亚像素具有稳定的空间分布而随机噪声分布不一致的观察结果，提出了一种新颖的 QSSM，通过帧间查询和帧内扫描有效地提取亚像素，同时一步消除噪声干扰。此外，AdaUp 旨在根据不同突发场景中多帧亚像素信息的的空间分布动态调整上采样内核，从而促进高分辨率细节空间排列的重建。在四个流行的合成和真实基准数据集上的大量实验表明，我们的方法实现了新的最先进性能。||
|**2024-08-16**|[MM-UNet: A Mixed MLP Architecture for Improved Ophthalmic Image Segmentation](http://arxiv.org/abs/2408.08600)|null|眼科图像分割是眼部疾病诊断的关键基础。虽然全卷积神经网络 (CNN) 常用于分割，但它们受到归纳偏差的限制，并且在建立远程依赖关系方面面临挑战。基于 Transformer 的模型解决了这些限制，但引入了大量的计算开销。最近，人们提出了一种简单而高效的多层感知机 (MLP) 架构用于图像分类，相对于高级 Transformer 实现了具有竞争力的性能。然而，其在眼科图像分割中的有效性仍未得到探索。在本文中，我们介绍了 MM-UNet，这是一种专为眼科图像分割而设计的有效混合 MLP 模型。在 MM-UNet 中，我们提出了一个多尺度 MLP (MMLP) 模块，该模块通过分组策略促进不同深度特征的交互，从而能够同时捕获全局和局部信息。我们对私人前节光学相干断层扫描 (AS-OCT) 图像数据集和公共眼底图像数据集进行了广泛的实验。结果表明，与最先进的深度分割网络相比，我们的 MM-UNet 模型具有优越性。||
|**2024-08-15**|[Beyond Uniform Query Distribution: Key-Driven Grouped Query Attention](http://arxiv.org/abs/2408.08454)|null|Transformer架构通过其自注意力机制彻底改变了深度学习，该机制可以有效地捕获上下文信息。然而，自注意力机制的内存占用对于长序列任务来说是一个重大挑战。分组查询注意力（GQA）通过将查询分组和对相应的键值头进行均值池化来解决这个问题，从而以灵活的方式减少了总体参数数量和内存需求，而不会对模型精度造成负面影响。在这项工作中，我们介绍了对GQA的增强，重点关注两种偏离分组静态特性的新方法：键分布式GQA（KDGQA）和动态键分布式GQA（DGQA），它们利用键头的范数信息来指导查询分配。具体来说，KDGQA关注每次前向传递过程中键头范数的比率，而DGQA检查这些比率在训练过程中的演变。此外，我们还提出了扰动GQA（PGQA）作为案例研究，它通过从注意力图中减去噪声来引入（静态）组形成的可变性。我们使用经过预训练的视觉Transformer进行的实验，用于在CIFAR-10、CIFAR-100、Food101和Tiny ImageNet等数据集上进行图像分类，证明了这些变体通过信息更丰富、自适应性更强的分组机制改进了原始GQA：特别是与GQA和其他变体相比，ViT-L在使用DGQA时精度提高了8%。我们进一步分析了键值头数量对性能的影响，强调了利用查询键亲和度的重要性。||
|**2024-08-15**|[Predictive uncertainty estimation in deep learning for lung carcinoma classification in digital pathology under real dataset shifts](http://arxiv.org/abs/2408.08432)|null|深度学习在各种数字病理学和医学图像分类任务中取得了巨大进展。将其整合到安全的临床决策支持系统中需要稳健可靠的模型。然而，现实世界的数据具有多样性，这些数据往往超出预期的来源分布。此外，当测试样本截然不同时，临床决策会受到很大影响。量化模型中的预测不确定性对于校准良好的预测以及确定何时（或不）信任模型至关重要。不幸的是，许多工作忽视了预测不确定性估计的重要性。本文评估了预测不确定性估计是否会增强基于深度学习的诊断决策系统的稳健性。我们研究了各种癌症分布变化场景对预测性能和校准的影响。我们首先系统地研究了三种改进预测不确定性的流行方法：蒙特卡洛 dropout、深度集成和基于全幻灯片图像中肺腺癌分类作为主要疾病的小样本学习。其次，我们比较了这些方法在临床相关分布变化（如包含主要疾病亚型和其他特征分析数据的分布内变化；包含差异很大的病例、不同器官来源和成像方式变化的分布外变化）下的性能和校准效果。虽然存在关于不确定性估计的研究，但据我们所知，目前还没有严格的大规模基准测试比较预测不确定性估计，包括肺癌分类的这些数据集变化。||
|**2024-08-15**|[SLCA++: Unleash the Power of Sequential Fine-tuning for Continual Learning with Pre-training](http://arxiv.org/abs/2408.08295)|**[link](https://github.com/gengdavid/slca)**|近年来，使用预训练的持续学习 (CLPT) 引起了广泛的兴趣，它不同于传统的从头开始训练的模式。使用强大的预训练模型 (PTM) 可以极大地促进知识迁移并缓解灾难性遗忘，但也存在将预训练知识过度拟合到特定下游任务的问题。目前的大多数努力通常保持 PTM 冻结，并结合特定任务的提示来指导表示学习，并在推理时进行提示选择。然而，由于提示参数容量有限，这种策略在持续学习中仅表现出次优的性能。相比之下，调整 PTM 的所有参数通常为表示学习提供了最大的潜力，这使得顺序微调 (Seq FT) 成为 CLPT 中被忽视的基本基线。为此，我们从 Seq FT 的角度深入分析了渐进式过拟合问题。考虑到过快的表示学习和有偏差的分类层构成了这个特殊问题，我们引入了具有分类器对齐的先进慢速学习器 (SLCA++) 框架，以释放 Seq FT 的力量，作为 CLPT 的强大基线方法。我们的方法涉及一个慢速学习器，以选择性地降低骨干参数的学习率，以及一个分类器对齐，以在后处理方式中对齐不相交的分类层。我们通过对称交叉熵损失进一步增强了 SL 的功效，并采用参数高效的策略来使用 SLCA++ 实现 Seq FT。在图像分类基准测试的各种持续学习场景中，我们的方法提供了实质性的改进，并且在很大程度上优于最先进的方法。代码：https://github.com/GengDavid/SLCA。||
|**2024-08-15**|[Learned Multimodal Compression for Autonomous Driving](http://arxiv.org/abs/2408.08211)|null|自动驾驶传感器会生成大量数据。在本文中，我们探索了用于自动驾驶的学习多模态压缩，特别针对 3D 物体检测。我们专注于摄像头和激光雷达模态，并探索了几种编码方法。一种方法涉及融合模态的联合编码，而其他方法涉及首先编码一种模态，然后对另一种模态进行条件编码。我们在 nuScenes 数据集上评估了这些编码方案的性能。我们的实验结果表明，与其他方法相比，融合模态的联合编码产生了更好的结果。||
|**2024-08-15**|[Category-Prompt Refined Feature Learning for Long-Tailed Multi-Label Image Classification](http://arxiv.org/abs/2408.08125)|**[link](https://github.com/jiexuanyan/cprfl)**|现实世界的数据始终呈现出长尾分布，通常涵盖多个类别。这种复杂性凸显了内容理解的挑战，特别是在需要长尾多标签图像分类 (LTMLC) 的场景中。在这种情况下，不平衡的数据分布和多目标识别构成了重大障碍。为了解决这个问题，我们提出了一种新颖有效的 LTMLC 方法，称为类别提示精炼特征学习 (CPRFL)，它利用不同类别之间的语义相关性并解耦每个类别的类别特定视觉表示。具体来说，CPRFL 使用预训练的 CLIP 嵌入初始化类别提示，并通过与视觉特征交互来解耦类别特定的视觉表示，从而促进头部和尾部类别之间语义相关性的建立。为了减轻视觉语义域偏差，我们设计了一种渐进式双路径反向传播机制，通过将上下文相关的视觉信息逐步整合到提示中来改进提示。同时，在改进提示的指导下，改进过程促进了类别特定视觉表示的逐步净化。此外，考虑到负样本和正样本的不平衡，我们采用非对称损失作为优化目标，以抑制所有类别中的负样本，并有可能提高从头到尾的识别性能。我们在两个 LTMLC 基准测试中验证了我们方法的有效性，大量实验表明我们的工作优于基线方法。代码可在 https://github.com/jiexuanyan/CPRFL 获取。||
|**2024-08-15**|[OC3D: Weakly Supervised Outdoor 3D Object Detection with Only Coarse Click Annotation](http://arxiv.org/abs/2408.08092)|null|基于激光雷达的户外 3D 物体检测受到了广泛关注。然而，使用激光雷达点云训练 3D 检测器通常依赖于昂贵的边界框标注。本文提出了 OC3D，这是一种创新的弱监督方法，只需要在 3D 点云的鸟瞰图上进行粗略点击。这里的一个关键挑战是缺乏来自这种简单点击标注的目标对象的完整几何描述。为了解决这个问题，我们提出的 OC3D 采用了两阶段策略。在第一阶段，我们首先设计了一种新颖的动静分类策略，然后提出了 Click2Box 和 Click2Mask 模块，分别为静态和动态实例生成框级和掩码级伪标签。在第二阶段，我们设计了一个 Mask2Box 模块，利用神经网络的学习能力将包含信息较少的掩码级伪标签更新为框级伪标签。在广泛使用的 KITTI 和 nuScenes 数据集上的实验结果表明，与弱监督 3D 检测方法相比，我们仅需粗略点击的 OC3D 即可实现最先进的性能。将 OC3D 与缺失点击挖掘策略相结合，我们提出了 OC3D++ 流程，该流程在 KITTI 数据集中仅需 0.2% 的标注成本即可实现与全监督方法相当的性能。||
|**2024-08-15**|[HAIR: Hypernetworks-based All-in-One Image Restoration](http://arxiv.org/abs/2408.08091)|**[link](https://github.com/toummHus/HAIR)**|图像修复是从退化的版本中恢复高质量的清晰图像，这是计算机视觉中的一项基本任务。最近图像修复的进展已经证明了能够同时解决各种退化的学习模型的有效性，即一体化图像修复模型。然而，这些现有方法通常对具有不同退化类型的图像使用相同的参数，这导致模型被迫在退化类型之间进行权衡，从而损害整体性能。为了解决这个问题，我们提出了HAIR，一种基于超网络的即插即用方法，它根据输入图像的内容为相应的网络动态生成参数。HAIR由两个主要组件组成：分类器（Cl）和超选择网络（HSN）。更具体地说，分类器是一个简单的图像分类网络，用于生成包含输入图像退化信息的全局信息向量（GIV）；而HSN可以看作是一个简单的全连接神经网络，接收GIV并输出相应模块的参数。大量实验表明，将HAIR融入架构可以在低成本的情况下显著提高不同模型在图像修复任务上的性能，尽管HAIR只生成参数，并没有改变这些模型的逻辑结构。通过将HAIR融入流行的架构Restormer，我们的方法在一系列图像修复任务上获得了优于或至少与当前最先进方法相当的性能。（代码和预训练的检查点可在此处获取。）||
|**2024-08-14**|[GRFormer: Grouped Residual Self-Attention for Lightweight Single Image Super-Resolution](http://arxiv.org/abs/2408.07484)|**[link](https://github.com/sisrformer/grformer)**|先前的工作表明，减少基于Transformer的单图像超分辨率（SISR）模型（例如SwinIR）的参数开销和计算量通常会导致性能下降。在本文中，我们提出了GRFormer，一种高效且轻量级的方法，它不仅减少了参数开销和计算量，而且还大大提高了性能。GRFormer的核心是分组残差自注意力（GRSA），它专门针对两个基本组件。首先，它引入了一种新颖的分组残差层（GRL）来代替自注意力中的查询、键、值（QKV）线性层，旨在有效地减少参数开销、计算量和性能损失。其次，它集成了紧凑的指数空间相对位置偏差（ES-RPB）来替代原始的相对位置偏差，以提高表示位置信息的能力，同时进一步减少参数数量。大量的实验结果表明，GRFormer在×2、×3和×4 SISR任务上优于最先进的基于Transformer的方法，特别是在DIV2K数据集上训练时，PSNR最高超过SOTA 0.23dB，同时仅在自注意力模块中将参数数量和MAC减少了约\textbf{60\%}和\textbf{49\%}。我们希望我们简单有效的方法可以轻松应用于基于窗口划分自注意力的SR模型，可以作为图像超分辨率进一步研究的有用工具。代码可在\url{https://github.com/sisrformer/GRFormer}获取。||
|**2024-08-14**|[One Step Diffusion-based Super-Resolution with Time-Aware Distillation](http://arxiv.org/abs/2408.07476)|**[link](https://github.com/learninghx/tad-sr)**|基于扩散的图像超分辨率 (SR) 方法已在从低分辨率图像重建具有精细细节的高分辨率图像方面展现出潜力。然而，这些方法通常需要数十甚至数百次迭代采样，导致延迟很大。最近，已经设计出通过知识蒸馏来提高基于扩散的 SR 模型采样效率的技术。尽管如此，在调整学生模型和教师模型的知识时，这些解决方案要么仅仅依赖于像素级损失约束，要么忽略了扩散模型在不同时间步长优先考虑不同级别信息的事实。为了实现有效且高效的图像超分辨率，我们提出了一种时间感知扩散蒸馏方法，称为 TAD-SR。具体来说，我们引入了一种新颖的分数蒸馏策略，以在轻微噪声扰动后对齐学生模型和教师模型输出之间的数据分布。这种蒸馏策略使学生网络能够更加专注于高频细节。此外，为了减轻由蒸馏引起的性能限制，我们集成了潜在对抗损失，并设计了一种时间感知鉴别器，利用扩散先验来有效区分真实图像和生成图像。在合成数据集和真实世界数据集上进行的大量实验表明，所提出的方法仅需一步采样即可实现与先前最先进 (SOTA) 方法和教师模型相当甚至更好的性能。代码可在 https://github.com/LearningHx/TAD-SR 获取。||
|**2024-08-14**|[Infra-YOLO: Efficient Neural Network Structure with Model Compression for Real-Time Infrared Small Object Detection](http://arxiv.org/abs/2408.07455)|null|虽然卷积神经网络在可见光目标检测方面取得了显著成就，但红外小目标检测由于信噪比低、目标结构不完整以及缺乏可靠的红外小目标数据集，仍然面临许多挑战。为了解决红外小目标数据集的局限性，本文构建了一个名为 InfraTiny 的新数据集，其中超过 85% 的边界框小于 32x32 像素（3218 张图像，共 20,893 个边界框）。本文提出了多尺度注意力机制模块（MSAM）和特征融合增强金字塔模块（FFAFPM），并将其部署到嵌入式设备上。MSAM 使网络能够通过获取不同的感受野来获得尺度感知信息，同时抑制背景噪声信息，增强特征提取能力。所提出的 FFAFPM 可以丰富语义信息，增强浅层特征和深层特征的融合，从而显著减少误报。通过将所提出的方法集成到 YOLO 模型（命名为 Infra-YOLO）中，提高了红外小目标检测性能。与 yolov3 相比，在 InfraTiny 数据集上，mAP@0.5 提高了 2.7%；与 yolov4 相比，提高了 2.5%。提出的 Infra-YOLO 还被迁移到无人机 (UAV) 的嵌入式设备上，用于实际应用场景，其中采用通道剪枝方法来减少 FLOPs，并在速度和精度之间取得平衡。即使使用剪枝方法将 Infra-YOLO 的参数减少了 88%，与 yolov3 相比，mAP@0.5 仍然提高了 0.7%，与 yolov4 相比提高了 0.5%。实验结果表明，与以往的基准方法相比，本文提出的 MSAM 和 FFAFPM 方法能够提高红外小目标检测性能。||
|**2024-08-14**|[RTAT: A Robust Two-stage Association Tracker for Multi-Object Tracking](http://arxiv.org/abs/2408.07344)|null|数据关联是基于检测的多目标跟踪 (MOT) 中必不可少的一部分。大多数跟踪器侧重于如何设计更好的数据关联策略来提高跟踪性能。基于规则的手工关联方法简单高效，但缺乏处理复杂场景的泛化能力。虽然学习型关联方法可以学习高阶上下文信息来处理各种复杂场景，但它们存在复杂度和成本较高的局限性。为了解决这些局限性，我们提出了一种鲁棒的两阶段关联跟踪器，称为 RTAT。第一阶段关联在轨迹片段和检测之间执行，以生成高纯度的轨迹片段，第二阶段关联在轨迹片段之间执行，以形成完整的轨迹。对于第一阶段关联，我们使用简单的数据关联策略，通过在分配过程中为匹配成本设置低阈值来生成高纯度的轨迹片段。我们在第二阶段基于消息传递 GNN 的框架进行轨迹片段关联。我们的方法将轨迹片段关联建模为分层图中的一系列边分类问题，可以递归地将短轨迹片段合并成更长的轨迹片段。我们的跟踪器 RTAT 在大多数主要 MOT 指标（HOTA、IDF1 和 AssA）方面在 MOT17 和 MOT20 基准测试集上排名第一。我们在 MOT17 上获得了 67.2 HOTA、84.7 IDF1 和 69.7 AssA，在 MOT20 上获得了 66.2 HOTA、82.5 IDF1 和 68.1 AssA。||
|**2024-08-14**|[Leveraging Perceptual Scores for Dataset Pruning in Computer Vision Tasks](http://arxiv.org/abs/2408.07243)|null|本文提出了一种图像评分方法，用于图像分类和语义分割任务中的核心集选择。该评分是图像的熵，通过其压缩版本的每像素比特数来估算。 因此，该评分是图像固有的，不需要监督或训练。 计算非常简单，并且易于获得，因为所有图像都以压缩格式存储。 我们选择这种评分背后的动机是，文献中提出的大多数其他评分计算成本都很高。更重要的是，我们需要一个能够捕捉图像感知复杂性的评分。熵就是这样一种度量，具有杂乱元素的图像往往具有更高的熵。然而，仅对低熵的标志性图像进行采样会导致学习偏差，并导致当前深度学习模型的整体测试性能下降。为了减轻这种偏差，我们使用了一种基于图的方法来增加所选样本的空间多样性。结果表明，这种简单的评分方法可以产生良好的效果，特别是在语义分割任务中。||
|**2024-08-13**|[Event-Stream Super Resolution using Sigma-Delta Neural Network](http://arxiv.org/abs/2408.06968)|null|本研究介绍了一种基于事件相机捕捉到的亮度变化来增强时间事件像素时空分辨率的新方法。这些相机由于其低分辨率和所收集数据的稀疏、异步性质而提出了独特的挑战。当前的事件超分辨率算法并没有针对事件相机产生的独特数据结构进行全面优化，导致在以改进的计算复杂度捕捉视觉场景的完整动态和细节方面效率低下。为了弥合这一差距，我们的研究提出了一种将二进制脉冲与Sigma Delta神经网络（SDNN）相结合的方法，利用时空约束学习机制来同时学习事件流的空间和时间分布。所提出的网络使用广泛认可的基准数据集进行评估，包括N-MNIST、CIFAR10-DVS、ASL-DVS和Event-NFS。采用了一个全面的评估框架，通过均方根误差（RMSE）评估准确性，并评估我们模型的计算效率。研究结果表明，与现有的最先进方法相比，该方法有了显著的改进，具体来说，该方法在计算效率方面优于最先进的性能，与传统的人工神经网络相比，事件稀疏性提高了17.04倍，突触操作效率提高了32.28倍，同时性能比脉冲神经网络提高了两倍。||
|**2024-08-13**|[Divide and Conquer: Improving Multi-Camera 3D Perception with 2D Semantic-Depth Priors and Input-Dependent Queries](http://arxiv.org/abs/2408.06901)|null|诸如使用多相机图像进行 3D 对象检测和鸟瞰图 (BEV) 分割等 3D 感知任务近来备受关注。尽管准确估计语义和 3D 场景布局对于这项任务至关重要，但现有技术往往忽略了语义和深度线索的协同效应，导致分类和位置估计误差的发生。此外，初始查询的输入独立性也限制了基于 Transformer 模型的学习能力。为了应对这些挑战，我们提出了一个输入感知 Transformer 框架，它利用语义和深度作为先验（命名为 SDTR）。我们的方法涉及使用一个 S-D 编码器，它显式地对语义和深度先验进行建模，从而解耦对象分类和位置估计的学习过程。此外，我们引入了一个先验引导查询构建器，它将语义先验合并到 Transformer 的初始查询中，从而产生更有效的输入感知查询。在 nuScenes 和 Lyft 基准数据集上的大量实验表明，我们的方法在 3D 对象检测和 BEV 分割任务中均达到了最先进的性能。||
|**2024-08-13**|[Efficient Search for Customized Activation Functions with Gradient Descent](http://arxiv.org/abs/2408.06820)|**[link](https://github.com/automl/grafs)**|不同的激活函数对于不同的深度学习模型效果最佳。为了利用这一点，我们利用基于梯度的近期神经架构搜索技术进展，为给定应用高效地识别高性能激活函数。我们提出了一个细粒度搜索单元，它结合了基本的数学运算来对激活函数进行建模，从而可以探索新的激活函数。我们的方法能够识别专门的激活函数，从而在我们尝试的每个模型中都能提高性能，从图像分类到语言模型。此外，识别出的激活函数表现出对相同类型的更大模型以及新数据集的强大可迁移性。重要的是，我们用于创建自定义激活函数的自动化过程比以前的方法效率高出几个数量级。它可以轻松地应用于任意的深度学习管道之上，因此为增强深度学习架构提供了一条很有希望的实践途径。||
|**2024-08-13**|[Integrating Saliency Ranking and Reinforcement Learning for Enhanced Object Detection](http://arxiv.org/abs/2408.06803)|**[link](https://github.com/mbar0075/sarlvision)**|随着目标检测方法的日益多样化，本研究探索了一系列实验，将基于强化学习 (RL) 的视觉注意力方法与显著性排序技术相结合，以研究透明且可持续的解决方案。通过整合显著性排序进行初始边界框预测，并随后应用 RL 技术通过多个时间步长的有限动作集来细化这些预测，本研究旨在提高 RL 目标检测的准确性。本研究以一系列实验的形式呈现，研究了各种图像特征提取方法的使用，并探索了用于深度强化学习定位代理训练的多种深度 Q 网络 (DQN) 架构变体。此外，我们专注于通过优先考虑轻量级和更快的模型来优化每个步骤的检测流程，同时还结合了对检测到的对象进行分类的功能，这是以前的 RL 方法所缺乏的。我们证明，通过使用 Pascal VOC 2007 数据集评估这些经过训练的代理的性能，开发出了更快、更优化的模型。值得注意的是，本研究中获得的最佳平均精度均值 (mAP) 为 51.4，超过了文献中基于 RL 的单目标检测器设定的基准。||
|**2024-08-13**|[Do Vision-Language Foundational models show Robust Visual Perception?](http://arxiv.org/abs/2408.06781)|**[link](https://github.com/shivam-chandhok/cpsc-540-project)**|近年来，视觉语言基础模型的进步使得开发能够执行视觉理解和推理任务的系统成为可能。然而，尚不清楚这些模型是否对分布变化具有鲁棒性，以及它们的性能和泛化能力在数据分布变化下的变化情况。在本项目中，我们努力回答“视觉语言基础模型是否像人类感知一样对分布变化具有鲁棒性？”这个问题。具体来说，我们考虑了各种视觉语言模型，并比较了这些系统的性能如何受到实际场景中常见的基于损坏的分布变化（例如*运动模糊、雾、雪、高斯噪声*）的影响。我们在上述分布变化下，对零样本图像分类任务的泛化能力进行了定性和定量分析。我们的代码将在\url{https://github.com/shivam-chandhok/CPSC-540-Project} 上提供。||
|**2024-08-13**|[Exploring Domain Shift on Radar-Based 3D Object Detection Amidst Diverse Environmental Conditions](http://arxiv.org/abs/2408.06772)|null|深度学习的快速发展及其与自动驾驶系统的集成导致了使用多模态传感器的 3D 感知技术的巨大进步。值得注意的是，在恶劣天气和不同光照条件下，雷达传感器与相机和激光雷达相比表现出更强的鲁棒性。本研究深入探讨了基于 4D 雷达的目标检测中经常被忽视但至关重要的问题——域转移，考察了不同的环境条件（如不同的天气模式和道路类型）如何影响 3D 目标检测性能。我们的研究结果突出了不同天气场景下的明显域转移，揭示了独特的数据集敏感性，这些敏感性强调了雷达点云生成的关键作用。此外，我们还证明，在不同道路类型之间切换，特别是从高速公路切换到城市环境，会引入显著的域转移，这强调了在不同道路环境中进行多样化数据收集的必要性。据我们所知，这是第一个对域转移对基于 4D 雷达的目标检测的影响进行全面分析的研究。我们相信，这项实证研究有助于理解雷达数据中域转移的复杂性，并为面对环境变化的数据收集策略提出了前进方向。||
|**2024-08-13**|[Coherence Awareness in Diffractive Neural Networks](http://arxiv.org/abs/2408.06681)|null|衍射神经网络在需要密集计算处理的应用中具有巨大的潜力。相当多的注意力集中在用于空间相干或空间非相干照明的衍射网络上。在这里，我们说明，与成像系统相反，在衍射网络中，空间相干度具有显著的影响。特别是，我们表明，当物体上的空间相干长度与光学系统保留的最小特征尺寸相当，非相干和相干极端都不能作为可接受的近似值。重要的是，这种情况在涉及主动照明的许多设置中是固有的，包括反射光显微镜、自动驾驶汽车和智能手机。根据这一观察结果，我们提出了一个通用的框架，用于针对任何指定的空间和时间相干度训练衍射网络，支持所有类型的线性和非线性层。使用我们的方法，我们对用于图像分类的网络进行了数值优化，并彻底研究了它们的性能对照明相干特性的依赖性。我们进一步引入了相干盲网络的概念，该网络增强了对照明条件变化的适应能力。我们的研究结果为在现实应用中采用全光神经网络奠定了基础，仅利用自然光。||
|**2024-08-13**|[Unified-IoU: For High-Quality Object Detection](http://arxiv.org/abs/2408.06636)|**[link](https://github.com/lxj-drifter/uiou_files)**|目标检测是计算机视觉领域的重要组成部分，而目标检测的效果直接取决于预测框的回归精度。作为模型训练的关键，IoU（Intersection over Union）极大地显示了当前预测框与真实框之间的差异。随后的研究人员不断地为IoU添加了更多的考虑因素，例如中心距离、纵横比等等。 然而，仅仅细化几何差异是有上限的；而且新的考虑指标与IoU本身之间存在潜在的联系，两者之间的直接加减可能会导致“过度考虑”的问题。基于此，我们提出了一种新的IoU损失函数，称为Unified-IoU（UIoU），它更加关注不同质量预测框之间的权重分配。具体来说，该损失函数以一种新颖的方式将模型的注意力从低质量预测框动态地转移到高质量预测框，以提高模型在高精度或密集数据集上的检测性能，并实现训练速度的平衡。我们提出的方法在多个数据集上都取得了更好的性能，特别是在高IoU阈值下，与其他改进的IoU损失相比，UIoU具有更显著的提升效果。我们的代码已公开在：https://github.com/lxj-drifter/UIOU_files。||
|**2024-08-12**|[Latent Disentanglement for Low Light Image Enhancement](http://arxiv.org/abs/2408.06245)|null|许多基于学习的微光图像增强 (LLIE) 算法都基于 Retinex 理论。然而，此类模型中基于 Retinex 的分解技术会引入噪声，从而限制其增强性能。在本文中，我们提出了一种基于潜在解耦的增强网络 (LDE-Net)，用于微光视觉任务。潜在解耦模块在潜在空间中解耦输入图像，以便在解耦的内容和照度组件中不保留任何噪声。对于 LLIE 任务，我们设计了一个内容感知嵌入 (CAE) 模块，该模块利用内容特征来指导照度组件的增强。对于下游任务（例如夜间无人机跟踪和微光物体检测），我们开发了一种基于潜在解耦框架的有效轻量级增强器。全面的定量和定性实验表明，我们的 LDE-Net 在各种 LLIE 基准测试中均明显优于最先进的方法。此外，将我们的框架应用于下游任务所获得的出色结果也证明了我们潜在解耦设计的有效性。||
|**2024-08-12**|[FruitNeRF: A Unified Neural Radiance Field based Fruit Counting Framework](http://arxiv.org/abs/2408.06190)|**[link](https://github.com/meyerls/fruitnerf)**|我们介绍FruitNeRF，这是一个统一的新型水果计数框架，它利用最先进的视图合成方法直接在3D中对任何类型的水果进行计数。我们的框架获取由单目相机捕获的一组无序姿态图像，并在每张图像中分割水果。为了使我们的系统独立于水果类型，我们采用了一个基础模型，可以为任何水果生成二进制分割掩码。利用RGB和语义这两种模式，我们训练了一个语义神经辐射场。通过对隐式水果场的均匀体积采样，我们获得了仅包含水果的点云。通过对提取的点云应用级联聚类，我们的方法实现了精确的水果计数。使用神经辐射场与传统方法（如目标跟踪或光流）相比具有显著优势，因为计数本身被提升到3D。我们的方法可以防止重复计算水果，并避免计算不相关的水果。我们使用真实世界和合成数据集来评估我们的方法。真实世界的数据集包括三棵手动计数了真实值的苹果树，一个具有一排苹果树和真实水果位置的基准苹果数据集，而合成数据集包含各种水果类型，包括苹果、李子、柠檬、梨、桃和芒果。此外，我们还评估了使用基础模型与U-Net进行水果计数的性能。||
|**2024-08-09**|[DeepInteraction++: Multi-Modality Interaction for Autonomous Driving](http://arxiv.org/abs/2408.05075)|**[link](https://github.com/fudan-zvg/deepinteraction)**|现有的高性能自动驾驶系统通常依赖于多模态融合策略来实现可靠的场景理解。然而，这种设计从根本上受到忽视模态特定优势的限制，最终阻碍了模型性能。为了解决这一限制，在这项工作中，我们引入了一种新的模态交互策略，该策略允许学习和维护各个模态的表示，从而能够在整个感知管道中利用其独特特征。为了证明所提出策略的有效性，我们设计了 DeepInteraction++，这是一个以多模态表征交互编码器和多模态预测交互解码器为特征的多模态交互框架。具体来说，编码器被实现为一个双流 Transformer，具有专门的注意力操作，用于在单独的模态特定表示之间进行信息交换和整合。我们的多模态表征学习结合了以对象为中心的精确基于采样的特征对齐和全局密集信息传播，这对于更具挑战性的规划任务至关重要。解码器旨在通过以统一的模态无关方式交替聚合来自不同表示的信息来迭代地改进预测，从而实现多模态预测交互。大量实验表明，所提出的框架在 3D 对象检测和端到端自动驾驶任务上均具有优越性能。我们的代码可在 https://github.com/fudan-zvg/DeepInteraction 获取。||
|**2024-08-09**|[RadarPillars: Efficient Object Detection from 4D Radar Point Clouds](http://arxiv.org/abs/2408.05020)|null|车载雷达系统已经发展到不仅提供距离、方位角和多普勒速度，还能提供高程数据。这个额外的维度允许将 4D 雷 dargestellt 成 3D 点云。因此，最初为激光雷达数据开发的现有深度学习 3D 目标检测方法经常被应用于这些雷达点云。然而，这忽略了 4D 雷达数据的特殊性，例如极端的稀疏性和速度信息的最佳利用。为了解决现有技术的这些差距，我们提出了 RadarPillars，一种基于支柱的目标检测网络。通过分解径向速度数据、引入 PillarAttention 进行高效的特征提取，以及研究层级缩放以适应雷达稀疏性，RadarPillars 在 View-of-Delft 数据集上的目标检测结果 significantly 优于现有技术。重要的是，这在大大减少参数数量的情况下实现了，在效率方面超过了现有方法，并实现了边缘设备上的实时性能。||
|**2024-08-09**|[LiD-FL: Towards List-Decodable Federated Learning](http://arxiv.org/abs/2408.04963)|null|联邦学习常用于存在许多未经验证的参与者的环境中。因此，对抗攻击下的联邦学习备受关注。本文提出了一个用于列表可解码联邦学习的算法框架，其中中央服务器维护一个模型列表，并保证至少有一个模型性能良好。该框架对诚实工作者的比例没有严格限制，将拜占庭联邦学习的适用性扩展到 adversaries 超过一半的情况。在对损失函数进行适当假设的情况下，我们证明了该方法的收敛性定理。实验结果（包括使用凸损失和非凸损失的图像分类任务）表明，所提出的算法可以 withstand 各种攻击下的恶意多数。||
|**2024-08-09**|[Hyper-YOLO: When Visual Object Detection Meets Hypergraph Computation](http://arxiv.org/abs/2408.04804)|**[link](https://github.com/imoonlab/hyper-yolo)**|我们推出了 Hyper-YOLO，这是一种新的目标检测方法，它集成了超图计算来捕捉视觉特征之间复杂的高阶相关性。传统的 YOLO 模型虽然功能强大，但其颈部设计存在局限性，限制了跨层特征的整合和高阶特征相互关系的利用。为了应对这些挑战，我们提出了超图计算赋能的语义收集和散射 (HGC-SCS) 框架，该框架将视觉特征图转换到语义空间，并构建用于高阶消息传播的超图。这使得模型能够同时获取语义和结构信息，超越了传统的以特征为中心的学习。Hyper-YOLO 在其主干网络中 incorporates 了所提出的混合聚合网络 (MANet)，以增强特征提取，并在其颈部引入了基于超图的跨层级和跨位置表示网络 (HyperC2Net)。HyperC2Net 跨越五个尺度运行，并打破了传统的网格结构，允许跨层级和位置进行复杂的高阶交互。这种组件的协同作用使 Hyper-YOLO 成为各种规模模型中最先进的架构，其在 COCO 数据集上的优越性能证明了这一点。具体来说，Hyper-YOLO-N 明显优于先进的 YOLOv8-N 和 YOLOv9-T， $\text{AP}^{val}$ 分别提高了 12% 和 9%。源代码位于 ttps://github.com/iMoonLab/Hyper-YOLO。||
|**2024-08-08**|[Data-Driven Pixel Control: Challenges and Prospects](http://arxiv.org/abs/2408.04767)|null|近年来，传感器技术的进步带来了像素级的高分辨率和高数据吞吐量。与此同时，越来越大（深层）神经网络（NN）的采用也使得计算机视觉取得了重大进展。目前，视觉智能的实现伴随着越来越高的计算复杂度、能量消耗和延迟。我们研究了一种数据驱动系统，它将像素级的动态传感与视频级的计算机视觉分析相结合，并提出了一个反馈控制回路，在不影响检测和跟踪精度的情况下，最大限度地减少传感器前端和计算后端之间的数据移动。我们的贡献有三方面：（1）我们引入了预期注意力机制，并证明了它可以通过稀疏激活像素实现高精度的预测；（2）利用反馈控制，我们证明了学习到的特征向量的维数可以通过增加稀疏性来显著降低；（3）我们模拟了模拟设计选择（例如不同的RGB或拜耳像素格式和模拟噪声），并研究了它们对数据驱动系统关键指标的影响。与传统的像素和深度学习模型相比，我们的系统显示出显著的性能提升。我们的系统仅激活30%的像素，就能实现10倍的带宽减少和15-30倍的能量延迟积（EDP）改进，而目标检测和跟踪精度只有微小的降低。根据模拟仿真，我们的系统可以实现205兆像素/秒（MP/s）的吞吐量，每MP功耗仅为110毫瓦，即EDP理论上提高了约30倍。||
|**2024-08-08**|[Enhanced Prototypical Part Network (EPPNet) For Explainable Image Classification Via Prototypes](http://arxiv.org/abs/2408.04606)|null|可解释人工智能 (xAI) 有可能增强基于人工智能的系统的透明度和可信度。尽管可以使用深度神经网络 (DNN) 做出准确的预测，但用于得出此类预测的过程通常难以解释。就人类可感知的友好表示而言，例如文本中的词组或图像中的超像素，基于原型的解释可以证明模型决策的合理性。在这项工作中，我们介绍了一种用于图像分类的 DNN 架构，即增强型原型部分网络 (EPPNet)，它在实现强大性能的同时发现了可用于解释分类结果的相关原型。这是通过引入一种新颖的聚类损失来实现的，该损失有助于发现更多相关的、人类可理解的原型。我们还引入了一个置信度得分，以根据发现的原型来评估结果的可解释性。我们的得分不仅考虑了学习到的原型的相关性，还考虑了模型的性能。我们在 CUB-200-2011 数据集上的评估表明，EPPNet 在分类准确性和可解释性方面均优于最先进的基于 xAI 的方法。||
|**2024-08-08**|[SAM2-Adapter: Evaluating & Adapting Segment Anything 2 in Downstream Tasks: Camouflage, Shadow, Medical Image Segmentation, and More](http://arxiv.org/abs/2408.04579)|null|大型模型（也称为基础模型）的出现极大地改变了人工智能研究格局，像 Segment Anything (SAM) 这样的模型在各种图像分割场景中取得了显著成功。尽管取得了进步，但 SAM 在处理一些复杂的低级分割任务（如伪装目标和医学影像）方面遇到了限制。为了应对这些挑战，我们在 2023 年推出了 SAM-Adapter，它在这些挑战性任务上展现出更优的性能。现在，随着架构增强、训练语料库更大的 Segment Anything 2 (SAM2) 的发布，我们重新评估了这些挑战。本文介绍了 SAM2-Adapter，它是第一个旨在克服 SAM2 中观察到的持续限制并在特定下游任务（包括医学图像分割、伪装（隐藏）目标检测和阴影检测）中实现新的最先进 (SOTA) 结果的适配器。SAM2-Adapter 建立在 SAM-Adapter 的优势之上，为各种应用提供了增强的泛化能力和可组合性。我们提供了广泛的实验结果，证明了 SAM2-Adapter 的有效性。我们展示了其潜力，并鼓励研究界利用我们的 SAM2-Adapter 和 SAM2 模型来实现卓越的分割结果。代码、预训练模型和数据处理协议可在 http://tianrun-chen.github.io/SAM-Adaptor/ 获取。||
|**2024-08-07**|[Impact Analysis of Data Drift Towards The Development of Safety-Critical Automotive System](http://arxiv.org/abs/2408.04476)|null|当代自动驾驶汽车研究的重要方向是开发安全关键系统，而最先进的人工智能 (AI) 算法，如计算机视觉 (CV)，可以在其中发挥重要作用。视觉模型在实时检测众多交通标志和障碍物方面具有巨大潜力，这对于避免事故和保护人类生命至关重要。尽管潜力巨大，但如果交通状况随时间推移而发生变化，基于计算机视觉的系统也存在严重的安全问题。本文分析了数据漂移如何影响视觉模型在交通标志检测方面的性能。本研究的新颖之处在于，它提供了一个基于 YOLO 的融合模型，该模型使用来自 CARLA 模拟器的漂移数据进行训练，并在目标检测方面提供了稳健且增强的性能。增强型模型的平均精度为 97.5%，而原始模型的精度为 58.27%。本文详细介绍了原始模型和融合模型的性能评估，有望对安全关键型汽车系统产生重大影响。||
|**2024-08-08**|[Multi-Scale and Detail-Enhanced Segment Anything Model for Salient Object Detection](http://arxiv.org/abs/2408.04326)|**[link](https://github.com/bellybeauty/mdsam)**|显著性目标检测（SOD）旨在识别和分割图像中最突出的目标。先进的SOD方法通常利用各种卷积神经网络（CNN）或Transformer进行深度特征提取。然而，这些方法在复杂情况下仍然表现出较低的性能和较差的泛化能力。最近，Segment Anything Model (SAM) 被提出作为一种视觉基础模型，它具有强大的分割和泛化能力。尽管如此，SAM 需要目标对象的精确提示，而这在 SOD 中是不可用的。此外，SAM 缺乏对多尺度和多层次信息的利用，以及对细粒度细节的整合。为了解决这些缺点，我们提出了一种用于 SOD 的多尺度和细节增强 SAM (MDSAM)。具体来说，我们首先介绍了一种轻量级多尺度适配器 (LMSA)，它允许 SAM 学习多尺度信息，而只需很少的可训练参数。然后，我们提出了一个多级融合模块 (MLFM)，以全面利用 SAM 编码器中的多级信息。最后，我们提出了一个细节增强模块 (DEM)，将 SAM 与细粒度细节相结合。实验结果表明，我们的模型在多个 SOD 数据集上具有优越的性能，并且在其他分割任务上具有很强的泛化能力。源代码发布在 https://github.com/BellyBeauty/MDSAM。||
|**2024-08-08**|[Dual-branch PolSAR Image Classification Based on GraphMAE and Local Feature Extraction](http://arxiv.org/abs/2408.04294)|null|极化合成孔径雷达 (PolSAR) 图像的标注是一个劳动密集型且耗时的过程。因此，使用有限的标签对 PolSAR 图像进行分类是遥感领域的一项具有挑战性的任务。近年来，自监督学习方法已被证明在稀疏标签的 PolSAR 图像分类中非常有效。然而，我们观察到在所研究的任务中缺乏对生成式自监督学习的研究。受此启发，我们在本文中提出了一种基于生成式自监督学习的双分支分类模型。第一个分支是超像素分支，它使用生成式自监督图掩码自动编码器学习超像素级的极化表示。为了获得更精细的分类结果，进一步结合了基于卷积神经网络的像素分支来学习像素级特征。最后使用融合的双分支特征进行分类以获得预测结果。在基准 Flevoland 数据集上的实验结果表明，我们的方法产生了有希望的分类结果。||
|**2024-08-08**|[Efficient Single Image Super-Resolution with Entropy Attention and Receptive Field Augmentation](http://arxiv.org/abs/2408.04158)|null|近年来，基于Transformer的深度模型在单图像超分辨率（SISR）方面取得了显著进展，极大地提升了轻量级SISR任务的性能。然而，由于多头自注意力（MSA）机制的复杂计算，这些模型通常面临着计算负担重和推理速度慢的问题，严重阻碍了它们的实际应用和部署。为了缓解模型效率和SR性能之间的矛盾，本文提出了一种高效的SR模型，称为熵注意力和感受野增强网络（EARFA），该模型由新颖的熵注意力（EA）模块和移位大核注意力（SLKA）模块组成。从信息论的角度来看，EA在高斯分布的条件下增加了中间特征的熵，为后续推理提供了信息更丰富的输入。另一方面，SLKA借助通道移位扩展了SR模型的感受野，这也有利于提高层次特征的多样性。由于EA和SLKA的实现不涉及复杂的计算（例如大量的矩阵乘法），因此所提出的方法能够在保持较好SR性能的同时，实现比基于Transformer的SR模型更快的非线性推理速度。大量实验表明，该模型能够在保证与其他先进模型相当的SR性能的同时，显著降低模型推理延迟。||
|**2024-08-07**|[FMiFood: Multi-modal Contrastive Learning for Food Image Classification](http://arxiv.org/abs/2408.03922)|null|食物图像分类是基于图像的膳食评估的基本步骤，其目的是从进餐场合图像中估计参与者的营养摄入量。食物图像的一个常见挑战是类内多样性和类间相似性，这会严重阻碍分类性能。为了解决这个问题，我们引入了一种新的多模态对比学习框架，称为 FMiFood，它通过整合额外的上下文信息（例如食物类别文本描述）来学习更具辨别力的特征，从而提高分类精度。具体来说，我们提出了一种灵活的匹配技术，改进了文本和图像嵌入之间的相似性匹配，以关注多个关键信息。此外，我们将分类目标纳入框架，并探索使用 GPT-4 来丰富文本描述并提供更详细的上下文。与现有方法相比，我们的方法在 UPMC-101 和 VFN 数据集上均表现出更好的性能。||
|**2024-08-07**|[Vision-Language Guidance for LiDAR-based Unsupervised 3D Object Detection](http://arxiv.org/abs/2408.03790)|**[link](https://github.com/chreisinger/ViLGOD)**|在激光雷达点云中进行精确的三维物体检测对于自动驾驶系统至关重要。为了实现最先进的性能，检测器的监督训练需要大量的人工标注数据，而获取这些数据成本高昂，并且仅限于预定义的物体类别。为了减少人工标注工作，最近的无监督物体检测方法为运动物体生成类别无关的伪标签，随后将其作为监督信号来引导检测器。尽管取得了可喜的结果，但这些方法不能提供类别标签，也不能很好地泛化到静态物体。此外，它们大多局限于包含来自同一场景的多个驾驶数据或来自精确校准和同步的相机设置的图像数据。为了克服这些限制，我们提出了一种视觉语言引导的无监督三维检测方法，该方法仅对激光雷达点云进行操作。我们将 CLIP 知识迁移到静态和移动物体的点簇分类中，我们通过利用激光雷达点云固有的时空信息进行聚类、跟踪以及边界框和标签细化来发现这些物体。我们的方法在 Waymo Open Dataset ( $+23~\text{AP}_{3D}$) 和 Argoverse 2 ($+7.9~\text{AP}_{3D}$ ) 上的性能优于最先进的无监督三维物体检测器，并且提供的类别标签不仅仅基于物体大小假设，标志着该领域的重大进步。||
|**2024-08-07**|[Intuitionistic Fuzzy Cognitive Maps for Interpretable Image Classification](http://arxiv.org/abs/2408.03745)|null|机器学习模型的可解释性至关重要，因为用户可能不愿意依赖其推论。直观模糊C均值聚类算法(iFCM)已被提出作为FCM的扩展，它通过估计犹豫度提供了一种自然机制来评估其输出质量，这一概念类似于人类在决策中的犹豫。为了应对可解释图像分类的挑战，本文介绍了一种名为可解释直观模糊C均值聚类算法(I2FCM)的新框架，该框架独立于领域，易于实现，并且可以应用于卷积神经网络(CNN)模型，使其具有可解释性。据我们所知，这是iFCM首次应用于图像分类。进一步的新贡献包括：专注于信息量最大的图像区域的特征提取过程；用于数据驱动确定iFCM直觉模糊互连的学习算法；基于图像内容的内在可解释分类方法。在图像分类的背景下，犹豫度被认为是图像分类到某个类别的置信度的度量。构建的iFCM模型区分最具代表性的图像语义，并利用因果关系对其进行分析。在公开可用的数据集上评估了所引入框架的有效性，实验结果证实，它可以提供增强的分类性能，同时提供可解释的推论。||
|**2024-08-07**|[CAS-ViT: Convolutional Additive Self-attention Vision Transformers for Efficient Mobile Applications](http://arxiv.org/abs/2408.03703)|**[link](https://github.com/tianfang-zhang/cas-vit)**|视觉Transformer（ViT）凭借其标记混合器强大的全局上下文能力，标志着神经网络的革命性进步。然而，尽管先前的工作已经做出了相当大的努力，但成对标记关联性和复杂的矩阵运算限制了其在资源受限场景和实时应用（例如移动设备）中的部署。在本文中，我们介绍了CAS-ViT：卷积加性自注意力视觉Transformer，以在移动应用中实现效率和性能之间的平衡。首先，我们认为，标记混合器获取全局上下文信息的能力取决于多种信息交互，例如空间域和通道域。随后，我们遵循这种范式构建了一种新颖的加性相似度函数，并提出了一种名为卷积加性标记混合器（CATM）的高效实现。这种简化大大减少了计算开销。我们在各种视觉任务（包括图像分类、目标检测、实例分割和语义分割）中评估了CAS-ViT。我们在GPU、ONNX和iPhone上进行的实验表明，与其他最先进的骨干网络相比，CAS-ViT实现了具有竞争力的性能，使其成为高效移动视觉应用的可行选择。我们的代码和模型可在以下网址获得：\url{https://github.com/Tianfang-Zhang/CAS-ViT}||
|**2024-08-06**|[SGSR: Structure-Guided Multi-Contrast MRI Super-Resolution via Spatio-Frequency Co-Query Attention](http://arxiv.org/abs/2408.03194)|null|磁共振成像 (MRI) 是一种领先的诊断方式，广泛应用于各种检查，通常需要采集多个对比度图像来表征不同的组织。然而，获取高分辨率 MRI 通常会延长扫描时间，这可能会引入运动伪影。因此，MRI 超分辨率成为解决这些挑战的一种很有前景的方法。早期的研究已经探索了使用多对比度进行 MRI 超分辨率 (MCSR)，而大多数研究没有充分利用丰富的对比度不变结构信息。为了充分利用多对比度 MRI 的这种关键先验知识，在这项工作中，我们提出了一种基于新的时空频率联合查询注意力 (CQA) 机制的新型结构引导 MCSR (SGSR) 框架。具体来说，CQA 使用共享的结构查询对多个对比度的特征执行注意力，该查询专门用于从不同对比度中提取、融合和细化共同结构。除了空间域之外，我们还提出了一种新的频域 CQA 模块，以实现更细粒度的结构细化。对 fastMRI 膝部数据和低场脑部 MRI 的广泛实验表明，SGSR 优于最先进的 MCSR 方法，具有统计学意义。||
|**2024-08-06**|[Dual-View Pyramid Pooling in Deep Neural Networks for Improved Medical Image Classification and Confidence Calibration](http://arxiv.org/abs/2408.02906)|null|空间池化（SP）和跨通道池化（CCP）算子分别被应用于深度神经网络（DNN）中聚合特征图的空间特征和像素级特征。它们的主要目标是在不明显削弱DNN性能的情况下减少计算和内存开销。然而，SP经常面临丢失细微特征表示的问题，而CCP很有可能忽略显著特征表示，这可能导致置信度问题校准错误和医学分类结果欠佳。为了解决这些问题，我们提出了一个新颖的双视图框架，首次通过分析空间特征和像素级特征之间的差异来系统地研究SP和CCP的相对作用。基于此框架，我们提出了一种新的池化方法，称为双视图金字塔池化（DVPP），用于聚合多尺度双视图特征。DVPP旨在通过从双轴角度充分利用SP和CCP算子的优点来提高医学图像分类和置信度校准性能。此外，我们还讨论了如何通过五种无参数实现来实现DVPP。在六个2D/3D医学图像分类任务上的大量实验表明，我们的DVPP在不同DNN的医学图像分类结果和置信度校准方面都优于最先进的池化方法。||
|**2024-08-06**|[Diverse Generation while Maintaining Semantic Coordination: A Diffusion-Based Data Augmentation Method for Object Detection](http://arxiv.org/abs/2408.02891)|null|最近的研究强调了数据增强在提高目标检测模型性能方面的关键作用。然而，现有方法往往难以有效地协调数据集多样性和语义协调性。为了弥合这一差距，我们引入了一种创新的增强技术，利用预训练的条件扩散模型来调节这种平衡。我们的方法包括开发类别亲和度矩阵和周围区域对齐策略，前者旨在增强数据集多样性，后者则确保在增强图像中保留语义协调性。广泛的实验评估证实了我们的方法在丰富数据集多样性的同时无缝保持语义协调性方面的有效性。与现有的替代方案相比，我们的方法在三个不同的目标检测模型上分别实现了+1.4AP、+0.9AP和+3.4AP的显著平均改进。||
|**2024-08-05**|[Pre-trained Encoder Inference: Revealing Upstream Encoders In Downstream Machine Learning Services](http://arxiv.org/abs/2408.02814)|**[link](https://github.com/fshp971/encoder-inference)**|虽然预训练编码器可以很容易地在网上获取，以便快速构建下游机器学习 (ML) 服务，但各种攻击也被设计出来，以破坏这些编码器的安全性和隐私性。虽然大多数攻击针对的是上游的编码器，但对于部署在下游机器学习服务中的编码器如何受到威胁，目前尚不清楚。本文揭示了一种新的漏洞：预训练编码器推理 (PEI) 攻击，它对隐藏在下游机器学习服务背后的编码器构成了隐私威胁。PEI 攻击只需提供对目标下游服务的 API 访问权限和一组候选编码器，就可以根据候选编码器推断出目标服务秘密使用了哪个编码器。我们在三个下游任务（图像分类、文本分类和文本到图像生成）上评估了 PEI 攻击针对现实世界编码器的攻击性能。实验表明，PEI 攻击在大多数情况下都能成功揭示隐藏的编码器，即使隐藏的编码器不在候选集中，也很少出错。我们还对最近的视觉语言模型 LLaVA 进行了案例研究，以说明 PEI 攻击有助于其他机器学习攻击，例如对抗性攻击。代码可在 https://github.com/fshp971/encoder-inference 获取。||
|**2024-08-05**|[HQOD: Harmonious Quantization for Object Detection](http://arxiv.org/abs/2408.02561)|**[link](https://github.com/Menace-Dragon/VP-QOD)**|任务不协调问题在现代目标检测器中普遍存在，导致分类和回归任务之间质量不一致。分类得分高但定位位置差的预测框或分类得分低但定位位置准确的预测框会在非极大值抑制后降低检测器的性能。此外，当目标检测器与量化感知训练 (QAT) 结合使用时，我们观察到任务不协调问题会进一步加剧，这被认为是量化检测器性能下降的主要原因之一。为了解决这个问题，我们提出了用于目标检测的协调量化 (HQOD) 框架，该框架包含两个组件。首先，我们提出了一种任务相关损失，以鼓励检测器在 QAT 期间专注于改进任务协调质量较低的样本。其次，结合了协调的交并比 (IoU) 损失，以平衡不同 IoU 级别回归分支的优化。所提出的 HQOD 可以很容易地集成到不同的 QAT 算法和检测器中。值得注意的是，在 MS COCO 数据集上，我们使用 ResNet-50 骨干网络的 4 位 ATSS 实现了 39.6% 的最先进 mAP，甚至超过了全精度模型。||
|**2024-08-05**|[FPT+: A Parameter and Memory Efficient Transfer Learning Method for High-resolution Medical Image Classification](http://arxiv.org/abs/2408.02426)|**[link](https://github.com/yijinhuang/fpt)**|大规模预训练模型的成功使得微调成为在下游任务中实现显著改进的标准方法。然而，微调预训练模型的整个参数集成本高昂。参数高效迁移学习 (PETL) 最近成为将预训练模型适应下游任务的一种经济高效的替代方案。尽管它具有优势，但不断增长的模型规模和输入分辨率给 PETL 带来了挑战，因为训练内存消耗的减少不如参数使用量那样有效。在本文中，我们介绍了细粒度提示调优增强版 (FPT+)，这是一种专为高分辨率医学图像分类设计的 PETL 方法，与其他 PETL 方法相比，它显着减少了内存消耗。FPT+ 通过训练轻量级侧网络并通过细粒度提示和融合模块从大型预训练模型 (LPM) 中访问预训练知识来执行迁移学习。具体来说，我们冻结 LPM 并构建一个可学习的轻量级侧网络。冻结的 LPM 处理高分辨率图像以提取细粒度特征，而侧网络采用相应的下采样低分辨率图像以最大程度地减少内存使用。为了使侧网络能够利用预训练知识，我们提出了细粒度提示和融合模块，它们协作通过 LPM 的中间激活来总结信息。我们在八个不同大小、模态和复杂性的医学图像数据集上评估了 FPT+。实验结果表明，FPT+ 优于其他 PETL 方法，仅使用微调整个 ViT-B 模型所需的可学习参数的 1.03% 和内存的 3.18%。我们的代码可在 https://github.com/YijinHuang/FPT 获取。||
|**2024-08-05**|[Low-Cost Self-Ensembles Based on Multi-Branch Transformation and Grouped Convolution](http://arxiv.org/abs/2408.02307)|**[link](https://github.com/hjdw2/sembg)**|近年来，低成本集成学习的进步已经证明了其在图像分类方面更高的效率。然而，现有的低成本集成方法与传统的集成学习相比，准确率相对较低。在本文中，我们提出了一种新的低成本集成学习方法，可以同时实现高效率和高分类性能。我们将一个CNN转换为多分支结构，无需引入额外的组件，这保持了与原始单一模型相同的计算复杂度，并且还通过分支的不同路径之间的充分分离来增强分支输出之间的多样性。此外，我们提出了一种新的策略，在分支中应用分组卷积，不同分支中的分组数量不同，这增加了分支输出的多样性。对于训练，我们采用知识蒸馏，使用输出的集合作为教师信号。输出之间的高度多样性能够形成一个强大的教师，增强单个分支的分类性能，从而提高整体的集成性能。实验结果表明，与以往的低成本集成方法相比，我们的方法实现了最先进的分类精度和更高的不确定性估计性能。代码可在https://github.com/hjdw2/SEMBG获取。||
|**2024-08-05**|[Network Fission Ensembles for Low-Cost Self-Ensembles](http://arxiv.org/abs/2408.02301)|**[link](https://github.com/hjdw2/nfe)**|最近用于图像分类的集成学习方法已经证明可以在低额外成本的情况下提高分类精度。然而，它们仍然需要多个训练好的模型来进行集成推理，当模型规模增加时，这最终会成为一个沉重的负担。在本文中，我们提出了一种低成本的集成学习和推理方法，称为网络裂变集成（NFE），它通过将传统网络自身转换为多出口结构来实现。从给定的初始网络开始，我们首先修剪一些权重以减少训练负担。然后，我们将剩余的权重分组到几个集合中，并使用每个集合创建多个辅助路径来构建多出口。我们称这个过程为网络裂变。通过这种方式，可以从单个网络获得多个输出，从而实现集成学习。由于此过程只是将现有网络结构更改为多出口结构，而无需使用额外的网络，因此集成学习和推理没有额外的计算负担。此外，通过从所有出口的多个损失中学习，多出口结构通过正则化提高了性能，即使网络稀疏性增加也能实现高性能。通过我们简单而有效的方法，与现有的集成方法相比，我们取得了显著的改进。代码可在https://github.com/hjdw2/NFE获取。||
|**2024-08-05**|[VoxelTrack: Exploring Voxel Representation for 3D Point Cloud Object Tracking](http://arxiv.org/abs/2408.02263)|null|目前基于激光雷达点云的三维单目标跟踪 (SOT) 方法通常依赖于基于点的表示网络。尽管取得了成功，但此类网络存在一些根本问题：1) 它包含池化操作来处理固有的无序点云，阻碍了对跟踪（回归任务）有用的三维空间信息的捕获。 2) 采用的集合抽象操作难以处理密度不一致的点云，也阻碍了三维空间信息的建模。为了解决这些问题，我们引入了一种名为 VoxelTrack 的新型跟踪框架。通过将固有的无序点云体素化为三维体素并通过稀疏卷积块提取其特征，VoxelTrack 可以有效地对精确且鲁棒的三维空间信息进行建模，从而指导对跟踪目标的准确定位预测。此外，VoxelTrack 结合了具有交叉迭代特征融合模块的双流编码器，以进一步探索用于跟踪的细粒度三维空间信息。受益于对精确三维空间信息进行建模，我们的 VoxelTrack 通过单个回归损失简化了跟踪流程。我们在三个广泛使用的数据集（包括 KITTI、NuScenes 和 Waymo Open Dataset）上进行了大量实验。实验结果表明，VoxelTrack 实现了最先进的性能（在三个数据集上分别达到了 88.3%、71.4% 和 63.6% 的平均精度），并且在单个 TITAN RTX GPU 上以 36 Fps 的实时速度优于现有的跟踪器。源代码和模型将发布。||
|**2024-08-04**|[VidModEx: Interpretable and Efficient Black Box Model Extraction for High-Dimensional Spaces](http://arxiv.org/abs/2408.02140)|**[link](https://github.com/vidmodex/vidmodex)**|在黑盒模型提取领域，依赖于软标签或代理数据集的传统方法难以扩展到高维输入空间，并且难以管理大量相互关联的类的复杂性。在这项工作中，我们提出了一种利用 SHAP（Shapley 加性解释）来增强合成数据生成的新方法。SHAP 量化了每个输入特征对受害者模型输出的单独贡献，促进了基于能量的 GAN 向期望输出的优化。这种方法显著提高了性能，实现了图像分类模型精度 16.45% 的提升，并扩展到视频分类模型，在 UCF11、UCF101、Kinetics 400、Kinetics 600 和 Something-Something V2 等具有挑战性的数据集上平均提高了 26.11%，最高提高了 33.36%。我们进一步证明了我们的方法在各种场景下的有效性和实用性，包括可用前 k 个预测概率、前 k 个预测标签和前 1 个标签的情况。||
|**2024-08-02**|[Spatial-Spectral Morphological Mamba for Hyperspectral Image Classification](http://arxiv.org/abs/2408.01372)|null|近年来，Transformer因其自注意力机制在高光谱图像分类（HSIC）领域备受关注，该机制提供了强大的分类性能。然而，这些模型面临着计算效率方面的重大挑战，因为它们的复杂性随序列长度呈二次增长。Mamba架构利用状态空间模型，为Transformer提供了一种更高效的替代方案。本文介绍了空间光谱形态学Mamba（MorpMamba）模型。在MorpMamba模型中，标记生成模块首先将高光谱图像（HSI）块转换为空间光谱标记。然后，这些标记由形态学块处理，该块使用深度可分离卷积运算计算结构和形状信息。提取的信息在特征增强模块中得到增强，该模块根据HSI样本的中心区域调整空间和光谱标记，从而允许在每个块内进行有效的信息融合。随后，在多头自注意力块中对标记进行细化，以进一步改善特征空间。最后，将组合信息输入状态空间块，用于分类和创建地面真实图。在广泛使用的高光谱（HS）数据集上进行的实验表明，MorpMamba模型在参数效率方面优于CNN和Transformer模型。||
|**2024-08-02**|[Underwater Object Detection Enhancement via Channel Stabilization](http://arxiv.org/abs/2408.01293)|**[link](https://github.com/aliman80/Underwater-Object-Detection-via-Channel-Stablization)**|复杂海洋环境加剧了目标检测的挑战。海洋垃圾危及水生生态系统，构成持续挑战。准确检测海洋垃圾对于减轻这种危害至关重要。我们的工作通过增强图像质量和评估检测方法来解决水下目标检测问题。我们使用 Detectron2 的主干网络，结合各种基础模型和配置来完成这项任务。我们提出了一种新颖的通道稳定技术以及简化的图像增强模型，以减少训练图像中的雾度和色偏，从而改进多尺度目标检测。图像处理后，我们测试了不同的 Detectron2 主干网络，以获得最佳检测精度。此外，我们应用了具有增强技术的锐化滤波器来突出显示目标轮廓，以便于识别。结果在 TrashCan 数据集（实例版本和材料版本）上进行了演示。性能最佳的主干网络方法结合了我们的通道稳定和增强技术。我们还将我们的 Detectron2 检测结果与 Deformable Transformer 进行了比较。在 TrashCan 1.0 的实例版本中，与基线相比，我们的方法在小目标的平均精度上实现了 9.53% 的绝对提升，在边界框检测方面实现了 7% 的绝对提升。代码将在以下位置开源：https://github.com/aliman80/Underwater- Object-Detection-via-Channel-Stablization||
|**2024-08-02**|[PGNeXt: High-Resolution Salient Object Detection via Pyramid Grafting Network](http://arxiv.org/abs/2408.01137)|null|我们从数据集和网络框架两个角度对更具挑战性的高分辨率显著性目标检测（HRSOD）进行了深入研究。为了弥补HRSOD数据集的不足，我们精心收集了一个大规模高分辨率显著性目标检测数据集，称为UHRSD，其中包含5920张来自现实复杂场景的4K-8K分辨率图像。所有图像均以像素级精细标注，远远超过以往的低分辨率SOD数据集。为了克服以往方法中采样深度与感受野大小之间的矛盾，我们提出了一种新的基于金字塔嫁接机制的HR-SOD任务单阶段框架。总的来说，采用基于Transformer和基于CNN的主干网络分别从不同分辨率的图像中提取特征，然后将这些特征从Transformer分支嫁接到CNN分支。提出了一种基于注意力的跨模型嫁接模块（CMGM），使CNN分支能够在解码过程中以不同源特征为指导，更全面地结合破碎的细节信息。此外，我们设计了一种注意力引导损失（AGL），以明确监督CMGM生成的注意力矩阵，帮助网络更好地与来自不同分支的注意力进行交互。在UHRSD和广泛使用的SOD数据集上的综合实验表明，我们的方法可以同时定位显著性目标并保留丰富的细节，优于最先进的方法。为了验证所提出框架的泛化能力，我们将其应用于伪装目标检测（COD）任务。值得注意的是，我们的方法在没有额外技巧的情况下优于大多数最先进的COD方法。||
|**2024-08-02**|[Effect of Fog Particle Size Distribution on 3D Object Detection Under Adverse Weather Conditions](http://arxiv.org/abs/2408.01085)|null|基于光谱信号的激光雷达传感器在为自动驾驶汽车系统提供有关目标物体的关键信息方面发挥着至关重要的作用。然而，大气中雾的存在会严重降低整个系统的性能。本论文分析了雾粒子尺寸分布在恶劣天气条件下对三维物体检测的作用。我们利用米氏理论和气象光学距离 (MOR) 来计算点云生成的衰减和后向散射系数值，并分析了在容易、中等和困难三种检测难度下，汽车、骑车人和行人案例中系统的整体精度。采用伽马和 Junge（幂律）分布对强对流雾和中等对流雾环境下的雾粒子尺寸分布进行数学建模。随后，我们根据后向散射系数值修改了 KITTI 数据集，并在 PV-RCNN++ 深层神经网络模型上对其进行了训练，用于在不同检测难度下的汽车、骑车人和行人案例。结果分析表明，系统的精度随目标物体维度的变化、雾环境的性质和检测难度的增加而发生显著变化，其中汽车的精度最高，约为 99%，行人的精度最低，约为 73%。||
|**2024-08-02**|[PINNs for Medical Image Analysis: A Survey](http://arxiv.org/abs/2408.01026)|null|将物理信息纳入机器学习框架正在改变医学图像分析 (MIA)。通过整合基础知识和控制物理定律，这些模型实现了增强的鲁棒性和可解释性。在这项工作中，我们探索了基于物理的方法在 MIA (PIMIA) 任务中的实用性，例如配准、生成、分类和重建。我们对 80 多篇关于专门用于 MIA 的基于物理的方法的论文进行了系统的文献回顾。我们提出了一个统一的分类法，以研究哪些物理知识和过程被建模，它们是如何表示的，以及将它们纳入 MIA 模型的策略。我们深入研究了广泛的图像分析任务，从成像、生成、预测、逆成像（超分辨率和重建）、配准到图像分析（分割和分类）。对于每个任务，我们都以表格的形式彻底检查并呈现了以物理为指导的核心操作、感兴趣区域（关于人体解剖学）、相应的成像方式、用于模型训练的数据集、采用的深度网络架构以及所利用的主要物理过程、方程式或原理。此外，我们还引入了一种新的度量标准，用于比较 PIMIA 方法在不同任务和数据集上的性能。基于本次综述，我们总结并提炼了我们对挑战、开放研究问题和未来研究方向的看法。我们重点介绍了 PIMIA 中的主要开放挑战，包括选择合适的物理先验和建立标准化的基准测试平台。||
|**2024-08-02**|[Visible-Thermal Multiple Object Tracking: Large-scale Video Dataset and Progressive Fusion Approach](http://arxiv.org/abs/2408.00969)|**[link](https://github.com/wqw123wqw/pftrack)**|可见光和热红外数据的互补优势在各种计算机视觉任务中得到了广泛应用，例如视觉跟踪、语义分割和目标检测，但在多目标跟踪（MOT）中却很少被探索。在这项工作中，我们贡献了一个用于 MOT 的大规模可见光-热红外视频基准，称为 VT-MOT。VT-MOT 具有以下主要优势。1）数据规模大，多样性高。VT-MOT 包含 582 个视频序列对，401k 帧对，来自监控、无人机和手持平台。2）跨模态对齐精度高。我们邀请了几位专业人士逐帧进行空间和时间对齐。3）标注密集且质量高。VT-MOT 拥有 399 万个由专业人员标注和双重检查的标注框，包括严重遮挡和目标重新获取（目标消失和重新出现）的挑战。为了提供一个强大的基线，我们设计了一个简单而有效的跟踪框架，该框架以渐进的方式有效地融合了两种模态的时间信息和互补信息，以实现鲁棒的可见光-热红外 MOT。我们在 VT-MOT 上进行了全面的实验，结果证明了该方法与现有技术的优越性和有效性。从评估结果和分析中，我们为可见光-热红外 MOT 确定了几个潜在的未来方向。该项目已发布在 https://github.com/wqw123wqw/PFTrack。||
|**2024-08-01**|[Joint Neural Networks for One-shot Object Recognition and Detection](http://arxiv.org/abs/2408.00701)|null|本文提出了一种新颖的联合神经网络方法来解决具有挑战性的一次性目标识别和检测任务。受孪生神经网络和最先进的多框检测方法的启发，联合神经网络能够对训练过程中未见过的类别执行目标识别和检测。遵循一次性目标识别/检测约束，训练和测试数据集不包含重叠类别，换句话说，所有测试类别在训练期间都未见过。联合网络架构能够通过查询和目标输入的堆叠卷积层有效地比较图像对，识别相同输入查询类别的模式，而无需依赖于围绕该类别的先前训练。所提出的方法在 MiniImageNet 数据集上实现了一次性目标识别的 61.41% 准确率，在 COCO 数据集上训练并在 Pascal VOC 数据集上测试时，实现了一次性目标检测的 47.1% mAP。代码可在 https://github.com/cjvargasc/JNN recog 和 https://github.com/cjvargasc/JNN detection/ 获取。||
|**2024-08-01**|[Harnessing Uncertainty-aware Bounding Boxes for Unsupervised 3D Object Detection](http://arxiv.org/abs/2408.00619)|null|无监督三维目标检测旨在从未标记的原始数据（如激光雷达点云）中识别感兴趣的目标。现有方法通常采用聚类算法生成的伪三维边界框（3D bboxes）来初始化模型训练，然后迭代地更新伪标签和训练模型。然而，伪边界框不可避免地包含噪声，并且这种不准确的标注会累积到最终模型中，从而影响性能。因此，为了减轻伪边界框的负面影响，我们引入了一种新的不确定性感知框架。具体而言，我们的方法包含两个主要部分：不确定性估计和不确定性正则化。(1) 在不确定性估计阶段，我们在主检测器旁边加入了一个额外的辅助检测分支。利用主检测器和辅助检测器之间的预测差异来估计边界框坐标级别的不确定性，包括位置、形状和方向。(2) 基于评估的不确定性，我们通过自适应地调整每个三维边界框坐标来正则化模型训练。对于具有高不确定性的伪边界框坐标，我们分配相对较低的损失权重。实验验证了所提出的方法对噪声伪边界框具有鲁棒性，与现有技术相比，在nuScenes和Lyft数据集上取得了显著的改进，在nuScenes数据集上AP $_{BEV}$提升了6.9%，AP$_{3D}$提升了2.5%，在Lyft数据集上AP$_{BEV}$提升了2.2%，AP$_{3D}$ 提升了1.0%。||
|**2024-08-01**|[U2UData: A Large-scale Cooperative Perception Dataset for Swarm UAVs Autonomous Flight](http://arxiv.org/abs/2408.00606)|null|现代无人机感知系统对遮挡敏感，且远程能力有限，这是提高低空经济任务性能的关键瓶颈。最近的研究表明，无人机间 (U2U) 协同感知系统具有彻底改变自动驾驶行业的巨大潜力。然而，缺乏大规模数据集阻碍了该领域的进展。本文介绍了 U2UData，这是第一个用于集群无人机自主飞行的协同感知大规模数据集。该数据集由在 U2USim 中自主飞行的三架无人机收集，覆盖了 9 平方公里的飞行区域。它包含 315,000 个激光雷达帧、945,000 个 RGB 和深度帧，以及 241 万个针对 3 个类别的带注释的 3D 边界框。它还包括覆盖所有飞行路线的亮度、温度、湿度、烟雾和气流值。U2USim 是第一个真实世界映射的集群无人机仿真环境。它以云南省为原型，包括 4 种地形、7 种天气条件和 8 种传感器类型。U2UData 引入了两个感知任务：协同 3D 对象检测和协同 3D 对象跟踪。本文提供了最近协同感知算法在这些任务上的综合基准。||
|**2024-08-01**|[MUFASA: Multi-View Fusion and Adaptation Network with Spatial Awareness for Radar Object Detection](http://arxiv.org/abs/2408.00565)|null|近年来，基于雷达目标检测的方法由于在恶劣天气下比激光雷达更稳健，在自动驾驶系统中取得了重大进展。然而，雷达点云的稀疏性对实现精确的目标检测提出了挑战，突出了有效和全面的特征提取技术的重要性。为了应对这一挑战，本文介绍了一种针对雷达点云的综合特征提取方法。本研究首先使用即插即用模块 GeoSPA 增强了检测网络的能力。它利用 Lalonde 特征来探索局部几何图案。此外，还设计了一种分布式多视图注意力机制 DEMVA，用于将整个数据集的共享信息与每个单独帧的全局信息相结合。通过采用这两个模块，我们提出了我们的方法 MUFASA，该方法通过改进特征提取来提高目标检测性能。该方法在 VoD 和 TJ4DRaDSet 数据集上进行了评估，以证明其有效性。特别是，我们在 VoD 数据集上取得了基于雷达方法中最先进的结果，mAP 为 50.24%。||
|**2024-07-31**|[Spatial Transformer Network YOLO Model for Agricultural Object Detection](http://arxiv.org/abs/2407.21652)|null|目标检测通过自主识别和定位感兴趣的目标，在计算机视觉领域发挥着至关重要的作用。You Only Look Once (YOLO) 模型是一种有效的单阶段检测器。然而，YOLO 在杂乱或部分遮挡的场景中面临挑战，并且难以处理小的、低对比度的目标。我们提出了一种将空间变换网络 (STN) 集成到 YOLO 中以提高性能的新方法。所提出的 STN-YOLO 旨在通过关注图像的重要区域并在检测过程之前提高模型的空间不变性来增强模型的有效性。我们提出的方法在定性和定量上都提高了目标检测性能。我们探讨了 STN 模块内不同定位网络的影响以及模型在不同空间变换中的鲁棒性。我们将 STN-YOLO 应用于农业目标检测的基准数据集以及来自最先进的植物表型温室设施的新数据集。我们的代码和数据集是公开可用的。||
|**2024-07-31**|[Evaluating SAM2's Role in Camouflaged Object Detection: From SAM to SAM2](http://arxiv.org/abs/2407.21596)|null|Meta AI研究院推出的通用对象分割模型Segment Anything Model (SAM)一经问世便受到了广泛关注，并对学术界产生了重大影响。为了将其应用扩展到视频领域，Meta进一步开发了Segment Anything Model 2 (SAM2)，这是一个能够进行视频和图像分割的统一模型。SAM2在适用领域、可提示分割精度和运行速度方面均优于其前身。然而，本报告揭示了SAM2在自动模式下无需提示即可感知图像中不同对象的能力相比SAM有所下降。具体而言，我们采用具有挑战性的伪装目标检测任务来评估这种性能下降，希望能够激励研究人员进一步探索SAM模型家族。本文的结果可在\url{https://github.com/luckybird1994/SAMCOD}获取。||
|**2024-07-31**|[InScope: A New Real-world 3D Infrastructure-side Collaborative Perception Dataset for Open Traffic Scenarios](http://arxiv.org/abs/2407.21581)|null|自动驾驶汽车的感知系统容易受到遮挡的影响，特别是从以车辆为中心的视角来看。这种遮挡会导致物体检测被遗漏，例如，较大的车辆（如卡车或公共汽车）可能会造成盲点，从而遮挡骑自行车的人或行人，加剧了与此类感知系统局限性相关的安全问题。为了应对这些挑战，车联网 (V2X) 范式建议采用基础设施侧感知系统 (IPS) 来补充自动驾驶汽车，以提供更广泛的感知范围。然而，现实世界中三维基础设施侧数据集的缺乏限制了 V2X 技术的进步。为了弥合这些差距，本文介绍了一个新的三维基础设施侧协同感知数据集，简称 inscope。值得注意的是，InScope 是第一个致力于通过在基础设施侧战略性地部署多位置激光雷达 (LiDAR) 系统来解决遮挡挑战的数据集。具体而言，InScope 包含 20 天的捕获时长，其中包含 303 条跟踪轨迹和 187,787 个由专家标注的三维边界框。通过基准测试分析，针对开放交通场景提出了四种不同的基准测试，包括协同三维目标检测、多源数据融合、数据域迁移和三维多目标跟踪任务。此外，还设计了一个新的指标来量化遮挡的影响，以便于评估各种算法之间的检测退化率。实验结果表明，利用 InScope 协助在现实世界场景中检测和跟踪三维多目标物体（尤其是在跟踪被遮挡、小型和远处物体方面）的性能有所提高。数据集和基准测试可在 https://github.com/xf-zh/InScope 获取。||
|**2024-07-31**|[Accelerating Image Super-Resolution Networks with Pixel-Level Classification](http://arxiv.org/abs/2407.21448)|null|近年来，对有效的超分辨率 (SR) 技术的需求激增，特别是对于分辨率范围为 2K 到 8K 的大尺寸图像。对于基于深度神经网络 (DNN) 的单图像超分辨率 (SISR)，由于计算限制，通常需要将图像分解成重叠的图像块。在这种图像块分解方案中，可以根据每个图像块的难度分配不同的计算资源，以便在保持超分辨率性能的同时进一步提高效率。然而，这种方法有一个局限性：计算资源在一个图像块内是均匀分配的，当图像块包含不同恢复难度级别的像素时，会导致效率降低。为了解决这个问题，我们提出了用于单图像超分辨率的像素级分类器 (PCSR)，这是一种在像素级自适应分配计算资源的新方法。PCSR 模型包含一个骨干网络、一个像素级分类器和一组具有不同容量的像素级上采样器。像素级分类器根据每个像素的恢复难度将其分配给适当的上采样器，从而优化计算资源的使用。我们的方法允许在推理过程中平衡性能和计算成本，而无需重新训练。我们的实验表明，在不同骨干网络模型和基准测试中，PCSR 在 PSNR-FLOP 权衡方面优于现有的图像块分配方法。代码可在 https://github.com/3587jjh/PCSR 获取。||
|**2024-07-30**|[Faithful and Plausible Natural Language Explanations for Image Classification: A Pipeline Approach](http://arxiv.org/abs/2407.20899)|null|现有的图像分类解释方法难以提供忠实且合理的解释。本文提出了一种事后自然语言解释方法来解决这个问题，该方法可以应用于任何基于 CNN 的分类器，而无需改变其训练过程或影响预测性能。通过分析有影响力的神经元和相应的激活图，该方法以结构化语义表示的形式生成对分类器决策过程的忠实描述，然后由语言模型将其转换为文本。通过这种流水线方法，生成的解释基于神经网络架构，在提供对分类过程的准确洞察力的同时，非专业人士也能理解。实验结果表明，用我们的方法构建的自然语言解释在合理性和忠实度方面明显优于其他方法。特别是，用户对神经网络结构的干预（神经元屏蔽）的有效性是基线的 3 倍。||
|**2024-07-30**|[What is YOLOv5: A deep look into the internal features of the popular object detector](http://arxiv.org/abs/2407.20892)|null|本研究全面分析了 YOLOv5 对象检测模型，考察了其架构、训练方法和性能。文中详细探讨了关键组件，包括跨阶段局部骨干网络和路径聚合网络。论文回顾了该模型在各种指标和硬件平台上的性能。此外，研究还讨论了从 Darknet 到 PyTorch 的转变及其对模型开发的影响。总的来说，这项研究深入了解了 YOLOv5 的能力及其在更广泛的目标检测领域中的地位，以及为什么它是受限边缘部署场景的流行选择。||
|**2024-07-31**|[Integer-Valued Training and Spike-Driven Inference Spiking Neural Network for High-performance and Energy-efficient Object Detection](http://arxiv.org/abs/2407.20708)|**[link](https://github.com/biclab/spikeyolo)**|受大脑启发的脉冲神经网络 (SNN) 与人工神经网络 (ANN) 相比，具有生物可解释性和低功耗的优势。由于性能不佳，SNN 的应用目前仅限于简单的分类任务。本研究致力于弥合 SNN 和 ANN 在目标检测性能上的差距。我们的设计围绕网络架构和脉冲神经元展开。首先，当将 YOLO 系列转换为相应的脉冲版本时，过于复杂的模块设计会导致脉冲退化。我们设计了一种 SpikeYOLO 架构来解决这个问题，方法是简化原始 YOLO 并结合元 SNN 模块。其次，目标检测对将膜电位转换为二进制脉冲时的量化误差更为敏感。为了应对这一挑战，我们设计了一种新的脉冲神经元，它在训练期间激活整数值，同时通过在推理期间扩展虚拟时间步长来保持脉冲驱动。所提出的方法在静态和神经形态目标检测数据集上均得到了验证。在静态 COCO 数据集上，我们获得了 66.2% 的 mAP@50 和 48.9% 的 mAP@50:95，分别比之前的最先进 SNN 高出 +15.0% 和 +18.7%。在神经形态 Gen1 数据集上，我们实现了 67.2% 的 mAP@50，比架构相同的 ANN 高出 +2.5%，并且能效提高了 5.7 倍。代码：https://github.com/BICLab/SpikeYOLO||
|**2024-08-01**|[The Susceptibility of Example-Based Explainability Methods to Class Outliers](http://arxiv.org/abs/2407.20678)|null|本研究探讨了类别异常值对黑盒机器学习模型的基于实例的可解释性方法有效性的影响。我们重新定义了现有的可解释性评估指标，例如针对基于实例方法的正确性和相关性，并引入了一个新的指标：可区分性。我们使用这些指标突出了当前基于实例的可解释性方法的缺点，包括那些试图抑制类别异常值的方法。我们对两个数据集（一个文本分类数据集和一个图像分类数据集）进行了实验，并评估了四种最先进的可解释性方法的性能。我们的研究结果强调需要强大的技术来应对类别异常值带来的挑战。||
|**2024-07-30**|[Efficient Channel Estimation for Millimeter Wave and Terahertz Systems Enabled by Integrated Super-resolution Sensing and Communication](http://arxiv.org/abs/2407.20607)|null|集成超分辨率感知与通信 (ISSAC) 已成为一种很有前景的技术，可以实现对关键参数（例如感知目标的角度）的超高精度感知。在本文中，我们针对采用混合模拟/数字波束赋形架构的毫米波 (mmWave) 和太赫兹 (THz) 系统，提出了一种由 ISSAC 支持的高效信道估计方案，该方案可显著降低导频开销和射频 (RF) 链路的成本。其关键思想是利用基于子空间的超分辨率算法（例如多信号分类 (MUSIC)）可以在不需要专用先验已知导频的情况下准确估计信道参数这一事实。具体来说，所提出的方法包括两个阶段。首先，在数据符号传输期间，以无导频的方式估计多径信道分量的角度。其次，使用极少的导频来估计多径信道系数。与完全依赖于信道训练的传统信道估计方案相比，我们的方法在第二阶段需要估计的参数要少得多。此外，获得信道多径角度后，当发送导频以估计信道路径增益时，就可以实现波束赋形增益。为了全面研究所提出方案的性能，我们考虑了基本的视距 (LoS) 信道和更一般的多径信道。我们将我们提出的方案的信道估计的最小均方误差 (MMSE) 和由此产生的波束赋形增益与完全依赖于信道训练的传统方案的性能进行了比较。结果表明，我们提出的方法明显优于基准方案。仿真结果验证了我们的理论发现。||
|**2024-07-30**|[Knowledge Fused Recognition: Fusing Hierarchical Knowledge for Image Recognition through Quantitative Relativity Modeling and Deep Metric Learning](http://arxiv.org/abs/2407.20600)|null|图像识别是深度度量学习的重要基线。图像类别层次知识描述了类间的相似性或不相似性。如何有效融合图像类别层次知识以增强图像识别仍然是一个具有挑战性的课题。在本文中，我们提出了一种新的基于深度度量学习的方法，以端到端的监督回归方式有效地融合图像类别的层次先验知识并提高图像识别性能。现有的结合图像分类的深度度量学习主要利用图像类别之间的定性相对性，即采样图像是否来自同一类别。本文还提出了一种新的三元组损失函数项，它利用了量化相对性，并将模型潜在空间中的距离与知识空间中的距离对齐，并将其纳入所提出的双模态融合方法中。实验结果表明，所提出的方法在 CIFAR-10、CIFAR-100、Mini-ImageNet 和 ImageNet-1K 数据集上提高了图像识别性能，并优于基线方法和现有方法。||
|**2024-07-30**|[Exploring Liquid Neural Networks on Loihi-2](http://arxiv.org/abs/2407.20590)|null|本研究探讨了液体神经网络 (LNN) 领域及其在神经形态硬件平台上的部署。它深入分析了液体状态机 (LSM)，并探讨了 LNN 架构对神经形态系统的适应性，重点介绍了理论基础和实际应用。我们介绍了一种在 CIFAR-10 数据集上进行图像分类的开创性方法，该方法是在最先进的神经形态硬件平台上实现液体神经网络 (LNN)。我们基于 Loihi-2 ASIC 的架构展现出卓越的性能，实现了 91.3% 的出色准确率，同时每帧仅消耗 213 微焦耳。这些结果强调了 LNN 在推进神经形态计算方面的巨大潜力，并在效率和准确性方面为该领域树立了新的标杆。||
|**2024-07-29**|[Deep Learning for Super-resolution Ultrasound Imaging with Spatiotemporal Data](http://arxiv.org/abs/2407.20407)|null|超分辨率超声成像 (SRUS) 是一个活跃的研究领域，因为它可以将微血管结构的分辨率提高十倍。SRUS 在临床上应用的限制包括采集时间长和图像处理时间长。深度学习方法可以缓解这些限制，以处理 SRUS 图像。在本研究中，我们提出了一种基于 ConvNeXt 架构对卷积神经网络进行现代改进的优化架构，并进一步定制了特征选择，以提高单个网络内 MB 检测和定位的特定任务的性能。我们采用最多五个连续图像帧的时空输入来增加检测到的 MB 数量。输出结构产生三种分类：中心图像帧中每个像素的 MB 检测布尔值，以及每个检测到的 MB 在 4 倍亚像素分辨率下的 x 和 z 偏移量。超声模拟基于 L22-14v 换能器（Verasonics）生成图像，用于所提出的 SRUS-ConvNeXt 网络的训练和测试。小鼠大脑的体内图像数据被用作架构的进一步验证。当配置为 3 帧时空输入时，所提出的网络在 F1 分数衡量的性能最高。当网络配置为单个输入帧时，实现了 {\lambda}/22 的最小定位误差。所提出的架构的灵活性允许扩展到 10 倍的 SRUS 图像放大，与典型的 U-Net 样式方法相比，对参数数量和后续推理时间增加的影响要小得多。该网络在开发用于实时图像形成的 SRUS 深度网络架构方面很有前景。||
|**2024-07-29**|[Diffusion Feedback Helps CLIP See Better](http://arxiv.org/abs/2407.20171)|**[link](https://github.com/baaivision/diva)**|对比语言-图像预训练模型 (CLIP) 擅长跨领域和模态抽象开放世界表示，已成为各种视觉和多模态任务的基础。然而，最近的研究表明，CLIP 存在严重的视觉缺陷，例如难以区分方向、数量、颜色、结构等。这些视觉缺陷也限制了建立在 CLIP 之上的多模态大型语言模型 (MLLM) 的感知能力。主要原因可能是用于训练 CLIP 的图像-文本对存在固有偏差，因为文本缺乏独特性且图像缺乏多样性。在这项工作中，我们为 CLIP 模型提出了一种简单的后训练方法，该方法主要通过自监督扩散过程来克服其视觉缺陷。我们介绍了 DIVA，它使用扩散模型作为 CLIP 的视觉助手。具体来说，DIVA 利用文本到图像扩散模型的生成反馈来优化 CLIP 表示，仅使用图像（没有相应的文本）。我们证明了 DIVA 在很大程度上提高了 CLIP 在具有挑战性的 MMVP-VLM 基准测试中的性能，该基准测试评估了细粒度的视觉能力（例如，3-7%），并增强了 MLLM 和视觉模型在多模态理解和分割任务上的性能。对 29 个图像分类和检索基准的广泛评估证实，我们的框架保留了 CLIP 强大的零样本能力。代码将在 https://github.com/baaivision/DIVA 上提供。||
|**2024-07-30**|[Efficient Face Super-Resolution via Wavelet-based Feature Enhancement Network](http://arxiv.org/abs/2407.19768)|**[link](https://github.com/pris-cv/wfen)**|人脸超分辨率旨在从低分辨率人脸图像重建高分辨率人脸图像。 以前的方法通常采用编码器-解码器结构来提取面部结构特征，其中直接下采样不可避免地会引入失真，尤其是对边缘等高频特征的失真。 为了解决这个问题，我们提出了一种基于小波的特征增强网络，它通过使用小波变换将输入特征无损地分解为高频和低频分量并分别处理它们来减轻特征失真。 为了提高面部特征提取的效率，进一步提出了全域 Transformer 来增强局部、区域和全局面部特征。 这些设计使我们的方法无需像以前的方法那样堆叠许多模块即可获得更好的性能。 实验表明，我们的方法有效地平衡了性能、模型大小和速度。 代码链接：https://github.com/PRIS-CV/WFEN。||
|**2024-07-26**|[Unifying Visual and Semantic Feature Spaces with Diffusion Models for Enhanced Cross-Modal Alignment](http://arxiv.org/abs/2407.18854)|null|图像分类模型在实际应用中经常表现出不稳定的性能，这是因为图像信息的变化，以及主体对象的视觉视角和照明差异所致。为了缓解这些挑战，现有研究通常会结合与视觉数据相匹配的附加模态信息来规范模型的学习过程，从而能够从复杂的图像区域中提取高质量的视觉特征。具体而言，在多模态学习领域，跨模态对齐被认为是一种有效的策略，它通过学习视觉和语义特征的域一致潜在特征空间来协调不同的模态信息。然而，由于多模态信息之间的异质性，例如特征分布和结构的差异，这种方法可能会面临局限性。为了解决这个问题，我们引入了一种多模态对齐和重建网络（MARNet），旨在增强模型对视觉噪声的抵抗力。重要的是，MARNet 包括一个跨模态扩散重建模块，用于平滑稳定地融合不同域的信息。在 Vireo-Food172 和 Ingredient-101 这两个基准数据集上进行的实验表明，MARNet 有效地提高了模型提取的图像信息的质量。它是一个即插即用的框架，可以快速集成到各种图像分类框架中，从而提高模型性能。||
|**2024-07-26**|[Local Binary Pattern(LBP) Optimization for Feature Extraction](http://arxiv.org/abs/2407.18665)|null|图像数据的快速增长导致了先进的图像处理和计算机视觉技术的发展，这些技术在图像分类、图像分割和模式识别等各种应用中至关重要。纹理是在许多图像处理任务中广泛使用的一个重要特征。因此，分析和理解纹理在图像分析和理解中起着至关重要的作用。局部二值模式（LBP）是一种强大的算子，它描述了图像的局部纹理特征。本文通过将算子分成三个矩阵，提出了一种新的LBP数学表示方法，其中两个矩阵始终是固定的，不依赖于输入数据。对这些固定矩阵进行了深入分析，并提出了一种新的算法来优化它们，以提高分类性能。优化过程基于奇异值分解（SVD）算法。因此，作者提出了最佳的LBP，可以有效地描述人脸图像的纹理。本文提供的几个实验结果令人信服地验证了优化后的LBP在人脸检测和面部表情识别任务中的效率和优越性。||
|**2024-07-26**|[Topology Optimization of Random Memristors for Input-Aware Dynamic SNN](http://arxiv.org/abs/2407.18625)|null|机器学习领域正经历着前所未有的发展，近期出现的例如大型语言模型和世界模拟器等人工智能模型就是最好的证明，这些模型是运行在数字计算机上的人工神经网络。然而，由于信号表示、优化、运行时可重构性和硬件架构的差异，它们在能效和对不同难度输入的流线型适应性方面仍然无法与人脑相媲美。为了应对这些基本挑战，我们引入了用于输入感知动态忆阻器脉冲神经网络（PRIME）的剪枝优化。在信号表示方面，PRIME采用泄漏积分放电神经元来模拟大脑固有的脉冲机制。PRIME从大脑的结构可塑性中汲取灵感，优化了随机忆阻器脉冲神经网络的拓扑结构，而无需进行昂贵的忆阻器电导微调。在运行时可重构性方面，受大脑计算深度动态调整的启发，PRIME采用输入感知的动态提前停止策略来最大程度地减少推理过程中的延迟，从而在不牺牲性能的情况下提高能效。在架构方面，PRIME利用了忆阻器内存计算， mirroring the brain and mitigating the von Neumann bottleneck。我们使用基于40纳米256 Kb忆阻器的内存计算宏，在神经形态图像分类和图像修复任务上验证了我们的系统。我们的结果表明，分类精度和Inception分数与软件基线相当，同时能效最高提升了62.50倍，计算负载最多节省了77.0%。该系统还表现出对模拟忆阻器随机突触噪声的鲁棒性。我们软硬件协同设计的模型为未来具有类脑能效和适应性的类脑神经形态计算铺平了道路。||
|**2024-07-26**|[VSSD: Vision Mamba with Non-Casual State Space Duality](http://arxiv.org/abs/2407.18559)|**[link](https://github.com/yuhengsss/vssd)**|视觉Transformer极大地推动了计算机视觉领域的发展，提供了强大的建模能力和全局感受野。然而，其高计算量限制了其在处理长序列时的适用性。为了解决这个问题，状态空间模型（SSM）因其线性计算复杂度而在视觉任务中受到关注。最近，Mamba2中引入了一种改进的SSM变体——状态空间对偶性（SSD），以提高模型性能和效率。然而，SSD/SSM固有的因果性质限制了它们在非因果视觉任务中的应用。为了解决这一限制，我们引入了视觉状态空间对偶性（VSSD）模型，它是一种非因果格式的SSD。具体来说，我们建议丢弃隐藏状态和标记之间交互的大小，同时保留它们的相对权重，这减轻了标记贡献对先前标记的依赖性。结合多扫描策略，我们证明了扫描结果可以整合以实现非因果关系，这不仅提高了SSD在视觉任务中的性能，还提高了其效率。我们在图像分类、检测和分割等各种基准测试中进行了广泛的实验，结果表明VSSD优于现有的基于SSM的模型。代码和权重可在\url{https://github.com/YuHengsss/VSSD}获取。||
|**2024-07-25**|[LION: Linear Group RNN for 3D Object Detection in Point Clouds](http://arxiv.org/abs/2407.18232)|**[link](https://github.com/happinesslz/LION)**|Transformer 在大规模三维点云感知任务（如三维目标检测）中的优势，受限于其在建模长距离关系时的二次计算成本。相比之下，线性循环神经网络 (RNN) 具有较低的计算复杂度，适用于长距离建模。为此，我们提出了一种简单有效的基于窗口的框架，该框架基于线性分组循环神经网络（即对分组特征执行线性 RNN），用于精确的三维目标检测，称为 LION。其关键特性是允许在比基于 Transformer 的方法大得多的组中进行充分的特征交互。然而，由于线性分组循环神经网络在处理空间建模方面的局限性，将其有效地应用于高度稀疏点云中的三维目标检测并非易事。为了解决这个问题，我们简单地引入了一个三维空间特征描述符，并将其集成到线性分组循环神经网络算子中，以增强其空间特征，而不是盲目地增加体素特征的扫描顺序数量。为了进一步解决高度稀疏点云中的挑战，我们提出了一种三维体素生成策略，利用线性分组循环神经网络作为自回归模型的自然属性来增强前景特征。大量实验验证了所提出组件的有效性以及我们的 LION 在不同线性分组循环神经网络算子（包括 Mamba、RWKV 和 RetNet）上的泛化性。此外，值得一提的是，我们的 LION-Mamba 在 Waymo、nuScenes、Argoverse V2 和 ONCE 数据集上均达到了最先进的水平。最后但同样重要的是，我们的方法支持在小型但流行的 KITTI 数据集上使用各种先进的线性循环神经网络算子（例如 RetNet、RWKV、Mamba、xLSTM 和 TTT），以便快速体验我们基于线性循环神经网络的框架。||
|**2024-07-25**|[GaussianSR: High Fidelity 2D Gaussian Splatting for Arbitrary-Scale Image Super-Resolution](http://arxiv.org/abs/2407.18046)|null|隐式神经表示（INR）显著推进了图像任意尺度超分辨率（ASSR）领域的发展。大多数现有的基于 INR 的 ASSR 网络首先使用编码器从给定的低分辨率图像中提取特征，然后通过多层感知机解码器渲染超分辨率结果。尽管这些方法已显示出良好的结果，但它们的性能受到编码特征中离散潜在代码表示能力有限的限制。在本文中，我们提出了一种名为 GaussianSR 的新型 ASSR 方法，该方法通过二维高斯 splatting（2DGS）克服了这一限制。与将像素视为离散点的传统方法不同，GaussianSR 将每个像素表示为连续的高斯场。通过渲染相互堆叠的高斯场，对编码特征同时进行细化和上采样。因此，建立了远程依赖关系以增强表示能力。此外，开发了一种分类器来动态地将高斯核分配给所有像素，以进一步提高灵活性。GaussianSR 的所有组件（即编码器、分类器、高斯核和解码器）都经过端到端的联合学习。实验表明，GaussianSR 比现有方法使用更少的参数实现了卓越的 ASSR 性能，同时享有可解释和内容感知的特征聚合。||
|**2024-07-25**|[A Novel Perception Entropy Metric for Optimizing Vehicle Perception with LiDAR Deployment](http://arxiv.org/abs/2407.17942)|null|开发有效的评估指标对于准确快速地测量激光雷达感知性能至关重要。一个主要问题是缺乏能够同时根据目标检测或点云数据生成快速准确评估的指标。在本研究中，我们提出了一种基于车辆网格占用概率的新型激光雷达感知熵指标。该指标反映了点云分布对车辆检测性能的影响。在此基础上，我们还引入了一种激光雷达部署优化模型，该模型使用基于差分进化的粒子群优化算法进行求解。对比实验表明，所提出的PE-VGOP在评估激光雷达感知性能方面与车辆检测真值的关联性超过0.98。此外，与基础部署相比，现场实验表明，所提出的优化模型可以显着增强各种类型激光雷达（包括RS-16、RS-32和RS-80）的感知能力。值得注意的是，它使RS-32激光雷达的检测召回率提高了25%。||
|**2024-07-25**|[Hierarchical Object Detection and Recognition Framework for Practical Plant Disease Diagnosis](http://arxiv.org/abs/2407.17906)|null|近年来，目标检测方法（OD；例如，基于 YOLO 的模型）已广泛应用于植物病害诊断。与分类方法（CL；例如，CNN 模型）相比，这些方法对距离变化表现出鲁棒性，并且在检测小病灶方面表现出色。然而，也存在一些问题，例如难以检测的疾病的诊断性能低和标记成本高。此外，由于无法对健康病例进行明确训练，因此存在误报的风险。我们提出了分层目标检测和识别框架 (HODRF)，这是一个复杂且高度集成的两阶段系统，结合了 OD 和 CL 的优势来进行植物病害诊断。在第一阶段，HODRF 使用 OD 来识别感兴趣区域 (ROI)，而无需指定疾病。在第二阶段，CL 会诊断 ROI 周围的疾病。HODRF 具有以下几个优点：(1) 由于 OD 仅检测一种类型的 ROI，因此 HODRF 可以通过利用其识别其他病灶的能力，利用有限的训练图像来检测疾病。(2) 虽然 OD 会过度检测健康病例，但 HODRF 通过在第二阶段使用 CL 显着减少了这些错误。(3) CL 的准确性在 HODRF 中有所提高，因为它可以识别作为 ROI 给出的诊断目标，从而降低了其对尺寸变化的敏感性。(4) HODRF 受益于 CL 较低的注释成本，使其能够从大量图像中学习。我们使用 YOLOv7 实现 HODRF 进行 OD，使用 EfficientNetV2 进行 CL，并在一个大规模数据集（4 种作物、20 种患病和健康类别、281K 张图像）上评估了其性能。HODRF 在健康数据上的表现优于单独使用 YOLOv7 5.8 到 21.5 个百分点，在宏观 F1 分数上优于 0.6 到 7.5 个百分点，并且与 EfficientNetV2 相比，宏观 F1 提高了 1.1 到 7.2 个百分点。||
|**2024-07-25**|[Advancing 3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey](http://arxiv.org/abs/2407.17877)|null|三维点云（3DPC）随着深度学习（DL）的进步得到了显著发展和益处。然而，深度学习面临着各种问题，包括缺乏数据或标注数据、训练数据和测试数据之间存在显著差距以及对计算资源的高要求。为此，深度迁移学习（DTL）通过利用从源数据/任务中获得的知识来训练目标数据/任务，从而减少了依赖性和成本，并得到了广泛研究。人们已经提出了许多DTL框架来对齐从同一场景的多次扫描中获得的点云。此外，作为DTL子集的域自适应（DA）已经过改进，通过处理噪声和缺失点来提高点云数据的质量。最终，微调和DA方法已经证明了它们在解决点云数据固有难题方面的有效性。本文首次对这方面进行了综述。它全面概述了利用DTL和域自适应（DA）理解3DPC的最新技术。因此，首先介绍了DTL的背景以及数据集和评估指标。文章引入了一个定义明确的分类法，并考虑了不同知识迁移策略和性能等不同方面，进行了详细的比较。本文涵盖了各种应用，例如3DPC目标检测、语义标注、分割、分类、配准、下采样/上采样和去噪。此外，文章还讨论了所提出框架的优点和局限性，确定了未解决的挑战，并提出了潜在的研究方向。||
|**2024-07-26**|[Unsqueeze [CLS] Bottleneck to Learn Rich Representations](http://arxiv.org/abs/2407.17671)|**[link](https://github.com/isl-cv/udi)**|基于蒸馏的自监督学习通常由于其激进的聚类过程和更尖锐的目标分布的实现而导致更压缩的表示。为了克服这一限制并保留来自输入的更多信息，我们引入了 UDI，概念化为非压缩蒸馏自监督学习 (SSL)。UDI 通过鼓励从分层抽样得到的局部预测的整合配置文件中提取多模态预测来丰富学习表示。我们的评估表明，UDI 不仅在实例级别上促进了语义上有意义的表示，在图像分类方面提供了优于或与最先进的 SSL 方法相当的结果，而且还有效地保留了输入的细微差别，从而在密集预测任务（包括目标检测和分割）中产生了显著改进。此外，UDI 在少样本图像分类中表现出色，提高了联合嵌入管道的可扩展性。我们还提供了各种可视化和消融研究，以进一步阐明 UDI 背后的机制。我们的源代码可在 https://github.com/ISL-CV/udi 获取。||
|**2024-07-24**|[PEEKABOO: Hiding parts of an image for unsupervised object localization](http://arxiv.org/abs/2407.17628)|**[link](https://github.com/hasibzunair/peekaboo)**|在无监督的情况下定位物体提出了重大挑战，因为缺乏关键的视觉信息，例如物体的形状、类型和数量，以及在监督设置中通常可用的标记物体类别的缺失。虽然最近无监督物体定位方法通过利用自监督视觉表示取得了重大进展，但它们通常需要计算密集型训练过程，导致在计算、可学习参数和数据方面的资源需求较高。它们还缺乏对视觉上下文的显式建模，这可能会限制它们在物体定位方面的准确性。为了应对这些挑战，我们提出了一个名为PEEKABOO的单阶段学习框架，用于通过图像掩码学习局部物体像素级和形状级的基于上下文的表示来进行无监督物体定位。其关键思想是有选择地隐藏图像的某些部分，并利用剩余的图像信息来推断物体的位置，而无需明确的监督。在各种基准数据集上的定量和定性实验结果表明，与最先进的方法相比，我们的方法在单个物体发现和无监督显著性物体检测任务中均表现出简单性、有效性和竞争力。代码和预训练模型可在以下网址获得：https://github.com/hasibzunair/peekaboo||
|**2024-07-24**|[Graph Neural Networks: A suitable Alternative to MLPs in Latent 3D Medical Image Classification?](http://arxiv.org/abs/2407.17219)|**[link](https://github.com/compai-lab/2024-miccai-grail-kiechle)**|最近的研究突出了自然图像基础模型作为强大特征提取器的能力，即使是在零样本设置下处理医学影像数据也是如此。最常见的是，一个浅层多层感知器 (MLP) 被附加到特征提取器上，以促进端到端学习和下游预测任务（如分类），从而代表了事实上的标准。然而，由于图神经网络 (GNN) 近年来已成为医学研究中各种任务的可行选择，我们将注意力转向 GNN 与 MLP 预测头相比在 3D 医学图像分类任务中的有效性，并提出将其作为潜在的替代方案。在我们的实验中，我们为每个体积数据集实例设计了一个主题级别的图。其中，通过 DINOv2 预训练视觉变换器 (ViT) 编码的体积中所有切片的潜在表示构成节点及其各自的节点特征。我们使用公共数据集对分类头的数值进行比较，并在实验中评估各种图构建和图卷积方法。我们的研究结果表明，与 MLP 预测头相比，GNN 在分类性能方面有所提高，并且在运行时间方面也有了实质性的改进。额外的鲁棒性评估进一步验证了 GNN 的良好性能，使其成为传统 MLP 分类头的合适替代方案。我们的代码可在以下网址公开获取：https://github.com/compai-lab/2024-miccai-grail-kiechle||
|**2024-07-24**|[ALPI: Auto-Labeller with Proxy Injection for 3D Object Detection using 2D Labels Only](http://arxiv.org/abs/2407.17197)|null|三维目标检测在自动驾驶、机器人技术和增强现实等各种应用中起着至关重要的作用。然而，训练三维检测器需要昂贵的精确标注，这阻碍了标注扩展到大型数据集。为了应对这一挑战，我们提出了一种弱监督三维标注器，它仅依赖于图像中的二维边界框标注以及尺寸先验。一个主要问题是，仅使用二维边界框来监督三维检测模型并不可靠，因为不同的三维姿态与其相同的二维投影之间存在歧义。我们引入了一种简单而有效且通用的解决方案：我们通过构造构建带有标注的三维代理对象，并将它们添加到训练数据集中。我们的方法只需要尺寸先验就可以适应新的类别。为了更好地使二维监督与三维检测保持一致，我们的方法通过二维损失的新颖表达来确保深度不变性。最后，为了检测更具挑战性的实例，我们的标注器遵循离线伪标签方案，逐步改进其三维伪标签。在 KITTI 数据集上的大量实验表明，我们的方法不仅在汽车类别上的性能与之前的工作相当或更好，而且在更具挑战性的类别上也实现了接近完全监督方法的性能。我们通过第一个在更具挑战性的 nuScenes 数据集上进行实验，进一步证明了我们方法的有效性和鲁棒性。我们还提出了一种设置，其中弱标签是从在 MS-COCO 上预先训练的二维检测器而不是人工标注获得的。||
|**2024-07-24**|[An Adaptive Gradient Regularization Method](http://arxiv.org/abs/2407.16944)|null|优化器在神经网络训练中起着至关重要的作用，它能够显著提高训练效率和性能。基于梯度的权重更新是优化器的核心部分。研究表明，对权重和梯度进行标准化和归一化操作可以加速训练过程并提高性能，例如权重标准化（WS）、权重归一化（WN）和梯度归一化（GN）；此外还有梯度中心化（GC）。在这项工作中，我们引入了一种新的优化技术，称为自适应梯度正则化（AGR），它基于梯度向量中的梯度大小，将梯度向量在所有维度上归一化为系数向量，并从原始梯度中减去梯度与其系数向量的乘积。它可以被视为一种自适应梯度裁剪方法。我们证明了AGR可以改善损失函数的Lipschitzness，从而获得更稳定的训练过程和更好的泛化性能。AGR非常容易嵌入到vanilla优化器（例如Adan和AdamW）中，只需三行代码即可实现。我们在图像生成、图像分类和语言表示方面进行了实验，结果表明我们的AGR方法提高了训练效果。||
|**2024-07-23**|[Deep Bayesian segmentation for colon polyps: Well-calibrated predictions in medical imaging](http://arxiv.org/abs/2407.16608)|null|结直肠息肉通常是良性病变，但如果不能及时识别和成功处理，可能会发展成癌症，并导致结肠黏膜病变，称为腺癌。如今，深度学习的进步已经证明，在医学诊断应用中，深度学习在图像分类和检测方面能够实现显著的性能。然而，这些模型容易出现过拟合，仅基于点估计做出决策可能会导致错误的预测。因此，为了获得更明智的决策，我们必须考虑点估计及其可靠的不确定性量化。在本文中，我们基于后验分布的灵活性构建了不同的贝叶斯神经网络方法，以开发结直肠息肉图像的语义分割。我们发现，这些模型不仅在该医学数据集的分割方面提供了最先进的性能，而且还产生了准确的不确定性估计。我们将乘法归一化流（MNF）和重新参数化技巧应用于UNET、FPN和LINKNET架构，并在确定性和贝叶斯版本中使用多个主干网络进行了测试。我们报告说，FPN + EfficientnetB7架构与MNF是最有希望的选择，因为它IOU为0.94，预期校准误差（ECE）为0.004，并且在识别难以检测的结直肠息肉方面具有优势，这在早期检测可以预防结肠癌发展的临床领域非常有效。||
|**2024-07-23**|[Dynamic Retraining-Updating Mean Teacher for Source-Free Object Detection](http://arxiv.org/abs/2407.16497)|**[link](https://github.com/lbktrinh/DRU)**|在目标检测领域，无监督域适应（UDA）旨在将知识从标记的源域迁移到未标记的目标域。然而，UDA 对标记源数据的依赖限制了其在隐私相关场景中的适用性。本研究重点关注无源目标检测（SFOD），它在不使用标记源数据的情况下使源训练检测器适应未标记的目标域。自训练的最新进展，特别是 Mean Teacher（MT）框架，为 SFOD 的部署带来了希望。然而，缺乏源监督会严重损害这些方法的稳定性。我们确定了两个主要问题，（1）由于来自学生模型的不合时宜的更新，教师模型的不可控退化，以及（2）学生模型倾向于复制来自不正确伪标签的错误，导致其陷入局部最优。这两个因素都会导致有害的循环依赖，从而导致最近的自训练框架中性能快速下降。为了应对这些挑战，我们提出了动态再训练-更新（DRU）机制，该机制主动管理学生训练和教师更新过程以实现协同进化训练。此外，我们引入了历史学生损失来减轻不正确伪标签的影响。我们的方法在多个域适应基准上的 SFOD 设置中实现了最先进的性能，可与甚至超越先进的 UDA 方法相媲美。代码将在 https://github.com/lbktrinh/DRU 发布。||
|**2024-07-23**|[Designing robust diffractive neural networks with improved transverse shift tolerance](http://arxiv.org/abs/2407.16456)|null|如今，各种具有实际意义的问题都可以使用人工神经网络有效地解决。这推动了人工神经网络光学实现的快速发展，其中由一组相位衍射光学元件 (DOE) 构成的衍射神经网络 (DNN) 引起了广泛的研究兴趣。在 DNN 的实际应用中，一个亟待解决的问题是对 DOE 的高定位精度要求。为了解决这个问题，我们提出了一种设计用于图像分类的 DNN 的方法，该方法考虑了 DNN 元件的定位误差（横向位移）。在该方法中，分类问题的求解误差由一个函数表示，该函数取决于 DOE 的相位函数和描述其横向位移的随机向量。该函数的数学期望值被用作梯度方法中的误差函数，用于计算考虑了 DOE 横向位移的 DNN。研究表明，该函数导数的计算对应于 DNN 训练方法，其中 DOE 具有随机的横向位移。使用所提出的梯度方法，设计的 DNN 对 DOE 的横向位移具有鲁棒性，并且能够解决可见光波长下手写数字分类问题。数值模拟表明，设计的 DNN 在横向位移高达 17 个波长的情况下仍具有良好的性能。||
|**2024-07-23**|[MonoWAD: Weather-Adaptive Diffusion Model for Robust Monocular 3D Object Detection](http://arxiv.org/abs/2407.16448)|**[link](https://github.com/visualaikhu/monowad)**|单目3D目标检测是自动驾驶中一项重要且具有挑战性的任务。现有方法主要集中于在晴朗天气条件下执行3D检测，这些场景的特点是能见度清晰且最佳。然而，自动驾驶的挑战在于需要能够处理天气条件的变化，例如雾天，而不仅仅是晴天。我们介绍了MonoWAD，这是一种新颖的、具有天气鲁棒性的单目3D目标检测器，它具有天气自适应扩散模型。它包含两个组件：（1）天气码本，用于记忆晴朗天气的知识并为任何输入生成天气参考特征；（2）天气自适应扩散模型，通过结合天气参考特征来增强输入特征的特征表示。这起到了注意力的作用，根据天气条件指示输入特征需要多少改进。为了实现这一目标，我们引入了天气自适应增强损失，以增强晴天和雾天天气条件下的特征表示。在各种天气条件下进行的大量实验表明，MonoWAD实现了天气鲁棒的单目3D目标检测。代码和数据集已发布在https://github.com/VisualAIKHU/MonoWAD。||
|**2024-07-23**|[ESOD: Efficient Small Object Detection on High-Resolution Images](http://arxiv.org/abs/2407.16424)|null|放大输入图像是提升小目标检测性能的一种直接有效的方法。但是，简单的图像放大在计算量和GPU内存占用方面都非常昂贵。实际上，小目标通常分布稀疏且局部聚集。因此，大量的特征提取计算浪费在了图像中非目标的背景区域。最近的一些工作尝试使用额外的网络挑选出包含目标的区域并执行传统的目标检测，但新引入的计算限制了它们的最终性能。在本文中，我们建议重用检测器的骨干网络进行特征级目标搜索和图像块切片，这可以避免冗余的特征提取并降低计算成本。结合稀疏检测头，我们能够在高分辨率输入（例如1080P或更大）上检测小目标，从而获得更高的性能。由此产生的高效小目标检测（ESOD）方法是一个通用框架，可以应用于基于CNN和ViT的检测器，以节省计算量和GPU内存开销。大量实验验证了我们方法的有效性和效率。特别是，我们的方法在VisDrone、UAVDT和TinyPerson等代表性数据集上始终优于SOTA检测器（例如，AP指标提升8%）。代码即将开源。||
|**2024-07-23**|[DeepClean: Integrated Distortion Identification and Algorithm Selection for Rectifying Image Corruptions](http://arxiv.org/abs/2407.16302)|null|图像和视频中的失真识别和校正对于在下游视觉应用中获得良好性能至关重要。我们不依赖于基于固定试错法的图像处理流程，而是提出了一种用于自动图像失真分类和校正的两级顺序规划方法。在较高级别，它检测输入图像中存在的损坏类别（如果有）。较低级别从一组外部提供的候选算法中选择要应用的特定算法。整个两级设置在推理过程中以单次前向传递的形式运行，并且要迭代查询，直到检索到原始图像。我们展示了与三个基线相比在 COCO 图像数据集上的目标检测任务上的改进，该数据集具有丰富的失真集。我们方法的优势在于其动态重新配置，以输入图像为条件，以及对推理时看不见的候选算法的泛化能力，因为它仅依赖于图像嵌入输出的比较。||
|**2024-07-23**|[Image Classification using Fuzzy Pooling in Convolutional Kolmogorov-Arnold Networks](http://arxiv.org/abs/2407.16268)|null|如今，深度学习模型越来越需要兼具可解释性和高准确性。我们提出了一种将柯尔莫哥洛夫-阿诺德网络 (KAN) 分类头和模糊池化集成到卷积神经网络 (CNN) 中的方法。通过利用 KAN 的可解释性和模糊逻辑的不确定性处理能力，该集成在图像分类任务中展现出提高性能的潜力。我们的比较分析表明，采用 KAN 和模糊池化的改进 CNN 架构实现了与传统模型相当甚至更高的准确率。研究结果突出了结合模糊逻辑和 KAN 来开发更具可解释性和效率的深度学习模型的有效性。未来的工作将致力于将这种方法扩展到更大的数据集。||
|**2024-07-23**|[HSVLT: Hierarchical Scale-Aware Vision-Language Transformer for Multi-Label Image Classification](http://arxiv.org/abs/2407.16244)|null|多标签图像分类的任务涉及识别单个图像中的多个对象。考虑到标签中包含的有价值的语义信息和图像中呈现的基本视觉特征，紧密的视觉-语言交互在提高分类性能方面起着至关重要的作用。此外，鉴于单个图像内对象大小和外观的潜在差异，关注不同尺度的特征有助于发现图像中可能存在的对象。最近，基于Transformer的方法通过利用建模长距离依赖性的优势，在多标签图像分类方面取得了巨大成功，但它们也存在一些局限性。首先，现有方法将视觉特征提取和跨模态融合视为独立的步骤，导致联合语义空间中的视觉-语言对齐不足。此外，它们仅提取视觉特征并在单一尺度上执行跨模态融合，忽略了具有不同特征的对象。为了解决这些问题，我们提出了一种具有两个吸引人设计的层次化尺度感知视觉-语言Transformer (HSVLT)：(1) 层次化多尺度架构，其中包含一个跨尺度聚合模块，该模块利用从多个尺度提取的联合多模态特征来识别图像中不同大小和外观的对象。(2) 交互式视觉-语言注意力，一种新颖的注意力机制模块，它紧密地集成了跨模态交互，从而能够联合更新视觉、语言和多模态特征。我们已经在三个基准数据集上评估了我们的方法。实验结果表明，HSVLT以较低的计算成本超越了最先进的方法。||
|**2024-07-23**|[Improved Few-Shot Image Classification Through Multiple-Choice Questions](http://arxiv.org/abs/2407.16145)|null|通过一个简单的多选语言提示，VQA 模型可以作为零样本图像分类器运行，生成分类标签。与典型的图像编码器相比，VQA 模型具有一个优势：VQA 生成的图像嵌入可以通过定制的语言提示融入最相关的视觉信息。然而，对于大多数任务，零样本 VQA 的性能缺乏，要么是因为类别名称不熟悉，要么是因为预训练数据和测试数据分布不同。我们提出了一种简单的方法，仅使用少量标记样本和一个多选问题来提高 VQA 图像分类性能。这种少样本方法无需训练，并保持了 VQA 模型的动态性和灵活性优势。我们的方法不依赖于最终的语言输出，而是使用多选问题来提取特定于提示的潜在表示，这些表示包含丰富的相关视觉信息。这些表示被组合起来创建一个最终的整体图像嵌入，然后通过参考从少量标记样本构建的潜在类别原型对其进行解码。我们证明了该方法在 MiniImageNet、Caltech-UCSD Birds 和 CIFAR-100 等常见的少样本任务上优于纯视觉编码器和零样本 VQA 基线，实现了令人印象深刻的性能。最后，我们展示了我们的方法在具有多种不同视觉属性（例如织物、款式、纹理和不同服装的视图）的设置中表现特别好，而其他少样本方法在这些方面表现不佳，因为我们可以仅根据感兴趣的语义特征定制图像表示。||
|**2024-07-23**|[Diffusion Prior-Based Amortized Variational Inference for Noisy Inverse Problems](http://arxiv.org/abs/2407.16125)|**[link](https://github.com/mlvlab/davi)**|最近关于反问题的研究提出了利用预训练扩散模型作为强大先验的后验采样器。这些尝试为在各种反问题中使用扩散模型铺平了道路。然而，现有方法需要计算量大的迭代采样过程，并且需要为每个测量值优化单独的解决方案，这导致可扩展性有限，并且缺乏对未见样本的泛化能力。为了解决这些限制，我们提出了一种新方法，即基于扩散先验的摊销变分推断 (DAVI)，它从摊销变分推断的角度利用扩散先验解决反问题。具体来说，我们的摊销推断没有进行单独的逐测量优化，而是学习一个函数，该函数将测量值直接映射到相应干净数据的隐式后验分布，即使对于未见过的测量值也能实现单步后验采样。在图像恢复任务上的大量实验，例如高斯去模糊、4倍超分辨率和使用两个基准数据集的框内修复，证明了我们的方法优于强基线的性能。代码可在 https://github.com/mlvlab/DAVI 获取。||
|**2024-07-19**|[DEPICT: Diffusion-Enabled Permutation Importance for Image Classification Tasks](http://arxiv.org/abs/2407.14509)|null|我们提出了一种基于排列的图像分类器解释方法。当前的图像模型解释方法（如激活图）仅限于像素空间中基于实例的解释，因此难以理解全局模型行为。相比之下，基于排列的表格数据分类器解释方法通过比较模型在排列特征前后对数据的性能来衡量特征重要性。我们提出了一种基于图像的模型解释方法，该方法在数据集图像中排列可解释的概念。给定一个标有特定概念（如标题）的图像数据集，我们在文本空间中对示例中的概念进行排列，然后通过文本条件扩散模型生成图像。然后，特征重要性反映为模型性能相对于未排列数据的变化。当应用于一组概念时，该方法会生成特征重要性排名。我们证明，这种方法可以恢复合成和现实世界图像分类任务中潜在的模型特征重要性。||
|**2024-07-19**|[Enhancing Layout Hotspot Detection Efficiency with YOLOv8 and PCA-Guided Augmentation](http://arxiv.org/abs/2407.14498)|null|本文提出了一种基于YOLO的布局热点检测框架，旨在提高设计规则检查（DRC）过程的效率和性能。我们的方法利用YOLOv8视觉模型来检测每个布局图像中的多个热点，即使在处理大型布局图像时也是如此。此外，为了提高模式匹配的有效性，我们引入了一种新颖的方法，使用通过主成分分析（PCA）提取的信息来增强布局图像。我们提出的方法的核心是一种利用PCA从布局图像中提取有价值的辅助信息的算法。然后将提取的信息作为附加颜色通道合并到布局图像中。这种增强显着提高了多热点检测的准确性，同时降低了目标检测算法的误报率。我们使用从ICCAD-2019基准数据集中发现的布局生成的四个数据集来评估我们框架的有效性。结果表明，我们的框架实现了大约83%（86%）的精度（召回率），同时将误报率保持在7.4%以下。此外，研究表明，所提出的增强方法可以将从未见过的（NSB）热点的检测能力提高约10%。||
|**2024-07-19**|[Large Kernel Distillation Network for Efficient Single Image Super-Resolution](http://arxiv.org/abs/2407.14340)|**[link](https://github.com/stella-von/LKDN)**|近年来，高效轻量级的单图像超分辨率 (SISR) 取得了显著进展。一种有效的方法是使用大核设计，这已被证明可以提高 SISR 模型的性能，同时降低其计算需求。然而，当前最先进的 (SOTA) 模型仍然面临着诸如计算成本高等问题。为了解决这些问题，我们在本文中提出了大核蒸馏网络 (LKDN)。我们的方法简化了模型结构，并引入了更高效的注意力模块，以降低计算成本，同时提高性能。具体来说，我们采用重新参数化技术来提高模型性能，而无需增加额外成本。我们还将一种来自其他任务的新优化器引入到 SISR 中，从而提高了训练速度和性能。我们的实验结果表明，LKDN 优于现有的轻量级超分辨率方法，并实现了最先进的性能。||
|**2024-07-19**|[Temporal Correlation Meets Embedding: Towards a 2nd Generation of JDE-based Real-Time Multi-Object Tracking](http://arxiv.org/abs/2407.14086)|**[link](https://github.com/yfzhang1214/tcbtrack)**|联合检测与嵌入（JDE）跟踪器通过将外观特征提取作为辅助任务，将重识别任务（ReID）嵌入到检测器中，在多目标跟踪（MOT）任务中表现出色，实现了推理速度和跟踪性能之间的平衡。然而，解决检测器和特征提取器之间的竞争一直是一个挑战。此外，将 ReID 任务直接嵌入 MOT 的问题仍未解决。外观特征缺乏高辨别性，导致其用途有限。在本文中，我们提出了一种使用互相关来捕获对象时间信息的新学习方法。特征提取网络不再仅仅训练每帧的外观特征，而是利用连续帧的特征热图来学习更丰富的运动特征，解决了类间特征相似性的挑战。此外，我们将学习方法应用于更轻量级的特征提取网络，并将特征匹配分数视为强线索而非辅助线索，采用适当的权重计算来反映我们获得的特征与 MOT 任务之间的兼容性。我们的跟踪器名为 TCBTrack，在多个公共基准测试（即 MOT17、MOT20 和 DanceTrack 数据集）上实现了最先进的性能。具体来说，在 DanceTrack 测试集上，我们实现了 56.8 HOTA、58.1 IDF1 和 92.5 MOTA，使其成为能够实现实时性能的最佳在线跟踪器。与其他跟踪器的比较评估证明，我们的跟踪器在速度、鲁棒性和准确性之间实现了最佳平衡。||
|**2024-07-18**|[GroupMamba: Parameter-Efficient and Accurate Group Visual State Space Model](http://arxiv.org/abs/2407.13772)|**[link](https://github.com/Amshaker/GroupMamba)**|最近，状态空间模型（SSM）在以亚二次复杂度建模长程依赖性方面表现出色。然而，纯基于 SSM 的模型仍然面临着与稳定性和在计算机视觉任务上实现最佳性能相关的挑战。本文解决了扩展基于 SSM 的计算机视觉模型所面临的挑战，特别是大型模型的不稳定性和低效性。为了解决这个问题，我们引入了调制组Mamba层，它将输入通道分成四组，并将我们提出的基于 SSM 的高效视觉单选择扫描（VSSS）块独立应用于每组，每个VSSS块在四个空间方向之一进行扫描。调制组Mamba层还将四个VSSS块封装到一个通道调制算子中，以改善跨通道通信。此外，我们引入了一种基于蒸馏的训练目标，以稳定大型模型的训练，从而持续提高性能。我们的综合实验结果证明了所提出的贡献的优势，在 ImageNet-1K 上的图像分类、目标检测、MS-COCO 上的实例分割以及 ADE20K 上的语义分割方面，其性能优于现有方法。我们包含 23M 参数的微型变体在 ImageNet-1K 上实现了 83.3% 的分类 Top-1 准确率的最佳性能，同时与相同模型大小的现有最佳 Mamba 设计相比，参数效率提高了 26%。我们的代码和模型可在以下网址获得：https://github.com/Amshaker/GroupMamba。||
|**2024-07-18**|[Research on Image Super-Resolution Reconstruction Mechanism based on Convolutional Neural Network](http://arxiv.org/abs/2407.13211)|null|超分辨率重建技术利用软件算法将从同一场景捕获的一组或多组低分辨率图像转换为高分辨率图像。近年来，单图像超分辨率算法领域取得了长足的进步，特别是基于深度学习技术的算法。然而，现有算法在重建过程中提取图像特征和非线性映射方法仍然具有挑战性。这些问题导致网络架构无法有效地利用不同级别的各种信息。高频细节的损失很大，最终重建的图像特征过于平滑，缺乏精细的纹理细节。这对图像的主观视觉质量产生了负面影响。目标是从低分辨率图像中恢复高质量、高分辨率的图像。在这项工作中，我们采用了一种增强的深度卷积神经网络模型，该模型包含多个卷积层，每个卷积层都配置了特定的滤波器和激活函数，以有效地捕获图像的各种特征。此外，我们采用残差学习策略来加速训练并增强网络的收敛性，同时利用亚像素卷积层来细化图像的高频细节和纹理。实验分析表明，与传统的双三次插值方法和其他几种基于学习的超分辨率方法相比，该模型在多个公共数据集上表现出优异的性能。此外，它证明了该模型在保持图像边缘和纹理方面的有效性。||
|**2024-07-18**|[Learning Camouflaged Object Detection from Noisy Pseudo Label](http://arxiv.org/abs/2407.13157)|null|现有的伪装目标检测 (COD) 方法严重依赖于大规模像素级标注训练集，这些数据集的构建既耗时又费力。虽然弱监督方法提供了更高的标注效率，但由于伪装图像中前景和背景之间的视觉界限不清，它们的性能远远落后。在本文中，我们探索了在伪装场景中使用边界框作为提示的潜力，并介绍了第一个弱半监督 COD 方法，旨在使用极其有限数量的全标记图像实现经济高效且高精度的伪装目标分割。值得注意的是，从如此有限的集合中学习不可避免地会产生带有严重噪声像素的伪标签。为了解决这个问题，我们提出了一种噪声校正损失，它有助于模型在早期学习阶段学习正确的像素，并在记忆阶段校正由噪声像素主导的错误风险梯度，最终实现从噪声标签中准确分割伪装目标。当仅使用 20% 的全标记数据时，我们的方法表现出优于最先进方法的性能。||
|**2024-07-18**|[DFMSD: Dual Feature Masking Stage-wise Knowledge Distillation for Object Detection](http://arxiv.org/abs/2407.13147)|null|近年来，当前主流的特征掩码蒸馏方法主要通过从教师网络的特征图中重建学生网络的选择性掩码区域来发挥作用。在这些方法中，注意力机制可以帮助识别空间上重要的区域和关键的对象感知通道线索，从而使重建的特征编码具有与教师特征类似的足够判别性和表示能力。然而，以前的特征掩码蒸馏方法主要解决同质知识蒸馏问题，而没有充分考虑到异构知识蒸馏场景。特别是，在异构蒸馏范式中，教师和学生框架之间的巨大差异不利于特征掩码，导致重建的学生特征恶化。在这项研究中，提出了一种新的双特征掩码异构蒸馏框架，称为DFMSD，用于目标检测。更具体地说，在双特征掩码框架中加入了一个阶段性自适应学习模块，因此学生模型可以逐步适应教师模型，以弥合异构网络之间的差距。此外，将掩码增强策略与阶段性学习相结合，从而自适应地增强对象感知掩码区域，以改进特征掩码重建。此外，在教师和学生网络之间的每个特征金字塔网络（FPN）层执行语义对齐，以生成一致的特征分布。我们针对目标检测任务的实验结果证明了我们方法的前景，表明DFMSD优于最先进的异构和同构蒸馏方法。||
|**2024-07-18**|[UCIP: A Universal Framework for Compressed Image Super-Resolution using Dynamic Prompt](http://arxiv.org/abs/2407.13108)|null|压缩图像超分辨率 (CSR) 旨在同时对压缩图像进行超分辨率处理，并解决由压缩引起的混合失真的挑战。然而，现有的 CSR 工作通常集中在单一压缩编解码器（即 JPEG）上，而忽略了实际应用中不同的传统或基于学习的编解码器，例如 HEVC、VVC、HIFIC 等。在这项工作中，我们提出了第一个通用 CSR 框架，称为 UCIP，它具有动态提示学习能力，旨在联合支持任何压缩编解码器/模式的 CSR 失真。具体来说，我们提出了一种高效的动态提示策略，使用少量空间大小为 1x1 的提示，挖掘内容/空间感知的任务自适应上下文信息，用于通用 CSR 任务。为了简化上下文信息挖掘，我们通过首次将主动标记混合器 (ATM) 应用于 CSR 任务，为我们的 UCIP 引入了一种新颖的类 MLP 框架骨干，其中全局信息建模仅在水平和垂直方向上进行偏移预测。我们还通过收集具有 6 种流行的不同传统和基于学习的编解码器（包括 JPEG、HEVC、VVC、HIFIC 等）的数据集，构建了一个用于 CSR 任务的多合一基准数据集，产生了 23 种常见失真。大量实验表明，我们的 UCIP 在通用 CSR 任务上具有一致且优异的性能。该项目可在 https://lixinustc.github.io/UCIP.github.io  找到。||
|**2024-07-17**|[LookupViT: Compressing visual information to a limited number of tokens](http://arxiv.org/abs/2407.12753)|null|视觉Transformer (ViT) 已成为众多行业级视觉解决方案的首选。但是，它们的推理成本对于许多设置来说可能过高，因为它们在每一层都计算自注意力，这在token数量上存在二次计算复杂度。另一方面，图像中的空间信息和视频中的时空信息通常是稀疏和冗余的。在这项工作中，我们介绍了 LookupViT，旨在利用这种信息稀疏性来降低 ViT 推理成本。LookupViT 提供了一种新颖的通用视觉Transformer块，它通过将信息从更高分辨率的token压缩到固定数量的token来运作。这些压缩后的token经过精细处理，而更高分辨率的token则通过计算成本更低的层传递。这两种token集之间的信息共享是通过双向交叉注意力机制实现的。这种方法具有多个优点 - (a) 通过标准的高级运算符，易于在标准 ML 加速器（GPU/TPU）上实现，(b) 适用于标准 ViT 及其变体，因此可以推广到各种任务，(c) 可以处理不同的token化和注意力方法。LookupViT 还为压缩后的token提供了灵活性，从而能够在单个训练模型中进行性能-计算权衡。我们展示了 LookupViT 在多个领域的有效性 - (a) 图像分类（ImageNet-1K 和 ImageNet-21K），(b) 视频分类（Kinetics400 和 Something-Something V2），(c) 使用冻结编码器的图像字幕（COCO-Captions）。LookupViT 在这些领域中将 FLOPs 减少了 2 倍，同时保持或提高了准确性。此外，LookupViT 还展示了在图像分类（ImageNet-C、R、A、O）上的开箱即用鲁棒性和泛化能力，比 ViT 提高了高达 4%。||
|**2024-07-17**|[Toward INT4 Fixed-Point Training via Exploring Quantization Error for Gradients](http://arxiv.org/abs/2407.12637)|null|网络量化通常将全精度权重和/或激活值转换为低比特固定点值，以加速推理过程。最近的网络量化方法进一步将梯度离散化为低比特固定点值，从而实现高效的训练。它们通常使用梯度的最小-最大范围设置量化间隔，或调整间隔以使整个梯度的量化误差最小化。在本文中，我们分析了低比特固定点训练中梯度的量化误差，并表明降低大梯度值的误差可以显著提高量化性能。基于此，我们推导了大梯度量化误差关于量化间隔的上界，并获得了最小化大梯度量化误差的间隔最优条件。我们还介绍了一种间隔更新算法，该算法自适应地调整量化间隔，以保持对大梯度的较小量化误差。实验结果证明了我们的量化方法对各种网络架构和比特宽度组合在各种任务上的有效性，包括图像分类、目标检测和超分辨率。||
|**2024-07-17**|[CerberusDet: Unified Multi-Task Object Detection](http://arxiv.org/abs/2407.12632)|**[link](https://github.com/ai-forever/cerberusdet)**|目标检测是计算机视觉中的核心任务。多年来，众多模型的开发显著提高了性能。然而，这些传统模型通常受到训练数据和定义的类别逻辑的限制。随着近年来语言视觉模型的兴起，出现了不受这些固定类别限制的新方法。尽管具有灵活性，但与具有固定类别的传统模型相比，此类开放词汇检测模型在准确性方面仍然不足。同时，当需要扩展类别或合并不同数据集进行训练时，更准确的特定于数据的模型面临挑战。由于不同的逻辑或类别定义冲突，后者通常无法组合，因此难以在不影响模型性能的情况下改进模型。在本文中，我们介绍了 CerberusDet，这是一个多头模型框架，旨在处理多个目标检测任务。所提出的模型建立在 YOLO 架构之上，并有效地共享来自骨干和颈部组件的视觉特征，同时保持独立的任务头。这种方法使 CerberusDet 能够非常有效地执行，同时仍然提供最佳结果。我们在 PASCAL VOC 数据集和 Objects365 数据集中的其他类别上评估了该模型，以证明其能力。CerberusDet 实现了与最先进的特定于数据的模型相当的结果，推理时间减少了 36%。与顺序运行单个模型相比，一起训练的任务越多，所提出的模型就越有效。训练和推理代码以及模型均以开源形式提供 (https://github.com/ai-forever/CerberusDet)。||
|**2024-07-17**|[Strawberry detection and counting based on YOLOv7 pruning and information based tracking algorithm](http://arxiv.org/abs/2407.12614)|null|草莓产业为佛罗里达州带来了巨大的经济效益，但监测草莓生长和产量的过程劳动密集且成本高昂。基于机器学习的检测和跟踪方法的开发已被用于帮助自动监测和预测草莓产量，但由于之前的研究仅将深度学习方法应用于花和果实的检测，而没有考虑机器视觉系统收集的图像数据集的独特特征，因此改进有限。本研究提出了一种深度学习模型（YOLOv7及其变体）检测头的优化剪枝方法，可以实现对草莓花、幼果和成熟果实的快速、精确检测。此后，一种增强的目标跟踪算法，称为基于信息的跟踪算法（IBTA），利用最佳检测结果，去除卡尔曼滤波器，并整合移动方向、速度和空间信息，以提高草莓花和果实跟踪的精度。提出的跨YOLOv7变体的检测头剪枝方法，特别是具有检测头3的Pruning-YOLOv7-tiny和具有检测头2和3的Pruning-YOLOv7-tiny，分别实现了最佳的推理速度（每秒163.9帧）和检测精度（89.1%）。另一方面，通过与质心跟踪算法（CTA）的比较，证明了IBTA的效果，IBTA的多目标跟踪精度（MOTA）和多目标跟踪精度（MOTP）分别比CTA高12.3%和6.0%。此外，其他目标跟踪评估指标，包括IDF1、IDR、IDP、MT和IDs，表明IBTA在草莓花和果实跟踪方面优于CTA。||
|**2024-07-18**|[Benchmarking Robust Self-Supervised Learning Across Diverse Downstream Tasks](http://arxiv.org/abs/2407.12588)|**[link](https://github.com/layer6ai-labs/ssl-robustness)**|大规模视觉模型因其前所未有的性能以及跨下游任务的多功能性，已成为许多应用程序中不可或缺的一部分。然而，这些基础模型的稳健性主要在单个任务（即图像分类）中进行了探索。其他常见视觉任务（如语义分割和深度估计）的脆弱性在很大程度上仍然未知。我们对跨多个下游任务的自监督视觉编码器的对抗鲁棒性进行了全面的实证评估。我们的攻击在编码器嵌入空间和下游任务输出级别进行操作。在这两种情况下，目前仅针对分类测试的最先进的对抗性微调技术都显着降低了其他任务的干净和鲁棒性能。由于基础模型的目的是同时满足多个应用程序，因此我们的研究结果表明需要更广泛地增强编码器鲁棒性。我们的代码可在 ${github.com/layer6ai-labs/ssl-robustness}$ 获取。||
|**2024-07-16**|[Improving Unsupervised Video Object Segmentation via Fake Flow Generation](http://arxiv.org/abs/2407.11714)|**[link](https://github.com/suhwan-cho/FakeFlow)**|无监督视频目标分割（VOS），也称为视频显著目标检测，旨在像素级别检测视频中最突出的目标。近年来，利用 RGB 图像和光流图的两流方法受到了广泛关注。然而，训练数据的有限性仍然是一个巨大的挑战。在本研究中，我们提出了一种新的数据生成方法，可以从单个图像模拟生成伪光流，从而为稳定的网络学习创建大规模训练数据。受光流图高度依赖于深度图的观察结果的启发，我们通过细化和增强每个图像的估计深度图来生成伪光流。通过结合我们模拟的图像-流对，我们在不依赖复杂模块的情况下，在所有公共基准数据集上实现了新的最先进性能。我们相信，我们的数据生成方法代表了未来 VOS 研究的潜在突破。||
|**2024-07-16**|[Relation DETR: Exploring Explicit Position Relation Prior for Object Detection](http://arxiv.org/abs/2407.11699)|**[link](https://github.com/xiuqhou/relation-detr)**|本文提出了一种增强DETR（DEtection TRansformer）收敛性和性能的通用方案。我们从一个新的角度研究了Transformer中的慢收敛问题，认为它源于自注意力机制对输入没有引入结构性偏差。为了解决这个问题，我们在使用所提出的定量宏观相关性（MC）度量验证其统计显著性后，探索将位置关系先验作为注意力偏差来增强目标检测。我们提出的方法称为Relation-DETR，它引入了一个编码器来构建位置关系嵌入，用于渐进式注意力细化，这进一步将DETR的传统流式管道扩展为对比关系管道，以解决非重复预测和正监督之间的冲突。在通用和特定任务数据集上的大量实验表明了我们方法的有效性。在相同的配置下，Relation-DETR取得了显著的改进（与DINO相比，AP提高了+2.0%），达到了最先进的性能（1倍设置下AP为51.7%，2倍设置下AP为52.1%），并且在COCO val2017上实现了比现有DETR检测器快得多的收敛速度（仅用2个训练周期就获得了超过40%的AP）。此外，所提出的关系编码器是一个通用的即插即用组件，可以为理论上任何类似DETR的方法带来明显的改进。此外，我们还引入了一个类别无关的目标检测数据集SA-Det-100k。在该数据集上的实验结果表明，所提出的显式位置关系使AP有了1.3%的明显提高，突出了其在通用目标检测方面的潜力。代码和数据集可在https://github.com/xiuqhou/Relation-DETR获取。||
|**2024-07-16**|[Probing the Efficacy of Federated Parameter-Efficient Fine-Tuning of Vision Transformers for Medical Image Classification](http://arxiv.org/abs/2407.11573)|null|随着大型预训练Transformer模型的出现，针对各种下游任务微调这些模型成为一个关键问题。训练数据的缺乏、数据孤岛的存在以及严格的隐私限制加剧了医学影像领域中的微调问题，因此迫切需要能够协同微调预训练模型的算法。此外，这些模型的庞大规模使得必须使用参数高效微调（PEFT）来减少联邦学习中的通信负担。在这项工作中，我们系统地研究了各种联邦PEFT策略，用于将视觉Transformer（ViT）模型（在大规模自然图像数据集上预训练）应用于医学图像分类。除了评估已知的PEFT技术外，我们还引入了PEFT算法的新联邦变体，例如视觉提示调优（VPT）、视觉提示的低秩分解、随机块注意力微调以及混合PEFT方法（如低秩自适应（LoRA）+VPT）。此外，我们还进行了全面的实证分析，以确定适用于联邦设置的最佳PEFT方法，并了解数据分布对联邦PEFT的影响，特别是在域外（OOD）和非独立同分布（non-IID）数据的情况下。这项研究的关键见解是，虽然大多数联邦PEFT方法在域内迁移中表现良好，但在处理OOD和非IID场景时，精度和效率之间存在巨大的权衡，这在医学影像中很常见。具体来说，微调/交换参数每减少一个数量级，精度就会下降4%。因此，初始模型的选择对于联邦PEFT至关重要。如果可能的话，最好使用从域内医学图像数据中学习到的医学基础模型，而不是通用视觉模型。||
|**2024-07-16**|[Bridge Past and Future: Overcoming Information Asymmetry in Incremental Object Detection](http://arxiv.org/abs/2407.11499)|**[link](https://github.com/isee-laboratory/bpf)**|在增量目标检测领域，知识蒸馏已被证明是缓解灾难性遗忘的有效方法。然而，以往的工作侧重于保留旧模型的知识，而忽略了图像可能同时包含来自过去、现在和未来阶段的类别。由于不同阶段对前景目标的定义不同，目标的共现使得优化目标在不同阶段不一致，这极大地限制了模型的性能。为了克服这个问题，我们提出了一种名为“桥接过去与未来”（BPF）的方法，它可以跨阶段对齐模型，确保一致的优化方向。此外，我们还提出了一种新颖的“未来蒸馏”（DwF）损失函数，充分利用背景概率来减轻对旧类别的遗忘，同时确保学习新类别的高度适应性。我们在 Pascal VOC 和 MS COCO 基准测试集上进行了广泛的实验。在没有记忆的情况下，BPF 在各种设置下都优于当前最先进的方法。代码可在 https://github.com/iSEE-Laboratory/BPF 获取。||
|**2024-07-16**|[Crowd-SAM: SAM as a Smart Annotator for Object Detection in Crowded Scenes](http://arxiv.org/abs/2407.11464)|**[link](https://github.com/felixcaae/crowdsam)**|在计算机视觉领域，目标检测是一项重要的任务，在许多场景中都有应用。然而，获取大量的标签可能具有挑战性，尤其是在拥挤的场景中。最近，Segment Anything Model (SAM) 被提出作为一种强大的零样本分割器，为实例分割任务提供了一种新颖的方法。然而，SAM 及其变体在处理拥挤和遮挡场景中的物体时，其准确性和效率往往会受到影响。在本文中，我们介绍了 Crowd-SAM，这是一个基于 SAM 的框架，旨在以少量可学习参数和最小限度标记图像的成本提高 SAM 在拥挤和遮挡场景中的性能。我们引入了高效的提示采样器 (EPS) 和部分-整体判别网络 (PWD-Net)，增强了拥挤场景中的掩码选择和准确性。尽管 Crowd-SAM 结构简单，但它在 CrowdHuman 和 CityPersons 等多个基准测试中，可与最先进的 (SOTA) 全监督目标检测方法相媲美。我们的代码可在 https://github.com/FelixCaae/CrowdSAM 获取。||
|**2024-07-16**|[Leveraging Segment Anything Model in Identifying Buildings within Refugee Camps (SAM4Refugee) from Satellite Imagery for Humanitarian Operations](http://arxiv.org/abs/2407.11381)|**[link](https://github.com/yunyagaotree/sam-adapter-for-refugee-dwelling-extraction)**|利用高分辨率卫星图像更新带有难民营的建筑物覆盖范围可以支持相关的人道主义行动。本研究探讨了利用“分割一切模型”（SAM）及其分支之一 SAM-Adapter 进行语义分割任务，以从卫星图像中提取建筑物。SAM-Adapter 是 SAM 的轻量级改编版本，在不同难民营的提取任务中，它都是一个强大的工具。我们的研究证明，与其他经典（例如 U-Net）或高级语义分割模型（例如 Transformer）相比，SAM-Adapter 在数据可用性有限的情况下表现出色。此外，还强调了放大技术对模型性能的影响，事实证明，超分辨率 (SR) 模型等方法对于提高模型性能非常宝贵。此外，该研究还揭示了一些有趣的现象，包括在使用放大图像数据进行训练时，模型在第一个训练时期快速收敛，这为未来的研究提供了机会。涵盖从数据准备、模型训练、模型推理到预测掩码的 Shapefile 生成的每个步骤的代码都可以在 GitHub 存储库中找到，从而使更广泛的科学界和人道主义行动受益。||
|**2024-07-16**|[Generative AI Driven Task-Oriented Adaptive Semantic Communications](http://arxiv.org/abs/2407.11354)|null|面向任务的语义通信 (TOSC) 被认为是一种很有前途的通信框架，可用于各种人工智能 (AI) 任务驱动型应用。现有的 TOSC 框架侧重于提取源数据的完整语义特征，并学习低维信道输入以在有限的带宽资源内传输它们。 虽然传输完整的语义特征可以保持数据含义的完整性，但这种方法无法达到 TOSC 的性能阈值。在本文中，我们提出了一种面向任务的自适应语义通信 (TasCom) 框架，旨在通过仅发送与任务相关的语义特征来有效地促进 AI 任务的执行。在 TasCom 框架中，我们首先提出了一种基于生成式人工智能 (GAI) 架构的生成式联合信源信道编码 (G-JSCC) 以实现高效的语义传输。然后，提出了一种自适应编码控制器 (ACC)，以找到所提出的 G-JSCC 的最佳编码方案，该方案允许对 AI 任务有重大贡献的语义特征优先占用有限的带宽资源进行无线传输。此外，我们提出了一种生成式训练算法来训练所提出的 TasCom 以获得最佳性能。仿真结果表明，所提出的 TasCom 在所有考虑的信道条件下，在目标检测和实例分割任务上均优于现有的 TOSC 和传统编解码器方案。||
|**2024-07-16**|[LaMI-DETR: Open-Vocabulary Detection with Language Model Instruction](http://arxiv.org/abs/2407.11335)|**[link](https://github.com/eternaldolphin/lami-detr)**|现有的开放词汇目标检测方法利用视觉语言模型 (VLMs)（如 CLIP）强大的开放词汇识别能力来增强性能。然而，出现了两个主要挑战：(1) 概念表示的缺陷，CLIP 文本空间中的类别名称缺乏文本和视觉知识。(2) 对基本类别的过度拟合倾向，在从 VLM 到检测器的迁移过程中，开放词汇知识偏向于基本类别。为了应对这些挑战，我们提出了语言模型指令 (LaMI) 策略，该策略利用视觉概念之间的关系，并在一个简单而有效的类 DETR 检测器（称为 LaMI-DETR）中应用它们。LaMI 利用 GPT 构建视觉概念，并利用 T5 研究类别之间的视觉相似性。这些类别间关系改进了概念表示，并避免了对基本类别的过度拟合。综合实验验证了我们的方法在相同的严格设置下优于现有方法，并且不依赖于外部训练资源。LaMI-DETR 在 OV-LVIS 上实现了 43.4 的罕见框 AP，超过了之前的最佳结果 7.8 个罕见框 AP。||
|**2024-07-16**|[TCFormer: Visual Recognition via Token Clustering Transformer](http://arxiv.org/abs/2407.11321)|**[link](https://github.com/zengwang430521/tcformer)**|Transformer模型在计算机视觉领域得到广泛应用并取得了显著成功。大多数最先进的方法将图像分割成规则网格，并使用视觉标记表示每个网格区域。然而，固定的标记分布忽略了不同图像区域的语义含义，导致性能欠佳。为了解决这个问题，我们提出了标记聚类Transformer（TCFormer），它根据语义含义生成动态视觉标记。我们的动态标记具有两个关键特征：（1）使用相同的视觉标记表示具有相似语义含义的图像区域，即使这些区域不邻接；（2）集中于具有宝贵细节的区域，并使用精细标记表示它们。通过对图像分类、人体姿态估计、语义分割和目标检测等各种应用的广泛实验，我们证明了TCFormer的有效性。这项工作的代码和模型可在https://github.com/zengwang430521/TCFormer获取。||
|**2024-07-16**|[Zero-Shot Adaptation for Approximate Posterior Sampling of Diffusion Models in Inverse Problems](http://arxiv.org/abs/2407.11288)|null|扩散模型已成为解决反问题的强大生成技术。尽管扩散模型已成功应用于各种成像反问题，但这些模型需要许多步骤才能收敛，导致推理时间较长。最近，扩散模型出现了一种趋势，即采用复杂的噪声调度，在较低的噪声水平下更频繁地迭代时间步长，从而改善图像生成和收敛速度。然而，将这些想法应用于解决扩散模型的反问题仍然具有挑战性，因为当使用经验调整来确定正向模型对数似然项权重时，这些噪声调度表现不佳。为了解决这些挑战，我们提出了零样本近似后验采样 (ZAPS) 方法，该方法利用了与零样本物理驱动深度学习的联系。ZAPS 固定了采样步数，并使用零样本训练和物理引导的损失函数来学习每个不规则时间步长的对数似然权重。我们将 ZAPS 应用于最近提出的扩散后验采样方法作为基线，尽管 ZAPS 也可以与其他后验采样扩散模型一起使用。我们进一步使用可学习对角元素的对角化方法来逼近先验对数的 Hessian，以提高计算效率。这些参数在给定计算预算下通过固定次数的 epochs 进行优化。我们对各种噪声反问题的研究结果（包括高斯和运动去模糊、修复和超分辨率）表明，ZAPS 减少了推理时间，提高了对不规则噪声调度的鲁棒性，并提高了重建质量。代码可在 https://github.com/ualcalar17/ZAPS 获取。||
|**2024-07-12**|[Region Attention Transformer for Medical Image Restoration](http://arxiv.org/abs/2407.09268)|**[link](https://github.com/yaziwel/region-attention-transformer-for-medical-image-restoration)**|基于Transformer的方法在医学图像恢复方面表现出令人印象深刻的结果，这归功于其在空间维度上的多头自注意力（MSA）机制。然而，大多数现有的Transformer在固定且粗略划分的区域内进行注意力计算（例如，整幅图像或固定块），导致来自不相关区域的干扰和连续图像内容的碎片化。为了克服这些挑战，我们引入了一种新颖的区域注意力Transformer（RAT），它利用了基于区域的多头自注意力机制（R-MSA）。R-MSA使用强大的Segment Anything Model (SAM)将输入图像动态地划分为非重叠的语义区域，然后在这些区域内执行自注意力。这种区域划分更加灵活和可解释，确保只有来自相似语义区域的像素相互补充，从而消除了来自不相关区域的干扰。此外，我们引入了一个焦点区域损失来引导我们的模型自适应地关注恢复高难度区域。大量实验表明，RAT在各种医学图像恢复任务中是有效的，包括PET图像合成、CT图像去噪和病理图像超分辨率。代码可在\href{https://github.com/Yaziwel/Region-Attention-Transformer-for-Medical-Image-Restoration.git}{https://github.com/RAT}获取。||
|**2024-07-12**|[Open Vocabulary Multi-Label Video Classification](http://arxiv.org/abs/2407.09073)|null|预训练的视觉语言模型 (VLM) 在开放词汇计算机视觉任务（如图像分类、目标检测和图像分割）方面取得了显著进展。最近的一些工作集中于将 VLM 扩展到视频中的开放词汇单标签动作分类。然而，先前的方法在整体视频理解方面存在不足，整体视频理解需要能够在开放词汇环境中同时识别多个动作和实体（例如视频中的对象）。我们将此问题表述为开放词汇多标签视频分类，并提出一种使预训练的 VLM（如 CLIP）适应解决此任务的方法。我们利用大型语言模型 (LLM) 为 VLM 提供有关类别标签的语义指导，通过两个关键贡献来提高其开放词汇性能。首先，我们提出了一种端到端可训练架构，该架构学习提示 LLM 生成 CLIP 文本编码器的软属性，使其能够识别新类别。其次，我们将时间建模模块集成到 CLIP 的视觉编码器中，以有效地对视频概念的时空动态进行建模，并提出一种新颖的正则化微调技术，以确保在视频域中强大的开放词汇分类性能。我们广泛的实验结果证明了我们的方法在多个基准数据集上的有效性。||
|**2024-07-12**|[DroneMOT: Drone-based Multi-Object Tracking Considering Detection Difficulties and Simultaneous Moving of Drones and Objects](http://arxiv.org/abs/2407.09051)|null|静态平台上的多目标跟踪（MOT），例如监控摄像头，已经取得了显著进展，各种范式提供了优异的性能。然而，传统的MOT方法在无人机等动态平台上的有效性会显著降低。这种下降归因于无人机场景下MOT的独特挑战：（1）图像平面中的物体通常较小、模糊且经常被遮挡，难以检测和识别；（2）无人机会从不同角度移动和观察物体，导致物体预测位置和特征嵌入的不可靠性。本文提出了DroneMOT，它首先提出了一种双域集成注意力（DIA）模块，该模块考虑了无人机的快速移动，以增强对小型、模糊和遮挡物体的无人机目标检测和特征嵌入。然后，引入了一种创新的运动驱动关联（MDA）方案，该方案考虑了无人机和物体的并发运动。在MDA中，提出了一种自适应特征同步（AFS）技术来更新从不同角度看到的物体特征。此外，采用了一种基于双运动的预测（DMP）方法来预测物体位置。最后，将改进的特征嵌入和预测位置相结合，以增强物体关联。在VisDrone2019-MOT和UAVDT数据集上的综合评估表明，DroneMOT在无人机MOT领域相较于现有技术提供了显著的性能提升。||
|**2024-07-12**|[CAMP: Continuous and Adaptive Learning Model in Pathology](http://arxiv.org/abs/2407.09030)|**[link](https://github.com/QuIIL/CAMP)**|病理学中存在着大量的诊断任务。传统的计算病理学将这些任务表述为独立的图像分类问题并分别解决，导致计算效率低下和成本高昂。为了应对这些挑战，我们提出了一种通用的、统一的、通用的框架，称为病理学中的持续自适应学习模型 (CAMP)，用于病理图像分类。CAMP 是一种生成式、高效且自适应的分类模型，它可以利用特定于病理学的先验知识和学习特定于任务的知识，以最小的计算成本不断适应任何分类任务，而不会忘记现有任务的知识。我们在 22 个数据集上评估了 CAMP，包括 1,171,526 个图像块和 11,811 张病理切片，涵盖 17 个分类任务。CAMP 在各种数据集和任务的图像块和切片级别上均实现了最先进的分类性能，与传统的分类模型相比，计算时间减少了高达 94%，存储内存减少了 85%。我们的结果表明，CAMP 可以为病理图像分类带来根本性的变革，为全面数字化和计算机化的病理实践铺平道路。||
|**2024-07-11**|[Manipulating a Tetris-Inspired 3D Video Representation](http://arxiv.org/abs/2407.08885)|null|视频摘要是一种以保留视频活动的方式执行视频压缩的技术。这种技术在监控应用中特别有用。尽管它仍然是一个新兴的研究领域，但在过去的二十年中，已经提出了几种不同的方法，这些方法随着应用、优化类型、数据馈送的性质等而变化。这些算法所需的主要数据来自某种对象跟踪方法。在本文中，我们讨论了适用于不同应用的不同时空数据表示。我们还提出了视频摘要算法的正式定义。我们进一步讨论了该定义的假设和修改，以简化问题的版本。我们探索了应用打包算法来解决视频摘要问题。由于数据的性质是三维的，我们在讨论中考虑了 3D 打包问题。本文还对不同的视频摘要方法和打包问题进行了广泛的文献综述。最后，我们研究了该算法的不同应用，以及前面讨论的不同数据表示如何简化问题。我们还讨论了本次讨论后可以探索的未来研究方向。||
|**2024-07-11**|[Local Clustering for Lung Cancer Image Classification via Sparse Solution Technique](http://arxiv.org/abs/2407.08800)|**[link](https://github.com/zzzzms/LocalClustering4LungCancer)**|在这项工作中，我们建议使用一种基于稀疏解技术的局部聚类方法来研究医学图像，特别是肺癌图像分类任务。我们将图像视为加权图中的顶点，将一对图像之间的相似性视为图中的边。可以假设同一个簇内的顶点共享相似的特征和属性，因此使得图聚类技术的应用对图像分类非常有用。最近，人们发现基于线性系统稀疏解的图聚类方法比传统的聚类方法（如谱聚类）能够更有效地识别聚类。我们建议使用两种新开发的基于线性系统稀疏解的局部聚类方法进行图像分类。此外，我们采用基于箱样条的紧框架小波方法对这些图像进行去噪，并帮助在聚类之前构建更好的邻接矩阵。我们的方法在图像分类方面表现非常有效。与其他最先进的方法相比，我们的方法效率更高，并且优于或等于它们。最后，我们将指出两种图像变形方法来构建更多的人工图像数据以增加标记图像的数量。||
|**2024-07-11**|[Approaching Outside: Scaling Unsupervised 3D Object Detection from 2D Scene](http://arxiv.org/abs/2407.08569)|**[link](https://github.com/ruiyang-061x/lise)**|无监督三维物体检测的目标是在没有明确监督信号的情况下，准确地检测非结构化环境中的物体。考虑到激光雷达点云的稀疏性，这项任务在检测远处或小型物体时，由于其固有的稀疏性和有限的空间分辨率，往往会导致性能下降。在本文中，我们率先尝试将激光雷达数据与二维图像相结合用于无监督三维物体检测，并介绍了一种称为LiDAR-2D自适应学习（LiSe）的新方法。我们认为，RGB图像可以作为激光雷达数据的有价值补充，提供精确的二维定位线索，尤其是在某些物体只有少量激光雷达点的情况下。考虑到两种模式的独特性，我们的框架设计了一个自适应学习流程，该流程结合了自适应采样和弱模型聚合策略。自适应采样策略在训练期间动态调整伪标签的分布，以对抗模型过度拟合易于检测到的样本（例如附近和大型物体）的趋势。这样做可以确保在不同的物体尺度和距离上实现平衡的学习轨迹。弱模型聚合组件整合了在不同伪标签分布下训练的模型的优势，最终形成一个强大而鲁棒的最终模型。实验评估验证了我们提出的LiSe方法的有效性，与现有技术相比，在nuScenes数据集上实现了+7.1% AP $_{BEV}$和+3.4% AP$_{3D}$的显著改进，在Lyft数据集上实现了+8.3% AP$_{BEV}$和+7.4% AP$_{3D}$ 的显著改进。||
|**2024-07-11**|[Projecting Points to Axes: Oriented Object Detection via Point-Axis Representation](http://arxiv.org/abs/2407.08489)|null|本文介绍了一种用于定向目标检测的点轴表示法，强调了其灵活性和几何直观性，它包含两个关键组件：点和轴。1）点描绘了目标的空间范围和轮廓，提供了详细的形状描述。2）轴定义了目标的主要方向，提供了对精确检测至关重要的方向线索。点轴表示法将位置和旋转解耦，解决了传统基于边界框的方法中常见的损失不连续性问题。为了在不引入额外标注的情况下进行有效优化，我们提出了最大投影损失来监督点集学习，以及交叉轴损失来进行鲁棒的轴表示学习。此外，利用这种表示法，我们提出了 Oriented DETR 模型，将 DETR 框架无缝集成，用于精确的点轴预测和端到端检测。实验结果表明，在定向目标检测任务中，该方法的性能有了显著提高。||
|**2024-07-11**|[Global Spatial-Temporal Information-based Residual ConvLSTM for Video Space-Time Super-Resolution](http://arxiv.org/abs/2407.08466)|null|时空视频超分辨率技术可以将低帧率、低分辨率的视频转换为高帧率、高分辨率的视频，从而增强视觉体验并促进更有效的信息传播。我们提出了一种用于时空视频超分辨率的卷积神经网络 (CNN)，称为 GIRNet。为了生成高度准确的特征并提高性能，所提出的网络集成了具有可变形卷积的特征级时间插值模块和基于全局时空信息的残差卷积长短期记忆 (convLSTM) 模块。在特征级时间插值模块中，我们利用可变形卷积来适应不同场景位置的对象的变形和尺度变化。与传统的卷积相比，这为从运动物体中提取特征提供了一种更有效的解决方案。我们的网络有效地利用前向和后向特征信息来确定帧间偏移，从而直接生成插值帧特征。在基于全局时空信息的残差 convLSTM 模块中，第一个 convLSTM 用于从输入特征中导出全局时空信息，第二个 convLSTM 使用先前计算的全局时空信息特征作为其初始单元状态。第二个 convLSTM 采用残差连接来保留空间信息，从而增强输出特征。在 Vimeo90K 数据集上的实验表明，所提出的方法在峰值信噪比（分别比 STARnet、TMNet 和 3DAttGAN 高 1.45 dB、1.14 dB 和 0.02 dB）、结构相似性指数（分别比 STARnet、TMNet 和 3DAttGAN 高 0.027、0.023 和 0.006）和视觉效果方面均优于现有技术。||
|**2024-07-11**|[Semi-Supervised Object Detection: A Survey on Progress from CNN to Transformer](http://arxiv.org/abs/2407.08460)|null|半监督学习技术的显著进步促使研究人员探索其在计算机视觉领域目标检测任务中的潜力。半监督目标检测 (SSOD) 利用少量标记数据集和大量未标记数据集的组合。这种方法有效地减少了对大型标记数据集的依赖，而获取这些数据集通常既昂贵又耗时。最初，SSOD 模型在有效利用未标记数据和管理未标记数据生成的伪标签中的噪声方面遇到了挑战。然而，最近的许多进展已经解决了这些问题，从而大大提高了 SSOD 的性能。本文全面回顾了从卷积神经网络 (CNN) 到 Transformer 的 27 项 SSOD 方法的最新发展。我们深入研究了半监督学习的核心组件及其与目标检测框架的集成，涵盖了数据增强技术、伪标签策略、一致性正则化和对抗训练方法。此外，我们对各种 SSOD 模型进行了比较分析，评估了它们的性能和架构差异。我们的目标是激发进一步的研究兴趣，以克服现有挑战并探索半监督学习在目标检测中的新方向。||
|**2024-07-11**|[PowerYOLO: Mixed Precision Model for Hardware Efficient Object Detection with Event Data](http://arxiv.org/abs/2407.08272)|null|车载解决方案中目标检测系统的性能必须尽可能高，响应时间要短，并且由于通常采用电池供电，因此能耗要低。因此，在设计此类解决方案时，我们面临着嵌入式视觉系统特有的挑战：将内存和计算复杂度高的算法适配到小型低功耗设备中的问题。在本文中，我们提出了 PowerYOLO——一种混合精度解决方案，它针对此类应用的三个基本要素。首先，我们提出了一种基于动态视觉传感器 (DVS) 的系统，这是一种新型传感器，具有低功耗要求，并且在光照条件变化的情况下也能很好地工作。正是这些特性使得事件相机在某些应用中可能优于帧相机。其次，为了确保高精度以及低内存和计算复杂度，我们建议对 YOLO 检测器的卷积权重使用 4 位宽度的二进制幂 (PoT) 量化，并对所有其他参数进行线性量化。最后，我们采用 PoT 方案并用位移代替乘法，通过一种特殊的卷积-批量归一化融合方案来提高此类解决方案的硬件加速效率。与标准方法相比，使用特定传感器、PoT 量化和特殊的批量归一化融合方案，形成了一个独特的系统，该系统将内存复杂度降低了近 8 倍，并大大简化了计算。这个高效的系统在 GEN1 DVS 数据集上实现了 0.301 的高 mAP 精度，标志着这种压缩模型的新水平。||
|**2024-07-11**|[Wind Power Assessment based on Super-Resolution and Downscaling -- A Comparison of Deep Learning Methods](http://arxiv.org/abs/2407.08259)|null|风力涡轮机的有效放置依赖于准确的当地风速预测。气候预测为了解长期风速条件提供了宝贵的见解，但其空间数据分辨率通常不足以进行精确的风力发电预测。深度学习方法，特别是为图像超分辨率开发的模型，为通过提高气候模型的空间分辨率来弥合这种尺度差距提供了一种有前景的解决方案。在本文中，我们比较了各种深度学习模型在两个不同任务上的性能：超分辨率（我们将人工粗化的 ERA5 数据映射到其原始分辨率）和降尺度（我们将原生 ERA5 映射到高分辨率 COSMO-REA6 数据）。我们根据模型在下游应用中预测长期风力的表现对其进行评估，强调空间风速分辨率对风力估计的影响。我们的研究结果强调了将模型和评估指标与其特定下游应用保持一致的重要性。我们证明，扩散模型通过更好地保持风速的分布和物理特性，在估算风能潜力方面优于其他模型。||
|**2024-07-11**|[Knowledge distillation to effectively attain both region-of-interest and global semantics from an image where multiple objects appear](http://arxiv.org/abs/2407.08257)|**[link](https://github.com/seonwhee-genome/rvernet)**|基于卷积神经网络（CNN）和Transformer的模型一直在稳步改进，并已应用于各种计算机视觉下游任务。然而，在目标检测任务中，准确地定位和分类图像中几乎无限种类的食物仍然具有挑战性。为了解决这些问题，我们首先使用Segment Anything模型（SAM）将食物分割为感兴趣区域（ROI），并将ROI以外的区域遮盖为黑色像素。这个过程将问题简化为单一分类，其标注和训练比目标检测简单得多。我们将只保留ROI的图像作为输入，对各种现成的模型进行微调，这些模型编码了它们自己的归纳偏差。其中，数据高效图像Transformer（DeiT）具有最佳的分类性能。然而，当食物的形状和纹理相似时，仅ROI图像的上下文特征不足以进行准确分类。因此，我们引入了一种新型的组合架构RveRNet，它由ROI、额外ROI和集成模块组成，使其能够同时考虑ROI和全局上下文。在对模糊食物图像进行分类时，RveRNet的F1分数比其他单个模型高10%。如果RveRNet的模块是具有CNN知识蒸馏的DeiT，则性能最佳。我们研究了如何使架构对排列和易位引起的输入噪声具有鲁棒性。结果表明，CNN教师的知识有多少可以提炼到DeiT和DeiT的先天优势之间存在权衡。代码公开于：https://github.com/Seonwhee-Genome/RveRNet。||
|**2024-07-11**|[GraphMamba: An Efficient Graph Structure Learning Vision Mamba for Hyperspectral Image Classification](http://arxiv.org/abs/2407.08255)|**[link](https://github.com/ahappyyang/GraphMamba)**|高光谱图像分类中，有效的光谱序列和地理空间信息的提取一直是热门话题。在光谱序列特征捕获方面，RNN和Transformer由于其远程特征捕获能力，已成为主流分类框架。在空间信息聚合方面，CNN通过增强感受野来尽可能多地保留完整的空间信息。然而，光谱特征捕获架构的计算效率较低，并且CNN缺乏感知空间上下文信息的灵活性。为了解决这些问题，本文提出了GraphMamba——一种高效的图结构学习视觉Mamba分类框架，该框架充分考虑了HSI特性，以实现深层次的空间-光谱信息挖掘。具体来说，我们提出了一种新的高光谱视觉GraphMamba处理范式（HVGM），该范式通过构建空间-光谱立方体来保留空间-光谱特征，并利用线性光谱编码来增强后续任务的可操作性。GraphMamba的核心组件包括用于提高计算效率的HyperMamba模块和用于自适应空间上下文感知的SpectralGCN模块。HyperMamba通过采用全局掩码（GM）来减轻杂波干扰，并引入了并行训练推理架构来缓解计算瓶颈。SpatialGCN结合了加权多跳聚合（WMA）空间编码，以关注高度相关的空间结构特征，从而灵活地聚合上下文信息，同时减轻空间噪声干扰。在三个不同尺度的真实HSI数据集上进行了广泛的实验，与最先进的分类框架相比，GraphMamba取得了最佳性能。||
|**2024-07-11**|[DMM: Disparity-guided Multispectral Mamba for Oriented Object Detection in Remote Sensing](http://arxiv.org/abs/2407.08132)|null|多光谱目标检测面临着模态间和模态内差异的挑战。最近的研究经常依赖于基于Transformer的模型来解决这些问题并实现跨模态融合检测。然而，Transformer的平方计算复杂度限制了它们的性能。受Mamba在长序列任务中效率和较低复杂度的启发，我们提出了视差引导的多光谱Mamba（DMM），这是一个多光谱目标检测框架，由视差引导的跨模态融合Mamba（DCFM）模块、多尺度目标感知注意力（MTA）模块和目标先验感知（TPA）辅助任务组成。DCFM模块利用模态之间的视差信息自适应地融合来自RGB和红外图像的特征，从而减轻模态间冲突。MTA模块旨在通过关注RGB模态内的相关目标区域来增强特征表示，解决模态内变化问题。TPA辅助任务利用单模态标签来指导MTA模块的优化，确保其关注目标及其局部上下文。在DroneVehicle和VEDAI数据集上的大量实验表明了我们方法的有效性，它在保持计算效率的同时优于最先进的方法。代码将在https://github.com/Another-0/DMM上提供。||
|**2024-07-10**|[MambaVision: A Hybrid Mamba-Transformer Vision Backbone](http://arxiv.org/abs/2407.08083)|**[link](https://github.com/nvlabs/mambavision)**|我们提出了一种名为 MambaVision 的新型混合 Mamba-Transformer 骨干网络，该网络专为视觉应用而设计。我们的核心贡献包括重新设计 Mamba 公式，以增强其对视觉特征进行高效建模的能力。此外，我们对将视觉Transformer（ViT）与 Mamba 相集成的可行性进行了全面的消融研究。我们的结果表明，在 Mamba 架构的最后几层配备多个自注意力块可以极大地提高模型捕获远程空间依赖关系的能力。基于我们的发现，我们引入了一系列具有层次结构的 MambaVision 模型，以满足各种设计标准。对于 ImageNet-1K 数据集上的图像分类任务，MambaVision 模型变体在 Top-1 准确率和图像吞吐量方面均达到了新的最先进水平 (SOTA)。在下游任务（例如 MS COCO 和 ADE20K 数据集上的目标检测、实例分割和语义分割）中，MambaVision 的性能优于规模相当的骨干网络，并展现出更佳的性能。代码：https://github.com/NVlabs/MambaVision。||
|**2024-07-09**|[CoLA: Conditional Dropout and Language-driven Robust Dual-modal Salient Object Detection](http://arxiv.org/abs/2407.06780)|**[link](https://github.com/ssecv/CoLA)**|深度/热信息有利于利用传统RGB图像检测显著性目标。然而，在双模态显著性目标检测（SOD）模型中，针对噪声输入和模态缺失的鲁棒性至关重要，但很少被研究。为了解决这个问题，我们引入了条件性丢弃和语言驱动（CoLA）框架，该框架包含两个核心组件。1）语言驱动质量评估（LQA）：利用带有提示学习器的预训练视觉语言模型，LQA在不需要额外质量标注的情况下重新校准图像贡献。这种方法有效地减轻了噪声输入的影响。2）条件性丢弃（CD）：一种学习方法，用于增强模型在模态缺失情况下的适应性，同时保持其在完整模态下的性能。CD作为一种插件式训练方案，将模态缺失视为条件，增强了各种双模态SOD模型的整体鲁棒性。大量实验表明，所提出的方法在模态完整和模态缺失两种情况下均优于最先进的双模态SOD模型。我们将开源代码。||
|**2024-07-09**|[Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting Region Captions](http://arxiv.org/abs/2407.06723)|null|人类使用组合性来描述复杂场景，使用带有链接和关系的简单文本描述来丰富描述。虽然视觉语言研究的目标是开发具有组合理解能力的模型，但这还没有反映在现有的数据集中，这些数据集在很大程度上仍然使用纯文本描述图像。在这项工作中，我们提出了一种新的标注策略，即基于图的字幕（GBC），它使用带有各种类型节点的标记图结构来描述图像。GBC 中的节点是通过以下方式创建的：首先，使用对象检测和密集字幕工具递归嵌套以发现和描述实体节点，然后在第二阶段通过使用新型节点、组合和实体之间的关系来突出显示，将它们链接在一起。由于所有 GBC 节点都包含纯文本描述，因此 GBC 保留了自然语言的灵活性，但也可以在其边缘编码分层信息。我们证明了 GBC 可以使用现成的多模态 LLM 和开放词汇检测模型自动生成，方法是构建一个新的数据集 GBC10M，为 CC12M 数据集中的大约 10M 张图像收集 GBC 标注。我们使用 GBC10M 来展示 GBC 发现的大量节点字幕，使用 CLIP 训练进行测量。我们表明，与其他数据集格式相比，使用 GBC 节点的注释（特别是存储在组合和关系节点中的注释）可以显着提高下游模型的性能。为了进一步探索 GBC 提供的机会，我们还提出了一种新的注意力机制，可以利用整个 GBC 图，并获得了令人鼓舞的实验结果，表明了合并图结构的额外好处。我们的数据集发布在 \url{https://huggingface.co/graph-based-captions}。||
|**2024-07-09**|[CTRL-F: Pairing Convolution with Transformer for Image Classification via Multi-Level Feature Cross-Attention and Representation Learning Fusion](http://arxiv.org/abs/2407.06673)|**[link](https://github.com/hosamsherif/ctrl-f)**|Transformer因其强大的容量和全局处理能力，在计算机视觉领域受到越来越多的关注。然而，Transformer是数据密集型的，与卷积神经网络（ConvNets）相比，其泛化能力受到限制，特别是在数据有限的情况下进行训练时，因为它们缺乏ConvNets中存在的内置空间归纳偏差。在本文中，我们致力于将卷积和Transformer的优势结合起来，以完成图像分类任务。为此，我们提出了一种新颖的轻量级混合网络，该网络通过表示学习融合和多级特征交叉注意（CTRL-F）将卷积与Transformer配对。我们的网络包括一个卷积分支和一个名为多级特征交叉注意（MFCA）的新型Transformer模块。MFCA模块对从不同卷积阶段获得的多级特征表示进行操作。它通过两个独立的Transformer分支处理从这些多级特征表示中提取的小块标记和大块标记，其中两个分支通过交叉注意机制进行通信和交换知识。我们使用称为自适应知识融合（AKF）和协作知识融合（CKF）的新型表示融合技术，将从卷积路径获得的局部响应与从MFCA模块获得的全局响应融合在一起。实验表明，我们的CTRL-F变体无论是在大数据上从头开始训练，还是在低数据情况下训练，都能获得最先进的性能。例如，CTRL-F在Oxford-102 Flowers和PlantVillage数据集上从头开始训练时，分别达到了82.24%和99.91%的top-1准确率，超过了最先进的模型，这展示了我们的模型在图像分类任务上的鲁棒性。代码位于：https://github.com/hosamsherif/CTRL-F||
|**2024-07-09**|[NoisyAG-News: A Benchmark for Addressing Instance-Dependent Noise in Text Classification](http://arxiv.org/abs/2407.06579)|null|现有的噪声标签学习研究主要集中在合成标签噪声上。虽然合成噪声具有明确的结构特性，但它往往不能准确地复制现实世界的噪声模式。近年来，人们一直在努力构建用于图像分类的、可泛化和可控的实例相关噪声数据集，这极大地促进了该领域鲁棒噪声学习的发展。然而，关于文本分类中噪声标签学习的研究仍然很少。为了更好地理解现实世界中文本分类环境中的标签噪声，我们通过人工标注构建了基准数据集NoisyAG-News。首先，我们分析了标注数据，以收集关于现实世界噪声的观察结果。我们定性和定量地证明了现实世界的噪声标签遵循实例相关的模式。随后，我们使用预训练语言模型和噪声处理技术，对NoisyAG-News及其相应的合成噪声数据集进行了全面的学习实验。我们的研究结果表明，虽然预训练模型对合成噪声具有鲁棒性，但它们在实例相关噪声面前却表现不佳，不同混淆程度的样本在训练和测试过程中表现出不一致的性能。这些现实世界的噪声模式提出了新的、重大的挑战，促使人们重新评估噪声标签处理方法。我们希望NoisyAG-News能够促进未来噪声标签学习解决方案的开发和评估。||
|**2024-07-09**|[UnmixingSR: Material-aware Network with Unsupervised Unmixing as Auxiliary Task for Hyperspectral Image Super-resolution](http://arxiv.org/abs/2407.06525)|null|基于深度学习 (DL) 的高光谱图像 (HIS) 超分辨率 (SR) 方法取得了显著成果，并在工业界和学术界引起了广泛关注。然而，大多数现有方法都在探索和学习低分辨率 (LR) 和高分辨率 (HR) HSI 之间的映射关系，导致在解决不适定 SR 问题时增加了不可靠性和不合理性。有趣的是，我们发现 LR 成像与混合像元现象相似。传感器阵列中的单个光电探测器接收由多种类别反射的反射信号，导致低空间分辨率和混合像元问题。受此观察的启发，本文提出了一种名为 UnmixingSR 的、组件感知的 HSI SR 网络，其中无监督 HU 作为辅助任务用于感知 HSI 的材料成分。我们将 HU 视为辅助任务，并通过探索 LR 和 HR  丰度之间的约束将其纳入 HSI SR 过程。我们没有仅仅学习 LR 和 HR HSI 之间的映射关系，而是利用 LR 丰度和 HR 丰度之间的联系来提高我们方法在解决 SR 问题时的稳定性。此外，所提出的解混过程可以作为即插即用的辅助任务嵌入到现有的深度 SR 模型中。高光谱实验结果表明，将解混过程作为辅助任务纳入 SR 问题是可行且合理的，并取得了优异的性能。代码可在以下网址获得||
|**2024-07-08**|[Enhancing super-resolution ultrasound localisation through multi-frame deconvolution exploiting spatiotemporal coherence](http://arxiv.org/abs/2407.06373)|null|通过微泡 (MB) 定位和跟踪实现的超分辨率超声成像，也称为超声定位显微镜，可以在动物和人体内对微血管进行非侵入性亚衍射分辨率成像。从获取的对比增强超声 (CEUS) 图像中定位的 MB 数量和定位精度直接影响最终的超分辨率微血管图像的质量。然而，CEUS 图像中不可忽略的噪声会使 MB 定位变得困难。为了提高 MB 定位性能，我们提出了一种多帧反卷积 (MF-Decon) 框架，该框架可以利用 CEUS 数据中固有的时空一致性，并基于总变差 (TV) 和去噪正则化 (RED) 设计新的空间和时间正则化器。基于 MF-Decon 框架，我们引入了两种新方法：具有空间和时间 TV 的 MF-Decon (MF-Decon+3DTV) 和具有空间 RED 和时间 TV 的 MF-Decon (MF-Decon+RED+TV)。计算机模拟结果表明，我们的方法在所有评估指标（包括精度、召回率、 $F_1$ 分数、平均定位误差和标准定位误差）方面均优于两种广泛使用的反卷积或归一化互相关方法。特别是，我们的方法将 MB 定位精度提高了 39%，并将召回率提高了 12%。使用我们的方法在公开可用的体内大鼠大脑数据集上生成的超分辨率微血管图显示出更少的噪声、更好的对比度、更高的分辨率和更多的血管结构。||
|**2024-07-08**|[GeoWATCH for Detecting Heavy Construction in Heterogeneous Time Series of Satellite Images](http://arxiv.org/abs/2407.06337)|null|从多传感器学习是一项挑战，因为存在时空错位以及分辨率和捕获光谱的差异。为此，我们推出了 GeoWATCH，这是一个灵活的框架，用于在来自多个传感器平台的长序列卫星图像上训练模型，该框架旨在处理图像分类、活动识别、物体检测或物体跟踪任务。我们的系统包括一种基于子图同构的新型部分权重加载机制，允许在多个训练周期内持续训练和修改网络。这使我们能够在很长一段时间内训练一系列模型，我们观察到，在我们调整配置的同时保持核心骨干的情况下，性能得到了提高。||
|**2024-07-08**|[Active Label Refinement for Robust Training of Imbalanced Medical Image Classification Tasks in the Presence of High Label Noise](http://arxiv.org/abs/2407.05973)|null|基于监督深度学习的医学图像分类的鲁棒性会被标签噪声显著削弱。为了提高存在噪声标签时的分类性能，目前已经提出了几种方法，但它们面临着一些挑战：1）难以处理类别不平衡的数据集，导致少数类样本经常被误认为是噪声样本；2）仅仅关注于使用噪声数据集最大化性能，而没有结合专家参与主动清理噪声标签。为了应对这些挑战，我们提出了一种结合了噪声标签学习（LNL）和主动学习的两阶段方法。这种方法不仅提高了存在噪声标签时医学图像分类的鲁棒性，而且在有限的标注预算下，通过重新标注重要的错误标签，迭代地提高了数据集的质量。此外，我们在LNL阶段引入了一种新的梯度方差方法，通过对代表性不足的样本进行采样，补充了基于损失的样本选择方法。通过使用两个不平衡的噪声医学分类数据集，我们证明了我们提出的技术在处理类别不平衡方面优于以往的方法，因为它不会将来自少数类的干净样本错误地识别为大部分是噪声样本。||
|**2024-07-08**|[Deform-Mamba Network for MRI Super-Resolution](http://arxiv.org/abs/2407.05969)|null|在本文中，我们提出了一种新的MR图像超分辨率架构，称为Deform-Mamba。不同于传统的CNN或基于Transformer的超分辨率方法，这些方法会遇到与局部感受野或高计算成本相关的挑战，我们的方法旨在有效地探索图像的局部和全局信息。具体来说，我们开发了一个Deform-Mamba编码器，它由两个分支组成：调制变形块和视觉Mamba块。我们还在瓶颈层设计了一个多视图上下文模块，以探索多视图上下文内容。由于编码器提取的特征包括内容自适应的局部信息和高效的全局信息，视觉Mamba解码器最终生成高质量的MR图像。此外，我们引入了一种对比边缘损失来促进边缘和对比度相关内容的重建。在IXI和fastMRI数据集上的定量和定性实验结果表明，我们的方法取得了具有竞争力的性能。||
|**2024-07-08**|[Multi-clue Consistency Learning to Bridge Gaps Between General and Oriented Object in Semi-supervised Detection](http://arxiv.org/abs/2407.05909)|**[link](https://github.com/facias914/sood-mcl)**|虽然现有的半监督目标检测（SSOD）方法在一般场景中表现良好，但它们在处理航空图像中的定向目标时遇到了挑战。我们通过实验发现，在半监督学习中，一般目标检测和定向目标检测之间存在三个差距：1）采样不一致：在从标记数据中选择正标签时，常用的中心采样不适用于长宽比较大的定向目标。2）分配不一致：平衡定向伪框的精度和定位质量带来了更大的挑战，这在从未标记数据中选择正标签时引入了更多噪声。3）置信度不一致：在考虑定向目标时，预测的分类和定位质量之间存在更多不匹配，影响了伪标签的选择。因此，我们提出了一个多线索一致性学习（MCL）框架，以弥合半监督检测中一般目标和定向目标之间的差距。具体来说，考虑到旋转目标的各种形状，我们专门设计了高斯中心分配来从标记数据中选择像素级正标签。然后，我们引入了尺度感知标签分配来选择像素级伪标签而不是不可靠的伪框，这是一种适用于各种尺度目标的分而治之策略。最后采用一致置信度软标签，通过保持预测结果的一致性来进一步提升检测器。在DOTA-v1.5和DOTA-v1.0基准数据集上的综合实验表明，我们提出的MCL方法在半监督定向目标检测任务中可以达到最先进的性能。||
|**2024-07-05**|[SH17: A Dataset for Human Safety and Personal Protective Equipment Detection in Manufacturing Industry](http://arxiv.org/abs/2407.04590)|**[link](https://github.com/ahmadmughees/sh17dataset)**|工作场所事故持续对人类安全构成重大风险，特别是在建筑和制造等行业，因此，有效的个人防护装备 (PPE) 合规性变得越来越重要。我们的研究重点是开发基于目标检测 (OD) 和卷积神经网络 (CNN) 的非侵入性技术，以检测和验证各种类型 PPE 的正确使用，例如安全帽、安全眼镜、口罩和防护服。本研究提出了 SH17 数据集，其中包含从不同工业环境中收集的 8,099 张带注释的图像，这些图像包含 75,994 个 17 个类别的实例，用于训练和验证 OD 模型。我们已经训练了最先进的 OD 模型进行基准测试，初步结果表明，You Only Look Once (YOLO)v9-e 模型变体的 PPE 检测精度超过 70.9%，达到了令人满意的水平。跨域数据集上的模型验证性能表明，集成这些技术可以显着改进安全管理系统，为努力满足人类安全法规和保护员工的行业提供可扩展且高效的解决方案。该数据集可在 https://github.com/ahmadmughees/sh17dataset 获取。||
|**2024-07-05**|[Multi-Branch Auxiliary Fusion YOLO with Re-parameterization Heterogeneous Convolutional for accurate object detection](http://arxiv.org/abs/2407.04381)|**[link](https://github.com/yang-0201/MAF-YOLO)**|由于多尺度特征融合的有效性，路径聚合特征金字塔网络 (PAFPN) 广泛应用于 YOLO 检测器中。然而，它无法同时高效且自适应地融合高级语义信息和低级空间信息。在本文中，我们提出了一个名为 MAF-YOLO 的新模型，它是一个具有名为多分支辅助特征金字塔网络 (MAFPN) 的新型目标检测框架。在 MAFPN 中，浅层辅助融合 (SAF) 模块旨在将主干网络的输出与颈部结合起来，保留最佳级别的浅层信息，以便于后续学习。同时，深度嵌入颈部的高级辅助融合 (AAF) 模块将更多样化的梯度信息传递到输出层。此外，我们提出的重新参数化的异构高效层聚合网络 (RepHELAN) 模块确保了整体模型架构和卷积设计都采用了异构大卷积核。因此，这保证了保留与小目标相关的信息，同时实现了多尺度感受野。最后，以 MAF-YOLO 的纳米版本为例，它在 COCO 数据集上仅用 3.76M 可学习参数和 10.51G FLOPs 即可达到 42.4% 的 AP，性能优于 YOLOv8n 约 5.1%。本研究的源代码可在以下网址获取：https://github.com/yang-0201/MAF-YOLO。||
|**2024-07-05**|[FeatureSORT: Essential Features for Effective Tracking](http://arxiv.org/abs/2407.04249)|null|在这项工作中，我们介绍了一种新颖的跟踪器，该跟踪器专为在线多目标跟踪而设计，其重点在于简单而有效。我们提供了多个特征模块，每个模块代表一个特定的外观信息。通过整合不同的外观特征，包括服装颜色、款式和目标方向，以及用于鲁棒嵌入提取的 ReID 网络，我们的跟踪器显著提高了在线跟踪精度。此外，我们建议结合更强大的检测器，并提供先进的后处理方法，进一步提升跟踪器的性能。在实时操作期间，我们建立测量来跟踪关联距离函数，其中包括 IoU、方向、颜色、样式和 ReID 特征相似性信息，其中每个指标分别计算。通过我们设计的特征相关距离函数，可以跟踪物体更长时间的遮挡，同时保持相对较低的身份切换次数。广泛的实验评估表明，跟踪精度和可靠性显着提高，身份切换减少和遮挡处理增强证明了这一点。这些进步不仅有助于推动目标跟踪领域的最新技术水平，而且为未来需要高精度和可靠性的研究和实际应用开辟了新途径。||
|**2024-07-05**|[AnySR: Realizing Image Super-Resolution as Any-Scale, Any-Resource](http://arxiv.org/abs/2407.04241)|**[link](https://github.com/crispyfeso4/anysr)**|为了提高单图像超分辨率 (SISR) 应用的效率和可扩展性，我们引入了 AnySR，将现有的任意尺度 SR 方法重建为任意尺度、任意资源的实现。与使用相同计算成本解决各种规模的 SR 任务的现成方法相比，我们的 AnySR 创新在于：1) 将任意尺度任务构建为任意资源实现，在不增加额外参数的情况下减少了较小规模的资源需求；2) 以特征交织的方式增强任意尺度性能，将尺度对以规则的间隔插入特征中，并确保正确的特征/尺度处理。我们通过重建大多数现有的任意尺度 SISR 方法并在五个流行的 SISR 测试数据集上进行验证，充分证明了 AnySR 的有效性。结果表明，我们的 AnySR 以计算效率更高的方式实现了 SISR 任务，并且性能与现有的任意尺度 SISR 方法相当。我们首次实现了 SISR 任务，不仅在文献中是任意尺度的，而且是任意资源的。代码可在 https://github.com/CrispyFeSo4/AnySR 获取。||
|**2024-07-05**|[AMD: Automatic Multi-step Distillation of Large-scale Vision Models](http://arxiv.org/abs/2407.04208)|null|基于Transformer的架构因其优越的性能已成为各种视觉任务的标准模型。随着模型规模的不断扩大，模型蒸馏在各种实际应用中变得极其重要，特别是在受计算资源限制的设备上。然而，当教师模型和学生模型之间存在较大的容量差距时，例如10倍的压缩率，现有的知识蒸馏方法效果会下降。在本文中，我们提出了一种名为自动多步蒸馏（AMD）的新方法，用于大规模视觉模型压缩。具体来说，我们的蒸馏过程分为多个步骤。首先，对教师模型进行蒸馏，形成一个中间的助教模型，然后进一步蒸馏到学生模型。我们引入了一个高效且有效的优化框架，来自动识别能够使学生模型性能最大化的最佳助教模型。我们在多个图像分类数据集上进行了广泛的实验，包括CIFAR-10、CIFAR-100和ImageNet。结果一致表明，我们的方法优于几种已建立的基线方法，为未来大规模视觉模型的知识蒸馏方法铺平了道路。||
|**2024-07-04**|[Attention Normalization Impacts Cardinality Generalization in Slot Attention](http://arxiv.org/abs/2407.04170)|null|以对象为中心的场景分解对于计算机视觉和机器人等领域的下游任务非常重要。最近提出的槽位注意力模块已经被一些衍生作品用于图像分割和视频目标跟踪，它是一种深度学习组件，可以在输入图像上执行无监督的以对象为中心的场景分解。它基于一种注意力架构，其中潜在的槽位向量（包含对象的压缩信息）关注来自输入图像的局部感知特征。在本文中，我们发现对注意力架构中聚合值进行归一化的设计决策对槽位注意力泛化到训练期间所见到的更多槽位和对象的能力有相当大的影响。我们认为，原始的槽位注意力归一化方案丢弃了像素先前分配给槽位的概率信息，这损害了其泛化能力。基于这些发现，我们提出并研究了替代的归一化方法，这些方法可以提高槽位注意力对不同槽位和对象数量的泛化能力，从而提高无监督图像分割任务的性能。||
|**2024-07-04**|[Detect Closer Surfaces that can be Seen: New Modeling and Evaluation in Cross-domain 3D Object Detection](http://arxiv.org/abs/2407.04061)|**[link](https://github.com/galaxy-zrx/edgehead)**|目前，域适应技术在自动驾驶三维目标检测领域的性能尚未达到理想水平，这主要是由于车辆尺寸的显著差异以及跨域应用时运行环境的不同。这些因素共同阻碍了从特定数据集中学习到的知识的有效迁移和应用。由于现有的评估指标最初是通过计算预测边界框和真实边界框之间的二维或三维重叠来设计用于单个域上的评估，因此它们经常会遇到由数据集之间的大小差异引起的过拟合问题。这引发了一个与评估三维目标检测模型跨域性能相关的基本问题：我们是否真的需要模型在跨域应用后保持其原始三维边界框的出色性能？从实际应用的角度来看，我们的主要关注点之一实际上是防止车辆与其他障碍物发生碰撞，特别是在跨域场景中，正确预测车辆尺寸要困难得多。换句话说，只要模型能够准确识别出距离自动驾驶车辆最近的表面，就足以有效避开障碍物。在本文中，我们提出了两个指标来衡量三维目标检测模型检测自动驾驶车辆传感器附近表面的能力，这可以用来更全面、更合理地评估其跨域性能。此外，我们提出了一个名为EdgeHead的优化头，用于引导模型更加关注可学习的较近表面，这可以极大地提高现有模型在我们的新指标下以及在原始BEV/3D指标下的跨域性能。||
|**2024-07-04**|[TrackPGD: A White-box Attack using Binary Masks against Robust Transformer Trackers](http://arxiv.org/abs/2407.03946)|null|使用Transformer骨干网络的目标跟踪器在视觉目标跟踪数据集上取得了强大的性能。然而，这些跟踪器的对抗鲁棒性在文献中尚未得到很好的研究。由于骨干网络的差异，为目标跟踪提出的对抗性白盒攻击不能迁移到所有类型的跟踪器。例如，像MixFormerM这样的Transformer跟踪器在黑盒攻击后仍然运行良好，特别是在预测目标二进制掩码方面。我们提出了一种名为TrackPGD的新型白盒攻击，它依靠预测的目标二进制掩码来攻击鲁棒的Transformer跟踪器。这种新攻击通过采用著名的SegPGD分割攻击来关注标注掩码，从而能够成功地对依赖Transformer骨干网络的跟踪器进行白盒攻击。实验结果表明，TrackPGD能够有效攻击基于Transformer的跟踪器，例如MixFormerM、OSTrackSTS和TransT-SEG，并在多个跟踪数据集上取得了成功。||
|**2024-07-04**|[DocXplain: A Novel Model-Agnostic Explainability Method for Document Image Classification](http://arxiv.org/abs/2407.03830)|null|深度学习（DL）彻底改变了文档图像分析领域，在各种任务中展现出超越人类的表现。然而，深度学习模型固有的黑盒性质仍然是其在行业中安全稳健部署的重大挑战。遗憾的是，尽管近年来大量研究致力于开发基于深度学习的文档分析系统，但解决其透明性方面的研究却相对较少。在本文中，我们旨在通过介绍 DocXplain 来弥合这一研究差距，这是一种新颖的模型无关的可解释性方法，专门设计用于为文档图像分类任务生成高可解释性的特征归因图。具体来说，我们的方法涉及将文档的前景和背景特征独立地分割成不同的文档元素，然后消融这些元素以分配特征重要性。我们在文档图像分类的背景下广泛评估了我们提出的方法，利用 4 种不同的评估指标、2 个广泛认可的文档基准数据集和 10 个最先进的文档图像分类模型。通过对 9 种现有的最先进的归因方法进行全面的定量和定性分析，我们证明了我们的方法在保真度和可解释性方面的优越性。据作者所知，这项工作提出了第一个专门针对文档图像量身定制的模型无关的基于归因的可解释性方法。我们预计我们的工作将极大地促进文档图像分类模型的透明度、公平性和鲁棒性研究的进展。||
|**2024-07-04**|[M^3:Manipulation Mask Manufacturer for Arbitrary-Scale Super-Resolution Mask](http://arxiv.org/abs/2407.03695)|null|在图像篡改定位（IML）领域，现有数据集数量少、质量差一直是主要问题。包含各种篡改类型的数据集将极大地提高IML模型的准确性。互联网上的图像（例如百度贴吧PS吧的图像）使用各种技术进行篡改，利用这些图像创建数据集将显著丰富我们数据中的篡改类型。然而，互联网上的图像存在分辨率和清晰度问题，通过简单地从原始图像中减去篡改图像获得的掩码包含各种噪声。这些噪声很难去除，导致掩码无法用于IML模型。受变化检测领域的启发，我们将原始图像和篡改图像视为同一图像随时间的变化，并将数据生成任务视为变化检测任务。然而，由于图像之间的清晰度问题，传统的变化检测模型表现不佳。因此，我们引入了一个超分辨率模块，并提出了篡改掩码生成器（MMM）框架。它增强了原始图像和篡改图像的分辨率，从而改善了图像细节，以便更好地进行比较。同时，该框架将原始图像和篡改图像转换为特征嵌入并进行拼接，有效地对上下文进行建模。此外，我们创建了篡改掩码生成器数据集（MMMD），这是一个涵盖了各种篡改技术的数据集。我们的目标是通过MMM和MMMD提供更真实的篡改数据，为图像取证和篡改检测领域做出贡献。有关MMMD和下载链接的详细信息，请访问：代码和数据集将公开。||
|**2024-07-03**|[Visual Grounding with Attention-Driven Constraint Balancing](http://arxiv.org/abs/2407.03243)|null|不同于目标检测，视觉定位任务需要检测由复杂的自由形式语言描述的对象。为了同时对这种复杂的语义和视觉表示进行建模，最近最先进的研究采用基于 Transformer 的模型来融合来自两种模态的特征，并进一步引入了各种模块来调节视觉特征，使其与语言表达保持一致并消除不相关的冗余信息。然而，它们的损失函数仍然采用常见的目标检测损失，只控制边界框回归输出，无法完全优化上述目标。为了解决这个问题，本文首先分析了基于 Transformer 模型的注意力机制。在此基础上，我们进一步提出了一个名为注意力驱动约束平衡（AttBalance）的新框架，以优化语言相关区域内视觉特征的行为。大量的实验结果表明，我们的方法带来了令人印象深刻的改进。具体来说，我们在四个不同基准上评估的五种不同模型上实现了持续的改进。此外，通过将我们的方法集成到 QRNet 中，我们获得了新的最先进的性能。||
|**2024-07-03**|[Category-Aware Dynamic Label Assignment with High-Quality Oriented Proposal](http://arxiv.org/abs/2407.03205)|null|航拍图像中的物体通常嵌入在复杂的背景中，并呈现任意方向。当使用定向边界框 (OBB) 表示任意方向的物体时，角度的周期性可能导致边界处标签回归值的不连续性，从而导致损失函数出现剧烈波动。为了解决这个问题，在定向检测框架中引入了一种基于复平面的 OBB 表示方法，并提出了一种三角损失函数。此外，利用对复杂背景环境和航拍图像中大型物体显著差异的先验知识，构建了一个 Conformer RPN 头部来预测角度信息。所提出的损失函数和 Conformer RPN 头部共同生成高质量的定向建议。针对仅依靠 IoU 进行建议标签分配的局限性，提出了一种基于预测类别反馈的类别感知动态标签分配方法。该方法使负样本选择更具代表性，确保分类和回归特征之间的一致性。在四个真实的定向检测数据集上进行了实验，结果表明，在参数调整和时间成本最小的情况下，定向目标检测的性能更优。具体而言，在 DOTA-v1.0、DOTA-v1.5、DIOR-R 和 HRSC2016 数据集上分别实现了 82.02%、71.99%、69.87% 和 98.77% 的平均精度均值 (mAP) 分数。||
|**2024-07-03**|[SegVG: Transferring Object Bounding Box to Segmentation for Visual Grounding](http://arxiv.org/abs/2407.03200)|**[link](https://github.com/weitaikang/segvg)**|与目标检测不同，视觉定位（Visual Grounding）旨在为每个文本-图像对检测一个边界框。这种为每个文本-图像数据提供一个边界框的方式提供了稀疏的监督信号。尽管先前的工作取得了令人瞩目的成果，但它们对标注的被动利用，即将边界框标注仅用作回归真值，导致了性能欠佳。在本文中，我们提出了SegVG，这是一种将边界框级标注转换为分割信号的新方法，以便为视觉定位提供额外的像素级监督。具体来说，我们提出了多层多任务编码器-解码器作为目标定位阶段，在该阶段中，我们学习回归查询和多个分割查询，分别通过在每个解码层中对边界框进行回归和分割来定位目标。这种方法使我们能够迭代地利用标注作为边界框级回归和像素级分割的信号。此外，由于骨干网络通常由从单模态任务中学习到的预训练参数初始化，并且用于回归和分割的查询都是静态可学习的嵌入，因此这三种类型的特征之间存在域差异，这会损害后续的目标定位。为了减轻这种差异，我们引入了三重对齐模块，其中查询、文本和视觉标记通过三重注意力机制进行三角更新，以共享相同的空间。在五个广泛使用的数据集上进行的大量实验验证了我们的方法达到了最先进的性能 (SOTA)。||
|**2024-07-03**|[Global Context Modeling in YOLOv8 for Pediatric Wrist Fracture Detection](http://arxiv.org/abs/2407.03163)|**[link](https://github.com/ruiyangju/yolo_global_context_fracture_detection)**|儿童在日常生活中经常遭受腕部损伤，而骨折损伤放射科医生通常需要在外科医生进行手术治疗之前分析和解释 X 光图像。深度学习的发展使神经网络模型能够作为计算机辅助诊断 (CAD) 工具来帮助医生和专家进行诊断。由于 YOLOv8 模型在目标检测任务中取得了令人满意的成功，因此它已被应用于骨折检测。全局上下文 (GC) 模块以轻量级的方式有效地对全局上下文进行建模，将其融入 YOLOv8 可以极大地提高模型性能。本文提出了用于骨折检测的 YOLOv8+GC 模型，它是具有 GC 模块的 YOLOv8 模型的改进版本。实验结果表明，与原始的 YOLOv8 模型相比，所提出的 YOLOv8-GC 模型在 GRAZPEDWRI-DX 数据集上将交并比阈值为 0.5 时的平均精度均值 (mAP 50) 从 63.58% 提高到 66.32%，达到了最先进的水平 (SOTA)。这项工作的实现代码可在 GitHub 上获取：https://github.com/RuiyangJu/YOLOv8_Global_Context_Fracture_Detection。||
|**2024-07-03**|[Applying Extended Object Tracking for Self-Localization of Roadside Radar Sensors](http://arxiv.org/abs/2407.03084)|null|智能交通系统 (ITS) 可以受益于路边 4D 毫米波雷达传感器，用于大规模交通监控，因为它们具有全天候功能、长感应范围和低制造成本。然而，在城市环境中，使用外部测量设备的定位方法存在局限性。此外，如果传感器安装由于环境影响而出现变化，则在仅在安装期间执行测量时无法对其进行校正。在本文中，我们提出了使用扩展目标跟踪 (EOT) 的路边雷达数据自定位方法。该方法分析传感器观察到的车辆跟踪轨迹和城市街道的航空激光扫描，将“直行”、“左转”、“右转”等驾驶行为标签分配给轨迹段和路段，并执行语义迭代最近点 (SICP) 算法来配准点云。该方法利用下游任务（目标跟踪）的结果进行定位。我们展示了亚米范围内的高精度以及非常低的方位误差。该方法还显示出良好的数据效率。评估在仿真和实际测试中均已完成。||
|**2024-07-03**|[YOLOv5, YOLOv8 and YOLOv10: The Go-To Detectors for Real-time Vision](http://arxiv.org/abs/2407.02988)|null|本文全面回顾了YOLO（You Only Look Once）目标检测算法的演进过程，重点关注YOLOv5、YOLOv8和YOLOv10。我们分析了这些版本在架构改进、性能提升以及边缘部署适用性方面的差异。YOLOv5引入了CSPDarknet骨干网络和Mosaic数据增强等重大创新，实现了速度和精度之间的平衡。YOLOv8在此基础上，通过增强特征提取和无锚框检测，提高了算法的通用性和性能。YOLOv10则凭借无NMS训练、空间通道解耦下采样以及大核卷积等技术实现了跨越式发展，以更低的计算开销实现了最先进的性能。我们的研究结果突出了YOLO算法在精度、效率和实时性能方面的逐步提升，特别强调了其在资源受限环境中的适用性。本综述提供了模型复杂度和检测精度之间权衡的见解，为针对特定边缘计算应用选择最合适的YOLO版本提供了指导。||
|**2024-07-03**|[ShiftAddAug: Augment Multiplication-Free Tiny Neural Network with Hybrid Computation](http://arxiv.org/abs/2407.02881)|null|缺乏乘法运算符（例如移位和加法）因其与硬件的兼容性而受到关注。然而，与具有相同结构的传统神经网络 (NN) 相比，采用这些运算符的神经网络 (NN) 通常表现出较低的精度。ShiftAddAug 使用成本高昂的乘法来增强高效但功能较弱的无乘法运算符，从而在没有任何推理开销的情况下提高性能。它将一个 ShiftAdd 小型神经网络放入一个大型乘法模型中，并鼓励将其训练为子模型以获得额外的监督。为了解决混合运算符之间的权重差异问题，提出了一种新的权重共享方法。此外，一种新颖的两阶段神经架构搜索用于为更小但更强的无乘法小型神经网络获得更好的增强效果。ShiftAddAug 的优越性通过图像分类和语义分割实验得到验证，始终如一地提供显着的增强。值得注意的是，与直接训练的对应模型相比，它在 CIFAR100 上的准确率提高了 4.95%，甚至超过了乘法神经网络的性能。||
|**2024-07-03**|[A Pairwise DomMix Attentive Adversarial Network for Unsupervised Domain Adaptive Object Detection](http://arxiv.org/abs/2407.02835)|null|无监督域自适应目标检测 (DAOD) 可以使在一个源域上训练的模型适应未标记的目标域，以进行目标检测。现有的无监督 DAOD 方法通常执行从目标域到源域的特征对齐。单向域迁移会忽略有关目标样本的信息，并在存在较大域差异时导致欠佳的自适应。因此，我们提出了一种具有域混合 (DomMix) 模块的成对注意力对抗网络，以缓解上述挑战。具体来说，采用深度混合来构建一个中间域，允许来自两个域的特征共享它们的差异。然后，应用成对注意力对抗网络，在不同尺度的图像级和实例级特征上进行注意力编码，并通过对抗学习优化域对齐。这使得网络能够专注于具有不同上下文信息的区域，并学习它们在不同域之间的相似性。在几个基准数据集上进行了广泛的实验，证明了我们提出的方法的优越性。||
|**2024-07-03**|[Data Overfitting for On-Device Super-Resolution with Dynamic Algorithm and Compiler Co-Design](http://arxiv.org/abs/2407.02813)|**[link](https://github.com/coulsonlee/dy-dca-ecc)**|深度神经网络 (DNN) 经常被应用于各种计算机视觉应用。如今，当前视频分发系统中一个新兴趋势是利用 DNN 的过拟合特性来执行视频分辨率提升。通过将视频分割成块并应用超分辨率 (SR) 模型对每个块进行过拟合，这种 SR 模型加视频块的方案能够取代传统的视频传输，从而提高视频质量和传输效率。然而，为了保证高性能，需要许多模型和块，这会导致用户端的模型切换和内存占用方面产生巨大的开销。为了解决这些问题，我们提出了一种由内容感知数据处理管道辅助的动态深度神经网络，以将模型数量减少到一个 (Dy-DCA)，这有助于在节省计算资源的同时提高性能。此外，为了在用户端实现真正的加速，我们设计了一个框架来优化 Dy-DCA 中的动态特征（例如，动态形状、大小和控制流），从而实现一系列编译优化，包括融合代码生成、静态执行计划等。通过采用这些技术，我们的方法在现成的手机上实现了更好的 PSNR 和实时性能 (33 FPS)。同时，在我们的编译优化的辅助下，我们实现了 1.7 倍的加速，同时节省了高达 1.61 倍的内存消耗。代码可在 https://github.com/coulsonlee/Dy-DCA-ECCV2024 获取。||
|**2024-07-03**|[Fine-Grained Scene Image Classification with Modality-Agnostic Adapter](http://arxiv.org/abs/2407.02769)|**[link](https://github.com/qunilcs/maa)**|在处理细粒度场景图像分类任务时，以往的大多数工作在进行多模态特征融合时都非常重视全局视觉特征。换句话说，模型的设计是基于对不同模态重要性的先验直觉。在本文中，我们提出了一种新的多模态特征融合方法，称为MAA（模态无关适配器），试图使模型自适应地学习不同模态在不同情况下的重要性，而无需在模型架构中进行先验设置。更具体地说，我们消除了分布中的模态差异，然后使用模态无关的Transformer编码器进行语义级别的特征融合。我们的实验表明，通过应用与以前方法相同的模态，MAA在基准测试中取得了最先进的结果。此外，值得一提的是，在使用MAA时，可以轻松添加新的模态，并进一步提升性能。代码可在https://github.com/quniLcs/MAA获取。||

## 生成模型

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-08-22**|[xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations](http://arxiv.org/abs/2408.12590)|null|我们推出了 xGen-VideoSyn-1，这是一种能够根据文本描述生成逼真场景的文本到视频 (T2V) 生成模型。基于 OpenAI 的 Sora 等最新进展，我们探索了潜在扩散模型 (LDM) 架构，并引入了一种视频变分自动编码器 (VidVAE)。VidVAE 在空间和时间上压缩视频数据，显著减少了视觉标记的长度以及生成长序列视频的计算需求。为了进一步解决计算成本问题，我们提出了一种分而治之的策略，可以在视频片段之间保持时间一致性。我们的扩散 Transformer (DiT) 模型结合了空间和时间自注意力层，能够在不同的时间范围和纵横比下实现稳健的泛化。我们从一开始就设计了一条数据处理管道，并收集了超过 1300 万个高质量的视频-文本对。该管道包括多个步骤，例如剪辑、文本检测、运动估计、美学评分，以及基于我们内部视频-LLM 模型的密集字幕。训练 VidVAE 和 DiT 模型分别需要大约 40 天和 642 个 H100 天。我们的模型支持以端到端的方式生成超过 14 秒的 720p 视频，并展示出与最先进的 T2V 模型相比具有竞争力的性能。||
|**2024-08-22**|[ssProp: Energy-Efficient Training for Convolutional Neural Networks with Scheduled Sparse Back Propagation](http://arxiv.org/abs/2408.12561)|**[link](https://github.com/lujiazho/ssprop)**|近年来，深度学习取得了显著进步，尤其是在生成模型方面，例如大型语言模型和概率扩散模型。然而，训练这些模型通常需要大量的计算资源，需要数十亿 petaFLOPs 的计算量。这种高资源消耗导致大量的能源使用和碳足迹，引发了严重的环境问题。反向传播 (BP) 是深度学习模型训练过程中计算开销的主要来源。为了推进节能训练的研究并允许在任何机器和设备上进行稀疏学习，我们提出了一种通用的、节能的卷积模块，可以无缝集成到任何深度学习架构中。具体来说，我们在反向传播过程中引入了通道稀疏性以及额外的梯度选择调度器，基于以下假设：反向传播通常是密集且低效的，这可能导致过拟合和高计算量。我们的实验表明，我们的方法在图像分类和生成任务上验证了可以减少 40% 的计算量，同时有可能提高模型性能。这种减少可以在大规模人工智能系统的研发阶段节省大量能源并减少碳足迹。此外，我们的方法以不同于 Dropout 的方式减轻了过拟合，使其可以与 Dropout 结合使用，以进一步提高模型性能并减少计算资源的使用。大量实验验证了我们的方法可以推广到各种数据集和任务，并且与各种深度学习架构和模块兼容。代码可在 https://github.com/lujiazho/ssProp 公开获取。||
|**2024-08-22**|[Show-o: One Single Transformer to Unify Multimodal Understanding and Generation](http://arxiv.org/abs/2408.12528)|**[link](https://github.com/showlab/show-o)**|We present a unified transformer, i.e., Show-o, that unifies multimodal understanding and generation. Unlike fully autoregressive models, Show-o unifies autoregressive and (discrete) diffusion modeling to adaptively handle inputs and outputs of various and mixed modalities. The unified model flexibly supports a wide range of vision-language tasks including visual question-answering, text-to-image generation, text-guided inpainting/extrapolation, and mixed-modality generation. Across various benchmarks, it demonstrates comparable or superior performance to existing individual models with an equivalent or larger number of parameters tailored for understanding or generation. This significantly highlights its potential as a next-generation foundation model. Code and models are released at https://github.com/showlab/Show-o.||
|**2024-08-22**|[FlexEdit: Marrying Free-Shape Masks to VLLM for Flexible Image Editing](http://arxiv.org/abs/2408.12429)|**[link](https://github.com/a-new-b/flex_edit)**|将视觉大型语言模型 (VLLM) 与扩散模型相结合，为基于人类语言指令执行图像编辑任务提供了一种强大的方法。然而，仅凭语言指令通常无法准确传达用户需求，尤其是当用户想要在图像的特定区域添加或替换元素时。幸运的是，掩码可以有效地指示要编辑的确切位置或元素，但它们要求用户在所需位置精确绘制形状，这对用户很不友好。为了解决这个问题，我们提出了 FlexEdit，一种端到端的图像编辑方法，它利用自由形状的掩码和语言指令来进行灵活编辑。我们的方法采用 VLLM 来理解图像内容、掩码和用户指令。此外，我们还引入了掩码增强适配器 (MEA)，它将 VLLM 的嵌入与图像数据融合在一起，确保了掩码信息和模型输出嵌入的无缝集成。此外，我们还构建了 FSMI-Edit，这是一个专门为自由形状掩码量身定制的基准，包括 8 种类型的自由形状掩码。大量实验表明，我们的方法在基于 LLM 的图像编辑中取得了最先进的结果，并且我们简单的提示技术在其有效性方面也很突出。代码和数据可以在 https://github.com/A-new-b/flex_edit. 找到。||
|**2024-08-22**|[4D Diffusion for Dynamic Protein Structure Prediction with Reference Guided Motion Alignment](http://arxiv.org/abs/2408.12419)|null|Protein structure prediction is pivotal for understanding the structure-function relationship of proteins, advancing biological research, and facilitating pharmaceutical development and experimental design. While deep learning methods and the expanded availability of experimental 3D protein structures have accelerated structure prediction, the dynamic nature of protein structures has received limited attention. This study introduces an innovative 4D diffusion model incorporating molecular dynamics (MD) simulation data to learn dynamic protein structures. Our approach is distinguished by the following components: (1) a unified diffusion model capable of generating dynamic protein structures, including both the backbone and side chains, utilizing atomic grouping and side-chain dihedral angle predictions; (2) a reference network that enhances structural consistency by integrating the latent embeddings of the initial 3D protein structures; and (3) a motion alignment module aimed at improving temporal structural coherence across multiple time steps. To our knowledge, this is the first diffusion-based model aimed at predicting protein trajectories across multiple time steps simultaneously. Validation on benchmark datasets demonstrates that our model exhibits high accuracy in predicting dynamic 3D structures of proteins containing up to 256 amino acids over 32 time steps, effectively capturing both local flexibility in stable states and significant conformational changes.||
|**2024-08-22**|[CODE: Confident Ordinary Differential Editing](http://arxiv.org/abs/2408.12418)|**[link](https://github.com/vita-epfl/code)**|Conditioning image generation facilitates seamless editing and the creation of photorealistic images. However, conditioning on noisy or Out-of-Distribution (OoD) images poses significant challenges, particularly in balancing fidelity to the input and realism of the output. We introduce Confident Ordinary Differential Editing (CODE), a novel approach for image synthesis that effectively handles OoD guidance images. Utilizing a diffusion model as a generative prior, CODE enhances images through score-based updates along the probability-flow Ordinary Differential Equation (ODE) trajectory. This method requires no task-specific training, no handcrafted modules, and no assumptions regarding the corruptions affecting the conditioning image. Our method is compatible with any diffusion model. Positioned at the intersection of conditional image generation and blind image restoration, CODE operates in a fully blind manner, relying solely on a pre-trained generative model. Our method introduces an alternative approach to blind restoration: instead of targeting a specific ground truth image based on assumptions about the underlying corruption, CODE aims to increase the likelihood of the input image while maintaining fidelity. This results in the most probable in-distribution image around the input. Our contributions are twofold. First, CODE introduces a novel editing method based on ODE, providing enhanced control, realism, and fidelity compared to its SDE-based counterpart. Second, we introduce a confidence interval-based clipping method, which improves CODE's effectiveness by allowing it to disregard certain pixels or information, thus enhancing the restoration process in a blind manner. Experimental results demonstrate CODE's effectiveness over existing methods, particularly in scenarios involving severe degradation or OoD inputs.||
|**2024-08-22**|[Dynamic PDB: A New Dataset and a SE(3) Model Extension by Integrating Dynamic Behaviors and Physical Properties in Protein Structures](http://arxiv.org/abs/2408.12413)|null|Despite significant progress in static protein structure collection and prediction, the dynamic behavior of proteins, one of their most vital characteristics, has been largely overlooked in prior research. This oversight can be attributed to the limited availability, diversity, and heterogeneity of dynamic protein datasets. To address this gap, we propose to enhance existing prestigious static 3D protein structural databases, such as the Protein Data Bank (PDB), by integrating dynamic data and additional physical properties. Specifically, we introduce a large-scale dataset, Dynamic PDB, encompassing approximately 12.6K proteins, each subjected to all-atom molecular dynamics (MD) simulations lasting 1 microsecond to capture conformational changes. Furthermore, we provide a comprehensive suite of physical properties, including atomic velocities and forces, potential and kinetic energies of proteins, and the temperature of the simulation environment, recorded at 1 picosecond intervals throughout the simulations. For benchmarking purposes, we evaluate state-of-the-art methods on the proposed dataset for the task of trajectory prediction. To demonstrate the value of integrating richer physical properties in the study of protein dynamics and related model design, we base our approach on the SE(3) diffusion model and incorporate these physical properties into the trajectory prediction process. Preliminary results indicate that this straightforward extension of the SE(3) model yields improved accuracy, as measured by MAE and RMSD, when the proposed physical properties are taken into consideration.||
|**2024-08-22**|[A Stable Polygamy Approach to Spectrum Access with Channel Reuse](http://arxiv.org/abs/2408.12402)|null|We introduce a new and broader formulation of the stable marriage problem (SMP), called the stable polygamy problem (SPP), where multiple individuals from a larger group $L$ of$ |L|
|**2024-08-22**|[LCM-SVC: Latent Diffusion Model Based Singing Voice Conversion with Inference Acceleration via Latent Consistency Distillation](http://arxiv.org/abs/2408.12354)|null|Any-to-any singing voice conversion (SVC) aims to transfer a target singer's timbre to other songs using a short voice sample. However many diffusion model based any-to-any SVC methods, which have achieved impressive results, usually suffered from low efficiency caused by a mass of inference steps. In this paper, we propose LCM-SVC, a latent consistency distillation (LCD) based latent diffusion model (LDM) to accelerate inference speed. We achieved one-step or few-step inference while maintaining the high performance by distilling a pre-trained LDM based SVC model, which had the advantages of timbre decoupling and sound quality. Experimental results show that our proposed method can significantly reduce the inference time and largely preserve the sound quality and timbre similarity comparing with other state-of-the-art SVC models. Audio samples are available at https://sounddemos.github.io/lcm-svc.||
|**2024-08-22**|[GarmentAligner: Text-to-Garment Generation via Retrieval-augmented Multi-level Corrections](http://arxiv.org/abs/2408.12352)|null|General text-to-image models bring revolutionary innovation to the fields of arts, design, and media. However, when applied to garment generation, even the state-of-the-art text-to-image models suffer from fine-grained semantic misalignment, particularly concerning the quantity, position, and interrelations of garment components. Addressing this, we propose GarmentAligner, a text-to-garment diffusion model trained with retrieval-augmented multi-level corrections. To achieve semantic alignment at the component level, we introduce an automatic component extraction pipeline to obtain spatial and quantitative information of garment components from corresponding images and captions. Subsequently, to exploit component relationships within the garment images, we construct retrieval subsets for each garment by retrieval augmentation based on component-level similarity ranking and conduct contrastive learning to enhance the model perception of components from positive and negative samples. To further enhance the alignment of components across semantic, spatial, and quantitative granularities, we propose the utilization of multi-level correction losses that leverage detailed component information. The experimental findings demonstrate that GarmentAligner achieves superior fidelity and fine-grained semantic alignment when compared to existing competitors.||
|**2024-08-20**|[Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model](http://arxiv.org/abs/2408.11039)|null|我们推出了 Transfusion，这是一种用于训练离散和连续数据的多模态模型的方法。Transfusion 将语言建模损失函数（下一个标记预测）与扩散模型相结合，以训练单个覆盖混合模态序列的 Transformer。我们从头开始在文本和图像数据的混合上预训练多个参数高达 7B 的 Transfusion 模型，并针对各种单模态和跨模态基准建立了规模法则。我们的实验表明，Transfusion 的扩展性明显优于量化图像并在离散图像标记上训练语言模型的方法。通过引入特定于模态的编码和解码层，我们可以进一步提高 Transfusion 模型的性能，甚至可以将每张图像压缩到仅 16 个图像块。我们进一步证明，将 Transfusion 方法扩展到 7B 参数和 2T 多模态标记，可以生成与类似规模的扩散模型和语言模型相媲美的图像和文本，从而兼具两者的优势。||
|**2024-08-20**|[MegaFusion: Extend Diffusion Models towards Higher-resolution Image Generation without Further Tuning](http://arxiv.org/abs/2408.11001)|**[link](https://github.com/ShaochengShen/MegaFusion)**|扩散模型凭借其令人印象深刻的能力，已经成为文本到图像生成领域的领跑者。然而，它们在训练过程中固定的图像分辨率常常导致在生成高分辨率图像时出现挑战，例如语义不准确和对象复制。本文介绍了 MegaFusion，这是一种新颖的方法，可以扩展现有的基于扩散的文本到图像生成模型，从而在无需额外微调或额外适应的情况下实现高效的高分辨率生成。具体而言，我们采用了一种创新的截断和中继策略来连接不同分辨率下的去噪过程，从而允许以从粗到精的方式生成高分辨率图像。此外，通过集成扩张卷积和噪声重新调度，我们进一步调整了模型的先验，以适应更高的分辨率。MegaFusion 的多功能性和有效性使其普遍适用于潜在空间和像素空间扩散模型，以及其他衍生模型。大量实验表明，MegaFusion 显着增强了现有模型生成百万像素和各种纵横比图像的能力，而仅需要大约 40% 的原始计算成本。||
|**2024-08-20**|[GreediRIS: Scalable Influence Maximization using Distributed Streaming Maximum Cover](http://arxiv.org/abs/2408.10982)|null|影响力最大化——在网络中识别k个有影响力的种子（顶点）子集的问题——是网络科学中的一个经典问题，有着广泛的应用。这个问题是NP难的，但是存在有效的多项式时间近似算法。然而，由于与随机抽样和大规模聚合相关的步骤的复杂性，扩展这些算法仍然是一项艰巨的任务。在本文中，我们提出了一种新的用于影响力最大化的并行分布式近似算法，该算法具有可证明的近似保证。我们提出的方法称为GreediRIS，它利用RandGreedi框架（一种用于分布式子模优化最先进的方法）来解决计算最大k覆盖的步骤。GreediRIS结合了分布式和流式计算模型以及剪枝技术，有效地解决了算法的通信瓶颈。在NERSC Perlmutter超级计算机上多达512个节点（32K个核心）的实验结果表明，GreediRIS可以实现良好的强扩展性能，保持质量，并显著优于其他最先进的分布式实现。例如，在512个节点上，GreediRIS性能最佳的变体在两种不同的扩散模型下，相对于最先进的并行实现，实现了28.99倍和36.35倍的几何平均加速比。我们还提出了GreediRIS的通信优化版本，该版本将速度进一步提高了两个数量级。||
|**2024-08-20**|[Kilometer-Scale Convection Allowing Model Emulation using Generative Diffusion Modeling](http://arxiv.org/abs/2408.10958)|null|风暴尺度对流允许模型 (CAM) 是预测雷暴和中尺度对流系统演变的重要工具，这些系统会导致破坏性的极端天气。通过明确解析大气中的对流动力学，它们为气象学家提供了预测灾害所需的细微差别。迄今为止，深度学习模型尚未被证明能够熟练地进行公里级大气模拟，尽管在分辨率较低的情况下，它们与最先进的全球中期天气预报相比具有竞争力。我们提出了一种名为 StormCast 的生成扩散模型，它模拟了高分辨率快速刷新 (HRRR) 模型——NOAA 最先进的 3 公里业务 CAM。StormCast 以公里级分辨率自回归预测 99 个状态变量，使用 1 小时的时间步长，在大气边界层中具有密集的垂直分辨率，并以 26 个天气变量为条件。我们提供的证据表明，成功学习了公里级动力学，包括对复合雷达反射率的 1-6 小时竞争性预测技巧，以及物理上逼真的对流簇演变、湿气上升气流和冷池形态。StormCast 预测在多小时预测中保持了多个预测变量的真实功率谱。总之，这些结果确立了自回归机器学习模拟 CAM 的潜力——为区域机器学习天气预测和未来气候灾害动态降尺度开辟了新的公里级前沿。||
|**2024-08-20**|[Large Point-to-Gaussian Model for Image-to-3D Generation](http://arxiv.org/abs/2408.10935)|null|Recently, image-to-3D approaches have significantly advanced the generation quality and speed of 3D assets based on large reconstruction models, particularly 3D Gaussian reconstruction models. Existing large 3D Gaussian models directly map 2D image to 3D Gaussian parameters, while regressing 2D image to 3D Gaussian representations is challenging without 3D priors. In this paper, we propose a large Point-to-Gaussian model, that inputs the initial point cloud produced from large 3D diffusion model conditional on 2D image to generate the Gaussian parameters, for image-to-3D generation. The point cloud provides initial 3D geometry prior for Gaussian generation, thus significantly facilitating image-to-3D Generation. Moreover, we present the \textbf{A}ttention mechanism, \textbf{P}rojection mechanism, and \textbf{P}oint feature extractor, dubbed as \textbf{APP} block, for fusing the image features with point cloud features. The qualitative and quantitative experiments extensively demonstrate the effectiveness of the proposed approach on GSO and Objaverse datasets, and show the proposed method achieves state-of-the-art performance.||
|**2024-08-20**|[LBC: Language-Based-Classifier for Out-Of-Variable Generalization](http://arxiv.org/abs/2408.10923)|**[link](https://github.com/sksmssh/lbcforoovgen)**|Large Language Models (LLMs) have great success in natural language processing tasks such as response generation. However, their use in tabular data has been limited due to their inferior performance compared to traditional machine learning models (TMLs) such as XGBoost. We find that the pre-trained knowledge of LLMs enables them to interpret new variables that appear in a test without additional training, a capability central to the concept of Out-of-Variable (OOV). From the findings, we propose a Language-Based-Classifier (LBC), a classifier that maximizes the benefits of LLMs to outperform TMLs on OOV tasks. LBC employs three key methodological strategies: 1) Categorical changes to adjust data to better fit the model's understanding, 2) Advanced order and indicator to enhance data representation to the model, and 3) Using verbalizer to map logit scores to classes during inference to generate model predictions. These strategies, combined with the pre-trained knowledge of LBC, emphasize the model's ability to effectively handle OOV tasks. We empirically and theoretically validate the superiority of LBC. LBC is the first study to apply an LLM-based model to OOV tasks. The source code is at https://github.com/ASDASDanonymous/Language-Based-Classifier-forOOVtasks.||
|**2024-08-20**|[A Grey-box Attack against Latent Diffusion Model-based Image Editing by Posterior Collapse](http://arxiv.org/abs/2408.10901)|null|Recent advancements in generative AI, particularly Latent Diffusion Models (LDMs), have revolutionized image synthesis and manipulation. However, these generative techniques raises concerns about data misappropriation and intellectual property infringement. Adversarial attacks on machine learning models have been extensively studied, and a well-established body of research has extended these techniques as a benign metric to prevent the underlying misuse of generative AI. Current approaches to safeguarding images from manipulation by LDMs are limited by their reliance on model-specific knowledge and their inability to significantly degrade semantic quality of generated images. In response to these shortcomings, we propose the Posterior Collapse Attack (PCA) based on the observation that VAEs suffer from posterior collapse during training. Our method minimizes dependence on the white-box information of target models to get rid of the implicit reliance on model-specific knowledge. By accessing merely a small amount of LDM parameters, in specific merely the VAE encoder of LDMs, our method causes a substantial semantic collapse in generation quality, particularly in perceptual consistency, and demonstrates strong transferability across various model architectures. Experimental results show that PCA achieves superior perturbation effects on image generation of LDMs with lower runtime and VRAM. Our method outperforms existing techniques, offering a more robust and generalizable solution that is helpful in alleviating the socio-technical challenges posed by the rapidly evolving landscape of generative AI.||
|**2024-08-20**|[Low-Quality Image Detection by Hierarchical VAE](http://arxiv.org/abs/2408.10885)|null|To make an employee roster, photo album, or training dataset of generative models, one needs to collect high-quality images while dismissing low-quality ones. This study addresses a new task of unsupervised detection of low-quality images. We propose a method that not only detects low-quality images with various types of degradation but also provides visual clues of them based on an observation that partial reconstruction by hierarchical variational autoencoders fails for low-quality images. The experiments show that our method outperforms several unsupervised out-of-distribution detection methods and also gives visual clues for low-quality images that help humans recognize them even in thumbnail view.||
|**2024-08-20**|[Hedging in Jump Diffusion Model with Transaction Costs](http://arxiv.org/abs/2408.10785)|null|We consider the jump-diffusion risky asset model and study its conditional prediction laws. Next, we explain the conditional least square hedging strategy and calculate its closed form for the jump-diffusion model, considering the Black-Scholes framework with interpretations related to investor priorities and transaction costs. We investigate the explicit form of this result for the particular case of the European call option under transaction costs and formulate recursive hedging strategies. Finally, we present a decision tree, table of values, and figures to support our results.||
|**2024-08-20**|[Generating Synthetic Fair Syntax-agnostic Data by Learning and Distilling Fair Representation](http://arxiv.org/abs/2408.10755)|null|Data Fairness is a crucial topic due to the recent wide usage of AI powered applications. Most of the real-world data is filled with human or machine biases and when those data are being used to train AI models, there is a chance that the model will reflect the bias in the training data. Existing bias-mitigating generative methods based on GANs, Diffusion models need in-processing fairness objectives and fail to consider computational overhead while choosing computationally-heavy architectures, which may lead to high computational demands, instability and poor optimization performance. To mitigate this issue, in this work, we present a fair data generation technique based on knowledge distillation, where we use a small architecture to distill the fair representation in the latent space. The idea of fair latent space distillation enables more flexible and stable training of Fair Generative Models (FGMs). We first learn a syntax-agnostic (for any data type) fair representation of the data, followed by distillation in the latent space into a smaller model. After distillation, we use the distilled fair latent space to generate high-fidelity fair synthetic data. While distilling, we employ quality loss (for fair distillation) and utility loss (for data utility) to ensure that the fairness and data utility characteristics remain in the distilled latent space. Our approaches show a 5%, 5% and 10% rise in performance in fairness, synthetic sample quality and data utility, respectively, than the state-of-the-art fair generative model.||
|**2024-08-16**|[Comparative Analysis of Generative Models: Enhancing Image Synthesis with VAEs, GANs, and Stable Diffusion](http://arxiv.org/abs/2408.08751)|null|本文探讨了三种主要的生成模型框架：变分自编码器 (VAE)、生成对抗网络 (GAN) 和稳定扩散模型。VAE 擅长学习潜在表征，但经常产生模糊的结果。GAN 可以生成逼真的图像，但面临着诸如模式崩溃之类的问题。稳定扩散模型虽然可以生成具有高度语义连贯性的高质量图像，但在计算资源方面要求很高。此外，本文还探讨了如何将 Grounding DINO 和 Grounded SAM 与稳定扩散模型相结合，通过利用先进的分割和修复技术来提高图像的准确性。该分析为针对不同应用选择合适的模型提供了指导，并强调了未来研究的领域。||
|**2024-08-16**|[ChatZero:Zero-shot Cross-Lingual Dialogue Generation via Pseudo-Target Language](http://arxiv.org/abs/2408.08724)|null|虽然大型语言模型 (LLM) 显示出惊人的能力，但在各种已发现的 LLM 激动人心的应用中，其他低资源语言的表现却不尽如人意。此外，大多数现有方法依赖于大规模对话语料库，因此在零样本场景下构建对话生成系统仍然是一项相当大的挑战。为了应对这一挑战，我们提出了一种基于跨语言代码转换方法的新型端到端零样本对话生成模型 ChatZero。首先，我们使用占位符构建代码转换语言和伪目标语言。然后，为了进行跨语言语义迁移，我们采用无监督对比学习来最小化源语言、代码转换语言和伪目标语言在高维语义空间中互为正例的语义差距。在多语言 DailyDialog 和 DSTC7-AVSD 数据集上的实验表明，ChatZero 在零样本情况下可以达到监督学习原始性能的 90% 以上，并且与其他基线方法相比达到了最先进的性能。||
|**2024-08-16**|[An End-to-End Model for Photo-Sharing Multi-modal Dialogue Generation](http://arxiv.org/abs/2408.08650)|null|照片分享多模态对话生成要求对话代理不仅要生成文本回复，还要在适当的时候分享照片。以往使用图像文本描述作为桥梁，将图像描述模型、文本生成模型和图像生成模型集成到流水线模型中来处理这种复杂的多模态任务。然而，用文本描述来表示图像可能会丢失重要的视觉细节和信息，并在复杂的对话系统中造成错误传播。此外，流水线模型将三个模型分别隔离，因为离散的图像文本描述阻碍了端到端的梯度传播。我们提出了第一个用于照片分享多模态对话生成的端到端模型，该模型将图像感知器和图像生成器与大型语言模型集成在一起。大型语言模型在输入端采用Q-Former来感知视觉图像。对于输出端的图像生成，我们提出了一种动态词汇转换矩阵，并使用直通和gumbel-softmax技术来对齐大型语言模型和稳定扩散模型，实现端到端的梯度传播。我们在PhotoChat和DialogCC数据集上进行了实验，以评估我们的端到端模型。与流水线模型相比，端到端模型在文本和图像生成的各种指标上都获得了最先进的性能。更多分析实验也验证了端到端模型对照片分享多模态对话生成的有效性。||
|**2024-08-16**|[Modeling the Neonatal Brain Development Using Implicit Neural Representations](http://arxiv.org/abs/2408.08647)|**[link](https://github.com/florentinbieder/neonatal-development-inr)**|人类大脑在妊娠晚期经历快速发育。本研究以发育中的人类连接组计划 (dHCP) 中早产儿和足月新生儿的 MR 图像为基础，对该年龄范围内婴儿大脑的新生儿发育进行建模。我们提出了一个神经网络，特别是一种隐式神经表示 (INR)，用于预测不同时间点的二维和三维图像。为了模拟特定受试者的发育过程，需要在 INR 的潜在空间中将年龄与受试者身份分离。我们提出了两种方法，即特定受试者潜在向量 (SSL) 和随机全局潜在增强 (SGLA)，以实现这种分离。我们对结果进行了分析，并将我们提出的模型与作为基线的年龄条件去噪扩散模型进行了比较。我们还表明，我们的方法可以以内存高效的方式应用，这对于 3D 数据尤其重要。||
|**2024-08-16**|[Sampling effects on Lasso estimation of drift functions in high-dimensional diffusion processes](http://arxiv.org/abs/2408.08638)|null|本文研究了扩散模型中漂移函数的高维参数估计问题，特别关注于在离散时间点观察到的 $d$维遍历扩散过程。假设参数向量是稀疏的，我们检验了用于未知参数的Lasso估计器的统计行为。我们的主要贡献是证明了Lasso估计器的oracle不等式，该不等式在我们分析中定义的三个特定集合的交集上成立。我们仔细控制了这些集合的概率，解决了我们研究的核心挑战。这种方法使我们能够推导出$l_1$和$l_2$ 范数的误差界，从而评估所提出的Lasso估计器的性能。我们的结果表明，在某些条件下，离散化误差可以忽略不计，从而使我们能够实现与观察过程的连续轨迹相同的最佳收敛速度。我们通过数值实验验证了我们的理论发现，实验结果表明，Lasso估计器在支持恢复方面明显优于最大似然估计器（MLE）。||
|**2024-08-16**|[Generative Dataset Distillation Based on Diffusion Model](http://arxiv.org/abs/2408.08610)|**[link](https://github.com/guang000/banko)**|本文介绍了我们针对 ECCV 2024 首届数据集蒸馏挑战赛生成赛道的方法。由于扩散模型因其高质量的生成效果已成为生成模型的主流，我们专注于基于扩散模型的蒸馏方法。考虑到该赛道只能使用生成模型在 10 分钟内为 CIFAR-100 和 Tiny-ImageNet 数据集生成固定数量的图像，因此我们需要使用能够高速生成图像的生成模型。在本研究中，我们提出了一种基于 Stable Diffusion 的新型生成数据集蒸馏方法。具体来说，我们使用能够高速、高质量地生成图像的 SDXL-Turbo 模型。与其他只能生成每类图像数（IPC）= 1 的扩散模型相比，我们的方法对 Tiny-ImageNet 可以实现 IPC = 10，对 CIFAR-100 可以实现 IPC = 20。此外，为了生成用于 CIFAR-100 和 Tiny-ImageNet 的高质量蒸馏数据集，我们使用类别信息作为文本提示，并对 SDXL-Turbo 模型进行数据增强。实验结果表明了所提出方法的有效性，我们在 ECCV 2024 DD 挑战赛生成赛道中获得了第三名。代码可在 https://github.com/Guang000/BANKO 获取。||
|**2024-08-16**|[RadioDiff: An Effective Generative Diffusion Model for Sampling-Free Dynamic Radio Map Construction](http://arxiv.org/abs/2408.08593)|**[link](https://github.com/unic-lab/radiodiff)**|无线电地图 (RM) 是一项很有前景的技术，它可以仅根据位置信息获取路径损耗，这对于降低 6G 网络应用中路径损耗估计的通信成本具有重要意义。然而，传统的 RM 构建方法要么计算量大，要么依赖于昂贵的基于采样的路径损耗测量。尽管基于神经网络 (NN) 的方法可以高效地构建无需采样的 RM，但其性能仍不理想。这主要是由于 RM 构建问题的生成特性与现有基于 NN 的方法所利用的判别建模之间存在错位。因此，为了提高 RM 构建性能，本文将无采样 RM 构建建模为条件生成问题，并提出了一种基于去噪扩散的方法 RadioDiff，以实现高质量的 RM 构建。此外，为了增强扩散模型从动态环境中提取特征的能力，采用具有自适应快速傅里叶变换模块的注意力 U-Net 作为骨干网络，以提高动态环境特征提取能力。同时，利用解耦扩散模型进一步提高 RM 的构建性能。此外，本文还首次从数据特征和神经网络训练方法两个角度，对为何 RM 构建是一个生成问题进行了全面的理论分析。实验结果表明，所提出的 RadioDiff 在准确性、结构相似性和峰值信噪比三个指标上均达到了最先进的性能。代码可在 https://github.com/UNIC-Lab/RadioDiff 获取。||
|**2024-08-16**|[A New Chinese Landscape Paintings Generation Model based on Stable Diffusion using DreamBooth](http://arxiv.org/abs/2408.08561)|null|This study mainly introduces a method combining the Stable Diffusion Model (SDM) and Parameter-Efficient Fine-Tuning method for generating Chinese Landscape Paintings. This training process is accelerated by combining LoRA with pre-trained SDM and DreamBooth with pre-trained SDM, respectively. On the Chinese Landscape Paintings Internet dataset used in this paper, this study finds that SDM combined with DreamBooth exhibits superior performance, outperforming other models, including the generic pre-trained SDM and LoRA-based fine-tuning SDM. The SDM combined with DreamBooth achieves a FID of 12.75 on the dataset and outperforms all other models in terms of expert evaluation, highlighting the model's versatility in the field of Chinese Landscape Paintings given the unique identifier, high fidelity and high quality. This study illustrates the potential of specialised fine-tuning method to improve the performance of SDM on domain-specific tasks, particularly in the domain of Landscape Paintings.||
|**2024-08-16**|[Linear combinations of latents in diffusion models: interpolation and beyond](http://arxiv.org/abs/2408.08558)|null|Generative models are crucial for applications like data synthesis and augmentation. Diffusion, Flow Matching and Continuous Normalizing Flows have shown effectiveness across various modalities, and rely on Gaussian latent variables for generation. As any generated object is directly associated with a particular latent variable, we can manipulate the variables to exert control over the generation process. However, standard approaches for combining latent variables, such as spherical interpolation, only apply or work well in special cases. Moreover, current methods for obtaining low-dimensional representations of the data, important for e.g. surrogate models for search and creative applications, are network and data modality specific. In this work we show that the standard methods to combine variables do not yield intermediates following the distribution the models are trained to expect. We propose Combination of Gaussian variables (COG), a novel interpolation method that addresses this, is easy to implement yet matches or improves upon current methods. COG addresses linear combinations in general and, as we demonstrate, also supports other operations including e.g. defining subspaces of the latent space, simplifying the creation of expressive low-dimensional spaces of high-dimensional objects using generative models based on Gaussian latents.||
|**2024-08-16**|[Scaling up Multimodal Pre-training for Sign Language Understanding](http://arxiv.org/abs/2408.08544)|null|Sign language serves as the primary meaning of communication for the deaf-mute community. Different from spoken language, it commonly conveys information by the collaboration of manual features, i.e., hand gestures and body movements, and non-manual features, i.e., facial expressions and mouth cues. To facilitate communication between the deaf-mute and hearing people, a series of sign language understanding (SLU) tasks have been studied in recent years, including isolated/continuous sign language recognition (ISLR/CSLR), gloss-free sign language translation (GF-SLT) and sign language retrieval (SL-RT). Sign language recognition and translation aims to understand the semantic meaning conveyed by sign languages from gloss-level and sentence-level, respectively. In contrast, SL-RT focuses on retrieving sign videos or corresponding texts from a closed-set under the query-by-example search paradigm. These tasks investigate sign language topics from diverse perspectives and raise challenges in learning effective representation of sign language videos. To advance the development of sign language understanding, exploring a generalized model that is applicable across various SLU tasks is a profound research direction.||
|**2024-08-15**|[Understanding the Local Geometry of Generative Model Manifolds](http://arxiv.org/abs/2408.08307)|null|深度生成模型在训练过程中使用有限数量的样本学习复杂数据流形的连续表示。对于预训练的生成模型，评估学习到的流形表示质量的常用方法是使用大量生成样本和真实样本计算全局指标，例如 Fréchet Inception Distance。然而，生成模型的性能在学习到的流形上并不一致，例如，对于像 Stable Diffusion 这样的基础模型，生成性能可能会根据去噪的条件或初始噪声向量而有很大差异。在本文中，我们研究了学习流形的局部几何形状与下游生成之间的关系。基于连续分段线性 (CPWL) 生成器的理论，我们使用三个几何描述符——缩放 ( $\psi$)、秩 ($\nu$) 和复杂度 ($\delta$ )——来局部地表征预训练生成模型流形。我们提供了定量和定性的证据，表明对于给定的潜在向量，局部描述符与生成的美学、伪影、不确定性甚至记忆有关。最后，我们证明了在局部几何形状上训练奖励模型可以控制学习分布下生成样本的可能性。||
|**2024-08-15**|[Accelerated Image-Aware Generative Diffusion Modeling](http://arxiv.org/abs/2408.08306)|null|本文提出了一种新的扩散模型的解析构造，其漂移和扩散参数在正向过程中产生指数级时间衰减的信噪比。反过来，该构造巧妙地利用自编码器在干净图像的结构上进行扩散系数的学习。所提出的方法显著加速了扩散过程，将所需的扩散时间步长从传统模型中常见的大约1000步减少到200-500步，而不会影响反向时间扩散中的图像质量。与传统模型通常使用耗时的多次运行不同，我们引入了一种并行数据驱动模型，以便在模型的单次运行中生成反向时间扩散轨迹。由此产生的集体块顺序生成模型不再需要基于MCMC的子采样校正来保障和提高图像质量，从而进一步提高了图像生成的加速。总的来说，这些进步产生了一个比传统方法快一个数量级的生成模型，同时保持了生成图像的高保真度和多样性，因此在快速图像合成任务中具有广泛的应用前景。||
|**2024-08-15**|[Marker or Markerless? Mode-Switchable Optical Tactile Sensing for Diverse Robot Tasks](http://arxiv.org/abs/2408.08276)|null|光学触觉传感器在机器人感知和操作任务中扮演着至关重要的角色。这些传感器的薄膜可以涂上标记或保持无标记，使其能够在标记或无标记模式下工作。然而，这种单一模式选择意味着传感器仅适用于操作或感知任务。虽然标记对于操作至关重要，但它们也可能遮挡相机，从而阻碍感知。标记模式和无标记模式之间的选择困境构成了一个重大障碍。为了解决这个问题，我们提出了一种新颖的可切换模式的光学触觉传感方法，该方法促进了两种模式之间的转换。通过生成模型实现从标记到无标记的转换，而其逆转换则使用稀疏监督回归模型实现。我们的方法允许单模光学传感器在标记和无标记模式下都能有效运行，而无需额外的硬件，使其非常适合感知和操作任务。大量实验验证了我们方法的有效性。对于感知任务，我们的方法将包含错误分类样本的类别数量减少了 2 个，并将接触区域分割 IoU 提高了 3.53%。对于操作任务，我们的方法在滑动检测中实现了 92.59% 的高成功率。代码、数据集和演示视频可在项目网站上获取：https://gitouni.github.io/Marker-Markerless-Transition/||
|**2024-08-15**|[Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding](http://arxiv.org/abs/2408.08252)|**[link](https://github.com/masa-ue/svdd)**|扩散模型擅长捕捉图像、分子、DNA、RNA 和蛋白质序列的自然设计空间。然而，我们通常的目标不仅仅是生成自然的设计，而是在保持这些设计空间自然性的同时优化下游奖励函数。现有的实现这一目标的方法通常需要“可微分”的代理模型（例如，分类器引导或 DPS），或者涉及计算成本高昂的扩散模型微调（例如，无分类器引导、基于 RL 的微调）。在我们的工作中，我们提出了一种解决这些挑战的新方法。我们的算法是一种迭代采样方法，它将软价值函数集成到预训练扩散模型的标准推理过程中，该函数可以提前预测中间噪声状态如何在未来带来高回报。值得注意的是，我们的方法避免了生成模型的微调，并且不需要构建可微分模型。这使我们能够 (1) 直接利用许多科学领域常用的不可微特征/奖励反馈，以及 (2) 将我们的方法以原则性的方式应用于最近的离散扩散模型。最后，我们展示了我们的算法在多个领域的有效性，包括图像生成、分子生成和 DNA/RNA 序列生成。代码可在 \href{https://github.com/masa-ue/SVDD}{https://github.com/masa-ue/SVDD} 获取。||
|**2024-08-15**|[Not Every Image is Worth a Thousand Words: Quantifying Originality in Stable Diffusion](http://arxiv.org/abs/2408.08184)|null|这项工作解决了量化文本到图像 (T2I) 生成扩散模型原创性的挑战，重点关注版权原创性。我们首先通过受控实验评估 T2I 模型的创新和泛化能力，结果表明，稳定的扩散模型可以通过足够多样化的训练数据有效地重建未见过的元素。然后，我们的主要见解是，模型熟悉的图像元素的概念和组合，以及在训练期间更多看到的图像元素，在模型的潜在空间中得到了更简洁的表示。因此，我们提出了一种利用文本反转的方法，根据模型重建图像所需的标记数量来衡量图像的原创性。我们的方法受到原创性法律定义的启发，旨在评估模型是否可以在不依赖特定提示或拥有模型训练数据的情况下生成原创内容。我们使用预先训练的稳定扩散模型和合成数据集来演示我们的方法，展示了标记数量与图像原创性之间的相关性。这项工作有助于理解生成模型中的原创性，并对版权侵权案件具有启示意义。||
|**2024-08-15**|[When Video Coding Meets Multimodal Large Language Models: A Unified Paradigm for Video Coding](http://arxiv.org/abs/2408.08093)|null|现有的编解码器旨在消除内在冗余，以创建用于压缩的紧凑表示。然而，来自多模态大型语言模型 (MLLM) 的强外部先验尚未在视频压缩中得到明确探索。在此，我们介绍了一种用于跨模态视频编码 (CMVC) 的统一范式，这是一种探索视频编码中的多模态表示和视频生成模型的开创性方法。具体来说，在编码器端，我们将视频分解为空间内容和运动组件，然后将其转换为不同的模态，以利用 MLLM 实现非常紧凑的表示。在解码过程中，利用先前编码的组件和视频生成模型来创建多种编码-解码模式，以优化特定解码要求的视频重建质量，包括文本到视频 (TT2V) 模式以确保高质量的语义信息和图像-文本到视频 (IT2V) 模式以实现极好的感知一致性。此外，我们为 IT2V 模式提出了一种通过低秩自适应 (LoRA) 调整的高效帧插值模型，以保证感知质量，这使得生成的运动线索能够平滑地表现。基准测试表明，TT2V 实现了有效的语义重建，而 IT2V 表现出具有竞争力的感知一致性。这些结果突出了未来视频编码研究的潜在方向。||
|**2024-08-15**|[Text2BIM: Generating Building Models Using a Large Language Model-based Multi-Agent Framework](http://arxiv.org/abs/2408.08054)|null|传统的BIM创作流程通常要求设计师掌握复杂且繁琐的建模命令，以便在BIM创作工具中实现他们的设计意图。这种额外的认知负担使设计过程变得复杂，并阻碍了BIM和基于模型的设计在AEC（建筑、工程和施工）行业中的应用。为了更直观地表达设计意图，我们提出了Text2BIM，这是一个基于LLM的多智能体框架，可以根据自然语言指令生成3D建筑模型。该框架协调多个LLM智能体进行协作和推理，将用户的文本输入转换为调用BIM创作工具API的指令代码，从而直接在软件中生成具有内部布局、外部围护结构和语义信息的、可编辑的BIM模型。此外，在智能体工作流程中引入了一个基于规则的模型检查器，利用预定义的领域知识来指导LLM智能体解决生成模型中的问题，并迭代地提高模型质量。我们进行了广泛的实验，比较和分析了三种不同LLM在所提出的框架下的性能。评估结果表明，我们的方法可以有效地生成高质量、结构合理的建筑模型，这些模型与用户输入中指定的抽象概念相一致。最后，我们开发了一个交互式软件原型，将该框架集成到BIM创作软件Vectorworks中，展示了通过聊天进行建模的潜力。||
|**2024-08-15**|[Accelerating High-Fidelity Waveform Generation via Adversarial Flow Matching Optimization](http://arxiv.org/abs/2408.08019)|**[link](https://github.com/sh-lee-prml/periodwave)**|本文介绍了 PeriodWave-Turbo，一种通过对抗流匹配优化实现高保真度和高效的波形生成模型。最近，条件流匹配 (CFM) 生成模型已成功应用于波形生成任务，利用单个矢量场估计目标进行训练。尽管这些模型可以生成高保真度的波形信号，但与仅需要单个生成步骤的基于 GAN 的模型相比，它们需要更多的 ODE 步骤。此外，由于噪声矢量场估计，生成的样本通常缺乏高频信息，这无法确保高频再现。为了解决这一限制，我们通过结合固定步长生成器修改来增强预训练的基于 CFM 的生成模型。我们利用重建损失和对抗反馈来加速高保真度波形生成。通过对抗流匹配优化，它只需要 1,000 步微调即可在各种客观指标上实现最先进的性能。此外，我们显着缩短了推理速度，从 16 步减少到 2 步或 4 步。此外，通过将 PeriodWave 的主干从 29M 参数扩展到 70M 参数以改进泛化能力，PeriodWave-Turbo 实现了前所未有的性能，在 LibriTTS 数据集上的语音质量感知评估 (PESQ) 得分为 4.454。音频样本、源代码和检查点将在 https://github.com/sh-lee-prml/PeriodWave 上提供。||
|**2024-08-15**|[Conditional Brownian Bridge Diffusion Model for VHR SAR to Optical Image Translation](http://arxiv.org/abs/2408.07947)|null|合成孔径雷达 (SAR) 成像技术具有独特优势，能够不受天气条件和时间限制地收集数据。然而，SAR 图像表现出复杂的回波散射模式和斑点噪声，需要专业知识才能解释。为了应对这一挑战，研究人员一直在进行将 SAR 图像转换为类似光学图像表示的研究，以帮助解释 SAR 数据。然而，现有的研究主要利用低分辨率卫星图像数据集，并且主要基于生成对抗网络 (GAN)，而 GAN 以其训练不稳定和保真度低而闻名。为了克服低分辨率数据使用和基于 GAN 方法的这些局限性，本文介绍了一种基于布朗桥扩散模型 (BBDM) 的条件图像到图像转换方法。我们在 MSAW 数据集上进行了综合实验，该数据集是 0.5 米超高分辨率 (VHR) 图像的 SAR 和光学图像对集合。实验结果表明，我们的方法在各种感知质量指标上均优于条件扩散模型 (CDM) 和基于 GAN 的模型。||
|**2024-08-15**|[Training Spatial-Frequency Visual Prompts and Probabilistic Clusters for Accurate Black-Box Transfer Learning](http://arxiv.org/abs/2408.07944)|null|尽管诸如预测 API 服务之类的黑盒预训练模型 (PTM) 日益普及，但由于数据分布差距，将通用模型直接应用于现实场景仍然存在重大挑战。考虑到数据缺乏和计算资源受限的情况，本文提出了一种新颖的参数高效迁移学习框架，用于黑盒设置下的视觉识别模型。我们的框架结合了两种新颖的训练技术。首先，我们通过生成空间和频域的视觉提示，将 PTM 的输入空间（即图像）与目标数据分布对齐。除了新颖的空间-频率混合视觉提示器之外，我们还设计了一种基于概率聚类的训练技术，可以增强输出空间（即预测概率）中的类别分离。在实验中，我们的模型在跨广泛视觉识别数据集的少样本迁移学习设置中表现出优越的性能，超过了最先进的基线。此外，我们还证明了所提出的方法可以有效地降低训练和推理阶段的计算成本。||
|**2024-08-13**|[Imagen 3](http://arxiv.org/abs/2408.07009)|null|我们推出了Imagen 3，这是一个可以根据文本提示生成高质量图像的潜在扩散模型。我们描述了我们对质量和责任的评估。在评估时，Imagen 3优于当时其他的最先进 (SOTA) 模型。此外，我们还讨论了围绕安全和代表性的问题，以及我们用来最大程度降低模型潜在危害的方法。||
|**2024-08-13**|[Low-Bitwidth Floating Point Quantization for Efficient High-Quality Diffusion Models](http://arxiv.org/abs/2408.06995)|null|扩散模型是一种新兴的图像生成模型，它通过使用深度神经网络迭代地对随机高斯噪声进行去噪来生成图像。这些模型通常表现出较高的计算和内存需求，因此需要有效的训练后量化来实现高性能推理。最近的研究提出了针对扩散模型的低比特宽度（例如，8 位或 4 位）量化方法，然而 4 位整数量化通常会导致图像质量低下。我们观察到，在几种广泛使用的硬件平台上，相同比特宽度（例如，8 位或 4 位）的浮点和整数算术运算在计算能力上几乎没有差异。因此，我们提出了一种有效的扩散模型浮点量化方法，与整数量化方法相比，它可以提供更好的图像质量。我们采用了一种对其他处理任务（特别是计算机视觉和自然语言任务）有效的浮点量化方法，并通过在量化过程中将全精度值映射到量化值的过程中集成权重舍入学习，针对扩散模型对其进行定制。我们全面研究了最先进的扩散模型中的整数和浮点量化方法。我们的浮点量化方法不仅比整数量化方法生成更高质量的图像，而且与全精度模型（32 位浮点）相比没有明显的质量下降，当权重和激活都量化为 8 位浮点值时，而在 4 位权重和 8 位激活的情况下，质量下降最小。||
|**2024-08-13**|[DCMSA: Multi-Head Self-Attention Mechanism Based on Deformable Convolution For Seismic Data Denoising](http://arxiv.org/abs/2408.06963)|null|在处理地震数据时，扩散模型经常难以充分捕捉局部特征和表达空间关系。这种限制使得扩散模型难以有效地从复杂结构中去除噪声。为了解决这个问题，我们提出了一种新的卷积注意力机制，称为基于可变形卷积的多头自注意力机制（DCMSA），实现了扩散模型与卷积注意力的有效融合。DCMSA的实现如下：首先，我们将DCMSA集成到UNet架构中，以增强网络识别和处理复杂地震数据的能力。接下来，扩散模型利用增强了DCMSA的UNet来处理噪声数据。结果表明，该方法有效地解决了扩散模型在捕捉局部特征和表达空间关系方面的不足，在噪声抑制和保留有意义的地震数据信息方面优于传统的扩散模型和标准神经网络。||
|**2024-08-13**|[Diffusion Model for Slate Recommendation](http://arxiv.org/abs/2408.06883)|null|列表推荐是一种常用于流媒体平台和电子商务网站的技术，用于将多个项目一起呈现。列表推荐的一个重大挑战是管理复杂的组合选择空间。传统方法通常通过假设用户一次只与一个项目交互来简化这个问题。然而，这种简化并不反映现实，因为用户经常同时与多个项目交互。在本文中，我们解决了通用的列表推荐问题，该问题考虑了同时参与多个项目的情况。我们提出了一种使用扩散模型的生成方法，利用其学习高维数据结构的能力。我们的模型通过克服组合选择空间的挑战，生成高质量的列表，最大限度地提高用户满意度。此外，我们的方法还增强了推荐的多样性。对音乐播放列表生成和电子商务捆绑推荐等应用进行的大量离线评估表明，我们的模型在相关性和多样性方面均优于最先进的基线。||
|**2024-08-13**|[Improving Synthetic Image Detection Towards Generalization: An Image Transformation Perspective](http://arxiv.org/abs/2408.06741)|**[link](https://github.com/ouxiang-li/safe)**|随着近年来生成式模型促进逼真图像合成，合成图像的泛滥也对社交平台产生了某些负面影响，因此开发有效的检测器迫在眉睫。当前的合成图像检测（SID）流程主要致力于构建通用的伪影特征，但忽略了SID训练范式。在本文中，我们重新审视了SID问题，并确定了当前训练范式中存在的两种普遍偏差，即弱化伪影特征和过拟合伪影特征。同时，我们发现合成图像的成像机制会导致像素之间局部相关性增强，这表明检测器应具备局部感知能力。鉴于此，我们提出了SAFE，一种采用三种简单图像变换的轻量级高效检测器。首先，针对弱化伪影特征，我们在图像预处理中用裁剪算子代替下采样算子，以帮助规避伪影失真。其次，针对过拟合伪影特征，我们加入了ColorJitter和RandomRotation作为额外的数据增强，以帮助缓解有限训练样本中颜色差异和语义差异带来的无关偏差。第三，为了实现局部感知，我们提出了一种针对SID的基于patch的随机掩码策略，迫使检测器在训练时关注局部区域。我们在一个开放世界数据集上进行了比较实验，该数据集包含由26种不同生成模型生成的合成图像。我们的流程实现了新的最先进性能，与现有方法相比，准确率显著提高了4.5%，平均精度提高了2.9%。||
|**2024-08-13**|[DiffLoRA: Generating Personalized Low-Rank Adaptation Weights with Diffusion](http://arxiv.org/abs/2408.06740)|null|个性化文本到图像生成因其能够根据用户定义的提示生成特定身份的高保真肖像而备受关注。现有方法通常涉及测试时微调或结合额外的预训练分支。然而，这些方法难以同时满足效率、身份保真度和保留模型原始生成能力的需求。在本文中，我们提出了 DiffLoRA，这是一种利用扩散模型作为超网络的新方法，可以根据参考图像预测个性化的低秩适应 (LoRA) 权重。通过将这些 LoRA 权重集成到文本到图像模型中，DiffLoRA 无需进一步训练即可在推理过程中实现个性化。此外，我们提出了一种面向身份的 LoRA 权重构建流程，以促进 DiffLoRA 的训练。通过利用此流程生成的数据集，我们的 DiffLoRA 可以持续生成高性能且准确的 LoRA 权重。大量评估证明了我们方法的有效性，在整个个性化过程中实现了时间效率和身份保真度的保持。||
|**2024-08-13**|[DiffSG: A Generative Solver for Network Optimization with Diffusion Model](http://arxiv.org/abs/2408.06701)|null|扩散生成模型以其在图像生成方面的出色表现而闻名，并在各种跨领域应用中得到广泛应用。然而，它们在通信领域的应用主要局限于数据建模和特征提取等辅助任务。与传统的机器学习方法相比，这些模型在网络优化等基本问题上具有更大的潜力。判别式深度学习由于其单步输入输出映射以及缺乏对解空间的全局感知，往往表现不佳，特别是在网络优化目标函数复杂的情况下。相比之下，扩散生成模型可以通过学习描述潜在解空间分布的参数来考虑更广泛的解决方案并表现出更强的泛化能力，并将更高的概率分配给更好的解决方案。我们提出了一种新的基于扩散模型的解决方案生成框架（DiffSG），它利用扩散生成模型的内在分布学习能力，根据给定的输入学习高质量的解决方案分布。此分布中的最优解具有很高的概率，因此可以通过重复采样有效地达到。我们验证了 DiffSG 在几个典型网络优化问题上的性能，包括混合整数非线性规划、凸优化和分层非凸优化。我们的结果表明，DiffSG 优于现有的基线。总之，我们证明了扩散生成模型在解决复杂网络优化问题方面的潜力，并概述了它们在通信领域更广泛应用的美好前景。||
|**2024-08-13**|[DC3DO: Diffusion Classifier for 3D Objects](http://arxiv.org/abs/2408.06693)|**[link](https://github.com/sgi-2023/3d-building-classification)**|受Geoffrey Hinton关于“为了识别形状，首先要学习生成它们”的生成建模思想的启发，我们探索了使用3D扩散模型进行物体分类。我们的方法是利用这些模型的密度估计，称为3D物体扩散分类器（DC3DO），它能够对3D形状进行零样本分类，而无需额外的训练。平均而言，我们的方法与其多视图 counterparts 相比，实现了12.5%的改进，证明了其在多模态推理方面优于判别性方法。DC3DO采用在ShapeNet上训练的类别条件扩散模型，并在椅子和汽车的点云上进行推理。这项工作突出了生成模型在3D物体分类中的潜力。||
|**2024-08-13**|[Leveraging Priors via Diffusion Bridge for Time Series Generation](http://arxiv.org/abs/2408.06672)|null|时间序列生成被广泛应用于模拟、数据增强和假设检验技术等实际应用中。近年来，扩散模型已成为时间序列生成的首选方法，强调基于历史或相关时间序列数据流的各种合成场景。由于时间序列具有固定时间顺序和数据缩放等独特特征，标准高斯先验可能不适用于一般的时序列生成。在本文中，我们探索了利用不同的先验分布进行合成。然后，我们提出了TimeBridge，一个利用扩散桥来学习所选先验分布和数据分布之间传输的框架，从而实现灵活的合成。我们的模型涵盖了时间序列扩散模型中的各种场景，它利用（i）数据和时间相关的先验进行无条件合成，以及（ii）将约束作为先验进行条件生成的数据规模保持合成。实验结果表明，我们的模型在无条件和条件时间序列生成任务中均达到了最先进的性能。||
|**2024-08-13**|[Hybrid SD: Edge-Cloud Collaborative Inference for Stable Diffusion Models](http://arxiv.org/abs/2408.06646)|null|Stable Diffusion Models (SDMs) have shown remarkable proficiency in image synthesis. However, their broad application is impeded by their large model sizes and intensive computational requirements, which typically require expensive cloud servers for deployment. On the flip side, while there are many compact models tailored for edge devices that can reduce these demands, they often compromise on semantic integrity and visual quality when compared to full-sized SDMs. To bridge this gap, we introduce Hybrid SD, an innovative, training-free SDMs inference framework designed for edge-cloud collaborative inference. Hybrid SD distributes the early steps of the diffusion process to the large models deployed on cloud servers, enhancing semantic planning. Furthermore, small efficient models deployed on edge devices can be integrated for refining visual details in the later stages. Acknowledging the diversity of edge devices with differing computational and storage capacities, we employ structural pruning to the SDMs U-Net and train a lightweight VAE. Empirical evaluations demonstrate that our compressed models achieve state-of-the-art parameter efficiency (225.8M) on edge devices with competitive image quality. Additionally, Hybrid SD reduces the cloud cost by 66% with edge-cloud collaborative inference.||
|**2024-08-09**|[Multi-Garment Customized Model Generation](http://arxiv.org/abs/2408.05206)|null|本文介绍了多服装定制模型生成方法，这是一个基于潜在扩散模型 (LDM) 的统一框架，旨在解决合成穿着自由组合的多件服装图像这一尚未开发的任务。该方法侧重于根据不同的文本提示生成穿着各种目标服装的定制模型。主要挑战在于如何在保持穿着模型自然外观的同时保留每件服装的复杂纹理，确保不同服装的信息不会相互干扰。为了应对这些挑战，我们首先开发了一种服装编码器，它是一个具有共享权重的可训练 UNet 副本，能够并行提取服装的详细特征。其次，我们的框架通过解耦多服装特征融合支持多服装的条件生成，允许多个服装特征被注入到骨干网络中，显著减轻了服装信息之间的冲突。此外，所提出的服装编码器是一个即插即用的模块，可以与其他扩展模块（如 IP-Adapter 和 ControlNet）组合，增强生成模型的多样性和可控性。大量实验表明，我们的方法优于现有方法，为生成多服装组合图像的任务开辟了新途径。||
|**2024-08-09**|[TaSL: Task Skill Localization and Consolidation for Language Model Continual Learning](http://arxiv.org/abs/2408.05200)|null|近年来，语言模型持续学习 (CL) 引起了极大的兴趣，因为它有可能在不重新训练的情况下使大型语言模型 (LLM) 适应动态的现实环境。该领域的一个关键挑战是灾难性遗忘，即模型在学习新任务时会丢失先前获得的知识。现有方法通常采用多个参数高效微调 (PEFT) 模块来获取每个任务的任务特定知识，但这些方法缺乏效率，并且忽略了通过任务交互进行知识迁移的潜力。在本文中，我们提出了一种新的语言模型持续学习框架，称为任务技能定位和整合 (TaSL)，它在不依赖记忆回放的情况下增强了知识迁移。TaSL 首先根据参数依赖关系将模型划分为“技能单元”，从而实现更精细的控制。然后，它采用一种新颖的分组技能定位技术来识别新任务的技能单元重要性分布。通过将此重要性分布与先前任务的重要性分布进行比较，我们实施了一种细粒度的技能整合策略，该策略保留了特定于任务的知识，从而防止遗忘，并更新了任务共享知识，从而促进了双向知识迁移。因此，TaSL 在保留先前知识和在新任务中表现出色之间取得了卓越的平衡。TaSL 还显示出强大的通用性，适用于通用模型，并且可针对 PEFT 方法（如 LoRA）进行定制。此外，它还表现出显著的可扩展性，允许与记忆回放集成以进一步提高性能。在两个具有不同模型大小（从 2.2 亿到 70 亿）的 CL 基准测试中进行的大量实验证明了 TaSL 及其变体在不同设置下的有效性。||
|**2024-08-09**|[Cell Morphology-Guided Small Molecule Generation with GFlowNets](http://arxiv.org/abs/2408.05196)|**[link](https://github.com/thematrixmaster/omics-guided-gfn)**|高内涵表型筛选，包括高内涵成像 (HCI)，近年来因其能够在没有蛋白质靶点先验知识的情况下表征新型疗法的特性而广受欢迎。当与深度学习技术相结合以预测和表示分子-表型相互作用时，这些进步有可能显著加速和增强药物发现应用。这项工作侧重于 HCI 引导的分子设计的新任务。分子设计的生成模型可以由 HCI 数据指导，例如使用将分子与其感兴趣的表型联系起来的监督模型作为奖励函数。然而，有限的标记数据，加上高维度的读数，会使这些方法的训练变得具有挑战性和不切实际。我们考虑了一种替代方法，其中我们利用无监督多模态联合嵌入来定义潜在相似性，作为 GFlowNets 的奖励。所提出的模型学习生成新的分子，这些分子可以产生与给定图像目标相似的表型效应，而无需依赖预先注释的表型标签。我们证明了所提出的方法可以生成与目标具有高度形态和结构相似性的分子，从而增加了相似生物活性的可能性，这一点已由独立的预测模型证实。||
|**2024-08-09**|[Social contagion under hybrid interactions](http://arxiv.org/abs/2408.05050)|null|阈值驱动模型和博弈论是描述社会系统中人类互动关系的两个基本范式。然而，在模拟社会传染过程中，同时结合这两种机制的模型在很大程度上被忽视了。在这里，我们研究了一个整合了混合互动形式的通用模型，该模型假设网络中的一部分节点由阈值机制驱动，而其余节点则表现出由其理性支配的模仿行为（在博弈论框架下）。我们的研究结果表明，传播动力学取决于采用的收益。对于正收益，增加高度理性节点的密度可以促进采用过程，并伴随着混合相变。理性程度可以调节传播速度，理性程度较低的模仿者会减缓传播速度。我们进一步发现，对于负收益的采用，结果是相反的。该模型可以为了解现实世界社交网络中社会传染现象的复杂动力学提供有价值的见解。||
|**2024-08-09**|[Retrieval-augmented code completion for local projects using large language models](http://arxiv.org/abs/2408.05026)|null|大型语言模型 (LLM) 在软件开发人员中越来越受欢迎。然而，商业解决方案和使用大型语言模型存在隐私和计算要求方面的问题。在这项工作中，我们专注于使用具有约 1.6 亿个参数的大型语言模型，这些模型适用于本地执行，并通过从本地项目中检索信息来增强。我们基于 Transformer 架构训练了两个模型，即生成式模型 GPT-2 和检索适应性 RETRO 模型，并在开源 Python 文件上进行训练，并对它们进行了经验评估和比较，证实了基于向量嵌入检索的优势。此外，我们通过上下文检索增强生成改进了模型的性能，该方法根据标记的 Jaccard 相似度检索代码片段。我们在更大的模型上评估了上下文检索增强生成，并得出结论，尽管该方法很简单，但比使用 RETRO 架构更合适。我们强调了正确的标记化在实现大型语言模型在代码补全方面的全部潜力方面的关键作用。||
|**2024-08-09**|[DreamCouple: Exploring High Quality Text-to-3D Generation Via Rectified Flow](http://arxiv.org/abs/2408.05008)|null|利用预训练文本到图像模型扩散模型作为3D模型训练先验的分数蒸馏采样（SDS）已经取得了显著的成功。目前，基于流的扩散模型已成为生成模型的新趋势。然而，在3D生成中将SDS应用于基于流的扩散模型仍未得到探索。我们的工作旨在弥合这一差距。在本文中，我们将SDS应用于校正流，并重新审视了这种新框架下的过度平滑问题。这个问题可以解释为模型学习了多条ODE轨迹的平均值。然后我们提出了DreamCouple，它不是随机采样噪声，而是使用校正流模型来寻找耦合噪声。其独特的耦合匹配（UCM）损失引导模型学习不同的轨迹，从而解决了过度平滑问题。我们将我们的方法应用于NeRF和3D高斯样条，并取得了最先进的性能。我们还发现了一些其他有趣的开放性问题，例如NeRF的初始化问题和更快的训练收敛速度。我们的代码将很快发布。||
|**2024-08-09**|[Pay Attention To Mean Fields For Point Cloud Generation](http://arxiv.org/abs/2408.04997)|**[link](https://github.com/kaechb/mdma)**|由于传统蒙特卡罗模拟的高计算成本，特别是在未来高亮度对撞机中的应用，使用机器学习生成对撞机数据在粒子物理学中变得越来越流行。我们提出了一种用于点云的生成模型，该模型采用基于注意力的聚合，同时保持相对于点数的线性计算复杂度。该模型在对抗性设置中进行训练，分别确保生成器和判别器的输入排列等变性和不变性。为了稳定已知的不稳定对抗训练，引入了特征匹配损失。我们在两个不同的数据集上评估了性能。第一个是顶夸克\textsc{JetNet150}数据集，尽管模型的参数数量明显减少，但其性能优于当前最先进的基于GAN的模型。第二个是CaloChallenge的数据集2，与第一个数据集相比，它包含的点云数量多达30倍。该模型及其相应的代码可在\url{https://github.com/kaechb/MDMA/tree/NeurIPS}获取。||
|**2024-08-09**|[TEAdapter: Supply abundant guidance for controllable text-to-music generation](http://arxiv.org/abs/2408.04865)|**[link](https://github.com/ashley1101/teadapter)**|Although current text-guided music generation technology can cope with simple creative scenarios, achieving fine-grained control over individual text-modality conditions remains challenging as user demands become more intricate. Accordingly, we introduce the TEAcher Adapter (TEAdapter), a compact plugin designed to guide the generation process with diverse control information provided by users. In addition, we explore the controllable generation of extended music by leveraging TEAdapter control groups trained on data of distinct structural functionalities. In general, we consider controls over global, elemental, and structural levels. Experimental results demonstrate that the proposed TEAdapter enables multiple precise controls and ensures high-quality music generation. Our module is also lightweight and transferable to any diffusion model architecture. Available code and demos will be found soon at https://github.com/Ashley1101/TEAdapter.||
|**2024-08-09**|[Adversarially Robust Industrial Anomaly Detection Through Diffusion Model](http://arxiv.org/abs/2408.04839)|null|Deep learning-based industrial anomaly detection models have achieved remarkably high accuracy on commonly used benchmark datasets. However, the robustness of those models may not be satisfactory due to the existence of adversarial examples, which pose significant threats to the practical deployment of deep anomaly detectors. Recently, it has been shown that diffusion models can be used to purify the adversarial noises and thus build a robust classifier against adversarial attacks. Unfortunately, we found that naively applying this strategy in anomaly detection (i.e., placing a purifier before an anomaly detector) will suffer from a high anomaly miss rate since the purifying process can easily remove both the anomaly signal and the adversarial perturbations, causing the later anomaly detector failed to detect anomalies. To tackle this issue, we explore the possibility of performing anomaly detection and adversarial purification simultaneously. We propose a simple yet effective adversarially robust anomaly detection method, \textit{AdvRAD}, that allows the diffusion model to act both as an anomaly detector and adversarial purifier. We also extend our proposed method for certified robustness to $l_2$ norm bounded perturbations. Through extensive experiments, we show that our proposed method exhibits outstanding (certified) adversarial robustness while also maintaining equally strong anomaly detection performance on par with the state-of-the-art methods on industrial anomaly detection benchmark datasets.||
|**2024-08-09**|[Next-Generation Wi-Fi Networks with Generative AI: Design and Insights](http://arxiv.org/abs/2408.04835)|null|Generative artificial intelligence (GAI), known for its powerful capabilities in image and text processing, also holds significant promise for the design and performance enhancement of future wireless networks. In this article, we explore the transformative potential of GAI in next-generation Wi-Fi networks, exploiting its advanced capabilities to address key challenges and improve overall network performance. We begin by reviewing the development of major Wi-Fi generations and illustrating the challenges that future Wi-Fi networks may encounter. We then introduce typical GAI models and detail their potential capabilities in Wi-Fi network optimization, performance enhancement, and other applications. Furthermore, we present a case study wherein we propose a retrieval-augmented LLM (RA-LLM)-enabled Wi-Fi design framework that aids in problem formulation, which is subsequently solved using a generative diffusion model (GDM)-based deep reinforcement learning (DRL) framework to optimize various network parameters. Numerical results demonstrate the effectiveness of our proposed algorithm in high-density deployment scenarios. Finally, we provide some potential future research directions for GAI-assisted Wi-Fi networks.||
|**2024-08-08**|[Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics](http://arxiv.org/abs/2408.04631)|null|我们提出了 Puppet-Master，一个可以作为部件级动力学运动先验的交互式视频生成模型。在测试时，给定单个图像和一组稀疏的运动轨迹（即拖动），Puppet-Master 可以合成一个视频，描绘逼真的部件级运动，忠实于给定的拖动交互。这是通过微调一个大规模预训练视频扩散模型来实现的，为此我们提出了一种新的条件化架构，以有效地注入拖动控制。更重要的是，我们引入了 all-to-first 注意力机制，它是对广泛采用的空间注意力模块的一种替代方案，它通过解决现有模型中的外观和背景问题，显著提高了生成质量。与其他在野外视频上训练并且主要移动整个对象的运动条件视频生成器不同，Puppet-Master 是从 Objaverse-Animation-HQ 学习的，这是一个新的经过策划的部件级运动剪辑数据集。我们提出了一种策略来自动过滤掉次优动画，并使用有意义的运动轨迹增强合成渲染。Puppet-Master 可以很好地泛化到各个类别的真实图像，并在真实世界基准测试中以零样本的方式优于现有方法。有关更多结果，请参阅我们的项目页面：vgg-puppetmaster.github.io。||
|**2024-08-08**|[Sketch2Scene: Automatic Generation of Interactive 3D Game Scenes from User's Casual Sketches](http://arxiv.org/abs/2408.04567)|null|三维内容生成是许多计算机图形学应用的核心，包括电子游戏、电影制作、虚拟现实和增强现实等。本文提出了一种新颖的基于深度学习的方法，可以根据用户的简单提示（例如手绘草图）自动生成交互式和可玩的三维游戏场景。基于草图的输入为内容创作过程提供了一种自然、便捷的方式来传达用户的设计意图。为了克服学习中的数据缺乏挑战（即缺乏大型三维场景训练数据），我们的方法利用预先训练的二维去噪扩散模型来生成场景的二维图像作为概念指导。在这个过程中，我们采用等距投影模式来排除未知的相机位姿，同时获得场景布局。从生成的等距图像中，我们使用预先训练的图像理解方法将图像分割成有意义的部分，例如离地物体、树木和建筑物，并提取二维场景布局。这些片段和布局随后被输入到程序内容生成 (PCG) 引擎，例如 Unity 或 Unreal 等三维视频游戏引擎，以创建三维场景。生成的三维场景可以无缝集成到游戏开发环境中，并且可以立即播放。大量测试表明，我们的方法可以高效地生成高质量和交互式的三维游戏场景，其布局与用户的意图紧密相符。||
|**2024-08-08**|[Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of Large Language Models](http://arxiv.org/abs/2408.04556)|**[link](https://github.com/cyp-jlu-ai/ba-lora)**|大型语言模型（LLM）在一系列自然语言处理（NLP）任务中表现出卓越的能力。然而，将LLM应用于下游任务通常需要计算密集型和内存密集型的微调过程。为了减轻这些负担，参数高效微调（PEFT）技术已成为一种很有前景的方法，可以在最小的计算开销下定制LLM。虽然PEFT方法具有显著的优势，但它们并没有完全解决预训练数据中普遍存在的偏差传播问题。在这项工作中，我们介绍了偏差感知低秩自适应（BA-LoRA），这是一种旨在抵消偏差继承的新型PEFT方法。BA-LoRA包含三个不同的正则化项：（1）一致性正则化器，（2）多样性正则化器，以及（3）奇异值分解正则化器。这些正则化器共同旨在提高生成模型在微调过程中的: 一致性、多样性和泛化能力。通过对各种自然语言理解（NLU）和自然语言生成（NLG）任务进行广泛的实验，采用LLaMA、Mistral和Gemma等优秀的LLM，我们证明了BA-LoRA的性能优于LoRA及其最先进的变体。此外，我们的方法有效地减轻了预训练偏差的不利影响，从而获得了更可靠和稳健的模型输出。代码可在https://github.com/cyp-jlu-ai/BA-LoRA获取。||
|**2024-08-08**|[On the Asymptotic Convergence of Subgraph Generated Models](http://arxiv.org/abs/2408.04541)|null|我们研究了一系列称为子图生成模型 (SUGM) 的随机图模型，该模型最初由 Chandrasekhar 和 Jackson 开发，其中高阶结构被明确包含在网络形成过程中。我们使用矩阵浓度不等式来证明从这种 SUGM 实现的网络的邻接矩阵收敛到预期邻接矩阵，作为网络大小的函数。我们应用这一结果来研究采样网络中中心性度量（例如度中心性、特征向量中心性和 Katz 中心性）对预期网络中相应中心性的集中性，从而证明可以从随机图模型的知识中预测节点重要性，而无需确切的网络数据。||
|**2024-08-08**|[NFDI4Health workflow and service for synthetic data generation, assessment and risk management](http://arxiv.org/abs/2408.04478)|null|个人健康数据对于科学进步，特别是人工智能 (AI) 的发展至关重要；然而，由于隐私问题，共享真实的患者信息通常受到限制。合成数据生成是应对这一挑战的一个很有前景的解决方案。这种技术可以创建全新的数据集，模仿真实数据的统计特性，同时保护机密患者信息。在本文中，我们介绍了在德国国家数据基础设施项目 NFDI4Health 的背景下开发的工作流程和不同服务。首先，概述了两种用于生成合成健康数据的最先进的人工智能工具（即 VAMBN 和 MultiNODEs）。此外，我们介绍了 SYNDAT（一个基于 Web 的公共工具），它允许用户可视化和评估所需生成模型提供的合成数据的质量和风险。此外，还使用来自阿尔茨海默病神经影像学倡议 (ADNI) 和罗伯特科赫研究所癌症登记数据中心 (RKI) 的数据展示了所提出方法和基于网络工具的实用性。||
|**2024-08-08**|[Deep Generative Models in Robotics: A Survey on Learning from Multimodal Demonstrations](http://arxiv.org/abs/2408.04380)|null|从演示中学习，即从数据中学习机器人行为模型的领域，随着深度生成模型的出现而越来越受欢迎。尽管该问题已经以模仿学习、行为克隆或逆强化学习等名称被研究多年，但传统方法依赖于不能很好地捕捉复杂数据分布或不能很好地扩展到大量演示的模型。近年来，机器人学习社区对使用深度生成模型来捕捉大型数据集的复杂性越来越感兴趣。在本综述中，我们旨在对去年深度生成模型在机器人技术中的应用进展进行统一而全面的回顾。我们介绍了社区探索的不同类型的模型，如基于能量的模型、扩散模型、动作价值图或生成对抗网络。我们还介绍了深度生成模型所应用的不同类型的应用，从抓取生成到轨迹生成或成本学习。生成模型最重要的元素之一是分布外泛化。在我们的综述中，我们回顾了社区为改进学习模型的泛化能力所做的不同决定。最后，我们强调了研究挑战，并提出了机器人技术中学习深度生成模型的一些未来方向。||
|**2024-08-08**|[Unveiling Hidden Visual Information: A Reconstruction Attack Against Adversarial Visual Information Hiding](http://arxiv.org/abs/2408.04261)|null|本文研究了基于对抗样本的图像加密在数据重建 (DR) 攻击下的安全漏洞。一种典型的图像加密方法是对抗性视觉信息隐藏 (AVIH)，它使用类型 I 对抗样本训练来保护图像识别任务中使用的图库数据集。在 AVIH 方法中，类型 I 对抗样本方法创建的图像看起来完全不同，但机器仍然可以识别为原始图像。此外，AVIH 方法可以使用预定义的私钥生成模型将加密图像恢复为原始形式。为了获得最佳安全性，建议为每个图像分配一个唯一的密钥；然而，存储限制可能需要一些图像共享相同的密钥模型。这给 AVIH 提出了一个关键的安全问题：在不被 DR 攻击攻破的情况下，有多少图像可以安全地共享相同的密钥模型？为了解决这个问题，我们针对 AVIH 加密方法引入了一种双重策略 DR 攻击，该攻击结合了 (1) 生成对抗损失和 (2) 增强的身份损失，这可以防止 DR 过拟合——这是机器学习中的一个类似问题。我们的数值结果通过图像识别和重新识别基准验证了这种方法，表明我们的策略可以显着提高重建图像的质量，从而减少需要密钥共享的加密图像数量。我们用于复现结果的源代码将很快提供。||
|**2024-08-08**|[Generating Fine-Grained Causality in Climate Time Series Data for Forecasting and Anomaly Detection](http://arxiv.org/abs/2408.04254)|null|理解时间序列变量之间的因果关系有助于时间序列数据分析，这在许多实际应用中非常重要，例如气候预测和极端天气警报。然而，在现实世界的复杂环境中，因果关系很难被完全观察到，例如来自部署的传感器网络的时空数据。因此，为了捕捉时空变量之间细粒度的因果关系，以便进行更准确、更可靠的时间序列分析，我们首先设计了一个名为 TBN Granger Causality 的概念性细粒度因果模型，该模型在先前具有时间滞后的神经 Granger Causality 中添加了时间尊重的贝叶斯网络，以抵消瞬时效应。其次，我们提出了一种端到端的深度生成模型 TacSas，它以生成的方式发现 TBN Granger Causality，以帮助预测时间序列数据并在预测期间检测可能的异常。在评估方面，除了因果发现基准 Lorenz-96 之外，我们还在气候基准 ERA5 上测试了 TacSas 的气候预测能力，并在 NOAA 的极端天气基准上测试了其极端天气警报能力。||
|**2024-08-08**|[InstantStyleGaussian: Efficient Art Style Transfer with 3D Gaussian Splatting](http://arxiv.org/abs/2408.04249)|null|我们提出了 InstantStyleGaussian，一种基于三维高斯 splatting (3DGS) 场景表示的新型三维风格迁移方法。通过输入目标风格图像，它可以快速生成新的 3D GS 场景。我们的方法在预先重建的 GS 场景上运行，将扩散模型与改进的迭代数据集更新策略相结合。它利用扩散模型生成目标风格图像，将这些新图像添加到训练数据集中，并使用该数据集迭代地更新和优化 GS 场景。大量的实验结果表明，我们的方法可以确保高质量的风格化场景，同时在风格迁移速度和一致性方面具有显著优势。||
|**2024-08-08**|[LLDif: Diffusion Models for Low-light Emotion Recognition](http://arxiv.org/abs/2408.04235)|null|本文介绍了一种名为LLDif的新型基于扩散的 facial expression recognition (FER) 框架，该框架专为极低光 (LL) 环境而设计。在此类条件下拍摄的图像通常存在亮度低和对比度显着降低的问题，这对传统方法提出了挑战。这些挑战包括图像质量差，这会严重降低情绪识别的准确性。LLDif 通过一种新颖的两阶段训练过程来解决这些问题，该过程结合了标签感知 CLIP (LA-CLIP)、嵌入先验网络 (PNET) 和基于 Transformer 的网络，该网络擅长处理低光图像的噪声。第一阶段涉及 LA-CLIP 生成联合嵌入先验分布 (EPD) 以指导 LLformer 进行标签恢复。在第二阶段，扩散模型 (DM) 利用 EPD 的紧凑性对 EPD 推理进行细化，以进行精确预测。在各种 LL-FER 数据集上的实验评估表明，LLDif 取得了具有竞争力的性能，突出了其在挑战性照明条件下增强 FER 应用的潜力。||
|**2024-08-06**|[MDT-A2G: Exploring Masked Diffusion Transformers for Co-Speech Gesture Generation](http://arxiv.org/abs/2408.03312)|null|扩散Transformer技术的最新进展极大地提高了高质量二维图像、三维视频和三维形状的生成能力。然而，Transformer架构在语音协同手势生成领域的有效性仍未得到充分探索，因为先前的方法主要采用卷积神经网络 (CNN) 或仅包含少量Transformer层的简单模型。为了弥合这一研究差距，我们引入了一种新颖的掩码扩散Transformer，用于语音协同手势生成，称为 MDT-A2G，它直接对姿态序列执行去噪过程。为了增强时间对齐的语音驱动手势的上下文推理能力，我们引入了一种新颖的掩码扩散Transformer。该模型采用专门设计的掩码建模方案来加强序列手势之间的时间关系学习，从而加快学习过程并生成连贯逼真的动作。除了音频之外，我们的 MDT-A2G 模型还集成了多模态信息，包括文本、情感和身份。此外，我们提出了一种高效的推理策略，通过利用先前计算的结果来减少去噪计算量，从而在性能下降可忽略不计的情况下实现加速。实验结果表明，MDT-A2G 在手势生成方面表现出色，其学习速度比传统扩散Transformer快 6 倍以上，推理速度比标准扩散模型快 5.7 倍。||
|**2024-08-06**|[IPAdapter-Instruct: Resolving Ambiguity in Image-based Conditioning using Instruct Prompts](http://arxiv.org/abs/2408.03209)|null|扩散模型不断推动着图像生成的最新技术边界，但该过程难以进行细微的控制：实践证明，文本提示不足以准确描述图像风格或精细的结构细节（例如人脸）。ControlNet 和 IPAdapter 通过以图像为条件来解决这一缺点，但每个实例都被限制为对单个条件后验进行建模：对于需要在同一工作流程中使用多个不同后验的实际用例，训练和使用多个适配器非常麻烦。我们提出了 IPAdapter-Instruct，它将自然图像条件与“指令”提示相结合，以在对同一条件图像的不同解释之间切换：风格迁移、对象提取、两者兼而有之，还是其他？与专门的单任务模型相比，IPAdapterInstruct 可以高效地学习多项任务，并且质量损失极小。||
|**2024-08-06**|[An Object is Worth 64x64 Pixels: Generating 3D Object via Image Diffusion](http://arxiv.org/abs/2408.03178)|null|我们介绍了一种通过名为“物体图像”的表示来生成具有 UV 贴图的逼真 3D 模型的新方法。这种方法将表面几何形状、外观和补丁结构封装在一个 64x64 像素的图像中，有效地将复杂的 3D 形状转换为更易于管理的 2D 格式。通过这样做，我们解决了多边形网格固有的几何和语义不规则性挑战。这种方法允许我们直接使用图像生成模型（例如 Diffusion Transformers）进行 3D 形状生成。在 ABO 数据集上进行的评估表明，我们生成的具有补丁结构的形状实现了与最近的 3D 生成模型相当的点云 FID，同时自然支持 PBR 材质生成。||
|**2024-08-06**|[Iterative CT Reconstruction via Latent Variable Optimization of Shallow Diffusion Models](http://arxiv.org/abs/2408.03156)|null|Image generative AI has garnered significant attention in recent years. In particular, the diffusion model, a core component of recent generative AI, produces high-quality images with rich diversity. In this study, we propose a novel CT reconstruction method by combining the denoising diffusion probabilistic model with iterative CT reconstruction. In sharp contrast to previous studies, we optimize the fidelity loss of CT reconstruction with respect to the latent variable of the diffusion model, instead of the image and model parameters. To suppress anatomical structure changes produced by the diffusion model, we shallow the diffusion and reverse processes, and fix a set of added noises in the reverse process to make it deterministic during inference. We demonstrate the effectiveness of the proposed method through sparse view CT reconstruction of 1/10 view projection data. Despite the simplicity of the implementation, the proposed method shows the capability of reconstructing high-quality images while preserving the patient's anatomical structure, and outperforms existing methods including iterative reconstruction, iterative reconstruction with total variation, and the diffusion model alone in terms of quantitative indices such as SSIM and PSNR. We also explore further sparse view CT using 1/20 view projection data with the same trained diffusion model. As the number of iterations increases, image quality improvement comparable to that of 1/10 sparse view CT reconstruction is achieved. In principle, the proposed method can be widely applied not only to CT but also to other imaging modalities such as MRI, PET, and SPECT.||
|**2024-08-06**|[Training-Free Condition Video Diffusion Models for single frame Spatial-Semantic Echocardiogram Synthesis](http://arxiv.org/abs/2408.03035)|**[link](https://github.com/gungui98/echo-free)**|Conditional video diffusion models (CDM) have shown promising results for video synthesis, potentially enabling the generation of realistic echocardiograms to address the problem of data scarcity. However, current CDMs require a paired segmentation map and echocardiogram dataset. We present a new method called Free-Echo for generating realistic echocardiograms from a single end-diastolic segmentation map without additional training data. Our method is based on the 3D-Unet with Temporal Attention Layers model and is conditioned on the segmentation map using a training-free conditioning method based on SDEdit. We evaluate our model on two public echocardiogram datasets, CAMUS and EchoNet-Dynamic. We show that our model can generate plausible echocardiograms that are spatially aligned with the input segmentation map, achieving performance comparable to training-based CDMs. Our work opens up new possibilities for generating echocardiograms from a single segmentation map, which can be used for data augmentation, domain adaptation, and other applications in medical imaging. Our code is available at \url{https://github.com/gungui98/echo-free}||
|**2024-08-06**|[Diffusion Model Meets Non-Exemplar Class-Incremental Learning and Beyond](http://arxiv.org/abs/2408.02983)|null|Non-exemplar class-incremental learning (NECIL) is to resist catastrophic forgetting without saving old class samples. Prior methodologies generally employ simple rules to generate features for replaying, suffering from large distribution gap between replayed features and real ones. To address the aforementioned issue, we propose a simple, yet effective \textbf{Diff}usion-based \textbf{F}eature \textbf{R}eplay (\textbf{DiffFR}) method for NECIL. First, to alleviate the limited representational capacity caused by fixing the feature extractor, we employ Siamese-based self-supervised learning for initial generalizable features. Second, we devise diffusion models to generate class-representative features highly similar to real features, which provides an effective way for exemplar-free knowledge memorization. Third, we introduce prototype calibration to direct the diffusion model's focus towards learning the distribution shapes of features, rather than the entire distribution. Extensive experiments on public datasets demonstrate significant performance gains of our DiffFR, outperforming the state-of-the-art NECIL methods by 3.0\% in average. The code will be made publicly available soon.||
|**2024-08-06**|[Empathy Level Alignment via Reinforcement Learning for Empathetic Response Generation](http://arxiv.org/abs/2408.02976)|null|Empathetic response generation, aiming at understanding the user's situation and feelings and respond empathically, is crucial in building human-like dialogue systems. Previous methods mainly focus on using maximum likelihood estimation as the optimization objective for training response generation models, without taking into account the empathy level alignment between generated responses and target responses. To this end, we propose an empathetic response generation using reinforcement learning (EmpRL) framework. The framework designs an effective empathy reward function and generates empathetic responses by maximizing the expected reward through reinforcement learning. Given the powerful text generation capability of pre-trained language models, EmpRL utilizes the pre-trained T5 model as the generator and conducts further training to initialize the policy. To align the empathy level between generated responses and target responses in the context, an empathy reward function containing three empathy communication mechanisms, i.e., emotional reaction, interpretation, and exploration, is constructed using pre-designed and pre-trained empathy identifiers. Finally, the proximal policy optimization algorithm is used to further train the policy to produce empathetic responses. Both automatic and manual evaluations demonstrate that the proposed EmpRL framework can improve the quality of generated responses, enhance the empathy level similarity between generated and target responses, and produce empathetic responses covering both affective and cognitive aspects.||
|**2024-08-06**|[Data-Driven Stochastic Closure Modeling via Conditional Diffusion Model and Neural Operator](http://arxiv.org/abs/2408.02965)|null|Closure models are widely used in simulating complex multiscale dynamical systems such as turbulence and the earth system, for which direct numerical simulation that resolves all scales is often too expensive. For those systems without a clear scale separation, deterministic and local closure models often lack enough generalization capability, which limits their performance in many real-world applications. In this work, we propose a data-driven modeling framework for constructing stochastic and non-local closure models via conditional diffusion model and neural operator. Specifically, the Fourier neural operator is incorporated into a score-based diffusion model, which serves as a data-driven stochastic closure model for complex dynamical systems governed by partial differential equations (PDEs). We also demonstrate how accelerated sampling methods can improve the efficiency of the data-driven stochastic closure model. The results show that the proposed methodology provides a systematic approach via generative machine learning techniques to construct data-driven stochastic closure models for multiscale dynamical systems with continuous spatiotemporal fields.||
|**2024-08-06**|[Diverse Generation while Maintaining Semantic Coordination: A Diffusion-Based Data Augmentation Method for Object Detection](http://arxiv.org/abs/2408.02891)|null|Recent studies emphasize the crucial role of data augmentation in enhancing the performance of object detection models. However,existing methodologies often struggle to effectively harmonize dataset diversity with semantic coordination.To bridge this gap, we introduce an innovative augmentation technique leveraging pre-trained conditional diffusion models to mediate this balance. Our approach encompasses the development of a Category Affinity Matrix, meticulously designed to enhance dataset diversity, and a Surrounding Region Alignment strategy, which ensures the preservation of semantic coordination in the augmented images. Extensive experimental evaluations confirm the efficacy of our method in enriching dataset diversity while seamlessly maintaining semantic coordination. Our method yields substantial average improvements of +1.4AP, +0.9AP, and +3.4AP over existing alternatives on three distinct object detection models, respectively.||
|**2024-08-05**|[Back-Projection Diffusion: Solving the Wideband Inverse Scattering Problem with Diffusion Models](http://arxiv.org/abs/2408.02866)|null|We present \textit{Wideband back-projection diffusion}, an end-to-end probabilistic framework for approximating the posterior distribution induced by the inverse scattering map from wideband scattering data. This framework leverages conditional diffusion models coupled with the underlying physics of wave-propagation and symmetries in the problem, to produce highly accurate reconstructions. The framework introduces a factorization of the score function into a physics-based latent representation inspired by the filtered back-propagation formula and a conditional score function conditioned on this latent representation. These two steps are also constrained to obey symmetries in the formulation while being amenable to compression by imposing the rank structure found in the filtered back-projection formula. As a result, empirically, our framework is able to provide sharp reconstructions effortlessly, even recovering sub-Nyquist features in the multiple-scattering regime. It has low-sample and computational complexity, its number of parameters scales sub-linearly with the target resolution, and it has stable training dynamics.||
|**2024-08-02**|[Conditional LoRA Parameter Generation](http://arxiv.org/abs/2408.01415)|null|生成模型在图像、视频和文本领域取得了显著的成功。受此启发，研究人员探索利用生成模型来生成神经网络参数。然而，这些努力受到参数大小和生成高性能参数的实用性限制。在本文中，我们提出了 COND P-DIFF，这是一种新颖的方法，证明了在微调过程中可控生成高性能参数的可行性，特别是对于 LoRA（低秩自适应）权重。具体来说，我们采用自动编码器来提取参数的有效潜在表示。然后，我们训练了一个条件潜在扩散模型，根据特定任务条件从随机噪声中合成高性能模型参数。在计算机视觉和自然语言处理领域的实验结果一致表明，COND P-DIFF 可以生成以给定任务为条件的高性能参数。此外，我们观察到 COND P-DIFF 生成的参数分布与通过正常优化方法获得的分布相比存在差异，表明具有一定程度的泛化能力。我们的工作为进一步探索条件驱动的参数生成铺平了道路，为神经网络的特定任务自适应提供了一个有希望的方向。||
|**2024-08-02**|[Autoencoders in Function Space](http://arxiv.org/abs/2408.01362)|null|自编码器，包括其原始的确定性形式和变分形式 (VAE)，已得到广泛应用。在科学应用中，由函数组成的数据通常很有意义；同样的视角在图像处理中也很有用。在实践中，离散化（科学中出现的微分方程）或像素化（图像）使问题变成有限维，但首先构思对函数进行操作的算法，然后才进行离散化或像素化，会导致更好的算法，这些算法可以在不同的离散化或像素化级别之间平滑运行。本文介绍、分析和部署了函数空间版本的自编码器 (FAE) 和变分自编码器 (FVAE)。即使在有限维度下，控制 VAE 的目标函数的良定义性也是一个微妙的问题，在函数空间上更是如此。只要数据分布与所选的生成模型兼容，FVAE 目标就具有良好的定义；例如，当数据来自随机微分方程时，就会发生这种情况。FAE 目标的适用范围更广，可以很容易地应用于由微分方程控制的数据。将这些目标与神经算子架构配对，从而可以在任何网格上进行评估，这使得自编码器能够在修复、超分辨率和科学数据的生成建模方面有新的应用。||
|**2024-08-02**|[TexGen: Text-Guided 3D Texture Generation with Multi-view Sampling and Resampling](http://arxiv.org/abs/2408.01291)|null|给定一个三维网格，我们的目标是合成与任意文本描述相对应的三维纹理。当前从采样视图生成和组合纹理的方法通常会导致明显的接缝或过度平滑。为了解决这些问题，我们提出了 TexGen，这是一种新颖的多视图采样和重采样框架，用于利用预训练的文本到图像扩散模型生成纹理。为了保持视图一致性采样，我们首先在 RGB 空间中维护一个纹理映射，该映射由去噪步骤参数化，并在扩散模型的每个采样步骤之后更新，以逐步减少视图差异。我们采用了一种注意力引导的多视图采样策略，在视图之间传播外观信息。为了保留纹理细节，我们开发了一种噪声重采样技术，该技术有助于估计噪声，根据文本提示和当前纹理图生成后续去噪步骤的输入。通过大量的定性和定量评估，我们证明了我们提出的方法可以为具有高度视图一致性和丰富外观细节的各种三维对象生成明显更好的纹理质量，优于当前最先进的方法。此外，我们提出的纹理生成技术还可以应用于纹理编辑，同时保留原始标识。更多实验结果可在 https://dong-huo.github.io/TexGen/ 获取。||
|**2024-08-02**|[A General Framework to Boost 3D GS Initialization for Text-to-3D Generation by Lexical Richness](http://arxiv.org/abs/2408.01269)|null|文本到3D内容创建最近备受关注，特别是随着3D高斯 splatting (GS) 的普及。通常，基于GS的方法包括两个关键阶段：初始化和渲染优化。为了实现初始化，现有工作直接应用随机球体初始化或3D扩散模型（例如Point-E）来导出初始形状。然而，这些策略存在两个关键且具有挑战性的问题：1）即使在训练之后，最终形状仍然与初始形状相似；2）只能从简单的文本（例如“一只狗”）生成形状，而不能从词汇更丰富的文本（例如“一只狗坐在飞机顶部”）生成形状。为了解决这些问题，本文提出了一种新的通用框架，以根据词汇丰富度来提升文本到3D生成的3D GS初始化。我们的关键思想是将3D高斯聚合成空间均匀的体素，以表示复杂形状，同时实现3D高斯之间的空间交互以及高斯和文本之间的语义交互。具体来说，我们首先构建体素化表示，其中每个体素包含一个3D高斯，其位置、比例和旋转固定，同时设置不透明度作为确定位置占用的唯一因素。然后，我们设计了一个初始化网络，主要由两个新组件组成：1）全局信息感知（GIP）块和2）高斯-文本融合（GTF）块。这种设计使每个3D高斯能够吸收来自其他区域的空间信息和来自文本的语义信息。大量实验表明，通过采用词汇简单、中等和困难的文本，我们的高质量3D GS初始化框架优于现有方法（例如Shap-E）。此外，我们的框架可以无缝插入到最先进的训练框架中（例如LucidDreamer），以实现语义一致的文本到3D生成。||
|**2024-08-02**|[CLIP4Sketch: Enhancing Sketch to Mugshot Matching through Dataset Augmentation using Diffusion Models](http://arxiv.org/abs/2408.01233)|null|人脸素描到照片的匹配是人脸识别中一项具有挑战性的任务，主要障碍在于标注的人脸素描的稀缺性以及素描和照片之间的模态差异。为了解决这个问题，我们提出了 CLIP4Sketch，这是一种利用扩散模型生成大量多样素描图像的新方法，有助于提高人脸识别系统在素描到照片匹配中的性能。我们的方法利用去噪扩散概率模型 (DDPM) 生成对身份和风格有明确控制的素描。我们将参考照片的 CLIP 和 Adaface 嵌入以及风格的文本描述作为扩散模型的条件。我们通过生成与照片相对应的综合素描数据集并在我们的合成数据上训练人脸识别模型来证明我们方法的有效性。我们的结果表明，与使用现有的有限真实人脸素描数据进行训练相比，素描到照片匹配的准确性有了显著提高，这验证了扩散模型在提高跨模态人脸识别系统性能方面的潜力。我们还将我们的数据集与使用基于 GAN 的方法生成的数据集进行了比较，以显示其优越性。||
|**2024-08-02**|[PSP-GEN: Stochastic inversion of the Process-Structure-Property chain in materials design through deep, generative probabilistic modeling](http://arxiv.org/abs/2408.01114)|null|逆材料设计是材料科学中的一个基础挑战，在许多行业中都有重要的应用。传统的逆结构-性能(SP)联系以识别具有目标性能的微结构的方法往往忽视了生产过程的可行性，导致设计的微结构可能无法制造。为了同时实现所需的性能和可行的制造工艺，需要对整个工艺-结构-性能(PSP)链进行逆向工程。然而，这项任务充满了挑战，包括整个建模链的随机性、微结构和工艺参数的高维性，以及逆问题的固有不适定性。本文提出了一个名为PSP-GEN的新框架，用于面向目标的材料设计，该框架通过使用深度生成模型对整个PSP链进行建模，有效地解决了这些挑战。它采用了两种连续的、微结构和性能感知的潜在变量集，第一组提供了一个低维表示，捕捉了微结构生成的随机性，而第二组则与加工参数直接相关。这种结构化的低维嵌入不仅简化了高维微结构数据的处理，而且还促进了基于梯度的优化技术的应用。该方法的有效性和效率在两相材料的逆向设计中得到了证明，其目标是设计具有目标有效渗透率的微结构。我们在具有挑战性的环境中比较了最先进的替代方案，这些环境包括有限的训练数据、没有训练数据的目标属性区域，以及工艺参数和微结构具有高维表示的设计任务。||
|**2024-08-02**|[Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding](http://arxiv.org/abs/2408.01096)|null|我们介绍一个项目，该项目旨在重现 15 世纪韩国宫廷音乐作品《治化坪》和《吹风形》，这两首作品是根据诗歌《龙飞天上歌》创作的。作为韩国音乐记谱法 Jeongganbo 的早期例子之一，现存版本仅包含基本的旋律。我们的研究团队受国家国乐中心委托，旨在将这段古老的旋律改编成适合六重奏表演的版本。我们使用通过定制的光学音乐识别获取的 Jeongganbo 数据，训练了一个类似 BERT 的掩码语言模型和一个编码器-解码器 Transformer 模型。我们还提出了一种编码方案，该方案严格遵循 Jeongganbo 的结构，并将音符持续时间表示为位置。机器转换后的《治化坪》和《吹风形》版本由专家进行了评估，并由国家国乐中心的宫廷音乐管弦乐团进行了演奏。我们的工作表明，如果结合精心设计，生成模型可以成功地应用于训练数据有限的传统音乐。||
|**2024-08-02**|[EIUP: A Training-Free Approach to Erase Non-Compliant Concepts Conditioned on Implicit Unsafe Prompts](http://arxiv.org/abs/2408.01014)|null|文本到图像的扩散模型已经展现出学习各种概念的能力。然而，值得注意的是，它们也可能生成不良输出，从而引发重大的安全问题。具体而言，可能会遇到诸如不安全内容 (NSFW) 和潜在的风格版权侵权等问题。由于图像生成以文本为条件，因此提示词净化是内容安全的一种直接解决方案。与 LLM 采取的方法类似，已经做出了一些努力，通过净化提示词来控制安全输出的生成。然而，同样重要的是要注意，即使做出了这些努力，无毒文本仍然存在生成不合规图像的风险，这被称为隐式不安全提示词。此外，一些现有工作对模型进行微调，以从模型权重中清除不需要的概念。每当概念更新时，这种类型的方法都需要多次训练迭代，这可能非常耗时，并且可能导致灾难性遗忘。为了应对这些挑战，我们提出了一种简单而有效的方法，将不合规概念纳入擦除提示词中。这种擦除提示词主动参与图像空间特征和文本嵌入的融合。通过注意力机制，我们的方法能够识别图像空间中不合规概念的特征表示。我们重新调整这些特征的权重，以有效抑制以原始隐式不安全提示词为条件的不安全图像的生成。与最先进的基线相比，我们的方法在实现高图像保真度的同时，还表现出卓越的擦除效果。警告：本文包含可能具有冒犯性的模型输出。||
|**2024-08-02**|[FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features for Highly Controllable Text-Driven Image Translation](http://arxiv.org/abs/2408.00998)|**[link](https://github.com/xianggao1102/fbsdiff)**|大规模文本到图像扩散模型是生成式人工智能和多模态技术发展中的一个革命性里程碑，允许基于自然语言文本提示生成非凡的图像。然而，此类模型缺乏可控性的问题限制了其在现实生活中内容创作中的实际适用性，因此人们一直关注于利用参考图像来控制文本到图像的合成。由于参考图像和生成图像之间存在密切的相关性，因此该问题也可以视为根据文本处理（或编辑）参考图像的任务，即文本驱动的图像到图像转换。本文提出了一种新颖、简洁、高效的方法，以即插即用的方式使预训练的大规模文本到图像 (T2I) 扩散模型适应图像到图像 (I2I) 范式，实现了高质量和通用的文本驱动的 I2I 转换，无需任何模型训练、模型微调或在线优化过程。为了使用参考图像指导 T2I 生成，我们建议使用 DCT 谱空间中相应不同频段的扩散特征对不同的引导因素进行建模，并相应地设计了一种新颖的频段替换层，该层在反向采样过程中用参考图像的相应对应部分动态替换扩散特征的某个 DCT 频段。我们证明，我们的方法可以灵活地实现高度可控的文本驱动的 I2I 转换，无论是在参考图像的引导因素还是引导强度方面，只需分别调整替换频段的类型和带宽即可。大量的定性和定量实验验证了我们的方法在 I2I 转换视觉质量、多功能性和可控性方面优于相关方法。||
|**2024-08-01**|[CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting Idiopathic Pulmonary Fibrosis Progression](http://arxiv.org/abs/2408.00938)|null|特发性肺纤维化 (IPF) 的进展与较高的患者死亡率显著相关。及早发现 IPF 进展对于及时启动治疗至关重要，这可以有效减缓疾病进展。然而，目前的临床标准将疾病进展定义为需要间隔一年的两次 CT 扫描，这带来了一个困境：只有在疾病已经进展后才能识别出疾病进展。为此，在本文中，我们开发了一种新的扩散模型，通过从初始 CT 扫描生成患者的后续 CT 扫描来准确预测 IPF 的进展。具体来说，根据临床先验知识，我们对传统的扩散模型进行了改进，并提出了一种临床信息残差扩散模型，称为 CIResDiff。CIResDiff 的关键创新包括：1) 执行目标区域预配准以对齐不同时间点的两次 CT 扫描的肺部区域，从而降低生成难度；2) 采用残差扩散而不是传统的扩散，使模型能够更多地关注两次 CT 扫描之间的差异（即病灶），而不是基本相同的解剖结构内容；3) 基于 CLIP 技术设计临床信息流程，将与诊断高度相关的肺功能信息整合到逆向过程中，以辅助生成。大量临床数据实验表明，我们的方法优于现有技术，可以有效预测 IPF 的进展。||
|**2024-07-31**|[Detecting, Explaining, and Mitigating Memorization in Diffusion Models](http://arxiv.org/abs/2407.21720)|**[link](https://github.com/yuxinwenrick/diffusion_memorization)**|扩散模型的最新突破展现出卓越的图像生成能力。然而，研究表明，一些输出仅仅是训练数据的复制品。这种复制行为对模型所有者提出了潜在的法律挑战，尤其是当生成的内容包含专有信息时。在这项工作中，我们介绍了一种简单而有效的方法，通过检查文本条件预测的幅度来检测记忆提示。我们提出的方法可以无缝集成，而不会破坏采样算法，并且即使在第一个生成步骤中也能提供高精度，每个提示只需生成一次。基于我们的检测策略，我们揭示了一种可解释的方法，该方法展示了单个词或标记对记忆的贡献。这为用户提供了一个交互式媒介来调整他们的提示。此外，我们提出了两种策略，即利用文本条件预测的幅度来减轻记忆，要么在推理过程中最小化，要么在训练过程中过滤。这些提出的策略有效地抵消了记忆，同时保持了高质量的生成。代码可在https://github.com/YuxinWenRick/diffusion_memorization获取。||
|**2024-07-31**|[Tora: Trajectory-oriented Diffusion Transformer for Video Generation](http://arxiv.org/abs/2407.21705)|null|扩散Transformer（DiT）的最新进展在生成高质量视频内容方面展现出非凡的能力。然而，基于Transformer的扩散模型在有效生成具有可控运动的视频方面的潜力仍然是一个探索有限的领域。本文介绍了Tora，这是第一个面向轨迹的DiT框架，它同时集成了文本、视觉和轨迹条件以生成视频。具体来说，Tora由轨迹提取器（TE）、时空DiT和运动引导融合器（MGF）组成。TE使用3D视频压缩网络将任意轨迹编码为分层的时空运动块。MGF将运动块集成到DiT块中，以生成遵循轨迹的一致视频。我们的设计与DiT的可扩展性无缝衔接，允许精确控制视频内容的动态，包括不同的持续时间、纵横比和分辨率。大量实验表明，Tora在实现高运动保真度的同时，还能精细地模拟物理世界的运动。页面可以在https://ali-videoai.github.io/tora_video找到。||
|**2024-07-31**|[Generative Diffusion Model for Seismic Imaging Improvement of Sparsely Acquired Data and Uncertainty Quantification](http://arxiv.org/abs/2407.21683)|null|从稀疏采集数据进行地震成像面临着图像质量低、不连续性和偏移假象等挑战。现有的基于卷积神经网络 (CNN) 的方法难以处理复杂的特征分布，无法有效评估不确定性，因此难以评估其处理结果的可靠性。为了解决这些问题，我们提出了一种使用生成扩散模型 (GDM) 的新方法。在训练阶段，我们使用稀疏数据的成像结果作为条件输入，结合密集数据成像结果的噪声版本，让网络预测添加的噪声。训练后，网络可以使用具有条件控制的生成过程，根据稀疏数据采集预测测试图像的成像结果。这种 GDM 不仅提高了图像质量并消除了稀疏数据造成的伪影，而且还通过利用 GDM 的概率性质自然地评估了不确定性。为了克服生成质量下降和大规模图像的内存负担，我们开发了一种能够有效解决这些问题的补丁融合策略。合成数据和现场数据实例表明，我们的方法显着提高了成像质量，并提供了有效的不确定性量化。||
|**2024-07-31**|[Conditioned Prompt-Optimization for Continual Deepfake Detection](http://arxiv.org/abs/2407.21554)|**[link](https://github.com/laitifranz/Prompt2Guard)**|生成模型的快速发展显著增强了数字内容创作的真实感和定制化程度。这些工具的功能越来越强大，而且易于使用，这促进了逼真伪造内容（称为“深度伪造”）的产生，引发了人们对其潜在滥用的严重担忧。作为应对，深度伪造内容检测机制的开发已取得显著进展，以识别由这些先进系统生成的内容。然而，现有方法往往难以适应不断发展的深度伪造生成技术。本文介绍了一种名为 Prompt2Guard 的新型解决方案，用于对图像进行无样本的持续深度伪造检测，该方案利用了视觉语言模型 (VLM) 和特定领域的多种模式提示。与先前受限于提示选择准确性或需要多次前向传递的基于 VLM 的方法相比，我们利用了具有只读提示的预测集成技术。只读提示不与 VLM 的内部表示交互，从而减少了对多次前向传递的需求。因此，我们提高了检测生成内容的效率和准确性。此外，我们的方法利用了专为深度伪造检测而设计的文本提示条件，我们证明这在我们的设置中是有益的。我们在 CDDB-Hard 上评估了 Prompt2Guard，这是一个持续深度伪造检测基准，由五个跨越多个领域和生成器的深度伪造检测数据集组成，取得了新的最先进的结果。此外，我们的结果强调了我们的方法在应对持续深度伪造检测挑战方面的有效性，为更稳健、更具适应性的深度伪造检测解决方案铺平了道路。||
|**2024-07-31**|[Explainable and Controllable Motion Curve Guided Cardiac Ultrasound Video Generation](http://arxiv.org/abs/2407.21490)|null|超声心动图视频是诊断心脏病的主要方式，但有限的数据对临床教学和机器学习训练都构成了挑战。近年来，视频生成模型已成为缓解这一问题的有希望的策略。然而，以往的方法在生成过程中往往依赖于整体条件，阻碍了对特定心脏结构的灵活运动控制。在此背景下，我们提出了一种可解释且可控的超声心动图视频生成方法，以初始帧和运动曲线作为指导。我们的贡献有三方面。首先，我们从每个心脏子结构中提取运动信息以构建运动曲线，使扩散模型能够通过修改这些曲线来合成定制的超声心动图视频。其次，我们提出了结构到运动对齐模块，它可以将语义特征映射到跨心脏结构的运动曲线上。第三，位置感知注意力机制旨在利用包含结构位置信息的高斯掩码来增强视频一致性。在三个超声心动图数据集上的大量实验表明，我们的方法在保真度和一致性方面优于其他方法。完整代码将在 https://github.com/mlmi-2024-72/ECM 发布。||
|**2024-07-31**|[Maverick: Efficient and Accurate Coreference Resolution Defying Recent Trends](http://arxiv.org/abs/2407.21489)|**[link](https://github.com/sapienzanlp/maverick-coref)**|Large autoregressive generative models have emerged as the cornerstone for achieving the highest performance across several Natural Language Processing tasks. However, the urge to attain superior results has, at times, led to the premature replacement of carefully designed task-specific approaches without exhaustive experimentation. The Coreference Resolution task is no exception; all recent state-of-the-art solutions adopt large generative autoregressive models that outperform encoder-based discriminative systems. In this work,we challenge this recent trend by introducing Maverick, a carefully designed - yet simple - pipeline, which enables running a state-of-the-art Coreference Resolution system within the constraints of an academic budget, outperforming models with up to 13 billion parameters with as few as 500 million parameters. Maverick achieves state-of-the-art performance on the CoNLL-2012 benchmark, training with up to 0.006x the memory resources and obtaining a 170x faster inference compared to previous state-of-the-art systems. We extensively validate the robustness of the Maverick framework with an array of diverse experiments, reporting improvements over prior systems in data-scarce, long-document, and out-of-domain settings. We release our code and models for research purposes at https://github.com/SapienzaNLP/maverick-coref.||
|**2024-07-31**|[Fine-gained Zero-shot Video Sampling](http://arxiv.org/abs/2407.21475)|null|Incorporating a temporal dimension into pretrained image diffusion models for video generation is a prevalent approach. However, this method is computationally demanding and necessitates large-scale video datasets. More critically, the heterogeneity between image and video datasets often results in catastrophic forgetting of the image expertise. Recent attempts to directly extract video snippets from image diffusion models have somewhat mitigated these problems. Nevertheless, these methods can only generate brief video clips with simple movements and fail to capture fine-grained motion or non-grid deformation. In this paper, we propose a novel Zero-Shot video Sampling algorithm, denoted as $\mathcal{ZS}^2$, capable of directly sampling high-quality video clips from existing image synthesis methods, such as Stable Diffusion, without any training or optimization. Specifically, $\mathcal{ZS}^2$ utilizes the dependency noise model and temporal momentum attention to ensure content consistency and animation coherence, respectively. This ability enables it to excel in related tasks, such as conditional and context-specialized video generation and instruction-guided video editing. Experimental results demonstrate that $\mathcal{ZS}^2$ achieves state-of-the-art performance in zero-shot video generation, occasionally outperforming recent supervised methods.   Homepage: \url{https://densechen.github.io/zss/}.||
|**2024-08-01**|[QuestGen: Effectiveness of Question Generation Methods for Fact-Checking Applications](http://arxiv.org/abs/2407.21441)|null|Verifying fact-checking claims poses a significant challenge, even for humans. Recent approaches have demonstrated that decomposing claims into relevant questions to gather evidence enhances the efficiency of the fact-checking process. In this paper, we provide empirical evidence showing that this question decomposition can be effectively automated. We demonstrate that smaller generative models, fine-tuned for the question generation task using data augmentation from various datasets, outperform large language models by up to 8%. Surprisingly, in some cases, the evidence retrieved using machine-generated questions proves to be significantly more effective for fact-checking than that obtained from human-written questions. We also perform manual evaluation of the decomposed questions to assess the quality of the questions generated.||
|**2024-07-31**|[A Plug-and-Play Method for Rare Human-Object Interactions Detection by Bridging Domain Gap](http://arxiv.org/abs/2407.21438)|**[link](https://github.com/lijunzhang01/cefa)**|Human-object interactions (HOI) detection aims at capturing human-object pairs in images and corresponding actions. It is an important step toward high-level visual reasoning and scene understanding. However, due to the natural bias from the real world, existing methods mostly struggle with rare human-object pairs and lead to sub-optimal results. Recently, with the development of the generative model, a straightforward approach is to construct a more balanced dataset based on a group of supplementary samples. Unfortunately, there is a significant domain gap between the generated data and the original data, and simply merging the generated images into the original dataset cannot significantly boost the performance. To alleviate the above problem, we present a novel model-agnostic framework called \textbf{C}ontext-\textbf{E}nhanced \textbf{F}eature \textbf{A}lignment (CEFA) module, which can effectively align the generated data with the original data at the feature level and bridge the domain gap. Specifically, CEFA consists of a feature alignment module and a context enhancement module. On one hand, considering the crucial role of human-object pairs information in HOI tasks, the feature alignment module aligns the human-object pairs by aggregating instance information. On the other hand, to mitigate the issue of losing important context information caused by the traditional discriminator-style alignment method, we employ a context-enhanced image reconstruction module to improve the model's learning ability of contextual cues. Extensive experiments have shown that our method can serve as a plug-and-play module to improve the detection performance of HOI models on rare categories\footnote{https://github.com/LijunZhang01/CEFA}.||
|**2024-07-31**|[Deformable 3D Shape Diffusion Model](http://arxiv.org/abs/2407.21428)|null|The Gaussian diffusion model, initially designed for image generation, has recently been adapted for 3D point cloud generation. However, these adaptations have not fully considered the intrinsic geometric characteristics of 3D shapes, thereby constraining the diffusion model's potential for 3D shape manipulation. To address this limitation, we introduce a novel deformable 3D shape diffusion model that facilitates comprehensive 3D shape manipulation, including point cloud generation, mesh deformation, and facial animation. Our approach innovatively incorporates a differential deformation kernel, which deconstructs the generation of geometric structures into successive non-rigid deformation stages. By leveraging a probabilistic diffusion model to simulate this step-by-step process, our method provides a versatile and efficient solution for a wide range of applications, spanning from graphics rendering to facial expression animation. Empirical evidence highlights the effectiveness of our approach, demonstrating state-of-the-art performance in point cloud generation and competitive results in mesh deformation. Additionally, extensive visual demonstrations reveal the significant potential of our approach for practical applications. Our method presents a unique pathway for advancing 3D shape manipulation and unlocking new opportunities in the realm of virtual reality.||
|**2024-07-30**|[Matting by Generation](http://arxiv.org/abs/2407.21017)|null|本文介绍了一种用于图像抠图的创新方法，该方法将传统的基于回归的任务重新定义为生成建模挑战。我们的方法利用了潜在扩散模型的功能，并结合了广泛的预训练知识，对抠图过程进行正则化。我们提出了新颖的架构创新，使我们的模型能够生成具有更高分辨率和细节的蒙版。所提出的方法用途广泛，可以执行无引导和基于引导的图像抠图，适应各种附加线索。我们对三个基准数据集的综合评估证明了我们方法在定量和定性方面的优越性能。结果不仅反映了我们方法的鲁棒性和有效性，还突出了其生成逼真、视觉上引人注目的蒙版的能力。本文的项目页面位于https://lightchaserx.github.io/matting-by-generation/||
|**2024-07-30**|[Add-SD: Rational Generation without Manual Reference](http://arxiv.org/abs/2407.21016)|**[link](https://github.com/ylingfeng/add-sd)**|扩散模型在视觉概括方面展现出非凡的能力。基于这一成功，我们引入了一种名为 Add-SD 的基于指令的对象添加流程，该流程可以自动将对象插入到逼真的场景中，并具有合理的尺寸和位置。与布局条件方法不同，Add-SD 仅依赖于简单的文本提示，而不是任何其他人力成本高昂的参考，例如边界框。我们的工作在三个方面做出了贡献：提出了一个包含大量指令图像对的数据集；微调扩散模型以实现合理生成；以及生成合成数据以促进下游任务。第一个方面涉及创建一个由原始图像和编辑图像对组成的 RemovalDataset，其中包含文本指令，其中已从原始图像中删除了一个对象，同时保持了背景中的强像素一致性。然后使用这些数据对微调 Stable Diffusion (SD) 模型。随后，预训练的 Add-SD 模型允许将预期对象插入到图像中，并具有良好的合理性。此外，我们大规模生成用于下游任务数据集的合成实例，特别是针对尾部类别，以缓解长尾问题。下游任务受益于具有增强多样性和合理性的丰富数据集。在 LVIS 验证集上的实验表明，Add-SD 在稀有类别上的 mAP 比基线提高了 4.3。代码和模型可在 https://github.com/ylingfeng/Add-SD 获取。||
|**2024-07-30**|[Dynamic Scene Understanding through Object-Centric Voxelization and Neural Rendering](http://arxiv.org/abs/2407.20908)|**[link](https://github.com/zyp123494/dynavol)**|从无监督视频中学习以对象为中心的表示是具有挑战性的。与大多数先前专注于分解二维图像的方法不同，我们提出了一种名为 DynaVol-S 的三维生成模型，用于动态场景，该模型能够在可微分的体渲染框架内进行以对象为中心的学习。其关键思想是执行以对象为中心的体素化，以捕捉场景的三维性质，从而推断单个空间位置上每个对象的占用概率。这些体素特征通过规范空间变形函数演化，并在具有组合式 NeRF 的逆向渲染管道中进行优化。此外，我们的方法集成了二维语义特征来创建三维语义网格，通过多个解耦的体素网格来表示场景。DynaVol-S 在动态场景的新颖视图合成和无监督分解任务中均明显优于现有模型。通过共同考虑几何结构和语义特征，它有效地解决了涉及复杂对象交互的现实世界场景的挑战。此外，一旦经过训练，明确意义的体素特征就可以实现二维场景分解方法无法实现的附加功能，例如通过编辑几何形状或操纵对象的运动轨迹来生成新场景。||
|**2024-07-30**|[Vulnerabilities in AI-generated Image Detection: The Challenge of Adversarial Attacks](http://arxiv.org/abs/2407.20836)|null|近年来，图像合成技术的进步，特别是GAN和扩散模型的出现，加剧了公众对虚假信息传播的担忧。为了解决这些担忧，人们提出了许多人工智能生成图像（AIGI）检测器，并在识别伪造图像方面取得了可喜的性能。然而，目前仍然缺乏对这些AIGI检测器对抗鲁棒性的系统理解。在本文中，我们研究了最先进的AIGI检测器在白盒和黑盒设置下对抗攻击的脆弱性，而这方面迄今为止很少被研究。针对AIGI检测任务，我们提出了一种新的攻击方法，它包含两个主要部分。首先，受真实图像和伪造图像在频域存在明显差异的启发，我们在频域添加扰动，以推动图像偏离其原始频率分布。其次，我们探索了代理模型的完整后验分布，以进一步缩小异构模型之间的差距，例如在CNN和ViT之间迁移对抗样本。这是通过引入一种新的训练后贝叶斯策略来实现的，该策略将单个代理模型转换为贝叶斯模型，从而能够使用一个预训练的代理模型模拟不同的受害者模型，而无需重新训练。我们将我们的方法命名为基于频率的训练后贝叶斯攻击，简称FPBA。通过FPBA，我们证明了对抗攻击对AIGI检测器来说是一个真正的威胁，因为FPBA可以跨模型、生成器、防御方法甚至跨生成器检测（这是一种至关重要的现实世界检测场景）成功地进行黑盒攻击。||
|**2024-07-30**|[Diffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learning](http://arxiv.org/abs/2407.20798)|null|我们介绍了扩散增强代理（DAAG），这是一个利用大型语言模型、视觉语言模型和扩散模型来提高具身代理强化学习中的样本效率和迁移学习的新框架。DAAG 后见之明通过使用扩散模型以时间和几何一致的方式转换视频来重新标记代理过去的经验，以使用我们称之为后见之明经验增强技术与目标指令保持一致。大型语言模型协调这个自主过程，不需要人工监督，使其非常适合终身学习场景。该框架减少了奖励标记数据所需的数量，以 1）微调充当奖励检测器的视觉语言模型，以及 2）训练 RL 代理执行新任务。我们展示了 DAAG 在涉及操作和导航的模拟机器人环境中的样本效率提升。我们的结果表明，DAAG 改善了奖励检测器的学习、过去经验的迁移和新任务的获取——这是开发高效终身学习代理的关键能力。补充材料和可视化可在我们的网站 https://sites.google.com/view/diffusion-augmented-agents/ 上获取。||
|**2024-07-30**|[SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models](http://arxiv.org/abs/2407.20756)|**[link](https://github.com/starriver030515/synthvlm)**|近年来，随着网络图像的兴起，管理和理解大规模图像数据集变得越来越重要。视觉大型语言模型 (VLLM) 近期因其强大的视觉理解能力而出现。然而，训练这些模型需要大量数据，这对效率、有效性、数据质量和隐私提出了挑战。在本文中，我们介绍了 SynthVLM，一种用于 VLLM 的新型数据合成管道。与现有从图像生成字幕的方法不同，SynthVLM 采用先进的扩散模型和高质量的字幕，从字幕中自动生成和选择高分辨率图像，从而创建精确对齐的图像-文本对。利用这些图像-文本对，我们在各种视觉问答任务上实现了最先进的 (SoTA) 性能，同时保持了较高的对齐质量并保留了先进的语言能力。此外，SynthVLM 在性能上超越了传统的基于 GPT-4 Vision 的字幕生成方法，同时显著降低了计算开销。至关重要的是，我们的方法依赖于纯粹生成的数据，确保了隐私的保护，仅用 10 万个数据点（仅为官方数据集大小的 18%）就实现了 SoTA 性能。||
|**2024-07-30**|[Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks](http://arxiv.org/abs/2407.20657)|null|最近出现的视觉语言基础模型，例如CLIP，在学习可迁移到各种下游任务和领域的表示方面表现出优越的能力。随着这种强大模型的出现，有效利用其能力来应对具有挑战性的视觉任务变得至关重要。另一方面，只有少数工作专注于设计能够很好地迁移到未知领域和模型架构的对抗性样本。在本文中，我们提出了一种名为PDCL-Attack的新型迁移攻击方法，该方法利用CLIP模型来增强基于生成模型的攻击框架生成的对抗性扰动的可迁移性。具体来说，我们利用文本的语义表示能力，特别是来自输入图像的真实类别标签，制定了一种有效的提示驱动特征指导。据我们所知，我们是第一个引入提示学习来增强可迁移生成攻击的。在各种跨域和跨模型设置下进行的大量实验从经验上验证了我们的方法，证明了其优于最先进方法的优越性。||
|**2024-07-30**|[FACL-Attack: Frequency-Aware Contrastive Learning for Transferable Adversarial Attacks](http://arxiv.org/abs/2407.20653)|null|众所周知，深度神经网络容易受到安全风险的影响，因为对抗样本具有内在的可迁移性。尽管最近基于生成模型的攻击取得了成功，证明了强大的可迁移性，但在现实世界的严格黑盒环境中（目标域和模型架构都未知）设计有效的攻击策略仍然是一个挑战。在本文中，我们试图探索频域中的特征对比方法，以生成在跨域和跨模型环境中都具有鲁棒性的对抗样本。为此，我们提出了两个仅在训练阶段使用的模块：一个频率感知域随机化（FADR）模块，用于随机化域变异的低频和高频分量；一个频率增强对比学习（FACL）模块，用于有效分离干净图像和扰动图像的域不变中频特征。我们通过广泛的跨域和跨模型实验，证明了我们生成的对抗扰动的强大可迁移性，同时保持了推理时间复杂度。||
|**2024-07-30**|[EgoSonics: Generating Synchronized Audio for Silent Egocentric Videos](http://arxiv.org/abs/2407.20592)|null|我们推出了 EgoSonics，这是一种根据无声以自我为中心的视频生成语义上有意义且同步的音轨的方法。为无声的以自我为中心的视频生成音频可以在虚拟现实、辅助技术或增强现有数据集方面开辟新的应用。现有的工作仅限于语音、音乐或撞击声等领域，无法轻松捕捉以自我为中心的视频中发现的广泛音频频率。EgoSonics 通过利用潜在扩散模型进行条件音频合成的优势来解决这些限制。我们首先将音频和视频数据编码和处理成适合生成的形式。编码后的数据用于训练我们的模型，以生成捕捉输入视频语义的音轨。我们提出的 SyncroNet 建立在 ControlNet 之上，提供控制信号，使时间同步到合成音频。广泛的评估表明，我们的模型在音频质量和我们新提出的同步评估方法方面优于现有工作。此外，我们展示了我们的模型在下游应用中的应用，以改进视频摘要。||
|**2024-07-30**|[DiffusionCounterfactuals: Inferring High-dimensional Counterfactuals with Guidance of Causal Representations](http://arxiv.org/abs/2407.20553)|null|在高维数据中准确估计反事实结果对于决策、理解因果关系和干预结果至关重要，这涵盖了医疗保健、经济学和社会科学等各个领域。然而，现有方法往往难以生成准确且一致的反事实结果，尤其是在因果关系复杂的情况下。我们提出了一个新的框架，它结合了因果机制和扩散模型，以生成由因果表示引导的高质量反事实样本。我们的方法引入了一种新颖的、有理论依据的训练和采样过程，使模型能够在多个干预步骤下持续生成准确的反事实高维数据。在各种合成和真实基准上的实验结果表明，所提出的方法在使用不同的评估指标生成准确和高质量的反事实结果方面优于最先进的方法。||
|**2024-07-26**|[Scalable Group Choreography via Variational Phase Manifold Learning](http://arxiv.org/abs/2407.18839)|null|从音乐中生成群舞动作是一项具有挑战性的任务，在工业中有多种应用。虽然已经提出了几种方法来解决这个问题，但大多数方法都优先考虑在数据集预定义的舞者数量的限制下优化舞蹈动作的保真度。这种限制阻碍了对现实世界应用的适应性。我们的研究解决了群舞编排中的可扩展性问题，同时保留了自然性和同步性。特别是，我们提出了一种基于相位的变分生成模型，用于在学习生成流形上生成群舞。我们的方法实现了高保真度的群舞动作生成，并能够生成无限数量的舞者，同时只消耗最少且恒定的内存量。在两个公共数据集上的大量实验表明，我们提出的方法大大优于现有的最新方法，并且可以扩展到训练数据之外的大量舞者。||
|**2024-07-26**|[Revision of calcium and scandium abundances in Am stars based on NLTE calculations and comparison with diffusion stellar evolution models](http://arxiv.org/abs/2407.18736)|null|针对54颗金属线(Am)星样本，获得了考虑了偏离局部热力学平衡(LTE)的钙(Ca)和钪(Sc)丰度的均匀数据集。研究发现，Ca和Sc的丰度与有效温度Teff相关，表面重力log g < 4的恒星，丰度随Teff的增加而增长的幅度高于log g > 4的恒星。Ca或Sc丰度与铁丰度或轴向旋转速度之间没有发现相关性。Am星表现出的[Ca/H]平均值高于[Sc/H]，丰度比[Ca/Sc] = 0.41 +/- 0.30。然而，在Teff > 9500 K时，表面重力log g > 4和log g < 4的Am星之间存在系统性差异的迹象。在7200 K <= Teff <= 10030 K范围内，铁的过量几乎相同。使用MESA代码计算的质量为1.5到2个太阳质量的恒星的演化扩散模型显示，表面丰度与在年龄大于600 Myr的三个疏散星团中观测到的Am星的Ca和Fe丰度非常吻合。对于昴宿星团中的年轻恒星，应考虑额外的化学分离机制来解释Am现象。我们测试了已发表的扩散恒星演化模型。Richer等人(2000)和Hui-Bon-Hoa等人(2022)的扩散模型表明，在自由湍流参数较大时，与疏散星团中Am星的观测结果一致：Ca和Fe的ω=1000，Sc的ω=500。没有哪个模型能够在Am型恒星天狼星的质量和年龄下重现其从He到Ni的表面丰度。本文提出的结果对于理解Am星的化学特性可能具有重要意义。||
|**2024-07-26**|[BCTR: Bidirectional Conditioning Transformer for Scene Graph Generation](http://arxiv.org/abs/2407.18715)|null|场景图生成（SGG）由于其组合性质仍然是一项具有挑战性的任务。先前的方法通过端到端的方式学习来提高预测效率。然而，这些方法表现出有限的性能，因为它们假设实体和谓词之间是单向条件作用的，导致信息交互不足。为了解决这个限制，我们提出了一种新的用于 SGG 的双向条件分解方法，在实体和谓词之间引入有效的交互。具体来说，我们开发了一个端到端的场景图生成模型，称为双向条件Transformer（BCTR），来实现我们的分解方法。BCTR 由两个关键模块组成。首先，双向条件生成器（BCG）促进了实体和谓词之间多阶段的交互式特征增强，使两种预测之间能够相互受益。其次，随机特征对齐（RFA）通过从预训练模型中提取多模态知识来规范特征空间，增强 BCTR 在尾部类别上的能力，而无需依赖统计先验。我们在 Visual Genome 和 Open Image V6 上进行了一系列实验，证明 BCTR 在这两个基准测试中都达到了最先进的性能。代码将在论文被接受后公开。||
|**2024-07-26**|[Adversarial Robustification via Text-to-Image Diffusion Models](http://arxiv.org/abs/2407.18658)|**[link](https://github.com/choidae1/robustify-t2i)**|对抗性鲁棒性传统上被认为是神经网络编码的一个具有挑战性的属性，需要大量的训练数据。然而，在最近采用现成模型的范例中，访问其训练数据通常是不可行或不切实际的，而大多数此类模型最初并没有考虑对抗性鲁棒性进行训练。在本文中，我们开发了一种可扩展且与模型无关的解决方案，无需使用任何数据即可实现对抗性鲁棒性。我们的直觉是将最近的文本到图像扩散模型视为可以优化以指定目标任务的“自适应”去噪器。基于此，我们提出：(a) 启动一个去噪和分类管道，提供针对对抗性攻击的可证明保证，以及 (b) 利用从文本到图像模型生成的少量合成参考图像，实现新的适应方案。我们的实验表明，我们应用于预训练 CLIP 的无数据方案可以提高其多样化零样本分类衍生物（同时保持其准确性）的（可证明的）对抗性鲁棒性，显着超越了利用完整训练数据的先前方法。我们的框架不仅适用于 CLIP，还可以轻松有效地应用于增强其他视觉分类器的鲁棒性。||
|**2024-07-26**|[Robust VAEs via Generating Process of Noise Augmented Data](http://arxiv.org/abs/2407.18632)|null|提升生成模型防御对抗性攻击的能力是机器学习领域的一个关键研究课题。我们的研究集中在一种特定类型的生成模型——变分自编码器（VAE）。与普遍看法和现有文献认为对训练数据注入噪声可以增强模型鲁棒性相反，我们的初步实验表明，简单地使用噪声增强技术并没有显著提高VAE的鲁棒性。事实上，它甚至降低了学习表示的质量，使得VAE更容易受到对抗性扰动的影响。本文介绍了一种新的框架，通过规范化原始数据和噪声增强数据之间潜在空间的差异来增强鲁棒性。通过将成对概率先验纳入标准变分下界，我们的方法显著增强了对抗性攻击的防御能力。我们的实证评估表明，这种称为鲁棒增强变分自编码器（RAVEN）的方法在广泛认可的基准数据集上，在抵抗对抗性输入方面表现出色。||
|**2024-07-26**|[Denoising Lévy Probabilistic Models](http://arxiv.org/abs/2407.18609)|null|研究扩散生成模型中非高斯噪声分布是一个开放性问题。高斯情况在实验和理论上都取得了成功，适用于基于分数和去噪公式的统一随机微分方程（SDE）框架。最近的研究表明，重尾噪声分布可以解决模式坍塌问题，并管理具有类别不平衡、重尾或异常值的数据集。Yoon 等人（NeurIPS 2023）引入了 Lévy-Ito 模型 (LIM)，将 SDE 框架扩展到具有 α 稳定噪声的重尾 SDE。尽管 LIM 具有理论上的优雅性和性能提升，但其复杂的数学原理可能会限制其可访问性和更广泛的采用。本研究采用了一种更简单的方法，通过使用 α 稳定噪声扩展去噪扩散概率模型 (DDPM)，创建了去噪 Lévy 概率模型 (DLPM)。我们使用基本的证明技术表明，DLPM 可以通过最小的更改简化为运行 vanilla DDPM，从而允许使用现有的实现并进行最小的更改。DLPM 和 LIM 具有不同的训练算法，并且与高斯情况不同，它们承认不同的后向过程和采样算法。我们的实验表明，DLPM 实现了更好的数据分布尾部覆盖范围、改进了不平衡数据集的生成，并通过更少的反向步骤缩短了计算时间。||
|**2024-07-26**|[Socially efficient mechanism on the minimum budget](http://arxiv.org/abs/2407.18515)|null|在策略代理的社会决策中，一个普遍的关注点在于社会利益和个人利益之间的平衡。因此，社会有效机制的设计不仅要最大化社会福利，还要激励代理人追求自身利益。在一类包含双重拍卖和交易网络等应用的广义模型下，本研究建立了一种社会有效 (SE)、主导策略激励相容 (DSIC) 和个人理性 (IR) 的机制，该机制使代理人的总支出预算最小化。本方法利用离散和已知类型域将一组约束简化为加权图中的最短路径问题。除了理论推导之外，我们还通过数值实验证实了所提出的机制的最优性，实验结果表明，对于大量的实例，该机制的预算严格低于 Vickery-Clarke-Groves (VCG) 机制。||
|**2024-07-26**|[Revisit Event Generation Model: Self-Supervised Learning of Event-to-Video Reconstruction with Implicit Neural Representations](http://arxiv.org/abs/2407.18500)|null|从事件数据中重建强度帧对于弥合基于事件的计算机视觉和基于帧的计算机视觉之间的差距至关重要，同时保持高时间分辨率和动态范围。以前的方法依赖于对合成数据的监督学习，这缺乏可解释性，并且存在过度拟合事件模拟器设置的风险。最近，基于自监督学习（SSL）的方法，它主要利用每帧光流通过光度一致性来估计强度，已被积极研究。然而，在光流不准确的情况下，它们容易出错。本文提出了一种新的 SSL 事件到视频重建方法，称为 EvINR，它不需要标记数据或光流估计。我们的核心理念是通过直接解决事件生成模型来重建强度帧，该模型本质上是一个偏微分方程（PDE），描述了如何根据时变亮度信号生成事件。具体来说，我们利用隐式神经表示（INR）来表示事件生成方程的解，该表示接收时空坐标 $(x, y, t)$ 并预测强度值。INR 被参数化为一个全连接的多层感知器（MLP），可以通过事件监督其时间导数来优化。为了使 EvINR 能够满足在线需求，我们提出了几种加速技术，可以大大加快训练过程。综合实验表明，我们的 EvINR 在均方误差 (MSE) 方面比之前的 SSL 方法提高了 38%，并且与 SoTA 监督方法相当或更优。项目页面：https://vlislab22.github.io/EvINR/。||
|**2024-07-26**|[Answerability Fields: Answerable Location Estimation via Diffusion Models](http://arxiv.org/abs/2407.18497)|null|在人工智能和机器人技术飞速发展的时代，让机器能够与环境互动并理解环境是一项至关重要的研究工作。在本文中，我们提出了“可回答性场”（Answerability Fields），这是一种预测复杂室内环境中可回答性的新方法。利用三维问答数据集，我们构建了一个全面的“可回答性场”数据集，其中包含来自 ScanNet 的各种场景和问题。我们使用扩散模型成功地推断和评估了这些“可回答性场”，证明了物体及其位置在回答场景内问题方面的重要性。我们的结果展示了“可回答性场”在指导场景理解任务方面的有效性，为其应用于增强智能体与其环境之间的交互奠定了基础。||
|**2024-07-26**|[Diffusion-Driven Semantic Communication for Generative Models with Bandwidth Constraints](http://arxiv.org/abs/2407.18468)|null|近年来，扩散模型由于其优越的生成能力，在人工智能生成内容（AIGC）中得到了广泛应用。结合语义通信，扩散模型被用于去噪、数据重建和内容生成等任务。然而，现有的基于扩散的生成模型没有考虑严格的带宽限制，这限制了其在无线通信中的应用。本文介绍了一种扩散驱动的语义通信框架，该框架采用先进的基于变分自编码器的压缩技术，适用于带宽受限的生成模型。我们设计的架构利用了扩散模型，其中通过无线信道的信号传输过程充当了扩散中的前向过程。为了降低带宽需求，我们在接收端结合了一个基于变分自编码器的下采样模块和一个配对的上采样模块，并进行重新参数化，以确保恢复的特征符合高斯分布。此外，我们推导了所提出系统的损失函数，并通过综合实验评估了其性能。我们的实验结果表明，在峰值信噪比（PSNR）等像素级指标和学习感知图像块相似度（LPIPS）等语义指标方面都有显著提高。与深度联合信源信道编码（DJSCC）相比，这些改进在压缩率和信噪比方面更为显著。||
|**2024-07-25**|[VGGHeads: A Large-Scale Synthetic Dataset for 3D Human Heads](http://arxiv.org/abs/2407.18245)|null|人头检测、关键点估计和 3D 人头模型拟合是许多应用中的重要任务。然而，传统的现实世界数据集经常受到偏差、隐私和伦理问题的困扰，而且它们是在实验室环境中记录的，这使得训练模型难以泛化。在此，我们介绍 VGGHeads——一个使用扩散模型生成的大规模合成数据集，用于人头检测和 3D 网格估计。我们的数据集包含超过 100 万张高分辨率图像，每张图像都标注了详细的 3D 人头网格、面部关键点和边界框。利用该数据集，我们引入了一种新的模型架构，能够从单张图像中一步同时进行人头检测和人头网格重建。通过广泛的实验评估，我们证明了在合成数据上训练的模型在真实图像上实现了强大的性能。此外，我们数据集的多功能性使其适用于广泛的任务，提供了人头的通用和全面表示。此外，我们还提供了关于合成数据生成流程的详细信息，使其能够被其他任务和领域重复使用。||
|**2024-07-25**|[Self-supervised pre-training with diffusion model for few-shot landmark detection in x-ray images](http://arxiv.org/abs/2407.18125)|null|近年来，深度神经网络已广泛应用于医学领域的不同任务，从图像分类和分割到 landmarks 检测。然而，这些技术在医学领域的应用往往受到数据稀缺的阻碍，无论是在可用标注还是图像方面。本研究介绍了一种新的基于扩散模型的自监督预训练协议，用于 X 光图像中的 landmarks 检测。我们的研究结果表明，所提出的自监督框架可以通过最少数量的可用标注训练图像（最多 50 张）提供准确的 landmarks 检测，在三个流行的 X 光基准数据集上优于 ImageNet 监督预训练和最先进的自监督预训练。据我们所知，这是首次探索将扩散模型用于 landmarks 检测中的自监督学习，这可能为缓解数据稀缺问题，在少样本机制中提供一种有价值的预训练方法。||
|**2024-07-25**|[AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild](http://arxiv.org/abs/2407.18034)|**[link](https://github.com/redorangeyellowy/AttentionHand)**|近年来，关于用于各种形式人机交互的3D手部重建的研究已经有很多。然而，由于极度缺乏自然场景下的3D手部数据集，自然场景下的3D手部重建仍然具有挑战性。特别是当手处于复杂姿势时，例如双手交互，外观相似性、自遮挡和深度模糊等问题使得重建更加困难。为了克服这些问题，我们提出了AttentionHand，一种新颖的文本驱动的可控手部图像生成方法。由于AttentionHand可以生成与3D手部标签良好对齐的各种各样的自然场景手部图像，我们可以获得新的3D手部数据集，并可以缓解室内和室外场景之间的域差距。我们的方法需要四种易于使用的模态（即RGB图像、来自3D标签的手部网格图像、边界框和文本提示）。这些模态通过编码阶段嵌入到潜在空间中。然后，通过文本注意阶段，来自给定文本提示的与手相关的标记被用来突出显示潜在嵌入中与手相关的区域。在突出显示的嵌入被馈送到视觉注意阶段后，通过使用基于扩散的管道，以全局和局部手部网格图像为条件，来关注嵌入中与手相关的区域。在解码阶段，最终特征被解码为新的手部图像，这些图像与给定的手部网格图像和文本提示良好对齐。因此，AttentionHand在文本到手部图像生成模型中达到了最先进的水平，并且通过使用AttentionHand生成的手部图像进行额外训练，3D手部网格重建的性能也得到了提高。||
|**2024-07-25**|[Segmentation-guided MRI reconstruction for meaningfully diverse reconstructions](http://arxiv.org/abs/2407.18026)|**[link](https://github.com/nikolasmorshuis/sgr)**|反问题，例如加速 MRI 重建，是病态的，并且存在无限多的可能且合理的解决方案。这不仅会导致重建图像的不确定性，还会导致语义分割等下游任务的不确定性。然而，这种不确定性在文献中大多没有得到分析，即使概率重建模型被普遍使用。这些模型可能容易忽略合理但不太可能的解决方案，例如罕见的病理学。基于基于扩散模型的 MRI 重建方法，我们在推理过程中添加了对扩散过程的指导，生成对应于上限和下限分割的两个有意义的不同重建。然后可以通过这些边界之间的差异来量化重建的不确定性，我们将其称为“不确定性边界”。我们分析了各种加速因子下上限和下限分割的行为，发现与重复采样相比，不确定性边界更可靠、更准确。代码可在 https://github.com/NikolasMorshuis/SGR 获取。||
|**2024-07-25**|[Self-Supervision Improves Diffusion Models for Tabular Data Imputation](http://arxiv.org/abs/2407.18013)|**[link](https://github.com/yixinliu233/simpdm)**|缺失数据的普遍性引发了人们对表格数据插补方法的广泛关注。扩散模型作为数据生成的前沿技术，在表格数据插补任务中展现出巨大潜力。然而，为了追求多样性，普通的扩散模型往往对初始化噪声很敏感，这阻碍了模型生成稳定准确的插补结果。此外，表格数据固有的稀疏性给扩散模型准确建模数据流形带来了挑战，影响了这些模型进行数据插补的鲁棒性。为了应对这些挑战，本文提出了一种名为自监督插补扩散模型（简称SimpDM）的先进扩散模型，专为表格数据插补任务而设计。为了减轻对噪声的敏感性，我们引入了一种自监督对齐机制，旨在规范模型，确保一致和稳定的插补预测。此外，我们在SimpDM中引入了一种精心设计的依赖于状态的数据增强策略，增强了扩散模型在处理有限数据时的鲁棒性。大量实验表明，SimpDM在各种情况下都能与最先进的插补方法相媲美或优于它们。||
|**2024-07-25**|[Lightweight Language-driven Grasp Detection using Conditional Consistency Model](http://arxiv.org/abs/2407.17967)|null|语言驱动的抓取检测是机器人技术中一项基础而又具有挑战性的任务，在工业中有着广泛的应用。在这项工作中，我们提出了一种新的语言驱动的抓取检测方法，该方法利用轻量级扩散模型的概念来实现快速推理。通过将扩散过程与自然语言中的抓取提示相结合，我们的方法可以有效地编码视觉和文本信息，从而实现更准确、更多功能的抓取定位，并与文本查询很好地保持一致。为了克服扩散模型中推理时间长的问题，我们在一致性模型中利用图像和文本特征作为条件，以减少推理过程中的去噪时间步长。大量的实验结果表明，我们的方法明显优于其他最近的抓取检测方法和轻量级扩散模型。我们进一步在真实的机器人实验中验证了我们的方法，以证明其快速推理的能力。||
|**2024-07-25**|[ReCorD: Reasoning and Correcting Diffusion for HOI Generation](http://arxiv.org/abs/2407.17911)|null|扩散模型通过利用自然语言引导多媒体内容的创作，彻底改变了图像生成领域。尽管此类生成模型取得了重大进展，但在描绘详细的人与物体交互（HOI）方面，尤其是在姿势和物体放置的准确性方面，仍然存在挑战。我们引入了一种名为推理和校正扩散（ReCorD）的免训练方法来解决这些挑战。我们的模型将潜在扩散模型与视觉语言模型相结合，以改进生成过程，确保对 HOI 的精确描绘。我们提出了一个交互感知推理模块来改进对交互的解释，以及一个交互校正模块来细化输出图像，从而更精确地生成 HOI。通过精心设计的姿势选择和物体定位过程，ReCorD 在生成图像时实现了更高的保真度，同时有效降低了计算需求。我们在三个基准数据集上进行了全面的实验，以证明在解决文本到图像生成任务方面的重大进展，展示了 ReCorD 通过在 HOI 分类分数、FID 和 Verb CLIP-Score 方面优于现有方法，从而准确呈现复杂交互的能力。项目网站：https://alberthkyhky.github.io/ReCorD/ 。||
|**2024-07-25**|[Amortized Posterior Sampling with Diffusion Prior Distillation](http://arxiv.org/abs/2407.17907)|null|我们提出了一种变分推断方法，用于从后验分布中采样以解决反问题。从预训练的扩散模型开始，我们的方法训练了一个条件流模型，以最小化建议的变分分布与通过扩散模型隐式定义的后验分布之间的差异。训练完成后，流模型能够通过单个 NFE 从后验分布中采样，并根据测量结果进行摊销。所提出的方法为提取扩散先验以进行高效的后验采样开辟了一条新途径。我们证明了我们的方法适用于欧几里得空间中的标准信号，以及流形上的信号。||
|**2024-07-25**|[Artificial Immunofluorescence in a Flash: Rapid Synthetic Imaging from Brightfield Through Residual Diffusion](http://arxiv.org/abs/2407.17882)|null|免疫荧光 (IF) 成像对于可视化生物标志物表达、细胞形态和评估药物治疗对亚细胞成分的影响至关重要。免疫荧光成像需要额外的染色过程，并且通常需要细胞固定，因此它也可能引入伪影并改变内源性细胞形态。一些免疫荧光染料价格昂贵或不易获得，因此阻碍了实验。最近的扩散模型从易于获取的明场 (BF) 图像合成高保真免疫荧光图像，提供了一种很有前景的解决方案，但由于噪声扩散过程，训练不稳定和推理时间慢阻碍了该方法的应用。本文提出了一种直接从明场图像以及细胞分割掩模中条件合成免疫荧光图像的新方法。我们的方法采用残差扩散过程，增强了稳定性并显着减少了推理时间。我们针对其他图像到图像合成模型进行了严格的评估，包括 UNets、GAN 和高级扩散模型。我们的模型在图像质量（MSE、PSNR 和 SSIM 中 p<0.05）、推理速度（比竞争扩散模型快 26 倍）以及细胞核和细胞体的准确分割结果（细胞核和细胞真阳性的平均 IOU 分别为 0.77 和 0.63）方面均有显着改进。这篇论文是该领域的重大进步，为细胞图像分析提供了强大而有效的工具。||
|**2024-07-25**|[DragText: Rethinking Text Embedding in Point-based Image Editing](http://arxiv.org/abs/2407.17843)|null|基于点的图像编辑可以通过内容拖动实现精确灵活的控制。然而，文本嵌入在编辑过程中的作用尚未得到彻底研究。一个尚未探索的重要方面是文本和图像嵌入之间的交互。在这项研究中，我们发现，在扩散模型中逐步编辑输入图像的过程中，文本嵌入保持不变。随着图像嵌入与其初始状态越来越不同，图像嵌入和文本嵌入之间的差异带来了重大挑战。此外，我们发现文本提示显著影响拖动过程，特别是在保持内容完整性和实现所需操作方面。为了利用这些见解，我们提出了DragText，它与拖动过程一起优化文本嵌入，以与修改后的图像嵌入配对。同时，我们规范文本优化过程，以保持原始文本提示的完整性。我们的方法可以无缝集成到现有的基于扩散的拖动方法中，只需几行代码。||
|**2024-07-23**|[Diffusion Models for Monocular Depth Estimation: Overcoming Challenging Conditions](http://arxiv.org/abs/2407.16698)|**[link](https://github.com/fabiotosi92/diffusion4robustdepth)**|我们提出了一种新方法，旨在解决单图像深度估计任务中由具有挑战性的、超出分布范围的数据带来的复杂性。我们从易于预测深度的图像开始（因为它们不存在不利因素），系统地生成新的、用户定义的场景，这些场景具有一系列挑战和相关的深度信息。这是通过利用具有深度感知控制的最先进的文本到图像扩散模型来实现的，该模型以从文本提示合成高质量图像内容而闻名，同时保留生成图像和源图像之间 3D 结构的一致性。任何单目深度网络的后续微调都是通过自蒸馏协议进行的，该协议考虑了使用我们的策略生成的图像及其对简单、非挑战性场景的自身深度预测。针对我们目的而定制的基准测试证明了我们提案的有效性和多功能性。||
|**2024-07-23**|[From Imitation to Refinement -- Residual RL for Precise Visual Assembly](http://arxiv.org/abs/2407.16677)|null|行为克隆（BC）是目前现实世界视觉操控学习的主导范式。然而，在需要局部纠正行为的任务中，例如多部件组装，仅从人类演示中学习鲁棒策略仍然具有挑战性。强化学习（RL）可以通过任务奖励监督和探索，允许策略获得局部纠正行为，从而减轻这些限制。本文探讨了使用强化学习微调来改进基于BC训练的策略在精确操作任务中的性能。我们分析并克服了使用强化学习直接训练包含现代架构组件（如扩散模型和动作分块）的策略网络相关的技术挑战。我们建议使用标准策略梯度方法和稀疏奖励，在冻结的BC训练扩散模型之上训练残差策略，我们称之为ResiP（用于精确操作的残差）。我们的实验结果表明，这种残差学习框架可以通过学习纠正动作，在高精度装配任务中显著提高成功率，超越基本的BC训练模型。我们还表明，通过将ResiP与师生蒸馏和视觉域随机化相结合，我们的方法可以直接从RGB图像中学习用于机器人装配的现实世界策略。视频和代码可在\url{https://residual-assembly.github.io}找到。||
|**2024-07-23**|[MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequence](http://arxiv.org/abs/2407.16655)|null|近年来，视频生成技术的进步主要依赖于扩散模型，但仅限于短视频内容。然而，这些方法在建模复杂叙事和在长时间内保持角色一致性方面往往不足，而这对于电影等长视频制作至关重要。我们提出了 MovieDreamer，一个新颖的分层框架，它结合了自回归模型和基于扩散的渲染的优势，开创了具有复杂情节发展和高视觉保真度的长视频生成。我们的方法利用自回归模型来实现全局叙事连贯性，预测视觉标记序列，然后通过扩散渲染将其转换为高质量的视频帧。这种方法类似于传统的电影制作过程，即将复杂的故事分解成可管理的场景拍摄。此外，我们采用多模态脚本，用详细的角色信息和视觉风格丰富场景描述，增强跨场景的连续性和角色身份。我们展示了跨多种电影类型的广泛实验，证明我们的方法不仅实现了卓越的视觉和叙事质量，而且有效地将生成内容的时长扩展到远远超出当前能力的范围。主页：https://aim-uofa.github.io/MovieDreamer/。||
|**2024-07-23**|[Knowledge-driven AI-generated data for accurate and interpretable breast ultrasound diagnoses](http://arxiv.org/abs/2407.16634)|null|数据驱动的深度学习模型在辅助放射科医生进行乳腺超声 (US) 诊断方面已展现出巨大潜力。然而，由于训练数据的长尾分布，导致模型在罕见病例中的准确性受限，从而限制了其有效性。本研究致力于解决利用长尾数据提高罕见病例诊断模型性能这一长期挑战。具体而言，我们引入了一个名为 TAILOR 的流程，它构建了一个知识驱动的生成模型来生成定制的合成数据。该生成模型使用 3,749 个病灶作为源数据，可以生成数百万张乳腺超声图像，特别是容易出错的罕见病例图像。生成的数据可进一步用于构建诊断模型，以进行准确且可解释的诊断。在前瞻性外部评估中，我们的诊断模型在特异性方面比九名放射科医生的平均水平高出 33.5%，且灵敏度相同，通过提供具有可解释决策过程的预测结果，提高了他们的诊断性能。此外，在导管原位癌 (DCIS) 中，我们的诊断模型在源数据中仅包含 34 个 DCIS 病灶的情况下，其性能也大幅优于所有放射科医生。我们相信，TAILOR 有可能扩展到各种疾病和影像模态。||
|**2024-07-23**|[DreamVTON: Customizing 3D Virtual Try-on with Personalized Diffusion Models](http://arxiv.org/abs/2407.16511)|null|基于图像的3D虚拟试穿（VTON）旨在根据人物和服装图像塑造3D人体模型，这种方法数据效率高（即无需昂贵的3D数据），但极具挑战性。最近，文本到3D方法在高保真3D人体生成方面取得了显著进步，展示了其在3D虚拟试穿方面的潜力。受个性化扩散模型（例如Dreambooth和LoRA）在2D VTON方面取得的巨大成功的启发，将个性化技术集成到基于扩散的文本到3D框架中以实现3D VTON的想法应运而生。然而，在预训练的扩散模型（例如StableDiffusion (SD)）中使用个性化模块会降低模型进行多视图或多域合成的能力，这不利于由分数蒸馏采样（SDS）损失引导的几何和纹理优化。在这项工作中，我们提出了一种名为DreamVTON的新型定制3D人体试穿模型，用于分别优化3D人体的几何形状和纹理。具体来说，我们提出了一种使用多概念LoRA的个性化SD，以提供有关特定人物和服装的生成先验，同时利用Densepose引导的ControlNet来保证不同相机视角下身体姿势的一致性先验。此外，为了避免个性化SD中不一致的多视图先验主导优化过程，DreamVTON引入了一种基于模板的优化机制，该机制采用掩码模板进行几何形状学习，并使用法线/RGB模板进行几何/纹理细节学习。此外，在几何优化阶段，DreamVTON将法线风格LoRA集成到个性化SD中，以增强法线贴图生成先验，从而促进平滑的几何建模。||
|**2024-07-23**|[qMRI Diffusor: Quantitative T1 Mapping of the Brain using a Denoising Diffusion Probabilistic Model](http://arxiv.org/abs/2407.16477)|null|定量MRI（qMRI）通过提供与组织特性相关的客观参数，与加权图像相比具有显著优势。基于深度学习的方法在从加权图像序列估计定量图方面已显示出有效性。在这项研究中，我们提出了qMRI Diffusor，这是一种利用深度生成模型进行qMRI的新方法。具体来说，我们实施了用于脑部T1量化的去噪扩散概率模型（DDPM），将定量图的估计构建为条件生成任务。将所提出的方法与残差神经网络（ResNet）和循环推理机（RIM）在体模和体内数据上进行了比较。结果表明，我们的方法在参数估计方面实现了更高的准确性和精度，以及更好的视觉性能。此外，我们的方法本质上结合了随机性，能够直接量化不确定性。因此，所提出的方法在定量MR成像方面具有很大的应用前景。||
|**2024-07-23**|[MonoWAD: Weather-Adaptive Diffusion Model for Robust Monocular 3D Object Detection](http://arxiv.org/abs/2407.16448)|**[link](https://github.com/visualaikhu/monowad)**|单目三维物体检测是自动驾驶中一项重要且具有挑战性的任务。现有方法主要集中于在理想天气条件下（以能见度清晰和最佳为特征的场景）进行三维检测。然而，自动驾驶的挑战在于需要能够处理天气条件的变化，例如雾天，而不仅仅是晴朗的天气。我们介绍了 MonoWAD，一种新颖的具有天气适应性扩散模型的天气鲁棒型单目三维物体检测器。它包含两个部分：(1) 天气码本，用于记忆晴朗天气的知识并为任何输入生成天气参考特征；(2) 天气适应性扩散模型，通过结合天气参考特征来增强输入特征的表示。这起到了注意力的作用，根据天气条件指示输入特征需要多少改进。为了实现这一目标，我们引入了天气适应性增强损失，以增强晴天和雾天天气条件下的特征表示。在各种天气条件下进行的大量实验表明，MonoWAD 实现了天气鲁棒的单目三维物体检测。代码和数据集已发布在 https://github.com/VisualAIKHU/MonoWAD。||
|**2024-07-23**|[FairFlow: An Automated Approach to Model-based Counterfactual Data Augmentation For NLP](http://arxiv.org/abs/2407.16431)|**[link](https://github.com/EwoeT/FairFlow)**|尽管语言模型不断发展，但它们仍然会无意间从训练数据中习得有害的社会偏见和刻板印象。这些固有偏见通常会导致在各种应用中产生不利影响。反事实数据增强 (CDA) 试图平衡训练数据中的人口统计属性，已成为自然语言处理中减轻偏见的广泛采用的方法。然而，许多现有的 CDA 方法依赖于使用手动编译的词对词典进行词语替换的技术。这些技术通常会导致脱离语境的替换，从而导致潜在的质量问题。另一方面，基于模型的技术的进步一直受到对平行训练数据需求的挑战。该领域的现有工作求助于手动生成的平行数据，这些数据收集成本高昂，因此规模有限。本文提出了 FairFlow，这是一种自动生成平行数据的方法，用于训练反事实文本生成器模型，从而限制了对人工干预的需求。此外，我们证明了 FairFlow 极大地克服了基于词典的词语替换方法的局限性，同时保持了良好的性能。||
|**2024-07-23**|[On Differentially Private 3D Medical Image Synthesis with Controllable Latent Diffusion Models](http://arxiv.org/abs/2407.16405)|**[link](https://github.com/compai-lab/2024-miccai-dgm-daum)**|通常情况下，公共医学图像数据集的小规模以及严格的隐私问题阻碍了数据驱动的深度学习模型在医学成像领域的进步。本研究解决了短轴视图下 3D 心脏 MRI 图像的这些挑战。我们提出了潜在扩散模型，该模型可以生成以医学属性为条件的合成图像，同时通过差分隐私模型训练确保患者隐私。据我们所知，这是第一个在 3D 医学图像生成中应用和量化差分隐私的工作。我们使用公共数据对模型进行预训练，然后使用差分隐私在英国生物银行数据集上进行微调。我们的实验表明，预训练可以显着提高模型性能，在 $\epsilon=10$ 时，Fr\'echet 初始距离 (FID) 达到 26.77，而未经预训练的模型为 92.52。此外，我们还探讨了隐私约束和图像质量之间的权衡，研究了更严格的隐私预算如何影响输出可控性并可能导致性能下降。我们的结果表明，在使用差分隐私进行训练时进行适当的考虑可以显着提高合成心脏 MRI 图像的质量，但在实现一致的医学真实性方面仍然存在显着挑战。||
|**2024-07-23**|[Ranking protein-protein models with large language models and graph neural networks](http://arxiv.org/abs/2407.16375)|**[link](https://github.com/haddocking/deeprank-gnn-esm)**|蛋白质-蛋白质相互作用 (PPI) 与多种疾病相关，包括癌症、感染和神经退行性疾病。获得这些 PPI 的三维结构信息是干扰这些相互作用或指导药物设计的的基础。可以使用各种策略对这些复合物进行建模，所有这些策略通常都会产生大量的模型。此过程中的一个挑战性步骤是从大量生成的模型中识别良好的模型（接近天然的 PPI 构象）。为了应对这一挑战，我们之前开发了 DeepRank-GNN-esm，这是一种基于图的深度学习算法，用于利用蛋白质语言模型的力量对建模的 PPI 结构进行排名。在这里，我们通过示例详细介绍了我们软件的使用。DeepRank-GNN-esm 可在 https://github.com/haddocking/DeepRank-GNN-esm 免费获取。||
|**2024-07-19**|[DEPICT: Diffusion-Enabled Permutation Importance for Image Classification Tasks](http://arxiv.org/abs/2407.14509)|null|我们提出了一种基于排列的图像分类器解释方法。当前的图像模型解释方法（如激活图）仅限于像素空间中基于实例的解释，难以理解全局模型行为。相比之下，基于排列的表格数据分类器解释方法通过比较模型在排列特征前后对数据的性能来衡量特征重要性。我们提出了一种基于图像的模型解释方法，该方法在数据集图像中排列可解释的概念。给定一个标有特定概念（如标题）的图像数据集，我们在文本空间中排列示例中的概念，然后通过文本条件扩散模型生成图像。然后，特征重要性通过模型性能相对于未排列数据的变化来反映。当应用于一组概念时，该方法会生成特征重要性排名。我们证明，这种方法可以在合成和现实世界的图像分类任务中恢复底层模型的特征重要性。||
|**2024-07-19**|[T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation](http://arxiv.org/abs/2407.14505)|**[link](https://github.com/KaiyueSun98/T2V-CompBench)**|文本到视频（T2V）生成模型已经取得了显著进展，但它们将不同的对象、属性、动作和运动组合到视频中的能力仍未得到探索。以前的文本到视频基准测试也忽略了评估这一重要能力。在这项工作中，我们对组合文本到视频生成进行了首次系统研究。我们提出了T2V-CompBench，这是第一个为组合文本到视频生成量身定制的基准测试。T2V-CompBench涵盖了组合性的各个方面，包括一致的属性绑定、动态属性绑定、空间关系、运动绑定、动作绑定、对象交互和生成计数。我们进一步精心设计了基于MLLM的指标、基于检测的指标和基于跟踪的指标的评估指标，这些指标可以更好地反映七个类别700个文本提示的组合文本到视频生成质量。通过与人类评估的相关性验证了所提出指标的有效性。我们还对各种文本到视频生成模型进行了基准测试，并对不同模型和不同组合类别进行了深入分析。我们发现组合文本到视频生成对于当前模型来说极具挑战性，我们希望我们的尝试能够为该方向的未来研究提供启示。||
|**2024-07-19**|[M2D2M: Multi-Motion Generation from Text with Discrete Diffusion Models](http://arxiv.org/abs/2407.14502)|null|我们介绍了多动作离散扩散模型 (M2D2M)，这是一种利用离散扩散模型的优势从多个动作的文本描述中生成人体动作的新方法。这种方法巧妙地解决了生成多动作序列的挑战，确保了动作之间的无缝过渡以及一系列动作之间的连贯性。M2D2M 的优势在于其在离散扩散模型中的动态转移概率，它根据运动标记之间的接近程度调整转移概率，鼓励不同模式之间的混合。辅之以包括独立和联合去噪步骤的两阶段采样策略，M2D2M 可以有效地生成长期、平滑且上下文相关的连贯人体运动序列，并利用针对单动作生成训练的模型。大量实验表明，M2D2M 超越了当前最先进的从文本描述生成动作的基准，展示了其在解释语言语义和生成动态逼真动作方面的功效。||
|**2024-07-19**|[Contrastive Learning with Counterfactual Explanations for Radiology Report Generation](http://arxiv.org/abs/2407.14474)|null|由于解剖学内容的共通性，放射影像及其相应的报告具有高度相似性。这种固有的数据偏差可能导致自动报告生成模型学习到纠缠不清的虚假表征，从而产生误诊报告。为了解决这些问题，我们提出了一种基于反事实解释的新型放射学报告生成框架 (CoFE)。反事实解释是一种有效的工具，通过询问“如果……会怎样”的场景来理解算法做出的决策是如何改变的。通过利用这一概念，CoFE 可以通过对比事实图像和反事实图像的表征来学习非虚假的视觉表征。具体而言，我们通过在阳性和阴性样本之间交换补丁来导出反事实图像，直到预测的诊断发生变化。这里，阳性和阴性样本是语义上最相似但具有不同诊断标签的样本。此外，CoFE 采用可学习的提示符来有效地微调预训练的大型语言模型，封装事实和反事实内容，以提供更通用的提示符表示。在两个基准数据集上的大量实验表明，利用反事实解释使 CoFE 能够生成语义连贯且事实完整的报告，并在语言生成和临床效果指标方面优于现有方法。||
|**2024-07-19**|[Co-synthesis of Histopathology Nuclei Image-Label Pairs using a Context-Conditioned Joint Diffusion Model](http://arxiv.org/abs/2407.14434)|null|在多类别组织病理学细胞核分析任务中，训练数据的缺乏成为学习型方法性能的主要瓶颈。为了应对这一挑战，以前的方法利用生成模型通过生成合成样本来增加数据。然而，现有方法往往忽视了在合成数据中考虑生物组织环境（例如形状、空间布局和组织类型）的重要性。此外，虽然生成模型在合成逼真的组织病理学图像方面表现出优越的性能，但现有方法都不能同时生成图像-标签对。在本文中，我们介绍了一种使用上下文条件联合扩散模型共同合成组织病理学细胞核图像和配对语义标签的新框架。我们建议使用具有结构相关文本提示的细胞核质心布局对扩散模型进行条件化，以将空间和结构上下文信息纳入生成目标。此外，我们通过使用与图像和语义标签同时合成的距离图生成实例级细胞核标签，增强了合成语义标签的粒度。我们证明了我们的框架在多机构、多器官和多模态数据集上生成高质量样本的有效性。我们的合成数据在下游的细胞核分割和分类任务中始终优于现有的增强方法。||
|**2024-07-19**|[Controllable and Efficient Multi-Class Pathology Nuclei Data Augmentation using Text-Conditioned Diffusion Models](http://arxiv.org/abs/2407.14426)|null|在计算病理学领域，深度学习算法在细胞核分割和分类等任务中取得了重大进展。然而，这些先进方法的潜力受到可用标记数据缺乏的限制。尽管已经积极探索通过最近的生成模型进行图像合成来应对这一挑战，但现有工作几乎没有涉及标签增强，并且主要限于单类别和无条件标签生成。在本文中，我们介绍了一种使用文本条件扩散模型进行多类别细胞核数据增强的新型两阶段框架。在第一阶段，我们通过联合扩散模型生成多类别语义标签和相应的实例图来创新细胞核标签合成，该模型由指定标签结构信息的文本提示进行条件化。在第二阶段，我们利用语义和文本条件潜在扩散模型来有效地生成与生成的细胞核标签图像一致的高质量病理图像。我们在大而多样的病理细胞核数据集上证明了我们方法的有效性，评估包括定性和定量分析，以及下游任务的评估。||
|**2024-07-19**|[Towards Assessing Data Replication in Music Generation with Music Similarity Metrics on Raw Audio](http://arxiv.org/abs/2407.14364)|**[link](https://github.com/roserbatlleroca/mira)**|近来音乐生成技术的进步引发了人们对人工智能在创造性音乐过程、当前商业模式和知识产权管理相关影响的多方面担忧。一个相关的挑战是人工智能生成的音乐中可能存在对训练集的复制和剽窃，这可能导致数据滥用和侵犯知识产权。为了解决这个问题，我们提出了音乐复制评估（MiRA）工具：一种独立于模型的开放式评估方法，该方法基于不同的音频音乐相似性指标来评估训练集的数据复制情况。我们通过在基于合成样本的不同音乐类型中进行受控复制实验，评估了五种指标识别精确复制的能力。我们的结果表明，所提出的方法可以估计精确的数据复制，其比例高于10%。通过引入MiRA工具，我们鼓励研究人员、开发人员和用户对音乐生成模型进行数据复制方面的公开评估，强调生成式人工智能在音乐领域中伦理、社会、法律和经济后果的重要性。||
|**2024-07-19**|[Stable Audio Open](http://arxiv.org/abs/2407.14358)|**[link](https://github.com/stability-ai/stable-audio-tools)**|开源生成模型对于社区至关重要，它允许进行微调并作为呈现新模型的基线。然而，目前大多数文本到音频模型都是私有的，艺术家和研究人员无法访问和构建。在这里，我们描述了使用知识共享数据训练的新型开放权重文本到音频模型的架构和训练过程。我们的评估表明，该模型的性能在各种指标上都与最先进的技术相媲美。值得注意的是，报告的 FDopenl3 结果（测量生成的逼真度）展示了其在 44.1kHz 下合成高质量立体声的潜力。||
|**2024-07-19**|[As Generative Models Improve, People Adapt Their Prompts](http://arxiv.org/abs/2407.14333)|null|我们进行了一项有1891名参与者的在线实验，收集并分析了超过18000条提示，以探索随着生成式人工智能模型能力的不断提高，提示的重要性将如何变化。我们实验中的每位参与者都被随机分配使用三种文本到图像的扩散模型中的一种：DALL-E 2、其更先进的后续模型DALL-E 3，或带有自动提示修改功能的DALL-E 3版本。然后，参与者被要求编写提示，在连续10次尝试中尽可能接近地再现目标图像。我们发现，使用DALL-E 3的参与者比使用DALL-E 2的参与者表现更好。这种性能差距对应于参与者图像与其目标图像的相似性存在显著差异，并且其原因在于：(1) DALL-E 3的技术能力增强，以及(2) 参与者为应对这些增强能力而进行的内生性提示变化。更具体地说，尽管参与者对他们被分配到的模型毫不知情，但分配到DALL-E 3的参与者编写的提示更长，彼此之间在语义上更相似，并且包含更多的描述性词语。此外，虽然分配到带有提示修改功能的DALL-E 3的参与者仍然优于分配到DALL-E 2的参与者，但自动提示修改功能将使用DALL-E 3带来的优势降低了58%。综上所述，我们的结果表明，随着模型的不断进步，人们将继续调整他们的提示，以利用新模型的能力。||
|**2024-07-19**|[Panoptic Segmentation of Mammograms with Text-To-Image Diffusion Model](http://arxiv.org/abs/2407.14326)|null|乳腺X线摄影对于乳腺癌监测和早期诊断至关重要。然而，分析乳腺X线照片对于放射科医生来说是一项艰巨的任务，他们通常每天要查看数百张乳腺X线照片，导致过度诊断和过度治疗。计算机辅助诊断 (CAD) 系统已被开发用于协助这一过程，但它们的能力，特别是在病灶分割方面，仍然有限。随着深度学习的最新进展，它们的性能可能会得到改善。最近，视觉语言扩散模型出现，在图像生成和迁移到各种下游任务方面表现出色。我们的目标是利用它们的能力在全景环境中进行乳腺病灶分割，其中包括语义和实例级预测。具体来说，我们建议利用来自稳定扩散模型的预训练特征作为最先进的全景分割架构的输入，从而实现对单个乳腺病灶的准确描绘。为了弥合自然图像和医学图像领域之间的差距，我们在该框架中纳入了乳腺X线摄影专用 MAM-E 扩散模型以及 BiomedCLIP 图像和文本编码器。我们在两个最近发布的乳腺X线摄影数据集 CDD-CESM 和 VinDr-Mammo 上评估了我们的方法。对于实例分割任务，我们注意到 40.25 AP0.1 和 46.82 AP0.05，以及 25.44 PQ0.1 和 26.92 PQ0.05。对于语义分割任务，我们分别获得了 38.86 和 40.92 的 Dice 分数。||
|**2024-07-18**|[LogoSticker: Inserting Logos into Diffusion Models for Customized Generation](http://arxiv.org/abs/2407.13752)|null|近年来，文本到图像模型定制的进步凸显了将新概念与少量示例相结合的重要性。然而，这些进展主要局限于广泛认可的主题，模型可以通过充分的共享先验知识相对容易地学习这些主题。相比之下，以独特图案和文本元素为特征的标识很难在扩散模型中建立共享知识，因此提出了独特的挑战。为了弥合这一差距，我们引入了标识插入任务。我们的目标是将标识特征插入到扩散模型中，并使其能够在不同的上下文中无缝合成。我们提出了一种新颖的两阶段流水线 LogoSticker 来解决此任务。首先，我们提出了actor-critic关系预训练算法，该算法解决了模型对标识潜在空间定位和与其他对象交互理解上的重大差距。其次，我们提出了一种解耦的标识学习算法，该算法能够对标识进行精确定位和特征提取。LogoSticker 可以在不同的上下文中准确、和谐地生成标识。我们全面验证了 LogoSticker 相对于定制方法和大模型（如 DALLE~3）的有效性。（项目页面：\href{https://mingkangz.github.io/logosticker}{https://mingkangz.github.io/logosticker}）。||
|**2024-07-18**|[Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review](http://arxiv.org/abs/2407.13734)|**[link](https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq)**|本教程全面概述了微调扩散模型以优化下游奖励函数的方法。虽然众所周知，扩散模型具有出色的生成建模能力，但在生物学等领域的实际应用需要生成能够最大化某些期望指标的样本（例如，RNA 中的翻译效率、分子中的对接分数、蛋白质中的稳定性）。在这些情况下，可以优化扩散模型，使其不仅生成逼真的样本，还能明确地最大化感兴趣的指标。此类方法基于强化学习 (RL) 的概念。我们解释了各种 RL 算法的应用，包括 PPO、可微分优化、奖励加权 MLE、值加权采样和路径一致性学习，这些算法专门针对微调扩散模型而设计。我们旨在探讨基本方面，例如不同基于 RL 的微调算法在各种场景下的优缺点、基于 RL 的微调与非基于 RL 的方法相比的优势，以及基于 RL 的微调的形式目标（目标分布）。此外，我们还将考察它们与相关主题的联系，例如分类器指导、Gflownets、基于流的扩散模型、路径积分控制理论以及从非规范化分布（如 MCMC）中采样。本教程的代码可在以下网址获取：https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq||
|**2024-07-18**|[PASTA: Controllable Part-Aware Shape Generation with Autoregressive Transformers](http://arxiv.org/abs/2407.13677)|null|对能够自动生成高质量、多样化3D对象的深度生成模型的需求不断增长，推动了3D内容创建过程自动化工具的巨大进步。在本文中，我们提出了PASTA，一种用于生成高质量3D形状的自回归Transformer架构。PASTA包含两个主要组件：一个将对象生成为长方体图元序列的自回归Transformer，以及一个使用Transformer解码器实现的混合网络，该解码器组合长方体序列并为每个对象合成高质量的网格。我们的模型分两个阶段进行训练：首先，我们仅使用带注释的长方体部分作为监督来训练自回归生成模型；接下来，我们使用显式3D监督（以水密网格的形式）来训练混合网络。对各种ShapeNet对象的评估展示了我们的模型能够根据不同的输入（例如，从头开始、从部分对象、从文本和图像）以及尺寸引导生成（通过对定义对象边界的边界框进行显式条件化）来执行形状生成的能力。此外，由于我们的模型考虑了3D对象的底层基于零件的结构，因此我们能够选择特定的零件并生成具有该零件有意义变化的形状。正如我们的实验所证明的那样，我们的模型生成的3D形状比现有的基于零件和非基于零件的方法更真实、更多样化，同时更易于实现和训练。||
|**2024-07-18**|[MeshSegmenter: Zero-Shot Mesh Semantic Segmentation via Texture Synthesis](http://arxiv.org/abs/2407.13675)|**[link](https://github.com/zimingzhong/MeshSegmenter)**|我们提出了MeshSegmenter，这是一个简单但有效的框架，专为零样本3D语义分割而设计。该模型成功地将2D分割模型的强大功能扩展到3D网格，可在不同的网格和分割描述中提供准确的3D分割。具体来说，我们的模型利用Segment Anything Model (SAM) 模型从3D形状渲染的图像中分割目标区域。鉴于纹理对分割的重要性，我们还利用预训练的稳定扩散模型从3D形状生成具有纹理的图像，并利用SAM从具有纹理的图像中分割目标区域。纹理补充了形状的分割，即使在几何上不突出的区域也能实现准确的3D分割，例如在汽车网格内分割车门。为了实现3D分割，我们从不同的视图渲染2D图像，并对纹理和非纹理图像进行分割。最后，我们开发了一种多视图重新投票方案，将来自不同视图的2D分割结果和置信度分数整合到3D网格上，确保3D分割结果的一致性，并消除特定视角下的不准确性。通过这些创新，MeshSegmenter在数量和质量上都提供了稳定可靠的3D分割结果，突出了其作为3D零样本分割领域变革性工具的潜力。代码可在\url{https://github.com/zimingzhong/MeshSegmenter}获取。||
|**2024-07-18**|[Open-Vocabulary 3D Semantic Segmentation with Text-to-Image Diffusion Models](http://arxiv.org/abs/2407.13642)|null|本文研究了使用在大规模图像-字幕对上预训练的扩散模型进行开放词汇3D语义理解。我们提出了一种名为Diff2Scene的新方法，它利用来自文本-图像生成模型的冻结表示以及显著性感知和几何感知掩码，用于开放词汇3D语义分割和视觉定位任务。Diff2Scene摆脱了任何标记的3D数据，并有效地识别3D场景中的物体、外观、材料、位置及其组成。我们证明，它优于竞争基线，并在很大程度上优于最先进的方法。特别是，Diff2Scene在ScanNet200上的性能比最先进的方法提高了12%。||
|**2024-07-18**|[Training-free Composite Scene Generation for Layout-to-Image Synthesis](http://arxiv.org/abs/2407.13609)|**[link](https://github.com/Papple-F/csg)**|近年来，文本到图像扩散模型的突破性进展极大地促进了从文本描述生成高保真、照片级真实图像的能力。然而，这些模型在从文本中解释空间排列方面往往存在困难，阻碍了它们生成具有精确空间配置图像的能力。为了弥合这一差距，布局到图像生成已成为一个有前途的方向。然而，基于训练的方法受到需要大量注释数据集的限制，导致数据采集成本高昂且概念范围受限。相反，免训练方法在复杂组合中准确定位和生成语义相似的对象方面面临挑战。本文介绍了一种新颖的免训练方法，旨在克服扩散调节阶段的对抗性语义交叉问题。通过选择性采样改进内部标记损失并通过注意力重新分配增强扩散过程，我们提出了两个创新约束：1）解决标记冲突以确保准确概念合成的标记间约束；2）改进像素间关系的自注意力约束。我们的评估证实了利用布局信息来指导扩散过程的有效性，可以生成内容更丰富、保真度和复杂性更高的图像。代码可在https://github.com/Papple-F/csg.git获取。||
|**2024-07-18**|[All Roads Lead to Rome? Exploring Representational Similarities Between Latent Spaces of Generative Image Models](http://arxiv.org/abs/2407.13449)|**[link](https://github.com/charumathib/thesis-latent-spaces)**|不同的生成图像模型是否会秘密地学习类似的底层表示？我们通过测量四种不同模型的潜在空间相似性来研究这个问题：变分自编码器 (VAEs)、生成对抗网络 (GANs)、归一化流 (NFs) 和扩散模型 (DMs)。我们的方法包括在冻结的潜在空间之间训练线性映射，以“缝合”任意编码器和解码器对，并在由此产生的“缝合”模型上测量基于输出和基于探针的指标。我们的主要发现是，即使潜在空间大小不同，性能良好的模型之间的潜在空间线性映射也能保留大多数视觉信息；对于 CelebA 模型，性别是最相似的可探测属性。最后，我们在一个 NF 上展示了潜在空间表示在训练早期就收敛了。||
|**2024-07-18**|[Movement-based models for abundance data](http://arxiv.org/abs/2407.13384)|null|我们开发了两种基于潜在连续运动模型的时空丰度数据统计模型。与当前统计生态学文献中的其他丰度模型不同，我们的模型特别关注个体运动与计数之间的明确联系，以及由此产生的时空自相关性。我们的第一个模型（快照）描述了具有假阴性检测误差的自由个体计数。我们的第二个模型（捕获）描述了移动个体在陷阱中的捕获和保留，它是使用公理化方法构建的，建立了三个简单原则，从中推导出捕获时间的密度是第二类沃尔泰拉积分方程的解。我们明确了由此生成的丰度场的时空均值和协方差结构，并为这两种模型开发了模拟方法。时空计数的联合分布是新多元分布的一个实例，这里将其称为演化类别多项分布，我们为此建立了一些关键属性。由于似然的一般表达式仍然难以处理，我们建议通过用多元高斯分布替换它来近似 MLE 拟合方法，这由中心极限定理证明是合理的，并且尊重均值和协方差结构。我们将此方法应用于果蝇在草地上释放并在陷阱阵列中反复捕获和计数的实验数据。我们估计扩散和平流参数，将我们的模型与生态扩散模型进行比较，并进行模拟研究以验证我们的分析。渐近一致性已通过实验验证。我们得出的结论是，我们可以仅使用丰度数据来估计运动参数，但必须了解避免低估扩散参数的必要条件。||
|**2024-07-18**|[A deep latent variable model for semi-supervised multi-unit soft sensing in industrial processes](http://arxiv.org/abs/2407.13310)|null|在许多工业过程中，明显缺乏数据限制了数据驱动软测量的发展。然而，通常有机会通过提高数据效率来学习更强大的模型。为了实现这一点，可以利用关于学习软测量数据的知识。利用工业数据经常具有的特性，我们引入了一种用于半监督多单元软测量的深度潜在变量模型。这种分层的生成模型能够联合建模不同的单元，并同时从标记和未标记数据中学习。使用两个数据集对多单元软测量进行了实证研究：一个单相流体流动的合成数据集和一个大型的油气井多相流动的真实数据集。我们表明，通过结合半监督学习和多任务学习，所提出的模型取得了优异的结果，优于当前解决此软测量问题的领先方法。我们还表明，当一个模型在多单元数据集上训练后，可以使用少量数据点对其进行微调，以适应以前未见过的单元。在这种微调过程中，未标记的数据提高了软测量的性能；值得注意的是，即使在没有标记数据的情况下也是如此。||
|**2024-07-18**|[URCDM: Ultra-Resolution Image Synthesis in Histopathology](http://arxiv.org/abs/2407.13277)|null|从组织病理学数据中诊断疾病需要对全切片图像 (WSI) 的各种分辨率进行全面分析。然而，由于专注于高保真图像块，现有的生成方法无法始终如一地表示 WSI 的层次结构。为了解决这个问题，我们提出了超分辨率级联扩散模型 (URCDM)，它能够以高分辨率合成整个组织病理学图像，同时真实地捕捉所有放大倍率下潜在解剖结构和病理学的细节。我们在三个独立的数据集（分别包含大脑、乳腺和肾脏组织）上评估了我们的方法，并超越了现有的最先进的多分辨率模型。此外，还进行了一项专家评估研究，结果表明，URCDM 在各种分辨率下始终如一地生成训练有素的评估者无法区分真伪的输出。所有代码和更多示例都可以在 GitHub 上找到。||
|**2024-07-16**|[Efficient Training with Denoised Neural Weights](http://arxiv.org/abs/2407.11966)|null|良好的权重初始化是降低深度神经网络 (DNN) 模型训练成本的有效措施。然而，如何初始化参数的选择具有挑战性，可能需要手动调整，这既耗时又容易出现人为错误。为了克服这些限制，这项工作朝着构建权重生成器来合成用于初始化的神经网络权重迈出了新的一步。我们以使用生成对抗网络 (GAN) 的图像到图像转换任务为例，因为它易于收集涵盖广泛范围的模型权重。具体来说，我们首先收集一个包含各种图像编辑概念及其相应训练权重的数据集，这些权重稍后将用于训练权重生成器。为了解决层间不同的特性和要预测的大量权重问题，我们将权重分成大小相等的块，并为每个块分配一个索引。随后，使用扩散模型在该数据集上进行训练，同时使用概念的文本条件和块索引。通过使用我们的扩散模型预测的去噪权重初始化图像转换模型，训练仅需 43.3 秒。与从头开始训练（即 Pix2pix）相比，我们在新概念上实现了 15 倍的训练时间加速，同时获得了更好的图像生成质量。||
|**2024-07-16**|[Context-Guided Diffusion for Out-of-Distribution Molecular and Protein Design](http://arxiv.org/abs/2407.11942)|**[link](https://github.com/leojklarner/context-guided-diffusion)**|生成模型具有加速新型分子疗法和材料发现关键步骤的潜力。扩散模型最近成为一种强大的方法，在无条件样本生成方面表现出色，并且在数据驱动引导下，在其训练领域内进行条件生成也表现出色。 然而，可靠地从训练数据之外的高价值区域进行采样仍然是一个开放的挑战——目前的方法主要集中在修改扩散过程本身。在本文中，我们开发了上下文引导扩散 (CGD)，这是一种简单的即插即用方法，它利用未标记数据和平滑约束来改进引导扩散模型的分布外泛化能力。我们证明，这种方法可以在各种设置中带来实质性的性能提升，包括连续、离散和图结构的扩散过程，并在药物发现、材料科学和蛋白质设计中都有应用。||
|**2024-07-16**|[Data-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model Co-development](http://arxiv.org/abs/2407.11784)|**[link](https://github.com/modelscope/data-juicer)**|大规模多模态生成模型的出现极大地促进了人工智能的发展，带来了前所未有的性能和功能水平。然而，由于以模型为中心和以数据为中心的发展路径在历史上是孤立的，导致结果欠佳和资源利用效率低下，优化这些模型仍然具有挑战性。为了解决这个问题，我们提出了一套全新的沙盒套件，专门用于集成的数据模型协同开发。这个沙盒提供了一个全面的实验平台，能够快速迭代和洞察力驱动的模型和数据改进。我们提出的“探测-分析-改进”工作流程，通过在最先进的类似LLaVA和基于DiT的模型上的应用验证，实现了显著的性能提升，例如登顶VBench排行榜。我们还从详尽的基准测试中获得了富有成效的见解，揭示了数据质量、多样性和模型行为之间至关重要的相互作用。为了促进对多模态数据和生成模型的更深入理解和未来发展，我们的代码、数据集和模型都维护在https://github.com/modelscope/data-juicer/blob/main/docs/Sandbox.md，并且可以访问。||
|**2024-07-16**|[Diffusion-driven self-assembly of emerin nanodomains at the nuclear envelope](http://arxiv.org/abs/2407.11758)|null|Emerin是一种核膜蛋白，在机械传导和核形状适应中起着重要的生物学作用，它在内核膜上自组装成纳米级的结构域。这些纳米域的大小和emerin占据率会随着施加的机械应力和与Emery-Dreifuss肌营养不良症(EDMD)相关的emerin突变而改变。通过理论和实验的结合，我们在这里表明，一个简单的反应扩散模型可以解释emerin纳米域的自组装。我们的模型与野生型emerin和EDMD相关emerin突变（无论是否存在外力）的emerin纳米域的大小和占据率的实验观察结果在数量上一致，并且允许从对emerin纳米域整体性质的观察结果成功预测emerin扩散系数。我们的研究结果从emerin及其核结合伙伴的关键反应和扩散特性的变化方面，为EDMD相关的emerin组织缺陷提供了物理解释。||
|**2024-07-16**|[Generating Multi-Modal and Multi-Attribute Single-Cell Counts with CFGen](http://arxiv.org/abs/2407.11734)|**[link](https://github.com/theislab/CFGen)**|单细胞RNA测序数据的生成模型在社区驱动的任务中显示出巨大的潜力，例如轨迹推断、批次效应去除和基因表达生成。然而，大多数最近从噪声生成合成单细胞的深度模型都是在预处理的连续基因表达近似值上运行的，忽略了单细胞数据固有的离散性和过度分散性，这限制了下游应用并阻碍了鲁棒噪声模型的结合。此外，基于深度学习的合成单细胞生成的关键方面仍未得到充分探索，例如可控的多模态和多标签生成及其在下游任务性能增强中的作用。这项工作提出了用于生成的细胞流（CFGen），这是一种基于流的用于多模态单细胞计数的条件生成模型，它明确考虑了数据的离散性。我们的结果表明，在考虑新的生成任务（例如以多个属性为条件和通过数据增强提高稀有细胞类型分类）的同时，可以改进关键生物学数据特征的恢复。通过在一组不同的生物数据集和设置中展示CFGen，我们提供了其对计算生物学和深度生成模型领域价值的证据。||
|**2024-07-16**|[Theoretical Insights into CycleGAN: Analyzing Approximation and Estimation Errors in Unpaired Data Generation](http://arxiv.org/abs/2407.11678)|null|本文重点分析了名为 CycleGAN 的非配对数据生成模型的过度风险。与经典的 GAN 不同，CycleGAN 不仅可以在两个非配对分布之间转换数据，还可以确保映射的一致性，这是由 CycleGAN 独有的循环一致性项所鼓励的。CycleGAN 中模型结构日益复杂以及循环一致性项的加入给误差分析带来了新的挑战。通过考虑模型架构和训练过程的影响，将风险分解为两个方面：逼近误差和估计误差。这两个误差项分别进行分析，并最终通过考虑它们之间的权衡来组合。每个部分都经过严格分析；通过构建最优传输映射的逼近来逼近误差，并通过使用 Rademacher 复杂度建立上限来估计误差。我们的分析不仅隔离了这些误差，还探讨了它们之间的权衡，这为 CycleGAN 的架构和训练过程如何影响其性能提供了理论见解。||
|**2024-07-16**|[Mask-guided cross-image attention for zero-shot in-silico histopathologic image generation with a diffusion model](http://arxiv.org/abs/2407.11664)|null|利用生成式人工智能创建虚拟数据有望成为计算病理学中对整个切片图像进行染色、成像和标注的一种经济高效的替代方案。扩散模型是生成虚拟图像的最先进解决方案，可提供无与伦比的保真度和真实感。使用外观迁移扩散模型可以实现零样本图像生成，从而促进快速应用并无需模型训练。然而，当前的外观迁移扩散模型是为自然图像设计的，其主要任务是将前景物体从源域迁移到目标域，而背景的重要性微不足道。然而，在计算病理学中，特别是在肿瘤学中，直接定义图像中的哪些物体应该被归类为前景和背景并不容易，因为图像中的所有物体都可能对详细了解肿瘤微环境至关重要。我们通过修改外观迁移引导，使用现有的分割掩码在特定类别 AdaIN 特征统计匹配之间交替，从而提高了外观迁移扩散模型对免疫组化染色图像的适用性。所提出的方法的性能在监督上皮分割的下游任务中得到了证明，结果表明，模型训练所需的标注数量可以减少 75%，优于基线方法。此外，我们咨询了 certified 病理学家，以探讨未来的改进方向。我们预计这项工作将激励零样本扩散模型在计算病理学中的应用，为生成具有无与伦比的保真度和真实感的虚拟图像提供一种有效的方法，这些图像对下游任务（如训练现有深度学习模型或微调基础模型）具有重要意义。||
|**2024-07-16**|[Magnetogram-to-Magnetogram: Generative Forecasting of Solar Evolution](http://arxiv.org/abs/2407.11659)|**[link](https://github.com/fpramunno/MAG2MAG)**|研究太阳磁场对于理解太阳内部的物理过程及其对行星际环境的影响至关重要。我们引入了一种新方法，使用基于去噪扩散概率模型 (DDPM) 的图像到图像转换技术来预测太阳视线 (LoS) 磁图的演变。我们的方法结合了图像质量的“计算机科学指标”和物理准确性的“物理指标”来评估模型性能。结果表明，DDPM 在保持太阳磁场的结构完整性、动态范围、磁通量和其他物理特征（如活动区域的大小）方面非常有效，甚至在耀斑情况下也优于传统的持续性模型。我们的目标是利用深度学习不仅用于可视化，而且将其作为望远镜的集成和交互式工具，增强我们对太阳耀斑等意外物理事件的理解。未来的研究将致力于整合更多样化的太阳数据，以提高我们生成模型的准确性和适用性。||
|**2024-07-16**|[CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging](http://arxiv.org/abs/2407.11652)|null|联邦学习 (FL) 提供了一种保护隐私的训练分散数据模型的方法。它在医疗保健领域具有巨大的潜力，但由于医疗图像数据在客户端之间的差异，以及注释有限，因此面临着挑战。本文介绍了跨客户端差异自适应联邦学习 (CCVA-FL) 来解决这些问题。CCVA-FL 旨在通过将图像转换到共同特征空间来最小化客户端之间的差异。它涉及专家对每个客户端的图像子集进行注释，然后选择数据复杂度最低的客户端作为目标客户端。然后，使用基于目标客户端注释图像的可扩展扩散模型和 Transformer (DiT) 生成合成医学图像。这些合成图像捕捉了多样性并代表了原始数据，并与其他客户端共享。然后，每个客户端使用图像到图像的转换将其本地图像转换到目标图像空间。转换后的图像随后用于联邦学习设置中，以开发服务器模型。我们的结果表明，CCVA-FL 通过有效解决客户端之间的数据分布差异，在不损害隐私的情况下，优于传统的联邦平均算法。||
|**2024-07-16**|[Scaling Diffusion Transformers to 16 Billion Parameters](http://arxiv.org/abs/2407.11633)|**[link](https://github.com/feizc/dit-moe)**|本文介绍了DiT-MoE，这是一种稀疏版本的扩散Transformer，它在保持高度优化的推理能力的同时，在规模和竞争力上都与密集网络相当。DiT-MoE包含两种简单的设计：共享专家路由和专家级平衡损失，从而捕获共同知识并减少不同路由专家之间的冗余。当应用于条件图像生成时，对专家专业化的深入分析获得了一些有趣的观察结果：（i）专家选择表现出对空间位置和去噪时间步长的偏好，而对不同的类别条件信息不敏感；（ii）随着MoE层越来越深，专家的选择逐渐从特定的空间位置转变为分散和平衡；（iii）专家专业化倾向于集中在早期时间步长，然后在经过一半时间后逐渐均匀。我们将其归因于扩散过程，该过程首先对低频空间信息进行建模，然后对高频复杂信息进行建模。基于以上指导，一系列DiT-MoE实验实现了与密集网络相当的性能，但在推理过程中需要的计算量要少得多。更令人鼓舞的是，我们展示了DiT-MoE在合成图像数据方面的潜力，将扩散模型扩展到16.5B参数，在512×512分辨率设置下获得了新的SoTA FID-50K分数1.80。项目页面：https://github.com/feizc/DiT-MoE。||
|**2024-07-12**|[Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text](http://arxiv.org/abs/2407.09364)|null|大型语言模型发展的显著进步模糊了人类和人工智能生成文本之间的界限。人工智能生成文本的日益普及及其检测难度给我们的社会带来了新的挑战。在本文中，我们提出了 WhosAI 来解决检测和归因人工智能生成文本的问题，这是一个三元组网络对比学习框架，旨在预测给定输入文本是由人类还是人工智能生成的，并揭示文本的作者。与大多数现有方法不同，我们提出的框架旨在从多个生成器同时学习语义相似性表示，从而平等地处理检测和归因任务。此外，WhosAI 与模型无关，并且可以通过将新的人工智能文本生成模型生成的实例合并到我们框架学习的嵌入空间中来扩展到新模型的发布。在包含 20 万篇新闻文章的 TuringBench 基准测试上的实验结果表明，我们提出的框架在图灵测试和作者归因任务中均取得了优异的结果，优于 TuringBench 基准测试排行榜中列出的所有方法。||
|**2024-07-12**|[Any-Property-Conditional Molecule Generation with Self-Criticism using Spanning Trees](http://arxiv.org/abs/2407.09357)|**[link](https://github.com/samsungsailmontreal/anymolgencritic)**|生成新分子极具挑战性，大多数表示方法会导致生成模型产生许多无效分子。基于生成树的图生成 (STGG) 是一种很有前景的方法，可以确保生成有效分子，在无条件生成方面优于最先进的 SMILES 和图扩散模型。在现实世界中，我们希望能够根据一种或多种所需属性（而不是无条件地）生成分子。因此，在这项工作中，我们将 STGG 扩展到多属性条件生成。我们的方法 STGG+ 结合了现代 Transformer 架构、训练期间属性的随机掩蔽（能够以任何属性子集和无分类器引导为条件）、辅助属性预测损失（允许模型对分子进行自我批评并选择最佳分子）和其他改进。我们证明了 STGG+ 在分布内和分布外条件生成以及奖励最大化方面均达到了最先进的性能。||
|**2024-07-12**|[PID: Physics-Informed Diffusion Model for Infrared Image Generation](http://arxiv.org/abs/2407.09299)|**[link](https://github.com/fangyuanmao/pid)**|红外成像技术因其在低能见度条件下的可靠传感能力而受到广泛关注，促使许多研究致力于将丰富的RGB图像转换为红外图像。然而，大多数现有的图像转换方法将红外图像视为一种风格变化，而忽略了潜在的物理规律，这限制了它们的实际应用。为了解决这些问题，我们提出了一种物理信息扩散（PID）模型，用于将RGB图像转换为符合物理规律的红外图像。我们的方法利用了扩散模型的迭代优化，并在训练过程中结合了基于红外规律先验知识的强物理约束。这种方法增强了转换后的红外图像与真实红外域之间的相似性，而无需增加额外的训练参数。实验结果表明，PID明显优于现有的最先进方法。我们的代码可在https://github.com/fangyuanmao/PID获取。||
|**2024-07-12**|[Learning Distances from Data with Normalizing Flows and Score Matching](http://arxiv.org/abs/2407.09297)|null|基于密度的距离 (DBD) 为度量学习问题提供了一种优雅的解决方案。通过定义一个黎曼度量，该度量随着概率密度的降低而增加，最短路径自然遵循数据流形，并且点根据数据的模态进行聚类。我们发现，现有的估计费马距离（一种特定选择的 DBD）的方法，由于 i) 不准确的密度估计和 ii) 依赖于在高维情况下越来越粗糙的基于图的路径，因此在低维和高维情况下都存在收敛性差的问题。为了解决这些问题，我们建议使用归一化流（一种具有易处理密度估计的生成模型）来学习密度，并采用从基于图的提议初始化的评分模型来使用平滑松弛方法。此外，我们引入了一种维度自适应的费马距离，它在扩展到高维时表现出更直观的特性，并提供更好的数值特性。我们的工作为实际应用基于密度的距离铺平了道路，特别是在高维空间中。||
|**2024-07-12**|[Surgical Text-to-Image Generation](http://arxiv.org/abs/2407.09230)|null|获取用于研究和开发的手术数据受到高昂标注成本以及实践和伦理限制的严重阻碍。利用合成生成的图像可以提供一种有价值的替代方案。在这项工作中，我们利用 CholecT50 数据集对文本到图像生成模型在手术领域的适应性进行了深入分析，该数据集提供了用手术动作三元组（器械、动词、目标）标注的手术图像。我们研究了各种语言模型，发现 T5 在基于三元组的文本输入区分手术动作方面提供了更独特的特征。我们的分析表明，长标题和基于三元组的标题之间存在高度一致性，支持使用基于三元组的标签。我们通过发现三元组文本嵌入在潜在空间中以器械为中心，并设计了一种基于器械的类别平衡技术来抵消手术数据中的不平衡和偏差，从而解决了在没有额外输入信号的情况下训练基于三元组标题的文本到图像模型的挑战，改进了训练的收敛性。我们扩展了基于扩散的生成模型 Imagen，开发了 Surgical Imagen，用于从基于三元组的文本提示生成逼真且与活动一致的手术图像。我们使用多种指标评估我们的模型，包括人类专家调查和 FID 和 CLIP 分数等自动化方法。我们从质量、一致性、推理、知识和鲁棒性等关键方面评估模型性能，证明了我们的方法在提供真实数据收集的现实替代方案方面的有效性。||
|**2024-07-12**|[Salt & Pepper Heatmaps: Diffusion-informed Landmark Detection Strategy](http://arxiv.org/abs/2407.09192)|null|解剖标志检测是指识别图像中用于临床测量的关键区域的过程。每个标志都是由临床医生标记的单个真实点。机器学习模型将标志的位置预测为由热图表示的概率区域。扩散模型由于其高质量的采样和模式覆盖范围，在生成建模中越来越受欢迎，导致其在医学图像处理中用于语义分割。扩散模型可以进一步调整以学习标志的分布。扩散模型的随机性捕获了标志预测中的波动，我们通过将其模糊成有意义的概率区域来利用这一点。在本文中，我们将自动解剖标志检测重新表述为一个精确的生成建模任务，生成一个少量热像素热图。我们的方法实现了最先进的MRE性能，并且SDR性能与现有工作相当。||
|**2024-07-12**|[Variational Inference via Smoothed Particle Hydrodynamics](http://arxiv.org/abs/2407.09186)|null|本文提出了一种基于光滑粒子流体动力学 (SPH) 的新型变分推理方法 SPH-ParVI，用于对部分已知密度（例如，直到常数）进行采样或使用梯度进行采样。SPH-ParVI 模拟了在目标密度驱动的外部效应下流体的流动；流体的瞬态或稳态近似于目标密度。连续流体通过 SPH 建模为相互作用的粒子系统 (IPS)，其中每个粒子都携带平滑的属性，并根据纳维-斯托克斯方程进行交互和演化。这种无网格的拉格朗日模拟方法为一类概率模型（例如在贝叶斯推理和生成建模中遇到的模型）提供了快速、灵活、可扩展和确定性的采样和推理。||
|**2024-07-12**|[Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors](http://arxiv.org/abs/2407.09136)|**[link](https://github.com/eth-lre/verify-then-generate)**|大型语言模型 (LLM) 为向所有人提供高质量的个性化教育提供了机会。一种有希望实现这一目标的方法是构建能够支持学生解决问题的对话辅导模型。然而，尽管现有的 LLM 在解决推理问题方面表现出色，但它们难以准确地检测学生的错误并根据这些错误调整反馈。受现实世界教学实践的启发，教师会识别学生的错误并根据错误定制他们的回应，我们专注于验证学生的解决方案，并展示了这种验证的基础如何提高导师回应生成的整体质量。我们收集了一个包含 1000 个逐步数学推理链的数据集，其中第一步错误由教师注释。我们通过实证表明，为当前模型找到学生解决方案中的错误具有挑战性。我们提出并评估了几种用于检测这些错误的验证器。通过自动评估和人工评估，我们表明，与现有基线相比，学生解决方案验证器引导生成模型针对学生错误生成更有针对性的回应，这些回应通常更正确，并且 hallucinations 更少。||
|**2024-07-12**|[Overcoming Catastrophic Forgetting in Tabular Data Classification: A Pseudorehearsal-based approach](http://arxiv.org/abs/2407.09039)|null|持续学习 (CL) 提出了一个重要挑战，即如何在不忘记先前获得的知识的情况下，适应不断变化的数据分布，同时整合新知识。在本文中，我们介绍了一种新的方法，称为基于表格数据排练的增量终身学习框架 (TRIL3)，旨在解决表格数据分类问题中的灾难性遗忘现象。TRIL3 使用基于原型的增量生成模型 XuILVQ 生成合成数据来保留旧知识，并使用 DNDF 算法（该算法已修改为以增量方式运行）来学习表格数据的分类任务，而无需存储旧样本。在经过不同的测试以获得足够的合成数据百分比并将 TRIL3 与其他可用的 CL 方案进行比较后，我们可以得出结论，TRIL3 的性能优于文献中的其他方案，仅使用 50% 的合成数据。||
|**2024-07-12**|[Aligning Diffusion Behaviors with Q-functions for Efficient Continuous Control](http://arxiv.org/abs/2407.09024)|null|基于语言模型对齐的最新进展，我们将离线强化学习制定为一个两阶段优化问题：首先在无奖励行为数据集上预训练表达能力强的生成策略，然后微调这些策略以与特定任务的标注（如 Q 值）对齐。这种策略允许我们利用丰富多样的行为数据来增强泛化能力，并使用最少的标注快速适应下游任务。特别地，我们引入了高效扩散对齐（EDA）来解决连续控制问题。EDA 利用扩散模型进行行为建模。然而，与之前的方法不同，我们将扩散策略表示为标量神经网络相对于动作输入的导数。这种表示至关重要，因为它可以直接计算扩散模型的密度，使其与现有的 LLM 对齐理论兼容。在策略微调期间，我们扩展了基于偏好的对齐方法（如直接偏好优化（DPO））以将扩散行为与连续 Q 函数对齐。我们在 D4RL 基准测试上的评估表明，EDA 在整体性能上优于所有基线方法。值得注意的是，EDA 在仅使用 1% 的 Q 值标记数据进行微调的情况下仍能保持约 95% 的性能，并且仍然优于其他几个基线方法。||
|**2024-07-11**|[Video Diffusion Alignment via Reward Gradients](http://arxiv.org/abs/2407.08737)|**[link](https://github.com/mihirp1998/vader)**|我们在构建基础视频扩散模型方面取得了重大进展。由于这些模型使用大规模无监督数据进行训练，因此使这些模型适应特定的下游任务变得至关重要。通过监督微调来调整这些模型需要收集视频的目标数据集，这既具有挑战性又乏味。在这项工作中，我们利用预先训练的奖励模型来调整视频扩散模型，这些奖励模型是通过基于强大的视觉判别模型的偏好学习的。这些模型包含关于生成的 RGB 像素的密集梯度信息，这对于在复杂搜索空间（例如视频）中的高效学习至关重要。我们表明，将这些奖励模型的梯度反向传播到视频扩散模型可以实现视频扩散模型在计算和样本方面的有效对齐。我们展示了各种奖励模型和视频扩散模型的结果，证明了我们的方法在奖励查询和计算方面比先前的无梯度方法效率更高。我们的代码、模型权重和更多可视化内容可在 https://vader-vid.github.io 获取。||
|**2024-07-11**|[Live2Diff: Live Stream Translation via Uni-directional Attention in Video Diffusion Models](http://arxiv.org/abs/2407.08701)|null|大型语言模型由于其时间上的单向注意机制（该机制对当前标记和先前标记之间的相关性进行建模），在生成文本和音频等流数据方面表现出了显著的效果。然而，尽管对实时视频处理的需求日益增长，但视频流的研究仍然远远落后。最先进的视频扩散模型利用双向时间注意力来建模当前帧和所有周围（即包括未来）帧之间的相关性，这阻碍了它们处理流视频。为了解决这个问题，我们提出了 Live2Diff，这是首次尝试设计具有单向时间注意力的视频扩散模型，专门针对实时流视频翻译。与以前的工作相比，我们的方法通过将当前帧与其前一帧和一些初始预热帧相关联来确保时间一致性和平滑度，而无需任何未来帧。此外，我们使用了一种高效的去噪方案，该方案采用 KV 缓存机制和流水线技术，以促进以交互式帧率进行流视频翻译。大量实验表明，所提出的注意力机制和流水线的有效性，在时间平滑度和/或效率方面优于以前的方法。||
|**2024-07-11**|[Scattering transforms on the sphere, application to large scale structure modelling](http://arxiv.org/abs/2407.08687)|null|散射变换是最近为研究高度非高斯过程而开发的一种新型汇总统计数据，已被证明在天文物理研究中非常有前景。特别是，它们允许人们从有限的数据中构建复杂非线性场的生成模型。在即将进行的宇宙学调查的背景下，有必要将这些工具扩展到球面数据。我们开发了球面上的散射变换，并专注于构建天体物理场最大熵生成模型。生成模型的质量，无论是在统计上还是视觉上，都非常令人满意，因此为未来的宇宙学研究开辟了广泛的新应用。||
|**2024-07-11**|[CAD-Prompted Generative Models: A Pathway to Feasible and Novel Engineering Designs](http://arxiv.org/abs/2407.08675)|null|文本到图像生成模型越来越多地被用于协助各个创意领域的概念生成，例如图形设计、用户界面设计和时装设计。然而，由于模型在生成可行设计概念的图像方面存在挑战，它们在工程设计中的应用仍然有限。为了解决这个问题，本文介绍了一种通过使用可行的 CAD 图像提示生成来提高设计可行性的方法。在这项工作中，通过一个自行车设计任务的案例研究，使用现成的文本到图像模型 Stable Diffusion 2.1，研究了该方法的有效性。在七个不同的生成设置中，使用不同的 CAD 图像提示权重，生成了一组多样化的自行车设计，并根据其感知的可行性和新颖性对这些设计进行了评估。结果表明，CAD 图像提示成功地帮助 Stable Diffusion 2.1 等文本到图像模型创建了明显更可行的设计图像。虽然在可行性和新颖性之间观察到了一般的权衡，但当提示权重保持在 0.35 左右的低水平时，设计可行性得到显着提高，而其新颖性与仅通过文本提示生成的设计保持一致。该案例研究的见解为工程设计过程的不同阶段选择合适的 CAD 图像提示权重提供了一些指导。如果使用得当，我们的 CAD 图像提示方法为文本到图像模型在工程设计中的更广泛应用打开了大门。||
|**2024-07-11**|[Controlling the Fidelity and Diversity of Deep Generative Models via Pseudo Density](http://arxiv.org/abs/2407.08659)|null|我们提出了一种对深度生成模型（如 GAN 和扩散模型）进行偏差的方法，以生成具有更高保真度或更高多样性的数据。我们的方法涉及通过一种新的个体样本度量标准（称为伪密度）来操纵训练数据和生成数据的分布，该度量标准基于来自真实样本的最近邻信息。我们的方法提供了三种不同的技术来调整深度生成模型的保真度和多样性：1）每样本扰动，可以对单个样本进行精确调整，使其具有更常见或更独特的特征；2）模型推理过程中的重要性采样，以增强生成数据的保真度或多样性；3）使用重要性采样进行微调，引导生成模型学习调整后的分布，从而控制保真度和多样性。此外，我们的微调方法展示了在最少迭代次数下改进预训练生成模型的 Frechet Inception Distance (FID) 的能力。||
|**2024-07-11**|[Adaptive Smooth Non-Stationary Bandits](http://arxiv.org/abs/2407.08654)|null|我们研究了一个 $K$臂非平稳老虎机模型，其中奖励随时间平滑变化，可以用奖励作为时间函数的赫尔德类假设来刻画。这种平滑变化由赫尔德指数$\beta$和系数$\lambda$参数化。虽然这个一般模型的各种子情况已经被单独研究，但我们首先对所有$K,\beta,\lambda$都确定了极小极大动态遗憾率。接下来，我们证明了这种最优动态遗憾可以自适应地实现，而无需知道$\beta,\lambda$。相比之下，即使在参数已知的情况下，以前的上界也只在$\beta\leq 1$和$\beta=2$的有限范围内知道(Slivkins, 2014; Krishnamurthy and Gopalan, 2021; Manegueu et al., 2021; Jia et al.,2023)。因此，我们的工作解决了这些不同的文献线索提出的开放性问题。我们还研究了在非平稳老虎机中获得更快的与间隔相关的遗憾率的问题。虽然众所周知，这种速率在一般情况下是不可能实现的(Garivier and Moulines, 2011)，但我们表明，允许安全臂的环境(Suk and Kpotufe, 2022)允许比$\sqrt{T}$ 的最坏情况缩放快得多的速率。虽然之前在这个方向上的工作集中在获得通常的对数遗憾界，如在平稳周期上的总和，但我们新的与间隔相关的速率揭示了非平稳性的新的乐观状态，即使是对数界也是悲观的。我们证明了我们新的与间隔相关的速率是紧的，并且它的可实现性（即，安全臂使之成为可能）在平滑的赫尔德类模型中有一个惊人的简单和干净的特征。||
|**2024-07-11**|[Latent Conditional Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph Mode](http://arxiv.org/abs/2407.08500)|null|连续时间动态图（CTDG）精确地模拟了不断演变的现实世界关系，引起了学术界和工业界对动态图学习的浓厚兴趣。然而，现有的CTDG模型面临着来自噪声和有限历史数据的挑战。图数据增强（GDA）成为了一种关键解决方案，但目前的方法主要集中在静态图上，难以有效解决CTDG固有的动态性问题。此外，这些方法通常需要大量的领域专业知识来进行参数调整，并且缺乏对增强效果的理论保证。为了解决这些问题，我们提出了Conda，这是一种专为CTDG量身定制的基于潜在扩散的新型GDA方法。Conda采用了一种类似三明治的架构，结合了变分自动编码器（VAE）和条件扩散模型，旨在为目标节点生成增强的历史邻居嵌入。与通过预训练在整个图上训练的传统扩散模型不同，Conda需要目标节点的历史邻居序列嵌入进行训练，从而促进更有针对性的增强。我们将Conda集成到CTDG模型中，并采用交替训练策略来优化性能。在六个广泛使用的现实世界数据集上的大量实验表明，我们的方法始终如一地提高了性能，特别是在历史数据有限的情况下。||
|**2024-07-11**|[Natural language is not enough: Benchmarking multi-modal generative AI for Verilog generation](http://arxiv.org/abs/2407.08473)|**[link](https://github.com/aichipdesign/chipgptv)**|自然语言接口在利用大型语言模型实现从高级规范自动生成 Verilog 方面展现出巨大潜力，引发了广泛关注。然而，本文阐述了视觉表示为具有空间复杂性的硬件架构的设计意图提供了至关重要的上下文信息，这可能超越了仅使用自然语言输入的效率。在此基础上，本文介绍了一个开源的多模态生成模型基准测试，该模型专为从视觉-语言输入合成 Verilog 而设计，涵盖了单个和复杂的模块。此外，我们还介绍了一个开源的视觉和自然语言 Verilog 查询语言框架，以促进高效且用户友好的多模态查询。为了评估所提出的多模态硬件生成式人工智能在 Verilog 生成任务中的性能，我们将其与仅依赖自然语言的流行方法进行了比较。结果表明，与仅基于自然语言的查询相比，多模态生成的 Verilog 在准确性方面有显著提高。我们希望在大硬件设计模型时代揭示一种新的硬件设计方法，从而促进更具多样性和效率的硬件设计方法。||
|**2024-07-11**|[A Comprehensive Survey on Human Video Generation: Challenges, Methods, and Insights](http://arxiv.org/abs/2407.08428)|**[link](https://github.com/wentaol86/awesome-human-body-video-generation)**|人体视频生成是一个充满活力且快速发展的领域，其目标是利用生成模型根据文本、音频和姿势等控制条件合成二维人体视频序列。由于其在电影、游戏和虚拟通信等领域具有广泛的应用潜力，因此生成自然逼真的人体视频至关重要。生成模型的最新进展为该领域的兴趣日益浓厚奠定了坚实的基础。尽管取得了重大进展，但由于角色的一致性、人体运动的复杂性以及与环境关系的困难，人体视频生成的任务仍然具有挑战性。本综述全面概述了人体视频生成的现状，据我们所知，这是该领域首个广泛的文献综述。我们首先介绍人体视频生成的基本原理，以及促进该领域发展的生成模型的演变。然后，我们研究了人体视频生成中三个关键子任务所采用的主要方法：文本驱动、音频驱动和姿势驱动的运动生成。这些领域将根据指导生成过程的条件进行探讨。此外，我们还收集了最常用的数据集和评估指标，这些指标对于评估生成视频的质量和真实性至关重要。最后，我们讨论了该领域当前面临的挑战，并提出了未来研究的可能方向。本综述旨在为研究界提供一个清晰而全面的人体视频生成进展概况，重点介绍已取得的里程碑和未来面临的挑战。||
|**2024-07-11**|[Diff-Tracker: Text-to-Image Diffusion Models are Unsupervised Trackers](http://arxiv.org/abs/2407.08394)|null|我们介绍了一种名为 Diff-Tracker 的新方法，用于解决具有挑战性的无监督视觉跟踪任务，该方法利用了预训练的文本到图像扩散模型。我们的主要思想是利用预训练扩散模型中封装的丰富知识，例如对图像语义和结构信息的理解，来解决无监督视觉跟踪问题。为此，我们设计了一个初始提示学习器，通过学习表示目标的提示，使扩散模型能够识别跟踪目标。此外，为了促进提示符动态适应目标的运动，我们提出了一种在线提示符更新器。在五个基准数据集上的大量实验表明，我们提出的方法是有效的，并且达到了最先进的性能。||
|**2024-07-09**|[ConceptExpress: Harnessing Diffusion Models for Single-image Unsupervised Concept Extraction](http://arxiv.org/abs/2407.07077)|**[link](https://github.com/haoosz/conceptexpress)**|虽然个性化文本到图像生成技术已经能够从多张图像中学习单个概念，但更实际但也更具挑战性的场景涉及在单个图像中学习多个概念。然而，现有的解决此场景的工作严重依赖于大量的人工标注。在本文中，我们介绍了一项名为无监督概念提取 (UCE) 的新任务，该任务考虑了没有任何人类概念知识的无监督设置。给定一张包含多个概念的图像，该任务的目标是仅依靠预训练扩散模型的现有知识来提取和重建单个概念。为此，我们提出了 ConceptExpress，它通过从两个方面释放预训练扩散模型的内在能力来解决 UCE。具体来说，概念定位方法通过利用扩散自注意力的空间对应关系来自动定位和分离显著概念；并且基于概念和概念标记之间的查找关联，概念优化过程学习表示每个单独概念的区分标记。最后，我们为 UCE 任务建立了一个定制的评估协议。大量实验表明，ConceptExpress 是 UCE 任务的一个很有前景的解决方案。我们的代码和数据可在以下网址获得：https://github.com/haoosz/ConceptExpress||
|**2024-07-09**|[Latent Space Imaging](http://arxiv.org/abs/2407.07052)|null|传统数字成像系统通常基于对规则网格像素进行暴力测量和处理。另一方面，人类视觉系统对从光感受器到视神经的数据进行了大量压缩，本质上是将图像信息编码成适合人脑处理的低带宽潜在空间表示。在这项工作中，我们建议采用类似的方法来开发人工智能视觉系统。潜在空间成像是一种新范式，它通过光学和软件的结合，将图像信息直接编码到生成模型的语义丰富的潜在空间中，从而大大减少了捕获过程中的带宽和内存需求。我们通过基于单像素相机的初始硬件原型展示了这一新原理。通过设计一种调制幅度以编码到生成模型的潜在空间中的方案，我们在成像过程中实现了 1:100 到 1:1,000 的压缩比，这体现了潜在空间成像在高效成像硬件方面的潜力，使其能够在未来应用于高速成像或硬件复杂度大大降低的特定任务相机。||
|**2024-07-09**|[Generative models of astrophysical fields with scattering transforms on the sphere](http://arxiv.org/abs/2407.07007)|**[link](https://github.com/astro-informatics/s2scat)**|散射变换是一种新类型的概括统计量，最近被开发用于研究高度非高斯过程，并已被证明在天文物理研究中非常有前景。 特别是，它们允许人们从有限的数据中构建复杂非线性场的生成模型，并且还被用作新的统计成分分离算法的基础。 在即将进行的宇宙学巡天观测的背景下，例如针对宇宙微波背景偏振的 LiteBIRD 或用于研究宇宙大尺度结构的 Rubin-LSST 和 Euclid，将这些工具扩展到球面数据是必要的。 我们在球面上开发了散射变换，并专注于构建几个天体物理场的最大熵生成模型。 我们从单个目标场构建均匀的天体物理和宇宙学场的生成模型，使用常见的统计数据（功率谱、像素概率密度函数和 Minkowski 泛函）将样本与目标场进行定量比较。 我们的采样场在统计和视觉上都与目标场非常吻合。 因此，这些生成模型为未来的天体物理学和宇宙学研究开辟了广泛的新应用； 特别是那些几乎没有模拟数据的领域。 我们将代码提供给社区，以便这项工作可以轻松复制和进一步开发。||
|**2024-07-09**|[RodinHD: High-Fidelity 3D Avatar Generation with Diffusion Models](http://arxiv.org/abs/2407.06938)|null|我们提出了 RodinHD，它可以从肖像图像生成高保真 3D 头像。现有方法无法捕捉到我们论文中解决的复杂细节，例如发型。我们首先发现了一个被忽视的问题，即在许多头像上顺序拟合三平面时出现的灾难性遗忘问题，这是由 MLP 解码器共享方案引起的。为了克服这个问题，我们提出了一种新颖的数据调度策略和权重整合正则化项，从而提高了解码器渲染更清晰细节的能力。此外，我们通过计算捕获丰富 2D 纹理线索的更细粒度的层次表示，并通过交叉注意力将它们注入到 3D 扩散模型的多个层中，从而优化了肖像图像的引导效果。当使用针对三平面优化的噪声调度对 46K 头像进行训练时，生成的模型可以生成比以前的方法细节明显更好的 3D 头像，并且可以泛化到野外肖像输入。||
|**2024-07-09**|[HumanRefiner: Benchmarking Abnormal Human Generation and Refining with Coarse-to-fine Pose-Reversible Guidance](http://arxiv.org/abs/2407.06937)|**[link](https://github.com/enderfga/humanrefiner)**|文本到图像扩散模型在条件图像生成方面取得了显著进展。然而，这些模型通常难以准确渲染具有人类特征的图像，导致四肢扭曲和其他异常。这个问题主要源于扩散模型对肢体质量的识别和评估不足。为了解决这个问题，我们引入了AbHuman，这是第一个专注于解剖异常的大规模合成人体基准。该基准包含 56K 张合成的人体图像，每张图像都标注了详细的边界框级别标签，识别了 18 个不同类别中的 147K 个人体异常。在此基础上，可以建立人体异常的识别，进而通过负面提示和引导等传统技术增强图像生成。为了进一步促进改进，我们提出了HumanRefiner，这是一种新颖的即插即用方法，用于在文本到图像生成中对人体异常进行从粗到细的细化。具体来说，HumanRefiner 利用自我诊断程序来检测和纠正与粗粒度异常人体姿势和细粒度异常级别相关的问题，促进姿势可逆的扩散生成。在 AbHuman 基准上的实验结果表明，HumanRefiner 显着减少了生成差异，与最先进的开源生成器 SDXL 相比，肢体质量提高了 2.9 倍，在人类评估中比 DALL-E 3 提高了 1.4 倍。我们的数据和代码可在 https://github.com/Enderfga/HumanRefiner 获取。||
|**2024-07-09**|[Maximum stress minimization via data-driven multifidelity topology design](http://arxiv.org/abs/2407.06746)|null|最大应力最小化问题是结构设计中最重要的课题之一。传统的基于梯度的拓扑优化方法需要通过松弛技术将原始问题转化为伪问题。由于其参数对优化有显著影响，因此，在不使用松弛技术的情况下，准确地解决最大应力最小化问题有望获得最佳性能。本文重点关注这一挑战，并研究与基于梯度的拓扑优化获得的解决方案相比，通过解决没有松弛技术的原始最大应力最小化问题，是否可以获得具有更多避免应力集中的设计。我们采用数据驱动的多保真度拓扑设计（MFTD），这是一种基于进化算法的无梯度拓扑优化。基本框架包括通过解决低保真度优化问题来生成候选解，通过高保真度正向分析评估这些解，并在没有灵敏度分析的情况下使用深度生成模型迭代更新它们。在本研究中，数据驱动的MFTD将通过使用p范数应力度量解决基于梯度的拓扑优化问题获得的优化设计纳入初始解，并基于具有贴体网格的高保真度分析来解决原始最大应力最小化问题。我们通过L型支架的基准测试证明了我们提出的方法的有效性。通过使用数据驱动的MFTD解决原始最大应力最小化问题，与初始解相比，在相同的最大应力值下，体积减少了高达22.6%。||
|**2024-07-09**|[Powerful and Flexible: Personalized Text-to-Image Generation via Reinforcement Learning](http://arxiv.org/abs/2407.06642)|**[link](https://github.com/wfanyue/dpg-t2i-personalization)**|个性化文本到图像模型允许用户为一个对象（由一组参考图像指定）生成不同风格的图像（由句子指定）。虽然基于扩散的生成模型已经取得了显著成果，但在扩散过程中，对象的视觉结构和细节经常发生意外的变化。一个主要原因是，这些基于扩散的方法在训练过程中通常采用简单的重建目标，这很难在生成图像和参考图像之间强制实现适当的结构一致性。为此，本文设计了一种新的强化学习框架，利用确定性策略梯度方法进行个性化文本到图像生成，该框架可以轻松地结合各种目标（可微分甚至不可微分）来监督扩散模型，从而提高生成图像的质量。在个性化文本到图像生成基准数据集上的实验结果表明，我们提出的方法在视觉保真度方面明显优于现有的最先进方法，同时保持了文本对齐。我们的代码可在以下网址获得：\url{https://github.com/wfanyue/DPG-T2I-Personalization}。||
|**2024-07-09**|[Ensembled Cold-Diffusion Restorations for Unsupervised Anomaly Detection](http://arxiv.org/abs/2407.06635)|**[link](https://github.com/snavalm/disyre)**|无监督异常检测 (UAD) 方法旨在通过将测试样本与从已知无异常的数据集中学习到的规范分布进行比较来识别异常。基于生成模型的方法通过生成无异常版本的测试图像来提供可解释性，但通常无法识别细微的异常。或者，使用特征建模或自监督方法（例如依赖于合成生成的异常的方法）不提供开箱即用的可解释性。在这项工作中，我们提出了一种结合了两种策略的优势的新方法：生成式冷扩散管道（即，使用不基于噪声的损坏的类似扩散的管道），该管道经过训练，目标是将合成损坏的图像恢复到正常、原始外观。为了支持我们的管道，我们引入了一种新的合成异常生成程序，称为 DAG，以及一种新的异常评分，它集成了以不同程度的异常为条件的恢复。我们的方法在三个不同的大脑 MRI 数据集中都超过了先前最先进的无监督异常检测水平。||
|**2024-07-09**|[Mobius: An High Efficient Spatial-Temporal Parallel Training Paradigm for Text-to-Video Generation Task](http://arxiv.org/abs/2407.06617)|**[link](https://github.com/youngfly/Mobius)**|受文本到图像（T2I）生成任务成功的启发，许多研究人员正致力于文本到视频（T2V）生成任务。大多数 T2V 框架通常继承自 T2I 模型，并添加额外的时序层训练以生成动态视频，这可以视为一项微调任务。然而，传统的 3D-Unet 是一种串行模式，时间层跟随空间层，根据其串行特征流，这将导致高 GPU 内存和训练时间消耗。我们认为，这种串行模式将随着大型扩散模型和海量数据集的出现而带来更高的训练成本，这不符合环保要求，也不利于 T2V 的发展。因此，我们提出了一种高效的时空并行训练范式，用于 T2V 任务，称为 Mobius。在我们提出的 3D-Unet 中，时间层和空间层是并行的，这优化了特征流和反向传播。Mobius 将节省 24% 的 GPU 内存和 12% 的训练时间，这可以极大地改善 T2V 微调任务，并为 AIGC 社区提供新的见解。我们将在未来发布我们的代码。||
|**2024-07-09**|[VQA-Diff: Exploiting VQA and Diffusion for Zero-Shot Image-to-3D Vehicle Asset Generation in Autonomous Driving](http://arxiv.org/abs/2407.06516)|null|从野外观察中生成 3D 车辆模型对于自动驾驶至关重要。现有的图像到 3D 方法不能很好地解决这个问题，因为它们仅仅从图像 RGB 信息中学习生成，而没有更深入地理解野外车辆（例如车型、制造商等）。这导致它们对具有遮挡或棘手视角的真实世界观察结果的零样本预测能力较差。为了解决这个问题，在这项工作中，我们提出了 VQA-Diff，这是一个利用野外车辆图像为自动驾驶创建逼真的 3D 车辆模型的新框架。VQA-Diff 利用视觉问答 (VQA) 模型中大型语言模型继承的真实世界知识进行鲁棒的零样本预测，并利用扩散模型中丰富的图像先验知识进行结构和外观生成。具体来说，我们利用多专家扩散模型策略生成结构信息，并采用主题驱动的结构控制生成机制对外观信息进行建模。因此，VQA-Diff 无需从现实世界中收集的大规模图像到 3D 车辆数据集中学习，仍然具有强大的零样本图像到新颖视图生成能力。我们在 Pascal 3D+、Waymo 和 Objaverse 等各种数据集上进行了实验，结果表明 VQA-Diff 在定性和定量上均优于现有的最先进方法。||
|**2024-07-05**|[Structural Constraint Integration in Generative Model for Discovery of Quantum Material Candidates](http://arxiv.org/abs/2407.04557)|null|已知的有机分子数以十亿计，但已发现的功能性无机材料却只占很小一部分，这对寻找新型量子材料的研究群体来说是一个特别突出的问题。基于机器学习的生成模型，尤其是扩散模型的最新进展，为生成新型稳定材料带来了巨大希望。然而，将几何模式融入材料生成仍然是一项挑战。在此，我们介绍了在生成模型中集成结构约束的方法 (SCIGEN)。我们的方法可以通过在每个扩散步骤之前，用扩散约束结构对去噪结构进行策略性掩蔽，从而将生成引导至约束输出，来修改任何经过训练的生成扩散模型。此外，我们从数学上证明了 SCIGEN 可以有效地从原始分布中执行条件采样，这对于生成稳定的约束材料至关重要。我们使用阿基米德格作为原型约束生成了 800 万种化合物，其中超过 10% 通过了多阶段稳定性预筛选。对 26,000 种存活化合物的 DFT（密度泛函理论）高通量计算表明，超过 50% 的化合物通过了 DFT 级别的结构优化。由于量子材料的性质与几何模式密切相关，我们的结果表明 SCIGEN 为生成量子材料候选材料提供了一个通用框架。||
|**2024-07-05**|[Unified continuous-time q-learning for mean-field game and mean-field control problems](http://arxiv.org/abs/2407.04521)|null|本文从代表性智能体的角度研究了平均场跳扩散模型中的连续时间q学习。为了克服无法直接观察到总体分布时的挑战，我们引入了解耦形式的集成q函数（解耦Iq函数），并建立了其与价值函数的鞅表征，这为平均场博弈（MFG）和平均场控制（MFC）问题提供了统一的策略评估规则。此外，根据求解MFG或MFC问题的任务，我们可以通过不同的方式利用解耦Iq函数来分别学习平均场均衡策略或平均场最优策略。因此，我们利用所有源于平均场交互的测试策略，设计了一种适用于MFG和MFC问题的统一q学习算法。对于跳扩散环境下的几个例子，包括LQ框架内外的例子，我们可以获得解耦Iq函数和价值函数的精确参数化，并从代表性智能体的角度说明我们的算法具有令人满意的性能。||
|**2024-07-05**|[Speed-accuracy trade-off for the diffusion models: Wisdom from nonequlibrium thermodynamics and optimal transport](http://arxiv.org/abs/2407.04495)|null|我们探讨了生成模型（称为扩散模型）与非平衡热力学中用于描述福克-普朗克方程的随机热力学之间的联系。基于随机热力学技术，我们推导出了扩散模型的速度-精度权衡，这是扩散模型中数据生成速度和精度之间的权衡关系。我们的结果表明，正向过程中的熵产生率会影响数据生成的误差。从随机热力学的角度来看，我们的结果为如何在扩散模型中最好地生成数据提供了定量见解。最佳学习方案由随机热力学中的保守力和最优传输理论中 2-Wasserstein 距离的空间测地线引入。我们用数值方法说明了具有不同噪声方案（如余弦方案、条件最优传输和最优传输）的扩散模型的速度-精度权衡的有效性。||
|**2024-07-05**|[PROUD: PaRetO-gUided Diffusion Model for Multi-objective Generation](http://arxiv.org/abs/2407.04493)|null|深度生成模型领域的最新进展集中于生成满足多个期望属性的样本。然而，普遍的方法是独立优化这些属性函数，从而忽略了它们之间的权衡。此外，属性优化通常没有被恰当地整合到生成模型中，导致生成质量（即生成样本的质量）的无必要妥协。为了解决这些问题，我们提出了一个约束优化问题。它寻求在确保生成样本位于多个属性目标的帕累托前沿的同时优化生成质量。这样的公式能够生成在相互冲突的属性函数上无法同时进一步改进的样本，并保持生成样本的良好质量。在此公式的基础上，我们引入了帕累托引导扩散模型 (PROUD)，其中去噪过程中的梯度被动态调整以提高生成质量，同时生成样本遵循帕累托最优性。在图像生成和蛋白质生成任务上的实验评估表明，与各种基线相比，我们的 PROUD 在逼近多个属性函数的帕累托最优性的同时，始终保持着卓越的生成质量。||
|**2024-07-05**|[VCD-Texture: Variance Alignment based 3D-2D Co-Denoising for Text-Guided Texturing](http://arxiv.org/abs/2407.04461)|null|最近关于三维形状纹理合成的研究极大地受益于快速发展的二维文本到图像的扩散模型，包括基于修复和基于优化的方案。然而，这些方法忽略了二维扩散模型和三维物体之间的模态差距，它们主要将三维物体渲染成二维图像并分别对每个图像进行纹理处理。在本文中，我们重新审视了纹理合成，并提出了一个基于方差对齐的三维-二维协同去噪框架，称为VCD-Texture，以解决这些问题。具体来说，我们首先在具有重新投影的三维注意力感受野的扩散自注意力模块中统一了二维和三维潜在特征学习。随后，将去噪后的多视图二维潜在特征聚合到三维空间，然后将其光栅化回二维空间，从而形成更一致的二维预测。然而，光栅化过程存在难以处理的方差偏差，我们提出的方差对齐从理论上解决了这个问题，实现了高保真纹理合成。此外，我们还提出了一种修复细化方法，以进一步改善存在冲突区域的细节。值得注意的是，目前还没有公开可用的基准来评估纹理合成，这阻碍了其发展。因此，我们构建了一个基于三个开源三维数据集的新评估集，并建议使用四个指标来全面验证纹理性能。综合实验表明，VCD-Texture相较于其他方法取得了优越的性能。||
|**2024-07-05**|[Benchmarking structure-based three-dimensional molecular generative models using GenBench3D: ligand conformation quality matters](http://arxiv.org/abs/2407.04424)|**[link](https://github.com/bbaillif/genbench3d)**|三维 (3D) 深度分子生成模型提供了基于 3D 依赖属性的目标导向生成的优势，例如结合腔内基于结构的设计的结合亲和力。 为评估 SMILES 或分子图生成器而创建的传统基准，例如 GuacaMol 或 MOSES，由于它们不评估生成的分子构象的质量，因此在评估 3D 生成器方面受到限制。 因此，我们在这项工作中开发了 GenBench3D，它实现了一个用于在结合腔内生成分子的新基准。 我们的主要贡献是 Validity3D 指标，它使用基于剑桥结构数据库中观察到的参考值的键长和价角的可能性来评估构象质量。 对 LiGAN、3D-SBDD、Pocket2Mol、TargetDiff、DiffSBDD 和 ResGen 模型进行了基准测试。 我们发现只有 0% 到 11% 的生成分子具有有效的构象。 对口袋中生成的分子进行局部弛豫，通过至少增加 40% 的 Validity3D，大大提高了所有模型的 Validity3D。 对于 LiGAN、3D-SBDD 或 TargetDiff，有效弛豫分子集显示的平均 Vina 分数（即更差）高于原始生成分子集，表明原始生成分子的结合亲和力可能被高估了。 使用其他评分函数（更重视配体应变）仅在使用有效的弛豫分子时才会产生改进的分数。 使用有效的弛豫分子，TargetDiff 和 Pocket2Mol 显示出比其他模型更好的中位 Vina、Glide 和 Gold PLP 分数。 我们已经在 GitHub 上公开发布了 GenBench3D 以供更广泛地使用：https://github.com/bbaillif/genbench3d||
|**2024-07-05**|[Improving Audio Generation with Visual Enhanced Caption](http://arxiv.org/abs/2407.04416)|null|生成模型在音频生成任务中取得了显著成果，但现有模型难以处理复杂和详细的提示，导致潜在的性能下降。我们假设这个问题源于训练数据的低质量和相对较小的数量。在这项工作中，我们的目标是创建一个带有丰富描述的大规模音频数据集，用于改进音频生成模型。我们开发了一个自动化管道，通过使用大型语言模型 (LLM) 将预测的视觉字幕、音频字幕和标签标签转换为全面的描述，从而为视听数据集生成详细的字幕。我们介绍了 Sound-VECaps，这是一个包含 166 万个高质量音频-字幕对的数据集，其中包含丰富的细节，包括音频事件顺序、发生地点和环境信息。我们证明，使用 Sound-VECaps 进行训练可以显著增强文本到音频生成模型理解和从复杂输入提示生成音频的能力，从而提高整体系统性能。此外，我们对 Sound-VECaps 在多个音频-语言任务中进行了消融研究，表明其在推进音频-文本表示学习方面的潜力。我们的数据集和模型可在网上获得。||
|**2024-07-05**|[Unsupervised Learning of Category-Level 3D Pose from Object-Centric Videos](http://arxiv.org/abs/2407.04384)|**[link](https://github.com/genintel/uns-obj-pose3d)**|类别级三维姿态估计是计算机视觉和机器人技术中的一个基本问题，例如对于具身代理或训练三维生成模型。然而，到目前为止，估计类别级物体姿态的方法要么需要大量的人工标注、CAD模型，要么需要来自RGB-D传感器的输入。相比之下，我们致力于解决仅从随意拍摄的以物体为中心的视频中学习估计类别级三维姿态的问题，无需人工监督。我们提出了一个两步流程：首先，我们引入了一种多视图对齐程序，该程序使用一种新颖且鲁棒的循环距离公式来确定视频之间的规范相机姿态，该公式使用重建的粗网格和DINOv2特征进行几何和外观匹配。其次，规范姿态和重建的网格使我们能够从单个图像中训练用于三维姿态估计的模型。具体来说，我们的模型通过预测二维图像中每个像素的模板网格中对应顶点的特征向量，来学习估计图像和原型三维模板之间的密集对应关系。我们证明，我们的方法在以物体为中心的视频的无监督对齐方面大大优于所有基线，并在实际应用中提供了可靠且鲁棒的预测。我们的代码和数据可在https://github.com/GenIntel/uns-obj-pose3d获取。||
|**2024-07-05**|[A Mapping Strategy for Interacting with Latent Audio Synthesis Using Artistic Materials](http://arxiv.org/abs/2407.04379)|null|本文提出了一种与生成式人工智能模型的潜在空间交互的映射策略。我们的方法涉及使用无监督特征学习对人类控制空间进行编码，并将其映射到音频合成模型的潜在空间。为了演示这种映射策略如何将高维传感器数据转化为深度生成模型的控制机制，我们提出了一个概念验证系统，该系统使用视觉草图来控制音频合成模型。我们借鉴 XAIxArts 中的新兴论述来讨论这种方法如何为艺术和创意环境中的 XAI 做出贡献，我们还讨论了它目前的局限性并提出了未来的研究方向。||
|**2024-07-05**|[MuseBarControl: Enhancing Fine-Grained Control in Symbolic Music Generation through Pre-Training and Counterfactual Loss](http://arxiv.org/abs/2407.04331)|null|自动生成符号音乐——根据特定人类需求量身定制的乐谱——对音乐家和爱好者来说非常有利。最近的研究表明，使用大型数据集和先进的Transformer架构取得了可喜的成果。然而，这些最先进的模型通常只对整首作品的节奏和风格等方面提供基本的控制，缺乏管理更精细细节的能力，例如在单个小节级别的控制。虽然微调预先训练好的符号音乐生成模型似乎是实现这种更精细控制的直接方法，但我们的研究表明这种方法存在挑战。该模型通常无法对新的、细粒度的小节级控制信号做出充分响应。为了解决这个问题，我们提出了两个创新的解决方案。首先，我们引入了一个预训练任务，旨在将控制信号直接与其相应的音乐符号链接起来，这有助于为后续的微调实现更有效的初始化。其次，我们实施了一种新的反事实损失，以促进生成的音乐与控制提示之间更好地保持一致。总的来说，这些技术显著增强了我们在小节级别控制音乐生成的能力，比传统方法提高了13.06%。我们的主观评价也证实，这种增强的控制并没有损害原始预训练生成模型的音乐质量。||
|**2024-07-03**|[DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents](http://arxiv.org/abs/2407.03300)|**[link](https://github.com/gcorso/disco-diffdock)**|扩散模型 (DM) 为生成式学习带来了革命性的变化。它们利用扩散过程将数据编码为简单的 Gaussian 分布。然而，将复杂且可能具有多模态的数据分布编码为单个连续 Gaussian 分布无疑是一个不必要的具有挑战性的学习问题。我们提出离散-连续潜在变量扩散模型 (DisCo-Diff)，通过引入互补的离散潜在变量来简化此任务。我们使用可学习的离散潜在变量增强 DM，并使用编码器进行推断，并对 DM 和编码器进行端到端训练。DisCo-Diff 不依赖于预先训练的网络，这使得该框架具有普遍适用性。离散潜在变量通过降低 DM 生成 ODE 的曲率，显著简化了学习 DM 复杂噪声到数据映射的过程。一个额外的自回归 Transformer 模型对离散潜在变量的分布进行建模，这是一个简单的步骤，因为 DisCo-Diff 只需要具有少量码本的少量离散变量。我们在玩具数据、多个图像合成任务以及分子对接上验证了 DisCo-Diff，发现引入离散潜在变量始终可以提高模型性能。例如，DisCo-Diff 在使用 ODE 采样器的类条件 ImageNet-64/128 数据集上实现了最先进的 FID 分数。||
|**2024-07-03**|[Improved Noise Schedule for Diffusion Training](http://arxiv.org/abs/2407.03297)|null|扩散模型已成为生成视觉信号的首选方法。然而，训练单个模型来预测不同级别的噪声提出了重大挑战，需要多次迭代并导致巨大的计算成本。为了加快收敛速度，人们引入了各种方法，例如损失加权策略设计和架构改进。在本研究中，我们提出了一种设计噪声调度的新方法，以增强扩散模型的训练。我们的主要见解是，对数信噪比（logSNR）的重要性采样（理论上等效于修改后的噪声调度）对于提高训练效率特别有利，特别是在增加 $\log \text{SNR}=0$ 附近的采样频率时。我们通过经验证明了我们的噪声调度优于标准余弦调度。此外，我们还重点介绍了我们的噪声调度设计在 ImageNet 基准测试中的优势，表明所设计的调度始终有利于不同的预测目标。||
|**2024-07-03**|[Spatio-Temporal Adaptive Diffusion Models for EEG Super-Resolution in Epilepsy Diagnosis](http://arxiv.org/abs/2407.03089)|null|脑电图 (EEG) 技术，特别是高密度脑电图 (HD EEG) 设备，广泛应用于神经科学等领域。HD EEG 设备通过在头皮上放置更多电极来提高 EEG 的空间分辨率，满足癫痫病灶定位等临床诊断应用的要求。然而，该技术面临着采集成本高、使用场景有限等挑战。本文提出了时空自适应扩散模型 (STADM)，率先利用扩散模型实现从低分辨率 (LR, 64 通道或更少) EEG 到高分辨率 (HR, 256 通道) EEG 的空间超分辨率 (SR) 重建。具体而言，设计了一种时空条件模块来提取 LR EEG 的时空特征，然后将其作为条件输入来指导扩散模型的反向去噪过程。此外，构建了一个多尺度 Transformer 去噪模块，利用多尺度卷积块和基于交叉注意力的扩散 Transformer 块进行条件引导，生成自适应于受试者的 SR EEG。实验结果表明，该方法有效提高了 LR EEG 的空间分辨率，并在数量上优于现有方法。此外，STADM 通过将合成的 SR EEG 应用于癫痫患者的分类和源定位任务，证明了其价值，表明其具有显著提高 LR EEG 空间分辨率的潜力。||
|**2024-07-03**|[Artificial Inductive Bias for Synthetic Tabular Data Generation in Data-Scarce Scenarios](http://arxiv.org/abs/2407.03080)|null|虽然使用深度生成模型 (DGM) 生成合成表格数据为数据稀缺和隐私问题提供了一种引人注目的解决方案，但其有效性依赖于大量的训练数据，而这些数据在现实应用中通常不可用。本文提出了一种新颖的方法，用于在有限的真实数据环境中使用 DGM 生成真实可靠的合成表格数据，从而解决了这一挑战。我们的方法提出了几种通过迁移学习和元学习技术在 DGM 中生成人工归纳偏差的方法。我们在该框架内探索并比较了四种不同的方法，证明了预训练和模型平均等迁移学习策略优于模型无关元学习和域随机搜索等元学习方法。我们使用两种最先进的 DGM（变分自动编码器和生成对抗网络）验证了我们的方法，表明我们的人工归纳偏差提高了合成数据的质量（通过 Jensen-Shannon 散度衡量），在使用我们提出的方法时，相对增益高达 50%。这种方法在各种 DGM 和机器学习任务中具有广泛的适用性，特别是在医疗保健和金融等数据稀缺通常是关键问题的领域。||
|**2024-07-03**|[Electromagnetic Property Sensing Based on Diffusion Model in ISAC System](http://arxiv.org/abs/2407.03075)|null|集成传感与通信 (ISAC) 为未来的无线系统开辟了许多颠覆性的机遇。在本文中，我们开发了一种新颖的 ISAC 方案，利用扩散模型来感知预定传感区域中目标的电磁 (EM) 特性。具体来说，我们首先利用从目标反射回来的通信和传感信号来估计传感信道。然后，我们采用扩散模型生成代表目标的点云，从而实现目标电磁特性分布的 3D 可视化。为了最小化真实点云和估计点云之间的平均 Chamfer 距离 (MCD)，我们在最大发射功率和每个用户设备 (UE) 的最小通信可达速率的约束下，进一步设计了通信和传感波束赋形矩阵。仿真结果证明了该方法在实现目标形状、相对介电常数和电导率的高质量重建方面的有效性。此外，该方法可以在传感区域的任何位置有效地感知目标的电磁特性。||
|**2024-07-03**|[Semantic-Aware Power Allocation for Generative Semantic Communications with Foundation Models](http://arxiv.org/abs/2407.03050)|null|扩散模型的最新进展为生成建模带来了重大突破。生成模型与语义通信 (SemCom) 的结合能够以超低速率实现高保真语义信息交换。本文提出了一种用于图像任务的新型生成式 SemCom 框架，其中预训练的基础模型分别充当语义编码器和解码器，用于语义特征提取和图像再生。文章对传输可靠性与再生图像的感知质量以及语义特征的语义值之间的数学关系进行了建模，这些关系是通过对 Kodak 数据集进行数值模拟获得的。我们还研究了语义感知功率分配问题，目标是在保证语义性能的同时最小化总功耗。为了解决这个问题，分别通过约束解耦和二分搜索提出了两种语义感知功率分配方法。数值结果表明，与传统方法相比，所提出的语义感知方法在总功耗方面表现出优越的性能。||
|**2024-07-03**|[SlerpFace: Face Template Protection via Spherical Linear Interpolation](http://arxiv.org/abs/2407.03043)|null|当代人脸识别系统使用从人脸图像中提取的特征模板来识别身份。为了增强隐私性，人脸模板保护技术被广泛用于隐藏存储在模板中的敏感身份和外观信息。本文识别了一种新兴的利用扩散模型的隐私攻击形式，它可以使先前的保护无效，称为反演攻击。这种攻击可以从模板中合成高质量、保留身份的人脸图像，从而暴露人的外貌。基于对扩散模型生成能力的研究，本文提出了一种防御措施来削弱这种攻击，即通过将模板旋转到类似噪声的分布。这是通过在其所在的超球面上对模板进行球面和线性插值（slerp）来有效实现的。为了增强旋转模板的不可逆性，本文进一步提出对模板的特征维度进行分组划分和丢弃。组的划分和每个组内的丢弃以有利于识别的的方式学习。所提出的技术被具体化为一种新的人脸模板保护技术，SlerpFace。大量实验表明，SlerpFace 提供了令人满意的识别精度和全面的隐私保护，可以抵御反演和其他攻击形式，优于现有技术。||
|**2024-07-03**|[An Organism Starts with a Single Pix-Cell: A Neural Cellular Diffusion for High-Resolution Image Synthesis](http://arxiv.org/abs/2407.03018)|**[link](https://github.com/xmed-lab/geca)**|生成模型旨在逼近真实数据的统计特性，从而能够合成与原始分布非常相似的新数据。生成对抗网络 (GAN) 和去噪扩散概率模型 (DDPM) 代表了生成模型的重大进步，它们分别从博弈论和热力学中汲取灵感。然而，从生物进化的角度探索生成模型在很大程度上仍未得到开发。在本文中，我们介绍了一种称为生成元胞自动机 (GeCA) 的新型模型系列，其灵感来自于生物体从单细胞的进化。GeCA 被评估为一种有效的视网膜疾病分类增强工具，可用于两种成像模式：眼底和光学相干断层扫描 (OCT)。在 OCT 成像中，数据稀缺且类别分布存在固有的偏差，GeCA 显着提高了 11 种不同眼科疾病的表现，与传统基线相比，平均 F1 分数提高了 12%。在类似的参数约束下，GeCA 的性能优于包含 UNet 或基于 Transformer 的最新去噪模型的扩散方法。代码可在以下网址获取：https://github.com/xmed-lab/GeCA。||
|**2024-07-03**|[Frequency-Controlled Diffusion Model for Versatile Text-Guided Image-to-Image Translation](http://arxiv.org/abs/2407.03006)|**[link](https://github.com/xianggao1102/fcdiffusion)**|近年来，大规模文本到图像 (T2I) 扩散模型已成为一种强大的图像到图像转换 (I2I) 工具，允许通过用户提供的文本提示进行开放域图像转换。本文提出了频率控制扩散模型 (FCDiffusion)，这是一个基于扩散的端到端框架，从频域角度为文本引导的 I2I 提供了一种新颖的解决方案。我们框架的核心是一个基于离散余弦变换的特征空间频域滤波模块，它在 DCT 域中滤波源图像的潜在特征，产生具有不同 DCT 谱带的滤波图像特征，作为预训练的潜在扩散模型的不同控制信号。我们发现，不同 DCT 谱带的控制信号在不同的相关性（例如，风格、结构、布局、轮廓等）上桥接了源图像和 T2I 生成的图像，从而使多功能 I2I 应用能够强调不同的 I2I 相关性，包括风格引导的内容创建、图像语义操作、图像场景转换和图像风格转换。与相关方法不同，FCDiffusion 建立了一个统一的文本引导 I2I 框架，只需在推理时切换不同的频率控制分支，即可适用于各种图像转换任务。广泛的定性和定量实验都证明了我们的方法在文本引导 I2I 方面的有效性和优越性。代码公开于：https://github.com/XiangGao1102/FCDiffusion。||
|**2024-07-03**|[Towards a Scalable Reference-Free Evaluation of Generative Models](http://arxiv.org/abs/2407.02961)|**[link](https://github.com/aziksh-ospanov/fkea)**|虽然生成模型的标准评估分数大多是基于参考的，但由于缺乏适用的参考数据集，对生成模型进行依赖参考的评估通常很困难。最近，人们提出了无参考的熵分数 VENDI 和 RKE 来评估生成数据的多样性。然而，从数据中估计这些分数会导致大规模生成模型的计算成本很高。在这项工作中，我们利用随机傅里叶特征框架来降低计算成本，并提出了基于傅里叶的核熵逼近 (FKEA) 方法。我们利用 FKEA 对核矩阵的近似特征谱来有效地估计上述熵分数。此外，我们展示了 FKEA 代理特征向量的应用，以揭示该方法在评估生成样本多样性时识别的模式。我们提供了 FKEA 评估算法的随机实现，其复杂度为 $O(n)$，随样本大小 $n$ 线性增长。我们广泛评估了 FKEA 在标准图像、文本和视频数据集中的数值性能。我们的实验结果表明，该方法应用于大规模生成模型具有可扩展性和可解释性。代码库可在 https://github.com/aziksh-ospanov/FKEA 获取。||

## LLM

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-08-22**|[AutoTest: Evolutionary Code Solution Selection with Test Cases](http://arxiv.org/abs/2408.12125)|null|With the development of code generation techniques, selecting the correct code solution from multiple candidate solutions has become a crucial task. This study proposes AutoTest, a novel technique that combines automated test case generation with code solution execution to optimize the selection process using an evolutionary genetic algorithm. Firstly, AutoTest utilizes large pre-trained language models such as codegen-16B, code-davinci-002, and incoder-6B to provide code solutions and their corresponding test cases. Then, by executing the code solutions and evaluating their performance on the test cases, a consensus set is formed. Fine-grained ranking is achieved through the selection, mutation, and crossover mechanisms based on the evolutionary genetic algorithm, with the adjustment of alpha and beta parameters. Finally, the best code solution is chosen. AutoTest demonstrates significant performance improvements on the HumanEval benchmark test. The HumanEval dataset consists of 164 programming problems, and AutoTest achieves approximately a 10% improvement over the baseline method in terms of pass@1 score.||
|**2024-08-21**|[Towards Evaluating Large Language Models on Sarcasm Understanding](http://arxiv.org/abs/2408.11319)|null|In the era of large language models (LLMs), the task of ``System I''~-~the fast, unconscious, and intuitive tasks, e.g., sentiment analysis, text classification, etc., have been argued to be successfully solved. However, sarcasm, as a subtle linguistic phenomenon, often employs rhetorical devices like hyperbole and figuration to convey true sentiments and intentions, involving a higher level of abstraction than sentiment analysis. There is growing concern that the argument about LLMs' success may not be fully tenable when considering sarcasm understanding. To address this question, we select eleven SOTA LLMs and eight SOTA pre-trained language models (PLMs) and present comprehensive evaluations on six widely used benchmark datasets through different prompting approaches, i.e., zero-shot input/output (IO) prompting, few-shot IO prompting, chain of thought (CoT) prompting. Our results highlight three key findings: (1) current LLMs underperform supervised PLMs based sarcasm detection baselines across six sarcasm benchmarks. This suggests that significant efforts are still required to improve LLMs' understanding of human sarcasm. (2) GPT-4 consistently and significantly outperforms other LLMs across various prompting methods, with an average improvement of 14.0\% $\uparrow$ . Claude 3 and ChatGPT demonstrate the next best performance after GPT-4. (3) Few-shot IO prompting method outperforms the other two methods: zero-shot IO and few-shot CoT. The reason is that sarcasm detection, being a holistic, intuitive, and non-rational cognitive process, is argued not to adhere to step-by-step logical reasoning, making CoT less effective in understanding sarcasm compared to its effectiveness in mathematical reasoning tasks.||
|**2024-08-20**|[Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution](http://arxiv.org/abs/2408.10548)|**[link](https://github.com/lanxiang1017/language-modeling-on-tabular-data-survey)**|Tabular data, a prevalent data type across various domains, presents unique challenges due to its heterogeneous nature and complex structural relationships. Achieving high predictive performance and robustness in tabular data analysis holds significant promise for numerous applications. Influenced by recent advancements in natural language processing, particularly transformer architectures, new methods for tabular data modeling have emerged. Early techniques concentrated on pre-training transformers from scratch, often encountering scalability issues. Subsequently, methods leveraging pre-trained language models like BERT have been developed, which require less data and yield enhanced performance. The recent advent of large language models, such as GPT and LLaMA, has further revolutionized the field, facilitating more advanced and diverse applications with minimal fine-tuning. Despite the growing interest, a comprehensive survey of language modeling techniques for tabular data remains absent. This paper fills this gap by providing a systematic review of the development of language modeling for tabular data, encompassing: (1) a categorization of different tabular data structures and data types; (2) a review of key datasets used in model training and tasks used for evaluation; (3) a summary of modeling techniques including widely-adopted data processing methods, popular architectures, and training objectives; (4) the evolution from adapting traditional Pre-training/Pre-trained language models to the utilization of large language models; (5) an identification of persistent challenges and potential future research directions in language modeling for tabular data analysis. GitHub page associated with this survey is available at: https://github.com/lanxiang1017/Language-Modeling-on-Tabular-Data-Survey.git.||
|**2024-08-20**|[Synergistic Approach for Simultaneous Optimization of Monolingual, Cross-lingual, and Multilingual Information Retrieval](http://arxiv.org/abs/2408.10536)|null|Information retrieval across different languages is an increasingly important challenge in natural language processing. Recent approaches based on multilingual pre-trained language models have achieved remarkable success, yet they often optimize for either monolingual, cross-lingual, or multilingual retrieval performance at the expense of others. This paper proposes a novel hybrid batch training strategy to simultaneously improve zero-shot retrieval performance across monolingual, cross-lingual, and multilingual settings while mitigating language bias. The approach fine-tunes multilingual language models using a mix of monolingual and cross-lingual question-answer pair batches sampled based on dataset size. Experiments on XQuAD-R, MLQA-R, and MIRACL benchmark datasets show that the proposed method consistently achieves comparable or superior results in zero-shot retrieval across various languages and retrieval tasks compared to monolingual-only or cross-lingual-only training. Hybrid batch training also substantially reduces language bias in multilingual retrieval compared to monolingual training. These results demonstrate the effectiveness of the proposed approach for learning language-agnostic representations that enable strong zero-shot retrieval performance across diverse languages.||
|**2024-08-20**|[Data Augmentation Integrating Dialogue Flow and Style to Adapt Spoken Dialogue Systems to Low-Resource User Groups](http://arxiv.org/abs/2408.10516)|null|This study addresses the interaction challenges encountered by spoken dialogue systems (SDSs) when engaging with users who exhibit distinct conversational behaviors, particularly minors, in scenarios where data are scarce. We propose a novel data augmentation framework to enhance SDS performance for user groups with limited resources. Our approach leverages a large language model (LLM) to extract speaker styles and a pre-trained language model (PLM) to simulate dialogue act history. This method generates enriched and personalized dialogue data, facilitating improved interactions with unique user demographics. Extensive experiments validate the efficacy of our methodology, highlighting its potential to foster the development of more adaptive and inclusive dialogue systems.||
|**2024-08-20**|[Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism](http://arxiv.org/abs/2408.10473)|null|Pre-trained language models (PLMs) are engineered to be robust in contextual understanding and exhibit outstanding performance in various natural language processing tasks. However, their considerable size incurs significant computational and storage costs. Modern pruning strategies employ one-shot techniques to compress PLMs without the need for retraining on task-specific or otherwise general data; however, these approaches often lead to an indispensable reduction in performance. In this paper, we propose SDS, a Sparse-Dense-Sparse pruning framework to enhance the performance of the pruned PLMs from a weight distribution optimization perspective. We outline the pruning process in three steps. Initially, we prune less critical connections in the model using conventional one-shot pruning methods. Next, we reconstruct a dense model featuring a pruning-friendly weight distribution by reactivating pruned connections with sparse regularization. Finally, we perform a second pruning round, yielding a superior pruned model compared to the initial pruning. Experimental results demonstrate that SDS outperforms the state-of-the-art pruning techniques SparseGPT and Wanda under an identical sparsity configuration. For instance, SDS reduces perplexity by 9.13 on Raw-Wikitext2 and improves accuracy by an average of 2.05% across multiple zero-shot benchmarks for OPT-125M with 2:4 sparsity.||
|**2024-08-19**|[Rhyme-aware Chinese lyric generator based on GPT](http://arxiv.org/abs/2408.10130)|null|Neural language representation models such as GPT, pre-trained on large-scale corpora, can effectively capture rich semantic patterns from plain text and be fine-tuned to consistently improve natural language generation performance. However, existing pre-trained language models used to generate lyrics rarely consider rhyme information, which is crucial in lyrics. Using a pre-trained model directly results in poor performance. To enhance the rhyming quality of generated lyrics, we incorporate integrated rhyme information into our model, thereby improving lyric generation performance.||
|**2024-08-19**|[Instruction-Based Molecular Graph Generation with Unified Text-Graph Diffusion Model](http://arxiv.org/abs/2408.09896)|**[link](https://github.com/ran1812/utgdiff)**|Recent advancements in computational chemistry have increasingly focused on synthesizing molecules based on textual instructions. Integrating graph generation with these instructions is complex, leading most current methods to use molecular sequences with pre-trained large language models. In response to this challenge, we propose a novel framework, named $\textbf{UTGDiff (Unified Text-Graph Diffusion Model)}$ , which utilizes language models for discrete graph diffusion to generate molecular graphs from instructions. UTGDiff features a unified text-graph transformer as the denoising network, derived from pre-trained language models and minimally modified to process graph data through attention bias. Our experimental results demonstrate that UTGDiff consistently outperforms sequence-based baselines in tasks involving instruction-based molecule generation and editing, achieving superior performance with fewer parameters given an equivalent level of pretraining corpus. Our code is availble at https://github.com/ran1812/UTGDiff.||
|**2024-08-19**|[A Population-to-individual Tuning Framework for Adapting Pretrained LM to On-device User Intent Prediction](http://arxiv.org/abs/2408.09815)|**[link](https://github.com/tsinghua-fib-lab/LLM-for-User-Intent)**|Mobile devices, especially smartphones, can support rich functions and have developed into indispensable tools in daily life. With the rise of generative AI services, smartphones can potentially transform into personalized assistants, anticipating user needs and scheduling services accordingly. Predicting user intents on smartphones, and reflecting anticipated activities based on past interactions and context, remains a pivotal step towards this vision. Existing research predominantly focuses on specific domains, neglecting the challenge of modeling diverse event sequences across dynamic contexts. Leveraging pre-trained language models (PLMs) offers a promising avenue, yet adapting PLMs to on-device user intent prediction presents significant challenges. To address these challenges, we propose PITuning, a Population-to-Individual Tuning framework. PITuning enhances common pattern extraction through dynamic event-to-intent transition modeling and addresses long-tailed preferences via adaptive unlearning strategies. Experimental results on real-world datasets demonstrate PITuning's superior intent prediction performance, highlighting its ability to capture long-tailed preferences and its practicality for on-device prediction scenarios.||
|**2024-08-19**|[Refining Packing and Shuffling Strategies for Enhanced Performance in Generative Language Models](http://arxiv.org/abs/2408.09621)|null|Packing and shuffling tokens is a common practice in training auto-regressive language models (LMs) to prevent overfitting and improve efficiency. Typically documents are concatenated to chunks of maximum sequence length (MSL) and then shuffled. However setting the atom size, the length for each data chunk accompanied by random shuffling, to MSL may lead to contextual incoherence due to tokens from different documents being packed into the same chunk. An alternative approach is to utilize padding, another common data packing strategy, to avoid contextual incoherence by only including one document in each shuffled chunk. To optimize both packing strategies (concatenation vs padding), we investigated the optimal atom size for shuffling and compared their performance and efficiency. We found that matching atom size to MSL optimizes performance for both packing methods (concatenation and padding), and padding yields lower final perplexity (higher performance) than concatenation at the cost of more training steps and lower compute efficiency. This trade-off informs the choice of packing methods in training language models.||
|**2024-08-16**|[Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models](http://arxiv.org/abs/2408.09053)|null|Parameter-efficient fine-tuning (PEFT) methods are increasingly used with pre-trained language models (PLMs) for continual learning (CL). These methods involve training a PEFT module for each new task and using similarity-based selection to route modules during inference. However, they face two major limitations: 1) interference with already learned modules and 2) suboptimal routing when composing modules. In this paper, we introduce a method that isolates the training of PEFT modules for task specialization. Then, before evaluation, it learns to compose the previously learned modules by training a router that leverages samples from a small memory. We evaluate our method in two CL setups using several benchmarks. Our results show that our method provides a better composition of PEFT modules, leading to better generalization and performance compared to previous methods.||
|**2024-08-16**|[Enhancing Discriminative Tasks by Guiding the Pre-trained Language Model with Large Language Model's Experience](http://arxiv.org/abs/2408.08553)|null|Large Language Models (LLMs) and pre-trained Language Models (LMs) have achieved impressive success on many software engineering tasks (e.g., code completion and code generation). By leveraging huge existing code corpora (e.g., GitHub), these models aim to understand the patterns in source code and use these patterns to predict code properties. However, fine-tuning LLMs is time-consuming and costly for end users and small organizations. Furthermore, fine-tuning LMs heavily depends on the amount and quality of datasets available. As a result, the current lack of data and the high cost of collecting it in real-world scenarios further limit the applicability of LMs. In this paper, we leverage the powerful generation capabilities of LLMs to enhance pre-trained LMs. Specifically, we use LLMs to generate domain-specific data, thereby improving the performance of pre-trained LMs on the target tasks. We conduct experiments by combining different LLMs in our generation phase and introducing various LMs to learn from the LLM-generated data. Then, we compare the performance of these LMs before and after learning the data. We find that LLM-generated data significantly enhances the performance of LMs. The improvement can reach up to 58.36% for fault localization and up to 6.09% for clone detection. Our study highlights that using LLMs to generate data for LMs can improve performance by a large margin.||
|**2024-08-12**|[Unleash The Power of Pre-Trained Language Models for Irregularly Sampled Time Series](http://arxiv.org/abs/2408.08328)|null|Pre-trained Language Models (PLMs), such as ChatGPT, have significantly advanced the field of natural language processing. This progress has inspired a series of innovative studies that explore the adaptation of PLMs to time series analysis, intending to create a unified foundation model that addresses various time series analytical tasks. However, these efforts predominantly focus on Regularly Sampled Time Series (RSTS), neglecting the unique challenges posed by Irregularly Sampled Time Series (ISTS), which are characterized by non-uniform sampling intervals and prevalent missing data. To bridge this gap, this work explores the potential of PLMs for ISTS analysis. We begin by investigating the effect of various methods for representing ISTS, aiming to maximize the efficacy of PLMs in this under-explored area. Furthermore, we present a unified PLM-based framework, ISTS-PLM, which integrates time-aware and variable-aware PLMs tailored for comprehensive intra and inter-time series modeling and includes a learnable input embedding layer and a task-specific output layer to tackle diverse ISTS analytical tasks. Extensive experiments on a comprehensive benchmark demonstrate that the ISTS-PLM, utilizing a simple yet effective series-based representation for ISTS, consistently achieves state-of-the-art performance across various analytical tasks, such as classification, interpolation, and extrapolation, as well as few-shot and zero-shot learning scenarios, spanning scientific domains like healthcare and biomechanics.||
|**2024-08-13**|[SceneGPT: A Language Model for 3D Scene Understanding](http://arxiv.org/abs/2408.06926)|null|Building models that can understand and reason about 3D scenes is difficult owing to the lack of data sources for 3D supervised training and large-scale training regimes. In this work we ask - How can the knowledge in a pre-trained language model be leveraged for 3D scene understanding without any 3D pre-training. The aim of this work is to establish whether pre-trained LLMs possess priors/knowledge required for reasoning in 3D space and how can we prompt them such that they can be used for general purpose spatial reasoning and object understanding in 3D. To this end, we present SceneGPT, an LLM based scene understanding system which can perform 3D spatial reasoning without training or explicit 3D supervision. The key components of our framework are - 1) a 3D scene graph, that serves as scene representation, encoding the objects in the scene and their spatial relationships 2) a pre-trained LLM that can be adapted with in context learning for 3D spatial reasoning. We evaluate our framework qualitatively on object and scene understanding tasks including object semantics, physical properties and affordances (object-level) and spatial understanding (scene-level).||
|**2024-08-12**|[LipidBERT: A Lipid Language Model Pre-trained on METiS de novo Lipid Library](http://arxiv.org/abs/2408.06150)|null|In this study, we generate and maintain a database of 10 million virtual lipids through METiS's in-house de novo lipid generation algorithms and lipid virtual screening techniques. These virtual lipids serve as a corpus for pre-training, lipid representation learning, and downstream task knowledge transfer, culminating in state-of-the-art LNP property prediction performance. We propose LipidBERT, a BERT-like model pre-trained with the Masked Language Model (MLM) and various secondary tasks. Additionally, we compare the performance of embeddings generated by LipidBERT and PhatGPT, our GPT-like lipid generation model, on downstream tasks. The proposed bilingual LipidBERT model operates in two languages: the language of ionizable lipid pre-training, using in-house dry-lab lipid structures, and the language of LNP fine-tuning, utilizing in-house LNP wet-lab data. This dual capability positions LipidBERT as a key AI-based filter for future screening tasks, including new versions of METiS de novo lipid libraries and, more importantly, candidates for in vivo testing for orgran-targeting LNPs. To the best of our knowledge, this is the first successful demonstration of the capability of a pre-trained language model on virtual lipids and its effectiveness in downstream tasks using web-lab data. This work showcases the clever utilization of METiS's in-house de novo lipid library as well as the power of dry-wet lab integration.||
|**2024-08-12**|[AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine Advertising](http://arxiv.org/abs/2408.05906)|**[link](https://github.com/cyberagentailab/adtec)**|With the increase in the more fluent ad texts automatically created by natural language generation technology, it is in the high demand to verify the quality of these creatives in a real-world setting. We propose AdTEC, the first public benchmark to evaluate ad texts in multiple aspects from the perspective of practical advertising operations. Our contributions are: (i) Defining five tasks for evaluating the quality of ad texts and building a dataset based on the actual operational experience of advertising agencies, which is typically kept in-house. (ii) Validating the performance of existing pre-trained language models (PLMs) and human evaluators on the dataset. (iii) Analyzing the characteristics and providing challenges of the benchmark. The results show that while PLMs have already reached the practical usage level in several tasks, human still outperforms in certain domains, implying that there is significant room for improvement in such area.||
|**2024-08-09**|[A Psychology-based Unified Dynamic Framework for Curriculum Learning](http://arxiv.org/abs/2408.05326)|null|Directly learning from examples of random difficulty levels is often challenging for both humans and machine learning models. A more effective strategy involves exposing learners to examples in a progressive order, from easy to difficult. Curriculum Learning (CL) has been proposed to implement this strategy in machine learning model training. However, two key challenges persist in CL framework design: defining the difficulty of training data and determining the appropriate amount of data to input at each training step. This paper presents a Psychology-based Unified Dynamic Framework for Curriculum Learning (PUDF), drawing inspiration from psychometrics. We quantify the difficulty of training data by applying Item Response Theory (IRT) to responses from Artificial Crowds (AC). This theory-driven IRT-AC approach leads to global (i.e., model-independent) and interpretable difficulty values. Leveraging IRT, we propose a Dynamic Data Selection via Model Ability Estimation (DDS-MAE) strategy to schedule the appropriate amount of data during model training. Since our difficulty labeling and model ability estimation are based on a consistent theory, namely IRT, their values are comparable within the same scope, potentially leading to a faster convergence compared to the other CL methods. Experimental results demonstrate that fine-tuning pre-trained language models with PUDF enhances their performance on the GLUE benchmark. Moreover, PUDF surpasses other state-of-the-art (SOTA) CL methods on the GLUE benchmark. We further explore the components of PUDF, namely the difficulty measurer (IRT-AC) and the training scheduler (DDS-MAE) qualitatively and quantitatively. Lastly, we conduct an ablation study to clarify which components of PUDF contribute to faster convergence and higher accuracy.||
|**2024-07-30**|[Affective Computing in the Era of Large Language Models: A Survey from the NLP Perspective](http://arxiv.org/abs/2408.04638)|null|Affective Computing (AC), integrating computer science, psychology, and cognitive science knowledge, aims to enable machines to recognize, interpret, and simulate human emotions.To create more value, AC can be applied to diverse scenarios, including social media, finance, healthcare, education, etc. Affective Computing (AC) includes two mainstream tasks, i.e., Affective Understanding (AU) and Affective Generation (AG). Fine-tuning Pre-trained Language Models (PLMs) for AU tasks has succeeded considerably. However, these models lack generalization ability, requiring specialized models for specific tasks. Additionally, traditional PLMs face challenges in AG, particularly in generating diverse and emotionally rich responses. The emergence of Large Language Models (LLMs), such as the ChatGPT series and LLaMA models, brings new opportunities and challenges, catalyzing a paradigm shift in AC. LLMs possess capabilities of in-context learning, common sense reasoning, and advanced sequence generation, which present unprecedented opportunities for AU. To provide a comprehensive overview of AC in the LLMs era from an NLP perspective, we summarize the development of LLMs research in this field, aiming to offer new insights. Specifically, we first summarize the traditional tasks related to AC and introduce the preliminary study based on LLMs. Subsequently, we outline the relevant techniques of popular LLMs to improve AC tasks, including Instruction Tuning and Prompt Engineering. For Instruction Tuning, we discuss full parameter fine-tuning and parameter-efficient methods such as LoRA, P-Tuning, and Prompt Tuning. In Prompt Engineering, we examine Zero-shot, Few-shot, Chain of Thought (CoT), and Agent-based methods for AU and AG. To clearly understand the performance of LLMs on different Affective Computing tasks, we further summarize the existing benchmarks and evaluation methods.||
|**2024-08-07**|[Is Child-Directed Speech Effective Training Data for Language Models?](http://arxiv.org/abs/2408.03617)|null|While high-performing language models are typically trained on hundreds of billions of words, human children become fluent language users with a much smaller amount of data. What are the features of the data they receive, and how do these features support language modeling objectives? To investigate this question, we train GPT-2 models on 29M words of English-language child-directed speech and a new matched, synthetic dataset (TinyDialogues), comparing to a heterogeneous blend of datasets from the BabyLM challenge. We evaluate both the syntactic and semantic knowledge of these models using developmentally-inspired evaluations. Through pretraining experiments, we test whether the global developmental ordering or the local discourse ordering of children's training data support high performance relative to other datasets. The local properties of the data affect model results, but somewhat surprisingly, global properties do not. Further, child language input is not uniquely valuable for training language models. These findings support the hypothesis that, rather than proceeding from better data, children's learning is instead substantially more efficient than current language modeling techniques.||
|**2024-08-02**|[Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer](http://arxiv.org/abs/2408.01402)|null|Decision Transformer (DT) has emerged as a promising class of algorithms in offline reinforcement learning (RL) tasks, leveraging pre-collected datasets and Transformer's capability to model long sequences. Recent works have demonstrated that using parts of trajectories from training tasks as prompts in DT enhances its performance on unseen tasks, giving rise to Prompt-DT methods. However, collecting data from specific environments can be both costly and unsafe in many scenarios, leading to suboptimal performance and limited few-shot prompt abilities due to the data-hungry nature of Transformer-based models. Additionally, the limited datasets used in pre-training make it challenging for Prompt-DT type of methods to distinguish between various RL tasks through prompts alone. To address these challenges, we introduce the Language model-initialized Prompt Decision Transformer (LPDT), which leverages pre-trained language models for meta-RL tasks and fine-tunes the model using Low-rank Adaptation (LoRA). We further incorporate prompt regularization to effectively differentiate between tasks based on prompt feature representations. Our approach integrates pre-trained language model and RL tasks seamlessly. Extensive empirical studies demonstrate that initializing with a pre-trained language model significantly enhances the performance of Prompt-DT on unseen tasks compared to baseline methods.||
|**2024-08-02**|[Reconsidering Token Embeddings with the Definitions for Pre-trained Language Models](http://arxiv.org/abs/2408.01308)|null|Learning token embeddings based on token co-occurrence statistics has proven effective for both pre-training and fine-tuning in natural language processing. However, recent studies have pointed out the distribution of learned embeddings degenerates into anisotropy, and even pre-trained language models (PLMs) suffer from a loss of semantics-related information in embeddings for low-frequency tokens. This study first analyzes fine-tuning dynamics of a PLM, BART-large, and demonstrates its robustness against degeneration. On the basis of this finding, we propose DefinitionEMB, a method that utilizes definitions to construct isotropically distributed and semantics-related token embeddings for PLMs while maintaining original robustness during fine-tuning. Our experiments demonstrate the effectiveness of leveraging definitions from Wiktionary to construct such embeddings for RoBERTa-base and BART-large. Furthermore, the constructed embeddings for low-frequency tokens improve the performance of these models across various GLUE and four text summarization datasets.||
|**2024-07-31**|[ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget](http://arxiv.org/abs/2408.00103)|**[link](https://github.com/SapienzaNLP/relik)**|Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in Natural Language Processing, serving as critical components in a wide range of applications. In this paper, we propose ReLiK, a Retriever-Reader architecture for both EL and RE, where, given an input text, the Retriever module undertakes the identification of candidate entities or relations that could potentially appear within the text. Subsequently, the Reader module is tasked to discern the pertinent retrieved entities or relations and establish their alignment with the corresponding textual spans. Notably, we put forward an innovative input representation that incorporates the candidate entities or relations alongside the text, making it possible to link entities or extract relations in a single forward pass and to fully leverage pre-trained language models contextualization capabilities, in contrast with previous Retriever-Reader-based methods, which require a forward pass for each candidate. Our formulation of EL and RE achieves state-of-the-art performance in both in-domain and out-of-domain benchmarks while using academic budget training and with up to 40x inference speed compared to competitors. Finally, we show how our architecture can be used seamlessly for Information Extraction (cIE), i.e. EL + RE, and setting a new state of the art by employing a shared Reader that simultaneously extracts entities and relations.||
|**2024-07-31**|[Large Language Monkeys: Scaling Inference Compute with Repeated Sampling](http://arxiv.org/abs/2407.21787)|null|Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit the amount of compute to only one attempt per problem. Here, we explore inference compute as another axis for scaling by increasing the number of generated samples. Across multiple tasks and models, we observe that coverage - the fraction of problems solved by any attempt - scales with the number of samples over four orders of magnitude. In domains like coding and formal proofs, where all answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-attempt state-of-the-art of 43% which uses more capable frontier models. Moreover, using current API pricing, amplifying the cheaper DeepSeek model with five samples is more cost-effective and solves more issues than paying a premium for one sample from GPT-4o or Claude 3.5 Sonnet. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. Finally, we find that identifying correct samples out of many generations remains an important direction for future research in domains without automatic verifiers. When solving math word problems from GSM8K and MATH, coverage with Llama-3 models grows to over 95% with 10,000 samples. However, common methods to pick correct solutions from a sample collection, such as majority voting or reward models, plateau beyond several hundred samples and fail to fully scale with the sample budget.||
|**2024-07-31**|[Learning Effective Representations for Retrieval Using Self-Distillation with Adaptive Relevance Margins](http://arxiv.org/abs/2407.21515)|null|Representation-based retrieval models, so-called biencoders, estimate the relevance of a document to a query by calculating the similarity of their respective embeddings. Current state-of-the-art biencoders are trained using an expensive training regime involving knowledge distillation from a teacher model and batch-sampling. Instead of relying on a teacher model, we contribute a novel parameter-free loss function for self-supervision that exploits the pre-trained language modeling capabilities of the encoder model as a training signal, eliminating the need for batch sampling by performing implicit hard negative mining. We investigate the capabilities of our proposed approach through extensive ablation studies, demonstrating that self-distillation can match the effectiveness of teacher distillation using only 13.5% of the data, while offering a speedup in training time between 3x and 15x compared to parametrized losses. Code and data is made openly available.||
|**2024-07-23**|[Table-Filling via Mean Teacher for Cross-domain Aspect Sentiment Triplet Extraction](http://arxiv.org/abs/2407.21052)|null|Cross-domain Aspect Sentiment Triplet Extraction (ASTE) aims to extract fine-grained sentiment elements from target domain sentences by leveraging the knowledge acquired from the source domain. Due to the absence of labeled data in the target domain, recent studies tend to rely on pre-trained language models to generate large amounts of synthetic data for training purposes. However, these approaches entail additional computational costs associated with the generation process. Different from them, we discover a striking resemblance between table-filling methods in ASTE and two-stage Object Detection (OD) in computer vision, which inspires us to revisit the cross-domain ASTE task and approach it from an OD standpoint. This allows the model to benefit from the OD extraction paradigm and region-level alignment. Building upon this premise, we propose a novel method named \textbf{T}able-\textbf{F}illing via \textbf{M}ean \textbf{T}eacher (TFMT). Specifically, the table-filling methods encode the sentence into a 2D table to detect word relations, while TFMT treats the table as a feature map and utilizes a region consistency to enhance the quality of those generated pseudo labels. Additionally, considering the existence of the domain gap, a cross-domain consistency based on Maximum Mean Discrepancy is designed to alleviate domain shift problems. Our method achieves state-of-the-art performance with minimal parameters and computational costs, making it a strong baseline for cross-domain ASTE.||
|**2024-07-28**|[Motamot: A Dataset for Revealing the Supremacy of Large Language Models over Transformer Models in Bengali Political Sentiment Analysis](http://arxiv.org/abs/2407.19528)|**[link](https://github.com/Mukaffi28/Bengali-Political-Sentiment-Analysis)**|Sentiment analysis is the process of identifying and categorizing people's emotions or opinions regarding various topics. Analyzing political sentiment is critical for understanding the complexities of public opinion processes, especially during election seasons. It gives significant information on voter preferences, attitudes, and current trends. In this study, we investigate political sentiment analysis during Bangladeshi elections, specifically examining how effectively Pre-trained Language Models (PLMs) and Large Language Models (LLMs) capture complex sentiment characteristics. Our study centers on the creation of the "Motamot" dataset, comprising 7,058 instances annotated with positive and negative sentiments, sourced from diverse online newspaper portals, forming a comprehensive resource for political sentiment analysis. We meticulously evaluate the performance of various PLMs including BanglaBERT, Bangla BERT Base, XLM-RoBERTa, mBERT, and sahajBERT, alongside LLMs such as Gemini 1.5 Pro and GPT 3.5 Turbo. Moreover, we explore zero-shot and few-shot learning strategies to enhance our understanding of political sentiment analysis methodologies. Our findings underscore BanglaBERT's commendable accuracy of 88.10% among PLMs. However, the exploration into LLMs reveals even more promising results. Through the adept application of Few-Shot learning techniques, Gemini 1.5 Pro achieves an impressive accuracy of 96.33%, surpassing the remarkable performance of GPT 3.5 Turbo, which stands at 94%. This underscores Gemini 1.5 Pro's status as the superior performer in this comparison.||
|**2024-07-26**|[Multi-turn Response Selection with Commonsense-enhanced Language Models](http://arxiv.org/abs/2407.18479)|null|As a branch of advanced artificial intelligence, dialogue systems are prospering. Multi-turn response selection is a general research problem in dialogue systems. With the assistance of background information and pre-trained language models, the performance of state-of-the-art methods on this problem gains impressive improvement. However, existing studies neglect the importance of external commonsense knowledge. Hence, we design a Siamese network where a pre-trained Language model merges with a Graph neural network (SinLG). SinLG takes advantage of Pre-trained Language Models (PLMs) to catch the word correlations in the context and response candidates and utilizes a Graph Neural Network (GNN) to reason helpful common sense from an external knowledge graph. The GNN aims to assist the PLM in fine-tuning, and arousing its related memories to attain better performance. Specifically, we first extract related concepts as nodes from an external knowledge graph to construct a subgraph with the context response pair as a super node for each sample. Next, we learn two representations for the context response pair via both the PLM and GNN. A similarity loss between the two representations is utilized to transfer the commonsense knowledge from the GNN to the PLM. Then only the PLM is used to infer online so that efficiency can be guaranteed. Finally, we conduct extensive experiments on two variants of the PERSONA-CHAT dataset, which proves that our solution can not only improve the performance of the PLM but also achieve an efficient inference.||
|**2024-07-23**|[TLCR: Token-Level Continuous Reward for Fine-grained Reinforcement Learning from Human Feedback](http://arxiv.org/abs/2407.16574)|null|Reinforcement Learning from Human Feedback (RLHF) leverages human preference data to train language models to align more closely with human essence. These human preference data, however, are labeled at the sequence level, creating a mismatch between sequence-level preference labels and tokens, which are autoregressively generated from the language model. Although several recent approaches have tried to provide token-level (i.e., dense) rewards for each individual token, these typically rely on predefined discrete reward values (e.g., positive: +1, negative: -1, neutral: 0), failing to account for varying degrees of preference inherent to each token. To address this limitation, we introduce TLCR (Token-Level Continuous Reward) for RLHF, which incorporates a discriminator trained to distinguish positive and negative tokens, and the confidence of the discriminator is used to assign continuous rewards to each token considering the context. Extensive experiments show that our proposed TLCR leads to consistent performance improvements over previous sequence-level or token-level discrete rewards on open-ended generation benchmarks.||
|**2024-07-22**|[Multilingual Fine-Grained News Headline Hallucination Detection](http://arxiv.org/abs/2407.15975)|null|The popularity of automated news headline generation has surged with advancements in pre-trained language models. However, these models often suffer from the ``hallucination'' problem, where the generated headline is not fully supported by its source article. Efforts to address this issue have predominantly focused on English, using over-simplistic classification schemes that overlook nuanced hallucination types. In this study, we introduce the first multilingual, fine-grained news headline hallucination detection dataset that contains over 11 thousand pairs in 5 languages, each annotated with detailed hallucination types by experts. We conduct extensive experiments on this dataset under two settings. First, we implement several supervised fine-tuning approaches as preparatory solutions and demonstrate this dataset's challenges and utilities. Second, we test various large language models' in-context learning abilities and propose two novel techniques, language-dependent demonstration selection and coarse-to-fine prompting, to boost the few-shot hallucination detection performance in terms of the example-F1 metric. We release this dataset to foster further research in multilingual, fine-grained headline hallucination detection.||
|**2024-07-20**|[Seal: Advancing Speech Language Models to be Few-Shot Learners](http://arxiv.org/abs/2407.14875)|null|Existing auto-regressive language models have demonstrated a remarkable capability to perform a new task with just a few examples in prompt, without requiring any additional training. In order to extend this capability to a multi-modal setting (i.e. speech and language), this paper introduces the Seal model, an abbreviation for speech language model. It incorporates a novel alignment method, in which Kullback-Leibler divergence loss is performed to train a projector that bridges a frozen speech encoder with a frozen language model decoder. The resulting Seal model exhibits robust performance as a few-shot learner on two speech understanding tasks. Additionally, consistency experiments are conducted to validate its robustness on different pre-trained language models.||
|**2024-07-18**|[LIMT: Language-Informed Multi-Task Visual World Models](http://arxiv.org/abs/2407.13466)|null|Most recent successes in robot reinforcement learning involve learning a specialized single-task agent.   However, robots capable of performing multiple tasks can be much more valuable in real-world applications.   Multi-task reinforcement learning can be very challenging due to the increased sample complexity and the potentially conflicting task objectives.   Previous work on this topic is dominated by model-free approaches.   The latter can be very sample inefficient even when learning specialized single-task agents.   In this work, we focus on model-based multi-task reinforcement learning.   We propose a method for learning multi-task visual world models, leveraging pre-trained language models to extract semantically meaningful task representations.   These representations are used by the world model and policy to reason about task similarity in dynamics and behavior.   Our results highlight the benefits of using language-driven task representations for world models and a clear advantage of model-based multi-task learning over the more common model-free paradigm.||
|**2024-07-18**|[AlcLaM: Arabic Dialectal Language Model](http://arxiv.org/abs/2407.13097)|**[link](https://github.com/amurtadha/alclam)**|Pre-trained Language Models (PLMs) are integral to many modern natural language processing (NLP) systems. Although multilingual models cover a wide range of languages, they often grapple with challenges like high inference costs and a lack of diverse non-English training data. Arabic-specific PLMs are trained predominantly on modern standard Arabic, which compromises their performance on regional dialects. To tackle this, we construct an Arabic dialectal corpus comprising 3.4M sentences gathered from social media platforms. We utilize this corpus to expand the vocabulary and retrain a BERT-based model from scratch. Named AlcLaM, our model was trained using only 13 GB of text, which represents a fraction of the data used by existing models such as CAMeL, MARBERT, and ArBERT, compared to 7.8%, 10.2%, and 21.3%, respectively. Remarkably, AlcLaM demonstrates superior performance on a variety of Arabic NLP tasks despite the limited training data. AlcLaM is available at GitHub https://github.com/amurtadha/Alclam and HuggingFace https://huggingface.co/rahbi.||
|**2024-07-23**|[Subgraph-Aware Training of Text-based Methods for Knowledge Graph Completion](http://arxiv.org/abs/2407.12703)|null|近年来，对预训练语言模型（PLM）进行微调已显示出改进知识图谱补全（KGC）的潜力。然而，大多数基于PLM的方法仅编码文本信息，而忽略了知识图谱（KG）的各种拓扑结构。在本文中，我们通过经验证实了KG的结构特性与基于PLM的方法的性能之间存在显著关系。为了利用结构化知识，我们提出了一种用于KGC的子图感知训练框架（SATKGC），它结合了（i）子图感知的mini-batching，以鼓励困难负采样，以及（ii）一种新的对比学习方法，以便在结构特性的基础上更多地关注更困难的实体和更困难的负三元组。据我们所知，这是第一个将子图的结构归纳偏差全面纳入微调PLM的研究。在四个KGC基准数据集上的大量实验结果证明了SATKGC的优越性。我们的代码已公开。||
|**2024-07-13**|[Minimizing PLM-Based Few-Shot Intent Detectors](http://arxiv.org/abs/2407.09943)|**[link](https://github.com/hdzhang-code/smallID)**|近期的研究表明，基于预训练语言模型 (PLM) 并使用有限的标记数据训练高效的意图检测器是可行的。然而，由于其庞大的规模，在资源受限的环境（如移动设备）中部署这些检测器带来了挑战。在这项工作中，我们旨在通过探索各种技术来解决这个问题，以最小化使用少量样本数据训练的基于 PLM 的意图检测器的规模。具体来说，我们利用大型语言模型 (LLM) 进行数据增强，采用先进的模型压缩方法进行知识蒸馏，并设计了一种称为 V-Prune 的词汇剪枝机制。通过这些方法，我们成功地将模型内存使用量（包括 Transformer 和词汇表）压缩了 21 倍，同时在四个真实世界基准测试中保持了几乎相同的性能水平。||
|**2024-07-12**|[Deep Bag-of-Words Model: An Efficient and Interpretable Relevance Architecture for Chinese E-Commerce](http://arxiv.org/abs/2407.09395)|null|查询和产品的文本相关性或文本匹配是电子商务搜索引擎的一项基本技术，用于确保显示的产品能够匹配查询意图。许多研究致力于提高搜索系统中相关性模型的性能。最近，像BERT这样的预训练语言模型在文本相关性任务上取得了可喜的成果。虽然这些模型在离线测试数据集上表现良好，但由于其高延迟，将预训练语言模型部署到在线系统仍然存在障碍。双塔模型因其能够协调性能和计算效率而在工业场景中得到广泛应用。遗憾的是，这种模型呈现出不透明的“黑盒”性质，这阻碍了开发人员进行专门的优化。在本文中，我们提出了一种高效且可解释的中文电商相关性架构——深度词袋（DeepBoW）模型。我们的方法建议将查询和产品编码为稀疏的词袋表示，它是一组词-权重对。权重表示对应词与原始文本之间的重要性或相关性得分。相关性得分通过查询和产品的稀疏词袋表示之间匹配词的累积来衡量。与通常存在黑盒缺点的流行的密集分布式表示相比，所提出的表示模型的最大优势是高度可解释和可干预，这对在线搜索引擎的部署和运营来说是一个优越的优势。此外，该模型的在线效率甚至优于最有效的密集表示内积形式...||
|**2024-07-12**|[One Stone, Four Birds: A Comprehensive Solution for QA System Using Supervised Contrastive Learning](http://arxiv.org/abs/2407.09011)|null|本文提出了一种新颖且全面的解决方案，通过监督对比学习 (SCL) 来增强问答 (QA) 系统的鲁棒性和效率。借助预训练语言模型，训练高性能问答系统已经变得非常简单，只需要少量数据和简单的微调。然而，尽管取得了最近的进展，现有的问答系统在功能和训练效率方面仍然存在重大缺陷。我们通过定义四个关键任务来解决功能问题：用户输入意图分类、域外输入检测、新意图发现和持续学习。然后，我们利用基于 SCL 的统一表示学习方法，有效地构建类内紧凑和类间分散的特征空间，促进已知意图分类和未知意图检测与发现。因此，只需对下游任务进行最少的额外调整，我们的方法就能显著提高模型效率，并在所有任务中实现新的最先进性能。||
|**2024-07-12**|[Domain-Hierarchy Adaptation via Chain of Iterative Reasoning for Few-shot Hierarchical Text Classification](http://arxiv.org/abs/2407.08959)|null|近年来，各种预训练语言模型（PLM）被提出，并在各种少样本任务上证明了其令人印象深刻的性能。然而，受限于PLM中非结构化先验知识，在复杂结构化场景（如层次文本分类（HTC））中难以保持一致的性能，尤其是在下游数据极其稀缺的情况下。主要的挑战是如何将PLM中非结构化的语义空间迁移到下游领域层次结构中。与之前直接执行多标签分类或使用图神经网络（GNN）注入标签层次结构的HTC工作不同，在这项工作中，我们研究了少样本设置下的HTC问题，以使PLM中的知识从非结构化的方式适应下游层次结构。从技术上讲，我们设计了一种简单有效的方法，称为层次迭代条件随机场（HierICRF），以搜索最具领域挑战性的方向，并将领域层次结构适应巧妙地构建为层次迭代语言建模问题，然后鼓励模型在推理过程中进行层次一致性自我修正，从而实现知识迁移并保持层次一致性。我们在各种架构上执行HierICRF，并在两个流行的HTC数据集上进行的大量实验表明，与之前最先进的（SOTA）基线相比，使用HierICRF的提示在少样本设置下显著提高了HTC性能，平均Micro-F1提高了28.80%到1.50%，Macro-F1提高了36.29%到1.5%，同时保持了SOTA的层次一致性性能。||
|**2024-07-11**|[Adversarial-MidiBERT: Symbolic Music Understanding Model Based on Unbias Pre-training and Mask Fine-tuning](http://arxiv.org/abs/2407.08306)|**[link](https://github.com/RS2002/Adversarial-MidiBERT)**|作为音乐信息检索 (MIR) 的重要组成部分，符号音乐理解 (SMU)  受到了广泛关注，因为它可以帮助音乐家和业余爱好者学习和创作音乐。近年来，预训练语言模型在 SMU 中得到广泛应用，因为符号音乐与自然语言具有高度相似性，并且预训练方式还有助于充分利用有限的音乐数据。然而，在预训练语言模型中观察到性别歧视、年龄歧视和种族主义等偏见问题，这归因于训练数据分布不均衡。它也对下游任务的性能产生重大影响，这在 SMU 中也同样存在。为了应对这一挑战，我们提出了 Adversarial-MidiBERT，这是一种基于 Transformer 的双向编码器表示 (BERT) 的符号音乐理解模型。我们引入了一种基于对抗学习的无偏见预训练方法，以最大程度地减少训练期间导致偏见的标记的参与。此外，我们提出了一种掩码微调方法来缩小预训练和微调之间的数据差距，这可以帮助模型更快地收敛并获得更好的性能。我们在四个音乐理解任务上评估了我们的方法，我们的方法在所有任务中都表现出色。我们模型的代码在 https://github.com/RS2002/Adversarial-MidiBERT 上公开可用。||
|**2024-07-10**|[Deconstructing What Makes a Good Optimizer for Language Models](http://arxiv.org/abs/2407.07972)|null|随着语言模型规模的扩大，训练成本越来越高，这促使人们不断尝试提高优化效率。尽管付出了这些努力，但 Adam 优化器仍然是使用最广泛的，因为它被普遍认为是最有效的方法。我们的目标是在自回归语言建模的背景下，比较几种优化算法，包括 SGD、Adafactor、Adam 和 Lion，涵盖各种模型大小、超参数和架构变体。我们的研究结果表明，除了 SGD 之外，这些算法在最佳性能和面对各种超参数选择时的表现都相当。我们的结果向实践者表明，优化器的选择可以根据实际情况来决定，例如内存限制和实现的难易程度，因为没有一种算法在性能或对超参数错误指定的稳定性方面成为明显的赢家。鉴于我们的发现，我们进一步剖析了这些方法，研究了 Adam 的两个简化版本：a）符号动量（Signum），我们发现它恢复了 Adam 的性能和超参数稳定性；b）Adalayer，我们引入的 Adam 的分层变体，用于研究 Adam 的预处理。对 Adalayer 的研究使我们得出结论，Adam 预处理的最大影响仅限于最后一层和 LayerNorm 参数，而且，也许令人惊讶的是，其余层可以使用 SGD 进行训练。||
|**2024-07-09**|[NoisyAG-News: A Benchmark for Addressing Instance-Dependent Noise in Text Classification](http://arxiv.org/abs/2407.06579)|null|现有的噪声标签学习研究主要集中于合成标签噪声。尽管合成噪声具有明确定义的结构特性，但它往往无法准确地复制现实世界中的噪声模式。近年来，人们致力于构建用于图像分类的通用且可控的实例依赖噪声数据集，这显著推动了该领域抗噪声学习的发展。然而，针对文本分类的噪声标签学习研究仍然很少。为了更好地理解现实世界文本分类环境中的标签噪声，我们通过人工标注构建了基准数据集 NoisyAG-News。首先，我们分析了标注数据，以收集关于现实世界噪声的观察结果。我们定性和定量地证明了现实世界的噪声标签遵循实例依赖模式。随后，我们使用预训练语言模型和噪声处理技术，对 NoisyAG-News 及其相应的合成噪声数据集进行了全面的学习实验。我们的研究结果表明，虽然预训练模型对合成噪声具有鲁棒性，但它们在面对实例依赖噪声时表现不佳，不同混淆程度的样本在训练和测试过程中表现出不一致的性能。这些现实世界的噪声模式带来了新的、重大的挑战，促使人们重新评估噪声标签处理方法。我们希望 NoisyAG-News 将促进未来噪声标签学习解决方案的开发和评估。||
|**2024-07-09**|[Enhancing Low-Resource NMT with a Multilingual Encoder and Knowledge Distillation: A Case Study](http://arxiv.org/abs/2407.06538)|null|神经机器翻译（NMT）仍然是一项艰巨的挑战，尤其是在处理低资源语言时。预训练的序列到序列（seq2seq）多语言模型，例如 mBART-50，已在各种低资源 NMT 任务中表现出令人印象深刻的性能。然而，它们的预训练仅限于 50 种语言，不支持许多低资源语言，尤其是印度次大陆的语言。扩展 mBART-50 的语言支持需要复杂的预训练，由于灾难性遗忘，可能会导致性能下降。考虑到这些不断扩大的挑战，本文探索了一个框架，该框架利用预训练语言模型的优势以及 seq2seq 架构中的知识蒸馏来促进低资源语言的翻译，包括 mBART-50 未涵盖的语言。所提出的框架采用基于多语言编码器的 seq2seq 模型作为基础架构，并随后使用互补的知识蒸馏技术来减轻不平衡训练的影响。我们的框架在四个印度语到印度语方向上的三种低资源印度语上进行了评估，与基线相比，BLEU-4 和 chrF 得到了显着提高。此外，我们进行人工评估以确认我们方法的有效性。我们的代码可在 https://github.com/raypretam/Two-step-low-res-NMT 公开获取。||
|**2024-07-07**|[Advancing Prompt Recovery in NLP: A Deep Dive into the Integration of Gemma-2b-it and Phi2 Models](http://arxiv.org/abs/2407.05233)|null|Prompt recovery, a crucial task in natural language processing, entails the reconstruction of prompts or instructions that language models use to convert input text into a specific output. Although pivotal, the design and effectiveness of prompts represent a challenging and relatively untapped field within NLP research. This paper delves into an exhaustive investigation of prompt recovery methodologies, employing a spectrum of pre-trained language models and strategies. Our study is a comparative analysis aimed at gauging the efficacy of various models on a benchmark dataset, with the goal of pinpointing the most proficient approach for prompt recovery. Through meticulous experimentation and detailed analysis, we elucidate the outstanding performance of the Gemma-2b-it + Phi2 model + Pretrain. This model surpasses its counterparts, showcasing its exceptional capability in accurately reconstructing prompts for text transformation tasks. Our findings offer a significant contribution to the existing knowledge on prompt recovery, shedding light on the intricacies of prompt design and offering insightful perspectives for future innovations in text rewriting and the broader field of natural language processing.||
|**2024-07-06**|[The Solution for the AIGC Inference Performance Optimization Competition](http://arxiv.org/abs/2407.04991)|null|近年来，基于Transformer架构的大规模预训练语言模型的快速发展为自然语言处理任务带来了革命性的变化。其中，ChatGPT因其具备人类水平的对话能力而广受欢迎，到2022年底已吸引超过1亿月活跃用户。与此同时，百度对其文心模型的商业化部署也通过人工智能驱动技术显著提升了营销效果。本文重点研究如何优化文心模型的高性能推理，着重于GPU加速和利用Paddle推理框架。我们采用了一系列技术，例如Faster Transformer以实现高效的模型处理，嵌入层剪枝以减少计算开销，以及FP16半精度推理以提高计算效率。此外，我们的方案还集成了高效的数据处理策略，采用多进程并行处理以最大程度地减少延迟。实验结果表明，与标准方法相比，我们优化的解决方案在推理速度上实现了高达8.96倍的提升，同时保持了具有竞争力的性能。||
|**2024-07-03**|[MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language Models](http://arxiv.org/abs/2407.02775)|null|知识蒸馏是一种有效的预训练语言模型压缩技术。虽然现有的知识蒸馏方法对于最典型的模型BERT表现良好，但它们在两个方面仍有提升空间：可以进一步探索关系级知识以提高模型性能；学生注意力头的设置可以更加灵活，以减少推理时间。因此，我们提出了一种新的知识蒸馏方法MLKD-BERT，用于在师生框架中提取多级知识。在GLUE基准测试和抽取式问答任务上的大量实验表明，我们的方法优于BERT上最先进的知识蒸馏方法。此外，MLKD-BERT可以灵活设置学生注意力头的数量，从而在性能下降很小的情况下大幅减少推理时间。||
|**2024-07-03**|[Supporting Cross-language Cross-project Bug Localization Using Pre-trained Language Models](http://arxiv.org/abs/2407.02732)|null|自动定位大型代码库中的错误对于开发人员来说仍然是一项重大挑战。现有技术由于依赖于应用程序特定的数据和庞大的模型规模，因此在通用性和部署方面常常遇到困难。本文提出了一种新颖的基于预训练语言模型 (PLM) 的错误定位技术，该技术超越了项目和语言的界限。我们的方法利用对比学习来增强错误报告和源代码的表示。然后，它利用一种结合了提交消息和代码段的新型排序方法。此外，我们引入了一种知识蒸馏技术，可以在不影响性能的情况下减小模型规模，以便实际部署。本文提出了几个关键优势。通过将代码段和提交消息分析与传统的代码文件级别检查相结合，我们的技术实现了更高的错误定位精度。此外，我们的模型在通用性方面表现出色——在来自各种项目和语言的代码上进行训练后，它可以有效地识别未见过的代码库中的错误。为了解决计算限制，我们提出了一种兼容 CPU 的解决方案。总而言之，我们提出的工作提出了一种高效、通用且高效的错误定位技术，具有现实部署的潜力。||
|**2024-07-02**|[Ensemble of pre-trained language models and data augmentation for hate speech detection from Arabic tweets](http://arxiv.org/abs/2407.02448)|null|如今，从阿拉伯语推文中识别仇恨言论引起了众多研究者的关注。为了解决这一分类任务，人们已经开发了许多系统和技术。然而，在这方面面临的两大挑战是有限的性能和数据不平衡问题。在本研究中，我们提出了一种新方法，利用集成学习和基于先前手动标记的半监督学习。我们通过将阿拉伯语推文分为5个不同的类别：非仇恨、一般仇恨、种族歧视、宗教歧视或性别歧视，在一个基准数据集上进行了实验。实验结果表明：(1) 基于预训练语言模型的集成学习优于现有的相关工作；(2) 我们提出的数据增强方法提高了阿拉伯语推文中仇恨言论检测的准确率，并优于现有的相关工作。我们的主要贡献是在阿拉伯语仇恨言论检测方面取得了令人鼓舞的结果。||
|**2024-07-02**|[Efficient Nearest Neighbor based Uncertainty Estimation for Natural Language Processing Tasks](http://arxiv.org/abs/2407.02138)|null|深度神经网络 (DNN) 包括预训练语言模型 (PLM) 中的可信预测对于现实世界中安全关键型应用至关重要。然而，DNN 经常面临不确定性估计问题，例如校准错误。特别地，需要多次随机推理的方法可以缓解这个问题，但推理成本高昂，使其不切实际。在本研究中，我们提出了 $k$近邻不确定性估计（$k$NN-UE），这是一种利用来自邻居的距离和邻居的标签存在率进行不确定性估计的方法。在情感分析、自然语言推理和命名实体识别方面的实验表明，我们提出的方法在置信度校准、选择性预测和分布外检测方面优于基线或最近基于密度的方法。此外，我们的分析表明，引入降维或受最近$k$ NN-LM 研究启发的近似最近邻搜索，可以在不显著降低估计性能的情况下，通过适当组合来减少推理开销。||
|**2024-07-01**|[Bridging the Gap: Transfer Learning from English PLMs to Malaysian English](http://arxiv.org/abs/2407.01374)|null|马来西亚英语是一种低资源的混合语，除了标准英语之外，它还包含马来语、汉语和泰米尔语的元素。命名实体识别 (NER) 模型在从马来西亚英语文本中捕获实体时表现不佳，因为它具有独特的形态句法适应、语义特征和语码转换（混合英语和马来语）。考虑到这些差距，我们引入了 MENmBERT 和 MENBERT，这是一种具有上下文理解能力的预训练语言模型，专为马来西亚英语量身定制。我们使用来自马来西亚英语新闻文章 (MEN) 数据集的手动注释实体和关系微调了 MENmBERT 和 MENBERT。这种微调过程使 PLM 能够学习表示，这些表示捕获与 NER 和 RE 任务相关的马来西亚英语的细微差别。与 bert-base-multilingual-cased 模型相比，MENmBERT 在 NER 和 RE 任务上分别实现了 1.52% 和 26.27% 的改进。尽管 NER 的整体性能没有显着提高，但我们进一步的分析表明，按 12 个实体标签进行评估时，性能有显着提高。这些发现表明，在特定语言和地理位置的语料库上预训练语言模型可能是提高低资源环境中 NER 性能的一种很有前景的方法。本文发布的数据集和代码为专注于马来西亚英语的 NLP 研究工作提供了宝贵的资源。||
|**2024-07-01**|[Language Portability Strategies for Open-domain Dialogue with Pre-trained Language Models from High to Low Resource Languages](http://arxiv.org/abs/2407.01315)|null|本文研究用于开放域对话系统的大型预训练语言模型 (PLM) 在高资源语言中的语言可移植性策略。具体来说，目标低资源语言 (L_T) 将使用法语进行模拟，因为它缺乏特定于任务的资源，并且允许我们进行人工评估，而源语言 (L_S) 为英语。出于显而易见的原因，最近使用此类模型进行开放域对话的工作大多是用英语开发的。然而，为每种可能的目标语言构建特定的 PLM 需要收集新的数据集，而且成本高昂。出于这个原因，我们希望尝试利用 L_S 和 L_T 中的所有现有资源（PLM 和数据），评估使用不同方法在 L_T 中可实现的性能。前两种方法评估了神经机器翻译 (NMT) 在不同级别的使用：TrainOnTarget，其中在 L_T 中微调之前翻译 L_S 数据集，以及 TestOnSource，其中 L_S 模型在推理过程中与 NMT 模块耦合。然后，全球第一个开放获取的多语言大型 PLM BLOOM [2] 的出现，使研究人员能够开发新的方法，旨在不仅利用模型的完全可访问性，还利用其多语言性和翻译能力。在这种情况下，首先在 L_S 中学习任务，然后使用 MAD-X 适配器架构 [16] 适应 L_T。在这两组实验中，模型在口语对话条件下与人类进行评估，并且可以根据感知的交互质量比较策略。||
|**2024-07-01**|[A Fingerprint for Large Language Models](http://arxiv.org/abs/2407.01235)|null|近期研究表明，扩展预训练语言模型可以在许多下游任务上实现最先进的性能，这使得大型语言模型（LLM）成为人工智能领域的热门研究课题。然而，由于从头开始训练LLM需要大量的资源，因此保护LLM的知识产权免遭侵权至关重要且紧迫。这促使本文作者提出了一种新颖的LLM黑盒指纹识别技术，该技术既不需要模型训练也不需要模型微调。我们首先证明LLM的输出跨越与每个模型相关的唯一向量空间。我们将所有权认证问题建模为评估受害模型空间与嫌疑模型输出空间之间相似性的任务。为了解决这个问题，我们提出了两种解决方案，其中第一个解决方案涉及验证可疑大型模型的输出是否与受害模型的输出位于相同的空间中，从而能够快速识别模型侵权；第二个解决方案重建LLM输出和受害模型的向量空间的并集，以解决受害模型遭受参数高效微调（PEFT）攻击的情况。实验结果表明，所提出的技术在所有权验证和抵御PEFT攻击方面取得了优异的性能。这项工作揭示了LLM的固有特性，并为黑盒场景下的LLM所有权验证提供了一种有前景的解决方案，确保了效率、通用性和实用性。||
|**2024-07-01**|[Development of Cognitive Intelligence in Pre-trained Language Models](http://arxiv.org/abs/2407.01047)|null|最近的研究表明，大型预训练语言模型 (PLM) 出现了认知能力。这些模型不断提高的认知一致性使其成为认知科学理论的候选者。先前对 PLM 涌现认知能力的研究很大程度上与模型训练路径无关，即侧重于最终的模型权重而不是中间步骤。然而，使用 PLM 构建合理的人类认知模型将受益于考虑其在训练期间的表现与儿童思维轨迹的发展一致性。在人类智力心理测量测试的指导下，我们选择了四组任务来研究十个流行的 PLM 家族的一致性，并评估它们可用的中间和最终训练步骤。这些任务是数字能力、语言能力、概念理解和流体推理。我们发现了一个惊人的规律：无论模型大小如何，PLM 的发展轨迹始终表现出一个与人类认知发展最大程度一致的窗口。在该窗口之前，训练似乎赋予“空白石板”模型以必要的结构，使其能够从经验中快速学习。在该窗口之后，训练似乎服务于降低损失的工程目标，而不是提高与人类认知一致性的科学目标。||
|**2024-07-01**|[Cross-Modal Attention Alignment Network with Auxiliary Text Description for zero-shot sketch-based image retrieval](http://arxiv.org/abs/2407.00979)|null|本文研究了基于零样本草图的图像检索问题 (ZS-SBIR)。先前的方法在只有类别标签甚至没有文本信息的双模态设置中解决该问题。然而，大规模预训练语言模型 (LLM) 的日益普及，展现出从网络规模数据中学习到的丰富知识，为我们提供了一个总结集体文本信息的机会。我们的主要创新在于使用文本数据作为图像的辅助信息，从而利用语言提供的固有的零样本泛化能力。为此，我们提出了一种名为“基于辅助文本描述的跨模态注意力对齐网络”的方法，用于零样本草图图像检索。该网络由三个部分组成：(i) 描述生成模块，通过使用几个疑问句提示LLM，为每个训练类别生成文本描述；(ii) 特征提取模块，包括用于草图和图像数据的两个ViT、用于提取每个训练类别句子标记的转换器；最后 (iii) 跨模态对齐模块，使用交叉注意力机制交换文本-草图和文本-图像的标记特征，并在局部和全局范围内对齐标记。在三个基准数据集上的大量实验表明，我们的方法优于最先进的 ZS-SBIR 方法。||
|**2024-06-30**|[NAIST Simultaneous Speech Translation System for IWSLT 2024](http://arxiv.org/abs/2407.00826)|null|本文描述了NAIST提交给IWSLT 2024评测活动同步赛道的系统：英语到{德语、日语、汉语}的语音到文本翻译和英语到日语的语音到语音翻译。我们开发了一种多语言端到端语音到文本翻译模型，该模型结合了两个预训练语言模型HuBERT和mBART。我们使用两种解码策略训练该模型：局部一致性（LA）和AlignAtt。提交的模型采用LA策略，因为它在之前的模型中优于AlignAtt策略。我们的语音到语音翻译方法是上述语音到文本模型与增量文本到语音（TTS）模块的级联，该模块包含音素估计模型、并行声学模型和并行WaveGAN声码器。我们通过将采用AlignAtt策略的Transformer架构应用于估计模型来改进增量TTS。结果表明，我们升级后的TTS模块有助于提高系统性能。||

## Transformer

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-08-22**|[Enhanced Parking Perception by Multi-Task Fisheye Cross-view Transformers](http://arxiv.org/abs/2408.12575)|null|Current parking area perception algorithms primarily focus on detecting vacant slots within a limited range, relying on error-prone homographic projection for both labeling and inference. However, recent advancements in Advanced Driver Assistance System (ADAS) require interaction with end-users through comprehensive and intelligent Human-Machine Interfaces (HMIs). These interfaces should present a complete perception of the parking area going from distinguishing vacant slots' entry lines to the orientation of other parked vehicles. This paper introduces Multi-Task Fisheye Cross View Transformers (MT F-CVT), which leverages features from a four-camera fisheye Surround-view Camera System (SVCS) with multihead attentions to create a detailed Bird-Eye View (BEV) grid feature map. Features are processed by both a segmentation decoder and a Polygon-Yolo based object detection decoder for parking slots and vehicles. Trained on data labeled using LiDAR, MT F-CVT positions objects within a 25m x 25m real open-road scenes with an average error of only 20 cm. Our larger model achieves an F-1 score of 0.89. Moreover the smaller model operates at 16 fps on an Nvidia Jetson Orin embedded board, with similar detection results to the larger one. MT F-CVT demonstrates robust generalization capability across different vehicles and camera rig configurations. A demo video from an unseen vehicle and camera rig is available at: https://streamable.com/jjw54x.||
|**2024-08-22**|[Transformers are Minimax Optimal Nonparametric In-Context Learners](http://arxiv.org/abs/2408.12186)|null|In-context learning (ICL) of large language models has proven to be a surprisingly effective method of learning a new task from only a few demonstrative examples. In this paper, we study the efficacy of ICL from the viewpoint of statistical learning theory. We develop approximation and generalization error bounds for a transformer composed of a deep neural network and one linear attention layer, pretrained on nonparametric regression tasks sampled from general function spaces including the Besov space and piecewise $\gamma$ -smooth class. We show that sufficiently trained transformers can achieve -- and even improve upon -- the minimax optimal estimation risk in context by encoding the most relevant basis representations during pretraining. Our analysis extends to high-dimensional or sequential data and distinguishes the \emph{pretraining} and \emph{in-context} generalization gaps. Furthermore, we establish information-theoretic lower bounds for meta-learners w.r.t. both the number of tasks and in-context examples. These findings shed light on the roles of task diversity and representation learning for ICL.||
|**2024-08-22**|[Deep Analysis of Time Series Data for Smart Grid Startup Strategies: A Transformer-LSTM-PSO Model Approach](http://arxiv.org/abs/2408.12129)|null|Grid startup, an integral component of the power system, holds strategic importance for ensuring the reliability and efficiency of the electrical grid. However, current methodologies for in-depth analysis and precise prediction of grid startup scenarios are inadequate. To address these challenges, we propose a novel method based on the Transformer-LSTM-PSO model. This model uniquely combines the Transformer's self-attention mechanism, LSTM's temporal modeling capabilities, and the parameter tuning features of the particle swarm optimization algorithm. It is designed to more effectively capture the complex temporal relationships in grid startup schemes. Our experiments demonstrate significant improvements, with our model achieving lower RMSE and MAE values across multiple datasets compared to existing benchmarks, particularly in the NYISO Electric Market dataset where the RMSE was reduced by approximately 15% and the MAE by 20% compared to conventional models. Our main contribution is the development of a Transformer-LSTM-PSO model that significantly enhances the accuracy and efficiency of smart grid startup predictions. The application of the Transformer-LSTM-PSO model represents a significant advancement in smart grid predictive analytics, concurrently fostering the development of more reliable and intelligent grid management systems.||
|**2024-08-21**|[Mixed Sparsity Training: Achieving 4 $\times$ FLOP Reduction for Transformer Pretraining](http://arxiv.org/abs/2408.11746)|null|Large language models (LLMs) have made significant strides in complex tasks, yet their widespread adoption is impeded by substantial computational demands. With hundreds of billion parameters, transformer-based LLMs necessitate months of pretraining across a high-end GPU cluster. However, this paper reveals a compelling finding: transformers exhibit considerable redundancy in pretraining computations, which motivates our proposed solution, Mixed Sparsity Training (MST), an efficient pretraining method that can reduce about $75\%$ of Floating Point Operations (FLOPs) while maintaining performance. MST integrates dynamic sparse training (DST) with Sparsity Variation (SV) and Hybrid Sparse Attention (HSA) during pretraining, involving three distinct phases: warm-up, ultra-sparsification, and restoration. The warm-up phase transforms the dense model into a sparse one, and the restoration phase reinstates connections. Throughout these phases, the model is trained with a dynamically evolving sparse topology and an HSA mechanism to maintain performance and minimize training FLOPs concurrently. Our experiment on GPT-2 showcases a FLOP reduction of $4\times$ without compromising performance.||
|**2024-08-21**|[Macformer: Transformer with Random Maclaurin Feature Attention](http://arxiv.org/abs/2408.11656)|null|Random feature attention (RFA) adopts random fourier feature (RFF) methods to approximate the softmax function, resulting in a linear time and space attention mechanism that enables the construction of an efficient Transformer. Inspired by RFA, we propose Macformer, a Transformer architecture that employs random Maclaurin features (RMF) to approximate various dot-product kernels, thereby accelerating attention computations for long sequence. Macformer consists of Random Maclaurin Feature Attention (RMFA) and pre-post Scaling Batch Normalization (ppSBN), the former is an unbiased approximation for dot-product kernelized attention and the later is a two-stage regularization mechanism guaranteeing the error of RMFA. We conducted toy experiments to demonstrate the efficiency of RMFA and ppSBN, and experiments on long range arena (LRA) benchmark to validate the acceleration and accuracy of Macformer with different dot-product kernels. Experiment results of Macformer are consistent with our theoretical analysis.||
|**2024-08-21**|[OAPT: Offset-Aware Partition Transformer for Double JPEG Artifacts Removal](http://arxiv.org/abs/2408.11480)|**[link](https://github.com/qmoq/oapt)**|Deep learning-based methods have shown remarkable performance in single JPEG artifacts removal task. However, existing methods tend to degrade on double JPEG images, which are prevalent in real-world scenarios. To address this issue, we propose Offset-Aware Partition Transformer for double JPEG artifacts removal, termed as OAPT. We conduct an analysis of double JPEG compression that results in up to four patterns within each 8x8 block and design our model to cluster the similar patterns to remedy the difficulty of restoration. Our OAPT consists of two components: compression offset predictor and image reconstructor. Specifically, the predictor estimates pixel offsets between the first and second compression, which are then utilized to divide different patterns. The reconstructor is mainly based on several Hybrid Partition Attention Blocks (HPAB), combining vanilla window-based self-attention and sparse attention for clustered pattern features. Extensive experiments demonstrate that OAPT outperforms the state-of-the-art method by more than 0.16dB in double JPEG image restoration task. Moreover, without increasing any computation cost, the pattern clustering module in HPAB can serve as a plugin to enhance other transformer-based image restoration methods. The code will be available at https://github.com/QMoQ/OAPT.git .||
|**2024-08-21**|[FATE: Focal-modulated Attention Encoder for Temperature Prediction](http://arxiv.org/abs/2408.11336)|**[link](https://github.com/tajamul21/fate)**|One of the major challenges of the twenty-first century is climate change, evidenced by rising sea levels, melting glaciers, and increased storm frequency. Accurate temperature forecasting is vital for understanding and mitigating these impacts. Traditional data-driven models often use recurrent neural networks (RNNs) but face limitations in parallelization, especially with longer sequences. To address this, we introduce a novel approach based on the FocalNet Transformer architecture. Our Focal modulation Attention Encoder (FATE) framework operates in a multi-tensor format, utilizing tensorized modulation to capture spatial and temporal nuances in meteorological data. Comparative evaluations against existing transformer encoders, 3D CNNs, LSTM, and ConvLSTM models show that FATE excels at identifying complex patterns in temperature data. Additionally, we present a new labeled dataset, the Climate Change Parameter dataset (CCPD), containing 40 years of data from Jammu and Kashmir on seven climate-related parameters. Experiments with real-world temperature datasets from the USA, Canada, and Europe show accuracy improvements of 12\%, 23\%, and 28\%, respectively, over current state-of-the-art models. Our CCPD dataset also achieved a 24\% improvement in accuracy. To support reproducible research, we have released the source code and pre-trained FATE model at \href{https://github.com/Tajamul21/FATE}{https://github.com/Tajamul21/FATE}.||
|**2024-08-21**|[HMT-UNet: A hybird Mamba-Transformer Vision UNet for Medical Image Segmentation](http://arxiv.org/abs/2408.11289)|null|In the field of medical image segmentation, models based on both CNN and Transformer have been thoroughly investigated. However, CNNs have limited modeling capabilities for long-range dependencies, making it challenging to exploit the semantic information within images fully. On the other hand, the quadratic computational complexity poses a challenge for Transformers. State Space Models (SSMs), such as Mamba, have been recognized as a promising method. They not only demonstrate superior performance in modeling long-range interactions, but also preserve a linear computational complexity. The hybrid mechanism of SSM (State Space Model) and Transformer, after meticulous design, can enhance its capability for efficient modeling of visual features. Extensive experiments have demonstrated that integrating the self-attention mechanism into the hybrid part behind the layers of Mamba's architecture can greatly improve the modeling capacity to capture long-range spatial dependencies. In this paper, leveraging the hybrid mechanism of SSM, we propose a U-shape architecture model for medical image segmentation, named Hybird Transformer vision Mamba UNet (HTM-UNet). We conduct comprehensive experiments on the ISIC17, ISIC18, CVC-300, CVC-ClinicDB, Kvasir, CVC-ColonDB, ETIS-Larib PolypDB public datasets and ZD-LCI-GIM private dataset. The results indicate that HTM-UNet exhibits competitive performance in medical image segmentation tasks. Our code is available at https://github.com/simzhangbest/HMT-Unet.||
|**2024-08-20**|[Out-of-Distribution Detection with Attention Head Masking for Multimodal Document Classification](http://arxiv.org/abs/2408.11237)|null|Detecting out-of-distribution (OOD) data is crucial in machine learning applications to mitigate the risk of model overconfidence, thereby enhancing the reliability and safety of deployed systems. The majority of existing OOD detection methods predominantly address uni-modal inputs, such as images or texts. In the context of multi-modal documents, there is a notable lack of extensive research on the performance of these methods, which have primarily been developed with a focus on computer vision tasks. We propose a novel methodology termed as attention head masking (AHM) for multi-modal OOD tasks in document classification systems. Our empirical results demonstrate that the proposed AHM method outperforms all state-of-the-art approaches and significantly decreases the false positive rate (FPR) compared to existing solutions up to 7.5\%. This methodology generalizes well to multi-modal data, such as documents, where visual and textual information are modeled under the same Transformer architecture. To address the scarcity of high-quality publicly available document datasets and encourage further research on OOD detection for documents, we introduce FinanceDocs, a new document AI dataset. Our code and dataset are publicly available.||
|**2024-08-20**|[EdgeNAT: Transformer for Efficient Edge Detection](http://arxiv.org/abs/2408.10527)|**[link](https://github.com/jhjie/edgenat)**|Transformers, renowned for their powerful feature extraction capabilities, have played an increasingly prominent role in various vision tasks. Especially, recent advancements present transformer with hierarchical structures such as Dilated Neighborhood Attention Transformer (DiNAT), demonstrating outstanding ability to efficiently capture both global and local features. However, transformers' application in edge detection has not been fully exploited. In this paper, we propose EdgeNAT, a one-stage transformer-based edge detector with DiNAT as the encoder, capable of extracting object boundaries and meaningful edges both accurately and efficiently. On the one hand, EdgeNAT captures global contextual information and detailed local cues with DiNAT, on the other hand, it enhances feature representation with a novel SCAF-MLA decoder by utilizing both inter-spatial and inter-channel relationships of feature maps. Extensive experiments on multiple datasets show that our method achieves state-of-the-art performance on both RGB and depth images. Notably, on the widely used BSDS500 dataset, our L model achieves impressive performances, with ODS F-measure and OIS F-measure of 86.0%, 87.6% for multi-scale input,and 84.9%, and 86.3% for single-scale input, surpassing the current state-of-the-art EDTER by 1.2%, 1.1%, 1.7%, and 1.6%, respectively. Moreover, as for throughput, our approach runs at 20.87 FPS on RTX 4090 GPU with single-scale input. The code for our method will be released soon.||
|**2024-08-20**|[PRformer: Pyramidal Recurrent Transformer for Multivariate Time Series Forecasting](http://arxiv.org/abs/2408.10483)|**[link](https://github.com/usualheart/prformer)**|The self-attention mechanism in Transformer architecture, invariant to sequence order, necessitates positional embeddings to encode temporal order in time series prediction. We argue that this reliance on positional embeddings restricts the Transformer's ability to effectively represent temporal sequences, particularly when employing longer lookback windows. To address this, we introduce an innovative approach that combines Pyramid RNN embeddings(PRE) for univariate time series with the Transformer's capability to model multivariate dependencies. PRE, utilizing pyramidal one-dimensional convolutional layers, constructs multiscale convolutional features that preserve temporal order. Additionally, RNNs, layered atop these features, learn multiscale time series representations sensitive to sequence order. This integration into Transformer models with attention mechanisms results in significant performance enhancements. We present the PRformer, a model integrating PRE with a standard Transformer encoder, demonstrating state-of-the-art performance on various real-world datasets. This performance highlights the effectiveness of our approach in leveraging longer lookback windows and underscores the critical role of robust temporal representations in maximizing Transformer's potential for prediction tasks. Code is available at this repository: \url{https://github.com/usualheart/PRformer}.||
|**2024-08-19**|[Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models](http://arxiv.org/abs/2408.10189)|null|Transformer architectures have become a dominant paradigm for domains like language modeling but suffer in many inference settings due to their quadratic-time self-attention. Recently proposed subquadratic architectures, such as Mamba, have shown promise, but have been pretrained with substantially less computational resources than the strongest Transformer models. In this work, we present a method that is able to distill a pretrained Transformer architecture into alternative architectures such as state space models (SSMs). The key idea to our approach is that we can view both Transformers and SSMs as applying different forms of mixing matrices over the token sequences. We can thus progressively distill the Transformer architecture by matching different degrees of granularity in the SSM: first matching the mixing matrices themselves, then the hidden units at each block, and finally the end-to-end predictions. Our method, called MOHAWK, is able to distill a Mamba-2 variant based on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens and a hybrid version (Hybrid Phi-Mamba) using 5B tokens. Despite using less than 1% of the training data typically used to train models from scratch, Phi-Mamba boasts substantially stronger performance compared to all past open-source non-Transformer models. MOHAWK allows models like SSMs to leverage computational resources invested in training Transformer-based architectures, highlighting a new avenue for building such models.||
|**2024-08-19**|[ML-CrAIST: Multi-scale Low-high Frequency Information-based Cross black Attention with Image Super-resolving Transformer](http://arxiv.org/abs/2408.09940)|**[link](https://github.com/alik033/ml-craist)**|Recently, transformers have captured significant interest in the area of single-image super-resolution tasks, demonstrating substantial gains in performance. Current models heavily depend on the network's extensive ability to extract high-level semantic details from images while overlooking the effective utilization of multi-scale image details and intermediate information within the network. Furthermore, it has been observed that high-frequency areas in images present significant complexity for super-resolution compared to low-frequency areas. This work proposes a transformer-based super-resolution architecture called ML-CrAIST that addresses this gap by utilizing low-high frequency information in multiple scales. Unlike most of the previous work (either spatial or channel), we operate spatial and channel self-attention, which concurrently model pixel interaction from both spatial and channel dimensions, exploiting the inherent correlations across spatial and channel axis. Further, we devise a cross-attention block for super-resolution, which explores the correlations between low and high-frequency information. Quantitative and qualitative assessments indicate that our proposed ML-CrAIST surpasses state-of-the-art super-resolution methods (e.g., 0.15 dB gain @Manga109 $\times$ 4). Code is available on: https://github.com/Alik033/ML-CrAIST.||
|**2024-08-19**|[Attention is a smoothed cubic spline](http://arxiv.org/abs/2408.09624)|null|We highlight a perhaps important but hitherto unobserved insight: The attention module in a transformer is a smoothed cubic spline. Viewed in this manner, this mysterious but critical component of a transformer becomes a natural development of an old notion deeply entrenched in classical approximation theory. More precisely, we show that with ReLU-activation, attention, masked attention, encoder-decoder attention are all cubic splines. As every component in a transformer is constructed out of compositions of various attention modules (= cubic splines) and feed forward neural networks (= linear splines), all its components -- encoder, decoder, and encoder-decoder blocks; multilayered encoders and decoders; the transformer itself -- are cubic or higher-order splines. If we assume the Pierce-Birkhoff conjecture, then the converse also holds, i.e., every spline is a ReLU-activated encoder. Since a spline is generally just $C^2$, one way to obtain a smoothed $C^\infty$ -version is by replacing ReLU with a smooth activation; and if this activation is chosen to be SoftMax, we recover the original transformer as proposed by Vaswani et al. This insight sheds light on the nature of the transformer by casting it entirely in terms of splines, one of the best known and thoroughly understood objects in applied mathematics.||
|**2024-08-18**|[A Unified Framework for Interpretable Transformers Using PDEs and Information Theory](http://arxiv.org/abs/2408.09523)|null|This paper presents a novel unified theoretical framework for understanding Transformer architectures by integrating Partial Differential Equations (PDEs), Neural Information Flow Theory, and Information Bottleneck Theory. We model Transformer information dynamics as a continuous PDE process, encompassing diffusion, self-attention, and nonlinear residual components. Our comprehensive experiments across image and text modalities demonstrate that the PDE model effectively captures key aspects of Transformer behavior, achieving high similarity (cosine similarity > 0.98) with Transformer attention distributions across all layers. While the model excels in replicating general information flow patterns, it shows limitations in fully capturing complex, non-linear transformations. This work provides crucial theoretical insights into Transformer mechanisms, offering a foundation for future optimizations in deep learning architectural design. We discuss the implications of our findings, potential applications in model interpretability and efficiency, and outline directions for enhancing PDE models to better mimic the intricate behaviors observed in Transformers, paving the way for more transparent and optimized AI systems.||
|**2024-08-18**|[Out-of-distribution generalization via composition: a lens through induction heads in Transformers](http://arxiv.org/abs/2408.09503)|**[link](https://github.com/jiajunsong629/ood-generalization-via-composition)**|Large language models (LLMs) such as GPT-4 sometimes appear to be creative, solving novel tasks often with a few demonstrations in the prompt. These tasks require the models to generalize on distributions different from those from training data -- which is known as out-of-distribution (OOD) generalization. Despite the tremendous success of LLMs, how they approach OOD generalization remains an open and underexplored question. We examine OOD generalization in settings where instances are generated according to hidden rules, including in-context learning with symbolic reasoning. Models are required to infer the hidden rules behind input prompts without any fine-tuning.   We empirically examined the training dynamics of Transformers on a synthetic example and conducted extensive experiments on a variety of pretrained LLMs, focusing on a type of components known as induction heads. We found that OOD generalization and composition are tied together -- models can learn rules by composing two self-attention layers, thereby achieving OOD generalization. Furthermore, a shared latent subspace in the embedding (or feature) space acts as a bridge for composition by aligning early layers and later layers, which we refer to as the common bridge representation hypothesis.||
|**2024-08-20**|[ELASTIC: Efficient Linear Attention for Sequential Interest Compression](http://arxiv.org/abs/2408.09380)|null|State-of-the-art sequential recommendation models heavily rely on transformer's attention mechanism. However, the quadratic computational and memory complexities of self attention have limited its scalability for modeling users' long range behaviour sequences. To address this problem, we propose ELASTIC, an Efficient Linear Attention for SequenTial Interest Compression, requiring only linear time complexity and decoupling model capacity from computational cost. Specifically, ELASTIC introduces a fixed length interest experts with linear dispatcher attention mechanism which compresses the long-term behaviour sequences to a significantly more compact representation which reduces up to 90% GPU memory usage with x2.7 inference speed up. The proposed linear dispatcher attention mechanism significantly reduces the quadratic complexity and makes the model feasible for adequately modeling extremely long sequences. Moreover, in order to retain the capacity for modeling various user interests, ELASTIC initializes a vast learnable interest memory bank and sparsely retrieves compressed user's interests from the memory with a negligible computational overhead. The proposed interest memory retrieval technique significantly expands the cardinality of available interest space while keeping the same computational cost, thereby striking a trade-off between recommendation accuracy and efficiency. To validate the effectiveness of our proposed ELASTIC, we conduct extensive experiments on various public datasets and compare it with several strong sequential recommenders. Experimental results demonstrate that ELASTIC consistently outperforms baselines by a significant margin and also highlight the computational efficiency of ELASTIC when modeling long sequences. We will make our implementation code publicly available.||
|**2024-08-17**|[EagleEye: Attention to Unveil Malicious Event Sequences from Provenance Graphs](http://arxiv.org/abs/2408.09217)|null|Securing endpoints is challenging due to the evolving nature of threats and attacks. With endpoint logging systems becoming mature, provenance-graph representations enable the creation of sophisticated behavior rules. However, adapting to the pace of emerging attacks is not scalable with rules. This led to the development of ML models capable of learning from endpoint logs. However, there are still open challenges: i) malicious patterns of malware are spread across long sequences of events, and ii) ML classification results are not interpretable. To address these issues, we develop and present EagleEye, a novel system that i) uses rich features from provenance graphs for behavior event representation, including command-line embeddings, ii) extracts long sequences of events and learns event embeddings, and iii) trains a lightweight Transformer model to classify behavior sequences as malicious or not. We evaluate and compare EagleEye against state-of-the-art baselines on two datasets, namely a new real-world dataset from a corporate environment, and the public DARPA dataset. On the DARPA dataset, at a false-positive rate of 1%, EagleEye detects $\approx$ 89% of all malicious behavior, outperforming two state-of-the-art solutions by an absolute margin of 38.5%. Furthermore, we show that the Transformer's attention mechanism can be leveraged to highlight the most suspicious events in a long sequence, thereby providing interpretation of malware alerts.||
|**2024-08-17**|[Linear Attention is Enough in Spatial-Temporal Forecasting](http://arxiv.org/abs/2408.09158)|null|As the most representative scenario of spatial-temporal forecasting tasks, the traffic forecasting task attracted numerous attention from machine learning community due to its intricate correlation both in space and time dimension. Existing methods often treat road networks over time as spatial-temporal graphs, addressing spatial and temporal representations independently. However, these approaches struggle to capture the dynamic topology of road networks, encounter issues with message passing mechanisms and over-smoothing, and face challenges in learning spatial and temporal relationships separately. To address these limitations, we propose treating nodes in road networks at different time steps as independent spatial-temporal tokens and feeding them into a vanilla Transformer to learn complex spatial-temporal patterns, design STformer achieving SOTA. Given its quadratic complexity, we introduce a variant NSTformer based on Nystr $\ddot{o}$ m method to approximate self-attention with linear complexity but even slightly better than former in a few cases astonishingly. Extensive experimental results on traffic datasets demonstrate that the proposed method achieves state-of-the-art performance at an affordable computational cost. Our code will be made available.||
|**2024-08-16**|[GeoTransformer: Enhancing Urban Forecasting with Geospatial Attention Mechanisms](http://arxiv.org/abs/2408.08852)|null|Recent advancements have focused on encoding urban spatial information into high-dimensional spaces, with notable efforts dedicated to integrating sociodemographic data and satellite imagery. These efforts have established foundational models in this field. However, the effective utilization of these spatial representations for urban forecasting applications remains under-explored. To address this gap, we introduce GeoTransformer, a novel structure that synergizes the Transformer architecture with geospatial statistics prior. GeoTransformer employs an innovative geospatial attention mechanism to incorporate extensive urban information and spatial dependencies into a unified predictive model. Specifically, we compute geospatial weighted attention scores between the target region and surrounding regions and leverage the integrated urban information for predictions. Extensive experiments on GDP and ride-share demand prediction tasks demonstrate that GeoTransformer significantly outperforms existing baseline models, showcasing its potential to enhance urban forecasting tasks.||
|**2024-08-16**|[Xpikeformer: Hybrid Analog-Digital Hardware Acceleration for Spiking Transformers](http://arxiv.org/abs/2408.08794)|null|This paper introduces Xpikeformer, a hybrid analog-digital hardware architecture designed to accelerate spiking neural network (SNN)-based transformer models. By combining the energy efficiency and temporal dynamics of SNNs with the powerful sequence modeling capabilities of transformers, Xpikeformer leverages mixed analog-digital computing techniques to enhance performance and energy efficiency. The architecture integrates analog in-memory computing (AIMC) for feedforward and fully connected layers, and a stochastic spiking attention (SSA) engine for efficient attention mechanisms. We detail the design, implementation, and evaluation of Xpikeformer, demonstrating significant improvements in energy consumption and computational efficiency. Through an image classification task and a wireless communication symbol detection task, we show that Xpikeformer can achieve software-comparable inference accuracy. Energy evaluations reveal that Xpikeformer achieves up to a $17.8$--$19.2\times$ reduction in energy consumption compared to state-of-the-art digital ANN transformers and up to a $5.9$--$6.8\times$ reduction compared to fully digital SNN transformers. Xpikeformer also achieves a $12.0\times$ speedup compared to the GPU implementation of spiking transformers.||
|**2024-08-15**|[Beyond Uniform Query Distribution: Key-Driven Grouped Query Attention](http://arxiv.org/abs/2408.08454)|null|Transformer架构通过其自注意力机制彻底改变了深度学习，该机制可以有效地捕获上下文信息。然而，自注意力的内存占用对长序列任务提出了重大挑战。分组查询注意力（GQA）通过将查询分组和对相应的键值头进行均值池化来解决这个问题，从而以灵活的方式减少了总体参数和内存需求，而不会对模型精度造成负面影响。在这项工作中，我们介绍了对GQA的增强，重点关注两种偏离分组静态性质的新方法：键分布式GQA（KDGQA）和动态键分布式GQA（DGQA），它们利用键头范数的信息来指导查询分配。具体来说，KDGQA关注每次前向传递过程中键头范数的比率，而DGQA检查这些比率在训练过程中的演变。此外，我们还提出了扰动GQA（PGQA）作为案例研究，它通过从注意力图中减去噪声来引入（静态）组形成的可变性。我们使用经过训练的视觉Transformer进行的实验，用于在CIFAR-10、CIFAR-100、Food101和Tiny ImageNet等数据集上进行图像分类，证明了这些变体在通过信息更丰富、自适应性更强的分组机制改进原始GQA方面的潜力：具体来说，与GQA和其他变体相比，ViT-L在使用DGQA时，准确率提高了8%。我们进一步分析了键值头数量对性能的影响，强调了利用查询键亲和度的重要性。||
|**2024-08-14**|[G $^2$V$^2$former: Graph Guided Video Vision Transformer for Face Anti-Spoofing](http://arxiv.org/abs/2408.07675)|null|In videos containing spoofed faces, we may uncover the spoofing evidence based on either photometric or dynamic abnormality, even a combination of both. Prevailing face anti-spoofing (FAS) approaches generally concentrate on the single-frame scenario, however, purely photometric-driven methods overlook the dynamic spoofing clues that may be exposed over time. This may lead FAS systems to conclude incorrect judgments, especially in cases where it is easily distinguishable in terms of dynamics but challenging to discern in terms of photometrics. To this end, we propose the Graph Guided Video Vision Transformer (G$^2$V$^2$ former), which combines faces with facial landmarks for photometric and dynamic feature fusion. We factorize the attention into space and time, and fuse them via a spatiotemporal block. Specifically, we design a novel temporal attention called Kronecker temporal attention, which has a wider receptive field, and is beneficial for capturing dynamic information. Moreover, we leverage the low-semantic motion of facial landmarks to guide the high-semantic change of facial expressions based on the motivation that regions containing landmarks may reveal more dynamic clues. Extensive experiments on nine benchmark datasets demonstrate that our method achieves superior performance under various scenarios. The codes will be released soon.||
|**2024-08-14**|[Graph Triple Attention Network: A Decoupled Perspective](http://arxiv.org/abs/2408.07654)|**[link](https://github.com/wangxiaotang0906/degta)**|Graph Transformers (GTs) have recently achieved significant success in the graph domain by effectively capturing both long-range dependencies and graph inductive biases. However, these methods face two primary challenges: (1) multi-view chaos, which results from coupling multi-view information (positional, structural, attribute), thereby impeding flexible usage and the interpretability of the propagation process. (2) local-global chaos, which arises from coupling local message passing with global attention, leading to issues of overfitting and over-globalizing. To address these challenges, we propose a high-level decoupled perspective of GTs, breaking them down into three components and two interaction levels: positional attention, structural attention, and attribute attention, alongside local and global interaction. Based on this decoupled perspective, we design a decoupled graph triple attention network named DeGTA, which separately computes multi-view attentions and adaptively integrates multi-view local and global information. This approach offers three key advantages: enhanced interpretability, flexible design, and adaptive integration of local and global information. Through extensive experiments, DeGTA achieves state-of-the-art performance across various datasets and tasks, including node classification and graph classification. Comprehensive ablation studies demonstrate that decoupling is essential for improving performance and enhancing interpretability. Our code is available at: https://github.com/wangxiaotang0906/DeGTA||
|**2024-08-14**|[Multi-periodicity dependency Transformer based on spectrum offset for radio frequency fingerprint identification](http://arxiv.org/abs/2408.07592)|null|Radio Frequency Fingerprint Identification (RFFI) has emerged as a pivotal task for reliable device authentication. Despite advancements in RFFI methods, background noise and intentional modulation features result in weak energy and subtle differences in the RFF features. These challenges diminish the capability of RFFI methods in feature representation, complicating the effective identification of device identities. This paper proposes a novel Multi-Periodicity Dependency Transformer (MPDFormer) to address these challenges. The MPDFormer employs a spectrum offset-based periodic embedding representation to augment the discrepency of intrinsic features. We delve into the intricacies of the periodicity-dependency attention mechanism, integrating both inter-period and intra-period attention mechanisms. This mechanism facilitates the extraction of both long and short-range periodicity-dependency features , accentuating the feature distinction whilst concurrently attenuating the perturbations caused by background noise and weak-periodicity features. Empirical results demonstrate MPDFormer's superiority over established baseline methods, achieving a 0.07s inference time on NVIDIA Jetson Orin NX.||
|**2024-08-14**|[Transformers and Large Language Models for Efficient Intrusion Detection Systems: A Comprehensive Survey](http://arxiv.org/abs/2408.07583)|null|With significant advancements in Transformers LLMs, NLP has extended its reach into many research fields due to its enhanced capabilities in text generation and user interaction. One field benefiting greatly from these advancements is cybersecurity. In cybersecurity, many parameters that need to be protected and exchanged between senders and receivers are in the form of text and tabular data, making NLP a valuable tool in enhancing the security measures of communication protocols. This survey paper provides a comprehensive analysis of the utilization of Transformers and LLMs in cyber-threat detection systems. The methodology of paper selection and bibliometric analysis is outlined to establish a rigorous framework for evaluating existing research. The fundamentals of Transformers are discussed, including background information on various cyber-attacks and datasets commonly used in this field. The survey explores the application of Transformers in IDSs, focusing on different architectures such as Attention-based models, LLMs like BERT and GPT, CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others. Furthermore, it explores the diverse environments and applications where Transformers and LLMs-based IDS have been implemented, including computer networks, IoT devices, critical infrastructure protection, cloud computing, SDN, as well as in autonomous vehicles. The paper also addresses research challenges and future directions in this area, identifying key issues such as interpretability, scalability, and adaptability to evolving threats, and more. Finally, the conclusion summarizes the findings and highlights the significance of Transformers and LLMs in enhancing cyber-threat detection capabilities, while also outlining potential avenues for further research and development.||
|**2024-08-14**|[GRFormer: Grouped Residual Self-Attention for Lightweight Single Image Super-Resolution](http://arxiv.org/abs/2408.07484)|**[link](https://github.com/sisrformer/grformer)**|Previous works have shown that reducing parameter overhead and computations for transformer-based single image super-resolution (SISR) models (e.g., SwinIR) usually leads to a reduction of performance. In this paper, we present GRFormer, an efficient and lightweight method, which not only reduces the parameter overhead and computations, but also greatly improves performance. The core of GRFormer is Grouped Residual Self-Attention (GRSA), which is specifically oriented towards two fundamental components. Firstly, it introduces a novel grouped residual layer (GRL) to replace the Query, Key, Value (QKV) linear layer in self-attention, aimed at efficiently reducing parameter overhead, computations, and performance loss at the same time. Secondly, it integrates a compact Exponential-Space Relative Position Bias (ES-RPB) as a substitute for the original relative position bias to improve the ability to represent position information while further minimizing the parameter count. Extensive experimental results demonstrate that GRFormer outperforms state-of-the-art transformer-based methods for $\times$2, $\times$3 and $\times$ 4 SISR tasks, notably outperforming SOTA by a maximum PSNR of 0.23dB when trained on the DIV2K dataset, while reducing the number of parameter and MACs by about \textbf{60\%} and \textbf{49\% } in only self-attention module respectively. We hope that our simple and effective method that can easily applied to SR models based on window-division self-attention can serve as a useful tool for further research in image super-resolution. The code is available at \url{https://github.com/sisrformer/GRFormer}.||
|**2024-08-13**|[Heterogeneous Space Fusion and Dual-Dimension Attention: A New Paradigm for Speech Enhancement](http://arxiv.org/abs/2408.06911)|null|Self-supervised learning has demonstrated impressive performance in speech tasks, yet there remains ample opportunity for advancement in the realm of speech enhancement research. In addressing speech tasks, confining the attention mechanism solely to the temporal dimension poses limitations in effectively focusing on critical speech features. Considering the aforementioned issues, our study introduces a novel speech enhancement framework, HFSDA, which skillfully integrates heterogeneous spatial features and incorporates a dual-dimension attention mechanism to significantly enhance speech clarity and quality in noisy environments. By leveraging self-supervised learning embeddings in tandem with Short-Time Fourier Transform (STFT) spectrogram features, our model excels at capturing both high-level semantic information and detailed spectral data, enabling a more thorough analysis and refinement of speech signals. Furthermore, we employ the innovative Omni-dimensional Dynamic Convolution (ODConv) technology within the spectrogram input branch, enabling enhanced extraction and integration of crucial information across multiple dimensions. Additionally, we refine the Conformer model by enhancing its feature extraction capabilities not only in the temporal dimension but also across the spectral domain. Extensive experiments on the VCTK-DEMAND dataset show that HFSDA is comparable to existing state-of-the-art models, confirming the validity of our approach.||
|**2024-08-13**|[FlatFusion: Delving into Details of Sparse Transformer-based Camera-LiDAR Fusion for Autonomous Driving](http://arxiv.org/abs/2408.06832)|null|The integration of data from diverse sensor modalities (e.g., camera and LiDAR) constitutes a prevalent methodology within the ambit of autonomous driving scenarios. Recent advancements in efficient point cloud transformers have underscored the efficacy of integrating information in sparse formats. When it comes to fusion, since image patches are dense in pixel space with ambiguous depth, it necessitates additional design considerations for effective fusion. In this paper, we conduct a comprehensive exploration of design choices for Transformer-based sparse cameraLiDAR fusion. This investigation encompasses strategies for image-to-3D and LiDAR-to-2D mapping, attention neighbor grouping, single modal tokenizer, and micro-structure of Transformer. By amalgamating the most effective principles uncovered through our investigation, we introduce FlatFusion, a carefully designed framework for sparse camera-LiDAR fusion. Notably, FlatFusion significantly outperforms state-of-the-art sparse Transformer-based methods, including UniTR, CMT, and SparseFusion, achieving 73.7 NDS on the nuScenes validation set with 10.1 FPS with PyTorch.||
|**2024-08-13**|[Bi-directional Contextual Attention for 3D Dense Captioning](http://arxiv.org/abs/2408.06662)|null|3D dense captioning is a task involving the localization of objects and the generation of descriptions for each object in a 3D scene. Recent approaches have attempted to incorporate contextual information by modeling relationships with object pairs or aggregating the nearest neighbor features of an object. However, the contextual information constructed in these scenarios is limited in two aspects: first, objects have multiple positional relationships that exist across the entire global scene, not only near the object itself. Second, it faces with contradicting objectives--where localization and attribute descriptions are generated better with tight localization, while descriptions involving global positional relations are generated better with contextualized features of the global scene. To overcome this challenge, we introduce BiCA, a transformer encoder-decoder pipeline that engages in 3D dense captioning for each object with Bi-directional Contextual Attention. Leveraging parallelly decoded instance queries for objects and context queries for non-object contexts, BiCA generates object-aware contexts, where the contexts relevant to each object is summarized, and context-aware objects, where the objects relevant to the summarized object-aware contexts are aggregated. This extension relieves previous methods from the contradicting objectives, enhancing both localization performance and enabling the aggregation of contextual features throughout the global scene; thus improving caption generation performance simultaneously. Extensive experiments on two of the most widely-used 3D dense captioning datasets demonstrate that our proposed method achieves a significant improvement over prior methods.||
|**2024-08-12**|[Body Transformer: Leveraging Robot Embodiment for Policy Learning](http://arxiv.org/abs/2408.06316)|null|In recent years, the transformer architecture has become the de facto standard for machine learning algorithms applied to natural language processing and computer vision. Despite notable evidence of successful deployment of this architecture in the context of robot learning, we claim that vanilla transformers do not fully exploit the structure of the robot learning problem. Therefore, we propose Body Transformer (BoT), an architecture that leverages the robot embodiment by providing an inductive bias that guides the learning process. We represent the robot body as a graph of sensors and actuators, and rely on masked attention to pool information throughout the architecture. The resulting architecture outperforms the vanilla transformer, as well as the classical multilayer perceptron, in terms of task completion, scaling properties, and computational efficiency when representing either imitation or reinforcement learning policies. Additional material including the open-source code is available at https://sferrazza.cc/bot_site.||
|**2024-08-12**|[DPDETR: Decoupled Position Detection Transformer for Infrared-Visible Object Detection](http://arxiv.org/abs/2408.06123)|null|Infrared-visible object detection aims to achieve robust object detection by leveraging the complementary information of infrared and visible image pairs. However, the commonly existing modality misalignment problem presents two challenges: fusing misalignment complementary features is difficult, and current methods cannot accurately locate objects in both modalities under misalignment conditions. In this paper, we propose a Decoupled Position Detection Transformer (DPDETR) to address these problems. Specifically, we explicitly formulate the object category, visible modality position, and infrared modality position to enable the network to learn the intrinsic relationships and output accurate positions of objects in both modalities. To fuse misaligned object features accurately, we propose a Decoupled Position Multispectral Cross-attention module that adaptively samples and aggregates multispectral complementary features with the constraint of infrared and visible reference positions. Additionally, we design a query-decoupled Multispectral Decoder structure to address the optimization gap among the three kinds of object information in our task and propose a Decoupled Position Contrastive DeNosing Training strategy to enhance the DPDETR's ability to learn decoupled positions. Experiments on DroneVehicle and KAIST datasets demonstrate significant improvements compared to other state-of-the-art methods. The code will be released at https://github.com/gjj45/DPDETR.||
|**2024-08-12**|[Optimizing Vision Transformers with Data-Free Knowledge Transfer](http://arxiv.org/abs/2408.05952)|null|The groundbreaking performance of transformers in Natural Language Processing (NLP) tasks has led to their replacement of traditional Convolutional Neural Networks (CNNs), owing to the efficiency and accuracy achieved through the self-attention mechanism. This success has inspired researchers to explore the use of transformers in computer vision tasks to attain enhanced long-term semantic awareness. Vision transformers (ViTs) have excelled in various computer vision tasks due to their superior ability to capture long-distance dependencies using the self-attention mechanism. Contemporary ViTs like Data Efficient Transformers (DeiT) can effectively learn both global semantic information and local texture information from images, achieving performance comparable to traditional CNNs. However, their impressive performance comes with a high computational cost due to very large number of parameters, hindering their deployment on devices with limited resources like smartphones, cameras, drones etc. Additionally, ViTs require a large amount of data for training to achieve performance comparable to benchmark CNN models. Therefore, we identified two key challenges in deploying ViTs on smaller form factor devices: the high computational requirements of large models and the need for extensive training data. As a solution to these challenges, we propose compressing large ViT models using Knowledge Distillation (KD), which is implemented data-free to circumvent limitations related to data availability. Additionally, we conducted experiments on object detection within the same environment in addition to classification tasks. Based on our analysis, we found that datafree knowledge distillation is an effective method to overcome both issues, enabling the deployment of ViTs on less resourceconstrained devices.||
|**2024-08-11**|[Sampling Foundational Transformer: A Theoretical Perspective](http://arxiv.org/abs/2408.05822)|null|The versatility of self-attention mechanism earned transformers great success in almost all data modalities, with limitations on the quadratic complexity and difficulty of training. To apply transformers across different data modalities, practitioners have to make specific clever data-modality-dependent constructions. In this paper, we propose Sampling Foundational Transformer (SFT) that can work on multiple data modalities (e.g., point cloud, graph, and sequence) and constraints (e.g., rotational-invariant). The existence of such model is important as contemporary foundational modeling requires operability on multiple data sources. For efficiency on large number of tokens, our model relies on our context aware sampling-without-replacement mechanism for both linear asymptotic computational complexity and real inference time gain. For efficiency, we rely on our newly discovered pseudoconvex formulation of transformer layer to increase model's convergence rate. As a model working on multiple data modalities, SFT has achieved competitive results on many benchmarks, while being faster in inference, compared to other very specialized models.||
|**2024-08-11**|[Efficient Diffusion Transformer with Step-wise Dynamic Attention Mediators](http://arxiv.org/abs/2408.05710)|null|This paper identifies significant redundancy in the query-key interactions within self-attention mechanisms of diffusion transformer models, particularly during the early stages of denoising diffusion steps. In response to this observation, we present a novel diffusion transformer framework incorporating an additional set of mediator tokens to engage with queries and keys separately. By modulating the number of mediator tokens during the denoising generation phases, our model initiates the denoising process with a precise, non-ambiguous stage and gradually transitions to a phase enriched with detail. Concurrently, integrating mediator tokens simplifies the attention module's complexity to a linear scale, enhancing the efficiency of global attention processes. Additionally, we propose a time-step dynamic mediator token adjustment mechanism that further decreases the required computational FLOPs for generation, simultaneously facilitating the generation of high-quality images within the constraints of varied inference budgets. Extensive experiments demonstrate that the proposed method can improve the generated image quality while also reducing the inference cost of diffusion transformers. When integrated with the recent work SiT, our method achieves a state-of-the-art FID score of 2.01. The source code is available at https://github.com/LeapLabTHU/Attention-Mediators.||
|**2024-08-10**|[PointMT: Efficient Point Cloud Analysis with Hybrid MLP-Transformer Architecture](http://arxiv.org/abs/2408.05508)|null|In recent years, point cloud analysis methods based on the Transformer architecture have made significant progress, particularly in the context of multimedia applications such as 3D modeling, virtual reality, and autonomous systems. However, the high computational resource demands of the Transformer architecture hinder its scalability, real-time processing capabilities, and deployment on mobile devices and other platforms with limited computational resources. This limitation remains a significant obstacle to its practical application in scenarios requiring on-device intelligence and multimedia processing. To address this challenge, we propose an efficient point cloud analysis architecture, \textbf{Point} \textbf{M}LP-\textbf{T}ransformer (PointMT). This study tackles the quadratic complexity of the self-attention mechanism by introducing a linear complexity local attention mechanism for effective feature aggregation. Additionally, to counter the Transformer's focus on token differences while neglecting channel differences, we introduce a parameter-free channel temperature adaptation mechanism that adaptively adjusts the attention weight distribution in each channel, enhancing the precision of feature aggregation. To improve the Transformer's slow convergence speed due to the limited scale of point cloud datasets, we propose an MLP-Transformer hybrid module, which significantly enhances the model's convergence speed. Furthermore, to boost the feature representation capability of point tokens, we refine the classification head, enabling point tokens to directly participate in prediction. Experimental results on multiple evaluation benchmarks demonstrate that PointMT achieves performance comparable to state-of-the-art methods while maintaining an optimal balance between performance and accuracy.||
|**2024-08-10**|[Your Context Is Not an Array: Unveiling Random Access Limitations in Transformers](http://arxiv.org/abs/2408.05506)|null|Despite their recent successes, Transformer-based large language models show surprising failure modes. A well-known example of such failure modes is their inability to length-generalize: solving problem instances at inference time that are longer than those seen during training. In this work, we further explore the root cause of this failure by performing a detailed analysis of model behaviors on the simple parity task. Our analysis suggests that length generalization failures are intricately related to a model's inability to perform random memory accesses within its context window. We present supporting evidence for this hypothesis by demonstrating the effectiveness of methodologies that circumvent the need for indexing or that enable random token access indirectly, through content-based addressing. We further show where and how the failure to perform random memory access manifests through attention map visualizations.||
|**2024-08-08**|[How Transformers Utilize Multi-Head Attention in In-Context Learning? A Case Study on Sparse Linear Regression](http://arxiv.org/abs/2408.04532)|null|Despite the remarkable success of transformer-based models in various real-world tasks, their underlying mechanisms remain poorly understood. Recent studies have suggested that transformers can implement gradient descent as an in-context learner for linear regression problems and have developed various theoretical analyses accordingly. However, these works mostly focus on the expressive power of transformers by designing specific parameter constructions, lacking a comprehensive understanding of their inherent working mechanisms post-training. In this study, we consider a sparse linear regression problem and investigate how a trained multi-head transformer performs in-context learning. We experimentally discover that the utilization of multi-heads exhibits different patterns across layers: multiple heads are utilized and essential in the first layer, while usually only a single head is sufficient for subsequent layers. We provide a theoretical explanation for this observation: the first layer preprocesses the context data, and the following layers execute simple optimization steps based on the preprocessed context. Moreover, we demonstrate that such a preprocess-then-optimize algorithm can significantly outperform naive gradient descent and ridge regression algorithms. Further experimental results support our explanations. Our findings offer insights into the benefits of multi-head attention and contribute to understanding the more intricate mechanisms hidden within trained transformers.||
|**2024-08-08**|[Efficient and Accurate Pneumonia Detection Using a Novel Multi-Scale Transformer Approach](http://arxiv.org/abs/2408.04290)|**[link](https://github.com/amirrezafateh/multi-scale-transformer-pneumonia)**|Pneumonia, a severe respiratory disease, poses significant diagnostic challenges, especially in underdeveloped regions. Traditional diagnostic methods, such as chest X-rays, suffer from variability in interpretation among radiologists, necessitating reliable automated tools. In this study, we propose a novel approach combining deep learning and transformer-based attention mechanisms to enhance pneumonia detection from chest X-rays. Our method begins with lung segmentation using a TransUNet model that integrates our specialized transformer module, which has fewer parameters compared to common transformers while maintaining performance. This model is trained on the "Chest Xray Masks and Labels" dataset and then applied to the Kermany and Cohen datasets to isolate lung regions, enhancing subsequent classification tasks. For classification, we employ pre-trained ResNet models (ResNet-50 and ResNet-101) to extract multi-scale feature maps, processed through our modified transformer module. By employing our specialized transformer, we attain superior results with significantly fewer parameters compared to common transformer models. Our approach achieves high accuracy rates of 92.79% on the Kermany dataset and 95.11% on the Cohen dataset, ensuring robust and efficient performance suitable for resource-constrained environments. "https://github.com/amirrezafateh/Multi-Scale-Transformer-Pneumonia"||
|**2024-08-08**|[Attention Mechanism and Context Modeling System for Text Mining Machine Translation](http://arxiv.org/abs/2408.04216)|null|This paper advances a novel architectural schema anchored upon the Transformer paradigm and innovatively amalgamates the K-means categorization algorithm to augment the contextual apprehension capabilities of the schema. The transformer model performs well in machine translation tasks due to its parallel computing power and multi-head attention mechanism. However, it may encounter contextual ambiguity or ignore local features when dealing with highly complex language structures. To circumvent this constraint, this exposition incorporates the K-Means algorithm, which is used to stratify the lexis and idioms of the input textual matter, thereby facilitating superior identification and preservation of the local structure and contextual intelligence of the language. The advantage of this combination is that K-Means can automatically discover the topic or concept regions in the text, which may be directly related to translation quality. Consequently, the schema contrived herein enlists K-Means as a preparatory phase antecedent to the Transformer and recalibrates the multi-head attention weights to assist in the discrimination of lexis and idioms bearing analogous semantics or functionalities. This ensures the schema accords heightened regard to the contextual intelligence embodied by these clusters during the training phase, rather than merely focusing on locational intelligence.||
|**2024-08-08**|[Efficient Single Image Super-Resolution with Entropy Attention and Receptive Field Augmentation](http://arxiv.org/abs/2408.04158)|null|Transformer-based deep models for single image super-resolution (SISR) have greatly improved the performance of lightweight SISR tasks in recent years. However, they often suffer from heavy computational burden and slow inference due to the complex calculation of multi-head self-attention (MSA), seriously hindering their practical application and deployment. In this work, we present an efficient SR model to mitigate the dilemma between model efficiency and SR performance, which is dubbed Entropy Attention and Receptive Field Augmentation network (EARFA), and composed of a novel entropy attention (EA) and a shifting large kernel attention (SLKA). From the perspective of information theory, EA increases the entropy of intermediate features conditioned on a Gaussian distribution, providing more informative input for subsequent reasoning. On the other hand, SLKA extends the receptive field of SR models with the assistance of channel shifting, which also favors to boost the diversity of hierarchical features. Since the implementation of EA and SLKA does not involve complex computations (such as extensive matrix multiplications), the proposed method can achieve faster nonlinear inference than Transformer-based SR models while maintaining better SR performance. Extensive experiments show that the proposed model can significantly reduce the delay of model inference while achieving the SR performance comparable with other advanced models.||
|**2024-08-09**|[Tree Attention: Topology-aware Decoding for Long-Context Attention on GPU clusters](http://arxiv.org/abs/2408.04093)|**[link](https://github.com/zyphra/tree_attention)**|Self-attention is the core mathematical operation of modern transformer architectures and is also a significant computational bottleneck due to its quadratic complexity in the sequence length. In this work, we derive the scalar energy function whose gradient computes the self-attention block, thus elucidating the theoretical underpinnings of self-attention, providing a Bayesian interpretation of the operation and linking it closely with energy-based models such as Hopfield Networks. Our formulation reveals that the reduction across the sequence axis can be efficiently computed in parallel through a tree reduction. Our algorithm, for parallelizing attention computation across multiple GPUs enables cross-device decoding to be performed asymptotically faster (up to 8x faster in our experiments) than alternative approaches such as Ring Attention, while also requiring significantly less communication volume and incurring 2x less peak memory. Our code is publicly available here: \url{https://github.com/Zyphra/tree_attention}.||
|**2024-08-07**|[Inter-Series Transformer: Attending to Products in Time Series Forecasting](http://arxiv.org/abs/2408.03872)|null|Time series forecasting is an important task in many fields ranging from supply chain management to weather forecasting. Recently, Transformer neural network architectures have shown promising results in forecasting on common time series benchmark datasets. However, application to supply chain demand forecasting, which can have challenging characteristics such as sparsity and cross-series effects, has been limited.   In this work, we explore the application of Transformer-based models to supply chain demand forecasting. In particular, we develop a new Transformer-based forecasting approach using a shared, multi-task per-time series network with an initial component applying attention across time series, to capture interactions and help address sparsity. We provide a case study applying our approach to successfully improve demand prediction for a medical device manufacturing company. To further validate our approach, we also apply it to public demand forecasting datasets as well and demonstrate competitive to superior performance compared to a variety of baseline and state-of-the-art forecast methods across the private and public datasets.||
|**2024-08-07**|[Surgformer: Surgical Transformer with Hierarchical Temporal Attention for Surgical Phase Recognition](http://arxiv.org/abs/2408.03867)|**[link](https://github.com/isyangshu/surgformer)**|Existing state-of-the-art methods for surgical phase recognition either rely on the extraction of spatial-temporal features at a short-range temporal resolution or adopt the sequential extraction of the spatial and temporal features across the entire temporal resolution. However, these methods have limitations in modeling spatial-temporal dependency and addressing spatial-temporal redundancy: 1) These methods fail to effectively model spatial-temporal dependency, due to the lack of long-range information or joint spatial-temporal modeling. 2) These methods utilize dense spatial features across the entire temporal resolution, resulting in significant spatial-temporal redundancy. In this paper, we propose the Surgical Transformer (Surgformer) to address the issues of spatial-temporal modeling and redundancy in an end-to-end manner, which employs divided spatial-temporal attention and takes a limited set of sparse frames as input. Moreover, we propose a novel Hierarchical Temporal Attention (HTA) to capture both global and local information within varied temporal resolutions from a target frame-centric perspective. Distinct from conventional temporal attention that primarily emphasizes dense long-range similarity, HTA not only captures long-term information but also considers local latent consistency among informative frames. HTA then employs pyramid feature aggregation to effectively utilize temporal information across diverse temporal resolutions, thereby enhancing the overall temporal representation. Extensive experiments on two challenging benchmark datasets verify that our proposed Surgformer performs favorably against the state-of-the-art methods. The code is released at https://github.com/isyangshu/Surgformer.||
|**2024-08-07**|[Bi-Level Spatial and Channel-aware Transformer for Learned Image Compression](http://arxiv.org/abs/2408.03842)|null|Recent advancements in learned image compression (LIC) methods have demonstrated superior performance over traditional hand-crafted codecs. These learning-based methods often employ convolutional neural networks (CNNs) or Transformer-based architectures. However, these nonlinear approaches frequently overlook the frequency characteristics of images, which limits their compression efficiency. To address this issue, we propose a novel Transformer-based image compression method that enhances the transformation stage by considering frequency components within the feature map. Our method integrates a novel Hybrid Spatial-Channel Attention Transformer Block (HSCATB), where a spatial-based branch independently handles high and low frequencies at the attention layer, and a Channel-aware Self-Attention (CaSA) module captures information across channels, significantly improving compression performance. Additionally, we introduce a Mixed Local-Global Feed Forward Network (MLGFFN) within the Transformer block to enhance the extraction of diverse and rich information, which is crucial for effective compression. These innovations collectively improve the transformation's ability to project data into a more decorrelated latent space, thereby boosting overall compression efficiency. Experimental results demonstrate that our framework surpasses state-of-the-art LIC methods in rate-distortion performance.||
|**2024-08-07**|[CAS-ViT: Convolutional Additive Self-attention Vision Transformers for Efficient Mobile Applications](http://arxiv.org/abs/2408.03703)|**[link](https://github.com/tianfang-zhang/cas-vit)**|Vision Transformers (ViTs) mark a revolutionary advance in neural networks with their token mixer's powerful global context capability. However, the pairwise token affinity and complex matrix operations limit its deployment on resource-constrained scenarios and real-time applications, such as mobile devices, although considerable efforts have been made in previous works. In this paper, we introduce CAS-ViT: Convolutional Additive Self-attention Vision Transformers, to achieve a balance between efficiency and performance in mobile applications. Firstly, we argue that the capability of token mixers to obtain global contextual information hinges on multiple information interactions, such as spatial and channel domains. Subsequently, we construct a novel additive similarity function following this paradigm and present an efficient implementation named Convolutional Additive Token Mixer (CATM). This simplification leads to a significant reduction in computational overhead. We evaluate CAS-ViT across a variety of vision tasks, including image classification, object detection, instance segmentation, and semantic segmentation. Our experiments, conducted on GPUs, ONNX, and iPhones, demonstrate that CAS-ViT achieves a competitive performance when compared to other state-of-the-art backbones, establishing it as a viable option for efficient mobile vision applications. Our code and model are available at: \url{https://github.com/Tianfang-Zhang/CAS-ViT}||
|**2024-08-06**|[TF-Locoformer: Transformer with Local Modeling by Convolution for Speech Separation and Enhancement](http://arxiv.org/abs/2408.03440)|**[link](https://github.com/merlresearch/tf-locoformer)**|Time-frequency (TF) domain dual-path models achieve high-fidelity speech separation. While some previous state-of-the-art (SoTA) models rely on RNNs, this reliance means they lack the parallelizability, scalability, and versatility of Transformer blocks. Given the wide-ranging success of pure Transformer-based architectures in other fields, in this work we focus on removing the RNN from TF-domain dual-path models, while maintaining SoTA performance. This work presents TF-Locoformer, a Transformer-based model with LOcal-modeling by COnvolution. The model uses feed-forward networks (FFNs) with convolution layers, instead of linear layers, to capture local information, letting the self-attention focus on capturing global patterns. We place two such FFNs before and after self-attention to enhance the local-modeling capability. We also introduce a novel normalization for TF-domain dual-path models. Experiments on separation and enhancement datasets show that the proposed model meets or exceeds SoTA in multiple benchmarks with an RNN-free architecture.||
|**2024-08-06**|[DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers](http://arxiv.org/abs/2408.03291)|null|Vision transformers (ViTs) have garnered significant attention for their performance in vision tasks; however, the high computational cost and significant latency issues have hinder widespread adoption. Post-training quantization (PTQ), a promising method for model compression, still faces accuracy degradation challenges with ViTs. There are two reasons for this: the existing quantization paradigm does not fit the power-law distribution of post-Softmax activations well, and accuracy inevitably decreases after reparameterizing post-LayerNorm activations. We propose a Distribution-Friendly and Outlier-Aware Post-training Quantization method for Vision Transformers, named DopQ-ViT. DopQ-ViT analyzes the inefficiencies of current quantizers and introduces a distribution-friendly Tan Quantizer called TanQ. TanQ focuses more on values near 1, more accurately preserving the power-law distribution of post-Softmax activations, and achieves favorable results. Moreover, when reparameterizing post-LayerNorm activations from channel-wise to layer-wise quantization, the accuracy degradation is mainly due to the significant impact of outliers in the scaling factors. Therefore, DopQ-ViT proposes a method to Search for the Optimal Scaling Factor, denoted as SOSF, which compensates for the influence of outliers and preserves the performance of the quantization model. DopQ-ViT has undergone extensive validation and demonstrates significant performance improvements in quantization models, particularly in low-bit settings.||
|**2024-08-06**|[Learning to Learn without Forgetting using Attention](http://arxiv.org/abs/2408.03219)|**[link](https://github.com/annaVettoruzzo/L2L_with_attention)**|Continual learning (CL) refers to the ability to continually learn over time by accommodating new knowledge while retaining previously learned experience. While this concept is inherent in human learning, current machine learning methods are highly prone to overwrite previously learned patterns and thus forget past experience. Instead, model parameters should be updated selectively and carefully, avoiding unnecessary forgetting while optimally leveraging previously learned patterns to accelerate future learning. Since hand-crafting effective update mechanisms is difficult, we propose meta-learning a transformer-based optimizer to enhance CL. This meta-learned optimizer uses attention to learn the complex relationships between model parameters across a stream of tasks, and is designed to generate effective weight updates for the current task while preventing catastrophic forgetting on previously encountered tasks. Evaluations on benchmark datasets like SplitMNIST, RotatedMNIST, and SplitCIFAR-100 affirm the efficacy of the proposed approach in terms of both forward and backward transfer, even on small sets of labeled data, highlighting the advantages of integrating a meta-learned optimizer within the continual learning framework.||
|**2024-08-05**|[LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local Attention and Mamba](http://arxiv.org/abs/2408.02615)|null|Recent Transformer-based diffusion models have shown remarkable performance, largely attributed to the ability of the self-attention mechanism to accurately capture both global and local contexts by computing all-pair interactions among input tokens. However, their quadratic complexity poses significant computational challenges for long-sequence inputs. Conversely, a recent state space model called Mamba offers linear complexity by compressing a filtered global context into a hidden state. Despite its efficiency, compression inevitably leads to information loss of fine-grained local dependencies among tokens, which are crucial for effective visual generative modeling. Motivated by these observations, we introduce Local Attentional Mamba (LaMamba) blocks that combine the strengths of self-attention and Mamba, capturing both global contexts and local details with linear complexity. Leveraging the efficient U-Net architecture, our model exhibits exceptional scalability and surpasses the performance of DiT across various model scales on ImageNet at 256x256 resolution, all while utilizing substantially fewer GFLOPs and a comparable number of parameters. Compared to state-of-the-art diffusion models on ImageNet 256x256 and 512x512, our largest model presents notable advantages, such as a reduction of up to 62\% GFLOPs compared to DiT-XL/2, while achieving superior performance with comparable or fewer parameters.||
|**2024-08-05**|[Toward Attention-based TinyML: A Heterogeneous Accelerated Architecture and Automated Deployment Flow](http://arxiv.org/abs/2408.02473)|null|One of the challenges for Tiny Machine Learning (tinyML) is keeping up with the evolution of Machine Learning models from Convolutional Neural Networks to Transformers. We address this by leveraging a heterogeneous architectural template coupling RISC-V processors with hardwired accelerators supported by an automated deployment flow. We demonstrate an Attention-based model in a tinyML power envelope with an octa-core cluster coupled with an accelerator for quantized Attention. Our deployment flow enables an end-to-end 8-bit MobileBERT, achieving leading-edge energy efficiency and throughput of 2960 GOp/J and 154 GOp/s at 32.5 Inf/s consuming 52.0 mW (0.65 V, 22 nm FD-SOI technology).||
|**2024-08-05**|[Cross-modulated Attention Transformer for RGBT Tracking](http://arxiv.org/abs/2408.02222)|null|Existing Transformer-based RGBT trackers achieve remarkable performance benefits by leveraging self-attention to extract uni-modal features and cross-attention to enhance multi-modal feature interaction and template-search correlation computation. Nevertheless, the independent search-template correlation calculations ignore the consistency between branches, which can result in ambiguous and inappropriate correlation weights. It not only limits the intra-modal feature representation, but also harms the robustness of cross-attention for multi-modal feature interaction and search-template correlation computation. To address these issues, we propose a novel approach called Cross-modulated Attention Transformer (CAFormer), which performs intra-modality self-correlation, inter-modality feature interaction, and search-template correlation computation in a unified attention model, for RGBT tracking. In particular, we first independently generate correlation maps for each modality and feed them into the designed Correlation Modulated Enhancement module, modulating inaccurate correlation weights by seeking the consensus between modalities. Such kind of design unifies self-attention and cross-attention schemes, which not only alleviates inaccurate attention weight computation in self-attention but also eliminates redundant computation introduced by extra cross-attention scheme. In addition, we propose a collaborative token elimination strategy to further improve tracking inference efficiency and accuracy. Extensive experiments on five public RGBT tracking benchmarks show the outstanding performance of the proposed CAFormer against state-of-the-art methods.||
|**2024-08-03**|[LAM3D: Leveraging Attention for Monocular 3D Object Detection](http://arxiv.org/abs/2408.01739)|null|Since the introduction of the self-attention mechanism and the adoption of the Transformer architecture for Computer Vision tasks, the Vision Transformer-based architectures gained a lot of popularity in the field, being used for tasks such as image classification, object detection and image segmentation. However, efficiently leveraging the attention mechanism in vision transformers for the Monocular 3D Object Detection task remains an open question. In this paper, we present LAM3D, a framework that Leverages self-Attention mechanism for Monocular 3D object Detection. To do so, the proposed method is built upon a Pyramid Vision Transformer v2 (PVTv2) as feature extraction backbone and 2D/3D detection machinery. We evaluate the proposed method on the KITTI 3D Object Detection Benchmark, proving the applicability of the proposed solution in the autonomous driving domain and outperforming reference methods. Moreover, due to the usage of self-attention, LAM3D is able to systematically outperform the equivalent architecture that does not employ self-attention.||
|**2024-08-03**|[AVESFormer: Efficient Transformer Design for Real-Time Audio-Visual Segmentation](http://arxiv.org/abs/2408.01708)|**[link](https://github.com/markxcloud/avesformer)**|Recently, transformer-based models have demonstrated remarkable performance on audio-visual segmentation (AVS) tasks. However, their expensive computational cost makes real-time inference impractical. By characterizing attention maps of the network, we identify two key obstacles in AVS models: 1) attention dissipation, corresponding to the over-concentrated attention weights by Softmax within restricted frames, and 2) inefficient, burdensome transformer decoder, caused by narrow focus patterns in early stages. In this paper, we introduce AVESFormer, the first real-time Audio-Visual Efficient Segmentation transformer that achieves fast, efficient and light-weight simultaneously. Our model leverages an efficient prompt query generator to correct the behaviour of cross-attention. Additionally, we propose ELF decoder to bring greater efficiency by facilitating convolutions suitable for local features to reduce computational burdens. Extensive experiments demonstrate that our AVESFormer significantly enhances model performance, achieving 79.9% on S4, 57.9% on MS3 and 31.2% on AVSS, outperforming previous state-of-the-art and achieving an excellent trade-off between performance and speed. Code can be found at https://github.com/MarkXCloud/AVESFormer.git.||
|**2024-08-02**|[Transformers are Universal In-context Learners](http://arxiv.org/abs/2408.01367)|null|Transformers are deep architectures that define "in-context mappings" which enable predicting new tokens based on a given set of tokens (such as a prompt in NLP applications or a set of patches for vision transformers). This work studies in particular the ability of these architectures to handle an arbitrarily large number of context tokens. To mathematically and uniformly address the expressivity of these architectures, we consider the case that the mappings are conditioned on a context represented by a probability distribution of tokens (discrete for a finite number of tokens). The related notion of smoothness corresponds to continuity in terms of the Wasserstein distance between these contexts. We demonstrate that deep transformers are universal and can approximate continuous in-context mappings to arbitrary precision, uniformly over compact token domains. A key aspect of our results, compared to existing findings, is that for a fixed precision, a single transformer can operate on an arbitrary (even infinite) number of tokens. Additionally, it operates with a fixed embedding dimension of tokens (this dimension does not increase with precision) and a fixed number of heads (proportional to the dimension). The use of MLP layers between multi-head attention layers is also explicitly controlled.||
|**2024-08-02**|[Actra: Optimized Transformer Architecture for Vision-Language-Action Models in Robot Learning](http://arxiv.org/abs/2408.01147)|null|Vision-language-action models have gained significant attention for their ability to model trajectories in robot learning. However, most existing models rely on Transformer models with vanilla causal attention, which we find suboptimal for processing segmented multi-modal sequences. Additionally, the autoregressive generation approach falls short in generating multi-dimensional actions. In this paper, we introduce Actra, an optimized Transformer architecture featuring trajectory attention and learnable action queries, designed for effective encoding and decoding of segmented vision-language-action trajectories in robot imitation learning. Furthermore, we devise a multi-modal contrastive learning objective to explicitly align different modalities, complementing the primary behavior cloning objective. Through extensive experiments conducted across various environments, Actra exhibits substantial performance improvement when compared to state-of-the-art models in terms of generalizability, dexterity, and precision.||
|**2024-08-02**|[An Efficient and Effective Transformer Decoder-Based Framework for Multi-Task Visual Grounding](http://arxiv.org/abs/2408.01120)|**[link](https://github.com/chenwei746/eevg)**|Most advanced visual grounding methods rely on Transformers for visual-linguistic feature fusion. However, these Transformer-based approaches encounter a significant drawback: the computational costs escalate quadratically due to the self-attention mechanism in the Transformer Encoder, particularly when dealing with high-resolution images or long context sentences. This quadratic increase in computational burden restricts the applicability of visual grounding to more intricate scenes, such as conversation-based reasoning segmentation, which involves lengthy language expressions. In this paper, we propose an efficient and effective multi-task visual grounding (EEVG) framework based on Transformer Decoder to address this issue, which reduces the cost in both language and visual aspects. In the language aspect, we employ the Transformer Decoder to fuse visual and linguistic features, where linguistic features are input as memory and visual features as queries. This allows fusion to scale linearly with language expression length. In the visual aspect, we introduce a parameter-free approach to reduce computation by eliminating background visual tokens based on attention scores. We then design a light mask head to directly predict segmentation masks from the remaining sparse feature maps. Extensive results and ablation studies on benchmarks demonstrate the efficiency and effectiveness of our approach. Code is available in https://github.com/chenwei746/EEVG.||
|**2024-08-02**|[Reconstructing Richtmyer-Meshkov instabilities from noisy radiographs using low dimensional features and attention-based neural networks](http://arxiv.org/abs/2408.00985)|null|A trained attention-based transformer network can robustly recover the complex topologies given by the Richtmyer-Meshkoff instability from a sequence of hydrodynamic features derived from radiographic images corrupted with blur, scatter, and noise. This approach is demonstrated on ICF-like double shell hydrodynamic simulations. The key component of this network is a transformer encoder that acts on a sequence of features extracted from noisy radiographs. This encoder includes numerous self-attention layers that act to learn temporal dependencies in the input sequences and increase the expressiveness of the model. This approach is demonstrated to exhibit an excellent ability to accurately recover the Richtmyer-Meshkov instability growth rates, even despite the gas-metal interface being greatly obscured by radiographic noise.||
|**2024-08-01**|[A deep spatio-temporal attention model of dynamic functional network connectivity shows sensitivity to Alzheimer's in asymptomatic individuals](http://arxiv.org/abs/2408.00378)|null|Alzheimer's disease (AD) progresses from asymptomatic changes to clinical symptoms, emphasizing the importance of early detection for proper treatment. Functional magnetic resonance imaging (fMRI), particularly dynamic functional network connectivity (dFNC), has emerged as an important biomarker for AD. Nevertheless, studies probing at-risk subjects in the pre-symptomatic stage using dFNC are limited. To identify at-risk subjects and understand alterations of dFNC in different stages, we leverage deep learning advancements and introduce a transformer-convolution framework for predicting at-risk subjects based on dFNC, incorporating spatial-temporal self-attention to capture brain network dependencies and temporal dynamics. Our model significantly outperforms other popular machine learning methods. By analyzing individuals with diagnosed AD and mild cognitive impairment (MCI), we studied the AD progression and observed a higher similarity between MCI and asymptomatic AD. The interpretable analysis highlights the cognitive-control network's diagnostic importance, with the model focusing on intra-visual domain dFNC when predicting asymptomatic AD subjects.||
|**2024-08-01**|[Advancing Medical Image Segmentation: Morphology-Driven Learning with Diffusion Transformer](http://arxiv.org/abs/2408.00347)|null|Understanding the morphological structure of medical images and precisely segmenting the region of interest or abnormality is an important task that can assist in diagnosis. However, the unique properties of medical imaging make clear segmentation difficult, and the high cost and time-consuming task of labeling leads to a coarse-grained representation of ground truth. Facing with these problems, we propose a novel Diffusion Transformer Segmentation (DTS) model for robust segmentation in the presence of noise. We propose an alternative to the dominant Denoising U-Net encoder through experiments applying a transformer architecture, which captures global dependency through self-attention. Additionally, we propose k-neighbor label smoothing, reverse boundary attention, and self-supervised learning with morphology-driven learning to improve the ability to identify complex structures. Our model, which analyzes the morphological representation of images, shows better results than the previous models in various medical imaging modalities, including CT, MRI, and lesion images.||
|**2024-08-01**|[Enhanced Structured State Space Models via Grouped FIR Filtering and Attention Sink Mechanisms](http://arxiv.org/abs/2408.00244)|null|Structured State Space Models (SSMs) have emerged as compelling alternatives to Transformer architectures, offering linear-time complexity and superior performance in various sequence modeling tasks. Despite their advantages, SSMs like the original Mamba-2 face training difficulties due to the sensitivities introduced by the extended series of recurrent matrix multiplications. In this paper, we propose an advanced architecture that mitigates these challenges by decomposing A-multiplications into multiple groups and optimizing positional encoding through Grouped Finite Impulse Response (FIR) filtering. This new structure, denoted as Grouped FIR-enhanced SSM (GFSSM), employs semiseparable matrices for efficient computation. Furthermore, inspired by the "attention sink" phenomenon identified in streaming language models, we incorporate a similar mechanism to enhance the stability and performance of our model over extended sequences. Our approach further bridges the gap between SSMs and Transformer architectures, offering a viable path forward for scalable and high-performing sequence modeling.||
|**2024-07-31**|[CC-SAM: SAM with Cross-feature Attention and Context for Ultrasound Image Segmentation](http://arxiv.org/abs/2408.00181)|null|The Segment Anything Model (SAM) has achieved remarkable successes in the realm of natural image segmentation, but its deployment in the medical imaging sphere has encountered challenges. Specifically, the model struggles with medical images that feature low contrast, faint boundaries, intricate morphologies, and small-sized objects. To address these challenges and enhance SAM's performance in the medical domain, we introduce a comprehensive modification. Firstly, we incorporate a frozen Convolutional Neural Network (CNN) branch as an image encoder, which synergizes with SAM's original Vision Transformer (ViT) encoder through a novel variational attention fusion module. This integration bolsters the model's capability to capture local spatial information, which is often paramount in medical imagery. Moreover, to further optimize SAM for medical imaging, we introduce feature and position adapters within the ViT branch, refining the encoder's representations. We see that compared to current prompting strategies to fine-tune SAM for ultrasound medical segmentation, the use of text descriptions that serve as text prompts for SAM helps significantly improve the performance. Leveraging ChatGPT's natural language understanding capabilities, we generate prompts that offer contextual information and guidance to SAM, enabling it to better understand the nuances of ultrasound medical images and improve its segmentation accuracy. Our method, in its entirety, represents a significant stride towards making universal image segmentation models more adaptable and efficient in the medical domain.||
|**2024-07-31**|[Dynamic Object Queries for Transformer-based Incremental Object Detection](http://arxiv.org/abs/2407.21687)|null|Incremental object detection (IOD) aims to sequentially learn new classes, while maintaining the capability to locate and identify old ones. As the training data arrives with annotations only with new classes, IOD suffers from catastrophic forgetting. Prior methodologies mainly tackle the forgetting issue through knowledge distillation and exemplar replay, ignoring the conflict between limited model capacity and increasing knowledge. In this paper, we explore \textit{dynamic object queries} for incremental object detection built on Transformer architecture. We propose the \textbf{Dy}namic object \textbf{Q}uery-based \textbf{DE}tection \textbf{TR}ansformer (DyQ-DETR), which incrementally expands the model representation ability to achieve stability-plasticity tradeoff. First, a new set of learnable object queries are fed into the decoder to represent new classes. These new object queries are aggregated with those from previous phases to adapt both old and new knowledge well. Second, we propose the isolated bipartite matching for object queries in different phases, based on disentangled self-attention. The interaction among the object queries at different phases is eliminated to reduce inter-class confusion. Thanks to the separate supervision and computation over object queries, we further present the risk-balanced partial calibration for effective exemplar replay. Extensive experiments demonstrate that DyQ-DETR significantly surpasses the state-of-the-art methods, with limited parameter overhead. Code will be made publicly available.||
|**2024-07-31**|[An Explainable Vision Transformer with Transfer Learning Combined with Support Vector Machine Based Efficient Drought Stress Identification](http://arxiv.org/abs/2407.21666)|null|Early detection of drought stress is critical for taking timely measures for reducing crop loss before the drought impact becomes irreversible. The subtle phenotypical and physiological changes in response to drought stress are captured by non-invasive imaging techniques and these imaging data serve as valuable resource for machine learning methods to identify drought stress. While convolutional neural networks (CNNs) are in wide use, vision transformers (ViTs) present a promising alternative in capturing long-range dependencies and intricate spatial relationships, thereby enhancing the detection of subtle indicators of drought stress. We propose an explainable deep learning pipeline that leverages the power of ViTs for drought stress detection in potato crops using aerial imagery. We applied two distinct approaches: a synergistic combination of ViT and support vector machine (SVM), where ViT extracts intricate spatial features from aerial images, and SVM classifies the crops as stressed or healthy and an end-to-end approach using a dedicated classification layer within ViT to directly detect drought stress. Our key findings explain the ViT model's decision-making process by visualizing attention maps. These maps highlight the specific spatial features within the aerial images that the ViT model focuses as the drought stress signature. Our findings demonstrate that the proposed methods not only achieve high accuracy in drought stress identification but also shedding light on the diverse subtle plant features associated with drought stress. This offers a robust and interpretable solution for drought stress monitoring for farmers to undertake informed decisions for improved crop management.||
|**2024-07-31**|[MSA2Net: Multi-scale Adaptive Attention-guided Network for Medical Image Segmentation](http://arxiv.org/abs/2407.21640)|**[link](https://github.com/xmindflow/msa-2net)**|Medical image segmentation involves identifying and separating object instances in a medical image to delineate various tissues and structures, a task complicated by the significant variations in size, shape, and density of these features. Convolutional neural networks (CNNs) have traditionally been used for this task but have limitations in capturing long-range dependencies. Transformers, equipped with self-attention mechanisms, aim to address this problem. However, in medical image segmentation it is beneficial to merge both local and global features to effectively integrate feature maps across various scales, capturing both detailed features and broader semantic elements for dealing with variations in structures. In this paper, we introduce MSA2Net, a new deep segmentation framework featuring an expedient design of skip-connections. These connections facilitate feature fusion by dynamically weighting and combining coarse-grained encoder features with fine-grained decoder feature maps. Specifically, we propose a Multi-Scale Adaptive Spatial Attention Gate (MASAG), which dynamically adjusts the receptive field (Local and Global contextual information) to ensure that spatially relevant features are selectively highlighted while minimizing background distractions. Extensive evaluations involving dermatology, and radiological datasets demonstrate that our MSA2Net outperforms state-of-the-art (SOTA) works or matches their performance. The source code is publicly available at https://github.com/xmindflow/MSA-2Net.||
|**2024-07-31**|[MART: MultiscAle Relational Transformer Networks for Multi-agent Trajectory Prediction](http://arxiv.org/abs/2407.21635)|null|多智能体轨迹预测对于自动驾驶和理解周围环境至关重要。近年来，基于学习的多智能体轨迹预测方法，例如主要依赖于图神经网络、图变换器和超图神经网络的方法，在真实世界数据集上表现出卓越的性能。然而，基于超图变换器的轨迹预测方法还有待探索。因此，我们提出了一种用于多智能体轨迹预测的多尺度关系变换器（MART）网络。MART是一种超图变换器架构，用于在变换器机制中考虑个体和群体行为。MART的核心模块是编码器，它由成对关系变换器（PRT）和超关系变换器（HRT）组成。编码器通过引入HRT扩展了关系变换器的能力，HRT将超边特征集成到变换器机制中，促使注意力权重关注群体关系。此外，我们提出了一种自适应组估计器（AGE），旨在推断现实环境中的复杂组关系。在三个真实世界数据集（NBA、SDD 和 ETH-UCY）上的大量实验表明，我们的方法实现了最先进的性能，在 NBA 数据集上将 ADE/FDE 提高了 3.9%/11.8%。代码可在 https://github.com/gist-ailab/MART 获取。||
|**2024-07-31**|[Interpretable correlator Transformer for image-like quantum matter data](http://arxiv.org/abs/2407.21502)|null|Due to their inherent capabilities of capturing non-local dependencies, Transformer neural networks have quickly been established as the paradigmatic architecture for large language models and image processing. Next to these traditional applications, machine learning methods have also been demonstrated to be versatile tools in the analysis of image-like data of quantum phases of matter, e.g. given snapshots of many-body wave functions obtained in ultracold atom experiments. While local correlation structures in image-like data of physical systems can reliably be detected, identifying phases of matter characterized by global, non-local structures with interpretable machine learning methods remains a challenge. Here, we introduce the correlator Transformer (CoTra), which classifies different phases of matter while at the same time yielding full interpretability in terms of physical correlation functions. The network's underlying structure is a tailored attention mechanism, which learns efficient ways to weigh local and non-local correlations for a successful classification. We demonstrate the versatility of the CoTra by detecting local order in the Heisenberg antiferromagnet, and show that local gauge constraints in one- and two-dimensional lattice gauge theories can be identified. Furthermore, we establish that the CoTra reliably detects non-local structures in images of correlated fermions in momentum space (Cooper pairs) and that it can distinguish percolating from non-percolating images.||
|**2024-08-01**|[DFE-IANet: A Method for Polyp Image Classification Based on Dual-domain Feature Extraction and Interaction Attention](http://arxiv.org/abs/2407.20843)|null|It is helpful in preventing colorectal cancer to detect and treat polyps in the gastrointestinal tract early. However, there have been few studies to date on designing polyp image classification networks that balance efficiency and accuracy. This challenge is mainly attributed to the fact that polyps are similar to other pathologies and have complex features influenced by texture, color, and morphology. In this paper, we propose a novel network DFE-IANet based on both spectral transformation and feature interaction. Firstly, to extract detailed features and multi-scale features, the features are transformed by the multi-scale frequency domain feature extraction (MSFD) block to extract texture details at the fine-grained level in the frequency domain. Secondly, the multi-scale interaction attention (MSIA) block is designed to enhance the network's capability of extracting critical features. This block introduces multi-scale features into self-attention, aiming to adaptively guide the network to concentrate on vital regions. Finally, with a compact parameter of only 4M, DFE-IANet outperforms the latest and classical networks in terms of efficiency. Furthermore, DFE-IANet achieves state-of-the-art (SOTA) results on the challenging Kvasir dataset, demonstrating a remarkable Top-1 accuracy of 93.94%. This outstanding accuracy surpasses ViT by 8.94%, ResNet50 by 1.69%, and VMamba by 1.88%. Our code is publicly available at https://github.com/PURSUETHESUN/DFE-IANet.||
|**2024-07-30**|[Interpretable Pre-Trained Transformers for Heart Time-Series Data](http://arxiv.org/abs/2407.20775)|**[link](https://github.com/harryjdavies/heartgpt)**|Decoder-only transformers are the backbone of the popular generative pre-trained transformer (GPT) series of large language models. In this work, we apply the same framework to periodic heart time-series data to create two pre-trained general purpose cardiac models, namely PPG-PT and ECG-PT. We demonstrate that both such pre-trained models are fully interpretable. This is achieved firstly through aggregate attention maps which show that the model focuses on similar points in previous cardiac cycles in order to make predictions and gradually broadens its attention in deeper layers. Next, tokens with the same value, that occur at different distinct points in the ECG and PPG cycle, form separate clusters in high dimensional space based on their phase as they propagate through the transformer blocks. Finally, we highlight that individual attention heads respond to specific physiologically relevent features, such as the dicrotic notch in PPG and the P-wave in ECG. It is also demonstrated that these pre-trained models can be easily fine-tuned for tasks such as classification of atrial fibrillation. In this specific example, the fine-tuning took 11 minutes of computer time, and achieved a leave-one-subject-out AUCs of 0.99 and 0.93 for ECG and PPG respectively. Importantly, these fine-tuned models are also fully explainable, with attention shifting to regions in the context that are strongly indicative of atrial fibrillation.||
|**2024-07-30**|[HandDAGT: A Denoising Adaptive Graph Transformer for 3D Hand Pose Estimation](http://arxiv.org/abs/2407.20542)|**[link](https://github.com/cwc1260/handdagt)**|The extraction of keypoint positions from input hand frames, known as 3D hand pose estimation, is crucial for various human-computer interaction applications. However, current approaches often struggle with the dynamic nature of self-occlusion of hands and intra-occlusion with interacting objects. To address this challenge, this paper proposes the Denoising Adaptive Graph Transformer, HandDAGT, for hand pose estimation. The proposed HandDAGT leverages a transformer structure to thoroughly explore effective geometric features from input patches. Additionally, it incorporates a novel attention mechanism to adaptively weigh the contribution of kinematic correspondence and local geometric features for the estimation of specific keypoints. This attribute enables the model to adaptively employ kinematic and local information based on the occlusion situation, enhancing its robustness and accuracy. Furthermore, we introduce a novel denoising training strategy aimed at improving the model's robust performance in the face of occlusion challenges. Experimental results show that the proposed model significantly outperforms the existing methods on four challenging hand pose benchmark datasets. Codes and pre-trained models are publicly available at https://github.com/cwc1260/HandDAGT.||
|**2024-07-30**|[DuA: Dual Attentive Transformer in Long-Term Continuous EEG Emotion Analysis](http://arxiv.org/abs/2407.20519)|null|Affective brain-computer interfaces (aBCIs) are increasingly recognized for their potential in monitoring and interpreting emotional states through electroencephalography (EEG) signals. Current EEG-based emotion recognition methods perform well with short segments of EEG data. However, these methods encounter significant challenges in real-life scenarios where emotional states evolve over extended periods. To address this issue, we propose a Dual Attentive (DuA) transformer framework for long-term continuous EEG emotion analysis. Unlike segment-based approaches, the DuA transformer processes an entire EEG trial as a whole, identifying emotions at the trial level, referred to as trial-based emotion analysis. This framework is designed to adapt to varying signal lengths, providing a substantial advantage over traditional methods. The DuA transformer incorporates three key modules: the spatial-spectral network module, the temporal network module, and the transfer learning module. The spatial-spectral network module simultaneously captures spatial and spectral information from EEG signals, while the temporal network module detects temporal dependencies within long-term EEG data. The transfer learning module enhances the model's adaptability across different subjects and conditions. We extensively evaluate the DuA transformer using a self-constructed long-term EEG emotion database, along with two benchmark EEG emotion databases. On the basis of the trial-based leave-one-subject-out cross-subject cross-validation protocol, our experimental results demonstrate that the proposed DuA transformer significantly outperforms existing methods in long-term continuous EEG emotion analysis, with an average enhancement of 5.28%.||
|**2024-07-31**|[A2SF: Accumulative Attention Scoring with Forgetting Factor for Token Pruning in Transformer Decoder](http://arxiv.org/abs/2407.20485)|null|Recently, large language models (LLM) based on transformers are facing memory bottleneck issues due to KV cache, especially in long sequence handling. Previous researches proposed KV cache compression techniques that identify insignificant tokens based on Accumulative Attention Scores and removes their items from KV cache, noting that only few tokens play an important role in attention operations. However, we have observed that the existing Accumulative Attention Score is not suitable for the transformer decoder structure. In the decoder model, the number of times the Attention Score accumulates varies depending on the order of token appearance due to the effect of masking, causing an uneven comparison between tokens. To solve this, we propose Accumulative Attention Score with Forgetting Factor (A2SF) technique, which introduces a Forgetting Factor in the Attention Score accumulation process. A2SF applies a penalty to the past Attention Score generated from old tokens by repeatedly multiplying the Forgetting Factor to the Attention Score over time. Therefore, older tokens receive a larger penalty, providing fairness among different ages of tokens. Through the fair comparison among tokens, we can more effectively select important tokens. We have verified the accuracy improvement through A2SF in the OPT and LLaMA models and A2SF improves the accuracy of LLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.||
|**2024-07-30**|[MimiQ: Low-Bit Data-Free Quantization of Vision Transformers with Encouraging Inter-Head Attention Similarity](http://arxiv.org/abs/2407.20021)|null|无数据量化 (DFQ) 是一种无需原始训练数据即可从全精度网络创建轻量级网络的技术，通常通过合成数据集实现。尽管已经提出了几种针对视觉Transformer (ViT) 架构的DFQ方法，但它们在低比特设置中无法实现高效性。通过检查现有方法，我们发现它们的合成数据产生了未对齐的注意力图，而真实样本的注意力图是高度对齐的。从对齐注意力的观察中，我们发现对齐合成数据的注意力图有助于提高量化 ViT 的整体性能。基于这一发现，我们设计了\aname，这是一种专为 ViT 设计的新型 DFQ 方法，专注于头部间注意力相似性。首先，我们通过对齐空间查询块相关的头部注意力响应来生成合成数据。然后，我们应用头部结构注意力蒸馏来将量化网络的注意力图与全精度教师网络的注意力图对齐。实验结果表明，所提出的方法明显优于基线方法，为无数据 ViT 量化设定了新的最先进性能。||
|**2024-07-29**|[Language-driven Grasp Detection with Mask-guided Attention](http://arxiv.org/abs/2407.19877)|null|抓取检测是机器人技术中的一项基本任务，在工业中有着广泛的应用。然而，传统方法通常难以处理遮挡问题，并且无法利用语言进行抓取。将自然语言融入抓取检测仍然是一项具有挑战性的任务，并且很大程度上尚未得到探索。为了解决这一问题，我们提出了一种新的基于掩码引导注意力的语言驱动抓取检测方法，该方法利用结合语义分割特征的Transformer注意力机制。我们的方法集成了视觉数据、分割掩码特征和自然语言指令，显著提高了抓取检测的准确性。我们的工作为语言驱动的抓取检测引入了一个新的框架，为语言驱动的机器人应用铺平了道路。大量实验表明，我们的方法明显优于其他近期基线方法，成功率提高了10.0%。我们还在真实的机器人实验中验证了我们的方法，证实了该方法的有效性。||
|**2024-07-29**|[Survey and Taxonomy: The Role of Data-Centric AI in Transformer-Based Time Series Forecasting](http://arxiv.org/abs/2407.19784)|null|在不断改进人工智能性能的过程中，除了开发更复杂的模型之外，研究人员也开始关注以数据为中心的人工智能这一新兴概念，它强调数据在系统性机器学习训练过程中的重要作用。尽管如此，模型的开发也在快速发展。Transformer 架构就是这种进步的成果之一，它在自然语言处理（NLP）、计算机视觉（CV）和时间序列预测（TSF）等多个领域都具有很高的能力。然而，它的性能在很大程度上取决于输入数据的预处理和输出数据的评估，因此未来的研究需要采用以数据为中心的方法。我们认为，以数据为中心的人工智能对于高效地训练人工智能模型，特别是基于 Transformer 的 TSF 模型至关重要。然而，基于 Transformer 的 TSF 与以数据为中心的人工智能的整合方面还存在差距。本次综述旨在通过基于所提出的分类法的广泛文献回顾来确定这一差距。我们从以数据为中心的人工智能的角度回顾了以往的研究工作，并打算为未来基于 Transformer 的架构和以数据为中心的人工智能的发展奠定基础。||
|**2024-07-29**|[Cross-Layer Feature Pyramid Transformer for Small Object Detection in Aerial Images](http://arxiv.org/abs/2407.19696)|null|由于物体通常较小，航拍图像中的物体检测一直是一项具有挑战性的任务。目前大多数检测器优先考虑新颖的检测框架，而经常忽略对特征金字塔网络等基本组件的研究。在本文中，我们介绍了跨层特征金字塔Transformer（CFPT），这是一种专为航拍图像中小物体检测而设计的，无需上采样的新型特征金字塔网络。CFPT结合了两个精心设计的具有线性计算复杂度的注意力块：跨层通道注意力（CCA）和跨层空间注意力（CSA）。CCA通过划分通道注意力标记组来感知沿空间维度的跨层全局信息，从而实现跨层交互；而CSA通过划分空间注意力标记组来感知沿通道维度的跨层全局信息，从而完成跨层交互。通过集成这些模块，CFPT能够一步实现跨层交互，从而避免了与逐元素求和和逐层传输相关的语义差距和信息丢失。此外，CFPT还整合了全局上下文信息，增强了对小物体的检测性能。为了进一步增强跨层交互过程中的位置感知能力，我们提出了基于层间互感受野的跨层一致相对位置编码（CCPE）。我们在两个具有挑战性的航拍图像目标检测数据集VisDrone2019-DET和TinyPerson上评估了CFPT的有效性。大量实验表明了CFPT的有效性，它在产生较低计算成本的同时，优于最先进的特征金字塔网络。代码将发布在https://github.com/duzw9311/CFPT。||
|**2024-07-29**|[Text2LiDAR: Text-guided LiDAR Point Cloud Generation via Equirectangular Transformer](http://arxiv.org/abs/2407.19628)|null|复杂的交通环境和多变的天气条件使得激光雷达数据的采集成本高昂且充满挑战。实现高质量、可控的激光雷达数据生成迫在眉睫，而使用文本进行控制是一种常见的做法，但该领域的研究还很少。为此，我们提出了 Text2LiDAR，这是第一个高效、多样化且可通过文本控制的激光雷达数据生成模型。具体来说，我们设计了一种等矩形变换器架构，利用设计的等矩形注意力机制以符合数据特征的方式捕获激光雷达特征。然后，我们设计了一个控制信号嵌入注入器，通过全局到局部的注意力机制有效地整合控制信号。此外，我们还设计了一个频率调制器，帮助模型恢复高频细节，确保生成点云的清晰度。为了促进该领域的发展并优化文本控制的生成性能，我们构建了 nuLiDARtext，它为来自 850 个场景的 34,149 个激光雷达点云提供了不同的文本描述。在 KITTI-360 和 nuScenes 数据集上进行的各种形式的无控制和文本控制生成实验表明了我们方法的优越性。||
|**2024-07-26**|[Dilated Strip Attention Network for Image Restoration](http://arxiv.org/abs/2407.18613)|null|图像修复是一项长期存在的研究任务，旨在从退化的图像中恢复潜在的清晰图像。由于自注意力机制能够有效地捕捉长距离依赖关系，近年来，基于Transformer的方法或一些基于注意力的卷积神经网络在许多图像修复任务中都取得了显著成果。然而，现有的注意力模块存在感受野有限或参数量过大的问题。为了更有效、更高效地整合上下文信息，本文提出了一种用于图像修复的扩张条带注意力网络（DSAN）。具体来说，为了从同一行或同一列的相邻像素中为每个像素收集更多的上下文信息，我们精心设计了一种扩张条带注意力（DSA）机制。通过水平和垂直地应用DSA操作，每个位置都可以从更广阔的区域中获取上下文信息。此外，我们在DSA中利用跨不同特征组的多尺度感受野来改进表示学习。大量实验表明，我们的DSAN在多种图像修复任务上优于现有最佳算法。||
|**2024-07-26**|[Skin Cancer Detection utilizing Deep Learning: Classification of Skin Lesion Images using a Vision Transformer](http://arxiv.org/abs/2407.18554)|null|皮肤癌检测仍然是医疗保健领域的一项重大挑战。常见的检测方法可能耗时长且需要人工辅助，这在许多国家都存在不足。先前的研究表明，卷积神经网络（CNN）可以通过自动化和与人类水平相当的准确性来有效地提供帮助。然而，尽管在过去几十年中取得了进展，但精度仍然有限，导致大量的错误分类，对人们的健康造成严重影响。因此，我们采用近年来基于自注意力机制思想开发的视觉Transformer（ViT），特别是两种配置的预训练ViT。在将它们与决策树分类器和k近邻（KNN）分类器等基线模型，以及CNN和不太复杂的ViT进行比较后，我们发现ViT在皮肤病变分类方面总体上具有更好的指标。特别是，我们更加重视黑色素瘤的表现，它是皮肤癌中最致命的一种。ViT-L32模型的准确率为91.57%，黑色素瘤召回率为58.54%，而ViT-L16模型的准确率为92.79%，黑色素瘤召回率为56.10%。这为更快、更准确的诊断提供了一种潜在的工具，并为医疗保健部门带来了全面的改进。||
|**2024-07-26**|[Multimodal Emotion Recognition using Audio-Video Transformer Fusion with Cross Attention](http://arxiv.org/abs/2407.18552)|null|理解情绪是人类沟通的基础。与依赖单一数据源（如语音或面部表情）的传统方法相比，整合音频和视频信号可以更全面地理解情绪状态。尽管具有潜力，但多模态情绪识别面临着重大挑战，特别是在同步、特征提取和不同数据源的融合方面。为了解决这些问题，本文介绍了一种基于transformer的新型模型，称为“具有交叉注意力的音视频Transformer融合（AVT-CA）”。AVT-CA模型采用transformer融合方法，有效地捕获和同步来自音频和视频输入的相互关联的特征，从而解决了同步问题。此外，AVT-CA内的交叉注意力机制有选择地提取和强调关键特征，同时丢弃来自两种模态的无关特征，解决了特征提取和融合的挑战。在CMU-MOSEI、RAVDESS和CREMA-D数据集上进行的大量实验分析证明了该模型的有效性。结果强调了AVT-CA在开发用于实际应用的精确可靠的多模态情绪识别系统方面的重要性。||
|**2024-07-26**|[DTFormer: A Transformer-Based Method for Discrete-Time Dynamic Graph Representation Learning](http://arxiv.org/abs/2407.18523)|null|离散时间动态图（DTDG）在现实世界中应用广泛，并且易于获取数据，因此受到了学术研究人员和行业从业者的广泛关注。DTDG 的表示学习已被广泛应用于对时变实体及其演化连接的动态建模。目前，DTDG 表示学习主要依赖于 GNN+RNN 架构，该架构体现了图神经网络 (GNN) 和循环神经网络 (RNN) 的固有局限性。随着模型架构的加深，GNN 遭受过度平滑问题的困扰，而 RNN 难以有效捕捉长期依赖关系。GNN+RNN 架构也难以扩展到大型图和长序列。此外，这些方法通常分别计算节点表示，并且只关注单个节点的特征，从而忽略了链接被预测的两个节点之间的行为交叉，例如两个节点出现在相同上下文或共享共同邻居的情况。本文提出了一种新的 DTDG 表示学习方法 DTFormer，它从传统的 GNN+RNN 框架转向基于 Transformer 的架构。我们的方法利用注意力机制同时处理每个时间戳的图中的拓扑信息和沿时间戳的图的时间动态，从而克服了 GNN 和 RNN 的上述基本缺陷。此外，我们通过结合节点之间的交叉关系和集成多补丁模块来增强模型的表达能力。在六个公共动态图基准数据集上进行的大量实验证实了我们模型的有效性，达到了 SOTA 性能。||
|**2024-07-25**|[CSWin-UNet: Transformer UNet with Cross-Shaped Windows for Medical Image Segmentation](http://arxiv.org/abs/2407.18070)|null|深度学习，特别是卷积神经网络（CNN）和Transformer架构，已成为医学图像分割领域广泛研究的焦点，并取得了令人瞩目的成果。然而，CNN 具有归纳偏差，这限制了它们在更复杂、更多样化的分割场景中的有效性。相反，虽然基于 Transformer 的方法擅长捕捉全局和远程语义细节，但它们的计算量很大。在本研究中，我们提出了 CSWin-UNet，一种新颖的 U 形分割方法，将 CSWin 自注意力机制融入 UNet 中，以促进水平和垂直条带自注意力。这种方法显著提高了计算效率和感受野交互。此外，我们创新的解码器利用内容感知重组算子，在预测核的指导下，策略性地重组特征，以实现精确的图像分辨率恢复。我们对包括突触多器官CT、心脏MRI和皮肤病变在内的不同数据集进行了广泛的实证评估，结果表明，CSWin-UNet在保持较低模型复杂度的同时，实现了较高的分割精度。||
|**2024-07-25**|[How Lightweight Can A Vision Transformer Be](http://arxiv.org/abs/2407.17783)|null|本文探讨了一种利用专家混合 (MoE) 来简化而非增强视觉Transformer的策略。MoE层中的每个专家都是一个SwiGLU前馈网络，其中V和W2在层间共享。不使用复杂的注意力或卷积机制。应用深度缩放来逐步减小隐藏层的大小，并分阶段增加专家的数量。使用分组查询注意力机制。我们研究了在小型数据集上进行预训练和不进行预训练的情况下所提出的方法，并研究了迁移学习在这种规模下是否有效。我们发现，即使在参数量为67万的情况下，该架构也具有竞争力。||
|**2024-07-25**|[Transformers on Markov Data: Constant Depth Suffices](http://arxiv.org/abs/2407.17686)|null|基于注意力的 Transformer 在模拟跨领域和模态的生成过程中取得了显著的成功。在本文中，我们研究了 Transformer 对从 k 阶马尔可夫过程中提取的数据的行为，其中序列中下一个符号的条件分布取决于观察到的前 k 个符号。我们凭经验观察到一个与先前发现相矛盾的惊人现象：当训练时间足够长时，即使 k 增长，具有固定深度和每层 1 个头的 Transformer 也能够在从 k 阶马尔可夫源提取的序列上实现较低的测试损失。此外，这种低测试损失是通过 Transformer 表示和学习上下文相关的条件经验分布的能力来实现的。在理论方面，我们的主要结果是，具有单个头和三层的 Transformer 可以表示 k 阶马尔可夫源的上下文相关的条件经验分布，这与我们的经验观察结果一致。在此过程中，我们证明了具有 O(log2(k)) 层的“仅注意力”Transformer 可以通过组合归纳头来表示上下文相关的条件经验分布，以跟踪序列中之前的 k 个符号。通过理解 Transformer 在马尔可夫源上的行为，这些结果为我们目前对 Transformer 如何学习捕捉上下文的机制提供了更多见解。||
|**2024-07-23**|[S-E Pipeline: A Vision Transformer (ViT) based Resilient Classification Pipeline for Medical Imaging Against Adversarial Attacks](http://arxiv.org/abs/2407.17587)|null|视觉Transformer（ViT）凭借其强大的自注意力机制，在医学影像自动精准疾病诊断领域正变得越来越流行。然而，ViT仍然容易受到对抗性攻击，这些攻击可能通过导致对关键疾病的故意错误分类来阻碍诊断过程。在本文中，我们提出了一种新颖的图像分类流水线，即S-E流水线，它执行多个预处理步骤，允许ViT在关键特征上进行训练，从而减少对抗性扰动的影响。我们的方法结合了分割和图像增强技术，例如对比度受限的自适应直方图均衡化（CLAHE）、非锐化掩蔽（UM）和高频增强滤波（HFE）作为预处理步骤，以识别即使在对抗性扰动后仍保持完整的关键特征。实验研究表明，我们新颖的流水线有助于降低对抗性攻击的影响，对于ViT-b32模型降低了72.22%，对于ViT-l32模型降低了86.58%。此外，我们展示了在NVIDIA Jetson Orin Nano板上的端到端部署我们提出的方法，以证明其在通常资源受限的现代手持设备中的实际应用案例。||
|**2024-07-24**|[Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models](http://arxiv.org/abs/2407.17406)|**[link](https://github.com/zhaoyd1/dep_transformer_grammars)**|句法Transformer语言模型旨在通过同时建模语法树和句子来实现更好的泛化能力。虽然先前的工作一直专注于向Transformer添加基于成分结构的信息，但我们引入了依赖Transformer语法（DTG），这是一类新的具有显式基于依赖的归纳偏差的Transformer语言模型。DTG通过修改注意力掩码来模拟具有受限注意力模式的依赖转移系统，通过相对位置编码合并堆栈信息，并通过结合token嵌入和操作嵌入来增强依赖弧表示。在标注了依赖树的句子数据集上训练时，DTG在保持与Transformer语言模型基线相当的困惑度的同时实现了更好的泛化能力。DTG也优于最近基于成分的模型，表明依赖关系可以更好地指导Transformer语言模型。我们的代码发布在https://github.com/zhaoyd1/Dep_Transformer_Grammars。||
|**2024-07-24**|[Embedding-Free Transformer with Inference Spatial Reduction for Efficient Semantic Segmentation](http://arxiv.org/abs/2407.17261)|**[link](https://github.com/hyunwoo137/edaformer)**|我们提出了编码器-解码器注意力Transformer，即EDAFormer，它由无嵌入Transformer（EFT）编码器和利用我们提出的无嵌入注意力（EFA）结构的全注意力解码器组成。提出的EFA是一种新颖的全局上下文建模机制，其重点是发挥全局非线性作用，而不是查询、键和值的特定作用。对于解码器，我们探索了考虑全局性的优化结构，可以提高语义分割性能。此外，我们提出了一种新颖的推理空间缩减（ISR）方法来提高计算效率。与以往的空间缩减注意力方法不同，我们的ISR方法在推理阶段进一步降低了键值分辨率，这可以减轻高效语义分割的计算性能权衡差距。我们的EDAFormer在三个公共基准测试（包括ADE20K、Cityscapes和COCO-Stuff）中，与现有的基于Transformer的语义分割模型相比，显示出最先进的性能和高效的计算。此外，我们的ISR方法在Cityscapes数据集上将计算成本降低了高达61%，而mIoU性能下降却很小。代码可在https://github.com/hyunwoo137/EDAFormer获取。||
|**2024-07-24**|[Improving ICD coding using Chapter based Named Entities and Attentional Models](http://arxiv.org/abs/2407.17230)|null|自然语言处理（NLP）领域的最新进展已促进了各个领域的自动化。然而，临床 NLP 通常依赖于可能无法准确反映现实情况的基准数据集。自动 ICD 编码是一项重要的 NLP 任务，通常使用过时且不平衡的数据集（如 MIMIC-III），由于存在许多误报，现有方法产生的微平均 F1 分数在 0.4 到 0.7 之间。我们的研究引入了一种增强的 ICD 编码方法，该方法通过使用基于章节的命名实体和注意力模型来提高 F1 分数。该方法将出院小结分类到 ICD-9 章节中，并使用特定于章节的数据开发注意力模型，从而无需考虑外部数据来进行代码识别。为了进行分类，我们使用第四章来消除偏差并影响关键实体和权重，而无需使用神经网络，从而创建准确的阈值并提供可解释性以供人工验证。验证后，我们使用具有注意力机制的双向门控循环单元（GRU）和具有多头注意力机制的 Transformer 架构，为第四章中的三个常见代码和三个不常见代码开发了注意力模型。这些模型的平均 Micro-F1 分数为 0.79 和 0.81，表明 ICD 编码的性能得到了显著提高。||
|**2024-07-24**|[RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer](http://arxiv.org/abs/2407.17140)|**[link](https://github.com/lyuwenyu/RT-DETR)**|在本报告中，我们介绍了RT-DETRv2，一种改进的实时检测Transformer（RT-DETR）。RT-DETRv2建立在之前的最先进实时检测器RT-DETR的基础上，并开放了一系列免费增强功能以提高灵活性和实用性，并优化了训练策略以提高性能。为了提高灵活性，我们建议在可变形注意力中为不同尺度的特征设置不同数量的采样点，以实现解码器对多尺度特征的选择性提取。为了增强实用性，我们提出了一个可选的离散采样算子来替换grid_sample算子，与YOLO相比，该算子是RT-DETR特有的。这消除了通常与DETR相关的部署限制。对于训练策略，我们提出了动态数据增强和尺度自适应超参数定制，以在不损失速度的情况下提高性能。源代码和预训练模型将在https://github.com/lyuwenyu/RT-DETR上提供。||
|**2024-07-24**|[LoFormer: Local Frequency Transformer for Image Deblurring](http://arxiv.org/abs/2407.16993)|**[link](https://github.com/deepmed-lab-ecnu/single-image-deblur)**|由于自注意力机制 (SA) 计算复杂度高，目前图像去模糊方法往往采用局部SA或粗粒度的全局SA方法，但这些方法存在着牺牲全局建模能力或缺乏细粒度相关性的缺陷。为了解决这一问题，在不牺牲细粒度细节的情况下有效地建模长距离依赖关系，我们提出了一种名为局部频率Transformer (LoFormer) 的新方法。在LoFormer的每个单元内，我们引入了频域局部通道SA (Freq-LC)，用于同时捕捉低频和高频局部窗口内的互协方差。这些操作的优势在于：(1) 确保粗粒度结构和细粒度细节的学习机会均等，以及 (2) 与粗粒度全局SA方法相比，探索更广泛的表示属性。此外，我们还引入了一个与Freq-LC互补的MLP门控机制，用于过滤无关特征，同时增强全局学习能力。我们的实验表明，LoFormer显著提高了图像去模糊任务的性能，在GoPro数据集上实现了34.09 dB的峰值信噪比，FLOPs为126G。https://github.com/DeepMed-Lab-ECNU/Single-Image-Deblur||
|**2024-07-23**|[TAPTRv2: Attention-based Position Update Improves Tracking Any Point](http://arxiv.org/abs/2407.16291)|null|本文提出了TAPTRv2，一种基于Transformer的方法，建立在TAPTR的基础上，用于解决任意点跟踪（TAP）任务。TAPTR借鉴了检测Transformer（DETR）的设计，将每个跟踪点表示为一个点查询，从而可以利用类似DETR的算法中经过充分研究的操作。TAPTRv2通过解决一个关键问题来改进TAPTR，该问题涉及TAPTR对代价量的依赖，代价量会污染点查询的内容特征，并对可见性预测和代价量计算产生负面影响。在TAPTRv2中，我们提出了一种新的基于注意力的位置更新（APU）操作，并使用键感知可变形注意力来实现。对于每个查询，此操作使用键感知注意力权重来组合其对应的可变形采样位置，以预测新的查询位置。这种设计基于以下观察：局部注意力本质上与代价量相同，两者都是通过查询与其周围特征之间的点积来计算的。通过引入这一新操作，TAPTRv2不仅消除了代价量计算的额外负担，而且还带来了实质性的性能提升。TAPTRv2超越了TAPTR，并在许多具有挑战性的数据集上实现了最先进的性能，证明了其优越性。||
|**2024-07-23**|[HSVLT: Hierarchical Scale-Aware Vision-Language Transformer for Multi-Label Image Classification](http://arxiv.org/abs/2407.16244)|null|多标签图像分类的任务是识别单个图像中的多个对象。考虑到标签中包含的有价值的语义信息和图像中呈现的基本视觉特征，紧密的视觉-语言交互在提高分类性能方面起着至关重要的作用。此外，鉴于单个图像中对象大小和外观的潜在差异，关注不同尺度的特征有助于发现图像中可能的对象。近年来，基于Transformer的方法通过利用建模远程依赖的优势，在多标签图像分类方面取得了巨大成功，但它们也有一些局限性。首先，现有方法将视觉特征提取和跨模态融合视为独立的步骤，导致联合语义空间中的视觉-语言对齐不足。此外，它们仅提取视觉特征并在单一尺度上执行跨模态融合，忽略了具有不同特征的对象。为了解决这些问题，我们提出了一种具有两个吸引人设计的层次化尺度感知视觉-语言Transformer（HSVLT）：(1) 一种层次化的多尺度架构，其中包含一个跨尺度聚合模块，该模块利用从多个尺度提取的联合多模态特征来识别图像中大小和外观各异的对象。(2) 交互式视觉-语言注意力，这是一种新颖的注意力机制模块，它紧密地集成了跨模态交互，从而能够对视觉、语言和多模态特征进行联合更新。我们已经在三个基准数据集上评估了我们的方法。实验结果表明，HSVLT以较低的计算成本超越了最先进的方法。||
|**2024-07-23**|[Channel-Partitioned Windowed Attention And Frequency Learning for Single Image Super-Resolution](http://arxiv.org/abs/2407.16232)|null|近年来，基于窗口的注意力机制在计算机视觉任务中展现出巨大潜力，尤其是在单图像超分辨率（SISR）方面。然而，它可能无法捕捉远距离标记之间的长距离依赖关系。此外，我们发现仅在空间域学习无法传达图像的频率内容，而频率内容是 SISR 的一个关键方面。为了解决这些问题，我们提出了一种新的通道分区注意力Transformer（CPAT），通过沿特征图的高度和宽度顺序扩展窗口来更好地捕捉长距离依赖关系。此外，我们还提出了一种新颖的空间-频率交互模块（SFIM），它结合了空间域和频率域的信息，以提供更全面的特征图信息。这包括关于频率内容的信息，并增强了整个图像的感受野。实验结果证明了我们提出的模块和架构的有效性。特别是，CPAT 超越了当前最先进的方法高达 0.31dB。||
|**2024-07-23**|[On the Benefits of Rank in Attention Layers](http://arxiv.org/abs/2407.16153)|null|基于注意力的机制在机器学习中被广泛使用，尤其是在 Transformer 模型中。然而，注意力矩阵的秩和注意力头的数量等超参数在该架构的所有实现中几乎都以相同的方式进行缩放，而缺乏理论上的 обоснование。在这项工作中，我们展示了注意力机制的秩和注意力头数量之间存在巨大的权衡。具体来说，我们提出了一个简单且自然的 ​​目标函数，该函数可以使用单个满秩注意力头表示任何上下文长度，但除非注意力头的数量是嵌入维度的指数级，否则即使对于较短的上下文长度，低秩注意力也无法逼近该目标函数。此外，我们证明了对于较短的上下文长度，增加深度允许使用低秩注意力逼近目标函数。对于较长的上下文，我们推测满秩注意力是必要的。最后，我们使用现成的 Transformer 模型进行实验，验证了我们的理论发现。||
|**2024-07-23**|[Diffusion Transformer Captures Spatial-Temporal Dependencies: A Theory for Gaussian Process Data](http://arxiv.org/abs/2407.16134)|null|作为 Sora 视频生成模型的支柱，扩散Transformer 成功扩展了扩散模型的容量，为高保真序列数据生成开辟了新途径。与图像等静态数据不同，序列数据由时间索引的连续数据帧组成，表现出丰富的时空依赖性。这些依赖性代表了潜在的动态模型，对于验证生成数据的有效性至关重要。在本文中，我们迈出了理论上的第一步，将扩散Transformer用于捕获时空依赖性。具体来说，我们针对具有不同衰减模式协方差函数的高斯过程数据，建立了扩散 Transformer 的分数逼近和分布估计保证。我们重点介绍了时空依赖性是如何被捕获的，以及它们如何影响学习效率。我们的研究提出了一种新颖的 Transformer 逼近理论，其中 Transformer 的作用是展开一种算法。我们通过数值实验支持了我们的理论结果，提供了强有力的证据表明时空依赖性在注意力层内被捕获，这与我们的逼近理论相一致。||
|**2024-07-22**|[Empirical Capacity Model for Self-Attention Neural Networks](http://arxiv.org/abs/2407.15425)|null|近年来，大型预训练自注意力神经网络，或称 Transformer，在各种任务中取得了巨大成功。模型在给定任务上的性能取决于其记忆和泛化训练数据的能力。拥有数十亿参数的大型 Transformer 模型理论上具有巨大的内容记忆能力。然而，目前用于优化的算法远未达到理论上的容量，而且容量也高度依赖于内容。在本文中，我们关注于使用常见的训练算法和合成训练数据获得的这些模型的记忆容量。基于结果，我们推导了一个适用于通用 Transformer 的经验容量模型 (ECM)。在可以定义任务目标记忆能力的情况下，ECM 可用于设计具有最佳参数数量的任务特定 Transformer 模型。||
|**2024-07-22**|[Towards Robust Vision Transformer via Masked Adaptive Ensemble](http://arxiv.org/abs/2407.15385)|null|对抗训练（AT）可以通过有意地将对抗样本注入训练数据来帮助提高视觉Transformer（ViT）对抗对抗攻击的鲁棒性。然而，这种对抗样本注入方式不可避免地会在一定程度上导致标准准确率下降，因此需要在标准准确率和鲁棒性之间进行权衡。此外，主流的对抗训练方案仍然容易受到自适应攻击的攻击。为了解决这些缺点，本文提出了一种新颖的ViT架构，包括一个检测器和一个分类器，它们通过我们新开发的自适应集成模块连接起来。具体来说，我们通过实验发现，检测对抗样本可以受益于引导反向传播技术。基于这一发现，我们引入了一种新的多头自注意力（MSA）机制来增强我们的检测器嗅探对抗样本的能力。然后，我们采用了一个带有两个编码器的分类器，分别从干净图像和对抗样本中提取视觉表示，并使用我们的自适应集成模块自适应地调整来自两个编码器的视觉表示的比例，以实现准确分类。这种设计使我们的ViT架构能够在标准准确率和鲁棒性之间取得更好的平衡。此外，我们的自适应集成技术允许我们屏蔽输入数据中随机的图像块子集，从而提高ViT对自适应攻击的鲁棒性，同时保持较高的标准准确率。实验结果表明，我们的ViT架构在CIFAR-10数据集上分别达到了90.3%和49.8%的最佳标准准确率和对抗鲁棒性。||
|**2024-07-22**|[Attention Beats Linear for Fast Implicit Neural Representation Generation](http://arxiv.org/abs/2407.15355)|**[link](https://github.com/roninton/anr)**|隐式神经表示（INR）作为一种数据表示方法越来越受欢迎，并已成为创新生成模型的先决条件。与推理效率较低的使用梯度的方法不同，采用超网络来生成多层感知器（MLP）中的参数（用于执行 INR 函数）已成为一种有前途且高效的替代方案。然而，作为一种全局连续函数，MLP 在对高度不连续的信号进行建模方面具有挑战性，导致训练阶段收敛缓慢且重建性能不准确。此外，MLP 需要大量的表示参数，这意味着数据表示效率低下。在本文中，我们提出了一种新颖的基于注意力的局部 INR（ANR），它由局部注意力层（LAL）和全局 MLP 组成，将坐标特征与数据特征相结合并将其转换为有意义的输出。随后，我们设计了一个实例表示框架，该框架提供了一个类似于 Transformer 的超网络，将数据实例表示为紧凑的表示向量。利用实例特定的表示向量和实例无关的 ANR 参数，目标信号可以很好地重建为连续函数。我们进一步解决了在获得超分辨率推理结果时使用变分坐标产生的混叠伪影。跨越四个数据集的大量实验展示了我们 ANR 方法的显著效果，例如在 CelebA 数据集上将 PSNR 值从 37.95dB 提高到 47.25dB。代码发布在 https://github.com/Roninton/ANR。||
|**2024-07-22**|[RoadPainter: Points Are Ideal Navigators for Topology transformER](http://arxiv.org/abs/2407.15349)|null|拓扑推理旨在提供对道路场景的精确理解，使自动驾驶系统能够识别安全高效的路线。在本文中，我们提出了RoadPainter，这是一种使用多视图图像检测和推理车道中心线拓扑结构的创新方法。RoadPainter背后的核心理念是从每个中心线掩码中提取一组点，以提高中心线预测的准确性。我们首先实现了一个Transformer解码器，它集成了混合注意力机制和真虚拟分离策略来预测粗略的车道中心线并建立拓扑关联。然后，我们在Transformer解码器预测的中心线点的引导下生成中心线实例掩码。此外，我们从每个掩码中导出另一组点，并将它们与先前检测到的中心线点组合以进行进一步细化。此外，我们引入了一个可选模块，该模块结合了标准定义（SD）地图，以进一步优化中心线检测并增强拓扑推理性能。在OpenLane-V2数据集上的实验评估证明了RoadPainter的最新性能。||
|**2024-07-22**|[Efficient Multi-disparity Transformer for Light Field Image Super-resolution](http://arxiv.org/abs/2407.15329)|null|本文介绍了多尺度视差Transformer (MDT)，这是一种为光场图像超分辨率（LFSR）量身定制的新型Transformer，它解决了传统方法中固有的对子孔径图像进行不加区分的处理所导致的计算冗余和视差纠缠问题。MDT采用多分支结构，每个分支利用独立的视差自注意力（DSA）机制来针对特定的视差范围，从而有效地降低了计算复杂度并解开了视差纠缠。在此架构的基础上，我们提出了LF-MDTNet，一种高效的LFSR网络。实验结果表明，LF-MDTNet在2倍和4倍尺度上分别比现有的最先进方法提高了0.37 dB和0.41 dB的PSNR，在参数更少、速度更快的情况下实现了优越的性能。||
|**2024-07-19**|[PolyFormer: Scalable Node-wise Filters via Polynomial Graph Transformer](http://arxiv.org/abs/2407.14459)|**[link](https://github.com/air029/polyformer)**|谱图神经网络在图表示学习方面表现出优异的性能。然而，许多当前的方法侧重于为所有节点使用共享的多项式系数，即学习节点统一的滤波器，这限制了滤波器对节点级任务的灵活性。最近的DSF试图通过基于位置编码学习节点级系数来克服这一限制。然而，位置编码的初始化和更新过程非常繁琐，阻碍了其在大规模图上的可扩展性。在这项工作中，我们提出了一种可扩展的节点级滤波器PolyAttn。利用注意力机制，PolyAttn可以高效地直接学习节点级滤波器，提供强大的表示能力。基于PolyAttn，我们引入了名为PolyFormer的完整模型。从图Transformer模型的角度来看，PolyFormer在节点内计算注意力分数，具有很高的可扩展性。此外，该模型捕获了频谱信息，在保持效率的同时增强了表达能力。凭借这些优势，PolyFormer为节点级任务提供了可扩展性和表达能力之间理想的平衡。大量实验表明，我们提出的方法在学习任意节点级滤波器方面表现出色，在同构图和异构图上均表现出优异的性能，并能处理包含多达1亿个节点的图。代码可在https://github.com/air029/PolyFormer获取。||
|**2024-07-19**|[TorchGT: A Holistic System for Large-scale Graph Transformer Training](http://arxiv.org/abs/2407.14106)|null|图神经网络变换器 (Graph Transformer) 是一种超越图神经网络 (GNN) 的新型图学习架构。尽管出现了鼓舞人心的算法进步，但它们的实际应用仍然有限，特别是在涉及数百万个节点的现实世界图上。我们观察到，现有的图神经网络变换器在处理大规模图时失败的主要原因是计算量大、可扩展性有限以及模型质量较差。基于这些观察，我们提出了 TorchGT，这是第一个高效、可扩展且准确的图神经网络变换器训练系统。TorchGT 在不同级别优化训练。在算法层面，TorchGT 利用图稀疏性，引入了一种计算效率高且精度保持的双重交织注意力机制。在运行时层面，TorchGT 通过轻量级通信的集群感知图并行机制跨工作节点扩展训练。在内核层面，弹性计算重构通过以动态方式减少内存访问延迟，进一步优化了计算。大量实验表明，TorchGT 可将训练速度提高 62.7 倍，并支持高达 100 万个节点的图序列长度。||
|**2024-07-18**|[DuoFormer: Leveraging Hierarchical Visual Representations by Local and Global Attention](http://arxiv.org/abs/2407.13920)|null|我们在此提出一种新颖的层次化Transformer模型，它巧妙地将卷积神经网络 (CNN) 的特征提取能力与视觉Transformer (ViT) 的高级表征潜力相结合。针对ViT缺乏归纳偏置和依赖大量训练数据集的问题，我们的模型采用CNN主干网络生成层次化的视觉表示。然后，这些表示通过一种创新的patch标记化方法适配为Transformer输入。我们还引入了一种“尺度注意力”机制，用于捕捉跨尺度依赖关系，补充patch注意力以增强空间理解并保留全局感知。我们的方法在小型和中等规模的医学数据集上显著优于基线模型，证明了其效率和泛化能力。这些组件被设计为即插即用的模块，适用于不同的CNN架构，并且可以适应多种应用。代码可在https://github.com/xiaoyatang/DuoFormer.git获取。||
|**2024-07-18**|[Attention in SRAM on Tenstorrent Grayskull](http://arxiv.org/abs/2407.13885)|**[link](https://github.com/moritztng/grayskull-attention)**|当Transformer的自注意力层实现使用SRAM而不是DRAM时，它们可以实现显著的加速。Tenstorrent Grayskull架构提供了一个大型SRAM，分布在核心网格中。这项工作为Grayskull提出了一个融合内核，通过结合矩阵乘法、注意力分数缩放和Softmax运算，专门利用其大型SRAM。此外，还提出了一个利用SRAM的专用Softmax内核和一个作为基线的CPU实现。在Grayskull上，从查询和键计算注意力权重时，Softmax运算占用了大部分运行时间。与CPU实现相比，专用Softmax内核的速度提升高达 $10\times$，并且融合内核内部的Softmax实现比专用Softmax内核快约$1.8\times$。所有实现的时间和内存复杂度都是序列长度的二次方。目前，对于普通用户来说，Grayskull e150比Nvidia H100 PCIe（最先进的GPU）便宜约$30\times$，并且SRAM容量约为其$1.5\times$ 。||
|**2024-07-18**|[Revisiting Attention for Multivariate Time Series Forecasting](http://arxiv.org/abs/2407.13806)|**[link](https://github.com/joeland4/fsatten-soatten)**|当前用于多元时间序列预测（MTSF）的Transformer方法都基于传统的注意力机制。它们涉及序列嵌入和对Q、K和V进行线性投影，然后在这个潜在空间内计算注意力。我们从未深入研究注意力机制来探索这种映射空间是否对MTSF是最优的。为了探究这个问题，本研究首先提出了一种基于频域空间的新型注意力机制——频谱注意力（FSatten）。它采用傅里叶变换进行嵌入，并引入了多头频谱缩放（MSS）来代替传统的Q和K线性映射。FSatten可以准确捕捉序列之间的周期性依赖关系，并且在不改变主流架构的情况下优于传统的注意力机制。我们进一步设计了一种更通用的方法，称为缩放正交注意力（SOatten）。我们提出了一种基于邻域相似性偏差的正交嵌入和头部耦合卷积（HCC），以指导模型学习全面的依赖模式。实验表明，FSatten和SOatten优于使用传统注意力的SOTA模型，使其成为MTSF基本注意力机制的良好替代方案。代码和日志文件将在以下地址发布：https://github.com/Joeland4/FSatten-SOatten。||
|**2024-07-18**|[GPSFormer: A Global Perception and Local Structure Fitting-based Transformer for Point Cloud Understanding](http://arxiv.org/abs/2407.13519)|**[link](https://github.com/changshuowang/GPSFormer)**|尽管预训练方法在点云理解方面取得了显著进展，但如何在不依赖外部数据的情况下直接从不规则点云中捕获复杂的形状信息仍然是一个巨大的挑战。为了解决这个问题，我们提出了GPSFormer，一种创新的基于全局感知和局部结构拟合的Transformer，它能够以极高的精度从点云中学习详细的形状信息。GPSFormer的核心是全局感知模块（GPM）和局部结构拟合卷积（LSFConv）。具体来说，GPM利用自适应可变形图卷积（ADGConv）识别特征空间中相似特征之间的短程依赖关系，并采用多头注意力机制（MHA）学习特征空间内所有位置之间的长程依赖关系，最终实现上下文表示的灵活学习。受泰勒级数的启发，我们设计了LSFConv，它可以从显式编码的局部几何结构中学习低阶基本信息和高阶细化信息。通过将GPM和LSFConv作为基本组件集成，我们构建了GPSFormer，这是一种能够有效捕获点云全局和局部结构的尖端Transformer。大量实验验证了GPSFormer在三个点云任务（形状分类、零件分割和少样本学习）中的有效性。GPSFormer的代码可在\url{https://github.com/changshuowang/GPSFormer}获取。||
|**2024-07-18**|[OAT: Object-Level Attention Transformer for Gaze Scanpath Prediction](http://arxiv.org/abs/2407.13335)|**[link](https://github.com/hkust-nisl/oat_ecc)**|视觉搜索在我们日常生活中非常重要。有效分配视觉注意力对于有效完成视觉搜索任务至关重要。先前的研究主要是在像素级别对图像中的视觉注意力空间分配进行建模，例如使用显著图。然而，新出现的证据表明，视觉注意力是由物体而非像素强度引导的。本文介绍了物体级注意力Transformer (OAT)，它可以预测人类在杂乱场景中搜索目标物体的扫描路径。OAT 使用编码器-解码器架构。编码器捕获图像中物体的位置和外观以及目标的信息。解码器通过整合编码器和解码器的输出特征，将注视扫描路径预测为一系列物体注视点。我们还提出了一种新的位置编码，可以更好地反映物体之间的空间关系。我们在亚马逊书籍封面数据集和我们收集的一个新的视觉搜索数据集上评估了 OAT。与基于空间注意力的算法相比，OAT 预测的注视扫描路径与人类注视模式更加一致，这在已建立的指标和基于行为的新指标上都得到了验证。我们的结果证明了 OAT 的泛化能力，因为它可以准确预测未见过的布局和目标物体的人类扫描路径。||
|**2024-07-18**|[Transformers with Stochastic Competition for Tabular Data Modelling](http://arxiv.org/abs/2407.13238)|**[link](https://github.com/avoskou/Transformers-with-Stochastic-Competition-for-Tabular-Data-Modelling)**|尽管表格数据在众多行业和领域中普遍存在且意义重大，但在深度学习领域，对其探索相对不足。即使在今天，神经网络的性能也常常落后于梯度提升决策树 (GBDT) 等技术。然而，最近出现的一些模型开始缩小这一差距，在各种设置中都优于 GBDT，并在该领域引起了越来越多的关注。受此进展的启发，我们引入了一种专门为表格数据设计的新型随机深度学习模型。该模型的基础是基于 Transformer 的架构，通过策略性的架构修改和利用两种形式的随机竞争，精心调整以适应表格数据的独特属性。首先，我们采用随机“局部赢者通吃”单元，通过随机性和稀疏性来提高泛化能力。其次，我们引入了一种新颖的嵌入层，该层通过随机竞争机制从备选线性嵌入层中进行选择。我们在各种广泛使用且公开可用的数据集上验证了该模型的有效性。我们证明，通过结合这些元素，我们的模型实现了高性能，标志着深度学习在表格数据应用方面取得了重大进展。||
|**2024-07-18**|[Transformer-based Single-Cell Language Model: A Survey](http://arxiv.org/abs/2407.13205)|null|Transformer凭借其出色的并行处理能力和高度灵活的注意力机制，在自然语言处理领域取得了显著成就。此外，越来越多的基于Transformer的研究被提出用于对单细胞数据进行建模。在本综述中，我们试图系统地总结基于Transformer的单细胞语言模型及其应用。首先，我们详细介绍了Transformer的结构和原理。然后，我们回顾了用于单细胞数据分析的单细胞语言模型和大型语言模型。此外，我们还探讨了单细胞语言模型的数据集及其在下游任务中的应用，如批次校正、细胞聚类、细胞类型注释、基因调控网络推断和扰动反应。最后，我们讨论了单细胞语言模型面临的挑战，并提出了有前景的研究方向。我们希望这篇综述能够为对单细胞语言模型方向感兴趣的研究人员提供最新的参考。||
|**2024-07-18**|[Unified-EGformer: Exposure Guided Lightweight Transformer for Mixed-Exposure Image Enhancement](http://arxiv.org/abs/2407.13170)|null|尽管人工智能在图像处理方面取得了长足进步，但在监控和摄影等许多现实场景中至关重要的混合曝光问题仍然没有得到充分解决。传统的图像增强技术和当前的transformer模型主要关注过度曝光或曝光不足，存在局限性。为了弥合这一差距，我们引入了统一曝光引导transformer（Unified-EGformer）。我们提出的解决方案建立在先进的transformer架构之上，配备了局部像素级细化和全局细化模块，用于色彩校正和图像整体调整。我们采用引导注意力机制来精确识别曝光受损区域，确保其适应各种现实条件。U-EGformer采用轻量级设计，内存占用（峰值内存）仅为约1134 MB（10万参数），推理时间为95毫秒（比平均速度快9.61倍），是监控和自动驾驶等实时应用的可行选择。此外，我们的模型具有高度的泛化能力，只需最少的微调即可处理多个任务和数据集，并使用单一架构。||
|**2024-07-17**|[Hybrid Dynamic Pruning: A Pathway to Efficient Transformer Inference](http://arxiv.org/abs/2407.12893)|null|在深度学习领域，Transformer 模型变得非常重要，其在从语言理解到图像识别等许多领域都带来了进步，涵盖了广泛的应用。尽管取得了成功，但由于其二次计算强度和内存需求，在实时应用（尤其是边缘设备）中部署这些模型带来了重大挑战。为了克服这些挑战，我们引入了一种新颖的混合动态剪枝 (HDP)，这是一种高效的算法-架构协同设计方法，它利用头部稀疏性、块稀疏性和近似机会来加速 Transformer，以减少注意力计算和减少内存访问。通过观察注意力分数和注意力头中的巨大冗余，我们提出了一种新颖的基于整数的行平衡块剪枝方法，在运行时剪枝注意力矩阵中不重要的块，还提出了基于整数的头部剪枝方法，在运行早期阶段检测和剪枝不重要的头部。我们还提出了一种减少注意力计算的近似方法。为了以更低的延迟和更高的功率效率有效地支持这些方法，我们提出了一种 HDP 协处理器架构。||
|**2024-07-17**|[Global-Local Similarity for Efficient Fine-Grained Image Recognition with Vision Transformers](http://arxiv.org/abs/2407.12891)|**[link](https://github.com/arkel23/GLSim)**|细粒度识别涉及对来自次级宏观类别的图像进行分类，由于类间差异小，这是一项具有挑战性的任务。为了克服这个问题，大多数方法都执行由特征提取主干网络支持的判别性特征选择，然后进行高级特征细化步骤。最近，许多研究表明视觉Transformer作为细粒度识别主干网络的潜力，但它们使用其注意力机制来选择判别性标记的计算成本可能很高。在这项工作中，我们提出了一种新颖且计算成本低的度量标准来识别图像中的判别区域。我们比较了由 CLS 标记（一种由 Transformer 用于分类的可学习标记）给出的图像全局表示与各个补丁的局部表示之间的相似性。我们选择与获得的裁剪区域具有最高相似性的区域，这些区域通过相同的 Transformer 编码器转发。最后，对原始表示和裁剪表示的高级特征进行进一步细化，以便进行更稳健的预测。通过广泛的实验评估，我们证明了所提出方法的有效性，在各种数据集上获得了良好的准确性。此外，与其他方法相比，我们的方法以低得多的计算成本实现了这些结果。代码和检查点可在以下网址获得：\url{https://github.com/arkel23/GLSim}。||
|**2024-07-17**|[CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision Transformer Inference](http://arxiv.org/abs/2407.12736)|null|视觉Transformer（ViT）代表了机器学习方法在计算机视觉领域的突破性转变。与传统方法不同，ViT采用在自然语言处理中广泛应用的自注意力机制来分析图像块。尽管ViT在建模视觉任务方面具有优势，但在硬件平台（尤其是现场可编程门阵列（FPGA））上部署ViT会带来相当大的挑战。这些挑战主要源于ViT的非线性计算以及高计算和内存需求。本文介绍了CHOSEN，这是一个软硬件协同设计框架，旨在解决这些挑战，并提供一个自动化框架，用于在FPGA上部署ViT，从而最大限度地提高性能。我们的框架建立在三个基本贡献之上：旨在最大化带宽的多内核设计，主要针对多DDR内存库的优势；表现出最小精度下降的近似非线性函数；以及对FPGA上可用逻辑块的有效利用；以及高效的编译器，通过提出一种用于设计空间探索的新算法来最大限度地提高计算内核的性能和内存效率，以找到实现最佳吞吐量和延迟的最佳硬件配置。与最先进的ViT加速器相比，CHOSEN在DeiT-S和DeiT-B模型上的吞吐量分别提高了1.5倍和1.42倍。||
|**2024-07-17**|[ARTEMIS: A Mixed Analog-Stochastic In-DRAM Accelerator for Transformer Neural Networks](http://arxiv.org/abs/2407.12638)|null|Transformer已成为自然语言处理（NLP）和计算机视觉的强大工具。通过注意力机制，与循环神经网络（RNN）和卷积神经网络（CNN）等传统方法相比，这些模型表现出显著的性能提升。然而，Transformer通常需要大量的执行时间，因为它们需要大量的计算和内存占用。内存内处理（PIM）和近内存计算（NMC）是加速Transformer的有前途的解决方案，因为它们提供了高计算并行性和内存带宽。然而，设计PIM/NMC架构来支持Transformer神经网络中各层之间需要移动的复杂操作和海量数据仍然是一个挑战。我们提出了ARTEMIS，一种用于Transformer模型的混合模拟-随机内存计算加速器。通过对传统DRAM阵列进行最小程度的修改，ARTEMIS通过使用一种新型DRAM金属-金属电容器支持随机计算进行乘法运算和时间模拟累加，从而有效地降低了Transformer模型执行的相关成本。我们的分析表明，与GPU、TPU、CPU和最先进的PIM Transformer硬件加速器相比，ARTEMIS表现出至少3.0倍的速度提升、1.8倍的能耗降低和1.9倍的能效提升。||
|**2024-07-17**|[Frequency Guidance Matters: Skeletal Action Recognition by Frequency-Aware Mixed Transformer](http://arxiv.org/abs/2407.12322)|**[link](https://github.com/wenhanwu95/freqmixformer)**|近年来，transformer 在对骨架序列的长时依赖关系建模方面展现出巨大潜力，因此在骨架动作识别领域受到越来越多的关注。然而，现有的基于 transformer 的方法严重依赖于朴素的注意力机制来捕获时空特征，这在学习具有相似运动模式的判别性表示方面存在不足。为了应对这一挑战，我们引入了频率感知混合 Transformer (FreqMixFormer)，专门用于识别具有细微判别性运动的相似骨骼动作。首先，我们引入了一个频率感知注意力模块，通过将关节特征嵌入到频率注意力图中来解构骨架频率表示，旨在根据频率系数区分判别性运动。随后，我们开发了一种混合 transformer 架构，将空间特征与频率特征相结合，以对全面的频率-空间模式进行建模。此外，我们还提出了一个时间 transformer 来提取跨帧的全局相关性。大量实验表明，FreqMiXFormer 在 3 个流行的骨架动作识别数据集（包括 NTU RGB+D、NTU RGB+D 120 和 NW-UCLA 数据集）上优于 SOTA。||
|**2024-07-16**|[Hierarchical Separable Video Transformer for Snapshot Compressive Imaging](http://arxiv.org/abs/2407.11946)|**[link](https://github.com/pwangcs/hisvit)**|Transformer在解决视频快照压缩成像（SCI）的反问题方面取得了最先进的性能，该问题的病态性源于空间掩蔽和时间混叠的混合退化。然而，以前的Transformer缺乏对这种退化的洞察力，因此性能和效率有限。在这项工作中，我们定制了一个高效的重建架构，在早期层中没有时间聚合，并使用分层可分离视频Transformer（HiSViT）作为构建块。HiSViT由多组跨尺度可分离多头自注意力（CSS-MSA）和门控自调制前馈网络（GSM-FFN）以及密集连接构成，每个网络都在不同尺度的独立通道部分内进行，用于多尺度交互和远程建模。通过将空间操作与时间操作分离，CSS-MSA引入了一种归纳偏差，即在帧内而不是帧之间更加关注，同时节省了计算开销。GSM-FFN旨在通过门控机制和分解的时空卷积来增强局部性。大量实验表明，我们的方法在复杂性和参数相当或更少的情况下，性能优于以前的方法> 0.5 dB。源代码和预训练模型已发布在https://github.com/pwangcs/HiSViT。||
|**2024-07-16**|[Relaxing Graph Transformers for Adversarial Attacks](http://arxiv.org/abs/2407.11764)|null|已有研究表明，图神经网络 (GNN) 容易受到对抗性攻击。尽管图变换器 (GT) 在多个基准测试中优于消息传递 GNN，但其对抗性鲁棒性尚未得到探索。然而，攻击 GT 具有挑战性，因为它们的位置编码 (PE) 和特殊的注意力机制难以区分。我们通过针对基于 (1) 随机游走 PE、(2) 成对最短路径 PE 和 (3) 谱 PE 的三种代表性架构来克服这些挑战，并提出第一个针对 GT 的自适应攻击。我们利用攻击来评估 (a) 节点分类中结构扰动的鲁棒性；以及 (b) （假新闻）图分类的节点注入攻击。我们的评估表明，它们可能非常脆弱，并强调了我们工作的重要性以及自适应攻击的必要性。||
|**2024-07-16**|[Video-Language Alignment Pre-training via Spatio-Temporal Graph Transformer](http://arxiv.org/abs/2407.11677)|**[link](https://github.com/gxym/stgt)**|视频语言对齐是一项重要的多模态任务，有利于各种下游应用，例如视频文本检索和视频问答。现有方法要么利用视频文本对中的多模态信息，要么应用全局和局部对齐技术来提高对齐精度。然而，这些方法往往无法充分探索视频内视觉标记之间以及不同视频文本对之间时空关系。在本文中，我们提出了一种新颖的时空图Transformer模块，用于统一学习视频语言对齐预训练（称为STGT）的时空上下文。具体来说，我们的STGT将时空图结构信息与Transformer块中的注意力相结合，有效地利用了时空上下文。通过这种方式，我们可以对视觉标记之间的关系进行建模，提高视频文本对齐精度，从而有利于下游任务。此外，我们提出了一种自相似性对齐损失，以探索视频和文本中固有的自相似性。通过对比学习实现的初始优化，可以进一步提高视频和文本之间的对齐精度。在包括视频文本检索和视频问答在内的具有挑战性的下游任务上的实验结果证明了我们方法的优越性能。||
|**2024-07-16**|[Probing the Efficacy of Federated Parameter-Efficient Fine-Tuning of Vision Transformers for Medical Image Classification](http://arxiv.org/abs/2407.11573)|null|随着大型预训练 Transformer 模型的出现，针对各种下游任务对这些模型进行微调成为一个关键问题。训练数据的缺乏、数据孤岛的存在以及严格的隐私限制加剧了医学影像领域微调问题的难度，对能够实现预训练模型协同微调的算法提出了强烈需求。此外，由于这些模型的规模庞大，需要使用参数高效微调（PEFT）来减少联邦学习中的通信负担。在这项工作中，我们系统地研究了各种联邦 PEFT 策略，用于使视觉 Transformer（ViT）模型（在大规模自然图像数据集上进行预训练）适应医学图像分类。除了评估已知的 PEFT 技术外，我们还引入了 PEFT 算法的联邦变体，例如视觉提示微调（VPT）、视觉提示的低秩分解、随机块注意力微调以及混合 PEFT 方法（如低秩适应（LoRA）+VPT）。此外，我们进行了全面的实证分析，以确定适用于联邦环境的最佳 PEFT 方法，并了解数据分布对联邦 PEFT 的影响，特别是对于域外（OOD）和非独立同分布（non-IID）数据。本研究的主要见解是，虽然大多数联邦 PEFT 方法在域内迁移中表现良好，但在处理 OOD 和非 IID 场景时，准确性和效率之间存在着巨大的权衡，而这在医学影像中很常见。具体而言，微调/交换参数每减少一个数量级，准确率就会下降 4%。因此，初始模型的选择对于联邦 PEFT 至关重要。如果可能的话，最好使用从域内医学图像数据中学习到的医学基础模型，而不是通用视觉模型。||
|**2024-07-16**|[Understanding Counting in Small Transformers: The Interplay between Attention and Feed-Forward Layers](http://arxiv.org/abs/2407.11542)|**[link](https://github.com/SPOC-group/counting-attention)**|我们对在直方图任务上训练的简单transformer模型进行了全面分析，该任务的目标是计算来自固定字母表的输入序列中每个项目的出现次数。尽管表面上很简单，但这项任务展现出丰富的现象学，使我们能够描述不同的架构组件如何促进不同算法解决方案的出现。特别是，我们展示了实现解决方案的两种性质不同的机制的存在，即基于关系和基于清单的计数。模型可以实现哪种解决方案，很大程度上取决于注意力机制、激活函数、记忆容量和序列开始标记的存在的精确选择。通过内省在计数任务上学习的模型，我们找到了这两种机制形成的证据。从更广泛的角度来看，我们的分析提供了一个框架，以理解transformer模型的不同架构组件的交互如何塑造不同的算法解决方案和近似值。||
|**2024-07-16**|[Not Another Imputation Method: A Transformer-based Model for Missing Values in Tabular Datasets](http://arxiv.org/abs/2407.11540)|**[link](https://github.com/cosbidev/naim)**|在训练和测试人工智能模型时，处理表格数据集中的缺失值是一项重大挑战，这个问题通常使用插补技术来解决。在此，我们介绍“非另一种插补方法”（NAIM），这是一种基于Transformer的新型模型，专门设计用于解决此问题，而无需传统的插补技术。NAIM 采用特征特定的嵌入和掩蔽自注意力机制，可以有效地从可用数据中学习，从而避免了对缺失值进行插补的必要性。此外，还引入了一种新的正则化技术，以增强模型从不完整数据中泛化的能力。我们在 5 个公开可用的表格数据集上对 NAIM 进行了广泛评估，证明其性能优于 6 种最先进的机器学习模型和 4 种深度学习模型，并在必要时将每种模型与 3 种不同的插补技术配对。结果突出了 NAIM 在存在缺失数据的情况下提高预测性能和鲁棒性的功效。为了促进在没有传统插补方法的情况下处理缺失数据的进一步研究和实际应用，我们在 https://github.com/cosbidev/NAIM 上提供了 NAIM 的代码。||
|**2024-07-16**|[Haze-Aware Attention Network for Single-Image Dehazing](http://arxiv.org/abs/2407.11505)|null|单图像去雾是计算机视觉中的一项关键挑战，旨在去除图像中的雾霾并恢复清晰的背景细节。认识到传统的基于物理模型的方法的局限性和当前基于注意力的解决方案的低效率，我们提出了一种新的去雾网络，它结合了创新的雾霾感知注意力模块 (HAAM) 和多尺度频率增强模块 (MFEM)。HAAM 受大气散射模型的启发，将物理原理巧妙地融入到高维特征中，以实现目标去雾。它在图像恢复过程中提取潜在特征，从而显着提升指标，而 MFEM 有效地增强了高频细节，从而避免了小波或傅里叶变换的复杂性。它采用多尺度场来提取和强调关键频率成分，同时最大限度地减少参数开销。我们的雾霾感知注意力网络 (HAA-Net) 集成了一个简单的 U-Net 框架，用于单图像去雾，在效率和有效性方面明显优于现有的基于注意力和变压器的模型。HAA-Net 在各种公共数据集上进行了测试，树立了新的性能基准。我们的工作不仅推动了图像去雾领域的发展，而且还为更广泛的计算机视觉应用中的注意力机制设计提供了见解。||
|**2024-07-16**|[RIMformer: An End-to-End Transformer for FMCW Radar Interference Mitigation](http://arxiv.org/abs/2407.11459)|null|调频连续波 (FMCW) 雷达在遥感领域发挥着举足轻重的作用。随着 FMCW 雷达部署密度的增加，相互干扰也随之加剧，这削弱了雷达的探测能力，并威胁到系统的可靠性和安全性。本文提出了一种基于Transformer的端到端 FMCW 雷达干扰抑制 (RIM) 新方法，称为 RIMformer。在 RIMformer 中，提出了一种双多头自注意力机制来捕获中频 (IF) 信号不同距离元素之间的相关性。此外，还集成了改进的卷积块，以利用卷积的强大功能来提取局部特征。该架构旨在以端到端的方式处理时域 IF 信号，从而避免了额外的手动数据处理步骤。改进后的解码器结构确保了网络的并行化，从而提高了计算效率。通过仿真和测量实验验证了该方法的准确性和有效性。结果表明，所提出的 RIMformer 可以有效地抑制干扰并恢复目标信号。||
|**2024-07-16**|[PADRe: A Unifying Polynomial Attention Drop-in Replacement for Efficient Vision Transformer](http://arxiv.org/abs/2407.11306)|null|我们提出了多项式注意力即插即用替换 (PADRe)，这是一个新颖且统一的框架，旨在替换 Transformer 模型中传统的自注意力机制。值得注意的是，最近出现的几种替代注意力机制，包括 Hyena、Mamba、SimA、Conv2Former 和 Castling-ViT，都可以被视为我们 PADRe 框架的特定实例。PADRe 利用多项式函数并借鉴了逼近论的既定结果，在不影响准确性的情况下提高了计算效率。PADRe 的关键组件包括乘法非线性，我们使用简单的、硬件友好的操作（例如 Hadamard 积）来实现这些操作，仅产生线性计算和内存成本。PADRe 进一步避免了使用 Softmax 等复杂函数的需要，但与传统的自注意力相比，它保持了相当或更高的准确性。我们评估了 PADRe 作为跨各种计算机视觉任务的自注意力替代品的有效性。这些任务包括图像分类、基于图像的 2D 对象检测和 3D 点云对象检测。实验结果表明，PADRe 的运行速度明显快于传统的自注意力（在服务器 GPU 和移动 NPU 上快 11 倍到 43 倍），同时在 Transformer 模型中替换自注意力时保持了相似的准确性。||
|**2024-07-15**|[Conquering images and the basis of transformative action](http://arxiv.org/abs/2407.11254)|null|我们快速沉浸在网络生活中，这让我们都病了。人工智能技术通过生成、个性化和传播迷人的图像，以令人作呕的精度和规模将大众的思想和心灵商品化。在线网络、人工智能 (AI)、社交媒体和数字新闻推送通过建立将我们的社区和身份细分和极化的叙述，来微调我们的信念和追求。与此同时，那些掌控着这些技术的人征服了我们内心生活、社会关系、地球和宇宙的最后疆域。在注意力经济中，我们的能动性受到限制，我们的活力因其自恋的追求和享乐而枯竭。生成式人工智能增强了那些使生命同质化和根除的力量，这不是通过某种愚蠢的“奇点”事件，而是通过贬低人类的创造力、劳动和社会生活。我们将使用一个破碎的镜头，来审视叙述和网络如何在心理、社会和算法层面影响我们。我们将讨论原子化的图像——疏远而不是激励个人的理想和追求——如何劫持人们的能动性，以维持毁灭他们的力量。我们将发现帝国是如何建立数字网络，以优化社会并鼓励自恋者强化社会二元对立，从而使消费、剥削和等级制度的不断扩张永久化。世界上的结构性等级制度通过我们信念和思想中的等级制度得到强化。只有将图像视为图像，并欣赏对立叙述之间的相似性，我们才能促进变革行动，并摆脱困扰我们生活的军国主义体系。||
|**2024-07-12**|[Region Attention Transformer for Medical Image Restoration](http://arxiv.org/abs/2407.09268)|**[link](https://github.com/yaziwel/region-attention-transformer-for-medical-image-restoration)**|基于Transformer的方法在医学图像恢复方面表现出令人印象深刻的结果，这归功于其在空间维度上的多头自注意力（MSA）机制。然而，大多数现有的Transformer在固定且粗略划分的区域（例如，整个图像或固定块）内进行注意力计算，导致来自不相关区域的干扰和连续图像内容的碎片化。为了克服这些挑战，我们引入了一种新的区域注意力Transformer（RAT），它利用了基于区域的多头自注意力机制（R-MSA）。R-MSA使用鲁棒的Segment Anything Model (SAM)将输入图像动态地划分为不重叠的语义区域，然后在这些区域内执行自注意力。这种区域划分更加灵活和可解释，确保只有来自相似语义区域的像素相互补充，从而消除了来自不相关区域的干扰。此外，我们引入了一个焦点区域损失来引导我们的模型自适应地关注恢复高难度区域。大量实验表明，RAT在各种医学图像恢复任务中是有效的，包括PET图像合成、CT图像去噪和病理图像超分辨率。代码可在https://github.com/Yaziwel/Region-Attention-Transformer-for-Medical-Image-Restoration.git获取。||
|**2024-07-12**|[DANIEL: A fast Document Attention Network for Information Extraction and Labelling of handwritten documents](http://arxiv.org/abs/2407.09103)|**[link](https://github.com/shulk97/daniel)**|从手写文档中提取信息传统上涉及三个不同的步骤：文档布局分析、手写文本识别和命名实体识别。最近的方法试图使用完全端到端的架构将这些步骤集成到一个过程中。尽管如此，当应用于纯文本信息提取时，这些集成方法的性能尚未与语言模型相匹配。在本文中，我们介绍了DANIEL（用于信息提取和标注的文档注意力网络），这是一种完全端到端的架构，集成了语言模型，专为全面理解手写文档而设计。DANIEL对整页文档执行布局识别、手写文本识别和命名实体识别。此外，它可以同时跨多种语言、布局和任务进行学习。对于命名实体识别，可以通过输入提示指定要应用的本体。该架构采用能够处理任何大小图像而无需调整大小的卷积编码器，并与基于transformer的语言模型的自回归解码器配对。DANIEL在四个数据集上取得了具有竞争力的结果，包括在RIMES 2009和M-POPP上手写文本识别和在IAM NER上命名实体识别方面的新SOTA性能。此外，DANIEL比现有方法快得多。我们在\url{https://github.com/Shulk97/daniel}上提供了源代码和训练模型的权重。||
|**2024-07-12**|[Beyond Image Prior: Embedding Noise Prior into Conditional Denoising Transformer](http://arxiv.org/abs/2407.09094)|**[link](https://github.com/yuanfeihuang/condformer)**|现有的基于学习的去噪方法通常训练模型从大规模数据集中泛化图像先验，但在现实世界场景中遇到的噪声分布变化时表现不佳。在这项工作中，我们通过强调噪声和图像先验之间的明显分离，为去噪挑战提出了一个新的视角。这一见解构成了我们开发条件优化框架的基础，旨在克服传统去噪框架的限制。为此，我们引入了一种局部噪声先验估计 (LoNPE) 算法，它可以直接从单个原始噪声图像中准确估计噪声先验。这种估计作为相机传感器成像环境的显式先验表示，与场景的图像先验不同。此外，我们设计了一个辅助可学习的 LoNPE 网络，专为 sRGB 噪声图像的实际应用而定制。利用估计的噪声先验，我们提出了一种新的条件去噪 Transformer (Condformer)，通过将噪声先验纳入条件自注意力机制。这种集成允许 Condformer 将优化过程分割成多个显式子空间，从而显着增强模型的泛化能力和灵活性。对合成数据集和真实世界数据集的广泛实验评估表明，所提出的方法比当前最先进的方法取得了优越的性能。源代码可在 https://github.com/YuanfeiHuang/Condformer 获取。||
|**2024-07-12**|[Global Attention-Guided Dual-Domain Point Cloud Feature Learning for Classification and Segmentation](http://arxiv.org/abs/2407.08994)|null|以往的研究已经证明了基于点的点云分析任务神经网络模型的有效性。然而，如何为原始点坐标生成有效的输入嵌入仍然是一个关键问题。此外，邻域聚合作为网络主干中的一个关键组件，其效率有限也是一个问题。在本文中，我们提出了一个全局注意力引导的双域特征学习网络（GAD）来解决上述问题。我们首先设计了上下文位置增强型Transformer（CPT）模块，该模块配备了改进的全局注意力机制，以生成全局感知的输入嵌入，作为后续聚合的指导。然后，级联双域K近邻特征融合（DKFF）模块，通过新的双域特征学习进行有效的特征聚合，该学习同时考虑了局部几何关系和长距离语义联系。在多个点云分析任务（例如分类、零件分割和场景语义分割）上的大量实验表明，所提出的方法和设计的模块具有优越的性能。||
|**2024-07-12**|[Deep Attention Driven Reinforcement Learning (DAD-RL) for Autonomous Vehicle Decision-Making in Dynamic Environment](http://arxiv.org/abs/2407.08932)|null|由于与周围车辆的动态交互，城市环境中的自动驾驶汽车 (AV) 决策本质上具有挑战性。为了安全规划，自动驾驶汽车必须了解场景中各种时空交互的重要性。当代工作使用巨大的 Transformer 架构来编码交互，主要用于轨迹预测，导致计算复杂性增加。为了在不影响时空理解和性能的情况下解决这个问题，我们提出了简单的深度注意力驱动强化学习 (DADRL) 框架，该框架动态分配并将周围车辆的重要性纳入到自动驾驶汽车的强化学习驱动决策过程中。我们引入了一种以自动驾驶汽车为中心的时空注意力编码 (STAE) 机制，用于学习与不同周围车辆的动态交互。为了理解地图和路线环境，我们采用环境编码器从环境地图中提取特征。时空表示与环境编码相结合，提供了全面的状态表示。由此产生的模型使用软演员评论家 (SAC) 算法进行训练。我们在没有交通信号灯的 SMARTS 城市基准场景中评估了所提出的框架，以证明 DADRL 优于最近的最新方法。此外，消融研究强调了环境编码器和时空注意力编码器在实现卓越性能方面的重要性。||
|**2024-07-11**|[TractGraphFormer: Anatomically Informed Hybrid Graph CNN-Transformer Network for Classification from Diffusion MRI Tractography](http://arxiv.org/abs/2407.08883)|null|利用深度神经网络研究大脑连接和非影像表型之间的关系越来越受到关注。然而，卷积网络设计往往忽略了大脑白质网络的局部和全局特性。我们引入了 TractGraphFormer，这是一个专为弥散磁共振成像纤维束追踪设计的混合图CNN-Transformer深度学习框架。该模型利用了白质结构的局部解剖特征和全局特征依赖性。图CNN模块捕获白质几何形状和灰质连接，以聚合解剖学上相似的白质连接的局部特征，而Transformer模块使用自注意力机制来增强全局信息学习。此外，TractGraphFormer 包含一个注意力模块，用于解释预测性的白质连接。在性别预测测试中，TractGraphFormer 在儿童 (n=9345) 和年轻人 (n=1065) 的大型数据集中表现出强大的性能。总的来说，我们的方法表明，WM 中广泛的连接可以预测个体的性别，并且在两个数据集中都识别出一致的预测性解剖束。所提出的方法强调了整合局部解剖信息和全局特征依赖性以提高基于弥散磁共振成像纤维束追踪的机器学习预测性能的潜力。||
|**2024-07-11**|[Jet Tagging with More-Interaction Particle Transformer](http://arxiv.org/abs/2407.08682)|null|本研究介绍了一种名为 More-Interaction Particle Transformer (MIParT) 的新型深度学习神经网络，用于喷注标记。该框架结合了我们自主设计的 More-Interaction Attention (MIA) 机制，增加了粒子交互嵌入的维度。我们使用顶夸克标记和夸克-胶子数据集测试了 MIParT。结果表明，MIParT 不仅在准确率和 AUC 指标上与 LorentzNet 持平，而且在背景抑制方面显著优于 ParT 模型。具体而言，在顶夸克标记数据集上，当信号效率为 30% 时，其背景抑制率提高了约 25%，在夸克-胶子数据集上提高了 3%。此外，MIParT 仅需要 ParT 30% 的参数量和 47% 的计算复杂度，证明了在降低模型复杂度且无需在大型数据集上进行大量预训练的情况下，也能实现高性能。这些结果表明，MIParT 有潜力提升粒子物理学中喷注标记和事件识别的效率基准。||
|**2024-07-11**|[Latent Spaces Enable Transformer-Based Dose Prediction in Complex Radiotherapy Plans](http://arxiv.org/abs/2407.08650)|**[link](https://github.com/edwardwang1/ldformer)**|越来越多的证据表明，立体定向消融体部放射治疗 (SABR) 可用于治疗肺部多发癌灶。多病灶肺部 SABR 计划十分复杂，创建起来需要耗费大量资源。本研究提出了一种新颖的两阶段潜在变换器框架 (LDFormer)，用于预测不同病灶数量的肺部 SABR 计划的剂量分布。在第一阶段，将患者解剖信息和剂量分布编码到潜在空间中。在第二阶段，变换器学习从解剖潜在特征预测剂量潜在特征。修改因果注意力以适应不同数量的病灶。在病灶内部和周围的剂量适形度方面，LDFormer 优于最先进的生成对抗网络，并且在考虑重叠病灶时，性能差距会更大。LDFormer 可以在 30 秒内在消费级硬件上生成 3D 剂量分布预测，并有可能帮助医生做出临床决策、降低资源成本并加速治疗计划制定。||
|**2024-07-11**|[FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](http://arxiv.org/abs/2407.08608)|null|注意力机制作为Transformer架构的核心层，是大语言模型和长上下文应用的瓶颈。FlashAttention阐述了一种通过最小化内存读/写来加速GPU上注意力计算的方法。然而，它尚未利用最新硬件的新功能，FlashAttention-2在H100 GPU上仅实现了35%的利用率。我们开发了三种主要技术来加速Hopper GPU上的注意力计算：利用Tensor Core和TMA的异步性来(1) 通过warp专业化来重叠整体计算和数据移动，以及(2) 交错块级矩阵乘法和softmax操作，以及(3) 利用硬件对FP8低精度支持的块量化和非相干处理。我们证明，我们的方法FlashAttention-3在使用FP16时，在H100 GPU上实现了1.5-2.0倍的加速，达到高达740 TFLOPs/s（75%的利用率），而在使用FP8时，接近1.2 PFLOPs/s。我们验证了FP8 FlashAttention-3的数值误差比基线FP8注意力机制低2.6倍。||
|**2024-07-12**|[ADMM Based Semi-Structured Pattern Pruning Framework For Transformer](http://arxiv.org/abs/2407.08334)|null|自然语言处理(NLP)通过Transformer模型取得了巨大的成功。然而，该模型拥有数亿甚至数十亿个参数，这对其在个人电脑或小型服务器上的部署造成了巨大的负担。为了解决这个问题，我们可以使模型的权重矩阵更加稀疏，或者压缩注意力层。模式剪枝是最重要的剪枝方法之一，它允许在每个划分的模式块中选择固定数量的参数并将其剪枝。然而，模式剪枝的效果受到每层权重区域内稀疏性的严格限制。在本文中，我们首先介绍了基于交替方向乘子法(ADMM)的模式剪枝框架，以重塑激活映射的分布。具体来说，我们建议将Transformer上的模式剪枝公式化为一个约束优化问题，并使用ADMM来优化该问题。通过这种方式，初始的密集特征图被转换为区域稀疏的特征图。因此，我们可以基于模式剪枝方法在获得更高压缩率的同时获得更好的性能。此外，本文还提供了具有局部稀疏性的ADMM的理论推导。最后，我们还将提出的基于ADMM的框架扩展到量化上，以证明其泛化能力，并使用SR-STE来避免梯度消失问题。我们对GLUE数据集上的分类任务进行了广泛的实验。值得注意的是，我们实现了50%的压缩率，同时在GLUE数据集上保持了80.1的总分。||
|**2024-07-11**|[HDT: Hierarchical Document Transformer](http://arxiv.org/abs/2407.08330)|**[link](https://github.com/autonomousvision/hdt)**|在本文中，我们提出了分层文档Transformer（HDT），这是一种针对结构化分层文档量身定制的新型稀疏Transformer架构。此类文档在众多领域（包括科学、法律或医学）中极为重要。然而，大多数现有解决方案效率低下，并且无法利用文档固有的结构。HDT通过引入辅助锚标记并将注意力机制重新设计为稀疏的多级层次结构来利用文档结构。这种方法促进了不同级别标记之间的信息交换，同时保持了稀疏性，从而在利用文档结构作为归纳偏差的同时提高了计算和内存效率。我们通过开发一种考虑文档层次结构的新型稀疏注意力内核来解决实现HDT依赖于样本的分层注意力模式的技术挑战。正如我们的实验所示，利用文档中存在的结构信息可以加快收敛速度，提高样本效率，并在下游任务中获得更好的性能。||
|**2024-07-11**|[Improving Dental Diagnostics: Enhanced Convolution with Spatial Attention Mechanism](http://arxiv.org/abs/2407.08114)|null|深度学习已经成为医疗保健领域的一种变革性工具，通过分析复杂的影像数据，在牙科诊断方面取得了重大进展。本文提出了一种增强的 ResNet50 架构，并集成了 SimAM 注意力模块，以解决牙科影像中对比度有限的挑战，并在降低计算需求的同时优化深度学习性能。SimAM 模块被添加到第二个 ResNet 模块之后，通过捕获空间依赖性和增强重要特征来改进特征提取。我们的模型在各种特征提取技术中均表现出优异的性能，F1 分数达到 0.676，优于 VGG、EfficientNet、DenseNet 和 AlexNet 等传统架构。这项研究强调了我们的方法在提高牙科影像分析的分类准确性和鲁棒性方面的有效性，突出了深度学习在提高牙科诊断准确性和效率方面的潜力。像我们这样的高级人工智能模型的集成有望彻底改变牙科诊断，为改善患者结果和更广泛地采用人工智能技术做出贡献。||
|**2024-07-10**|[MambaVision: A Hybrid Mamba-Transformer Vision Backbone](http://arxiv.org/abs/2407.08083)|**[link](https://github.com/nvlabs/mambavision)**|我们提出了一种名为 MambaVision 的新型混合 Mamba-Transformer 骨干网络，该网络专为视觉应用而设计。我们的核心贡献包括重新设计 Mamba 公式，以增强其对视觉特征进行有效建模的能力。此外，我们对将视觉 Transformer (ViT) 与 Mamba 集成的可行性进行了全面的消融研究。我们的结果表明，在 Mamba 架构的最后几层配备多个自注意力块可以极大地提高模型捕获远程空间依赖关系的能力。基于我们的发现，我们引入了一系列具有分层架构的 MambaVision 模型，以满足各种设计标准。对于 ImageNet-1K 数据集上的图像分类任务，MambaVision 模型变体在 Top-1 准确率和图像吞吐量方面实现了新的最先进 (SOTA) 性能。在下游任务（如 MS COCO 和 ADE20K 数据集上的目标检测、实例分割和语义分割）中，MambaVision 优于规模相当的骨干网络，并展现出更优越的性能。代码：https://github.com/NVlabs/MambaVision。||
|**2024-07-10**|[Study on Aspect Ratio Variability toward Robustness of Vision Transformer-based Vehicle Re-identification](http://arxiv.org/abs/2407.07842)|null|视觉Transformer（ViT）在车辆重识别（ReID）任务中表现出色。然而，图像或视频输入的非正方形纵横比可能会严重影响重识别的性能。为了解决这个问题，我们在本文中提出了一种新的基于ViT的ReID框架，它融合了在各种纵横比上训练的模型。我们的主要贡献有三方面：（i）我们分析了VeRi-776和VehicleID数据集上的纵横比性能，并根据原始图像的纵横比指导输入设置。(ii)我们在ViT分块过程中引入了分块混合的图像内增强（由空间注意力分数引导），并实现了不均匀步幅以更好地匹配目标纵横比。(iii)我们提出了一种动态特征融合的ReID网络，增强了模型的鲁棒性。我们的ReID方法在VehicleID数据集上实现了91.0%的平均精度均值（mAP），显著优于最接近的现有技术水平（CAL）结果80.9%。||
|**2024-07-10**|[PosFormer: Recognizing Complex Handwritten Mathematical Expression with Position Forest Transformer](http://arxiv.org/abs/2407.07764)|**[link](https://github.com/sjtu-deepvisionlab/posformer)**|手写数学表达式识别（HMER）在数字化教育和自动化办公等人机交互场景中有着广泛的应用。近年来，采用编码器-解码器架构的基于序列的模型已被普遍用于解决这项任务，方法是直接预测表达式图像的LaTeX序列。然而，这些方法只是隐式地学习LaTeX提供的语法规则，由于复杂的结构关系和多样化的书写风格，可能无法描述符号之间的位置和层次关系。为了克服这一挑战，我们提出了一种用于HMER的位置森林转换器（PosFormer），它联合优化了两个任务：表达式识别和位置识别，从而明确地实现位置感知的符号特征表示学习。具体来说，我们首先设计了一个位置森林，将数学表达式建模为森林结构，并解析符号之间的相对位置关系。在不需要额外标注的情况下，每个符号在森林中都被分配了一个位置标识符，以表示其相对空间位置。其次，我们提出了一个隐式注意力校正模块，以便在基于序列的解码器架构中准确地捕捉HMER的注意力。大量实验验证了PosFormer的优越性，它始终优于最先进的方法，在单行CROHME 2014/2016/2019、多行M2E和复杂MNE数据集上分别获得了2.03%/1.22%/2.00%、1.83%和4.62%的提升，而没有增加任何延迟或计算成本。代码可在https://github.com/SJTU-DeepVisionLab/PosFormer获取。||
|**2024-07-10**|[SvANet: A Scale-variant Attention-based Network for Small Medical Object Segmentation](http://arxiv.org/abs/2407.07720)|**[link](https://github.com/anthonyweidai/SvANet)**|早期发现和准确诊断可以预测恶性疾病转化的风险，从而提高有效治疗的可能性。出现轻微症状和小面积感染区域是不祥的征兆，也是疾病早期诊断的首要任务。深度学习算法，如卷积神经网络（CNN），已被用于分割自然或医学物体，并显示出良好的效果。然而，由于 CNN 中的卷积和池化操作会导致信息丢失和压缩缺陷，因此分析图像中小区域的医学物体仍然是一个挑战。这些损失和缺陷随着网络深度的增加而变得越来越严重，特别是对于小型医学物体。为了应对这些挑战，我们提出了一种新的基于尺度变化注意力机制的网络 (SvANet)，用于医学图像中精确的小尺度物体分割。SvANet 由蒙特卡洛注意力机制、尺度变化注意力机制和视觉Transformer组成，它结合了跨尺度特征并减轻了压缩伪影，从而增强了对小型医学物体的辨别能力。定量实验结果表明，SvANet 具有优越的性能，在分割肾脏肿瘤、皮肤病变、肝脏肿瘤、息肉、手术切除细胞、视网膜血管和精子方面，平均 Dice 系数分别达到了 96.12%、96.11%、89.79%、84.15%、80.25%、73.05% 和 72.58%，这些物体在 KiTS23、ISIC 2018、ATLAS、PolypGen、TissueNet、FIVES 和 SpermHealth 数据集中所占的图像面积均不到 1%。||
|**2024-07-09**|[CAPformer: Compression-Aware Pre-trained Transformer for Low-Light Image Enhancement](http://arxiv.org/abs/2407.07056)|null|弱光图像增强(LLIE)随着手机摄影需求的激增而取得了进步，但许多现有方法忽略了压缩，而压缩是资源受限的手机摄影的一个关键问题。大多数LLIE方法忽视了这一点，阻碍了它们的有效性。在这项研究中，我们调查了JPEG压缩对弱光图像的影响，并揭示了由于暗区普遍存在低像素值，JPEG压缩导致大量信息丢失。因此，我们提出了压缩感知预训练Transformer (CAPformer)，采用一种新的预训练策略，从未压缩的弱光图像中学习无损信息。此外，提出的亮度引导自注意力(BGSA)机制增强了合理的信息收集。实验表明，我们的方法在减轻压缩对LLIE的影响方面具有优越性，展示了其在资源受限场景下改进LLIE的潜力。||
|**2024-07-09**|[ERQ: Error Reduction for Post-Training Quantization of Vision Transformers](http://arxiv.org/abs/2407.06794)|null|视觉Transformer（ViT）的训练后量化（PTQ）因其在模型压缩方面的效率而备受关注。然而，现有方法通常忽略了量化权重和激活之间错综复杂的相互依赖性，导致了相当大的量化误差。在本文中，我们提出了ERQ，一种精心设计的两步PTQ方法，用于依次减少由激活和权重量化引起的量化误差。ERQ首先引入了激活量化误差减少（Aqer），该方法策略性地将激活量化误差的最小化制定为岭回归问题，并通过使用全精度更新权重来解决该问题。随后，ERQ引入了权重量化误差减少（Wqer），该方法采用迭代方法来减轻由权重量化引起的量化误差。在每次迭代中，采用经验导出的高效代理来细化量化权重的舍入方向，并结合岭回归求解器来减少权重量化误差。实验结果证明了我们方法的有效性。值得注意的是，对于W3A4 ViT-S，ERQ在准确率方面比最先进的GPTQ高出22.36%。||
|**2024-07-09**|[Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules](http://arxiv.org/abs/2407.06677)|null|在Transformer中，是否总是需要从浅层到深层计算token？Vanilla Transformer及其变体的持续成功似乎给出了毫无疑问的“是”。然而，在这项工作中，我们试图打破这种深度有序的惯例，提出了一种名为模块混合（MoM）的新颖架构，其动机是基于一种直觉，即任何层，无论其位置如何，只要具备所需的处理能力，都可以用来计算token。MoM的构建始于一组有限的模块，这些模块由多头注意力机制和前馈网络定义，每个模块都由其独特的参数化来区分。然后，两个路由器迭代地从集合中选择注意力模块和前馈模块来处理token。这种选择在token的前向传递过程中动态扩展计算图，最终形成模块的组合。我们证明了MoM不仅为Transformer及其众多变体提供了一个统一的框架，而且还提供了一种灵活且可学习的方法来减少Transformer参数化中的冗余。我们使用OpenWebText预训练了各种MoM。实验结果表明，不同参数量的MoM在GLUE和XSUM基准测试中始终优于Vanilla Transformer。更有趣的是，在固定的参数预算下，与GPT-2-large相比，MoM-large能够将计算图的深度增加38%以上，从而在GLUE上获得1.4的绝对增益，在XSUM上获得1的绝对增益。另一方面，MoM-large还可以将深度减少60%以上，同时每层包含更多模块，与GPT-2-large相比，TFLOPs减少了16%，内存使用量减少了43%，同时保持了相当的性能。||
|**2024-07-09**|[CTRL-F: Pairing Convolution with Transformer for Image Classification via Multi-Level Feature Cross-Attention and Representation Learning Fusion](http://arxiv.org/abs/2407.06673)|**[link](https://github.com/hosamsherif/ctrl-f)**|Transformer因其强大的容量和全局处理能力，在计算机视觉领域受到越来越多的关注。然而，Transformer是数据密集型的，与卷积神经网络（ConvNets）相比，其泛化能力受到限制，特别是在数据有限的情况下进行训练时，因为它们缺乏ConvNets中存在的内置空间归纳偏差。在本文中，我们致力于将卷积和Transformer的优势结合起来，以完成图像分类任务。为此，我们提出了一种名为CTRL-F的新型轻量级混合网络，该网络通过表示学习融合和多级特征交叉注意力将卷积与Transformer配对。我们的网络包括一个卷积分支和一个名为多级特征交叉注意力（MFCA）的新型Transformer模块。MFCA模块对在不同卷积阶段获得的多级特征表示进行操作。它通过两个独立的Transformer分支处理从这些多级特征表示中提取的小块标记和大块标记，这两个分支通过交叉注意力机制进行通信和知识交换。我们使用称为自适应知识融合（AKF）和协作知识融合（CKF）的新型表示融合技术，将从卷积路径获得的局部响应与从MFCA模块获得的全局响应融合在一起。实验表明，无论是在大数据集上从头开始训练，还是在数据量少的情况下，我们的CTRL-F变体都能达到最先进的性能。例如，CTRL-F在Oxford-102 Flowers和PlantVillage数据集上从头开始训练时，Top-1准确率分别达到82.24%和99.91%，超过了最先进的模型，这体现了我们模型在图像分类任务上的鲁棒性。代码地址：https://github.com/hosamsherif/CTRL-F||
|**2024-07-09**|[Enhancing spatial auditory attention decoding with neuroscience-inspired prototype training](http://arxiv.org/abs/2407.06498)|null|空间听觉注意解码 (Sp-AAD) 技术旨在通过神经记录来确定多说话者场景中的听觉注意方向。尽管最近的 Sp-AAD 算法取得了成功，但其性能受到脑电图数据中特定于试验的特征的阻碍。本研究旨在针对这些特征提高解码性能。神经科学研究表明，空间听觉注意可以反映在不同频带脑电图能量的拓扑分布中。这一见解促使我们提出原型训练，这是一种受神经科学启发的 Sp-AAD 方法。该方法构建了具有增强的能量分布表示和减少的特定于试验的特征的原型，使模型能够更好地捕获听觉注意特征。为了实施原型训练，进一步提出了一种采用脑电图小波变换的 EEGWaveNet。详细的实验表明，采用原型训练的 EEGWaveNet 在各种数据集上均优于其他竞争模型，并且也验证了所提出方法的有效性。作为一种独立于模型架构的训练方法，原型训练为 Sp-AAD 领域提供了新的见解。||
|**2024-07-08**|[FGA: Fourier-Guided Attention Network for Crowd Count Estimation](http://arxiv.org/abs/2407.06110)|null|人群计数在城市规划、人群管理和公共安全等领域越来越具有社会意义。本文介绍了一种新颖的用于人群计数估计的注意力机制——傅里叶引导注意力（FGA），旨在解决现有基于卷积的注意力网络中全局模式捕获效率低下的问题。FGA 通过利用快速傅里叶变换 (FFT) 以及用于全局特征的空间注意力和用于半全局和局部特征的通道注意力卷积，有效地捕获了包括全尺度全局模式在内的多尺度信息。FGA 的架构涉及一种双路径方法：(1) 通过 FFT 处理全尺度全局特征的路径，允许在频域中高效提取信息，以及 (2) 使用传统卷积和通道注意力处理剩余特征图以获取半全局和局部特征的路径。这种双路径架构使 FGA 能够无缝集成频率和空间信息，增强其捕获不同人群模式的能力。我们将 FGA 应用于两个流行的人群计数工作 CSRNet 和 CANNet 的最后几层，以评估模块在基准数据集（如 ShanghaiTech-A、ShanghaiTech-B、UCF-CC-50 和 JHU++ 人群）上的性能。实验表明，基于均方误差 (MSE) 和平均绝对误差 (MAE) 指标，所有数据集都有显着改进，显示出与最近最先进方法相当的性能。此外，我们利用 Grad-CAM 热图，使用定性分析来说明可解释性，以显示 FGA 在捕获人群模式方面的有效性。||
|**2024-07-08**|[HiT-SR: Hierarchical Transformer for Efficient Image Super-Resolution](http://arxiv.org/abs/2407.05878)|null|Transformer 在包括图像超分辨率 (SR) 在内的计算机视觉任务中表现出了良好的性能。然而，流行的基于 Transformer 的 SR 方法通常采用计算复杂度与窗口大小呈二次关系的窗口自注意力机制，导致采用固定的较小窗口，感受野有限。在本文中，我们提出了一种将基于 Transformer 的 SR 网络转换为分层 Transformer (HiT-SR) 的通用策略，在保持高效设计的同时，利用多尺度特征提升 SR 性能。具体来说，我们首先将常用的固定小窗口替换为扩展的分层窗口，以聚合不同尺度的特征并建立远程依赖关系。考虑到大窗口所需的密集计算，我们进一步设计了一种空间-通道相关性方法，该方法对窗口大小具有线性复杂度，可以有效地从分层窗口中收集空间和通道信息。大量实验验证了我们 HiT-SR 的有效性和效率，我们改进后的 SwinIR-Light、SwinIR-NG 和 SRFormer-Light 版本以更少的参数、FLOPs 和更快的速度（约 7 倍）获得了最先进的 SR 结果。||
|**2024-07-08**|[MSTF: Multiscale Transformer for Incomplete Trajectory Prediction](http://arxiv.org/abs/2407.05671)|null|运动预测在自动驾驶系统中起着至关重要的作用，它使车辆能够根据周围车辆的预测执行碰撞警告和合理的局部路径规划。然而，普遍的方法通常假设观察到的轨迹是完整的，而忽略了物体遮挡、范围限制和传感器故障导致的缺失值可能产生的影响。这种疏忽不可避免地会影响轨迹预测的准确性。为了应对这一挑战，我们提出了一个名为多尺度变换器（MSTF）的端到端框架，该框架专为不完整轨迹预测而精心设计。MSTF集成了多尺度注意力头（MAH）和基于信息增量的模式自适应（IIPA）模块。具体来说，MAH组件利用多头注意力机制，从不同的时间粒度同时捕获轨迹序列的多尺度运动表示。这种方法有助于在不同尺度上对运动中的全局依赖关系进行建模，从而减轻缺失值的不利影响。此外，IIPA模块通过分析数据中的缺失模式，自适应地提取跨时间步长的运动连续性表示。连续性表示在更高层次上描绘了运动趋势，引导MSTF生成与运动连续性一致的预测。我们使用两个大规模真实世界数据集评估了我们提出的MSTF模型。实验结果表明，MSTF在不完整轨迹预测任务中优于最先进（SOTA）模型，展示了其在解决自动驾驶系统运动预测中缺失值挑战方面的有效性。||
|**2024-07-08**|[Graph Attention with Random Rewiring](http://arxiv.org/abs/2407.05649)|null|图神经网络 (GNN) 已成为图结构深度学习的基础。现代 GNN 的关键范式包括消息传递、图重连和图 Transformer。本文介绍了具有随机结构的图重连注意力机制 (GRASS)，这是一种结合了这三种范式优点的新型 GNN 架构。GRASS 通过叠加随机正则图来重新连接输入图，增强了远程信息传播，同时保留了输入图的结构特征。它还采用了一种专为图结构数据量身定制的独特加性注意力机制，在保持计算效率的同时提供了图归纳偏差。我们的实证评估表明，GRASS 在多个基准数据集上实现了最先进的性能，证实了其实用性。||
|**2024-07-08**|[On the Power of Convolution Augmented Transformer](http://arxiv.org/abs/2407.05591)|null|Transformer架构在语言建模方面引发了革命性的进步。然而，最近的架构方法，例如状态空间模型，已经弥合了性能差距。受此启发，我们研究了卷积增强型Transformer（CAT）在召回、复制和长度泛化任务中的优势。CAT在注意力层的K/Q/V嵌入中加入了卷积滤波器。通过CAT，我们展示了卷积的局部性与注意力的全局视图协同作用。与诸如Mamba或Transformer等类似架构不同，CAT可以使用单层可证明地解决关联召回（AR）和复制任务，同时还享有保证的长度泛化能力。我们还通过描述卷积如何通过总结上下文窗口和创建突出的摘要标记来参与注意力，从而减轻对完全注意力的需求，从而建立了卷积和注意力之间的计算权衡。对真实数据集的评估证实了我们的发现，并证明CAT及其变体确实增强了语言建模性能。||
|**2024-07-05**|[Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations](http://arxiv.org/abs/2407.04543)|**[link](https://github.com/namednil/step)**|模型需要适当的归纳偏置才能有效地从少量数据中学习并在训练分布之外进行系统泛化。 虽然 Transformer 具有高度的通用性和强大的功能，但它们仍然可以从增强的结构性归纳偏置中受益，以完成 seq2seq 任务，尤其是那些涉及句法转换的任务，例如将主动语态转换为被动语态或语义解析。在本文中，我们建议通过中间预训练来增强 Transformer 的结构性归纳偏置，以根据转换的描述对依存树执行合成生成的句法转换。我们的实验表明，这有助于对组块等句法任务进行少样本学习，并且还提高了语义解析的结构泛化能力。我们的分析表明，中间预训练会导致注意力头能够跟踪需要对哪些标记应用哪些句法转换，并且模型可以在下游任务中利用这些注意力头。||
|**2024-07-05**|[LayerShuffle: Enhancing Robustness in Vision Transformers by Randomizing Layer Execution Order](http://arxiv.org/abs/2407.04513)|null|由于人工神经网络的架构和训练方式，它们通常对测试时的剪枝、替换或打乱层顺序的操作缺乏鲁棒性。然而，这些特性对于不同的应用场景非常重要，例如在分布式神经网络架构中，执行顺序无法得到保证，或者网络的某些部分在推理过程中可能发生故障。为了解决这些问题，我们提出了一系列针对视觉Transformer的训练方法，其最重要的组成部分是在训练时随机化注意力模块的执行顺序。我们证明，使用我们提出的方法，假设可以容忍在相同模型规模下精度降低（约20%），视觉Transformer确实能够适应测试时任意层的执行顺序。我们还发现，我们训练的模型可以彼此随机合并，生成功能完备的“科学怪人”模型，并且与源模型相比，性能没有损失。最后，我们在测试时对模型进行层剪枝，发现其性能下降平缓。||
|**2024-07-05**|[Hard-Attention Gates with Gradient Routing for Endoscopic Image Computing](http://arxiv.org/abs/2407.04400)|**[link](https://github.com/cosmoimd/feature-selection-gates)**|为了解决胃肠道息肉大小评估中的过拟合问题并增强模型泛化能力，我们的研究引入了特征选择门（FSG）或硬注意力门（HAG）以及梯度路由（GR）来进行动态特征选择。该技术旨在通过促进稀疏连接来增强卷积神经网络（CNN）和视觉Transformer（ViT），从而减少过拟合并增强泛化能力。HAG 通过使用可学习权重的稀疏化来实现这一点，作为一种正则化策略。GR 通过独立于主模型的两次前向传递优化 HAG 参数，进一步完善了这一过程，以改进特征重新加权。我们的评估涵盖了多个数据集，包括用于广泛影响评估的 CIFAR-100 和专注于息肉大小估计的专业内窥镜数据集（REAL-Colon、Misawa 和 SUN），涵盖超过 370,000 帧图像中的 200 多个息肉。研究结果表明，我们增强的 HAG 网络大大提高了与息肉大小相关的二分类和三分类任务的性能。具体而言，CNN 在二分类中的 F1 分数提高到 87.8%，而在三分类中，ViT-T 模型的 F1 分数达到 76.5%，优于传统的 CNN 和 ViT-T 模型。为了促进进一步的研究，我们发布了代码库，其中包括 CNN、多流 CNN、ViT 和 HAG 增强变体的实现。该资源旨在标准化内窥镜数据集的使用，为胃肠道息肉大小估计提供公开的训练-验证-测试拆分，以进行可靠和可比较的研究。代码库可在 github.com/cosmoimd/feature-selection-gates 获取。||
|**2024-07-05**|[Batch Transformer: Look for Attention in Batch](http://arxiv.org/abs/2407.04218)|null|人脸表情识别 (FER) 在计算机视觉领域受到了广泛关注，尤其是在人机交互等“自然环境”中。然而，FER 图像存在遮挡、低分辨率、姿态变化、光照变化和主观性等不确定因素，其中包括一些与目标标签不匹配的表情。因此，从一张包含噪声的单个图像中获取的信息很少，而且不可信。这可能会显著降低 FER 任务的性能。为了解决这个问题，我们提出了一种批量转换器 (BT)，它包含所提出的类别批量注意力 (CBA) 模块，通过训练一批图像中反映的特征，而不是来自单个图像的信息，来防止噪声数据过拟合并提取可靠信息。我们还提出了多级注意力 (MLA) 机制，通过捕获每个级别之间的相关性来防止过度拟合特定特征。在本文中，我们提出了一个结合上述方案的批量转换器网络 (BTN)。在各种 FER 基准数据集上的实验结果表明，所提出的 BTN 在 FER 数据集中始终优于最先进的方法。代表性结果证明了所提出的 BTN 在 FER 中的前景。||
|**2024-07-04**|[Adaptive Step-size Perception Unfolding Network with Non-local Hybrid Attention for Hyperspectral Image Reconstruction](http://arxiv.org/abs/2407.04024)|null|深度展开方法和Transformer架构最近在高光谱图像（HSI）重建方面展现出良好的结果。然而，仍然存在两个问题：（1）在数据子问题中，大多数方法使用可学习参数表示步长。然而，对于不同的光谱通道，特征和真实值之间的误差是不相等的。(2) Transformer难以平衡感受野大小和像素级细节信息。为了克服上述缺点，我们提出了一种自适应步长感知展开网络（ASPUN），这是一种基于FISTA算法的深度展开网络，它使用自适应步长感知模块来估计每个光谱通道的更新步长。此外，我们设计了一个非局部混合注意力Transformer（NHAT）模块，用于充分利用Transformer的感受野优势。通过将NLHA插入非局部信息聚合（NLIA）模块，展开网络可以获得更好的重建结果。实验结果表明，我们的ASPUN优于现有的SOTA算法，并取得了最佳性能。||
|**2024-07-03**|[Towards Attention-based Contrastive Learning for Audio Spoof Detection](http://arxiv.org/abs/2407.03514)|null|视觉Transformer（ViT）在计算机视觉分类任务中取得了重大进展。最近，Gong等人（2021）将基于注意力的建模方法引入了几项音频任务。然而，使用ViT进行音频欺骗检测任务的研究相对较少。我们弥合了这一差距，并将ViT引入到这项任务中。基于对SSAST（Gong等人，2022）音频ViT模型进行微调的朴素基线模型实现了次优的等错误率（EER）。为了提高性能，我们提出了一种新颖的基于注意力的对比学习框架（SSAST-CL），该框架使用交叉注意力来辅助表示学习。实验表明，我们的框架成功地 disentangled 了真实和欺骗类别，并有助于学习更好的分类器来完成这项任务。通过适当的数据增强策略，在我们的框架上训练的模型在ASVSpoof 2021挑战赛中取得了具有竞争力的性能。我们提供了比较和消融研究来证明我们的观点。||
|**2024-07-03**|[STF: Sentence Transformer Fine-Tuning For Topic Categorization With Limited Data](http://arxiv.org/abs/2407.03253)|null|如今，从推文中进行主题分类引起了相当多的研究关注。由于这些研究工作，人们提出了不同的分类系统。然而，由于标记数据的数量有限，导致性能指标低下，它们面临着重大挑战。我们提出了句子转换器微调 (STF)，这是一个主题检测系统，它利用预训练的句子转换器模型和微调来准确地对推文主题进行分类。此外，我们还进行了广泛的参数敏感性分析，以针对我们的主题分类任务微调 STF 参数，从而获得最佳性能结果。在两个基准数据集上的实验表明：(1) 所提出的 STF 可以有效地用于对推文主题进行分类，并且优于最新的最先进方法，(2) 所提出的 STF 不需要大量的标记推文就能达到良好的准确性，而这是许多最先进方法的局限性。我们的主要贡献是通过应用预训练的句子转换器语言模型在推文主题分类方面取得了可喜的成果。||
|**2024-07-03**|[Visual Grounding with Attention-Driven Constraint Balancing](http://arxiv.org/abs/2407.03243)|null|与目标检测不同，视觉定位任务需要检测由复杂的自由形式语言描述的对象。为了同时对这种复杂的语义和视觉表示进行建模，最近最先进的研究采用基于Transformer的模型来融合来自两种模态的特征，并进一步引入了各种模块来调制视觉特征，使其与语言表达对齐并消除不相关的冗余信息。然而，它们的损失函数仍然采用常见的目标检测损失，仅仅控制边界框回归输出，未能完全优化上述目标。为了解决这个问题，本文首先分析了基于Transformer模型的注意力机制。在此基础上，我们进一步提出了一个名为注意力驱动约束平衡（AttBalance）的新框架，以优化语言相关区域内视觉特征的行为。大量的实验结果表明，我们的方法带来了令人印象深刻的改进。具体来说，我们在四个不同基准上评估的五种不同模型上都取得了持续的改进。此外，通过将我们的方法集成到QRNet中，我们实现了新的最先进的性能。||
|**2024-07-03**|[Learning Disentangled Representation in Object-Centric Models for Visual Dynamics Prediction via Transformers](http://arxiv.org/abs/2407.03216)|null|最近的研究表明，以对象为中心的表示可以极大地提高学习动力学的准确性，同时也增强了可解释性。在这项工作中，我们将这一想法更进一步，提出了以下问题：“学习解耦表示能否进一步提高以对象为中心的模型中视觉动力学预测的准确性？” 虽然之前已经有一些尝试学习静态图像的解耦表示\citep{nsb}，但据我们所知，我们的工作是第一个尝试在视频的通用环境中做到这一点的工作，而没有对对象可能具有的属性类型做出任何特定假设。我们架构的关键构建块是“块”的概念，其中多个块共同构成一个对象。每个块都表示为给定数量的可学习概念向量的线性组合，并在学习过程中迭代优化。我们模型中的块是以无监督的方式发现的，通过类似于发现槽\citep{slot_attention}的方式关注对象掩码，以学习密集的以对象为中心的表示。我们在发现的块上采用 Transformer 进行自注意力机制来预测下一个状态，从而发现视觉动力学。我们在几个二维和三维基准数据集上进行了一系列实验，证明我们的架构 (1) 可以发现语义上有意义的块 (2) 与最先进的以对象为中心的模型相比，有助于提高动力学预测的准确性 (3) 在训练期间未见过特定属性组合的 OOD 设置中表现明显更好。我们的实验强调了发现解耦表示对于视觉动力学预测的重要性。||
|**2024-07-03**|[Relating CNN-Transformer Fusion Network for Change Detection](http://arxiv.org/abs/2407.03178)|**[link](https://github.com/nust-machine-intelligence-laboratory/rctnet)**|虽然深度学习，特别是卷积神经网络（CNN），已经彻底改变了遥感（RS）变化检测（CD），但现有方法由于忽略了全局上下文和不完整的变化学习，经常遗漏关键特征。此外，transformer 网络难以处理低级细节。RCTNet 通过引入以下内容解决了这些限制：\textbf{(1)} 早期融合骨干网络，用于尽早利用空间和时间特征；\textbf{(2)} 跨阶段聚合（CSA）模块，用于增强时间表示；\textbf{(3)} 多尺度特征融合（MSF）模块，用于在解码器中丰富特征提取；\textbf{(4)} 高效自解密注意力（ESA）模块，利用 Transformer 捕获全局信息和细粒度细节，以实现准确的变化检测。大量实验表明，RCTNet 明显优于传统的遥感图像变化检测方法，显示出显著的改进，并在准确性和计算成本之间取得了最佳平衡。||
|**2024-07-03**|[ISWSST: Index-space-wave State Superposition Transformers for Multispectral Remotely Sensed Imagery Semantic Segmentation](http://arxiv.org/abs/2407.03033)|null|目前，多光谱遥感图像(MSRSI) 语义分割任务面临以下问题：1) 通常只考虑单域特征（即空间域或频率域）；2) 编码器中的下采样操作通常会导致边缘提取精度损失；3) 没有充分考虑 MSRSI 的多通道特征；4) 没有充分利用遥感的先验知识。为了解决上述问题，受量子力学的启发，首次提出了一种用于 MSRSI 语义分割的指标-空间-波态叠加Transformer (ISWSST)，其优势如下：1) 通过自适应投票决策（即集成学习思想）叠加或融合指标、空间和波态来模拟量子叠加，从而成为更强大的分类器并提高分割精度；2) 设计了一种无损小波金字塔编码器-解码器模块，基于小波变换和逆小波变换对图像进行无损重建，模拟量子纠缠，避免边缘提取损失；3) 提出结合多光谱特征（即遥感指数和通道注意力机制）从原始分辨率图像中准确提取地面物体；4) 引入量子力学来解释 ISWSST 的潜在优势。实验表明，ISWSST 在 MSRSI 分割任务中得到了验证，并且优于最先进的架构，有效地提高了分割和边缘提取的精度。代码将在我们的论文被接受后公开。||
|**2024-07-03**|[Graph and Skipped Transformer: Exploiting Spatial and Temporal Modeling Capacities for Efficient 3D Human Pose Estimation](http://arxiv.org/abs/2407.02990)|null|近年来，单目三维人体姿态估计 (HPE) 中的 2D 到 3D 姿态提升引起了广泛的研究兴趣。基于 GNN 的方法和基于 Transformer 的方法由于其先进的空间和时间特征学习能力，已成为主流架构。然而，现有方法通常在空间和时间域中构建关节和帧注意力对齐，导致密集连接，从而引入相当大的局部冗余和计算开销。在本文中，我们采用全局方法来利用时空信息，并通过简洁的图和跳跃 Transformer 架构实现高效的 3D HPE。具体来说，在空间编码阶段，部署粗粒度身体部位以构建具有完全数据驱动自适应拓扑的空间图网络，确保模型在各种姿态下的灵活性和泛化能力。在时间编码和解码阶段，提出了一种简单而有效的跳跃 Transformer 来捕获长期时间依赖性并实现分层特征聚合。还开发了一种直接的数据滚动策略，将动态信息引入 2D 姿态序列。在 Human3.6M、MPI-INF-3DHP 和 Human-Eva 基准测试中进行了广泛的实验。G-SFormer 系列方法与以前的最先进技术相比，仅用大约 10% 的参数就实现了卓越的性能，并显着降低了计算复杂度。此外，G-SFormer 还表现出对检测到的 2D 姿态不准确性的出色鲁棒性。||
|**2024-07-03**|[ADFQ-ViT: Activation-Distribution-Friendly Post-Training Quantization for Vision Transformers](http://arxiv.org/abs/2407.02763)|null|视觉Transformer（ViT）在各种计算机视觉任务中都表现出色，但其庞大的参数量导致内存和计算需求显著增加，阻碍了其在资源受限设备上的有效推理。量化已成为缓解这些挑战的一种很有前景的解决方案，但现有方法在低比特情况下仍然存在显著的精度损失。我们将此问题归因于ViT中LayerNorm后和GELU后激活的独特分布，这使得传统的硬件友好型量化器效率低下，尤其是在低比特情况下。为了解决这个问题，我们提出了一种名为Activation-Distribution-Friendly post-training Quantization for Vision Transformers (ADFQ-ViT)的新颖框架。具体来说，我们引入了Per-Patch Outlier-aware Quantizer来处理LayerNorm后激活中的不规则异常值。该量化器在保持阈值以上最小值子集全精度的情况下，将均匀量化器的粒度细化到每个补丁级别。为了处理GELU后激活在正负区域之间的非均匀分布，我们设计了Shift-Log2 Quantizer，它将所有元素移位到正区域，然后应用log2量化。此外，我们提出了Attention-score enhanced Module-wise Optimization，通过重构误差来调整每个量化器的参数，以进一步减轻量化误差。大量实验表明，ADFQ-ViT在4比特图像分类、目标检测和实例分割任务中比各种基线都有显著改进。具体来说，在将ViT-B模型量化到4比特时，我们在ImageNet数据集上的Top-1准确率提高了10.23%。||
|**2024-07-02**|[A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models](http://arxiv.org/abs/2407.02646)|**[link](https://github.com/dakingrai/awesome-mechanistic-interpretability-lm-papers)**|机械可解释性 (MI) 是可解释性的一个新兴子领域，旨在通过逆向工程神经网络模型的内部计算来理解它。近年来，MI 在解释基于 Transformer 的语言模型 (LM) 方面引起了极大的关注，产生了许多新颖的见解，但也带来了新的挑战。然而，目前还没有工作全面回顾这些见解和挑战，特别是作为该领域新人的指南。为了填补这一空白，我们提供了一份全面的综述，概述了 MI 中的基本研究对象、用于其研究的技术、评估 MI 结果的方法，以及使用 MI 理解 LM 所产生的重要发现和应用。特别是，我们为初学者提供了一个路线图，以帮助他们在该领域导航并利用 MI 为自己谋福利。最后，我们还指出了该领域目前的差距，并讨论了未来可能的发展方向。||
|**2024-07-02**|[On the Anatomy of Attention](http://arxiv.org/abs/2407.02423)|null|我们引入一种范畴论图表形式体系，以便系统地关联和推理机器学习模型。我们的图表以直观的方式呈现架构，但不会丢失必要的细节，其中模型之间的自然关系通过图形变换来捕捉，并且重要的差异和相似性可以一目了然地识别出来。在本文中，我们将重点关注注意力机制：将民间传说转化为数学推导，并构建文献中注意力变体的分类法。作为以我们的形式主义为基础的实证研究的第一个例子，我们确定了注意力机制中反复出现的解剖学成分，我们对其进行了详尽的重组，以探索注意力机制的变化空间。||
|**2024-07-02**|[Efficient Sparse Attention needs Adaptive Token Release](http://arxiv.org/abs/2407.02328)|null|近年来，大型语言模型 (LLM) 在各种以文本为中心的的任务中展现出卓越的能力。然而，其“大”规模带来了巨大的计算和存储挑战，尤其是在管理 Transformer 的键值状态方面，这限制了其更广泛的适用性。因此，我们建议自适应地从缓存中释放资源并重建必要的键值状态。特别是，我们通过一个轻量级的控制器模块来近似理想的top- $K$稀疏注意力来实现这一点。该模块保留具有最高 top-$K$ 注意力权重的标记，并同时重建已丢弃但必要的标记，这些标记可能对未来的解码至关重要。自然语言生成和建模方面的综合实验表明，我们的方法不仅在性能方面与完全注意力机制相比具有竞争力，而且还实现了高达 221.8% 的显著吞吐量提升。复制代码可在 https://github.com/WHUIR/ADORE 上获得。||

