---
layout: default
---

## Updated on 2024.06.17
> Usage instructions: [here](./docs/README.md#usage)

## 多模态

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models**|[2406.10228](http://arxiv.org/abs/2406.10228)|null|**多模态大型模型 (MLLM) 的快速发展展现了其在处理视觉和语言融合任务方面的惊人能力。然而，目前大多数模型和基准测试都局限于视觉和文本上下文范围较窄的场景。当面对复杂的理解任务时，这些模型往往表现不佳，因为这些任务需要在大量不相关的、可能具有误导性的文本和图像信息中进行导航。为了弥合这一差距，我们引入了一项新的、要求更高的任务，称为交错图文理解 (IITC)。这项任务要求模型辨别并忽略图像和文本中的多余元素，以准确回答问题，并遵循复杂的指令来精确定位相关图像。为了支持这项任务，我们进一步构建了一个新的 VEGA 数据集，该数据集专为科学内容的 IITC 任务而定制，并设计了一个子任务，即图文关联 (ITA)，以完善图像-文本关联技能。我们对四个领先的闭源模型以及使用 VEGA 的各种开源模型进行了评估，结果突出了 IITC 任务的严苛性。即使是最先进的模型，如 Gemini-1.5-pro 和 GPT4V，也只取得了有限的成功。通过采用多任务、多尺度的后训练策略，我们在 IITC 任务上为 MLLM  建立了一个稳健的基线，在图像关联方面达到了 85.8% 的准确率，Rouge 得分达到了 0.508。这些结果验证了我们的数据集在提高 MLLM  对细微图文理解能力方面的有效性。 
**|
|**2024-06-14**|**VideoGUI: A Benchmark for GUI Automation from Instructional Videos**|[2406.10227](http://arxiv.org/abs/2406.10227)|null|**图形用户界面（GUI）自动化通过协助完成计算机任务，为提高人类生产力带来了巨大的希望。现有的任务制定主要集中于可以通过单一语言指令指定的简单任务，例如“插入新幻灯片”。在这项工作中，我们介绍了 VideoGUI，这是一个新颖的多模态基准测试，旨在评估 GUI 助手在以视觉为中心的 GUI 任务上的表现。我们的基准测试源自高质量的网络教学视频，专注于涉及专业和新颖软件（例如，Adobe Photoshop 或 Stable Diffusion WebUI）以及复杂活动（例如，视频编辑）的任务。VideoGUI 通过分层流程评估 GUI 助手，允许识别它们可能失败的特定级别：（i）高级规划：在没有语言描述的情况下，根据视觉条件重建程序性子任务；（ii）中级规划：根据视觉状态（即屏幕截图）和目标生成精确动作叙述的序列；（iii）原子动作执行：执行特定动作，例如准确单击指定元素。对于每个级别，我们设计了跨各个维度的评估指标，以提供清晰的信号，例如原子动作执行中单击、拖动、键入和滚动的个体性能。我们对 VideoGUI 的评估表明，即使是最先进的大型多模态模型 GPT4o 在以视觉为中心的 GUI 任务上也表现不佳，尤其是在高级规划方面。**|
|**2024-06-14**|**Short Film Dataset (SFD): A Benchmark for Story-Level Video Understanding**|[2406.10221](http://arxiv.org/abs/2406.10221)|null|**近年来，视觉语言模型的进步极大地推动了视频理解的发展。然而，现有的数据集和任务存在显著的局限性。大多数数据集仅限于事件有限、叙述范围狭窄的短视频。例如，包含教学和以自我为中心的视频的数据集通常只记录一个人在单个场景中的活动。尽管一些电影数据集提供了更丰富的内容，但它们通常仅限于短期任务，缺乏公开可用的视频，并且由于在大型语言模型训练中使用了电影论坛和其他资源，因此经常遇到数据泄露问题。为了解决上述限制，我们提出了包含 1,078 部公开可用的业余电影的短片数据集 (SFD)，该数据集包含各种类型，并且数据泄露问题极少。SFD 以多项选择和开放式问答的形式提供面向长期故事的视频任务。我们广泛的实验强调了解决 SFD 任务需要进行长期推理。值得注意的是，我们发现电影脚本中存在强烈的信号，导致人类和大型语言模型的性能相当。我们还发现，与仅使用视觉数据相比，当前模型的性能明显低于人类。 
**|
|**2024-06-14**|**DevBench: A multimodal developmental benchmark for language learning**|[2406.10215](http://arxiv.org/abs/2406.10215)|null|**视觉语言模型和儿童的学习轨迹有多么相似（或不同）？最近的建模工作试图通过构建使用更少数据（尤其是多模态自然数据）训练的模型来理解模型和人类数据效率之间的差距。然而，此类模型通常在成人水平的基准上进行评估，测试的语言能力广度有限，并且没有与行为数据进行直接比较。我们引入了 DevBench，这是一个多模态基准测试，包含七项语言评估任务，涵盖词汇、句法和语义能力领域，以及儿童和成人的行为数据。我们针对这些任务评估了一组视觉语言模型，不仅比较了模型和人类的准确性，还比较了他们的响应模式。在各项任务中，模型在接近人类响应模式方面表现出差异，并且在任务中表现更好的模型也更接近人类行为响应。我们还检查了 OpenCLIP 在训练过程中的发展轨迹，发现更多的训练会使其更接近成人响应模式。因此，DevBench 提供了一个用于比较模型与人类语言发展的基准。这些比较突出了模型和人类语言学习过程的不同之处，为了解改进语言模型的切入点提供了思路。 
**|
|**2024-06-14**|**Detecting and Evaluating Medical Hallucinations in Large Vision Language Models**|[2406.10185](http://arxiv.org/abs/2406.10185)|null|**大型视觉语言模型 (LVLM) 在医疗应用中越来越不可或缺，包括医学视觉问答和影像报告生成。虽然这些模型继承了基础大型语言模型 (LLM) 的强大功能，但它们也继承了易产生幻觉的倾向，这在容错率极低的医疗环境中是一个重大问题。然而，目前还没有专门针对医学领域幻觉检测和评估的方法或基准。为了弥合这一差距，我们引入了 Med-HallMark，这是第一个专门为医学多模态领域内的幻觉检测和评估而设计的基准。该基准提供了多任务幻觉支持、多方面的幻觉数据和分层的幻觉分类。此外，我们提出了 MediHall Score，这是一种新的医学评估指标，旨在通过分层评分系统评估 LVLM 的幻觉，该系统考虑了幻觉的严重程度和类型，从而能够对潜在的临床影响进行细致的评估。我们还提出了 MediHallDetector，这是一种为精确检测幻觉而设计的新型医学 LVLM，它采用多任务训练进行幻觉检测。通过广泛的实验评估，我们使用我们的基准为流行的 LVLM 建立了基线。研究结果表明，与传统指标相比，MediHall Score 对幻觉影响提供了更细致的理解，并证明了 MediHallDetector 的增强性能。我们希望这项工作能够显着提高 LVLM 在医疗应用中的可靠性。这项工作的所有资源将很快发布。 
**|
|**2024-06-14**|**CarLLaVA: Vision language models for camera-only closed-loop driving**|[2406.10165](http://arxiv.org/abs/2406.10165)|null|**In this technical report, we present CarLLaVA, a Vision Language Model (VLM) for autonomous driving, developed for the CARLA Autonomous Driving Challenge 2.0. CarLLaVA uses the vision encoder of the LLaVA VLM and the LLaMA architecture as backbone, achieving state-of-the-art closed-loop driving performance with only camera input and without the need for complex or expensive labels. Additionally, we show preliminary results on predicting language commentary alongside the driving output. CarLLaVA uses a semi-disentangled output representation of both path predictions and waypoints, getting the advantages of the path for better lateral control and the waypoints for better longitudinal control. We propose an efficient training recipe to train on large driving datasets without wasting compute on easy, trivial data. CarLLaVA ranks 1st place in the sensor track of the CARLA Autonomous Driving Challenge 2.0 outperforming the previous state of the art by 458% and the best concurrent submission by 32.6%.**|
|**2024-06-14**|**RoboGolf: Mastering Real-World Minigolf with a Reflective Multi-Modality Vision-Language Model**|[2406.10157](http://arxiv.org/abs/2406.10157)|null|**Minigolf, a game with countless court layouts, and complex ball motion, constitutes a compelling real-world testbed for the study of embodied intelligence. As it not only challenges spatial and kinodynamic reasoning but also requires reflective and corrective capacities to address erroneously designed courses. We introduce RoboGolf, a framework that perceives dual-camera visual inputs with nested VLM-empowered closed-loop control and reflective equilibrium loop. Extensive experiments demonstrate the effectiveness of RoboGolf on challenging minigolf courts including those that are impossible to finish.**|
|**2024-06-14**|**SkySenseGPT: A Fine-Grained Instruction Tuning Dataset and Model for Remote Sensing Vision-Language Understanding**|[2406.10100](http://arxiv.org/abs/2406.10100)|**[link](https://github.com/luo-z13/skysensegpt)**|**Remote Sensing Large Multi-Modal Models (RSLMMs) are developing rapidly and showcase significant capabilities in remote sensing imagery (RSI) comprehension. However, due to the limitations of existing datasets, RSLMMs have shortcomings in understanding the rich semantic relations among objects in complex remote sensing scenes. To unlock RSLMMs' complex comprehension ability, we propose a large-scale instruction tuning dataset FIT-RS, containing 1,800,851 instruction samples. FIT-RS covers common interpretation tasks and innovatively introduces several complex comprehension tasks of escalating difficulty, ranging from relation reasoning to image-level scene graph generation. Based on FIT-RS, we build the FIT-RSFG benchmark. Furthermore, we establish a new benchmark to evaluate the fine-grained relation comprehension capabilities of LMMs, named FIT-RSRC. Based on combined instruction data, we propose SkySenseGPT, which achieves outstanding performance on both public datasets and FIT-RSFG, surpassing existing RSLMMs. We hope the FIT-RS dataset can enhance the relation comprehension capability of RSLMMs and provide a large-scale fine-grained data source for the remote sensing community. The dataset will be available at https://github.com/Luo-Z13/SkySenseGPT**|
|**2024-06-14**|**First Multi-Dimensional Evaluation of Flowchart Comprehension for Multimodal Large Language Models**|[2406.10057](http://arxiv.org/abs/2406.10057)|null|**随着多模态大型语言模型（MLLM）技术的快速发展，其综合能力日益强大。为了评估MLLM的各种能力，涌现出 numerous 评估系统。但目前仍缺乏一种全面的方法来评估MLLM在与流程图相关的任务中的表现，而流程图在日常生活和工作中至关重要。我们提出了第一个综合性方法FlowCE，用于从多个维度评估MLLM在与流程图相关的任务中的能力。它涵盖了对MLLM在流程图上的推理、定位识别、信息提取、逻辑验证和总结等方面的能力评估。然而，我们发现即使是GPT4o模型也只获得了56.63分。在开源模型中，Phi-3-Vision获得了最高的49.97分。我们希望FlowCE能够为未来基于流程图的多模态大型语言模型（MLLM）研究做出贡献。我们将开源该项目：\url{https://github.com/360AILAB-NLP/FlowCE} 
**|
|**2024-06-14**|**Details Make a Difference: Object State-Sensitive Neurorobotic Task Planning**|[2406.09988](http://arxiv.org/abs/2406.09988)|**[link](https://github.com/xiao-wen-sun/ossa)**|**一个物体的状态反映了它当前的状态或状况，这对机器人的任务规划和操作非常重要。然而，检测物体的状态并为机器人生成状态敏感的计划具有挑战性。近年来，预训练的大型语言模型（LLM）和视觉语言模型（VLM）在生成计划方面表现出了令人印象深刻的能力。然而，据我们所知，几乎没有任何关于LLM或VLM是否也能生成物体状态敏感计划的研究。为了研究这个问题，我们引入了一个物体状态敏感代理（OSSA），这是一个由预训练神经网络赋能的任务规划代理。我们为OSSA提出了两种方法：（i）由预训练的视觉处理模块（密集字幕模型，DCM）和自然语言处理模型（LLM）组成的模块化模型，以及（ii）仅由VLM组成的单片模型。为了定量评估这两种方法的性能，我们使用了桌面场景，其中任务是清理桌面。我们提供了一个考虑物体状态的多模态基准数据集。我们的结果表明，这两种方法都可以用于物体状态敏感的任务，但单片方法优于模块化方法。OSSA的代码可在\url{https://github.com/Xiao-wen-Sun/OSSA}获取。 
**|

## 6DOF Object Pose

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-06**|**Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking**|[2406.04316](http://arxiv.org/abs/2406.04316)|null|**6D物体姿态估计是计算机视觉中一项至关重要但极具挑战性的任务，其面临的主要挑战是缺乏大规模数据集。这种稀缺性阻碍了对模型性能的全面评估，限制了研究进展。此外，可用实例或类别的数量有限也限制了其应用。为了解决这些问题，本文提出了 Omni6DPose，这是一个以对象类别多样性、规模大和对象材质多样性为特征的大型数据集。Omni6DPose 主要分为三个部分：ROPE（真实 6D 物体姿态估计数据集），包含 332K 张图像，对 149 个类别中 581 个实例进行了超过 150 万次标注；SOPE（模拟 6D 物体姿态估计数据集），包含在混合现实环境中创建的 475K 张图像，并进行深度模拟，对相同 149 个类别中 4162 个实例进行了超过 500 万次标注；以及在 ROPE 和 SOPE 中均使用的手动对齐的真实扫描物体。由于存在大量变化和歧义，Omni6DPose 本身就极具挑战性。为了应对这一挑战，我们引入了 GenPose++，它是 SOTA 类别级姿态估计框架的增强版本，它包含两项关键改进：语义感知特征提取和基于聚类的聚合。此外，我们还提供了全面的基准分析，以评估先前方法在这个大规模数据集上在 6D 物体姿态估计和姿态跟踪方面的性能。 
**|
|**2024-06-05**|**Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices**|[2406.02977](http://arxiv.org/abs/2406.02977)|null|**随着机器人和增强现实应用越来越依赖于精确高效的6D物体姿态估计，边缘设备上的实时性能对于更具交互性和响应性的系统变得至关重要。我们提出的稀疏颜色代码网络（SCCN）体现了一种清晰简洁的管道设计，可以有效地满足这一需求。SCCN对RGB图像中的目标物体进行像素级预测，利用基本物体几何特征的稀疏性来加速透视n点（PnP）计算过程。此外，它引入了一种新颖的基于像素级几何的物体对称表示，该表示与初始姿态预测无缝集成，有效地解决了对称物体的歧义性。 值得注意的是，SCCN在NVIDIA Jetson AGX Xavier上分别实现了在基准LINEMOD数据集和遮挡LINEMOD数据集上每秒19帧（FPS）和6 FPS的估计速率，同时在这些速率下始终保持较高的估计精度。 
**|
|**2024-05-19**|**Advancing 6-DoF Instrument Pose Estimation in Variable X-Ray Imaging Geometries**|[2405.11677](http://arxiv.org/abs/2405.11677)|**[link](https://github.com/cviviers/YOLOv5-6D-Pose)**|**在微创手术中，对手术器械进行精确的 6 自由度姿态估计可以显著改善治疗策略和最终手术结果。现有的深度学习方法已经取得了准确的结果，但它们需要针对每个对象定制方法，并且需要费力的设置和训练环境（通常延伸到广泛的模拟），同时缺乏实时计算能力。我们提出了一种用于 X 射线系统中 6 自由度姿态估计任务的通用数据采集方法、一种新颖且通用的 YOLOv5-6D 姿态架构（用于准确快速的对象姿态估计）以及一种在单目锥束 X 射线图像采集几何形状考虑下进行手术螺钉姿态估计的完整方法。所提出的 YOLOv5-6D 姿态模型在公共基准测试中取得了具有竞争力的结果，同时在 GPU 上的速度相当快，为 42 FPS。此外，该方法可以泛化到不同的 X 射线采集几何形状和语义图像复杂性，从而能够在不同领域实现准确的姿态估计。最后，所提出的方法在脊柱手术期间对骨螺钉姿态估计进行了测试，用于计算机辅助引导。该模型通过 0.1 ADD-S 指标实现了 92.41% 的精度，证明了其在提高手术精度和患者预后方面具有良好的应用前景。YOLOv5-6D 的代码可在 https://github.com/cviviers/YOLOv5-6D-Pose 公开获取。 
**|
|**2024-05-18**|**PS6D: Point Cloud Based Symmetry-Aware 6D Object Pose Estimation in Robot Bin-Picking**|[2405.11257](http://arxiv.org/abs/2405.11257)|null|**6D物体姿态估计在许多领域都扮演着至关重要的角色，特别是在工业工件抓取方面。针对锈迹、高反射率和缺乏纹理等挑战，本文介绍了一种基于点云的姿态估计框架（PS6D）。PS6D专注于细长和多对称物体。它通过注意力引导的特征提取模块提取多尺度特征，设计了对称感知的旋转损失和中心距离敏感的平移损失，将每个点的姿态回归到实例的质心，然后使用两阶段聚类方法完成实例分割和姿态估计。来自Sil'eane和IPA数据集的对象以及来自工业实践的典型工件被用于生成数据和评估算法。与最先进的方法相比，PS6D在F $_{1_{inst}}$ 方面提高了11.5%，在召回率方面提高了14.8%。PS6D的主要部分已部署到Mech-Mind的软件中，并在分拣实验中达到了91.7%的成功率，标志着其在工业姿态估计任务中的应用。 
**|
|**2024-05-31**|**Deep Learning-Based Object Pose Estimation: A Comprehensive Survey**|[2405.07801](http://arxiv.org/abs/2405.07801)|**[link](https://github.com/cnjianliu/awesome-object-pose-estimation)**|**Object pose estimation is a fundamental computer vision problem with broad applications in augmented reality and robotics. Over the past decade, deep learning models, due to their superior accuracy and robustness, have increasingly supplanted conventional algorithms reliant on engineered point pair features. Nevertheless, several challenges persist in contemporary methods, including their dependency on labeled training data, model compactness, robustness under challenging conditions, and their ability to generalize to novel unseen objects. A recent survey discussing the progress made on different aspects of this area, outstanding challenges, and promising future directions, is missing. To fill this gap, we discuss the recent advances in deep learning-based object pose estimation, covering all three formulations of the problem, \emph{i.e.}, instance-level, category-level, and unseen object pose estimation. Our survey also covers multiple input data modalities, degrees-of-freedom of output poses, object properties, and downstream tasks, providing the readers with a holistic understanding of this field. Additionally, it discusses training paradigms of different domains, inference modes, application areas, evaluation metrics, and benchmark datasets, as well as reports the performance of current state-of-the-art methods on these benchmarks, thereby facilitating the readers in selecting the most suitable method for their application. Finally, the survey identifies key challenges, reviews the prevailing trends along with their pros and cons, and identifies promising directions for future research. We also keep tracing the latest works at https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation.**|
|**2024-05-02**|**IntervenGen: Interventional Data Generation for Robust and Data-Efficient Robot Imitation Learning**|[2405.01472](http://arxiv.org/abs/2405.01472)|null|**Imitation learning is a promising paradigm for training robot control policies, but these policies can suffer from distribution shift, where the conditions at evaluation time differ from those in the training data. A popular approach for increasing policy robustness to distribution shift is interactive imitation learning (i.e., DAgger and variants), where a human operator provides corrective interventions during policy rollouts. However, collecting a sufficient amount of interventions to cover the distribution of policy mistakes can be burdensome for human operators. We propose IntervenGen (I-Gen), a novel data generation system that can autonomously produce a large set of corrective interventions with rich coverage of the state space from a small number of human interventions. We apply I-Gen to 4 simulated environments and 1 physical environment with object pose estimation error and show that it can increase policy robustness by up to 39x with only 10 human interventions. Videos and more results are available at https://sites.google.com/view/intervengen2024.**|
|**2024-04-17**|**GeoReF: Geometric Alignment Across Shape Variation for Category-level Object Pose Refinement**|[2404.11139](http://arxiv.org/abs/2404.11139)|null|**物体姿态优化对于鲁棒的物体姿态估计至关重要。先前的工作在实例级物体姿态优化方面取得了重大进展。然而，由于类别内较大的形状变化以及目标物体与形状先验之间的差异，类别级姿态优化是一个更具挑战性的问题。为了应对这些挑战，我们介绍了一种新颖的类别级物体姿态优化架构。我们的方法集成了 HS 层和可学习的仿射变换，旨在增强几何信息的提取和对齐。此外，我们引入了一种跨点云变换机制，可以有效地融合不同的数据源。最后，我们通过结合形状先验信息进行平移和尺寸误差预测，突破了模型的极限。我们进行了广泛的实验来证明所提出框架的有效性。通过大量的定量实验，我们证明了在所有指标上，我们的方法都比基线方法有显著的改进。 
**|
|**2024-04-08**|**Learning a Category-level Object Pose Estimator without Pose Annotations**|[2404.05626](http://arxiv.org/abs/2404.05626)|null|**三维物体姿态估计是一项具有挑战性的任务。以往的工作总是需要数千张带有标注姿态的物体图像来学习三维姿态对应关系，这对于标注来说既费力又耗时。在本文中，我们提出在没有姿态标注的情况下学习类别级三维物体姿态估计器。我们没有使用手动标注的图像，而是利用扩散模型（例如，Zero-1-to-3）生成一组姿态差异可控的图像，并建议使用这些图像来学习我们的物体姿态估计器。直接使用原始扩散模型会导致图像出现姿态噪声和伪影。为了解决这个问题，首先，我们利用从专门设计的对比姿态学习中学习到的图像编码器来过滤不合理的细节并提取图像特征图。此外，我们提出了一种新的学习策略，允许模型从那些生成的图像集中学习物体姿态，而无需知道其规范姿态的对齐方式。实验结果表明，我们的方法能够从单次拍摄设置（作为姿态定义）中进行类别级物体姿态估计，同时在少样本类别级物体姿态估计基准测试中显著优于其他最先进的方法。 
**|
|**2024-03-28**|**Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation**|[2403.19527](http://arxiv.org/abs/2403.19527)|**[link](https://github.com/leeiieeo/ag-pose)**|**类别级 6D 物体姿态估计旨在估计特定类别中未见实例的旋转、平移和尺寸。在这一领域，基于密集对应的算法取得了领先的性能。然而，它们没有明确地考虑不同实例的局部和全局几何信息，导致对形状变化显著的未见实例的泛化能力较差。为了解决这个问题，我们提出了一种新的用于类别级 6D 物体姿态估计的实例自适应几何感知关键点学习算法（AG-Pose），它包括两个关键设计：（1）第一个设计是实例自适应关键点检测模块，它可以自适应地检测一组稀疏关键点来表示各种实例的几何结构。（2）第二个设计是几何感知特征聚合模块，它可以有效地将局部和全局几何信息整合到关键点特征中。这两个模块可以协同工作，为未见过的实例建立鲁棒的关键点级对应关系，从而增强模型的泛化能力。在 CAMERA25 和 REAL275 数据集上的实验结果表明，所提出的 AG-Pose 在没有类别特定形状先验的情况下，大幅度优于现有算法。 
**|
|**2024-06-01**|**Object Pose Estimation via the Aggregation of Diffusion Features**|[2403.18791](http://arxiv.org/abs/2403.18791)|**[link](https://github.com/tianfu18/diff-feats-pose)**|**从图像中估计物体姿态是 3D 场景理解的一项关键任务，最近的方法在非常大的基准测试中显示出可喜的结果。 然而，这些方法在处理未见过物体时性能会显著下降。我们认为这是由于图像特征的泛化能力有限造成的。为了解决这个问题，我们深入分析了扩散模型（例如 Stable Diffusion）的特征，这些特征在对未见过物体进行建模方面具有巨大潜力。 在此分析的基础上，我们创新性地将这些扩散特征引入物体姿态估计。 为此，我们提出了三种不同的架构，可以有效地捕获和聚合不同粒度的扩散特征，大大提高了物体姿态估计的泛化能力。 我们的方法在三个流行的基准数据集 LM、O-LM 和 T-LESS 上的性能大大优于最先进的方法。 尤其值得一提的是，我们的方法在未见过物体上取得了比之前最佳结果更高的准确率：在 Unseen LM 上为 98.2% vs. 93.5%，在 Unseen O-LM 上为 85.9% vs. 76.3%，显示出我们方法强大的泛化能力。 我们的代码已发布在 https://github.com/Tianfu18/diff-feats-pose。 
**|

## nerf

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors**|[2406.10111](http://arxiv.org/abs/2406.10111)|null|**从低分辨率输入视图实现高分辨率新视角合成（HRNVS）是一项具有挑战性的任务，因为缺乏高分辨率数据。以前的方法从低分辨率输入视图优化高分辨率神经辐射场（NeRF），但渲染速度缓慢。在这项工作中，我们基于三维高斯渲染（3DGS）来开发我们的方法，因为它能够以更快的渲染速度生成高质量的图像。为了缓解高分辨率合成的数据短缺问题，我们建议利用现成的二维扩散先验，通过分数蒸馏采样（SDS）将二维知识提取到三维空间。然而，由于生成先验带来的随机性，将SDS直接应用于基于高斯的3D超分辨率会导致不希望出现且冗余的3D高斯基元。为了缓解这个问题，我们引入了两种简单而有效的技术来减少SDS引入的随机扰动。具体来说，我们 1）使用退火策略缩小SDS中扩散时间步长的范围；2）在密集化过程中随机丢弃冗余的高斯基元。大量实验表明，我们提出的GaussainSR方法可以在合成和真实世界数据集上，仅使用低分辨率输入就能获得高质量的HRNVS结果。项目页面：https://chchnii.github.io/GaussianSR/ 
**|
|**2024-06-14**|**RaNeuS: Ray-adaptive Neural Surface Reconstruction**|[2406.09801](http://arxiv.org/abs/2406.09801)|**[link](https://github.com/wangyida/ra-neus)**|**我们的目标是利用可微分辐射场（例如 NeRF）来重建详细的 3D 表面，以及生成标准的新颖视图渲染。目前已有一些相关方法可以执行此类任务，通常是利用符号距离场 (SDF)。然而，最先进的方法仍然无法正确重建小规模细节，例如树叶、绳索和纺织品表面。考虑到不同的方法使用全局常数 Eikonal 正则化来制定和优化从 SDF 到辐射场的投影，我们通过射线加权因子进行改进，以优先考虑渲染和零交叉表面拟合，并在建立完美 SDF 的基础上进行。我们建议自适应地调整符号距离场上的正则化，以便不令人满意的渲染射线不会强制执行无效的强 Eikonal 正则化，并允许来自学习良好的辐射区域的梯度有效地反向传播到 SDF。因此，平衡这两个目标以生成准确和详细的表面。此外，考虑到 SDF 中的零交叉表面和辐射场中的渲染点之间是否存在几何偏差，投影也可以根据优化期间不同的 3D 位置进行调整。我们提出的 RaNeuS 在合成数据集和真实数据集上都得到了广泛评估，在新颖视图合成和几何重建方面均取得了最先进的结果。 
**|
|**2024-06-13**|**Neural NeRF Compression**|[2406.08943](http://arxiv.org/abs/2406.08943)|null|**Neural Radiance Fields (NeRFs) have emerged as powerful tools for capturing detailed 3D scenes through continuous volumetric representations. Recent NeRFs utilize feature grids to improve rendering quality and speed; however, these representations introduce significant storage overhead. This paper presents a novel method for efficiently compressing a grid-based NeRF model, addressing the storage overhead concern. Our approach is based on the non-linear transform coding paradigm, employing neural compression for compressing the model's feature grids. Due to the lack of training data involving many i.i.d scenes, we design an encoder-free, end-to-end optimized approach for individual scenes, using lightweight decoders. To leverage the spatial inhomogeneity of the latent feature grids, we introduce an importance-weighted rate-distortion objective and a sparse entropy model employing a masking mechanism. Our experimental results validate that our proposed method surpasses existing works in terms of grid-based NeRF compression efficacy and reconstruction quality.**|
|**2024-06-13**|**OpenMaterial: A Comprehensive Dataset of Complex Materials for 3D Reconstruction**|[2406.08894](http://arxiv.org/abs/2406.08894)|null|**Recent advances in deep learning such as neural radiance fields and implicit neural representations have significantly propelled the field of 3D reconstruction. However, accurately reconstructing objects with complex optical properties, such as metals and glass, remains a formidable challenge due to their unique specular and light-transmission characteristics. To facilitate the development of solutions to these challenges, we introduce the OpenMaterial dataset, comprising 1001 objects made of 295 distinct materials-including conductors, dielectrics, plastics, and their roughened variants- and captured under 723 diverse lighting conditions. To this end, we utilized physics-based rendering with laboratory-measured Indices of Refraction (IOR) and generated high-fidelity multiview images that closely replicate real-world objects. OpenMaterial provides comprehensive annotations, including 3D shape, material type, camera pose, depth, and object mask. It stands as the first large-scale dataset enabling quantitative evaluations of existing algorithms on objects with diverse and challenging materials, thereby paving the way for the development of 3D reconstruction algorithms capable of handling complex material properties.**|
|**2024-06-13**|**Gaussian-Forest: Hierarchical-Hybrid 3D Gaussian Splatting for Compressed Scene Modeling**|[2406.08759](http://arxiv.org/abs/2406.08759)|null|**近年来，新视角合成领域见证了三维高斯 splatting 技术的兴起，该技术以基于点的方式表示场景并通过光栅化进行渲染。与依赖于光线追踪的辐射场不同，这种方法展现出卓越的渲染质量和速度。然而，三维高斯的显式和非结构化特性带来了巨大的存储挑战，阻碍了其更广泛的应用。为了应对这一挑战，我们引入了高斯森林建模框架，该框架将场景分层表示为混合三维高斯森林。每个混合高斯保留其独特的显式属性，同时与其兄弟高斯共享隐式属性，从而使用更少的变量优化参数化。此外，我们还设计了自适应生长和修剪策略，确保在复杂区域的详细表示，并显著减少所需高斯的数量。大量实验表明，高斯森林不仅保持了相当的速度和质量，而且实现了超过 10 倍的压缩率，标志着高效场景建模的重大进步。代码可在 https://github.com/Xian-Bei/GaussianForest 获取。 
**|
|**2024-06-12**|**OpenObj: Open-Vocabulary Object-Level Neural Radiance Fields with Fine-Grained Understanding**|[2406.08009](http://arxiv.org/abs/2406.08009)|**[link](https://github.com/BIT-DYN/OpenObj)**|**近年来，利用视觉语言模型 (VLM) 进行开放词汇 3D 场景重建引起了人们的极大兴趣，这些模型在开放集检索方面展现出卓越的能力。然而，现有方法存在一些局限性：它们要么侧重于学习逐点特征，导致语义理解模糊，要么仅仅处理对象级重建，从而忽略了对象内部的复杂细节。为了应对这些挑战，我们引入了 OpenObj，这是一种构建具有细粒度理解的开放词汇对象级神经辐射场 (NeRF) 的创新方法。本质上，OpenObj 建立了一个强大的框架，用于在对象级别进行高效且严密的场景建模和理解。此外，我们将零件级特征纳入神经场，从而能够对对象内部进行细致的表示。这种方法在捕获对象级实例的同时保持了细粒度的理解。在多个数据集上的结果表明，OpenObj 在零样本语义分割和检索任务中实现了卓越的性能。此外，OpenObj 支持多种规模的现实世界机器人任务，包括全局移动和局部操作。 
**|
|**2024-06-12**|**Spatial Annealing Smoothing for Efficient Few-shot Neural Rendering**|[2406.07828](http://arxiv.org/abs/2406.07828)|**[link](https://github.com/pulangk97/SANeRF)**|**基于混合表征的神经辐射场 (NeRF) 在重建场景以进行视图合成方面表现出令人印象深刻的能力，并具有很高的效率。然而，由于过拟合问题，当视图输入稀疏时，它们的性能会显著下降。虽然已经设计了各种正则化策略来应对这些挑战，但它们通常依赖于低效的假设或与混合模型不兼容。显然，需要一种在混合框架内保持效率并提高对稀疏视图的鲁棒性的方法。在本文中，我们介绍了一种准确高效的少样本神经渲染方法，称为空间退火平滑正则化 NeRF (SANeRF)，它专为预过滤驱动的混合表征架构而设计。我们实现了从初始大值开始的样本空间大小的指数级减少。这种方法对于稳定训练阶段的早期阶段至关重要，并显着有助于增强后续的细节细化过程。我们的大量实验表明，只需添加一行代码，与当前的少样本 NeRF 方法相比，SANeRF 就能提供卓越的渲染质量和更快的重建速度。值得注意的是，在 Blender 数据集上，SANeRF 的 PSNR 比 FreeNeRF 高 0.3 dB，同时重建速度提高了 700 倍。 
**|
|**2024-06-11**|**Neural Gaffer: Relighting Any Object via Diffusion**|[2406.07520](http://arxiv.org/abs/2406.07520)|null|**单图像重打光是项具有挑战性的任务，它涉及对几何、材质和光照之间复杂相互作用的推理。许多先前的方法要么只支持特定类别的图像（例如肖像），要么需要特殊的拍摄条件（例如使用手电筒）。或者，一些方法将场景明确分解为内在组件，例如法线和BRDF，但这可能不准确或表达不足。在这项工作中，我们提出了一种新颖的端到端二维重打光扩散模型，称为Neural Gaffer，它可以拍摄任何物体的单个图像，并可以在任何新的环境光照条件下合成准确、高质量的重打光图像，只需简单地将图像生成器设置为以目标环境图为条件，而无需进行显式场景分解。我们的方法建立在预先训练的扩散模型的基础上，并在合成的重打光数据集上对其进行微调，揭示并利用了扩散模型中存在的对光照的内在理解。我们在合成图像和来自互联网的真实图像上评估了我们的模型，并证明了它在泛化性和准确性方面的优势。此外，通过与其他生成方法相结合，我们的模型可以实现许多下游二维任务，例如基于文本的重打光和对象插入。我们的模型还可以作为三维任务（例如对辐射场进行重打光）的强大重打光先验。**|
|**2024-06-11**|**Active Scout: Multi-Target Tracking Using Neural Radiance Fields in Dense Urban Environments**|[2406.07431](http://arxiv.org/abs/2406.07431)|null|**我们研究了高度遮挡的城市环境（例如城市中的高层建筑）中的追踪-逃避游戏，其中侦察员（四旋翼飞行器）跟踪地面上的多个动态目标。我们证明了可以使用来自不同有利位置的 RGB 和深度图像——在线——构建城市的基于神经辐射场 (NeRF) 的表示。这种表示用于计算信息增益，以探索城市未知部分并跟踪目标——从而提供一种完全基于第一性原理的方法来主动跟踪动态目标。我们使用基于费城和纽约市开放街道地图数据的定制模拟器证明，我们可以在 300 步内探索和定位 20 个静止目标。这比不使用主动感知的贪婪基线慢。但对于主动躲避遮挡物的动态目标，我们证明我们的方法最多可以将跟踪误差保持在 200 米以内；贪婪基线的跟踪误差可高达 600 米。我们观察到侦察策略中的一些有趣特性，例如，它会定期切换注意力以跟踪不同的目标，随着 NeRF 表示质量随时间推移而提高，侦察在目标跟踪方面也变得更好。 
**|
|**2024-06-11**|**Cinematic Gaussians: Real-Time HDR Radiance Fields with Depth of Field**|[2406.07329](http://arxiv.org/abs/2406.07329)|null|**辐射场方法代表了从多视图照片重建复杂场景的最新技术。然而，这些重建通常存在以下一个或两个限制：首先，它们通常以低动态范围 (LDR) 表示场景，这限制了它们在光照均匀的环境中的使用，并阻碍了沉浸式观看体验。其次，它们依赖于针孔相机模型，假设所有场景元素在输入图像中都是对焦的，这带来了实际挑战，并使新视图合成过程中的重新对焦变得复杂。为了解决这些限制，我们提出了一种基于 3D 高斯 splatting 的轻量级方法，该方法利用具有不同曝光时间、光圈和焦距的场景多视图 LDR 图像作为输入来重建高动态范围 (HDR) 辐射场。通过结合基于薄透镜相机模型的高斯解析卷积以及色调映射模块，我们的重建能够渲染具有灵活重对焦功能的 HDR 内容。我们证明了我们对 HDR 和景深的组合处理促进了实时电影渲染，其性能优于现有技术水平。 
**|

## 分类/检测/识别/分割

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**SatDiffMoE: A Mixture of Estimation Method for Satellite Image Super-resolution with Latent Diffusion Models**|[2406.10225](http://arxiv.org/abs/2406.10225)|null|**During the acquisition of satellite images, there is generally a trade-off between spatial resolution and temporal resolution (acquisition frequency) due to the onboard sensors of satellite imaging systems. High-resolution satellite images are very important for land crop monitoring, urban planning, wildfire management and a variety of applications. It is a significant yet challenging task to achieve high spatial-temporal resolution in satellite imaging. With the advent of diffusion models, we can now learn strong generative priors to generate realistic satellite images with high resolution, which can be utilized to promote the super-resolution task as well. In this work, we propose a novel diffusion-based fusion algorithm called \textbf{SatDiffMoE} that can take an arbitrary number of sequential low-resolution satellite images at the same location as inputs, and fuse them into one high-resolution reconstructed image with more fine details, by leveraging and fusing the complementary information from different time points. Our algorithm is highly flexible and allows training and inference on arbitrary number of low-resolution images. Experimental results show that our proposed SatDiffMoE method not only achieves superior performance for the satellite image super-resolution tasks on a variety of datasets, but also gets an improved computational efficiency with reduced model parameters, compared with previous methods.**|
|**2024-06-14**|**EFM3D: A Benchmark for Measuring Progress Towards 3D Egocentric Foundation Models**|[2406.10224](http://arxiv.org/abs/2406.10224)|null|**The advent of wearable computers enables a new source of context for AI that is embedded in egocentric sensor data. This new egocentric data comes equipped with fine-grained 3D location information and thus presents the opportunity for a novel class of spatial foundation models that are rooted in 3D space. To measure progress on what we term Egocentric Foundation Models (EFMs) we establish EFM3D, a benchmark with two core 3D egocentric perception tasks. EFM3D is the first benchmark for 3D object detection and surface regression on high quality annotated egocentric data of Project Aria. We propose Egocentric Voxel Lifting (EVL), a baseline for 3D EFMs. EVL leverages all available egocentric modalities and inherits foundational capabilities from 2D foundation models. This model, trained on a large simulated dataset, outperforms existing methods on the EFM3D benchmark.**|
|**2024-06-14**|**YOLOv1 to YOLOv10: A comprehensive review of YOLO variants and their application in the agricultural domain**|[2406.10139](http://arxiv.org/abs/2406.10139)|null|**这篇综述调查了从 YOLOv1 到最先进的 YOLOv10 的各种 YOLO 变体在农业发展中的变革潜力。主要目标是阐明这些尖端的物体检测模型如何能够重振和优化农业的各个方面，从作物监测到牲畜管理。它旨在实现关键目标，包括确定农业中的当代挑战、详细评估 YOLO 的增量进步，以及探索其在农业中的具体应用。这是首批将最新的 YOLOv10 包含进来的综述之一，为其对人工智能和自动化时代精准农业和可持续农业实践的影响提供了新的视角。此外，该综述对 YOLO 的性能进行了批判性分析，综合了现有研究，并预测了未来趋势。通过仔细研究 YOLO 变体中包含的独特功能及其在现实世界中的应用，本综述为 YOLO 变体与农业之间不断发展的关系提供了宝贵的见解。这些发现有助于深入理解精准农业和可持续农业实践的潜力，标志着在农业领域整合先进的物体检测技术方面迈出了重要的一步。 
**|
|**2024-06-14**|**GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors**|[2406.10111](http://arxiv.org/abs/2406.10111)|null|**从低分辨率输入视图实现高分辨率新视图合成 (HRNVS) 是一项具有挑战性的任务，因为缺乏高分辨率数据。先前的方法从低分辨率输入视图优化高分辨率神经辐射场 (NeRF)，但渲染速度缓慢。在这项工作中，我们基于 3D 高斯样条 (3DGS) 开发了我们的方法，因为它能够以更快的渲染速度生成高质量图像。为了缓解更高分辨率合成的数据短缺问题，我们建议利用现成的 2D 扩散先验，通过分数蒸馏采样 (SDS) 将 2D 知识提取到 3D 中。然而，由于生成先验带来的随机性，将 SDS 直接应用于基于高斯的 3D 超分辨率会导致不希望的和冗余的 3D 高斯基元。为了缓解这个问题，我们引入了两种简单而有效的技术来减少 SDS 引入的随机扰动。具体来说，我们 1) 使用退火策略缩小 SDS 中扩散时间步长的范围；2) 在密集化过程中随机丢弃冗余的高斯基元。大量实验表明，我们提出的 GaussainSR 仅使用合成数据集和真实世界数据集上的低分辨率输入，就可以在 HRNVS 上获得高质量的结果。项目页面：https://chchnii.github.io/GaussianSR/ 
**|
|**2024-06-14**|**Forgetting Order of Continual Learning: Examples That are Learned First are Forgotten Last**|[2406.09935](http://arxiv.org/abs/2406.09935)|null|**灾难性遗忘是持续学习中的一个重大挑战，模型在学习新数据时经常会忘记之前的任务。我们的实证分析表明，灾难性遗忘与样本学习速度之间存在很强的相关性：早期学习的样本很少被遗忘，而后期学习的样本更容易被遗忘。我们证明，基于回放的持续学习方法可以利用这种现象，通过关注中等学习速度的样本进行复习。我们介绍了一种名为“金发姑娘”（Goldilocks）的新型回放缓冲区采样方法，该方法过滤掉学习速度过快或过慢的样本，保留学习速度适中的样本。“金发姑娘”改进了现有的持续学习算法，在多个图像分类任务中取得了最先进的性能。 
**|
|**2024-06-14**|**Exact Sparse Representation Recovery in Signal Demixing and Group BLASSO**|[2406.09922](http://arxiv.org/abs/2406.09922)|null|**本文简要介绍了(Carioni 和 Del Grande, arXiv:2311.08072, 2023) 中提出的凸正则化优化问题中稀疏表示恢复的理论。我们关注于未知量属于巴拿赫空间且测量值取自希尔伯特空间的情况，探索了此类设置下优化问题最小值点的性质。具体来说，我们分析了一个Tikhonov正则化凸优化问题，其中 $y_0$是测量数据，$w$表示噪声，$\lambda$是正则化参数。通过引入度量非退化源条件 (MNDSC) 并考虑足够小的$\lambda$和$w$ ，我们为问题建立了精确稀疏表示恢复 (ESRR)，这意味着最小值点是唯一的，并且可以精确地恢复原始数据的稀疏表示。然后，我们通过两个新的应用强调了这一理论结果的实际意义：信号分离和使用 Group BLASSO 的超分辨率。这些应用强调了我们结果的广泛适用性和重要性，展示了其在不同领域的潜力。 
**|
|**2024-06-14**|**Robust compressive tracking via online weighted multiple instance learning**|[2406.09914](http://arxiv.org/abs/2406.09914)|null|**Developing a robust object tracker is a challenging task due to factors such as occlusion, motion blur, fast motion, illumination variations, rotation, background clutter, low resolution and deformation across the frames. In the literature, lots of good approaches based on sparse representation have already been presented to tackle the above problems. However, most of the algorithms do not focus on the learning of sparse representation. They only consider the modeling of target appearance and therefore drift away from the target with the imprecise training samples. By considering all the above factors in mind, we have proposed a visual object tracking algorithm by integrating a coarse-to-fine search strategy based on sparse representation and the weighted multiple instance learning (WMIL) algorithm. Compared with the other trackers, our approach has more information of the original signal with less complexity due to the coarse-to-fine search method, and also has weights for important samples. Thus, it can easily discriminate the background features from the foreground. Furthermore, we have also selected the samples from the un-occluded sub-regions to efficiently develop the strong classifier. As a consequence, a stable and robust object tracker is achieved to tackle all the aforementioned problems. Experimental results with quantitative as well as qualitative analysis on challenging benchmark datasets show the accuracy and efficiency of our method.**|
|**2024-06-14**|**Bayesian Conditioned Diffusion Models for Inverse Problems**|[2406.09768](http://arxiv.org/abs/2406.09768)|null|**扩散模型最近在许多图像重建任务中表现出色，这些任务涉及基于前向测量算子的逆问题。一个常见的框架是使用与任务无关的无条件模型，这些模型随后经过后验条件化以进行重建，这种方法通常存在任务性能欠佳的问题。虽然也有人提出了特定于任务的条件模型，但目前的方法启发式地将测量数据作为朴素的输入通道注入，这会导致采样不准确。在这里，我们解决了扩散模型的最佳条件化问题，以解决图像重建过程中出现的具有挑战性的逆问题。具体来说，我们提出了一种新的基于分数函数的扩散模型贝叶斯条件化技术BCDM，该技术基于给定测量数据的期望图像的条件分布。我们严格推导出表达和训练条件分数函数的理论。最后，我们展示了所提出的技术在图像去混叠、去模糊、超分辨率和修复方面的最新性能。 
**|
|**2024-06-14**|**Automated GIS-Based Framework for Detecting Crosswalk Changes from Bi-Temporal High-Resolution Aerial Images**|[2406.09731](http://arxiv.org/abs/2406.09731)|null|**Identification of changes in pavement markings has become crucial for infrastructure monitoring, maintenance, development, traffic management, and safety. Automated extraction of roadway geometry is critical in helping with this, given the increasing availability of high-resolution images and advancements in computer vision and object detection. Specifically, due to the substantial volume of satellite and high-resolution aerial images captured at different time instances, change detection has become a viable solution. In this study, an automated framework is developed to detect changes in crosswalks of Orange, Osceola, and Seminole counties in Florida, utilizing data extracted from high-resolution images obtained at various time intervals. Specifically, for Orange County, crosswalk changes between 2019 and 2021 were manually extracted, verified, and categorized as either new or modified crosswalks. For Seminole County, the developed model was used to automatically extract crosswalk changes between 2018 and 2021, while for Osceola County, changes between 2019 and 2020 were extracted. Findings indicate that Orange County witnessed approximately 2,094 crosswalk changes, with 312 occurring on state roads. In Seminole and Osceola counties, on the other hand, 1,040 and 1,402 crosswalk changes were observed on both local and state roads, respectively. Among these, 340 and 344 were identified on state roads in Seminole and Osceola, respectively. Spatiotemporal changes observed in crosswalks can be utilized to regularly update the existing crosswalk inventories, which is essential for agencies engaged in traffic and safety studies. Data extracted from these crosswalk changes can be combined with traffic and crash data to provide valuable insights to policymakers.**|
|**2024-06-14**|**An alternate approach for estimating grain-growth kinetics**|[2406.09653](http://arxiv.org/abs/2406.09653)|null|**Rate of grain growth, which aides in achieving desired properties in polycrystalline materials, is conventionally estimated by measuring the size of grains and tracking its change in micrographs reflecting the temporal evolution. Techniques adopting this conventional approach demand an absolute distinction between the grains and the interface separating them to yield an accurate result. Edge-detection, segmentation and other deep-learning algorithms are increasingly adopted to expose the network of boundaries and the associated grains precisely. An alternate approach for measuring grain-growth kinetics, that curtails the need for advanced image-processing treatment, is presented in this work. Grain-growth rate in the current technique is ascertained by \textit{counting} the number of triple-( and quadruple-) junctions, and monitoring its change during the microstructural evolution. The shifted focus of this junction-based treatment minimises the significance of a well-defined grain-boundary network, and consequently, the involvement of the sophisticated techniques that expose them. A regression-based object-detection algorithm is extended to realise, and count, the number of junctions in polycrystalline microstructures. By examining the change in the number of junctions with time, the growth rate is subsequently determined.Growth kinetics estimated by the present junction-based approach, across a wide-range of multiphase polycrystalline microstructures, agree convincingly with the outcomes of the conventional treatment.Besides offering a novel technique for grain-growth measurement, the analysis accompanying the current work unravels a trend, compatible with the topological events, in the progressive evolution of the triple-junctions count. The present approach, through its underlying algorithm, provides a promising option for monitoring grain-growth during in-situ investigations.**|

## 模型压缩/优化

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**Self-Knowledge Distillation for Learning Ambiguity**|[2406.09719](http://arxiv.org/abs/2406.09719)|null|**Recent language models have shown remarkable performance on natural language understanding (NLU) tasks. However, they are often sub-optimal when faced with ambiguous samples that can be interpreted in multiple ways, over-confidently predicting a single label without consideration for its correctness. To address this issue, we propose a novel self-knowledge distillation method that enables models to learn label distributions more accurately by leveraging knowledge distilled from their lower layers. This approach also includes a learning phase that re-calibrates the unnecessarily strengthened confidence for training samples judged as extremely ambiguous based on the distilled distribution knowledge. We validate our method on diverse NLU benchmark datasets and the experimental results demonstrate its effectiveness in producing better label distributions. Particularly, through the process of re-calibrating the confidence for highly ambiguous samples, the issue of over-confidence when predictions for unseen samples do not match with their ground-truth labels has been significantly alleviated. This has been shown to contribute to generating better distributions than the existing state-of-the-art method. Moreover, our method is more efficient in training the models compared to the existing method, as it does not involve additional training processes to refine label distributions.**|
|**2024-06-14**|**Frequency-mix Knowledge Distillation for Fake Speech Detection**|[2406.09664](http://arxiv.org/abs/2406.09664)|null|**在电话场景中，打击语音欺骗攻击的虚假语音检测 (FSD) 任务极具挑战性。数据增强 (DA) 方法被认为是解决电话场景中 FSD 任务的有效手段，通常分为时域和频域两个阶段。虽然每种方法都有其优点，但都可能导致信息丢失。为了解决这个问题，我们提出了一种新颖的 DA 方法，即频率混合 (Freqmix)，并引入了 Freqmix 知识蒸馏 (FKD) 来增强模型信息提取和泛化能力。具体来说，我们使用 Freqmix 增强的数据作为教师模型的输入，而学生模型的输入则经过时域 DA 方法处理。我们使用多级特征蒸馏方法来恢复信息并提高模型的泛化能力。我们的方法在 ASVspoof 2021 LA 数据集上取得了最先进的结果，比基线提高了 31%，并且在 ASVspoof 2021 DF 数据集上表现出竞争力。 
**|
|**2024-06-13**|**RobustSAM: Segment Anything Robustly on Degraded Images**|[2406.09627](http://arxiv.org/abs/2406.09627)|null|**图像分割领域中，Segment Anything Model (SAM) 模型凭借其强大的零样本分割能力和灵活的提示系统，已成为一种变革性的方法。然而，图像质量下降会影响其性能。为了解决这一局限性，我们提出了 Robust Segment Anything Model (RobustSAM)，它在保持 SAM 的提示性和零样本泛化能力的同时，增强了其在低质量图像上的性能。我们的方法利用预训练的 SAM 模型，仅增加了少量参数和计算需求。RobustSAM 的额外参数可以在 8 个 GPU 上用 30 小时内完成优化，这证明了其对于典型研究实验室的可行性和实用性。我们还介绍了 Robust-Seg 数据集，该数据集包含 688K 个图像-掩码对，具有不同的退化程度，旨在优化我们模型的训练和评估。跨多个分割任务和数据集的大量实验表明，RobustSAM 具有优越的性能，尤其是在零样本条件下，突出了其在广泛实际应用中的潜力。此外，我们的方法已被证明可以有效提高基于 SAM 的下游任务（例如单图像去雾和去模糊）的性能。 
**|
|**2024-06-13**|**Contextual Distillation Model for Diversified Recommendation**|[2406.09021](http://arxiv.org/abs/2406.09021)|null|**推荐的多样性与准确性在改善用户体验方面同样重要。现有的研究，例如行列式点过程（DPP）和最大边缘相关性（MMR），采用贪婪范式来迭代地选择同时优化准确性和多样性的项目。然而，先前的方法通常表现出二次复杂度，将其应用限制在重排序阶段，并且不适用于候选项目池更大的其他推荐阶段，例如预排序和排序阶段。在本文中，我们提出了上下文蒸馏模型（CDM），这是一种解决多样化的有效推荐模型，适用于工业推荐流程的所有阶段的部署。具体来说，CDM 利用同一用户请求中的候选项目作为上下文来增强结果的多样性。我们提出了一种对比上下文编码器，它采用注意力机制对正面和负面上下文进行建模。对于 CDM 的训练，我们将每个目标项目与其上下文嵌入进行比较，并利用知识蒸馏框架来学习 MMR 算法下每个目标项目的获胜概率，其中教师来自 MMR 输出。在推理过程中，通过推荐和学生模型分数的线性组合来执行排序，确保多样性和效率。我们对两个工业数据集进行了离线评估，并在短视频平台快手上对 CDM 进行了在线 A/B 测试。指标显示，推荐质量和多样性方面均有显着提高，这有力地证明了 CDM 的有效性。 
**|
|**2024-06-12**|**Unveiling Incomplete Modality Brain Tumor Segmentation: Leveraging Masked Predicted Auto-Encoder and Divergence Learning**|[2406.08634](http://arxiv.org/abs/2406.08634)|null|**脑肿瘤分割仍然是一项重大挑战，特别是在多模态磁共振成像 (MRI) 的情况下，临床环境中经常出现模态图像缺失的情况，导致分割精度降低。为了解决这个问题，我们提出了一种称为掩码预测预训练的新策略，能够从不完整的模态数据中学习鲁棒的特征。此外，在微调阶段，我们利用知识蒸馏技术来对齐完整模态数据和缺失模态数据之间的特征，同时增强模型的鲁棒性。值得注意的是，我们利用 Holder 伪散度而不是 KLD 来计算蒸馏损失，从而提供更好的数学可解释性和性质。在 BRATS2018 和 BRATS2020 数据集上的大量实验表明，与现有的最先进方法相比，该方法显著提高了性能。 
**|
|**2024-06-14**|**Adaptive Teaching with Shared Classifier for Knowledge Distillation**|[2406.08528](http://arxiv.org/abs/2406.08528)|**[link](https://github.com/random2314235/atsc)**|**Knowledge distillation (KD) is a technique used to transfer knowledge from an overparameterized teacher network to a less-parameterized student network, thereby minimizing the incurred performance loss. KD methods can be categorized into offline and online approaches. Offline KD leverages a powerful pretrained teacher network, while online KD allows the teacher network to be adjusted dynamically to enhance the learning effectiveness of the student network. Recently, it has been discovered that sharing the classifier of the teacher network can significantly boost the performance of the student network with only a minimal increase in the number of network parameters. Building on these insights, we propose adaptive teaching with a shared classifier (ATSC). In ATSC, the pretrained teacher network self-adjusts to better align with the learning needs of the student network based on its capabilities, and the student network benefits from the shared classifier, enhancing its performance. Additionally, we extend ATSC to environments with multiple teachers. We conduct extensive experiments, demonstrating the effectiveness of the proposed KD method. Our approach achieves state-of-the-art results on the CIFAR-100 and ImageNet datasets in both single-teacher and multiteacher scenarios, with only a modest increase in the number of required model parameters. The source code is publicly available at https://github.com/random2314235/ATSC.**|
|**2024-06-12**|**DistilDoc: Knowledge Distillation for Visually-Rich Document Applications**|[2406.08226](http://arxiv.org/abs/2406.08226)|null|**This work explores knowledge distillation (KD) for visually-rich document (VRD) applications such as document layout analysis (DLA) and document image classification (DIC). While VRD research is dependent on increasingly sophisticated and cumbersome models, the field has neglected to study efficiency via model compression. Here, we design a KD experimentation methodology for more lean, performant models on document understanding (DU) tasks that are integral within larger task pipelines. We carefully selected KD strategies (response-based, feature-based) for distilling knowledge to and from backbones with different architectures (ResNet, ViT, DiT) and capacities (base, small, tiny). We study what affects the teacher-student knowledge gap and find that some methods (tuned vanilla KD, MSE, SimKD with an apt projector) can consistently outperform supervised student training. Furthermore, we design downstream task setups to evaluate covariate shift and the robustness of distilled DLA models on zero-shot layout-aware document visual question answering (DocVQA). DLA-KD experiments result in a large mAP knowledge gap, which unpredictably translates to downstream robustness, accentuating the need to further explore how to efficiently obtain more semantic document layout awareness.**|
|**2024-06-12**|**Low-Complexity Acoustic Scene Classification Using Parallel Attention-Convolution Network**|[2406.08119](http://arxiv.org/abs/2406.08119)|null|**这项工作是我们提交给DCASE2023挑战赛任务1的一个改进系统。我们提出了一种低复杂度的声场景分类方法，该方法采用并行注意力-卷积网络，该网络由四个模块组成，包括预处理、融合、全局和局部上下文信息提取。所提出的网络在从每个音频片段中捕获全局和局部上下文信息方面计算效率很高。此外，我们将其他技术集成到我们的方法中，例如知识蒸馏、数据增强和自适应残差归一化。在DCASE2023挑战赛的官方数据集上进行评估时，我们的方法获得了56.10%的最高准确率，参数量为5.21千，乘加运算次数为144万次。它在准确性和复杂度上都超过了DCASE2023挑战赛的前两名系统，获得了最先进的结果。代码位于：https://github.com/Jessytan/Low-complexity-ASC。 
**|
|**2024-06-12**|**Guiding Frame-Level CTC Alignments Using Self-knowledge Distillation**|[2406.07909](http://arxiv.org/abs/2406.07909)|null|**Transformer encoder with connectionist temporal classification (CTC) framework is widely used for automatic speech recognition (ASR). However, knowledge distillation (KD) for ASR displays a problem of disagreement between teacher-student models in frame-level alignment which ultimately hinders it from improving the student model's performance. In order to resolve this problem, this paper introduces a self-knowledge distillation (SKD) method that guides the frame-level alignment during the training time. In contrast to the conventional method using separate teacher and student models, this study introduces a simple and effective method sharing encoder layers and applying the sub-model as the student model. Overall, our approach is effective in improving both the resource efficiency as well as performance. We also conducted an experimental analysis of the spike timings to illustrate that the proposed method improves performance by reducing the alignment disagreement.**|
|**2024-06-12**|**Small Scale Data-Free Knowledge Distillation**|[2406.07876](http://arxiv.org/abs/2406.07876)|**[link](https://github.com/osvai/ssd-kd)**|**无数据知识蒸馏能够利用大型教师网络学习到的知识来增强小型学生网络的训练，而无需访问原始训练数据，从而避免了实际应用中的隐私、安全和专有风险。在这一研究方向上，现有方法通常遵循一种反演-蒸馏范式，其中使用在预训练教师网络指导下动态训练的生成对抗网络来合成用于知识蒸馏的大规模样本集。在本文中，我们重新审视了这种常见的无数据知识蒸馏范式，并通过“用于知识蒸馏的小规模反演数据”的视角展示了在整体训练效率方面仍有相当大的提升空间。基于三个经验证的观察结果，这些观察结果表明了在数据反演和蒸馏过程中如何平衡类别分布在合成样本多样性和难度方面的权衡的重要性，我们提出了小规模无数据知识蒸馏SSD-KD。具体而言，SSD-KD引入了一个调制函数来平衡合成样本，并引入了一个优先采样函数来选择合适的样本，这些函数由动态回放缓冲区和强化学习策略辅助。因此，SSD-KD可以在极小规模的合成样本（例如，比原始训练数据规模少10倍）上进行蒸馏训练，这使得整体训练效率比许多主流方法快一到两个数量级，同时保持优于或可媲美的模型性能，这一点在流行的图像分类和语义分割基准测试中得到了证明。代码可在https://github.com/OSVAI/SSD-KD获取。 
**|

## OCR

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**Enhancing Question Answering on Charts Through Effective Pre-training Tasks**|[2406.10085](http://arxiv.org/abs/2406.10085)|null|**要完全理解一个文档，仅使用文本信息是不够的。理解视觉线索，如布局和图表，也是必要的。虽然当前最先进的文档理解方法（基于 OCR 和非基于 OCR 的）运作良好，但尚未对其功能和局限性进行全面分析。因此，在这项工作中，我们解决了当前 VisualQA 模型在应用于图表时存在的局限性。为了调查最先进模型的缺点，我们以 ChartQA 为例进行了全面的行为分析。我们的研究结果表明，现有模型在回答与图表结构和视觉上下文以及数值信息相关的问题时表现不佳。为了解决这些问题，我们提出了三个简单的预训练任务，这些任务从结构视觉知识及其对数值问题的理解方面加强了现有模型。我们在三个图表数据集（包括提取性和抽象性问题数据集）上评估了我们预训练的模型（称为 MatCha-v2），并观察到它比基线模型平均提高了 1.7%。 
**|
|**2024-06-14**|**OSPC: Detecting Harmful Memes with Large Language Model as a Catalyst**|[2406.09779](http://arxiv.org/abs/2406.09779)|null|**作为网络上快速传播个人观点和立场的媒介，迷因也给社会偏见和歧视的传播带来了重大挑战。本研究提出了一种在新加坡多元文化和多语言环境下检测有害迷因的新方法。我们的方法整合了图像描述、光学字符识别 (OCR) 和大型语言模型 (LLM) 分析，以全面理解和分类有害迷因。该系统利用 BLIP 模型进行图像描述，使用 PP-OCR 和 TrOCR 进行多种语言的文本识别，并使用 Qwen LLM 进行细致入微的语言理解，能够识别以英语、中文、马来语和泰米尔语创建的迷因中的有害内容。为了提高系统性能，我们利用 GPT-4V 标注的额外数据对方法进行了微调，旨在将 GPT-4V 对有害迷因的理解能力提炼到我们的系统中。我们的框架在由新加坡人工智能举办的网络安全奖挑战赛的公开排行榜上名列前茅，AUROC 为 0.7749，准确率为 0.7087，远远领先于其他团队。值得注意的是，我们的方法优于之前的基准，FLAVA 的 AUROC 为 0.5695，VisualBERT 的 AUROC 为 0.5561。**|
|**2024-06-12**|**M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation**|[2406.08255](http://arxiv.org/abs/2406.08255)|**[link](https://github.com/amazon-science/m3t-multi-modal-translation-bench)**|**文档翻译对神经机器翻译 (NMT) 系统提出了挑战。大多数文档级 NMT 系统依赖于精心整理的句子级平行数据，假设可以从文档中完美地提取文本及其精确的阅读顺序。这些系统也倾向于忽略额外的视觉线索，例如文档布局，认为它们无关紧要。然而，现实世界的文档通常具有复杂的文本布局，这与这些假设相矛盾。从光学字符识别 (OCR) 或启发式规则中提取信息会导致错误，并且布局（例如，段落、标题）可能会传达文本不同部分之间的关系。这种复杂性在广泛使用的 PDF 文档中尤为明显，这些文档以视觉方式呈现信息。本文通过介绍 M3T 来解决这一差距，M3T 是一个新颖的基准数据集，专为评估 NMT 系统在翻译半结构化文档的综合任务上的性能而定制。该数据集旨在弥合文档级 NMT 系统中的评估差距，承认现实世界应用程序中丰富的文本布局带来的挑战。 
**|
|**2024-06-10**|**VCR: Visual Caption Restoration**|[2406.06462](http://arxiv.org/abs/2406.06462)|**[link](https://github.com/tianyu-z/vcr)**|**我们提出了视觉字幕修复 (VCR)，这是一项新颖的视觉语言任务，它挑战模型利用图像中的像素级提示来准确地修复被部分遮挡的文本。这项任务源于以下观察结果：嵌入在图像中的文本与常见的视觉元素和自然语言有着本质的不同，因为它需要对视觉、文本和嵌入在图像中的文本进行模态对齐。虽然许多工作已经将嵌入在图像中的文本整合到视觉问答任务中，但这些任务的方法通常依赖于光学字符识别或掩码语言建模，从而将任务简化为主要基于文本的处理。然而，在 VCR 中，基于文本的处理变得无效，因为准确的文本修复取决于来自提供的图像、上下文和来自被遮挡文本的微小暴露区域的细微线索的组合信息。我们开发了一个流程，使用图像-字幕对生成 VCR 任务的合成图像，并通过可调节的字幕可见性来控制任务难度。利用此流程，我们使用来自维基百科的带有字幕的图像构建了一个名为 VCR-Wiki 的 VCR 数据集，其中包含 211 万个英文实体和 34.6 万个中文实体，分为简单和困难两种变体。我们的结果表明，当前的视觉语言模型在 VCR 任务中的表现明显落后于人类，仅仅在我们的数据集上微调模型并不会带来显著的改进。我们发布了 VCR-Wiki 和数据构建代码，以促进未来的研究。**|
|**2024-06-07**|**Scaling Automatic Extraction of Pseudocode**|[2406.04635](http://arxiv.org/abs/2406.04635)|null|**学术论文中的伪代码提供了一种表达其中算法的简洁方法。伪代码也可以被视为一种中间表示，有助于弥合编程语言和自然语言之间的差距。访问大量伪代码集合可以带来各种好处，从增强算法理解、促进进一步的算法设计，到支持基于 NLP 或计算机视觉的模型完成自动代码生成和光学字符识别 (OCR) 等任务。我们通过从 arXiv 论文中提取近 320,000 个伪代码示例，创建了一个大型伪代码集合。这个过程涉及扫描超过 220 万篇学术论文，其中 1,000 篇经过人工检查和标记。鉴于集合固有的异质性，我们的方法包括一个为优化覆盖范围而定制的提取机制，以及一个基于随机抽样的验证机制，以检查其准确性和可靠性。此外，我们还提供了对常见伪代码结构的见解，并辅以聚类和统计分析。值得注意的是，这些分析表明伪代码的使用呈指数级增长，突出了它们日益增长的重要性。 
**|
|**2024-06-06**|**CORU: Comprehensive Post-OCR Parsing and Receipt Understanding Dataset**|[2406.04493](http://arxiv.org/abs/2406.04493)|**[link](https://github.com/update-for-integrated-business-ai/coru)**|**In the fields of Optical Character Recognition (OCR) and Natural Language Processing (NLP), integrating multilingual capabilities remains a critical challenge, especially when considering languages with complex scripts such as Arabic. This paper introduces the Comprehensive Post-OCR Parsing and Receipt Understanding Dataset (CORU), a novel dataset specifically designed to enhance OCR and information extraction from receipts in multilingual contexts involving Arabic and English. CORU consists of over 20,000 annotated receipts from diverse retail settings, including supermarkets and clothing stores, alongside 30,000 annotated images for OCR that were utilized to recognize each detected line, and 10,000 items annotated for detailed information extraction. These annotations capture essential details such as merchant names, item descriptions, total prices, receipt numbers, and dates. They are structured to support three primary computational tasks: object detection, OCR, and information extraction. We establish the baseline performance for a range of models on CORU to evaluate the effectiveness of traditional methods, like Tesseract OCR, and more advanced neural network-based approaches. These baselines are crucial for processing the complex and noisy document layouts typical of real-world receipts and for advancing the state of automated multilingual document processing. Our datasets are publicly accessible (https://github.com/Update-For-Integrated-Business-AI/CORU).**|
|**2024-06-03**|**Generalized Jersey Number Recognition Using Multi-task Learning With Orientation-guided Weight Refinement**|[2406.01033](http://arxiv.org/abs/2406.01033)|null|**Jersey number recognition (JNR) has always been an important task in sports analytics. Improving recognition accuracy remains an ongoing challenge because images are subject to blurring, occlusion, deformity, and low resolution. Recent research has addressed these problems using number localization and optical character recognition. Some approaches apply player identification schemes to image sequences, ignoring the impact of human body rotation angles on jersey digit identification. Accurately predicting the number of jersey digits by using a multi-task scheme to recognize each individual digit enables more robust results. Based on the above considerations, this paper proposes a multi-task learning method called the angle-digit refine scheme (ADRS), which combines human body orientation angles and digit number clues to recognize athletic jersey numbers. Based on our experimental results, our approach increases inference information, significantly improving prediction accuracy. Compared to state-of-the-art methods, which can only handle a single type of sport, the proposed method produces a more diverse and practical JNR application. The incorporation of diverse types of team sports such as soccer, football, basketball, volleyball, and baseball into our dataset contributes greatly to generalized JNR in sports analytics. Our accuracy achieves 64.07% on Top-1 and 89.97% on Top-2, with corresponding F1 scores of 67.46% and 90.64%, respectively.**|
|**2024-05-30**|**Scaling up archival text analysis with the blockmodeling of n-gram networks -- A case study of Bulgaria's representation in the Osservatore Romano (January-May 1877)**|[2405.20156](http://arxiv.org/abs/2405.20156)|null|**本文旨在通过应用网络聚类方法分析 1877 年 1 月至 5 月期间出版的 123 期《罗马观察报》对保加利亚的报道，从而弥合档案文本分析与网络分析之间的差距。本研究利用光学字符识别和广义同质性块模型构建了相关关键词网络。包括“保加利亚”和“俄罗斯”的集合网络同构性较高，并且与“德国”、“英国”和“战争”的集合网络有很大程度的重叠。在结构方面，这两个网络的块模型呈现出清晰的核心-半边缘-边缘结构，反映了报纸报道中概念之间的关系。该报的词汇选择有效地消解了保加利亚民族复兴运动的合法性，突出了罗马教廷对该报编辑路线的影响。 
**|
|**2024-05-30**|**Towards Unified Multi-granularity Text Detection with Interactive Attention**|[2405.19765](http://arxiv.org/abs/2405.19765)|null|**现有的OCR引擎或文档图像分析系统通常依赖于针对不同场景和粒度的文本检测训练单独的模型，这会导致巨大的计算复杂性和资源需求。在本文中，我们介绍了“检测任何文本”（DAT），这是一种先进的范例，它将场景文本检测、布局分析和文档页面检测无缝地统一到一个 cohesive 的端到端模型中。这种设计使 DAT 能够有效地管理不同粒度的文本实例，包括*单词*、*行*、*段落*和*页面*。DAT 的一项关键创新是跨粒度交互注意力模块，它通过关联不同文本查询的结构信息，显著增强了不同粒度文本实例的表示学习。因此，它使模型能够在多个文本粒度上实现互惠互利的检测性能。此外，基于提示的分割模块优化了任意曲率和复杂布局文本的检测结果，从而提高了 DAT 的准确性并扩展了其在现实世界中的适用性。实验结果表明，DAT 在各种文本相关基准测试中均达到了最先进的性能，包括多方向/任意形状场景文本检测、文档布局分析和页面检测任务。 
**|
|**2024-05-28**|**RealitySummary: On-Demand Mixed Reality Document Enhancement using Large Language Models**|[2405.18620](http://arxiv.org/abs/2405.18620)|null|**我们推出了 RealitySummary，这是一种混合现实阅读助手，可以使用按需文本提取、摘要和增强功能来增强任何印刷或数字文档。虽然增强阅读工具承诺通过覆盖数字内容来增强物理阅读体验，但以前的系统通常需要预处理文档，这限制了它们的通用性和现实用例。在本文中，我们利用大型语言模型探索按需文档增强。为了解不同文档的通用技术，我们首先进行了一项探索性设计研究，确定了五类文档增强功能（摘要、增强、导航、比较和提取）。在此基础上，我们开发了一个概念验证系统，可以使用 Google Cloud OCR 和 GPT-4 自动提取和汇总文本，然后使用 Microsoft Hololens 2 和 Apple Vision Pro 在文档周围嵌入信息。我们展示了六种特定文档增强的实时示例：1) 摘要，2) 比较表，3) 时间线，4) 关键词列表，5) 摘要突出显示和 6) 信息卡。可用性研究 (N=12) 和野外研究 (N=11) 的结果突出了按需 MR 文档增强的潜在好处以及未来研究的机会。 
**|

## 生成模型

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**SatDiffMoE: A Mixture of Estimation Method for Satellite Image Super-resolution with Latent Diffusion Models**|[2406.10225](http://arxiv.org/abs/2406.10225)|null|**在获取卫星图像时，由于卫星成像系统机载传感器的限制，通常需要在空间分辨率和时间分辨率（获取频率）之间进行权衡。高分辨率卫星图像对于土地作物监测、城市规划、野火管理和各种应用非常重要。实现高时空分辨率卫星成像是一项重要且具有挑战性的任务。随着扩散模型的出现，我们现在可以学习强大的生成先验，以生成具有高分辨率的真实卫星图像，这也可以用于促进超分辨率任务。在这项工作中，我们提出了一种新的基于扩散的融合算法，称为 SatDiffMoE，它可以将任意数量的同一位置的连续低分辨率卫星图像作为输入，并通过利用和融合来自不同时间点的补充信息，将它们融合成一个具有更多精细细节的高分辨率重建图像。我们的算法非常灵活，允许对任意数量的低分辨率图像进行训练和推理。实验结果表明，我们提出的 SatDiffMoE 方法不仅在各种数据集上的卫星图像超分辨率任务中取得了优异的性能，而且与以前的方法相比，还提高了计算效率并减少了模型参数。 
**|
|**2024-06-14**|**Universal randomised signatures for generative time series modelling**|[2406.10214](http://arxiv.org/abs/2406.10214)|**[link](https://github.com/niklaswalter/randomised-signature-timeseries-generation)**|**随机签名已被提出作为一种灵活且易于实现的替代方案，用于替代已建立的路径签名方法。在本文中，我们采用随机签名，本着水库计算的精神，为金融时间序列数据引入了一个生成模型。具体来说，我们提出了一种基于离散时间随机签名的全新Wasserstein类型距离。这种概率测度空间上的度量捕捉了（条件）分布之间的距离。我们关于随机签名在以基础路径为输入的连续函数空间上的通用逼近性的新结果证明了其使用的合理性。然后，我们将度量作为损失函数，用于基于水库神经随机微分方程的合成时间序列数据的非对抗性生成器模型中。我们将模型的结果与现有文献中的基准进行了比较。 
**|
|**2024-06-14**|**DiffusionBlend: Learning 3D Image Prior through Position-aware Diffusion Score Blending for 3D Computed Tomography Reconstruction**|[2406.10211](http://arxiv.org/abs/2406.10211)|null|**Diffusion models face significant challenges when employed for large-scale medical image reconstruction in real practice such as 3D Computed Tomography (CT). Due to the demanding memory, time, and data requirements, it is difficult to train a diffusion model directly on the entire volume of high-dimensional data to obtain an efficient 3D diffusion prior. Existing works utilizing diffusion priors on single 2D image slice with hand-crafted cross-slice regularization would sacrifice the z-axis consistency, which results in severe artifacts along the z-axis. In this work, we propose a novel framework that enables learning the 3D image prior through position-aware 3D-patch diffusion score blending for reconstructing large-scale 3D medical images. To the best of our knowledge, we are the first to utilize a 3D-patch diffusion prior for 3D medical image reconstruction. Extensive experiments on sparse view and limited angle CT reconstruction show that our DiffusionBlend method significantly outperforms previous methods and achieves state-of-the-art performance on real-world CT reconstruction problems with high-dimensional 3D image (i.e., $256 \times 256 \times 500$ ). Our algorithm also comes with better or comparable computational efficiency than previous state-of-the-art methods.**|
|**2024-06-14**|**Make It Count: Text-to-Image Generation with an Accurate Number of Objects**|[2406.10210](http://arxiv.org/abs/2406.10210)|null|**Despite the unprecedented success of text-to-image diffusion models, controlling the number of depicted objects using text is surprisingly hard. This is important for various applications from technical documents, to children's books to illustrating cooking recipes. Generating object-correct counts is fundamentally challenging because the generative model needs to keep a sense of separate identity for every instance of the object, even if several objects look identical or overlap, and then carry out a global computation implicitly during generation. It is still unknown if such representations exist. To address count-correct generation, we first identify features within the diffusion model that can carry the object identity information. We then use them to separate and count instances of objects during the denoising process and detect over-generation and under-generation. We fix the latter by training a model that predicts both the shape and location of a missing object, based on the layout of existing ones, and show how it can be used to guide denoising with correct object count. Our approach, CountGen, does not depend on external source to determine object layout, but rather uses the prior from the diffusion model itself, creating prompt-dependent and seed-dependent layouts. Evaluated on two benchmark datasets, we find that CountGen strongly outperforms the count-accuracy of existing baselines.**|
|**2024-06-14**|**Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual Visual Text Rendering**|[2406.10208](http://arxiv.org/abs/2406.10208)|null|**Recently, Glyph-ByT5 has achieved highly accurate visual text rendering performance in graphic design images. However, it still focuses solely on English and performs relatively poorly in terms of visual appeal. In this work, we address these two fundamental limitations by presenting Glyph-ByT5-v2 and Glyph-SDXL-v2, which not only support accurate visual text rendering for 10 different languages but also achieve much better aesthetic quality. To achieve this, we make the following contributions: (i) creating a high-quality multilingual glyph-text and graphic design dataset consisting of more than 1 million glyph-text pairs and 10 million graphic design image-text pairs covering nine other languages, (ii) building a multilingual visual paragraph benchmark consisting of 1,000 prompts, with 100 for each language, to assess multilingual visual spelling accuracy, and (iii) leveraging the latest step-aware preference learning approach to enhance the visual aesthetic quality. With the combination of these techniques, we deliver a powerful customized multilingual text encoder, Glyph-ByT5-v2, and a strong aesthetic graphic generation model, Glyph-SDXL-v2, that can support accurate spelling in 10 different languages. We perceive our work as a significant advancement, considering that the latest DALL-E3 and Ideogram 1.0 still struggle with the multilingual visual text rendering task.**|
|**2024-06-14**|**Crafting Parts for Expressive Object Composition**|[2406.10197](http://arxiv.org/abs/2406.10197)|null|**Text-to-image generation from large generative models like Stable Diffusion, DALLE-2, etc., have become a common base for various tasks due to their superior quality and extensive knowledge bases. As image composition and generation are creative processes the artists need control over various parts of the images being generated. We find that just adding details about parts in the base text prompt either leads to an entirely different image (e.g., missing/incorrect identity) or the extra part details simply being ignored. To mitigate these issues, we introduce PartCraft, which enables image generation based on fine-grained part-level details specified for objects in the base text prompt. This allows more control for artists and enables novel object compositions by combining distinctive object parts. PartCraft first localizes object parts by denoising the object region from a specific diffusion process. This enables each part token to be localized to the right object region. After obtaining part masks, we run a localized diffusion process in each of the part regions based on fine-grained part descriptions and combine them to produce the final image. All the stages of PartCraft are based on repurposing a pre-trained diffusion model, which enables it to generalize across various domains without training. We demonstrate the effectiveness of part-level control provided by PartCraft qualitatively through visual examples and quantitatively in comparison to the contemporary baselines.**|
|**2024-06-14**|**Enhancing Incomplete Multi-modal Brain Tumor Segmentation with Intra-modal Asymmetry and Inter-modal Dependency**|[2406.10175](http://arxiv.org/abs/2406.10175)|null|**近年来，基于深度学习的多模态MRI图像脑肿瘤分割(BTS)模型取得了显著进展。然而，实践中一个常见问题是，由于扫描方案和患者情况不同，某些模态的图像可能无法获得，这使得从不完整MRI模态进行分割成为一个难题。以往的方法试图通过融合可获得的多模态特征、利用注意力机制和使用生成模型合成缺失模态来解决这一问题。然而，这些方法忽略了医学图像分割的内在问题，例如训练样本有限，特别是对于存在肿瘤的情况。此外，这些方法需要针对每个缺失模态子集训练和部署特定的模型。为了解决这些问题，我们提出了一种新的方法，从两个方面增强BTS模型。首先，我们引入了一个预训练阶段，生成一个涵盖各种肿瘤形状和脑部解剖结构组合的多样化预训练数据集。其次，我们提出了一个后训练阶段，使模型能够在只有部分模态可用的情况下，重建预测结果中缺失的模态。为了实现预训练阶段，我们从概念上将MRI图像解耦为两部分：“解剖结构”和“肿瘤”。我们使用从不同训练样本的解剖结构和肿瘤部分生成的合成数据对BTS模型进行预训练。……大量实验表明，我们提出的方法相对于基线方法显著提高了性能，并在三个脑肿瘤分割数据集BRATS2020、BRATS2018和BRATS2015上取得了新的最先进结果。 
**|
|**2024-06-14**|**Training-free Camera Control for Video Generation**|[2406.10126](http://arxiv.org/abs/2406.10126)|null|**我们提出了一种无需训练且鲁棒的解决方案，用于为现成的视频扩散模型提供相机运动控制。与以往的工作不同，我们的方法不需要在相机标注数据集上进行任何监督微调，也不需要通过数据增强进行自监督训练。相反，它可以与大多数预训练的视频扩散模型即插即用，并以单个图像或文本提示作为输入生成相机可控视频。我们工作的灵感来自于中间层潜在表示对生成结果具有的布局先验，因此重新排列其中的噪声像素也将使输出内容重新分配。由于相机运动也可以看作是由视角变化引起的像素重排，因此如果视频的噪声潜在表示发生相应的变化，则可以按照特定的相机运动对视频进行重新组织。在此基础上，我们提出了CamTrol方法，该方法为视频扩散模型提供了强大的相机控制能力。它通过两阶段过程实现。首先，我们在3D点云空间中通过显式相机运动对图像布局重排进行建模。其次，我们利用一系列重新排列的图像形成的噪声潜在表示的布局先验，生成具有相机运动的视频。大量实验表明，我们的方法在控制生成视频的相机运动方面具有鲁棒性。此外，我们还展示了我们的方法在生成具有动态内容的3D旋转视频方面可以产生令人印象深刻的结果。项目页面：https://lifedecoder.github.io/CamTrol/。 
**|
|**2024-06-14**|**Precipitation Nowcasting Using Physics Informed Discriminator Generative Models**|[2406.10108](http://arxiv.org/abs/2406.10108)|null|**短临预报利用实时大气条件来预测短期天气。包括 PySTEPS 在内的最先进模型在准确预测极端天气事件方面遇到困难，因为它们的分布模式不可预测。在本研究中，我们设计了一个基于物理的神经网络，利用荷兰皇家气象研究所 (KNMI) 的降水和气象数据进行降水短临预报。该模型的灵感来自于新颖的物理信息鉴别器 GAN (PID-GAN) 公式，将基于物理的监督直接集成到对抗学习框架中。所提出的模型采用 GAN 结构，以矢量量化生成对抗网络 (VQ-GAN) 和 Transformer 作为生成器，并以时间鉴别器作为鉴别器。我们的研究结果表明，PID-GAN 模型在下游降水短临预报指标方面优于数值模型和最先进的深度生成模型。 
**|
|**2024-06-14**|**Group and Shuffle: Efficient Structured Orthogonal Parametrization**|[2406.10019](http://arxiv.org/abs/2406.10019)|null|**神经网络规模的不断增大导致对高效微调方法的需求日益增长。最近，一种正交微调范式被提出，它使用正交矩阵来调整预训练模型的权重。在本文中，我们介绍了一种新的结构化矩阵类别，它统一并概括了先前工作中的结构化类别。我们研究了此类的属性，并在此基础上构建了结构化正交参数化。然后，我们使用这种参数化来修改正交微调框架，从而提高参数和计算效率。我们在不同的领域对我们的方法进行了实证验证，包括文本到图像扩散模型的适应和语言建模中的下游任务微调。此外，我们还针对正交卷积调整了我们的结构，并使用 1-Lipschitz 神经网络进行了实验。 
**|

## LLM

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Pre-trained Language Models**|[2406.10130](http://arxiv.org/abs/2406.10130)|null|**预训练语言模型 (PLM) 已被证实包含有害信息，例如社会偏见，这可能会造成负面社会影响，甚至在应用中带来灾难性后果。以前针对这个问题的工作主要集中在使用黑盒方法（例如探测）通过观察模型输出来检测和量化 PLM 中的社会偏见。因此，以前的去偏方法主要是在新构建的反刻板印象数据集上微调甚至预训练语言模型，而这些数据集的成本很高。在这项工作中，我们试图通过引入“社会偏见神经元”的概念来揭示语言模型内部社会偏见的奥秘。具体来说，我们提出了“集成梯度差距 (IG $^2$)”来准确查明语言模型中可归因于不良行为（例如社会偏见）的单元（即神经元）。通过将不良行为形式化为语言的分布属性，我们使用带有情感的提示来引出与这些情感相关的敏感词类别（人口统计）。因此，我们的 IG$^2$ 将不同人口统计数据的分布不均归因于特定的社会偏见神经元，这些神经元跟踪 PLM 单元内部不需要的行为轨迹以实现互操作性。此外，基于我们可解释的技术，进一步提出了“偏见神经元抑制 (BNS)”来减轻社会偏见。通过研究 BERT、RoBERTa 及其与去偏 FairBERTa 的归因差异，IG$^2$ 使我们能够定位和抑制已识别的神经元，并进一步减轻不良行为。根据 StereoSet 中的先前指标衡量，我们的模型以低成本实现了更高程度的公平性，同时保持了语言建模能力。 
**|
|**2024-06-14**|**Experiments in News Bias Detection with Pre-Trained Neural Transformers**|[2406.09938](http://arxiv.org/abs/2406.09938)|null|**The World Wide Web provides unrivalled access to information globally, including factual news reporting and commentary. However, state actors and commercial players increasingly spread biased (distorted) or fake (non-factual) information to promote their agendas. We compare several large, pre-trained language models on the task of sentence-level news bias detection and sub-type classification, providing quantitative and qualitative results.**|
|**2024-06-14**|**Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity**|[2406.09790](http://arxiv.org/abs/2406.09790)|null|**Semantic Textual Similarity (STS) constitutes a critical research direction in computational linguistics and serves as a key indicator of the encoding capabilities of embedding models. Driven by advances in pre-trained language models and contrastive learning techniques, leading sentence representation methods can already achieved average Spearman's correlation scores of approximately 86 across seven STS benchmarks in SentEval. However, further improvements have become increasingly marginal, with no existing method attaining an average score higher than 87 on these tasks. This paper conducts an in-depth analysis of this phenomenon and concludes that the upper limit for Spearman's correlation scores using contrastive learning is 87.5. To transcend this ceiling, we propose an innovative approach termed Pcc-tuning, which employs Pearson's correlation coefficient as a loss function to refine model performance beyond contrastive learning. Experimental results demonstrate that Pcc-tuning markedly surpasses previous state-of-the-art strategies, raising the Spearman's correlation score to above 90.**|
|**2024-06-13**|**Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models**|[2406.09206](http://arxiv.org/abs/2406.09206)|null|**Active learning is an iterative labeling process that is used to obtain a small labeled subset, despite the absence of labeled data, thereby enabling to train a model for supervised tasks such as text classification. While active learning has made considerable progress in recent years due to improvements provided by pre-trained language models, there is untapped potential in the often neglected unlabeled portion of the data, although it is available in considerably larger quantities than the usually small set of labeled data. Here we investigate how self-training, a semi-supervised approach where a model is used to obtain pseudo-labels from the unlabeled data, can be used to improve the efficiency of active learning for text classification. Starting with an extensive reproduction of four previous self-training approaches, some of which are evaluated for the first time in the context of active learning or natural language processing, we devise HAST, a new and effective self-training strategy, which is evaluated on four text classification benchmarks, on which it outperforms the reproduced self-training approaches and reaches classification results comparable to previous experiments for three out of four datasets, using only 25% of the data.**|
|**2024-06-13**|**Generating Speakers by Prompting Listener Impressions for Pre-trained Multi-Speaker Text-to-Speech Systems**|[2406.08812](http://arxiv.org/abs/2406.08812)|null|**本文提出了一种语音合成系统，允许用户通过描述说话者特征的提示来指定和控制合成语音的声学特征。与以往的方法不同，我们的方法利用听者的印象来构建提示，这些提示更容易收集，并且更自然地与说话者特征的日常描述相一致。我们采用低秩适应（LoRA）技术，快速地将预训练的语言模型调整到我们的需求，从而便于从提示文本中提取与说话者相关的特征。此外，与其他提示驱动的文本到语音（TTS）系统不同，我们将提示到说话者模块与多说话者TTS系统分离，增强了系统灵活性，并提高了与各种预训练的多说话者TTS系统的兼容性。此外，对于提示到说话者特征模块，我们还比较了判别方法和基于流匹配的生成方法，我们发现结合这两种方法可以帮助系统更好地从提示中同时捕捉与说话者相关的信息，并生成更高保真度的语音。 
**|
|**2024-06-12**|**Unraveling Code-Mixing Patterns in Migration Discourse: Automated Detection and Analysis of Online Conversations on Reddit**|[2406.08633](http://arxiv.org/abs/2406.08633)|null|**全球移民模式的激增凸显了将移民无缝融入东道社区的必要性，这需要包容和值得信赖的公共服务。尽管北欧国家拥有强大的公共部门基础设施，但近期移民在获取这些服务方面经常遇到障碍，加剧了社会差距并削弱了信任。 在这项努力中，解决数字不平等和语言多样性至关重要。 本文探讨了代码混合的使用，这是一种在多语言使用者中普遍存在的交流策略，用于 Reddit 等社交媒体平台上与移民相关的讨论。我们提出了多语言代码混合文本识别集成学习 (ELMICT)，这是一种旨在自动检测与移民相关的讨论中代码混合消息的新方法。ELMICT 利用集成学习技术来组合多个分词器的输出和预训练的语言模型，在识别跨各种语言和上下文的代码混合方面表现出高性能（F1 值大于 0.95），特别是在跨语言零样本条件下（平均 F1 值大于 0.70）。此外，ELMICT 的使用有助于分析与移民相关的主题中代码混合的流行程度，与 Reddit 上的其他主题类别相比，揭示了移民社区关注的话题。我们的研究结果揭示了移民在社交媒体平台上采用的交流策略，为开发包容性数字公共服务和对话系统提供了启示。通过解决本研究中提出的研究问题，我们有助于理解移民话语中的语言多样性，并为构建多元文化社会信任的更有效工具铺平道路。 
**|
|**2024-06-12**|**Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL**|[2406.08426](http://arxiv.org/abs/2406.08426)|null|**根据自然语言问题生成准确的SQL（文本到SQL）是一个长期存在的问题，因为用户问题理解、数据库模式理解和SQL生成都具有挑战性。传统的文本到SQL系统包括人工工程和深度神经网络。随后，预训练语言模型（PLM）被开发出来并用于文本到SQL任务，取得了可喜的性能。随着现代数据库变得越来越复杂，相应的用户问题也更具挑战性，理解能力有限的PLM可能会导致错误的SQL生成。这就需要更复杂和更有针对性的优化方法，而这反过来又限制了基于PLM的系统的应用。最近，大型语言模型（LLM）在自然语言理解方面表现出显著的能力，因为模型规模不断扩大。因此，整合基于LLM的实现可以为文本到SQL研究带来独特的机会、挑战和解决方案。在这篇综述中，我们全面回顾了基于LLM的文本到SQL。具体来说，我们简要概述了当前的挑战和文本到SQL的演变过程。然后，我们详细介绍了用于评估文本到SQL系统的数据集和指标。之后，我们系统地分析了基于LLM的文本到SQL的最新进展。最后，我们讨论了该领域 remaining challenges 并对未来方向提出了期望。 
**|
|**2024-06-12**|**Leveraging Large Language Models for Web Scraping**|[2406.08246](http://arxiv.org/abs/2406.08246)|null|**大型语言模型 (LLM) 在复制人类任务和提高生产力方面表现出非凡的能力。然而，由于优先考虑流畅性而非事实准确性，以及处理特定信息的局限性，LLM 直接应用于数据提取存在局限性。为了克服这些局限性，本研究利用预训练 LLM 的知识表示能力和 RAG 模型的目标信息访问能力，研究了一种通用的、准确的 RAG 模型数据抓取方案，该方案专为语言生成而设计。为了以更模块化和可解释的方式获取知识，我们使用具有潜在知识检索器的预训练语言模型，允许模型从大型语料库中检索文档并对其进行关注。我们利用 RAG 模型架构，深入分析了它们在以下三项任务中的能力：(i) HTML 元素的语义分类，(ii) 对 HTML 文本进行分块以实现有效理解，以及 (iii) 比较不同 LLM 和排序算法的结果。虽然之前的工作已经开发了用于 HTML 理解和提取的专用架构和训练程序，但我们表明，在标准自然语言上进行预训练并添加有效的分块、搜索和排序算法的 LLM 可以证明是一种有效的数据抓取工具，可以从非结构化文本中提取复杂数据。未来的研究方向包括解决所提出的基于 RAG 的数据提取框架中的来源跟踪和动态知识更新挑战。通过克服这些限制，这种方法有可能彻底改变从大量文本信息库中提取数据的方式。 
**|
|**2024-06-12**|**Guiding In-Context Learning of LLMs through Quality Estimation for Machine Translation**|[2406.07970](http://arxiv.org/abs/2406.07970)|**[link](https://github.com/JoyeBright/ICLviaQE)**|**大型语言模型 (LLM) 的输出质量，尤其是在机器翻译 (MT) 中，与查询（即待翻译文本）一起提供的上下文示例 (ICE) 的质量密切相关。这些 ICE 的有效性受多种因素的影响，例如源文本的领域、ICE 的呈现顺序、示例数量以及使用的提示模板。自然地，选择最具影响力的 ICE 取决于理解这些因素如何影响最终的翻译质量，而这最终依赖于翻译参考或人工判断。本文提出了一种新的上下文学习 (ICL) 方法，该方法依赖于特定领域质量估计 (QE) 指导的搜索算法。我们的方法利用 XGLM 模型，无需翻译参考即可估计最终的翻译质量，从而为 MT 选择有效的 ICE 以最大限度地提高翻译质量。我们的结果表明，与现有的 ICL 方法相比，该方法有显著改进，并且与微调预训练语言模型 (PLM)（特别是 mBART-50）相比，翻译性能更高。 
**|
|**2024-06-12**|**Label-aware Hard Negative Sampling Strategies with Momentum Contrastive Learning for Implicit Hate Speech Detection**|[2406.07886](http://arxiv.org/abs/2406.07886)|**[link](https://github.com/hanyang-hcc-lab/lahn)**|**检测未直接表达仇恨的隐性仇恨言论仍然是一个挑战。最近的研究尝试通过将对比学习应用于预训练语言模型（如BERT和RoBERTa）来检测隐性仇恨言论，但与基于交叉熵损失的学习相比，所提出的模型仍然没有显著优势。我们发现，基于随机抽样批次数据的对比学习并不能鼓励模型学习困难负样本。在这项工作中，我们提出了标签感知困难负样本采样策略（LAHN），该策略鼓励模型使用动量集成的对比学习，从困难负样本中学习详细特征，而不是随机批次中的简单负样本。在数据集内和跨数据集的隐性仇恨言论检测任务中，LAHN 的性能均优于现有模型。代码可在 https://github.com/Hanyang-HCC-Lab/LAHN 获取。 
**|

## Transformer

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**Whisper-Flamingo: Integrating Visual Features into Whisper for Audio-Visual Speech Recognition and Translation**|[2406.10082](http://arxiv.org/abs/2406.10082)|**[link](https://github.com/roudimit/whisper-flamingo)**|**Audio-Visual Speech Recognition (AVSR) uses lip-based video to improve performance in noise. Since videos are harder to obtain than audio, the video training data of AVSR models is usually limited to a few thousand hours. In contrast, speech models such as Whisper are trained with hundreds of thousands of hours of data, and thus learn a better speech-to-text decoder. The huge training data difference motivates us to adapt Whisper to handle video inputs. Inspired by Flamingo which injects visual features into language models, we propose Whisper-Flamingo which integrates visual features into the Whisper speech recognition and translation model with gated cross attention. Our audio-visual Whisper-Flamingo outperforms audio-only Whisper on English speech recognition and En-X translation for 6 languages in noisy conditions. Moreover, Whisper-Flamingo is a versatile model and conducts all of these tasks using one set of parameters, while prior methods are trained separately on each language.**|
|**2024-06-14**|**Simul-Whisper: Attention-Guided Streaming Whisper with Truncation Detection**|[2406.10052](http://arxiv.org/abs/2406.10052)|null|**As a robust and large-scale multilingual speech recognition model, Whisper has demonstrated impressive results in many low-resource and out-of-distribution scenarios. However, its encoder-decoder structure hinders its application to streaming speech recognition. In this paper, we introduce Simul-Whisper, which uses the time alignment embedded in Whisper's cross-attention to guide auto-regressive decoding and achieve chunk-based streaming ASR without any fine-tuning of the pre-trained model. Furthermore, we observe the negative effect of the truncated words at the chunk boundaries on the decoding results and propose an integrate-and-fire-based truncation detection model to address this issue. Experiments on multiple languages and Whisper architectures show that Simul-Whisper achieves an average absolute word error rate degradation of only 1.46% at a chunk size of 1 second, which significantly outperforms the current state-of-the-art baseline.**|
|**2024-06-14**|**IFA: Interaction Fidelity Attention for Entire Lifelong Behaviour Sequence Modeling**|[2406.09742](http://arxiv.org/abs/2406.09742)|null|**用户的终身行为序列提供了丰富的用户偏好信息，并在推荐任务中获得了显著的改进，但也显著增加了计算量。为了满足在线服务中严格的延迟要求，通常会根据与目标商品的相似度对一个较短的子序列进行采样。然而，不在子序列中的商品会被丢弃，导致严重的信息丢失。在本文中，我们提出了一种新的高效范式来对完整的终身序列进行建模，称为交互保真度注意力（IFA）。在IFA中，我们将候选集中的所有目标商品一次性输入模型，并利用线性Transformer来降低候选集和序列之间交叉注意力的的时间复杂度，而不会丢失任何交互信息。我们还额外地对所有目标商品之间的关系进行建模，以实现最佳集合生成，并设计了损失函数以提高训练和推理的一致性。我们在快手推荐系统的离线和在线实验中证明了我们模型的有效性和效率。 
**|
|**2024-06-13**|**Multi-Modal Retrieval For Large Language Model Based Speech Recognition**|[2406.09618](http://arxiv.org/abs/2406.09618)|null|**检索是一种广泛采用的方法，用于利用外部信息改进语言模型。随着该领域向多模态大型语言模型发展，重要的是扩展纯文本方法，将其他模态也纳入检索中，以便应用于各种机器学习任务和数据类型。在这项工作中，我们提出了两种多模态检索方法：kNN-LM 和交叉注意力技术。我们通过将检索方法应用于可访问外部信息的自动语音识别任务，凭经验证明了其有效性。在此设置下，我们证明了基于语音的多模态检索优于基于文本的检索，并且与多模态语言模型基线相比，词错误率降低了高达 50%。此外，我们在 Spoken-Squad 问答数据集上取得了最先进的识别结果。 
**|
|**2024-06-13**|**Cross-Modal Learning for Anomaly Detection in Fused Magnesium Smelting Process: Methodology and Benchmark**|[2406.09016](http://arxiv.org/abs/2406.09016)|null|**熔镁炉 (FMF) 是氧化镁生产中的关键工业设备，异常检测对其高效、稳定和安全运行起着至关重要的作用。现有的异常检测方法主要集中于使用过程变量（如电弧电流）分析主要异常或基于异常视觉特征构建神经网络，而忽略了跨模态信息的内在关联。本文提出了一种跨模态 Transformer（称为 FmFormer），旨在通过探索视觉特征（视频）和过程变量（电流）之间的相关性来促进熔融镁冶炼过程中的异常检测。我们的方法引入了一种新颖的标记化范式，以多尺度方式有效地弥合 3D 视频模态和 1D 电流模态之间的巨大维度差距，从而实现像素级异常检测的分层重建。随后，FmFormer 利用自注意力机制学习每个模态内的内部特征，并利用双向交叉注意力机制捕获模态间的相关性。为了验证所提出方法的有效性，我们还提出了一个开创性的熔融镁冶炼过程跨模态基准，该基准包含超过 220 万个样本的同步采集的视频和电流数据。利用跨模态学习，所提出的 FmFormer 在检测异常方面实现了最先进的性能，特别是在电流波动和浓密水雾造成的视觉遮挡等极端干扰下。所提出的方法和基准经过一些修改后可能适用于其他工业应用。该基准将在 https://github.com/GaochangWu/FMF-Benchmark 上发布。 
**|
|**2024-06-13**|**Dual Attribute-Spatial Relation Alignment for 3D Visual Grounding**|[2406.08907](http://arxiv.org/abs/2406.08907)|null|**三维视觉定位是一个新兴的研究领域，致力于在三维物理世界和自然语言之间建立联系，这对于实现具身智能至关重要。在本文中，我们提出了 DASANet，一种双重属性-空间关系对齐网络，它分别对语言和 3D 视觉模态之间的对象属性和空间关系特征进行建模和对齐。我们将语言和 3D 点云输入分解为两个独立的部分，并设计了一个双分支注意力模块，以分别对分解后的输入进行建模，同时通过交叉注意力在属性-空间特征融合中保留全局上下文。我们的 DASANet 在 Nr3D 数据集上实现了最高的定位精度 65.1%，比最佳竞争对手高 1.3%。此外，两个分支的可视化证明了我们的方法是高效且高度可解释的。 
**|
|**2024-06-12**|**Diffusion-Promoted HDR Video Reconstruction**|[2406.08204](http://arxiv.org/abs/2406.08204)|null|**高动态范围 (HDR) 视频重建旨在从以交替曝光拍摄的低动态范围 (LDR) 帧生成 HDR 视频。大多数现有工作仅依赖于基于回归的范式，导致了诸如重影伪影和饱和区域细节丢失等不利影响。在本文中，我们提出了一种用于 HDR 视频重建的扩散促进方法，称为 HDR-V-Diff，它结合了扩散模型来捕捉 HDR 分布。因此，HDR-V-Diff 可以重建具有逼真细节的 HDR 视频，同时减少重影伪影。然而，直接引入视频扩散模型会带来巨大的计算负担。为了减轻这种负担，我们首先提出了一种 HDR 隐式扩散模型 (HDR-LDM) 来学习单个 HDR 帧的分布先验。具体来说，HDR-LDM 结合了一种色调映射策略将 HDR 帧压缩到隐空间中，并采用了一种新颖的曝光嵌入方法将曝光信息聚合到扩散过程中。然后，我们提出了一个时间一致性对齐模块 (TCAM) 来学习时间信息作为 HDR-LDM 的补充，该模块在视频帧的不同尺度上进行从粗到精的特征对齐。最后，我们设计了一种零初始化交叉注意力 (ZiCA) 机制，以有效地整合学习到的分布先验和时间信息来生成 HDR 帧。大量实验验证了 HDR-V-Diff 在多个代表性数据集上实现了最先进的结果。 
**|
|**2024-06-12**|**Labeling Comic Mischief Content in Online Videos with a Multimodal Hierarchical-Cross-Attention Model**|[2406.07841](http://arxiv.org/abs/2406.07841)|**[link](https://github.com/RiTUAL-UH/Comic-Mischief-Prediction)**|**我们解决了在线媒体中检测可疑内容的挑战，特别是有趣恶作剧这一子类别。这类内容将暴力、成人内容或讽刺与幽默等元素结合在一起，使其难以被检测到。采用多模态方法对于捕捉有趣恶作剧内容中固有的微妙细节至关重要。为了解决这个问题，我们针对有趣恶作剧检测任务提出了一种新颖的端到端多模态系统。作为这项贡献的一部分，我们发布了一个针对目标任务的新数据集，该数据集包含三种模态：视频、文本（视频字幕和隐藏式字幕）和音频。我们还设计了一个包含字幕的分层交叉注意模型（HICCAP），以捕捉这些模态之间错综复杂的关系。结果表明，与强大的基线模型和用于有趣恶作剧检测及其类型分类的最先进模型相比，所提出的方法取得了显著的改进。这强调了我们的系统赋能用户、使其能够对自己选择观看的在线内容做出明智决定的潜力。此外，我们还在 UCF101、HMDB51 和 XD-Violence 数据集上进行了实验，将我们的模型与其他最先进的方法进行了比较，展示了我们提出的模型在各种场景下的出色性能。 
**|
|**2024-06-11**|**M-LRM: Multi-view Large Reconstruction Model**|[2406.07648](http://arxiv.org/abs/2406.07648)|null|**Despite recent advancements in the Large Reconstruction Model (LRM) demonstrating impressive results, when extending its input from single image to multiple images, it exhibits inefficiencies, subpar geometric and texture quality, as well as slower convergence speed than expected.   It is attributed to that, LRM formulates 3D reconstruction as a naive images-to-3D translation problem, ignoring the strong 3D coherence among the input images. In this paper, we propose a Multi-view Large Reconstruction Model (M-LRM) designed to efficiently reconstruct high-quality 3D shapes from multi-views in a 3D-aware manner. Specifically, we introduce a multi-view consistent cross-attention scheme to enable M-LRM to accurately query information from the input images. Moreover, we employ the 3D priors of the input multi-view images to initialize the tri-plane tokens. Compared to LRM, the proposed M-LRM can produce a tri-plane NeRF with $128 \times 128$ resolution and generate 3D shapes of high fidelity. Experimental studies demonstrate that our model achieves a significant performance gain and faster training convergence than LRM. Project page: https://murphylmf.github.io/M-LRM/**|
|**2024-06-11**|**MS-Diffusion: Multi-subject Zero-shot Image Personalization with Layout Guidance**|[2406.07209](http://arxiv.org/abs/2406.07209)|**[link](https://github.com/MS-Diffusion/MS-Diffusion)**|**Recent advancements in text-to-image generation models have dramatically enhanced the generation of photorealistic images from textual prompts, leading to an increased interest in personalized text-to-image applications, particularly in multi-subject scenarios. However, these advances are hindered by two main challenges: firstly, the need to accurately maintain the details of each referenced subject in accordance with the textual descriptions; and secondly, the difficulty in achieving a cohesive representation of multiple subjects in a single image without introducing inconsistencies. To address these concerns, our research introduces the MS-Diffusion framework for layout-guided zero-shot image personalization with multi-subjects. This innovative approach integrates grounding tokens with the feature resampler to maintain detail fidelity among subjects. With the layout guidance, MS-Diffusion further improves the cross-attention to adapt to the multi-subject inputs, ensuring that each subject condition acts on specific areas. The proposed multi-subject cross-attention orchestrates harmonious inter-subject compositions while preserving the control of texts. Comprehensive quantitative and qualitative experiments affirm that this method surpasses existing models in both image and text fidelity, promoting the development of personalized text-to-image generation.**|

## 3DGS

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting**|[2406.10219](http://arxiv.org/abs/2406.10219)|null|**近年来，新视角合成技术的进步实现了实时渲染速度和高重建精度。三维高斯 splatting (3D-GS) 是一种基础的基于点的三维场景参数化表示方法，它将场景建模为大量三维高斯函数的集合。复杂的场景可能包含数百万个高斯函数，导致存储和内存需求巨大，限制了 3D-GS 在资源有限的设备上的可行性。目前，通过剪枝高斯函数来压缩这些预训练模型的技术依赖于结合启发式方法来确定要移除哪些高斯函数。在本文中，我们提出了一种基于原理的空间敏感性剪枝评分方法，该方法优于这些方法。它被计算为训练视图上的重建误差相对于每个高斯函数的空间参数的二阶近似。此外，我们提出了一个多轮剪枝-精炼流程，该流程可以应用于任何预训练的 3D-GS 模型，而无需更改训练流程。在剪枝掉 88.44% 的高斯函数后，我们观察到我们的 PUP 3D-GS 流程将 3D-GS 的平均渲染速度提高了 2.65 倍，同时保留了更多显著的前景信息，并在来自 Mip-NeRF 360、Tanks & Temples 和 Deep Blending 数据集的场景上实现了比以前的剪枝技术更高的图像质量指标。 
**|
|**2024-06-14**|**GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors**|[2406.10111](http://arxiv.org/abs/2406.10111)|null|**从低分辨率输入视图实现高分辨率新视角合成 (HRNVS) 是一项具有挑战性的任务，因为缺乏高分辨率数据。以前的方法从低分辨率输入视图优化高分辨率神经辐射场 (NeRF)，但渲染速度缓慢。在这项工作中，我们基于 3D 高斯样条 (3DGS) 开发了我们的方法，因为它能够以更快的渲染速度生成高质量图像。为了缓解更高分辨率合成的数据短缺问题，我们建议通过使用分数蒸馏采样 (SDS) 将 2D 知识提取到 3D 中来利用现成的 2D 扩散先验。然而，由于生成先验带来的随机性，将 SDS 直接应用于基于高斯的 3D 超分辨率会导致不希望出现且冗余的 3D 高斯图元。为了缓解这个问题，我们引入了两种简单但有效的技术来减少 SDS 引入的随机扰动。具体来说，我们 1) 使用退火策略缩小 SDS 中扩散时间步长的范围；2) 在密集化过程中随机丢弃冗余的高斯图元。大量实验表明，我们提出的 GaussainSR 仅使用合成数据集和真实世界数据集上的低分辨率输入，即可获得 HRNVS 的高质量结果。项目页面：https://chchnii.github.io/GaussianSR/ 
**|
|**2024-06-14**|**GradeADreamer: Enhanced Text-to-3D Generation Using Gaussian Splatting and Multi-View Diffusion**|[2406.09850](http://arxiv.org/abs/2406.09850)|**[link](https://github.com/trapoom555/gradeadreamer)**|**文本到3D生成已经展现出良好的效果，但仍然面临着一些常见的挑战，例如多面 Janus 问题以及高质量资产生成时间过长的问题。在本文中，我们通过引入一种名为 GradeADreamer 的新型三阶段训练流水线来解决这些问题。该流水线能够仅使用单个 RTX 3090 GPU 在 30 分钟内生成高质量的资产。我们提出的方法采用多视图扩散模型 MVDream 生成高斯球体作为先验，然后使用 StableDiffusion 优化几何形状和纹理。实验结果表明，与之前最先进的方法相比，我们的方法显著减轻了多面 Janus 问题，并实现了最高的平均用户偏好排名。项目代码可在 https://github.com/trapoom555/GradeADreamer 获取。 
**|
|**2024-06-14**|**Unified Gaussian Primitives for Scene Representation and Rendering**|[2406.09733](http://arxiv.org/abs/2406.09733)|null|**在计算机图形学中，寻找一种统一的场景表示方法仍然是一个研究挑战。传统的基于网格的表示方法不适用于密集的、模糊的元素，并且为过滤和可微渲染引入了额外的复杂性。相反，基于体素的表示方法难以模拟坚硬的表面，并且存在内存需求大的问题。我们提出了一种基于三维高斯分布的通用渲染基元，用于统一场景表示，其特征是具有从光滑表面到模糊元素的多种外观，以及基于物理的散射，以实现准确的全局照明。我们基于非指数传输制定了基元的渲染理论，并推导出与蒙特卡罗路径追踪兼容的高效渲染操作。新的表示方法可以从不同的来源转换而来，包括网格和三维高斯 splatting，并且由于其可微性，可以通过透射率优化进一步细化。我们展示了我们的表示方法在各种渲染应用中的多功能性，例如全局照明和外观编辑，同时自然地支持任意照明条件。此外，我们将我们的表示方法与现有的体积表示方法进行了比较，突出了其在细节再现方面的效率。 
**|
|**2024-06-13**|**Modeling Ambient Scene Dynamics for Free-view Synthesis**|[2406.09395](http://arxiv.org/abs/2406.09395)|null|**我们引入了一种新颖的方法，用于从单目捕捉中动态合成自由视角的周围场景，为观看体验带来沉浸式的质量。我们的方法建立在3D高斯渲染（3DGS）的最新进展之上，该技术可以忠实地重建复杂的静态场景。先前将3DGS扩展到表示动态的尝试仅限于有界场景或需要多摄像机捕捉，并且通常无法推广到未见过的运动，从而限制了它们的实际应用。我们的方法通过利用环境运动的周期性来学习运动轨迹模型，以及仔细的正则化来克服这些限制。我们还提出了一些重要的实用策略，以提高基线3DGS静态重建的视觉质量，并提高对GPU内存密集型学习至关重要的内存效率。我们展示了具有复杂纹理和精细结构元素的多个周围自然场景的高质量逼真新颖视图合成。 
**|
|**2024-06-13**|**GGHead: Fast and Generalizable 3D Gaussian Heads**|[2406.09377](http://arxiv.org/abs/2406.09377)|null|**Learning 3D head priors from large 2D image collections is an important step towards high-quality 3D-aware human modeling. A core requirement is an efficient architecture that scales well to large-scale datasets and large image resolutions. Unfortunately, existing 3D GANs struggle to scale to generate samples at high resolutions due to their relatively slow train and render speeds, and typically have to rely on 2D superresolution networks at the expense of global 3D consistency. To address these challenges, we propose Generative Gaussian Heads (GGHead), which adopts the recent 3D Gaussian Splatting representation within a 3D GAN framework. To generate a 3D representation, we employ a powerful 2D CNN generator to predict Gaussian attributes in the UV space of a template head mesh. This way, GGHead exploits the regularity of the template's UV layout, substantially facilitating the challenging task of predicting an unstructured set of 3D Gaussians. We further improve the geometric fidelity of the generated 3D representations with a novel total variation loss on rendered UV coordinates. Intuitively, this regularization encourages that neighboring rendered pixels should stem from neighboring Gaussians in the template's UV space. Taken together, our pipeline can efficiently generate 3D heads trained only from single-view 2D image observations. Our proposed framework matches the quality of existing 3D head GANs on FFHQ while being both substantially faster and fully 3D consistent. As a result, we demonstrate real-time generation and rendering of high-quality 3D-consistent heads at $1024^2$ resolution for the first time.**|
|**2024-06-14**|**AV-GS: Learning Material and Geometry Aware Priors for Novel View Acoustic Synthesis**|[2406.08920](http://arxiv.org/abs/2406.08920)|null|**Novel view acoustic synthesis (NVAS) aims to render binaural audio at any target viewpoint, given a mono audio emitted by a sound source at a 3D scene. Existing methods have proposed NeRF-based implicit models to exploit visual cues as a condition for synthesizing binaural audio. However, in addition to low efficiency originating from heavy NeRF rendering, these methods all have a limited ability of characterizing the entire scene environment such as room geometry, material properties, and the spatial relation between the listener and sound source. To address these issues, we propose a novel Audio-Visual Gaussian Splatting (AV-GS) model. To obtain a material-aware and geometry-aware condition for audio synthesis, we learn an explicit point-based scene representation with an audio-guidance parameter on locally initialized Gaussian points, taking into account the space relation from the listener and sound source. To make the visual scene model audio adaptive, we propose a point densification and pruning strategy to optimally distribute the Gaussian points, with the per-point contribution in sound propagation (e.g., more points needed for texture-less wall surfaces as they affect sound path diversion). Extensive experiments validate the superiority of our AV-GS over existing alternatives on the real-world RWAS and simulation-based SoundSpaces datasets.**|
|**2024-06-13**|**Gaussian-Forest: Hierarchical-Hybrid 3D Gaussian Splatting for Compressed Scene Modeling**|[2406.08759](http://arxiv.org/abs/2406.08759)|null|**The field of novel-view synthesis has recently witnessed the emergence of 3D Gaussian Splatting, which represents scenes in a point-based manner and renders through rasterization. This methodology, in contrast to Radiance Fields that rely on ray tracing, demonstrates superior rendering quality and speed. However, the explicit and unstructured nature of 3D Gaussians poses a significant storage challenge, impeding its broader application. To address this challenge, we introduce the Gaussian-Forest modeling framework, which hierarchically represents a scene as a forest of hybrid 3D Gaussians. Each hybrid Gaussian retains its unique explicit attributes while sharing implicit ones with its sibling Gaussians, thus optimizing parameterization with significantly fewer variables. Moreover, adaptive growth and pruning strategies are designed, ensuring detailed representation in complex regions and a notable reduction in the number of required Gaussians. Extensive experiments demonstrate that Gaussian-Forest not only maintains comparable speed and quality but also achieves a compression rate surpassing 10 times, marking a significant advancement in efficient scene modeling. Codes are available at https://github.com/Xian-Bei/GaussianForest.**|
|**2024-06-12**|**ICE-G: Image Conditional Editing of 3D Gaussian Splats**|[2406.08488](http://arxiv.org/abs/2406.08488)|null|**近年来，出现了许多创建高质量3D资产和场景的技术。然而，在编辑这些对象时，现有方法要么速度慢，要么在质量上妥协，要么无法提供足够的定制化。我们引入了一种新方法，可以从单个参考视图快速编辑3D模型。我们的技术首先对编辑图像进行分割，然后使用DINO特征在选定的分割数据集视图中匹配语义上对应的区域。然后，可以以语义合理的方式将编辑图像特定区域的颜色或纹理更改自动应用于其他视图。这些编辑后的视图充当更新后的数据集，以进一步训练和重新样式化3D场景。最终结果是一个经过编辑的3D模型。我们的框架支持各种编辑任务，例如手动局部编辑、基于对应的任何示例图像的风格迁移，以及来自多个示例图像的不同风格的组合。我们使用高斯样条作为我们的主要3D表示，因为它速度快且易于局部编辑，但我们的技术也适用于其他方法，如NeRFs。我们通过多个例子表明，我们的方法可以产生更高质量的结果，同时提供对编辑的细粒度控制。项目页面：ice-gaussian.github.io 
**|
|**2024-06-12**|**Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models**|[2406.08475](http://arxiv.org/abs/2406.08475)|null|**从单张RGB图像创建逼真的人物化身是一个颇具吸引力但极具挑战性的问题。由于其不适定性，近期工作利用在大型数据集上预训练的2D扩散模型的强大先验。尽管2D扩散模型表现出强大的泛化能力，但它们无法提供具有保证3D一致性的多视图形状先验。我们提出了Human 3Diffusion：通过显式3D一致性扩散创建逼真的人物化身。我们的主要见解是，2D多视图扩散和3D重建模型可以相互提供补充信息，通过将它们紧密耦合，我们可以充分利用两种模型的潜力。我们引入了一种新颖的图像条件生成式3D高斯 splat 重建模型，它利用了来自2D多视图扩散模型的先验，并提供了显式的3D表示，这进一步指导了2D反向采样过程以获得更好的3D一致性。实验表明，我们提出的框架优于最先进的方法，并能够从单张RGB图像创建逼真的人物化身，在几何形状和外观方面均实现了高保真度。大量的消融实验也验证了我们设计的有效性，(1) 生成式3D重建中的多视图2D先验条件和 (2) 通过显式3D表示对采样轨迹进行一致性细化。我们的代码和模型将在https://yuxuan-xue.com/human-3diffusion上发布。 
**|

## 3D/CG

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**A Two-Stage Masked Autoencoder Based Network for Indoor Depth Completion**|[2406.09792](http://arxiv.org/abs/2406.09792)|**[link](https://github.com/kailaisun/indoor-depth-completion)**|**深度图像在三维重建、自动驾驶、增强现实、机器人导航和场景理解等领域有着广泛的应用。商用级深度相机难以感知明亮、光滑、透明和远处表面的深度。尽管现有的深度补全方法已经取得了显著进展，但应用于复杂的室内场景时，它们的性能仍然有限。为了解决这些问题，我们提出了一种基于两步 Transformer 的室内深度补全网络。与现有的深度补全方法不同，我们采用基于掩码自动编码器的自监督预训练编码器来学习缺失深度值的有效潜在表示；然后我们提出了一个基于标记融合机制的解码器，以从联合 RGB 和不完整深度图像中完成（即重建）完整深度。与现有方法相比，我们提出的网络在 Matterport3D 数据集上实现了最先进的性能。 此外，为了验证深度补全任务的重要性，我们将我们的方法应用于室内三维重建。代码、数据集和演示可在 https://github.com/kailaisun/Indoor-Depth-Completion 获取。 
**|
|**2024-06-14**|**Grounding Image Matching in 3D with MASt3R**|[2406.09756](http://arxiv.org/abs/2406.09756)|null|**图像匹配是所有高性能三维视觉算法和流程中的核心组成部分。然而，尽管匹配本质上是一个与相机姿态和场景几何密切相关的3D问题，但它通常被视为一个2D问题。这看似合理，因为匹配的目标是在2D像素场之间建立对应关系，但似乎也是一个潜在的危险选择。在这项工作中，我们采取了不同的立场，并建议将匹配视为一项3D任务，并使用DUSt3R，这是一个最近出现的、基于Transformer的强大3D重建框架。基于点图回归，该方法在匹配具有极端视点变化的视图时表现出令人印象深刻的鲁棒性，但精度有限。我们的目标是在保持其鲁棒性的同时，提高这种方法的匹配能力。因此，我们建议在DUSt3R网络中增加一个新的头部，输出密集的局部特征，并使用额外的匹配损失进行训练。我们进一步解决了密集匹配的二次复杂度问题，如果不仔细处理，这对于下游应用来说速度会非常慢。我们引入了一种快速互惠匹配方案，它不仅将匹配速度提高了几个数量级，而且还具有理论上的保证，并且最终产生了更好的结果。大量实验表明，我们提出的MASt3R方法在多个匹配任务上明显优于现有技术。特别是，在极具挑战性的无地图定位数据集上，它的VCRE AUC比已发表的最佳方法提高了30%（绝对改进）。 
**|
|**2024-06-13**|**ImageNet3D: Towards General-Purpose Object-Level 3D Understanding**|[2406.09613](http://arxiv.org/abs/2406.09613)|**[link](https://github.com/wufeim/imagenet3d)**|**A vision model with general-purpose object-level 3D understanding should be capable of inferring both 2D (e.g., class name and bounding box) and 3D information (e.g., 3D location and 3D viewpoint) for arbitrary rigid objects in natural images. This is a challenging task, as it involves inferring 3D information from 2D signals and most importantly, generalizing to rigid objects from unseen categories. However, existing datasets with object-level 3D annotations are often limited by the number of categories or the quality of annotations. Models developed on these datasets become specialists for certain categories or domains, and fail to generalize. In this work, we present ImageNet3D, a large dataset for general-purpose object-level 3D understanding. ImageNet3D augments 200 categories from the ImageNet dataset with 2D bounding box, 3D pose, 3D location annotations, and image captions interleaved with 3D information. With the new annotations available in ImageNet3D, we could (i) analyze the object-level 3D awareness of visual foundation models, and (ii) study and develop general-purpose models that infer both 2D and 3D information for arbitrary rigid objects in natural images, and (iii) integrate unified 3D models with large language models for 3D-related reasoning.. We consider two new tasks, probing of object-level 3D awareness and open vocabulary pose estimation, besides standard classification and pose estimation. Experimental results on ImageNet3D demonstrate the potential of our dataset in building vision models with stronger general-purpose object-level 3D understanding.**|
|**2024-06-13**|**Multiagent Multitraversal Multimodal Self-Driving: Open MARS Dataset**|[2406.09383](http://arxiv.org/abs/2406.09383)|null|**大规模数据集推动了基于人工智能的自动驾驶汽车研究的最新进展。然而，这些数据集通常是从一辆汽车对某个地点的一次性通行中收集的，缺乏多智能体交互或对同一地点的重复遍历。这些信息可能会改变自动驾驶汽车的感知、预测和规划能力。为了弥合这一差距，我们与自动驾驶公司May Mobility合作，推出了MARS数据集，该数据集统一了支持多智能体、多路径和多模态自动驾驶汽车研究的场景。更具体地说，MARS是通过在某个地理区域内行驶的车队收集的。每辆车都有自己的路线，不同的车辆可能会出现在附近的位置。每辆车都配备了激光雷达和环视RGB摄像头。我们在MARS中策划了两个子集：一个子集促进了多辆车同时出现在同一地点的协同驾驶，另一个子集通过多辆车对同一地点的异步遍历实现了记忆回溯。我们进行了位置识别和神经重建实验。更重要的是，MARS带来了新的研究机会和挑战，例如多路径三维重建、多智能体感知和无监督目标发现。我们的数据和代码可以在https://ai4ce.github.io/MARS/找到。**|
|**2024-06-13**|**LRM-Zero: Training Large Reconstruction Models with Synthesized Data**|[2406.09371](http://arxiv.org/abs/2406.09371)|null|**We present LRM-Zero, a Large Reconstruction Model (LRM) trained entirely on synthesized 3D data, achieving high-quality sparse-view 3D reconstruction. The core of LRM-Zero is our procedural 3D dataset, Zeroverse, which is automatically synthesized from simple primitive shapes with random texturing and augmentations (e.g., height fields, boolean differences, and wireframes). Unlike previous 3D datasets (e.g., Objaverse) which are often captured or crafted by humans to approximate real 3D data, Zeroverse completely ignores realistic global semantics but is rich in complex geometric and texture details that are locally similar to or even more intricate than real objects. We demonstrate that our LRM-Zero, trained with our fully synthesized Zeroverse, can achieve high visual quality in the reconstruction of real-world objects, competitive with models trained on Objaverse. We also analyze several critical design choices of Zeroverse that contribute to LRM-Zero's capability and training stability. Our work demonstrates that 3D reconstruction, one of the core tasks in 3D vision, can potentially be addressed without the semantics of real-world objects. The Zeroverse's procedural synthesis code and interactive visualization are available at: https://desaixie.github.io/lrm-zero/.**|
|**2024-06-13**|**OpenMaterial: A Comprehensive Dataset of Complex Materials for 3D Reconstruction**|[2406.08894](http://arxiv.org/abs/2406.08894)|null|**近年来，诸如神经辐射场和隐式神经表示等深度学习技术的进步显著推动了三维重建领域的发展。然而，由于金属和玻璃等具有复杂光学特性的物体具有独特的镜面反射和透光特性，因此对其进行精确重建仍然是一项艰巨的挑战。为了促进针对这些挑战的解决方案的开发，我们引入了 OpenMaterial 数据集，该数据集包含 1001 个由 295 种不同材料（包括导体、电介质、塑料及其粗糙变体）制成的物体，并在 723 种不同的照明条件下进行了捕捉。为此，我们利用基于物理的渲染技术，结合实验室测量的折射率 (IOR)，生成了高度逼真的多视图图像，逼真地再现了真实世界的物体。OpenMaterial 提供了全面的注释，包括三维形状、材料类型、相机姿态、深度和物体掩码。它是第一个能够对具有多样性和挑战性材料的物体进行现有算法定量评估的大规模数据集，从而为开发能够处理复杂材料特性的三维重建算法铺平了道路。**|
|**2024-06-12**|**Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models**|[2406.08475](http://arxiv.org/abs/2406.08475)|null|**从单张RGB图像创建逼真的人物化身是一个极具吸引力但充满挑战性的问题。由于其本身的不适定性，近期工作利用了在大规模数据集上预训练的2D扩散模型的强大先验。尽管2D扩散模型表现出强大的泛化能力，但它们无法提供具有保证3D一致性的多视图形状先验。我们提出了Human 3Diffusion：通过显式3D一致性扩散创建逼真的人物化身。我们的主要见解是，2D多视图扩散和3D重建模型可以为彼此提供补充信息，并且通过将它们紧密耦合，我们可以充分利用两种模型的潜力。我们介绍了一种新颖的图像条件生成式3D Gaussian Splats重建模型，该模型利用了来自2D多视图扩散模型的先验，并提供了显式的3D表示，从而进一步指导2D逆向采样过程以获得更好的3D一致性。实验表明，我们提出的框架优于最先进的方法，并且能够从单张RGB图像创建逼真的人物化身，在几何形状和外观方面都实现了高保真度。广泛的消融研究也验证了我们设计的有效性，(1) 生成式3D重建中的多视图2D先验条件和(2) 通过显式3D表示对采样轨迹进行一致性细化。我们的代码和模型将在https://yuxuan-xue.com/human-3diffusion上发布。 
**|
|**2024-06-12**|**Category-level Neural Field for Reconstruction of Partially Observed Objects in Indoor Environment**|[2406.08176](http://arxiv.org/abs/2406.08176)|null|**神经隐式表示法在 3D 重建领域取得了诸多成功案例，引起了广泛关注。为了进一步应用于场景理解或编辑等领域，一些研究已经展示了在对象组合重建方面的进展。尽管它们在已观察区域表现出色，但在重建部分观察到的对象方面仍然存在局限性。为了更好地解决这个问题，我们引入了类别级神经场，它可以学习场景中属于同一类别的对象之间有意义的共同 3D 信息。我们的关键思想是根据观察到的形状对对象进行子分类，以便更好地训练类别级模型。然后，我们利用神经场来执行具有挑战性的部分观察对象配准任务，方法是选择基于射线的不确定性来选择和对齐代表性对象。在仿真和真实世界数据集上的实验表明，我们的方法改进了多个类别中未观察部分的重建效果。 
**|
|**2024-06-11**|**M-LRM: Multi-view Large Reconstruction Model**|[2406.07648](http://arxiv.org/abs/2406.07648)|null|**尽管大型重建模型（LRM）近年来取得了令人瞩目的进展，但当将其输入从单张图像扩展到多张图像时，它表现出效率低下、几何和纹理质量欠佳以及收敛速度低于预期等问题。这归因于LRM将3D重建视为一个简单的图像到3D的转换问题，而忽略了输入图像之间强大的3D一致性。在本文中，我们提出了一种多视图大型重建模型（M-LRM），旨在以3D感知的方式从多视图有效地重建高质量的3D形状。具体来说，我们引入了一种多视图一致性交叉注意机制，使M-LRM能够准确地从输入图像中查询信息。此外，我们利用输入多视图图像的3D先验信息来初始化三平面token。与LRM相比，我们提出的M-LRM可以生成分辨率为128×128的三平面NeRF，并生成高保真度的3D形状。实验研究表明，与LRM相比，我们的模型实现了显著的性能提升和更快的训练收敛速度。项目页面：https://murphylmf.github.io/M-LRM/ 
**|
|**2024-06-11**|**NeRSP: Neural 3D Reconstruction for Reflective Objects with Sparse Polarized Images**|[2406.07111](http://arxiv.org/abs/2406.07111)|null|**我们提出了NeRSP，一种利用稀疏偏振图像重建反射表面的神经三维重建技术。反射表面的重建极具挑战性，因为镜面反射与视点相关，因此违反了多视图立体的多视图一致性。另一方面，稀疏图像输入作为一种实际的采集设置，由于缺乏对应匹配，通常会导致结果不完整或失真。本文利用偏振图像，共同解决了来自稀疏输入和反射表面的挑战。我们从偏振图像形成模型和多视图方位角一致性中推导出光度和几何线索，它们共同优化了通过隐式神经表示建模的表面几何形状。根据我们对合成数据集和真实数据集的实验，我们仅用 6 个视图作为输入就实现了最先进的表面重建结果。 
**|

## 各类学习方式

| Publish Date | Title | PDF | Code | Abstract |
|:---------|:-----------------------|:------|:------|:-------------------------------------------------|
|**2024-06-14**|**Shelf-Supervised Multi-Modal Pre-Training for 3D Object Detection**|[2406.10115](http://arxiv.org/abs/2406.10115)|null|**最先进的三维物体探测器通常在海量标注数据集上训练。然而，标注三维边界框仍然非常昂贵且耗时，特别是对于激光雷达而言。因此，最近的研究表明，使用未标记数据进行自监督预训练可以提高有限标签情况下的检测精度。目前的方法将图像领域自监督学习的最佳实践应用于点云（例如对比学习）。然而，公开可用的三维数据集比用于基于图像的自监督学习的数据集要小得多，多样性也差得多，这限制了其有效性。然而，我们注意到，此类数据通常以多模态方式收集，通常与图像配对。我们认为，与其仅使用自监督目标进行预训练，不如使用在互联网规模图像数据上训练的基于图像的基础模型来引导点云表示。具体来说，我们提出了一种“现成监督”方法（例如，使用现成的图像基础模型进行监督），用于从配对的 RGB 和激光雷达数据中生成零样本三维边界框。使用此类伪标签预训练三维探测器比先前的自监督预训练任务能显著提高半监督检测精度。重要的是，我们证明了基于图像的“现成监督”有助于训练仅使用激光雷达和多模态（RGB + 激光雷达）的探测器。我们证明了我们的方法在 nuScenes 和 WOD 上的有效性，在有限数据设置下显著优于先前的工作。 
**|
|**2024-06-14**|**InstructRL4Pix: Training Diffusion for Image Editing by Reinforcement Learning**|[2406.09973](http://arxiv.org/abs/2406.09973)|null|**基于指令的图像编辑在使用自然人类语言操纵图像的视觉内容方面取得了巨大进展。然而，现有模型受到数据集质量的限制，无法准确定位具有复杂对象关系的图像中的编辑区域。在本文中，我们提出了强化学习引导的图像编辑方法 (InstructRL4Pix)，以训练扩散模型生成由目标对象的注意力图引导的图像。我们的方法通过计算注意力图之间的距离作为奖励函数，并使用近端策略优化 (PPO) 对扩散模型进行微调，从而最大化奖励模型的输出。我们在对象插入、移除、替换和变换方面评估了我们的模型。实验结果表明，InstructRL4Pix突破了传统数据集的局限性，利用无监督学习优化编辑目标，并根据自然的人类指令实现精准的图像编辑。 
**|
|**2024-06-14**|**Neural Concept Binder**|[2406.09949](http://arxiv.org/abs/2406.09949)|**[link](https://github.com/ml-research/neuralconceptbinder)**|**The challenge in object-based visual reasoning lies in generating descriptive yet distinct concept representations. Moreover, doing this in an unsupervised fashion requires human users to understand a model's learned concepts and potentially revise false concepts. In addressing this challenge, we introduce the Neural Concept Binder, a new framework for deriving discrete concept representations resulting in what we term "concept-slot encodings". These encodings leverage both "soft binding" via object-centric block-slot encodings and "hard binding" via retrieval-based inference. The Neural Concept Binder facilitates straightforward concept inspection and direct integration of external knowledge, such as human input or insights from other AI models like GPT-4. Additionally, we demonstrate that incorporating the hard binding mechanism does not compromise performance; instead, it enables seamless integration into both neural and symbolic modules for intricate reasoning tasks, as evidenced by evaluations on our newly introduced CLEVR-Sudoku dataset.**|
|**2024-06-14**|**Forgetting Order of Continual Learning: Examples That are Learned First are Forgotten Last**|[2406.09935](http://arxiv.org/abs/2406.09935)|null|**Catastrophic forgetting poses a significant challenge in continual learning, where models often forget previous tasks when trained on new data. Our empirical analysis reveals a strong correlation between catastrophic forgetting and the learning speed of examples: examples learned early are rarely forgotten, while those learned later are more susceptible to forgetting. We demonstrate that replay-based continual learning methods can leverage this phenomenon by focusing on mid-learned examples for rehearsal. We introduce Goldilocks, a novel replay buffer sampling method that filters out examples learned too quickly or too slowly, keeping those learned at an intermediate speed. Goldilocks improves existing continual learning algorithms, leading to state-of-the-art performance across several image classification tasks.**|
|**2024-06-14**|**Exploring the Benefits of Vision Foundation Models for Unsupervised Domain Adaptation**|[2406.09896](http://arxiv.org/abs/2406.09896)|**[link](https://github.com/tue-mps/vfm-uda)**|**在计算机视觉领域，实现跨不同数据域的稳健泛化仍然是一项重大挑战。这一挑战在安全关键型应用中尤为重要，因为在这些应用中，基于深度神经网络的系统必须在训练期间未见的各种环境条件下可靠地执行任务。我们的研究调查了视觉基础模型（VFM）和无监督域适应（UDA）方法在语义分割任务中的泛化能力是否互补。结果表明，将VFM与UDA相结合有两个主要好处：（a）它可以在保持VFM的分布外性能的同时，实现更好的UDA性能，以及（b）它使某些耗时的UDA组件变得冗余，从而显著提高推理速度。具体来说，在模型大小相同的情况下，由此产生的VFM-UDA方法比之前的非VFM最佳方法实现了8.4倍的速度提升，同时在UDA设置中将性能提高了+1.2 mIoU，在分布外泛化方面提高了+6.1 mIoU。此外，当我们使用参数多3.6倍的VFM时，VFM-UDA方法仍然保持了3.3倍的速度提升，同时将UDA性能提高了+3.1 mIoU，将分布外性能提高了+10.3 mIoU。这些结果强调了将VFM与UDA相结合的显著优势，为语义分割中的无监督域适应设定了新的标准和基线。 
**|
|**2024-06-14**|**Federated Learning driven Large Language Models for Swarm Intelligence: A Survey**|[2406.09831](http://arxiv.org/abs/2406.09831)|null|**联邦学习 (FL) 为训练大型语言模型 (LLM) 提供了一个引人注目的框架，同时解决了数据隐私和去中心化挑战。本文综述了大型语言模型联邦学习的最新进展，特别关注机器遗忘，这是遵守“被遗忘权”等隐私法规的关键方面。在联邦 LLM 的背景下，机器遗忘涉及系统地、安全地从学习的模型中删除个人数据贡献，而无需从头开始重新训练。我们探索了各种能够实现有效遗忘的策略，例如扰动技术、模型分解和增量学习，并强调了它们对维护模型性能和数据隐私的影响。此外，我们还 بررسی了文献中的案例研究和实验结果，以评估这些方法在现实场景中的有效性和效率。我们的调查表明，人们对开发更健壮和可扩展的联邦遗忘方法越来越感兴趣，这表明人工智能伦理与分布式机器学习技术交叉领域是未来研究的重要方向。 
**|
|**2024-06-14**|**GeoSEE: Regional Socio-Economic Estimation With a Large Language Model**|[2406.09799](http://arxiv.org/abs/2406.09799)|null|**超越传统的调查方式，将异构数据源与人工智能驱动的推理模型相结合，为衡量大范围地理区域的社会经济状况（如贫困和人口）带来了新的机遇。本研究提出了一种名为 GeoSEE 的方法，该方法可以利用由大型语言模型 (LLM) 驱动的统一管道来估算各种社会经济指标。GeoSEE 接收一组不同的信息模块（包括从卫星图像预先构建的模块），并针对每个指标和国家/地区选择要使用的模块进行估算。这种选择由 LLM 的先验社会地理知识引导，其功能类似于领域专家的见解。然后，系统在以自然语言文本格式聚合来自所选模块的结果后，通过上下文学习计算目标指标。对处于不同发展阶段的国家/地区进行的全面评估表明，我们的方法在无监督和少样本情况下均优于其他预测模型。这种方法在数据稀缺的发展中国家或欠发达国家的可靠性能，以及其成本效益，凸显了其在全球范围内持续支持和监测可持续发展目标（如减贫和公平增长）进展的潜力。 
**|
|**2024-06-14**|**Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity**|[2406.09790](http://arxiv.org/abs/2406.09790)|null|**语义文本相似度（STS）是计算语言学中的一个关键研究方向，是衡量嵌入模型编码能力的关键指标。在预训练语言模型和对比学习技术的推动下，领先的句子表示方法在 SentEval 的七个 STS 基准测试中已经可以达到约 86 的平均 Spearman 相关系数得分。然而，进一步的改进变得越来越微不足道，现有的方法在这些任务上的平均得分都没有超过 87。本文深入分析了这一现象，得出结论：使用对比学习的 Spearman 相关系数得分的上限为 87.5。为了突破这一上限，我们提出了一种名为 Pcc-tuning 的创新方法，该方法采用 Pearson 相关系数作为损失函数，以改进对比学习之外的模型性能。实验结果表明，Pcc-tuning 明显优于先前最先进的策略，将 Spearman 相关系数得分提高到 90 以上。 
**|
|**2024-06-14**|**Unsupervised Monocular Depth Estimation Based on Hierarchical Feature-Guided Diffusion**|[2406.09782](http://arxiv.org/abs/2406.09782)|null|**Unsupervised monocular depth estimation has received widespread attention because of its capability to train without ground truth. In real-world scenarios, the images may be blurry or noisy due to the influence of weather conditions and inherent limitations of the camera. Therefore, it is particularly important to develop a robust depth estimation model. Benefiting from the training strategies of generative networks, generative-based methods often exhibit enhanced robustness. In light of this, we employ a well-converging diffusion model among generative networks for unsupervised monocular depth estimation. Additionally, we propose a hierarchical feature-guided denoising module. This model significantly enriches the model's capacity for learning and interpreting depth distribution by fully leveraging image features to guide the denoising process. Furthermore, we explore the implicit depth within reprojection and design an implicit depth consistency loss. This loss function serves to enhance the performance of the model and ensure the scale consistency of depth within a video sequence. We conduct experiments on the KITTI, Make3D, and our self-collected SIMIT datasets. The results indicate that our approach stands out among generative-based models, while also showcasing remarkable robustness.**|
|**2024-06-14**|**Fine-Grained Urban Flow Inference with Multi-scale Representation Learning**|[2406.09710](http://arxiv.org/abs/2406.09710)|null|**Fine-grained urban flow inference (FUFI) is a crucial transportation service aimed at improving traffic efficiency and safety. FUFI can infer fine-grained urban traffic flows based solely on observed coarse-grained data. However, most of existing methods focus on the influence of single-scale static geographic information on FUFI, neglecting the interactions and dynamic information between different-scale regions within the city. Different-scale geographical features can capture redundant information from the same spatial areas. In order to effectively learn multi-scale information across time and space, we propose an effective fine-grained urban flow inference model called UrbanMSR, which uses self-supervised contrastive learning to obtain dynamic multi-scale representations of neighborhood-level and city-level geographic information, and fuses multi-scale representations to improve fine-grained accuracy. The fusion of multi-scale representations enhances fine-grained. We validate the performance through extensive experiments on three real-world datasets. The resutls compared with state-of-the-art methods demonstrate the superiority of the proposed model.**|

