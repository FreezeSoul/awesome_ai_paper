---
layout: default
---

## Updated on 2024.06.18
> Usage instructions: [here](./docs/README.md#usage)

## 多模态

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-06-17**|[LLaNA: Large Language and NeRF Assistant](http://arxiv.org/abs/2406.11840)|null|多模态大型语言模型 (MLLM) 在理解图像和 3D 数据方面表现出色。然而，这两种模态在整体捕捉物体的外观和几何形状方面都存在缺陷。同时，神经辐射场 (NeRF) 已成为一种日益普遍的模态，它将信息编码在简单多层感知器 (MLP) 的权重中，可以同时编码物体的几何形状和逼真的外观。本文研究了将 NeRF 纳入 MLLM 的可行性和有效性。我们创建了 LLaNA，这是第一个能够执行 NeRF 描述和问答等新任务的通用 NeRF 语言助手。值得注意的是，我们的方法直接处理 NeRF 的 MLP 权重以提取有关所表示对象的信息，而无需渲染图像或实例化 3D 数据结构。此外，我们构建了一个包含 NeRF 数据集以及用于各种 NeRF 语言任务的文本注释，无需人工干预。基于此数据集，我们开发了一个基准来评估我们方法的 NeRF 理解能力。结果表明，处理 NeRF 权重比从 NeRF 中提取 2D 或 3D 表示表现更好。|
|**2024-06-17**|[MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs](http://arxiv.org/abs/2406.11833)|**[link](https://github.com/liuziyu77/mmdu)**|生成自然且有意义的响应以与多模态人类输入进行交流是大视觉语言模型 (LVLM) 的一项基本能力。虽然当前的开源 LVLM 在简化场景（例如单轮单图像输入）中表现出良好的性能，但它们在现实世界的对话场景中却表现不佳，例如在具有多轮和多图像的长上下文历史中遵循指令。现有的 LVLM 基准主要集中在单项选择题或简短回答上，这些并不能充分评估 LVLM 在现实世界人机交互应用中的能力。因此，我们引入了 MMDU，一个综合基准测试，以及 MMDU-45k，一个大规模指令微调数据集，旨在评估和改进 LVLM 在多轮和多图像对话中的能力。我们采用聚类算法从开源维基百科中找到相关的图像和文本描述，并在 GPT-4o 模型的帮助下由人工标注者构建问答对。MMDU 最多包含 18k 个图像+文本标记、20 张图像和 27 轮对话，这比以前的基准测试至少长 5 倍，对当前的 LVLM 构成了挑战。我们使用 MMDU 对 15 个代表性 LVLM 进行的深入分析表明，由于对话指令微调数据有限，开源 LVLM 落后于闭源 LVLM。我们证明，在 MMDU-45k 上对开源 LVLM 进行微调可以显著缩小这一差距，生成更长、更准确的对话，并提高 MMDU 和现有基准测试的得分（MMStar：+1.1%，MathVista：+1.5%，ChartQA：+1.2%）。我们的贡献为弥合当前 LVLM 模型与现实应用需求之间的差距铺平了道路。该项目可在 https://github.com/Liuziyu77/MMDU 获取。|
|**2024-06-17**|[Unveiling Encoder-Free Vision-Language Models](http://arxiv.org/abs/2406.11832)|**[link](https://github.com/baaivision/eve)**|现有的视觉语言模型 (VLM) 主要依赖视觉编码器提取视觉特征，然后利用大型语言模型 (LLM) 执行视觉语言任务。然而，视觉编码器在抽象视觉表示（例如分辨率、纵横比和语义先验）方面设置了强烈的归纳偏差，这可能会阻碍 VLM 的灵活性和效率。训练接受无缝视觉和语言输入（即没有视觉编码器）的纯 VLM 仍然具有挑战性，并且很少被探索。实证观察表明，没有编码器的直接训练会导致收敛缓慢和巨大的性能差距。在这项工作中，我们弥合了基于编码器和无编码器模型之间的差距，并提出了一个简单而有效的训练方法来实现纯 VLM。具体来说，我们通过彻底的实验揭示了有效训练无编码器 VLM 的关键方面：(1) 在一个统一的解码器内桥接视觉语言表示；(2) 通过额外监督增强视觉识别能力。借助这些策略，我们推出了 EVE，这是一种可以高效训练和推理的无编码器视觉语言模型。值得注意的是，仅利用 35M 公开可访问的数据，EVE 就可以在多个视觉语言基准测试中与具有类似能力的基于编码器的 VLM 相媲美。它明显优于具有神秘训练程序和未公开训练数据的对应模型 Fuyu-8B。我们相信 EVE 为开发跨模态的纯解码器架构提供了一条透明且有效的途径。我们的代码和模型可在以下网址公开获取：https://github.com/baaivision/EVE。|
|**2024-06-17**|[On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning](http://arxiv.org/abs/2406.11823)|**[link](https://github.com/naver-ai/elva)**|近年来，语言和视觉助手的进步展现出令人印象深刻的能力，但缺乏透明度，限制了更广泛的研究和可重复性。虽然开源模型可以有效地处理一般的图像任务，但它们在处理复杂的视觉情境文本理解时面临着计算需求高的挑战。此类任务通常需要增加标记输入和大型视觉模块来利用高分辨率信息。如何在模型大小和数据重要性之间取得平衡仍然是一个悬而未决的问题。本研究旨在通过识别关键组件并创建具有受限推理成本的高效模型来重新定义视觉语言模型的设计。通过策略性地制定数据集、优化视觉模块和增强监督技术，我们在保持高性能的同时，显著提高了推理吞吐量。从 1.6 亿到 130 亿参数不等的模型的广泛实验为模型优化提供了见解。我们将在 https://github.com/naver-ai/elva 上完全开源我们的代码库、模型和数据集。|
|**2024-06-17**|[VideoLLM-online: Online Video Large Language Model for Streaming Video](http://arxiv.org/abs/2406.11816)|null|最近的大型语言模型增强了视觉能力，使其能够理解图像、视频和交错的视觉语言内容。然而，这些大型多模态模型的学习方法通常将视频视为预先确定的片段，这使得它们在处理流式视频输入方面效率较低。在本文中，我们提出了一种新颖的视频流学习 (LIVE) 框架，该框架支持在连续视频流中进行时间对齐、长上下文和实时对话。我们的 LIVE 框架包含实现视频流对话的综合方法，包括：（1）旨在对连续流输入执行语言建模的训练目标，（2）将离线时间注释转换为流对话格式的数据生成方案，以及（3）优化的推理管道，以加速模型在现实世界视频流中的响应速度。借助我们的 LIVE 框架，我们在 Llama-2/Llama-3 的基础上构建了 VideoLLM-online 模型，并展示了其在处理流式视频方面的显著优势。例如，平均而言，我们的模型可以在 A100 GPU 上以超过 10 FPS 的速度支持 5 分钟视频片段中的流式对话。此外，它还在公共离线视频基准测试中展示了最先进的性能，例如识别、字幕和预测。代码、模型、数据和演示已在 https://showlab.github.io/videollm-online 上提供。|
|**2024-06-17**|[LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning](http://arxiv.org/abs/2406.11815)|null|近年来，指令微调的大型多模态模型 (LMM) 在图像字幕和视觉问答等多项任务中取得了成功；然而，如何将这些模型应用于机器人领域仍然是一个开放性问题。以往用于机器人应用的 LMM 已经接受了大量语言和动作数据的训练，但它们在不同环境下的泛化能力往往不尽如人意。为了解决这个问题，我们引入了 LLARVA，这是一种使用新型指令微调方法训练的模型，该方法利用结构化提示来统一各种机器人学习任务、场景和环境。此外，我们还表明，预测中间二维表示（我们将其称为“视觉轨迹”）有助于进一步对齐机器人学习的视觉和动作空间。我们从 Open X-Embodiment 数据集中生成了 850 万个图像-视觉轨迹对，用于预训练我们的模型，并在 RLBench 模拟器中的 12 个不同任务以及实体 Franka Emika Panda 7 自由度机器人上进行了评估。我们的实验取得了良好的性能，表明 LLARVA 使用二维和语言表示，与其他几个当代基线模型相比表现出色，并且可以泛化到各种机器人环境和配置中。|
|**2024-06-17**|[See It from My Perspective: Diagnosing the Western Cultural Bias of Large Vision-Language Models in Image Understanding](http://arxiv.org/abs/2406.11665)|**[link](https://github.com/amith-ananthram/see-it-from-my-perspective)**|视觉-语言模型 (VLM) 可以用多种语言回答关于图像的查询。然而，除了语言之外，文化也会影响我们看待事物的方式。例如，来自西方文化的人更关注图像中的中心人物，而来自东方文化的人则更关注场景环境。在这项工作中，我们提出了一项新颖的研究，该研究证明并定位了 VLM 在图像理解中的西方偏见。我们使用文化多样性的图像和注释，在主观和客观视觉任务中评估大型 VLM。我们发现，在每个任务中，VLM 在西方子集上的表现优于东方子集。追踪这种偏见来源的对照实验强调了，即使推理是用英语进行的，在纯文本预训练中使用多样化的语言组合对于构建公平的 VLM 至关重要。此外，虽然以目标文化的语言进行提示可以减少偏见，但这并不能替代构建更能代表世界语言的人工智能。|
|**2024-06-17**|[Multimodal Learning To Improve Segmentation With Intraoperative CBCT & Preoperative CT](http://arxiv.org/abs/2406.11650)|null|术中医学影像，特别是锥束计算机断层扫描 (CBCT)，尽管视觉质量较低，但仍是促进计算机辅助介入治疗的重要工具。虽然这种降级的图像质量会影响下游分割，但高质量术前扫描的可用性为改进提供了潜力。在这里，我们考虑一种可以使用术前 CT 和术中 CBCT 扫描的情况，但是扫描之间的对齐（配准）并不完美。我们提出了一种多模态学习方法，融合了粗略对齐的 CBCT 和 CT 扫描，并研究了 CBCT 质量和错位（促进错位的仿射和弹性变换）对最终分割性能的影响。作为一个应用场景，我们专注于肝脏和肝脏肿瘤语义分割，并评估术中图像质量和错位对分割性能的影响。为此，将高质量、标记的 CT 定义为术前数据，并将其用作模拟术中 CBCT 的基础。我们表明，融合术前 CT 和模拟的术中 CBCT 大多可以提高分割性能，并且即使明显错位的术前数据也有可能提高分割性能。|
|**2024-06-17**|[AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic Manipulation](http://arxiv.org/abs/2406.11548)|null|对于机器人系统与现实世界物体进行稳定交互而言，反思和纠正错误的能力至关重要。观察到多模态大语言模型 (MLLM) 的泛化和推理能力，先前的方法旨在利用这些模型来相应地增强机器人系统。然而，这些方法通常侧重于使用额外的 MLLM 进行高级规划校正，而对失败样本的利用有限，无法校正低级接触姿态。为了解决这一差距，我们提出了一种自主交互校正 (AIC) MLLM，它利用先前的低级交互经验来校正 SE(3) 姿态预测。具体来说，AIC MLLM 最初经过微调，以获得姿态预测和反馈提示理解能力。我们通过与物体的交互精心设计了两种类型的提示指令：1) 视觉掩码，用于突出显示不可移动的部分以进行位置校正；2) 文本描述，用于指示旋转校正的潜在方向。在推理过程中，引入了反馈信息提取模块来识别失败原因，从而允许 AIC MLLM 使用相应的提示自适应地校正姿态预测。为了进一步增强操作稳定性，我们设计了一种测试时适应策略，使 AIC MLLM 能够更好地适应当前场景配置。最后，我们在模拟和现实环境中进行了广泛的实验来评估所提出的方法。结果表明，我们的 AIC MLLM 可以通过利用交互体验提示有效地纠正失败样本。现实世界的演示可以在 https://sites.google.com/view/aic-mllm 找到。|
|**2024-06-17**|[MedThink: Inducing Medical Large-scale Visual Language Models to Hallucinate Less by Thinking More](http://arxiv.org/abs/2406.11451)|null|当大型视觉语言模型（LVLM）应用于多模态医学生成任务时，它们会遇到严重的模型幻觉问题。这严重损害了模型的生成准确性，使得LVLM难以在现实世界的医疗场景中实施以协助医生进行诊断。增强下游医学生成任务的训练数据是解决模型幻觉问题的有效方法。此外，医学领域训练数据的有限可用性和隐私问题极大地阻碍了模型的准确性和泛化能力。在本文中，我们介绍了一种模仿人类认知过程来构建细粒度指令对的方法，并将思维链（CoT）的概念从推理场景应用于训练场景，从而提出了一种称为MedThink的方法。我们对各种LVLM的实验表明，我们专为医学领域量身定制的新型数据构建方法显着提高了模型在医学图像报告生成任务中的性能，并大大减少了幻觉。这项工作的所有资源将很快发布。|

## 6DOF Object Pose

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-06-06**|[Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking](http://arxiv.org/abs/2406.04316)|null|6D物体姿态估计是计算机视觉中一项至关重要但极具挑战性的任务，其面临的主要问题是大规模数据集的严重缺乏。这种稀缺性阻碍了对模型性能的全面评估，限制了研究进展。此外，可用实例或类别的数量有限也限制了其应用。为了解决这些问题，本文提出了Omni6DPose，这是一个以对象类别多样性、规模大和对象材质多样性为特征的大型数据集。Omni6DPose主要分为三个部分：ROPE（真实6D物体姿态估计数据集），包含33万张图像，涵盖149个类别、581个实例的超过150万个标注；SOPE（模拟6D物体姿态估计数据集），包含47.5万张在混合现实环境中创建的具有深度模拟的图像，涵盖149个类别、4162个实例的超过500万个标注；以及在ROPE和SOPE中使用的经过手动对齐的真实扫描物体。由于存在大量的变化和歧义，Omni6DPose本身就极具挑战性。为了应对这一挑战，我们引入了GenPose++，它是SOTA类别级姿态估计框架的增强版本， incorporates two pivotal improvements: Semantic-aware feature extraction and Clustering-based aggregation。此外，我们还提供了全面的基准分析，以评估先前方法在这个大规模数据集上在6D物体姿态估计和姿态跟踪方面的性能。|
|**2024-06-05**|[Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices](http://arxiv.org/abs/2406.02977)|null|随着机器人和增强现实应用越来越依赖于精确高效的6D物体姿态估计，边缘设备上的实时性能对于更具交互性和响应性的系统变得至关重要。我们提出的稀疏颜色代码网络（SCCN）体现了一种清晰简洁的流程设计，可以有效地满足这一需求。SCCN对RGB图像中的目标物体进行像素级预测，利用基本物体几何特征的稀疏性来加速透视n点（PnP）计算过程。此外，它引入了一种新的基于像素级几何的物体对称表示，可以与初始姿态预测无缝集成，有效地解决了对称物体的歧义性。SCCN在NVIDIA Jetson AGX Xavier上分别在基准LINEMOD数据集和遮挡LINEMOD数据集上实现了每秒19帧（FPS）和6帧的估计速率，同时在这些速率下始终保持较高的估计精度。|
|**2024-05-19**|[Advancing 6-DoF Instrument Pose Estimation in Variable X-Ray Imaging Geometries](http://arxiv.org/abs/2405.11677)|**[link](https://github.com/cviviers/YOLOv5-6D-Pose)**|在微创手术中，对手术器械进行精确的 6 自由度 (6-DoF) 位姿估计可以显著改善治疗策略和最终手术结果。现有的深度学习方法已经取得了准确的结果，但它们需要针对每个对象定制方法，并且需要费力的设置和训练环境，通常需要进行大量的模拟，同时缺乏实时计算能力。我们提出了一种用于 X 射线系统中 6 自由度位姿估计任务的通用数据采集方法，一种新颖的通用 YOLOv5-6D 位姿架构，用于精确快速地进行目标位姿估计，以及一种在单目锥束 X 射线图像采集几何形状考虑下进行手术螺钉位姿估计的完整方法。所提出的 YOLOv5-6D 位姿模型在公共基准测试中取得了具有竞争力的结果，同时在 GPU 上以 42 FPS 的速度运行速度相当快。此外，该方法可以泛化到不同的 X 射线采集几何形状和语义图像复杂度，从而能够在不同的领域进行精确的位姿估计。最后，所提出的方法在脊柱手术期间用于计算机辅助引导的骨螺钉位姿估计。该模型通过 0.1 ADD-S 指标实现了 92.41% 的精度，展示了一种提高手术精度和患者预后的有前景的方法。YOLOv5-6D 的代码可在 https://github.com/cviviers/YOLOv5-6D-Pose 公开获取。|
|**2024-05-18**|[PS6D: Point Cloud Based Symmetry-Aware 6D Object Pose Estimation in Robot Bin-Picking](http://arxiv.org/abs/2405.11257)|null|6D物体姿态估计在许多领域都扮演着至关重要的角色，特别是在工业工件的抓取方面。鉴于存在锈蚀、高反射率和缺乏纹理等挑战，本文介绍了一种基于点云的姿态估计框架（PS6D）。PS6D专注于细长和多对称物体。它通过注意力引导的特征提取模块提取多尺度特征，设计了一种对称感知的旋转损失和中心距离敏感的平移损失，将每个点的姿态回归到实例的质心，然后使用两阶段聚类方法完成实例分割和姿态估计。来自Sil'eane和IPA数据集的物体以及工业实践中的典型工件被用于生成数据和评估算法。与最先进的方法相比，PS6D在F $_{1_{inst}}$ 方面提高了11.5%，在召回率方面提高了14.8%。PS6D的主要部分已经部署到Mech-Mind的软件中，在分拣实验中取得了91.7%的成功率，标志着其在工业姿态估计任务中的应用。|
|**2024-05-31**|[Deep Learning-Based Object Pose Estimation: A Comprehensive Survey](http://arxiv.org/abs/2405.07801)|**[link](https://github.com/cnjianliu/awesome-object-pose-estimation)**|物体姿态估计是计算机视觉中的一个基本问题，在增强现实和机器人技术中有着广泛的应用。在过去的十年中，深度学习模型由于其卓越的准确性和鲁棒性，越来越多地取代了依赖于工程点对特征的传统算法。然而，当代方法仍然存在若干挑战，包括对标记训练数据的依赖、模型紧凑性、在挑战性条件下的鲁棒性以及泛化到未见过的新物体能力。目前缺少一篇综述来讨论该领域的进展、面临的挑战和未来有希望的方向。为了填补这一空白，我们讨论了基于深度学习的物体姿态估计的最新进展，涵盖了该问题的所有三种形式，即实例级、类别级和未见过物体的姿态估计。我们的综述还涵盖了多种输入数据模态、输出姿态的自由度、物体属性和下游任务，为读者提供了对该领域的全面理解。此外，它还讨论了不同领域的训练范式、推理模式、应用领域、评估指标和基准数据集，并报告了当前最先进方法在这些基准数据集上的性能，从而方便读者为其应用选择最合适的方法。最后，该综述指出了关键挑战，回顾了当前的趋势及其优缺点，并指出了未来研究的有希望的方向。我们还会在 https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation 上持续跟踪最新工作。|
|**2024-05-02**|[IntervenGen: Interventional Data Generation for Robust and Data-Efficient Robot Imitation Learning](http://arxiv.org/abs/2405.01472)|null|模仿学习是一种很有前景的机器人控制策略训练范式，但这些策略可能会受到分布偏移的影响，即评估时的条件与训练数据中的条件不同。提高策略对分布偏移鲁棒性的一种流行方法是交互式模仿学习（即DAgger及其变体），其中人类操作员在策略执行过程中提供纠正性干预。然而，收集足够多的干预措施以覆盖策略错误的分布对于人类操作员来说可能是一项繁重的任务。我们提出了 IntervenGen (I-Gen)，这是一种新颖的数据生成系统，它可以根据少量的人工干预，自主生成大量覆盖状态空间的纠正性干预措施。我们将 I-Gen 应用于 4 个模拟环境和 1 个具有物体姿态估计误差的物理环境，结果表明，只需 10 次人工干预，它就可以将策略的鲁棒性提高 39 倍。视频和更多结果可在 https://sites.google.com/view/intervengen2024 上获取。|
|**2024-04-17**|[GeoReF: Geometric Alignment Across Shape Variation for Category-level Object Pose Refinement](http://arxiv.org/abs/2404.11139)|null|物体姿态细化对于稳健的物体姿态估计至关重要。先前的工作在实例级物体姿态细化方面取得了重大进展。然而，由于类别内形状变化较大以及目标物体与形状先验之间的差异，类别级姿态细化是一个更具挑战性的问题。为了应对这些挑战，我们引入了一种用于类别级物体姿态细化的新型架构。我们的方法集成了 HS 层和可学习的仿射变换，旨在增强几何信息的提取和对齐。此外，我们引入了一种跨点云变换机制，可以有效地融合不同的数据源。最后，我们通过结合形状先验信息进行平移和尺寸误差预测来突破模型的极限。我们进行了广泛的实验来证明所提出框架的有效性。通过大量的定量实验，我们证明了在所有指标上都比基线方法有了很大的改进。|
|**2024-04-08**|[Learning a Category-level Object Pose Estimator without Pose Annotations](http://arxiv.org/abs/2404.05626)|null|三维物体姿态估计是一项具有挑战性的任务。以往的工作总是需要数千张带有标注姿态的物体图像来学习三维姿态对应关系，这对于标注来说既费力又耗时。在本文中，我们提出在没有姿态标注的情况下学习类别级三维物体姿态估计器。我们没有使用手动标注的图像，而是利用扩散模型（例如，Zero-1-to-3）生成一组具有受控姿态差异的图像，并建议使用这些图像来学习我们的物体姿态估计器。直接使用原始的扩散模型会导致图像出现姿态噪声和伪影。为了解决这个问题，首先，我们利用从专门设计的对比姿态学习中学习到的图像编码器来过滤不合理的细节并提取图像特征图。此外，我们提出了一种新的学习策略，允许模型从那些生成的图像集中学习物体姿态，而无需知道其规范姿态的对齐方式。实验结果表明，我们的方法能够从单次拍摄设置（作为姿态定义）中进行类别级物体姿态估计，同时在少样本类别级物体姿态估计基准测试中显著优于其他最先进的方法。|
|**2024-03-28**|[Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation](http://arxiv.org/abs/2403.19527)|**[link](https://github.com/leeiieeo/ag-pose)**|类别级 6D 物体姿态估计旨在估计特定类别中不可见实例的旋转、平移和尺寸。在这个领域，基于密集对应的算法已经取得了领先的性能。然而，它们没有明确地考虑不同实例的局部和全局几何信息，导致对具有显著形状变化的不可见实例的泛化能力较差。为了解决这个问题，我们提出了一种新的用于类别级 6D 物体姿态估计的实例自适应和几何感知关键点学习方法（AG-Pose），它包括两个关键设计：（1）第一个设计是实例自适应关键点检测模块，它可以自适应地检测一组稀疏关键点来表示各种实例的几何结构。（2）第二个设计是几何感知特征聚合模块，它可以有效地将局部和全局几何信息整合到关键点特征中。这两个模块可以协同工作，为不可见实例建立鲁棒的关键点级对应关系，从而增强模型的泛化能力。在 CAMERA25 和 REAL275 数据集上的实验结果表明，所提出的 AG-Pose 在没有类别特定形状先验的情况下，大幅度优于现有最好方法。|
|**2024-06-01**|[Object Pose Estimation via the Aggregation of Diffusion Features](http://arxiv.org/abs/2403.18791)|**[link](https://github.com/tianfu18/diff-feats-pose)**|从图像中估计物体姿态是3D场景理解的一项关键任务，最近的方法在非常大的基准数据集上显示出良好的结果。然而，这些方法在处理未见过物体时性能会显著下降。我们认为这是由于图像特征的泛化能力有限造成的。为了解决这个问题，我们深入分析了扩散模型（如Stable Diffusion）的特征，这些特征在对未见过物体进行建模方面具有巨大潜力。基于这一分析，我们创新性地将这些扩散特征引入到物体姿态估计中。为此，我们提出了三种不同的架构，可以有效地捕捉和聚合不同粒度的扩散特征，大大提高了物体姿态估计的泛化能力。我们的方法在三个流行的基准数据集LM、O-LM和T-LESS上，以相当大的优势超过了最先进的方法。特别是，我们的方法在未见过物体上取得了比之前最佳结果更高的精度：在Unseen LM上为98.2% vs. 93.5%，在Unseen O-LM上为85.9% vs. 76.3%，显示了我们方法强大的泛化能力。我们的代码已发布在https://github.com/Tianfu18/diff-feats-pose。|

## nerf

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-06-17**|[LLaNA: Large Language and NeRF Assistant](http://arxiv.org/abs/2406.11840)|null|多模态大型语言模型 (MLLM) 在理解图像和 3D 数据方面表现出色。然而，这两种模态在全面捕捉对象的appearance和几何形状方面都存在缺陷。同时，神经辐射场 (NeRF) 已成为一种日益普遍的模态，它将信息编码在简单多层感知器 (MLP) 的权重中，可以同时编码对象的几何形状和逼真的外观。本文研究了将 NeRF 引入 MLLM 的可行性和有效性。我们创建了 LLaNA，这是第一个能够执行 NeRF 图像描述和问答等新任务的通用 NeRF 语言助手。值得注意的是，我们的方法直接处理 NeRF 的 MLP 权重来提取有关表示对象的信息，而无需渲染图像或物化 3D 数据结构。此外，我们构建了一个包含 NeRF 数据集及其文本注释的数据集，用于各种 NeRF 语言任务，无需人工干预。基于此数据集，我们开发了一个基准来评估我们方法的 NeRF 理解能力。结果表明，处理 NeRF 权重比从 NeRF 中提取 2D 或 3D 表示更有效。|
|**2024-06-17**|[InterNeRF: Scaling Radiance Fields via Parameter Interpolation](http://arxiv.org/abs/2406.11737)|null|神经辐射场 (NeRF) 在大型真实场景中具有无与伦比的保真度。扩展 NeRF 的一种常见方法是将场景划分为多个区域，每个区域都有自己的参数。如果采用简单实现，这种方法会受到测试时缩放性差以及外观和几何形状不一致的限制。我们提出了一种名为 InterNeRF 的新型架构，用于使用模型参数的子集渲染目标视图。我们的方法支持核外训练和渲染，在仅略微增加训练时间的情况下提高了总模型容量。我们展示了在多房间场景中的显著改进，同时在标准基准测试中保持竞争力。|
|**2024-06-17**|[Projecting Radiance Fields to Mesh Surfaces](http://arxiv.org/abs/2406.11570)|null|辐射场能够生成高保真度且渲染速度快的图像，但难以操纵。我们结合了辐射场和网格表面的优点，有效地实现了不同外观间的虚拟化身纹理迁移。我们使用三维高斯散射将源表示为辐射场，然后将高斯投影到目标网格上。我们的流程包括源预处理、目标矢量化和纹理投影。在纯CPU计算中，投影在1.12秒内完成，而基线技术“逐面纹理投影”和“光线投射”分别需要31秒和4.1分钟。这种方法降低了计算要求，使其适用于从低端移动设备到高端计算机的更广泛设备。|
|**2024-06-16**|[Learning Relighting and Intrinsic Decomposition in Neural Radiance Fields](http://arxiv.org/abs/2406.11077)|null|从神经辐射场中提取反射率和阴影等内在成分的任务越来越受到关注。然而，目前的方法主要集中在合成场景和孤立对象上，忽略了具有背景的真实场景的复杂性。为了解决这一差距，我们的研究引入了一种将重新照明与内在分解相结合的方法。通过利用场景中的光照变化来生成伪标签，我们的方法在不需要真实数据的情况下为内在分解提供指导。我们的方法基于物理约束，确保了跨不同场景类型的鲁棒性，并减少了对预训练模型或手工先验的依赖。我们在合成数据集和真实数据集上验证了我们的方法，取得了令人信服的结果。此外，我们的方法在图像编辑任务中的适用性也展现出可喜的结果。|
|**2024-06-15**|[fNeRF: High Quality Radiance Fields from Practical Cameras](http://arxiv.org/abs/2406.10633)|null|近年来，神经辐射场的开发使得从多视角相机数据中对场景和物体进行前所未有的真实感3D重建成为可能。然而，以往的方法使用过于简化的针孔相机模型，导致散焦模糊被“烘焙”到重建的辐射场中。我们提出了一种对光线投射的改进，利用透镜的光学特性来增强存在散焦模糊时的场景重建。这使我们能够从具有有限孔径的实际相机的测量结果中提高辐射场重建的质量。我们表明，与针孔模型和其他散焦模糊模型的近似相比，所提出的模型更接近地匹配了实际相机的散焦模糊行为，特别是在存在部分遮挡的情况下。这使我们能够实现更清晰的重建，在合成数据集和真实数据集上，将所有聚焦图像的验证PSNR提高了3 dB。|
|**2024-06-15**|[Federated Neural Radiance Field for Distributed Intelligence](http://arxiv.org/abs/2406.10474)|null|新视角合成（NVS）是许多增强现实（AR）和虚拟现实（VR）应用中的重要技术。最近提出的神经辐射场（NeRF）方法在NVS任务中表现出优异的性能，并已应用于其他相关领域。然而，由于严格的法规和隐私问题，某些具有分布式数据存储的应用场景可能会对NeRF方法获取训练图像构成挑战。为了克服这一挑战，我们专注于FedNeRF，这是一种基于联邦学习（FL）的NeRF方法，它利用不同数据所有者可用的图像，同时保护数据隐私。在本文中，我们首先构建了一个资源丰富、功能多样的联邦学习测试平台。然后，我们在这样一个实际的联邦学习系统中部署FedNeRF算法，并在部分客户端选择的情况下进行FedNeRF实验。预计本文提出的FedNeRF方法研究将有助于促进NeRF方法在分布式数据存储场景中的未来应用。|
|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|在非结构化旅游环境中拍摄的照片经常呈现出多变的外观和短暂的遮挡，这对准确的场景重建提出了挑战，并在新视角合成中导致了伪影。尽管先前的方法已经将神经辐射场 (NeRF) 与其他可学习模块相结合来处理动态外观并消除瞬态对象，但其大量的训练需求和缓慢的渲染速度限制了实际部署。最近，三维高斯散射 (3DGS) 已成为 NeRF 的一种很有前途的替代方案，它提供了卓越的训练和推理效率以及更好的渲染质量。本文介绍了 Wild-GS，这是一种针对不受约束的照片集优化的 3DGS 创新改编，同时保留了其效率优势。Wild-GS 通过每个三维高斯固有的材质属性、每张图像的全局照明和相机属性以及逐点反射率的局部方差来确定其外观。与先前在图像空间中对参考特征进行建模的方法不同，Wild-GS 通过对从参考图像中提取的三平面进行采样，将像素外观特征明确地与其对应的局部高斯对齐。这种新颖的设计有效地将参考视图的高频细节外观转移到三维空间，并大大加快了训练过程。此外，二维可见性图和深度正则化分别用于减轻瞬态效应和约束几何形状。大量实验表明，Wild-GS 在所有现有技术中实现了最先进的渲染性能以及最高的训练和推理效率。|
|**2024-06-14**|[GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors](http://arxiv.org/abs/2406.10111)|null|从低分辨率输入视图实现高分辨率新视图合成 (HRNVS) 是一项具有挑战性的任务，因为缺乏高分辨率数据。以前的方法从低分辨率输入视图优化高分辨率神经辐射场 (NeRF)，但渲染速度缓慢。在这项工作中，我们基于 3D 高斯渲染 (3DGS) 开发了我们的方法，因为它能够以更快的渲染速度生成高质量的图像。为了缓解高分辨率合成的数据短缺问题，我们建议利用现成的 2D 扩散先验，通过分数蒸馏采样 (SDS) 将 2D 知识提取到 3D。然而，由于生成先验带来的随机性，将 SDS 直接应用于基于高斯的 3D 超分辨率会导致不希望和冗余的 3D 高斯基元。为了缓解这个问题，我们引入了两种简单而有效的技术来减少 SDS 引入的随机扰动。具体来说，我们 1) 使用退火策略缩小 SDS 中扩散时间步长的范围；2) 在密集化过程中随机丢弃冗余的高斯基元。大量实验表明，我们提出的 GaussainSR 可以在合成和真实世界数据集上仅使用低分辨率输入就能获得高质量的 HRNVS 结果。项目页面：https://chchnii.github.io/GaussianSR/|
|**2024-06-14**|[RaNeuS: Ray-adaptive Neural Surface Reconstruction](http://arxiv.org/abs/2406.09801)|**[link](https://github.com/wangyida/ra-neus)**|我们的目标是利用可微分的辐射场（例如 NeRF）来重建详细的 3D 表面，以及生成标准的新颖视图渲染。已经有相关的方法来执行此类任务，通常是利用带符号距离场 (SDF)。然而，最先进的方法仍然无法正确重建小尺度细节，例如树叶、绳索和纺织品表面。考虑到不同的方法通过全局常数 Eikonal 正则化来制定和优化从 SDF 到辐射场的投影，我们通过逐射线加权因子进行改进，以优先考虑渲染和零交叉表面拟合，并在建立完美 SDF 的基础上进行。我们建议自适应地调整带符号距离场上的正则化，以便不令人满意的渲染光线不会强制执行无效的强 Eikonal 正则化，并允许来自具有良好学习辐射的区域的梯度有效地反向传播到 SDF。因此，平衡这两个目标以生成准确和详细的表面。此外，关于 SDF 中的零交叉表面和辐射场中的渲染点之间是否存在几何偏差，投影也会根据优化期间不同的 3D 位置进行调整。我们提出的 RaNeuS 在合成数据集和真实数据集上都进行了广泛的评估，在新颖视图合成和几何重建方面均取得了最先进的结果。|
|**2024-06-13**|[Neural NeRF Compression](http://arxiv.org/abs/2406.08943)|null|神经辐射场 (NeRFs) 已成为通过连续体积表示捕获详细 3D 场景的强大工具。最近的 NeRF 利用特征网格来提高渲染质量和速度；然而，这些表示引入了大量的存储开销。本文提出了一种有效压缩基于网格的 NeRF 模型的新方法，解决了存储开销问题。我们的方法基于非线性变换编码范式，采用神经压缩来压缩模型的特征网格。由于缺乏涉及许多独立同分布场景的训练数据，我们为单个场景设计了一种无编码器、端到端优化的轻量级解码器方法。为了利用潜在特征网格的空间不均匀性，我们引入了重要性加权的率失真目标函数和采用掩蔽机制的稀疏熵模型。我们的实验结果表明，我们提出的方法在基于网格的 NeRF 压缩效率和重建质量方面优于现有工作。|

## 分类/检测/识别/分割

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-06-17**|[Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%](http://arxiv.org/abs/2406.11837)|**[link](https://github.com/zh460045050/vqgan-lc)**|在以 VQGAN 为代表的图像量化领域，图像被编码成从预定义大小的码本中提取的离散token。近期的研究进展，特别是 LLAMA 3，表明扩大码本可以显著提高模型性能。然而，VQGAN 及其衍生模型，如 VQGAN-FC（因子化码本）和 VQGAN-EMA，在扩大码本大小和提高码本利用率方面仍然面临挑战。例如，VQGAN-FC 只能学习最大大小为 16,384 的码本，在 ImageNet 上的利用率通常低于 12%。本文提出了一种新的图像量化模型，称为 VQGAN-LC（大型码本），它将码本大小扩展到 100,000，实现了超过 99% 的利用率。与之前优化每个码本条目的方法不同，我们的方法首先使用预训练的视觉编码器提取的 100,000 个特征初始化码本。然后，优化集中于训练一个投影器，使整个码本与 VQGAN-LC 中编码器的特征分布对齐。我们证明了我们的模型在各种任务上的性能优于现有模型，包括图像重建、图像分类、使用 GPT 的自回归图像生成，以及使用基于扩散和基于流的生成模型进行图像创作。代码和模型可在 https://github.com/zh460045050/VQGAN-LC 获取。|
|**2024-06-17**|[V3Det Challenge 2024 on Vast Vocabulary and Open Vocabulary Object Detection: Methods and Results](http://arxiv.org/abs/2406.11739)|null|在现实场景中检测物体是一项复杂的任务，因为它面临着各种挑战，包括物体类别的广泛性以及可能遇到以前未知或未见过的物体。 这些挑战使得开发公共基准和挑战成为必要，以推动物体检测领域的发展。 受之前COCO和LVIS挑战赛成功的启发，我们与第四届开放世界视觉研讨会：开放世界中的视觉感知学习（VPLOW）合作，在2024年美国西雅图举行的CVPR上组织了V3Det挑战赛2024。这项挑战赛旨在推动物体检测研究的边界，并鼓励该领域的创新。V3Det挑战赛2024包括两个赛道：1）海量词汇物体检测：该赛道侧重于从13204个类别的庞大集合中检测物体，测试检测算法识别和定位不同物体能力。 2）开放词汇物体检测：该赛道更进一步，要求算法从开放的类别集中检测物体，包括未知物体。 在接下来的部分中，我们将对参与者提交的解决方案进行全面总结和分析。 通过分析提出的方法和解决方案，我们旨在启发海量词汇和开放词汇物体检测的未来研究方向，推动该领域的进步。 挑战赛主页：https://v3det.openxlab.org.cn/challenge|
|**2024-06-17**|[YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection](http://arxiv.org/abs/2406.11641)|null|目前基于图像的无人机检测方法主要依赖于YOLOv5等通用目标检测算法。虽然这些算法能够有效地在均匀背景下识别无人机，但在复杂、纹理丰富的环境中往往表现不佳。在这种情况下，无人机可以无缝地融入背景，产生伪装效果，从而对检测质量产生负面影响。为了解决这个问题，我们提出了一种名为YOLO-FEDER FusionNet的新型深度学习架构。与传统方法不同，YOLO-FEDER FusionNet将通用目标检测方法与伪装目标检测技术的专业优势相结合，以增强无人机检测能力。对YOLO-FEDER FusionNet的全面评估表明，该模型效率高，并在减少漏检和误报方面都有显著改进。|
|**2024-06-17**|[Cross-domain Open-world Discovery](http://arxiv.org/abs/2406.11422)|null|在许多实际应用中，测试数据通常会出现类别偏移，其特点是出现新的类别，以及由于特征分布与模型训练时的特征分布不同而产生的分布偏移。然而，现有方法要么在开放世界环境中发现新类别，要么假设存在领域偏移但无法发现新类别。在这项工作中，我们考虑了一个跨领域开放世界发现设置，目标是在领域偏移的情况下将样本分配给已见类别并发现未见类别。为了解决这一挑战性问题，我们提出了 CROW，这是一种基于原型的方法，它引入了由基础模型结构良好的表示空间支持的“先聚类后匹配”策略。通过这种方式，CROW 通过将聚类与先前见过的类别进行稳健匹配来发现新类别，然后使用专为跨领域开放世界发现设计的目标函数对表示空间进行微调。在图像分类基准数据集上的大量实验结果表明，CROW 优于其他基线方法，在 75 个实验设置中平均性能提高了 8%。|
|**2024-06-17**|[A Dictionary Based Approach for Removing Out-of-Focus Blur](http://arxiv.org/abs/2406.11330)|null|随着深度学习模型的兴起，图像去模糊领域取得了巨大进步。这些模型虽然高效，但计算成本高，能源消耗大。基于字典学习的方法在图像去噪和单图像超分辨率方面已显示出良好的结果。我们提出将 Isidoro、Romano 和 Milanfar 提出的快速准确图像超分辨率 (RAISR) 算法扩展到散焦模糊去除任务。我们定义了一种清晰度质量度量，它与图像的感知质量非常吻合。还提出了一种基于资产配置管理的基于度量的混合策略。与流行的去模糊方法相比，我们的方法平均提高了约 13% (PSNR) 和 10% (SSIM)。此外，我们的混合方案减少了恢复后的振铃伪影。|
|**2024-06-17**|[Low-power Ship Detection in Satellite Images Using Neuromorphic Hardware](http://arxiv.org/abs/2406.11319)|null|将地球观测图像数据从卫星传输到地面站会消耗大量的电力和带宽。对于海上船舶检测，机载数据处理可以识别船舶并减少发送到地面的数据量。然而，大多数在轨捕获的图像只包含水体或陆地，空中客车船舶检测数据集显示只有 22.1% 的图像包含船舶。我们设计了一个低功耗的两阶段系统来优化性能，而不是依赖于单个复杂模型。第一阶段是一个轻量级的二元分类器，充当检测船舶存在的门控机制。此阶段运行在 BrainChip 的 Akida 1.0 上，它利用激活稀疏性来最大程度地减少动态功耗。第二阶段采用 YOLOv5 目标检测模型来识别船舶的位置和大小。这种方法实现了 76.9% 的平均精度 (mAP)，通过减少误报，仅在包含船舶的图像上评估时，该精度提高到 79.3%。此外，我们计算出在 NVIDIA Jetson Nano 设备上评估完整验证集需要 111.4 kJ 的能量。我们的两阶段系统将此能耗降低到 27.3 kJ，不到四分之一，证明了异构计算系统的效率。|
|**2024-06-17**|[Semi-Supervised Domain Adaptation Using Target-Oriented Domain Augmentation for 3D Object Detection](http://arxiv.org/abs/2406.11313)|**[link](https://github.com/rasd3/toda)**|三维目标检测对于自动驾驶和机器人等应用至关重要。然而，在现实环境中，由于传感器升级、天气变化和地理差异导致的传感器数据分布变化会对检测性能产生负面影响。半监督域适应（SSDA）旨在通过将知识从具有丰富标记数据的源域迁移到标记数据稀缺的目标域来应对这些挑战。本文提出了一种新的SSDA方法，称为面向目标的域增强（TODA），该方法专为基于激光雷达的三维目标检测而设计。TODA有效地利用了所有可用数据，包括源域中的标记数据以及目标域中的标记数据和未标记数据，以提高域适应性能。TODA由两个阶段组成：TargetMix和AdvMix。TargetMix采用混合增强技术，结合激光雷达传感器特性，以促进源域和目标域之间的特征对齐。AdvMix将逐点对抗性增强与混合增强相结合，扰动未标记数据，以对齐目标域中标记数据和未标记数据中的特征。我们在具有挑战性的域适应任务上进行的实验表明，TODA的性能明显优于现有的专为三维目标检测设计的域适应技术。代码可在以下网址获取：https://github.com/rasd3/TODA。|
|**2024-06-17**|[Syn-to-Real Unsupervised Domain Adaptation for Indoor 3D Object Detection](http://arxiv.org/abs/2406.11311)|null|在室内3D物体检测中使用合成数据，可以大大减少3D标注所需的人工，并训练出有效的零样本检测器。然而，跨合成到真实室内数据集的复杂域迁移问题仍未得到充分探索。本文提出了一种新的面向对象的层次化域对齐（OHDA）框架，用于室内3D物体检测的合成到真实无监督域自适应。我们的方法包括一种对象感知增强策略，以有效地使源域数据多样化，并且我们引入了一个由对抗训练分支和伪标签分支组成的双分支自适应框架，以便同时实现整体级别和类别级别的域对齐。针对室内无监督域自适应的特点，我们提出了两种方案来进一步优化伪标签。我们从合成数据集3D-FRONT到真实世界数据集ScanNetV2和SUN RGB-D的适应结果表明，与仅使用源数据的基线相比，mAP25分别显著提高了9.7%和9.1%，并且始终优于从2D和3D室外场景改编的方法。代码将在论文被接收后公开。|
|**2024-06-17**|[BaFTA: Backprop-Free Test-Time Adaptation For Zero-Shot Vision-Language Models](http://arxiv.org/abs/2406.11309)|null|像CLIP这样的大规模预训练视觉语言模型在不同领域展现出卓越的零样本图像分类能力。为了在保持零样本范式的情况下提高CLIP的性能，各种测试时提示调优方法被引入，通过无监督学习目标在推理过程中优化类别嵌入。然而，这些方法在选择合适的学习率以防止在测试时适应过程中缺乏验证数据导致的训练崩溃方面经常遇到挑战。在本研究中，我们提出了一种新的免反向传播算法BaFTA，用于视觉语言模型的测试时适应。我们的方法不是微调文本提示来优化类别嵌入，而是在对齐文本和视觉嵌入的投影嵌入空间内使用在线聚类直接估计类别中心点。我们通过使用Rényi熵评估每个预测的可靠性，动态聚合来自估计和原始类别嵌入以及不同增强视图的预测。通过大量实验，我们证明了BaFTA在有效性和效率方面始终优于最先进的测试时适应方法。|
|**2024-06-17**|[VideoVista: A Versatile Benchmark for Video Understanding and Reasoning](http://arxiv.org/abs/2406.11303)|null|尽管在大型多模态模型（LMM）的快速发展的推动下，视频分析取得了重大突破，但仍然缺乏一个通用的评估基准来全面评估这些模型在视频理解和推理方面的性能。为了解决这个问题，我们提出了 VideoVista，这是一个视频问答基准，它整合了跨越不同内容类别、持续时间和能力的挑战。具体来说，VideoVista 包含从 3,400 个视频中提取的 25,000 个问题，涵盖 14 个类别（例如，Howto、电影和娱乐），持续时间从几秒到超过 10 分钟不等。此外，它还包含 19 种理解任务（例如，异常检测、交互理解）和 8 种推理任务（例如，逻辑推理、因果推理）。为了实现这一点，我们提出了一个自动数据构建框架，利用强大的 GPT-4o 以及先进的分析工具（例如，视频分割、对象分割和跟踪）。我们还利用此框架构建训练数据，以增强与视频相关的 LMM（Video-LMM）的能力。通过对尖端模型进行全面和定量的评估，我们发现：1）Video-LMM 在涉及时间定位、对象跟踪和异常检测的细粒度视频任务中面临困难；2）Video-LMM 的逻辑和关系推理能力较差；3）开源 Video-LMM 的性能明显低于 GPT-4o 和 Gemini-1.5，落后 20 个百分点。这突出了 VideoVista 在推进能够准确理解视频并执行精确推理的 LMM 方面将发挥的关键作用。|

## 模型压缩/优化

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-06-17**|[On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning](http://arxiv.org/abs/2406.11823)|**[link](https://github.com/naver-ai/elva)**|近年来，语言和视觉助手的进步展现出令人印象深刻的能力，但也存在缺乏透明度的问题，限制了更广泛的研究和可重复性。虽然开源模型可以有效地处理一般的图像任务，但它们在处理复杂的视觉定位文本理解时，面临着计算需求高的挑战。此类任务通常需要增加输入的标记数量和更大的视觉模块，以利用高分辨率信息。如何在模型大小和数据重要性之间取得平衡仍然是一个有待解决的问题。本研究旨在通过识别关键组件和创建具有受限推理成本的高效模型来重新定义视觉语言模型的设计。通过策略性地制定数据集、优化视觉模块和增强监督技术，我们在保持高性能的同时，显著提高了推理吞吐量。对参数量从 1.6 亿到 130 亿不等的模型进行的大量实验，为模型优化提供了见解。我们将在 https://github.com/naver-ai/elva 上完全开源我们的代码库、模型和数据集。|
|**2024-06-17**|[NLDF: Neural Light Dynamic Fields for Efficient 3D Talking Head Generation](http://arxiv.org/abs/2406.11259)|null|基于神经辐射场模型的说话头部生成已经展现出良好的视觉效果。然而，由于需要对数百个采样点进行繁重的计算才能合成一个像素，NeRF 缓慢的渲染速度严重限制了其应用。为了解决这个问题，本文提出了一种新颖的神经光动态场模型 (NLDF)，旨在实现高质量的三维说话人脸生成，并显著提高速度。NLDF 基于光段表示光场，并使用深度网络一次性学习整个光束的信息。在学习过程中，应用了知识蒸馏技术，并使用基于 NeRF 的合成结果来指导 NLDF 中光段的正确着色。此外，本文还提出了一种新颖的主动池训练策略，以关注高频运动，特别是说话者的嘴巴和眉毛。该方法有效地表示了三维说话视频生成中的面部光动态，与基于 NeRF 的最先进方法相比，其速度提高了约 30 倍，并具有相当的生成视觉质量。|
|**2024-06-17**|[ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking](http://arxiv.org/abs/2406.11257)|**[link](https://github.com/gaffey/excp)**|大型语言模型 (LLM) 近来在人工智能领域引起了广泛关注。然而，这些模型的训练过程在计算和存储容量方面提出了重大挑战，因此压缩检查点已成为一个迫切问题。在本文中，我们提出了一种新颖的极限检查点压缩 (ExCP) 框架，该框架可显著减少训练检查点所需的存储空间，同时实现几乎无损的性能。我们首先计算相邻检查点的残差，以获得基本但稀疏的信息，从而实现更高的压缩率。为了进一步挖掘检查点中的冗余参数，我们提出了一种权重-动量联合缩减方法，以利用模型优化过程中的另一个重要信息，即动量。具体来说，我们利用模型和优化器的信息来尽可能多地丢弃参数，同时保留关键信息以确保最佳性能。此外，我们利用非均匀量化来进一步压缩检查点的存储空间。我们广泛评估了我们提出的 ExCP 框架，评估范围涵盖了从 4.1 亿到 70 亿参数的多个模型，并展示了在保持强大性能的同时显著减少存储空间。例如，我们对 Pythia-410M 模型实现了大约 70 倍的压缩，最终性能在各种下游任务上与原始模型一样准确。代码将在 https://github.com/Gaffey/ExCP 上提供。|
|**2024-06-17**|[STEVE Series: Step-by-Step Construction of Agent Systems in Minecraft](http://arxiv.org/abs/2406.11247)|null|构建以大型语言模型 (LLM) 为核心的具身智能体系统是一个很有前景的方向。由于在现实世界中部署和训练此类智能体的成本高昂且存在不可控因素，我们决定在 Minecraft 环境中开始我们的探索。我们的 STEVE 系列智能体可以在虚拟环境中完成基本任务，以及更具挑战性的任务，例如导航甚至创造性任务，其效率远远超过以前的最先进方法，达到 2.5 到 7.3 倍。我们首先使用一个原始的大型语言模型，并通过视觉编码器和在我们收集的高质量数据集 STEVE-21K 上训练的动作代码库对其进行增强。随后，我们使用 Critic 和记忆模块对其进行强化，将其转变为一个复杂的系统。最后，我们构建了一个分层的多个智能体系统。我们最近的工作探索了如何通过知识蒸馏来精简智能体系统。未来，我们将探索 STEVE 智能体在现实世界中的更多潜在应用。|
|**2024-06-16**|[Outlier Reduction with Gated Attention for Improved Post-training Quantization in Large Sequence-to-sequence Speech Foundation Models](http://arxiv.org/abs/2406.11022)|null|本文探讨了在 Whisper 语音基础模型系列中，知识蒸馏后训练后量化 (PTQ) 的改进。我们解决了权重和激活张量中异常值的问题，众所周知，这些异常值会影响基于 Transformer 的语言和视觉模型的量化质量。将这一观察结果扩展到 Whisper，我们证明了当基于 Transformer 的模型被训练用于执行自动语音识别时，这些异常值也存在，因此需要针对 PTQ 采取缓解策略。我们表明，最近提出的学生模型注意力块中的门控机制可以减少异常值，从而实现有效的 8 位量化，并且与没有采用门控机制的学生模型相比，可以降低词错误率。|
|**2024-06-16**|[Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions](http://arxiv.org/abs/2406.10861)|null|联邦学习 (FL) 是一种分布式且保护隐私的机器学习范式，它协调多个客户端训练模型，同时将原始数据保存在本地。然而，这种传统的联邦学习带来了一些挑战，包括隐私风险、数据异构性、通信瓶颈和系统异构性问题。为了应对这些挑战，知识蒸馏 (KD) 自 2020 年以来已广泛应用于联邦学习。知识蒸馏是一种经过验证且有效的模型压缩和增强算法。知识蒸馏的核心概念是通过在中间层或输出层交换 logits 来促进模型之间的知识转移。这些特性使知识蒸馏成为解决联邦学习中长期挑战的绝佳方案。迄今为止，很少有综述总结和分析知识蒸馏如何有效应用于联邦学习的当前趋势和方法。本文旨在全面概述基于知识蒸馏的联邦学习，重点关注解决上述挑战。首先，我们概述了基于知识蒸馏的联邦学习，包括其动机、基础知识、分类以及与传统联邦学习的比较以及知识蒸馏应该在哪里执行。我们还在附录中分析了基于知识蒸馏的联邦学习中的关键因素，包括教师、知识、数据和方法。我们讨论了知识蒸馏如何应对联邦学习中的挑战，包括隐私保护、数据异构性、通信效率和个性化。最后，我们讨论了基于知识蒸馏的联邦学习算法面临的挑战和未来的研究方向。我们希望本次调查能够为联邦学习领域的研究人员和从业者提供见解和指导。|
|**2024-06-14**|[Self-Knowledge Distillation for Learning Ambiguity](http://arxiv.org/abs/2406.09719)|null|近期的语言模型在自然语言理解（NLU）任务中展现出卓越的性能。然而，当面对可以多重解读的歧义样本时，它们往往表现不佳，过度自信地预测单一标签而未考虑其正确性。为了解决这个问题，我们提出了一种新颖的自我知识蒸馏方法，通过利用从模型底层蒸馏出的知识，使模型能够更准确地学习标签分布。这种方法还包括一个学习阶段，根据蒸馏出的分布知识，重新校准被判定为极其模糊的训练样本的不必要增强的置信度。我们在不同的 NLU 基准数据集上验证了我们的方法，实验结果证明了其在生成更好的标签分布方面的有效性。特别是，通过重新校准高度模糊样本的置信度的过程，显著缓解了未见样本的预测与其真实标签不匹配时的过度自信问题。这已被证明有助于生成比现有最先进方法更好的分布。此外，与现有方法相比，我们的方法在训练模型方面效率更高，因为它不涉及额外的训练过程来优化标签分布。|
|**2024-06-14**|[Frequency-mix Knowledge Distillation for Fake Speech Detection](http://arxiv.org/abs/2406.09664)|null|在电话场景中，用于对抗语音欺骗攻击的假语音检测 (FSD) 任务极具挑战性。数据增强 (DA) 方法被认为是解决电话场景中 FSD 任务的有效手段，通常分为时域和频域两个阶段。虽然每种方法都有其优势，但都可能导致信息丢失。为了解决这个问题，我们提出了一种新的数据增强方法，即频率混合 (Freqmix)，并引入了 Freqmix 知识蒸馏 (FKD) 来增强模型的信息提取和泛化能力。具体来说，我们使用 Freqmix 增强的数据作为教师模型的输入，而学生模型的输入则经过时域数据增强方法处理。我们使用多级特征蒸馏方法来恢复信息并提高模型的泛化能力。我们的方法在 ASVspoof 2021 LA 数据集上取得了最先进的结果，与基线相比提高了 31%，并在 ASVspoof 2021 DF 数据集上表现出竞争力。|
|**2024-06-13**|[RobustSAM: Segment Anything Robustly on Degraded Images](http://arxiv.org/abs/2406.09627)|null|Segment Anything Model (SAM)作为一种变革性的图像分割方法已经出现，它以其强大的零样本分割能力和灵活的提示系统而备受赞誉。然而，它的性能在处理低质量图像时会受到挑战。为了解决这一局限性，我们提出了Robust Segment Anything Model (RobustSAM)，它在保留SAM的可提示性和零样本泛化能力的同时，增强了其在低质量图像上的性能。我们的方法利用了预训练的SAM模型，仅增加了少量参数和计算需求。RobustSAM的额外参数可以在8个GPU上用30小时内完成优化，这证明了其对于典型研究实验室的可行性和实用性。我们还介绍了Robust-Seg数据集，这是一个包含688K图像-掩码对的集合，这些图像-掩码对具有不同的退化程度，旨在优化我们模型的训练和评估。跨多个分割任务和数据集的大量实验结果证实了RobustSAM的优越性能，特别是在零样本条件下，突出了其在广泛现实应用中的潜力。此外，我们的方法已被证明可以有效提高基于SAM的下游任务（如单图像去雾和去模糊）的性能。|
|**2024-06-13**|[Contextual Distillation Model for Diversified Recommendation](http://arxiv.org/abs/2406.09021)|null|推荐的多样性与准确性在改善用户体验方面同样重要。现有研究，例如行列式点过程（DPP）和最大边缘相关性（MMR），采用贪婪范式来迭代地选择同时优化准确性和多样性的项目。然而，先前的方法通常表现出二次复杂度，将其应用限制在重排序阶段，并且不适用于具有更大候选项目池的其他推荐阶段，例如预排序和排序阶段。在本文中，我们提出了上下文蒸馏模型（CDM），这是一种解决多样化的有效推荐模型，适用于工业推荐流程的所有阶段的部署。具体来说，CDM利用同一用户请求中的候选项目作为上下文来增强结果的多样性。我们提出了一种对比上下文编码器，它采用注意力机制来对正面和负面上下文进行建模。对于CDM的训练，我们将每个目标项目与其上下文嵌入进行比较，并利用知识蒸馏框架来学习MMR算法下每个目标项目的获胜概率，其中教师来自MMR输出。在推理过程中，排名是通过推荐模型得分和学生模型得分的线性组合来执行的，从而确保了多样性和效率。我们在两个工业数据集上执行离线评估，并在短视频平台快手上进行CDM的在线A/B测试。正如指标所示，在推荐质量和多样性方面观察到的显着增强，为CDM的有效性提供了强大的优势。|

## OCR

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-06-17**|[GUICourse: From General Vision Language Models to Versatile GUI Agents](http://arxiv.org/abs/2406.11317)|**[link](https://github.com/yiye3/guicourse)**|利用图形用户界面（GUI）进行人机交互对于访问各种数字工具至关重要。视觉语言模型（VLM）的最新进展凸显了开发多功能代理以帮助人类完成 GUI 导航任务的巨大潜力。然而，当前的 VLM 在基本能力（OCR 和 grounding）和 GUI 知识（GUI 元素的功能和控制方法）方面面临挑战，这阻碍了它们成为实用的 GUI 代理。为了解决这些挑战，我们贡献了 GUICourse，这是一套用于从通用 VLM 训练基于视觉的 GUI 代理的数据集。首先，我们介绍了 GUIEnv 数据集，以增强 VLM 的 OCR 和 grounding 能力。然后，我们介绍了 GUIAct 和 GUIChat 数据集，以丰富它们对 GUI 组件和交互的知识。实验表明，我们的 GUI 代理在常见 GUI 任务上的表现优于其基线 VLM。即使是小型 GUI 代理（具有 31 亿个参数）仍然可以很好地完成单步和多步 GUI 任务。最后，我们通过消融研究分析了该代理训练阶段的不同变体。我们的源代码和数据集已发布在 https://github.com/yiye3/GUICourse。|
|**2024-06-17**|[Unifying Multimodal Retrieval via Document Screenshot Embedding](http://arxiv.org/abs/2406.11251)|null|在现实世界中，文档以不同的格式和多种形式组织。传统的检索管道需要定制的文档解析技术和内容提取模块来准备用于索引的输入。这个过程繁琐且容易出错，并且会丢失信息。为此，我们提出了文档截图嵌入（DSE），这是一种新颖的检索范式，将文档截图视为统一的输入格式，它不需要任何内容提取预处理并保留文档中的所有信息（例如，文本、图像和布局）。DSE 利用大型视觉语言模型将文档截图直接编码为用于检索的密集表示。为了评估我们的方法，我们首先制作了 Wiki-SS 数据集，这是一个包含 130 万个维基百科网页截图的语料库，用于回答自然问题数据集中的问题。在这样的文本密集型文档检索环境中，与其他依赖解析的文本检索方法相比，DSE 显示出具有竞争力的有效性。例如，在 top-1 检索精度方面，DSE 比 BM25 高出 17 个百分点。此外，在幻灯片检索的多模态任务中，DSE 在 nDCG@10 上明显优于 OCR 文本检索方法 15 个百分点以上。这些实验表明，DSE 是一种有效的文档检索范式，适用于各种类型的文档。模型检查点、代码和 Wiki-SS 集合将被发布。|
|**2024-06-14**|[Enhancing Question Answering on Charts Through Effective Pre-training Tasks](http://arxiv.org/abs/2406.10085)|null|为了完全理解文档，仅使用文本信息是不够的，理解视觉线索（例如布局和图表）也很重要。虽然目前最先进的文档理解方法（基于 OCR 和不基于 OCR 的方法）运作良好，但尚未对其能力和局限性进行全面分析。因此，在这项工作中，我们解决了当前 VisualQA 模型应用于图表时存在的局限性。为了调查最先进模型的缺陷，我们以 ChartQA 为案例研究，进行了全面的行为分析。我们的研究结果表明，现有模型在回答与图表结构和视觉上下文以及数字信息相关的问题时表现不佳。为了解决这些问题，我们提出了三个简单的预训练任务，这些任务在结构视觉知识及其对数字问题的理解方面加强了现有模型。我们在三个图表数据集（包括提取性和抽象性问题数据集）上评估了我们预先训练的模型（称为 MatCha-v2），并观察到它比基线模型平均提高了 1.7%。|
|**2024-06-14**|[OSPC: Detecting Harmful Memes with Large Language Model as a Catalyst](http://arxiv.org/abs/2406.09779)|null|作为一种在互联网上快速传播个人观点和立场的媒介，迷因也给社会偏见和歧视的传播带来了重大挑战。本研究提出了一种在新加坡多元文化和多语言环境下检测有害迷因的新方法。我们的方法整合了图像描述、光学字符识别 (OCR) 和大型语言模型 (LLM) 分析，以全面理解和分类有害迷因。该系统利用 BLIP 模型进行图像描述，使用 PP-OCR 和 TrOCR 进行多语言文本识别，并使用 Qwen LLM 进行细致入微的语言理解，能够识别以英语、中文、马来语和泰米尔语创建的迷因中的有害内容。为了提高系统的性能，我们利用 GPT-4V 标记的额外数据对我们的方法进行了微调，旨在将 GPT-4V 对有害迷因的理解能力提炼到我们的系统中。我们的框架在由新加坡人工智能举办的网络安全奖挑战赛的公开排行榜上名列前茅，AUROC 为 0.7749，准确率为 0.7087，远远领先于其他团队。值得注意的是，我们的方法优于以前的基准，FLAVA 的 AUROC 为 0.5695，VisualBERT 的 AUROC 为 0.5561。|
|**2024-06-12**|[M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation](http://arxiv.org/abs/2406.08255)|**[link](https://github.com/amazon-science/m3t-multi-modal-translation-bench)**|文档翻译对神经机器翻译 (NMT) 系统提出了挑战。大多数文档级 NMT 系统依赖于精心整理的句子级平行数据，假设从文档中完美提取文本及其精确的阅读顺序。这些系统也倾向于忽略额外的视觉线索，例如文档布局，认为它们无关紧要。然而，现实世界的文档通常具有复杂的文本布局，这与这些假设相矛盾。从光学字符识别 (OCR) 或启发式规则中提取信息可能会导致错误，并且布局（例如，段落、标题）可能会传达文本中远处部分之间的关系。这种复杂性在广泛使用的 PDF 文档中尤为明显，这些文档以视觉方式呈现信息。本文通过介绍 M3T 来解决这一差距，M3T 是一个专门用于评估半结构化文档的全面翻译任务的 NMT 系统的新型基准数据集。该数据集旨在弥合文档级 NMT 系统中的评估差距，承认现实应用程序中丰富文本布局带来的挑战。|
|**2024-06-10**|[VCR: Visual Caption Restoration](http://arxiv.org/abs/2406.06462)|**[link](https://github.com/tianyu-z/vcr)**|我们引入了视觉字幕恢复 (VCR)，这是一项新颖的视觉语言任务，该任务挑战模型利用图像中像素级别的提示来准确地恢复部分被遮挡的文本。这项任务源于这样一个观察，即嵌入在图像中的文本与常见的视觉元素和自然语言有着本质的区别，因为它需要对视觉、文本和嵌入在图像中的文本进行模态对齐。虽然许多工作已经将嵌入在图像中的文本整合到视觉问答任务中，但这些任务的方法通常依赖于光学字符识别或掩码语言建模，从而将任务简化为主要基于文本的处理。然而，基于文本的处理在VCR中变得无效，因为准确的文本恢复取决于来自所提供图像、上下文和来自被遮挡文本的微小暴露区域的微妙线索的组合信息。我们开发了一个流程，使用图像-字幕对生成VCR任务的合成图像，并可调整字幕的可见性以控制任务难度。利用这个流程，我们使用来自维基百科的带有字幕的图像构建了一个名为VCR-Wiki的VCR数据集，其中包含211万个英文实体和34.6万个中文实体，分为简单和困难两种变体。我们的结果表明，当前的视觉语言模型在VCR任务中明显落后于人类的表现，仅仅在我们的数据集上微调模型并不会带来明显的改进。我们发布了VCR-Wiki和数据构建代码，以促进未来的研究。|
|**2024-06-07**|[Scaling Automatic Extraction of Pseudocode](http://arxiv.org/abs/2406.04635)|null|学术论文中的伪代码提供了一种简洁的方式来表达其中实现的算法。伪代码也可以被认为是一种中间表示，有助于弥合编程语言和自然语言之间的差距。访问大量的伪代码可以带来各种好处，从增强算法理解、促进进一步的算法设计，到支持基于 NLP 或计算机视觉的模型，用于自动代码生成和光学字符识别 (OCR) 等任务。我们通过从 arXiv 论文中提取近 320,000 个伪代码示例，创建了一个大型伪代码集合。这个过程涉及扫描超过 220 万篇学术论文，其中 1,000 篇经过人工检查和标记。鉴于集合固有的异质性，我们的方法包括一个为优化覆盖范围而定制的提取机制，以及一个基于随机抽样的验证机制，以检查其准确性和可靠性。此外，我们还提供了对常见伪代码结构的见解，并辅以聚类和统计分析。值得注意的是，这些分析表明伪代码的使用呈指数级增长，突出了它们日益增长的重要性。|
|**2024-06-06**|[CORU: Comprehensive Post-OCR Parsing and Receipt Understanding Dataset](http://arxiv.org/abs/2406.04493)|**[link](https://github.com/update-for-integrated-business-ai/coru)**|在光学字符识别（OCR）和自然语言处理（NLP）领域，集成多语言功能仍然是一项关键挑战，尤其是在考虑阿拉伯语等复杂文字语言时。本文介绍了综合性OCR后解析和收据理解数据集（CORU），这是一个专门设计用于增强多语言环境（涉及阿拉伯语和英语）下收据的OCR和信息提取的新数据集。CORU包含来自不同零售环境（包括超市和服装店）的20,000多张带注释的收据，以及30,000张用于OCR的带注释图像（用于识别每个检测到的行）和10,000个标注了详细信息提取的项目。这些注释捕获了基本细节，例如商家名称、商品描述、总价、收据编号和日期。它们的结构支持三个主要的计算任务：目标检测、OCR和信息提取。我们在CORU上建立了一系列模型的基线性能，以评估Tesseract OCR等传统方法以及更先进的基于神经网络的方法的有效性。这些基线对于处理现实世界收据中常见的复杂且嘈杂的文档布局以及推进自动多语言文档处理的现状至关重要。我们的数据集可公开访问（https://github.com/Update-For-Integrated-Business-AI/CORU）。|
|**2024-06-03**|[Generalized Jersey Number Recognition Using Multi-task Learning With Orientation-guided Weight Refinement](http://arxiv.org/abs/2406.01033)|null|运动服号码识别 (JNR) 一直是体育分析中一项重要任务。由于图像存在模糊、遮挡、变形和低分辨率等问题，提高识别精度仍然是一项持续的挑战。最近的研究已经使用号码定位和光学字符识别来解决这些问题。一些方法将球员识别方案应用于图像序列，忽略了人体旋转角度对球衣数字识别的影响。通过使用多任务方案来识别每个数字，可以准确预测球衣数字的数量，从而获得更稳健的结果。基于以上考虑，本文提出了一种称为角度-数字细化方案 (ADRS) 的多任务学习方法，该方法结合人体方向角度和数字线索来识别运动服号码。根据我们的实验结果，我们的方法增加了推理信息，显着提高了预测精度。与只能处理单一运动类型的现有技术方法相比，所提出的方法产生了更加多样化和实用的 JNR 应用。将足球、橄榄球、篮球、排球和棒球等多种团队运动纳入我们的数据集中，极大地促进了体育分析中 JNR 的推广。我们的方法在 Top-1 上的准确率达到 64.07%，在 Top-2 上的准确率达到 89.97%，相应的 F1 分数分别为 67.46% 和 90.64%。|
|**2024-05-30**|[Scaling up archival text analysis with the blockmodeling of n-gram networks -- A case study of Bulgaria's representation in the Osservatore Romano (January-May 1877)](http://arxiv.org/abs/2405.20156)|null|本文旨在通过应用网络聚类方法分析 1877 年 1 月至 5 月期间出版的 123 期《罗马观察报》中对保加利亚的报道，从而弥合档案文本分析与网络分析之间的差距。本研究利用光学字符识别和广义同质性块模型构建相关关键词网络。包括“保加利亚”和“俄罗斯”这两个词集在内的网络结构基本相同，并且与“德国”、“英国”和“战争”的网络结构有很大程度的重叠。从结构上看，这两个网络的块模型呈现出清晰的核心-半边缘-边缘结构，反映了报纸报道中各概念之间的关系。该报的词汇选择有效地消解了保加利亚民族复兴运动的合法性，突出了罗马教廷对该报编辑路线的影响。|

## 生成模型

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-06-17**|[Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%](http://arxiv.org/abs/2406.11837)|**[link](https://github.com/zh460045050/vqgan-lc)**|在以 VQGAN 为代表的图像量化领域中，该过程将图像编码为从具有预定义大小的码本中提取的离散token。近期的进展，特别是 LLAMA 3，表明扩大码本可以显著提高模型性能。然而，VQGAN 及其衍生模型，如 VQGAN-FC（因子化码本）和 VQGAN-EMA，在扩大码本大小和提高码本利用率方面仍然面临挑战。例如，VQGAN-FC 被限制学习最大大小为 16,384 的码本，在 ImageNet 上的利用率通常低于 12%。在这项工作中，我们提出了一种名为 VQGAN-LC（大型码本）的新型图像量化模型，它将码本大小扩展到 100,000，实现了超过 99% 的利用率。与之前优化每个码本条目的方法不同，我们的方法首先使用由预训练视觉编码器提取的 100,000 个特征初始化码本。然后，优化集中于训练一个投影器，该投影器使整个码本与 VQGAN-LC 中编码器的特征分布对齐。我们证明了我们的模型在各种任务中优于其 counterparts，包括图像重建、图像分类、使用 GPT 的自回归图像生成，以及使用基于扩散和基于流的生成模型进行图像创建。代码和模型可在 https://github.com/zh460045050/VQGAN-LC 获取。|
|**2024-06-17**|[RetinaGS: Scalable Training for Dense Scene Rendering with Billion-Scale 3D Gaussians](http://arxiv.org/abs/2406.11836)|null|在这项工作中，我们探索了在大规模、高分辨率数据集上训练高参数 3D 高斯样条 (3DGS) 模型的可能性。我们为 3DGS 设计了一种通用的模型并行训练方法，称为 RetinaGS，它使用适当的渲染方程，可以应用于任何场景和任意分布的高斯基元。它使我们能够探索 3DGS 在基元数量和训练分辨率方面的缩放行为，这些行为以前难以探索，并超越了先前最先进的重建质量。当使用我们的方法增加基元数量时，我们观察到视觉质量明显提高的积极趋势。我们还首次尝试在完整 MatrixCity 数据集上训练具有超过 10 亿个基元的 3DGS 模型，该模型获得了良好的视觉质量。|
|**2024-06-17**|[Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models](http://arxiv.org/abs/2406.11831)|null|基于解码器-only transformer的大型语言模型（LLM）在文本理解能力方面已经表现出优于CLIP和T5系列模型的能力。然而，在文本到图像扩散模型中利用当前先进LLM的范式仍有待探索。我们观察到一个不寻常的现象：直接使用大型语言模型作为prompt编码器会显著降低图像生成中的prompt遵循能力。我们确定了造成这个问题背后的两个主要障碍。一个是LLM中下一个token预测训练与扩散模型中对判别性prompt特征的要求之间的不匹配。另一个是由解码器-only架构引入的固有位置偏差。为了解决这个问题，我们提出了一个新的框架来充分利用LLM的能力。通过精心设计的使用指南，我们有效地增强了用于prompt编码的文本表示能力，并消除了其固有的位置偏差。这使我们能够灵活地将最先进的LLM集成到文本到图像生成模型中。此外，我们还提供了一种将多个LLM融合到我们框架中的有效方法。考虑到transformer架构所展现出的出色性能和扩展能力，我们进一步设计了一个基于该框架的LLM-Infused Diffusion Transformer（LI-DiT）。我们进行了广泛的实验，以验证LI-DiT在模型规模和数据规模上的有效性。受益于LLM的固有能力和我们的创新设计，LI-DiT的prompt理解性能轻松超越了最先进的开源模型以及包括Stable Diffusion 3、DALL-E 3和Midjourney V6在内的主流闭源商业模型。功能强大的LI-DiT-10B将在进一步优化和安全检查后发布。|
|**2024-06-17**|[MegaScenes: Scene-Level View Synthesis at Scale](http://arxiv.org/abs/2406.11819)|null|场景级新视角合成（NVS）是许多视觉和图形应用的基础。最近，姿势条件扩散模型通过从二维基础模型中提取三维信息取得了显著进展，但这些方法受到缺乏场景级训练数据的限制。常见的数据库选择要么包含孤立的对象（Objaverse），要么包含具有有限姿势分布的以对象为中心的场景（DTU、CO3D）。在本文中，我们利用互联网照片集创建了一个大型场景级数据库，称为MegaScenes，其中包含来自世界各地的超过10万个运动结构（SfM）重建。互联网照片代表了一种可扩展的数据源，但也带来了光照和瞬态对象等挑战。我们解决了这些问题，进一步创建了一个适合NVS任务的子集。此外，我们分析了最先进的NVS方法的失败案例，并显著提高了生成的一致性。通过大量的实验，我们验证了我们的数据库和方法在生成真实场景中的有效性。有关数据库和代码的详细信息，请参见我们的项目页面：https://megascenes.github.io。|
|**2024-06-17**|[DiffMM: Multi-Modal Diffusion Model for Recommendation](http://arxiv.org/abs/2406.11781)|null|像抖音和YouTube这样的在线多模态分享平台的兴起使得个性化推荐系统能够将多种模态（如视觉、文本和音频）纳入用户表示中。然而，解决这些系统中的数据稀疏性挑战仍然是一个关键问题。为了解决这一局限性，最近的研究引入了自监督学习技术来增强推荐系统。然而，这些方法通常依赖于简单的随机增强或直观的跨视图信息，这可能会引入不相关的噪声，并且无法准确地将多模态上下文与用户-项目交互建模相一致。为了填补这一研究空白，我们提出了一种新的用于推荐的多模态图扩散模型，称为DiffMM。我们的框架将模态感知图扩散模型与跨模态对比学习范式相结合，以改进模态感知用户表示学习。这种集成促进了多模态特征信息与协同关系建模之间更好的对齐。我们的方法利用扩散模型的生成能力自动生成一个感知不同模态的用户-项目图，从而促进在用户-项目交互建模中纳入有用的多模态知识。我们在三个公开数据集上进行了广泛的实验，一致证明了我们的DiffMM相对于各种竞争基线的优越性。有关开源模型实现的详细信息，您可以访问以下网址获取我们提出的框架的源代码：https://github.com/HKUDS/DiffMM。|
|**2024-06-17**|[Transcendence: Generative Models Can Outperform The Experts That Train Them](http://arxiv.org/abs/2406.11741)|null|生成模型的训练目标很简单，即模仿其训练数据所诱导的条件概率分布。因此，当使用人类生成的数据进行训练时，我们不能期望人工智能模型在其原始目标上超越人类。在这项工作中，我们研究了超越现象：即生成模型实现的能力超过生成其数据的专家的能力。我们通过训练一个自回归Transformer从棋谱中学习下棋来证明超越现象，并表明训练后的模型有时可以取得比数据集中所有棋手都好的表现。我们从理论上证明了低温采样能够实现超越，并在实验上对其进行了严格的评估。最后，我们讨论了超越的其他来源，为在更广泛的背景下进一步研究这一现象奠定了基础。|
|**2024-06-17**|[Latent Denoising Diffusion GAN: Faster sampling, Higher image quality](http://arxiv.org/abs/2406.11713)|**[link](https://github.com/thanhluantrinh/lddgan)**|扩散模型正在成为生成高保真度和多样化图像的强大解决方案，在许多情况下甚至优于GAN。然而，其缓慢的推理速度阻碍了其在实时应用中的潜力。为了解决这个问题，DiffusionGAN利用条件GAN大幅减少了去噪步骤并加快了推理速度。其改进版本Wavelet Diffusion通过将数据转换为小波空间进一步加快了这一过程，从而提高了效率。尽管如此，这些模型在速度和图像质量方面仍落后于GAN。为了弥合这些差距，本文介绍了潜在去噪扩散GAN（Latent Denoising Diffusion GAN），它采用预训练的自编码器将图像压缩到紧凑的潜在空间中，从而显著提高推理速度和图像质量。此外，我们提出了一种加权学习策略来增强图像的多样性和质量。在CIFAR-10、CelebA-HQ和LSUN-Church数据集上的实验结果证明，我们的模型在扩散模型中实现了最先进的运行速度。与之前的DiffusionGAN和Wavelet Diffusion相比，我们的模型在所有评估指标上都显示出显著的改进。代码和预训练模型：\url{https://github.com/thanhluantrinh/LDDGAN.git}|
|**2024-06-17**|[Diffusion Generative Modelling for Divide-and-Conquer MCMC](http://arxiv.org/abs/2406.11664)|null|分而治之马尔可夫链蒙特卡洛 (MCMC) 是一种并行化马尔可夫链蒙特卡洛采样的策略，它在数据集的不相交子集上运行独立采样器并合并它们的输出。文献中一个持续的挑战是如何有效地执行这种合并，而不必对后验分布做出分布假设。我们建议使用扩散生成模型来拟合子后验分布的密度近似。这种方法在具有挑战性的合并问题上优于现有方法，同时与现有的密度估计方法相比，其计算成本可以更有效地扩展到高维问题。|
|**2024-06-17**|[An approach to non-equilibrium statistical physics using variational Bayesian inference](http://arxiv.org/abs/2406.11630)|null|我们讨论了一种对由耦合在一起的对象组成的系统进行数学建模的方法，该方法使用生成模型来描述构成此类系统的物体状态（或轨迹）之间的依赖关系。这类系统范围很广，包括开放系统或非平衡系统，与自组织系统尤其相关。由此产生的变分自由能原理 (FEP) 与直接使用随机动力系统相比具有一定的优势，特别是它更易于处理，并根据系统组件之间的耦合特性，对联合系统的演化方式提供了一种简洁的解释。使用 FEP，我们可以将一个物体的动力学建模为一个变分推理过程，因为变分自由能（或惊奇）是其动力学的李雅普诺夫函数。简而言之，我们认为使用生成模型来表示和跟踪子系统之间的关系，可以引导我们得到一种关于交互系统的特定统计理论。反过来，该理论使我们能够构建尊重子系统之间已知关系的嵌套模型。我们指出，一个物理对象符合 FEP 并不一定意味着该对象在字面上执行推理；相反，这是一种有用的解释性虚构，它用自由能梯度上的“隐式”流动代替了对象的“显式”动力学——这种虚构可能被对象本身所接受，也可能不被接受。|
|**2024-06-17**|[GECOBench: A Gender-Controlled Text Dataset and Benchmark for Quantifying Biases in Explanations](http://arxiv.org/abs/2406.11547)|**[link](https://github.com/braindatalab/gecobench)**|大型预训练语言模型已在许多应用中流行起来，并构成自然语言处理 (NLP) 中许多下游任务的重要支柱。应用“可解释人工智能”(XAI) 技术来丰富此类模型的输出，对于确保其质量和阐明其内部工作机制至关重要。然而，大型语言模型是在包含各种偏差（例如性别偏差）的大量数据上训练的，这会影响模型权重以及潜在的行为。目前，尚不清楚这种偏差在何种程度上也会以可能不利的方式影响模型解释。我们创建了一个性别控制文本数据集 GECO，其中其他相同的句子以男性和女性形式出现。这产生了用于性别分类任务的基本事实“世界解释”，从而能够客观地评估 XAI 方法的正确性。我们还提供了 GECOBench，这是一个严格的定量评估框架，对流行的 XAI 方法进行基准测试，将它们应用于经过不同程度微调的预训练语言模型。这使我们能够研究预训练如何在模型解释中引发不必要的偏差，以及微调可以在多大程度上减轻这种解释偏差。我们展示了解释性能与微调层数之间的明确依赖关系，其中观察到 XAI 方法特别受益于微调或嵌入层的完整再训练。值得注意的是，这种关系适用于在同一任务上实现相似分类性能的模型。因此，我们强调了所提出的性别控制数据集和新颖的基准测试方法对于新型 XAI 方法的研发具有实用性。所有代码，包括数据集生成、模型训练、评估和可视化，都可以在以下网址获得：https://github.com/braindatalab/gecobench|

## LLM

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-06-17**|[mDPO: Conditional Preference Optimization for Multimodal Large Language Models](http://arxiv.org/abs/2406.11839)|null|直接偏好优化 (DPO) 已被证明是一种有效的大语言模型 (LLM) 对齐方法。最近的研究尝试将 DPO 应用于多模态场景，但发现难以实现一致的改进。通过对比实验，我们发现了多模态偏好优化中的无条件偏好问题，即模型忽略了图像条件。为了解决这个问题，我们提出了 mDPO，这是一种多模态 DPO 目标，通过同时优化图像偏好来防止过度优先考虑仅语言偏好。此外，我们引入了一个奖励锚点，强制选择响应的奖励为正，从而避免了其可能性降低——这是相对偏好优化固有的问题。在两个不同规模的多模态 LLM 和三个广泛使用的基准测试上的实验表明，mDPO 有效地解决了多模态偏好优化中的无条件偏好问题，并显着提高了模型性能，特别是在减少幻觉方面。|
|**2024-06-17**|[Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models](http://arxiv.org/abs/2406.11831)|null|基于解码器Transformer的大语言模型（LLM）在文本理解能力方面已经展现出优于CLIP和T5系列模型的优势。然而，如何将当前先进的LLM应用于文本到图像的扩散模型仍然是一个有待探索的领域。我们观察到一个不寻常的现象：直接使用大型语言模型作为提示编码器会显著降低图像生成中的提示遵循能力。我们确定了造成这个问题背后的两个主要障碍。一个是LLM中预测下一个词的训练目标与扩散模型中对判别性提示特征的要求之间存在偏差。另一个是由仅解码器架构引入的固有位置偏差。为了解决这个问题，我们提出了一个全新的框架来充分利用LLM的能力。通过精心设计的使用指南，我们有效地增强了用于提示编码的文本表示能力，并消除了其固有的位置偏差。这使我们能够灵活地将最先进的LLM集成到文本到图像的生成模型中。此外，我们还提供了一种将多个LLM融合到我们框架中的有效方法。考虑到Transformer架构所展现出的出色性能和扩展能力，我们进一步设计了一种基于该框架的LLM注入式扩散Transformer（LI-DiT）。我们进行了广泛的实验，以验证LI-DiT在模型规模和数据规模上的有效性。得益于LLM的固有能力和我们的创新设计，LI-DiT的提示理解性能轻松超越了最先进的开源模型以及包括Stable Diffusion 3、DALL-E 3和Midjourney V6在内的主流闭源商业模型。功能强大的LI-DiT-10B将在经过进一步优化和安全检查后发布。|
|**2024-06-17**|[VideoLLM-online: Online Video Large Language Model for Streaming Video](http://arxiv.org/abs/2406.11816)|null|近年来，大型语言模型已经具备了视觉能力，能够理解图像、视频和交错的视觉语言内容。然而，这些大型多模态模型的学习方法通常将视频视为预先确定的片段，这使得它们在处理流式视频输入方面效率较低。在本文中，我们提出了一种新颖的视频流学习（LIVE）框架，该框架支持在连续视频流中进行时间对齐、长上下文和实时的对话。我们的LIVE框架包含实现视频流对话的综合方法，包括：（1）旨在对连续流输入执行语言建模的训练目标，（2）将离线时间注释转换为流对话格式的数据生成方案，以及（3）优化的推理流程，以加快模型在现实世界视频流中的响应速度。借助我们的LIVE框架，我们在Llama-2/Llama-3的基础上构建了VideoLLM-online模型，并展示了其在处理流媒体视频方面的显著优势。例如，平均而言，我们的模型可以在A100 GPU上以超过10 FPS的速度支持5分钟视频片段中的流媒体对话。此外，它还在公共离线视频基准测试中展示了最先进的性能，例如识别、字幕和预测。代码、模型、数据和演示已在https://showlab.github.io/videollm-online上提供。|
|**2024-06-17**|[How Do Large Language Models Acquire Factual Knowledge During Pretraining?](http://arxiv.org/abs/2406.11813)|null|尽管近期观察到大型语言模型 (LLM) 可以存储大量的 factual knowledge，但对于它们如何通过预训练获取 factual knowledge 的机制，人们的理解仍然有限。这项工作通过研究 LLM 在预训练期间如何获取 factual knowledge 来解决这一差距。研究结果揭示了关于预训练期间 factual knowledge 获取动态的几个重要见解。首先，与直觉相反，我们观察到，对更多数据的预训练并没有显著提高模型获取和维护 factual knowledge 的能力。其次，训练步数与 factual knowledge 的记忆遗忘和泛化之间存在幂律关系，并且使用重复训练数据训练的 LLM 表现出更快的遗忘速度。第三，使用更大的批大小训练 LLM 可以增强模型对遗忘的鲁棒性。总的来说，我们的观察表明 LLM 预训练中的 factual knowledge 获取是通过逐步增加在每个步骤中预训练数据中出现的 factual knowledge 的概率来实现的。然而，这种增加会被随后的遗忘所稀释。基于这种解释，我们可以对最近观察到的 LLM 行为提供合理的解释，例如 LLM 在长尾知识上的糟糕表现以及对预训练语料库进行重复数据删除的好处。|
|**2024-06-17**|[DataComp-LM: In search of the next generation of training sets for language models](http://arxiv.org/abs/2406.11794)|null|我们推出了面向语言模型的数据比较测试平台 (DCLM)，这是一个用于控制数据集实验以改进语言模型的测试平台。作为 DCLM 的一部分，我们提供了一个从 Common Crawl 中提取的包含 240 万亿个标记的标准化语料库、基于 OpenLM 框架的有效预训练方案以及一套包含 53 个下游评估的广泛套件。DCLM 基准测试的参与者可以尝试各种数据整理策略，例如去重、过滤和数据混合，模型规模从 4.12 亿到 70 亿个参数不等。作为 DCLM 的基线，我们进行了广泛的实验，发现基于模型的过滤是组装高质量训练集的关键。生成的数据集 DCLM-Baseline 使从头开始训练一个包含 70 亿个参数的语言模型成为可能，该模型在 MMLU 上的 5-shot 准确率达到 64%，训练使用了 2.6 万亿个标记。与之前最先进的开放数据语言模型 MAP-Neo 相比，DCLM-Baseline 在 MMLU 上实现了 6.6 个百分点的改进，而计算量减少了 40%。我们的基线模型在 MMLU 上也与 Mistral-7B-v0.3 和 Llama 3 8B 相当（63% 和 66%），并且在平均 53 个自然语言理解任务上的表现相似，而训练使用的计算量比 Llama 3 8B 少 6.6 倍。我们的结果突出了数据集设计对训练语言模型的重要性，并为进一步研究数据整理提供了一个起点。|
|**2024-06-17**|[CELL your Model: Contrastive Explanation Methods for Large Language Models](http://arxiv.org/abs/2406.11785)|null|黑盒深度神经网络分类模型的出现引发了对其决策进行解释的需求。然而，对于生成式人工智能（如大型语言模型（LLM））来说，没有类别预测需要解释。相反，人们可以询问LLM为何对给定提示输出特定响应。在本文中，我们通过提出据我们所知第一个仅需要黑盒/查询访问的对比解释方法来回答这个问题。我们的解释表明，LLM对给定提示输出回复的原因是，如果稍微修改提示，LLM会给出不同的回复，这些回复要么不太可取，要么与原始回复相矛盾。关键见解是，对比解释只需要对用户有意义的距离函数，而不是特定响应（即类别标签）的实际值表示。我们提供了两种寻找对比解释的算法：i）一种短视算法，虽然在创建对比方面有效，但需要多次模型调用；ii）一种预算算法，这是我们的主要算法贡献，它可以智能地创建符合查询预算的对比，这对于较长的上下文是必要的。我们展示了这些方法在各种自然语言任务上的有效性，例如开放文本生成、自动红队和解释对话降级。|
|**2024-06-17**|[Multi-Layer Ranking with Large Language Models for News Source Recommendation](http://arxiv.org/abs/2406.11745)|null|为了寻找新闻事件的可靠信息来源，我们引入了一项新的专家推荐任务，旨在根据专家先前引用的陈述来识别可信赖的来源。为此，我们构建了一个名为 NewsQuote 的新数据集，该数据集包含从新闻文章集合中提取的 23,571 对引言-发言者对。我们将推荐任务制定为根据专家与给定查询相关的可能性来检索专家。我们还提出了一个采用大型语言模型的多层排名框架，以提高推荐性能。我们的结果表明，采用基于上下文学习的 LLM 排名器和基于多层排名的过滤器可以显着提高推荐系统的预测质量和行为质量。|
|**2024-06-17**|[Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models](http://arxiv.org/abs/2406.11736)|null|大型语言模型 (LLM) 性能优越的主要驱动力之一是用于对齐微调的大量人工标注自然语言数据的可用性。这促使研究人员探索自训练方法，以减少对人工标注的过度依赖。然而，目前自训练的成功主要是在自然语言场景中观察到的，而不是在越来越重要的神经符号场景中。为此，我们提出了一个名为 ENVISIONS 的环境引导神经符号自训练框架。它旨在克服两个主要挑战：(1) 符号数据的稀缺性，以及 (2) LLM 在处理符号语言方面的能力有限。在三个不同领域进行的广泛评估证明了我们方法的有效性。此外，我们还进行了全面分析，以揭示 ENVISIONS 成功的影响因素，从而为该领域的未来研究提供宝贵见解。代码将在 \url{https://github.com/xufangzhi/ENVISIONS} 提供。|
|**2024-06-17**|[Meta Reasoning for Large Language Models](http://arxiv.org/abs/2406.11698)|null|我们介绍了元推理提示 (MRP)，这是一种受人类元推理启发，为大型语言模型 (LLM) 设计的、新颖且高效的系统提示方法。传统的基于上下文学习的推理技术，例如思维树，虽然很有前景，但由于其专业性，缺乏跨不同任务的一致最先进性能。MRP 通过引导 LLM 根据每个任务的特定要求动态选择和应用不同的推理方法来解决这一限制，从而优化性能和计算效率。使用 MRP，LLM 推理分两个阶段运行。最初，LLM 使用任务输入线索和可用方法的目标描述来确定最合适的推理方法。随后，它应用所选方法来完成任务。这种动态策略反映了人类的元推理，使模型能够在广泛的问题领域中脱颖而出。我们通过综合基准评估了 MRP 的有效性。结果表明，MRP 在不同任务中均达到或接近最先进的性能。MRP 代表了使 LLM 能够识别跨问题的认知挑战并利用不同推理方法的优势的重大进步，从而增强了它们高效处理多样化和复杂问题领域的能力。每个 LLM 都应该有一个元推理提示，以充分发挥其潜力，并确保在不断变化的挑战和应用环境中的适应性。|
|**2024-06-17**|[HoLLMwood: Unleashing the Creativity of Large Language Models in Screenwriting via Role Playing](http://arxiv.org/abs/2406.11683)|null|生成式人工智能在计算机视觉领域展现出了前所未有的创造力，但在自然语言处理领域尚未观察到类似现象。特别是，由于文学创作的高度复杂性，大型语言模型（LLM）难以创作出达到人类专家水平的文学作品。在本文中，我们提出了 HoLLMwood，这是一个自动化框架，旨在释放 LLM 的创造力并探索其在剧本创作方面的潜力，而剧本创作是一项要求极高的任务。模仿人类创作过程，我们将 LLM 分配给现实场景中的不同角色。除了将 LLM 视为“编剧”的常见做法外，我们还将 LLM 用作“编辑”，负责向“编剧”提供反馈和修改建议。此外，为了丰富角色和深化情节，我们引入了角色扮演机制，并将 LLM 用作可以相互交流和互动的“演员”。对自动生成剧本的评估表明，HoLLMwood 在连贯性、相关性、趣味性和整体质量方面明显优于强大的基线模型。|

## Transformer

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-06-17**|[Composing Object Relations and Attributes for Image-Text Matching](http://arxiv.org/abs/2406.11820)|null|我们研究了用于图像-文本匹配的视觉语义嵌入问题。大多数现有工作利用定制的交叉注意力机制来执行跨图像和文本两种模态的局部对齐。尽管这种方法比单模态双编码器方法更强大，但计算成本很高。这项工作介绍了一种双编码器图像-文本匹配模型，利用场景图来表示字幕，其中节点表示对象和属性，并通过关系边相互连接。利用图注意力网络，我们的模型有效地编码了对象-属性和对象-对象语义关系，从而形成了一个鲁棒且快速的系统。将字幕表示为场景图，可以利用图神经网络强大的关系归纳偏差来有效地学习对象-属性和对象-对象关系。为了训练模型，我们提出了在整体级别（图像-字幕）和局部级别（图像-对象实体）上对齐图像和字幕的损失函数，我们证明这是模型成功的关键。我们的模型被称为对象关系和属性组合模型，简称CORA。在两个著名的图像-文本检索基准数据集Flickr30K和MSCOCO上的实验结果表明，CORA在召回率方面优于现有的计算成本高昂的交叉注意力方法，同时实现了双编码器的快速计算速度。|
|**2024-06-17**|[AnyMaker: Zero-shot General Object Customization via Decoupled Dual-Level ID Injection](http://arxiv.org/abs/2406.11643)|null|基于文本到图像的对象定制旨在根据文本提示和参考图像生成具有相同身份 (ID) 的目标图像，并已取得重大进展。然而，最近的定制研究主要集中在特定任务上，例如人物定制或虚拟试穿，而忽略了通用的对象定制。为此，我们引入了 AnyMaker，这是一个创新的零样本对象定制框架，能够生成具有高 ID 保真度和灵活文本可编辑性的通用对象。AnyMaker 的功效源于其新颖的通用 ID 提取、双层 ID 注入和 ID 感知解耦。具体来说，通用 ID 提取模块使用一组自监督模型提取足够的 ID 信息，以处理针对通用对象的各种定制任务。然后，为了在不损害生成过程中文本可编辑性的情况下，尽可能多地向扩散 UNet 提供提取的 ID，我们设计了一个全局-局部双层 ID 注入模块，其中全局语义 ID 被注入到文本描述中，而局部 ID 细节则通过新添加的交叉注意力模块直接注入到模型中。此外，我们提出了一个 ID 感知解耦模块，用于从提取的表示中分离与 ID 相关的信息和非 ID 元素，以实现身份和文本描述的高保真生成。为了验证我们的方法并促进通用对象定制的研究，我们创建了第一个大规模通用 ID 数据集，即多类别 ID 一致性 (MC-IDC) 数据集，其中包含 31.5 万个文本图像样本和 1 万个类别。实验表明，AnyMaker 在通用对象定制方面表现出卓越的性能，并在相应任务中优于专门方法。代码和数据集将很快发布。|
|**2024-06-17**|[Simple Yet Efficient: Towards Self-Supervised FG-SBIR with Unified Sample Feature Alignment](http://arxiv.org/abs/2406.11551)|null|细粒度基于草图的图像检索 (FG-SBIR) 旨在最小化嵌入空间中草图和对应图像之间的距离。然而，解决方案日益复杂，主要由于细粒度草图的抽象性，可扩展性受到阻碍。在本文中，我们提出了一种简单而有效的方法来缩小两种模式之间的差距。它主要促进了样本内和样本间统一的互信息共享，而不是将它们视为模态之间的单一特征对齐问题。具体来说，我们的方法包括：(i) 采用双重权重共享网络来优化草图和图像域内的对齐，这也有效地缓解了模型学习饱和问题。(ii) 引入基于对比损失的目标优化函数，以增强模型对样本内和样本间特征进行对齐的能力。(iii) 提出了一种由自注意力和交叉注意力相结合的可学习 TRSM，以促进标记之间的特征表示，进一步增强嵌入空间中的样本对齐。我们的框架在基于 CNN 和 ViT 的骨干网络上取得了优异的结果。大量实验表明，它优于现有方法。我们还介绍了 Cloths-V1，这是第一个专业的服装草图和图像数据集，用于验证我们的方法，并将有利于其他应用。|
|**2024-06-17**|[Learning from Exemplars for Interactive Image Segmentation](http://arxiv.org/abs/2406.11472)|null|交互式图像分割使用户能够以最少的交互与机器进行交互，从而逐步细化目标对象的分割掩码。先前的研究已经证明了通过交互式分割提取单个目标掩码的惊人性能。然而，现有方法忽略了先前交互对象的语义信息，可以进一步探索这些信息以加速对同一类别中多个目标的交互式分割。为此，我们针对同一类别中的单个对象和多个对象引入了新颖的交互式分割框架。具体来说，我们的模型利用 Transformer 主干网络从图像和交互中提取以交互为中心的视觉特征，以获得令人满意的目标掩码作为样本。对于多个对象，我们提出了一个样本信息模块，以增强对目标类别对象之间相似性的学习。为了组合来自不同模块的注意力特征，我们结合了交叉注意力块和特征融合模块。在主流基准数据集上进行的实验表明，与以前的方法相比，我们的模型取得了优越的性能。特别是，我们的模型将用户的劳动量减少了约 15%，需要少点击两次才能达到目标 IoU 85% 和 90%。结果突出了我们的模型作为灵活实用的标注工具的潜力。源代码将在发布后发布。|
|**2024-06-17**|[Analysing the Behaviour of Tree-Based Neural Networks in Regression Tasks](http://arxiv.org/abs/2406.11437)|**[link](https://github.com/petersamoaa/tree_based_nn_error_analysis)**|深度学习领域极大地扩展了源代码分析的边界，特别是通过利用抽象语法树 (AST) 等结构化表示。虽然这些方法已在分类任务中证明了有效性，但它们在回归应用（例如从源代码预测执行时间）中的效果仍未得到充分探索。本文致力于解码基于树的神经网络模型在这种回归挑战中的行为。我们将已建立的模型（基于树的卷积神经网络 (CNN)、Code2Vec 和基于 Transformer 的方法）的应用扩展到通过将源代码解析为 AST 来预测其执行时间。我们的比较分析表明，虽然这些模型是代码表示的基准，但在执行回归任务时表现出局限性。为了解决这些缺陷，我们提出了一种新颖的双 transformer 方法，该方法同时对源代码标记和 AST 表示进行操作，并采用交叉注意力机制来增强两个域之间的可解释性。此外，我们探索了图神经网络 (GNN) 对这个基于树的问题的适应性，从理论上证明了由于 AST 的图形性质而具有的内在兼容性。对现实世界数据集的实证评估表明，我们的双 transformer 模型优于所有其他基于树的神经网络和基于 GNN 的模型。此外，我们提出的双 transformer 在不同的数据集上表现出卓越的适应性和鲁棒性。|
|**2024-06-17**|[DiTTo-TTS: Efficient and Scalable Zero-Shot Text-to-Speech with Diffusion Transformer](http://arxiv.org/abs/2406.11427)|null|大规模扩散模型在图像、视频和音频等多种模态中展现出卓越的生成能力。然而，文本到语音（TTS）系统通常涉及特定领域的建模因素（例如，音素和音素级时长）以确保文本和语音之间精确的时间对齐，这阻碍了扩散模型在 TTS 中的效率和可扩展性。在这项工作中，我们提出了一种高效且可扩展的扩散Transformer（DiT），它利用现成的预训练文本和语音编码器。我们的方法通过交叉注意力机制和语音表示总长度的预测来解决文本语音对齐的挑战。为此，我们增强了 DiT 架构以适应 TTS，并通过将语义指导纳入语音的潜在空间来改进对齐。我们将训练数据集和模型大小分别扩展到 82K 小时和 7.9 亿个参数。我们广泛的实验表明，这种无需特定领域建模的大规模 TTS 扩散模型不仅简化了训练流程，而且在自然度、清晰度和说话人相似度方面，其零样本性能优于或可与最先进的 TTS 模型相媲美。我们的语音样本可在 https://ditto-tts.github.io 获取。|
|**2024-06-16**|[STAR: Scale-wise Text-to-image generation via Auto-Regressive representations](http://arxiv.org/abs/2406.10797)|null|我们提出了STAR，一个采用尺度自回归范式的文本到图像模型。与仅限于在预定类别集合内进行类别条件合成的VAR不同，我们的STAR通过三个关键设计实现了文本驱动的开放集生成：为了提高具有未见过的对象和概念组合的多样性和泛化能力，我们引入了一个预训练的文本编码器来提取文本约束的表示，然后将其用作指导。为了改善生成的图像与细粒度文本指导之间的交互，使结果更可控，我们在每个尺度上都加入了额外的交叉注意力层。鉴于不同尺度之间存在自然的结构关联，我们利用了二维旋转位置编码（RoPE），并将其调整为标准化版本。这确保了对不同尺度上标记图中相对位置的一致解释，并稳定了训练过程。大量实验表明，STAR在保真度、图像文本一致性和美学质量方面超过了现有的基准。我们的研究结果强调了自回归方法在高质量图像合成领域的潜力，为目前由扩散方法主导的文本到图像领域提供了有希望的新方向。|
|**2024-06-15**|[CrossFuse: A Novel Cross Attention Mechanism based Infrared and Visible Image Fusion Approach](http://arxiv.org/abs/2406.10581)|**[link](https://github.com/hli1221/crossfuse)**|多模态视觉信息融合旨在将多传感器数据集成到包含更多互补信息和更少冗余特征的单个图像中。然而，互补信息很难提取，特别是对于红外和可见光图像，这两种模态之间存在很大的相似性差距。常见的交叉注意力模块只考虑相关性，相反，图像融合任务需要关注互补性（不相关性）。因此，本文提出了一种新的交叉注意机制（CAM）来增强互补信息。此外，提出了一种基于两阶段训练策略的融合方案来生成融合图像。第一阶段，针对每种模态训练两个结构相同的自动编码器网络。然后，在固定编码器的情况下，在第二阶段训练CAM和解码器。利用训练好的CAM，将从两种模态中提取的特征融合成一个融合特征，其中互补信息得到增强，冗余特征得到减少。最后，可以通过训练好的解码器生成融合图像。实验结果表明，与现有的融合网络相比，我们提出的融合方法获得了最先进的融合性能。代码可在https://github.com/hli1221/CrossFuse获取。|
|**2024-06-14**|[Whisper-Flamingo: Integrating Visual Features into Whisper for Audio-Visual Speech Recognition and Translation](http://arxiv.org/abs/2406.10082)|**[link](https://github.com/roudimit/whisper-flamingo)**|音频-视觉语音识别 (AVSR) 利用基于嘴唇的视频来提高在噪声环境下的性能。由于视频比音频更难获取，AVSR 模型的视频训练数据通常只有几千小时。相比之下，Whisper 等语音模型使用数十万小时的数据进行训练，因此可以学习到更好的语音到文本解码器。巨大的训练数据差异促使我们调整 Whisper 以处理视频输入。受 Flamingo 将视觉特征注入语言模型的启发，我们提出了 Whisper-Flamingo，它使用门控交叉注意力将视觉特征集成到 Whisper 语音识别和翻译模型中。在嘈杂条件下，我们的音频-视觉 Whisper-Flamingo 在 6 种语言的英语语音识别和英语-X 翻译方面优于纯音频 Whisper。此外，Whisper-Flamingo 是一个多功能模型，可以使用一组参数执行所有这些任务，而先前的方法是在每种语言上单独训练的。|
|**2024-06-14**|[Simul-Whisper: Attention-Guided Streaming Whisper with Truncation Detection](http://arxiv.org/abs/2406.10052)|null|作为一种强大且规模庞大的多语言语音识别模型，Whisper 在许多低资源和 out-of-distribution 场景中表现出色。然而，其编码器-解码器结构阻碍了其在流式语音识别中的应用。在本文中，我们介绍了 Simul-Whisper，它利用 Whisper 交叉注意力机制中嵌入的时间对齐信息来指导自回归解码，并在无需对预训练模型进行任何微调的情况下实现基于块的流式自动语音识别。此外，我们观察到块边界处截断词对解码结果的负面影响，并提出了一种基于脉冲神经元的截断检测模型来解决这个问题。在多种语言和 Whisper 架构上的实验表明，Simul-Whisper 在 1 秒的块大小下平均绝对词错误率仅下降 1.46%，明显优于当前最先进的基线模型。|

## 3DGS

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-06-17**|[RetinaGS: Scalable Training for Dense Scene Rendering with Billion-Scale 3D Gaussians](http://arxiv.org/abs/2406.11836)|null|在这项工作中，我们探索了在大规模、高分辨率数据集上训练高参数 3D 高斯 splatting (3DGS) 模型的可能性。我们为 3DGS 设计了一种通用的模型并行训练方法，名为 RetinaGS，它使用适当的渲染方程，可以应用于任何场景和任意分布的高斯基元。它使我们能够探索 3DGS 在基元数量和训练分辨率方面的缩放行为，而这些在以前是难以探索的，并超越了先前最先进的重建质量。当使用我们的方法增加基元数量时，我们观察到视觉质量有明显的积极趋势。我们还首次尝试在完整 MatrixCity 数据集上训练具有超过 10 亿个基元的 3DGS 模型，该模型获得了非常不错的视觉质量。|
|**2024-06-17**|[Effective Rank Analysis and Regularization for Enhanced 3D Gaussian Splatting](http://arxiv.org/abs/2406.11672)|null|从多视角图像进行三维重建是计算机视觉和图形学中的基本挑战之一。近年来，三维高斯 splatting (3DGS) 已成为一种很有前景的技术，能够以高质量的三维重建实现实时渲染。该方法利用三维高斯表示和平铺 splatting 技术，绕过了昂贵的神经场查询。尽管具有潜力，但由于高斯会收敛成具有一个主导方差的各向异性高斯，3DGS 仍面临着挑战，包括针状伪影、次优几何形状和不准确的法线。我们建议使用有效秩分析来检查三维高斯基元的形状统计数据，并确定高斯确实收敛成有效秩为 1 的针状形状。为了解决这个问题，我们引入了有效秩作为正则化，它限制了高斯的结构。我们新的正则化方法增强了法线和几何重建，同时减少了针状伪影。该方法可以作为附加模块集成到其他 3DGS 变体中，在不影响视觉保真度的情况下提高其质量。|
|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|在非结构化旅游环境中拍摄的照片经常呈现出多变的外观和短暂的遮挡，这对精确的场景重建提出了挑战，并在新视角合成中导致了伪影。尽管先前的方法已经将神经辐射场 (NeRF) 与其他可学习模块相结合来处理动态外观和消除短暂对象，但其大量的训练需求和缓慢的渲染速度限制了实际部署。最近，3D 高斯 splatting (3DGS) 已成为 NeRF 的一种很有前途的替代方案，它提供了卓越的训练和推理效率以及更好的渲染质量。本文介绍了 Wild-GS，这是一种针对不受约束的照片集优化的 3DGS 创新改编版，同时保留了其效率优势。Wild-GS 通过每张图像的固有材质属性、全局照明和相机属性，以及逐点反射率的局部变化来确定每个 3D 高斯的外观。与先前在图像空间中对参考特征进行建模的方法不同，Wild-GS 通过对从参考图像中提取的三平面进行采样，将像素外观特征明确地与相应的局部高斯对齐。这种新颖的设计有效地将参考视图的高频细节外观转移到 3D 空间，并显著加快了训练过程。此外，利用 2D 可见性图和深度正则化分别减轻瞬态效应和约束几何形状。大量实验表明，Wild-GS 在所有现有技术中实现了最先进的渲染性能以及最高的训练和推理效率。|
|**2024-06-14**|[L4GM: Large 4D Gaussian Reconstruction Model](http://arxiv.org/abs/2406.10324)|null|我们提出了L4GM，这是第一个能够从单视角视频输入生成动画对象的4D大型重建模型，并且只需一次几秒钟的前馈传递即可完成。我们成功的关键是一个新颖的多视角视频数据集，其中包含从Objaverse精心策划和渲染的动画对象。该数据集描绘了4.4万个不同的对象，包含11万个以48个视角渲染的动画，产生了1200万个视频，总计3亿帧。为了实现可扩展性，我们保持L4GM的简洁性，并直接构建在LGM之上，LGM是一个预先训练的3D大型重建模型，可以从多视角图像输入生成3D高斯椭球体。L4GM从以低帧率采样的视频帧中输出每帧3D高斯 splatting 表示，然后将表示上采样到更高的帧率以实现时间平滑度。我们在基础LGM中添加了时间自注意力层，以帮助它学习跨时间的一致性，并利用每时间步多视角渲染损失来训练模型。通过训练插值模型将表示上采样到更高的帧率，该模型生成中间3D高斯表示。我们展示了仅在合成数据上训练的L4GM在真实世界的视频上具有极好的泛化能力，可以生成高质量的动画3D资产。|
|**2024-06-14**|[PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting](http://arxiv.org/abs/2406.10219)|null|近年来，新颖视图合成的进步实现了实时的渲染速度和高重建精度。三维高斯 splatting (3D-GS) 是一种基础的基于点的三维场景参数化表示方法，它将场景建模为大量三维高斯函数的集合。复杂的场景可能包含数百万个高斯函数，导致巨大的存储和内存需求，从而限制了 3D-GS 在资源有限的设备上的可行性。目前，通过修剪高斯函数来压缩这些预训练模型的技术依赖于结合启发式方法来确定要移除哪些高斯函数。在本文中，我们提出了一种基于原理的空间敏感性修剪评分方法，该方法优于这些方法。它被计算为训练视图上的重建误差相对于每个高斯函数的空间参数的二阶近似。此外，我们提出了一个多轮修剪-优化流程，该流程可以应用于任何预训练的 3D-GS 模型，而无需更改训练流程。在修剪了 88.44% 的高斯函数后，我们观察到我们的 PUP 3D-GS 流程将 3D-GS 的平均渲染速度提高了 2.65 倍，同时保留了更多显著的前景信息，并在 Mip-NeRF 360、Tanks & Temples 和 Deep Blending 数据集的场景上实现了比先前修剪技术更高的图像质量指标。|
|**2024-06-14**|[GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors](http://arxiv.org/abs/2406.10111)|null|从低分辨率输入视图实现高分辨率新视角合成 (HRNVS) 是一项具有挑战性的任务，因为缺乏高分辨率数据。 以前的方法从低分辨率输入视图优化高分辨率神经辐射场 (NeRF)，但渲染速度慢。 在这项工作中，我们基于 3D 高斯 splatting (3DGS) 开发了我们的方法，因为它能够以更快的渲染速度生成高质量的图像。 为了缓解更高分辨率合成的数据短缺问题，我们建议利用现成的 2D 扩散先验，通过分数蒸馏采样 (SDS) 将 2D 知识提取到 3D 中。 然而，由于生成先验带来的随机性，将 SDS 直接应用于基于高斯的 3D 超分辨率会导致不希望和冗余的 3D 高斯基元。 为了缓解这个问题，我们引入了两种简单而有效的技术来减少 SDS 引入的随机扰动。 具体来说，我们 1) 使用退火策略缩小 SDS 中扩散时间步长的范围； 2) 在压缩过程中随机丢弃冗余的高斯基元。 大量实验表明，我们提出的 GaussainSR 可以在合成和真实世界数据集上仅使用低分辨率输入即可获得 HRNVS 的高质量结果。 项目页面：https://chchnii.github.io/GaussianSR/|
|**2024-06-14**|[GradeADreamer: Enhanced Text-to-3D Generation Using Gaussian Splatting and Multi-View Diffusion](http://arxiv.org/abs/2406.09850)|**[link](https://github.com/trapoom555/gradeadreamer)**|文本到3D生成已经展现出可观的结果，但仍然面临着一些常见挑战，例如多面体Janus问题和高质量资源生成时间过长等问题。在本文中，我们通过引入一种名为GradeADreamer的新颖的三阶段训练流程来解决这些问题。该流程能够仅使用单个RTX 3090 GPU在30分钟内生成高质量的资源。我们提出的方法采用多视图扩散模型MVDream生成高斯球体作为先验，然后使用StableDiffusion优化几何形状和纹理。实验结果表明，与先前最先进的方法相比，我们的方法显著减轻了多面体Janus问题，并获得了最高的平均用户偏好排名。项目代码可在https://github.com/trapoom555/GradeADreamer获取。|
|**2024-06-14**|[Unified Gaussian Primitives for Scene Representation and Rendering](http://arxiv.org/abs/2406.09733)|null|在计算机图形学中，寻找一种统一的场景表示方法仍然是一个研究挑战。传统的基于网格的表示方法不适用于密集的模糊元素，并且为过滤和可微渲染引入了额外的复杂性。相反，基于体素的表示方法难以对硬表面进行建模，并且存在内存需求大的问题。我们提出了一种基于三维高斯分布的通用渲染基元，用于统一场景表示，其特点是具有从光滑表面到模糊元素的多样外观，以及基于物理的散射，以实现精确的全局照明。我们基于非指数传输制定了该基元的渲染理论，并推导出高效的渲染操作，使其与蒙特卡洛路径追踪兼容。这种新的表示方法可以从不同的来源转换而来，包括网格和三维高斯 splatting，并且由于其可微性，可以通过透射率优化进一步细化。我们展示了我们的表示方法在全局照明和外观编辑等各种渲染应用中的多功能性，同时自然地支持任意照明条件。此外，我们将我们的表示方法与现有的体积表示方法进行了比较，突出了其在细节再现方面的效率。|
|**2024-06-13**|[Modeling Ambient Scene Dynamics for Free-view Synthesis](http://arxiv.org/abs/2406.09395)|null|我们提出了一种新颖的动态自由视点合成方法，可以从单目捕捉中合成环境场景，为观看体验带来沉浸式的质量。我们的方法建立在3D高斯渲染(3DGS)的最新进展之上，该技术可以忠实地重建复杂的静态场景。先前将3DGS扩展到表示动态的尝试仅限于有界场景或需要多相机捕捉，并且通常无法泛化到未见过的运动，从而限制了它们的实际应用。我们的方法通过利用环境运动的周期性来学习运动轨迹模型，并结合仔细的正则化来克服这些限制。我们还提出了一些重要的实用策略，以提高基线3DGS静态重建的视觉质量，并提高对GPU内存密集型学习至关重要的内存效率。我们展示了几个具有复杂纹理和精细结构元素的周围自然场景的高质量逼真新颖视图合成。|
|**2024-06-13**|[GGHead: Fast and Generalizable 3D Gaussian Heads](http://arxiv.org/abs/2406.09377)|null|从大型二维图像集中学习三维头部先验信息是实现高质量三维感知人体建模的重要步骤。其核心要求是构建一种能够很好地扩展到大型数据集和高分辨率图像的有效架构。遗憾的是，现有的三维生成对抗网络（GAN）由于其训练和渲染速度相对较慢，难以扩展到生成高分辨率样本，并且通常不得不依赖二维超分辨率网络，但这是以牺牲全局三维一致性为代价的。为了应对这些挑战，我们提出了生成式高斯头部（GGHead）方法，该方法在三维 GAN 框架内采用了最新的三维高斯渲染表示。为了生成三维表示，我们采用了一个强大的二维卷积神经网络（CNN）生成器来预测模板头部网格的 UV 空间中的高斯属性。通过这种方式，GGHead 利用了模板 UV 布局的规律性，极大地促进了预测非结构化三维高斯集这一具有挑战性的任务。我们进一步通过一种新颖的渲染 UV 坐标上的总变差损失来提高生成的三维表示的几何保真度。直观地说，这种正则化鼓励相邻渲染像素应该来自模板 UV 空间中的相邻高斯。综上所述，我们的流程可以有效地生成仅从单视图二维图像观察中训练得到的三维头部。我们提出的框架在 FFHQ 数据集上的质量与现有的三维头部 GAN 相当，同时速度明显更快，并且完全保持了三维一致性。因此，我们首次展示了以 1024² 分辨率实时生成和渲染高质量、三维一致的头部。|

## 各类学习方式

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-06-17**|[Mix-Domain Contrastive Learning for Unpaired H&E-to-IHC Stain Translation](http://arxiv.org/abs/2406.11799)|null|苏木精-伊红 (H&E) 染色到免疫组化 (IHC) 染色的图像转换技术为精确的癌症诊断提供了一种很有前景的解决方案，特别是在缺乏专业医疗人员和昂贵设备资源匮乏的地区。考虑到 H&E-IHC 图像对之间存在像素级别的错位，当前的研究探索了图像对相同位置的图像块之间的病理学一致性。然而，大多数方法过度强调域或图像块之间的对应关系，而忽略了非对应目标提供的辅助信息。在本文中，我们提出了一种混合域对比学习 (MDCL) 方法，以利用非配对 H&E 到 IHC 染色转换中的监督信息。具体来说，所提出的 MDCL 方法通过估计锚点图像块与匹配图像中所有图像块之间的相关性来聚合域间和域内的病理学信息，鼓励网络从混合域中学习额外的对比知识。通过混合域病理信息聚合，MDCL 增强了对应图像块之间的病理学一致性，以及生成的 IHC 图像中不同位置的图像块之间的成分差异。在两个 H&E 到 IHC 染色转换数据集（即 MIST 和 BCI）上的大量实验表明，所提出的方法在多个指标上均达到了最先进的性能。|
|**2024-06-17**|[A Brief Survey on Leveraging Large Scale Vision Models for Enhanced Robot Grasping](http://arxiv.org/abs/2406.11786)|null|在现实世界场景中，机器人抓取是一项艰巨的运动任务，构成了在各行各业部署高性能机器人的主要障碍。值得注意的是，数据的稀缺使得学习模型的抓取尤其具有挑战性。近年来，计算机视觉领域的进步见证了基于互联网海量数据的成功无监督训练机制的发展，现在几乎所有杰出的模型都利用了预训练的骨干网络。在此背景下，我们开始研究大规模视觉预训练在提高机器人抓取性能方面的潜在优势。这篇初步的文献综述阐明了关键挑战，并为机器人操作的视觉预训练的未来研究方向进行了展望。|
|**2024-06-17**|[DiffMM: Multi-Modal Diffusion Model for Recommendation](http://arxiv.org/abs/2406.11781)|null|像抖音和YouTube这样的在线多模态分享平台的兴起使得个性化推荐系统能够将多种模态（如视觉、文本和音频）纳入用户表示中。然而，解决这些系统中数据稀疏性的挑战仍然是一个关键问题。为了解决这一局限性，最近的研究引入了自监督学习技术来增强推荐系统。然而，这些方法通常依赖于简单的随机增强或直观的跨视图信息，这可能会引入不相关的噪声，并且无法准确地将多模态上下文与用户-项目交互建模对齐。为了填补这一研究空白，我们提出了一种新的用于推荐的多模态图扩散模型，称为DiffMM。我们的框架将模态感知图扩散模型与跨模态对比学习范式相结合，以改进模态感知用户表示学习。这种整合促进了多模态特征信息与协同关系建模之间更好的对齐。我们的方法利用扩散模型的生成能力自动生成一个感知不同模态的用户-项目图，从而促进在用户-项目交互建模中纳入有用的多模态知识。我们在三个公共数据集上进行了广泛的实验，一致证明了我们的DiffMM相对于各种竞争基线的优越性。有关开源模型实现的详细信息，您可以访问以下地址获取我们提出的框架的源代码：https://github.com/HKUDS/DiffMM。|
|**2024-06-17**|[Zero-Shot Generalization during Instruction Tuning: Insights from Similarity and Granularity](http://arxiv.org/abs/2406.11721)|**[link](https://github.com/hbx-hbx/dynamics_of_zero-shot_generalization)**|理解对齐技术始于理解指令微调带来的零样本泛化能力，但人们对其机制知之甚少。现有的研究主要局限于任务层面，而没有考虑到任务是人为定义的，对于大型语言模型来说，仅仅是由token和表示组成的。这条研究路线仅限于从任务对的角度研究任务之间的迁移，很少有研究从数据本身的角度理解零样本泛化。为了弥合这一差距，我们首先通过多个指标证明，指令微调过程中的零样本泛化发生在非常早期的阶段。接下来，我们从数据相似性和粒度两个角度研究了零样本泛化的促进因素，证实了在指令微调的早期阶段遇到高度相似和细粒度的训练数据，而不受限于定义的“任务”，能够实现更好的泛化。最后，我们提出了一种更合理的训练数据编排方法，即以测试为中心的多轮编排，并展示了其在促进持续学习和进一步降低损失方面的有效性。我们首次证明，指令微调过程中的零样本泛化是训练数据和测试数据之间在实例级别的基于相似性的泛化形式。我们希望我们的分析能够促进对指令微调过程中零样本泛化的理解，并有助于开发更对齐的大型语言模型。我们的代码发布在https://github.com/HBX-hbx/dynamics_of_zero-shot_generalization。|
|**2024-06-17**|[Multiple Descents in Unsupervised Learning: The Role of Noise, Domain Shift and Anomalies](http://arxiv.org/abs/2406.11703)|null|双下降现象最近在监督学习中受到关注。它挑战了偏差-方差权衡的传统观点，展示了一种令人惊讶的行为。随着模型复杂度的增加，测试误差最初会下降，直到达到模型开始过拟合训练集的某个点，导致测试误差上升。然而，与经典理论不同的是，当超过一定程度的过参数化时，误差会再次下降。我们研究了双下降现象在无监督学习中的存在，这是一个很少受到关注且尚未完全理解的领域。我们使用欠完备自动编码器 (AE) 对各种应用进行了广泛的实验，例如处理噪声数据、域偏移和异常。我们使用合成数据和真实数据，并确定了上述所有应用中模型、时期和样本方面的双下降现象。最后，我们评估了自动编码器在检测异常和减轻数据集之间域偏移方面的可用性。我们的研究结果表明，过参数化模型不仅可以提高重建性能，还可以增强下游任务的能力。|
|**2024-06-17**|[Making Old Things New: A Unified Algorithm for Differentially Private Clustering](http://arxiv.org/abs/2406.11649)|null|作为数据分析和无监督学习的支柱，私有聚类问题已在各种隐私模型下得到了广泛研究。集中式差分隐私是其中最早的模型，并且该问题也已针对本地和随机变换进行了研究。在每种情况下，目标都是设计一种算法，以尽可能小的误差私下计算聚类。对每种变体的研究都催生了新的算法：因此，私有聚类算法的图景相当复杂。在本文中，我们展示了一种已有 20 年历史的算法，只需稍作修改即可适用于任何这些模型。这提供了一个统一的视角：在匹配几乎所有先前已知结果的同时，它允许我们改进其中的一些结果，并将其扩展到一个新的隐私模型，即持续观察设置，其中输入随时间而变化，并且算法必须在每个时间步输出一个新的解决方案。|
|**2024-06-17**|[Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!](http://arxiv.org/abs/2406.11629)|null|利用大型语言模型 (LLM) 作为评判者来评估 LLM 的性能最近受到了关注。然而，这种方法同时引入了来自 LLM 的潜在偏差，引发了对评估结果可靠性的担忧。为了缓解这个问题，我们提出并研究了两种版本的少样本上下文提示，即强化和无监督 ICL，用于帮助 GPT-4o 作为评判者进行单答案评分。基于设计的提示，我们研究了扩展上下文示例数量对评估的一致性和质量的影响。此外，我们首先揭示了 GPT-4o 作为评判者在成对比较中的符号偏差，然后提出了一种简单而有效的方法来减轻它。实验结果表明，先进的长上下文 LLM，例如 GPT-4o，在少样本机制中的表现优于零样本机制。同时，实验结果进一步验证了符号偏差缓解方法的有效性。|
|**2024-06-17**|[Wide Area VISTA Extra-galactic Survey (WAVES): Unsupervised star-galaxy separation on the WAVES-Wide photometric input catalogue using UMAP and ${\rm{\scriptsize HDBSCAN}}$](http://arxiv.org/abs/2406.11611)|null|星系分离是创建河外光谱巡天目标星表的关键步骤。倾向于包容性的分类器可能会将伪星包含在内，浪费光纤时间，而更保守的分类器可能会忽略星系，从而损害完整性，进而影响巡天目标。为了避免监督方法中训练集引入的偏差，我们采用了一种无监督机器学习方法。我们利用广域 VISTA 河外星系巡天 (WAVES)-Wide 星表的光度数据，该星表包含 9 波段 $u-K_s$ 数据，并利用 ${\rm P{\scriptsize RO} F{\scriptsize OUND}}$ 提取颜色、流量和视尺寸信息，创建了一个特征空间。我们应用非线性降维方法 UMAP（均匀流形近似和投影）结合分类器 ${\rm{\scriptsize HDBSCAN}}$ 对恒星和星系进行分类。我们使用来自 Gaia、SDSS、GAMA 和 DESI 的真实星表，根据基线颜色和形态方法验证了我们的方法。在 AB 星等限制为 $Z = 21.2$ 的情况下，我们正确识别了 99.72% 的星系，在整个真实样本中，F1 得分为 0.9970，而基线方法的 F1 得分为 0.9871。与基线方法 (0.9780) 相比，我们的方法具有更高的纯度 (0.9966)，从而提高了效率，识别出的星系或不明来源减少了 11%，在 4MOST 仪器上节省了大约 70,000 个光纤小时。我们获得了具有挑战性的来源（包括类星体、致密星系和低表面亮度星系）的可靠分类统计数据，分别检索到其中的 95.1%、84.6% 和 99.5%。角聚类分析验证了我们的分类，表明无论基线分类如何，都与预期的星系聚类一致。|
|**2024-06-17**|[CM2-Net: Continual Cross-Modal Mapping Network for Driver Action Recognition](http://arxiv.org/abs/2406.11340)|null|驾驶员动作识别通过整合红外和深度等多种模态，在增强驾驶员与车辆的交互和确保驾驶安全方面取得了显著进展。然而，与仅使用RGB模态相比，在车厢环境中为所有类型的非RGB模态收集大量数据始终是一项费力且昂贵的工作。因此，以前的工作建议通过微调在RGB视频上预训练的模型来独立学习每个非RGB模态，但这些方法在面对新出现的模态时，由于较大的域差异，在提取信息特征方面效果较差。相比之下，我们提出了一种连续跨模态映射网络（CM2-Net），利用先前学习的模态的指导性提示，持续学习每个新出现的模态。具体来说，我们开发了累积跨模态映射提示（ACMP），将从先前模态中学习到的判别性和信息性特征映射到新出现的模态的特征空间中。然后，当面对新出现的模态时，这些映射的特征能够为应该提取和优先考虑哪些特征提供有效的提示。这些提示在整个持续学习过程中不断积累，从而进一步提高识别性能。在Drive&Act数据集上进行的大量实验表明，CM2-Net在单模态和多模态驾驶员动作识别方面均具有优越的性能。|
|**2024-06-17**|[Syn-to-Real Unsupervised Domain Adaptation for Indoor 3D Object Detection](http://arxiv.org/abs/2406.11311)|null|在室内3D目标检测中使用合成数据，可以大大减少3D标注所需的人工，并训练有效的零样本检测器。然而，跨越合成到真实室内数据集的复杂域偏移问题仍未得到充分探索。本文提出了一种新的面向对象的层次化域对齐（OHDA）框架，用于室内3D目标检测中的合成到真实无监督域自适应。我们的方法包括一种对象感知增强策略，以有效地使源域数据多样化，并且我们引入了一个由对抗训练分支和伪标签分支组成的双分支自适应框架，以便同时实现整体级别和类别级别的域对齐。针对室内无监督域自适应，我们提出了两种专门设计的方案，进一步改进了伪标签。我们从合成数据集3D-FRONT到真实世界数据集ScanNetV2和SUN RGB-D的适应结果表明，与仅使用源数据的基线相比，mAP25分别显著提高了9.7%和9.1%，并且始终优于从2D和3D室外场景适应的方法。代码将在论文被接收后公开。|

