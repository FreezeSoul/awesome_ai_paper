---
layout: default
---

## Updated on 2024.07.27
> Usage instructions: [here](./docs/README.md#usage)

## 多模态

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-07-25**|[Cross-Vendor Reproducibility of Radiomics-based Machine Learning Models for Computer-aided Diagnosis](http://arxiv.org/abs/2407.18060)|null|背景：机器学习模型在前列腺癌检测中跨不同 MRI 厂商的再现性仍然是一项重大挑战。方法：本研究调查了使用 Pyradiomics 和 MRCradiomics 库从 T2 加权 MRI 图像中提取的放射组学特征训练的支持向量机 (SVM) 和随机森林 (RF) 模型。使用最大相关最小冗余 (MRMR) 技术进行特征选择。我们旨在通过多模态学习和特征融合来增强临床决策支持。结果：我们的 SVM 模型利用 Pyradiomics 和 MRCradiomics 的组合特征，在 Multi-Improd 数据集（西门子扫描仪）上实现了 0.74 的 AUC，但在飞利浦测试集上降至 0.60。RF 模型显示出类似的趋势，仅使用 Pyradiomics 特征的模型具有显著的鲁棒性（飞利浦上的 AUC 为 0.78）。结论：这些发现证明了多模态特征整合在提高机器学习模型的稳健性和泛化性方面，可用于前列腺癌检测的临床决策支持。这项研究标志着朝着开发可靠的 AI 驱动诊断工具迈出的重要一步，该工具可在各种成像平台上保持功效。||
|**2024-07-25**|[Enhancing Model Performance: Another Approach to Vision-Language Instruction Tuning](http://arxiv.org/abs/2407.17813)|null|大型语言模型 (LLM) 与视觉语言 (VL) 任务的集成是人工智能领域的一项变革性发展，突出了 LLM 作为通用聊天机器人的巨大潜力。然而，这种演变的当前趋势侧重于视觉和语言的集成，以创建能够在更多样化和现实世界环境中运行的模型。我们提出了一种称为瓶颈适配器的新方法，专门用于增强这些复杂模型的多模态功能，通过称为多模态模型调整 (MMT) 的过程实现整个多模态 LLM 框架的联合优化。我们的方法利用轻量级适配器连接图像编码器和 LLM，无需大型、复杂的神经网络。与传统的模块化训练方案不同，我们的方法采用端到端优化机制，结合适配器，可以使用更小的参数集促进联合优化。我们的方法表现出强大的性能，准确率达到 90.12%，优于人类水平 (88.4%) 和 LaVIN-7B (89.41%)。||
|**2024-07-25**|[KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models](http://arxiv.org/abs/2407.17773)|**[link](https://github.com/ey242/kiva)**|本文研究大型多模态模型 (LMM) 与人类成人和儿童在视觉类比推理方面的差异。“视觉类比”是从一幅图像中推断出抽象规则并将其应用于另一幅图像。虽然存在用于测试 LMM 视觉推理的基准，但它们需要高级技能，并且忽略了即使是幼儿也能做到的基本视觉类比。受发展心理学的启发，我们提出了一个包含 1,400 种日常物体视觉变换的新基准，以测试 LMM 在视觉类比推理方面的能力，并将它们与儿童和成人进行比较。我们将评估分为三个阶段：识别变化的内容（例如，颜色、数量等），变化的方式（例如，添加一个物体），以及将规则应用于新场景。我们的研究结果表明，虽然像 GPT-4V、LLaVA-1.5 和 MANTIS 这样的模型可以有效地识别“什么”发生了变化，但它们难以量化“如何”变化并将此规则外推到新物体上。相比之下，儿童和成人在所有三个阶段都表现出更强的类比推理能力。此外，测试中最强的模型 GPT-4V 在涉及颜色和大小等简单视觉属性的任务中表现更好，这与人类成人的更快反应时间相关。相反，更复杂的任务，例如数量、旋转和反射，需要对 3D 物理世界进行广泛的认知处理和理解，这些任务带来了更大的挑战。总而言之，这些发现突出了在主要由 2D 图像和文本组成的数据上训练模型的局限性。||
|**2024-07-24**|[Testing Large Language Models on Driving Theory Knowledge and Skills for Connected Autonomous Vehicles](http://arxiv.org/abs/2407.17211)|null|处理长尾极端情况是自动驾驶汽车（AV）面临的主要挑战。虽然大型语言模型（LLM）具有通过出色的泛化和解释能力来处理极端情况的巨大潜力，并且在自动驾驶应用方面越来越受到研究兴趣，但仍有一些技术障碍需要克服，例如严格的模型性能和对LLM的巨大计算资源需求。在本文中，我们研究了一种应用远程或边缘LLM来支持自动驾驶的新方法。这种LLM辅助驾驶系统的一个关键问题是评估LLM对驾驶理论和技能的理解，确保它们有资格为CAV执行安全关键的驾驶辅助任务。我们针对几种专有的LLM模型（OpenAI GPT模型、百度文心一言和阿里通义千问）和开源LLM模型（清华MiniCPM-2B和MiniCPM-Llama3-V2.5）设计并运行了包含500多道多项选择题的驾驶理论测试。实验测量了模型的准确性、成本和处理延迟。实验结果表明，虽然GPT-4模型通过了测试，并具有改进的领域知识，文心一言的准确率为85%（略低于86%的及格线），但包括GPT-3.5在内的其他LLM模型均未通过测试。对于包含图像的测试题，多模态模型GPT4-o的准确率高达96%，MiniCPM-Llama3-V2.5的准确率达到76%。虽然GPT-4在CAV驾驶辅助应用方面具有更大的潜力，但使用GPT-4模型的成本要高得多，几乎是使用GPT3.5的50倍。这些结果有助于决定是否将现有的LLM用于CAV应用，并在模型性能和成本之间取得平衡。||
|**2024-07-24**|[Selective Vision-Language Subspace Projection for Few-shot CLIP](http://arxiv.org/abs/2407.16977)|**[link](https://github.com/zhuhsingyuu/ssp)**|像CLIP这样的视觉语言模型能够将不同模态的数据映射到一个统一的特征空间中，通过测量给定图像和文本的相似性来实现零样本/少样本推理。然而，大多数现有方法忽略了CLIP编码特征中的模态差异，这表现为文本和图像特征彼此相距甚远，导致分类性能有限。为了解决这个问题，我们引入了一种称为选择性视觉语言子空间投影（SSP）的方法，它结合了局部图像特征，并利用它们作为桥梁来增强图像-文本对之间的对齐。具体来说，我们的SSP框架包括两个并行模块：视觉投影器和语言投影器。两个投影器都利用局部图像特征来跨越图像和文本各自的子空间，从而将图像和文本特征投影到各自的子空间中以实现对齐。此外，我们的方法只需要进行无需训练的矩阵计算，并且可以无缝集成到先进的基于CLIP的少样本学习框架中。在11个数据集上的大量实验表明，SSP具有优越的文本-图像对齐能力，优于最先进的对齐方法。代码可在https://github.com/zhuhsingyuu/SSP获取。||
|**2024-07-23**|[Imperfect Vision Encoders: Efficient and Robust Tuning for Vision-Language Models](http://arxiv.org/abs/2407.16526)|null|视觉语言模型 (VLM) 在视觉问答和图像描述方面表现出令人印象深刻的能力，成为视觉模型和语言模型之间的关键纽带。然而，现有的开源 VLM 严重依赖于预训练和冻结的视觉编码器（例如 CLIP）。尽管 CLIP 在不同领域具有鲁棒性，但它仍然表现出不可忽视的图像理解错误。这些错误会传播到 VLM 响应中，导致性能欠佳。在我们的工作中，我们提出了一种高效且稳健的方法来更新 VLM 中的视觉编码器。我们的方法有选择地和局部地更新编码器，从而在发生先前错误的数据上显着提高性能，同时保持整体鲁棒性。此外，我们证明了我们的方法在持续的少样本更新期间的有效性。我们的方法的特点是理论基础、通用性和计算效率。||
|**2024-07-23**|[Chameleon: Images Are What You Need For Multimodal Learning Robust To Missing Modalities](http://arxiv.org/abs/2407.16243)|null|多模态学习相较于单模态架构表现出显著的性能提升。然而，如果缺少一种或多种模态，多模态学习方法的性能往往会下降。这可能是由于常用的多分支设计包含特定模态流，使得模型依赖于完整模态集的可用性。在这项工作中，我们提出了一种鲁棒的文本-视觉多模态学习方法，称为Chameleon，它完全不同于传统的多分支设计。为了实现这一点，我们提出了通过将文本模态编码为视觉表示来将输入模态统一为一种格式。因此，我们的方法不需要特定模态分支来学习模态无关的多模态表示，使其对缺失的模态具有鲁棒性。我们在四个流行的具有挑战性的数据集上进行了广泛的实验，包括Hateful Memes、UPMC Food-101、MM-IMDb和Ferramenta。Chameleon不仅在训练/测试时所有模态都存在的情况下实现了优越的性能，而且在缺少模态的情况下也表现出显著的弹性。||
|**2024-07-22**|[AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot Anomaly Detection](http://arxiv.org/abs/2407.15795)|**[link](https://github.com/caoyunkang/adaclip)**|零样本异常检测 (ZSAD) 的目标是在来自任意新类别的图像中识别异常。本研究介绍了用于 ZSAD 任务的 AdaCLIP，它利用了预训练的视觉语言模型 (VLM) CLIP。AdaCLIP 将可学习的提示语纳入 CLIP，并通过在辅助标注的异常检测数据上进行训练来优化它们。提出了两种类型的可学习提示语：静态和动态。静态提示语在所有图像之间共享，用于初步调整 CLIP 以适应 ZSAD。相反，动态提示语针对每个测试图像生成，为 CLIP 提供动态适应能力。静态和动态提示语的组合被称为混合提示语，可以提高 ZSAD 性能。在来自工业和医疗领域的 14 个真实世界异常检测数据集上进行的大量实验表明，AdaCLIP 优于其他 ZSAD 方法，并且可以更好地泛化到不同的类别甚至领域。最后，我们的分析强调了多样化的辅助数据和优化的提示语对于增强泛化能力的重要性。代码可在 https://github.com/caoyunkang/AdaCLIP 获取。||
|**2024-07-22**|[CLIP with Generative Latent Replay: a Strong Baseline for Incremental Learning](http://arxiv.org/abs/2407.15793)|**[link](https://github.com/aimagelab/mammoth)**|随着Transformer和视觉语言模型（VLM，例如CLIP）的出现，大型预训练模型已成为增强持续学习场景中性能的常用策略。这导致了许多提示策略的发展，这些策略可以有效地微调基于Transformer的模型，而不会屈服于灾难性遗忘。然而，这些方法难以将模型专门用于与预训练显着不同的领域，并难以保持其零样本能力。在这项工作中，我们提出了用于增量提示学习的持续生成训练，这是一种减轻遗忘并适应VLM的新方法，它利用生成式重放来使提示与任务保持一致。我们还引入了一个新的指标来评估CL基准测试中的零样本能力。通过对不同领域的广泛实验，我们证明了我们的框架在适应新任务的同时提高零样本能力的有效性。进一步的分析表明，我们的方法可以弥合与联合提示调整的差距。代码库可在https://github.com/aimagelab/mammoth获取。||
|**2024-07-22**|[Concept-Based Interpretable Reinforcement Learning with Limited to No Human Labels](http://arxiv.org/abs/2407.15786)|null|近来，强化学习（RL）的进展主要利用基于神经网络的策略进行决策，但这些模型通常缺乏可解释性，对利益相关者的理解和信任构成挑战。概念瓶颈模型通过将人类可理解的概念整合到神经网络中，提供了一种可解释的替代方案。然而，先前工作的一个重大限制是假设在训练期间这些概念的人工标注很容易获得，需要人工标注员持续实时输入。为了克服这一限制，我们引入了一种新的训练方案，使强化学习算法能够通过仅查询人类标记一小部分数据，或者在极端情况下，在没有任何人工标记的情况下，有效地学习基于概念的策略。我们的算法LICORICE包含三个主要贡献：将概念学习和强化学习训练交织在一起，使用概念集成来主动选择信息丰富的数据点进行标记，以及使用简单策略去除概念数据的相关性。我们展示了LICORICE如何在三种环境中将人工标记工作量减少到500个或更少的概念标签。最后，我们提出了一个初步研究，以探索如何使用强大的视觉语言模型从原始视觉输入中推断概念，而无需明确的标签，并且对性能的影响最小。||
|**2024-07-22**|[LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding](http://arxiv.org/abs/2407.15754)|**[link](https://github.com/longvideobench/longvideobench)**|大型多模态模型 (LMM) 正在处理越来越长、越来越丰富的输入。尽管取得了进展，但很少有公共基准可以衡量这种发展。为了弥补这一差距，我们引入了 LongVideoBench，这是一个问答基准测试，其特点是视频和语言交错输入，时长可达一小时。我们的基准测试包括 3,763 个不同长度的网络收集视频及其字幕，涵盖各种主题，旨在全面评估 LMM 对长期多模态理解的能力。为此，我们将主要挑战理解为从长输入中准确检索和推理详细的多模态信息。因此，我们构建了一个名为“指称推理”的新型视频问答任务。具体而言，作为问题的一部分，它包含一个指称查询，该查询引用相关的视频上下文，称为指称上下文。然后，模型需要根据指称上下文对相关的视频细节进行推理。遵循指称推理的范式，我们策划了 17 个细粒度类别中 6,678 个人工标注的多选题，建立了最全面的长格式视频理解基准测试之一。评估表明，即使是最先进的专有模型（例如 GPT-4o、Gemini-1.5-Pro、GPT-4-Turbo），LongVideoBench 也提出了重大挑战，而开源模型的性能差距更大。此外，我们的结果表明，只有当模型能够处理更多帧时，它们在基准测试中的性能才会提高，这使得 LongVideoBench 成为评估未来一代长上下文 LMM 的宝贵基准。||
|**2024-07-22**|[Zero-Shot Embeddings Inform Learning and Forgetting with Vision-Language Encoders](http://arxiv.org/abs/2407.15731)|null|尽管大型视觉语言基础模型大量涌现，但对这些模型微调后学习和遗忘结果的估计在很大程度上仍未得到探索。受对比双编码器中模态差距重要性研究的启发，我们提出了模态间内度量（IIMM）。IIMM结合了量化图像嵌入之间相似性和错误图像与标签嵌入对之间相似性的项，可以有效预测微调后的性能变化。我们对四种最先进的视觉语言模型（CLIP、SigLIP、CoCa、EVA-02-CLIP）和五种微调技术（全微调、BitFit、注意力权重调整、LoRA、CLIP-Adapter）进行了广泛的实证分析，结果表明存在强烈的、统计学上显著的线性关系：在IIMM得分较高的任务上进行微调会产生更大的域内性能提升，但也会导致更严重的域外性能下降，一些参数高效的微调（PEFT）方法表现出极端的遗忘现象。我们将我们的度量与来自最先进模型选择方法的迁移分数进行了比较，结果表明IIMM对准确率提升的预测能力明显更强。只需对目标数据进行一次前向传递，实践者就可以利用这一关键见解，启发式地评估模型在微调后预期改进的程度。如果进一步了解模型在一些不同任务上的性能，这种启发式方法还可以进一步演变为在训练新任务时预测预期性能变化的强有力工具。||
|**2024-07-22**|[SAM2CLIP2SAM: Vision Language Model for Segmentation of 3D CT Scans for Covid-19 Detection](http://arxiv.org/abs/2407.15728)|null|本文提出了一种能够集成到任何模型和方法中的有效图像分割新方法；我们选择的范例是医学图像（3D 胸部 CT 扫描）分类以进行 Covid-19 检测。我们的方法包括结合视觉语言模型来分割 CT 扫描，然后将其输入到名为 RACNet 的深度神经网络架构中以进行 Covid-19 检测。特别是，引入了一种名为 SAM2CLIP2SAM 的新型框架用于分割，该框架利用 Segment Anything Model (SAM) 和 Contrastive Language-Image Pre-Training (CLIP) 的优势来准确分割 CT 扫描中的左右肺，随后将这些分割后的输出输入 RACNet 以对 COVID-19 和非 COVID-19 病例进行分类。首先，SAM 为 CT 扫描中的每个切片生成多个基于部分的分割掩码；然后 CLIP 仅选择与感兴趣区域 (ROI) 相关的掩码，即左右肺；最后，将这些 ROI 作为提示提供给 SAM，并生成肺部的最终分割掩码。在两个 Covid-19 标注数据库中进行了实验，结果表明，当我们的方法用于 CT 扫描分割时，性能得到了提高。||
|**2024-07-22**|[HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning](http://arxiv.org/abs/2407.15680)|**[link](https://github.com/google/haloquest)**|幻觉一直是大语言模型的一个主要问题，并且在多模态方面仍然是一个关键挑战，因为视觉语言模型 (VLM) 不仅要处理文本输入，还要处理视觉输入。尽管 VLM 取得了快速进展，但用于评估和解决多模态幻觉的资源仍然有限，并且主要集中在评估上。这项工作介绍了 HaloQuest，这是一个新颖的视觉问答数据集，它捕获了多模态幻觉的各个方面，例如错误前提、上下文不足和视觉挑战。HaloQuest 的一个新颖之处在于利用合成图像（除了真实图像）来实现大规模数据集创建。HaloQuest 包含超过 7.7K 个示例，涵盖各种类别，旨在成为 VLM 的挑战性基准和推进多模态推理的微调数据集。我们的实验表明，当前的模型难以处理 HaloQuest，所有开源 VLM 的准确率都低于 36%。另一方面，在 HaloQuest 上进行微调可以显着降低幻觉率，同时保持标准推理任务的性能。我们的结果发现，使用生成图像进行基准测试与真实图像高度相关（r=0.97）。最后但同样重要的是，我们提出了一种新的自动评估机制 Auto-Eval，该机制与人类评估员高度相关（r=0.99），用于评估 VLM。总而言之，这项工作在理解、评估和减轻 VLM 中的幻觉方面取得了具体进展，朝着未来更可靠的多模态人工智能系统迈出了重要一步。||
|**2024-07-22**|[In-Context Learning Improves Compositional Understanding of Vision-Language Models](http://arxiv.org/abs/2407.15487)|**[link](https://github.com/hoezey/vlm-compositionality)**|视觉语言模型 (VLM) 在大量下游任务中表现出非凡的能力。然而，由于训练数据中存在对象偏差，组合图像理解仍然是一项相当困难的任务。在这项工作中，我们通过对 VLM 中的组合理解进行广泛的基准测试来调查造成这种能力不足的原因。我们比较了对比模型和生成模型，并分析了它们在架构、预训练数据、训练任务和损失方面的差异。此外，我们利用上下文学习 (ICL) 作为一种方法，来提高 VLM 在给定图像的情况下执行更复杂推理和理解的能力。我们广泛的实验表明，我们提出的方法在多个组合理解数据集上的表现优于基线模型。||
|**2024-07-19**|[Multimodal Misinformation Detection using Large Vision-Language Models](http://arxiv.org/abs/2407.14321)|null|misinformation的日益泛滥及其惊人影响促使工业界和学术界都致力于开发 misinformation 检测和事实核查的方法。大型语言模型 (LLM) 的最新进展在各种任务中都表现出色，但 LLM 是否以及如何帮助 misinformation 检测仍有待探索。大多数现有的最先进方法要么不考虑证据而只关注与声明相关的特征，要么假设证据是提供的。很少有方法将证据检索视为 misinformation 检测的一部分，而是依赖于微调模型。在本文中，我们研究了 LLM 在零样本设置下进行 misinformation 检测的潜力。我们将证据检索组件纳入流程中，因为从各种来源收集相关信息对于检测声明的真实性至关重要。为此，我们提出了一种使用 LLM 和大型视觉语言模型 (LVLM) 进行多模态证据检索的新型重新排序方法。检索到的证据样本（图像和文本）用作基于 LVLM 的多模态事实验证方法 (LVLM4FV) 的输入。为了进行公平的评估，我们通过为图像和文本检索注释更完整的证据样本集，解决了现有证据检索数据集中证据样本的ground truth 不完整的问题。我们在两个数据集上的实验结果证明了所提出的方法在证据检索和事实验证任务方面的优越性，以及与监督基线相比更好的跨数据集泛化能力。||
|**2024-07-19**|[Patch-based Intuitive Multimodal Prototypes Network (PIMPNet) for Alzheimer's Disease classification](http://arxiv.org/abs/2407.14277)|**[link](https://github.com/desantilisa/PIMPNet3D/blob/main/README.md)**|类似结构性磁共振成像 (sMRI) 等体积神经影像学检查通常用于支持阿尔茨海默病 (AD) 等痴呆症的临床诊断。神经放射学家检查 3D sMRI 以检测和监测由 AD 引起的脑形态异常，例如整体和/或局部脑萎缩以及特征结构的形状改变。基于深度学习 (DL) 模型开发用于分析 sMRI 以诊断 AD 的诊断系统存在强烈的研究兴趣。然而，需要将从 sMRI 检查中提取的解剖信息与患者年龄一起解释，才能区分 AD 模式与正常衰老过程中的规律改变。在这种情况下，部分原型神经网络在“设计可解释”的架构中集成了深度学习的计算优势，并在医学影像应用中显示出可观的结果。我们介绍了 PIMPNet，这是第一个用于 3D 图像和人口统计数据的可解释多模态模型，应用于根据 3D sMRI 和患者年龄对 AD 进行二元分类。尽管与单模态模型相比，年龄原型没有提高预测性能，但这为模型设计和多模态原型训练过程的未来工作奠定了基础。||
|**2024-07-19**|[Words2Contact: Identifying Support Contacts from Verbal Instructions Using Foundation Models](http://arxiv.org/abs/2407.14229)|null|本文介绍了 Words2Contact，这是一个利用大型语言模型和视觉语言模型进行语言引导的多接触点放置流程。我们的方法是语言辅助遥操作和人机合作的关键组成部分，在这种情况下，人类操作员可以使用自然语言在全身伸展或操作之前指示机器人放置支撑接触点的位置。Words2Contact 将人类操作员的口头指令转换为接触点放置预测；它还处理迭代修正，直到人类对机器人在其视野中识别的接触位置感到满意为止。我们对最先进的 LLM 和 VLM 的规模和性能进行了基准测试，以进行接触预测。我们证明了迭代校正过程的有效性，表明即使是新手用户也可以快速学习如何指示系统获得准确的位置。最后，我们使用 Talos 人形机器人在现实世界的实验中验证了 Words2Contact，该机器人由人类操作员指示将支撑接触点放置在不同的位置和表面上，以避免在伸手去拿远处物体时跌倒。||
|**2024-07-19**|[Multi-modal Relation Distillation for Unified 3D Representation Learning](http://arxiv.org/abs/2407.14007)|null|近年来，面向三维点云的多模态预训练通过对齐三维形状及其对应的二维图像和语言描述中的异构特征，取得了令人瞩目的成果。然而，当前简单直接的解决方案往往忽略了样本之间复杂的结构关系，可能限制了多模态学习的全部潜力。为了解决这个问题，我们引入了多模态关系蒸馏（MRD），这是一个三模态预训练框架，旨在有效地将可靠的大型视觉语言模型（VLM）蒸馏到三维骨干网络中。MRD旨在捕获每个模态内的内部关系以及不同模态之间的交叉关系，并生成更具判别性的三维形状表示。值得注意的是，MRD在下游零样本分类任务和跨模态检索任务中取得了显著改进，实现了新的最先进性能。||
|**2024-07-18**|[Which objects help me to act effectively? Reasoning about physically-grounded affordances](http://arxiv.org/abs/2407.13811)|null|为了与开放世界进行有效的交互，机器人应该理解与已知和未知物体的交互如何帮助它们实现目标。这种理解的一个关键方面在于检测物体的可供性，它表示通过以各种方式操纵物体可以实现的潜在效果。我们的方法利用大型语言模型（LLM）和视觉语言模型（VLM）的对话来实现开放世界可供性检测。给定开放词汇描述的预期动作和效果，可以找到环境中有用的物体。通过将我们的系统基于物理世界，我们考虑了机器人的具身性和它遇到的物体的内在属性。在我们的实验中，我们已经证明，我们的方法可以根据不同的具身性或预期效果产生定制的输出。该方法能够从一组干扰项中选择一个有用的物体。微调 VLM 的物理属性提高了整体性能。这些结果强调了将可供性搜索基于物理世界的重要性，要考虑到机器人的具身性和物体的物理属性。||
|**2024-07-18**|[Visual Haystacks: Answering Harder Questions About Sets of Images](http://arxiv.org/abs/2407.13766)|**[link](https://github.com/visual-haystacks/vhs_benchmark)**|近年来，大型多模态模型 (LMM) 在单图像视觉问答领域取得了重大进展。然而，当面对需要跨越大量图像集合的查询时，这些模型面临着巨大的挑战，类似于现实世界中的场景，如在大型相册中搜索、在互联网上查找特定信息或通过卫星图像监测环境变化。本文探讨了多图像视觉问答 (MIQA) 任务：给定一组大型图像和一个自然语言查询，任务是生成一个相关且有依据的答案。我们提出了一个新的公共基准测试集，称为“视觉草垛 (VHs)”，专门用于评估 LMM 在非相关图像集上进行视觉检索和推理的能力，我们在该基准测试集上进行了全面的评估，结果表明，即使是强大的闭源模型也难以胜任。为了解决这些缺陷，我们引入了 MIRAGE（多图像检索增强生成），这是一种专为 LMM 量身定制的新型检索/问答框架，它以显著的效率和准确率提升来应对 MIQA 的挑战，超越了基线方法。我们的评估表明，MIRAGE 在 VHs 基准测试集上的表现优于闭源 GPT-4o 模型高达 11%，并且与以文本为中心的多阶段方法相比，效率提升高达 3.4 倍。||
|**2024-07-18**|[BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models](http://arxiv.org/abs/2407.13442)|null|视觉语言模型 (VLM) 通过视觉编码器和大型语言模型 (LLM) 的组合来感知世界。视觉编码器在大规模视觉文本数据集上进行预训练，提供对视觉数据的零样本泛化能力，而 LLM 则赋予 VLM 高推理能力。这使得 VLM 无需微调即可在广泛的基准测试中实现高性能，展现出零样本或少样本学习能力。然而，最近的研究表明，VLM 容易出现幻觉。这种不良行为会降低可靠性和可信度，从而使用户无法完全信任 VLM 的输出。为了增强可信度并更好地解决 VLM 的幻觉问题，我们整理了一个新的评估数据集，称为前后幻觉数据集 (BEAF)，并引入了新的指标：真实理解 (TU)、无知 (IG)、固执 (SB) 和犹豫不决 (ID)。与之前只关注构建问答的工作不同，我们基准测试的关键思想是通过图像编辑模型操纵视觉场景信息，并根据场景变化设计指标。这使我们能够通过观察感知变化的能力来清楚地评估 VLM 是否正确理解了给定的场景。我们还凭借我们的双轴视图（视觉和文本）可视化了图像级的对象关系。通过使用我们的数据集评估 VLM，我们观察到我们的指标揭示了 VLM 幻觉的不同方面，这些方面以前从未被报道过。项目页面：\url{https://beafbench.github.io/}||
|**2024-07-17**|[R+X: Retrieval and Execution from Everyday Human Videos](http://arxiv.org/abs/2407.12957)|null|我们提出了R+X，这是一个框架，它使机器人能够从人类执行日常任务的长篇幅、未标记的第一人称视频中学习技能。在接收到人类的语言指令后，R+X首先检索包含相关行为的短视频片段，然后通过在这种行为上调整上下文内模仿学习方法来执行技能。通过利用视觉语言模型 (VLM) 进行检索，R+X 不需要对视频进行任何手动注释，并且通过利用上下文内学习进行执行，机器人可以立即执行指令的技能，而无需在检索到的视频上进行一段时间的训练。对一系列日常家居任务的研究表明，R+X 成功地将未标记的人类视频转化为强大的机器人技能，并且 R+X 的性能优于最近的几种替代方法。视频可在 https://www.robot-learning.uk/r-plus-x 上获取。||
|**2024-07-16**|[ChatBCG: Can AI Read Your Slide Deck?](http://arxiv.org/abs/2407.12875)|null|像 GPT4o 和 Gemini Flash 这样的多模态模型在推理和摘要任务方面表现出色，其性能接近人类水平。然而，我们发现，当被要求执行非常具体的“阅读和估计”任务时，尤其是在商务演示文稿中的视觉图表环境下，这些模型的表现不如人类。本文评估了 GPT 4o 和 Gemini Flash-1.5 在回答关于标记图表（数据在图表上清晰标注）和未标记图表（数据未清晰标注，必须从 X 轴和 Y 轴推断）数据的简单问题时的准确性。我们得出结论，如果演示文稿包含任何复杂或未标记的图表，这些模型目前无法准确地端到端地阅读演示文稿。即使用户创建的演示文稿仅包含标记图表，该模型也只能完美地端到端地阅读 15 个标记图表中的 7-8 个。有关幻灯片演示文稿中的所有图表，请访问 https://www.repromptai.com/chat_bcg||
|**2024-07-17**|[E5-V: Universal Embeddings with Multimodal Large Language Models](http://arxiv.org/abs/2407.12580)|**[link](https://github.com/kongds/e5-v)**|多模态大型语言模型 (MLLM) 在通用视觉和语言理解方面已展现出巨大的潜力。然而，使用 MLLM 进行多模态信息表示的研究仍然相对较少。在这项工作中，我们介绍了一个名为 E5-V 的新框架，旨在使 MLLM 适应实现通用的多模态嵌入。我们的研究结果表明，与先前的方法相比，MLLM 在表示多模态输入方面具有巨大潜力。通过利用带有提示的 MLLM，E5-V 有效地弥合了不同类型输入之间的模态差距，即使在没有微调的情况下也能在多模态嵌入方面表现出色。我们为 E5-V 提出了一种单模态训练方法，其中模型仅在文本对上进行训练。这种方法与传统的图文对多模态训练相比有了显著改进，同时将训练成本降低了约 95%。此外，这种方法还消除了对昂贵的多模态训练数据收集的需求。跨四种类型任务的大量实验证明了 E5-V 的有效性。作为一个通用的多模态模型，E5-V 不仅在每项任务中都达到了最先进的性能，而且往往还超越了它，尽管它只在单一模态上进行了训练。||
|**2024-07-17**|[VisionTrap: Vision-Augmented Trajectory Prediction Guided by Textual Descriptions](http://arxiv.org/abs/2407.12345)|null|预测其他道路参与者的未来轨迹对于自动驾驶汽车来说是一项至关重要的任务。现有的轨迹预测方法主要使用由检测和跟踪系统生成的代理轨迹和高清地图作为输入。在这项工作中，我们提出了一种新方法，该方法还结合了来自环视摄像头的视觉输入，允许模型利用人类凝视和手势、道路状况、车辆转向信号等视觉线索，这些线索在先前的方法中通常对模型是隐藏的。此外，我们使用由视觉语言模型 (VLM) 生成并由大型语言模型 (LLM) 细化的文本描述作为训练期间的监督，以指导模型从输入数据中学习什么。尽管使用了这些额外的输入，我们的方法实现了 53 毫秒的延迟，使其可用于实时处理，这明显快于具有类似性能的先前单代理预测方法。我们的实验表明，视觉输入和文本描述都有助于提高轨迹预测性能，我们的定性分析强调了模型如何能够利用这些额外的输入。最后，在这项工作中，我们创建并发布了nuScenes-Text数据集，该数据集通过为每个场景添加丰富的文本注释来增强已建立的nuScenes数据集，证明了利用VLM对轨迹预测的积极影响。我们的项目页面是https://moonseokha.github.io/VisionTrap/||
|**2024-07-17**|[ModalChorus: Visual Probing and Alignment of Multi-modal Embeddings via Modal Fusion Map](http://arxiv.org/abs/2407.12315)|null|多模态嵌入是视觉语言模型的基础，例如 CLIP 嵌入，这是使用最广泛的文本图像嵌入。然而，这些嵌入容易受到跨模态特征细微错位的 ảnh hưởng，导致模型性能下降和泛化能力减弱。为了解决这个问题，我们设计了 ModalChorus，一个用于视觉探测和多模态嵌入对齐的交互式系统。ModalChorus 主要提供一个两阶段的过程：1）使用模态融合映射（MFM）进行嵌入探测，这是一种新颖的参数化降维方法，它集成了度量和非度量目标以增强模态融合；2）嵌入对齐，允许用户交互式地表达点集和集合对齐的意图。将 CLIP 嵌入与现有降维（例如，t-SNE 和 MDS）和数据融合（例如，数据上下文图）方法进行定量和定性比较，证明了 MFM 在展示常见视觉语言数据集上的跨模态特征方面的优势。案例研究表明，ModalChorus 可以促进直观地发现错位并在从零样本分类到跨模态检索和生成的场景中进行有效的重新对齐。||
|**2024-07-17**|[VCP-CLIP: A visual context prompting model for zero-shot anomaly segmentation](http://arxiv.org/abs/2407.12276)|**[link](https://github.com/xiaozhen228/vcp-clip)**|近年来，诸如CLIP之类的大规模视觉语言模型在零样本异常分割（ZSAS）任务中展现出巨大潜力，它们利用统一模型，通过精心设计的文本提示直接检测任何未见过产品的异常。然而，现有方法通常假设待检测产品的类别是已知的，从而设置特定于产品的文本提示，这在数据隐私场景中难以实现。此外，即使是同一类型的产品，由于特定组件和生产过程的差异，也会表现出显著差异，这对文本提示的设计提出了重大挑战。为此，我们提出了一种基于CLIP的视觉上下文提示模型（VCP-CLIP），用于ZSAS任务。VCP-CLIP背后的见解是利用视觉上下文提示来激活CLIP的异常语义感知能力。具体来说，我们首先设计了一个Pre-VCP模块，将全局视觉信息嵌入到文本提示中，从而消除了对特定于产品的提示的需求。然后，我们提出了一个新颖的Post-VCP模块，利用图像的细粒度特征调整文本嵌入。在对10个真实工业异常分割数据集进行的广泛实验中，VCP-CLIP在ZSAS任务中取得了最先进的性能。代码可在https://github.com/xiaozhen228/VCP-CLIP获取。||
|**2024-07-16**|[Reflective Instruction Tuning: Mitigating Hallucinations in Large Vision-Language Models](http://arxiv.org/abs/2407.11422)|null|大型视觉语言模型 (LVLM) 在各种视觉语言任务中都表现出良好的性能。然而，它们仍然容易产生幻觉，生成与视觉内容或指令不一致的输出。虽然已经提出了各种缓解策略，但它们往往忽略了导致幻觉的一个关键因素：训练过程中缺乏细粒度推理监督。如果没有中间推理步骤，模型可能会在指令和响应之间建立肤浅的捷径，无法内化固有的推理逻辑。为了解决这一挑战，我们提出了反思指令调整，它将原理学习融入到视觉指令调整中。与以往仅从响应中学习的方法不同，我们的方法需要模型预测能够证明响应正确或错误的原因。这促使模型更深入地参与每个响应背后的细粒度推理，从而提高模型的推理能力。为了促进这种方法，我们提出了 REVERIE，这是第一个具有反思性原理注释的大规模指令调整数据集。REVERIE 包含 115k 个机器生成的推理指令，每个指令都经过精心注释，对应一对正确和混淆的响应，以及解释每个响应正确性或错误性背后的理由的全面原理。在多个 LVLM 基准测试上的实验结果表明，使用 REVERIE 数据集进行反思指令调整比基线模型产生了显著的性能提升，证明了从原理中反思的有效性。项目页面位于 https://zjr2000.github.io/projects/reverie。||
|**2024-07-16**|[Mask-Free Neuron Concept Annotation for Interpreting Neural Networks in Medical Domain](http://arxiv.org/abs/2407.11375)|**[link](https://github.com/ailab-kyunghee/mammi)**|近年来，深度神经网络的进步为辅助疾病诊断和医疗决策带来了希望。 然而，为了确保符合法规的AI模型决策过程透明，需要全面了解模型的内部运作机制。 然而，以往的方法严重依赖于昂贵的像素级标注数据集来解释模型，这在医学领域是一个很大的缺陷。 在本文中，我们提出了一种新的医学神经元概念标注方法，称为无掩码医学模型解释（MAMMI），以应对这些挑战。 通过使用视觉语言模型，我们的方法不再需要像素级掩码来进行神经元概念标注。 与其他解释方法相比，MAMMI 取得了优越的性能，证明了其在为医学图像分析中的神经元提供丰富表示方面的功效。 我们在 NIH 胸部 X 光片训练的模型上进行的实验验证了 MAMMI 的有效性，展示了其在医学领域实现透明临床决策的潜力。 代码可在 https://github.com/ailab-kyunghee/MAMMI 获取。||
|**2024-07-16**|[LaMI-DETR: Open-Vocabulary Detection with Language Model Instruction](http://arxiv.org/abs/2407.11335)|**[link](https://github.com/eternaldolphin/lami-detr)**|现有的开放词汇目标检测方法通过利用视觉语言模型 (VLM)（例如 CLIP）强大的开放词汇识别能力来增强性能。然而，出现了两个主要挑战：(1) 概念表示的缺陷，其中 CLIP 文本空间中的类别名称缺乏文本和视觉知识。(2) 对基本类别的过度拟合趋势，在从 VLM 到检测器的迁移过程中，开放词汇知识偏向于基本类别。为了应对这些挑战，我们提出了语言模型指令 (LaMI) 策略，该策略利用视觉概念之间的关系，并在一种简单而有效的类 DETR 检测器（称为 LaMI-DETR）中应用它们。LaMI 利用 GPT 来构建视觉概念，并利用 T5 来研究跨类别的视觉相似性。这些类别间关系改进了概念表示，并避免了对基本类别的过度拟合。全面的实验验证了我们的方法在相同的严格设置下优于现有方法，并且不依赖于外部训练资源。LaMI-DETR 在 OV-LVIS 上实现了 43.4 的罕见框 AP，超过了之前的最佳结果 7.8 个罕见框 AP。||
|**2024-07-16**|[Large Vision-Language Models as Emotion Recognizers in Context Awareness](http://arxiv.org/abs/2407.11300)|null|上下文感知情绪识别 (CAER) 是一项复杂且重要的任务，需要从各种上下文线索中感知情绪。先前的方法主要侧重于设计复杂的架构来从图像中提取情绪线索。然而，它们的知识仅限于特定的训练数据集，并且可能反映了标注者主观的情绪偏差。此外，在现实应用中获取大量标记数据通常具有挑战性。在本文中，我们系统地探索了利用大型视觉语言模型 (LVLM) 从三个范式赋能 CAER 任务的潜力：1) 我们在两个 CAER 数据集上微调 LVLM，这是将大型模型迁移到下游任务的最常见方式。2) 我们设计了零样本和少样本模式来评估 LVLM 在数据有限甚至完全不可见的情况下的性能。在这种情况下，我们提出了一个免训练框架来充分利用 LVLM 的上下文学习 (ICL) 能力。具体来说，我们开发了一种基于图像相似度的排序算法来检索示例；随后，将指令、检索到的示例和测试示例组合起来输入 LVLM 以获得相应的情绪判断。3) 为了利用 LVLM 丰富的知识库，我们将思维链 (CoT) 纳入我们的框架，以增强模型的推理能力并提供可解释的结果。广泛的实验和分析表明，LVLM 在不同范式下的 CAER 任务中均取得了具有竞争力的性能。值得注意的是，少样本设置下的优越性能表明 LVLM 在无需大量训练的情况下完成特定任务的可行性。||
|**2024-07-15**|[OpenPSG: Open-set Panoptic Scene Graph Generation via Large Multimodal Models](http://arxiv.org/abs/2407.11213)|null|全景场景图生成 (PSG) 旨在分割对象并识别它们之间的关系，从而实现对图像的结构化理解。先前的方法侧重于预测预定义的对象和关系类别，因此限制了它们在开放世界场景中的应用。随着大型多模态模型 (LMM) 的快速发展，开放集目标检测和分割取得了重大进展，但 PSG 中的开放集关系预测仍未得到探索。在本文中，我们专注于与预训练的开放集全景分割模型相结合的开放集关系预测任务，以实现真正的开放集全景场景图生成 (OpenPSG)。我们的 OpenPSG 利用 LMM 以自回归的方式实现开放集关系预测。我们引入了一个关系查询转换器来有效地提取对象对的视觉特征并估计它们之间关系的存在。后者可以通过过滤不相关的对来提高预测效率。最后，我们设计了生成和判断指令，以在 PSG 中以自回归的方式执行开放集关系预测。据我们所知，我们是第一个提出开放集 PSG 任务的人。大量实验表明，我们的方法在开放集关系预测和全景场景图生成方面实现了最先进的性能。代码可在 \url{https://github.com/franciszzj/OpenPSG} 获取。||
|**2024-07-15**|[Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?](http://arxiv.org/abs/2407.10956)|**[link](https://github.com/xlang-ai/spider2-v)**|数据科学和工程工作流程通常跨越多个阶段，从仓储到编排，使用BigQuery、dbt和Airbyte等工具。随着视觉语言模型（VLM）在多模态理解和代码生成方面的进步，基于VLM的代理可以通过生成SQL查询、Python代码和GUI操作来自动化这些工作流程。这种自动化可以提高专家的生产力，同时使大规模数据分析更容易获得。在本文中，我们介绍了Spider2-V，这是第一个专注于专业数据科学和工程工作流程的多模态代理基准测试，它包含494个真实计算机环境中的真实任务，并整合了20个企业级专业应用程序。这些任务源自现实世界的用例，评估多模态代理通过在企业数据软件系统中编写代码和管理GUI来执行数据相关任务的能力。为了平衡真实模拟和评估的简单性，我们致力于开发任务设置的自动配置，并为每个任务精心设计评估指标。此外，我们还为多模态代理提供了这些企业数据软件系统的全面文档。我们的实证评估表明，现有的最先进的基于LLM/VLM的代理无法可靠地自动化完整的数据工作流程（成功率为14.0%）。即使在逐步指导下，这些代理在需要细粒度、知识密集型GUI操作（16.2%）和涉及远程云托管工作区（10.6%）的任务中仍然表现不佳。我们希望Spider2-V能够为自主多模态代理转变数据科学和工程工作流程的自动化铺平道路。我们的代码和数据可在https://spider2-v.github.io获取。||
|**2024-07-15**|[Benchmarking Vision Language Models for Cultural Understanding](http://arxiv.org/abs/2407.10920)|null|Foundation models and vision-language pre-training have notably advanced Vision Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their performance has been typically assessed on general scene understanding - recognizing objects, attributes, and actions - rather than cultural comprehension. This study introduces CulturalVQA, a visual question-answering benchmark aimed at assessing VLM's geo-diverse cultural understanding. We curate a collection of 2,378 image-question pairs with 1-5 answers per question representing cultures from 11 countries across 5 continents. The questions probe understanding of various facets of culture such as clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on CulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of cultural understanding across regions, with strong cultural understanding capabilities for North America while significantly lower performance for Africa. We observe disparity in their performance across cultural facets too, with clothing, rituals, and traditions seeing higher performances than food and drink. These disparities help us identify areas where VLMs lack cultural understanding and demonstrate the potential of CulturalVQA as a comprehensive evaluation set for gauging VLM progress in understanding diverse cultures.||
|**2024-07-15**|[GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via VLM](http://arxiv.org/abs/2407.10870)|null|大型视觉语言模型（LVLM），例如生成式预训练 Transformer 4-omni（GPT-4o），是新兴的多模态基础模型，在医疗保健、工业和学术领域等无数应用中，作为强大的人工智能（AI）辅助工具具有巨大潜力。 尽管此类基础模型在各种通用任务中表现良好，但在没有微调的情况下，它们在专业任务中的能力通常有限。 然而，由于巨大的计算/内存/数据集要求，对大型基础模型进行全面微调具有挑战性。 我们证明，即使没有微调，GPT-4o 也可以解码来自前臂超声数据的  手势，并且可以通过少量样本的上下文学习得到改进。||
|**2024-07-15**|[FabGPT: An Efficient Large Multimodal Model for Complex Wafer Defect Knowledge Queries](http://arxiv.org/abs/2407.10810)|null|智能是推进集成电路（IC）制造的关键。大型多模态模型（LMM）的最新突破解锁了图像和文本理解的空前能力，促进了智能制造的发展。我们利用LMM的强大功能，推出了FabGPT，这是一种用于晶圆缺陷知识查询的定制化IC制造大型多模态模型。FabGPT在扫描电子显微镜（SEM）图像缺陷检测、根本原因分析以及提供有关制造工艺的专家问答（Q&A）方面表现出色。FabGPT匹配增强的多模态特征，自动检测复杂晶圆背景下的微小缺陷，并减少手动阈值设置的主观性。此外，所提出的调制模块和交互式语料库训练策略将晶圆缺陷知识嵌入到预训练模型中，有效地平衡了与缺陷知识和原始知识相关的问答查询，并减轻了模态偏差问题。对内部晶圆厂数据（SEM-WaD）的实验表明，我们的FabGPT在晶圆缺陷检测和知识查询方面实现了显著的性能提升。||
|**2024-07-16**|[Qwen2 Technical Report](http://arxiv.org/abs/2407.10671)|**[link](https://github.com/qwenlm/qwen2)**|This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning.   The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach.   To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.||
|**2024-07-12**|[Open Vocabulary Multi-Label Video Classification](http://arxiv.org/abs/2407.09073)|null|预训练的视觉语言模型（VLM）在开放词汇计算机视觉任务（例如图像分类、目标检测和图像分割）方面取得了重大进展。最近的一些工作集中于将VLM扩展到视频中的开放词汇单标签动作分类。然而，先前的方法在整体视频理解方面存在不足，整体视频理解需要在开放词汇环境中同时识别多个动作和实体（例如视频中的对象）的能力。我们将此问题表述为开放词汇多标签视频分类，并提出了一种方法来调整预训练的VLM（例如CLIP）以解决此任务。我们利用大型语言模型（LLM）为VLM提供关于类别标签的语义指导，通过两个关键贡献来提高其开放词汇性能。首先，我们提出了一种端到端可训练架构，该架构学习提示LLM为CLIP文本编码器生成软属性，使其能够识别新类别。其次，我们将时间建模模块集成到CLIP的视觉编码器中，以有效地对视频概念的时空动态进行建模，并提出了一种新颖的正则化微调技术，以确保在视频域中强大的开放词汇分类性能。我们广泛的实验结果证明了我们的方法在多个基准数据集上的有效性。||
|**2024-07-12**|[LAPT: Label-driven Automated Prompt Tuning for OOD Detection with Vision-Language Models](http://arxiv.org/abs/2407.08966)|**[link](https://github.com/ybzh/lapt)**|分布外 (OOD) 检测对于模型可靠性至关重要，因为它可以识别来自未知类的样本并减少由于意外输入造成的错误。视觉语言模型 (VLM)（例如 CLIP）通过集成多模态信息，正在成为用于 OOD 检测的强大工具。然而，此类系统的实际应用受到手动提示工程的挑战，这需要领域专业知识并且对语言细微差别很敏感。在本文中，我们介绍了标签驱动的自动提示调整 (LAPT)，这是一种减少手动提示工程需求的新型 OOD 检测方法。我们使用自动挖掘的分布内 (ID) 类名和负标签来开发感知分布的提示。通过图像合成和检索方法自动收集链接到这些类标签的训练样本，从而无需手动即可进行提示学习。我们利用简单的交叉熵损失进行提示优化，并采用跨模态和跨分布混合策略分别减少图像噪声和探索分布之间的中间空间。LAPT 框架自主运行，只需要 ID 类名作为输入，无需人工干预。通过大量实验，LAPT 始终优于手动制作的提示，为 OOD 检测树立了新标准。此外，LAPT 不仅增强了 ID 和 OOD 样本之间的区别，还提高了 ID 分类精度并增强了对协变量偏移的泛化鲁棒性，从而在具有挑战性的全谱 OOD 检测任务中获得了出色的性能。代码可在 \url{https://github.com/YBZh/LAPT} 获取。||
|**2024-07-11**|[CXR-Agent: Vision-language models for chest X-ray interpretation with uncertainty aware radiology reporting](http://arxiv.org/abs/2407.08811)|null|近年来，大型视觉语言模型在解释复杂图像和使用高级推理生成自然语言描述方面展现出潜力。医学本质上是多模态的，它结合了扫描图像和基于文本的病史来撰写报告，这使得它有利于从人工智能能力的飞跃中受益。我们评估了几个数据集和基准上公开可用的、最先进的、基础视觉语言模型在胸部 X 光片解释方面的性能。我们使用线性探针来评估各种组件的性能，包括 CheXagent 的视觉转换器和 Q-former，它们在许多不同的数据集上的表现优于行业标准 Torch X-ray Vision 模型，显示出强大的泛化能力。重要的是，我们发现视觉语言模型经常会自信地产生幻觉语言，这会减慢临床解释的速度。基于这些发现，我们开发了一种基于代理的视觉语言方法，用于报告生成，使用 CheXagent 的线性探针和 BioViL-T 的短语 grounding 工具，根据病理的可能性生成具有不确定性感知的放射学报告，并对其进行定位和描述。我们通过开发一个评估平台，与呼吸系统专家进行用户研究，使用 NLP 指标、胸部 X 光片基准和临床评估，彻底评估了我们的视觉语言代理。我们的结果表明，人工智能生成的报告在准确性、可解释性和安全性方面都有相当大的改进。我们强调分别分析正常和异常扫描结果的重要性。最后，我们强调需要更大的配对（扫描和报告）数据集以及数据增强，以解决在这些大型视觉语言模型中出现的过拟合问题。||
|**2024-07-11**|[HiRes-LLaVA: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models](http://arxiv.org/abs/2407.08706)|null|高分辨率输入使大型视觉语言模型 (LVLM) 能够识别更精细的视觉细节，从而增强其理解能力。为了降低高分辨率输入带来的训练和计算成本，一种有前景的方向是使用滑动窗口将输入切片成均匀的块，每个块都与经过良好训练的视觉编码器的输入大小相匹配。虽然这种切片策略效率很高，但它会导致原始输入的碎片化，即跨块丢失上下文信息和空间几何的连续性，从而对跨块上下文感知和位置特定任务的性能产生负面影响。为了克服这些缺点，我们引入了 HiRes-LLaVA，这是一种新颖的框架，旨在有效处理任何大小的高分辨率输入，而不会改变原始的上下文和几何信息。HiRes-LLaVA 包含两个创新组件：(i) SliceRestore 适配器，它将切片的块重建为其原始形式，通过下上采样和卷积层有效地提取全局和局部特征，以及 (ii) 自挖掘采样器，用于根据自身压缩视觉标记，在保留原始上下文和位置信息的同时减少训练开销。为了评估处理上下文碎片的能力，我们构建了一个新的基准测试 EntityGrid-QA，其中包含与边缘相关和与位置相关的任务。我们的综合实验表明 HiRes-LLaVA 在现有公共基准测试和 EntityGrid-QA 上的优越性，尤其是在面向文档的任务上，为处理高分辨率输入树立了新标准。||
|**2024-07-11**|[Robotic Control via Embodied Chain-of-Thought Reasoning](http://arxiv.org/abs/2407.08693)|null|学习机器人控制策略的一个关键限制是它们无法在训练数据之外进行泛化。最近关于视觉语言动作模型 (VLA) 的研究表明，使用大型互联网预训练视觉语言模型作为学习机器人策略的支柱可以显著提高其鲁棒性和泛化能力。然而，大型视觉语言模型在其他领域最令人兴奋的能力之一是它们能够通过复杂问题进行迭代推理。这种能力能否应用于机器人领域，使策略能够在采取行动之前通过推理给定任务来提高性能？由于可用的训练示例相对简单，因此简单地使用“思维链”(CoT) 风格的提示对于标准 VLA 的效果要差得多。此外，纯粹对子任务进行语义推理（这在常规 CoT 中很常见）对于需要根据感官观察和机器人状态进行推理的机器人策略来说是不够的。为此，我们为 VLA 引入了具身思维链推理 (ECoT)，其中我们训练 VLA 在预测机器人动作之前对计划、子任务、动作以及视觉基础特征（如对象边界框和末端执行器位置）执行多个推理步骤。我们设计了一个可扩展的管道，用于在大型机器人数据集上为 ECoT 生成合成训练数据。我们证明，ECoT 将 OpenVLA（当前最强大的开源 VLA 策略）在具有挑战性的泛化任务中的绝对成功率提高了 28%，而无需任何额外的机器人训练数据。此外，ECoT 使人类更容易解释策略的失败并使用自然语言纠正其行为。||
|**2024-07-11**|[Bootstrapping Vision-language Models for Self-supervised Remote Physiological Measurement](http://arxiv.org/abs/2407.08507)|null|基于面部视频的远程生理测量是一个很有前景的研究领域，它以非接触方式检测人体生命体征（例如心率、呼吸频率）。传统方法大多是有监督学习，需要收集大量的面部视频和同步记录的光电容积脉搏波 (PPG) 信号。为了解决这个问题，自监督学习最近受到了关注；然而，由于缺乏真实PPG信号，其性能有限。在本文中，我们提出了一种新颖的自监督框架，成功地将流行的视觉语言模型（VLM）集成到远程生理测量任务中。给定一段面部视频，我们首先使用不同的rPPG信号频率增强其正负视频样本。接下来，我们引入了一种面向频率的视觉文本对生成方法，通过从正负样本中仔细创建对比时空图，并设计适当的文本提示来描述它们信号频率的相对比率。我们采用预训练的VLM来提取这些形成的视觉文本对的特征，然后估计rPPG信号。我们开发了一系列生成性和对比性学习机制来优化VLM，包括文本引导的视觉地图重建任务、视觉文本对比学习任务以及频率对比和排序任务。总的来说，我们的方法首次将VLM应用于视觉和文本模态中频率相关知识的提取和对齐。在四个基准数据集上的大量实验表明，它明显优于现有的自监督方法。||
|**2024-07-11**|[Specialist vision-language models for clinical ophthalmology](http://arxiv.org/abs/2407.08410)|**[link](https://github.com/robbieholland/specialistvlms)**|临床医生需要花费大量时间查看医学影像，并以文本形式记录他们对患者诊断、转诊和治疗的发现。视觉语言模型 (VLM) 可以自动解读图像并将其发现总结为文本，在减轻临床工作量和增加患者获得高质量医疗服务的机会方面具有巨大潜力。虽然基础模型引起了医学界的极大兴趣，但尚不清楚它们的一般能力是否能转化为现实世界的临床效用。在这项工作中，我们发现，与执业眼科医生相比，基础 VLM 在对老年性黄斑变性 (AMD) 患者的护理至关重要的专家任务中表现明显不佳。为了解决这个问题，我们首先确定了基于图像的临床决策所需的基本能力，然后制定了一个课程，以有选择地训练 VLM 掌握这些技能。由此产生的模型 RetinaVLM 可以被指示编写报告，该报告在疾病分期（F1 分数为 0.63 对 0.11）和患者转诊（0.67 对 0.39）方面明显优于领先的基础医学 VLM 编写的报告，并且接近初级眼科医生的诊断性能（他们在各自任务上的得分分别为 0.77 和 0.78）。此外，在一项由两位具有长达 32 年经验的资深眼科医生参与的读者研究中，RetinaVLM 的报告被认为与具有长达 10 年经验的初级眼科医生编写的报告同样正确（78.6% 对 82.1%）和完整（均为 78.6%）。这些结果表明，我们基于课程的方法为将通用的基础医学 VLM 专业化以处理现实世界的临床任务提供了一个蓝图。||
|**2024-07-11**|[Enhancing Robustness of Vision-Language Models through Orthogonality Learning and Cross-Regularization](http://arxiv.org/abs/2407.08374)|null|像 CLIP 这类视觉语言模型 (VLM)  如何进行有效的微调以适应特定的下游任务正受到越来越多的关注。先前的工作主要集中在使用 prompt learning 来使 CLIP 适应各种下游任务，然而，当在小数据集上进行微调时，这种方法容易出现过拟合问题。在本文中，我们介绍了一种正交微调方法，可以有效地更新预训练权重，增强鲁棒性和泛化能力，同时进一步利用交叉正则化策略来保持 VLM 零样本泛化能力的稳定性，我们称之为 \textbf{\textit{OrthCR}}。具体来说，我们在 Transformer 架构中无缝地注入了可训练的正交矩阵，并使用 Cayley 参数化来强制执行正交约束，受益于范数保持特性，从而实现稳定和更快的收敛。为了减轻训练过程中与正交约束的偏差，我们进一步采用了一种交叉正则化策略，以旁路的方式利用初始预训练权重。此外，为了丰富下游任务的样本多样性，我们首先探索了 Cutout 数据增强技术，以促进高效的微调，并从正交学习的角度理解我们的方法如何提高特定下游任务的性能并保持泛化能力。除了现有的 prompt learning 技术之外，我们还进行了广泛的实验，以证明我们的方法能够明确地引导预训练权重空间来表示特定于任务的知识，并在\textit{base-to-base/base-to-new}、\textit{跨数据集迁移}和\textit{领域泛化}评估中表现出具有竞争力的泛化能力。||
|**2024-07-11**|[Explore the Potential of CLIP for Training-Free Open Vocabulary Semantic Segmentation](http://arxiv.org/abs/2407.08268)|**[link](https://github.com/leaves162/cliptrase)**|作为一种视觉语言模型，CLIP凭借其零样本能力极大地推进了开放词汇语义分割（OVSS）的发展。尽管取得了成功，但由于其最初的图像级对齐训练，其在OVSS中的应用面临着挑战，这影响了其在需要详细局部上下文的任务中的性能。我们的研究深入探讨了CLIP的[CLS]标记对图像块特征相关性的影响，揭示了“全局”图像块的主导地位阻碍了局部特征的区分。为了克服这个问题，我们提出了CLIPtrase，这是一种新颖的无需训练的语义分割策略，通过重新校准图像块之间的自相关性来增强局部特征感知。这种方法在分割精度和保持对象之间语义连贯性方面表现出显著的改进。实验表明，我们在9个分割基准测试中平均领先CLIP 22.3%，优于现有的最先进的无需训练方法。代码已在以下地址公开：https://github.com/leaves162/CLIPtrase。||
|**2024-07-11**|[AddressCLIP: Empowering Vision-Language Models for City-wide Image Address Localization](http://arxiv.org/abs/2407.08156)|**[link](https://github.com/xsx1001/addressclip)**|本研究介绍了社交媒体和摄影新闻带来的一个新问题，称为图像地址定位（IAL），旨在预测拍摄图像的可读文本地址。现有的两阶段方法涉及预测地理坐标并将其转换为人类可读的地址，这可能会导致歧义并且资源密集。相比之下，我们提出了一个名为 AddressCLIP 的端到端框架来解决语义更丰富的问题，它包含两个关键要素：i）图像-文本对齐，通过对比学习将图像与地址和场景描述对齐，以及 ii）图像-地理匹配，根据流形学习在空间距离方面约束图像特征。此外，我们还构建了三个来自匹兹堡和旧金山的不同规模的数据集，专门用于 IAL 问题。实验表明，我们的方法在所提出的数据集上取得了令人信服的性能，并且优于视觉语言模型的代表性迁移学习方法。此外，广泛的消融和可视化展示了所提出方法的有效性。数据集和源代码可在 https://github.com/xsx1001/AddressCLIP 获取。||
|**2024-07-10**|[RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization](http://arxiv.org/abs/2407.08044)|**[link](https://github.com/huangowen/rolora)**|低秩适应 (LoRA) 作为一种典型的参数高效微调 (PEFT) 方法，通过仅更新大型语言模型 (LLM) 中的一小部分权重，显著提高了训练效率。近年来，仅权重量化技术也被应用于 LoRA 方法，以减少微调的内存占用。然而，将权重-激活量化应用于 LoRA 流程的研究还不够充分，我们观察到性能大幅下降，这主要是由于激活异常值的存在。在这项工作中，我们提出了 RoLoRA，这是第一个基于 LoRA 的有效权重-激活量化方案。RoLoRA 利用旋转来消除异常值，并提出了旋转感知微调，以在旋转后的 LLM 中保留无异常值的特性。实验结果表明，在权重-激活设置中，RoLoRA 持续提高了低比特 LoRA 的收敛性和训练后量化的鲁棒性。我们在 LLaMA2-7B/13B、LLaMA3-8B 模型上评估了 RoLoRA，与 LoRA 基线相比，在常识推理任务上，4 位权重-激活量化的 LLaMA2-13B 的绝对精度提高了 29.5%。我们进一步证明了其在大型多模态模型 (LLaVA-1.5-7B) 上的有效性。代码可在 https://github.com/HuangOwen/RoLoRA 获取。||
|**2024-07-10**|[LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models](http://arxiv.org/abs/2407.07895)|**[link](https://github.com/LLaVA-VL/LLaVA-NeXT)**|视觉指令微调在增强大型多模态模型 (LMM) 的能力方面取得了重大进展。然而，现有的开放 LMM 主要集中在单图像任务上，它们在多图像场景中的应用仍有待探索。此外，先前的 LMM 研究分别解决了不同的场景，使得无法通过新兴功能泛化跨场景。为此，我们引入了 LLaVA-NeXT-Interleave，它在 LMM 中同时处理多图像、多帧（视频）、多视图（3D）和多patch（单图像）场景。为了实现这些功能，我们将交错数据格式视为通用模板，并使用 1,177.6k 个样本编译了 M4-Instruct 数据集，涵盖 4 个主要领域，包括 14 个任务和 41 个数据集。我们还策划了 LLaVA-Interleave Bench 以全面评估 LMM 的多图像性能。通过大量实验，LLaVA-NeXT-Interleave 在多图像、视频和 3D 基准测试中取得了领先的结果，同时保持了单图像任务的性能。此外，我们的模型还展现出一些新兴功能，例如跨不同设置和模态迁移任务。代码可在 https://github.com/LLaVA-VL/LLaVA-NeXT 获取。||
|**2024-07-10**|[Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs](http://arxiv.org/abs/2407.07775)|null|导航研究的一个难以实现的目标是构建一个能够理解包括自然语言和图像在内的多模态指令并执行有用导航的智能体。为了实现这一目标，我们研究了一类广泛适用的导航任务，我们称之为带有演示路径的多模态指令导航（MINT），其中环境先验是通过先前记录的演示视频提供的。视觉语言模型（VLM）的最新进展为实现这一目标指明了一条充满希望的道路，因为它展示了感知和推理多模态输入的能力。然而，VLM 通常被训练用于预测文本输出，如何最好地将其用于导航是一个开放的研究问题。为了解决 MINT 问题，我们提出了 Mobility VLA，这是一种分层的视觉-语言-动作（VLA）导航策略，它结合了长上下文 VLM 的环境理解和常识推理能力，以及基于拓扑图的鲁棒的低级导航策略。高级策略由一个长上下文 VLM 组成，它将演示路径视频和多模态用户指令作为输入，以在路径视频中找到目标帧。接下来，低级策略使用目标帧和离线构建的拓扑图在每个时间步长生成机器人动作。我们在一个 836 平方米的真实世界环境中评估了 Mobility VLA，结果表明，Mobility VLA 在以前未解决的多模态指令（例如“我应该把这个还到哪里？”）上具有很高的端到端成功率，同时手持一个塑料箱。||
|**2024-07-09**|[Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model](http://arxiv.org/abs/2407.07053)|**[link](https://github.com/zwq2018/multi-modal-self-instruct)**|虽然目前大多数大型多模态模型 (LMM) 已经可以理解自然场景和肖像照片，但它们对抽象图像（例如图表、地图或布局）的理解和视觉推理能力仍然相当初级。它们经常难以完成简单的日常任务，例如从时钟读取时间、理解流程图或使用路线图规划路线。鉴于此，我们设计了一种多模态自指示方法，利用大型语言模型及其代码能力，在日常场景中合成大量抽象图像和视觉推理指令。我们的策略轻松创建了一个包含 11,193 条指令的多模态基准测试，涵盖八个视觉场景：图表、表格、模拟地图、仪表盘、流程图、关系图、平面图和视觉谜题。这个由简单的线条和几何元素构建的基准测试暴露了大多数先进的 LMM（如 Claude-3.5-Sonnet 和 GPT-4o）在抽象图像理解、空间关系推理和视觉元素归纳方面的不足。此外，为了验证我们合成数据的质量，我们使用 62,476 条合成的图表、表格和路线图指令微调了一个 LMM。结果表明，图表理解和地图导航性能有所提高，也显示出对其他视觉推理任务的潜在好处。我们的代码可在以下网址获得：https://github.com/zwq2018/Multi-modal-Self-instruct。||
|**2024-07-09**|[Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action Localization](http://arxiv.org/abs/2407.07024)|**[link](https://github.com/hyunjs/stov-tal)**|时序动作定位 (TAL) 中的词汇量受到大规模标注数据集稀缺性的限制。为了解决这个问题，最近的研究结合了强大的预训练视觉语言模型 (VLM)，例如 CLIP，来执行开放词汇表 TAL (OV-TAL)。然而，与在大量图像/视频-文本对上训练的 VLM 不同，现有的 OV-TAL 方法仍然依赖于小型、完全标记的 TAL 数据集来训练动作定位器。在本文中，我们探索了使用未标记的 YouTube 视频进行 OV-TAL 自训练的可扩展性。我们的自训练方法包括两个阶段。首先，在人工标记的 TAL 数据集上训练一个类别无关的动作定位器，并使用它为未标记的视频生成伪标签。其次，将大规模伪标签数据集与人工标记的数据集相结合，以训练定位器。大量实验表明，在自训练中利用网络规模的视频可以显着增强动作定位器的泛化能力。此外，我们重点介绍了现有 OV-TAL 评估方案中存在的问题，并提出了一种新的评估方案。代码发布在 https://github.com/HYUNJS/STOV-TAL||
|**2024-07-09**|[CoLA: Conditional Dropout and Language-driven Robust Dual-modal Salient Object Detection](http://arxiv.org/abs/2407.06780)|**[link](https://github.com/ssecv/CoLA)**|深度/热信息有利于使用传统RGB图像检测显著性目标。然而，在双模态显著性目标检测（SOD）模型中，针对噪声输入和模态缺失的鲁棒性至关重要，但很少被研究。为了解决这个问题，我们引入了条件Dropout和语言驱动（CoLA）框架，该框架包含两个核心组件。1）语言驱动质量评估（LQA）：利用带有提示学习器的预训练视觉语言模型，LQA在不需要额外质量标注的情况下重新校准图像贡献。这种方法有效地减轻了噪声输入的影响。2）条件Dropout（CD）：一种学习方法，用于增强模型在模态缺失场景下的适应性，同时保持其在完整模态下的性能。CD作为一种插件式训练方案，将模态缺失视为条件，增强了各种双模态SOD模型的整体鲁棒性。大量实验表明，所提出的方法在模态完整和模态缺失条件下均优于最先进的双模态SOD模型。代码将在论文被接收后开源。||
|**2024-07-09**|[LVLM-empowered Multi-modal Representation Learning for Visual Place Recognition](http://arxiv.org/abs/2407.06730)|null|视觉位置识别 (VPR) 由于视角变化和外观变化很大，因此仍然具有挑战性。主流工作通过开发各种特征聚合方法将深度特征转换为稳健而紧凑的全局表示来应对这些挑战。不幸的是，在具有挑战性的条件下无法获得令人满意的结果。我们从一个新的角度出发，尝试通过融合图像数据和视觉场景的文本描述来构建具有判别性的全局表示。动机有两个：（1）当前的大型视觉语言模型 (LVLM) 在视觉指令跟随方面表现出非凡的涌现能力，因此提供了一种高效灵活的图像文本描述生成方式；（2）文本描述提供了对场景的高级理解，对环境变化表现出很强的鲁棒性。尽管很有前景，但利用 LVLM 构建多模态 VPR 解决方案在高效的多模态融合方面仍然具有挑战性。此外，LVLM 不可避免地会产生一些不准确的描述，这使得情况变得更加困难。为了应对这些挑战，我们提出了一种新颖的多模态 VPR 解决方案。它首先使预训练的视觉和语言基础模型适应 VPR，以提取图像和文本特征，然后将这些特征输入特征组合器以相互增强。作为主要组件，特征组合器首先提出了一个逐符号注意力块，以根据文本符号与图像数据的相关性自适应地重新校准文本符号，然后开发了一个高效的交叉注意力融合模块，以在不同模态之间传播信息。增强的多模态特征被压缩到特征描述符中以执行检索。实验结果表明，我们的方法在图像描述符维度明显较小的情况下，大大优于最先进的方法。||
|**2024-07-08**|[A Single Transformer for Scalable Vision-Language Modeling](http://arxiv.org/abs/2407.06438)|**[link](https://github.com/yangyi-chen/solo)**|我们提出了 SOLO，一个用于可扩展视觉语言建模的单一 Transformer 模型。目前的大型视觉语言模型 (LVLM)，例如 LLaVA，大多采用异构架构，将预训练的视觉编码器与大型语言模型 (LLM) 连接起来，以促进视觉识别和复杂推理。虽然通过相对轻量级的训练获得了显著的性能，但我们发现了四个主要的扩展性限制：(1) 视觉能力受到预训练视觉编码器的限制，这些编码器通常比 LLM 小一个数量级。(2) 异构架构使已建立的硬件和软件基础设施的使用变得复杂。(3) 对这种架构进行规模法则研究必须考虑三个独立的组件——视觉编码器、连接器和 LLM，这使得分析变得复杂。(4) 使用现有的视觉编码器通常需要遵循预定义的图像输入预处理规范，例如，通过将输入整形为固定分辨率的方形图像，这在处理和训练高分辨率图像或具有不寻常纵横比的图像时会遇到困难。像 SOLO 这样的统一单一 Transformer 架构有效地解决了 LVLMs 中的这些可扩展性问题；然而，它在现代环境中的有限采用可能是由于缺乏可靠的训练方法来平衡两种模态并确保数十亿级模型的稳定训练。在本文中，我们介绍了第一个用于开发 SOLO 的开源训练方法，SOLO 是一个使用中等学术资源的开源 7B LVLM。训练方法包括从 LLM 初始化、在 ImageNet 和网络规模数据上进行顺序预训练，以及在我们策划的高质量数据集上进行指令微调。在广泛的评估中，SOLO 表现出与 LLaVA-v1.5-7B 相当的性能，尤其是在视觉数学推理方面表现出色。||
|**2024-07-08**|[Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision](http://arxiv.org/abs/2407.06189)|**[link](https://github.com/orrzohar/Video-STaR)**|大型视觉语言模型 (LVLM) 的性能取决于其训练数据集的规模和质量。现有的视频指令调整数据集缺乏多样性，因为它们是通过提示大型语言模型使用视频字幕生成问答对而得出的，因此大多是描述性的。同时，存在许多具有不同标签和监督的标记视频数据集——然而，我们发现将它们集成到 LVLM 中并非易事。在此，我们提出了使用增强推理的视频自训练 (Video-STaR)，这是第一个视频自训练方法。Video-STaR 允许利用任何标记的视频数据集进行视频指令调整。在 Video-STaR 中，LVLM 在指令生成和微调之间循环，我们证明这 (I) 提高了一般视频理解能力，并且 (II) 使 LVLM 能够适应现有监督下的新型下游任务。在生成过程中，LVLM 被提示提出答案。然后过滤答案，只保留包含原始视频标签的答案，然后在生成的数据集上重新训练 LVLM。通过仅对包含正确视频标签的生成答案进行训练，Video-STaR 利用这些现有的视频标签作为视频指令调整的弱监督。我们的结果表明，经过 Video-STaR 增强后的 LVLM 在 (I) 常规视频问答（TempCompass 性能提高了 10%）和 (II) 下游任务（Video-STaR 将 Kinetics700-QA 的准确率提高了 20%，并将 FineDiving 上的动作质量评估提高了 15%）中均表现出更好的性能。||
|**2024-07-09**|[HyCIR: Boosting Zero-Shot Composed Image Retrieval with Synthetic Labels](http://arxiv.org/abs/2407.05795)|null|组合图像检索 (CIR) 旨在根据带有文本的查询图像检索图像。当前的零样本 CIR (ZS-CIR) 方法试图在不使用昂贵的三元组标记训练数据集的情况下解决 CIR 任务。然而，ZS-CIR 和三元组监督 CIR 之间的差距仍然很大。在这项工作中，我们提出了混合 CIR (HyCIR)，它使用合成标签来提高 ZS-CIR 的性能。提出了一种新的 CIR 标签合成流程 (SynCir)，其中只需要未标记的图像。首先，根据视觉相似度提取图像对。其次，基于视觉语言模型和 LLM 为每个图像对生成查询文本。第三，基于语义相似度在语言空间中进一步过滤数据。为了提高 ZS-CIR 的性能，我们提出了一种混合训练策略，可以同时使用 ZS-CIR 监督和合成 CIR 三元组。采用了两种对比学习方法。一种是使用大规模未标记图像数据集来学习具有良好泛化能力的图像到文本映射。另一种是使用合成的 CIR 三元组来学习 CIR 任务的更好映射。我们的方法在常见的 CIR 基准测试：CIRR 和 CIRCO 上实现了最先进的零样本性能。||
|**2024-07-07**|[Multimodal Language Models for Domain-Specific Procedural Video Summarization](http://arxiv.org/abs/2407.05419)|null|视频是一种强大的媒介，可以通过长格式教程传达思想、讲述故事和提供详细的说明。此类教程对于按照自己的节奏学习新技能非常有价值，但由于其长度和密集的内容，可能会让人不知所措。观众经常会寻找特定信息，例如精确的测量值或分步执行细节，因此必须有效地提取和总结关键片段。人们非常需要一个能够总结和检测长视频中的亮点的智能、时间敏感的视频助手。多模态大型语言模型的最新进展为开发此类助手提供了有希望的解决方案。我们的研究探索了使用多模态模型来增强特定领域内的视频摘要和分步指令生成。这些模型需要理解跨视频帧的动作之间的时间事件和关系。我们的方法侧重于微调 TimeChat，以提高其在特定领域（烹饪和医疗程序）中的性能。通过在特定领域的数据集（如烹饪领域的 Tasty 和医疗程序领域的 MedVidQA）上训练模型，我们旨在增强其生成简洁、准确的教学视频摘要的能力。我们整理并重构了这些数据集，以创建高质量的以视频为中心的指令数据。我们的研究结果表明，当在特定领域的程序数据上进行微调时，TimeChat 可以显着改善长格式视频中关键指令步骤的提取和总结。这项研究证明了专门的多模式模型通过提供针对每个领域的独特方面量身定制的个性化分步指导来协助完成实际任务的潜力。||
|**2024-07-07**|[Multimodal Prompt Learning with Missing Modalities for Sentiment Analysis and Emotion Recognition](http://arxiv.org/abs/2407.05374)|**[link](https://github.com/zrguo/MPLMM)**|多模态模型的发展显著推进了多模态情感分析和情绪识别。然而，在现实应用中，各种缺失模态情况的存在常常导致模型性能下降。本文提出了一种新颖的使用提示学习的多模态Transformer框架来解决模态缺失问题。我们的方法引入了三种类型的提示：生成提示、缺失信号提示和缺失类型提示。这些提示能够生成缺失的模态特征，并促进模态内和模态间信息的学习。通过提示学习，我们实现了可训练参数数量的大幅减少。我们提出的方法在所有评估指标上都明显优于其他方法。大量的实验和消融研究证明了我们方法的有效性和鲁棒性，展示了其有效处理缺失模态的能力。||
|**2024-07-07**|[WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks](http://arxiv.org/abs/2407.05291)|**[link](https://github.com/servicenow/workarena)**|大型语言模型 (LLM) 模仿人类智能的能力导致了基于 LLM 的自主代理的激增。尽管最近的 LLM 似乎能够根据用户指令进行计划和推理，但它们将这些能力应用于自主任务解决的有效性仍未得到充分探索。在企业环境中尤其如此，因为自动化代理有望产生重大影响。为了填补这一空白，我们提出了 WorkArena++，这是一个包含 682 个任务的新基准，这些任务对应于知识工作者日常执行的现实工作流程。WorkArena++ 旨在评估 Web 代理的计划、解决问题、逻辑/算术推理、检索和上下文理解能力。我们对最先进的 LLM 和视觉语言模型 (VLM) 以及人类工作者的实证研究表明，此类模型要成为工作场所中有用的助手面临着若干挑战。除了基准之外，我们还提供了一种机制，可以毫不费力地生成数千个真实观察/行动轨迹，这些轨迹可用于微调现有模型。总的来说，我们希望这项工作能够成为帮助社区朝着有能力的自主代理方向发展的一种有用资源。该基准可以在 https://github.com/ServiceNow/WorkArena/tree/workarena-plus-plus 找到。||
|**2024-07-05**|[Multimodal Classification via Modal-Aware Interactive Enhancement](http://arxiv.org/abs/2407.04587)|null|由于存在臭名昭著的模态不平衡问题，多模态学习（MML）会导致优化不平衡现象，从而难以达到令人满意的性能。最近，一些具有代表性的方法被提出用于提高性能，主要集中在自适应调整每个模态的优化，以重新平衡主导模态和非主导模态的学习速度。为了更好地促进多模态学习中模型信息的交互，在本文中，我们提出了一种新的多模态学习方法，称为模态感知交互增强（MIE）。具体来说，我们首先利用基于锐度感知最小化（SAM）的优化策略在前向阶段平滑学习目标。然后，借助SAM的几何特性，我们提出了一种梯度修正策略，在反向阶段施加不同模态之间的影响。因此，我们可以提高泛化能力，同时缓解多模态学习中的模态遗忘现象。在广泛使用的数据集上进行的大量实验表明，我们提出的方法可以优于各种最先进的基线，以实现最佳性能。||
|**2024-07-04**|[MiniGPT-Med: Large Language Model as a General Interface for Radiology Diagnosis](http://arxiv.org/abs/2407.04106)|**[link](https://github.com/vision-cair/minigpt-med)**|近年来，人工智能 (AI) 的快速发展为医疗保健领域带来了重大的突破，特别是在诊断程序的改进方面。然而，以往的研究往往局限于有限的功能。本研究介绍了 MiniGPT-Med，这是一个源于大规模语言模型并专为医疗应用而设计的视觉语言模型。MiniGPT-Med 在各种成像模式（包括 X 光、CT 扫描和 MRI）中均表现出非凡的多功能性，从而增强了其实用性。该模型能够执行医学报告生成、视觉问答 (VQA) 以及医学图像疾病识别等任务。它对图像和文本临床数据的集成处理显著提高了诊断准确性。我们的实证评估证实，MiniGPT-Med 在疾病定位、医学报告生成和 VQA 基准测试中均表现出色，这标志着在缩小放射学实践辅助差距方面迈出了重要一步。此外，它在医学报告生成方面达到了最先进的性能，比之前的最佳模型提高了19%的准确率。MiniGPT-Med 有望成为放射学诊断的通用接口，从而提高各种医学影像应用的诊断效率。||
|**2024-07-04**|[Fully Fine-tuned CLIP Models are Efficient Few-Shot Learners](http://arxiv.org/abs/2407.04003)|null|提示调优通过训练一小部分参数，可以有效地增强预训练视觉语言模型 (VLM) 在下游任务上的性能。然而，当将调优后的模型应用于不同的数据集或领域时，它们往往会牺牲灵活性和适应性。在本文中，我们探索了通过精细微调整个 VLM 来捕获特定任务信息的可能性，同时最大限度地减少参数调整。在有限的监督下对特定任务进行整个 VLM 微调时，过拟合和灾难性遗忘成为事实上的因素。为了缓解这些问题，我们提出了一个名为 CLIP-CITE 的框架，通过设计一个判别性的视觉-文本任务，进一步以监督的方式对齐视觉-文本语义，并集成知识蒸馏技术来保留获得的知识。在少样本学习、基础到新泛化、域泛化和跨域泛化设置下的广泛实验结果表明，我们的方法在有限监督下有效地提高了特定任务的性能，同时保留了 VLM 在其他数据集上的通用性。||
|**2024-07-04**|[Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks](http://arxiv.org/abs/2407.03967)|**[link](https://github.com/amitkparekh/cogelot)**|仅根据多模态模型在分布外数据上的性能来评估其泛化能力，无法捕捉其真正的鲁棒性。本研究引入了一个全面的评估框架，系统地检验了指令和输入在这些模型泛化能力中的作用，并考虑了架构设计、跨语言和视觉模态的输入扰动以及任务复杂性的增加。所提出的框架揭示了多模态模型对极端指令扰动的弹性和它们对观察变化的脆弱性，引发了对过度拟合虚假相关性的担忧。通过在当前基于 Transformer 的机器人操作任务多模态模型上应用此评估框架，我们发现了局限性，并建议未来的改进应侧重于架构和训练创新，以更好地整合多模态输入，通过优先考虑对输入内容的敏感性而不是偶然的相关性来增强模型的泛化能力。||
|**2024-07-04**|[Concept Bottleneck Models Without Predefined Concepts](http://arxiv.org/abs/2407.03921)|null|近年来，可解释的概念型模型，如概念瓶颈模型 (CBM)，引起了人们的广泛兴趣。这类模型首先预测人类可解释的概念，然后将这些概念映射到输出类别。为了减少对人工标注概念的依赖，最近的研究工作已将预训练的黑盒模型后验地转换为可解释的 CBM。然而，这些方法预先定义了一组概念，假设黑盒模型在其表示中编码了哪些概念。在这项工作中，我们通过利用无监督概念发现来自动提取概念，从而消除了这一假设，无需人工标注或预定义的概念集。我们进一步引入了一种依赖于输入的概念选择机制，以确保在所有类别中仅使用一小部分概念。我们证明，我们的方法提高了下游性能，并缩小了与黑盒模型的性能差距，同时在分类中使用的概念要少得多。最后，我们演示了大型视觉语言模型如何干预最终的模型权重以纠正模型错误。||
|**2024-07-04**|[M $\mathbf5$ -- A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks](http://arxiv.org/abs/2407.03791)|null|自ChatGPT发布以来，自然语言处理领域经历了快速发展，特别是在大型语言模型（LLM）及其多模态对应物大型多模态模型（LMM）方面。尽管LLM具有令人印象深刻的能力，但正如各种纯文本基准测试所证明的那样，LLM在不同语言和文化背景下 often 表现出显著的性能差异。然而，目前的研究缺乏针对多模态视觉语言环境的此类基准。为了弥补这一差距，本研究引入了M5，这是第一个旨在评估多语言和多文化背景下不同视觉语言任务的LMM的综合基准。M5包括涵盖五个任务和41种语言的八个数据集，重点关注代表性不足的语言和文化多样化的图像。此外，我们还介绍了两个新的数据集，M5-VGR和M5-VLOD，其中包括一项新的视觉语言异常检测任务，在该任务中，所有评估的开源模型都未能显著超过随机基线。通过广泛的评估和分析，我们重点强调了资源丰富语言和资源匮乏语言之间存在巨大的、与任务无关的性能差异。此外，我们还发现，在多语言环境中，更大的模型不一定优于较小的模型。||
|**2024-07-04**|[Meta-optimized Angular Margin Contrastive Framework for Video-Language Representation Learning](http://arxiv.org/abs/2407.03788)|null|数据质量是决定视频-语言表示学习效果的首要因素。然而，以往数据中的视频-文本对通常不能完美对齐，这可能导致视频-语言表示不能准确反映跨模态语义。此外，以往数据还存在概念分布不均匀的问题，从而影响了在不受欢迎主题上的下游性能。为了解决这些问题，我们提出了一个带有减法角度边际的对比目标函数，以规范跨模态表示，使其达到完美的相似性。此外，为了适应非均匀的概念分布，我们提出了一个多层感知器（MLP）参数化的加权函数，将损失值映射到样本权重，从而能够在整个训练过程中动态调整模型的关注点。在少量无偏元数据的指导下，并通过大型视觉-语言模型生成的视频-文本数据进行增强，我们改进了视频-语言表示，并在常用的视频问答和文本-视频检索数据集上取得了优异的性能。||
|**2024-07-04**|[Visualizing Dialogues: Enhancing Image Selection through Dialogue Understanding with Large Language Models](http://arxiv.org/abs/2407.03615)|**[link](https://github.com/MiuLab/VisualDialog)**|近年来，对话系统的进步凸显了整合多模态响应的重要性，这种响应能够通过多种模态来传达信息，而不仅仅依赖于基于文本的交互。这种丰富性不仅提高了整体的交流效率，还增强了对话体验的质量。然而，现有的对话到图像检索方法由于预训练视觉语言模型 (VLM) 在准确理解复杂对话方面的局限性而面临挑战。为了解决这个问题，我们提出了一种新方法，利用大型语言模型 (LLM) 强大的推理能力来生成精确的对话相关视觉描述符，从而促进与图像的无缝连接。在基准数据上进行的大量实验验证了我们提出的方法在提取简洁准确的视觉描述符方面的有效性，从而显著提高了对话到图像检索的性能。此外，我们的研究结果证明了该方法在不同视觉线索、各种 LLM 和不同数据集上的泛化能力，突出了其在实际应用中的实用性和潜在影响。||
|**2024-07-04**|[Lateralization LoRA: Interleaved Instruction Tuning with Modality-Specialized Adaptations](http://arxiv.org/abs/2407.03604)|null|视觉语言模型 (VLM) 的最新进展导致了能够理解和生成交错图像和文本的视觉语言通用模型 (VLG) 的发展。尽管取得了这些进步，但 VLG 在遵循用户指令进行交错文本和图像生成方面仍然存在困难。为了解决这个问题，我们引入了 LeafInstruct，这是第一个开源的交错指令调整数据，包含跨 10 多个领域的 30,000 多个高质量实例。由于现有 VLG 的规模庞大，我们选择进行参数高效的调整。然而，我们观察到使用标准 LoRA 调整的 VLG 通常在交错文本图像生成中表现出较差的性能。我们将此问题归因于模态干扰和缺乏模态专用适应性设计。因此，我们提出了一种受大脑偏侧化概念启发的新型模态专用适应方法——Lateralization LoRA。Lateralization LoRA 采用混合方法，结合了传统的线性 LoRA 和用于生成文本和图像的卷积 LoRA，通过利用模态特定的结构和参数集来生成高质量的文本和图像。我们使用 LeafInstruct 数据集对 VLG（即 EMU2）进行 Lateralization LoRA 指令调整。大量实验表明，使用 Lateralization LoRA 调整的 EMU2 实现了最先进的性能，在复杂的交错任务中明显优于基线模型。||
|**2024-07-03**|[HEMM: Holistic Evaluation of Multimodal Foundation Models](http://arxiv.org/abs/2407.03418)|**[link](https://github.com/pliang279/hemm)**|能够全面处理文本、图像、视频、音频和其他感官模态的多模态基础模型正越来越多地应用于各种现实应用中。然而，考虑到可能存在的各种建模决策、任务和领域，描述和研究多模态基础模型的进展具有挑战性。在本文中，我们介绍了多模态模型的整体评估 (HEMM)，以系统地评估多模态基础模型在一组 3 个维度上的能力：基本技能、信息流和现实用例。基本的多模态技能是解决问题所需的内部能力，例如学习跨模态的交互、细粒度对齐、多步骤推理以及处理外部知识的能力。信息流研究多模态内容在任务期间如何通过查询、翻译、编辑和融合发生变化。用例涵盖了现实世界多媒体、情感计算、自然科学、医疗保健和人机交互应用中引入的特定领域挑战。通过对 HEMM 中 30 个任务的全面实验，我们 (1) 确定了对当今模型构成挑战的关键数据集维度（例如，基本技能、信息流和用例），以及 (2) 提炼了关于不同建模维度（例如，规模、预训练数据、多模态对齐、预训练和指令微调目标）如何影响性能的性能趋势。我们关于具有挑战性的多模态交互、用例以及需要推理和外部知识的任务、数据和模型规模的好处以及指令微调的影响的结论，为多模态基础模型的未来工作提供了可操作的见解。||
|**2024-07-03**|[Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation](http://arxiv.org/abs/2407.03056)|**[link](https://github.com/miccunifi/kdpl)**|视觉语言模型 (VLM) 在未见过的任务上表现出非凡的零样本泛化能力，但在有限数据下泛化到下游任务的性能不如监督方法。提示学习正在成为一种参数高效的 VLM 自适应方法，但最先进的方法需要带注释的样本。在本文中，我们提出了一种基于无监督知识蒸馏的新型提示学习方法，该方法从更强大的模型中提取知识。我们的方法称为知识蒸馏提示学习 (KDPL)，可以集成到现有的提示学习技术中，并消除了适应过程中对标记示例的需求。我们对十多个标准基准数据集进行的实验表明，KDPL 在提高学习提示的泛化能力方面非常有效，可以解决零样本域泛化、零样本跨数据集泛化和零样本基础到新类泛化问题。KDPL 不需要用于适应的基本事实标签，此外，我们还表明，即使在没有任何训练类名知识的情况下，它也可以用于有效地迁移知识。代码可在 https://github.com/miccunifi/KDPL 公开获取。||
|**2024-07-03**|[SAFT: Towards Out-of-Distribution Generalization in Fine-Tuning](http://arxiv.org/abs/2407.03036)|null|在机器学习领域，处理训练数据中的分布变化，即所谓的分布外 (OOD) 泛化，是一项重大挑战。虽然像 CLIP 这样的预训练视觉语言模型已经展现出卓越的零样本性能，但模型对下游任务的进一步适应会导致 OOD 数据出现不良的性能下降。在这项工作中，我们引入了用于微调的稀疏适应 (SAFT) 方法，该方法可以防止微调过程中遗忘预训练模型中的通用知识。SAFT 仅更新梯度幅度较大的一小部分重要参数，同时保持其他参数冻结。SAFT 易于实现且概念简单。大量实验表明，仅使用 0.1% 的模型参数，SAFT 就可以显著提高 CLIP 的性能。在多个基准测试中，它始终优于基线方法。在 ImageNet 及其变体的少样本学习基准测试中，在 OOD 设置下，SAFT 比传统的微调方法平均提高了 5.15% 的性能。||
|**2024-07-03**|[Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective](http://arxiv.org/abs/2407.02814)|null|在大型数据集上预训练的视觉语言模型 (VLM) 可能会通过将性别信息与特定对象或场景相关联而无意中学习到偏见。当前的方法侧重于修改输入并监控模型输出概率分数的变化，但往往难以从模型组件的角度全面理解偏见。我们提出了一个结合因果中介分析的框架，用于测量和映射 VLM 内偏见产生和传播的路径。这种方法使我们能够确定干预措施对模型偏差的直接影响，以及干预措施通过不同模型组件介导的对偏差的间接影响。我们的结果表明，图像特征是偏见的主要来源，其影响远高于文本特征，具体而言，在 MSCOCO 和 PASCAL-SENTENCE 数据集中分别占偏见的 32.57% 和 12.63%。值得注意的是，图像编码器的贡献超过了文本编码器和深度融合编码器。进一步的实验表明，语言和视觉模态的贡献是一致且不冲突的。因此，专注于模糊图像编码器中对模型偏见贡献最大的性别表征，可以有效地将 MSCOCO 和 PASCAL-SENTENCE 数据集中的偏见分别减少 22.03% 和 9.04%，而性能损失最小，计算量也没有增加。||
|**2024-07-03**|[MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context](http://arxiv.org/abs/2407.02730)|**[link](https://github.com/dongzizhu/medvh)**|大型视觉语言模型 (LVLM) 最近在自然图像和文本数据的各种任务中取得了优异的性能，这激发了大量关于 LVLM 微调和训练的研究。尽管取得了这些进步，但很少有研究关注这些模型在更小的数据集上微调时对幻觉的鲁棒性。在这项研究中，我们引入了一个新的基准数据集，即医学视觉幻觉测试 (MedVH)，用于评估特定领域 LVLM 的幻觉。 MedVH 包含五项任务，用于评估医学环境中 LVLM 的幻觉，其中包括全面理解文本和视觉输入以及生成长文本响应的任务。我们对通用 LVLM 和医学 LVLM 进行的广泛实验表明，尽管医学 LVLM 在标准医学任务中表现出良好的性能，但它们特别容易受到幻觉的影响，通常比通用模型更容易受到影响，这引发了人们对这些特定领域模型可靠性的严重担忧。为了使医学 LVLM 在实际应用中真正发挥价值，它们不仅必须准确地整合医学知识，还必须保持强大的推理能力以防止幻觉。我们的工作为未来对这些研究的评估铺平了道路。||
|**2024-07-02**|[Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models](http://arxiv.org/abs/2407.02716)|null|对预训练的视觉语言模型 (VLM) 进行微调已在医学图像和文本描述协同作用方面展现出卓越的能力。然而，许多预训练数据集受到患者隐私问题的限制，可能包含会对下游性能产生负面影响的噪声。此外，对多模态生成的日益依赖加剧了这个问题，因为它容易受到对抗性攻击。为了研究在对抗性噪声数据上训练的 VLM 如何在下游医学任务中执行，我们首先使用多模态对抗性攻击来制作噪声上游数据集。通过我们的综合分析，我们揭示了适度的噪声增强了模型的鲁棒性和可迁移性，但增加噪声水平会对下游任务性能产生负面影响。为了缓解这个问题，我们提出了校正对抗性噪声 (RAN) 框架，这是一种旨在有效防御对抗性攻击并在微调期间纠正上游噪声影响的方法。||
|**2024-07-02**|[D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data and eXpert model predictions](http://arxiv.org/abs/2407.02604)|null|大型视觉语言模型（VLM）已经从研究阶段发展到适用于通用用例的阶段，取得了令人难以置信的进步。LLaVA-Med 是一种开创性的生物医学大型语言和视觉助手，可以执行多模态生物医学图像和数据分析，为放射科医生提供自然语言界面。虽然它具有高度的通用性，并且可以处理多模态数据，但它目前受到大型语言模型领域现有挑战的限制。回复中的幻觉和不精确性可能导致误诊，这在目前阻碍了 VLM 的临床适应性。为了在医疗保健领域创建精确、用户友好的模型，我们提出了 D-Rax，这是一种特定领域、对话式的放射学辅助工具，可用于获取有关特定放射图像的见解。在这项研究中，我们增强了胸部 X 光（CXR）图像的对话分析，以支持放射学报告，提供来自医学成像的全面见解，并帮助制定准确的诊断。D-Rax 的实现是通过在我们策划的增强型指令跟随数据上微调 LLaVA-Med 架构来实现的，这些数据包括图像、指令以及从 MIMIC-CXR 成像数据、CXR 相关视觉问答 (VQA) 对和多个专家 AI 模型的预测结果中得出的疾病诊断和人口统计学预测。我们观察到，在对开放式和封闭式对话进行评估时，响应在统计学上都有显著改善。D-Rax 利用最先进的诊断模型与 VLM 相结合的力量，使临床医生能够使用自然语言与医学图像进行交互，这有可能简化他们的决策过程，提高诊断准确性并节省他们的时间。||
|**2024-07-02**|[Understanding Alignment in Multimodal LLMs: A Comprehensive Study](http://arxiv.org/abs/2407.02477)|null|偏好对齐已成为提升大型语言模型 (LLM) 性能的关键组成部分，但其对多模态大型语言模型 (MLLM) 的影响仍相对缺乏研究。与语言模型类似，用于图像理解任务的 MLLM 也面临着诸如幻觉之类的挑战。在 MLLM 中，幻觉不仅可以通过陈述错误的事实发生，还可以通过产生与图像内容不一致的响应来发生。MLLM 对齐的主要目标是鼓励这些模型使响应与图像信息更加一致。最近，多项工作引入了 MLLM 的偏好数据集，并研究了不同的对齐方法，包括直接偏好优化 (DPO) 和近端策略优化 (PPO)。然而，由于数据集、基础模型类型和对齐方法的不同，目前尚不清楚哪些具体因素对这些工作中报告的改进贡献最大。在本文中，我们独立分析了 MLLM 中偏好对齐的各个方面。我们首先将对齐算法分为两组，离线（如 DPO）和在线（如在线 DPO），并表明结合离线和在线方法可以在某些情况下提高模型的性能。我们回顾了各种已发布的多模态偏好数据集，并讨论了其构建细节如何影响模型性能。基于这些见解，我们引入了一种创建多模态偏好数据的新方法，称为偏差驱动幻觉采样 (BDHS)，它既不需要额外的注释也不需要外部模型，并表明它可以在各种基准测试中实现与先前发布的多模态模型对齐工作相当的性能。||
|**2024-07-02**|[Conceptual Codebook Learning for Vision-Language Models](http://arxiv.org/abs/2407.02350)|null|在本文中，我们提出了概念码本学习（CoCoLe），这是一种针对视觉语言模型（VLM）的新型微调方法，旨在解决在少量样本情况下对下游任务进行微调时提高VLM泛化能力的挑战。我们认识到，视觉概念（如纹理、形状和颜色）可以自然地跨域迁移，并且在泛化任务中发挥着至关重要的作用。受这一有趣发现的启发，我们学习了一个由视觉概念作为键、概念提示作为值的概念码本，它充当图像编码器输出和文本编码器输入之间的桥梁。具体来说，对于给定的图像，我们利用码本识别与类嵌入相关的最相关的概念提示，以执行分类。此外，我们还结合了一个手工制作的概念缓存作为正则化，以缓解低样本情况下出现的过拟合问题。我们观察到，这种概念码本学习方法能够增强视觉和语言模态之间的对齐。大量的实验结果表明，我们的CoCoLe方法在各种评估设置（包括从基础到新的泛化、跨数据集评估和域泛化任务）中都明显优于现有的最先进方法。详细的消融研究进一步证实了CoCoLe中每个组件的有效性。||
|**2024-07-02**|[Synthetic Multimodal Question Generation](http://arxiv.org/abs/2407.02233)|null|多模态检索增强生成 (MMRAG) 是一种强大的多模态文档问答方法。评估 MMRAG 的一个关键挑战是缺乏与目标问题风格和模态相匹配的高质量数据集。鉴于此，我们提出了 SMMQG，一个合成数据生成框架。SMMQG 利用检索器、大型语言模型 (LLM) 和大型多模态模型 (LMM) 之间的相互作用，直接从多模态文档中生成问答对，并使问题符合指定的风格和模态。我们使用 SMMQG 从维基百科文档中生成了一个包含 1024 个问题的 MMRAG 数据集，并使用该数据集评估了最先进的模型，揭示了只有通过特定风格和模态的评估数据才能获得的模型性能洞察。接下来，我们通过人工研究来衡量 SMMQG 产生的数据的质量。我们发现，我们的合成数据的质量与众包基准 MMQA 的质量相当，并且使用这两个数据集的下游评估结果非常一致。||
|**2024-07-02**|[Multi-Modal Video Dialog State Tracking in the Wild](http://arxiv.org/abs/2407.02218)|null|我们提出了 MST-MIXER，这是一个基于通用多模态状态跟踪方案的新型视频对话模型。目前声称能够执行多模态状态跟踪的模型在两个主要方面存在不足：(1) 它们要么只跟踪一种模态（主要是视觉输入），要么 (2) 它们针对的是不能反映现实世界复杂性的合成数据集。我们的模型解决了这两个限制，试图弥合这一关键的研究差距。具体来说，MST-MIXER 首先跟踪每个输入模态中最重要的成分。然后，它通过使用一种新颖的多模态图结构学习方法学习局部潜在图，从而预测每个模态所选成分缺失的底层结构。随后，将学习到的局部图和特征一起解析，形成一个在所有模态混合上运行的全局图，从而进一步细化其结构和节点嵌入。最后，利用细粒度的图节点特征来增强骨干视觉语言模型 (VLM) 的隐藏状态。MST-MIXER 在五个具有挑战性的基准测试中取得了新的最先进成果。||

## 6DOF Object Pose

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-07-16**|[NeuSurfEmb: A Complete Pipeline for Dense Correspondence-based 6D Object Pose Estimation without CAD Models](http://arxiv.org/abs/2407.12207)|**[link](https://github.com/ethz-asl/neusurfemb)**|当前6D物体姿态估计的最佳方法假设可以使用CAD模型，并且需要用户手动设置基于物理的渲染（PBR）流程来生成合成训练数据。这两个因素都限制了这些方法在现实场景中的应用。在这项工作中，我们提出了一个不需要CAD模型的流程，并且只需一小组真实图像作为输入，就可以训练出最先进的姿态估计器。我们的方法基于NeuS2物体表示，我们通过基于运动恢复结构（SfM）和物体无关分割的半自动化程序来学习该表示。我们利用NeuS2的新颖视图合成能力和简单的剪切粘贴增强技术来自动生成逼真的物体渲染，我们使用这些渲染来训练基于对应的SurfEmb姿态估计器。我们在LINEMOD-Occlusion数据集上评估了我们的方法，广泛研究了其各个组件的影响，并展示了其相对于基于CAD模型和PBR数据的方法的竞争性能。我们还展示了该流程在自收集的现实世界物体上的易用性和有效性，表明我们的方法优于最先进的无CAD模型方法，具有更好的精度和对轻度遮挡的鲁棒性。为了让机器人社区从该系统中受益，我们将在https://www.github.com/ethz-asl/neusurfemb公开发布该系统。||
|**2024-06-06**|[Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking](http://arxiv.org/abs/2406.04316)|null|6D物体姿态估计是计算机视觉中一项至关重要但极具挑战性的任务，其面临的主要问题是大规模数据集的严重缺乏。这种稀缺性阻碍了对模型性能的全面评估，限制了研究进展。此外，可用实例或类别的数量有限也限制了其应用。为了解决这些问题，本文介绍了Omni6DPose，这是一个以对象类别多样性、规模大和对象材料种类繁多为特征的大型数据集。Omni6DPose主要分为三个部分：ROPE（真实6D物体姿态估计数据集），包含332K张图像，涵盖149个类别中581个实例的超过150万个标注；SOPE（模拟6D物体姿态估计数据集），包含在混合现实环境中创建的475K张图像，通过深度模拟生成，涵盖与ROPE相同的149个类别中4162个实例的超过500万个标注；以及在ROPE和SOPE中都使用的手动对齐的真实扫描物体。由于存在大量的变化和歧义，Omni6DPose本身就极具挑战性。为了应对这一挑战，我们推出了GenPose++，它是SOTA类别级姿态估计框架的增强版本，融合了两项关键改进：语义感知特征提取和基于聚类的聚合。此外，我们还提供了全面的基准测试分析，以评估先前方法在这个大规模数据集上在6D物体姿态估计和姿态跟踪方面的性能。||
|**2024-06-05**|[Sparse Color-Code Net: Real-Time RGB-Based 6D Object Pose Estimation on Edge Devices](http://arxiv.org/abs/2406.02977)|null|随着机器人和增强现实应用越来越依赖于精确高效的6D物体姿态估计，边缘设备上的实时性能对于更具交互性和响应性的系统提出了要求。我们提出的稀疏颜色代码网络（SCCN）体现了一种清晰简洁的流程设计，可以有效地满足这一需求。SCCN利用基本物体几何特征的稀疏性来加速透视n点（PnP）计算过程，对RGB图像中的目标物体进行像素级预测。此外，它引入了一种新颖的基于像素级几何的物体对称表示，该表示与初始姿态预测无缝集成，有效地解决了对称物体的歧义问题。值得注意的是，SCCN在英伟达Jetson AGX Xavier平台上，对于基准LINEMOD数据集和遮挡LINEMOD数据集，分别实现了每秒19帧（FPS）和6帧的估计速率，同时在这些速率下始终保持较高的估计精度。||
|**2024-05-31**|[Deep Learning-Based Object Pose Estimation: A Comprehensive Survey](http://arxiv.org/abs/2405.07801)|**[link](https://github.com/cnjianliu/awesome-object-pose-estimation)**|物体姿态估计是计算机视觉中的一个基本问题，在增强现实和机器人技术中有着广泛的应用。在过去的十年中，深度学习模型由于其卓越的准确性和鲁棒性，已经逐渐取代了依赖于工程点对特征的传统算法。然而，当代方法仍然存在一些挑战，包括它们对标记训练数据的依赖性、模型紧凑性、在挑战性条件下的鲁棒性以及泛化到新颖的未见过物体。目前缺乏一项最新的综述来讨论该领域不同方面的进展、突出挑战和有希望的未来方向。为了填补这一空白，我们讨论了基于深度学习的物体姿态估计的最新进展，涵盖了该问题的所有三种形式，即实例级、类别级和未见过物体的姿态估计。我们的综述还涵盖了多种输入数据模态、输出姿态的自由度、物体属性和下游任务，为读者提供了对该领域的全面理解。此外，还讨论了不同领域的训练范式、推理模式、应用领域、评估指标和基准数据集，并报告了当前最先进方法在这些基准上的性能，从而方便读者根据自己的应用选择最合适的方法。最后，该综述指出了关键挑战，回顾了主要趋势及其优缺点，并指出了未来研究的有希望的方向。我们还会在 https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation 上持续跟踪最新工作。||
|**2024-03-28**|[Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation](http://arxiv.org/abs/2403.19527)|**[link](https://github.com/leeiieeo/ag-pose)**|类别级 6D 物体姿态估计旨在估计特定类别中未见过实例的旋转、平移和大小。在这一领域，基于密集对应的算法取得了领先的性能。然而，它们没有明确地考虑不同实例的局部和全局几何信息，导致对形状变化显著的未见过实例的泛化能力较差。为了解决这个问题，我们提出了一种新的实例自适应和几何感知关键点学习方法，用于类别级 6D 物体姿态估计 (AG-Pose)，它包括两个关键设计：（1）第一个设计是实例自适应关键点检测模块，它可以自适应地检测一组稀疏的关键点，用于表示各种实例的几何结构。(2) 第二个设计是几何感知特征聚合模块，它可以有效地将局部和全局几何信息整合到关键点特征中。这两个模块可以协同工作，为未见过的实例建立鲁棒的关键点级对应关系，从而增强模型的泛化能力。在 CAMERA25 和 REAL275 数据集上的实验结果表明，所提出的 AG-Pose 在没有类别特定形状先验的情况下，大幅度优于现有技术方法。||
|**2024-06-01**|[Object Pose Estimation via the Aggregation of Diffusion Features](http://arxiv.org/abs/2403.18791)|**[link](https://github.com/tianfu18/diff-feats-pose)**|从图像中估计物体姿态是3D场景理解的一项关键任务，最近的方法在非常大的基准测试中显示出良好的结果。然而，这些方法在处理未见过的物体时性能会显著下降。我们认为这是由于图像特征的泛化能力有限造成的。为了解决这个问题，我们深入分析了扩散模型（如Stable Diffusion）的特征，这些特征在对未见过的物体进行建模方面具有巨大潜力。基于这一分析，我们创新性地将这些扩散特征引入到物体姿态估计中。为此，我们提出了三种不同的架构，可以有效地捕获和聚合不同粒度的扩散特征，大大提高了物体姿态估计的泛化能力。我们的方法在三个流行的基准数据集LM、O-LM和T-LESS上，以相当大的优势优于现有技术水平的方法。特别是，我们的方法在未见过的物体上实现了比先前最佳技术更高的准确率：在Unseen LM上为98.2%对93.5%，在Unseen O-LM上为85.9%对76.3%，显示了我们方法强大的泛化能力。我们的代码发布在https://github.com/Tianfu18/diff-feats-pose。||
|**2024-03-24**|[KITchen: A Real-World Benchmark and Dataset for 6D Object Pose Estimation in Kitchen Environments](http://arxiv.org/abs/2403.16238)|null|尽管最近在用于机器人抓取的 6D 物体姿态估计方法方面取得了进展，但这些方法在现有数据集上的能力与其在现实世界移动操作任务中的效率之间仍然存在巨大的性能差距，特别是当机器人完全依赖其单目以自我为中心的视野 (FOV) 时。现有的现实世界数据集主要集中在桌面抓取场景，其中机械臂放置在固定位置，并且物体集中在固定外部相机视野的中心。评估此类数据集的性能可能无法准确反映在厨房环境中日常移动操作任务中遇到的挑战，例如从更高的架子、水槽、洗碗机、烤箱、冰箱或微波炉中检索物体。为了解决这一差距，我们提出了 KITchen，这是一个专门为估计厨房环境中不同位置的物体的 6D 姿态而设计的新基准。为此，我们记录了一个综合数据集，其中包含在一个人形机器人以自我为中心的视角下，在两个不同的厨房中捕获的 111 个厨房物体的约 205k 个真实世界 RGBD 图像。随后，我们开发了一个半自动注释管道，以简化此类数据集的标记过程，从而以最少的人力生成 2D 对象标签、2D 对象分割掩码和 6D 对象姿态。基准、数据集和注释管道可在 https://kitchen-dataset.github.io/KITchen 获取。||
|**2024-03-22**|[DITTO: Demonstration Imitation by Trajectory Transformation](http://arxiv.org/abs/2403.15203)|null|快速便捷地教授机器人新技能对于更广泛地采用机器人系统至关重要。在这项工作中，我们通过一个两阶段过程解决了从单个 RGB-D 视频记录中进行一次性模仿的问题。在第一阶段（离线阶段），我们提取演示的轨迹。这需要分割被操纵的物体并确定它们相对于次要物体（如容器）的相对运动。随后，在实时在线轨迹生成阶段，我们首先重新检测所有物体，然后将演示轨迹扭曲到当前场景，最后，我们用机器人跟踪轨迹。为了完成这些步骤，我们的方法利用了几个辅助模型，包括用于分割、相对物体姿态估计和抓取预测的模型。我们系统地评估了对应和重新检测方法的不同组合，以验证我们在一系列不同任务中的设计决策。具体来说，我们收集了十种不同任务的演示，包括拾放任务以及铰接物体操作。最后，我们在真实的机器人系统上进行了广泛的评估，以证明我们的方法在现实世界场景中的有效性和实用性。我们在 http://ditto.cs.uni-freiburg.de 上公开提供了代码。||
|**2024-03-21**|[Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation](http://arxiv.org/abs/2403.14559)|null|在二维图像中定位预定义的三维关键点是建立用于6自由度物体姿态估计的三维-二维对应关系的有效方法。然而，不可见关键点的不可靠定位结果会降低对应关系的质量。在本文中，我们通过定位可见性方面的关键点来解决这个问题。由于关键点可见性信息在当前的数据集收集过程中缺失，我们提出了一种有效的方法，可以从可用的物体级标注中生成二进制可见性标签，用于非对称物体和对称物体的关键点。我们进一步基于PageRank算法从二进制标签中推导出实值可见性感知重要性。利用我们可见性感知重要性的灵活性，我们通过将可见性感知重要性与最先进的姿态估计算法以及附加的位置编码相结合，构建了VAPO（可见性感知姿态估计器）。在流行的姿态估计基准上进行了广泛的实验，包括Linemod、Linemod-Occlusion和YCB-V。结果表明，VAPO改进了关键点对应关系和最终估计的姿态，并明显达到了最先进的性能。||
|**2024-03-18**|[GenFlow: Generalizable Recurrent Flow for 6D Pose Refinement of Novel Objects](http://arxiv.org/abs/2403.11510)|null|尽管基于学习的方法在6D物体姿态估计方面取得了进展，但对于新物体，精度和可扩展性之间的权衡仍然存在。具体来说，以前针对新物体的方法没有很好地利用目标物体的3D形状信息，因为它们侧重于通过间接处理形状来实现泛化，从而降低了效率。我们提出了GenFlow，这是一种在目标物体形状的指导下实现精度和对新物体泛化的方法。我们的方法预测渲染图像和观察图像之间的光流，并迭代地细化6D姿态。它通过3D形状约束和从端到端可微系统学习到的可泛化几何知识来提高性能。我们通过设计级联网络架构进一步改进了我们的模型，以利用多尺度相关性和从粗到精的细化。GenFlow在RGB和RGB-D情况下均在未见物体姿态估计基准测试中排名第一。它还实现了与现有的最先进的已见物体姿态估计方法相当的性能，而无需任何微调。||
|**2024-03-14**|[MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion](http://arxiv.org/abs/2403.09309)|null|杂乱的料箱拣选环境对姿态估计模型提出了挑战。尽管深度学习取得了令人瞩目的进步，但单视图 RGB 姿态估计模型在杂乱的动态环境中表现不佳。利用视频场景中包含的丰富时间信息有可能增强模型处理遮挡和环境动态性的不利影响的能力。此外，联合目标检测和姿态估计模型更适合利用任务的相互依赖性来提高两项任务的准确性。为此，我们提出了一种基于注意力的多目标 6D 姿态估计时间融合方法，该方法可以在视频序列的多个帧中积累信息。我们的 MOTPose 方法将一系列图像作为输入，并在一次前向传递中对所有目标执行联合目标检测和姿态估计。它学习使用基于交叉注意力的融合模块在多个时间步长上聚合目标嵌入和目标参数。我们在物理逼真的杂乱料箱拣选数据集 SynPick 和 YCB-Video 数据集上评估了我们的方法，并证明了改进的姿态估计精度以及更好的目标检测精度。||

## nerf

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-07-18**|[EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian Splatting](http://arxiv.org/abs/2407.13520)|null|近年来，随着神经辐射场 (NeRF) 和 3D 高斯散射 (3DGS) 的发展，3D 去模糊重建技术取得了显著进展。 尽管这些技术可以从模糊的图像输入中恢复相对清晰的 3D 重建，但它们在处理严重模糊和复杂相机运动方面仍然面临局限性。 为了解决这些问题，我们提出了事件辅助的 3D 高斯散射去模糊重建 (EaDeblur-GS)，它集成了事件相机数据以增强 3DGS 对运动模糊的鲁棒性。 通过使用自适应偏差估计器 (ADE) 网络来估计高斯中心偏差并使用新的损失函数，EaDeblur-GS 可以实时实现清晰的 3D 重建，其性能可与最先进的方法相媲美。||
|**2024-07-10**|[3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes](http://arxiv.org/abs/2407.07090)|null|基于粒子的辐射场表示法，例如 3D 高斯 splatting，在复杂场景的重建和重新渲染方面取得了巨大成功。大多数现有方法通过光栅化渲染粒子，将它们投影到屏幕空间图块中，以便按排序顺序进行处理。而这项工作则考虑对粒子进行光线追踪，构建边界体积层次结构，并使用高性能 GPU 光线追踪硬件为每个像素投射光线。为了有效处理大量半透明粒子，我们描述了一种专门的渲染算法，该算法使用边界网格封装粒子，以利用快速的光线三角形相交，并按深度顺序对成批相交进行着色。光线追踪的优势在计算机图形学中是众所周知的：处理用于阴影和反射等次级照明效果的非相干光线，从机器人技术中常见的高度扭曲的相机进行渲染，对光线进行随机采样等等。使用我们的渲染器，与光栅化相比，这种灵活性几乎不需要任何成本。实验结果证明了我们方法的速度和准确性，以及在计算机图形学和视觉方面的几种应用。我们进一步提出了对基本高斯表示的相关改进，包括简单使用广义核函数，这可以显著减少粒子命中次数。||
|**2024-07-07**|[GaussReg: Fast 3D Registration with Gaussian Splatting](http://arxiv.org/abs/2407.05254)|null|点云配准是大规模三维场景扫描和重建的基本问题。在深度学习的帮助下，配准方法得到了显著发展，已接近成熟阶段。随着神经辐射场（NeRF）的引入，它凭借强大的视图合成能力成为最流行的三维场景表示方法。对于NeRF表示，大规模场景重建也需要对其进行配准。然而，这一主题极度缺乏探索。这是由于对具有隐式表示的两个场景之间的几何关系进行建模存在固有挑战。现有方法通常将隐式表示转换为显式表示以进行进一步配准。最近，引入了高斯 splatting（GS），它采用显式三维高斯函数。这种方法在保持高质量渲染的同时，显著提高了渲染速度。给定两个具有显式GS表示的场景，在这项工作中，我们探索了它们之间的三维配准任务。为此，我们提出了 GaussReg，一种快速且准确的由粗到精的框架。粗略阶段遵循现有的点云配准方法，并估计来自GS的点云的粗略对齐。我们进一步提出了一种新的图像引导的精细配准方法，该方法从GS渲染图像，为精确对齐提供更详细的几何信息。为了支持全面评估，我们仔细构建了一个名为 ScanNet-GSReg 的场景级数据集，其中包含从 ScanNet 数据集中获得的1379个场景，并收集了一个名为 GSReg 的真实世界数据集。实验结果表明，我们的方法在多个数据集上实现了最先进的性能。我们的 GaussReg 比 HLoc（SuperPoint 作为特征提取器，SuperGlue 作为匹配器）快44倍，并且精度相当。||
|**2024-07-04**|[CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion Blur Images](http://arxiv.org/abs/2407.03923)|null|神经辐射场 (NeRFs) 因其高质量的新视角渲染能力而备受关注，促使人们对其在各种现实世界案例中的应用展开研究。其中一个关键挑战是相机在曝光时间内移动造成的运动模糊，这阻碍了对 3D 场景进行精确重建。在本研究中，我们提出了连续刚体运动感知高斯 splatting (CRiM-GS)，以实时渲染速度从模糊图像中重建精确的 3D 场景。考虑到实际的相机运动模糊过程包含复杂的运动模式，我们基于神经常微分方程 (ODE) 预测相机的连续运动。具体来说，我们利用刚体变换对相机运动进行建模，并进行适当的正则化，以保持物体的形状和大小。此外，我们在 \textit{SE(3)} 域中引入连续可变形 3D 变换，通过确保更高的自由度使刚体变换适应现实问题。通过重新审视基本相机理论并采用先进的神经网络训练技术，我们实现了对连续相机轨迹的精确建模。我们进行了广泛的实验，证明了该方法在基准数据集上的定量和定性性能均达到了最先进水平。||
|**2024-06-26**|[Trimming the Fat: Efficient Compression of 3D Gaussian Splats through Pruning](http://arxiv.org/abs/2406.18214)|null|近年来，由于神经辐射场和最近出现的3D高斯渲染（3DGS）模型提供了端到端训练的能力，3D模型的使用得到了推广。后者具有显著优势，因为它本身可以简化训练过程中的快速收敛并提供广泛的可编辑性。然而，尽管发展迅速，但关于这些模型的可扩展性的文献仍处于起步阶段。在本研究中，我们针对解决这一差距采取了一些初步措施，展示了一种能够实现此类模型的内存和计算可扩展性的方法。具体来说，我们提出了“Trimming the fat”，这是一种基于梯度的迭代式后剪枝技术，用于消除模型中编码的冗余信息。我们在广泛认可的基准测试集上的实验结果证明了我们方法的有效性，表明在保持甚至改进基线性能的同时，可以去除高达75%的高斯函数。我们的方法实现了约50倍的压缩，同时保持了与基线模型相似的性能，并且能够将计算速度提高到600帧/秒。||
|**2024-06-21**|[Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks](http://arxiv.org/abs/2406.15149)|null|仿真器是自主机器人学习的强大工具，因为它们提供了可扩展的数据生成、灵活的设计和轨迹优化。然而，将从仿真数据中学习到的行为迁移到现实世界中是非常困难的，通常需要通过计算量大的域随机化方法或进一步的模型微调来缓解。我们提出了一种方法来提高现实世界中视觉四旋翼导航任务的泛化能力和对分布变化的鲁棒性。为此，我们首先通过将高斯 splatting 与四旋翼飞行动力学相结合来构建模拟器，然后使用 Liquid 神经网络训练鲁棒的导航策略。通过这种方式，我们获得了一个全栈模仿学习协议，它结合了三维高斯 splatting 辐射场渲染、专家演示训练数据的巧妙编程以及 Liquid 网络的任务理解能力的进步。通过一系列定量飞行测试，我们证明了在单个模拟场景中学习到的导航技能可以直接稳健地迁移到现实世界中。我们进一步展示了在剧烈的分布和物理环境变化下，在训练环境之外保持性能的能力。我们学习的 Liquid 策略，仅在从逼真的模拟室内飞行中精选的单目标操作上训练，可以推广到户外真实硬件平台上的多步徒步旅行。||
|**2024-06-14**|[Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections](http://arxiv.org/abs/2406.10373)|null|在非结构化旅游环境中拍摄的照片经常呈现出多变的外观和短暂的遮挡，这对精确的场景重建提出了挑战，并在新视角合成中导致了伪影。虽然先前的方法已经将神经辐射场（NeRF）与其他可学习模块相结合来处理动态外观并消除瞬态对象，但其大量的训练需求和缓慢的渲染速度限制了实际部署。最近，3D 高斯散射（3DGS）已成为 NeRF 的一个有前途的替代方案，它提供了卓越的训练和推理效率以及更好的渲染质量。本文介绍了 Wild-GS，这是一种针对不受约束的照片集合优化的 3DGS 创新改编，同时保留了其效率优势。Wild-GS 通过每个图像的固有材质属性、全局照明和相机属性以及逐点反射率的局部变化来确定每个 3D 高斯函数的外观。与先前在图像空间中对参考特征进行建模的方法不同，Wild-GS 通过对从参考图像中提取的三平面进行采样，将像素外观特征明确对齐到相应的局部高斯函数。这种新颖的设计有效地将参考视图的高频细节外观转移到 3D 空间，并显着加快了训练过程。此外，利用 2D 可见性图和深度正则化分别减轻瞬态效应和约束几何形状。大量实验表明，Wild-GS 在所有现有技术中实现了最先进的渲染性能以及最高的训练和推理效率。||
|**2024-06-06**|[A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation](http://arxiv.org/abs/2406.04253)|null|三维建模一直是计算机视觉和计算机图形学的重要领域。近年来，由于神经表示和生成模型的突破，我们目睹了三维建模的快速发展。三维人体建模作为游戏和动画等许多现实世界应用的核心，引起了人们的广泛关注。在过去的几年里，出现了大量关于创建三维人体化身的工作，为三维人体建模形成了一个新的、丰富的知识库。文献的规模之大，使得个人难以跟踪所有的工作。本综述旨在从重建和生成两个角度全面概述这些新兴的三维人体化身建模技术。首先，我们回顾了具有代表性的三维人体重建方法，包括基于像素对齐隐函数、神经辐射场和三维高斯散射等方法。然后，我们总结了具有代表性的三维人体生成方法，特别是那些使用大型语言模型（如CLIP）、扩散模型和各种三维表示的方法，这些方法展示了最先进的性能。最后，我们讨论了我们对现有方法的反思以及三维人体化身建模面临的开放挑战，为未来的研究指明了方向。||
|**2024-06-13**|[3D-HGS: 3D Half-Gaussian Splatting](http://arxiv.org/abs/2406.02720)|**[link](https://github.com/lihaolin88/3d-half-gaussian-splatting)**|照片级真实感三维重建是三维计算机视觉中的一个基本问题。由于最近神经渲染技术的出现，该领域取得了相当大的进步。这些技术主要致力于学习三维场景的体积表示，并通过渲染得到的损失函数来细化这些表示。其中，三维高斯 splatting (3D-GS) 已成为一种重要方法，其性能超越了神经辐射场 (NeRFs)。3D-GS 使用参数化的三维高斯函数来建模空间位置和颜色信息，并结合基于图块的快速渲染技术。尽管其渲染性能和速度都非常出色，但使用三维高斯核在准确表示不连续函数方面存在固有的局限性，特别是在形状不连续的边缘和角落，以及颜色不连续的不同纹理之间。为了解决这个问题，我们建议采用三维半高斯 (3D-HGS) 核，它可以作为即插即用的核使用。我们的实验表明，它们能够在不影响渲染速度的情况下，提高当前与 3D-GS 相关方法的性能，并在各种数据集上实现最先进的渲染性能。||
|**2024-06-04**|[FastLGS: Speeding up Language Embedded Gaussians with Feature Grid Mapping](http://arxiv.org/abs/2406.01916)|null|语义交互式辐射场因其具有促进用户友好和自动化现实世界 3D 场景理解应用的潜力而一直是一项吸引人的任务。 然而，要在辐射场中同时实现高质量、高效和零样本能力的语义是一项具有挑战性的任务。 在这项工作中，我们提出了 FastLGS，这是一种支持在高分辨率下 3D 高斯样条 (3DGS) 中进行实时开放词汇查询的方法。 我们提出了语义特征网格来保存多视图 CLIP 特征，这些特征是基于 Segment Anything Model (SAM) 掩码提取的，并将网格映射到低维特征，用于通过 3DGS 进行语义场训练。 训练完成后，我们可以通过渲染特征的特征网格恢复像素对齐的 CLIP 嵌入，以进行开放词汇查询。 与其他最先进方法的比较证明，FastLGS 在速度和精度方面均能达到第一的性能，其中 FastLGS 比 LERF 快 98 倍，比 LangSplat 快 4 倍。 同时，实验表明 FastLGS 具有适应性，并且与许多下游任务兼容，例如 3D 分割和 3D 对象修复，可以轻松应用于其他 3D 操作系统。||
|**2024-05-30**|[ $\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous Driving](http://arxiv.org/abs/2405.20323)|**[link](https://github.com/nnanhuang/s3gaussian)**|逼真的街道场景三维重建是开发自动驾驶真实世界模拟器的关键技术。尽管神经辐射场 (NeRF) 对驾驶场景的重建效果很好，但三维高斯散射 (3DGS) 凭借其更快的速度和更明确的表示，成为一个很有前途的方向。然而，大多数现有的街道 3DGS 方法都需要跟踪的三维车辆边界框来分解静态和动态元素以进行有效重建，这限制了它们在野外场景中的应用。为了在无需昂贵标注的情况下实现高效的三维场景重建，我们提出了一种自监督的街道高斯（$\textit{S}^3$Gaussian）方法，利用四维一致性来分解动态和静态元素。我们使用三维高斯函数来表示每个场景以保持其清晰度，并进一步结合时空场网络来紧凑地建模四维动态。我们在具有挑战性的 Waymo-Open 数据集上进行了大量实验，以评估我们方法的有效性。我们的 $\textit{S}^3$ Gaussian 展示了在不使用三维标注的情况下分解静态和动态场景的能力，并取得了最佳性能。代码可在以下网址获得：https://github.com/nnanhuang/S3Gaussian/。||
|**2024-05-28**|[RT-GS2: Real-Time Generalizable Semantic Segmentation for 3D Gaussian Representations of Radiance Fields](http://arxiv.org/abs/2405.18033)|null|高斯渲染技术通过实现实时的高性能渲染，彻底改变了新视角合成的世界。最近，研究重点集中在为下游任务丰富这些3D表示的语义信息。在本文中，我们介绍了RT-GS2，这是第一个采用高斯渲染技术的可泛化语义分割方法。虽然现有的基于高斯渲染的方法依赖于场景特定的训练，但RT-GS2展示了泛化到未见场景的能力。我们的方法采用了一种新方法，首先以自监督的方式提取视图无关的3D高斯特征，然后进行新颖的视图依赖/视图无关（VDVI）特征融合，以增强不同视图之间的语义一致性。在三个不同数据集上的大量实验表明，RT-GS2在语义分割质量方面优于最先进的方法，例如在Replica数据集上的mIoU提高了8.01%。此外，我们的方法实现了27.03 FPS的实时性能，与现有方法相比实现了惊人的901倍加速。据我们所知，这项工作通过引入第一个用于辐射场3D高斯表示的实时可泛化语义分割方法，代表了该领域的重大进步。||
|**2024-05-29**|[PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian Splatting](http://arxiv.org/abs/2405.16829)|null|神经辐射场 (NeRFs) 在合成大规模场景的逼真图像方面表现出了非凡的能力。然而，它们经常受到细节丢失和渲染时间长的困扰。三维高斯 splatting 最近被引入作为一种有效的替代方案，可以实现高保真视觉效果和更快的渲染性能。尽管如此，扩展三维高斯 splatting 仍然充满了挑战。具体来说，大规模场景需要整合来自多个尺度和不同视点的对象，这通常会导致效率下降，因为高斯需要在细节级别之间取得平衡。此外，从大规模数据集中通过 COLMAP 生成初始化点不仅计算量大，而且容易导致重建不完整。为了应对这些挑战，我们提出了采用 NeRF 初始化的金字塔式三维高斯 splatting (PyGS)。我们的方法采用以金字塔形式排列的分层高斯集合来表示场景。金字塔的顶层由一些大的高斯函数组成，而随后的每一层都包含更密集的小高斯函数集合。我们通过以不同的频率对快速训练的基于网格的 NeRF 进行采样，从而有效地初始化这些金字塔高斯函数。我们将这些金字塔高斯函数分组到簇中，并使用紧凑的加权网络在渲染过程中动态确定每个簇中每个金字塔级别的影响，同时考虑相机视点。我们的方法在多个大规模数据集上实现了显著的性能飞跃，渲染速度比当前最先进的方法快 400 多倍。||
|**2024-05-11**|[Direct Learning of Mesh and Appearance via 3D Gaussian Splatting](http://arxiv.org/abs/2405.06945)|null|准确重建包含显式几何信息的3D场景既有吸引力又具有挑战性。几何重建可以受益于结合可微分的表观模型，例如神经辐射场和3D高斯 splatting (3DGS)。在这项工作中，我们提出了一个可学习的场景模型，它将3DGS与显式几何表示（即网格）结合起来。我们的模型以端到端的方式学习网格和外观，我们将3D高斯函数绑定到网格面上，并执行3DGS的可微分渲染以获得光度监督。该模型创建了一个有效的信息通路来监督场景学习，包括网格。实验结果表明，学习到的场景模型不仅实现了最先进的渲染质量，而且还支持使用显式网格进行操作。此外，由于网格和外观的端到端学习，我们的模型在适应场景更新方面具有独特优势。||

## 分类/检测/识别/分割

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-07-25**|[LION: Linear Group RNN for 3D Object Detection in Point Clouds](http://arxiv.org/abs/2407.18232)|**[link](https://github.com/happinesslz/LION)**|Transformer 在大规模 3D 点云感知任务（例如 3D 对象检测）中的优势受到其在建模远程关系时二次计算成本的限制。相比之下，线性 RNN 具有较低的计算复杂度，适用于远程建模。为此，我们提出了一种简单有效的基于窗口的框架，该框架建立在线性分组 RNN（即对分组特征执行线性 RNN）的基础上，用于精确的 3D 对象检测，称为 LION。其关键特性是允许在比基于 Transformer 的方法大得多的组中进行充分的特征交互。然而，由于线性分组 RNN 在处理空间建模方面的局限性，将其有效地应用于高度稀疏点云中的 3D 对象检测并非易事。为了解决这个问题，我们简单地引入了一个 3D 空间特征描述符，并将其集成到线性分组 RNN 算子中，以增强其空间特征，而不是盲目地增加体素特征的扫描顺序数量。为了进一步应对高度稀疏点云中的挑战，我们提出了一种 3D 体素生成策略，由于线性分组 RNN 作为自回归模型的自然属性，可以使前景特征更加密集。大量实验验证了所提出组件的有效性以及我们的 LION 在不同线性分组 RNN 算子（包括 Mamba、RWKV 和 RetNet）上的泛化能力。此外，值得一提的是，我们的 LION-Mamba 在 Waymo、nuScenes、Argoverse V2 和 ONCE 数据集上实现了最先进的性能。最后但同样重要的是，我们的方法支持在小型但流行的 KITTI 数据集上使用各种先进的线性 RNN 算子（例如 RetNet、RWKV、Mamba、xLSTM 和 TTT），以便快速体验我们基于线性 RNN 的框架。||
|**2024-07-25**|[GaussianSR: High Fidelity 2D Gaussian Splatting for Arbitrary-Scale Image Super-Resolution](http://arxiv.org/abs/2407.18046)|null|隐式神经表示（INR）显著推进了图像任意尺度超分辨率（ASSR）领域的发展。大多数现有的基于INR的ASSR网络首先使用编码器从给定的低分辨率图像中提取特征，然后通过多层感知机解码器渲染超分辨率结果。尽管这些方法已显示出良好的结果，但其性能受到编码特征中离散潜在代码表示能力有限的限制。在本文中，我们提出了一种名为GaussianSR的新型ASSR方法，该方法通过二维高斯 splatting（2DGS）克服了这一限制。与将像素视为离散点的传统方法不同，GaussianSR将每个像素表示为连续的高斯场。通过渲染相互堆叠的高斯场，对编码特征进行同时细化和上采样。因此，建立了远程依赖关系以增强表示能力。此外，开发了一种分类器，可以动态地为所有像素分配高斯核，以进一步提高灵活性。GaussianSR的所有组件（即编码器、分类器、高斯核和解码器）都经过端到端的联合学习。实验表明，与现有方法相比，GaussianSR以更少的参数实现了卓越的ASSR性能，同时享有可解释且内容感知的特征聚合。||
|**2024-07-25**|[A Novel Perception Entropy Metric for Optimizing Vehicle Perception with LiDAR Deployment](http://arxiv.org/abs/2407.17942)|null|开发有效的评估指标对于准确快速地测量激光雷达感知性能至关重要。一个主要问题是缺乏能够同时根据目标检测或点云数据生成快速准确评估的指标。在这项研究中，我们提出了一种基于车辆网格占用概率的新型激光雷达感知熵指标。该指标反映了点云分布对车辆检测性能的影响。在此基础上，我们还引入了一种激光雷达部署优化模型，该模型使用基于差分进化的粒子群优化算法求解。对比实验表明，所提出的PE-VGOP在评估激光雷达感知性能方面与车辆检测真值的关联性超过0.98。此外，与基础部署相比，现场实验表明，所提出的优化模型可以显着增强各种类型激光雷达（包括RS-16、RS-32和RS-80）的感知能力。值得注意的是，它使RS-32激光雷达的检测召回率提高了25%。||
|**2024-07-25**|[Hierarchical Object Detection and Recognition Framework for Practical Plant Disease Diagnosis](http://arxiv.org/abs/2407.17906)|null|近年来，目标检测方法（OD；例如，基于 YOLO 的模型）已广泛应用于植物病害诊断。与分类方法（CL；例如，CNN 模型）相比，这些方法表现出对距离变化的鲁棒性，并且在检测小病斑方面表现出色。然而，也存在一些问题，例如对难检测疾病的诊断性能较低以及标注成本高。此外，由于无法对健康案例进行明确训练，因此存在误报的风险。我们提出了层次化目标检测与识别框架（HODRF），这是一个复杂且高度集成的两阶段系统，结合了 OD 和 CL 的优势进行植物病害诊断。在第一阶段，HODRF 使用 OD 来识别感兴趣区域（ROI），而无需指定疾病。在第二阶段，CL 诊断 ROI 周围的疾病。HODRF 具有以下几个优点：（1）由于 OD 只检测一种类型的 ROI，HODRF 可以通过利用其识别其他病斑的能力，利用有限的训练图像检测疾病。(2) 虽然 OD 会过度检测健康案例，但 HODRF 通过在第二阶段使用 CL 显着减少了这些错误。(3) CL 的准确性在 HODRF 中有所提高，因为它可以识别作为 ROI 给出的诊断目标，从而降低了其对大小变化的敏感性。(4) HODRF 受益于 CL 较低的标注成本，使其能够从大量图像中学习。我们使用 YOLOv7 实现 HODRF 进行 OD，并使用 EfficientNetV2 实现 CL，并在一个大型数据集（4 种作物、20 种患病和健康类别、281K 张图像）上评估了其性能。HODRF 在健康数据上的表现优于单独使用 YOLOv7 5.8 到 21.5 个点，在宏观 F1 分数上优于 0.6 到 7.5 个点，并且与 EfficientNetV2 相比，宏观 F1 提高了 1.1 到 7.2 个点。||
|**2024-07-25**|[Advancing 3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey](http://arxiv.org/abs/2407.17877)|null|三维点云（3DPC）随着深度学习（DL）的进步得到了显著发展和益处。然而，后者面临着各种问题，包括缺乏数据或标注数据、训练数据和测试数据之间存在显著差距以及对高计算资源的要求。为此，深度迁移学习（DTL）通过利用从源数据/任务中获得的知识来训练目标数据/任务，从而减少了依赖性和成本，并得到了广泛研究。人们已经提出了许多DTL框架来对齐从同一场景的多次扫描中获得的点云。此外，作为DTL子集的域自适应（DA）已经过改进，通过处理噪声和缺失点来提高点云数据的质量。最终，微调和DA方法已经证明了它们在解决点云数据固有难题方面的有效性。本文首次对这一方面进行了综述。它全面概述了利用DTL和域自适应（DA）理解3DPC的最新技术。因此，首先介绍了DTL的背景以及数据集和评估指标。文章引入了一个定义明确的分类法，并考虑了不同的知识迁移策略和性能等方面，进行了详细的比较。本文涵盖了各种应用，例如3DPC目标检测、语义标注、分割、分类、配准、下采样/上采样和去噪。此外，文章还讨论了所提出框架的优点和局限性，指出了开放的挑战，并提出了潜在的研究方向。||
|**2024-07-24**|[Unsqueeze [CLS] Bottleneck to Learn Rich Representations](http://arxiv.org/abs/2407.17671)|null|基于蒸馏的自监督学习由于其根本的聚类过程和更尖锐的目标分布的实现，通常会导致更压缩的表示。为了克服这一限制并保留来自输入的更多信息，我们引入了 UDI，概念化为非压缩蒸馏的自监督学习 (SSL)。UDI 通过鼓励从分层采样得到的局部预测的整合配置文件中提取的多模态预测来丰富学习到的表示。我们的评估表明，UDI 不仅在实例级别上促进了语义上有意义的表示，在图像分类方面提供了优于或可与最先进的 SSL 方法相媲美的结果，而且还有效地保留了输入的细微差别，从而在密集预测任务（包括目标检测和分割）中产生了显著改进。此外，UDI 在小样本图像分类中表现出色，提高了联合嵌入流程的可扩展性。我们还提供了各种可视化和消融研究，以进一步阐明 UDI 背后的机制。我们的源代码可在 https://github.com/ISL-CV/udi 获取。||
|**2024-07-24**|[PEEKABOO: Hiding parts of an image for unsupervised object localization](http://arxiv.org/abs/2407.17628)|**[link](https://github.com/hasibzunair/peekaboo)**|在无监督的情况下定位物体提出了重大挑战，因为缺乏关键的视觉信息，例如物体的形状、类型和数量，以及在监督设置中通常可用的标记物体类别的缺失。虽然最近无监督物体定位方法通过利用自监督视觉表示取得了重大进展，但它们通常需要计算密集型训练过程，导致在计算、可学习参数和数据方面的资源需求较高。它们还缺乏对视觉上下文的显式建模，这可能会限制它们在物体定位方面的准确性。为了应对这些挑战，我们提出了一个名为PEEKABOO的单阶段学习框架，用于通过图像掩码学习局部物体像素级和形状级的基于上下文的表示来进行无监督物体定位。其关键思想是有选择地隐藏图像的某些部分，并利用剩余的图像信息来推断物体的位置，而无需明确的监督。在各种基准数据集上的定量和定性实验结果表明，与最先进的方法相比，我们的方法在单个物体发现和无监督显著性物体检测任务中均表现出简单性、有效性和竞争力。代码和预训练模型可在以下网址获得：https://github.com/hasibzunair/peekaboo||
|**2024-07-24**|[Graph Neural Networks: A suitable Alternative to MLPs in Latent 3D Medical Image Classification?](http://arxiv.org/abs/2407.17219)|**[link](https://github.com/compai-lab/2024-miccai-grail-kiechle)**|最近的研究突出了自然图像基础模型作为强大特征提取器的能力，即使是在零样本设置下处理医学影像数据也是如此。最常见的是，一个浅层多层感知器 (MLP) 被附加到特征提取器上，以促进端到端学习和下游预测任务（如分类），从而代表了事实上的标准。然而，由于图神经网络 (GNN) 近年来已成为医学研究中各种任务的可行选择，我们将注意力转向 GNN 与 MLP 预测头相比在 3D 医学图像分类任务中的有效性，并提出将其作为潜在的替代方案。在我们的实验中，我们为每个体积数据集实例设计了一个主题级别的图。其中，通过 DINOv2 预训练视觉变换器 (ViT) 编码的体积中所有切片的潜在表示构成节点及其各自的节点特征。我们使用公共数据集对分类头的数值进行比较，并在实验中评估各种图构建和图卷积方法。我们的研究结果表明，与 MLP 预测头相比，GNN 在分类性能方面有所提高，并且在运行时间方面也有了实质性的改进。额外的鲁棒性评估进一步验证了 GNN 的良好性能，使其成为传统 MLP 分类头的合适替代方案。我们的代码可在以下网址公开获取：https://github.com/compai-lab/2024-miccai-grail-kiechle||
|**2024-07-24**|[ALPI: Auto-Labeller with Proxy Injection for 3D Object Detection using 2D Labels Only](http://arxiv.org/abs/2407.17197)|null|三维目标检测在自动驾驶、机器人技术和增强现实等各种应用中起着至关重要的作用。然而，训练三维检测器需要昂贵的精确标注，这阻碍了标注扩展到大型数据集。为了应对这一挑战，我们提出了一种弱监督三维标注器，它仅依赖于图像中的二维边界框标注以及尺寸先验。一个主要问题是，仅使用二维边界框来监督三维检测模型并不可靠，因为不同的三维姿态与其相同的二维投影之间存在歧义。我们引入了一种简单而有效且通用的解决方案：我们通过构造构建带有标注的三维代理对象，并将它们添加到训练数据集中。我们的方法只需要尺寸先验就可以适应新的类别。为了更好地使二维监督与三维检测保持一致，我们的方法通过二维损失的新颖表达来确保深度不变性。最后，为了检测更具挑战性的实例，我们的标注器遵循离线伪标签方案，逐步改进其三维伪标签。在 KITTI 数据集上的大量实验表明，我们的方法不仅在汽车类别上的性能与之前的工作相当或更好，而且在更具挑战性的类别上也实现了接近完全监督方法的性能。我们通过第一个在更具挑战性的 nuScenes 数据集上进行实验，进一步证明了我们方法的有效性和鲁棒性。我们还提出了一种设置，其中弱标签是从在 MS-COCO 上预先训练的二维检测器而不是人工标注获得的。||
|**2024-07-24**|[An Adaptive Gradient Regularization Method](http://arxiv.org/abs/2407.16944)|null|优化器在神经网络训练中起着至关重要的作用，它能够显著提高训练效率和性能。基于梯度的权重更新是优化器的核心部分。研究表明，对权重和梯度进行标准化和归一化操作可以加速训练过程并提高性能，例如权重标准化（WS）、权重归一化（WN）和梯度归一化（GN）；此外还有梯度中心化（GC）。在这项工作中，我们引入了一种新的优化技术，称为自适应梯度正则化（AGR），它基于梯度向量中的梯度大小，将梯度向量在所有维度上归一化为系数向量，并从原始梯度中减去梯度与其系数向量的乘积。它可以被视为一种自适应梯度裁剪方法。我们证明了AGR可以改善损失函数的Lipschitzness，从而获得更稳定的训练过程和更好的泛化性能。AGR非常容易嵌入到vanilla优化器（例如Adan和AdamW）中，只需三行代码即可实现。我们在图像生成、图像分类和语言表示方面进行了实验，结果表明我们的AGR方法提高了训练效果。||
|**2024-07-23**|[Deep Bayesian segmentation for colon polyps: Well-calibrated predictions in medical imaging](http://arxiv.org/abs/2407.16608)|null|结直肠息肉通常是良性病变，但如果不能及时识别和成功处理，可能会发展成癌症，并导致结肠黏膜病变，称为腺癌。如今，深度学习的进步已经证明，在医学诊断应用中，深度学习在图像分类和检测方面能够实现显著的性能。然而，这些模型容易出现过拟合，仅基于点估计做出决策可能会导致错误的预测。因此，为了获得更明智的决策，我们必须考虑点估计及其可靠的不确定性量化。在本文中，我们基于后验分布的灵活性构建了不同的贝叶斯神经网络方法，以开发结直肠息肉图像的语义分割。我们发现，这些模型不仅在该医学数据集的分割方面提供了最先进的性能，而且还产生了准确的不确定性估计。我们将乘法归一化流（MNF）和重新参数化技巧应用于UNET、FPN和LINKNET架构，并在确定性和贝叶斯版本中使用多个主干网络进行了测试。我们报告说，FPN + EfficientnetB7架构与MNF是最有希望的选择，因为它IOU为0.94，预期校准误差（ECE）为0.004，并且在识别难以检测的结直肠息肉方面具有优势，这在早期检测可以预防结肠癌发展的临床领域非常有效。||
|**2024-07-23**|[Dynamic Retraining-Updating Mean Teacher for Source-Free Object Detection](http://arxiv.org/abs/2407.16497)|**[link](https://github.com/lbktrinh/DRU)**|在目标检测领域，无监督域适应（UDA）旨在将知识从标记的源域迁移到未标记的目标域。然而，UDA 对标记源数据的依赖限制了其在隐私相关场景中的适用性。本研究重点关注无源目标检测（SFOD），它在不使用标记源数据的情况下使源训练检测器适应未标记的目标域。自训练的最新进展，特别是 Mean Teacher（MT）框架，为 SFOD 的部署带来了希望。然而，缺乏源监督会严重损害这些方法的稳定性。我们确定了两个主要问题，（1）由于来自学生模型的不合时宜的更新，教师模型的不可控退化，以及（2）学生模型倾向于复制来自不正确伪标签的错误，导致其陷入局部最优。这两个因素都会导致有害的循环依赖，从而导致最近的自训练框架中性能快速下降。为了应对这些挑战，我们提出了动态再训练-更新（DRU）机制，该机制主动管理学生训练和教师更新过程以实现协同进化训练。此外，我们引入了历史学生损失来减轻不正确伪标签的影响。我们的方法在多个域适应基准上的 SFOD 设置中实现了最先进的性能，可与甚至超越先进的 UDA 方法相媲美。代码将在 https://github.com/lbktrinh/DRU 发布。||
|**2024-07-23**|[Designing robust diffractive neural networks with improved transverse shift tolerance](http://arxiv.org/abs/2407.16456)|null|如今，各种具有实际意义的问题都可以使用人工神经网络有效地解决。这推动了人工神经网络光学实现的快速发展，其中由一组相位衍射光学元件 (DOE) 构成的衍射神经网络 (DNN) 引起了广泛的研究兴趣。在 DNN 的实际应用中，一个亟待解决的问题是对 DOE 的高定位精度要求。为了解决这个问题，我们提出了一种设计用于图像分类的 DNN 的方法，该方法考虑了 DNN 元件的定位误差（横向位移）。在该方法中，分类问题的求解误差由一个函数表示，该函数取决于 DOE 的相位函数和描述其横向位移的随机向量。该函数的数学期望值被用作梯度方法中的误差函数，用于计算考虑了 DOE 横向位移的 DNN。研究表明，该函数导数的计算对应于 DNN 训练方法，其中 DOE 具有随机的横向位移。使用所提出的梯度方法，设计的 DNN 对 DOE 的横向位移具有鲁棒性，并且能够解决可见光波长下手写数字分类问题。数值模拟表明，设计的 DNN 在横向位移高达 17 个波长的情况下仍具有良好的性能。||
|**2024-07-23**|[MonoWAD: Weather-Adaptive Diffusion Model for Robust Monocular 3D Object Detection](http://arxiv.org/abs/2407.16448)|**[link](https://github.com/visualaikhu/monowad)**|单目3D目标检测是自动驾驶中一项重要且具有挑战性的任务。现有方法主要集中于在晴朗天气条件下执行3D检测，这些场景的特点是能见度清晰且最佳。然而，自动驾驶的挑战在于需要能够处理天气条件的变化，例如雾天，而不仅仅是晴天。我们介绍了MonoWAD，这是一种新颖的、具有天气鲁棒性的单目3D目标检测器，它具有天气自适应扩散模型。它包含两个组件：（1）天气码本，用于记忆晴朗天气的知识并为任何输入生成天气参考特征；（2）天气自适应扩散模型，通过结合天气参考特征来增强输入特征的特征表示。这起到了注意力的作用，根据天气条件指示输入特征需要多少改进。为了实现这一目标，我们引入了天气自适应增强损失，以增强晴天和雾天天气条件下的特征表示。在各种天气条件下进行的大量实验表明，MonoWAD实现了天气鲁棒的单目3D目标检测。代码和数据集已发布在https://github.com/VisualAIKHU/MonoWAD。||
|**2024-07-23**|[ESOD: Efficient Small Object Detection on High-Resolution Images](http://arxiv.org/abs/2407.16424)|null|放大输入图像是提升小目标检测性能的一种直接有效的方法。但是，简单的图像放大在计算量和GPU内存占用方面都非常昂贵。实际上，小目标通常分布稀疏且局部聚集。因此，大量的特征提取计算浪费在了图像中非目标的背景区域。最近的一些工作尝试使用额外的网络挑选出包含目标的区域并执行传统的目标检测，但新引入的计算限制了它们的最终性能。在本文中，我们建议重用检测器的骨干网络进行特征级目标搜索和图像块切片，这可以避免冗余的特征提取并降低计算成本。结合稀疏检测头，我们能够在高分辨率输入（例如1080P或更大）上检测小目标，从而获得更高的性能。由此产生的高效小目标检测（ESOD）方法是一个通用框架，可以应用于基于CNN和ViT的检测器，以节省计算量和GPU内存开销。大量实验验证了我们方法的有效性和效率。特别是，我们的方法在VisDrone、UAVDT和TinyPerson等代表性数据集上始终优于SOTA检测器（例如，AP指标提升8%）。代码即将开源。||
|**2024-07-23**|[DeepClean: Integrated Distortion Identification and Algorithm Selection for Rectifying Image Corruptions](http://arxiv.org/abs/2407.16302)|null|图像和视频中的失真识别和校正对于在下游视觉应用中获得良好性能至关重要。我们不依赖于基于固定试错法的图像处理流程，而是提出了一种用于自动图像失真分类和校正的两级顺序规划方法。在较高级别，它检测输入图像中存在的损坏类别（如果有）。较低级别从一组外部提供的候选算法中选择要应用的特定算法。整个两级设置在推理过程中以单次前向传递的形式运行，并且要迭代查询，直到检索到原始图像。我们展示了与三个基线相比在 COCO 图像数据集上的目标检测任务上的改进，该数据集具有丰富的失真集。我们方法的优势在于其动态重新配置，以输入图像为条件，以及对推理时看不见的候选算法的泛化能力，因为它仅依赖于图像嵌入输出的比较。||
|**2024-07-23**|[Image Classification using Fuzzy Pooling in Convolutional Kolmogorov-Arnold Networks](http://arxiv.org/abs/2407.16268)|null|如今，深度学习模型越来越需要兼具可解释性和高准确性。我们提出了一种将柯尔莫哥洛夫-阿诺德网络 (KAN) 分类头和模糊池化集成到卷积神经网络 (CNN) 中的方法。通过利用 KAN 的可解释性和模糊逻辑的不确定性处理能力，该集成在图像分类任务中展现出提高性能的潜力。我们的比较分析表明，采用 KAN 和模糊池化的改进 CNN 架构实现了与传统模型相当甚至更高的准确率。研究结果突出了结合模糊逻辑和 KAN 来开发更具可解释性和效率的深度学习模型的有效性。未来的工作将致力于将这种方法扩展到更大的数据集。||
|**2024-07-23**|[HSVLT: Hierarchical Scale-Aware Vision-Language Transformer for Multi-Label Image Classification](http://arxiv.org/abs/2407.16244)|null|多标签图像分类的任务涉及识别单个图像中的多个对象。考虑到标签中包含的有价值的语义信息和图像中呈现的基本视觉特征，紧密的视觉-语言交互在提高分类性能方面起着至关重要的作用。此外，鉴于单个图像内对象大小和外观的潜在差异，关注不同尺度的特征有助于发现图像中可能存在的对象。最近，基于Transformer的方法通过利用建模长距离依赖性的优势，在多标签图像分类方面取得了巨大成功，但它们也存在一些局限性。首先，现有方法将视觉特征提取和跨模态融合视为独立的步骤，导致联合语义空间中的视觉-语言对齐不足。此外，它们仅提取视觉特征并在单一尺度上执行跨模态融合，忽略了具有不同特征的对象。为了解决这些问题，我们提出了一种具有两个吸引人设计的层次化尺度感知视觉-语言Transformer (HSVLT)：(1) 层次化多尺度架构，其中包含一个跨尺度聚合模块，该模块利用从多个尺度提取的联合多模态特征来识别图像中不同大小和外观的对象。(2) 交互式视觉-语言注意力，一种新颖的注意力机制模块，它紧密地集成了跨模态交互，从而能够联合更新视觉、语言和多模态特征。我们已经在三个基准数据集上评估了我们的方法。实验结果表明，HSVLT以较低的计算成本超越了最先进的方法。||
|**2024-07-23**|[Improved Few-Shot Image Classification Through Multiple-Choice Questions](http://arxiv.org/abs/2407.16145)|null|通过一个简单的多选语言提示，VQA 模型可以作为零样本图像分类器运行，生成分类标签。与典型的图像编码器相比，VQA 模型具有一个优势：VQA 生成的图像嵌入可以通过定制的语言提示融入最相关的视觉信息。然而，对于大多数任务，零样本 VQA 的性能缺乏，要么是因为类别名称不熟悉，要么是因为预训练数据和测试数据分布不同。我们提出了一种简单的方法，仅使用少量标记样本和一个多选问题来提高 VQA 图像分类性能。这种少样本方法无需训练，并保持了 VQA 模型的动态性和灵活性优势。我们的方法不依赖于最终的语言输出，而是使用多选问题来提取特定于提示的潜在表示，这些表示包含丰富的相关视觉信息。这些表示被组合起来创建一个最终的整体图像嵌入，然后通过参考从少量标记样本构建的潜在类别原型对其进行解码。我们证明了该方法在 MiniImageNet、Caltech-UCSD Birds 和 CIFAR-100 等常见的少样本任务上优于纯视觉编码器和零样本 VQA 基线，实现了令人印象深刻的性能。最后，我们展示了我们的方法在具有多种不同视觉属性（例如织物、款式、纹理和不同服装的视图）的设置中表现特别好，而其他少样本方法在这些方面表现不佳，因为我们可以仅根据感兴趣的语义特征定制图像表示。||
|**2024-07-23**|[Diffusion Prior-Based Amortized Variational Inference for Noisy Inverse Problems](http://arxiv.org/abs/2407.16125)|**[link](https://github.com/mlvlab/davi)**|最近关于反问题的研究提出了利用预训练扩散模型作为强大先验的后验采样器。这些尝试为在各种反问题中使用扩散模型铺平了道路。然而，现有方法需要计算量大的迭代采样过程，并且需要为每个测量值优化单独的解决方案，这导致可扩展性有限，并且缺乏对未见样本的泛化能力。为了解决这些限制，我们提出了一种新方法，即基于扩散先验的摊销变分推断 (DAVI)，它从摊销变分推断的角度利用扩散先验解决反问题。具体来说，我们的摊销推断没有进行单独的逐测量优化，而是学习一个函数，该函数将测量值直接映射到相应干净数据的隐式后验分布，即使对于未见过的测量值也能实现单步后验采样。在图像恢复任务上的大量实验，例如高斯去模糊、4倍超分辨率和使用两个基准数据集的框内修复，证明了我们的方法优于强基线的性能。代码可在 https://github.com/mlvlab/DAVI 获取。||
|**2024-07-19**|[DEPICT: Diffusion-Enabled Permutation Importance for Image Classification Tasks](http://arxiv.org/abs/2407.14509)|null|我们提出了一种基于排列的图像分类器解释方法。当前的图像模型解释方法（如激活图）仅限于像素空间中基于实例的解释，因此难以理解全局模型行为。相比之下，基于排列的表格数据分类器解释方法通过比较模型在排列特征前后对数据的性能来衡量特征重要性。我们提出了一种基于图像的模型解释方法，该方法在数据集图像中排列可解释的概念。给定一个标有特定概念（如标题）的图像数据集，我们在文本空间中对示例中的概念进行排列，然后通过文本条件扩散模型生成图像。然后，特征重要性反映为模型性能相对于未排列数据的变化。当应用于一组概念时，该方法会生成特征重要性排名。我们证明，这种方法可以恢复合成和现实世界图像分类任务中潜在的模型特征重要性。||
|**2024-07-19**|[Enhancing Layout Hotspot Detection Efficiency with YOLOv8 and PCA-Guided Augmentation](http://arxiv.org/abs/2407.14498)|null|本文提出了一种基于YOLO的布局热点检测框架，旨在提高设计规则检查（DRC）过程的效率和性能。我们的方法利用YOLOv8视觉模型来检测每个布局图像中的多个热点，即使在处理大型布局图像时也是如此。此外，为了提高模式匹配的有效性，我们引入了一种新颖的方法，使用通过主成分分析（PCA）提取的信息来增强布局图像。我们提出的方法的核心是一种利用PCA从布局图像中提取有价值的辅助信息的算法。然后将提取的信息作为附加颜色通道合并到布局图像中。这种增强显着提高了多热点检测的准确性，同时降低了目标检测算法的误报率。我们使用从ICCAD-2019基准数据集中发现的布局生成的四个数据集来评估我们框架的有效性。结果表明，我们的框架实现了大约83%（86%）的精度（召回率），同时将误报率保持在7.4%以下。此外，研究表明，所提出的增强方法可以将从未见过的（NSB）热点的检测能力提高约10%。||
|**2024-07-19**|[Large Kernel Distillation Network for Efficient Single Image Super-Resolution](http://arxiv.org/abs/2407.14340)|**[link](https://github.com/stella-von/LKDN)**|近年来，高效轻量级的单图像超分辨率 (SISR) 取得了显著进展。一种有效的方法是使用大核设计，这已被证明可以提高 SISR 模型的性能，同时降低其计算需求。然而，当前最先进的 (SOTA) 模型仍然面临着诸如计算成本高等问题。为了解决这些问题，我们在本文中提出了大核蒸馏网络 (LKDN)。我们的方法简化了模型结构，并引入了更高效的注意力模块，以降低计算成本，同时提高性能。具体来说，我们采用重新参数化技术来提高模型性能，而无需增加额外成本。我们还将一种来自其他任务的新优化器引入到 SISR 中，从而提高了训练速度和性能。我们的实验结果表明，LKDN 优于现有的轻量级超分辨率方法，并实现了最先进的性能。||
|**2024-07-19**|[Temporal Correlation Meets Embedding: Towards a 2nd Generation of JDE-based Real-Time Multi-Object Tracking](http://arxiv.org/abs/2407.14086)|null|联合检测与嵌入（JDE）跟踪器通过将外观特征提取作为辅助任务，将重识别任务（ReID）嵌入到检测器中，在多目标跟踪（MOT）任务中表现出色，实现了推理速度和跟踪性能之间的平衡。然而，解决检测器和特征提取器之间的竞争一直是一个挑战。此外，将 ReID 任务直接嵌入 MOT 的问题仍未解决。外观特征缺乏高辨别性，导致其用途有限。在本文中，我们提出了一种使用互相关来捕获对象时间信息的新学习方法。特征提取网络不再仅仅训练每帧的外观特征，而是利用连续帧的特征热图来学习更丰富的运动特征，解决了类间特征相似性的挑战。此外，我们将学习方法应用于更轻量级的特征提取网络，并将特征匹配分数视为强线索而非辅助线索，采用适当的权重计算来反映我们获得的特征与 MOT 任务之间的兼容性。我们的跟踪器名为 TCBTrack，在多个公共基准测试（即 MOT17、MOT20 和 DanceTrack 数据集）上实现了最先进的性能。具体来说，在 DanceTrack 测试集上，我们实现了 56.8 HOTA、58.1 IDF1 和 92.5 MOTA，使其成为能够实现实时性能的最佳在线跟踪器。与其他跟踪器的比较评估证明，我们的跟踪器在速度、鲁棒性和准确性之间实现了最佳平衡。||
|**2024-07-18**|[GroupMamba: Parameter-Efficient and Accurate Group Visual State Space Model](http://arxiv.org/abs/2407.13772)|**[link](https://github.com/Amshaker/GroupMamba)**|最近，状态空间模型（SSM）在以亚二次复杂度建模长程依赖性方面表现出色。然而，纯基于 SSM 的模型仍然面临着与稳定性和在计算机视觉任务上实现最佳性能相关的挑战。本文解决了扩展基于 SSM 的计算机视觉模型所面临的挑战，特别是大型模型的不稳定性和低效性。为了解决这个问题，我们引入了调制组Mamba层，它将输入通道分成四组，并将我们提出的基于 SSM 的高效视觉单选择扫描（VSSS）块独立应用于每组，每个VSSS块在四个空间方向之一进行扫描。调制组Mamba层还将四个VSSS块封装到一个通道调制算子中，以改善跨通道通信。此外，我们引入了一种基于蒸馏的训练目标，以稳定大型模型的训练，从而持续提高性能。我们的综合实验结果证明了所提出的贡献的优势，在 ImageNet-1K 上的图像分类、目标检测、MS-COCO 上的实例分割以及 ADE20K 上的语义分割方面，其性能优于现有方法。我们包含 23M 参数的微型变体在 ImageNet-1K 上实现了 83.3% 的分类 Top-1 准确率的最佳性能，同时与相同模型大小的现有最佳 Mamba 设计相比，参数效率提高了 26%。我们的代码和模型可在以下网址获得：https://github.com/Amshaker/GroupMamba。||
|**2024-07-18**|[Research on Image Super-Resolution Reconstruction Mechanism based on Convolutional Neural Network](http://arxiv.org/abs/2407.13211)|null|超分辨率重建技术利用软件算法将从同一场景捕获的一组或多组低分辨率图像转换为高分辨率图像。近年来，单图像超分辨率算法领域取得了长足的进步，特别是基于深度学习技术的算法。然而，现有算法在重建过程中提取图像特征和非线性映射方法仍然具有挑战性。这些问题导致网络架构无法有效地利用不同级别的各种信息。高频细节的损失很大，最终重建的图像特征过于平滑，缺乏精细的纹理细节。这对图像的主观视觉质量产生了负面影响。目标是从低分辨率图像中恢复高质量、高分辨率的图像。在这项工作中，我们采用了一种增强的深度卷积神经网络模型，该模型包含多个卷积层，每个卷积层都配置了特定的滤波器和激活函数，以有效地捕获图像的各种特征。此外，我们采用残差学习策略来加速训练并增强网络的收敛性，同时利用亚像素卷积层来细化图像的高频细节和纹理。实验分析表明，与传统的双三次插值方法和其他几种基于学习的超分辨率方法相比，该模型在多个公共数据集上表现出优异的性能。此外，它证明了该模型在保持图像边缘和纹理方面的有效性。||
|**2024-07-18**|[Learning Camouflaged Object Detection from Noisy Pseudo Label](http://arxiv.org/abs/2407.13157)|null|现有的伪装目标检测 (COD) 方法严重依赖于大规模像素级标注训练集，这些数据集的构建既耗时又费力。虽然弱监督方法提供了更高的标注效率，但由于伪装图像中前景和背景之间的视觉界限不清，它们的性能远远落后。在本文中，我们探索了在伪装场景中使用边界框作为提示的潜力，并介绍了第一个弱半监督 COD 方法，旨在使用极其有限数量的全标记图像实现经济高效且高精度的伪装目标分割。值得注意的是，从如此有限的集合中学习不可避免地会产生带有严重噪声像素的伪标签。为了解决这个问题，我们提出了一种噪声校正损失，它有助于模型在早期学习阶段学习正确的像素，并在记忆阶段校正由噪声像素主导的错误风险梯度，最终实现从噪声标签中准确分割伪装目标。当仅使用 20% 的全标记数据时，我们的方法表现出优于最先进方法的性能。||
|**2024-07-18**|[DFMSD: Dual Feature Masking Stage-wise Knowledge Distillation for Object Detection](http://arxiv.org/abs/2407.13147)|null|近年来，当前主流的特征掩码蒸馏方法主要通过从教师网络的特征图中重建学生网络的选择性掩码区域来发挥作用。在这些方法中，注意力机制可以帮助识别空间上重要的区域和关键的对象感知通道线索，从而使重建的特征编码具有与教师特征类似的足够判别性和表示能力。然而，以前的特征掩码蒸馏方法主要解决同质知识蒸馏问题，而没有充分考虑到异构知识蒸馏场景。特别是，在异构蒸馏范式中，教师和学生框架之间的巨大差异不利于特征掩码，导致重建的学生特征恶化。在这项研究中，提出了一种新的双特征掩码异构蒸馏框架，称为DFMSD，用于目标检测。更具体地说，在双特征掩码框架中加入了一个阶段性自适应学习模块，因此学生模型可以逐步适应教师模型，以弥合异构网络之间的差距。此外，将掩码增强策略与阶段性学习相结合，从而自适应地增强对象感知掩码区域，以改进特征掩码重建。此外，在教师和学生网络之间的每个特征金字塔网络（FPN）层执行语义对齐，以生成一致的特征分布。我们针对目标检测任务的实验结果证明了我们方法的前景，表明DFMSD优于最先进的异构和同构蒸馏方法。||
|**2024-07-18**|[UCIP: A Universal Framework for Compressed Image Super-Resolution using Dynamic Prompt](http://arxiv.org/abs/2407.13108)|null|压缩图像超分辨率 (CSR) 旨在同时对压缩图像进行超分辨率处理，并解决由压缩引起的混合失真的挑战。然而，现有的 CSR 工作通常集中在单一压缩编解码器（即 JPEG）上，而忽略了实际应用中不同的传统或基于学习的编解码器，例如 HEVC、VVC、HIFIC 等。在这项工作中，我们提出了第一个通用 CSR 框架，称为 UCIP，它具有动态提示学习能力，旨在联合支持任何压缩编解码器/模式的 CSR 失真。具体来说，我们提出了一种高效的动态提示策略，使用少量空间大小为 1x1 的提示，挖掘内容/空间感知的任务自适应上下文信息，用于通用 CSR 任务。为了简化上下文信息挖掘，我们通过首次将主动标记混合器 (ATM) 应用于 CSR 任务，为我们的 UCIP 引入了一种新颖的类 MLP 框架骨干，其中全局信息建模仅在水平和垂直方向上进行偏移预测。我们还通过收集具有 6 种流行的不同传统和基于学习的编解码器（包括 JPEG、HEVC、VVC、HIFIC 等）的数据集，构建了一个用于 CSR 任务的多合一基准数据集，产生了 23 种常见失真。大量实验表明，我们的 UCIP 在通用 CSR 任务上具有一致且优异的性能。该项目可在 https://lixinustc.github.io/UCIP.github.io  找到。||
|**2024-07-17**|[LookupViT: Compressing visual information to a limited number of tokens](http://arxiv.org/abs/2407.12753)|null|视觉Transformer (ViT) 已成为众多行业级视觉解决方案的首选。但是，它们的推理成本对于许多设置来说可能过高，因为它们在每一层都计算自注意力，这在token数量上存在二次计算复杂度。另一方面，图像中的空间信息和视频中的时空信息通常是稀疏和冗余的。在这项工作中，我们介绍了 LookupViT，旨在利用这种信息稀疏性来降低 ViT 推理成本。LookupViT 提供了一种新颖的通用视觉Transformer块，它通过将信息从更高分辨率的token压缩到固定数量的token来运作。这些压缩后的token经过精细处理，而更高分辨率的token则通过计算成本更低的层传递。这两种token集之间的信息共享是通过双向交叉注意力机制实现的。这种方法具有多个优点 - (a) 通过标准的高级运算符，易于在标准 ML 加速器（GPU/TPU）上实现，(b) 适用于标准 ViT 及其变体，因此可以推广到各种任务，(c) 可以处理不同的token化和注意力方法。LookupViT 还为压缩后的token提供了灵活性，从而能够在单个训练模型中进行性能-计算权衡。我们展示了 LookupViT 在多个领域的有效性 - (a) 图像分类（ImageNet-1K 和 ImageNet-21K），(b) 视频分类（Kinetics400 和 Something-Something V2），(c) 使用冻结编码器的图像字幕（COCO-Captions）。LookupViT 在这些领域中将 FLOPs 减少了 2 倍，同时保持或提高了准确性。此外，LookupViT 还展示了在图像分类（ImageNet-C、R、A、O）上的开箱即用鲁棒性和泛化能力，比 ViT 提高了高达 4%。||
|**2024-07-17**|[Toward INT4 Fixed-Point Training via Exploring Quantization Error for Gradients](http://arxiv.org/abs/2407.12637)|null|网络量化通常将全精度权重和/或激活值转换为低比特固定点值，以加速推理过程。最近的网络量化方法进一步将梯度离散化为低比特固定点值，从而实现高效的训练。它们通常使用梯度的最小-最大范围设置量化间隔，或调整间隔以使整个梯度的量化误差最小化。在本文中，我们分析了低比特固定点训练中梯度的量化误差，并表明降低大梯度值的误差可以显著提高量化性能。基于此，我们推导了大梯度量化误差关于量化间隔的上界，并获得了最小化大梯度量化误差的间隔最优条件。我们还介绍了一种间隔更新算法，该算法自适应地调整量化间隔，以保持对大梯度的较小量化误差。实验结果证明了我们的量化方法对各种网络架构和比特宽度组合在各种任务上的有效性，包括图像分类、目标检测和超分辨率。||
|**2024-07-17**|[CerberusDet: Unified Multi-Task Object Detection](http://arxiv.org/abs/2407.12632)|**[link](https://github.com/ai-forever/cerberusdet)**|目标检测是计算机视觉中的核心任务。多年来，众多模型的开发显著提高了性能。然而，这些传统模型通常受到训练数据和定义的类别逻辑的限制。随着近年来语言视觉模型的兴起，出现了不受这些固定类别限制的新方法。尽管具有灵活性，但与具有固定类别的传统模型相比，此类开放词汇检测模型在准确性方面仍然不足。同时，当需要扩展类别或合并不同数据集进行训练时，更准确的特定于数据的模型面临挑战。由于不同的逻辑或类别定义冲突，后者通常无法组合，因此难以在不影响模型性能的情况下改进模型。在本文中，我们介绍了 CerberusDet，这是一个多头模型框架，旨在处理多个目标检测任务。所提出的模型建立在 YOLO 架构之上，并有效地共享来自骨干和颈部组件的视觉特征，同时保持独立的任务头。这种方法使 CerberusDet 能够非常有效地执行，同时仍然提供最佳结果。我们在 PASCAL VOC 数据集和 Objects365 数据集中的其他类别上评估了该模型，以证明其能力。CerberusDet 实现了与最先进的特定于数据的模型相当的结果，推理时间减少了 36%。与顺序运行单个模型相比，一起训练的任务越多，所提出的模型就越有效。训练和推理代码以及模型均以开源形式提供 (https://github.com/ai-forever/CerberusDet)。||
|**2024-07-17**|[Strawberry detection and counting based on YOLOv7 pruning and information based tracking algorithm](http://arxiv.org/abs/2407.12614)|null|草莓产业为佛罗里达州带来了巨大的经济效益，但监测草莓生长和产量的过程劳动密集且成本高昂。基于机器学习的检测和跟踪方法的开发已被用于帮助自动监测和预测草莓产量，但由于之前的研究仅将深度学习方法应用于花和果实的检测，而没有考虑机器视觉系统收集的图像数据集的独特特征，因此改进有限。本研究提出了一种深度学习模型（YOLOv7及其变体）检测头的优化剪枝方法，可以实现对草莓花、幼果和成熟果实的快速、精确检测。此后，一种增强的目标跟踪算法，称为基于信息的跟踪算法（IBTA），利用最佳检测结果，去除卡尔曼滤波器，并整合移动方向、速度和空间信息，以提高草莓花和果实跟踪的精度。提出的跨YOLOv7变体的检测头剪枝方法，特别是具有检测头3的Pruning-YOLOv7-tiny和具有检测头2和3的Pruning-YOLOv7-tiny，分别实现了最佳的推理速度（每秒163.9帧）和检测精度（89.1%）。另一方面，通过与质心跟踪算法（CTA）的比较，证明了IBTA的效果，IBTA的多目标跟踪精度（MOTA）和多目标跟踪精度（MOTP）分别比CTA高12.3%和6.0%。此外，其他目标跟踪评估指标，包括IDF1、IDR、IDP、MT和IDs，表明IBTA在草莓花和果实跟踪方面优于CTA。||
|**2024-07-18**|[Benchmarking Robust Self-Supervised Learning Across Diverse Downstream Tasks](http://arxiv.org/abs/2407.12588)|**[link](https://github.com/layer6ai-labs/ssl-robustness)**|大规模视觉模型因其前所未有的性能以及跨下游任务的多功能性，已成为许多应用程序中不可或缺的一部分。然而，这些基础模型的稳健性主要在单个任务（即图像分类）中进行了探索。其他常见视觉任务（如语义分割和深度估计）的脆弱性在很大程度上仍然未知。我们对跨多个下游任务的自监督视觉编码器的对抗鲁棒性进行了全面的实证评估。我们的攻击在编码器嵌入空间和下游任务输出级别进行操作。在这两种情况下，目前仅针对分类测试的最先进的对抗性微调技术都显着降低了其他任务的干净和鲁棒性能。由于基础模型的目的是同时满足多个应用程序，因此我们的研究结果表明需要更广泛地增强编码器鲁棒性。我们的代码可在 ${github.com/layer6ai-labs/ssl-robustness}$ 获取。||
|**2024-07-16**|[Improving Unsupervised Video Object Segmentation via Fake Flow Generation](http://arxiv.org/abs/2407.11714)|**[link](https://github.com/suhwan-cho/FakeFlow)**|无监督视频目标分割（VOS），也称为视频显著目标检测，旨在像素级别检测视频中最突出的目标。近年来，利用 RGB 图像和光流图的两流方法受到了广泛关注。然而，训练数据的有限性仍然是一个巨大的挑战。在本研究中，我们提出了一种新的数据生成方法，可以从单个图像模拟生成伪光流，从而为稳定的网络学习创建大规模训练数据。受光流图高度依赖于深度图的观察结果的启发，我们通过细化和增强每个图像的估计深度图来生成伪光流。通过结合我们模拟的图像-流对，我们在不依赖复杂模块的情况下，在所有公共基准数据集上实现了新的最先进性能。我们相信，我们的数据生成方法代表了未来 VOS 研究的潜在突破。||
|**2024-07-16**|[Relation DETR: Exploring Explicit Position Relation Prior for Object Detection](http://arxiv.org/abs/2407.11699)|**[link](https://github.com/xiuqhou/relation-detr)**|本文提出了一种增强DETR（DEtection TRansformer）收敛性和性能的通用方案。我们从一个新的角度研究了Transformer中的慢收敛问题，认为它源于自注意力机制对输入没有引入结构性偏差。为了解决这个问题，我们在使用所提出的定量宏观相关性（MC）度量验证其统计显著性后，探索将位置关系先验作为注意力偏差来增强目标检测。我们提出的方法称为Relation-DETR，它引入了一个编码器来构建位置关系嵌入，用于渐进式注意力细化，这进一步将DETR的传统流式管道扩展为对比关系管道，以解决非重复预测和正监督之间的冲突。在通用和特定任务数据集上的大量实验表明了我们方法的有效性。在相同的配置下，Relation-DETR取得了显著的改进（与DINO相比，AP提高了+2.0%），达到了最先进的性能（1倍设置下AP为51.7%，2倍设置下AP为52.1%），并且在COCO val2017上实现了比现有DETR检测器快得多的收敛速度（仅用2个训练周期就获得了超过40%的AP）。此外，所提出的关系编码器是一个通用的即插即用组件，可以为理论上任何类似DETR的方法带来明显的改进。此外，我们还引入了一个类别无关的目标检测数据集SA-Det-100k。在该数据集上的实验结果表明，所提出的显式位置关系使AP有了1.3%的明显提高，突出了其在通用目标检测方面的潜力。代码和数据集可在https://github.com/xiuqhou/Relation-DETR获取。||
|**2024-07-16**|[Probing the Efficacy of Federated Parameter-Efficient Fine-Tuning of Vision Transformers for Medical Image Classification](http://arxiv.org/abs/2407.11573)|null|随着大型预训练Transformer模型的出现，针对各种下游任务微调这些模型成为一个关键问题。训练数据的缺乏、数据孤岛的存在以及严格的隐私限制加剧了医学影像领域中的微调问题，因此迫切需要能够协同微调预训练模型的算法。此外，这些模型的庞大规模使得必须使用参数高效微调（PEFT）来减少联邦学习中的通信负担。在这项工作中，我们系统地研究了各种联邦PEFT策略，用于将视觉Transformer（ViT）模型（在大规模自然图像数据集上预训练）应用于医学图像分类。除了评估已知的PEFT技术外，我们还引入了PEFT算法的新联邦变体，例如视觉提示调优（VPT）、视觉提示的低秩分解、随机块注意力微调以及混合PEFT方法（如低秩自适应（LoRA）+VPT）。此外，我们还进行了全面的实证分析，以确定适用于联邦设置的最佳PEFT方法，并了解数据分布对联邦PEFT的影响，特别是在域外（OOD）和非独立同分布（non-IID）数据的情况下。这项研究的关键见解是，虽然大多数联邦PEFT方法在域内迁移中表现良好，但在处理OOD和非IID场景时，精度和效率之间存在巨大的权衡，这在医学影像中很常见。具体来说，微调/交换参数每减少一个数量级，精度就会下降4%。因此，初始模型的选择对于联邦PEFT至关重要。如果可能的话，最好使用从域内医学图像数据中学习到的医学基础模型，而不是通用视觉模型。||
|**2024-07-16**|[Bridge Past and Future: Overcoming Information Asymmetry in Incremental Object Detection](http://arxiv.org/abs/2407.11499)|**[link](https://github.com/isee-laboratory/bpf)**|在增量目标检测领域，知识蒸馏已被证明是缓解灾难性遗忘的有效方法。然而，以往的工作侧重于保留旧模型的知识，而忽略了图像可能同时包含来自过去、现在和未来阶段的类别。由于不同阶段对前景目标的定义不同，目标的共现使得优化目标在不同阶段不一致，这极大地限制了模型的性能。为了克服这个问题，我们提出了一种名为“桥接过去与未来”（BPF）的方法，它可以跨阶段对齐模型，确保一致的优化方向。此外，我们还提出了一种新颖的“未来蒸馏”（DwF）损失函数，充分利用背景概率来减轻对旧类别的遗忘，同时确保学习新类别的高度适应性。我们在 Pascal VOC 和 MS COCO 基准测试集上进行了广泛的实验。在没有记忆的情况下，BPF 在各种设置下都优于当前最先进的方法。代码可在 https://github.com/iSEE-Laboratory/BPF 获取。||
|**2024-07-16**|[Crowd-SAM: SAM as a Smart Annotator for Object Detection in Crowded Scenes](http://arxiv.org/abs/2407.11464)|**[link](https://github.com/felixcaae/crowdsam)**|在计算机视觉领域，目标检测是一项重要的任务，在许多场景中都有应用。然而，获取大量的标签可能具有挑战性，尤其是在拥挤的场景中。最近，Segment Anything Model (SAM) 被提出作为一种强大的零样本分割器，为实例分割任务提供了一种新颖的方法。然而，SAM 及其变体在处理拥挤和遮挡场景中的物体时，其准确性和效率往往会受到影响。在本文中，我们介绍了 Crowd-SAM，这是一个基于 SAM 的框架，旨在以少量可学习参数和最小限度标记图像的成本提高 SAM 在拥挤和遮挡场景中的性能。我们引入了高效的提示采样器 (EPS) 和部分-整体判别网络 (PWD-Net)，增强了拥挤场景中的掩码选择和准确性。尽管 Crowd-SAM 结构简单，但它在 CrowdHuman 和 CityPersons 等多个基准测试中，可与最先进的 (SOTA) 全监督目标检测方法相媲美。我们的代码可在 https://github.com/FelixCaae/CrowdSAM 获取。||
|**2024-07-16**|[Leveraging Segment Anything Model in Identifying Buildings within Refugee Camps (SAM4Refugee) from Satellite Imagery for Humanitarian Operations](http://arxiv.org/abs/2407.11381)|**[link](https://github.com/yunyagaotree/sam-adapter-for-refugee-dwelling-extraction)**|利用高分辨率卫星图像更新带有难民营的建筑物覆盖范围可以支持相关的人道主义行动。本研究探讨了利用“分割一切模型”（SAM）及其分支之一 SAM-Adapter 进行语义分割任务，以从卫星图像中提取建筑物。SAM-Adapter 是 SAM 的轻量级改编版本，在不同难民营的提取任务中，它都是一个强大的工具。我们的研究证明，与其他经典（例如 U-Net）或高级语义分割模型（例如 Transformer）相比，SAM-Adapter 在数据可用性有限的情况下表现出色。此外，还强调了放大技术对模型性能的影响，事实证明，超分辨率 (SR) 模型等方法对于提高模型性能非常宝贵。此外，该研究还揭示了一些有趣的现象，包括在使用放大图像数据进行训练时，模型在第一个训练时期快速收敛，这为未来的研究提供了机会。涵盖从数据准备、模型训练、模型推理到预测掩码的 Shapefile 生成的每个步骤的代码都可以在 GitHub 存储库中找到，从而使更广泛的科学界和人道主义行动受益。||
|**2024-07-16**|[Generative AI Driven Task-Oriented Adaptive Semantic Communications](http://arxiv.org/abs/2407.11354)|null|面向任务的语义通信 (TOSC) 被认为是一种很有前途的通信框架，可用于各种人工智能 (AI) 任务驱动型应用。现有的 TOSC 框架侧重于提取源数据的完整语义特征，并学习低维信道输入以在有限的带宽资源内传输它们。 虽然传输完整的语义特征可以保持数据含义的完整性，但这种方法无法达到 TOSC 的性能阈值。在本文中，我们提出了一种面向任务的自适应语义通信 (TasCom) 框架，旨在通过仅发送与任务相关的语义特征来有效地促进 AI 任务的执行。在 TasCom 框架中，我们首先提出了一种基于生成式人工智能 (GAI) 架构的生成式联合信源信道编码 (G-JSCC) 以实现高效的语义传输。然后，提出了一种自适应编码控制器 (ACC)，以找到所提出的 G-JSCC 的最佳编码方案，该方案允许对 AI 任务有重大贡献的语义特征优先占用有限的带宽资源进行无线传输。此外，我们提出了一种生成式训练算法来训练所提出的 TasCom 以获得最佳性能。仿真结果表明，所提出的 TasCom 在所有考虑的信道条件下，在目标检测和实例分割任务上均优于现有的 TOSC 和传统编解码器方案。||
|**2024-07-16**|[LaMI-DETR: Open-Vocabulary Detection with Language Model Instruction](http://arxiv.org/abs/2407.11335)|**[link](https://github.com/eternaldolphin/lami-detr)**|现有的开放词汇目标检测方法利用视觉语言模型 (VLMs)（如 CLIP）强大的开放词汇识别能力来增强性能。然而，出现了两个主要挑战：(1) 概念表示的缺陷，CLIP 文本空间中的类别名称缺乏文本和视觉知识。(2) 对基本类别的过度拟合倾向，在从 VLM 到检测器的迁移过程中，开放词汇知识偏向于基本类别。为了应对这些挑战，我们提出了语言模型指令 (LaMI) 策略，该策略利用视觉概念之间的关系，并在一个简单而有效的类 DETR 检测器（称为 LaMI-DETR）中应用它们。LaMI 利用 GPT 构建视觉概念，并利用 T5 研究类别之间的视觉相似性。这些类别间关系改进了概念表示，并避免了对基本类别的过度拟合。综合实验验证了我们的方法在相同的严格设置下优于现有方法，并且不依赖于外部训练资源。LaMI-DETR 在 OV-LVIS 上实现了 43.4 的罕见框 AP，超过了之前的最佳结果 7.8 个罕见框 AP。||
|**2024-07-16**|[TCFormer: Visual Recognition via Token Clustering Transformer](http://arxiv.org/abs/2407.11321)|**[link](https://github.com/zengwang430521/tcformer)**|Transformer模型在计算机视觉领域得到广泛应用并取得了显著成功。大多数最先进的方法将图像分割成规则网格，并使用视觉标记表示每个网格区域。然而，固定的标记分布忽略了不同图像区域的语义含义，导致性能欠佳。为了解决这个问题，我们提出了标记聚类Transformer（TCFormer），它根据语义含义生成动态视觉标记。我们的动态标记具有两个关键特征：（1）使用相同的视觉标记表示具有相似语义含义的图像区域，即使这些区域不邻接；（2）集中于具有宝贵细节的区域，并使用精细标记表示它们。通过对图像分类、人体姿态估计、语义分割和目标检测等各种应用的广泛实验，我们证明了TCFormer的有效性。这项工作的代码和模型可在https://github.com/zengwang430521/TCFormer获取。||
|**2024-07-16**|[Zero-Shot Adaptation for Approximate Posterior Sampling of Diffusion Models in Inverse Problems](http://arxiv.org/abs/2407.11288)|null|扩散模型已成为解决反问题的强大生成技术。尽管扩散模型已成功应用于各种成像反问题，但这些模型需要许多步骤才能收敛，导致推理时间较长。最近，扩散模型出现了一种趋势，即采用复杂的噪声调度，在较低的噪声水平下更频繁地迭代时间步长，从而改善图像生成和收敛速度。然而，将这些想法应用于解决扩散模型的反问题仍然具有挑战性，因为当使用经验调整来确定正向模型对数似然项权重时，这些噪声调度表现不佳。为了解决这些挑战，我们提出了零样本近似后验采样 (ZAPS) 方法，该方法利用了与零样本物理驱动深度学习的联系。ZAPS 固定了采样步数，并使用零样本训练和物理引导的损失函数来学习每个不规则时间步长的对数似然权重。我们将 ZAPS 应用于最近提出的扩散后验采样方法作为基线，尽管 ZAPS 也可以与其他后验采样扩散模型一起使用。我们进一步使用可学习对角元素的对角化方法来逼近先验对数的 Hessian，以提高计算效率。这些参数在给定计算预算下通过固定次数的 epochs 进行优化。我们对各种噪声反问题的研究结果（包括高斯和运动去模糊、修复和超分辨率）表明，ZAPS 减少了推理时间，提高了对不规则噪声调度的鲁棒性，并提高了重建质量。代码可在 https://github.com/ualcalar17/ZAPS 获取。||
|**2024-07-12**|[Region Attention Transformer for Medical Image Restoration](http://arxiv.org/abs/2407.09268)|**[link](https://github.com/yaziwel/region-attention-transformer-for-medical-image-restoration)**|基于Transformer的方法在医学图像恢复方面表现出令人印象深刻的结果，这归功于其在空间维度上的多头自注意力（MSA）机制。然而，大多数现有的Transformer在固定且粗略划分的区域内进行注意力计算（例如，整幅图像或固定块），导致来自不相关区域的干扰和连续图像内容的碎片化。为了克服这些挑战，我们引入了一种新颖的区域注意力Transformer（RAT），它利用了基于区域的多头自注意力机制（R-MSA）。R-MSA使用强大的Segment Anything Model (SAM)将输入图像动态地划分为非重叠的语义区域，然后在这些区域内执行自注意力。这种区域划分更加灵活和可解释，确保只有来自相似语义区域的像素相互补充，从而消除了来自不相关区域的干扰。此外，我们引入了一个焦点区域损失来引导我们的模型自适应地关注恢复高难度区域。大量实验表明，RAT在各种医学图像恢复任务中是有效的，包括PET图像合成、CT图像去噪和病理图像超分辨率。代码可在\href{https://github.com/Yaziwel/Region-Attention-Transformer-for-Medical-Image-Restoration.git}{https://github.com/RAT}获取。||
|**2024-07-12**|[Open Vocabulary Multi-Label Video Classification](http://arxiv.org/abs/2407.09073)|null|预训练的视觉语言模型 (VLM) 在开放词汇计算机视觉任务（如图像分类、目标检测和图像分割）方面取得了显著进展。最近的一些工作集中于将 VLM 扩展到视频中的开放词汇单标签动作分类。然而，先前的方法在整体视频理解方面存在不足，整体视频理解需要能够在开放词汇环境中同时识别多个动作和实体（例如视频中的对象）。我们将此问题表述为开放词汇多标签视频分类，并提出一种使预训练的 VLM（如 CLIP）适应解决此任务的方法。我们利用大型语言模型 (LLM) 为 VLM 提供有关类别标签的语义指导，通过两个关键贡献来提高其开放词汇性能。首先，我们提出了一种端到端可训练架构，该架构学习提示 LLM 生成 CLIP 文本编码器的软属性，使其能够识别新类别。其次，我们将时间建模模块集成到 CLIP 的视觉编码器中，以有效地对视频概念的时空动态进行建模，并提出一种新颖的正则化微调技术，以确保在视频域中强大的开放词汇分类性能。我们广泛的实验结果证明了我们的方法在多个基准数据集上的有效性。||
|**2024-07-12**|[DroneMOT: Drone-based Multi-Object Tracking Considering Detection Difficulties and Simultaneous Moving of Drones and Objects](http://arxiv.org/abs/2407.09051)|null|静态平台上的多目标跟踪（MOT），例如监控摄像头，已经取得了显著进展，各种范式提供了优异的性能。然而，传统的MOT方法在无人机等动态平台上的有效性会显著降低。这种下降归因于无人机场景下MOT的独特挑战：（1）图像平面中的物体通常较小、模糊且经常被遮挡，难以检测和识别；（2）无人机会从不同角度移动和观察物体，导致物体预测位置和特征嵌入的不可靠性。本文提出了DroneMOT，它首先提出了一种双域集成注意力（DIA）模块，该模块考虑了无人机的快速移动，以增强对小型、模糊和遮挡物体的无人机目标检测和特征嵌入。然后，引入了一种创新的运动驱动关联（MDA）方案，该方案考虑了无人机和物体的并发运动。在MDA中，提出了一种自适应特征同步（AFS）技术来更新从不同角度看到的物体特征。此外，采用了一种基于双运动的预测（DMP）方法来预测物体位置。最后，将改进的特征嵌入和预测位置相结合，以增强物体关联。在VisDrone2019-MOT和UAVDT数据集上的综合评估表明，DroneMOT在无人机MOT领域相较于现有技术提供了显著的性能提升。||
|**2024-07-12**|[CAMP: Continuous and Adaptive Learning Model in Pathology](http://arxiv.org/abs/2407.09030)|**[link](https://github.com/QuIIL/CAMP)**|病理学中存在着大量的诊断任务。传统的计算病理学将这些任务表述为独立的图像分类问题并分别解决，导致计算效率低下和成本高昂。为了应对这些挑战，我们提出了一种通用的、统一的、通用的框架，称为病理学中的持续自适应学习模型 (CAMP)，用于病理图像分类。CAMP 是一种生成式、高效且自适应的分类模型，它可以利用特定于病理学的先验知识和学习特定于任务的知识，以最小的计算成本不断适应任何分类任务，而不会忘记现有任务的知识。我们在 22 个数据集上评估了 CAMP，包括 1,171,526 个图像块和 11,811 张病理切片，涵盖 17 个分类任务。CAMP 在各种数据集和任务的图像块和切片级别上均实现了最先进的分类性能，与传统的分类模型相比，计算时间减少了高达 94%，存储内存减少了 85%。我们的结果表明，CAMP 可以为病理图像分类带来根本性的变革，为全面数字化和计算机化的病理实践铺平道路。||
|**2024-07-11**|[Manipulating a Tetris-Inspired 3D Video Representation](http://arxiv.org/abs/2407.08885)|null|视频摘要是一种以保留视频活动的方式执行视频压缩的技术。这种技术在监控应用中特别有用。尽管它仍然是一个新兴的研究领域，但在过去的二十年中，已经提出了几种不同的方法，这些方法随着应用、优化类型、数据馈送的性质等而变化。这些算法所需的主要数据来自某种对象跟踪方法。在本文中，我们讨论了适用于不同应用的不同时空数据表示。我们还提出了视频摘要算法的正式定义。我们进一步讨论了该定义的假设和修改，以简化问题的版本。我们探索了应用打包算法来解决视频摘要问题。由于数据的性质是三维的，我们在讨论中考虑了 3D 打包问题。本文还对不同的视频摘要方法和打包问题进行了广泛的文献综述。最后，我们研究了该算法的不同应用，以及前面讨论的不同数据表示如何简化问题。我们还讨论了本次讨论后可以探索的未来研究方向。||
|**2024-07-11**|[Local Clustering for Lung Cancer Image Classification via Sparse Solution Technique](http://arxiv.org/abs/2407.08800)|**[link](https://github.com/zzzzms/LocalClustering4LungCancer)**|在这项工作中，我们建议使用一种基于稀疏解技术的局部聚类方法来研究医学图像，特别是肺癌图像分类任务。我们将图像视为加权图中的顶点，将一对图像之间的相似性视为图中的边。可以假设同一个簇内的顶点共享相似的特征和属性，因此使得图聚类技术的应用对图像分类非常有用。最近，人们发现基于线性系统稀疏解的图聚类方法比传统的聚类方法（如谱聚类）能够更有效地识别聚类。我们建议使用两种新开发的基于线性系统稀疏解的局部聚类方法进行图像分类。此外，我们采用基于箱样条的紧框架小波方法对这些图像进行去噪，并帮助在聚类之前构建更好的邻接矩阵。我们的方法在图像分类方面表现非常有效。与其他最先进的方法相比，我们的方法效率更高，并且优于或等于它们。最后，我们将指出两种图像变形方法来构建更多的人工图像数据以增加标记图像的数量。||
|**2024-07-11**|[Approaching Outside: Scaling Unsupervised 3D Object Detection from 2D Scene](http://arxiv.org/abs/2407.08569)|**[link](https://github.com/ruiyang-061x/lise)**|无监督三维物体检测的目标是在没有明确监督信号的情况下，准确地检测非结构化环境中的物体。考虑到激光雷达点云的稀疏性，这项任务在检测远处或小型物体时，由于其固有的稀疏性和有限的空间分辨率，往往会导致性能下降。在本文中，我们率先尝试将激光雷达数据与二维图像相结合用于无监督三维物体检测，并介绍了一种称为LiDAR-2D自适应学习（LiSe）的新方法。我们认为，RGB图像可以作为激光雷达数据的有价值补充，提供精确的二维定位线索，尤其是在某些物体只有少量激光雷达点的情况下。考虑到两种模式的独特性，我们的框架设计了一个自适应学习流程，该流程结合了自适应采样和弱模型聚合策略。自适应采样策略在训练期间动态调整伪标签的分布，以对抗模型过度拟合易于检测到的样本（例如附近和大型物体）的趋势。这样做可以确保在不同的物体尺度和距离上实现平衡的学习轨迹。弱模型聚合组件整合了在不同伪标签分布下训练的模型的优势，最终形成一个强大而鲁棒的最终模型。实验评估验证了我们提出的LiSe方法的有效性，与现有技术相比，在nuScenes数据集上实现了+7.1% AP $_{BEV}$和+3.4% AP$_{3D}$的显著改进，在Lyft数据集上实现了+8.3% AP$_{BEV}$和+7.4% AP$_{3D}$ 的显著改进。||
|**2024-07-11**|[Projecting Points to Axes: Oriented Object Detection via Point-Axis Representation](http://arxiv.org/abs/2407.08489)|null|本文介绍了一种用于定向目标检测的点轴表示法，强调了其灵活性和几何直观性，它包含两个关键组件：点和轴。1）点描绘了目标的空间范围和轮廓，提供了详细的形状描述。2）轴定义了目标的主要方向，提供了对精确检测至关重要的方向线索。点轴表示法将位置和旋转解耦，解决了传统基于边界框的方法中常见的损失不连续性问题。为了在不引入额外标注的情况下进行有效优化，我们提出了最大投影损失来监督点集学习，以及交叉轴损失来进行鲁棒的轴表示学习。此外，利用这种表示法，我们提出了 Oriented DETR 模型，将 DETR 框架无缝集成，用于精确的点轴预测和端到端检测。实验结果表明，在定向目标检测任务中，该方法的性能有了显著提高。||
|**2024-07-11**|[Global Spatial-Temporal Information-based Residual ConvLSTM for Video Space-Time Super-Resolution](http://arxiv.org/abs/2407.08466)|null|时空视频超分辨率技术可以将低帧率、低分辨率的视频转换为高帧率、高分辨率的视频，从而增强视觉体验并促进更有效的信息传播。我们提出了一种用于时空视频超分辨率的卷积神经网络 (CNN)，称为 GIRNet。为了生成高度准确的特征并提高性能，所提出的网络集成了具有可变形卷积的特征级时间插值模块和基于全局时空信息的残差卷积长短期记忆 (convLSTM) 模块。在特征级时间插值模块中，我们利用可变形卷积来适应不同场景位置的对象的变形和尺度变化。与传统的卷积相比，这为从运动物体中提取特征提供了一种更有效的解决方案。我们的网络有效地利用前向和后向特征信息来确定帧间偏移，从而直接生成插值帧特征。在基于全局时空信息的残差 convLSTM 模块中，第一个 convLSTM 用于从输入特征中导出全局时空信息，第二个 convLSTM 使用先前计算的全局时空信息特征作为其初始单元状态。第二个 convLSTM 采用残差连接来保留空间信息，从而增强输出特征。在 Vimeo90K 数据集上的实验表明，所提出的方法在峰值信噪比（分别比 STARnet、TMNet 和 3DAttGAN 高 1.45 dB、1.14 dB 和 0.02 dB）、结构相似性指数（分别比 STARnet、TMNet 和 3DAttGAN 高 0.027、0.023 和 0.006）和视觉效果方面均优于现有技术。||
|**2024-07-11**|[Semi-Supervised Object Detection: A Survey on Progress from CNN to Transformer](http://arxiv.org/abs/2407.08460)|null|半监督学习技术的显著进步促使研究人员探索其在计算机视觉领域目标检测任务中的潜力。半监督目标检测 (SSOD) 利用少量标记数据集和大量未标记数据集的组合。这种方法有效地减少了对大型标记数据集的依赖，而获取这些数据集通常既昂贵又耗时。最初，SSOD 模型在有效利用未标记数据和管理未标记数据生成的伪标签中的噪声方面遇到了挑战。然而，最近的许多进展已经解决了这些问题，从而大大提高了 SSOD 的性能。本文全面回顾了从卷积神经网络 (CNN) 到 Transformer 的 27 项 SSOD 方法的最新发展。我们深入研究了半监督学习的核心组件及其与目标检测框架的集成，涵盖了数据增强技术、伪标签策略、一致性正则化和对抗训练方法。此外，我们对各种 SSOD 模型进行了比较分析，评估了它们的性能和架构差异。我们的目标是激发进一步的研究兴趣，以克服现有挑战并探索半监督学习在目标检测中的新方向。||
|**2024-07-11**|[PowerYOLO: Mixed Precision Model for Hardware Efficient Object Detection with Event Data](http://arxiv.org/abs/2407.08272)|null|车载解决方案中目标检测系统的性能必须尽可能高，响应时间要短，并且由于通常采用电池供电，因此能耗要低。因此，在设计此类解决方案时，我们面临着嵌入式视觉系统特有的挑战：将内存和计算复杂度高的算法适配到小型低功耗设备中的问题。在本文中，我们提出了 PowerYOLO——一种混合精度解决方案，它针对此类应用的三个基本要素。首先，我们提出了一种基于动态视觉传感器 (DVS) 的系统，这是一种新型传感器，具有低功耗要求，并且在光照条件变化的情况下也能很好地工作。正是这些特性使得事件相机在某些应用中可能优于帧相机。其次，为了确保高精度以及低内存和计算复杂度，我们建议对 YOLO 检测器的卷积权重使用 4 位宽度的二进制幂 (PoT) 量化，并对所有其他参数进行线性量化。最后，我们采用 PoT 方案并用位移代替乘法，通过一种特殊的卷积-批量归一化融合方案来提高此类解决方案的硬件加速效率。与标准方法相比，使用特定传感器、PoT 量化和特殊的批量归一化融合方案，形成了一个独特的系统，该系统将内存复杂度降低了近 8 倍，并大大简化了计算。这个高效的系统在 GEN1 DVS 数据集上实现了 0.301 的高 mAP 精度，标志着这种压缩模型的新水平。||
|**2024-07-11**|[Wind Power Assessment based on Super-Resolution and Downscaling -- A Comparison of Deep Learning Methods](http://arxiv.org/abs/2407.08259)|null|风力涡轮机的有效放置依赖于准确的当地风速预测。气候预测为了解长期风速条件提供了宝贵的见解，但其空间数据分辨率通常不足以进行精确的风力发电预测。深度学习方法，特别是为图像超分辨率开发的模型，为通过提高气候模型的空间分辨率来弥合这种尺度差距提供了一种有前景的解决方案。在本文中，我们比较了各种深度学习模型在两个不同任务上的性能：超分辨率（我们将人工粗化的 ERA5 数据映射到其原始分辨率）和降尺度（我们将原生 ERA5 映射到高分辨率 COSMO-REA6 数据）。我们根据模型在下游应用中预测长期风力的表现对其进行评估，强调空间风速分辨率对风力估计的影响。我们的研究结果强调了将模型和评估指标与其特定下游应用保持一致的重要性。我们证明，扩散模型通过更好地保持风速的分布和物理特性，在估算风能潜力方面优于其他模型。||
|**2024-07-11**|[Knowledge distillation to effectively attain both region-of-interest and global semantics from an image where multiple objects appear](http://arxiv.org/abs/2407.08257)|**[link](https://github.com/seonwhee-genome/rvernet)**|基于卷积神经网络（CNN）和Transformer的模型一直在稳步改进，并已应用于各种计算机视觉下游任务。然而，在目标检测任务中，准确地定位和分类图像中几乎无限种类的食物仍然具有挑战性。为了解决这些问题，我们首先使用Segment Anything模型（SAM）将食物分割为感兴趣区域（ROI），并将ROI以外的区域遮盖为黑色像素。这个过程将问题简化为单一分类，其标注和训练比目标检测简单得多。我们将只保留ROI的图像作为输入，对各种现成的模型进行微调，这些模型编码了它们自己的归纳偏差。其中，数据高效图像Transformer（DeiT）具有最佳的分类性能。然而，当食物的形状和纹理相似时，仅ROI图像的上下文特征不足以进行准确分类。因此，我们引入了一种新型的组合架构RveRNet，它由ROI、额外ROI和集成模块组成，使其能够同时考虑ROI和全局上下文。在对模糊食物图像进行分类时，RveRNet的F1分数比其他单个模型高10%。如果RveRNet的模块是具有CNN知识蒸馏的DeiT，则性能最佳。我们研究了如何使架构对排列和易位引起的输入噪声具有鲁棒性。结果表明，CNN教师的知识有多少可以提炼到DeiT和DeiT的先天优势之间存在权衡。代码公开于：https://github.com/Seonwhee-Genome/RveRNet。||
|**2024-07-11**|[GraphMamba: An Efficient Graph Structure Learning Vision Mamba for Hyperspectral Image Classification](http://arxiv.org/abs/2407.08255)|**[link](https://github.com/ahappyyang/GraphMamba)**|高光谱图像分类中，有效的光谱序列和地理空间信息的提取一直是热门话题。在光谱序列特征捕获方面，RNN和Transformer由于其远程特征捕获能力，已成为主流分类框架。在空间信息聚合方面，CNN通过增强感受野来尽可能多地保留完整的空间信息。然而，光谱特征捕获架构的计算效率较低，并且CNN缺乏感知空间上下文信息的灵活性。为了解决这些问题，本文提出了GraphMamba——一种高效的图结构学习视觉Mamba分类框架，该框架充分考虑了HSI特性，以实现深层次的空间-光谱信息挖掘。具体来说，我们提出了一种新的高光谱视觉GraphMamba处理范式（HVGM），该范式通过构建空间-光谱立方体来保留空间-光谱特征，并利用线性光谱编码来增强后续任务的可操作性。GraphMamba的核心组件包括用于提高计算效率的HyperMamba模块和用于自适应空间上下文感知的SpectralGCN模块。HyperMamba通过采用全局掩码（GM）来减轻杂波干扰，并引入了并行训练推理架构来缓解计算瓶颈。SpatialGCN结合了加权多跳聚合（WMA）空间编码，以关注高度相关的空间结构特征，从而灵活地聚合上下文信息，同时减轻空间噪声干扰。在三个不同尺度的真实HSI数据集上进行了广泛的实验，与最先进的分类框架相比，GraphMamba取得了最佳性能。||
|**2024-07-11**|[DMM: Disparity-guided Multispectral Mamba for Oriented Object Detection in Remote Sensing](http://arxiv.org/abs/2407.08132)|null|多光谱目标检测面临着模态间和模态内差异的挑战。最近的研究经常依赖于基于Transformer的模型来解决这些问题并实现跨模态融合检测。然而，Transformer的平方计算复杂度限制了它们的性能。受Mamba在长序列任务中效率和较低复杂度的启发，我们提出了视差引导的多光谱Mamba（DMM），这是一个多光谱目标检测框架，由视差引导的跨模态融合Mamba（DCFM）模块、多尺度目标感知注意力（MTA）模块和目标先验感知（TPA）辅助任务组成。DCFM模块利用模态之间的视差信息自适应地融合来自RGB和红外图像的特征，从而减轻模态间冲突。MTA模块旨在通过关注RGB模态内的相关目标区域来增强特征表示，解决模态内变化问题。TPA辅助任务利用单模态标签来指导MTA模块的优化，确保其关注目标及其局部上下文。在DroneVehicle和VEDAI数据集上的大量实验表明了我们方法的有效性，它在保持计算效率的同时优于最先进的方法。代码将在https://github.com/Another-0/DMM上提供。||
|**2024-07-10**|[MambaVision: A Hybrid Mamba-Transformer Vision Backbone](http://arxiv.org/abs/2407.08083)|**[link](https://github.com/nvlabs/mambavision)**|我们提出了一种名为 MambaVision 的新型混合 Mamba-Transformer 骨干网络，该网络专为视觉应用而设计。我们的核心贡献包括重新设计 Mamba 公式，以增强其对视觉特征进行高效建模的能力。此外，我们对将视觉Transformer（ViT）与 Mamba 相集成的可行性进行了全面的消融研究。我们的结果表明，在 Mamba 架构的最后几层配备多个自注意力块可以极大地提高模型捕获远程空间依赖关系的能力。基于我们的发现，我们引入了一系列具有层次结构的 MambaVision 模型，以满足各种设计标准。对于 ImageNet-1K 数据集上的图像分类任务，MambaVision 模型变体在 Top-1 准确率和图像吞吐量方面均达到了新的最先进水平 (SOTA)。在下游任务（例如 MS COCO 和 ADE20K 数据集上的目标检测、实例分割和语义分割）中，MambaVision 的性能优于规模相当的骨干网络，并展现出更佳的性能。代码：https://github.com/NVlabs/MambaVision。||
|**2024-07-09**|[CoLA: Conditional Dropout and Language-driven Robust Dual-modal Salient Object Detection](http://arxiv.org/abs/2407.06780)|**[link](https://github.com/ssecv/CoLA)**|深度/热信息有利于利用传统RGB图像检测显著性目标。然而，在双模态显著性目标检测（SOD）模型中，针对噪声输入和模态缺失的鲁棒性至关重要，但很少被研究。为了解决这个问题，我们引入了条件性丢弃和语言驱动（CoLA）框架，该框架包含两个核心组件。1）语言驱动质量评估（LQA）：利用带有提示学习器的预训练视觉语言模型，LQA在不需要额外质量标注的情况下重新校准图像贡献。这种方法有效地减轻了噪声输入的影响。2）条件性丢弃（CD）：一种学习方法，用于增强模型在模态缺失情况下的适应性，同时保持其在完整模态下的性能。CD作为一种插件式训练方案，将模态缺失视为条件，增强了各种双模态SOD模型的整体鲁棒性。大量实验表明，所提出的方法在模态完整和模态缺失两种情况下均优于最先进的双模态SOD模型。我们将开源代码。||
|**2024-07-09**|[Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting Region Captions](http://arxiv.org/abs/2407.06723)|null|人类使用组合性来描述复杂场景，使用带有链接和关系的简单文本描述来丰富描述。虽然视觉语言研究的目标是开发具有组合理解能力的模型，但这还没有反映在现有的数据集中，这些数据集在很大程度上仍然使用纯文本描述图像。在这项工作中，我们提出了一种新的标注策略，即基于图的字幕（GBC），它使用带有各种类型节点的标记图结构来描述图像。GBC 中的节点是通过以下方式创建的：首先，使用对象检测和密集字幕工具递归嵌套以发现和描述实体节点，然后在第二阶段通过使用新型节点、组合和实体之间的关系来突出显示，将它们链接在一起。由于所有 GBC 节点都包含纯文本描述，因此 GBC 保留了自然语言的灵活性，但也可以在其边缘编码分层信息。我们证明了 GBC 可以使用现成的多模态 LLM 和开放词汇检测模型自动生成，方法是构建一个新的数据集 GBC10M，为 CC12M 数据集中的大约 10M 张图像收集 GBC 标注。我们使用 GBC10M 来展示 GBC 发现的大量节点字幕，使用 CLIP 训练进行测量。我们表明，与其他数据集格式相比，使用 GBC 节点的注释（特别是存储在组合和关系节点中的注释）可以显着提高下游模型的性能。为了进一步探索 GBC 提供的机会，我们还提出了一种新的注意力机制，可以利用整个 GBC 图，并获得了令人鼓舞的实验结果，表明了合并图结构的额外好处。我们的数据集发布在 \url{https://huggingface.co/graph-based-captions}。||
|**2024-07-09**|[CTRL-F: Pairing Convolution with Transformer for Image Classification via Multi-Level Feature Cross-Attention and Representation Learning Fusion](http://arxiv.org/abs/2407.06673)|null|Transformer因其强大的容量和全局处理能力，在计算机视觉领域受到越来越多的关注。然而，Transformer是数据密集型的，与卷积神经网络（ConvNets）相比，其泛化能力受到限制，特别是在数据有限的情况下进行训练时，因为它们缺乏ConvNets中存在的内置空间归纳偏差。在本文中，我们致力于将卷积和Transformer的优势结合起来，以完成图像分类任务。为此，我们提出了一种新颖的轻量级混合网络，该网络通过表示学习融合和多级特征交叉注意（CTRL-F）将卷积与Transformer配对。我们的网络包括一个卷积分支和一个名为多级特征交叉注意（MFCA）的新型Transformer模块。MFCA模块对从不同卷积阶段获得的多级特征表示进行操作。它通过两个独立的Transformer分支处理从这些多级特征表示中提取的小块标记和大块标记，其中两个分支通过交叉注意机制进行通信和交换知识。我们使用称为自适应知识融合（AKF）和协作知识融合（CKF）的新型表示融合技术，将从卷积路径获得的局部响应与从MFCA模块获得的全局响应融合在一起。实验表明，我们的CTRL-F变体无论是在大数据上从头开始训练，还是在低数据情况下训练，都能获得最先进的性能。例如，CTRL-F在Oxford-102 Flowers和PlantVillage数据集上从头开始训练时，分别达到了82.24%和99.91%的top-1准确率，超过了最先进的模型，这展示了我们的模型在图像分类任务上的鲁棒性。代码位于：https://github.com/hosamsherif/CTRL-F||
|**2024-07-09**|[NoisyAG-News: A Benchmark for Addressing Instance-Dependent Noise in Text Classification](http://arxiv.org/abs/2407.06579)|null|现有的噪声标签学习研究主要集中在合成标签噪声上。虽然合成噪声具有明确的结构特性，但它往往不能准确地复制现实世界的噪声模式。近年来，人们一直在努力构建用于图像分类的、可泛化和可控的实例相关噪声数据集，这极大地促进了该领域鲁棒噪声学习的发展。然而，关于文本分类中噪声标签学习的研究仍然很少。为了更好地理解现实世界中文本分类环境中的标签噪声，我们通过人工标注构建了基准数据集NoisyAG-News。首先，我们分析了标注数据，以收集关于现实世界噪声的观察结果。我们定性和定量地证明了现实世界的噪声标签遵循实例相关的模式。随后，我们使用预训练语言模型和噪声处理技术，对NoisyAG-News及其相应的合成噪声数据集进行了全面的学习实验。我们的研究结果表明，虽然预训练模型对合成噪声具有鲁棒性，但它们在实例相关噪声面前却表现不佳，不同混淆程度的样本在训练和测试过程中表现出不一致的性能。这些现实世界的噪声模式提出了新的、重大的挑战，促使人们重新评估噪声标签处理方法。我们希望NoisyAG-News能够促进未来噪声标签学习解决方案的开发和评估。||
|**2024-07-09**|[UnmixingSR: Material-aware Network with Unsupervised Unmixing as Auxiliary Task for Hyperspectral Image Super-resolution](http://arxiv.org/abs/2407.06525)|null|基于深度学习 (DL) 的高光谱图像 (HIS) 超分辨率 (SR) 方法取得了显著成果，并在工业界和学术界引起了广泛关注。然而，大多数现有方法都在探索和学习低分辨率 (LR) 和高分辨率 (HR) HSI 之间的映射关系，导致在解决不适定 SR 问题时增加了不可靠性和不合理性。有趣的是，我们发现 LR 成像与混合像元现象相似。传感器阵列中的单个光电探测器接收由多种类别反射的反射信号，导致低空间分辨率和混合像元问题。受此观察的启发，本文提出了一种名为 UnmixingSR 的、组件感知的 HSI SR 网络，其中无监督 HU 作为辅助任务用于感知 HSI 的材料成分。我们将 HU 视为辅助任务，并通过探索 LR 和 HR  丰度之间的约束将其纳入 HSI SR 过程。我们没有仅仅学习 LR 和 HR HSI 之间的映射关系，而是利用 LR 丰度和 HR 丰度之间的联系来提高我们方法在解决 SR 问题时的稳定性。此外，所提出的解混过程可以作为即插即用的辅助任务嵌入到现有的深度 SR 模型中。高光谱实验结果表明，将解混过程作为辅助任务纳入 SR 问题是可行且合理的，并取得了优异的性能。代码可在以下网址获得||
|**2024-07-08**|[Enhancing super-resolution ultrasound localisation through multi-frame deconvolution exploiting spatiotemporal coherence](http://arxiv.org/abs/2407.06373)|null|通过微泡 (MB) 定位和跟踪实现的超分辨率超声成像，也称为超声定位显微镜，可以在动物和人体内对微血管进行非侵入性亚衍射分辨率成像。从获取的对比增强超声 (CEUS) 图像中定位的 MB 数量和定位精度直接影响最终的超分辨率微血管图像的质量。然而，CEUS 图像中不可忽略的噪声会使 MB 定位变得困难。为了提高 MB 定位性能，我们提出了一种多帧反卷积 (MF-Decon) 框架，该框架可以利用 CEUS 数据中固有的时空一致性，并基于总变差 (TV) 和去噪正则化 (RED) 设计新的空间和时间正则化器。基于 MF-Decon 框架，我们引入了两种新方法：具有空间和时间 TV 的 MF-Decon (MF-Decon+3DTV) 和具有空间 RED 和时间 TV 的 MF-Decon (MF-Decon+RED+TV)。计算机模拟结果表明，我们的方法在所有评估指标（包括精度、召回率、 $F_1$ 分数、平均定位误差和标准定位误差）方面均优于两种广泛使用的反卷积或归一化互相关方法。特别是，我们的方法将 MB 定位精度提高了 39%，并将召回率提高了 12%。使用我们的方法在公开可用的体内大鼠大脑数据集上生成的超分辨率微血管图显示出更少的噪声、更好的对比度、更高的分辨率和更多的血管结构。||
|**2024-07-08**|[GeoWATCH for Detecting Heavy Construction in Heterogeneous Time Series of Satellite Images](http://arxiv.org/abs/2407.06337)|null|从多传感器学习是一项挑战，因为存在时空错位以及分辨率和捕获光谱的差异。为此，我们推出了 GeoWATCH，这是一个灵活的框架，用于在来自多个传感器平台的长序列卫星图像上训练模型，该框架旨在处理图像分类、活动识别、物体检测或物体跟踪任务。我们的系统包括一种基于子图同构的新型部分权重加载机制，允许在多个训练周期内持续训练和修改网络。这使我们能够在很长一段时间内训练一系列模型，我们观察到，在我们调整配置的同时保持核心骨干的情况下，性能得到了提高。||
|**2024-07-08**|[Active Label Refinement for Robust Training of Imbalanced Medical Image Classification Tasks in the Presence of High Label Noise](http://arxiv.org/abs/2407.05973)|null|基于监督深度学习的医学图像分类的鲁棒性会被标签噪声显著削弱。为了提高存在噪声标签时的分类性能，目前已经提出了几种方法，但它们面临着一些挑战：1）难以处理类别不平衡的数据集，导致少数类样本经常被误认为是噪声样本；2）仅仅关注于使用噪声数据集最大化性能，而没有结合专家参与主动清理噪声标签。为了应对这些挑战，我们提出了一种结合了噪声标签学习（LNL）和主动学习的两阶段方法。这种方法不仅提高了存在噪声标签时医学图像分类的鲁棒性，而且在有限的标注预算下，通过重新标注重要的错误标签，迭代地提高了数据集的质量。此外，我们在LNL阶段引入了一种新的梯度方差方法，通过对代表性不足的样本进行采样，补充了基于损失的样本选择方法。通过使用两个不平衡的噪声医学分类数据集，我们证明了我们提出的技术在处理类别不平衡方面优于以往的方法，因为它不会将来自少数类的干净样本错误地识别为大部分是噪声样本。||
|**2024-07-08**|[Deform-Mamba Network for MRI Super-Resolution](http://arxiv.org/abs/2407.05969)|null|在本文中，我们提出了一种新的MR图像超分辨率架构，称为Deform-Mamba。不同于传统的CNN或基于Transformer的超分辨率方法，这些方法会遇到与局部感受野或高计算成本相关的挑战，我们的方法旨在有效地探索图像的局部和全局信息。具体来说，我们开发了一个Deform-Mamba编码器，它由两个分支组成：调制变形块和视觉Mamba块。我们还在瓶颈层设计了一个多视图上下文模块，以探索多视图上下文内容。由于编码器提取的特征包括内容自适应的局部信息和高效的全局信息，视觉Mamba解码器最终生成高质量的MR图像。此外，我们引入了一种对比边缘损失来促进边缘和对比度相关内容的重建。在IXI和fastMRI数据集上的定量和定性实验结果表明，我们的方法取得了具有竞争力的性能。||
|**2024-07-08**|[Multi-clue Consistency Learning to Bridge Gaps Between General and Oriented Object in Semi-supervised Detection](http://arxiv.org/abs/2407.05909)|**[link](https://github.com/facias914/sood-mcl)**|虽然现有的半监督目标检测（SSOD）方法在一般场景中表现良好，但它们在处理航空图像中的定向目标时遇到了挑战。我们通过实验发现，在半监督学习中，一般目标检测和定向目标检测之间存在三个差距：1）采样不一致：在从标记数据中选择正标签时，常用的中心采样不适用于长宽比较大的定向目标。2）分配不一致：平衡定向伪框的精度和定位质量带来了更大的挑战，这在从未标记数据中选择正标签时引入了更多噪声。3）置信度不一致：在考虑定向目标时，预测的分类和定位质量之间存在更多不匹配，影响了伪标签的选择。因此，我们提出了一个多线索一致性学习（MCL）框架，以弥合半监督检测中一般目标和定向目标之间的差距。具体来说，考虑到旋转目标的各种形状，我们专门设计了高斯中心分配来从标记数据中选择像素级正标签。然后，我们引入了尺度感知标签分配来选择像素级伪标签而不是不可靠的伪框，这是一种适用于各种尺度目标的分而治之策略。最后采用一致置信度软标签，通过保持预测结果的一致性来进一步提升检测器。在DOTA-v1.5和DOTA-v1.0基准数据集上的综合实验表明，我们提出的MCL方法在半监督定向目标检测任务中可以达到最先进的性能。||
|**2024-07-05**|[SH17: A Dataset for Human Safety and Personal Protective Equipment Detection in Manufacturing Industry](http://arxiv.org/abs/2407.04590)|**[link](https://github.com/ahmadmughees/sh17dataset)**|工作场所事故持续对人类安全构成重大风险，特别是在建筑和制造等行业，因此，有效的个人防护装备 (PPE) 合规性变得越来越重要。我们的研究重点是开发基于目标检测 (OD) 和卷积神经网络 (CNN) 的非侵入性技术，以检测和验证各种类型 PPE 的正确使用，例如安全帽、安全眼镜、口罩和防护服。本研究提出了 SH17 数据集，其中包含从不同工业环境中收集的 8,099 张带注释的图像，这些图像包含 75,994 个 17 个类别的实例，用于训练和验证 OD 模型。我们已经训练了最先进的 OD 模型进行基准测试，初步结果表明，You Only Look Once (YOLO)v9-e 模型变体的 PPE 检测精度超过 70.9%，达到了令人满意的水平。跨域数据集上的模型验证性能表明，集成这些技术可以显着改进安全管理系统，为努力满足人类安全法规和保护员工的行业提供可扩展且高效的解决方案。该数据集可在 https://github.com/ahmadmughees/sh17dataset 获取。||
|**2024-07-05**|[Multi-Branch Auxiliary Fusion YOLO with Re-parameterization Heterogeneous Convolutional for accurate object detection](http://arxiv.org/abs/2407.04381)|**[link](https://github.com/yang-0201/MAF-YOLO)**|由于多尺度特征融合的有效性，路径聚合特征金字塔网络 (PAFPN) 广泛应用于 YOLO 检测器中。然而，它无法同时高效且自适应地融合高级语义信息和低级空间信息。在本文中，我们提出了一个名为 MAF-YOLO 的新模型，它是一个具有名为多分支辅助特征金字塔网络 (MAFPN) 的新型目标检测框架。在 MAFPN 中，浅层辅助融合 (SAF) 模块旨在将主干网络的输出与颈部结合起来，保留最佳级别的浅层信息，以便于后续学习。同时，深度嵌入颈部的高级辅助融合 (AAF) 模块将更多样化的梯度信息传递到输出层。此外，我们提出的重新参数化的异构高效层聚合网络 (RepHELAN) 模块确保了整体模型架构和卷积设计都采用了异构大卷积核。因此，这保证了保留与小目标相关的信息，同时实现了多尺度感受野。最后，以 MAF-YOLO 的纳米版本为例，它在 COCO 数据集上仅用 3.76M 可学习参数和 10.51G FLOPs 即可达到 42.4% 的 AP，性能优于 YOLOv8n 约 5.1%。本研究的源代码可在以下网址获取：https://github.com/yang-0201/MAF-YOLO。||
|**2024-07-05**|[FeatureSORT: Essential Features for Effective Tracking](http://arxiv.org/abs/2407.04249)|null|在这项工作中，我们介绍了一种新颖的跟踪器，该跟踪器专为在线多目标跟踪而设计，其重点在于简单而有效。我们提供了多个特征模块，每个模块代表一个特定的外观信息。通过整合不同的外观特征，包括服装颜色、款式和目标方向，以及用于鲁棒嵌入提取的 ReID 网络，我们的跟踪器显著提高了在线跟踪精度。此外，我们建议结合更强大的检测器，并提供先进的后处理方法，进一步提升跟踪器的性能。在实时操作期间，我们建立测量来跟踪关联距离函数，其中包括 IoU、方向、颜色、样式和 ReID 特征相似性信息，其中每个指标分别计算。通过我们设计的特征相关距离函数，可以跟踪物体更长时间的遮挡，同时保持相对较低的身份切换次数。广泛的实验评估表明，跟踪精度和可靠性显着提高，身份切换减少和遮挡处理增强证明了这一点。这些进步不仅有助于推动目标跟踪领域的最新技术水平，而且为未来需要高精度和可靠性的研究和实际应用开辟了新途径。||
|**2024-07-05**|[AnySR: Realizing Image Super-Resolution as Any-Scale, Any-Resource](http://arxiv.org/abs/2407.04241)|**[link](https://github.com/crispyfeso4/anysr)**|为了提高单图像超分辨率 (SISR) 应用的效率和可扩展性，我们引入了 AnySR，将现有的任意尺度 SR 方法重建为任意尺度、任意资源的实现。与使用相同计算成本解决各种规模的 SR 任务的现成方法相比，我们的 AnySR 创新在于：1) 将任意尺度任务构建为任意资源实现，在不增加额外参数的情况下减少了较小规模的资源需求；2) 以特征交织的方式增强任意尺度性能，将尺度对以规则的间隔插入特征中，并确保正确的特征/尺度处理。我们通过重建大多数现有的任意尺度 SISR 方法并在五个流行的 SISR 测试数据集上进行验证，充分证明了 AnySR 的有效性。结果表明，我们的 AnySR 以计算效率更高的方式实现了 SISR 任务，并且性能与现有的任意尺度 SISR 方法相当。我们首次实现了 SISR 任务，不仅在文献中是任意尺度的，而且是任意资源的。代码可在 https://github.com/CrispyFeSo4/AnySR 获取。||
|**2024-07-05**|[AMD: Automatic Multi-step Distillation of Large-scale Vision Models](http://arxiv.org/abs/2407.04208)|null|基于Transformer的架构因其优越的性能已成为各种视觉任务的标准模型。随着模型规模的不断扩大，模型蒸馏在各种实际应用中变得极其重要，特别是在受计算资源限制的设备上。然而，当教师模型和学生模型之间存在较大的容量差距时，例如10倍的压缩率，现有的知识蒸馏方法效果会下降。在本文中，我们提出了一种名为自动多步蒸馏（AMD）的新方法，用于大规模视觉模型压缩。具体来说，我们的蒸馏过程分为多个步骤。首先，对教师模型进行蒸馏，形成一个中间的助教模型，然后进一步蒸馏到学生模型。我们引入了一个高效且有效的优化框架，来自动识别能够使学生模型性能最大化的最佳助教模型。我们在多个图像分类数据集上进行了广泛的实验，包括CIFAR-10、CIFAR-100和ImageNet。结果一致表明，我们的方法优于几种已建立的基线方法，为未来大规模视觉模型的知识蒸馏方法铺平了道路。||
|**2024-07-04**|[Attention Normalization Impacts Cardinality Generalization in Slot Attention](http://arxiv.org/abs/2407.04170)|null|以对象为中心的场景分解对于计算机视觉和机器人等领域的下游任务非常重要。最近提出的槽位注意力模块已经被一些衍生作品用于图像分割和视频目标跟踪，它是一种深度学习组件，可以在输入图像上执行无监督的以对象为中心的场景分解。它基于一种注意力架构，其中潜在的槽位向量（包含对象的压缩信息）关注来自输入图像的局部感知特征。在本文中，我们发现对注意力架构中聚合值进行归一化的设计决策对槽位注意力泛化到训练期间所见到的更多槽位和对象的能力有相当大的影响。我们认为，原始的槽位注意力归一化方案丢弃了像素先前分配给槽位的概率信息，这损害了其泛化能力。基于这些发现，我们提出并研究了替代的归一化方法，这些方法可以提高槽位注意力对不同槽位和对象数量的泛化能力，从而提高无监督图像分割任务的性能。||
|**2024-07-04**|[Detect Closer Surfaces that can be Seen: New Modeling and Evaluation in Cross-domain 3D Object Detection](http://arxiv.org/abs/2407.04061)|**[link](https://github.com/galaxy-zrx/edgehead)**|目前，域适应技术在自动驾驶三维目标检测领域的性能尚未达到理想水平，这主要是由于车辆尺寸的显著差异以及跨域应用时运行环境的不同。这些因素共同阻碍了从特定数据集中学习到的知识的有效迁移和应用。由于现有的评估指标最初是通过计算预测边界框和真实边界框之间的二维或三维重叠来设计用于单个域上的评估，因此它们经常会遇到由数据集之间的大小差异引起的过拟合问题。这引发了一个与评估三维目标检测模型跨域性能相关的基本问题：我们是否真的需要模型在跨域应用后保持其原始三维边界框的出色性能？从实际应用的角度来看，我们的主要关注点之一实际上是防止车辆与其他障碍物发生碰撞，特别是在跨域场景中，正确预测车辆尺寸要困难得多。换句话说，只要模型能够准确识别出距离自动驾驶车辆最近的表面，就足以有效避开障碍物。在本文中，我们提出了两个指标来衡量三维目标检测模型检测自动驾驶车辆传感器附近表面的能力，这可以用来更全面、更合理地评估其跨域性能。此外，我们提出了一个名为EdgeHead的优化头，用于引导模型更加关注可学习的较近表面，这可以极大地提高现有模型在我们的新指标下以及在原始BEV/3D指标下的跨域性能。||
|**2024-07-04**|[TrackPGD: A White-box Attack using Binary Masks against Robust Transformer Trackers](http://arxiv.org/abs/2407.03946)|null|使用Transformer骨干网络的目标跟踪器在视觉目标跟踪数据集上取得了强大的性能。然而，这些跟踪器的对抗鲁棒性在文献中尚未得到很好的研究。由于骨干网络的差异，为目标跟踪提出的对抗性白盒攻击不能迁移到所有类型的跟踪器。例如，像MixFormerM这样的Transformer跟踪器在黑盒攻击后仍然运行良好，特别是在预测目标二进制掩码方面。我们提出了一种名为TrackPGD的新型白盒攻击，它依靠预测的目标二进制掩码来攻击鲁棒的Transformer跟踪器。这种新攻击通过采用著名的SegPGD分割攻击来关注标注掩码，从而能够成功地对依赖Transformer骨干网络的跟踪器进行白盒攻击。实验结果表明，TrackPGD能够有效攻击基于Transformer的跟踪器，例如MixFormerM、OSTrackSTS和TransT-SEG，并在多个跟踪数据集上取得了成功。||
|**2024-07-04**|[DocXplain: A Novel Model-Agnostic Explainability Method for Document Image Classification](http://arxiv.org/abs/2407.03830)|null|深度学习（DL）彻底改变了文档图像分析领域，在各种任务中展现出超越人类的表现。然而，深度学习模型固有的黑盒性质仍然是其在行业中安全稳健部署的重大挑战。遗憾的是，尽管近年来大量研究致力于开发基于深度学习的文档分析系统，但解决其透明性方面的研究却相对较少。在本文中，我们旨在通过介绍 DocXplain 来弥合这一研究差距，这是一种新颖的模型无关的可解释性方法，专门设计用于为文档图像分类任务生成高可解释性的特征归因图。具体来说，我们的方法涉及将文档的前景和背景特征独立地分割成不同的文档元素，然后消融这些元素以分配特征重要性。我们在文档图像分类的背景下广泛评估了我们提出的方法，利用 4 种不同的评估指标、2 个广泛认可的文档基准数据集和 10 个最先进的文档图像分类模型。通过对 9 种现有的最先进的归因方法进行全面的定量和定性分析，我们证明了我们的方法在保真度和可解释性方面的优越性。据作者所知，这项工作提出了第一个专门针对文档图像量身定制的模型无关的基于归因的可解释性方法。我们预计我们的工作将极大地促进文档图像分类模型的透明度、公平性和鲁棒性研究的进展。||
|**2024-07-04**|[M^3:Manipulation Mask Manufacturer for Arbitrary-Scale Super-Resolution Mask](http://arxiv.org/abs/2407.03695)|null|在图像篡改定位（IML）领域，现有数据集数量少、质量差一直是主要问题。包含各种篡改类型的数据集将极大地提高IML模型的准确性。互联网上的图像（例如百度贴吧PS吧的图像）使用各种技术进行篡改，利用这些图像创建数据集将显著丰富我们数据中的篡改类型。然而，互联网上的图像存在分辨率和清晰度问题，通过简单地从原始图像中减去篡改图像获得的掩码包含各种噪声。这些噪声很难去除，导致掩码无法用于IML模型。受变化检测领域的启发，我们将原始图像和篡改图像视为同一图像随时间的变化，并将数据生成任务视为变化检测任务。然而，由于图像之间的清晰度问题，传统的变化检测模型表现不佳。因此，我们引入了一个超分辨率模块，并提出了篡改掩码生成器（MMM）框架。它增强了原始图像和篡改图像的分辨率，从而改善了图像细节，以便更好地进行比较。同时，该框架将原始图像和篡改图像转换为特征嵌入并进行拼接，有效地对上下文进行建模。此外，我们创建了篡改掩码生成器数据集（MMMD），这是一个涵盖了各种篡改技术的数据集。我们的目标是通过MMM和MMMD提供更真实的篡改数据，为图像取证和篡改检测领域做出贡献。有关MMMD和下载链接的详细信息，请访问：代码和数据集将公开。||
|**2024-07-03**|[Visual Grounding with Attention-Driven Constraint Balancing](http://arxiv.org/abs/2407.03243)|null|不同于目标检测，视觉定位任务需要检测由复杂的自由形式语言描述的对象。为了同时对这种复杂的语义和视觉表示进行建模，最近最先进的研究采用基于 Transformer 的模型来融合来自两种模态的特征，并进一步引入了各种模块来调节视觉特征，使其与语言表达保持一致并消除不相关的冗余信息。然而，它们的损失函数仍然采用常见的目标检测损失，只控制边界框回归输出，无法完全优化上述目标。为了解决这个问题，本文首先分析了基于 Transformer 模型的注意力机制。在此基础上，我们进一步提出了一个名为注意力驱动约束平衡（AttBalance）的新框架，以优化语言相关区域内视觉特征的行为。大量的实验结果表明，我们的方法带来了令人印象深刻的改进。具体来说，我们在四个不同基准上评估的五种不同模型上实现了持续的改进。此外，通过将我们的方法集成到 QRNet 中，我们获得了新的最先进的性能。||
|**2024-07-03**|[Category-Aware Dynamic Label Assignment with High-Quality Oriented Proposal](http://arxiv.org/abs/2407.03205)|null|航拍图像中的物体通常嵌入在复杂的背景中，并呈现任意方向。当使用定向边界框 (OBB) 表示任意方向的物体时，角度的周期性可能导致边界处标签回归值的不连续性，从而导致损失函数出现剧烈波动。为了解决这个问题，在定向检测框架中引入了一种基于复平面的 OBB 表示方法，并提出了一种三角损失函数。此外，利用对复杂背景环境和航拍图像中大型物体显著差异的先验知识，构建了一个 Conformer RPN 头部来预测角度信息。所提出的损失函数和 Conformer RPN 头部共同生成高质量的定向建议。针对仅依靠 IoU 进行建议标签分配的局限性，提出了一种基于预测类别反馈的类别感知动态标签分配方法。该方法使负样本选择更具代表性，确保分类和回归特征之间的一致性。在四个真实的定向检测数据集上进行了实验，结果表明，在参数调整和时间成本最小的情况下，定向目标检测的性能更优。具体而言，在 DOTA-v1.0、DOTA-v1.5、DIOR-R 和 HRSC2016 数据集上分别实现了 82.02%、71.99%、69.87% 和 98.77% 的平均精度均值 (mAP) 分数。||
|**2024-07-03**|[SegVG: Transferring Object Bounding Box to Segmentation for Visual Grounding](http://arxiv.org/abs/2407.03200)|**[link](https://github.com/weitaikang/segvg)**|与目标检测不同，视觉定位（Visual Grounding）旨在为每个文本-图像对检测一个边界框。这种为每个文本-图像数据提供一个边界框的方式提供了稀疏的监督信号。尽管先前的工作取得了令人瞩目的成果，但它们对标注的被动利用，即将边界框标注仅用作回归真值，导致了性能欠佳。在本文中，我们提出了SegVG，这是一种将边界框级标注转换为分割信号的新方法，以便为视觉定位提供额外的像素级监督。具体来说，我们提出了多层多任务编码器-解码器作为目标定位阶段，在该阶段中，我们学习回归查询和多个分割查询，分别通过在每个解码层中对边界框进行回归和分割来定位目标。这种方法使我们能够迭代地利用标注作为边界框级回归和像素级分割的信号。此外，由于骨干网络通常由从单模态任务中学习到的预训练参数初始化，并且用于回归和分割的查询都是静态可学习的嵌入，因此这三种类型的特征之间存在域差异，这会损害后续的目标定位。为了减轻这种差异，我们引入了三重对齐模块，其中查询、文本和视觉标记通过三重注意力机制进行三角更新，以共享相同的空间。在五个广泛使用的数据集上进行的大量实验验证了我们的方法达到了最先进的性能 (SOTA)。||
|**2024-07-03**|[Global Context Modeling in YOLOv8 for Pediatric Wrist Fracture Detection](http://arxiv.org/abs/2407.03163)|**[link](https://github.com/ruiyangju/yolo_global_context_fracture_detection)**|儿童在日常生活中经常遭受腕部损伤，而骨折损伤放射科医生通常需要在外科医生进行手术治疗之前分析和解释 X 光图像。深度学习的发展使神经网络模型能够作为计算机辅助诊断 (CAD) 工具来帮助医生和专家进行诊断。由于 YOLOv8 模型在目标检测任务中取得了令人满意的成功，因此它已被应用于骨折检测。全局上下文 (GC) 模块以轻量级的方式有效地对全局上下文进行建模，将其融入 YOLOv8 可以极大地提高模型性能。本文提出了用于骨折检测的 YOLOv8+GC 模型，它是具有 GC 模块的 YOLOv8 模型的改进版本。实验结果表明，与原始的 YOLOv8 模型相比，所提出的 YOLOv8-GC 模型在 GRAZPEDWRI-DX 数据集上将交并比阈值为 0.5 时的平均精度均值 (mAP 50) 从 63.58% 提高到 66.32%，达到了最先进的水平 (SOTA)。这项工作的实现代码可在 GitHub 上获取：https://github.com/RuiyangJu/YOLOv8_Global_Context_Fracture_Detection。||
|**2024-07-03**|[Applying Extended Object Tracking for Self-Localization of Roadside Radar Sensors](http://arxiv.org/abs/2407.03084)|null|智能交通系统 (ITS) 可以受益于路边 4D 毫米波雷达传感器，用于大规模交通监控，因为它们具有全天候功能、长感应范围和低制造成本。然而，在城市环境中，使用外部测量设备的定位方法存在局限性。此外，如果传感器安装由于环境影响而出现变化，则在仅在安装期间执行测量时无法对其进行校正。在本文中，我们提出了使用扩展目标跟踪 (EOT) 的路边雷达数据自定位方法。该方法分析传感器观察到的车辆跟踪轨迹和城市街道的航空激光扫描，将“直行”、“左转”、“右转”等驾驶行为标签分配给轨迹段和路段，并执行语义迭代最近点 (SICP) 算法来配准点云。该方法利用下游任务（目标跟踪）的结果进行定位。我们展示了亚米范围内的高精度以及非常低的方位误差。该方法还显示出良好的数据效率。评估在仿真和实际测试中均已完成。||
|**2024-07-03**|[YOLOv5, YOLOv8 and YOLOv10: The Go-To Detectors for Real-time Vision](http://arxiv.org/abs/2407.02988)|null|本文全面回顾了YOLO（You Only Look Once）目标检测算法的演进过程，重点关注YOLOv5、YOLOv8和YOLOv10。我们分析了这些版本在架构改进、性能提升以及边缘部署适用性方面的差异。YOLOv5引入了CSPDarknet骨干网络和Mosaic数据增强等重大创新，实现了速度和精度之间的平衡。YOLOv8在此基础上，通过增强特征提取和无锚框检测，提高了算法的通用性和性能。YOLOv10则凭借无NMS训练、空间通道解耦下采样以及大核卷积等技术实现了跨越式发展，以更低的计算开销实现了最先进的性能。我们的研究结果突出了YOLO算法在精度、效率和实时性能方面的逐步提升，特别强调了其在资源受限环境中的适用性。本综述提供了模型复杂度和检测精度之间权衡的见解，为针对特定边缘计算应用选择最合适的YOLO版本提供了指导。||
|**2024-07-03**|[ShiftAddAug: Augment Multiplication-Free Tiny Neural Network with Hybrid Computation](http://arxiv.org/abs/2407.02881)|null|缺乏乘法运算符（例如移位和加法）因其与硬件的兼容性而受到关注。然而，与具有相同结构的传统神经网络 (NN) 相比，采用这些运算符的神经网络 (NN) 通常表现出较低的精度。ShiftAddAug 使用成本高昂的乘法来增强高效但功能较弱的无乘法运算符，从而在没有任何推理开销的情况下提高性能。它将一个 ShiftAdd 小型神经网络放入一个大型乘法模型中，并鼓励将其训练为子模型以获得额外的监督。为了解决混合运算符之间的权重差异问题，提出了一种新的权重共享方法。此外，一种新颖的两阶段神经架构搜索用于为更小但更强的无乘法小型神经网络获得更好的增强效果。ShiftAddAug 的优越性通过图像分类和语义分割实验得到验证，始终如一地提供显着的增强。值得注意的是，与直接训练的对应模型相比，它在 CIFAR100 上的准确率提高了 4.95%，甚至超过了乘法神经网络的性能。||
|**2024-07-03**|[A Pairwise DomMix Attentive Adversarial Network for Unsupervised Domain Adaptive Object Detection](http://arxiv.org/abs/2407.02835)|null|无监督域自适应目标检测 (DAOD) 可以使在一个源域上训练的模型适应未标记的目标域，以进行目标检测。现有的无监督 DAOD 方法通常执行从目标域到源域的特征对齐。单向域迁移会忽略有关目标样本的信息，并在存在较大域差异时导致欠佳的自适应。因此，我们提出了一种具有域混合 (DomMix) 模块的成对注意力对抗网络，以缓解上述挑战。具体来说，采用深度混合来构建一个中间域，允许来自两个域的特征共享它们的差异。然后，应用成对注意力对抗网络，在不同尺度的图像级和实例级特征上进行注意力编码，并通过对抗学习优化域对齐。这使得网络能够专注于具有不同上下文信息的区域，并学习它们在不同域之间的相似性。在几个基准数据集上进行了广泛的实验，证明了我们提出的方法的优越性。||
|**2024-07-03**|[Data Overfitting for On-Device Super-Resolution with Dynamic Algorithm and Compiler Co-Design](http://arxiv.org/abs/2407.02813)|**[link](https://github.com/coulsonlee/dy-dca-ecc)**|深度神经网络 (DNN) 经常被应用于各种计算机视觉应用。如今，当前视频分发系统中一个新兴趋势是利用 DNN 的过拟合特性来执行视频分辨率提升。通过将视频分割成块并应用超分辨率 (SR) 模型对每个块进行过拟合，这种 SR 模型加视频块的方案能够取代传统的视频传输，从而提高视频质量和传输效率。然而，为了保证高性能，需要许多模型和块，这会导致用户端的模型切换和内存占用方面产生巨大的开销。为了解决这些问题，我们提出了一种由内容感知数据处理管道辅助的动态深度神经网络，以将模型数量减少到一个 (Dy-DCA)，这有助于在节省计算资源的同时提高性能。此外，为了在用户端实现真正的加速，我们设计了一个框架来优化 Dy-DCA 中的动态特征（例如，动态形状、大小和控制流），从而实现一系列编译优化，包括融合代码生成、静态执行计划等。通过采用这些技术，我们的方法在现成的手机上实现了更好的 PSNR 和实时性能 (33 FPS)。同时，在我们的编译优化的辅助下，我们实现了 1.7 倍的加速，同时节省了高达 1.61 倍的内存消耗。代码可在 https://github.com/coulsonlee/Dy-DCA-ECCV2024 获取。||
|**2024-07-03**|[Fine-Grained Scene Image Classification with Modality-Agnostic Adapter](http://arxiv.org/abs/2407.02769)|**[link](https://github.com/qunilcs/maa)**|在处理细粒度场景图像分类任务时，以往的大多数工作在进行多模态特征融合时都非常重视全局视觉特征。换句话说，模型的设计是基于对不同模态重要性的先验直觉。在本文中，我们提出了一种新的多模态特征融合方法，称为MAA（模态无关适配器），试图使模型自适应地学习不同模态在不同情况下的重要性，而无需在模型架构中进行先验设置。更具体地说，我们消除了分布中的模态差异，然后使用模态无关的Transformer编码器进行语义级别的特征融合。我们的实验表明，通过应用与以前方法相同的模态，MAA在基准测试中取得了最先进的结果。此外，值得一提的是，在使用MAA时，可以轻松添加新的模态，并进一步提升性能。代码可在https://github.com/quniLcs/MAA获取。||

## 生成模型

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-07-25**|[VGGHeads: A Large-Scale Synthetic Dataset for 3D Human Heads](http://arxiv.org/abs/2407.18245)|null|人头检测、关键点估计和 3D 人头模型拟合是许多应用中的重要任务。然而，传统的现实世界数据集经常受到偏差、隐私和伦理问题的困扰，而且它们是在实验室环境中记录的，这使得训练模型难以泛化。在此，我们介绍 VGGHeads——一个使用扩散模型生成的大规模合成数据集，用于人头检测和 3D 网格估计。我们的数据集包含超过 100 万张高分辨率图像，每张图像都标注了详细的 3D 人头网格、面部关键点和边界框。利用该数据集，我们引入了一种新的模型架构，能够从单张图像中一步同时进行人头检测和人头网格重建。通过广泛的实验评估，我们证明了在合成数据上训练的模型在真实图像上实现了强大的性能。此外，我们数据集的多功能性使其适用于广泛的任务，提供了人头的通用和全面表示。此外，我们还提供了关于合成数据生成流程的详细信息，使其能够被其他任务和领域重复使用。||
|**2024-07-25**|[Self-supervised pre-training with diffusion model for few-shot landmark detection in x-ray images](http://arxiv.org/abs/2407.18125)|null|近年来，深度神经网络已广泛应用于医学领域的不同任务，从图像分类和分割到 landmarks 检测。然而，这些技术在医学领域的应用往往受到数据稀缺的阻碍，无论是在可用标注还是图像方面。本研究介绍了一种新的基于扩散模型的自监督预训练协议，用于 X 光图像中的 landmarks 检测。我们的研究结果表明，所提出的自监督框架可以通过最少数量的可用标注训练图像（最多 50 张）提供准确的 landmarks 检测，在三个流行的 X 光基准数据集上优于 ImageNet 监督预训练和最先进的自监督预训练。据我们所知，这是首次探索将扩散模型用于 landmarks 检测中的自监督学习，这可能为缓解数据稀缺问题，在少样本机制中提供一种有价值的预训练方法。||
|**2024-07-25**|[AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild](http://arxiv.org/abs/2407.18034)|null|近年来，关于用于各种形式人机交互的3D手部重建的研究已经有很多。然而，由于极度缺乏自然场景下的3D手部数据集，自然场景下的3D手部重建仍然具有挑战性。特别是当手处于复杂姿势时，例如双手交互，外观相似性、自遮挡和深度模糊等问题使得重建更加困难。为了克服这些问题，我们提出了AttentionHand，一种新颖的文本驱动的可控手部图像生成方法。由于AttentionHand可以生成与3D手部标签良好对齐的各种各样的自然场景手部图像，我们可以获得新的3D手部数据集，并可以缓解室内和室外场景之间的域差距。我们的方法需要四种易于使用的模态（即RGB图像、来自3D标签的手部网格图像、边界框和文本提示）。这些模态通过编码阶段嵌入到潜在空间中。然后，通过文本注意阶段，来自给定文本提示的与手相关的标记被用来突出显示潜在嵌入中与手相关的区域。在突出显示的嵌入被馈送到视觉注意阶段后，通过使用基于扩散的管道，以全局和局部手部网格图像为条件，来关注嵌入中与手相关的区域。在解码阶段，最终特征被解码为新的手部图像，这些图像与给定的手部网格图像和文本提示良好对齐。因此，AttentionHand在文本到手部图像生成模型中达到了最先进的水平，并且通过使用AttentionHand生成的手部图像进行额外训练，3D手部网格重建的性能也得到了提高。||
|**2024-07-25**|[Segmentation-guided MRI reconstruction for meaningfully diverse reconstructions](http://arxiv.org/abs/2407.18026)|null|反问题，例如加速 MRI 重建，是病态的，并且存在无限多的可能且合理的解决方案。这不仅会导致重建图像的不确定性，还会导致语义分割等下游任务的不确定性。然而，这种不确定性在文献中大多没有得到分析，即使概率重建模型被普遍使用。这些模型可能容易忽略合理但不太可能的解决方案，例如罕见的病理学。基于基于扩散模型的 MRI 重建方法，我们在推理过程中添加了对扩散过程的指导，生成对应于上限和下限分割的两个有意义的不同重建。然后可以通过这些边界之间的差异来量化重建的不确定性，我们将其称为“不确定性边界”。我们分析了各种加速因子下上限和下限分割的行为，发现与重复采样相比，不确定性边界更可靠、更准确。代码可在 https://github.com/NikolasMorshuis/SGR 获取。||
|**2024-07-25**|[Self-Supervision Improves Diffusion Models for Tabular Data Imputation](http://arxiv.org/abs/2407.18013)|**[link](https://github.com/yixinliu233/simpdm)**|缺失数据的普遍性引发了人们对表格数据插补方法的广泛关注。扩散模型作为数据生成的前沿技术，在表格数据插补任务中展现出巨大潜力。然而，为了追求多样性，普通的扩散模型往往对初始化噪声很敏感，这阻碍了模型生成稳定准确的插补结果。此外，表格数据固有的稀疏性给扩散模型准确建模数据流形带来了挑战，影响了这些模型进行数据插补的鲁棒性。为了应对这些挑战，本文提出了一种名为自监督插补扩散模型（简称SimpDM）的先进扩散模型，专为表格数据插补任务而设计。为了减轻对噪声的敏感性，我们引入了一种自监督对齐机制，旨在规范模型，确保一致和稳定的插补预测。此外，我们在SimpDM中引入了一种精心设计的依赖于状态的数据增强策略，增强了扩散模型在处理有限数据时的鲁棒性。大量实验表明，SimpDM在各种情况下都能与最先进的插补方法相媲美或优于它们。||
|**2024-07-25**|[Lightweight Language-driven Grasp Detection using Conditional Consistency Model](http://arxiv.org/abs/2407.17967)|null|语言驱动的抓取检测是机器人技术中一项基础而又具有挑战性的任务，在工业中有着广泛的应用。在这项工作中，我们提出了一种新的语言驱动的抓取检测方法，该方法利用轻量级扩散模型的概念来实现快速推理。通过将扩散过程与自然语言中的抓取提示相结合，我们的方法可以有效地编码视觉和文本信息，从而实现更准确、更多功能的抓取定位，并与文本查询很好地保持一致。为了克服扩散模型中推理时间长的问题，我们在一致性模型中利用图像和文本特征作为条件，以减少推理过程中的去噪时间步长。大量的实验结果表明，我们的方法明显优于其他最近的抓取检测方法和轻量级扩散模型。我们进一步在真实的机器人实验中验证了我们的方法，以证明其快速推理的能力。||
|**2024-07-25**|[ReCorD: Reasoning and Correcting Diffusion for HOI Generation](http://arxiv.org/abs/2407.17911)|null|扩散模型通过利用自然语言引导多媒体内容的创作，彻底改变了图像生成领域。尽管此类生成模型取得了重大进展，但在描绘详细的人与物体交互（HOI）方面，尤其是在姿势和物体放置的准确性方面，仍然存在挑战。我们引入了一种名为推理和校正扩散（ReCorD）的免训练方法来解决这些挑战。我们的模型将潜在扩散模型与视觉语言模型相结合，以改进生成过程，确保对 HOI 的精确描绘。我们提出了一个交互感知推理模块来改进对交互的解释，以及一个交互校正模块来细化输出图像，从而更精确地生成 HOI。通过精心设计的姿势选择和物体定位过程，ReCorD 在生成图像时实现了更高的保真度，同时有效降低了计算需求。我们在三个基准数据集上进行了全面的实验，以证明在解决文本到图像生成任务方面的重大进展，展示了 ReCorD 通过在 HOI 分类分数、FID 和 Verb CLIP-Score 方面优于现有方法，从而准确呈现复杂交互的能力。项目网站：https://alberthkyhky.github.io/ReCorD/ 。||
|**2024-07-25**|[Amortized Posterior Sampling with Diffusion Prior Distillation](http://arxiv.org/abs/2407.17907)|null|我们提出了一种变分推断方法，用于从后验分布中采样以解决反问题。从预训练的扩散模型开始，我们的方法训练了一个条件流模型，以最小化建议的变分分布与通过扩散模型隐式定义的后验分布之间的差异。训练完成后，流模型能够通过单个 NFE 从后验分布中采样，并根据测量结果进行摊销。所提出的方法为提取扩散先验以进行高效的后验采样开辟了一条新途径。我们证明了我们的方法适用于欧几里得空间中的标准信号，以及流形上的信号。||
|**2024-07-25**|[Artificial Immunofluorescence in a Flash: Rapid Synthetic Imaging from Brightfield Through Residual Diffusion](http://arxiv.org/abs/2407.17882)|null|免疫荧光 (IF) 成像对于可视化生物标志物表达、细胞形态和评估药物治疗对亚细胞成分的影响至关重要。免疫荧光成像需要额外的染色过程，并且通常需要细胞固定，因此它也可能引入伪影并改变内源性细胞形态。一些免疫荧光染料价格昂贵或不易获得，因此阻碍了实验。最近的扩散模型从易于获取的明场 (BF) 图像合成高保真免疫荧光图像，提供了一种很有前景的解决方案，但由于噪声扩散过程，训练不稳定和推理时间慢阻碍了该方法的应用。本文提出了一种直接从明场图像以及细胞分割掩模中条件合成免疫荧光图像的新方法。我们的方法采用残差扩散过程，增强了稳定性并显着减少了推理时间。我们针对其他图像到图像合成模型进行了严格的评估，包括 UNets、GAN 和高级扩散模型。我们的模型在图像质量（MSE、PSNR 和 SSIM 中 p<0.05）、推理速度（比竞争扩散模型快 26 倍）以及细胞核和细胞体的准确分割结果（细胞核和细胞真阳性的平均 IOU 分别为 0.77 和 0.63）方面均有显着改进。这篇论文是该领域的重大进步，为细胞图像分析提供了强大而有效的工具。||
|**2024-07-25**|[DragText: Rethinking Text Embedding in Point-based Image Editing](http://arxiv.org/abs/2407.17843)|null|基于点的图像编辑可以通过内容拖动实现精确灵活的控制。然而，文本嵌入在编辑过程中的作用尚未得到彻底研究。一个尚未探索的重要方面是文本和图像嵌入之间的交互。在这项研究中，我们发现，在扩散模型中逐步编辑输入图像的过程中，文本嵌入保持不变。随着图像嵌入与其初始状态越来越不同，图像嵌入和文本嵌入之间的差异带来了重大挑战。此外，我们发现文本提示显著影响拖动过程，特别是在保持内容完整性和实现所需操作方面。为了利用这些见解，我们提出了DragText，它与拖动过程一起优化文本嵌入，以与修改后的图像嵌入配对。同时，我们规范文本优化过程，以保持原始文本提示的完整性。我们的方法可以无缝集成到现有的基于扩散的拖动方法中，只需几行代码。||
|**2024-07-23**|[Diffusion Models for Monocular Depth Estimation: Overcoming Challenging Conditions](http://arxiv.org/abs/2407.16698)|**[link](https://github.com/fabiotosi92/diffusion4robustdepth)**|我们提出了一种新方法，旨在解决单图像深度估计任务中由具有挑战性的、超出分布范围的数据带来的复杂性。我们从易于预测深度的图像开始（因为它们不存在不利因素），系统地生成新的、用户定义的场景，这些场景具有一系列挑战和相关的深度信息。这是通过利用具有深度感知控制的最先进的文本到图像扩散模型来实现的，该模型以从文本提示合成高质量图像内容而闻名，同时保留生成图像和源图像之间 3D 结构的一致性。任何单目深度网络的后续微调都是通过自蒸馏协议进行的，该协议考虑了使用我们的策略生成的图像及其对简单、非挑战性场景的自身深度预测。针对我们目的而定制的基准测试证明了我们提案的有效性和多功能性。||
|**2024-07-23**|[From Imitation to Refinement -- Residual RL for Precise Visual Assembly](http://arxiv.org/abs/2407.16677)|null|行为克隆（BC）是目前现实世界视觉操控学习的主导范式。然而，在需要局部纠正行为的任务中，例如多部件组装，仅从人类演示中学习鲁棒策略仍然具有挑战性。强化学习（RL）可以通过任务奖励监督和探索，允许策略获得局部纠正行为，从而减轻这些限制。本文探讨了使用强化学习微调来改进基于BC训练的策略在精确操作任务中的性能。我们分析并克服了使用强化学习直接训练包含现代架构组件（如扩散模型和动作分块）的策略网络相关的技术挑战。我们建议使用标准策略梯度方法和稀疏奖励，在冻结的BC训练扩散模型之上训练残差策略，我们称之为ResiP（用于精确操作的残差）。我们的实验结果表明，这种残差学习框架可以通过学习纠正动作，在高精度装配任务中显著提高成功率，超越基本的BC训练模型。我们还表明，通过将ResiP与师生蒸馏和视觉域随机化相结合，我们的方法可以直接从RGB图像中学习用于机器人装配的现实世界策略。视频和代码可在\url{https://residual-assembly.github.io}找到。||
|**2024-07-23**|[MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequence](http://arxiv.org/abs/2407.16655)|null|近年来，视频生成技术的进步主要依赖于扩散模型，但仅限于短视频内容。然而，这些方法在建模复杂叙事和在长时间内保持角色一致性方面往往不足，而这对于电影等长视频制作至关重要。我们提出了 MovieDreamer，一个新颖的分层框架，它结合了自回归模型和基于扩散的渲染的优势，开创了具有复杂情节发展和高视觉保真度的长视频生成。我们的方法利用自回归模型来实现全局叙事连贯性，预测视觉标记序列，然后通过扩散渲染将其转换为高质量的视频帧。这种方法类似于传统的电影制作过程，即将复杂的故事分解成可管理的场景拍摄。此外，我们采用多模态脚本，用详细的角色信息和视觉风格丰富场景描述，增强跨场景的连续性和角色身份。我们展示了跨多种电影类型的广泛实验，证明我们的方法不仅实现了卓越的视觉和叙事质量，而且有效地将生成内容的时长扩展到远远超出当前能力的范围。主页：https://aim-uofa.github.io/MovieDreamer/。||
|**2024-07-23**|[Knowledge-driven AI-generated data for accurate and interpretable breast ultrasound diagnoses](http://arxiv.org/abs/2407.16634)|null|数据驱动的深度学习模型在辅助放射科医生进行乳腺超声 (US) 诊断方面已展现出巨大潜力。然而，由于训练数据的长尾分布，导致模型在罕见病例中的准确性受限，从而限制了其有效性。本研究致力于解决利用长尾数据提高罕见病例诊断模型性能这一长期挑战。具体而言，我们引入了一个名为 TAILOR 的流程，它构建了一个知识驱动的生成模型来生成定制的合成数据。该生成模型使用 3,749 个病灶作为源数据，可以生成数百万张乳腺超声图像，特别是容易出错的罕见病例图像。生成的数据可进一步用于构建诊断模型，以进行准确且可解释的诊断。在前瞻性外部评估中，我们的诊断模型在特异性方面比九名放射科医生的平均水平高出 33.5%，且灵敏度相同，通过提供具有可解释决策过程的预测结果，提高了他们的诊断性能。此外，在导管原位癌 (DCIS) 中，我们的诊断模型在源数据中仅包含 34 个 DCIS 病灶的情况下，其性能也大幅优于所有放射科医生。我们相信，TAILOR 有可能扩展到各种疾病和影像模态。||
|**2024-07-23**|[DreamVTON: Customizing 3D Virtual Try-on with Personalized Diffusion Models](http://arxiv.org/abs/2407.16511)|null|基于图像的3D虚拟试穿（VTON）旨在根据人物和服装图像塑造3D人体模型，这种方法数据效率高（即无需昂贵的3D数据），但极具挑战性。最近，文本到3D方法在高保真3D人体生成方面取得了显著进步，展示了其在3D虚拟试穿方面的潜力。受个性化扩散模型（例如Dreambooth和LoRA）在2D VTON方面取得的巨大成功的启发，将个性化技术集成到基于扩散的文本到3D框架中以实现3D VTON的想法应运而生。然而，在预训练的扩散模型（例如StableDiffusion (SD)）中使用个性化模块会降低模型进行多视图或多域合成的能力，这不利于由分数蒸馏采样（SDS）损失引导的几何和纹理优化。在这项工作中，我们提出了一种名为DreamVTON的新型定制3D人体试穿模型，用于分别优化3D人体的几何形状和纹理。具体来说，我们提出了一种使用多概念LoRA的个性化SD，以提供有关特定人物和服装的生成先验，同时利用Densepose引导的ControlNet来保证不同相机视角下身体姿势的一致性先验。此外，为了避免个性化SD中不一致的多视图先验主导优化过程，DreamVTON引入了一种基于模板的优化机制，该机制采用掩码模板进行几何形状学习，并使用法线/RGB模板进行几何/纹理细节学习。此外，在几何优化阶段，DreamVTON将法线风格LoRA集成到个性化SD中，以增强法线贴图生成先验，从而促进平滑的几何建模。||
|**2024-07-23**|[qMRI Diffusor: Quantitative T1 Mapping of the Brain using a Denoising Diffusion Probabilistic Model](http://arxiv.org/abs/2407.16477)|null|定量MRI（qMRI）通过提供与组织特性相关的客观参数，与加权图像相比具有显著优势。基于深度学习的方法在从加权图像序列估计定量图方面已显示出有效性。在这项研究中，我们提出了qMRI Diffusor，这是一种利用深度生成模型进行qMRI的新方法。具体来说，我们实施了用于脑部T1量化的去噪扩散概率模型（DDPM），将定量图的估计构建为条件生成任务。将所提出的方法与残差神经网络（ResNet）和循环推理机（RIM）在体模和体内数据上进行了比较。结果表明，我们的方法在参数估计方面实现了更高的准确性和精度，以及更好的视觉性能。此外，我们的方法本质上结合了随机性，能够直接量化不确定性。因此，所提出的方法在定量MR成像方面具有很大的应用前景。||
|**2024-07-23**|[MonoWAD: Weather-Adaptive Diffusion Model for Robust Monocular 3D Object Detection](http://arxiv.org/abs/2407.16448)|**[link](https://github.com/visualaikhu/monowad)**|单目三维物体检测是自动驾驶中一项重要且具有挑战性的任务。现有方法主要集中于在理想天气条件下（以能见度清晰和最佳为特征的场景）进行三维检测。然而，自动驾驶的挑战在于需要能够处理天气条件的变化，例如雾天，而不仅仅是晴朗的天气。我们介绍了 MonoWAD，一种新颖的具有天气适应性扩散模型的天气鲁棒型单目三维物体检测器。它包含两个部分：(1) 天气码本，用于记忆晴朗天气的知识并为任何输入生成天气参考特征；(2) 天气适应性扩散模型，通过结合天气参考特征来增强输入特征的表示。这起到了注意力的作用，根据天气条件指示输入特征需要多少改进。为了实现这一目标，我们引入了天气适应性增强损失，以增强晴天和雾天天气条件下的特征表示。在各种天气条件下进行的大量实验表明，MonoWAD 实现了天气鲁棒的单目三维物体检测。代码和数据集已发布在 https://github.com/VisualAIKHU/MonoWAD。||
|**2024-07-23**|[FairFlow: An Automated Approach to Model-based Counterfactual Data Augmentation For NLP](http://arxiv.org/abs/2407.16431)|**[link](https://github.com/EwoeT/FairFlow)**|尽管语言模型不断发展，但它们仍然会无意间从训练数据中习得有害的社会偏见和刻板印象。这些固有偏见通常会导致在各种应用中产生不利影响。反事实数据增强 (CDA) 试图平衡训练数据中的人口统计属性，已成为自然语言处理中减轻偏见的广泛采用的方法。然而，许多现有的 CDA 方法依赖于使用手动编译的词对词典进行词语替换的技术。这些技术通常会导致脱离语境的替换，从而导致潜在的质量问题。另一方面，基于模型的技术的进步一直受到对平行训练数据需求的挑战。该领域的现有工作求助于手动生成的平行数据，这些数据收集成本高昂，因此规模有限。本文提出了 FairFlow，这是一种自动生成平行数据的方法，用于训练反事实文本生成器模型，从而限制了对人工干预的需求。此外，我们证明了 FairFlow 极大地克服了基于词典的词语替换方法的局限性，同时保持了良好的性能。||
|**2024-07-23**|[On Differentially Private 3D Medical Image Synthesis with Controllable Latent Diffusion Models](http://arxiv.org/abs/2407.16405)|**[link](https://github.com/compai-lab/2024-miccai-dgm-daum)**|通常情况下，公共医学图像数据集的小规模以及严格的隐私问题阻碍了数据驱动的深度学习模型在医学成像领域的进步。本研究解决了短轴视图下 3D 心脏 MRI 图像的这些挑战。我们提出了潜在扩散模型，该模型可以生成以医学属性为条件的合成图像，同时通过差分隐私模型训练确保患者隐私。据我们所知，这是第一个在 3D 医学图像生成中应用和量化差分隐私的工作。我们使用公共数据对模型进行预训练，然后使用差分隐私在英国生物银行数据集上进行微调。我们的实验表明，预训练可以显着提高模型性能，在 $\epsilon=10$ 时，Fr\'echet 初始距离 (FID) 达到 26.77，而未经预训练的模型为 92.52。此外，我们还探讨了隐私约束和图像质量之间的权衡，研究了更严格的隐私预算如何影响输出可控性并可能导致性能下降。我们的结果表明，在使用差分隐私进行训练时进行适当的考虑可以显着提高合成心脏 MRI 图像的质量，但在实现一致的医学真实性方面仍然存在显着挑战。||
|**2024-07-23**|[Ranking protein-protein models with large language models and graph neural networks](http://arxiv.org/abs/2407.16375)|**[link](https://github.com/haddocking/deeprank-gnn-esm)**|蛋白质-蛋白质相互作用 (PPI) 与多种疾病相关，包括癌症、感染和神经退行性疾病。获得这些 PPI 的三维结构信息是干扰这些相互作用或指导药物设计的的基础。可以使用各种策略对这些复合物进行建模，所有这些策略通常都会产生大量的模型。此过程中的一个挑战性步骤是从大量生成的模型中识别良好的模型（接近天然的 PPI 构象）。为了应对这一挑战，我们之前开发了 DeepRank-GNN-esm，这是一种基于图的深度学习算法，用于利用蛋白质语言模型的力量对建模的 PPI 结构进行排名。在这里，我们通过示例详细介绍了我们软件的使用。DeepRank-GNN-esm 可在 https://github.com/haddocking/DeepRank-GNN-esm 免费获取。||
|**2024-07-19**|[DEPICT: Diffusion-Enabled Permutation Importance for Image Classification Tasks](http://arxiv.org/abs/2407.14509)|null|我们提出了一种基于排列的图像分类器解释方法。当前的图像模型解释方法（如激活图）仅限于像素空间中基于实例的解释，难以理解全局模型行为。相比之下，基于排列的表格数据分类器解释方法通过比较模型在排列特征前后对数据的性能来衡量特征重要性。我们提出了一种基于图像的模型解释方法，该方法在数据集图像中排列可解释的概念。给定一个标有特定概念（如标题）的图像数据集，我们在文本空间中排列示例中的概念，然后通过文本条件扩散模型生成图像。然后，特征重要性通过模型性能相对于未排列数据的变化来反映。当应用于一组概念时，该方法会生成特征重要性排名。我们证明，这种方法可以在合成和现实世界的图像分类任务中恢复底层模型的特征重要性。||
|**2024-07-19**|[T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation](http://arxiv.org/abs/2407.14505)|**[link](https://github.com/KaiyueSun98/T2V-CompBench)**|文本到视频（T2V）生成模型已经取得了显著进展，但它们将不同的对象、属性、动作和运动组合到视频中的能力仍未得到探索。以前的文本到视频基准测试也忽略了评估这一重要能力。在这项工作中，我们对组合文本到视频生成进行了首次系统研究。我们提出了T2V-CompBench，这是第一个为组合文本到视频生成量身定制的基准测试。T2V-CompBench涵盖了组合性的各个方面，包括一致的属性绑定、动态属性绑定、空间关系、运动绑定、动作绑定、对象交互和生成计数。我们进一步精心设计了基于MLLM的指标、基于检测的指标和基于跟踪的指标的评估指标，这些指标可以更好地反映七个类别700个文本提示的组合文本到视频生成质量。通过与人类评估的相关性验证了所提出指标的有效性。我们还对各种文本到视频生成模型进行了基准测试，并对不同模型和不同组合类别进行了深入分析。我们发现组合文本到视频生成对于当前模型来说极具挑战性，我们希望我们的尝试能够为该方向的未来研究提供启示。||
|**2024-07-19**|[M2D2M: Multi-Motion Generation from Text with Discrete Diffusion Models](http://arxiv.org/abs/2407.14502)|null|我们介绍了多动作离散扩散模型 (M2D2M)，这是一种利用离散扩散模型的优势从多个动作的文本描述中生成人体动作的新方法。这种方法巧妙地解决了生成多动作序列的挑战，确保了动作之间的无缝过渡以及一系列动作之间的连贯性。M2D2M 的优势在于其在离散扩散模型中的动态转移概率，它根据运动标记之间的接近程度调整转移概率，鼓励不同模式之间的混合。辅之以包括独立和联合去噪步骤的两阶段采样策略，M2D2M 可以有效地生成长期、平滑且上下文相关的连贯人体运动序列，并利用针对单动作生成训练的模型。大量实验表明，M2D2M 超越了当前最先进的从文本描述生成动作的基准，展示了其在解释语言语义和生成动态逼真动作方面的功效。||
|**2024-07-19**|[Contrastive Learning with Counterfactual Explanations for Radiology Report Generation](http://arxiv.org/abs/2407.14474)|null|由于解剖学内容的共通性，放射影像及其相应的报告具有高度相似性。这种固有的数据偏差可能导致自动报告生成模型学习到纠缠不清的虚假表征，从而产生误诊报告。为了解决这些问题，我们提出了一种基于反事实解释的新型放射学报告生成框架 (CoFE)。反事实解释是一种有效的工具，通过询问“如果……会怎样”的场景来理解算法做出的决策是如何改变的。通过利用这一概念，CoFE 可以通过对比事实图像和反事实图像的表征来学习非虚假的视觉表征。具体而言，我们通过在阳性和阴性样本之间交换补丁来导出反事实图像，直到预测的诊断发生变化。这里，阳性和阴性样本是语义上最相似但具有不同诊断标签的样本。此外，CoFE 采用可学习的提示符来有效地微调预训练的大型语言模型，封装事实和反事实内容，以提供更通用的提示符表示。在两个基准数据集上的大量实验表明，利用反事实解释使 CoFE 能够生成语义连贯且事实完整的报告，并在语言生成和临床效果指标方面优于现有方法。||
|**2024-07-19**|[Co-synthesis of Histopathology Nuclei Image-Label Pairs using a Context-Conditioned Joint Diffusion Model](http://arxiv.org/abs/2407.14434)|null|在多类别组织病理学细胞核分析任务中，训练数据的缺乏成为学习型方法性能的主要瓶颈。为了应对这一挑战，以前的方法利用生成模型通过生成合成样本来增加数据。然而，现有方法往往忽视了在合成数据中考虑生物组织环境（例如形状、空间布局和组织类型）的重要性。此外，虽然生成模型在合成逼真的组织病理学图像方面表现出优越的性能，但现有方法都不能同时生成图像-标签对。在本文中，我们介绍了一种使用上下文条件联合扩散模型共同合成组织病理学细胞核图像和配对语义标签的新框架。我们建议使用具有结构相关文本提示的细胞核质心布局对扩散模型进行条件化，以将空间和结构上下文信息纳入生成目标。此外，我们通过使用与图像和语义标签同时合成的距离图生成实例级细胞核标签，增强了合成语义标签的粒度。我们证明了我们的框架在多机构、多器官和多模态数据集上生成高质量样本的有效性。我们的合成数据在下游的细胞核分割和分类任务中始终优于现有的增强方法。||
|**2024-07-19**|[Controllable and Efficient Multi-Class Pathology Nuclei Data Augmentation using Text-Conditioned Diffusion Models](http://arxiv.org/abs/2407.14426)|null|在计算病理学领域，深度学习算法在细胞核分割和分类等任务中取得了重大进展。然而，这些先进方法的潜力受到可用标记数据缺乏的限制。尽管已经积极探索通过最近的生成模型进行图像合成来应对这一挑战，但现有工作几乎没有涉及标签增强，并且主要限于单类别和无条件标签生成。在本文中，我们介绍了一种使用文本条件扩散模型进行多类别细胞核数据增强的新型两阶段框架。在第一阶段，我们通过联合扩散模型生成多类别语义标签和相应的实例图来创新细胞核标签合成，该模型由指定标签结构信息的文本提示进行条件化。在第二阶段，我们利用语义和文本条件潜在扩散模型来有效地生成与生成的细胞核标签图像一致的高质量病理图像。我们在大而多样的病理细胞核数据集上证明了我们方法的有效性，评估包括定性和定量分析，以及下游任务的评估。||
|**2024-07-19**|[Towards Assessing Data Replication in Music Generation with Music Similarity Metrics on Raw Audio](http://arxiv.org/abs/2407.14364)|**[link](https://github.com/roserbatlleroca/mira)**|近来音乐生成技术的进步引发了人们对人工智能在创造性音乐过程、当前商业模式和知识产权管理相关影响的多方面担忧。一个相关的挑战是人工智能生成的音乐中可能存在对训练集的复制和剽窃，这可能导致数据滥用和侵犯知识产权。为了解决这个问题，我们提出了音乐复制评估（MiRA）工具：一种独立于模型的开放式评估方法，该方法基于不同的音频音乐相似性指标来评估训练集的数据复制情况。我们通过在基于合成样本的不同音乐类型中进行受控复制实验，评估了五种指标识别精确复制的能力。我们的结果表明，所提出的方法可以估计精确的数据复制，其比例高于10%。通过引入MiRA工具，我们鼓励研究人员、开发人员和用户对音乐生成模型进行数据复制方面的公开评估，强调生成式人工智能在音乐领域中伦理、社会、法律和经济后果的重要性。||
|**2024-07-19**|[Stable Audio Open](http://arxiv.org/abs/2407.14358)|**[link](https://github.com/stability-ai/stable-audio-tools)**|开源生成模型对于社区至关重要，它允许进行微调并作为呈现新模型的基线。然而，目前大多数文本到音频模型都是私有的，艺术家和研究人员无法访问和构建。在这里，我们描述了使用知识共享数据训练的新型开放权重文本到音频模型的架构和训练过程。我们的评估表明，该模型的性能在各种指标上都与最先进的技术相媲美。值得注意的是，报告的 FDopenl3 结果（测量生成的逼真度）展示了其在 44.1kHz 下合成高质量立体声的潜力。||
|**2024-07-19**|[As Generative Models Improve, People Adapt Their Prompts](http://arxiv.org/abs/2407.14333)|null|我们进行了一项有1891名参与者的在线实验，收集并分析了超过18000条提示，以探索随着生成式人工智能模型能力的不断提高，提示的重要性将如何变化。我们实验中的每位参与者都被随机分配使用三种文本到图像的扩散模型中的一种：DALL-E 2、其更先进的后续模型DALL-E 3，或带有自动提示修改功能的DALL-E 3版本。然后，参与者被要求编写提示，在连续10次尝试中尽可能接近地再现目标图像。我们发现，使用DALL-E 3的参与者比使用DALL-E 2的参与者表现更好。这种性能差距对应于参与者图像与其目标图像的相似性存在显著差异，并且其原因在于：(1) DALL-E 3的技术能力增强，以及(2) 参与者为应对这些增强能力而进行的内生性提示变化。更具体地说，尽管参与者对他们被分配到的模型毫不知情，但分配到DALL-E 3的参与者编写的提示更长，彼此之间在语义上更相似，并且包含更多的描述性词语。此外，虽然分配到带有提示修改功能的DALL-E 3的参与者仍然优于分配到DALL-E 2的参与者，但自动提示修改功能将使用DALL-E 3带来的优势降低了58%。综上所述，我们的结果表明，随着模型的不断进步，人们将继续调整他们的提示，以利用新模型的能力。||
|**2024-07-19**|[Panoptic Segmentation of Mammograms with Text-To-Image Diffusion Model](http://arxiv.org/abs/2407.14326)|null|乳腺X线摄影对于乳腺癌监测和早期诊断至关重要。然而，分析乳腺X线照片对于放射科医生来说是一项艰巨的任务，他们通常每天要查看数百张乳腺X线照片，导致过度诊断和过度治疗。计算机辅助诊断 (CAD) 系统已被开发用于协助这一过程，但它们的能力，特别是在病灶分割方面，仍然有限。随着深度学习的最新进展，它们的性能可能会得到改善。最近，视觉语言扩散模型出现，在图像生成和迁移到各种下游任务方面表现出色。我们的目标是利用它们的能力在全景环境中进行乳腺病灶分割，其中包括语义和实例级预测。具体来说，我们建议利用来自稳定扩散模型的预训练特征作为最先进的全景分割架构的输入，从而实现对单个乳腺病灶的准确描绘。为了弥合自然图像和医学图像领域之间的差距，我们在该框架中纳入了乳腺X线摄影专用 MAM-E 扩散模型以及 BiomedCLIP 图像和文本编码器。我们在两个最近发布的乳腺X线摄影数据集 CDD-CESM 和 VinDr-Mammo 上评估了我们的方法。对于实例分割任务，我们注意到 40.25 AP0.1 和 46.82 AP0.05，以及 25.44 PQ0.1 和 26.92 PQ0.05。对于语义分割任务，我们分别获得了 38.86 和 40.92 的 Dice 分数。||
|**2024-07-18**|[LogoSticker: Inserting Logos into Diffusion Models for Customized Generation](http://arxiv.org/abs/2407.13752)|null|近年来，文本到图像模型定制的进步凸显了将新概念与少量示例相结合的重要性。然而，这些进展主要局限于广泛认可的主题，模型可以通过充分的共享先验知识相对容易地学习这些主题。相比之下，以独特图案和文本元素为特征的标识很难在扩散模型中建立共享知识，因此提出了独特的挑战。为了弥合这一差距，我们引入了标识插入任务。我们的目标是将标识特征插入到扩散模型中，并使其能够在不同的上下文中无缝合成。我们提出了一种新颖的两阶段流水线 LogoSticker 来解决此任务。首先，我们提出了actor-critic关系预训练算法，该算法解决了模型对标识潜在空间定位和与其他对象交互理解上的重大差距。其次，我们提出了一种解耦的标识学习算法，该算法能够对标识进行精确定位和特征提取。LogoSticker 可以在不同的上下文中准确、和谐地生成标识。我们全面验证了 LogoSticker 相对于定制方法和大模型（如 DALLE~3）的有效性。（项目页面：\href{https://mingkangz.github.io/logosticker}{https://mingkangz.github.io/logosticker}）。||
|**2024-07-18**|[Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review](http://arxiv.org/abs/2407.13734)|**[link](https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq)**|本教程全面概述了微调扩散模型以优化下游奖励函数的方法。虽然众所周知，扩散模型具有出色的生成建模能力，但在生物学等领域的实际应用需要生成能够最大化某些期望指标的样本（例如，RNA 中的翻译效率、分子中的对接分数、蛋白质中的稳定性）。在这些情况下，可以优化扩散模型，使其不仅生成逼真的样本，还能明确地最大化感兴趣的指标。此类方法基于强化学习 (RL) 的概念。我们解释了各种 RL 算法的应用，包括 PPO、可微分优化、奖励加权 MLE、值加权采样和路径一致性学习，这些算法专门针对微调扩散模型而设计。我们旨在探讨基本方面，例如不同基于 RL 的微调算法在各种场景下的优缺点、基于 RL 的微调与非基于 RL 的方法相比的优势，以及基于 RL 的微调的形式目标（目标分布）。此外，我们还将考察它们与相关主题的联系，例如分类器指导、Gflownets、基于流的扩散模型、路径积分控制理论以及从非规范化分布（如 MCMC）中采样。本教程的代码可在以下网址获取：https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq||
|**2024-07-18**|[PASTA: Controllable Part-Aware Shape Generation with Autoregressive Transformers](http://arxiv.org/abs/2407.13677)|null|对能够自动生成高质量、多样化3D对象的深度生成模型的需求不断增长，推动了3D内容创建过程自动化工具的巨大进步。在本文中，我们提出了PASTA，一种用于生成高质量3D形状的自回归Transformer架构。PASTA包含两个主要组件：一个将对象生成为长方体图元序列的自回归Transformer，以及一个使用Transformer解码器实现的混合网络，该解码器组合长方体序列并为每个对象合成高质量的网格。我们的模型分两个阶段进行训练：首先，我们仅使用带注释的长方体部分作为监督来训练自回归生成模型；接下来，我们使用显式3D监督（以水密网格的形式）来训练混合网络。对各种ShapeNet对象的评估展示了我们的模型能够根据不同的输入（例如，从头开始、从部分对象、从文本和图像）以及尺寸引导生成（通过对定义对象边界的边界框进行显式条件化）来执行形状生成的能力。此外，由于我们的模型考虑了3D对象的底层基于零件的结构，因此我们能够选择特定的零件并生成具有该零件有意义变化的形状。正如我们的实验所证明的那样，我们的模型生成的3D形状比现有的基于零件和非基于零件的方法更真实、更多样化，同时更易于实现和训练。||
|**2024-07-18**|[MeshSegmenter: Zero-Shot Mesh Semantic Segmentation via Texture Synthesis](http://arxiv.org/abs/2407.13675)|**[link](https://github.com/zimingzhong/MeshSegmenter)**|我们提出了MeshSegmenter，这是一个简单但有效的框架，专为零样本3D语义分割而设计。该模型成功地将2D分割模型的强大功能扩展到3D网格，可在不同的网格和分割描述中提供准确的3D分割。具体来说，我们的模型利用Segment Anything Model (SAM) 模型从3D形状渲染的图像中分割目标区域。鉴于纹理对分割的重要性，我们还利用预训练的稳定扩散模型从3D形状生成具有纹理的图像，并利用SAM从具有纹理的图像中分割目标区域。纹理补充了形状的分割，即使在几何上不突出的区域也能实现准确的3D分割，例如在汽车网格内分割车门。为了实现3D分割，我们从不同的视图渲染2D图像，并对纹理和非纹理图像进行分割。最后，我们开发了一种多视图重新投票方案，将来自不同视图的2D分割结果和置信度分数整合到3D网格上，确保3D分割结果的一致性，并消除特定视角下的不准确性。通过这些创新，MeshSegmenter在数量和质量上都提供了稳定可靠的3D分割结果，突出了其作为3D零样本分割领域变革性工具的潜力。代码可在\url{https://github.com/zimingzhong/MeshSegmenter}获取。||
|**2024-07-18**|[Open-Vocabulary 3D Semantic Segmentation with Text-to-Image Diffusion Models](http://arxiv.org/abs/2407.13642)|null|本文研究了使用在大规模图像-字幕对上预训练的扩散模型进行开放词汇3D语义理解。我们提出了一种名为Diff2Scene的新方法，它利用来自文本-图像生成模型的冻结表示以及显著性感知和几何感知掩码，用于开放词汇3D语义分割和视觉定位任务。Diff2Scene摆脱了任何标记的3D数据，并有效地识别3D场景中的物体、外观、材料、位置及其组成。我们证明，它优于竞争基线，并在很大程度上优于最先进的方法。特别是，Diff2Scene在ScanNet200上的性能比最先进的方法提高了12%。||
|**2024-07-18**|[Training-free Composite Scene Generation for Layout-to-Image Synthesis](http://arxiv.org/abs/2407.13609)|null|近年来，文本到图像扩散模型的突破性进展极大地促进了从文本描述生成高保真、照片级真实图像的能力。然而，这些模型在从文本中解释空间排列方面往往存在困难，阻碍了它们生成具有精确空间配置图像的能力。为了弥合这一差距，布局到图像生成已成为一个有前途的方向。然而，基于训练的方法受到需要大量注释数据集的限制，导致数据采集成本高昂且概念范围受限。相反，免训练方法在复杂组合中准确定位和生成语义相似的对象方面面临挑战。本文介绍了一种新颖的免训练方法，旨在克服扩散调节阶段的对抗性语义交叉问题。通过选择性采样改进内部标记损失并通过注意力重新分配增强扩散过程，我们提出了两个创新约束：1）解决标记冲突以确保准确概念合成的标记间约束；2）改进像素间关系的自注意力约束。我们的评估证实了利用布局信息来指导扩散过程的有效性，可以生成内容更丰富、保真度和复杂性更高的图像。代码可在https://github.com/Papple-F/csg.git获取。||
|**2024-07-18**|[All Roads Lead to Rome? Exploring Representational Similarities Between Latent Spaces of Generative Image Models](http://arxiv.org/abs/2407.13449)|**[link](https://github.com/charumathib/thesis-latent-spaces)**|不同的生成图像模型是否会秘密地学习类似的底层表示？我们通过测量四种不同模型的潜在空间相似性来研究这个问题：变分自编码器 (VAEs)、生成对抗网络 (GANs)、归一化流 (NFs) 和扩散模型 (DMs)。我们的方法包括在冻结的潜在空间之间训练线性映射，以“缝合”任意编码器和解码器对，并在由此产生的“缝合”模型上测量基于输出和基于探针的指标。我们的主要发现是，即使潜在空间大小不同，性能良好的模型之间的潜在空间线性映射也能保留大多数视觉信息；对于 CelebA 模型，性别是最相似的可探测属性。最后，我们在一个 NF 上展示了潜在空间表示在训练早期就收敛了。||
|**2024-07-18**|[Movement-based models for abundance data](http://arxiv.org/abs/2407.13384)|null|我们开发了两种基于潜在连续运动模型的时空丰度数据统计模型。与当前统计生态学文献中的其他丰度模型不同，我们的模型特别关注个体运动与计数之间的明确联系，以及由此产生的时空自相关性。我们的第一个模型（快照）描述了具有假阴性检测误差的自由个体计数。我们的第二个模型（捕获）描述了移动个体在陷阱中的捕获和保留，它是使用公理化方法构建的，建立了三个简单原则，从中推导出捕获时间的密度是第二类沃尔泰拉积分方程的解。我们明确了由此生成的丰度场的时空均值和协方差结构，并为这两种模型开发了模拟方法。时空计数的联合分布是新多元分布的一个实例，这里将其称为演化类别多项分布，我们为此建立了一些关键属性。由于似然的一般表达式仍然难以处理，我们建议通过用多元高斯分布替换它来近似 MLE 拟合方法，这由中心极限定理证明是合理的，并且尊重均值和协方差结构。我们将此方法应用于果蝇在草地上释放并在陷阱阵列中反复捕获和计数的实验数据。我们估计扩散和平流参数，将我们的模型与生态扩散模型进行比较，并进行模拟研究以验证我们的分析。渐近一致性已通过实验验证。我们得出的结论是，我们可以仅使用丰度数据来估计运动参数，但必须了解避免低估扩散参数的必要条件。||
|**2024-07-18**|[A deep latent variable model for semi-supervised multi-unit soft sensing in industrial processes](http://arxiv.org/abs/2407.13310)|null|在许多工业过程中，明显缺乏数据限制了数据驱动软测量的发展。然而，通常有机会通过提高数据效率来学习更强大的模型。为了实现这一点，可以利用关于学习软测量数据的知识。利用工业数据经常具有的特性，我们引入了一种用于半监督多单元软测量的深度潜在变量模型。这种分层的生成模型能够联合建模不同的单元，并同时从标记和未标记数据中学习。使用两个数据集对多单元软测量进行了实证研究：一个单相流体流动的合成数据集和一个大型的油气井多相流动的真实数据集。我们表明，通过结合半监督学习和多任务学习，所提出的模型取得了优异的结果，优于当前解决此软测量问题的领先方法。我们还表明，当一个模型在多单元数据集上训练后，可以使用少量数据点对其进行微调，以适应以前未见过的单元。在这种微调过程中，未标记的数据提高了软测量的性能；值得注意的是，即使在没有标记数据的情况下也是如此。||
|**2024-07-18**|[URCDM: Ultra-Resolution Image Synthesis in Histopathology](http://arxiv.org/abs/2407.13277)|null|从组织病理学数据中诊断疾病需要对全切片图像 (WSI) 的各种分辨率进行全面分析。然而，由于专注于高保真图像块，现有的生成方法无法始终如一地表示 WSI 的层次结构。为了解决这个问题，我们提出了超分辨率级联扩散模型 (URCDM)，它能够以高分辨率合成整个组织病理学图像，同时真实地捕捉所有放大倍率下潜在解剖结构和病理学的细节。我们在三个独立的数据集（分别包含大脑、乳腺和肾脏组织）上评估了我们的方法，并超越了现有的最先进的多分辨率模型。此外，还进行了一项专家评估研究，结果表明，URCDM 在各种分辨率下始终如一地生成训练有素的评估者无法区分真伪的输出。所有代码和更多示例都可以在 GitHub 上找到。||
|**2024-07-16**|[Efficient Training with Denoised Neural Weights](http://arxiv.org/abs/2407.11966)|null|良好的权重初始化是降低深度神经网络 (DNN) 模型训练成本的有效措施。然而，如何初始化参数的选择具有挑战性，可能需要手动调整，这既耗时又容易出现人为错误。为了克服这些限制，这项工作朝着构建权重生成器来合成用于初始化的神经网络权重迈出了新的一步。我们以使用生成对抗网络 (GAN) 的图像到图像转换任务为例，因为它易于收集涵盖广泛范围的模型权重。具体来说，我们首先收集一个包含各种图像编辑概念及其相应训练权重的数据集，这些权重稍后将用于训练权重生成器。为了解决层间不同的特性和要预测的大量权重问题，我们将权重分成大小相等的块，并为每个块分配一个索引。随后，使用扩散模型在该数据集上进行训练，同时使用概念的文本条件和块索引。通过使用我们的扩散模型预测的去噪权重初始化图像转换模型，训练仅需 43.3 秒。与从头开始训练（即 Pix2pix）相比，我们在新概念上实现了 15 倍的训练时间加速，同时获得了更好的图像生成质量。||
|**2024-07-16**|[Context-Guided Diffusion for Out-of-Distribution Molecular and Protein Design](http://arxiv.org/abs/2407.11942)|**[link](https://github.com/leojklarner/context-guided-diffusion)**|生成模型具有加速新型分子疗法和材料发现关键步骤的潜力。扩散模型最近成为一种强大的方法，在无条件样本生成方面表现出色，并且在数据驱动引导下，在其训练领域内进行条件生成也表现出色。 然而，可靠地从训练数据之外的高价值区域进行采样仍然是一个开放的挑战——目前的方法主要集中在修改扩散过程本身。在本文中，我们开发了上下文引导扩散 (CGD)，这是一种简单的即插即用方法，它利用未标记数据和平滑约束来改进引导扩散模型的分布外泛化能力。我们证明，这种方法可以在各种设置中带来实质性的性能提升，包括连续、离散和图结构的扩散过程，并在药物发现、材料科学和蛋白质设计中都有应用。||
|**2024-07-16**|[Data-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model Co-development](http://arxiv.org/abs/2407.11784)|**[link](https://github.com/modelscope/data-juicer)**|大规模多模态生成模型的出现极大地促进了人工智能的发展，带来了前所未有的性能和功能水平。然而，由于以模型为中心和以数据为中心的发展路径在历史上是孤立的，导致结果欠佳和资源利用效率低下，优化这些模型仍然具有挑战性。为了解决这个问题，我们提出了一套全新的沙盒套件，专门用于集成的数据模型协同开发。这个沙盒提供了一个全面的实验平台，能够快速迭代和洞察力驱动的模型和数据改进。我们提出的“探测-分析-改进”工作流程，通过在最先进的类似LLaVA和基于DiT的模型上的应用验证，实现了显著的性能提升，例如登顶VBench排行榜。我们还从详尽的基准测试中获得了富有成效的见解，揭示了数据质量、多样性和模型行为之间至关重要的相互作用。为了促进对多模态数据和生成模型的更深入理解和未来发展，我们的代码、数据集和模型都维护在https://github.com/modelscope/data-juicer/blob/main/docs/Sandbox.md，并且可以访问。||
|**2024-07-16**|[Diffusion-driven self-assembly of emerin nanodomains at the nuclear envelope](http://arxiv.org/abs/2407.11758)|null|Emerin是一种核膜蛋白，在机械传导和核形状适应中起着重要的生物学作用，它在内核膜上自组装成纳米级的结构域。这些纳米域的大小和emerin占据率会随着施加的机械应力和与Emery-Dreifuss肌营养不良症(EDMD)相关的emerin突变而改变。通过理论和实验的结合，我们在这里表明，一个简单的反应扩散模型可以解释emerin纳米域的自组装。我们的模型与野生型emerin和EDMD相关emerin突变（无论是否存在外力）的emerin纳米域的大小和占据率的实验观察结果在数量上一致，并且允许从对emerin纳米域整体性质的观察结果成功预测emerin扩散系数。我们的研究结果从emerin及其核结合伙伴的关键反应和扩散特性的变化方面，为EDMD相关的emerin组织缺陷提供了物理解释。||
|**2024-07-16**|[Generating Multi-Modal and Multi-Attribute Single-Cell Counts with CFGen](http://arxiv.org/abs/2407.11734)|**[link](https://github.com/theislab/CFGen)**|单细胞RNA测序数据的生成模型在社区驱动的任务中显示出巨大的潜力，例如轨迹推断、批次效应去除和基因表达生成。然而，大多数最近从噪声生成合成单细胞的深度模型都是在预处理的连续基因表达近似值上运行的，忽略了单细胞数据固有的离散性和过度分散性，这限制了下游应用并阻碍了鲁棒噪声模型的结合。此外，基于深度学习的合成单细胞生成的关键方面仍未得到充分探索，例如可控的多模态和多标签生成及其在下游任务性能增强中的作用。这项工作提出了用于生成的细胞流（CFGen），这是一种基于流的用于多模态单细胞计数的条件生成模型，它明确考虑了数据的离散性。我们的结果表明，在考虑新的生成任务（例如以多个属性为条件和通过数据增强提高稀有细胞类型分类）的同时，可以改进关键生物学数据特征的恢复。通过在一组不同的生物数据集和设置中展示CFGen，我们提供了其对计算生物学和深度生成模型领域价值的证据。||
|**2024-07-16**|[Theoretical Insights into CycleGAN: Analyzing Approximation and Estimation Errors in Unpaired Data Generation](http://arxiv.org/abs/2407.11678)|null|本文重点分析了名为 CycleGAN 的非配对数据生成模型的过度风险。与经典的 GAN 不同，CycleGAN 不仅可以在两个非配对分布之间转换数据，还可以确保映射的一致性，这是由 CycleGAN 独有的循环一致性项所鼓励的。CycleGAN 中模型结构日益复杂以及循环一致性项的加入给误差分析带来了新的挑战。通过考虑模型架构和训练过程的影响，将风险分解为两个方面：逼近误差和估计误差。这两个误差项分别进行分析，并最终通过考虑它们之间的权衡来组合。每个部分都经过严格分析；通过构建最优传输映射的逼近来逼近误差，并通过使用 Rademacher 复杂度建立上限来估计误差。我们的分析不仅隔离了这些误差，还探讨了它们之间的权衡，这为 CycleGAN 的架构和训练过程如何影响其性能提供了理论见解。||
|**2024-07-16**|[Mask-guided cross-image attention for zero-shot in-silico histopathologic image generation with a diffusion model](http://arxiv.org/abs/2407.11664)|null|利用生成式人工智能创建虚拟数据有望成为计算病理学中对整个切片图像进行染色、成像和标注的一种经济高效的替代方案。扩散模型是生成虚拟图像的最先进解决方案，可提供无与伦比的保真度和真实感。使用外观迁移扩散模型可以实现零样本图像生成，从而促进快速应用并无需模型训练。然而，当前的外观迁移扩散模型是为自然图像设计的，其主要任务是将前景物体从源域迁移到目标域，而背景的重要性微不足道。然而，在计算病理学中，特别是在肿瘤学中，直接定义图像中的哪些物体应该被归类为前景和背景并不容易，因为图像中的所有物体都可能对详细了解肿瘤微环境至关重要。我们通过修改外观迁移引导，使用现有的分割掩码在特定类别 AdaIN 特征统计匹配之间交替，从而提高了外观迁移扩散模型对免疫组化染色图像的适用性。所提出的方法的性能在监督上皮分割的下游任务中得到了证明，结果表明，模型训练所需的标注数量可以减少 75%，优于基线方法。此外，我们咨询了 certified 病理学家，以探讨未来的改进方向。我们预计这项工作将激励零样本扩散模型在计算病理学中的应用，为生成具有无与伦比的保真度和真实感的虚拟图像提供一种有效的方法，这些图像对下游任务（如训练现有深度学习模型或微调基础模型）具有重要意义。||
|**2024-07-16**|[Magnetogram-to-Magnetogram: Generative Forecasting of Solar Evolution](http://arxiv.org/abs/2407.11659)|**[link](https://github.com/fpramunno/MAG2MAG)**|研究太阳磁场对于理解太阳内部的物理过程及其对行星际环境的影响至关重要。我们引入了一种新方法，使用基于去噪扩散概率模型 (DDPM) 的图像到图像转换技术来预测太阳视线 (LoS) 磁图的演变。我们的方法结合了图像质量的“计算机科学指标”和物理准确性的“物理指标”来评估模型性能。结果表明，DDPM 在保持太阳磁场的结构完整性、动态范围、磁通量和其他物理特征（如活动区域的大小）方面非常有效，甚至在耀斑情况下也优于传统的持续性模型。我们的目标是利用深度学习不仅用于可视化，而且将其作为望远镜的集成和交互式工具，增强我们对太阳耀斑等意外物理事件的理解。未来的研究将致力于整合更多样化的太阳数据，以提高我们生成模型的准确性和适用性。||
|**2024-07-16**|[CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging](http://arxiv.org/abs/2407.11652)|null|联邦学习 (FL) 提供了一种保护隐私的训练分散数据模型的方法。它在医疗保健领域具有巨大的潜力，但由于医疗图像数据在客户端之间的差异，以及注释有限，因此面临着挑战。本文介绍了跨客户端差异自适应联邦学习 (CCVA-FL) 来解决这些问题。CCVA-FL 旨在通过将图像转换到共同特征空间来最小化客户端之间的差异。它涉及专家对每个客户端的图像子集进行注释，然后选择数据复杂度最低的客户端作为目标客户端。然后，使用基于目标客户端注释图像的可扩展扩散模型和 Transformer (DiT) 生成合成医学图像。这些合成图像捕捉了多样性并代表了原始数据，并与其他客户端共享。然后，每个客户端使用图像到图像的转换将其本地图像转换到目标图像空间。转换后的图像随后用于联邦学习设置中，以开发服务器模型。我们的结果表明，CCVA-FL 通过有效解决客户端之间的数据分布差异，在不损害隐私的情况下，优于传统的联邦平均算法。||
|**2024-07-16**|[Scaling Diffusion Transformers to 16 Billion Parameters](http://arxiv.org/abs/2407.11633)|**[link](https://github.com/feizc/dit-moe)**|本文介绍了DiT-MoE，这是一种稀疏版本的扩散Transformer，它在保持高度优化的推理能力的同时，在规模和竞争力上都与密集网络相当。DiT-MoE包含两种简单的设计：共享专家路由和专家级平衡损失，从而捕获共同知识并减少不同路由专家之间的冗余。当应用于条件图像生成时，对专家专业化的深入分析获得了一些有趣的观察结果：（i）专家选择表现出对空间位置和去噪时间步长的偏好，而对不同的类别条件信息不敏感；（ii）随着MoE层越来越深，专家的选择逐渐从特定的空间位置转变为分散和平衡；（iii）专家专业化倾向于集中在早期时间步长，然后在经过一半时间后逐渐均匀。我们将其归因于扩散过程，该过程首先对低频空间信息进行建模，然后对高频复杂信息进行建模。基于以上指导，一系列DiT-MoE实验实现了与密集网络相当的性能，但在推理过程中需要的计算量要少得多。更令人鼓舞的是，我们展示了DiT-MoE在合成图像数据方面的潜力，将扩散模型扩展到16.5B参数，在512×512分辨率设置下获得了新的SoTA FID-50K分数1.80。项目页面：https://github.com/feizc/DiT-MoE。||
|**2024-07-12**|[Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text](http://arxiv.org/abs/2407.09364)|null|大型语言模型发展的显著进步模糊了人类和人工智能生成文本之间的界限。人工智能生成文本的日益普及及其检测难度给我们的社会带来了新的挑战。在本文中，我们提出了 WhosAI 来解决检测和归因人工智能生成文本的问题，这是一个三元组网络对比学习框架，旨在预测给定输入文本是由人类还是人工智能生成的，并揭示文本的作者。与大多数现有方法不同，我们提出的框架旨在从多个生成器同时学习语义相似性表示，从而平等地处理检测和归因任务。此外，WhosAI 与模型无关，并且可以通过将新的人工智能文本生成模型生成的实例合并到我们框架学习的嵌入空间中来扩展到新模型的发布。在包含 20 万篇新闻文章的 TuringBench 基准测试上的实验结果表明，我们提出的框架在图灵测试和作者归因任务中均取得了优异的结果，优于 TuringBench 基准测试排行榜中列出的所有方法。||
|**2024-07-12**|[Any-Property-Conditional Molecule Generation with Self-Criticism using Spanning Trees](http://arxiv.org/abs/2407.09357)|**[link](https://github.com/samsungsailmontreal/anymolgencritic)**|生成新分子极具挑战性，大多数表示方法会导致生成模型产生许多无效分子。基于生成树的图生成 (STGG) 是一种很有前景的方法，可以确保生成有效分子，在无条件生成方面优于最先进的 SMILES 和图扩散模型。在现实世界中，我们希望能够根据一种或多种所需属性（而不是无条件地）生成分子。因此，在这项工作中，我们将 STGG 扩展到多属性条件生成。我们的方法 STGG+ 结合了现代 Transformer 架构、训练期间属性的随机掩蔽（能够以任何属性子集和无分类器引导为条件）、辅助属性预测损失（允许模型对分子进行自我批评并选择最佳分子）和其他改进。我们证明了 STGG+ 在分布内和分布外条件生成以及奖励最大化方面均达到了最先进的性能。||
|**2024-07-12**|[PID: Physics-Informed Diffusion Model for Infrared Image Generation](http://arxiv.org/abs/2407.09299)|**[link](https://github.com/fangyuanmao/pid)**|红外成像技术因其在低能见度条件下的可靠传感能力而受到广泛关注，促使许多研究致力于将丰富的RGB图像转换为红外图像。然而，大多数现有的图像转换方法将红外图像视为一种风格变化，而忽略了潜在的物理规律，这限制了它们的实际应用。为了解决这些问题，我们提出了一种物理信息扩散（PID）模型，用于将RGB图像转换为符合物理规律的红外图像。我们的方法利用了扩散模型的迭代优化，并在训练过程中结合了基于红外规律先验知识的强物理约束。这种方法增强了转换后的红外图像与真实红外域之间的相似性，而无需增加额外的训练参数。实验结果表明，PID明显优于现有的最先进方法。我们的代码可在https://github.com/fangyuanmao/PID获取。||
|**2024-07-12**|[Learning Distances from Data with Normalizing Flows and Score Matching](http://arxiv.org/abs/2407.09297)|null|基于密度的距离 (DBD) 为度量学习问题提供了一种优雅的解决方案。通过定义一个黎曼度量，该度量随着概率密度的降低而增加，最短路径自然遵循数据流形，并且点根据数据的模态进行聚类。我们发现，现有的估计费马距离（一种特定选择的 DBD）的方法，由于 i) 不准确的密度估计和 ii) 依赖于在高维情况下越来越粗糙的基于图的路径，因此在低维和高维情况下都存在收敛性差的问题。为了解决这些问题，我们建议使用归一化流（一种具有易处理密度估计的生成模型）来学习密度，并采用从基于图的提议初始化的评分模型来使用平滑松弛方法。此外，我们引入了一种维度自适应的费马距离，它在扩展到高维时表现出更直观的特性，并提供更好的数值特性。我们的工作为实际应用基于密度的距离铺平了道路，特别是在高维空间中。||
|**2024-07-12**|[Surgical Text-to-Image Generation](http://arxiv.org/abs/2407.09230)|null|获取用于研究和开发的手术数据受到高昂标注成本以及实践和伦理限制的严重阻碍。利用合成生成的图像可以提供一种有价值的替代方案。在这项工作中，我们利用 CholecT50 数据集对文本到图像生成模型在手术领域的适应性进行了深入分析，该数据集提供了用手术动作三元组（器械、动词、目标）标注的手术图像。我们研究了各种语言模型，发现 T5 在基于三元组的文本输入区分手术动作方面提供了更独特的特征。我们的分析表明，长标题和基于三元组的标题之间存在高度一致性，支持使用基于三元组的标签。我们通过发现三元组文本嵌入在潜在空间中以器械为中心，并设计了一种基于器械的类别平衡技术来抵消手术数据中的不平衡和偏差，从而解决了在没有额外输入信号的情况下训练基于三元组标题的文本到图像模型的挑战，改进了训练的收敛性。我们扩展了基于扩散的生成模型 Imagen，开发了 Surgical Imagen，用于从基于三元组的文本提示生成逼真且与活动一致的手术图像。我们使用多种指标评估我们的模型，包括人类专家调查和 FID 和 CLIP 分数等自动化方法。我们从质量、一致性、推理、知识和鲁棒性等关键方面评估模型性能，证明了我们的方法在提供真实数据收集的现实替代方案方面的有效性。||
|**2024-07-12**|[Salt & Pepper Heatmaps: Diffusion-informed Landmark Detection Strategy](http://arxiv.org/abs/2407.09192)|null|解剖标志检测是指识别图像中用于临床测量的关键区域的过程。每个标志都是由临床医生标记的单个真实点。机器学习模型将标志的位置预测为由热图表示的概率区域。扩散模型由于其高质量的采样和模式覆盖范围，在生成建模中越来越受欢迎，导致其在医学图像处理中用于语义分割。扩散模型可以进一步调整以学习标志的分布。扩散模型的随机性捕获了标志预测中的波动，我们通过将其模糊成有意义的概率区域来利用这一点。在本文中，我们将自动解剖标志检测重新表述为一个精确的生成建模任务，生成一个少量热像素热图。我们的方法实现了最先进的MRE性能，并且SDR性能与现有工作相当。||
|**2024-07-12**|[Variational Inference via Smoothed Particle Hydrodynamics](http://arxiv.org/abs/2407.09186)|null|本文提出了一种基于光滑粒子流体动力学 (SPH) 的新型变分推理方法 SPH-ParVI，用于对部分已知密度（例如，直到常数）进行采样或使用梯度进行采样。SPH-ParVI 模拟了在目标密度驱动的外部效应下流体的流动；流体的瞬态或稳态近似于目标密度。连续流体通过 SPH 建模为相互作用的粒子系统 (IPS)，其中每个粒子都携带平滑的属性，并根据纳维-斯托克斯方程进行交互和演化。这种无网格的拉格朗日模拟方法为一类概率模型（例如在贝叶斯推理和生成建模中遇到的模型）提供了快速、灵活、可扩展和确定性的采样和推理。||
|**2024-07-12**|[Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors](http://arxiv.org/abs/2407.09136)|**[link](https://github.com/eth-lre/verify-then-generate)**|大型语言模型 (LLM) 为向所有人提供高质量的个性化教育提供了机会。一种有希望实现这一目标的方法是构建能够支持学生解决问题的对话辅导模型。然而，尽管现有的 LLM 在解决推理问题方面表现出色，但它们难以准确地检测学生的错误并根据这些错误调整反馈。受现实世界教学实践的启发，教师会识别学生的错误并根据错误定制他们的回应，我们专注于验证学生的解决方案，并展示了这种验证的基础如何提高导师回应生成的整体质量。我们收集了一个包含 1000 个逐步数学推理链的数据集，其中第一步错误由教师注释。我们通过实证表明，为当前模型找到学生解决方案中的错误具有挑战性。我们提出并评估了几种用于检测这些错误的验证器。通过自动评估和人工评估，我们表明，与现有基线相比，学生解决方案验证器引导生成模型针对学生错误生成更有针对性的回应，这些回应通常更正确，并且 hallucinations 更少。||
|**2024-07-12**|[Overcoming Catastrophic Forgetting in Tabular Data Classification: A Pseudorehearsal-based approach](http://arxiv.org/abs/2407.09039)|null|持续学习 (CL) 提出了一个重要挑战，即如何在不忘记先前获得的知识的情况下，适应不断变化的数据分布，同时整合新知识。在本文中，我们介绍了一种新的方法，称为基于表格数据排练的增量终身学习框架 (TRIL3)，旨在解决表格数据分类问题中的灾难性遗忘现象。TRIL3 使用基于原型的增量生成模型 XuILVQ 生成合成数据来保留旧知识，并使用 DNDF 算法（该算法已修改为以增量方式运行）来学习表格数据的分类任务，而无需存储旧样本。在经过不同的测试以获得足够的合成数据百分比并将 TRIL3 与其他可用的 CL 方案进行比较后，我们可以得出结论，TRIL3 的性能优于文献中的其他方案，仅使用 50% 的合成数据。||
|**2024-07-12**|[Aligning Diffusion Behaviors with Q-functions for Efficient Continuous Control](http://arxiv.org/abs/2407.09024)|null|基于语言模型对齐的最新进展，我们将离线强化学习制定为一个两阶段优化问题：首先在无奖励行为数据集上预训练表达能力强的生成策略，然后微调这些策略以与特定任务的标注（如 Q 值）对齐。这种策略允许我们利用丰富多样的行为数据来增强泛化能力，并使用最少的标注快速适应下游任务。特别地，我们引入了高效扩散对齐（EDA）来解决连续控制问题。EDA 利用扩散模型进行行为建模。然而，与之前的方法不同，我们将扩散策略表示为标量神经网络相对于动作输入的导数。这种表示至关重要，因为它可以直接计算扩散模型的密度，使其与现有的 LLM 对齐理论兼容。在策略微调期间，我们扩展了基于偏好的对齐方法（如直接偏好优化（DPO））以将扩散行为与连续 Q 函数对齐。我们在 D4RL 基准测试上的评估表明，EDA 在整体性能上优于所有基线方法。值得注意的是，EDA 在仅使用 1% 的 Q 值标记数据进行微调的情况下仍能保持约 95% 的性能，并且仍然优于其他几个基线方法。||
|**2024-07-11**|[Video Diffusion Alignment via Reward Gradients](http://arxiv.org/abs/2407.08737)|**[link](https://github.com/mihirp1998/vader)**|我们在构建基础视频扩散模型方面取得了重大进展。由于这些模型使用大规模无监督数据进行训练，因此使这些模型适应特定的下游任务变得至关重要。通过监督微调来调整这些模型需要收集视频的目标数据集，这既具有挑战性又乏味。在这项工作中，我们利用预先训练的奖励模型来调整视频扩散模型，这些奖励模型是通过基于强大的视觉判别模型的偏好学习的。这些模型包含关于生成的 RGB 像素的密集梯度信息，这对于在复杂搜索空间（例如视频）中的高效学习至关重要。我们表明，将这些奖励模型的梯度反向传播到视频扩散模型可以实现视频扩散模型在计算和样本方面的有效对齐。我们展示了各种奖励模型和视频扩散模型的结果，证明了我们的方法在奖励查询和计算方面比先前的无梯度方法效率更高。我们的代码、模型权重和更多可视化内容可在 https://vader-vid.github.io 获取。||
|**2024-07-11**|[Live2Diff: Live Stream Translation via Uni-directional Attention in Video Diffusion Models](http://arxiv.org/abs/2407.08701)|null|大型语言模型由于其时间上的单向注意机制（该机制对当前标记和先前标记之间的相关性进行建模），在生成文本和音频等流数据方面表现出了显著的效果。然而，尽管对实时视频处理的需求日益增长，但视频流的研究仍然远远落后。最先进的视频扩散模型利用双向时间注意力来建模当前帧和所有周围（即包括未来）帧之间的相关性，这阻碍了它们处理流视频。为了解决这个问题，我们提出了 Live2Diff，这是首次尝试设计具有单向时间注意力的视频扩散模型，专门针对实时流视频翻译。与以前的工作相比，我们的方法通过将当前帧与其前一帧和一些初始预热帧相关联来确保时间一致性和平滑度，而无需任何未来帧。此外，我们使用了一种高效的去噪方案，该方案采用 KV 缓存机制和流水线技术，以促进以交互式帧率进行流视频翻译。大量实验表明，所提出的注意力机制和流水线的有效性，在时间平滑度和/或效率方面优于以前的方法。||
|**2024-07-11**|[Scattering transforms on the sphere, application to large scale structure modelling](http://arxiv.org/abs/2407.08687)|null|散射变换是最近为研究高度非高斯过程而开发的一种新型汇总统计数据，已被证明在天文物理研究中非常有前景。特别是，它们允许人们从有限的数据中构建复杂非线性场的生成模型。在即将进行的宇宙学调查的背景下，有必要将这些工具扩展到球面数据。我们开发了球面上的散射变换，并专注于构建天体物理场最大熵生成模型。生成模型的质量，无论是在统计上还是视觉上，都非常令人满意，因此为未来的宇宙学研究开辟了广泛的新应用。||
|**2024-07-11**|[CAD-Prompted Generative Models: A Pathway to Feasible and Novel Engineering Designs](http://arxiv.org/abs/2407.08675)|null|文本到图像生成模型越来越多地被用于协助各个创意领域的概念生成，例如图形设计、用户界面设计和时装设计。然而，由于模型在生成可行设计概念的图像方面存在挑战，它们在工程设计中的应用仍然有限。为了解决这个问题，本文介绍了一种通过使用可行的 CAD 图像提示生成来提高设计可行性的方法。在这项工作中，通过一个自行车设计任务的案例研究，使用现成的文本到图像模型 Stable Diffusion 2.1，研究了该方法的有效性。在七个不同的生成设置中，使用不同的 CAD 图像提示权重，生成了一组多样化的自行车设计，并根据其感知的可行性和新颖性对这些设计进行了评估。结果表明，CAD 图像提示成功地帮助 Stable Diffusion 2.1 等文本到图像模型创建了明显更可行的设计图像。虽然在可行性和新颖性之间观察到了一般的权衡，但当提示权重保持在 0.35 左右的低水平时，设计可行性得到显着提高，而其新颖性与仅通过文本提示生成的设计保持一致。该案例研究的见解为工程设计过程的不同阶段选择合适的 CAD 图像提示权重提供了一些指导。如果使用得当，我们的 CAD 图像提示方法为文本到图像模型在工程设计中的更广泛应用打开了大门。||
|**2024-07-11**|[Controlling the Fidelity and Diversity of Deep Generative Models via Pseudo Density](http://arxiv.org/abs/2407.08659)|null|我们提出了一种对深度生成模型（如 GAN 和扩散模型）进行偏差的方法，以生成具有更高保真度或更高多样性的数据。我们的方法涉及通过一种新的个体样本度量标准（称为伪密度）来操纵训练数据和生成数据的分布，该度量标准基于来自真实样本的最近邻信息。我们的方法提供了三种不同的技术来调整深度生成模型的保真度和多样性：1）每样本扰动，可以对单个样本进行精确调整，使其具有更常见或更独特的特征；2）模型推理过程中的重要性采样，以增强生成数据的保真度或多样性；3）使用重要性采样进行微调，引导生成模型学习调整后的分布，从而控制保真度和多样性。此外，我们的微调方法展示了在最少迭代次数下改进预训练生成模型的 Frechet Inception Distance (FID) 的能力。||
|**2024-07-11**|[Adaptive Smooth Non-Stationary Bandits](http://arxiv.org/abs/2407.08654)|null|我们研究了一个 $K$臂非平稳老虎机模型，其中奖励随时间平滑变化，可以用奖励作为时间函数的赫尔德类假设来刻画。这种平滑变化由赫尔德指数$\beta$和系数$\lambda$参数化。虽然这个一般模型的各种子情况已经被单独研究，但我们首先对所有$K,\beta,\lambda$都确定了极小极大动态遗憾率。接下来，我们证明了这种最优动态遗憾可以自适应地实现，而无需知道$\beta,\lambda$。相比之下，即使在参数已知的情况下，以前的上界也只在$\beta\leq 1$和$\beta=2$的有限范围内知道(Slivkins, 2014; Krishnamurthy and Gopalan, 2021; Manegueu et al., 2021; Jia et al.,2023)。因此，我们的工作解决了这些不同的文献线索提出的开放性问题。我们还研究了在非平稳老虎机中获得更快的与间隔相关的遗憾率的问题。虽然众所周知，这种速率在一般情况下是不可能实现的(Garivier and Moulines, 2011)，但我们表明，允许安全臂的环境(Suk and Kpotufe, 2022)允许比$\sqrt{T}$ 的最坏情况缩放快得多的速率。虽然之前在这个方向上的工作集中在获得通常的对数遗憾界，如在平稳周期上的总和，但我们新的与间隔相关的速率揭示了非平稳性的新的乐观状态，即使是对数界也是悲观的。我们证明了我们新的与间隔相关的速率是紧的，并且它的可实现性（即，安全臂使之成为可能）在平滑的赫尔德类模型中有一个惊人的简单和干净的特征。||
|**2024-07-11**|[Latent Conditional Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph Mode](http://arxiv.org/abs/2407.08500)|null|连续时间动态图（CTDG）精确地模拟了不断演变的现实世界关系，引起了学术界和工业界对动态图学习的浓厚兴趣。然而，现有的CTDG模型面临着来自噪声和有限历史数据的挑战。图数据增强（GDA）成为了一种关键解决方案，但目前的方法主要集中在静态图上，难以有效解决CTDG固有的动态性问题。此外，这些方法通常需要大量的领域专业知识来进行参数调整，并且缺乏对增强效果的理论保证。为了解决这些问题，我们提出了Conda，这是一种专为CTDG量身定制的基于潜在扩散的新型GDA方法。Conda采用了一种类似三明治的架构，结合了变分自动编码器（VAE）和条件扩散模型，旨在为目标节点生成增强的历史邻居嵌入。与通过预训练在整个图上训练的传统扩散模型不同，Conda需要目标节点的历史邻居序列嵌入进行训练，从而促进更有针对性的增强。我们将Conda集成到CTDG模型中，并采用交替训练策略来优化性能。在六个广泛使用的现实世界数据集上的大量实验表明，我们的方法始终如一地提高了性能，特别是在历史数据有限的情况下。||
|**2024-07-11**|[Natural language is not enough: Benchmarking multi-modal generative AI for Verilog generation](http://arxiv.org/abs/2407.08473)|**[link](https://github.com/aichipdesign/chipgptv)**|自然语言接口在利用大型语言模型实现从高级规范自动生成 Verilog 方面展现出巨大潜力，引发了广泛关注。然而，本文阐述了视觉表示为具有空间复杂性的硬件架构的设计意图提供了至关重要的上下文信息，这可能超越了仅使用自然语言输入的效率。在此基础上，本文介绍了一个开源的多模态生成模型基准测试，该模型专为从视觉-语言输入合成 Verilog 而设计，涵盖了单个和复杂的模块。此外，我们还介绍了一个开源的视觉和自然语言 Verilog 查询语言框架，以促进高效且用户友好的多模态查询。为了评估所提出的多模态硬件生成式人工智能在 Verilog 生成任务中的性能，我们将其与仅依赖自然语言的流行方法进行了比较。结果表明，与仅基于自然语言的查询相比，多模态生成的 Verilog 在准确性方面有显著提高。我们希望在大硬件设计模型时代揭示一种新的硬件设计方法，从而促进更具多样性和效率的硬件设计方法。||
|**2024-07-11**|[A Comprehensive Survey on Human Video Generation: Challenges, Methods, and Insights](http://arxiv.org/abs/2407.08428)|**[link](https://github.com/wentaol86/awesome-human-body-video-generation)**|人体视频生成是一个充满活力且快速发展的领域，其目标是利用生成模型根据文本、音频和姿势等控制条件合成二维人体视频序列。由于其在电影、游戏和虚拟通信等领域具有广泛的应用潜力，因此生成自然逼真的人体视频至关重要。生成模型的最新进展为该领域的兴趣日益浓厚奠定了坚实的基础。尽管取得了重大进展，但由于角色的一致性、人体运动的复杂性以及与环境关系的困难，人体视频生成的任务仍然具有挑战性。本综述全面概述了人体视频生成的现状，据我们所知，这是该领域首个广泛的文献综述。我们首先介绍人体视频生成的基本原理，以及促进该领域发展的生成模型的演变。然后，我们研究了人体视频生成中三个关键子任务所采用的主要方法：文本驱动、音频驱动和姿势驱动的运动生成。这些领域将根据指导生成过程的条件进行探讨。此外，我们还收集了最常用的数据集和评估指标，这些指标对于评估生成视频的质量和真实性至关重要。最后，我们讨论了该领域当前面临的挑战，并提出了未来研究的可能方向。本综述旨在为研究界提供一个清晰而全面的人体视频生成进展概况，重点介绍已取得的里程碑和未来面临的挑战。||
|**2024-07-11**|[Diff-Tracker: Text-to-Image Diffusion Models are Unsupervised Trackers](http://arxiv.org/abs/2407.08394)|null|我们介绍了一种名为 Diff-Tracker 的新方法，用于解决具有挑战性的无监督视觉跟踪任务，该方法利用了预训练的文本到图像扩散模型。我们的主要思想是利用预训练扩散模型中封装的丰富知识，例如对图像语义和结构信息的理解，来解决无监督视觉跟踪问题。为此，我们设计了一个初始提示学习器，通过学习表示目标的提示，使扩散模型能够识别跟踪目标。此外，为了促进提示符动态适应目标的运动，我们提出了一种在线提示符更新器。在五个基准数据集上的大量实验表明，我们提出的方法是有效的，并且达到了最先进的性能。||
|**2024-07-09**|[ConceptExpress: Harnessing Diffusion Models for Single-image Unsupervised Concept Extraction](http://arxiv.org/abs/2407.07077)|**[link](https://github.com/haoosz/conceptexpress)**|虽然个性化文本到图像生成技术已经能够从多张图像中学习单个概念，但更实际但也更具挑战性的场景涉及在单个图像中学习多个概念。然而，现有的解决此场景的工作严重依赖于大量的人工标注。在本文中，我们介绍了一项名为无监督概念提取 (UCE) 的新任务，该任务考虑了没有任何人类概念知识的无监督设置。给定一张包含多个概念的图像，该任务的目标是仅依靠预训练扩散模型的现有知识来提取和重建单个概念。为此，我们提出了 ConceptExpress，它通过从两个方面释放预训练扩散模型的内在能力来解决 UCE。具体来说，概念定位方法通过利用扩散自注意力的空间对应关系来自动定位和分离显著概念；并且基于概念和概念标记之间的查找关联，概念优化过程学习表示每个单独概念的区分标记。最后，我们为 UCE 任务建立了一个定制的评估协议。大量实验表明，ConceptExpress 是 UCE 任务的一个很有前景的解决方案。我们的代码和数据可在以下网址获得：https://github.com/haoosz/ConceptExpress||
|**2024-07-09**|[Latent Space Imaging](http://arxiv.org/abs/2407.07052)|null|传统数字成像系统通常基于对规则网格像素进行暴力测量和处理。另一方面，人类视觉系统对从光感受器到视神经的数据进行了大量压缩，本质上是将图像信息编码成适合人脑处理的低带宽潜在空间表示。在这项工作中，我们建议采用类似的方法来开发人工智能视觉系统。潜在空间成像是一种新范式，它通过光学和软件的结合，将图像信息直接编码到生成模型的语义丰富的潜在空间中，从而大大减少了捕获过程中的带宽和内存需求。我们通过基于单像素相机的初始硬件原型展示了这一新原理。通过设计一种调制幅度以编码到生成模型的潜在空间中的方案，我们在成像过程中实现了 1:100 到 1:1,000 的压缩比，这体现了潜在空间成像在高效成像硬件方面的潜力，使其能够在未来应用于高速成像或硬件复杂度大大降低的特定任务相机。||
|**2024-07-09**|[Generative models of astrophysical fields with scattering transforms on the sphere](http://arxiv.org/abs/2407.07007)|**[link](https://github.com/astro-informatics/s2scat)**|散射变换是一种新类型的概括统计量，最近被开发用于研究高度非高斯过程，并已被证明在天文物理研究中非常有前景。 特别是，它们允许人们从有限的数据中构建复杂非线性场的生成模型，并且还被用作新的统计成分分离算法的基础。 在即将进行的宇宙学巡天观测的背景下，例如针对宇宙微波背景偏振的 LiteBIRD 或用于研究宇宙大尺度结构的 Rubin-LSST 和 Euclid，将这些工具扩展到球面数据是必要的。 我们在球面上开发了散射变换，并专注于构建几个天体物理场的最大熵生成模型。 我们从单个目标场构建均匀的天体物理和宇宙学场的生成模型，使用常见的统计数据（功率谱、像素概率密度函数和 Minkowski 泛函）将样本与目标场进行定量比较。 我们的采样场在统计和视觉上都与目标场非常吻合。 因此，这些生成模型为未来的天体物理学和宇宙学研究开辟了广泛的新应用； 特别是那些几乎没有模拟数据的领域。 我们将代码提供给社区，以便这项工作可以轻松复制和进一步开发。||
|**2024-07-09**|[RodinHD: High-Fidelity 3D Avatar Generation with Diffusion Models](http://arxiv.org/abs/2407.06938)|null|我们提出了 RodinHD，它可以从肖像图像生成高保真 3D 头像。现有方法无法捕捉到我们论文中解决的复杂细节，例如发型。我们首先发现了一个被忽视的问题，即在许多头像上顺序拟合三平面时出现的灾难性遗忘问题，这是由 MLP 解码器共享方案引起的。为了克服这个问题，我们提出了一种新颖的数据调度策略和权重整合正则化项，从而提高了解码器渲染更清晰细节的能力。此外，我们通过计算捕获丰富 2D 纹理线索的更细粒度的层次表示，并通过交叉注意力将它们注入到 3D 扩散模型的多个层中，从而优化了肖像图像的引导效果。当使用针对三平面优化的噪声调度对 46K 头像进行训练时，生成的模型可以生成比以前的方法细节明显更好的 3D 头像，并且可以泛化到野外肖像输入。||
|**2024-07-09**|[HumanRefiner: Benchmarking Abnormal Human Generation and Refining with Coarse-to-fine Pose-Reversible Guidance](http://arxiv.org/abs/2407.06937)|**[link](https://github.com/enderfga/humanrefiner)**|文本到图像扩散模型在条件图像生成方面取得了显著进展。然而，这些模型通常难以准确渲染具有人类特征的图像，导致四肢扭曲和其他异常。这个问题主要源于扩散模型对肢体质量的识别和评估不足。为了解决这个问题，我们引入了AbHuman，这是第一个专注于解剖异常的大规模合成人体基准。该基准包含 56K 张合成的人体图像，每张图像都标注了详细的边界框级别标签，识别了 18 个不同类别中的 147K 个人体异常。在此基础上，可以建立人体异常的识别，进而通过负面提示和引导等传统技术增强图像生成。为了进一步促进改进，我们提出了HumanRefiner，这是一种新颖的即插即用方法，用于在文本到图像生成中对人体异常进行从粗到细的细化。具体来说，HumanRefiner 利用自我诊断程序来检测和纠正与粗粒度异常人体姿势和细粒度异常级别相关的问题，促进姿势可逆的扩散生成。在 AbHuman 基准上的实验结果表明，HumanRefiner 显着减少了生成差异，与最先进的开源生成器 SDXL 相比，肢体质量提高了 2.9 倍，在人类评估中比 DALL-E 3 提高了 1.4 倍。我们的数据和代码可在 https://github.com/Enderfga/HumanRefiner 获取。||
|**2024-07-09**|[Maximum stress minimization via data-driven multifidelity topology design](http://arxiv.org/abs/2407.06746)|null|最大应力最小化问题是结构设计中最重要的课题之一。传统的基于梯度的拓扑优化方法需要通过松弛技术将原始问题转化为伪问题。由于其参数对优化有显著影响，因此，在不使用松弛技术的情况下，准确地解决最大应力最小化问题有望获得最佳性能。本文重点关注这一挑战，并研究与基于梯度的拓扑优化获得的解决方案相比，通过解决没有松弛技术的原始最大应力最小化问题，是否可以获得具有更多避免应力集中的设计。我们采用数据驱动的多保真度拓扑设计（MFTD），这是一种基于进化算法的无梯度拓扑优化。基本框架包括通过解决低保真度优化问题来生成候选解，通过高保真度正向分析评估这些解，并在没有灵敏度分析的情况下使用深度生成模型迭代更新它们。在本研究中，数据驱动的MFTD将通过使用p范数应力度量解决基于梯度的拓扑优化问题获得的优化设计纳入初始解，并基于具有贴体网格的高保真度分析来解决原始最大应力最小化问题。我们通过L型支架的基准测试证明了我们提出的方法的有效性。通过使用数据驱动的MFTD解决原始最大应力最小化问题，与初始解相比，在相同的最大应力值下，体积减少了高达22.6%。||
|**2024-07-09**|[Powerful and Flexible: Personalized Text-to-Image Generation via Reinforcement Learning](http://arxiv.org/abs/2407.06642)|**[link](https://github.com/wfanyue/dpg-t2i-personalization)**|个性化文本到图像模型允许用户为一个对象（由一组参考图像指定）生成不同风格的图像（由句子指定）。虽然基于扩散的生成模型已经取得了显著成果，但在扩散过程中，对象的视觉结构和细节经常发生意外的变化。一个主要原因是，这些基于扩散的方法在训练过程中通常采用简单的重建目标，这很难在生成图像和参考图像之间强制实现适当的结构一致性。为此，本文设计了一种新的强化学习框架，利用确定性策略梯度方法进行个性化文本到图像生成，该框架可以轻松地结合各种目标（可微分甚至不可微分）来监督扩散模型，从而提高生成图像的质量。在个性化文本到图像生成基准数据集上的实验结果表明，我们提出的方法在视觉保真度方面明显优于现有的最先进方法，同时保持了文本对齐。我们的代码可在以下网址获得：\url{https://github.com/wfanyue/DPG-T2I-Personalization}。||
|**2024-07-09**|[Ensembled Cold-Diffusion Restorations for Unsupervised Anomaly Detection](http://arxiv.org/abs/2407.06635)|**[link](https://github.com/snavalm/disyre)**|无监督异常检测 (UAD) 方法旨在通过将测试样本与从已知无异常的数据集中学习到的规范分布进行比较来识别异常。基于生成模型的方法通过生成无异常版本的测试图像来提供可解释性，但通常无法识别细微的异常。或者，使用特征建模或自监督方法（例如依赖于合成生成的异常的方法）不提供开箱即用的可解释性。在这项工作中，我们提出了一种结合了两种策略的优势的新方法：生成式冷扩散管道（即，使用不基于噪声的损坏的类似扩散的管道），该管道经过训练，目标是将合成损坏的图像恢复到正常、原始外观。为了支持我们的管道，我们引入了一种新的合成异常生成程序，称为 DAG，以及一种新的异常评分，它集成了以不同程度的异常为条件的恢复。我们的方法在三个不同的大脑 MRI 数据集中都超过了先前最先进的无监督异常检测水平。||
|**2024-07-09**|[Mobius: An High Efficient Spatial-Temporal Parallel Training Paradigm for Text-to-Video Generation Task](http://arxiv.org/abs/2407.06617)|**[link](https://github.com/youngfly/Mobius)**|受文本到图像（T2I）生成任务成功的启发，许多研究人员正致力于文本到视频（T2V）生成任务。大多数 T2V 框架通常继承自 T2I 模型，并添加额外的时序层训练以生成动态视频，这可以视为一项微调任务。然而，传统的 3D-Unet 是一种串行模式，时间层跟随空间层，根据其串行特征流，这将导致高 GPU 内存和训练时间消耗。我们认为，这种串行模式将随着大型扩散模型和海量数据集的出现而带来更高的训练成本，这不符合环保要求，也不利于 T2V 的发展。因此，我们提出了一种高效的时空并行训练范式，用于 T2V 任务，称为 Mobius。在我们提出的 3D-Unet 中，时间层和空间层是并行的，这优化了特征流和反向传播。Mobius 将节省 24% 的 GPU 内存和 12% 的训练时间，这可以极大地改善 T2V 微调任务，并为 AIGC 社区提供新的见解。我们将在未来发布我们的代码。||
|**2024-07-09**|[VQA-Diff: Exploiting VQA and Diffusion for Zero-Shot Image-to-3D Vehicle Asset Generation in Autonomous Driving](http://arxiv.org/abs/2407.06516)|null|从野外观察中生成 3D 车辆模型对于自动驾驶至关重要。现有的图像到 3D 方法不能很好地解决这个问题，因为它们仅仅从图像 RGB 信息中学习生成，而没有更深入地理解野外车辆（例如车型、制造商等）。这导致它们对具有遮挡或棘手视角的真实世界观察结果的零样本预测能力较差。为了解决这个问题，在这项工作中，我们提出了 VQA-Diff，这是一个利用野外车辆图像为自动驾驶创建逼真的 3D 车辆模型的新框架。VQA-Diff 利用视觉问答 (VQA) 模型中大型语言模型继承的真实世界知识进行鲁棒的零样本预测，并利用扩散模型中丰富的图像先验知识进行结构和外观生成。具体来说，我们利用多专家扩散模型策略生成结构信息，并采用主题驱动的结构控制生成机制对外观信息进行建模。因此，VQA-Diff 无需从现实世界中收集的大规模图像到 3D 车辆数据集中学习，仍然具有强大的零样本图像到新颖视图生成能力。我们在 Pascal 3D+、Waymo 和 Objaverse 等各种数据集上进行了实验，结果表明 VQA-Diff 在定性和定量上均优于现有的最先进方法。||
|**2024-07-05**|[Structural Constraint Integration in Generative Model for Discovery of Quantum Material Candidates](http://arxiv.org/abs/2407.04557)|null|已知的有机分子数以十亿计，但已发现的功能性无机材料却只占很小一部分，这对寻找新型量子材料的研究群体来说是一个特别突出的问题。基于机器学习的生成模型，尤其是扩散模型的最新进展，为生成新型稳定材料带来了巨大希望。然而，将几何模式融入材料生成仍然是一项挑战。在此，我们介绍了在生成模型中集成结构约束的方法 (SCIGEN)。我们的方法可以通过在每个扩散步骤之前，用扩散约束结构对去噪结构进行策略性掩蔽，从而将生成引导至约束输出，来修改任何经过训练的生成扩散模型。此外，我们从数学上证明了 SCIGEN 可以有效地从原始分布中执行条件采样，这对于生成稳定的约束材料至关重要。我们使用阿基米德格作为原型约束生成了 800 万种化合物，其中超过 10% 通过了多阶段稳定性预筛选。对 26,000 种存活化合物的 DFT（密度泛函理论）高通量计算表明，超过 50% 的化合物通过了 DFT 级别的结构优化。由于量子材料的性质与几何模式密切相关，我们的结果表明 SCIGEN 为生成量子材料候选材料提供了一个通用框架。||
|**2024-07-05**|[Unified continuous-time q-learning for mean-field game and mean-field control problems](http://arxiv.org/abs/2407.04521)|null|本文从代表性智能体的角度研究了平均场跳扩散模型中的连续时间q学习。为了克服无法直接观察到总体分布时的挑战，我们引入了解耦形式的集成q函数（解耦Iq函数），并建立了其与价值函数的鞅表征，这为平均场博弈（MFG）和平均场控制（MFC）问题提供了统一的策略评估规则。此外，根据求解MFG或MFC问题的任务，我们可以通过不同的方式利用解耦Iq函数来分别学习平均场均衡策略或平均场最优策略。因此，我们利用所有源于平均场交互的测试策略，设计了一种适用于MFG和MFC问题的统一q学习算法。对于跳扩散环境下的几个例子，包括LQ框架内外的例子，我们可以获得解耦Iq函数和价值函数的精确参数化，并从代表性智能体的角度说明我们的算法具有令人满意的性能。||
|**2024-07-05**|[Speed-accuracy trade-off for the diffusion models: Wisdom from nonequlibrium thermodynamics and optimal transport](http://arxiv.org/abs/2407.04495)|null|我们探讨了生成模型（称为扩散模型）与非平衡热力学中用于描述福克-普朗克方程的随机热力学之间的联系。基于随机热力学技术，我们推导出了扩散模型的速度-精度权衡，这是扩散模型中数据生成速度和精度之间的权衡关系。我们的结果表明，正向过程中的熵产生率会影响数据生成的误差。从随机热力学的角度来看，我们的结果为如何在扩散模型中最好地生成数据提供了定量见解。最佳学习方案由随机热力学中的保守力和最优传输理论中 2-Wasserstein 距离的空间测地线引入。我们用数值方法说明了具有不同噪声方案（如余弦方案、条件最优传输和最优传输）的扩散模型的速度-精度权衡的有效性。||
|**2024-07-05**|[PROUD: PaRetO-gUided Diffusion Model for Multi-objective Generation](http://arxiv.org/abs/2407.04493)|null|深度生成模型领域的最新进展集中于生成满足多个期望属性的样本。然而，普遍的方法是独立优化这些属性函数，从而忽略了它们之间的权衡。此外，属性优化通常没有被恰当地整合到生成模型中，导致生成质量（即生成样本的质量）的无必要妥协。为了解决这些问题，我们提出了一个约束优化问题。它寻求在确保生成样本位于多个属性目标的帕累托前沿的同时优化生成质量。这样的公式能够生成在相互冲突的属性函数上无法同时进一步改进的样本，并保持生成样本的良好质量。在此公式的基础上，我们引入了帕累托引导扩散模型 (PROUD)，其中去噪过程中的梯度被动态调整以提高生成质量，同时生成样本遵循帕累托最优性。在图像生成和蛋白质生成任务上的实验评估表明，与各种基线相比，我们的 PROUD 在逼近多个属性函数的帕累托最优性的同时，始终保持着卓越的生成质量。||
|**2024-07-05**|[VCD-Texture: Variance Alignment based 3D-2D Co-Denoising for Text-Guided Texturing](http://arxiv.org/abs/2407.04461)|null|最近关于三维形状纹理合成的研究极大地受益于快速发展的二维文本到图像的扩散模型，包括基于修复和基于优化的方案。然而，这些方法忽略了二维扩散模型和三维物体之间的模态差距，它们主要将三维物体渲染成二维图像并分别对每个图像进行纹理处理。在本文中，我们重新审视了纹理合成，并提出了一个基于方差对齐的三维-二维协同去噪框架，称为VCD-Texture，以解决这些问题。具体来说，我们首先在具有重新投影的三维注意力感受野的扩散自注意力模块中统一了二维和三维潜在特征学习。随后，将去噪后的多视图二维潜在特征聚合到三维空间，然后将其光栅化回二维空间，从而形成更一致的二维预测。然而，光栅化过程存在难以处理的方差偏差，我们提出的方差对齐从理论上解决了这个问题，实现了高保真纹理合成。此外，我们还提出了一种修复细化方法，以进一步改善存在冲突区域的细节。值得注意的是，目前还没有公开可用的基准来评估纹理合成，这阻碍了其发展。因此，我们构建了一个基于三个开源三维数据集的新评估集，并建议使用四个指标来全面验证纹理性能。综合实验表明，VCD-Texture相较于其他方法取得了优越的性能。||
|**2024-07-05**|[Benchmarking structure-based three-dimensional molecular generative models using GenBench3D: ligand conformation quality matters](http://arxiv.org/abs/2407.04424)|**[link](https://github.com/bbaillif/genbench3d)**|三维 (3D) 深度分子生成模型提供了基于 3D 依赖属性的目标导向生成的优势，例如结合腔内基于结构的设计的结合亲和力。 为评估 SMILES 或分子图生成器而创建的传统基准，例如 GuacaMol 或 MOSES，由于它们不评估生成的分子构象的质量，因此在评估 3D 生成器方面受到限制。 因此，我们在这项工作中开发了 GenBench3D，它实现了一个用于在结合腔内生成分子的新基准。 我们的主要贡献是 Validity3D 指标，它使用基于剑桥结构数据库中观察到的参考值的键长和价角的可能性来评估构象质量。 对 LiGAN、3D-SBDD、Pocket2Mol、TargetDiff、DiffSBDD 和 ResGen 模型进行了基准测试。 我们发现只有 0% 到 11% 的生成分子具有有效的构象。 对口袋中生成的分子进行局部弛豫，通过至少增加 40% 的 Validity3D，大大提高了所有模型的 Validity3D。 对于 LiGAN、3D-SBDD 或 TargetDiff，有效弛豫分子集显示的平均 Vina 分数（即更差）高于原始生成分子集，表明原始生成分子的结合亲和力可能被高估了。 使用其他评分函数（更重视配体应变）仅在使用有效的弛豫分子时才会产生改进的分数。 使用有效的弛豫分子，TargetDiff 和 Pocket2Mol 显示出比其他模型更好的中位 Vina、Glide 和 Gold PLP 分数。 我们已经在 GitHub 上公开发布了 GenBench3D 以供更广泛地使用：https://github.com/bbaillif/genbench3d||
|**2024-07-05**|[Improving Audio Generation with Visual Enhanced Caption](http://arxiv.org/abs/2407.04416)|null|生成模型在音频生成任务中取得了显著成果，但现有模型难以处理复杂和详细的提示，导致潜在的性能下降。我们假设这个问题源于训练数据的低质量和相对较小的数量。在这项工作中，我们的目标是创建一个带有丰富描述的大规模音频数据集，用于改进音频生成模型。我们开发了一个自动化管道，通过使用大型语言模型 (LLM) 将预测的视觉字幕、音频字幕和标签标签转换为全面的描述，从而为视听数据集生成详细的字幕。我们介绍了 Sound-VECaps，这是一个包含 166 万个高质量音频-字幕对的数据集，其中包含丰富的细节，包括音频事件顺序、发生地点和环境信息。我们证明，使用 Sound-VECaps 进行训练可以显著增强文本到音频生成模型理解和从复杂输入提示生成音频的能力，从而提高整体系统性能。此外，我们对 Sound-VECaps 在多个音频-语言任务中进行了消融研究，表明其在推进音频-文本表示学习方面的潜力。我们的数据集和模型可在网上获得。||
|**2024-07-05**|[Unsupervised Learning of Category-Level 3D Pose from Object-Centric Videos](http://arxiv.org/abs/2407.04384)|**[link](https://github.com/genintel/uns-obj-pose3d)**|类别级三维姿态估计是计算机视觉和机器人技术中的一个基本问题，例如对于具身代理或训练三维生成模型。然而，到目前为止，估计类别级物体姿态的方法要么需要大量的人工标注、CAD模型，要么需要来自RGB-D传感器的输入。相比之下，我们致力于解决仅从随意拍摄的以物体为中心的视频中学习估计类别级三维姿态的问题，无需人工监督。我们提出了一个两步流程：首先，我们引入了一种多视图对齐程序，该程序使用一种新颖且鲁棒的循环距离公式来确定视频之间的规范相机姿态，该公式使用重建的粗网格和DINOv2特征进行几何和外观匹配。其次，规范姿态和重建的网格使我们能够从单个图像中训练用于三维姿态估计的模型。具体来说，我们的模型通过预测二维图像中每个像素的模板网格中对应顶点的特征向量，来学习估计图像和原型三维模板之间的密集对应关系。我们证明，我们的方法在以物体为中心的视频的无监督对齐方面大大优于所有基线，并在实际应用中提供了可靠且鲁棒的预测。我们的代码和数据可在https://github.com/GenIntel/uns-obj-pose3d获取。||
|**2024-07-05**|[A Mapping Strategy for Interacting with Latent Audio Synthesis Using Artistic Materials](http://arxiv.org/abs/2407.04379)|null|本文提出了一种与生成式人工智能模型的潜在空间交互的映射策略。我们的方法涉及使用无监督特征学习对人类控制空间进行编码，并将其映射到音频合成模型的潜在空间。为了演示这种映射策略如何将高维传感器数据转化为深度生成模型的控制机制，我们提出了一个概念验证系统，该系统使用视觉草图来控制音频合成模型。我们借鉴 XAIxArts 中的新兴论述来讨论这种方法如何为艺术和创意环境中的 XAI 做出贡献，我们还讨论了它目前的局限性并提出了未来的研究方向。||
|**2024-07-05**|[MuseBarControl: Enhancing Fine-Grained Control in Symbolic Music Generation through Pre-Training and Counterfactual Loss](http://arxiv.org/abs/2407.04331)|null|自动生成符号音乐——根据特定人类需求量身定制的乐谱——对音乐家和爱好者来说非常有利。最近的研究表明，使用大型数据集和先进的Transformer架构取得了可喜的成果。然而，这些最先进的模型通常只对整首作品的节奏和风格等方面提供基本的控制，缺乏管理更精细细节的能力，例如在单个小节级别的控制。虽然微调预先训练好的符号音乐生成模型似乎是实现这种更精细控制的直接方法，但我们的研究表明这种方法存在挑战。该模型通常无法对新的、细粒度的小节级控制信号做出充分响应。为了解决这个问题，我们提出了两个创新的解决方案。首先，我们引入了一个预训练任务，旨在将控制信号直接与其相应的音乐符号链接起来，这有助于为后续的微调实现更有效的初始化。其次，我们实施了一种新的反事实损失，以促进生成的音乐与控制提示之间更好地保持一致。总的来说，这些技术显著增强了我们在小节级别控制音乐生成的能力，比传统方法提高了13.06%。我们的主观评价也证实，这种增强的控制并没有损害原始预训练生成模型的音乐质量。||
|**2024-07-03**|[DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents](http://arxiv.org/abs/2407.03300)|**[link](https://github.com/gcorso/disco-diffdock)**|扩散模型 (DM) 为生成式学习带来了革命性的变化。它们利用扩散过程将数据编码为简单的 Gaussian 分布。然而，将复杂且可能具有多模态的数据分布编码为单个连续 Gaussian 分布无疑是一个不必要的具有挑战性的学习问题。我们提出离散-连续潜在变量扩散模型 (DisCo-Diff)，通过引入互补的离散潜在变量来简化此任务。我们使用可学习的离散潜在变量增强 DM，并使用编码器进行推断，并对 DM 和编码器进行端到端训练。DisCo-Diff 不依赖于预先训练的网络，这使得该框架具有普遍适用性。离散潜在变量通过降低 DM 生成 ODE 的曲率，显著简化了学习 DM 复杂噪声到数据映射的过程。一个额外的自回归 Transformer 模型对离散潜在变量的分布进行建模，这是一个简单的步骤，因为 DisCo-Diff 只需要具有少量码本的少量离散变量。我们在玩具数据、多个图像合成任务以及分子对接上验证了 DisCo-Diff，发现引入离散潜在变量始终可以提高模型性能。例如，DisCo-Diff 在使用 ODE 采样器的类条件 ImageNet-64/128 数据集上实现了最先进的 FID 分数。||
|**2024-07-03**|[Improved Noise Schedule for Diffusion Training](http://arxiv.org/abs/2407.03297)|null|扩散模型已成为生成视觉信号的首选方法。然而，训练单个模型来预测不同级别的噪声提出了重大挑战，需要多次迭代并导致巨大的计算成本。为了加快收敛速度，人们引入了各种方法，例如损失加权策略设计和架构改进。在本研究中，我们提出了一种设计噪声调度的新方法，以增强扩散模型的训练。我们的主要见解是，对数信噪比（logSNR）的重要性采样（理论上等效于修改后的噪声调度）对于提高训练效率特别有利，特别是在增加 $\log \text{SNR}=0$ 附近的采样频率时。我们通过经验证明了我们的噪声调度优于标准余弦调度。此外，我们还重点介绍了我们的噪声调度设计在 ImageNet 基准测试中的优势，表明所设计的调度始终有利于不同的预测目标。||
|**2024-07-03**|[Spatio-Temporal Adaptive Diffusion Models for EEG Super-Resolution in Epilepsy Diagnosis](http://arxiv.org/abs/2407.03089)|null|脑电图 (EEG) 技术，特别是高密度脑电图 (HD EEG) 设备，广泛应用于神经科学等领域。HD EEG 设备通过在头皮上放置更多电极来提高 EEG 的空间分辨率，满足癫痫病灶定位等临床诊断应用的要求。然而，该技术面临着采集成本高、使用场景有限等挑战。本文提出了时空自适应扩散模型 (STADM)，率先利用扩散模型实现从低分辨率 (LR, 64 通道或更少) EEG 到高分辨率 (HR, 256 通道) EEG 的空间超分辨率 (SR) 重建。具体而言，设计了一种时空条件模块来提取 LR EEG 的时空特征，然后将其作为条件输入来指导扩散模型的反向去噪过程。此外，构建了一个多尺度 Transformer 去噪模块，利用多尺度卷积块和基于交叉注意力的扩散 Transformer 块进行条件引导，生成自适应于受试者的 SR EEG。实验结果表明，该方法有效提高了 LR EEG 的空间分辨率，并在数量上优于现有方法。此外，STADM 通过将合成的 SR EEG 应用于癫痫患者的分类和源定位任务，证明了其价值，表明其具有显著提高 LR EEG 空间分辨率的潜力。||
|**2024-07-03**|[Artificial Inductive Bias for Synthetic Tabular Data Generation in Data-Scarce Scenarios](http://arxiv.org/abs/2407.03080)|null|虽然使用深度生成模型 (DGM) 生成合成表格数据为数据稀缺和隐私问题提供了一种引人注目的解决方案，但其有效性依赖于大量的训练数据，而这些数据在现实应用中通常不可用。本文提出了一种新颖的方法，用于在有限的真实数据环境中使用 DGM 生成真实可靠的合成表格数据，从而解决了这一挑战。我们的方法提出了几种通过迁移学习和元学习技术在 DGM 中生成人工归纳偏差的方法。我们在该框架内探索并比较了四种不同的方法，证明了预训练和模型平均等迁移学习策略优于模型无关元学习和域随机搜索等元学习方法。我们使用两种最先进的 DGM（变分自动编码器和生成对抗网络）验证了我们的方法，表明我们的人工归纳偏差提高了合成数据的质量（通过 Jensen-Shannon 散度衡量），在使用我们提出的方法时，相对增益高达 50%。这种方法在各种 DGM 和机器学习任务中具有广泛的适用性，特别是在医疗保健和金融等数据稀缺通常是关键问题的领域。||
|**2024-07-03**|[Electromagnetic Property Sensing Based on Diffusion Model in ISAC System](http://arxiv.org/abs/2407.03075)|null|集成传感与通信 (ISAC) 为未来的无线系统开辟了许多颠覆性的机遇。在本文中，我们开发了一种新颖的 ISAC 方案，利用扩散模型来感知预定传感区域中目标的电磁 (EM) 特性。具体来说，我们首先利用从目标反射回来的通信和传感信号来估计传感信道。然后，我们采用扩散模型生成代表目标的点云，从而实现目标电磁特性分布的 3D 可视化。为了最小化真实点云和估计点云之间的平均 Chamfer 距离 (MCD)，我们在最大发射功率和每个用户设备 (UE) 的最小通信可达速率的约束下，进一步设计了通信和传感波束赋形矩阵。仿真结果证明了该方法在实现目标形状、相对介电常数和电导率的高质量重建方面的有效性。此外，该方法可以在传感区域的任何位置有效地感知目标的电磁特性。||
|**2024-07-03**|[Semantic-Aware Power Allocation for Generative Semantic Communications with Foundation Models](http://arxiv.org/abs/2407.03050)|null|扩散模型的最新进展为生成建模带来了重大突破。生成模型与语义通信 (SemCom) 的结合能够以超低速率实现高保真语义信息交换。本文提出了一种用于图像任务的新型生成式 SemCom 框架，其中预训练的基础模型分别充当语义编码器和解码器，用于语义特征提取和图像再生。文章对传输可靠性与再生图像的感知质量以及语义特征的语义值之间的数学关系进行了建模，这些关系是通过对 Kodak 数据集进行数值模拟获得的。我们还研究了语义感知功率分配问题，目标是在保证语义性能的同时最小化总功耗。为了解决这个问题，分别通过约束解耦和二分搜索提出了两种语义感知功率分配方法。数值结果表明，与传统方法相比，所提出的语义感知方法在总功耗方面表现出优越的性能。||
|**2024-07-03**|[SlerpFace: Face Template Protection via Spherical Linear Interpolation](http://arxiv.org/abs/2407.03043)|null|当代人脸识别系统使用从人脸图像中提取的特征模板来识别身份。为了增强隐私性，人脸模板保护技术被广泛用于隐藏存储在模板中的敏感身份和外观信息。本文识别了一种新兴的利用扩散模型的隐私攻击形式，它可以使先前的保护无效，称为反演攻击。这种攻击可以从模板中合成高质量、保留身份的人脸图像，从而暴露人的外貌。基于对扩散模型生成能力的研究，本文提出了一种防御措施来削弱这种攻击，即通过将模板旋转到类似噪声的分布。这是通过在其所在的超球面上对模板进行球面和线性插值（slerp）来有效实现的。为了增强旋转模板的不可逆性，本文进一步提出对模板的特征维度进行分组划分和丢弃。组的划分和每个组内的丢弃以有利于识别的的方式学习。所提出的技术被具体化为一种新的人脸模板保护技术，SlerpFace。大量实验表明，SlerpFace 提供了令人满意的识别精度和全面的隐私保护，可以抵御反演和其他攻击形式，优于现有技术。||
|**2024-07-03**|[An Organism Starts with a Single Pix-Cell: A Neural Cellular Diffusion for High-Resolution Image Synthesis](http://arxiv.org/abs/2407.03018)|null|生成模型旨在逼近真实数据的统计特性，从而能够合成与原始分布非常相似的新数据。生成对抗网络 (GAN) 和去噪扩散概率模型 (DDPM) 代表了生成模型的重大进步，它们分别从博弈论和热力学中汲取灵感。然而，从生物进化的角度探索生成模型在很大程度上仍未得到开发。在本文中，我们介绍了一种称为生成元胞自动机 (GeCA) 的新型模型系列，其灵感来自于生物体从单细胞的进化。GeCA 被评估为一种有效的视网膜疾病分类增强工具，可用于两种成像模式：眼底和光学相干断层扫描 (OCT)。在 OCT 成像中，数据稀缺且类别分布存在固有的偏差，GeCA 显着提高了 11 种不同眼科疾病的表现，与传统基线相比，平均 F1 分数提高了 12%。在类似的参数约束下，GeCA 的性能优于包含 UNet 或基于 Transformer 的最新去噪模型的扩散方法。代码可在以下网址获取：https://github.com/xmed-lab/GeCA。||
|**2024-07-03**|[Frequency-Controlled Diffusion Model for Versatile Text-Guided Image-to-Image Translation](http://arxiv.org/abs/2407.03006)|**[link](https://github.com/xianggao1102/fcdiffusion)**|近年来，大规模文本到图像 (T2I) 扩散模型已成为一种强大的图像到图像转换 (I2I) 工具，允许通过用户提供的文本提示进行开放域图像转换。本文提出了频率控制扩散模型 (FCDiffusion)，这是一个基于扩散的端到端框架，从频域角度为文本引导的 I2I 提供了一种新颖的解决方案。我们框架的核心是一个基于离散余弦变换的特征空间频域滤波模块，它在 DCT 域中滤波源图像的潜在特征，产生具有不同 DCT 谱带的滤波图像特征，作为预训练的潜在扩散模型的不同控制信号。我们发现，不同 DCT 谱带的控制信号在不同的相关性（例如，风格、结构、布局、轮廓等）上桥接了源图像和 T2I 生成的图像，从而使多功能 I2I 应用能够强调不同的 I2I 相关性，包括风格引导的内容创建、图像语义操作、图像场景转换和图像风格转换。与相关方法不同，FCDiffusion 建立了一个统一的文本引导 I2I 框架，只需在推理时切换不同的频率控制分支，即可适用于各种图像转换任务。广泛的定性和定量实验都证明了我们的方法在文本引导 I2I 方面的有效性和优越性。代码公开于：https://github.com/XiangGao1102/FCDiffusion。||
|**2024-07-03**|[Towards a Scalable Reference-Free Evaluation of Generative Models](http://arxiv.org/abs/2407.02961)|**[link](https://github.com/aziksh-ospanov/fkea)**|虽然生成模型的标准评估分数大多是基于参考的，但由于缺乏适用的参考数据集，对生成模型进行依赖参考的评估通常很困难。最近，人们提出了无参考的熵分数 VENDI 和 RKE 来评估生成数据的多样性。然而，从数据中估计这些分数会导致大规模生成模型的计算成本很高。在这项工作中，我们利用随机傅里叶特征框架来降低计算成本，并提出了基于傅里叶的核熵逼近 (FKEA) 方法。我们利用 FKEA 对核矩阵的近似特征谱来有效地估计上述熵分数。此外，我们展示了 FKEA 代理特征向量的应用，以揭示该方法在评估生成样本多样性时识别的模式。我们提供了 FKEA 评估算法的随机实现，其复杂度为 $O(n)$，随样本大小 $n$ 线性增长。我们广泛评估了 FKEA 在标准图像、文本和视频数据集中的数值性能。我们的实验结果表明，该方法应用于大规模生成模型具有可扩展性和可解释性。代码库可在 https://github.com/aziksh-ospanov/FKEA 获取。||

## LLM

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-07-23**|[TLCR: Token-Level Continuous Reward for Fine-grained Reinforcement Learning from Human Feedback](http://arxiv.org/abs/2407.16574)|null|人类反馈强化学习 (RLHF) 利用人类偏好数据来训练语言模型，使其更符合人类本质。然而，这些人类偏好数据是在序列级别进行标注的，这就在序列级别的偏好标签和由语言模型自回归生成的标记之间造成了不匹配。尽管最近有几种方法试图为每个标记提供标记级别（即密集）的奖励，但这些方法通常依赖于预定义的离散奖励值（例如，正：+1，负：-1，中性：0），而没有考虑到每个标记固有的不同偏好程度。为了解决这一局限性，我们引入了用于 RLHF 的 TLCR（标记级别连续奖励），它包含一个经过训练的鉴别器，用于区分正面和负面标记，并利用鉴别器的置信度为每个标记分配连续奖励，同时考虑上下文。大量实验表明，我们提出的 TLCR 在开放式生成基准测试中，与之前的序列级别或标记级别离散奖励相比，始终能带来性能提升。||
|**2024-07-22**|[Multilingual Fine-Grained News Headline Hallucination Detection](http://arxiv.org/abs/2407.15975)|null|随着预训练语言模型的进步，自动新闻标题生成越来越受欢迎。然而，这些模型经常面临“幻觉”问题，即生成的标题没有得到其来源文章的完全支持。解决这个问题的努力主要集中在英语上，使用过于简化的分类方案，忽略了细微的幻觉类型。在本研究中，我们介绍了第一个多语言、细粒度的新闻标题幻觉检测数据集，其中包含 5 种语言的 1.1 万多对标题，每对标题都由专家标注了详细的幻觉类型。我们在两种设置下对该数据集进行了广泛的实验。首先，我们实施了几种监督微调方法作为预备解决方案，并证明了该数据集的挑战和效用。其次，我们测试了各种大型语言模型的上下文学习能力，并提出了两种新技术：语言相关的演示选择和从粗到精的提示，以提高少样本幻觉检测在示例 F1 指标方面的性能。我们发布此数据集是为了促进多语言、细粒度标题幻觉检测的进一步研究。||
|**2024-07-20**|[Seal: Advancing Speech Language Models to be Few-Shot Learners](http://arxiv.org/abs/2407.14875)|null|现有的自回归语言模型已经展现出仅需少量样本提示即可执行新任务的非凡能力，而无需任何额外训练。为了将此能力扩展到多模态环境（即语音和语言），本文引入了 Seal 模型，它是语音语言模型的缩写。它采用了一种新颖的对齐方法，其中执行 Kullback-Leibler 散度损失来训练一个投影器，该投影器连接了一个冻结的语音编码器和一个冻结的语言模型解码器。由此产生的 Seal 模型在两个语音理解任务中作为少样本学习器表现出强大的性能。此外，还进行了一致性实验，以验证其在不同预训练语言模型上的稳健性。||
|**2024-07-18**|[LIMT: Language-Informed Multi-Task Visual World Models](http://arxiv.org/abs/2407.13466)|null|最近在机器人强化学习方面取得的大多数成功都涉及学习专门的单任务智能体。然而，能够执行多项任务的机器人在现实应用中可能更有价值。由于样本复杂性增加以及潜在的任务目标冲突，多任务强化学习可能非常具有挑战性。先前关于该主题的工作主要集中在无模型方法上。即使在学习专门的单任务智能体时，后者也可能非常缺乏样本效率。在这项工作中，我们专注于基于模型的多任务强化学习。我们提出了一种学习多任务视觉世界模型的方法，利用预训练的语言模型来提取语义上有意义的任务表示。世界模型和策略使用这些表示来推理动力学和行为中的任务相似性。我们的结果突出了将语言驱动的任务表示用于世界模型的好处，以及基于模型的多任务学习相对于更常见的无模型范式的明显优势。||
|**2024-07-18**|[AlcLaM: Arabic Dialectal Language Model](http://arxiv.org/abs/2407.13097)|**[link](https://github.com/amurtadha/alclam)**|预训练语言模型 (PLM) 是许多现代自然语言处理 (NLP) 系统中不可或缺的一部分。尽管多语言模型涵盖了多种语言，但它们经常面临推理成本高和缺乏多样化的非英语训练数据等挑战。阿拉伯语特定 PLM 主要使用现代标准阿拉伯语进行训练，这影响了它们在地区方言上的表现。为了解决这个问题，我们构建了一个阿拉伯方言语料库，其中包含从社交媒体平台收集的 340 万个句子。我们利用这个语料库来扩展词汇量并从头开始重新训练基于 BERT 的模型。我们的模型名为 AlcLaM，仅使用 13 GB 的文本进行训练，这与现有模型（如 CAMeL、MARBERT 和 ArBERT）所用数据相比仅占一小部分，分别为 7.8%、10.2% 和 21.3%。值得注意的是，尽管训练数据有限，但 AlcLaM 在各种阿拉伯语 NLP 任务中均表现出优异的性能。AlcLaM 可在 GitHub (https://github.com/amurtadha/Alclam) 和 HuggingFace (https://huggingface.co/rahbi) 上获取。||
|**2024-07-23**|[Subgraph-Aware Training of Text-based Methods for Knowledge Graph Completion](http://arxiv.org/abs/2407.12703)|null|近年来，对预训练语言模型（PLM）进行微调已显示出改进知识图谱补全（KGC）的潜力。然而，大多数基于PLM的方法仅编码文本信息，而忽略了知识图谱（KG）的各种拓扑结构。在本文中，我们通过经验证实了KG的结构特性与基于PLM的方法的性能之间存在显著关系。为了利用结构化知识，我们提出了一种用于KGC的子图感知训练框架（SATKGC），该框架结合了(i) 子图感知小批量处理以鼓励困难负采样，以及(ii) 一种新的对比学习方法，以便在结构特性的基础上更多地关注更困难的实体和更困难的负三元组。据我们所知，这是第一个将子图的结构归纳偏差全面纳入微调PLM的研究。在四个KGC基准测试上的大量实验结果证明了SATKGC的优越性。我们的代码已公开。||
|**2024-07-13**|[Minimizing PLM-Based Few-Shot Intent Detectors](http://arxiv.org/abs/2407.09943)|**[link](https://github.com/hdzhang-code/smallID)**|最近的研究表明，基于预训练语言模型 (PLM) 并使用有限的标记数据训练高效的意图检测器是可行的。然而，由于模型规模庞大，在移动设备等资源受限的环境中部署这些检测器面临着挑战。为了解决这个问题，我们探索了在少量样本数据训练的PLM意图检测器上最小化其规模的技术。具体来说，我们利用大型语言模型（LLM）进行数据增强，采用先进的模型压缩方法进行知识蒸馏，并设计了一种名为V-Prune的词汇剪枝机制。通过这些方法，我们在模型内存使用方面成功实现了21倍的压缩率，包括Transformer和词汇表，同时在四个真实世界的基准测试中保持了几乎相同的性能水平。||
|**2024-07-12**|[Deep Bag-of-Words Model: An Efficient and Interpretable Relevance Architecture for Chinese E-Commerce](http://arxiv.org/abs/2407.09395)|null|查询和产品的文本相关性或文本匹配是电子商务搜索引擎的一项基本技术，其目的是确保展示的产品能够匹配查询意图。许多研究致力于提高搜索系统中相关性模型的性能。近年来，像 BERT 这样的预训练语言模型在文本相关性任务上取得了可喜的成果。虽然这些模型在线下测试集上表现良好，但由于其高延迟，将预训练语言模型部署到在线系统仍然存在障碍。双塔模型因其能够兼顾性能和计算效率而被广泛应用于工业场景。遗憾的是，这类模型呈现出不透明的“黑盒”性质，这阻碍了开发人员进行专门的优化。在本文中，我们提出了一种高效且可解释的中文电商相关性架构——深度词袋（DeepBoW）模型。我们的方法建议将查询和产品编码为稀疏的词袋表示，它是一组词-权重对。权重表示对应词与原始文本之间的重要性或相关性得分。相关性得分通过查询和产品的稀疏词袋表示之间匹配词的累积来衡量。与通常存在黑盒缺点的流行的密集分布式表示相比，所提出的表示模型的最大优势是高度可解释和可干预，这对在线搜索引擎的部署和运营来说是一个优越的优势。此外，该模型的在线效率甚至优于最有效的密集表示内积形式...||
|**2024-07-12**|[One Stone, Four Birds: A Comprehensive Solution for QA System Using Supervised Contrastive Learning](http://arxiv.org/abs/2407.09011)|null|本文提出了一种新颖且全面的解决方案，通过监督对比学习 (SCL) 来增强问答 (QA) 系统的鲁棒性和效率。利用预训练语言模型，训练高性能问答系统已经变得非常简单，只需要少量数据和简单的微调。然而，尽管最近取得了进展，但现有的问答系统在功能性和训练效率方面仍然存在重大缺陷。我们通过定义四个关键任务来解决功能性问题：用户输入意图分类、域外输入检测、新意图发现和持续学习。然后，我们利用基于 SCL 的统一表示学习方法，有效地构建类内紧凑和类间分散的特征空间，促进已知意图分类和未知意图检测与发现。因此，只需对下游任务进行最少的额外调整，我们的方法就能显著提高模型效率，并在所有任务中实现新的最先进性能。||
|**2024-07-12**|[Domain-Hierarchy Adaptation via Chain of Iterative Reasoning for Few-shot Hierarchical Text Classification](http://arxiv.org/abs/2407.08959)|null|近年来，各种预训练语言模型（PLM）被提出，并在各种少样本任务上证明了其令人印象深刻的性能。然而，受限于PLM中非结构化先验知识，在复杂结构化场景（如层次文本分类（HTC））中难以保持一致的性能，尤其是在下游数据极其稀缺的情况下。主要的挑战是如何将PLM中非结构化的语义空间迁移到下游领域层次结构中。与之前直接执行多标签分类或使用图神经网络（GNN）注入标签层次结构的HTC工作不同，在这项工作中，我们研究了少样本设置下的HTC问题，以使PLM中的知识从非结构化的方式适应下游层次结构。从技术上讲，我们设计了一种简单有效的方法，称为层次迭代条件随机场（HierICRF），以搜索最具领域挑战性的方向，并将领域层次结构适应巧妙地构建为层次迭代语言建模问题，然后鼓励模型在推理过程中进行层次一致性自我修正，从而实现知识迁移并保持层次一致性。我们在各种架构上执行HierICRF，并在两个流行的HTC数据集上进行的大量实验表明，与之前最先进的（SOTA）基线相比，使用HierICRF的提示在少样本设置下显著提高了HTC性能，平均Micro-F1提高了28.80%到1.50%，Macro-F1提高了36.29%到1.5%，同时保持了SOTA的层次一致性性能。||
|**2024-07-11**|[Adversarial-MidiBERT: Symbolic Music Understanding Model Based on Unbias Pre-training and Mask Fine-tuning](http://arxiv.org/abs/2407.08306)|**[link](https://github.com/RS2002/Adversarial-MidiBERT)**|作为音乐信息检索 (MIR) 的重要组成部分，符号音乐理解 (SMU)  受到了广泛关注，因为它可以帮助音乐家和业余爱好者学习和创作音乐。近年来，预训练语言模型在 SMU 中得到广泛应用，因为符号音乐与自然语言具有高度相似性，并且预训练方式还有助于充分利用有限的音乐数据。然而，在预训练语言模型中观察到性别歧视、年龄歧视和种族主义等偏见问题，这归因于训练数据分布不均衡。它也对下游任务的性能产生重大影响，这在 SMU 中也同样存在。为了应对这一挑战，我们提出了 Adversarial-MidiBERT，这是一种基于 Transformer 的双向编码器表示 (BERT) 的符号音乐理解模型。我们引入了一种基于对抗学习的无偏见预训练方法，以最大程度地减少训练期间导致偏见的标记的参与。此外，我们提出了一种掩码微调方法来缩小预训练和微调之间的数据差距，这可以帮助模型更快地收敛并获得更好的性能。我们在四个音乐理解任务上评估了我们的方法，我们的方法在所有任务中都表现出色。我们模型的代码在 https://github.com/RS2002/Adversarial-MidiBERT 上公开可用。||
|**2024-07-10**|[Deconstructing What Makes a Good Optimizer for Language Models](http://arxiv.org/abs/2407.07972)|null|随着语言模型规模的扩大，训练成本越来越高，这促使人们不断尝试提高优化效率。尽管付出了这些努力，但 Adam 优化器仍然是使用最广泛的，因为它被普遍认为是最有效的方法。我们的目标是在自回归语言建模的背景下，比较几种优化算法，包括 SGD、Adafactor、Adam 和 Lion，涵盖各种模型大小、超参数和架构变体。我们的研究结果表明，除了 SGD 之外，这些算法在最佳性能和面对各种超参数选择时的表现都相当。我们的结果向实践者表明，优化器的选择可以根据实际情况来决定，例如内存限制和实现的难易程度，因为没有一种算法在性能或对超参数错误指定的稳定性方面成为明显的赢家。鉴于我们的发现，我们进一步剖析了这些方法，研究了 Adam 的两个简化版本：a）符号动量（Signum），我们发现它恢复了 Adam 的性能和超参数稳定性；b）Adalayer，我们引入的 Adam 的分层变体，用于研究 Adam 的预处理。对 Adalayer 的研究使我们得出结论，Adam 预处理的最大影响仅限于最后一层和 LayerNorm 参数，而且，也许令人惊讶的是，其余层可以使用 SGD 进行训练。||
|**2024-07-09**|[NoisyAG-News: A Benchmark for Addressing Instance-Dependent Noise in Text Classification](http://arxiv.org/abs/2407.06579)|null|现有的噪声标签学习研究主要集中于合成标签噪声。尽管合成噪声具有明确定义的结构特性，但它往往无法准确地复制现实世界中的噪声模式。近年来，人们致力于构建用于图像分类的通用且可控的实例依赖噪声数据集，这显著推动了该领域抗噪声学习的发展。然而，针对文本分类的噪声标签学习研究仍然很少。为了更好地理解现实世界文本分类环境中的标签噪声，我们通过人工标注构建了基准数据集 NoisyAG-News。首先，我们分析了标注数据，以收集关于现实世界噪声的观察结果。我们定性和定量地证明了现实世界的噪声标签遵循实例依赖模式。随后，我们使用预训练语言模型和噪声处理技术，对 NoisyAG-News 及其相应的合成噪声数据集进行了全面的学习实验。我们的研究结果表明，虽然预训练模型对合成噪声具有鲁棒性，但它们在面对实例依赖噪声时表现不佳，不同混淆程度的样本在训练和测试过程中表现出不一致的性能。这些现实世界的噪声模式带来了新的、重大的挑战，促使人们重新评估噪声标签处理方法。我们希望 NoisyAG-News 将促进未来噪声标签学习解决方案的开发和评估。||
|**2024-07-09**|[Enhancing Low-Resource NMT with a Multilingual Encoder and Knowledge Distillation: A Case Study](http://arxiv.org/abs/2407.06538)|null|神经机器翻译（NMT）仍然是一项艰巨的挑战，尤其是在处理低资源语言时。预训练的序列到序列（seq2seq）多语言模型，例如 mBART-50，已在各种低资源 NMT 任务中表现出令人印象深刻的性能。然而，它们的预训练仅限于 50 种语言，不支持许多低资源语言，尤其是印度次大陆的语言。扩展 mBART-50 的语言支持需要复杂的预训练，由于灾难性遗忘，可能会导致性能下降。考虑到这些不断扩大的挑战，本文探索了一个框架，该框架利用预训练语言模型的优势以及 seq2seq 架构中的知识蒸馏来促进低资源语言的翻译，包括 mBART-50 未涵盖的语言。所提出的框架采用基于多语言编码器的 seq2seq 模型作为基础架构，并随后使用互补的知识蒸馏技术来减轻不平衡训练的影响。我们的框架在四个印度语到印度语方向上的三种低资源印度语上进行了评估，与基线相比，BLEU-4 和 chrF 得到了显着提高。此外，我们进行人工评估以确认我们方法的有效性。我们的代码可在 https://github.com/raypretam/Two-step-low-res-NMT 公开获取。||
|**2024-07-07**|[Advancing Prompt Recovery in NLP: A Deep Dive into the Integration of Gemma-2b-it and Phi2 Models](http://arxiv.org/abs/2407.05233)|null|Prompt recovery, a crucial task in natural language processing, entails the reconstruction of prompts or instructions that language models use to convert input text into a specific output. Although pivotal, the design and effectiveness of prompts represent a challenging and relatively untapped field within NLP research. This paper delves into an exhaustive investigation of prompt recovery methodologies, employing a spectrum of pre-trained language models and strategies. Our study is a comparative analysis aimed at gauging the efficacy of various models on a benchmark dataset, with the goal of pinpointing the most proficient approach for prompt recovery. Through meticulous experimentation and detailed analysis, we elucidate the outstanding performance of the Gemma-2b-it + Phi2 model + Pretrain. This model surpasses its counterparts, showcasing its exceptional capability in accurately reconstructing prompts for text transformation tasks. Our findings offer a significant contribution to the existing knowledge on prompt recovery, shedding light on the intricacies of prompt design and offering insightful perspectives for future innovations in text rewriting and the broader field of natural language processing.||
|**2024-07-06**|[The Solution for the AIGC Inference Performance Optimization Competition](http://arxiv.org/abs/2407.04991)|null|近年来，基于Transformer架构的大规模预训练语言模型的快速发展为自然语言处理任务带来了革命性的变化。其中，ChatGPT因其具备人类水平的对话能力而广受欢迎，到2022年底已吸引超过1亿月活跃用户。与此同时，百度对其文心模型的商业化部署也通过人工智能驱动技术显著提升了营销效果。本文重点研究如何优化文心模型的高性能推理，着重于GPU加速和利用Paddle推理框架。我们采用了一系列技术，例如Faster Transformer以实现高效的模型处理，嵌入层剪枝以减少计算开销，以及FP16半精度推理以提高计算效率。此外，我们的方案还集成了高效的数据处理策略，采用多进程并行处理以最大程度地减少延迟。实验结果表明，与标准方法相比，我们优化的解决方案在推理速度上实现了高达8.96倍的提升，同时保持了具有竞争力的性能。||
|**2024-07-03**|[MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language Models](http://arxiv.org/abs/2407.02775)|null|知识蒸馏是一种有效的预训练语言模型压缩技术。虽然现有的知识蒸馏方法对于最典型的模型BERT表现良好，但它们在两个方面仍有提升空间：可以进一步探索关系级知识以提高模型性能；学生注意力头的设置可以更加灵活，以减少推理时间。因此，我们提出了一种新的知识蒸馏方法MLKD-BERT，用于在师生框架中提取多级知识。在GLUE基准测试和抽取式问答任务上的大量实验表明，我们的方法优于BERT上最先进的知识蒸馏方法。此外，MLKD-BERT可以灵活设置学生注意力头的数量，从而在性能下降很小的情况下大幅减少推理时间。||
|**2024-07-03**|[Supporting Cross-language Cross-project Bug Localization Using Pre-trained Language Models](http://arxiv.org/abs/2407.02732)|null|自动定位大型代码库中的错误对于开发人员来说仍然是一项重大挑战。现有技术由于依赖于应用程序特定的数据和庞大的模型规模，因此在通用性和部署方面常常遇到困难。本文提出了一种新颖的基于预训练语言模型 (PLM) 的错误定位技术，该技术超越了项目和语言的界限。我们的方法利用对比学习来增强错误报告和源代码的表示。然后，它利用一种结合了提交消息和代码段的新型排序方法。此外，我们引入了一种知识蒸馏技术，可以在不影响性能的情况下减小模型规模，以便实际部署。本文提出了几个关键优势。通过将代码段和提交消息分析与传统的代码文件级别检查相结合，我们的技术实现了更高的错误定位精度。此外，我们的模型在通用性方面表现出色——在来自各种项目和语言的代码上进行训练后，它可以有效地识别未见过的代码库中的错误。为了解决计算限制，我们提出了一种兼容 CPU 的解决方案。总而言之，我们提出的工作提出了一种高效、通用且高效的错误定位技术，具有现实部署的潜力。||
|**2024-07-02**|[Ensemble of pre-trained language models and data augmentation for hate speech detection from Arabic tweets](http://arxiv.org/abs/2407.02448)|null|如今，从阿拉伯语推文中识别仇恨言论引起了众多研究者的关注。为了解决这一分类任务，人们已经开发了许多系统和技术。然而，在这方面面临的两大挑战是有限的性能和数据不平衡问题。在本研究中，我们提出了一种新方法，利用集成学习和基于先前手动标记的半监督学习。我们通过将阿拉伯语推文分为5个不同的类别：非仇恨、一般仇恨、种族歧视、宗教歧视或性别歧视，在一个基准数据集上进行了实验。实验结果表明：(1) 基于预训练语言模型的集成学习优于现有的相关工作；(2) 我们提出的数据增强方法提高了阿拉伯语推文中仇恨言论检测的准确率，并优于现有的相关工作。我们的主要贡献是在阿拉伯语仇恨言论检测方面取得了令人鼓舞的结果。||
|**2024-07-02**|[Efficient Nearest Neighbor based Uncertainty Estimation for Natural Language Processing Tasks](http://arxiv.org/abs/2407.02138)|null|深度神经网络 (DNN) 包括预训练语言模型 (PLM) 中的可信预测对于现实世界中安全关键型应用至关重要。然而，DNN 经常面临不确定性估计问题，例如校准错误。特别地，需要多次随机推理的方法可以缓解这个问题，但推理成本高昂，使其不切实际。在本研究中，我们提出了 $k$近邻不确定性估计（$k$NN-UE），这是一种利用来自邻居的距离和邻居的标签存在率进行不确定性估计的方法。在情感分析、自然语言推理和命名实体识别方面的实验表明，我们提出的方法在置信度校准、选择性预测和分布外检测方面优于基线或最近基于密度的方法。此外，我们的分析表明，引入降维或受最近$k$ NN-LM 研究启发的近似最近邻搜索，可以在不显著降低估计性能的情况下，通过适当组合来减少推理开销。||
|**2024-07-01**|[Bridging the Gap: Transfer Learning from English PLMs to Malaysian English](http://arxiv.org/abs/2407.01374)|null|马来西亚英语是一种低资源的混合语，除了标准英语之外，它还包含马来语、汉语和泰米尔语的元素。命名实体识别 (NER) 模型在从马来西亚英语文本中捕获实体时表现不佳，因为它具有独特的形态句法适应、语义特征和语码转换（混合英语和马来语）。考虑到这些差距，我们引入了 MENmBERT 和 MENBERT，这是一种具有上下文理解能力的预训练语言模型，专为马来西亚英语量身定制。我们使用来自马来西亚英语新闻文章 (MEN) 数据集的手动注释实体和关系微调了 MENmBERT 和 MENBERT。这种微调过程使 PLM 能够学习表示，这些表示捕获与 NER 和 RE 任务相关的马来西亚英语的细微差别。与 bert-base-multilingual-cased 模型相比，MENmBERT 在 NER 和 RE 任务上分别实现了 1.52% 和 26.27% 的改进。尽管 NER 的整体性能没有显着提高，但我们进一步的分析表明，按 12 个实体标签进行评估时，性能有显着提高。这些发现表明，在特定语言和地理位置的语料库上预训练语言模型可能是提高低资源环境中 NER 性能的一种很有前景的方法。本文发布的数据集和代码为专注于马来西亚英语的 NLP 研究工作提供了宝贵的资源。||
|**2024-07-01**|[Language Portability Strategies for Open-domain Dialogue with Pre-trained Language Models from High to Low Resource Languages](http://arxiv.org/abs/2407.01315)|null|本文研究用于开放域对话系统的大型预训练语言模型 (PLM) 在高资源语言中的语言可移植性策略。具体来说，目标低资源语言 (L_T) 将使用法语进行模拟，因为它缺乏特定于任务的资源，并且允许我们进行人工评估，而源语言 (L_S) 为英语。出于显而易见的原因，最近使用此类模型进行开放域对话的工作大多是用英语开发的。然而，为每种可能的目标语言构建特定的 PLM 需要收集新的数据集，而且成本高昂。出于这个原因，我们希望尝试利用 L_S 和 L_T 中的所有现有资源（PLM 和数据），评估使用不同方法在 L_T 中可实现的性能。前两种方法评估了神经机器翻译 (NMT) 在不同级别的使用：TrainOnTarget，其中在 L_T 中微调之前翻译 L_S 数据集，以及 TestOnSource，其中 L_S 模型在推理过程中与 NMT 模块耦合。然后，全球第一个开放获取的多语言大型 PLM BLOOM [2] 的出现，使研究人员能够开发新的方法，旨在不仅利用模型的完全可访问性，还利用其多语言性和翻译能力。在这种情况下，首先在 L_S 中学习任务，然后使用 MAD-X 适配器架构 [16] 适应 L_T。在这两组实验中，模型在口语对话条件下与人类进行评估，并且可以根据感知的交互质量比较策略。||
|**2024-07-01**|[A Fingerprint for Large Language Models](http://arxiv.org/abs/2407.01235)|null|近期研究表明，扩展预训练语言模型可以在许多下游任务上实现最先进的性能，这使得大型语言模型（LLM）成为人工智能领域的热门研究课题。然而，由于从头开始训练LLM需要大量的资源，因此保护LLM的知识产权免遭侵权至关重要且紧迫。这促使本文作者提出了一种新颖的LLM黑盒指纹识别技术，该技术既不需要模型训练也不需要模型微调。我们首先证明LLM的输出跨越与每个模型相关的唯一向量空间。我们将所有权认证问题建模为评估受害模型空间与嫌疑模型输出空间之间相似性的任务。为了解决这个问题，我们提出了两种解决方案，其中第一个解决方案涉及验证可疑大型模型的输出是否与受害模型的输出位于相同的空间中，从而能够快速识别模型侵权；第二个解决方案重建LLM输出和受害模型的向量空间的并集，以解决受害模型遭受参数高效微调（PEFT）攻击的情况。实验结果表明，所提出的技术在所有权验证和抵御PEFT攻击方面取得了优异的性能。这项工作揭示了LLM的固有特性，并为黑盒场景下的LLM所有权验证提供了一种有前景的解决方案，确保了效率、通用性和实用性。||
|**2024-07-01**|[Development of Cognitive Intelligence in Pre-trained Language Models](http://arxiv.org/abs/2407.01047)|null|最近的研究表明，大型预训练语言模型 (PLM) 出现了认知能力。这些模型不断提高的认知一致性使其成为认知科学理论的候选者。先前对 PLM 涌现认知能力的研究很大程度上与模型训练路径无关，即侧重于最终的模型权重而不是中间步骤。然而，使用 PLM 构建合理的人类认知模型将受益于考虑其在训练期间的表现与儿童思维轨迹的发展一致性。在人类智力心理测量测试的指导下，我们选择了四组任务来研究十个流行的 PLM 家族的一致性，并评估它们可用的中间和最终训练步骤。这些任务是数字能力、语言能力、概念理解和流体推理。我们发现了一个惊人的规律：无论模型大小如何，PLM 的发展轨迹始终表现出一个与人类认知发展最大程度一致的窗口。在该窗口之前，训练似乎赋予“空白石板”模型以必要的结构，使其能够从经验中快速学习。在该窗口之后，训练似乎服务于降低损失的工程目标，而不是提高与人类认知一致性的科学目标。||
|**2024-07-01**|[Cross-Modal Attention Alignment Network with Auxiliary Text Description for zero-shot sketch-based image retrieval](http://arxiv.org/abs/2407.00979)|null|本文研究了基于零样本草图的图像检索问题 (ZS-SBIR)。先前的方法在只有类别标签甚至没有文本信息的双模态设置中解决该问题。然而，大规模预训练语言模型 (LLM) 的日益普及，展现出从网络规模数据中学习到的丰富知识，为我们提供了一个总结集体文本信息的机会。我们的主要创新在于使用文本数据作为图像的辅助信息，从而利用语言提供的固有的零样本泛化能力。为此，我们提出了一种名为“基于辅助文本描述的跨模态注意力对齐网络”的方法，用于零样本草图图像检索。该网络由三个部分组成：(i) 描述生成模块，通过使用几个疑问句提示LLM，为每个训练类别生成文本描述；(ii) 特征提取模块，包括用于草图和图像数据的两个ViT、用于提取每个训练类别句子标记的转换器；最后 (iii) 跨模态对齐模块，使用交叉注意力机制交换文本-草图和文本-图像的标记特征，并在局部和全局范围内对齐标记。在三个基准数据集上的大量实验表明，我们的方法优于最先进的 ZS-SBIR 方法。||
|**2024-06-30**|[NAIST Simultaneous Speech Translation System for IWSLT 2024](http://arxiv.org/abs/2407.00826)|null|本文描述了NAIST提交给IWSLT 2024评测活动同步赛道的系统：英语到{德语、日语、汉语}的语音到文本翻译和英语到日语的语音到语音翻译。我们开发了一种多语言端到端语音到文本翻译模型，该模型结合了两个预训练语言模型HuBERT和mBART。我们使用两种解码策略训练该模型：局部一致性（LA）和AlignAtt。提交的模型采用LA策略，因为它在之前的模型中优于AlignAtt策略。我们的语音到语音翻译方法是上述语音到文本模型与增量文本到语音（TTS）模块的级联，该模块包含音素估计模型、并行声学模型和并行WaveGAN声码器。我们通过将采用AlignAtt策略的Transformer架构应用于估计模型来改进增量TTS。结果表明，我们升级后的TTS模块有助于提高系统性能。||

## Transformer

| Publish Date | Title | Code | Abstract |
|:---------|:-----------------------|:------|:-------------------------------------------------|
|**2024-07-25**|[CSWin-UNet: Transformer UNet with Cross-Shaped Windows for Medical Image Segmentation](http://arxiv.org/abs/2407.18070)|null|深度学习，尤其是卷积神经网络（CNN）和 Transformer 架构，已成为医学图像分割领域广泛研究的焦点，并取得了令人瞩目的成果。然而，CNN 具有归纳偏差，这限制了它们在更复杂、更多样化的分割场景中的有效性。相反，虽然基于 Transformer 的方法擅长捕获全局和远程语义细节，但它们的计算量很大。在这项研究中，我们提出了 CSWin-UNet，一种新颖的 U 形分割方法，将 CSWin 自注意力机制融入 UNet 中，以促进水平和垂直条带自注意力。这种方法显著增强了计算效率和感受野交互。此外，我们创新的解码器利用内容感知的重组算子，在预测核的指导下策略性地重组特征，以实现精确的图像分辨率恢复。我们对包括突触多器官 CT、心脏 MRI 和皮肤病变在内的不同数据集进行了广泛的实证评估，结果表明，CSWin-UNet 在保持较低模型复杂度的同时，实现了较高的分割精度。||
|**2024-07-25**|[How Lightweight Can A Vision Transformer Be](http://arxiv.org/abs/2407.17783)|null|本文探讨了一种利用混合专家模型 (MoE) 来简化而非增强视觉Transformer的策略。MoE层中的每个专家都是一个SwiGLU前馈网络，其中V和W2在层间共享。该模型不采用任何复杂的注意力或卷积机制。模型采用深度缩放来逐步减小隐藏层的大小，并分阶段增加专家的数量。模型还使用了分组查询注意力机制。我们研究了在小型数据集上进行预训练和不进行预训练的情况下该方法的表现，并探讨了迁移学习在这种规模下是否有效。我们发现，即使在参数量仅为67万的情况下，该架构仍然具有竞争力。||
|**2024-07-25**|[Transformers on Markov Data: Constant Depth Suffices](http://arxiv.org/abs/2407.17686)|null|基于注意力的 Transformer 在模拟跨领域和模态的生成过程方面取得了显著成功。在本文中，我们研究了 Transformer 对来自 k 阶马尔可夫过程的数据的行为，其中序列中下一个符号的条件分布取决于观察到的前 k 个符号。我们凭经验观察到一个令人惊讶的现象，这与之前的发现相矛盾：当训练时间足够长时，即使 k 增长，具有固定深度和每层 1 个头的 Transformer 也能够在从 k 阶马尔可夫源绘制的序列上实现较低的测试损失。此外，这种低测试损失是通过 Transformer 表示和学习上下文相关的条件经验分布的能力来实现的。在理论方面，我们的主要结果是，具有单个头和三层的 Transformer 可以表示 k 阶马尔可夫源的上下文相关的条件经验分布，这与我们的经验观察结果一致。在此过程中，我们证明了具有  O(log2(k)) 层的“仅注意力”Transformer 可以通过组合归纳头来表示上下文相关的条件经验分布，以跟踪序列中之前的 k 个符号。这些结果通过理解 Transformer 在马尔可夫源上的行为，为了解 Transformer 如何学习捕获上下文提供了更多见解。||
|**2024-07-23**|[S-E Pipeline: A Vision Transformer (ViT) based Resilient Classification Pipeline for Medical Imaging Against Adversarial Attacks](http://arxiv.org/abs/2407.17587)|null|视觉Transformer（ViT）凭借其强大的自注意力机制，在医学影像自动精准疾病诊断领域正变得越来越流行。然而，ViT仍然容易受到对抗性攻击，这些攻击可能通过导致对关键疾病的故意错误分类来阻碍诊断过程。在本文中，我们提出了一种新颖的图像分类流水线，即S-E流水线，它执行多个预处理步骤，允许ViT在关键特征上进行训练，从而减少对抗性扰动的影响。我们的方法结合了分割和图像增强技术，例如对比度受限的自适应直方图均衡化（CLAHE）、非锐化掩蔽（UM）和高频增强滤波（HFE）作为预处理步骤，以识别即使在对抗性扰动后仍保持完整的关键特征。实验研究表明，我们新颖的流水线有助于降低对抗性攻击的影响，对于ViT-b32模型降低了72.22%，对于ViT-l32模型降低了86.58%。此外，我们展示了在NVIDIA Jetson Orin Nano板上的端到端部署我们提出的方法，以证明其在通常资源受限的现代手持设备中的实际应用案例。||
|**2024-07-24**|[Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models](http://arxiv.org/abs/2407.17406)|**[link](https://github.com/zhaoyd1/dep_transformer_grammars)**|句法Transformer语言模型旨在通过同时建模语法树和句子来实现更好的泛化能力。虽然先前的工作一直专注于将基于成分的结构添加到Transformer中，但我们引入了依赖Transformer语法（DTG），这是一类新的具有显式依赖关系归纳偏差的Transformer语言模型。DTG通过修改注意力掩码来模拟具有受限注意力模式的依赖转移系统，通过相对位置编码合并堆栈信息，并通过标记嵌入和操作嵌入的组合来增强依赖弧表示。在标注了依赖树的句子数据集上训练时，DTG在保持与Transformer语言模型基线相当的困惑度的同时，实现了更好的泛化能力。DTG也优于最近基于成分的模型，表明依赖关系可以更好地指导Transformer语言模型。我们的代码发布在https://github.com/zhaoyd1/Dep_Transformer_Grammars。||
|**2024-07-24**|[Embedding-Free Transformer with Inference Spatial Reduction for Efficient Semantic Segmentation](http://arxiv.org/abs/2407.17261)|**[link](https://github.com/hyunwoo137/edaformer)**|我们提出了编码器-解码器注意力Transformer模型EDAFormer，它由无嵌入Transformer (EFT) 编码器和利用我们提出的无嵌入注意力 (EFA) 结构的全注意力解码器组成。我们提出的EFA是一种新颖的全局上下文建模机制，它侧重于发挥全局非线性的作用，而不是查询、键和值的具体作用。对于解码器，我们探索了考虑全局性的优化结构，可以提高语义分割性能。此外，我们提出了一种新的推理空间压缩 (ISR) 方法来提高计算效率。与以往的空间压缩注意力方法不同，我们的ISR方法在推理阶段进一步降低了键值分辨率，可以减轻高效语义分割的计算性能权衡差距。与现有的基于Transformer的语义分割模型相比，我们的EDAFormer在ADE20K、Cityscapes和COCO-Stuff三个公共基准测试中展现了最先进的性能和高效的计算能力。此外，我们的ISR方法在Cityscapes数据集上将计算成本降低了高达61%，而mIoU性能损失最小。代码可在https://github.com/hyunwoo137/EDAFormer获取。||
|**2024-07-24**|[Improving ICD coding using Chapter based Named Entities and Attentional Models](http://arxiv.org/abs/2407.17230)|null|自然语言处理 (NLP) 近期的进步已促成其在各个领域的自动化应用。然而，临床 NLP 通常依赖于基准数据集，这些数据集可能无法准确反映现实场景。自动 ICD 编码是一项至关重要的 NLP 任务，通常使用过时且不平衡的数据集，如 MIMIC-III，由于存在许多误报，现有方法的微平均 F1 分数在 0.4 到 0.7 之间。我们的研究引入了一种增强的 ICD 编码方法，通过使用基于章节的命名实体和注意力模型来提高 F1 分数。该方法将出院小结分类到 ICD-9 章节中，并使用特定于章节的数据开发注意力模型，从而无需考虑外部数据来进行代码识别。对于分类，我们使用第四章来消除偏差并影响关键实体和权重，无需神经网络，从而创建准确的阈值并提供可解释性以供人工验证。验证后，我们使用具有注意力的双向门控循环单元 (GRU) 和具有多头注意力机制的 Transformer 架构，为第四章中的三个常见代码和三个不常见代码开发了注意力模型。这些模型的平均微观 F1 分数为 0.79 和 0.81，表明 ICD 编码的性能得到显著提高。||
|**2024-07-24**|[RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer](http://arxiv.org/abs/2407.17140)|**[link](https://github.com/lyuwenyu/RT-DETR)**|在本报告中，我们介绍了RT-DETRv2，一种改进的实时检测Transformer（RT-DETR）。RT-DETRv2建立在之前的最先进实时检测器RT-DETR的基础上，并开放了一系列免费增强功能以提高灵活性和实用性，并优化了训练策略以提高性能。为了提高灵活性，我们建议在可变形注意力中为不同尺度的特征设置不同数量的采样点，以实现解码器对多尺度特征的选择性提取。为了增强实用性，我们提出了一个可选的离散采样算子来替换grid_sample算子，与YOLO相比，该算子是RT-DETR特有的。这消除了通常与DETR相关的部署限制。对于训练策略，我们提出了动态数据增强和尺度自适应超参数定制，以在不损失速度的情况下提高性能。源代码和预训练模型将在https://github.com/lyuwenyu/RT-DETR上提供。||
|**2024-07-24**|[LoFormer: Local Frequency Transformer for Image Deblurring](http://arxiv.org/abs/2407.16993)|**[link](https://github.com/deepmed-lab-ecnu/single-image-deblur)**|由于自注意力机制 (SA) 计算复杂度高，目前图像去模糊方法往往采用局部SA或粗粒度的全局SA方法，但这些方法存在着牺牲全局建模能力或缺乏细粒度相关性的缺陷。为了解决这一问题，在不牺牲细粒度细节的情况下有效地建模长距离依赖关系，我们提出了一种名为局部频率Transformer (LoFormer) 的新方法。在LoFormer的每个单元内，我们引入了频域局部通道SA (Freq-LC)，用于同时捕捉低频和高频局部窗口内的互协方差。这些操作的优势在于：(1) 确保粗粒度结构和细粒度细节的学习机会均等，以及 (2) 与粗粒度全局SA方法相比，探索更广泛的表示属性。此外，我们还引入了一个与Freq-LC互补的MLP门控机制，用于过滤无关特征，同时增强全局学习能力。我们的实验表明，LoFormer显著提高了图像去模糊任务的性能，在GoPro数据集上实现了34.09 dB的峰值信噪比，FLOPs为126G。https://github.com/DeepMed-Lab-ECNU/Single-Image-Deblur||
|**2024-07-23**|[TAPTRv2: Attention-based Position Update Improves Tracking Any Point](http://arxiv.org/abs/2407.16291)|null|本文提出了TAPTRv2，一种基于Transformer的方法，建立在TAPTR的基础上，用于解决任意点跟踪（TAP）任务。TAPTR借鉴了检测Transformer（DETR）的设计，将每个跟踪点表示为一个点查询，从而可以利用类似DETR的算法中经过充分研究的操作。TAPTRv2通过解决一个关键问题来改进TAPTR，该问题涉及TAPTR对代价量的依赖，代价量会污染点查询的内容特征，并对可见性预测和代价量计算产生负面影响。在TAPTRv2中，我们提出了一种新的基于注意力的位置更新（APU）操作，并使用键感知可变形注意力来实现。对于每个查询，此操作使用键感知注意力权重来组合其对应的可变形采样位置，以预测新的查询位置。这种设计基于以下观察：局部注意力本质上与代价量相同，两者都是通过查询与其周围特征之间的点积来计算的。通过引入这一新操作，TAPTRv2不仅消除了代价量计算的额外负担，而且还带来了实质性的性能提升。TAPTRv2超越了TAPTR，并在许多具有挑战性的数据集上实现了最先进的性能，证明了其优越性。||
|**2024-07-23**|[HSVLT: Hierarchical Scale-Aware Vision-Language Transformer for Multi-Label Image Classification](http://arxiv.org/abs/2407.16244)|null|多标签图像分类的任务是识别单个图像中的多个对象。考虑到标签中包含的有价值的语义信息和图像中呈现的基本视觉特征，紧密的视觉-语言交互在提高分类性能方面起着至关重要的作用。此外，鉴于单个图像中对象大小和外观的潜在差异，关注不同尺度的特征有助于发现图像中可能的对象。近年来，基于Transformer的方法通过利用建模远程依赖的优势，在多标签图像分类方面取得了巨大成功，但它们也有一些局限性。首先，现有方法将视觉特征提取和跨模态融合视为独立的步骤，导致联合语义空间中的视觉-语言对齐不足。此外，它们仅提取视觉特征并在单一尺度上执行跨模态融合，忽略了具有不同特征的对象。为了解决这些问题，我们提出了一种具有两个吸引人设计的层次化尺度感知视觉-语言Transformer（HSVLT）：(1) 一种层次化的多尺度架构，其中包含一个跨尺度聚合模块，该模块利用从多个尺度提取的联合多模态特征来识别图像中大小和外观各异的对象。(2) 交互式视觉-语言注意力，这是一种新颖的注意力机制模块，它紧密地集成了跨模态交互，从而能够对视觉、语言和多模态特征进行联合更新。我们已经在三个基准数据集上评估了我们的方法。实验结果表明，HSVLT以较低的计算成本超越了最先进的方法。||
|**2024-07-23**|[Channel-Partitioned Windowed Attention And Frequency Learning for Single Image Super-Resolution](http://arxiv.org/abs/2407.16232)|null|近年来，基于窗口的注意力机制在计算机视觉任务中展现出巨大潜力，尤其是在单图像超分辨率（SISR）方面。然而，它可能无法捕捉远距离标记之间的长距离依赖关系。此外，我们发现仅在空间域学习无法传达图像的频率内容，而频率内容是 SISR 的一个关键方面。为了解决这些问题，我们提出了一种新的通道分区注意力Transformer（CPAT），通过沿特征图的高度和宽度顺序扩展窗口来更好地捕捉长距离依赖关系。此外，我们还提出了一种新颖的空间-频率交互模块（SFIM），它结合了空间域和频率域的信息，以提供更全面的特征图信息。这包括关于频率内容的信息，并增强了整个图像的感受野。实验结果证明了我们提出的模块和架构的有效性。特别是，CPAT 超越了当前最先进的方法高达 0.31dB。||
|**2024-07-23**|[On the Benefits of Rank in Attention Layers](http://arxiv.org/abs/2407.16153)|null|基于注意力的机制在机器学习中被广泛使用，尤其是在 Transformer 模型中。然而，注意力矩阵的秩和注意力头的数量等超参数在该架构的所有实现中几乎都以相同的方式进行缩放，而缺乏理论上的 обоснование。在这项工作中，我们展示了注意力机制的秩和注意力头数量之间存在巨大的权衡。具体来说，我们提出了一个简单且自然的 ​​目标函数，该函数可以使用单个满秩注意力头表示任何上下文长度，但除非注意力头的数量是嵌入维度的指数级，否则即使对于较短的上下文长度，低秩注意力也无法逼近该目标函数。此外，我们证明了对于较短的上下文长度，增加深度允许使用低秩注意力逼近目标函数。对于较长的上下文，我们推测满秩注意力是必要的。最后，我们使用现成的 Transformer 模型进行实验，验证了我们的理论发现。||
|**2024-07-23**|[Diffusion Transformer Captures Spatial-Temporal Dependencies: A Theory for Gaussian Process Data](http://arxiv.org/abs/2407.16134)|null|作为 Sora 视频生成模型的支柱，扩散Transformer 成功扩展了扩散模型的容量，为高保真序列数据生成开辟了新途径。与图像等静态数据不同，序列数据由时间索引的连续数据帧组成，表现出丰富的时空依赖性。这些依赖性代表了潜在的动态模型，对于验证生成数据的有效性至关重要。在本文中，我们迈出了理论上的第一步，将扩散Transformer用于捕获时空依赖性。具体来说，我们针对具有不同衰减模式协方差函数的高斯过程数据，建立了扩散 Transformer 的分数逼近和分布估计保证。我们重点介绍了时空依赖性是如何被捕获的，以及它们如何影响学习效率。我们的研究提出了一种新颖的 Transformer 逼近理论，其中 Transformer 的作用是展开一种算法。我们通过数值实验支持了我们的理论结果，提供了强有力的证据表明时空依赖性在注意力层内被捕获，这与我们的逼近理论相一致。||
|**2024-07-22**|[Empirical Capacity Model for Self-Attention Neural Networks](http://arxiv.org/abs/2407.15425)|null|近年来，大型预训练自注意力神经网络，或称 Transformer，在各种任务中取得了巨大成功。模型在给定任务上的性能取决于其记忆和泛化训练数据的能力。拥有数十亿参数的大型 Transformer 模型理论上具有巨大的内容记忆能力。然而，目前用于优化的算法远未达到理论上的容量，而且容量也高度依赖于内容。在本文中，我们关注于使用常见的训练算法和合成训练数据获得的这些模型的记忆容量。基于结果，我们推导了一个适用于通用 Transformer 的经验容量模型 (ECM)。在可以定义任务目标记忆能力的情况下，ECM 可用于设计具有最佳参数数量的任务特定 Transformer 模型。||
|**2024-07-22**|[Towards Robust Vision Transformer via Masked Adaptive Ensemble](http://arxiv.org/abs/2407.15385)|null|对抗训练（AT）可以通过有意地将对抗样本注入训练数据来帮助提高视觉Transformer（ViT）对抗对抗攻击的鲁棒性。然而，这种对抗样本注入方式不可避免地会在一定程度上导致标准准确率下降，因此需要在标准准确率和鲁棒性之间进行权衡。此外，主流的对抗训练方案仍然容易受到自适应攻击的攻击。为了解决这些缺点，本文提出了一种新颖的ViT架构，包括一个检测器和一个分类器，它们通过我们新开发的自适应集成模块连接起来。具体来说，我们通过实验发现，检测对抗样本可以受益于引导反向传播技术。基于这一发现，我们引入了一种新的多头自注意力（MSA）机制来增强我们的检测器嗅探对抗样本的能力。然后，我们采用了一个带有两个编码器的分类器，分别从干净图像和对抗样本中提取视觉表示，并使用我们的自适应集成模块自适应地调整来自两个编码器的视觉表示的比例，以实现准确分类。这种设计使我们的ViT架构能够在标准准确率和鲁棒性之间取得更好的平衡。此外，我们的自适应集成技术允许我们屏蔽输入数据中随机的图像块子集，从而提高ViT对自适应攻击的鲁棒性，同时保持较高的标准准确率。实验结果表明，我们的ViT架构在CIFAR-10数据集上分别达到了90.3%和49.8%的最佳标准准确率和对抗鲁棒性。||
|**2024-07-22**|[Attention Beats Linear for Fast Implicit Neural Representation Generation](http://arxiv.org/abs/2407.15355)|**[link](https://github.com/roninton/anr)**|隐式神经表示（INR）作为一种数据表示方法越来越受欢迎，并已成为创新生成模型的先决条件。与推理效率较低的使用梯度的方法不同，采用超网络来生成多层感知器（MLP）中的参数（用于执行 INR 函数）已成为一种有前途且高效的替代方案。然而，作为一种全局连续函数，MLP 在对高度不连续的信号进行建模方面具有挑战性，导致训练阶段收敛缓慢且重建性能不准确。此外，MLP 需要大量的表示参数，这意味着数据表示效率低下。在本文中，我们提出了一种新颖的基于注意力的局部 INR（ANR），它由局部注意力层（LAL）和全局 MLP 组成，将坐标特征与数据特征相结合并将其转换为有意义的输出。随后，我们设计了一个实例表示框架，该框架提供了一个类似于 Transformer 的超网络，将数据实例表示为紧凑的表示向量。利用实例特定的表示向量和实例无关的 ANR 参数，目标信号可以很好地重建为连续函数。我们进一步解决了在获得超分辨率推理结果时使用变分坐标产生的混叠伪影。跨越四个数据集的大量实验展示了我们 ANR 方法的显著效果，例如在 CelebA 数据集上将 PSNR 值从 37.95dB 提高到 47.25dB。代码发布在 https://github.com/Roninton/ANR。||
|**2024-07-22**|[RoadPainter: Points Are Ideal Navigators for Topology transformER](http://arxiv.org/abs/2407.15349)|null|拓扑推理旨在提供对道路场景的精确理解，使自动驾驶系统能够识别安全高效的路线。在本文中，我们提出了RoadPainter，这是一种使用多视图图像检测和推理车道中心线拓扑结构的创新方法。RoadPainter背后的核心理念是从每个中心线掩码中提取一组点，以提高中心线预测的准确性。我们首先实现了一个Transformer解码器，它集成了混合注意力机制和真虚拟分离策略来预测粗略的车道中心线并建立拓扑关联。然后，我们在Transformer解码器预测的中心线点的引导下生成中心线实例掩码。此外，我们从每个掩码中导出另一组点，并将它们与先前检测到的中心线点组合以进行进一步细化。此外，我们引入了一个可选模块，该模块结合了标准定义（SD）地图，以进一步优化中心线检测并增强拓扑推理性能。在OpenLane-V2数据集上的实验评估证明了RoadPainter的最新性能。||
|**2024-07-22**|[Efficient Multi-disparity Transformer for Light Field Image Super-resolution](http://arxiv.org/abs/2407.15329)|null|本文介绍了多尺度视差Transformer (MDT)，这是一种为光场图像超分辨率（LFSR）量身定制的新型Transformer，它解决了传统方法中固有的对子孔径图像进行不加区分的处理所导致的计算冗余和视差纠缠问题。MDT采用多分支结构，每个分支利用独立的视差自注意力（DSA）机制来针对特定的视差范围，从而有效地降低了计算复杂度并解开了视差纠缠。在此架构的基础上，我们提出了LF-MDTNet，一种高效的LFSR网络。实验结果表明，LF-MDTNet在2倍和4倍尺度上分别比现有的最先进方法提高了0.37 dB和0.41 dB的PSNR，在参数更少、速度更快的情况下实现了优越的性能。||
|**2024-07-19**|[PolyFormer: Scalable Node-wise Filters via Polynomial Graph Transformer](http://arxiv.org/abs/2407.14459)|**[link](https://github.com/air029/polyformer)**|谱图神经网络在图表示学习方面表现出优异的性能。然而，许多当前的方法侧重于为所有节点使用共享的多项式系数，即学习节点统一的滤波器，这限制了滤波器对节点级任务的灵活性。最近的DSF试图通过基于位置编码学习节点级系数来克服这一限制。然而，位置编码的初始化和更新过程非常繁琐，阻碍了其在大规模图上的可扩展性。在这项工作中，我们提出了一种可扩展的节点级滤波器PolyAttn。利用注意力机制，PolyAttn可以高效地直接学习节点级滤波器，提供强大的表示能力。基于PolyAttn，我们引入了名为PolyFormer的完整模型。从图Transformer模型的角度来看，PolyFormer在节点内计算注意力分数，具有很高的可扩展性。此外，该模型捕获了频谱信息，在保持效率的同时增强了表达能力。凭借这些优势，PolyFormer为节点级任务提供了可扩展性和表达能力之间理想的平衡。大量实验表明，我们提出的方法在学习任意节点级滤波器方面表现出色，在同构图和异构图上均表现出优异的性能，并能处理包含多达1亿个节点的图。代码可在https://github.com/air029/PolyFormer获取。||
|**2024-07-19**|[TorchGT: A Holistic System for Large-scale Graph Transformer Training](http://arxiv.org/abs/2407.14106)|null|图神经网络变换器 (Graph Transformer) 是一种超越图神经网络 (GNN) 的新型图学习架构。尽管出现了鼓舞人心的算法进步，但它们的实际应用仍然有限，特别是在涉及数百万个节点的现实世界图上。我们观察到，现有的图神经网络变换器在处理大规模图时失败的主要原因是计算量大、可扩展性有限以及模型质量较差。基于这些观察，我们提出了 TorchGT，这是第一个高效、可扩展且准确的图神经网络变换器训练系统。TorchGT 在不同级别优化训练。在算法层面，TorchGT 利用图稀疏性，引入了一种计算效率高且精度保持的双重交织注意力机制。在运行时层面，TorchGT 通过轻量级通信的集群感知图并行机制跨工作节点扩展训练。在内核层面，弹性计算重构通过以动态方式减少内存访问延迟，进一步优化了计算。大量实验表明，TorchGT 可将训练速度提高 62.7 倍，并支持高达 100 万个节点的图序列长度。||
|**2024-07-18**|[DuoFormer: Leveraging Hierarchical Visual Representations by Local and Global Attention](http://arxiv.org/abs/2407.13920)|null|我们在此提出一种新颖的层次化Transformer模型，它巧妙地将卷积神经网络 (CNN) 的特征提取能力与视觉Transformer (ViT) 的高级表征潜力相结合。针对ViT缺乏归纳偏置和依赖大量训练数据集的问题，我们的模型采用CNN主干网络生成层次化的视觉表示。然后，这些表示通过一种创新的patch标记化方法适配为Transformer输入。我们还引入了一种“尺度注意力”机制，用于捕捉跨尺度依赖关系，补充patch注意力以增强空间理解并保留全局感知。我们的方法在小型和中等规模的医学数据集上显著优于基线模型，证明了其效率和泛化能力。这些组件被设计为即插即用的模块，适用于不同的CNN架构，并且可以适应多种应用。代码可在https://github.com/xiaoyatang/DuoFormer.git获取。||
|**2024-07-18**|[Attention in SRAM on Tenstorrent Grayskull](http://arxiv.org/abs/2407.13885)|**[link](https://github.com/moritztng/grayskull-attention)**|当Transformer的自注意力层实现使用SRAM而不是DRAM时，它们可以实现显著的加速。Tenstorrent Grayskull架构提供了一个大型SRAM，分布在核心网格中。这项工作为Grayskull提出了一个融合内核，通过结合矩阵乘法、注意力分数缩放和Softmax运算，专门利用其大型SRAM。此外，还提出了一个利用SRAM的专用Softmax内核和一个作为基线的CPU实现。在Grayskull上，从查询和键计算注意力权重时，Softmax运算占用了大部分运行时间。与CPU实现相比，专用Softmax内核的速度提升高达 $10\times$，并且融合内核内部的Softmax实现比专用Softmax内核快约$1.8\times$。所有实现的时间和内存复杂度都是序列长度的二次方。目前，对于普通用户来说，Grayskull e150比Nvidia H100 PCIe（最先进的GPU）便宜约$30\times$，并且SRAM容量约为其$1.5\times$ 。||
|**2024-07-18**|[Revisiting Attention for Multivariate Time Series Forecasting](http://arxiv.org/abs/2407.13806)|**[link](https://github.com/joeland4/fsatten-soatten)**|当前用于多元时间序列预测（MTSF）的Transformer方法都基于传统的注意力机制。它们涉及序列嵌入和对Q、K和V进行线性投影，然后在这个潜在空间内计算注意力。我们从未深入研究注意力机制来探索这种映射空间是否对MTSF是最优的。为了探究这个问题，本研究首先提出了一种基于频域空间的新型注意力机制——频谱注意力（FSatten）。它采用傅里叶变换进行嵌入，并引入了多头频谱缩放（MSS）来代替传统的Q和K线性映射。FSatten可以准确捕捉序列之间的周期性依赖关系，并且在不改变主流架构的情况下优于传统的注意力机制。我们进一步设计了一种更通用的方法，称为缩放正交注意力（SOatten）。我们提出了一种基于邻域相似性偏差的正交嵌入和头部耦合卷积（HCC），以指导模型学习全面的依赖模式。实验表明，FSatten和SOatten优于使用传统注意力的SOTA模型，使其成为MTSF基本注意力机制的良好替代方案。代码和日志文件将在以下地址发布：https://github.com/Joeland4/FSatten-SOatten。||
|**2024-07-18**|[GPSFormer: A Global Perception and Local Structure Fitting-based Transformer for Point Cloud Understanding](http://arxiv.org/abs/2407.13519)|**[link](https://github.com/changshuowang/GPSFormer)**|尽管预训练方法在点云理解方面取得了显著进展，但如何在不依赖外部数据的情况下直接从不规则点云中捕获复杂的形状信息仍然是一个巨大的挑战。为了解决这个问题，我们提出了GPSFormer，一种创新的基于全局感知和局部结构拟合的Transformer，它能够以极高的精度从点云中学习详细的形状信息。GPSFormer的核心是全局感知模块（GPM）和局部结构拟合卷积（LSFConv）。具体来说，GPM利用自适应可变形图卷积（ADGConv）识别特征空间中相似特征之间的短程依赖关系，并采用多头注意力机制（MHA）学习特征空间内所有位置之间的长程依赖关系，最终实现上下文表示的灵活学习。受泰勒级数的启发，我们设计了LSFConv，它可以从显式编码的局部几何结构中学习低阶基本信息和高阶细化信息。通过将GPM和LSFConv作为基本组件集成，我们构建了GPSFormer，这是一种能够有效捕获点云全局和局部结构的尖端Transformer。大量实验验证了GPSFormer在三个点云任务（形状分类、零件分割和少样本学习）中的有效性。GPSFormer的代码可在\url{https://github.com/changshuowang/GPSFormer}获取。||
|**2024-07-18**|[OAT: Object-Level Attention Transformer for Gaze Scanpath Prediction](http://arxiv.org/abs/2407.13335)|**[link](https://github.com/hkust-nisl/oat_ecc)**|视觉搜索在我们日常生活中非常重要。有效分配视觉注意力对于有效完成视觉搜索任务至关重要。先前的研究主要是在像素级别对图像中的视觉注意力空间分配进行建模，例如使用显著图。然而，新出现的证据表明，视觉注意力是由物体而非像素强度引导的。本文介绍了物体级注意力Transformer (OAT)，它可以预测人类在杂乱场景中搜索目标物体的扫描路径。OAT 使用编码器-解码器架构。编码器捕获图像中物体的位置和外观以及目标的信息。解码器通过整合编码器和解码器的输出特征，将注视扫描路径预测为一系列物体注视点。我们还提出了一种新的位置编码，可以更好地反映物体之间的空间关系。我们在亚马逊书籍封面数据集和我们收集的一个新的视觉搜索数据集上评估了 OAT。与基于空间注意力的算法相比，OAT 预测的注视扫描路径与人类注视模式更加一致，这在已建立的指标和基于行为的新指标上都得到了验证。我们的结果证明了 OAT 的泛化能力，因为它可以准确预测未见过的布局和目标物体的人类扫描路径。||
|**2024-07-18**|[Transformers with Stochastic Competition for Tabular Data Modelling](http://arxiv.org/abs/2407.13238)|null|尽管表格数据在众多行业和领域中普遍存在且意义重大，但在深度学习领域，对其探索相对不足。即使在今天，神经网络的性能也常常落后于梯度提升决策树 (GBDT) 等技术。然而，最近出现的一些模型开始缩小这一差距，在各种设置中都优于 GBDT，并在该领域引起了越来越多的关注。受此进展的启发，我们引入了一种专门为表格数据设计的新型随机深度学习模型。该模型的基础是基于 Transformer 的架构，通过策略性的架构修改和利用两种形式的随机竞争，精心调整以适应表格数据的独特属性。首先，我们采用随机“局部赢者通吃”单元，通过随机性和稀疏性来提高泛化能力。其次，我们引入了一种新颖的嵌入层，该层通过随机竞争机制从备选线性嵌入层中进行选择。我们在各种广泛使用且公开可用的数据集上验证了该模型的有效性。我们证明，通过结合这些元素，我们的模型实现了高性能，标志着深度学习在表格数据应用方面取得了重大进展。||
|**2024-07-18**|[Transformer-based Single-Cell Language Model: A Survey](http://arxiv.org/abs/2407.13205)|null|Transformer凭借其出色的并行处理能力和高度灵活的注意力机制，在自然语言处理领域取得了显著成就。此外，越来越多的基于Transformer的研究被提出用于对单细胞数据进行建模。在本综述中，我们试图系统地总结基于Transformer的单细胞语言模型及其应用。首先，我们详细介绍了Transformer的结构和原理。然后，我们回顾了用于单细胞数据分析的单细胞语言模型和大型语言模型。此外，我们还探讨了单细胞语言模型的数据集及其在下游任务中的应用，如批次校正、细胞聚类、细胞类型注释、基因调控网络推断和扰动反应。最后，我们讨论了单细胞语言模型面临的挑战，并提出了有前景的研究方向。我们希望这篇综述能够为对单细胞语言模型方向感兴趣的研究人员提供最新的参考。||
|**2024-07-18**|[Unified-EGformer: Exposure Guided Lightweight Transformer for Mixed-Exposure Image Enhancement](http://arxiv.org/abs/2407.13170)|null|尽管人工智能在图像处理方面取得了长足进步，但在监控和摄影等许多现实场景中至关重要的混合曝光问题仍然没有得到充分解决。传统的图像增强技术和当前的transformer模型主要关注过度曝光或曝光不足，存在局限性。为了弥合这一差距，我们引入了统一曝光引导transformer（Unified-EGformer）。我们提出的解决方案建立在先进的transformer架构之上，配备了局部像素级细化和全局细化模块，用于色彩校正和图像整体调整。我们采用引导注意力机制来精确识别曝光受损区域，确保其适应各种现实条件。U-EGformer采用轻量级设计，内存占用（峰值内存）仅为约1134 MB（10万参数），推理时间为95毫秒（比平均速度快9.61倍），是监控和自动驾驶等实时应用的可行选择。此外，我们的模型具有高度的泛化能力，只需最少的微调即可处理多个任务和数据集，并使用单一架构。||
|**2024-07-17**|[Hybrid Dynamic Pruning: A Pathway to Efficient Transformer Inference](http://arxiv.org/abs/2407.12893)|null|在深度学习领域，Transformer 模型变得非常重要，其在从语言理解到图像识别等许多领域都带来了进步，涵盖了广泛的应用。尽管取得了成功，但由于其二次计算强度和内存需求，在实时应用（尤其是边缘设备）中部署这些模型带来了重大挑战。为了克服这些挑战，我们引入了一种新颖的混合动态剪枝 (HDP)，这是一种高效的算法-架构协同设计方法，它利用头部稀疏性、块稀疏性和近似机会来加速 Transformer，以减少注意力计算和减少内存访问。通过观察注意力分数和注意力头中的巨大冗余，我们提出了一种新颖的基于整数的行平衡块剪枝方法，在运行时剪枝注意力矩阵中不重要的块，还提出了基于整数的头部剪枝方法，在运行早期阶段检测和剪枝不重要的头部。我们还提出了一种减少注意力计算的近似方法。为了以更低的延迟和更高的功率效率有效地支持这些方法，我们提出了一种 HDP 协处理器架构。||
|**2024-07-17**|[Global-Local Similarity for Efficient Fine-Grained Image Recognition with Vision Transformers](http://arxiv.org/abs/2407.12891)|**[link](https://github.com/arkel23/GLSim)**|细粒度识别涉及对来自次级宏观类别的图像进行分类，由于类间差异小，这是一项具有挑战性的任务。为了克服这个问题，大多数方法都执行由特征提取主干网络支持的判别性特征选择，然后进行高级特征细化步骤。最近，许多研究表明视觉Transformer作为细粒度识别主干网络的潜力，但它们使用其注意力机制来选择判别性标记的计算成本可能很高。在这项工作中，我们提出了一种新颖且计算成本低的度量标准来识别图像中的判别区域。我们比较了由 CLS 标记（一种由 Transformer 用于分类的可学习标记）给出的图像全局表示与各个补丁的局部表示之间的相似性。我们选择与获得的裁剪区域具有最高相似性的区域，这些区域通过相同的 Transformer 编码器转发。最后，对原始表示和裁剪表示的高级特征进行进一步细化，以便进行更稳健的预测。通过广泛的实验评估，我们证明了所提出方法的有效性，在各种数据集上获得了良好的准确性。此外，与其他方法相比，我们的方法以低得多的计算成本实现了这些结果。代码和检查点可在以下网址获得：\url{https://github.com/arkel23/GLSim}。||
|**2024-07-17**|[CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision Transformer Inference](http://arxiv.org/abs/2407.12736)|null|视觉Transformer（ViT）代表了机器学习方法在计算机视觉领域的突破性转变。与传统方法不同，ViT采用在自然语言处理中广泛应用的自注意力机制来分析图像块。尽管ViT在建模视觉任务方面具有优势，但在硬件平台（尤其是现场可编程门阵列（FPGA））上部署ViT会带来相当大的挑战。这些挑战主要源于ViT的非线性计算以及高计算和内存需求。本文介绍了CHOSEN，这是一个软硬件协同设计框架，旨在解决这些挑战，并提供一个自动化框架，用于在FPGA上部署ViT，从而最大限度地提高性能。我们的框架建立在三个基本贡献之上：旨在最大化带宽的多内核设计，主要针对多DDR内存库的优势；表现出最小精度下降的近似非线性函数；以及对FPGA上可用逻辑块的有效利用；以及高效的编译器，通过提出一种用于设计空间探索的新算法来最大限度地提高计算内核的性能和内存效率，以找到实现最佳吞吐量和延迟的最佳硬件配置。与最先进的ViT加速器相比，CHOSEN在DeiT-S和DeiT-B模型上的吞吐量分别提高了1.5倍和1.42倍。||
|**2024-07-17**|[ARTEMIS: A Mixed Analog-Stochastic In-DRAM Accelerator for Transformer Neural Networks](http://arxiv.org/abs/2407.12638)|null|Transformer已成为自然语言处理（NLP）和计算机视觉的强大工具。通过注意力机制，与循环神经网络（RNN）和卷积神经网络（CNN）等传统方法相比，这些模型表现出显著的性能提升。然而，Transformer通常需要大量的执行时间，因为它们需要大量的计算和内存占用。内存内处理（PIM）和近内存计算（NMC）是加速Transformer的有前途的解决方案，因为它们提供了高计算并行性和内存带宽。然而，设计PIM/NMC架构来支持Transformer神经网络中各层之间需要移动的复杂操作和海量数据仍然是一个挑战。我们提出了ARTEMIS，一种用于Transformer模型的混合模拟-随机内存计算加速器。通过对传统DRAM阵列进行最小程度的修改，ARTEMIS通过使用一种新型DRAM金属-金属电容器支持随机计算进行乘法运算和时间模拟累加，从而有效地降低了Transformer模型执行的相关成本。我们的分析表明，与GPU、TPU、CPU和最先进的PIM Transformer硬件加速器相比，ARTEMIS表现出至少3.0倍的速度提升、1.8倍的能耗降低和1.9倍的能效提升。||
|**2024-07-17**|[Frequency Guidance Matters: Skeletal Action Recognition by Frequency-Aware Mixed Transformer](http://arxiv.org/abs/2407.12322)|null|近年来，transformer 在对骨架序列的长时依赖关系建模方面展现出巨大潜力，因此在骨架动作识别领域受到越来越多的关注。然而，现有的基于 transformer 的方法严重依赖于朴素的注意力机制来捕获时空特征，这在学习具有相似运动模式的判别性表示方面存在不足。为了应对这一挑战，我们引入了频率感知混合 Transformer (FreqMixFormer)，专门用于识别具有细微判别性运动的相似骨骼动作。首先，我们引入了一个频率感知注意力模块，通过将关节特征嵌入到频率注意力图中来解构骨架频率表示，旨在根据频率系数区分判别性运动。随后，我们开发了一种混合 transformer 架构，将空间特征与频率特征相结合，以对全面的频率-空间模式进行建模。此外，我们还提出了一个时间 transformer 来提取跨帧的全局相关性。大量实验表明，FreqMiXFormer 在 3 个流行的骨架动作识别数据集（包括 NTU RGB+D、NTU RGB+D 120 和 NW-UCLA 数据集）上优于 SOTA。||
|**2024-07-16**|[Hierarchical Separable Video Transformer for Snapshot Compressive Imaging](http://arxiv.org/abs/2407.11946)|**[link](https://github.com/pwangcs/hisvit)**|Transformer在解决视频快照压缩成像（SCI）的反问题方面取得了最先进的性能，该问题的病态性源于空间掩蔽和时间混叠的混合退化。然而，以前的Transformer缺乏对这种退化的洞察力，因此性能和效率有限。在这项工作中，我们定制了一个高效的重建架构，在早期层中没有时间聚合，并使用分层可分离视频Transformer（HiSViT）作为构建块。HiSViT由多组跨尺度可分离多头自注意力（CSS-MSA）和门控自调制前馈网络（GSM-FFN）以及密集连接构成，每个网络都在不同尺度的独立通道部分内进行，用于多尺度交互和远程建模。通过将空间操作与时间操作分离，CSS-MSA引入了一种归纳偏差，即在帧内而不是帧之间更加关注，同时节省了计算开销。GSM-FFN旨在通过门控机制和分解的时空卷积来增强局部性。大量实验表明，我们的方法在复杂性和参数相当或更少的情况下，性能优于以前的方法> 0.5 dB。源代码和预训练模型已发布在https://github.com/pwangcs/HiSViT。||
|**2024-07-16**|[Relaxing Graph Transformers for Adversarial Attacks](http://arxiv.org/abs/2407.11764)|null|已有研究表明，图神经网络 (GNN) 容易受到对抗性攻击。尽管图变换器 (GT) 在多个基准测试中优于消息传递 GNN，但其对抗性鲁棒性尚未得到探索。然而，攻击 GT 具有挑战性，因为它们的位置编码 (PE) 和特殊的注意力机制难以区分。我们通过针对基于 (1) 随机游走 PE、(2) 成对最短路径 PE 和 (3) 谱 PE 的三种代表性架构来克服这些挑战，并提出第一个针对 GT 的自适应攻击。我们利用攻击来评估 (a) 节点分类中结构扰动的鲁棒性；以及 (b) （假新闻）图分类的节点注入攻击。我们的评估表明，它们可能非常脆弱，并强调了我们工作的重要性以及自适应攻击的必要性。||
|**2024-07-16**|[Video-Language Alignment Pre-training via Spatio-Temporal Graph Transformer](http://arxiv.org/abs/2407.11677)|**[link](https://github.com/gxym/stgt)**|视频语言对齐是一项重要的多模态任务，有利于各种下游应用，例如视频文本检索和视频问答。现有方法要么利用视频文本对中的多模态信息，要么应用全局和局部对齐技术来提高对齐精度。然而，这些方法往往无法充分探索视频内视觉标记之间以及不同视频文本对之间时空关系。在本文中，我们提出了一种新颖的时空图Transformer模块，用于统一学习视频语言对齐预训练（称为STGT）的时空上下文。具体来说，我们的STGT将时空图结构信息与Transformer块中的注意力相结合，有效地利用了时空上下文。通过这种方式，我们可以对视觉标记之间的关系进行建模，提高视频文本对齐精度，从而有利于下游任务。此外，我们提出了一种自相似性对齐损失，以探索视频和文本中固有的自相似性。通过对比学习实现的初始优化，可以进一步提高视频和文本之间的对齐精度。在包括视频文本检索和视频问答在内的具有挑战性的下游任务上的实验结果证明了我们方法的优越性能。||
|**2024-07-16**|[Probing the Efficacy of Federated Parameter-Efficient Fine-Tuning of Vision Transformers for Medical Image Classification](http://arxiv.org/abs/2407.11573)|null|随着大型预训练 Transformer 模型的出现，针对各种下游任务对这些模型进行微调成为一个关键问题。训练数据的缺乏、数据孤岛的存在以及严格的隐私限制加剧了医学影像领域微调问题的难度，对能够实现预训练模型协同微调的算法提出了强烈需求。此外，由于这些模型的规模庞大，需要使用参数高效微调（PEFT）来减少联邦学习中的通信负担。在这项工作中，我们系统地研究了各种联邦 PEFT 策略，用于使视觉 Transformer（ViT）模型（在大规模自然图像数据集上进行预训练）适应医学图像分类。除了评估已知的 PEFT 技术外，我们还引入了 PEFT 算法的联邦变体，例如视觉提示微调（VPT）、视觉提示的低秩分解、随机块注意力微调以及混合 PEFT 方法（如低秩适应（LoRA）+VPT）。此外，我们进行了全面的实证分析，以确定适用于联邦环境的最佳 PEFT 方法，并了解数据分布对联邦 PEFT 的影响，特别是对于域外（OOD）和非独立同分布（non-IID）数据。本研究的主要见解是，虽然大多数联邦 PEFT 方法在域内迁移中表现良好，但在处理 OOD 和非 IID 场景时，准确性和效率之间存在着巨大的权衡，而这在医学影像中很常见。具体而言，微调/交换参数每减少一个数量级，准确率就会下降 4%。因此，初始模型的选择对于联邦 PEFT 至关重要。如果可能的话，最好使用从域内医学图像数据中学习到的医学基础模型，而不是通用视觉模型。||
|**2024-07-16**|[Understanding Counting in Small Transformers: The Interplay between Attention and Feed-Forward Layers](http://arxiv.org/abs/2407.11542)|**[link](https://github.com/SPOC-group/counting-attention)**|我们对在直方图任务上训练的简单transformer模型进行了全面分析，该任务的目标是计算来自固定字母表的输入序列中每个项目的出现次数。尽管表面上很简单，但这项任务展现出丰富的现象学，使我们能够描述不同的架构组件如何促进不同算法解决方案的出现。特别是，我们展示了实现解决方案的两种性质不同的机制的存在，即基于关系和基于清单的计数。模型可以实现哪种解决方案，很大程度上取决于注意力机制、激活函数、记忆容量和序列开始标记的存在的精确选择。通过内省在计数任务上学习的模型，我们找到了这两种机制形成的证据。从更广泛的角度来看，我们的分析提供了一个框架，以理解transformer模型的不同架构组件的交互如何塑造不同的算法解决方案和近似值。||
|**2024-07-16**|[Not Another Imputation Method: A Transformer-based Model for Missing Values in Tabular Datasets](http://arxiv.org/abs/2407.11540)|**[link](https://github.com/cosbidev/naim)**|在训练和测试人工智能模型时，处理表格数据集中的缺失值是一项重大挑战，这个问题通常使用插补技术来解决。在此，我们介绍“非另一种插补方法”（NAIM），这是一种基于Transformer的新型模型，专门设计用于解决此问题，而无需传统的插补技术。NAIM 采用特征特定的嵌入和掩蔽自注意力机制，可以有效地从可用数据中学习，从而避免了对缺失值进行插补的必要性。此外，还引入了一种新的正则化技术，以增强模型从不完整数据中泛化的能力。我们在 5 个公开可用的表格数据集上对 NAIM 进行了广泛评估，证明其性能优于 6 种最先进的机器学习模型和 4 种深度学习模型，并在必要时将每种模型与 3 种不同的插补技术配对。结果突出了 NAIM 在存在缺失数据的情况下提高预测性能和鲁棒性的功效。为了促进在没有传统插补方法的情况下处理缺失数据的进一步研究和实际应用，我们在 https://github.com/cosbidev/NAIM 上提供了 NAIM 的代码。||
|**2024-07-16**|[Haze-Aware Attention Network for Single-Image Dehazing](http://arxiv.org/abs/2407.11505)|null|单图像去雾是计算机视觉中的一项关键挑战，旨在去除图像中的雾霾并恢复清晰的背景细节。认识到传统的基于物理模型的方法的局限性和当前基于注意力的解决方案的低效率，我们提出了一种新的去雾网络，它结合了创新的雾霾感知注意力模块 (HAAM) 和多尺度频率增强模块 (MFEM)。HAAM 受大气散射模型的启发，将物理原理巧妙地融入到高维特征中，以实现目标去雾。它在图像恢复过程中提取潜在特征，从而显着提升指标，而 MFEM 有效地增强了高频细节，从而避免了小波或傅里叶变换的复杂性。它采用多尺度场来提取和强调关键频率成分，同时最大限度地减少参数开销。我们的雾霾感知注意力网络 (HAA-Net) 集成了一个简单的 U-Net 框架，用于单图像去雾，在效率和有效性方面明显优于现有的基于注意力和变压器的模型。HAA-Net 在各种公共数据集上进行了测试，树立了新的性能基准。我们的工作不仅推动了图像去雾领域的发展，而且还为更广泛的计算机视觉应用中的注意力机制设计提供了见解。||
|**2024-07-16**|[RIMformer: An End-to-End Transformer for FMCW Radar Interference Mitigation](http://arxiv.org/abs/2407.11459)|null|调频连续波 (FMCW) 雷达在遥感领域发挥着举足轻重的作用。随着 FMCW 雷达部署密度的增加，相互干扰也随之加剧，这削弱了雷达的探测能力，并威胁到系统的可靠性和安全性。本文提出了一种基于Transformer的端到端 FMCW 雷达干扰抑制 (RIM) 新方法，称为 RIMformer。在 RIMformer 中，提出了一种双多头自注意力机制来捕获中频 (IF) 信号不同距离元素之间的相关性。此外，还集成了改进的卷积块，以利用卷积的强大功能来提取局部特征。该架构旨在以端到端的方式处理时域 IF 信号，从而避免了额外的手动数据处理步骤。改进后的解码器结构确保了网络的并行化，从而提高了计算效率。通过仿真和测量实验验证了该方法的准确性和有效性。结果表明，所提出的 RIMformer 可以有效地抑制干扰并恢复目标信号。||
|**2024-07-16**|[PADRe: A Unifying Polynomial Attention Drop-in Replacement for Efficient Vision Transformer](http://arxiv.org/abs/2407.11306)|null|我们提出了多项式注意力即插即用替换 (PADRe)，这是一个新颖且统一的框架，旨在替换 Transformer 模型中传统的自注意力机制。值得注意的是，最近出现的几种替代注意力机制，包括 Hyena、Mamba、SimA、Conv2Former 和 Castling-ViT，都可以被视为我们 PADRe 框架的特定实例。PADRe 利用多项式函数并借鉴了逼近论的既定结果，在不影响准确性的情况下提高了计算效率。PADRe 的关键组件包括乘法非线性，我们使用简单的、硬件友好的操作（例如 Hadamard 积）来实现这些操作，仅产生线性计算和内存成本。PADRe 进一步避免了使用 Softmax 等复杂函数的需要，但与传统的自注意力相比，它保持了相当或更高的准确性。我们评估了 PADRe 作为跨各种计算机视觉任务的自注意力替代品的有效性。这些任务包括图像分类、基于图像的 2D 对象检测和 3D 点云对象检测。实验结果表明，PADRe 的运行速度明显快于传统的自注意力（在服务器 GPU 和移动 NPU 上快 11 倍到 43 倍），同时在 Transformer 模型中替换自注意力时保持了相似的准确性。||
|**2024-07-15**|[Conquering images and the basis of transformative action](http://arxiv.org/abs/2407.11254)|null|我们快速沉浸在网络生活中，这让我们都病了。人工智能技术通过生成、个性化和传播迷人的图像，以令人作呕的精度和规模将大众的思想和心灵商品化。在线网络、人工智能 (AI)、社交媒体和数字新闻推送通过建立将我们的社区和身份细分和极化的叙述，来微调我们的信念和追求。与此同时，那些掌控着这些技术的人征服了我们内心生活、社会关系、地球和宇宙的最后疆域。在注意力经济中，我们的能动性受到限制，我们的活力因其自恋的追求和享乐而枯竭。生成式人工智能增强了那些使生命同质化和根除的力量，这不是通过某种愚蠢的“奇点”事件，而是通过贬低人类的创造力、劳动和社会生活。我们将使用一个破碎的镜头，来审视叙述和网络如何在心理、社会和算法层面影响我们。我们将讨论原子化的图像——疏远而不是激励个人的理想和追求——如何劫持人们的能动性，以维持毁灭他们的力量。我们将发现帝国是如何建立数字网络，以优化社会并鼓励自恋者强化社会二元对立，从而使消费、剥削和等级制度的不断扩张永久化。世界上的结构性等级制度通过我们信念和思想中的等级制度得到强化。只有将图像视为图像，并欣赏对立叙述之间的相似性，我们才能促进变革行动，并摆脱困扰我们生活的军国主义体系。||
|**2024-07-12**|[Region Attention Transformer for Medical Image Restoration](http://arxiv.org/abs/2407.09268)|**[link](https://github.com/yaziwel/region-attention-transformer-for-medical-image-restoration)**|基于Transformer的方法在医学图像恢复方面表现出令人印象深刻的结果，这归功于其在空间维度上的多头自注意力（MSA）机制。然而，大多数现有的Transformer在固定且粗略划分的区域（例如，整个图像或固定块）内进行注意力计算，导致来自不相关区域的干扰和连续图像内容的碎片化。为了克服这些挑战，我们引入了一种新的区域注意力Transformer（RAT），它利用了基于区域的多头自注意力机制（R-MSA）。R-MSA使用鲁棒的Segment Anything Model (SAM)将输入图像动态地划分为不重叠的语义区域，然后在这些区域内执行自注意力。这种区域划分更加灵活和可解释，确保只有来自相似语义区域的像素相互补充，从而消除了来自不相关区域的干扰。此外，我们引入了一个焦点区域损失来引导我们的模型自适应地关注恢复高难度区域。大量实验表明，RAT在各种医学图像恢复任务中是有效的，包括PET图像合成、CT图像去噪和病理图像超分辨率。代码可在https://github.com/Yaziwel/Region-Attention-Transformer-for-Medical-Image-Restoration.git获取。||
|**2024-07-12**|[DANIEL: A fast Document Attention Network for Information Extraction and Labelling of handwritten documents](http://arxiv.org/abs/2407.09103)|**[link](https://github.com/shulk97/daniel)**|从手写文档中提取信息传统上涉及三个不同的步骤：文档布局分析、手写文本识别和命名实体识别。最近的方法试图使用完全端到端的架构将这些步骤集成到一个过程中。尽管如此，当应用于纯文本信息提取时，这些集成方法的性能尚未与语言模型相匹配。在本文中，我们介绍了DANIEL（用于信息提取和标注的文档注意力网络），这是一种完全端到端的架构，集成了语言模型，专为全面理解手写文档而设计。DANIEL对整页文档执行布局识别、手写文本识别和命名实体识别。此外，它可以同时跨多种语言、布局和任务进行学习。对于命名实体识别，可以通过输入提示指定要应用的本体。该架构采用能够处理任何大小图像而无需调整大小的卷积编码器，并与基于transformer的语言模型的自回归解码器配对。DANIEL在四个数据集上取得了具有竞争力的结果，包括在RIMES 2009和M-POPP上手写文本识别和在IAM NER上命名实体识别方面的新SOTA性能。此外，DANIEL比现有方法快得多。我们在\url{https://github.com/Shulk97/daniel}上提供了源代码和训练模型的权重。||
|**2024-07-12**|[Beyond Image Prior: Embedding Noise Prior into Conditional Denoising Transformer](http://arxiv.org/abs/2407.09094)|**[link](https://github.com/yuanfeihuang/condformer)**|现有的基于学习的去噪方法通常训练模型从大规模数据集中泛化图像先验，但在现实世界场景中遇到的噪声分布变化时表现不佳。在这项工作中，我们通过强调噪声和图像先验之间的明显分离，为去噪挑战提出了一个新的视角。这一见解构成了我们开发条件优化框架的基础，旨在克服传统去噪框架的限制。为此，我们引入了一种局部噪声先验估计 (LoNPE) 算法，它可以直接从单个原始噪声图像中准确估计噪声先验。这种估计作为相机传感器成像环境的显式先验表示，与场景的图像先验不同。此外，我们设计了一个辅助可学习的 LoNPE 网络，专为 sRGB 噪声图像的实际应用而定制。利用估计的噪声先验，我们提出了一种新的条件去噪 Transformer (Condformer)，通过将噪声先验纳入条件自注意力机制。这种集成允许 Condformer 将优化过程分割成多个显式子空间，从而显着增强模型的泛化能力和灵活性。对合成数据集和真实世界数据集的广泛实验评估表明，所提出的方法比当前最先进的方法取得了优越的性能。源代码可在 https://github.com/YuanfeiHuang/Condformer 获取。||
|**2024-07-12**|[Global Attention-Guided Dual-Domain Point Cloud Feature Learning for Classification and Segmentation](http://arxiv.org/abs/2407.08994)|null|以往的研究已经证明了基于点的点云分析任务神经网络模型的有效性。然而，如何为原始点坐标生成有效的输入嵌入仍然是一个关键问题。此外，邻域聚合作为网络主干中的一个关键组件，其效率有限也是一个问题。在本文中，我们提出了一个全局注意力引导的双域特征学习网络（GAD）来解决上述问题。我们首先设计了上下文位置增强型Transformer（CPT）模块，该模块配备了改进的全局注意力机制，以生成全局感知的输入嵌入，作为后续聚合的指导。然后，级联双域K近邻特征融合（DKFF）模块，通过新的双域特征学习进行有效的特征聚合，该学习同时考虑了局部几何关系和长距离语义联系。在多个点云分析任务（例如分类、零件分割和场景语义分割）上的大量实验表明，所提出的方法和设计的模块具有优越的性能。||
|**2024-07-12**|[Deep Attention Driven Reinforcement Learning (DAD-RL) for Autonomous Vehicle Decision-Making in Dynamic Environment](http://arxiv.org/abs/2407.08932)|null|由于与周围车辆的动态交互，城市环境中的自动驾驶汽车 (AV) 决策本质上具有挑战性。为了安全规划，自动驾驶汽车必须了解场景中各种时空交互的重要性。当代工作使用巨大的 Transformer 架构来编码交互，主要用于轨迹预测，导致计算复杂性增加。为了在不影响时空理解和性能的情况下解决这个问题，我们提出了简单的深度注意力驱动强化学习 (DADRL) 框架，该框架动态分配并将周围车辆的重要性纳入到自动驾驶汽车的强化学习驱动决策过程中。我们引入了一种以自动驾驶汽车为中心的时空注意力编码 (STAE) 机制，用于学习与不同周围车辆的动态交互。为了理解地图和路线环境，我们采用环境编码器从环境地图中提取特征。时空表示与环境编码相结合，提供了全面的状态表示。由此产生的模型使用软演员评论家 (SAC) 算法进行训练。我们在没有交通信号灯的 SMARTS 城市基准场景中评估了所提出的框架，以证明 DADRL 优于最近的最新方法。此外，消融研究强调了环境编码器和时空注意力编码器在实现卓越性能方面的重要性。||
|**2024-07-11**|[TractGraphFormer: Anatomically Informed Hybrid Graph CNN-Transformer Network for Classification from Diffusion MRI Tractography](http://arxiv.org/abs/2407.08883)|null|利用深度神经网络研究大脑连接和非影像表型之间的关系越来越受到关注。然而，卷积网络设计往往忽略了大脑白质网络的局部和全局特性。我们引入了 TractGraphFormer，这是一个专为弥散磁共振成像纤维束追踪设计的混合图CNN-Transformer深度学习框架。该模型利用了白质结构的局部解剖特征和全局特征依赖性。图CNN模块捕获白质几何形状和灰质连接，以聚合解剖学上相似的白质连接的局部特征，而Transformer模块使用自注意力机制来增强全局信息学习。此外，TractGraphFormer 包含一个注意力模块，用于解释预测性的白质连接。在性别预测测试中，TractGraphFormer 在儿童 (n=9345) 和年轻人 (n=1065) 的大型数据集中表现出强大的性能。总的来说，我们的方法表明，WM 中广泛的连接可以预测个体的性别，并且在两个数据集中都识别出一致的预测性解剖束。所提出的方法强调了整合局部解剖信息和全局特征依赖性以提高基于弥散磁共振成像纤维束追踪的机器学习预测性能的潜力。||
|**2024-07-11**|[Jet Tagging with More-Interaction Particle Transformer](http://arxiv.org/abs/2407.08682)|null|本研究介绍了一种名为 More-Interaction Particle Transformer (MIParT) 的新型深度学习神经网络，用于喷注标记。该框架结合了我们自主设计的 More-Interaction Attention (MIA) 机制，增加了粒子交互嵌入的维度。我们使用顶夸克标记和夸克-胶子数据集测试了 MIParT。结果表明，MIParT 不仅在准确率和 AUC 指标上与 LorentzNet 持平，而且在背景抑制方面显著优于 ParT 模型。具体而言，在顶夸克标记数据集上，当信号效率为 30% 时，其背景抑制率提高了约 25%，在夸克-胶子数据集上提高了 3%。此外，MIParT 仅需要 ParT 30% 的参数量和 47% 的计算复杂度，证明了在降低模型复杂度且无需在大型数据集上进行大量预训练的情况下，也能实现高性能。这些结果表明，MIParT 有潜力提升粒子物理学中喷注标记和事件识别的效率基准。||
|**2024-07-11**|[Latent Spaces Enable Transformer-Based Dose Prediction in Complex Radiotherapy Plans](http://arxiv.org/abs/2407.08650)|**[link](https://github.com/edwardwang1/ldformer)**|越来越多的证据表明，立体定向消融体部放射治疗 (SABR) 可用于治疗肺部多发癌灶。多病灶肺部 SABR 计划十分复杂，创建起来需要耗费大量资源。本研究提出了一种新颖的两阶段潜在变换器框架 (LDFormer)，用于预测不同病灶数量的肺部 SABR 计划的剂量分布。在第一阶段，将患者解剖信息和剂量分布编码到潜在空间中。在第二阶段，变换器学习从解剖潜在特征预测剂量潜在特征。修改因果注意力以适应不同数量的病灶。在病灶内部和周围的剂量适形度方面，LDFormer 优于最先进的生成对抗网络，并且在考虑重叠病灶时，性能差距会更大。LDFormer 可以在 30 秒内在消费级硬件上生成 3D 剂量分布预测，并有可能帮助医生做出临床决策、降低资源成本并加速治疗计划制定。||
|**2024-07-11**|[FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](http://arxiv.org/abs/2407.08608)|null|注意力机制作为Transformer架构的核心层，是大语言模型和长上下文应用的瓶颈。FlashAttention阐述了一种通过最小化内存读/写来加速GPU上注意力计算的方法。然而，它尚未利用最新硬件的新功能，FlashAttention-2在H100 GPU上仅实现了35%的利用率。我们开发了三种主要技术来加速Hopper GPU上的注意力计算：利用Tensor Core和TMA的异步性来(1) 通过warp专业化来重叠整体计算和数据移动，以及(2) 交错块级矩阵乘法和softmax操作，以及(3) 利用硬件对FP8低精度支持的块量化和非相干处理。我们证明，我们的方法FlashAttention-3在使用FP16时，在H100 GPU上实现了1.5-2.0倍的加速，达到高达740 TFLOPs/s（75%的利用率），而在使用FP8时，接近1.2 PFLOPs/s。我们验证了FP8 FlashAttention-3的数值误差比基线FP8注意力机制低2.6倍。||
|**2024-07-12**|[ADMM Based Semi-Structured Pattern Pruning Framework For Transformer](http://arxiv.org/abs/2407.08334)|null|自然语言处理(NLP)通过Transformer模型取得了巨大的成功。然而，该模型拥有数亿甚至数十亿个参数，这对其在个人电脑或小型服务器上的部署造成了巨大的负担。为了解决这个问题，我们可以使模型的权重矩阵更加稀疏，或者压缩注意力层。模式剪枝是最重要的剪枝方法之一，它允许在每个划分的模式块中选择固定数量的参数并将其剪枝。然而，模式剪枝的效果受到每层权重区域内稀疏性的严格限制。在本文中，我们首先介绍了基于交替方向乘子法(ADMM)的模式剪枝框架，以重塑激活映射的分布。具体来说，我们建议将Transformer上的模式剪枝公式化为一个约束优化问题，并使用ADMM来优化该问题。通过这种方式，初始的密集特征图被转换为区域稀疏的特征图。因此，我们可以基于模式剪枝方法在获得更高压缩率的同时获得更好的性能。此外，本文还提供了具有局部稀疏性的ADMM的理论推导。最后，我们还将提出的基于ADMM的框架扩展到量化上，以证明其泛化能力，并使用SR-STE来避免梯度消失问题。我们对GLUE数据集上的分类任务进行了广泛的实验。值得注意的是，我们实现了50%的压缩率，同时在GLUE数据集上保持了80.1的总分。||
|**2024-07-11**|[HDT: Hierarchical Document Transformer](http://arxiv.org/abs/2407.08330)|**[link](https://github.com/autonomousvision/hdt)**|在本文中，我们提出了分层文档Transformer（HDT），这是一种针对结构化分层文档量身定制的新型稀疏Transformer架构。此类文档在众多领域（包括科学、法律或医学）中极为重要。然而，大多数现有解决方案效率低下，并且无法利用文档固有的结构。HDT通过引入辅助锚标记并将注意力机制重新设计为稀疏的多级层次结构来利用文档结构。这种方法促进了不同级别标记之间的信息交换，同时保持了稀疏性，从而在利用文档结构作为归纳偏差的同时提高了计算和内存效率。我们通过开发一种考虑文档层次结构的新型稀疏注意力内核来解决实现HDT依赖于样本的分层注意力模式的技术挑战。正如我们的实验所示，利用文档中存在的结构信息可以加快收敛速度，提高样本效率，并在下游任务中获得更好的性能。||
|**2024-07-11**|[Improving Dental Diagnostics: Enhanced Convolution with Spatial Attention Mechanism](http://arxiv.org/abs/2407.08114)|null|深度学习已经成为医疗保健领域的一种变革性工具，通过分析复杂的影像数据，在牙科诊断方面取得了重大进展。本文提出了一种增强的 ResNet50 架构，并集成了 SimAM 注意力模块，以解决牙科影像中对比度有限的挑战，并在降低计算需求的同时优化深度学习性能。SimAM 模块被添加到第二个 ResNet 模块之后，通过捕获空间依赖性和增强重要特征来改进特征提取。我们的模型在各种特征提取技术中均表现出优异的性能，F1 分数达到 0.676，优于 VGG、EfficientNet、DenseNet 和 AlexNet 等传统架构。这项研究强调了我们的方法在提高牙科影像分析的分类准确性和鲁棒性方面的有效性，突出了深度学习在提高牙科诊断准确性和效率方面的潜力。像我们这样的高级人工智能模型的集成有望彻底改变牙科诊断，为改善患者结果和更广泛地采用人工智能技术做出贡献。||
|**2024-07-10**|[MambaVision: A Hybrid Mamba-Transformer Vision Backbone](http://arxiv.org/abs/2407.08083)|**[link](https://github.com/nvlabs/mambavision)**|我们提出了一种名为 MambaVision 的新型混合 Mamba-Transformer 骨干网络，该网络专为视觉应用而设计。我们的核心贡献包括重新设计 Mamba 公式，以增强其对视觉特征进行有效建模的能力。此外，我们对将视觉 Transformer (ViT) 与 Mamba 集成的可行性进行了全面的消融研究。我们的结果表明，在 Mamba 架构的最后几层配备多个自注意力块可以极大地提高模型捕获远程空间依赖关系的能力。基于我们的发现，我们引入了一系列具有分层架构的 MambaVision 模型，以满足各种设计标准。对于 ImageNet-1K 数据集上的图像分类任务，MambaVision 模型变体在 Top-1 准确率和图像吞吐量方面实现了新的最先进 (SOTA) 性能。在下游任务（如 MS COCO 和 ADE20K 数据集上的目标检测、实例分割和语义分割）中，MambaVision 优于规模相当的骨干网络，并展现出更优越的性能。代码：https://github.com/NVlabs/MambaVision。||
|**2024-07-10**|[Study on Aspect Ratio Variability toward Robustness of Vision Transformer-based Vehicle Re-identification](http://arxiv.org/abs/2407.07842)|null|视觉Transformer（ViT）在车辆重识别（ReID）任务中表现出色。然而，图像或视频输入的非正方形纵横比可能会严重影响重识别的性能。为了解决这个问题，我们在本文中提出了一种新的基于ViT的ReID框架，它融合了在各种纵横比上训练的模型。我们的主要贡献有三方面：（i）我们分析了VeRi-776和VehicleID数据集上的纵横比性能，并根据原始图像的纵横比指导输入设置。(ii)我们在ViT分块过程中引入了分块混合的图像内增强（由空间注意力分数引导），并实现了不均匀步幅以更好地匹配目标纵横比。(iii)我们提出了一种动态特征融合的ReID网络，增强了模型的鲁棒性。我们的ReID方法在VehicleID数据集上实现了91.0%的平均精度均值（mAP），显著优于最接近的现有技术水平（CAL）结果80.9%。||
|**2024-07-10**|[PosFormer: Recognizing Complex Handwritten Mathematical Expression with Position Forest Transformer](http://arxiv.org/abs/2407.07764)|**[link](https://github.com/sjtu-deepvisionlab/posformer)**|手写数学表达式识别（HMER）在数字化教育和自动化办公等人机交互场景中有着广泛的应用。近年来，采用编码器-解码器架构的基于序列的模型已被普遍用于解决这项任务，方法是直接预测表达式图像的LaTeX序列。然而，这些方法只是隐式地学习LaTeX提供的语法规则，由于复杂的结构关系和多样化的书写风格，可能无法描述符号之间的位置和层次关系。为了克服这一挑战，我们提出了一种用于HMER的位置森林转换器（PosFormer），它联合优化了两个任务：表达式识别和位置识别，从而明确地实现位置感知的符号特征表示学习。具体来说，我们首先设计了一个位置森林，将数学表达式建模为森林结构，并解析符号之间的相对位置关系。在不需要额外标注的情况下，每个符号在森林中都被分配了一个位置标识符，以表示其相对空间位置。其次，我们提出了一个隐式注意力校正模块，以便在基于序列的解码器架构中准确地捕捉HMER的注意力。大量实验验证了PosFormer的优越性，它始终优于最先进的方法，在单行CROHME 2014/2016/2019、多行M2E和复杂MNE数据集上分别获得了2.03%/1.22%/2.00%、1.83%和4.62%的提升，而没有增加任何延迟或计算成本。代码可在https://github.com/SJTU-DeepVisionLab/PosFormer获取。||
|**2024-07-10**|[SvANet: A Scale-variant Attention-based Network for Small Medical Object Segmentation](http://arxiv.org/abs/2407.07720)|**[link](https://github.com/anthonyweidai/SvANet)**|早期发现和准确诊断可以预测恶性疾病转化的风险，从而提高有效治疗的可能性。出现轻微症状和小面积感染区域是不祥的征兆，也是疾病早期诊断的首要任务。深度学习算法，如卷积神经网络（CNN），已被用于分割自然或医学物体，并显示出良好的效果。然而，由于 CNN 中的卷积和池化操作会导致信息丢失和压缩缺陷，因此分析图像中小区域的医学物体仍然是一个挑战。这些损失和缺陷随着网络深度的增加而变得越来越严重，特别是对于小型医学物体。为了应对这些挑战，我们提出了一种新的基于尺度变化注意力机制的网络 (SvANet)，用于医学图像中精确的小尺度物体分割。SvANet 由蒙特卡洛注意力机制、尺度变化注意力机制和视觉Transformer组成，它结合了跨尺度特征并减轻了压缩伪影，从而增强了对小型医学物体的辨别能力。定量实验结果表明，SvANet 具有优越的性能，在分割肾脏肿瘤、皮肤病变、肝脏肿瘤、息肉、手术切除细胞、视网膜血管和精子方面，平均 Dice 系数分别达到了 96.12%、96.11%、89.79%、84.15%、80.25%、73.05% 和 72.58%，这些物体在 KiTS23、ISIC 2018、ATLAS、PolypGen、TissueNet、FIVES 和 SpermHealth 数据集中所占的图像面积均不到 1%。||
|**2024-07-09**|[CAPformer: Compression-Aware Pre-trained Transformer for Low-Light Image Enhancement](http://arxiv.org/abs/2407.07056)|null|弱光图像增强(LLIE)随着手机摄影需求的激增而取得了进步，但许多现有方法忽略了压缩，而压缩是资源受限的手机摄影的一个关键问题。大多数LLIE方法忽视了这一点，阻碍了它们的有效性。在这项研究中，我们调查了JPEG压缩对弱光图像的影响，并揭示了由于暗区普遍存在低像素值，JPEG压缩导致大量信息丢失。因此，我们提出了压缩感知预训练Transformer (CAPformer)，采用一种新的预训练策略，从未压缩的弱光图像中学习无损信息。此外，提出的亮度引导自注意力(BGSA)机制增强了合理的信息收集。实验表明，我们的方法在减轻压缩对LLIE的影响方面具有优越性，展示了其在资源受限场景下改进LLIE的潜力。||
|**2024-07-09**|[ERQ: Error Reduction for Post-Training Quantization of Vision Transformers](http://arxiv.org/abs/2407.06794)|null|视觉Transformer（ViT）的训练后量化（PTQ）因其在模型压缩方面的效率而备受关注。然而，现有方法通常忽略了量化权重和激活之间错综复杂的相互依赖性，导致了相当大的量化误差。在本文中，我们提出了ERQ，一种精心设计的两步PTQ方法，用于依次减少由激活和权重量化引起的量化误差。ERQ首先引入了激活量化误差减少（Aqer），该方法策略性地将激活量化误差的最小化制定为岭回归问题，并通过使用全精度更新权重来解决该问题。随后，ERQ引入了权重量化误差减少（Wqer），该方法采用迭代方法来减轻由权重量化引起的量化误差。在每次迭代中，采用经验导出的高效代理来细化量化权重的舍入方向，并结合岭回归求解器来减少权重量化误差。实验结果证明了我们方法的有效性。值得注意的是，对于W3A4 ViT-S，ERQ在准确率方面比最先进的GPTQ高出22.36%。||
|**2024-07-09**|[Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules](http://arxiv.org/abs/2407.06677)|null|在Transformer中，是否总是需要从浅层到深层计算token？Vanilla Transformer及其变体的持续成功似乎给出了毫无疑问的“是”。然而，在这项工作中，我们试图打破这种深度有序的惯例，提出了一种名为模块混合（MoM）的新颖架构，其动机是基于一种直觉，即任何层，无论其位置如何，只要具备所需的处理能力，都可以用来计算token。MoM的构建始于一组有限的模块，这些模块由多头注意力机制和前馈网络定义，每个模块都由其独特的参数化来区分。然后，两个路由器迭代地从集合中选择注意力模块和前馈模块来处理token。这种选择在token的前向传递过程中动态扩展计算图，最终形成模块的组合。我们证明了MoM不仅为Transformer及其众多变体提供了一个统一的框架，而且还提供了一种灵活且可学习的方法来减少Transformer参数化中的冗余。我们使用OpenWebText预训练了各种MoM。实验结果表明，不同参数量的MoM在GLUE和XSUM基准测试中始终优于Vanilla Transformer。更有趣的是，在固定的参数预算下，与GPT-2-large相比，MoM-large能够将计算图的深度增加38%以上，从而在GLUE上获得1.4的绝对增益，在XSUM上获得1的绝对增益。另一方面，MoM-large还可以将深度减少60%以上，同时每层包含更多模块，与GPT-2-large相比，TFLOPs减少了16%，内存使用量减少了43%，同时保持了相当的性能。||
|**2024-07-09**|[CTRL-F: Pairing Convolution with Transformer for Image Classification via Multi-Level Feature Cross-Attention and Representation Learning Fusion](http://arxiv.org/abs/2407.06673)|null|Transformer因其强大的容量和全局处理能力，在计算机视觉领域受到越来越多的关注。然而，Transformer是数据密集型的，与卷积神经网络（ConvNets）相比，其泛化能力受到限制，特别是在数据有限的情况下进行训练时，因为它们缺乏ConvNets中存在的内置空间归纳偏差。在本文中，我们致力于将卷积和Transformer的优势结合起来，以完成图像分类任务。为此，我们提出了一种名为CTRL-F的新型轻量级混合网络，该网络通过表示学习融合和多级特征交叉注意力将卷积与Transformer配对。我们的网络包括一个卷积分支和一个名为多级特征交叉注意力（MFCA）的新型Transformer模块。MFCA模块对在不同卷积阶段获得的多级特征表示进行操作。它通过两个独立的Transformer分支处理从这些多级特征表示中提取的小块标记和大块标记，这两个分支通过交叉注意力机制进行通信和知识交换。我们使用称为自适应知识融合（AKF）和协作知识融合（CKF）的新型表示融合技术，将从卷积路径获得的局部响应与从MFCA模块获得的全局响应融合在一起。实验表明，无论是在大数据集上从头开始训练，还是在数据量少的情况下，我们的CTRL-F变体都能达到最先进的性能。例如，CTRL-F在Oxford-102 Flowers和PlantVillage数据集上从头开始训练时，Top-1准确率分别达到82.24%和99.91%，超过了最先进的模型，这体现了我们模型在图像分类任务上的鲁棒性。代码地址：https://github.com/hosamsherif/CTRL-F||
|**2024-07-09**|[Enhancing spatial auditory attention decoding with neuroscience-inspired prototype training](http://arxiv.org/abs/2407.06498)|null|空间听觉注意解码 (Sp-AAD) 技术旨在通过神经记录来确定多说话者场景中的听觉注意方向。尽管最近的 Sp-AAD 算法取得了成功，但其性能受到脑电图数据中特定于试验的特征的阻碍。本研究旨在针对这些特征提高解码性能。神经科学研究表明，空间听觉注意可以反映在不同频带脑电图能量的拓扑分布中。这一见解促使我们提出原型训练，这是一种受神经科学启发的 Sp-AAD 方法。该方法构建了具有增强的能量分布表示和减少的特定于试验的特征的原型，使模型能够更好地捕获听觉注意特征。为了实施原型训练，进一步提出了一种采用脑电图小波变换的 EEGWaveNet。详细的实验表明，采用原型训练的 EEGWaveNet 在各种数据集上均优于其他竞争模型，并且也验证了所提出方法的有效性。作为一种独立于模型架构的训练方法，原型训练为 Sp-AAD 领域提供了新的见解。||
|**2024-07-08**|[FGA: Fourier-Guided Attention Network for Crowd Count Estimation](http://arxiv.org/abs/2407.06110)|null|人群计数在城市规划、人群管理和公共安全等领域越来越具有社会意义。本文介绍了一种新颖的用于人群计数估计的注意力机制——傅里叶引导注意力（FGA），旨在解决现有基于卷积的注意力网络中全局模式捕获效率低下的问题。FGA 通过利用快速傅里叶变换 (FFT) 以及用于全局特征的空间注意力和用于半全局和局部特征的通道注意力卷积，有效地捕获了包括全尺度全局模式在内的多尺度信息。FGA 的架构涉及一种双路径方法：(1) 通过 FFT 处理全尺度全局特征的路径，允许在频域中高效提取信息，以及 (2) 使用传统卷积和通道注意力处理剩余特征图以获取半全局和局部特征的路径。这种双路径架构使 FGA 能够无缝集成频率和空间信息，增强其捕获不同人群模式的能力。我们将 FGA 应用于两个流行的人群计数工作 CSRNet 和 CANNet 的最后几层，以评估模块在基准数据集（如 ShanghaiTech-A、ShanghaiTech-B、UCF-CC-50 和 JHU++ 人群）上的性能。实验表明，基于均方误差 (MSE) 和平均绝对误差 (MAE) 指标，所有数据集都有显着改进，显示出与最近最先进方法相当的性能。此外，我们利用 Grad-CAM 热图，使用定性分析来说明可解释性，以显示 FGA 在捕获人群模式方面的有效性。||
|**2024-07-08**|[HiT-SR: Hierarchical Transformer for Efficient Image Super-Resolution](http://arxiv.org/abs/2407.05878)|null|Transformer 在包括图像超分辨率 (SR) 在内的计算机视觉任务中表现出了良好的性能。然而，流行的基于 Transformer 的 SR 方法通常采用计算复杂度与窗口大小呈二次关系的窗口自注意力机制，导致采用固定的较小窗口，感受野有限。在本文中，我们提出了一种将基于 Transformer 的 SR 网络转换为分层 Transformer (HiT-SR) 的通用策略，在保持高效设计的同时，利用多尺度特征提升 SR 性能。具体来说，我们首先将常用的固定小窗口替换为扩展的分层窗口，以聚合不同尺度的特征并建立远程依赖关系。考虑到大窗口所需的密集计算，我们进一步设计了一种空间-通道相关性方法，该方法对窗口大小具有线性复杂度，可以有效地从分层窗口中收集空间和通道信息。大量实验验证了我们 HiT-SR 的有效性和效率，我们改进后的 SwinIR-Light、SwinIR-NG 和 SRFormer-Light 版本以更少的参数、FLOPs 和更快的速度（约 7 倍）获得了最先进的 SR 结果。||
|**2024-07-08**|[MSTF: Multiscale Transformer for Incomplete Trajectory Prediction](http://arxiv.org/abs/2407.05671)|null|运动预测在自动驾驶系统中起着至关重要的作用，它使车辆能够根据周围车辆的预测执行碰撞警告和合理的局部路径规划。然而，普遍的方法通常假设观察到的轨迹是完整的，而忽略了物体遮挡、范围限制和传感器故障导致的缺失值可能产生的影响。这种疏忽不可避免地会影响轨迹预测的准确性。为了应对这一挑战，我们提出了一个名为多尺度变换器（MSTF）的端到端框架，该框架专为不完整轨迹预测而精心设计。MSTF集成了多尺度注意力头（MAH）和基于信息增量的模式自适应（IIPA）模块。具体来说，MAH组件利用多头注意力机制，从不同的时间粒度同时捕获轨迹序列的多尺度运动表示。这种方法有助于在不同尺度上对运动中的全局依赖关系进行建模，从而减轻缺失值的不利影响。此外，IIPA模块通过分析数据中的缺失模式，自适应地提取跨时间步长的运动连续性表示。连续性表示在更高层次上描绘了运动趋势，引导MSTF生成与运动连续性一致的预测。我们使用两个大规模真实世界数据集评估了我们提出的MSTF模型。实验结果表明，MSTF在不完整轨迹预测任务中优于最先进（SOTA）模型，展示了其在解决自动驾驶系统运动预测中缺失值挑战方面的有效性。||
|**2024-07-08**|[Graph Attention with Random Rewiring](http://arxiv.org/abs/2407.05649)|null|图神经网络 (GNN) 已成为图结构深度学习的基础。现代 GNN 的关键范式包括消息传递、图重连和图 Transformer。本文介绍了具有随机结构的图重连注意力机制 (GRASS)，这是一种结合了这三种范式优点的新型 GNN 架构。GRASS 通过叠加随机正则图来重新连接输入图，增强了远程信息传播，同时保留了输入图的结构特征。它还采用了一种专为图结构数据量身定制的独特加性注意力机制，在保持计算效率的同时提供了图归纳偏差。我们的实证评估表明，GRASS 在多个基准数据集上实现了最先进的性能，证实了其实用性。||
|**2024-07-08**|[On the Power of Convolution Augmented Transformer](http://arxiv.org/abs/2407.05591)|null|Transformer架构在语言建模方面引发了革命性的进步。然而，最近的架构方法，例如状态空间模型，已经弥合了性能差距。受此启发，我们研究了卷积增强型Transformer（CAT）在召回、复制和长度泛化任务中的优势。CAT在注意力层的K/Q/V嵌入中加入了卷积滤波器。通过CAT，我们展示了卷积的局部性与注意力的全局视图协同作用。与诸如Mamba或Transformer等类似架构不同，CAT可以使用单层可证明地解决关联召回（AR）和复制任务，同时还享有保证的长度泛化能力。我们还通过描述卷积如何通过总结上下文窗口和创建突出的摘要标记来参与注意力，从而减轻对完全注意力的需求，从而建立了卷积和注意力之间的计算权衡。对真实数据集的评估证实了我们的发现，并证明CAT及其变体确实增强了语言建模性能。||
|**2024-07-05**|[Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations](http://arxiv.org/abs/2407.04543)|**[link](https://github.com/namednil/step)**|模型需要适当的归纳偏置才能有效地从少量数据中学习并在训练分布之外进行系统泛化。 虽然 Transformer 具有高度的通用性和强大的功能，但它们仍然可以从增强的结构性归纳偏置中受益，以完成 seq2seq 任务，尤其是那些涉及句法转换的任务，例如将主动语态转换为被动语态或语义解析。在本文中，我们建议通过中间预训练来增强 Transformer 的结构性归纳偏置，以根据转换的描述对依存树执行合成生成的句法转换。我们的实验表明，这有助于对组块等句法任务进行少样本学习，并且还提高了语义解析的结构泛化能力。我们的分析表明，中间预训练会导致注意力头能够跟踪需要对哪些标记应用哪些句法转换，并且模型可以在下游任务中利用这些注意力头。||
|**2024-07-05**|[LayerShuffle: Enhancing Robustness in Vision Transformers by Randomizing Layer Execution Order](http://arxiv.org/abs/2407.04513)|null|由于人工神经网络的架构和训练方式，它们通常对测试时的剪枝、替换或打乱层顺序的操作缺乏鲁棒性。然而，这些特性对于不同的应用场景非常重要，例如在分布式神经网络架构中，执行顺序无法得到保证，或者网络的某些部分在推理过程中可能发生故障。为了解决这些问题，我们提出了一系列针对视觉Transformer的训练方法，其最重要的组成部分是在训练时随机化注意力模块的执行顺序。我们证明，使用我们提出的方法，假设可以容忍在相同模型规模下精度降低（约20%），视觉Transformer确实能够适应测试时任意层的执行顺序。我们还发现，我们训练的模型可以彼此随机合并，生成功能完备的“科学怪人”模型，并且与源模型相比，性能没有损失。最后，我们在测试时对模型进行层剪枝，发现其性能下降平缓。||
|**2024-07-05**|[Hard-Attention Gates with Gradient Routing for Endoscopic Image Computing](http://arxiv.org/abs/2407.04400)|null|为了解决胃肠道息肉大小评估中的过拟合问题并增强模型泛化能力，我们的研究引入了特征选择门（FSG）或硬注意力门（HAG）以及梯度路由（GR）来进行动态特征选择。该技术旨在通过促进稀疏连接来增强卷积神经网络（CNN）和视觉Transformer（ViT），从而减少过拟合并增强泛化能力。HAG 通过使用可学习权重的稀疏化来实现这一点，作为一种正则化策略。GR 通过独立于主模型的两次前向传递优化 HAG 参数，进一步完善了这一过程，以改进特征重新加权。我们的评估涵盖了多个数据集，包括用于广泛影响评估的 CIFAR-100 和专注于息肉大小估计的专业内窥镜数据集（REAL-Colon、Misawa 和 SUN），涵盖超过 370,000 帧图像中的 200 多个息肉。研究结果表明，我们增强的 HAG 网络大大提高了与息肉大小相关的二分类和三分类任务的性能。具体而言，CNN 在二分类中的 F1 分数提高到 87.8%，而在三分类中，ViT-T 模型的 F1 分数达到 76.5%，优于传统的 CNN 和 ViT-T 模型。为了促进进一步的研究，我们发布了代码库，其中包括 CNN、多流 CNN、ViT 和 HAG 增强变体的实现。该资源旨在标准化内窥镜数据集的使用，为胃肠道息肉大小估计提供公开的训练-验证-测试拆分，以进行可靠和可比较的研究。代码库可在 github.com/cosmoimd/feature-selection-gates 获取。||
|**2024-07-05**|[Batch Transformer: Look for Attention in Batch](http://arxiv.org/abs/2407.04218)|null|人脸表情识别 (FER) 在计算机视觉领域受到了广泛关注，尤其是在人机交互等“自然环境”中。然而，FER 图像存在遮挡、低分辨率、姿态变化、光照变化和主观性等不确定因素，其中包括一些与目标标签不匹配的表情。因此，从一张包含噪声的单个图像中获取的信息很少，而且不可信。这可能会显著降低 FER 任务的性能。为了解决这个问题，我们提出了一种批量转换器 (BT)，它包含所提出的类别批量注意力 (CBA) 模块，通过训练一批图像中反映的特征，而不是来自单个图像的信息，来防止噪声数据过拟合并提取可靠信息。我们还提出了多级注意力 (MLA) 机制，通过捕获每个级别之间的相关性来防止过度拟合特定特征。在本文中，我们提出了一个结合上述方案的批量转换器网络 (BTN)。在各种 FER 基准数据集上的实验结果表明，所提出的 BTN 在 FER 数据集中始终优于最先进的方法。代表性结果证明了所提出的 BTN 在 FER 中的前景。||
|**2024-07-04**|[Adaptive Step-size Perception Unfolding Network with Non-local Hybrid Attention for Hyperspectral Image Reconstruction](http://arxiv.org/abs/2407.04024)|null|深度展开方法和Transformer架构最近在高光谱图像（HSI）重建方面展现出良好的结果。然而，仍然存在两个问题：（1）在数据子问题中，大多数方法使用可学习参数表示步长。然而，对于不同的光谱通道，特征和真实值之间的误差是不相等的。(2) Transformer难以平衡感受野大小和像素级细节信息。为了克服上述缺点，我们提出了一种自适应步长感知展开网络（ASPUN），这是一种基于FISTA算法的深度展开网络，它使用自适应步长感知模块来估计每个光谱通道的更新步长。此外，我们设计了一个非局部混合注意力Transformer（NHAT）模块，用于充分利用Transformer的感受野优势。通过将NLHA插入非局部信息聚合（NLIA）模块，展开网络可以获得更好的重建结果。实验结果表明，我们的ASPUN优于现有的SOTA算法，并取得了最佳性能。||
|**2024-07-03**|[Towards Attention-based Contrastive Learning for Audio Spoof Detection](http://arxiv.org/abs/2407.03514)|null|视觉Transformer（ViT）在计算机视觉分类任务中取得了重大进展。最近，Gong等人（2021）将基于注意力的建模方法引入了几项音频任务。然而，使用ViT进行音频欺骗检测任务的研究相对较少。我们弥合了这一差距，并将ViT引入到这项任务中。基于对SSAST（Gong等人，2022）音频ViT模型进行微调的朴素基线模型实现了次优的等错误率（EER）。为了提高性能，我们提出了一种新颖的基于注意力的对比学习框架（SSAST-CL），该框架使用交叉注意力来辅助表示学习。实验表明，我们的框架成功地 disentangled 了真实和欺骗类别，并有助于学习更好的分类器来完成这项任务。通过适当的数据增强策略，在我们的框架上训练的模型在ASVSpoof 2021挑战赛中取得了具有竞争力的性能。我们提供了比较和消融研究来证明我们的观点。||
|**2024-07-03**|[STF: Sentence Transformer Fine-Tuning For Topic Categorization With Limited Data](http://arxiv.org/abs/2407.03253)|null|如今，从推文中进行主题分类引起了相当多的研究关注。由于这些研究工作，人们提出了不同的分类系统。然而，由于标记数据的数量有限，导致性能指标低下，它们面临着重大挑战。我们提出了句子转换器微调 (STF)，这是一个主题检测系统，它利用预训练的句子转换器模型和微调来准确地对推文主题进行分类。此外，我们还进行了广泛的参数敏感性分析，以针对我们的主题分类任务微调 STF 参数，从而获得最佳性能结果。在两个基准数据集上的实验表明：(1) 所提出的 STF 可以有效地用于对推文主题进行分类，并且优于最新的最先进方法，(2) 所提出的 STF 不需要大量的标记推文就能达到良好的准确性，而这是许多最先进方法的局限性。我们的主要贡献是通过应用预训练的句子转换器语言模型在推文主题分类方面取得了可喜的成果。||
|**2024-07-03**|[Visual Grounding with Attention-Driven Constraint Balancing](http://arxiv.org/abs/2407.03243)|null|与目标检测不同，视觉定位任务需要检测由复杂的自由形式语言描述的对象。为了同时对这种复杂的语义和视觉表示进行建模，最近最先进的研究采用基于Transformer的模型来融合来自两种模态的特征，并进一步引入了各种模块来调制视觉特征，使其与语言表达对齐并消除不相关的冗余信息。然而，它们的损失函数仍然采用常见的目标检测损失，仅仅控制边界框回归输出，未能完全优化上述目标。为了解决这个问题，本文首先分析了基于Transformer模型的注意力机制。在此基础上，我们进一步提出了一个名为注意力驱动约束平衡（AttBalance）的新框架，以优化语言相关区域内视觉特征的行为。大量的实验结果表明，我们的方法带来了令人印象深刻的改进。具体来说，我们在四个不同基准上评估的五种不同模型上都取得了持续的改进。此外，通过将我们的方法集成到QRNet中，我们实现了新的最先进的性能。||
|**2024-07-03**|[Learning Disentangled Representation in Object-Centric Models for Visual Dynamics Prediction via Transformers](http://arxiv.org/abs/2407.03216)|null|最近的研究表明，以对象为中心的表示可以极大地提高学习动力学的准确性，同时也增强了可解释性。在这项工作中，我们将这一想法更进一步，提出了以下问题：“学习解耦表示能否进一步提高以对象为中心的模型中视觉动力学预测的准确性？” 虽然之前已经有一些尝试学习静态图像的解耦表示\citep{nsb}，但据我们所知，我们的工作是第一个尝试在视频的通用环境中做到这一点的工作，而没有对对象可能具有的属性类型做出任何特定假设。我们架构的关键构建块是“块”的概念，其中多个块共同构成一个对象。每个块都表示为给定数量的可学习概念向量的线性组合，并在学习过程中迭代优化。我们模型中的块是以无监督的方式发现的，通过类似于发现槽\citep{slot_attention}的方式关注对象掩码，以学习密集的以对象为中心的表示。我们在发现的块上采用 Transformer 进行自注意力机制来预测下一个状态，从而发现视觉动力学。我们在几个二维和三维基准数据集上进行了一系列实验，证明我们的架构 (1) 可以发现语义上有意义的块 (2) 与最先进的以对象为中心的模型相比，有助于提高动力学预测的准确性 (3) 在训练期间未见过特定属性组合的 OOD 设置中表现明显更好。我们的实验强调了发现解耦表示对于视觉动力学预测的重要性。||
|**2024-07-03**|[Relating CNN-Transformer Fusion Network for Change Detection](http://arxiv.org/abs/2407.03178)|**[link](https://github.com/nust-machine-intelligence-laboratory/rctnet)**|虽然深度学习，特别是卷积神经网络（CNN），已经彻底改变了遥感（RS）变化检测（CD），但现有方法由于忽略了全局上下文和不完整的变化学习，经常遗漏关键特征。此外，transformer 网络难以处理低级细节。RCTNet 通过引入以下内容解决了这些限制：\textbf{(1)} 早期融合骨干网络，用于尽早利用空间和时间特征；\textbf{(2)} 跨阶段聚合（CSA）模块，用于增强时间表示；\textbf{(3)} 多尺度特征融合（MSF）模块，用于在解码器中丰富特征提取；\textbf{(4)} 高效自解密注意力（ESA）模块，利用 Transformer 捕获全局信息和细粒度细节，以实现准确的变化检测。大量实验表明，RCTNet 明显优于传统的遥感图像变化检测方法，显示出显著的改进，并在准确性和计算成本之间取得了最佳平衡。||
|**2024-07-03**|[ISWSST: Index-space-wave State Superposition Transformers for Multispectral Remotely Sensed Imagery Semantic Segmentation](http://arxiv.org/abs/2407.03033)|null|目前，多光谱遥感图像(MSRSI) 语义分割任务面临以下问题：1) 通常只考虑单域特征（即空间域或频率域）；2) 编码器中的下采样操作通常会导致边缘提取精度损失；3) 没有充分考虑 MSRSI 的多通道特征；4) 没有充分利用遥感的先验知识。为了解决上述问题，受量子力学的启发，首次提出了一种用于 MSRSI 语义分割的指标-空间-波态叠加Transformer (ISWSST)，其优势如下：1) 通过自适应投票决策（即集成学习思想）叠加或融合指标、空间和波态来模拟量子叠加，从而成为更强大的分类器并提高分割精度；2) 设计了一种无损小波金字塔编码器-解码器模块，基于小波变换和逆小波变换对图像进行无损重建，模拟量子纠缠，避免边缘提取损失；3) 提出结合多光谱特征（即遥感指数和通道注意力机制）从原始分辨率图像中准确提取地面物体；4) 引入量子力学来解释 ISWSST 的潜在优势。实验表明，ISWSST 在 MSRSI 分割任务中得到了验证，并且优于最先进的架构，有效地提高了分割和边缘提取的精度。代码将在我们的论文被接受后公开。||
|**2024-07-03**|[Graph and Skipped Transformer: Exploiting Spatial and Temporal Modeling Capacities for Efficient 3D Human Pose Estimation](http://arxiv.org/abs/2407.02990)|null|近年来，单目三维人体姿态估计 (HPE) 中的 2D 到 3D 姿态提升引起了广泛的研究兴趣。基于 GNN 的方法和基于 Transformer 的方法由于其先进的空间和时间特征学习能力，已成为主流架构。然而，现有方法通常在空间和时间域中构建关节和帧注意力对齐，导致密集连接，从而引入相当大的局部冗余和计算开销。在本文中，我们采用全局方法来利用时空信息，并通过简洁的图和跳跃 Transformer 架构实现高效的 3D HPE。具体来说，在空间编码阶段，部署粗粒度身体部位以构建具有完全数据驱动自适应拓扑的空间图网络，确保模型在各种姿态下的灵活性和泛化能力。在时间编码和解码阶段，提出了一种简单而有效的跳跃 Transformer 来捕获长期时间依赖性并实现分层特征聚合。还开发了一种直接的数据滚动策略，将动态信息引入 2D 姿态序列。在 Human3.6M、MPI-INF-3DHP 和 Human-Eva 基准测试中进行了广泛的实验。G-SFormer 系列方法与以前的最先进技术相比，仅用大约 10% 的参数就实现了卓越的性能，并显着降低了计算复杂度。此外，G-SFormer 还表现出对检测到的 2D 姿态不准确性的出色鲁棒性。||
|**2024-07-03**|[ADFQ-ViT: Activation-Distribution-Friendly Post-Training Quantization for Vision Transformers](http://arxiv.org/abs/2407.02763)|null|视觉Transformer（ViT）在各种计算机视觉任务中都表现出色，但其庞大的参数量导致内存和计算需求显著增加，阻碍了其在资源受限设备上的有效推理。量化已成为缓解这些挑战的一种很有前景的解决方案，但现有方法在低比特情况下仍然存在显著的精度损失。我们将此问题归因于ViT中LayerNorm后和GELU后激活的独特分布，这使得传统的硬件友好型量化器效率低下，尤其是在低比特情况下。为了解决这个问题，我们提出了一种名为Activation-Distribution-Friendly post-training Quantization for Vision Transformers (ADFQ-ViT)的新颖框架。具体来说，我们引入了Per-Patch Outlier-aware Quantizer来处理LayerNorm后激活中的不规则异常值。该量化器在保持阈值以上最小值子集全精度的情况下，将均匀量化器的粒度细化到每个补丁级别。为了处理GELU后激活在正负区域之间的非均匀分布，我们设计了Shift-Log2 Quantizer，它将所有元素移位到正区域，然后应用log2量化。此外，我们提出了Attention-score enhanced Module-wise Optimization，通过重构误差来调整每个量化器的参数，以进一步减轻量化误差。大量实验表明，ADFQ-ViT在4比特图像分类、目标检测和实例分割任务中比各种基线都有显著改进。具体来说，在将ViT-B模型量化到4比特时，我们在ImageNet数据集上的Top-1准确率提高了10.23%。||
|**2024-07-02**|[A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models](http://arxiv.org/abs/2407.02646)|**[link](https://github.com/dakingrai/awesome-mechanistic-interpretability-lm-papers)**|机械可解释性 (MI) 是可解释性的一个新兴子领域，旨在通过逆向工程神经网络模型的内部计算来理解它。近年来，MI 在解释基于 Transformer 的语言模型 (LM) 方面引起了极大的关注，产生了许多新颖的见解，但也带来了新的挑战。然而，目前还没有工作全面回顾这些见解和挑战，特别是作为该领域新人的指南。为了填补这一空白，我们提供了一份全面的综述，概述了 MI 中的基本研究对象、用于其研究的技术、评估 MI 结果的方法，以及使用 MI 理解 LM 所产生的重要发现和应用。特别是，我们为初学者提供了一个路线图，以帮助他们在该领域导航并利用 MI 为自己谋福利。最后，我们还指出了该领域目前的差距，并讨论了未来可能的发展方向。||
|**2024-07-02**|[On the Anatomy of Attention](http://arxiv.org/abs/2407.02423)|null|我们引入一种范畴论图表形式体系，以便系统地关联和推理机器学习模型。我们的图表以直观的方式呈现架构，但不会丢失必要的细节，其中模型之间的自然关系通过图形变换来捕捉，并且重要的差异和相似性可以一目了然地识别出来。在本文中，我们将重点关注注意力机制：将民间传说转化为数学推导，并构建文献中注意力变体的分类法。作为以我们的形式主义为基础的实证研究的第一个例子，我们确定了注意力机制中反复出现的解剖学成分，我们对其进行了详尽的重组，以探索注意力机制的变化空间。||
|**2024-07-02**|[Efficient Sparse Attention needs Adaptive Token Release](http://arxiv.org/abs/2407.02328)|null|近年来，大型语言模型 (LLM) 在各种以文本为中心的的任务中展现出卓越的能力。然而，其“大”规模带来了巨大的计算和存储挑战，尤其是在管理 Transformer 的键值状态方面，这限制了其更广泛的适用性。因此，我们建议自适应地从缓存中释放资源并重建必要的键值状态。特别是，我们通过一个轻量级的控制器模块来近似理想的top- $K$稀疏注意力来实现这一点。该模块保留具有最高 top-$K$ 注意力权重的标记，并同时重建已丢弃但必要的标记，这些标记可能对未来的解码至关重要。自然语言生成和建模方面的综合实验表明，我们的方法不仅在性能方面与完全注意力机制相比具有竞争力，而且还实现了高达 221.8% 的显著吞吐量提升。复制代码可在 https://github.com/WHUIR/ADORE 上获得。||

